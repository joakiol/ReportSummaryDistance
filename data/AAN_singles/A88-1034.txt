CANONICAL  REPRESENTATION IN NLP  SYSTEM DESIGN=A CRIT ICAL  EVALUATIONKent Wittenburg and Jim BarnettMCC3500 West Balcones Center DriveAustin, TX 78759ABSTRACTThis paper is a critical evaluation of an approach to con-trol in natural language processing systems which makes useof canonical structures as a way of collapsing multipleanalyses in individual components.
We give an overviewhere of how the Lucy natural language interface system cur-rently realizes this control model and then evaluate what wetake to be the strengths and weaknesses of such an approach.In particular, we conclude that the use of canonical struc-tures can restrain combinatorial explosion in the search, butat the cost of breaking down the barriers between modulesand of letting processing concerns infect the declarativerepresentation of information.1 In t roduct ionThe traditional design for natural language processingsystems is one in which processing proceeds a sentence at atime, with syntactic analysis feeding subsequent semanticand discourse analysis in a "conduit" fashion, to borrow acharacterization used in a somewhat different setting by Ap-pelt (1982).
The basic advantages of this design stem fromthe fact that it is inherently modular: control is simple,modules can be developed and debugged independently.
Thema~n disadvantage from the processing point of view is thatthe search can explode as each module detects ambiguitiesthat cannot be resolved until later.
12 Although well-knownalternatives to conduit models exist -- the most obvious beingthe proposal to interleave syntax, semantics, and discourseprocessing -- we sense that the simplicity and modularity ofsome form of the conduit model continue to be the determin-ing design factor in most applied natural anguage systems todate, at least for those that can be said to have independentmodules to start with.
In this paper we will discuss the prosand cons of a design paradigm that stays within the basicconduit model.
It is a paradigm characterized by the at-tempt to procrastinate the resolution of ambiguity by meansof representing families of analyses with canonical represen-tations.The discussion will appear as follows.
In Section 2 we at-tempt to define and justify the use of canonical represen-tations, highlighting their appeal from a processing point ofview.
We then give a brief overview of how this paradigmhas been applied in the natural language interface prototype1For example, Martin, Church, and Patil (1981) mentionthat their phrase structure grammar produced 958 parses forthe naturally occurring sentence In as much a8 allocatingcosts i8 a tough job \[ would like to have the total co~tsrelated to each product.2Appelt (1982) mentions other problems with the conduitmodel having to do with its inability to account for inter-actions, .say, between linguistic choices and gestural ones inlanguage generation models.called Lucy (Rich et al 1987), Our main points appear inSection 4, where we assess the consequences of these designdecisions for three of the Lucy system modules.
The conclu-sion attempts to generalize from this experience.
Our mainpurpose here is thus to evaluate this general design paradigmby presenting a case history of that design applied in a par-ticular project.2 Canon ica l  representat ionsA frequent response to the problem of an explosion ofsyntactic parses in natural language systems is to have theparsing module assign canonical representations to familiesof structures.
Church (1980), Martin, Church, and Patil(1981), Marcus, Hindle, and Fleck (1983), Pereira (1983), Pul-man (1985), and Wittenburg (1987) have all advocated someform of this idea, which has sometimes gone under the nameof pseudo-attachment.
These canonical structures are un-ambiguous from the point of view of the parser/grammar,but have several different semantic translations when itcomes to intepretation.
The advantage of this approach isthat the semantics module might be able to choose quicklybetween the multiple translations, even though the syntaxcould not choose between the parses.
For example, instead ofenumerating all possible prepositional phrase attachments,the grammar could force a consistent attachment (either highor low or perhaps a flat n-ary branching tree) and returnonly a single purse for strings of multiple PPs.
Semanticprocessing could then expand the canonical structure andconsider the alternatives when it had the information eces-sary to choose among them.
Information that could help inthis delayed choice would be semantic translations of thenouns and verbs that carry constraints on their possiblemodifiers.
We will take such examples in syntax and seman-tics as paragon cases of the general design strategy that con-cerns us.Figures 1 and 2 present the paradigm in a moreschematic way.
Figure 1 shows a search space that branchesthree ways at the first two depths and two ways at depththree.
Imagine that that the search in Module 1 representsthe parsing of a particular sentence where two structures arethree-ways ambiguous, a third is two-ways ambiguous, and aparse exists for each combination of the three.
A semanticscomponent that took over in Module 2 would be faced withtranslating an exponentially growing number of parses, inthis case 18.
Underscores in Module 2 are intended torepresent ill-formedness from the perspective of this com-ponent; thus Figure 1 indicates that only one of the 18 inputsto Module 2 passes muster, i.e., only one of the parses is well-formed from a semantic point of view.If the grammar were changed by finding a single canoni-cal representation for each of the structures that are am-biguous in Figure 1, the search tree for the parsing of thissame sentence would be as shown in Figure 2.Then, as the semantics module takes over, it would beginenumerating the alternatives that each of these canonicalsyntactic structures actually represents.
As we indicate inFigure 2, we assume that there is sufficient (semantic) infor-mation available to immediately rule out unproductivebranches.
In the ideal case, the combinatorics of Figure 1may be completely circumvented, leaving the basic flow ofcontrol intact.253success of canonical strategies will thus be determined by theextent to which it is possible to 1) find well-motivatedrepresentations that allow painless recovery of the alter-natives, 2) choose the correct points to unpack the struc-tures, and 3) do 1 and 2 without undue cost to the rest of thesystem.Module AModule BFigure 1: Search in a conduit modelModule AModule BF igure  2: Search  in a canon ica l  mode lNote, however, that the canonical representationparadigm as we have presented it does not reduce the size ofthe overall search in the worst case.
The eanonicalized nodesin Module 1 of Figure 2 still have to be expanded in Module2 -- the expansion has merely been delayed.
But of coursesemantic information still may not be able to rule out thechoices, and if not, the same combinatories in Module 1 ofFigure 1 would appear in Module 2 of Figure 2.
Any gain inefficiency will come solely from being able to prune thesearch tree quickly because or the presence of informationthat would not have been available at an earlier stage.. The3 Canon ica l  s t ruc tures  inLucyLucy is a natural language interface prototype that hasbeen built by the Lingo group at MCC (Rich et al 1987).One of the alms has been to design an interface system thatis portable across applications, and thus strong modularityhas been one of the central design factors.
Figure 3 showsthe basic system design; it is a classic conduit model wherecontrol passes from syntax to semantics and thence to dis-course and pragmatics.SentenceParsing$?m~rff.lcs$?rrtanucsx[/---7"DiscourseprocessingF igure  3: The  Lucy  sys temIn many cases Lucy's parser produces a single parse foran input sentence.
The resulting structural description is thenunpacked and disambiguated in the semantics module.
AsFigure 3 shows, semantic processing in Lucy procedes in twostages.
In Stage 1, the semantic processing module rewritesthe parse output as a set of logical assertions.
The predicatesin these assertions are English words, taken from the parsetree, so that the output of this initial stage can be consideredto represvnt the uninterpreted predicational structure of thesentence, which abstracts away from the meaning of in-254dividual words.
3 In Stage 2 Lucy uses a set of semanticmapping rules to translate the Stage 1 assertions into thevocabulary of a knowledge base, and then considers thevarious interpretations, letting through only those that aresemantically consistent, where consistency is defined in termsof the knowledge base's class hierarchy.
(This amounts tochecking for semantic subcategorization restrictions.)
This in-terpreted logical form is suitable input for discourse andpragmatic processing, and, ultimately, for the backendprogram.The Lucy system uses canonical structures to deal withthe following types of ambiguity: semantic sense ambiguity,idiom recognition, noun-noun bracketing, prepositionalphrase attachment,  and quantifier scope assignment.
Wesummarize a few of these treatments here.
See Rich et al(1987) for more detail.Lucy assigns the same syntactic analysis to the literal andidiomatic readings of a sentence.
4 Thus the parser producesa canonical representation for idioms that amounts to a fullstructural description for the literal reading of the sentence.Then the Stage 1 procedure, which rewrites the parser outputinto logical assertions, uses an idiom dictionary to produceseparate sets of assertions for the idiomatic and literal read-ings.
Note that in this case the canonical structure must beexpanded quite soon.
This is because it is impossible to begintranslating assertions into the language of the knowledgebase without knowing whether the translation is to be literaland compositional or idiomatic and global.
Furthermore,producing a logical form involves making a commitmentabout how many objects we are talking about, and theidiomatic and literal readings may imply the existence of dif-ferent numbers of referents.
5 Thus, though idioms can passthrough the syntax untouched, they require an early commit-ment in semantics.In the case of noun compounds, the parser assigns acanonical right-branching structure which Stage 1 processingrewrites into a flat list of nouns.
Stage 2 processing is thenfree to assign to the compound any bracketing for which it3The design of this level of Lucy is influenced by Hobbs(1985), which advocates a level of "surfaey" logical formwith predicates close to actual English words and a structuresimilar to the syntactic structure of the sentence.4At present, Lucy can treat strings of adjectives andnouns as idioms, as well as verb/particle andverb/preposition compounds.
We've done experimental workthat indicates that there is no problem in extending thisapproach to handle full VP idioms, such as "kick thebucket, = but this functionality is not yet part of the system.5Lucy's logical form incorporates the notion of a dis-course referent (see Kamp (1984), Helm (1982)), and thecreation of a discourse referent implies the possibility ofanaphoric reference (within the range of accessibility of thereferent.)
Thus, when a noun phrase "a bucket" or Uthebucket = occurs, we normally can refer back to it with " i t ' ;however, if we use the idiom "kick the bucket" to mean"die' ,  no such anaphora is possible.
Hence idioms must bedetected before discourse referents are created.
As noted infootnote above, Lucy does not yet deal with full VP idiomslike Wkick the bucket, = but awareness of the effect suchidioms would have on our discourse processing strategy is anadditional argument for locating the idiom module relativelyearly in post-syntactic processing.can find an interpretation.
The semantic mapping rules con-tain compounding entries for nouns, allowing separatespecifications for the semantics of a noun as a head of a com-pound and as a modifier.
It is also possible for an entry tospecify that the "semantic head" of a compound should beflipped (e.g., in the case of "a stone l ion',  which is a stoneand not a lion.)
In this case, the canonical structure does nothave to be unpacked until a translation for the constituent isrequired.In the case of prepositions, the parser attaches them atthe highest point in the tree with an indication of theirdomain, i.e., the subtree within which they can be attached.The high attachment is not altered in Stage 1.
In Stage 2,after the nouns and verbs have been translated, Lucy at-tempts to attach prepositional phrases and other post-modifiers, checking to see which translations are consistentwith which attachments.
For example, in "I saw the boy onMonday," the parser would attach "on Monday = high, as-signing a structure indicating that both "saw" and "boy"were possible attachment sites.
The lexical entry for "on"would state that "(on x y)" can mean "(temporally-located-in x y)" if x is an event and y a day.
Lucy would then accept"saw-on" as a reading, but not "boy-on" (assuming there isno entry giving a reading for "(on x y)" where x is a personand y a date.)
In postponing PP attachment until the end ofthe semantic translation routine, Lucy assumes that I) thetranslations of the nouns and verbs are more likely to con-strain the readings and attachments of the prepositions thanvice-versa, and 2) that the resulting translation can be builtup piecemeal, with the translations of the PPs "added in" tothe translations of the nouns and verbs.
One result of thisstrategy is that verb/particle and verb/preposition com-pounds must be treated as idioms, since in these cases themeaning is not cumulative.
(It would be hard to assign in-dependently motivated meanings to "look" and "up" thatwould combine to give the meaning "look up" in "I lookedthe word up. '
)Finally, Lucy, like most other systems, does not assignquantifier scope either in the parse tree or in the first stagesof semantic processing; scope assignment is postponed untilthe Stage 2 translation into the language of the knowledgebase is completed.
64 Consequences  for themodu lesThe Lucy experiment has shown that it is possible topush the technique of canonical representations quite farindeed, thus maintaining the overall simplicity of a conduitcontrol model with a sentence as the basic unit of data.However, the consequences for the knowledge sources in-volved within each of the modules have been far-reaching.We next review some of those consequences for the grammar,for the syntax-semantics relations, and for those parts ofsemantics proper having to do with sortal consistency ofterms in the knowledge base.4 .1  The  grammarFor each of the phenomena discussed in the previous see-6At present, Lucy uses no knowledge except that con-tained in the class hierarchy.
Such information is not usefulfor determining quantifier scope, so Lucy gives a defaultleft-right assignment.255tion, the Lucy grammar (i.e., its syntactic lexicon and rulebase) was hand-tooled to pack a number of analyses into asingle canonical parse.
The goal we had been aiming for, infact, was to return only a single parse for any given inputsentence.
In some cases, the effects on the grammar wererelatively minor.
Forcing high attachment of PPs, for ex-ample, involved a slight augmentation of syntactic featurestructures in the categories and rules such that low attach-ments led to feature clashes when the parser tried to incor-porate such modified constituents into, say, a higher verbphrase.
Forcing right-branching analyses of noun-noun com-pounds was comparable.
However, where there were inter-actions involving lexical ambiguity, the canonicalization ofthe grammar had far more radical effects.
Interactionsamong subcategorization f potential phr~al  verb heads andambiguity between prepositions and particles provides onetelling example.
We give a brief history here of this case inorder to illustrate the kind of effects on declarative infor-mation that canonicalization can lead to.We began with the goal of finding a canonical form toconflate structures of the following sort since syntax alonewould have insufficient information to force a choice betweenthem:John looked lup the mountain 1John \[looked up I the mountainWhat  should the canonical form be in such a case?
Onecould either analyze such sentences as an intransitive verbfollowed by a PP or as a transitive verb + particle combina-tion.
We chose the former since the PP reading seems to bethe one more generally available, whereas the presence of aparticle reading depended on there being an entry in ouridiom dictionary, which in Lucy is accessed only after theparse is complete.
The semantic mapping rules we producedthen had to create two logical forms from the one canonicalanalysis; the first corresponded irectly to the PP structure,the other to the phrasal verb structure, even though that lat-ter structure as such was not present in the syntax.We explored several options for writing a grammar thatwould produce a PP bracketing, and only this bracketing, insuch cases.
The one we settled on led us to derivation trees7 like the following:7Lucy uses a form of categorial grammar in its syntacticcomponent.
FaG and fa> stand for backward functionapplication and forward function application, respectively.Function application is the basic binary reduction rule in thegrammar.
It applies a functor category, such as a verbwhich is looking for some argument, to a category that cansatisfy an argument role.
Pp-raising is a unary rule thatmakes S-modifiers out of basic PPs and fin is the unary rulethat lifts VPs to the category for finite verbs, adding thesubject argument.
See Wittenburg (1986) for details of Lucystyle grammars,  Uszkoreit (1986), Karttunen (1987), andZeevat, Klein, and Calder (1987) for related versions ofCategorial Unification Grammars.SS\S.
.
.
.
.
.
.
.
.
.
.
.
pp - r~ is lngS PP.
.
.
.
.
.
.
.
.
.
.
.
fa< .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fa>S \NP.
.
.
.
f lnNP VP PP /NP  NPJohn looked up ~he mounta lnIn order to be sure that  this was the ONLY parse givenby our grammar in such cases, we had to be sure that therewas no particle analysis for this same sentence.
However, wedid of course have to allow particle-type bracketings when noprepositional-type bracketing was available as, for instance,in sentences like =John looked it up= or =John caught up'.This we did by having prepositions, not verbs, always takethe NP  as an argument if there was ~t preposition/particleintervening between the verb and the NP  and by havingverbs take the NP  as an argument if there were nopreposition/particle intervening.
Particles then took a com-plete VP  as a left argument.
An  input sentence such as=John looked it up = thus produced the following uniquederivation, which was interpreted with the particle readingonly.SS\NP.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
f lnVP.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
f~<VP.
.
.
.
.
.
.
.
.
.
.
fa>NP VP/NP NP VP\VPJohn looked 1~ upNow our analysis was complete.
Our goals were ach-ieved.
But consider what the effects were on the grammar.In order to get an analysis for "John looked it up = we had toassign a transitive verb entry to "look ?, even though it wasreally only the two-word entry "look up = that was transitive,not ' look" itself.
In order to get an analysis for sentence.~like "John caught up" we had to assign an intransitive ntryto "caught ' ,  even though =caught up ' ,  not =caught ' ,  wasthe actual intransitive form.
Also, the analysis of verb par-ticles failed to reflect the fact that English particles do ap-pear between verbs and verb objects--in this grammar par-ticles were specifically excluded from this position in order toavoid particle/preposition ambiguities.
So our entire motiva-tion for grammatical analyses was now being driven by theneed to stamp out alternative derivations and no longer byprincipled linguistic concerns.
The casualties to the grammarincluded principled assignments of categories to words in thelexicon, principled definitions of categories themselves, andprincipled connections between syntactic structures and theinterpretations they were capable of producing.Our example illustrates how far things may go.
This isnot to say, however, that any form of canonicalization in-variably has such devastating consequences for the grammar.The effects of canonicalization of PP modifier attachmentsseem to be relatively minor, for instance.
As an anonymousreviewer stated so clearly, whether canonicalization is likelyto work or not depends on the locality of the phenomenonthe canonicalization is attempting to account for.
The lessany other grammatical  processes are sensitive to the inter-256nals of a canonical representation, the better the prospectsfor success.
However, there are surprisingly few cases whereno other grammatical processes are affected.
This samereviewer mentioned an interesting example involving noun-noun compounds.
Structural ambiguity within noun-nouncompounds might seem to be one of the most promisingcases for canonicalization i English given that most gram-matical processes are not sensitive to the internal structureof NPs.
However, when the grammar includes generalizedconjunction, problems quickly surface.
Consider an am-biguous sequence such as "N1 and N2 N3 V ' .
In order toencompass such examples, the canonicalization of compoundspresumably needs to be extended so that only one of the twoobvious analyses will be parsed.
But subject-verb agreementwill be affected by the choice of structure, and it seems dif-ficult to see how any straightforward solution could accountfor all cases of agreement and still return only a single parse.Thus we see that the internal structure of the NP does mat-ter after all, since conjunction and percolation of agreementfeatures are affected.
Attempts to extend canonicalization tocases in which even the most basic constituency is undeter-mined seems even less likely to succeed.
Examples such as"look up the word" along with others such as "I want thechicken to have lunch" share an uncertainty about what thebasic constituents in question really are.4.2 S tage  1 semant icsThe main consequence of canonicalization for Stage 1semantic processing, which corresponds to the semantictranslation step, is an increase in complexity.
In particular,the domain of locality for translations from syntactic struc-tures to semantic forms is affected.
An immediate con-sequence is that the mapping from parse structures to logicalassertions is less transparent than that in approaches thatmaintain a homomorphism between syntax and semanticssuch as Montague grammars and related phrase structureframeworks (e.g., Klein and Sag 1985).
For eanonicalizedstructures, the syntax-semantics mapping cannot take placein a local, compositional manner.
We discuss PPs as an ex-ample.First, consider canonicalization of prepositional phrases intheir role as modifiers.
In Lucy the syntax attaches PPs high,and Stage 1 processing produces pecial =Attach = assertionsthat are interpreted in such a way as to ultimately producethe set of possible attachments.
Thus in the example "I sawthe man on a hill with a telescope" shown below, the syntaxresults in a representation i dicating modifiers and their at-tachment dom(ains).
Stage 1 semantics processing producesseveral basic assertions as well as one =Attach" assertionwhose arguments consist of a list of (referents of) potentialattachment sites followed by a sequence of prepositionalphrases that are to be attached.Syntax:\[mod: \[prep: withpobJ: a telescope\]dom: \[mod: \[prep: onpobJ: a hill\]dom: \[subJ: Ipred: \[verb: sawobJ: the man\]\]33Stage I semantics:( I  x l )(man x2)(see el x l  x2)(hill x3)(telescope x4)(Attach (el x2)(on argl x3)(wlth argl x4))Note that the structure of the attachment assertion bears nosimple relation to the structure of the syntactic analysis.Producing the semantics assertions entails conducting asearch on the attachment domain, pulling out relevant sub-parts, and reassembling them into a different form.
Thetranslation process here is thus no longer a simple function ofthe translation of the PP and the translation of the con-stituent hat the PP attaches to.When the canonicalization i cludes not only the collaps-ing of attachment sites for PP modifiers but also structuresinvolving two-word verbs as discussed above, the complexityof the translation step goes up again.
In the case of "look upthe word' ,  there is only one prepositional phrase to ?ttachand only one place to attach it, hut processing is complicatedby the fact that we must check for the particle reading of"look up."
Where there is such a reading, we must generatea separate translation, with a branch in the subsequentsearch, even though the verb and the preposition are notparts of a single constituent, either in the syntax or in therest of Stage 1 semantics.
The meaning of the whole sen-tence thus contains readings that are not (simple) functionsof the meaning of the constituents in the parse tree.
Asimilar problem would take place wherever prepositionalphrases could be taken as arguments to a verb rather than asmodifiers of it.
(See the discussion below of indirect objectswith "to" and "for ' .
)Additional complications for PPs arise in sentences with"be" and a prepositional phrase.
The natural semantics for"John is next to Mary" would have "next to Mary" either asa predicate of "John" or as an argument o "be.
m In thesecases, the Lucy grammar still attaches the PP high to thepseudo-constituent "John is ' .
The Stage 1 routine then hasto detach the (first) PP from its position high in the tree andmove it down into the VP.
The resulting translation can bederived compositionally from the transformed parse tree, butnot from the original one.
Thus, even in the seeminglystraightforward case of prepositional phrases, the relation be-tween syntax and semantics has become opaque, with thereadings often differing significantly from the "natural" in-terpretations of the parse tree.
One concrete result of thiscomplexity is that the Stage 1 routine in Lucy is procedurallyrather than declaratively stated.
It is not a particularlytroublesome routine, but the complicated conditionalizedtransformations it performs would be hard to expressdeclaratively.4.3 S tage  2 semant icsStage 2 semantics in Lucy represents the transition fromsurface linguistic structure to a deeper, knowledge-based257form of representation.
In syntax and in Stage 1 semanticrepresentation the lexical items are English words.
DuringStage 2 processing these are are translated into the predi-cates of a domain knowledge base.
Thus, by the time Stage 2processing is finished, all information about the surface lin-guistic form is gone.
However, aa a result of canonicalization,the Stage 2 semantic module ends up doing (explicitly orimplicitly) the syntactic processing that has been put off byearlier components.
Since the o , tput  of Stage 2 semantics issupposed to represent he meaning of the sentence, modifierattachment must be resolved.
Consider the case of PP at-tachment again.
The part of the module that determines at-tachment must  know that crossed branches are not allowed;that is, in a string like "I saw a man on the hill with atelescope', if "on a hill" modifies "saw" then "with atelescope" cannot modify "a man."
Thus, the Stage 2 com-ponent must keep track of the interactions of the differentproposed attachments,  and this involves knowledge of thesyntactic tree structure.
Thus,  information that properlybelongs in the syntactic module ends up being duplicated inthe semantics.
Furthermore, if other modules, e.g.
discourse,need detailed syntactic information, the semantics com-ponent will have to go back and update the syntactic struc-ture to reflect the ultimate attachment of the PPs.In some cases, lexical information may also have to bepassed along fairly far into Stage 2 semantics.
Consider thecase of the delayed attachment of a PP that might be asemantic indirect object ( ' I  sent a letter to Mary" in thesense equivalent o "I sent Mary a letter. '
)  The problemhere is that some verbs ( ' send ' ,  "give", etc.)
take "to" asan indirect object marker, while a smaller class of verbs( 'buy ' ,  " f ind' ,  etc.)
take "for" as a marker.
The modulewill need to know what the surface verb was to make theattachment properly (in order to avoid interpreting "forJohn" as the recipient in "I sent it for John ' ,  etc.)
Ingeneral, attachment is often sensitive to the lexical items in-volved, and delaying attachment decisions entails importingsurface lexical, as well as syntactic, information into a partof the system that is more naturally thought of as operatingon 'pure meaning'  plus world knowledge.
In short, upstreamsyntactic information is contaminating downstream semanticprocessing.Finally, even if we are willing to accept such distortionsin the semantics, there are cases involving "of" where lateattachment seems to be impossible.
Normally, a phrase ofthe form NP1 Prep NP2 denotes a subset of the denotationof NP1 (e.g., a man in a sweater is a man and not asweater.)
However, "a bottle of beer" is often taken todenote the beer, rather than the bottle.
For example, you canpour, drink, or dilute a bottle of beer, though you can donone of these things to a simple glass bottle.
Therefore, ifsemantic processing involves checking for sortal consistency(subcategorization), as Stage 2 semantics in Lucy does, eitherPPs with "of" will have to be attached beforeverb/argument pairs are checked for consistency, or seman-tics will reject sentences that in fact have good readings.
Forexample, if "drink" subcategorizes for a liquid as its directobject, and 'a  bott le'  denotes a piece of glass (of the rightsize and shape, etc.
), then "drink a bottle" will fail sortalconsistency checking, even though "drink a bottle of beer"would succeed.
We could say that "bottle" also denotes acertain quantity of liquid, but by doing so we introduce ar-tificial ambiguity into the unambiguous sentence "I found abottle on the beach" (since one could certainly find a quan-tity of liquid on a beach).
8 The best solution would be totreat "of" separately from other prepositions, determining9 attachment earlier in the processing.
However, the addedcomplication that such treatment would entail reinforces thepoint that, even in cases where canonicalization seems in-nocuous to the syntax, the side-effects on semantic process-ing can be significant.Reflecting on the effect of canonicalization on semanticprocessing, we see that, as remarked above in the discussionof syntax, the locality of the construction in question is animportant factor.
In the case of noun-noun compounding, ithappens that there are few interactions between the internalstructure of the canonicalized construction and the rest ofthe sentence.
Accordingly, canonicalization of these struc-tures provides a painless way of avoiding early branching inthe search.
Prepositional phrases, however, although theyshow a high degree of locality in the syntax, are involved incomplex, non-local interactions in the semantics, with a cor-responding complication of the processing.
In such cases,canonicalization can still be made to work, but only at aprice.5 Conc lus ionWe believe that the Lucy experiment with canonicalrepresentations has generally succeeded in lowering theamount of effort Lucy spends on search.
The parser usuallyreturns a single analysis, instead of many, and the semanticsmodule usually succeeds in ruling out most of the possibilitieswhen they are finally unpacked.
A further benefit is thatdebugging some individual modules has been made easier.We have found, in particular, that debugging a grammarthat typically produces only one or a very small number ofparses is much easier than when the grammar eturns, say,hundreds of parses for a given sentence.But what of the hidden costs to the system?
The courseof our research has caused us to step back and question thewhole idea of canonical structures for two primary reasons:first, canonical structures tend to let declarative informationbe far too influenced by processing concerns; second, modulesleak in such designs, essentially doing away with one of themain arguments for such control models in the first place.There are rather serious practical, as well as theoretical,consequences when canonical forms make their way into thegrammar in the way discussed in Section 4.1.
First is theproblem of lexical acquisition when lexical category assign-ments become so off-beat.
Second, with arcane relations be-tween syntactic output and semantic result as discussed inSection 4.2, it becomes difficult to see how such systemscould be easily used for other purposes than the specific onesthey have been written for.
For instance, it is hard to seehow multilingual systems could relate grammars when in-dividua\[ grammars have been so heavily influenced by the ac-cidental vagaries of processing concerns in that language.
Itis also hard to see how a generation system could easilymake use of such grammars,  since the mapping rules willtend to be complicated and fundamental ly unidirectional.The moral to be drawn from the remarks in Section 4.3seems to be that a canonical structure model, at least in itsextreme form, does not permit us to maintain the modularityof a traditional conduit model.
If we return to Figure 2above, it is clear that when we finally begin enumerating thebranching that has simply been delayed in the canonical out-8Furthermore, almost any physical object can serve as acontainer: "We had lunch at the dump.
I drunk a hubcap ofbeer and ate a distributor cap of pate.
"9There is some evidence for treating "of" as a member ofa distinct syntactic class.
For one thing, "of ' ,  unlike otherprepositions, cannot attach to sentences (though it can markan argument of the verb: "the time has come, the Walrussaid, to talk of many things.. . '
)258put of module A, we will still have to use the informationthat fundamentally belongs in module A, even though we aredoing this processing in module B.
The effect is that we willrequire passing along information from box to box.
Thus, weend up doing interleaving whether we want to or not.Although these conclusions eem to be damning for thegeneral design philosophy, we should note that our attemptsat evaluation here are open to the criticism that a single casehistory does not necessarily justify general conclusions abouta design philosophy.
There is.
always the possibility that thedesign wasn't applied "right" in the case at hand.
In par-ticular, we should distinguish the proposal for hand-toolingcanonical representations into a grammar as we have done inLucy from the proposal for automatically inferring higherlevel generalizations from modules that themselves have stillbeen driven by principled linguistic concerns.
The proposalsof Church and Patil (1982) fall more into this latter camp,and it is a goal of the ongoing redesign efforts in Lucy toincorporate some version of automatic generalization.Despite the negatives, it is possible that for some NLPapplications the balance could still tip in favor of usingcanonical representations for some limited set of structuressuch as noun compounding or PP modifier attachment.
Ap-plications that have no pretensions of being fully general oreasily extensible may be willing to pay the price thatcanonicalization exacts in order to avoid a more complexdesign and still achieve acceptable performance results.
Infact, we expect that the need for methods that incorporatesome form of delayed evaluation will continue to be pressingin natural language analysis, and in view of the short supplyof such methods currently available, canonicalization maycontinue to have its place in the near term.
However, ourconclusion after two years of pursuing such techniques i thatconduit control models using canonical structures ultimatelyoffer no real alternative to more complex designs in whichcontrol is interleaved among modules.6 AcknowledgementsThis paper reports on work undertaken by the Lingoproject at MCC in 1986 and 1987.
Other members of Lingoconnected with this work include Elaine Rich, JoaSchlossberg, Kelly Shuldberg, Carl Weir, Greg Whittemore,and Dave Wroblewski.
Elaine Rich, in particular, has con-tributed much to the debates over issues discussed here andhas commented on earlier drafts.
We'd like to acknowledgeDave Wroblewski's role in these areas also, as well as his es-sential contributions to implementing Lucy.
Finally, thecomments of an anonymous reviewer were very useful to usin revising an earlier draft.REFERENCESAppelt, D. 1982.
Planning Natural-Language Ut-terances to Satisfy Multiple Goals.
Technicalreport no.
259, A.I.
Center, SRI International.Church, K., and R. Patil.
1982.
Coping with SyntacticAmbiguity or How to Put  the Block in the Boxon the Table.
Journal of Computational Linguis-tics 8:139-149.Helm, I.
1982.
The Semantics of Definite and IndefiniteNoun Phrases.
Ph.D. dissertation, University ofMassachussetts.Hobbs, J.
1985.
Ontological Promiscuity.
In Proceed-ings of the 23rd Annual Meeting of the Associa-tion for Computational Linguistics, pp.
61-69.Kamp.
H. 1984.
A Theory of Truth and SemanticRepresentation.
In Groenendijk et al (eds),Truth, Interpretation, and Information, pp.
1-41.Foris.Karttunen, L. 1987.
Radical Lexicalism.
To appear inM.
Baltin and A. Kroch (eds), New Conceptionsof Phrase Structure, MIT Press.Klein, E., and I.
Sag.
1985.
Type-driven Translation.Linguistics and Philosophy 8:163-201.Marcus, M., D. Hindle, and M. Fleck.
1983.
D-Theory:Talking about Talking about Trees.
In Proceed-ings of the 21st Annual Meeting of the Associa-tion for Computational Linguistics, pp.
129-136.Martin, W., K. Church, and R. Patti.
1981.
PreliminaryAnalysis of a Breadth-First Parsing Algorithm:Theoretical and Experimental Results, technicalreport no.
MIT/LCS/TR-261,  Massachusetts In-stitute of Technology.Pereira, F. 1983.
Logic for Natural Language Analysis.Technical report no.
275, A.I.
Center, SRI Inter-national.Pulman, S. G. 1983.
Generalized Phrase StructureGrammar,  Earley's Algorithm, and the minimisa-tion of Recursion.
In K. Sparck Jones andY.
Wilks (eds), Automatic Natural LanguageParsing, pp.
117-131.
Halsted.Rich, E., J. Barnett, K. Wittenburg, andD.
Wroblewski.
1987.
Ambiguity Procrastination.In Proceedings of A.A.AI-87, pp.
571-576.Uszkoreit, H. 1986.
Categorial Unification Grammars.In Proceedings of Coling 1986, pp.
187-194.Wittenburg, K. 1986.
Natural Language Parsing withCombinatory Categorial Grammars in a Graph-Unification-Based Formalism.
Ph.D. disser-tation, University of Texas at Austin.Wittenburg, K. 1987.
Extraposition from NP asAnaphora.
In G. Huck and A. Ojeda (eds), Syn-tax and Semantics, Volume 20: DiscontinuousConstituencies, pp.
427-444.
Academic.Zeevat, H., E. Klein, and J. Calder.
1986.
UnificationCategorial Grammar.
In Edinburgh WorkingPapers in Cognitive Science, Volume 1,Categorial Grammar,  Unification Grammar, andParsing.
Centre for Cognitive Science, pp.195-222.
University of Edinburgh.259
