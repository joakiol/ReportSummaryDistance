Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 18?26,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsD-Confidence: an active learning strategy which efficiently identifies smallclassesNuno EscudeiroInstituto Superior de Engenharia do PortoRua Dr Anto?nio Bernardino de Almeida, 431Porto, P-4200-072 PORTO, PortugalLIAAD-INESC PORTO L.A.Rua de Ceuta, 118 - 6Porto, P-4050-190 PORTO, Portugalnfe@isep.ipp.ptAl?
?pio JorgeLIAAD-INESC PORTO L.A.Rua de Ceuta, 118 - 6Porto, P-4050-190 PORTO, Portugalamjorge@fc.up.ptAbstractIn some classification tasks, such as those re-lated to the automatic building and mainte-nance of text corpora, it is expensive to ob-tain labeled examples to train a classifier.
Insuch circumstances it is common to have mas-sive corpora where a few examples are la-beled (typically a minority) while others arenot.
Semi-supervised learning techniques tryto leverage the intrinsic information in unla-beled examples to improve classification mod-els.
However, these techniques assume thatthe labeled examples cover all the classes tolearn which might not stand.
In the pres-ence of an imbalanced class distribution get-ting labeled examples from minority classesmight be very costly if queries are randomlyselected.
Active learning allows asking an or-acle to label new examples, that are criteri-ously selected, and does not assume a previ-ous knowledge of all classes.
D-Confidenceis an active learning approach that is effectivewhen in presence of imbalanced training sets.In this paper we discuss the performance of d-Confidence over text corpora.
We show empir-ically that d-Confidence reduces the numberof queries required to identify examples fromall classes to learn when compared to confi-dence, a common active learning criterion.1 IntroductionClassification tasks require a number of previouslylabeled cases.
A major bottleneck is that case label-ing is a laborious task requiring significant humaneffort.
This effort is particularly high in the case oftext documents, web pages and other unstructuredobjects.The effort required to retrieve representative la-beled examples to learn a classification model is notonly related to the number of distinct classes (Adamiet al, 2005); it is also related to class distribution inthe available pool of examples.
On a highly imbal-anced class distribution, it is particularly demandingto identify examples from minority classes.
These,however, may be important in terms of represen-tativeness.
Failing to identify cases from under-represented classes may have costs.
Minority classesmay correspond to specific information needs whichare relevant for specific subgroups of users.
In manysituations, such as fraud detection, clinical diagno-sis, news (Ribeiro and Escudeiro, 2008) and Webresources (Escudeiro and Jorge, 2006), we face theproblem of imbalanced class distributions.The aim of our current work is to get a classifica-tion model that is able to fully recognize the targetconcept, including all the classes to learn no materhow frequent or rare they are.Our main goal is to identify representative exam-ples for each class in the absence of previous de-scriptions of some or all the classes.
Furthermore,this must be achieved with a reduced number of la-beled examples in order to reduce the labeling effort.There are several learning schemes available forclassification.
The supervised setting allows usersto specify arbitrary concepts.
However, it requires afully labeled training set, which is prohibitive whenthe labeling cost is high and, besides that, it requireslabeled cases from all classes.
Semi-supervisedlearning allows users to state specific needs without18requiring extensive labeling (Chapelle et al 2006)but still requires that labeled examples fully coverthe target concept.
Unsupervised learning does notrequire any labeling but users have no chance to tai-lor clusters to their specific needs and there is noguarantee that the induced clusters are aligned withthe classes to learn.
In active learning, that seemsmore adequate to our goals, the learner is allowed toask an oracle (typically a human) to label examples?
these requests are called queries.
The most infor-mative queries are selected by the learning algorithminstead of being randomly selected as is the case insupervised learning.In this paper we evaluate the performance of d-Confidence (Escudeiro and Jorge, 2009) on text cor-pora.
D-Confidence is an active learning approachthat tends to explore unseen regions in case space,thus selecting cases from unseen classes faster ?with fewer queries ?
than traditional active learn-ing approaches.
D-Confidence selects queries basedon a criterion that aggregates the posterior classifierconfidence ?
a traditional active learning criterion ?and the distance between queries and known classes.This criterion is biased towards cases that do not be-long to known classes (low confidence) and that arelocated in unseen areas in case space (high distanceto known classes).
D-confidence is more effectivethan confidence alone in achieving an homogeneouscoverage of target classes.In the rest of this paper we start by reviewing ac-tive learning, in section 2.
Section 3 describes d-Confidence.
The evaluation process is presented insection 4 and we state our conclusions and expecta-tions for future work in section 5.2 Active LearningActive learning approaches (Angluin, 1988; Cohnet al, 1994; Muslea et al, 2006) reduce label com-plexity ?
the number of queries that are necessaryand sufficient to learn a concept ?
by analyzing un-labeled cases and selecting the most useful onesonce labeled.
Queries may be artificially generated(Baum, 1991) ?
the query construction paradigm?
or selected from a pool (Cohn et al, 1990) or astream of data ?
the query filtering paradigm.
Ourcurrent work is developed under the query filteringapproach.The general idea in active learning is to estimatethe value of labeling one unlabeled case.
Query-By-Committee (Seung et al, 1992), for example, usesa set of classifiers ?
the committee ?
to identify thecase with the highest disagreement.
Schohn et al(2000) worked on active learning for Support Vec-tor Machines (SVM) selecting queries ?
cases to belabeled ?
by their proximity to the dividing hyper-plane.
Their results are, in some cases, better than ifall available data is used to train.
Cohn et al (1996)describe an optimal solution for pool-based activelearning that selects the case that, once labeled andadded to the training set, produces the minimum ex-pected error.
This approach, however, requires highcomputational effort.
Previous active learning ap-proaches (providing non-optimal solutions) aim atreducing uncertainty by selecting the next query asthe unlabeled example on which the classifier is lessconfident.Batch mode active learning ?
selecting a batch ofqueries instead of a single one before retraining ?
isuseful when computational time for training is crit-ical.
Brinker (2003) proposes a selection strategy,tailored for SVM, that combines closeness to the di-viding hyperplane ?
assuring a reduction in the ver-sion space close to one half ?
with diversity amongselected cases ?
assuring that newly added examplesprovide additional reduction of version space.
Hoi etal.
(2006) suggest a new batch mode active learningrelying on the Fisher information matrix to ensuresmall redundancy among selected cases.
Li et al(2006) compute diversity within selected cases fromtheir conditional error.Dasgupta (2005) defines theoretical bounds show-ing that active learning has exponentially smaller la-bel complexity than supervised learning under someparticular and restrictive constraints.
This work isextended in Kaariainen (2006) by relaxing some ofthese constraints.
An important conclusion of thiswork is that the gains of active learning are muchmore evident in the initial phase of the learningprocess, after which these gains degrade and thespeed of learning drops to that of passive learn-ing.
Agnostic Active learning (Balcan et al, 2006),A2, achieves an exponential improvement over theusual sample complexity of supervised learning inthe presence of arbitrary forms of noise.
This modelis studied by Hanneke (2007) setting general bounds19on label complexity.All these approaches assume that we have an ini-tial labeled set covering all the classes of interest.Clustering has also been explored to providean initial structure to data or to suggest valuablequeries.
Adami et al (2005) merge clustering andoracle labeling to bootstrap a predefined hierarchyof classes.
Although the original clusters providesome structure to the input, this approach still de-mands for a high validation effort, especially whenthese clusters are not aligned with class labels.
Das-gupta et al (2008) propose a cluster-based methodthat consistently improves label complexity over su-pervised learning.
Their method detects and exploitsclusters that are loosely aligned with class labels.Among other paradigms, it is common that activelearning methods select the queries which are clos-est to the decision boundary of the current classi-fier.
These methods focus on improving the decisionfunctions for the classes that are already known, i.e.,those having labeled cases present in the training set.The work presented in this paper diverges classifierattention to other regions increasing the chances offinding new labels.3 D-Confidence Active LearningGiven a target concept with an arbitrary number ofclasses together with a sample of unlabeled exam-ples from the target space (the working set), ourpurpose is to identify representative cases coveringall classes while posing as few queries as possible,where a query consists of requesting a label to a spe-cific case.
The working set is assumed to be repre-sentative of the class space ?
the representativenessassumption (Liu and Motoda, 2001).Active learners commonly search for queries inthe neighborhood of the decision boundary, whereclass uncertainty is higher.
Limiting case selec-tion to the uncertainty region seems adequate whenwe have at least one labeled case from each class.This class representativeness is assumed by all ac-tive learning methods.
In such a scenario, selectingqueries from the uncertainty region is very effectivein reducing version space.Nevertheless, our focus is on text corpora whereonly few labeled examples exist and when we arestill looking for exemplary cases to qualify the con-cept to learn.
Under these circumstances ?
whilewe do not have labeled cases covering all classes?
the uncertainty region, as perceived by the activelearner, is just a subset of the real uncertainty region.Being limited to this partial view of the concept, thelearner is more likely to waste queries.
The amountof the uncertainty region that the learner misses is re-lated to the number of classes to learn that have notyet been identified as well as to the class distributionin the training set.The intuition behinf d-Confidence is that queryselection should be based not only on classifier con-fidence but also on distance to previously labeledcases.
In the presence of two cases with equally lowconfidence d-Confidence selects the one that is far-ther apart from what is already know, i.e., from pre-viously labeled cases.3.1 D-ConfidenceCommon active learning approaches rely on classi-fier confidence to select queries (Angluin, 1988) andassume that the pre-labeled set covers all the labelsto learn ?
this assumption does not hold in our sce-nario.
These approaches use the current classifica-tion model at each iteration to compute the posteriorconfidence on each known class for each unlabeledcase.
Then, they select, as the next query, the unla-beled case with the lowest confidence.D-Confidence, weighs the confidence of the clas-sifier with the inverse of the distance between thecase at hand and previously known classes.This bias is expected to favor a faster coverageof case space, exhibiting a tendency to explore un-known areas.
As a consequence, it provides fasterconvergence than confidence alone.
This drift to-wards unexplored regions and unknown classes isachieved by selecting the case with the lowest d-Confidence as the next query.
Lowest d-Confidenceis achieved by combining low confidence ?
probablyindicating cases from unknown classes ?
with highdistance to known classes ?
pointing to unseen re-gions in the case space.
This effect produces signif-icant differences in the behavior of the learning pro-cess.
Common active learners focus on the uncer-tainty region asking queries that are expected to nar-row it down.
The issue is that the uncertainty regionis determined by the labels we known at a given it-eration.
Focusing our search for queries exclusively20Table 1: d-Confidence algorithm.
(1) given W ; L1 and K(2) compute distance among cases in W(3) i = 1(4) while (not stopping criteria) {(5) Ui = W ?
Li(6) learn hi from Li(7) apply hi to Ui generating confi(uj , ck)(8) for(ujinUi){(9) disti(uj , ck) = aggrIndivDistk(ui, ck)(10) dconfi(uj , ck) =confi(uj ,ck)disti(uj ,ck)(11) dCi(uj) = agConfk(dconfi(uj , ck))(12) }(13) qi = uj : dCi(uj) = minu(dCi(u))(14) Li+1 = Li?
< qi, label(qi) >(15) i+ +(16) }on this region, while we are still looking for exem-plary cases on some labels that are not yet known, isnot effective.
Unknown classes hardly come by un-less they are represented in the current uncertaintyregion.In Table 1 we present the d-Confidence algorithm?
an active learning proposal specially tailored toachieve a class representative coverage fast.W is the working set, a representative sample ofcases from the problem space.
Li is a subset of W .Members of Li are the cases in W whose labels areknown at iteration i. U , a subset of W , is the setof unlabeled examples.
At iteration i, Ui is the (set)difference between W and Li; K is the number oftarget concept classes, ck; hi represents the classifierlearned at iteration i; qi is the query at iteration i;Ci is the set of classes known at iteration i ?
thatis the set of distinct classes from all Li elements;confi(uj , ck) is the posterior confidence on class ckgiven case uj , at iteration i.D-Confidence for unlabeled cases is computed atsteps (8) to (12) in Table 1 as explained below.
In(13) the case with the minimum d-Confidence is se-lected as the next query.
This query is added to thelabeled set (14), and removed from the unlabeledpool, and the whole process iterates.Computing d-Confidence d-Confidence is ob-tained as the ratio between confidence and distanceamong cases and known classes (Equation 1).argmaxk(conf (ck|u)medianj (dist (u,Xlabj,k)))(1)For a given unlabeled case, u, the classifier gen-erates the posterior confidence w.r.t.
known classes(7).
Confidence is then divided by an indicator ofthe distance, dist(), between unlabeled case u andall labeled cases belonging to class ck, Xlabj,k (9).This distance indicator is the median of the dis-tances between case u and all cases in Xlabj,k.
Themedian is expected to soften the effect of outliers.At step (10) we compute dconfi(u, ck) ?
the d-Confidence for each known class, ck, given the caseu ?
by dividing class confidence for a given case byaggregated distance to that class.Finally, d-Confidence of the case is computed,dCi(u), as the maximum d-Confidence on individ-ual classes, agConfk(confi(u, ck)) , at step (11).4 EvaluationD-Confidence was evaluated on two text corpora.We have selected a stratified sample from the 20Newsgroups (NG) ?
with 500 documents ?
and an-other one from the R52 set of the Reuters-21578collection (R52) ?
with 1000 documents.
The NGdataset has documents from 20 distinct classes whilethe R52 dataset has documents from 52 distinctclasses.
These samples have been selected becausethey have distinct class distributions.The class distribution of NG is fairly balanced(Figure 1) with a maximum frequency of 35 and aminimum frequency of 20.Figure 1: Class distribution in NG dataset21On the other hand, the R52 dataset presents anhighly imbalanced class distribution (Figure 2).Figure 2: Class distribution in R52 datasetThe most frequent class in R52 has a frequency of435 while the least frequent has only 2 examples inthe dataset.
This dataset has 42 classes, out of 52,with a fequency below 10.4.1 Experimental SettingWe have used Support Vector Machine classifiers(SVM) with linear kernels in all experiments.In all the experiments we have compared the per-formance of d-Confidence against confidence ?
acommon active learning setting where query selec-tion is based on low posterior confidence of the cur-rent classifier.
This comparison is important to eval-uate our proposal since d-Confidence is derived fromconfidence by means of an aggregation with distancein case space.
Comparing both these criteria, oneagainst the other, will provide evidence on the per-formance gains, or losses, of d-Confidence on textwhen compared to confidence, its baseline.We have performed 10-fold cross validation on alldatasets for standard confidence and d-Confidenceactive learning.
The labels in the training set are hid-den from the classifier.
In each iteration, the activelearning algorithm asks for the label of a single case.For the initial iteration in each fold we give two la-beled cases ?
from two distinct classes ?
to the clas-sifier.
The two initial classes are chosen for eachfold, so that different class combinations occur indifferent folds.
Given an initial class to be present inL1, the specific cases to include in L1 are randomlysampled from the set of cases on that class.
Giventhe fold, the same L1 is used for all experiments.4.2 ResultsOur experiments assess the ability of d-Confidenceto reduce the labeling effort when compared to con-fidence.We have recorded, for each dataset, the numberof distinct labels already identified and the progressof the error on the test set for each iteration (gen-eralization error).
From these, we have computed,for each dataset, the mean number of known classesand mean generalization error in each iteration overall the cross validation folds (Figures 3 and 4).The chart legends use c for confidence, dc for d-Confidence, e for generalization error and kc for thenumber of known classes.
For convenience of rep-resentation the number of classes that are known ateach iteration has been normalized to the total num-ber of classes in the dataset thus being transformedinto the percentage of known classes instead of theabsolute number of known classes.
This way thenumber of known classes and generalization errorare both bounded in the same range (between 0 and1) and we can conveniently represented them in thesame chart.Figure 3: Known classes and error in NG datasetMeans are micro-averages ?
all the cases areequally weighted ?
over all iterations for a givendataset and a given selection criterion (confidenceor d-Confidence).
Besides the overall number ofqueries required to retrieve labels from all classesand generalization error, we have also observed themean number of queries that are required to retrievethe first case for each class (Tables 2 to 4) ?
referredto as first hit.22Figure 4: Known classes and error in R52 datasetWe have performed significance tests, t-tests, forthe differences of the means observed when usingconfidence and d-Confidence.
Statistically differentmeans, at a significance level of 5%, are bold faced.When computing first hit for a given class wehave omitted the experiments where the labeled setfor the first iteration contains cases from that class.Figures 5 and 6 give an overview of the number ofqueries that are required in each setting to first hit agiven number of distinct classes.Figure 5: Queries required to identify bunches of distinctclasses in NG datasetA benchmark based on random selection is alsoprovided ?
averaged over 10 random samples.
Wehave recorded the number of queries required toidentify bunches of distinct classes in multiples of10 for R52 and multiples of 4 in NG.Table 2: Class distribution (freq) and first hit (c-fh anddc-fh) for the NG dataset.Class Freq c-fh dc-fh1 29 36.9 35.72 22 41.9 41.13 21 57.3 76.94 34 23.5 5.95 35 18.9 20.26 24 37.1 15.47 21 53.6 11.38 24 32.9 13.19 25 36.3 9.110 22 41.1 48.911 22 42.5 3.512 24 28.6 4.313 28 18.8 20.414 28 25.8 5.415 22 27.4 6.216 28 14.9 2.617 23 21.4 27.918 26 34.5 7.719 22 22.2 21.220 20 26.7 6.9mean 32.1 19.2Figure 6: Queries required to identify bunches of distinctclasses in R52 dataset23Table 3: Class distribution (Freq) and first hit (c-fh anddc-fh) for the R52 dataset.
Only for those classes whered-Confidence outperforms confidence with statistical sig-nificance at 5% significance level.Class Freq c-fh dc-fh1 239 10.1 1.62 5 7.2 1.38 3 103.8 76.69 7 68.6 6.610 2 80.0 10.011 40 83.4 41.714 2 173.7 110.615 3 115.6 64.716 7 96.7 16.818 5 68.7 62.922 2 244.4 197.623 30 153.4 36.725 4 173.3 102.926 2 214.1 123.927 5 206.7 184.928 2 213.3 85.229 2 137.6 44.830 3 159.3 52.131 2 159.1 144.832 2 179.7 123.933 30 160.8 76.134 15 175.6 108.736 2 167.4 107.837 3 118.0 99.540 2 140.0 104.743 4 313.1 256.444 14 216.3 144.546 12 206 126.747 2 233.7 16748 3 153.2 84.149 35 226 106.950 3 144.3 75.551 3 148.5 51.152 2 258.8 196.5mean 156.2 94.0Table 4: Class distribution (Freq) and first hit (c-fh anddc-fh) for the R52 dataset.
Only for those classes whered-Confidence does not outperforms confidence.Class Freq c-fh dc-fh3 3 11.2 18.04 2 36.4 72.95 6 23.1 50.76 11 39.7 49.77 4 40.1 89.112 2 128.8 136.013 435 91.9 107.817 9 117.0 135.619 2 123.6 19.120 3 171.7 171.121 2 196.2 224.024 4 118.6 178.735 4 146.1 183.538 3 158.5 166.439 2 152.2 150.441 5 143.6 154.542 3 188.9 202.845 3 175.5 198.7mean 114.6 128.324Figure 7: Average gain of d-Confidence to confidence.Classes are sorted by increasing order of their frequency.4.3 DiscussionThe charts in Figures 3 and 4 confirm the resultsthat have been previously reported for standard non-textual datasets (Escudeiro and Jorge, 2009), w.r.t.identification of cases from unknown classes, i.e.,d-Confidence reduces the labeling effort that is re-quired to identify examples from all classes.
How-ever, the error rate gets worse in the R52 dataset.D-Confidence gets to know more classes from thetarget concept earlier although less sharply.
In theR52 dataset we are exchanging accuracy by repre-sentativeness.
This might be desirable or not, de-pending on the specifc task we are dealing with.
Ifwe are trying to learn a target concept but we do notknow examples from all the classes to learn ?
for in-stance if we are in the early stage of a classificationproblem ?
this effect might be desirable so we canget a full specification of the target concept with areduced labeling effort.It is interesting to notice that d-Confidence out-performs confidence to a greater extent on minorityclasses.
This is obvious in R52 if we compute thecumulative average of the gain in labeling effort thatis provided by d-Confidence when compared to con-fidence (Figure 7).The gain for each class is defined as the number ofqueries required by d-Confidence to first hit the classminus the ones that are required by confidence.
Tocompute the moving average, these gains are sortedin increasing order of the class frequency.
The aver-age gain starts at -128, for a class with frequency 2,and decreases to the overall average of -36 as classfrequency increases up to 435.
The bigger gains areobserved in the minority classes.
Although not asobvious as in R52 this same behaviour is also ob-served in the NG dataset.Figures 5 and 6, as well as Tables 2 to 4, show thatd-Confidence reduces the labeling effort required toidentify unknown classes when compared to confi-dence.
When selecting cases to label randomly, thefirst bunch of 10 distinct classes is found as fast aswith d-Confidence but, from there on, when rareclasses come by, d-Confidence takes the lead.
Theoutcome is quite different in the NG dataset.
In thisdataset d-Confidence still outperforms confidencebut it is beaten by random selection of cases afteridentifying 13.3 classes on average (after 22 querieson average).
This observation led us to suspect thatwhen in presence of balanced datasets, d-Confidenceidentifies new classes faster than random selection inthe initial phase of the learning process but selectingcases by chance is better to identify cases in the lat-est stage of collecting exemplary cases, when fewclasses remain undetected.5 Conclusions and Future WorkThe evaluation procedure that we have performedprovided statistical evidence on the performance ofd-Confidence over text corpora when compared toconfidence.
Although the evaluation has been per-formed only on two datasets, the conclusions wehave reached point out some interesting results.D-Confidence reduces the labeling effort andidentifies exemplary cases for all classes faster thatconfidence.
This gain is bigger for minority classes,which are the ones where the benefits are more rele-vant.D-Confidence performs better in imbalanceddatasets where it provides significant gains thatgreatly reduce the labeling effort.
For balanceddatasets, d-Confidence seems to be valuable in theearly stage of the classification task, when fewclasses are known.
In the later stages, random se-lection of cases seems faster in identifying the fewmissing classes.
However, d-Confidence consis-tently outperforms confidence.The main drawback of d-Confidence when ap-plied on imbalanced text corpora is that the reduc-tion in the labeling effort that is achieved in iden-tifying unknown classes is obtained at the cost of25increasing error.
This increase in error is probablydue to the fact that we are diverting the classifierfrom focusing on the decision function of the major-ity classes to focus on finding new, minority, classes.As a consequence the classification model gener-ated by d-Confidence is able of identifying more dis-tinct classes faster but gets less sharp in each one ofthem.
This is particularly harmful for accuracy sincea more fuzzy decision boundary for majority classesmight cause many erroneous guesses with a negativeimpact on error.We are now exploring semi-supervised learningto leverage the intrinsic value of unlabeled cases sowe can benefit from the reduction in labeling effortprovided by d-Confidence without increasing error.6 ReferencesG.
Adami, P. Avesani, and D. Sona.
Clustering doc-uments into a web directory for bootstrapping a su-pervised classification.
Data and Knowledge Engi-neering, 54:301325, 2005.D.
Angluin.
Queries and concept learning.
Ma-chine Learning, 2:319342, 1988.M.-F. Balcan, A. Beygelzimer, and J. Langford.Agnostic active learning.
In In ICML, pages 6572.ICML, 2006.E.
Baum.
Neural net alorithms that learn inpolynomial time from examples and queries.
IEEETransactions in Neural Networks, 2:519, 1991.K.
Brinker.
Incorporating diversity in activelearning with support vector machines.
In Proceed-ings of the Twentieth International Conference onMachine Learning, 2003.O.
Chapelle, B. Schoelkopf and A. Zien (Eds).Semi-supervised Learning.
MIT Press, Cambridge,MA, 2006.D.
Cohn, L. Atlas, and R. Ladner.
Training con-nectionist networks with queries and selective sam-pling.
In Advances in Neural Information Process-ing Systems, 1990.D.
Cohn, L. Atlas, and R. Ladner.
Improving gen-eralization with active learning.
Machine Learning,(15):201221, 1994.D.
Cohn, Z. Ghahramani, and M. Jordan.
Activelearning with statistical models.
Journal of ArtificialIntelligence Research, 4:129145, 1996.S.
Dasgupta.
Coarse sample complexity bonds foractive learning.
In Advances in Neural InformationProcessing Systems 18.
2005.S.
Dasgupta and D. Hsu.
Hierarchical samplingfor active learning.
In Proceedings of the 25th Inter-national Conference on Machine Learning, 2008.N.
Escudeiro and A.M. Jorge.
Efficient coverageof case space with active learning.
In P. M. L. M.R.
Lus Seabra Lopes, Nuno Lau, editor, Progress inArtificial Intelligence, Proceedings of the 14th Por-tuguese Conference on Artificial Intelligence (EPIA2009), volume 5816, pages 411422.
Springer, 2009.S.
Hanneke.
A bound on the label complexityof agnostic active learning.
In Proceedings of the24th International Conference on Machine Learn-ing, 2007.S.
Hoi, R. Jin, and M. Lyu.
Large-scale text cat-egorization by batch mode active learning.
In Pro-ceedings of the World Wide Web Conference, 2006.M.
Kaariainen.
Algorithmic Learning Theory,chapter Active learning in the non-realizable case,pages 63 77.
Springer Berlin / Heidelberg, 2006.M.
Li and I. Sethi.
Confidence-based active learn-ing.
IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 28:12511261, 2006.H.
Liu and H. Motoda.
Instance Selection andConstruction for Data Mining.
Kluver AcademicPublishers, 2001.I.
Muslea, S. Minton, and C. A. Knoblock.
Activelearning with multiple views.
Journal of ArtificialIntelligence Research, 27:203233, 2006.P.
Ribeiro and N. Escudeiro.
On-line news ?a lacarte.
In Proceedings of the European Conference onthe Use of Modern Information and CommunicationTechnologies, 2008.N.
Roy and A. McCallum.
Toward optimal activelearning through sampling estimation of error reduc-tion.
In Proceedings of the International Conferenceon Machine Learning, 2001.G.
Schohn and D. Cohn.
Less is more: Activelearning with support vector machines.
In Proceed-ings of the International Conference on MachineLearning, 2000.H.
Seung, M. Opper, and H. Sompolinsky.
Queryby committee.
In Proceedings of the 5th An-nual Workshop on Computational Learning Theory,1992.26
