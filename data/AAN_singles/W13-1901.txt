Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 1?9,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsEarlier Identification of Epilepsy Surgery Candidates Using NaturalLanguage ProcessingPawel Matykiewicz1, Kevin Bretonnel Cohen2, Katherine D. Holland1, Tracy A. Glauser1,Shannon M. Standridge1, Karin M. Verspoor3,4, and John Pestian1?1 Cincinnati Children?s Hospital Medical Center, Cincinnati OH USA2 University of Colorado, Denver, CO3 National ICT Australia and 4The University of Melbourne, Melbourne, Australia?corresponding author: john.pestian@cchmc.orgAbstractThis research analyzed the clinical notesof epilepsy patients using techniques fromcorpus linguistics and machine learningand predicted which patients are can-didates for neurosurgery, i.e.
have in-tractable epilepsy, and which are not.Information-theoretic and machine learn-ing techniques are used to determinewhether and how sets of clinic notesfrom patients with intractable and non-intractable epilepsy are different.
The re-sults show that it is possible to predictfrom an early stage of treatment which pa-tients will fall into one of these two cate-gories based only on text data.
These re-sults have broad implications for develop-ing clinical decision support systems.1 Introduction and SignificanceEpilepsy is a disease characterized by recurrentseizures that may cause irreversible brain damage.While there are no national registries, epidemiolo-gists have shown that roughly three million Amer-icans require $17.6 billion USD in care annuallyto treat their epilepsy (Epilepsy Foundation, 2012;Begley et al 2000).
Epilepsy is defined by theoccurrence of two or more unprovoked seizuresin a year.
Approximately 30% of those individ-uals with epilepsy will have seizures that do notrespond to anti-epileptic drugs (Kwan and Brodie,2000).
This population of individuals is said tohave intractable or drug-resistant epilepsy (Kwanet al 2010).Select intractable epilepsy patients are candi-dates for a variety of neurosurgical procedures thatablate the portion of the brain known to cause theseizure.
On average, the gap between the ini-tial clinical visit when the diagnosis of epilepsyis made and surgery is six years.
If it were pos-sible to predict which patients should be consid-ered candidates for referral to surgery earlier in thecourse of treatment, years of damaging seizures,under-employment, and psychosocial distress maybe avoided.
It is this gap that motivates this re-search.In this study, we examine the differences be-tween the clinical notes of patients early in theirtreatment course with the intent of predictingwhich patients will eventually be diagnosed as in-tractable versus which will be amenable to drug-based treatment.
The null hypothesis is thatthere will be no detectable differences betweenthe clinic notes of patients who go on to a di-agnosis of intractable epilepsy and patients whodo not progress to the diagnosis of intractableepilepsy (figure 1).
To further elucidate the phe-nomenon, we look at both the patient?s earli-est clinical notes and notes from a progressionof time points.
Here we expect to gain insightinto how the linguistic characteristics (and natu-ral language processing-based classification per-formance) evolve over treatment course.
We alsostudy the linguistic features that characterize thedifferences between the document sets from thetwo groups of patients.
We anticipate that this ap-proach will ultimately be adapted for various clin-ical decision support systems.2 Background2.1 Related workAlthough there has been extensive work on build-ing predictive models of disease progression andof mortality risk, few models take advantage ofnatural language processing in addressing thistask.
(Abhyankar et al 2012) used univariate anal-ysis, multivariate logistic regression, sensitivityanalyses, and Cox proportional hazards models topredict 30-day and 1-year survival of overweight1and obese Intensive Care Unit patients.
As one ofthe features in their system, they used smoking sta-tus extracted from patient records by natural lan-guage processing techniques.
(Himes et al 2009) used a Bayesian networkmodel to predict which asthma patients would goon to develop chronic obstructive pulmonary dis-ease.
As one of their features, they also usedsmoking status extracted from patient records bynatural language processing techniques.
(Huang et al under review) is the work mostsimilar to our own.
They evaluated the ability ofa Naive Bayesian classifier to predict future diag-noses of depression six months prior and twelvemonths prior to the actual diagnoses.
They useda number of feature types, including fielded datasuch as billing codes, ICD-9 CM diagnoses, andothers, as well as data drawn from natural lan-guage processing.In particular, they used an optimized version ofthe NCBO Annotator (Jonquet et al 2009) to rec-ognize terms from 22 clinically relevant ontolo-gies and classify them additionally as to whetherthey were negated or related to the patient?s fam-ily history.
Their system demonstrated an abilityto predict diagnoses of depression both six monthsand one year prior to the actual diagnoses at a ratethat exceeds the success of primary care practi-tioners in diagnosing active depression.Considering this body of work overall, naturallanguage processing techniques have played a mi-nor role, providing only a fraction of a much largerset of features?just one feature, in the first twostudies discussed.
In contrast, in our work natu-ral language processing is the central aspect of thesolution.2.2 Theoretical background to theapproaches used in this workIn comparing the document sets from the two pa-tient populations, we make use of two lines of in-quiry.
In the first, we use information-theoreticmethods to determine whether or not the contentsof the data sets are different, and if they are dif-ferent, to characterize the differences.
In the sec-ond, we make use of a practical method from ap-plied machine learning.
In particular, we deter-mine whether it is possible to train a classifier todistinguish between documents from the two setsof patients, given an appropriate classification al-gorithm and a reasonable set of features.From information-theoretic methods, we takeKullback-Leibler divergence as a way to deter-mine whether the contents of the two sets of docu-ments are the same or different.
Kullback-Leiblerdivergence is the relative entropy of two probabil-ity mass functions?
?a measure of how differenttwo probability distributions (over the same eventspace) are?
(Manning and Schuetze, 1999).
Thismeasure has been previously used to assess thesimilarity of corpora (Verspoor et al 2009).
De-tails of the calculation of Kullback-Leibler diver-gence are given in the Methods section.
Kullback-Leibler divergence has a lower bound of zero; witha value of zero, the two document sets would beidentical.
A value of 0.005 is assumed to corre-spond to near-identity.From practical applications of machine learn-ing, we test whether or not it is possible to train aclassifier to distinguish between documents fromthe two document sets.
The line of thought here isthat provided that we have an appropriate classifi-cation algorithm and a reasonable feature set, thenif clinic notes from the two document sets are in-deed different, it should be possible to train a clas-sifier to distinguish between them with reasonableaccuracy.3 Materials and methods3.1 MaterialsThe experimental protocol was approved by ourlocal Institutional Review Board (#2012-1646).Neurology clinic notes were extracted from theelectronic medical record system.
Records weresampled from two groups of patients: 1) thosewith intractable epilepsy referred for and eventu-ally undergoing epilepsy surgery and 2) those withepilepsy who were responsive to medications andnever referred for surgical evaluation.
They werealso sampled at three time periods before the ?zeropoint?, the date at which patients were either re-ferred for surgery or the date of last seizure for thenon-intractable group.
Table 1 shows the distribu-tion of patients and clinic notes.3.2 MethodsAs described in the introduction, we appliedinformation-theoretic and machine learning tech-niques to determine whether the two documentcollections were different (or differentiable).2Non-Intractable Intractable-12 to 0 355 (127) 641 (155)-6 to +6 453 (128) 898 (155)0 to +12 months 454 (132) 882 (149)Table 1: Progress note and patient counts (inparentheses) for each time period.
A minus signindicates the period before surgery referral datefor intractable epilepsy patients and before lastseizure for non-intractable patients.
A plus signindicates the period after surgery referral for in-tractable epilepsy patients and after last seizure fornon-intractable patients.
Zero is the surgery refer-ral date or date of last seizure for the two popula-tions, respectively.3.2.1 Feature extractionFeatures for both the calculation of Kullback-Leibler divergence and the machine learningexperiment were unigrams, bigrams, tri-grams, and quadrigrams.
We applied theNational Library of Medicine stopword listhttp://mbr.nlm.nih.gov/Download/2009/WordCounts/wrd_stop.
All wordswere lower-cased, all numerals were substitutedwith the string NUMB for abstraction, and allnon-ASCII characters were removed.3.3 Information-theoretic approachKullback-Leibler divergence compares probabilitydistribution of words or n-grams between differentdatasets DKL(P ||Q).
In particular, it measureshow much information is lost if distribution Q isused to approximate distribution P .
This method,however, gives an asymmetric dissimilarity mea-sure.
Jensen-Shannon divergence is probably themost popular symmetrization of DKL and is de-fined as follows:DJS =12DKL(P ||Q) +12DKL(Q||P ) (1)whereDKL(P ||Q) =?w?P?Q(p(w|cP ) logp(w|cP )p(w|cQ))(2)By Zipf?s law any corpus of natural language willhave a very long tail of infrequent words.
To ac-count for this effect we use DJS for the top Nmost frequent words/n-grams.
We use Laplacesmoothing to account for words or n-grams thatdid not appear in one of the corpora.We also aim to uncover terms that distinguishone corpus from another.
We use a metamor-phic DJS test, log-likelihood ratios, and weightedSVM features.
Log-likelihood score will help usunderstand where precisely the two corpora differ.nij =kijkiP + kiA(3)mij =kPj + kQjkQP + kPP + kQA + kPA(4)LL(w) = 2?i,jkij lognijmij(5)3.4 Machine learningFor the classification experiment, we used an im-plementation of the libsvm support vector ma-chine package that was ported to R (Dimitriadouet al 2011).
Features were extracted as describedabove in Section 3.2.1.
We used a cosine kernel.The optimal C regularization parameter was esti-mated on a scale from 2?1 to 215.3.5 Characterizing differences between thedocument setsWe used a variety of methods to characterizedifferences between the document sets: log-likelihood ratio, SVM normal vector components,and a technique adapted from metamorphic test-ing.3.5.1 Applying metamorphic testing toKullback-Leibler divergenceAs one of our methods for characterizing differ-ences between the two document sets, we used anadaptation of metamorphic testing, inspired by thework of (Murphy and Kaiser, 2008) on applyingmetamorphic testing to machine learning applica-tions.
The intuition behind metamorphic testing isthat given some output for a given input, it shouldbe possible to predict in general terms what theeffect of some alternation in the input should beon the output.
For example, given some Kullback-Leibler divergence for some set of features, it ispossible to predict how Kullback-Leibler diver-gence will change if a feature is added to or sub-tracted from the feature vector.
We adapted thisobservation by iteratively subtracting all featuresone by one and ranking them according to howmuch of an effect on the Kullback-Leibler diver-gence its removal had.3Figure 1: Two major paths in epilepsy care.
Atthe begining of epilepsy care two groups of pa-tients are indistinguishable.
Subsequently, the twogroups diverge.4 Results4.1 Kullback-Leibler (Jensen-Shannon)divergenceTable 2 shows the Kullback-Leibler divergence,calculated as Jensen-Shannon divergence, forthree overlapping time periods?the year preced-ing surgery referral, the period from 6 months be-fore surgery referral to six months after surgery re-ferral, and the year following surgery referral, forthe intractable epilepsy patients; and, for the non-intractable epilepsy patients, the same time peri-ods with reference to the last seizure date.As can be seen in the left-most column (-12 to0), at one year prior, the clinic notes of patientswho will require surgery and patients who willnot require surgery cannot easily be discriminatedby Kullback-Leibler divergence?the divergenceis only just above the .005 near-identity thresholdeven when 8000 unique n-grams are considered.
Ifthe -6 to +6 and 0 to +12 time periods are exam-ined, we see that the divergence increases as wereach and then pass the period of surgery (or moveinto the year following the last seizure, for the non-intractable patients), indicating that the differencebetween the two collections becomes more pro-nounced as treatment progresses.
The divergencefor these time periods does pass the assumed near-identity threshold for larger numbers of n-grams,n-grams -12 to 0months-6 to +6months0 to +12months125 0.00125 0.00193 0.00244250 0.00167 0.00229 0.00286500 0.00266 0.00326 0.003891000 0.00404 0.00494 0.005852000 0.00504 0.00618 0.007184000 0.00535 0.00657 0.007708000 0.00555 0.00681 0.00796Table 2: Kullback-Leibler divergence (calculatedas Jensen-Shannon divergence) for difference be-tween progress notes of the two groups of patients.Results are shown for the period 1 year before, 6months before and 6 months after, and one yearafter surgery referral for the intractable epilepsypatients and the last seizure for non-intractable pa-tients.
0 represents the date of surgery referral forthe intractable epilepsy patients and date of lastseizure for the non-intractable patients.largely accounted for by terms that are unique toone notes set or the other.4.2 Classification with support vectormachinesTable 3 shows the results of building support vec-tor machines to classify individual notes as be-longing to the intractable epilepsy or the non-intractable epilepsy patient population.
Three timeperiods are evaluated, as described above.
Thenumber of features is varied by row.
For eachcell, the average F-measure from 20-fold cross-validation is shown.As can be seen in the left-most column (-12 to0), at one year prior to referral to surgery refer-ral date or last seizure, the patients who will be-come intractable epilepsy patients can be distin-guished from the patients who will become non-intractable epilepsy patients purely on the basis ofnatural language processing-based classificationwith an F-measure as high as 0.95.
This supportsthe conclusion that the two document sets are in-deed different, and furthermore illustrates that thisdifference can be used to predict which patientswill require surgical intervention.4.3 Characterizing the differences betweenclinic notes from the two patientpopulationsTables 4 and 5 show the results of three meth-ods for differentiating between the document col-4n-grams -12 to 0months-6 to +6months0 to +12months125 0.8885 0.9217 0.9476250 0.8928 0.9297 0.9572500 0.9107 0.9367 0.96671000 0.9245 0.9496 0.96922000 0.9417 0.9595 0.97894000 0.9469 0.9661 0.98008000 0.9510 0.9681 0.9810Table 3: Average F1 for the three time periodsdescribed above, with increasing numbers of fea-tures.
Values are the average of 20-fold cross-validation.
See Figure 2 for an explanation of thetime periods.lections representing the two patient populations.The methodology for each is described above.
Themost strongly distinguishing features when justthe 125 most frequent features are used are shownin Table 4, and the most strongly distinguishingfeatures when the 8,000 most frequent features areused are shown in Table 5.
Impressionistically,two trends emerge.
One is that more clearly clini-cally significant features are shown to have strongdiscriminatory power when the 8,000 most fre-quent features are used than when the 125 mostfrequent features are used.
This result is sup-ported by the Kullback-Leibler divergence results,which demonstrated the most divergent vocabular-ies with larger numbers of n-grams.
The othertrend is that the SVM classifier does a better jobof picking out clinically relevant features.
Thishas implications for the design of clinical decisionsupport systems that utilize our approach.5 Discussion5.1 Behavior of Kullback-Leibler divergenceKullback-Leibler divergence varies with the num-ber of words considered.
When the vocabulariesof two document sets are merged and the wordsare ordered by overall frequency, the further downthe list we go, the higher the Kullback-Leiblerdivergence can be expected to be.
This is be-cause the highest-frequency words in the com-bined set will generally be frequent in both sourcecorpora, and therefore carry similar probabilitymass.
As we progress further down the list offrequency-ranked words, we include progressivelyless-common words, with diverse usage patterns,which are likely to reflect the differences betweenthe two document sets, if there are any.
Thus, theKullback-Leibler divergence will rise.To understand the intuition here, imagine look-ing at the Kullback-Leibler divergence when justthe 50 most-common words are considered.
Thesewill be primarily function words, and their distri-butions are unlikely to differ much between thetwo document sets unless the syntax of the twocorpora is radically different.
Beyond this set ofvery frequent common words will be words thatmay be relatively frequent in one set as comparedto the other, contributing to divergence betweenthe sets.In Table 2, the observed behavior for our twodocument collections follows this expected pat-tern.
However, the divergence between the vocab-ularies remains close to the assumed near-identitythreshold of 0.005, even when larger numbers ofn-grams are considered.
The divergence never ex-ceeds 0.01; this level of divergence for larger num-bers of n-grams is consistent with prior analyses ofhighly similar corpora (Verspoor et al 2009).We attribute this similarity to two factors.
Thefirst is that both document sets derive from a singledepartment within a single hospital; a relativelysmall number of doctors are responsible for au-thoring the notes and there may exist specific hos-pital protocols related to their content.
The secondis that the clinical contexts from which our twodocument sets are derived are highly related, inthat all the patients are epilepsy patients.
While wehave demonstrated that there are clear differencesbetween the two sets, it is also to be expected thatthey would have many words in common.
Thenature of clinical notes combined with the shareddisease context results in generally consistent vo-cabulary and hence low overall divergence.5.2 Behavior of classifierTable 3 demonstrates that classifier performanceincreases as the number of features increases.
Thisindicates that as more terms are considered, thebasis for differentiating between the two differentdocument collections is stronger.Examining the SVM normal vector components(SVMW) in Tables 4 and 5, we find that unigrams,bigrams and trigrams are useful in differentiationbetween the two patient populations.
While noquadrigrams appear in this table, they may in factcontribute to classifier performance.
We will per-form an ablation study in future work to quantify5JS metamorphic test (JSMT) Log-likelihood ratio (LLR) SVM normal vector compo-nents (SVMW)family = -0.000114 none = 623.702323 bilaterally = -19.009380normal = -0.000106 family = -445.117177 age.NUMB = 17.981459seizure = -0.000053 NUMB.NUMB.NUMB.NUMB= 422.953816review = 17.250652problems = -0.000053 normal = -244.603033 based = -14.846495none = 0.000043 problems = -207.021130 family.history = -14.659653detailed = -0.000037 left = 176.434519 NUMB = -14.422525including = -0.000036 bid = 142.105691 lower = -13.553434risks = -0.000033 NUMB = 136.255678 mother = -13.436694NUMB = 0.000032 detailed = -133.012908 first = -13.001744concerns = -0.000032 right = 120.453596 including = -12.800433NUMB.NUMB.NUMB.NUMB= 0.000031seizure = -120.047686 extremities = 11.709199additional = -0.000029 including = -119.061518 documented = -11.441394brain = -0.000026 risks = -116.543250 awake = -11.418535NUMB.NUMB = 0.000022 concerns = -101.366110 hpi = 11.121019minutes = -0.000021 additional = -95.880792 follow = -10.550802NUMB.minutes = -0.000020 clear = 83.848170 neurology = -10.533895reviewed = -0.000018 brain = -74.267220 call = -10.422606history = -0.000017 seizures = 71.937757 effects = 10.298221noted = -0.000017 one = 65.203819 brain = -9.900864upper = -0.000017 epilepsy = 46.383564 weight = 9.819712well = -0.000015 hpi = 45.932630 patient.s = -9.603531side = -0.000015 minutes = -45.278770 discussed = -9.473544bilaterally = -0.000014 NUMB.NUMB.NUMB =43.320354today = 9.390896motor.normal = -0.000014 negative = 42.914770 allergies = -9.346146notes = -0.000014 NUMB.minutes = -42.909968 NUMB.NUMB.NUMB.NUMB= 9.342800Spearman correlation betweenJSMT and LLR = 0.912454Spearman correlation betweenLLR and SVMW = 0.086784Spearman correlation betweenSVMW and JSMT = 0.101965Table 4: Comparison of three different methods for finding the strongest differentiating features.
Thistable shows features for the -12 to 0 periods with the 125 most frequent features.
The JSMT and LLRstatistics give values greater than zero.
We add sign to indicate which corpus has higher relative fre-quency of the feature: a positive value indicates that the relative frequency of the feature is greater in theintractable group, while a negative value indicates that the relative frequency of the feature is greater inthe non-intractable group.
The last row shows the correlation between two different ranking statistics.6JS metamorphic test (JSMT) Log-likelihood ratio (LLR) SVM normal vector compo-nents (SVMW)family = -0.000118 family = -830.329965 john = -4.645071normal = -0.000109 normal = -745.882086 lamotrigine = 4.320412seizure = -0.000057 problems = -386.238711 surgery = 4.299546problems = -0.000057 seizure = -369.342334 jane = 4.091609none = 0.000047 none = 337.461504 epilepsy.surgery = 4.035633including = -0.000040 detailed = -262.240496 janet = -3.970101detailed = -0.000040 including = -255.076808 excellent.control = -3.946283additional.concerns = -0.000038 additional.concerns.noted =-246.603655excellent = -3.920620additional.concerns.noted =-0.000038concerns.noted = -246.603655 NUMB.seizure = -3.886997concerns.noted = -0.000038 additional.concerns = -243.353912mother = -3.801364NUMB = -0.000036 NUMB.NUMB.NUMB.NUMB= 238.065700jen = 3.568809concerns = -0.000036 risks = -232.741511 back = -3.319477risks = -0.000036 concerns = -228.805299 visit = -3.264600NUMB.NUMB.NUMB.NUMB= 0.000035additional = -204.462411 james = 3.174763additional = -0.000033 brain = -182.413340 NUMB.NUMB.NUMB.normal= -3.024471brain = -0.000030 NUMB = -162.992065 continue = -3.011293NUMB.NUMB = -0.000026 surgery = 153.646067 idiopathic.localization = -2.998177minutes = -0.000025 minutes = -142.761961 idiopathic.localization.related =-2.998177surgery = 0.000024 NUMB.minutes = -134.048116 increase = 2.948187NUMB.minutes = -0.000023 diff = -131.388230 diastat = -2.937431diff = -0.000023 NUMB.NUMB = -125.067347 taking = -2.902673history = -0.000021 reviewed = -116.013417 lamictal = 2.898987reviewed = -0.000021 noted = -114.241532 going = 2.862764noted = -0.000021 idiopathic = -112.331060 described = 2.844830upper = -0.000020 shaking = -112.186858 epilepsy = 2.745872Spearman correlation betweenJSMT and LLR = 0.782918Spearman correlation betweenLLR and SVMW = 0.039860Spearman correlation betweenSVMW and JSMT = 0.165159Table 5: Comparison of three different methods for finding the strongest differentiating features.
Thistable shows features for the -12 to 0 periods with the 8,000 most frequent features.
The JSMT andLLR statistics give values greater than zero.
We add sign to indicate which corpus has higher relativefrequency of the feature: a positive value indicates that the relative frequency of the feature is greater inthe intractable group, while a negative value indicates that the relative frequency of the feature is greaterin the non-intractable group.
The last row shows the correlation between two different ranking statistics.7the contribution of the different feature sets.
In ad-dition, we find that table 5 shows many clinicallyrelevant terms, such as seizure frequency (?ex-cellent [seizure] control?
), epilepsy type (?local-ization related [epilepsy]?
), etiology classification(?idiopathic [epilepsy]?
), and drug names (?lamot-rigine?, ?diastat?, ?lamictal?
), giving nearly com-plete history of the present illness.6 ConclusionThe classification results from our machine learn-ing experiments support rejection of the null hy-pothesis of no detectable differences between theclinic notes of patients who will progress to thediagnosis of intractable epilepsy and patients whodo not progress to the diagnosis of intractableepilepsy.
The results show that we can predictfrom an early stage of treatment which patientswill fall into these two classes based only on tex-tual data from the neurology clinic notes.
As intu-ition would suggest, we find that the notes becomemore divergent and the ability to predict outcomeimproves as time progresses, but the most impor-tant point is that the outcome can be predictedfrom the earliest time period.SVM classification demonstrates a stronger re-sult than the information-theoretic measures, usesless data, and needs just a single run.
However, itis important to note that we cannot entirely relyon the argument from classification as the solemethodology in testing whether or not two doc-ument sets are similar or different.
If the find-ing is positive, i.e., it is possible to train a classi-fier to distinguish between documents drawn fromthe two document sets, then interpreting the re-sults is straightforward.
However, if documentsdrawn from the two document sets are not foundto be distinguishable by a classifier, one mustconsider the possibility of multiple possible con-founds, such as selection of an inappropriate clas-sification algorithm, extraction of the wrong fea-tures, bugs in the feature extraction software, etc.Having established that the two sets of clinicalnotes differ, we noted some identifying features ofclinic notes from the two populations, particularlywhen more terms were considered.The Institute of Medicine explains that ?.
.
.
toaccommodate the reality that although profes-sional judgment will always be vital to shapingcare, the amount of information required for anygiven decision is moving beyond unassisted hu-man capacity (Olsen et al 2007).?
This is surelythe case for those who care for the epileptic pa-tient.
Technology like natural language processingwill ultimately serve as a basis for stable clinicaldecision support tools.
It, however, is not a deci-sion making tool.
Decision making is the respon-sibility of professional judgement.
That judge-ment will labor over such questions as: what isthe efficacy of neurosurgery, what will be the longterm outcome, will there be any lasting damage,are we sure that all the medications have beentested, and how the family will adjust to a pooroutcome.
In the end, it is that judgement that willdecide what is best; that decision will be supportedby research like what is presented here.7 AcknowledgementsThis work was supported in part by the NationalInstitutes of Health, Grants #1R01LM011124-01,and 1R01NS045911-01; the Cincinnati Chil-dren?s Hospital Medical Center?s: Research Foun-dation, Department of Pediatric Surgery and theDepartment of Paediatrics?s divisions of Neurol-ogy and Biomedical Informatics.
We also wishto acknowledge the clinical and surgical wisdomprovided by Drs.
John J. Hutton & Hansel M.Greiner, MD.
K. Bretonnel Cohen was supportedby grants XXX YYY ZZZ.
Karin Verspoor wassupported by NICTA, which is funded by the Aus-tralian Government as represented by the Depart-ment of Broadband, Communications and the Dig-ital Economy and the Australian Research Coun-cil.References[Abhyankar et al012] Swapna Abhyankar, Kira Leis-hear, Fiona M. Callaghan, Dina Demner-Fushman,and Clement J. McDonald.
2012.
Lower short- andlong-term mortality associated with overweight andobesity in a large cohort study of adult intensive careunit patients.
Critical Care, 16.
[Begley et al000] Charles E Begley, Melissa Famu-lari, John F Annegers, David R Lairson, Thomas FReynolds, Sharon Coan, Stephanie Dubinsky,Michael E Newmark, Cynthia Leibson, EL So, et al2000.
The cost of epilepsy in the united states: Anestimate from population-based clinical and surveydata.
Epilepsia, 41(3):342?351.
[Dimitriadou et al011] Evgenia Dimitriadou, KurtHornik, Friedrich Leisch, David Meyer, and An-dreas Weingessel, 2011. e1071: Misc Func-tions of the Department of Statistics (e1071), TU8Wien.
http://CRAN.R-project.org/package=e1071.R package version 1.5.
[Epilepsy Foundation2012] Epilepsy Foundation,2012.
What is Epilepsy: Incidence and Prevalence.http://www.epilepsyfoundation.org/ aboutepilepsy/whatisepilepsy/ statistics.cfm.
[Himes et al009] Blanca E. Himes, Yi Dai, Isaac S.Kohane, Scott T. Weiss, and Marco F. Ramoni.2009.
Prediction of chronic obstructive pulmonarydisease (copd) in asthma patients using electronicmedical records.
Journal of the American MedicalInformatics Association, 16(3):371?379.
[Huang et alnder review] Sandy H. Huang, Paea LeP-endu, Srinivasan V Iyer, Anna Bauer-Mehren, CliffOlson, and Nigam H. Shah.
under review.
Develop-ing computational models for predicting diagnosesof depression.
In American Medical Informatics As-sociation.
[Jonquet et al009] Clement Jonquet, Nigam H. Shah,Cherie H. Youn, Mark A. Musen, Chris Callendar,and Margaret-Anne Storey.
2009.
NCBO Annota-tor: Semantic annotation of biomedical data.
In 8thInternational Semantic Web Conference.
[Kwan and Brodie2000] Patrick Kwan and Martin JBrodie.
2000.
Early identification of refrac-tory epilepsy.
New England Journal of Medicine,342(5):314?319.
[Kwan et al010] Patrick Kwan, Alexis Arzimanoglou,Anne T Berg, Martin J Brodie, W Allen Hauser,Gary Mathern, Solomon L Moshe?, Emilio Perucca,Samuel Wiebe, and Jacqueline French.
2010.
Defi-nition of drug resistant epilepsy: consensus proposalby the ad hoc task force of the ilae commission ontherapeutic strategies.
Epilepsia, 51(6):1069?1077.
[Manning and Schuetze1999] Christopher Manningand Hinrich Schuetze.
1999.
Foundations ofstatistical natural language processing.
MIT Press.
[Murphy and Kaiser2008] Christian Murphy and GailKaiser.
2008.
Improving the dependability of ma-chine learning applications.
[Olsen et al007] LeighAnne Olsen, Dara Aisner, andJ Michael McGinnis.
2007.
The learning healthcaresystem.
[Verspoor et al009] K. Verspoor, K.B.
Cohen, andL.
Hunter.
2009.
The textual characteristics of tradi-tional and open access scientific journals are similar.BMC Bioinformatics, 10(1):183.9
