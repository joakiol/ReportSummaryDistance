Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1093?1103, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsSketch Algorithms for Estimating Point Queries in NLPAmit Goyal and Hal Daume?
IIIUniversity of Maryland{amit,hal}@umiacs.umd.eduGraham CormodeAT&T Labs?Researchgraham@research.att.comAbstractMany NLP tasks rely on accurate statis-tics from large corpora.
Tracking com-plete statistics is memory intensive, so recentwork has proposed using compact approx-imate ?sketches?
of frequency distributions.We describe 10 sketch methods, including ex-isting and novel variants.
We compare andstudy the errors (over-estimation and under-estimation) made by the sketches.
We evaluateseveral sketches on three important NLP prob-lems.
Our experiments show that one sketchperforms best for all the three tasks.1 IntroductionSince the emergence of the World Wide Web, so-cial media and mobile devices, we have ever largerand richer examples of text data.
Such vast cor-pora have led to leaps in the performance of manylanguage-based tasks: the concept is that simplemodels trained on big data can outperform morecomplex models with fewer examples.
However,this new view comes with its own challenges: prin-cipally, how to effectively represent such large datasets so that model parameters can be efficiently ex-tracted?
One answer is to adopt compact summariesof corpora in the form of probabilistic ?sketches?.In recent years, the field of Natural Language Pro-cessing (NLP) has seen tremendous growth and in-terest in the use of approximation, randomization,and streaming techniques for large-scale problems(Brants et al 2007; Turney, 2008).
Much of thiswork relies on tracking very many statistics.
For ex-ample, storing approximate counts (Talbot and Os-borne, 2007; Van Durme and Lall, 2009a; Goyaland Daume?
III, 2011a), computing approximate as-sociation scores like Pointwise Mutual Information(Li et al 2008; Van Durme and Lall, 2009b; Goyaland Daume?
III, 2011a), finding frequent items (liken-grams) (Goyal et al 2009), building streaminglanguage models (Talbot and Brants, 2008; Leven-berg and Osborne, 2009), and distributional similar-ity (Ravichandran et al 2005; Van Durme and Lall,2010).
All these problems ultimately depend on ap-proximate counts of items (such as n-grams, wordpairs and word-context pairs).
Thus we focus onsolving this central problem in the context of NLPapplications.Sketch algorithms (Charikar et al 2004; Cor-mode, 2011) are a memory- and time-efficient so-lution to answering point queries.
Recently in NLP,we (Goyal and Daume?
III, 2011a) demonstrated thata version of the Count-Min sketch (Cormode andMuthukrishnan, 2004) accurately solves three large-scale NLP problems using small bounded memoryfootprint.
However, there are several other sketch al-gorithms, and it is not clear why this instance shouldbe preferred amongst these.
In this work, we con-duct a systematic study and compare many sketchtechniques which answer point queries with focuson large-scale NLP tasks.
While sketches have beenevaluated within the database community for find-ing frequent items (Cormode and Hadjieleftheriou,2008) and join-size estimation (Rusu and Dobra,2007), this is the first comparative study for NLPproblems.Our work includes three contributions: (1)We propose novel variants of existing sketchesby extending the idea of conservative update tothem.
We propose Count sketch (Charikar et al2004) with conservative update (COUNT-CU) andCount-mean-min sketch with conservative update1093(CMM-CU).
The motivation behind proposing newsketches is inspired by the success of Count-Minsketch with conservative update in our earlier work(Goyal and Daume?
III, 2011a).
(2) We empiricallycompare and study the errors in approximate countsfor several sketches.
Errors can be over-estimation,under-estimation, or a combination of the two.
Wealso evaluate their performance via Pointwise Mu-tual Information and LogLikelihood Ratio.
(3) Weuse sketches to solve three important NLP problems.Our experiments show that sketches can be very ef-fective for these tasks, and that the best results areobtained using the ?conservative update?
technique.Across all the three tasks, one sketch (CM-CU) per-forms best.2 SketchesIn this section, we review existing sketch algorithmsfrom the literature, and propose novel variants basedon the idea of conservative update (Estan and Vargh-ese, 2002).
The term ?sketch?
refers to a class ofalgorithm that represents a large data set with acompact summary, typically much smaller than thefull size of the input.
Given an input of N items(x1, x2 .
.
.
xN ), each item x (where x is drawn fromsome domain U ) is mapped via hash functions intoa small sketch vector that records frequency infor-mation.
Thus, the sketch does not store the itemsexplicitly, but only information about the frequencydistribution.
Sketches support fundamental querieson their input such as point, range and inner productqueries to be quickly answered approximately.
Inthis paper, we focus on point queries, which ask forthe (approximate) count of a given item.The algorithms we consider are randomized andapproximate.
They have two user-chosen parame-ters  and ?.
 controls the amount of tolerable errorin the returned count and ?
controls the probabilitywith which the error exceeds the bound .
Thesevalues of  and ?
determine respectively the widthw and depth d of a two-dimensional array sk[?, ?]
ofcount information.
The depth d denotes the num-ber of hash functions employed by the sketch algo-rithms.Sketch Operations.
Every sketch has two opera-tions: UPDATE and QUERY to update and estimatethe count of an item.
They all guarantee essentiallyconstant time operation (technically, this grows asO(log(1? )
but in practice this is set to a constant)per UPDATE and QUERY.
Moreover, sketches can becombined: given two sketches s1 and s2 computed(using the same parameters w and d, and same setof d hash functions) over different inputs, a sketchof the combined input is obtained by adding the in-dividual sketches, entry-wise.
The time to performthe COMBINE operation on sketches is O(d ?
w),independent of the data size.
This property enablessketches to be implemented in distributed setting,where each machine computes the sketch over asmall portion of the corpus and makes it scalableto large datasets.2.1 Existing sketch algorithmsThis section describes sketches from the literature:Count-Min sketch (CM): The CM (Cormode andMuthukrishnan, 2004) sketch has been used effec-tively for many large scale problems across sev-eral areas, such as Security (Schechter et al 2010),Machine Learning (Shi et al 2009; Aggarwaland Yu, 2010), Privacy (Dwork et al 2010), andNLP (Goyal and Daume?
III, 2011a).
The sketchstores an array of size d ?
w counters, along with dhash functions (drawn from a pairwise-independentfamily), one for each row of the array.
Given an in-put ofN items (x1, x2 .
.
.
xN ), each of the hash func-tions hk:U ?
{1 .
.
.
w}, ?1 ?
k ?
d, takes an itemfrom the input and maps it into a counter indexed bythe corresponding hash function.UPDATE: For each new item ?x?
with count c, thesketch is updated as:sk[k, hk(x)]?
sk[k, hk(x)] + c, ?1 ?
k ?
d.QUERY: Since multiple items are hashed to thesame index for each array row, the stored frequencyin each row is guaranteed to overestimate the truecount, making it a biased estimator.
Therefore, toanswer the point query (QUERY (x)), CM returns theminimum over all the d positions x is stored.c?
(x) = mink sk[k, hk(x)], ?1 ?
k ?
d.Setting w=2 and d=log(1? )
ensures all reportedfrequencies by CM exceed the true frequencies byat most N with probability of at least 1 ?
?.
Thismakes the space used by the algorithm O(1 log1?
).Spectral Bloom Filters (SBF): Cohen and Matias(2003) proposed SBF, an extension to BloomFilters (Bloom, 1970) to answer point queries.
The1094UPDATE and QUERY procedures for SBF are thesame as Count-Min (CM) sketch, except that therange of all the hash functions for SBF are the fullarray: hk:U ?
{1 .
.
.
w ?
d},?1 ?
k ?
d. WhileCM and SBF are very similar, only CM providesguarantees on the query error.Count-mean-min (CMM): The motivation behindthe CMM (Deng and Rafiei, 2007) sketch is toprovide an unbiased estimator for Count-Min(CM) sketch.
The construction of CMM sketchis identical to the CM sketch, while the QUERYprocedure differs.
Instead of returning the minimumvalue over the d counters (indexed by d hashfunctions), CMM deducts the value of estimatednoise from each of the d counters, and return themedian of the d residues.
The noise is estimatedas (N ?
sk[k, hk(x)])/(w ?
1).
Nevertheless,the median estimate (f?1) over the d residues canoverestimate more than the original CM sketch minestimate (f?2), so we return min (f?1,f?2) as the finalestimate for CMM sketch.
CMM gives the sametheoretical guarantees as Count sketch (below).Count sketch (COUNT) (Charikar et al 2004):COUNT (aka Fast-AGMS) keeps two hashfunctions for each row, hk maps items onto[1, w], and gk maps items onto {?1,+1}.
UP-DATE: For each new item ?x?
with count c:sk[k, hk(x)]?
sk[k, hk(x)] + c ?
gk(x), ?1 ?
k ?
d.QUERY: the median over the d rows is an unbiasedestimator of the point query:?c(x) = mediank sk[k, hk(x)] ?
gk(x), ?1 ?
k ?
d.Setting w= 22 and d=log(4? )
ensures that all re-ported frequencies have error at most (?Ni=1 f2i )1/2?
N with probability at least 1??.
The space usedby the algorithm is O( 12 log1?
).2.2 Conservative Update sketch algorithmsIn this section, we propose novel variants of existingsketches (see Section 2) by combining them with theconservative update process (Estan and Varghese,2002).
The idea of conservative update (also knownas Minimal Increase (Cohen and Matias, 2003)) is toonly increase counts in the sketch by the minimumamount needed to ensure the estimate remains accu-rate.
It can easily be applied to Count-Min (CM)sketch and Spectral Bloom Filters (SBF) to furtherimprove the estimate of a point query.
Goyal andDaume?
III (2011a) showed that CM sketch withconservative update reduces the amount of over-estimation error by a factor of at least 1.5, and alsoimproves performance on three NLP tasks.Note that while conservative update for CM andSBF never increases the error, there is no guaranteedimprovement.
The method relies on seeing multipleupdates in sequence.
When a large corpus is beingsummarized in a distributed setting, we can applyconservative update on each sketch independentlybefore combining the sketches together (see ?SketchOperations?
in Section 2).Count-Min sketch with conservative update(CM-CU): The QUERY procedure for CM-CU(Cormode, 2009; Goyal and Daume?
III, 2011a) isidentical to Count-Min.
However, to UPDATE anitem ?x?
with frequency c, we first compute the fre-quency c?
(x) of this item from the existing data struc-ture (?1 ?
k ?
d, c?
(x) = mink sk[k, hk(x)]) andthe counts are updated according to:sk[k, hk(x)]?
max{sk[k, hk(x)], c?
(x) + c} (?
).The intuition is that, since the point query returnsthe minimum of all the d values, we update acounter only if it is necessary as indicated by (?
).This heuristic avoids unnecessarily updating countervalues to reduce the over-estimation error.Spectral Bloom Filters with conservative update(SBF-CU): The QUERY procedure for SBF-CU(Cohen and Matias, 2003) is identical to SBF.SBF-CU UPDATE procedure is similar to CM-CU,with the difference that all d hash functions have thecommon range d?
w.Count-mean-min with conservative update(CMM-CU): We propose a new variant to reducethe over-estimation error for CMM sketch.
Theconstruction of CMM-CU is identical to CM-CU.However, due to conservative update, each row ofthe sketch is not updated for every update, hencethe sum of counts over each row (?i sk[k, i],?1 ?
k ?
d) is not equal to input size N .Hence, the estimated noise to be subtracted here is(?i sk[k, i]?
sk[k, hk(x)]) / (w ?
1).
CMM-CUdeducts the value of estimated noise from each ofthe d counters, and returns the median of the dresidues as the point query.Count sketch with conservative update (COUNT-CU): We propose a new variant to reduce over-estimation error for the COUNT sketch.
TheQUERY procedure for COUNT-CU is the same as1095COUNT.
The UPDATE procedure follows the sameoutline as CM-CU, but uses the current estimatec?
(x) from the COUNT sketch, i.e.c?
(x) = mediank sk[k, hk(x)] ?
gk(x), ?1 ?
k ?
d.Note, this heuristic is not as strong as for CM-CUand SBF-CU because COUNT can have both over-estimate and under-estimate errors.Lossy counting with conservative update (LCU-WS): LCU-WS (Goyal and Daume?
III, 2011b) wasproposed to reduce the amount of over-estimationerror for CM-CU sketch, without incurring toomuch under-estimation error.
This scheme is in-spired by lossy counting (Manku and Motwani,2002).
In this approach, the input sequence is con-ceptually divided into windows, each containing 1/?items.
The size of each window is equal to size ofthe sketch i.e.
d ?
w. Note that there are ?N win-dows; let t denote the index of current window.
Atwindow boundaries, ?
1 ?
i ?
d, 1 ?
j ?
w,if (sk[i, j] > 0 and sk[i, j] ?
t), then sk[i, j] ?sk[i, j]?1.
The idea is to remove the contribution ofsmall items colliding in the same entry, while not al-tering the count of frequent items.
The current win-dow index is used to draw this distinction.
Here, allreported frequencies f?
have both under and over es-timation error: f ?
?N ?
f?
?
f + N .Lossy counting with conservative update II(LCU-SWS): This is a variant of the previousscheme, where the counts of the sketch are de-creased more conservatively.
Hence, this schemehas worse over-estimation error compared to LCU-WS, with better under-estimation.
Here, only thosecounts are decremented which are at most the squareroot of current window index, t. At window bound-aries, ?
1 ?
i ?
d, 1 ?
j ?
w, if (sk[i, j] > 0 andsk[i, j] ?
d?te), then sk[i, j]?
sk[i, j]?
1.
LCU-SWS has similar analytical bounds to LCU-WS.3 Intrinsic EvaluationsWe empirically compare and study the errors in ap-proximate counts for all 10 sketches.
Errors can beover-estimation, under-estimation, or a combinationof the two.
We also study the behavior of approxi-mate Pointwise Mutual Information and Log Likeli-hood Ratio for the sketches.3.1 Experimental SetupDATA: We took 50 million random sentences fromGigaword (Graff, 2003).
We split this data in10 chunks of 5 million sentences each.
Since allsketches have probabilistic bounds, we report aver-age results over these 10 chunks.
For each chunk,we generate counts of all word pairs within a win-dow size 7.
This results in an average stream size of194 million word pair tokens and 33.5 million wordpair types per chunk.To compare error in various sketch counts, first wecompute the exact counts of all the word pairs.
Sec-ond, we store the counts of all the word pairs in allthe sketches.
Third, we query sketches to generateapproximate counts of all the word pairs.
Recall, wedo not store the word pairs explicitly in sketches butonly a compact summary of the associated counts.We fix the size of each sketch to be w = 20?1063and d = 3.
We keep the size of sketches equalto allow fair comparison among them.
Prior work(Deng and Rafiei, 2007; Goyal and Daume?
III,2011a) showed with fixed sketch size, a small num-ber of hash functions (d=number of hash functions)with large w (or range) give rise to small error overcounts.
Next, we group all word pairs with thesame true frequency into a single bucket.
We thencompute the Mean Relative Error (MRE) in each ofthese buckets.
Because different sketches have dif-ferent accuracy behavior on low, mid, and high fre-quency counts, making this distinction based on fre-quency lets us determine the regions in which dif-ferent sketches perform best.
Mean Relative Error(MRE) is defined as the average of absolute differ-ence between the predicted and the exact value di-vided by the exact value over all the word pairs ineach bucket.3.2 Studying the Error in CountsWe study the errors produced by all 10 sketches.Since various sketches result in different errors onlow, mid, and high frequency counts, we plot the re-sults with a linear error scale (Fig.
1(a)) to highlightthe performance for low frequency counts, and witha log error scale (Fig.
1(b)) for mid and high fre-quency counts.We make several observations on low frequencycounts from Fig.
1(a).
(1) Count-Min (CM) and1096100 101 10202468True frequency counts of word pairs (log scale)Mean RelativeErrorCMCM?CUSBFSBF?CUCOUNTCOUNT?CUCMMCMM?CULCU?SWSLCU?WS(a) Focusing on low frequency counts100 102 10410?410?2100True frequency counts of word pairs (log scale)Mean RelativeError (logscale)CMCM?CUCOUNTCOUNT?CUCMMCMM?CULCU?SWSLCU?WS(b) Focusing on mid and high frequency countsFigure 1: Comparing several sketches for input size of 75 million word pairs.
Size of each sketch: w = 20?1063 and d = 3.
Allitems with same exact count are put in one bucket and we plot Mean Relative Error on the y-axis with exact counts on the x-axis.Spectral Bloom Filters (SBF) have identical MREfor word pairs.
Using conservative update with CM(CM-CU) and SBF (SBF-CU) reduces the MREby a factor of 1.5.
MRE for CM-CU and SBF-CU is also identical.
(2) COUNT has better MREthan CM-CU and using conservative update withCOUNT (COUNT-CU) further reduces the MRE.
(3) CMM has better MRE than COUNT and usingconservative update with CMM (CMM-CU) fur-ther reduces the MRE.
(4) Lossy counting with con-servative update variants (LCU-SWS, LCU-WS)have comparable MRE to COUNT-CU and CMM-CU respectively.In Fig.
1(b), we do not plot the SBF variantsas SBF and CM variants had identical MRE inFig.
1(a).
From Figure 1(b), we observe that,CM, COUNT, COUNT-CU, CMM, CMM-CUsketches have worse MRE than CM-CU, LCU-SWS, and LCU-WS for mid and high frequencycounts.
CM-CU, LCU-SWS, and LCU-WS havezero MRE for all the counts > 1000.To summarize the above observations, for thoseNLP problems where we cannot afford to makeerrors on mid and high frequency counts, weshould employ CM-CU, LCU-SWS, and LCU-WS sketches.
If we want to reduce the error onlow frequency counts, LCU-WS generates least er-ror.
For NLP tasks where we can allow error on midand high frequency counts but not on low frequency100 101 102 103?1?0.500.511.52True frequency counts of word pairs (log scale)Mean RelativeErrorCOUNT?CU?OECOUNT?CU?UECMM?CU?OECMM?CU?UELCU?SWS?OELCU?SWS?UELCU?WS?OELCU?WS?UEFigure 2: Compare several sketches on over-estimation andunder-estimation errors with respect to exact counts.counts, CMM-CU sketch is best.3.3 Examining OE and UE errorsIn many NLP applications, we are willing to tolerateeither over-estimation or under-estimation errors.Hence we breakdown the error into over-estimation(OE) and under-estimation (UE) errors for the sixbest-performing sketches (COUNT, COUNT-CU,CMM, CMM-CU, LCU-SWS, and LCU-WS).
Toaccomplish that, rather than using absolute error val-ues, we divide the values into over-estimation (pos-itive), and under-estimation (negative) error buck-1097101 102 103 104 10500.20.40.60.81Top?KRecallCM?CUCOUNT?CUCMM?CULCU?SWSLCU?WS(a) PMI101 102 103 104 1050.50.60.70.80.91Top?KRecallCM?CUCOUNT?CUCMM?CULCU?SWSLCU?WS(b) LLRFigure 3: Evaluate the approximate PMI and LLR rankings (obtained using various sketches) with the exact rankings.ets.
Hence, to compute the over-estimation MRE,we take the average of positive values over all theitems in each bucket.
For under-estimation, wetake the average over the negative values.
Wecan make several interesting observations from Fig-ure 2: (1) Comparing COUNT-CU and LCU-SWS, we learn that both have the same over-estimation errors.
However, LCU-SWS has lessunder-estimation error than COUNT-CU.
There-fore, LCU-SWS is always better than COUNT-CU.
(2) LCU-WS has less over-estimation thanLCU-SWS but with more under-estimation error onmid frequency counts.
LCU-WS has less under-estimation error than COUNT-CU.
(3) CMM-CUhas the least over-estimation error and most under-estimation error among all the compared sketches.From the above experiments, we conclude thattasks sensitive to under-estimation should use theCM-CU sketch, which guarantees over-estimation.However, if we are willing to make some under-estimation error with less over-estimation error,then LCU-WS and LCU-SWS are recommended.Lastly, to have minimal over-estimation error withwillingness to accept large under-estimation error,CMM-CU is recommended.3.4 Evaluating association scores rankingLast, in many NLP problems, we are interested in as-sociation rankings obtained using Pointwise MutualInformation (PMI) and Log Likelihood Ratio (LLR).In this experiment, we compare the word pairs asso-ciation rankings obtained using PMI and LLR fromseveral sketches and exact word pair counts.
We userecall to measure the number of top-K sorted wordpairs that are found in both the rankings.In Figure 3(a), we compute the recall for CM-CU, COUNT-CU, CMM-CU, LCU-SWS, andLCU-WS sketches at several top-K thresholds ofword pairs for approximate PMI ranking.
Wecan make several observations from Figure 3(a).COUNT-CU has the worst recall for almost all thetop-K settings.
For top-K values less than 750, allsketches except COUNT-CU have comparable re-call.
Meanwhile, for K greater than 750, LCU-WShas the best recall.
The is because PMI is sensitiveto low frequency counts (Church and Hanks, 1989),over-estimation of the counts of low frequency wordpairs can make their approximate PMI scores worse.In Figure 3(b), we compare the LLR rankings.
Fortop-K values less than 1000, all the sketches havecomparable recall.
For top-K values greater than1000, CM-CU, LCU-SWS, and LCU-WS performbetter.
The reason for such a behavior is due to LLRfavoring high frequency word pairs, and COUNT-CU and CMM-CU making under-estimation erroron high frequency word pairs.To summarize, to maintain top-K PMI rank-ings making over-estimation error is not desirable.Hence, LCU-WS is recommended for PMI rank-ings.
For LLR, producing under-estimation error isnot preferable and therefore, CM-CU, LCU-WS,and LCU-SWS are recommended.1098Test Set Random Buckets NeighborModel CM-CU CMM-CU LCU-WS CM-CU CMM-CU LCU-WS CM-CU CMM-CU LCU-WS50M 87.2 74.3 86.5 83.9 72.9 83.2 71.7 64.7 72.1100M 90.4 79.0 91.0 86.5 76.9 86.9 73.4 67.2 74.7200M 93.3 83.1 92.9 88.3 80.1 88.4 75.0 69.0 75.4500M 94.4 86.6 94.1 89.3 83.4 89.3 75.7 70.8 75.51B 94.4 88.7 94.4 89.5 85.1 89.5 75.8 71.9 75.8Exact 94.5 89.5 75.8Table 1: Pseudo-words evaluation on accuracy metric for selectional preferences using several sketches of different sizes againstthe exact.
There is no statistically significant difference (at p < 0.05 using bootstrap resampling) among bolded numbers.4 Extrinsic Evaluation4.1 Experimental SetupWe study three important NLP applications, andcompare the three best-performing sketches: Count-Min sketch with conservative update (CM-CU),Count-mean-min with conservative update (CMM-CU), and Lossy counting with conservative update(LCU-WS).
The above mentioned 3 sketches are se-lected from 10 sketches (see Section 2) consideringthese sketches make errors on different ranges of thecounts: low, mid and, high frequency counts as seenin our intrinsic evaluations in Section 3.
The goalof this experiment is to show the effectiveness ofsketches on large-scale language processing tasks.These adhere to the premise that simple methodsusing large data can dominate more complex mod-els.
We purposefully select simple methods as theyuse approximate counts and associations directly tosolve these tasks.
This allows us to have a fair com-parison among different sketches, and to more di-rectly see the impact of different choices of sketchon the task outcome.
Of course, sketches are stillbroadly applicable to many NLP problems where wewant to count (many) items or compute associations:e.g.
language models, Statistical Machine Transla-tion, paraphrasing, bootstrapping and label propaga-tion for automatically creating a knowledge base andfinding interesting patterns in social media.Data: We use Gigaword (Graff, 2003) and a 50%portion of a copy of news web (GWB50) crawledby (Ravichandran et al 2005).
The raw size ofGigaword (GW) and GWB50 is 9.8 GB and 49GB with 56.78 million and 462.60 sentences respec-tively.
For both the corpora, we split the text intosentences, tokenize and convert into lower-case.4.2 Pseudo-Words EvaluationIn NLP, it is difficult and time consuming to createannotated test sets.
This problem has motivated theuse of pseudo-words to automatically create the testsets without human annotation.
The pseudo-wordsare a common way to evaluate selectional prefer-ences models (Erk, 2007; Bergsma et al 2008) thatmeasure the strength of association between a predi-cate and its argument filler, e.g., that the noun ?song?is likely to be the object of the verb ?sing?.A pseudo-word is the conflation of two words(e.g.
song/dance).
One word is the original in a sen-tence, and the second is the confounder.
For exam-ple, in our task of selectional preferences, the systemhas to decide for the verb ?sing?
which is the correctobject between ?song?/?dance?.
Recently, Cham-bers and Jurafsky (2010) proposed a simple baselinebased on co-occurrence counts of words, which hasstate-of-the-art performance on pseudo-words eval-uation for selectional preferences.We use a simple approach (without any typed de-pendency data) similar to Chambers and Jurafsky(2010), where we count all word pairs (except wordpairs involving stop words) that appear within a win-dow of size 3 from Gigaword (9.8 GB).
That gen-erates 970 million word pair tokens (stream size)and 94 million word pair types.
Counts of all the94 million unique word pairs are stored in CM-CU, CMM-CU, and LCU-WS.
For a target verb,we return that noun which has higher co-occurrencecount with it, as the correct selectional preference.We evaluate on Chambers and Jurafsky?s three testsets1 (excluding instances involving stop words) thatare based on different strategies in selecting con-founders: Random (4081 instances), Buckets (40281http://www.usna.edu/Users/cs/nchamber/data/pseudowords/1099100 102 104 1060.40.60.81Cumulative ProportionTrue Frequency of word pairs (log?scale)Pseudo test setsSO test setFigure 4: Determining the proportion of low, mid and highfrequency test word pairs in Gigaword (GW).instances), and Neighbor (3881 instances).
To eval-uate against the exact counts, we compute exactcounts for only those word pairs that are present inthe test sets.
Accuracy is used for evaluation andis defined as the percentage of number of correctlyidentified pseudo words.In Fig.
4, we plot the cumulative proportion oftrue frequency counts of all word pairs (from thethree tests) in Gigaword (GW).
To include unseenword pairs from test set in GW on log-scale in Fig.4, we increment the true counts of all the word pairsby 1.
This plot demonstrates that 45% of word-pairs are unseen in GW, and 67% of word pairs havecounts less than 10.
Hence, to perform better on thistask, it is essential to accurately maintain counts ofrare word pairs.In Table 1, we vary the size of all sketches (50million (M ), 100M , 200M , 500M and 1 billion(1B) counters) with 3 hash functions to comparethem against the exact counts.
It takes 1.8 GB un-compressed space to maintain the exact counts onthe disk.
Table 1 shows that with sketches of size> 200M on all the three test sets, CM-CU andLCU-WS are comparable to exact.
However, theCMM-CU sketch performs less well.
We conjec-ture the reason for such a behavior is due to loss ofrecall (information about low frequency word pairs)by under-estimation error.
For this task CM-CU andLCU-WS scales to storing 94M unique word pairsusing 200M integer (4 bytes each) counters (using800 MB) < 1.8 GB to maintain exact counts.
More-over, these results are comparable to Chambers andJurafsky?s state-of-the-art framework.Data Exact CM-CU CMM-CU LCU-WSGW 74.2 74.0 65.3 72.9GWB50 81.2 80.9 74.9 78.3Table 2: Evaluating Semantic Orientation on accuracy metricusing several sketches of 2 billion counters against exact.
Boldand italic numbers denote no statistically significant difference.4.3 Finding Semantic Orientation of a wordGiven a word, the task of finding its Semantic Ori-entation (SO) (Turney and Littman, 2003) is to de-termine if the word is more probable to be used inpositive or negative connotation.
We use Turney andLittman?s (2003) state-of-the-art framework to com-pute the SO of a word.
We use same seven pos-itive words (good, nice, excellent, positive, fortu-nate, correct, and superior) and same seven nega-tive words (bad, nasty, poor, negative, unfortunate,wrong, and inferior) from their framework as seeds.The SO of a given word is computed based on thestrength of its association with the seven positivewords and the seven negative words.
Associationscores are computed via Pointwise Mutual Informa-tion (PMI).
We compute the SO of a word ?w?
as:SO(W) =?p?Pos PMI(p,w)?
?n?Neg PMI(n,w)where, Pos and Neg denote the seven positive andnegative seeds respectively.
If this score is negative,we predict the word as negative; otherwise, we pre-dict it as positive.
We use the General Inquirer lex-icon2 (Stone et al 1966) as a benchmark to eval-uate the semantic orientation similar to Turney andLittman?s (2003) work.
Our test set consists of 1611positive and 1987 negative words.
Accuracy is usedfor evaluation and is defined as the percentage ofnumber of correctly identified SO words.We evaluate SO of words on two differentsized corpora (see Section 4.1): Gigaword (GW)(9.8GB), and GW with 50% news web corpus(GWB50) (49GB).
We fix the size of all sketchesto 2 billion (2B) counters with 5 hash functions.
Westore exact counts of all words in a hash table forboth GW and GWB50.
We count all word pairs(except word pairs involving stop words) that appearwithin a window of size 7 from GW and GWB50.This yields 2.67 billion(B) tokens and .19B types2The General Inquirer lexicon is freely available at http://www.wjh.harvard.edu/?inquirer/1100Test Set WS-203 MC-30Model CM-CUCMM-CULCU-WSCM-CUCMM-CULCU-WSPMI10M .58 .25 .28 .67 .20 .1650M .44 .23 .41 .61 .22 .31200M .53 .44 .47 .57 .28 .43Exact .52 .50LLR10M .47 .27 .29 .50 .29 .1050M .42 .31 .34 .48 .32 .35200M .41 .35 .39 .40 .31 .40Exact .42 .41Table 3: Evaluating distributional similarity using sketches.Scores are evaluated using rank correlation ?.
Bold and italicnumbers denote no statistically significant difference.from GW and 13.20B tokens and 0.8B types fromGWB50.
Next, we compare the sketches against theexact counts over two different size corpora.Table 2 shows that increasing the amount of dataimproves the accuracy of identifying the SO of aword.
We get an absolute increase of 7 percentagepoints (with exact counts) in accuracy (The 95% sta-tistical significance boundary for accuracy is about?
1.5.
), when we add 50% web data (GWB50).CM-CU results are equivalent to exact counts for allthe corpus sizes.
These results are also comparableto Turney?s (2003) accuracy of 82.84%.
However,CMM-CU results are worse by absolute 8.7 pointsand 6 points on GW and GWB50 respectively withrespect to CM-CU.
LCU-WS is better than CMM-CU but worse than CM-CU.
Using 2B integer (4bytes each) counters (bounded memory footprint of8 GB), CM-CU scales to 0.8B word pair types (Ittakes 16 GB uncompressed disk space to store exactcounts of all the unique word pair types.
).Figure 4 has similar frequency distribution ofword pairs3 in SO test set as pseudo-words evalu-ation test sets word pairs.
Hence, CMM-CU againhas substantially worse results than CM-CU due toloss of recall (information about low frequency wordpairs) by under-estimation error.
We can concludethat for this task CM-CU is best.4.4 Distributional SimilarityDistributional similarity is based on the distribu-tional hypothesis that similar terms appear in simi-3Consider only those pairs in which one word appears in theseed list and the other word appears in the test set.lar contexts (Firth, 1968; Harris, 1954).
The contextvector for each term is represented by the strengthof association between the term and each of the lex-ical, semantic, syntactic, and/or dependency unitsthat co-occur with it.
For this work, we define con-text for a given term as the surrounding words ap-pearing in a window of 2 words to the left and 2words to the right.
The context words are concate-nated along with their positions -2, -1, +1, and +2.We use PMI and LLR to compute the associationscore (AS) between the term and each of the contextto generate the context vector.
We use the cosinesimilarity measure to find the distributional similar-ity between the context vectors for each of the terms.We use two test sets which consist of word pairs,and their corresponding human rankings.
We gener-ate the word pair rankings using distributional sim-ilarity.
We report the Spearman?s rank correlationcoefficient (?)
between the human and distributionalsimilarity rankings.
We report results on two testsets: WS-203: A set of 203 word pairs marked ac-cording to similarity (Agirre et al 2009).
MC-30:A set of 30 noun pairs (Miller and Charles, 1991).We evaluate distributional similarity on Giga-word (GW) (9.8GB) (see Section 4.1).
First, westore exact counts of all words and contexts in a hashtable from GW.
Next, we count all the word-contextpairs and store them in CM-CU, CMM-CU, andLCU-WS sketches.
That generates a stream ofsize 3.35 billion (3.35B) word-context pair tokensand 215 million unique word-context pair types (Ittakes 4.6 GB uncompressed disk space to store exactcounts of all these unique word-context pair types.
).For every target word in the test set, we maintaintop-1000 approximate AS scores contexts using apriority queue, by passing over the corpus a secondtime.
Finally, we use cosine similarity with theseapproximate top-K context vectors to compute dis-tributional similarity.In Table 3, we vary the size of all sketches across10 million (M ), 50M , and 200M counters with 3hash functions.
The results using PMI shows thatCM-CU has best ?
on both WS-203 and MC-30test sets.
The results for LLR in Table 3 show simi-lar trends with CM-CU having best results on smallsize sketches.
Thus, CM-CU scales using 10Mcounters (using fixed memory of 40 MB versus 4.6GB to store exact counts).
These results are compa-1101rable against the state-of-the-art results for distribu-tional similarity (Agirre et al 2009).On this task CM-CU is best as it avoids lossof recall (information about low frequency wordpairs) due to under-estimation error.
For a targetword that has low frequency, using CMM-CU willnot generate any contexts for it, as it will havelarge under-estimation error for word-context pairscounts.
This phenomenon is demonstrated in Ta-ble 3, where CMM-CU and LCU-WS have worseresult with small size sketches.5 ConclusionIn this work, we systematically studied the problemof estimating point queries using different sketch al-gorithms.
As far as we know, this represents thefirst comparative study to demonstrate the relativebehavior of sketches in the context of NLP applica-tions.
We proposed two novel sketch variants: Countsketch (Charikar et al 2004) with conservative up-date (COUNT-CU) and Count-mean-min sketchwith conservative update (CMM-CU).
We empiri-cally showed that CMM-CU has under-estimationerror with small over-estimation error, CM-CU hasonly over-estimation error, and LCU-WS has moreunder-estimation error than over-estimation error.Finally, we demonstrated CM-CU has better re-sults on all three tasks: pseudo-words evaluationfor selectional preferences, finding semantic orien-tation task, and distributional similarity.
This showsthat maintaining information about low frequencyitems (even with over-estimation error) is better thanthrowing away information (under-estimation error)about rare items.Future work is to reduce the bit size of eachcounter (instead of the number of counters), as hasbeen tried for other summaries (Talbot and Osborne,2007; Talbot, 2009; Van Durme and Lall, 2009a) inNLP.
However, it may be challenging to combinethis with conservative update.AcknowledgmentsThis work was partially supported by NSF AwardIIS-1139909.
Thanks to Suresh Venkatasubrama-nian for useful discussions and the anonymous re-viewers for many helpful comments.ReferencesCharu C. Aggarwal and Philip S. Yu.
2010.
On classi-fication of high-cardinality data streams.
In SDM?10,pages 802?813.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In NAACL ?09: Pro-ceedings of HLT-NAACL.Shane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative learning of selectional preference fromunlabeled text.
In Proc.
EMNLP, pages 59?68, Hon-olulu, Hawaii, October.Burton H. Bloom.
1970.
Space/time trade-offs in hashcoding with allowable errors.
Communications of theACM, 13:422?426.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In Proceedings of EMNLP-CoNLL.Nathanael Chambers and Dan Jurafsky.
2010.
Improv-ing the use of pseudo-words for evaluating selectionalpreferences.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,ACL ?10, pages 445?453.
Association for Computa-tional Linguistics.Moses Charikar, Kevin Chen, and Martin Farach-Colton.2004.
Finding frequent items in data streams.
Theor.Comput.
Sci., 312:3?15, January.K.
Church and P. Hanks.
1989.
Word Associa-tion Norms, Mutual Information and Lexicography.In Proceedings of ACL, pages 76?83, Vancouver,Canada, June.Saar Cohen and Yossi Matias.
2003.
Spectral bloom fil-ters.
In Proceedings of the 2003 ACM SIGMOD in-ternational conference on Management of data, SIG-MOD ?03, pages 241?252.
ACM.Graham Cormode and Marios Hadjieleftheriou.
2008.Finding frequent items in data streams.
In VLDB.Graham Cormode and S. Muthukrishnan.
2004.
An im-proved data stream summary: The count-min sketchand its applications.
J. Algorithms.Graham Cormode.
2009.
Encyclopedia entry on ?Count-Min Sketch?.
In Encyclopedia of Database Systems,pages 511?516.
Springer.Graham Cormode.
2011.
Sketch techniques for approx-imate query processing.
Foundations and Trends inDatabases.
NOW publishers.Fan Deng and Davood Rafiei.
2007.
New estimation al-gorithms for streaming data: Count-min can do more.http://webdocs.cs.ualberta.ca/.Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N.Rothblum, and Sergey Yekhanin.
2010.
Pan-privatestreaming algorithms.
In Proceedings of ICS.1102Katrin Erk.
2007.
A simple, similarity-based model forselectional preferences.
In Proceedings of the 45th An-nual Meeting of the Association of Computational Lin-guistics, volume 45, pages 216?223.
Association forComputational Linguistics.Cristian Estan and George Varghese.
2002.
New di-rections in traffic measurement and accounting.
SIG-COMM Comput.
Commun.
Rev., 32(4).J.
Firth.
1968.
A synopsis of linguistic theory 1930-1955.
In F. Palmer, editor, Selected Papers of J. R.Firth.
Longman.Amit Goyal and Hal Daume?
III.
2011a.
Approximatescalable bounded space sketch for large data NLP.
InEmpirical Methods in Natural Language Processing(EMNLP).Amit Goyal and Hal Daume?
III.
2011b.
Lossy con-servative update (LCU) sketch: Succinct approximatecount storage.
In Conference on Artificial Intelligence(AAAI).Amit Goyal, Hal Daume?
III, and Suresh Venkatasubra-manian.
2009.
Streaming for large scale NLP: Lan-guage modeling.
In NAACL.D.
Graff.
2003.
English Gigaword.
Linguistic Data Con-sortium, Philadelphia, PA, January.Z.
Harris.
1954.
Distributional structure.
Word 10 (23),pages 146?162.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for SMT.
InEMNLP, August.Ping Li, Kenneth Ward Church, and Trevor Hastie.
2008.One sketch for all: Theory and application of condi-tional random sampling.
In Neural Information Pro-cessing Systems, pages 953?960.G.
S. Manku and R. Motwani.
2002.
Approximate fre-quency counts over data streams.
In VLDB.G.A.
Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and nlp: using localitysensitive hash function for high speed noun clustering.In Proceedings of ACL.Florin Rusu and Alin Dobra.
2007.
Statistical analysis ofsketch estimators.
In SIGMOD ?07.
ACM.Stuart Schechter, Cormac Herley, and Michael Mitzen-macher.
2010.
Popularity is everything: a newapproach to protecting passwords from statistical-guessing attacks.
In Proceedings of the 5th USENIXconference on Hot topics in security, HotSec?10, pages1?8, Berkeley, CA, USA.
USENIX Association.Qinfeng Shi, James Petterson, Gideon Dror, John Lang-ford, Alex Smola, and S.V.N.
Vishwanathan.
2009.Hash kernels for structured data.
Journal MachineLearning Research, 10:2615?2637, December.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General Inquirer:A Computer Approach to Content Analysis.
MITPress.David Talbot and Thorsten Brants.
2008.
Randomizedlanguage models via perfect hash functions.
In Pro-ceedings of ACL-08: HLT.David Talbot and Miles Osborne.
2007.
SmoothedBloom filter language models: Tera-scale LMs on thecheap.
In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Processingand Computational Natural Language Learning (EMNLP-CoNLL).David Talbot.
2009.
Succinct approximate counting ofskewed data.
In IJCAI?09: Proceedings of the 21stinternational jont conference on Artifical intelligence.Peter D. Turney and Michael L. Littman.
2003.
Measur-ing praise and criticism: Inference of semantic orienta-tion from association.
ACM Trans.
Inf.
Syst., 21:315?346, October.Peter D. Turney.
2008.
A uniform approach to analogies,synonyms, antonyms, and associations.
In Proceed-ings of COLING 2008.Benjamin Van Durme and Ashwin Lall.
2009a.
Prob-abilistic counting with randomized storage.
In IJ-CAI?09: Proceedings of the 21st international jontconference on Artifical intelligence.Benjamin Van Durme and Ashwin Lall.
2009b.
Stream-ing pointwise mutual information.
In Advances inNeural Information Processing Systems 22.Benjamin Van Durme and Ashwin Lall.
2010.
Onlinegeneration of locality sensitive hash signatures.
InProceedings of the ACL 2010 Conference Short Pa-pers, pages 231?235, July.1103
