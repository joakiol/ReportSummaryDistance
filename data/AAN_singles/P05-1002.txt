Proceedings of the 43rd Annual Meeting of the ACL, pages 10?17,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsScaling Conditional Random Fields Using Error-Correcting CodesTrevor CohnDepartment of Computer Scienceand Software EngineeringUniversity of Melbourne, Australiatacohn@csse.unimelb.edu.auAndrew SmithDivision of InformaticsUniversity of EdinburghUnited Kingdoma.p.smith-2@sms.ed.ac.ukMiles OsborneDivision of InformaticsUniversity of EdinburghUnited Kingdommiles@inf.ed.ac.ukAbstractConditional Random Fields (CRFs) havebeen applied with considerable success toa number of natural language processingtasks.
However, these tasks have mostlyinvolved very small label sets.
Whendeployed on tasks with larger labelsets, the requirements for computationalresources mean that training becomesintractable.This paper describes a method for train-ing CRFs on such tasks, using error cor-recting output codes (ECOC).
A numberof CRFs are independently trained on theseparate binary labelling tasks of distin-guishing between a subset of the labelsand its complement.
During decoding,these models are combined to produce apredicted label sequence which is resilientto errors by individual models.Error-correcting CRF training is muchless resource intensive and has a muchfaster training time than a standardlyformulated CRF, while decodingperformance remains quite comparable.This allows us to scale CRFs to previouslyimpossible tasks, as demonstrated by ourexperiments with large label sets.1 IntroductionConditional random fields (CRFs) (Lafferty etal., 2001) are probabilistic models for labellingsequential data.
CRFs are undirected graphicalmodels that define a conditional distribution overlabel sequences given an observation sequence.They allow the use of arbitrary, overlapping,non-independent features as a result of their globalconditioning.
This allows us to avoid makingunwarranted independence assumptions over theobservation sequence, such as those required bytypical generative models.Efficient inference and training methods existwhen the graphical structure of the model formsa chain, where each position in a sequence isconnected to its adjacent positions.
CRFs have beenapplied with impressive empirical results to thetasks of named entity recognition (McCallum andLi, 2003), simplified part-of-speech (POS) tagging(Lafferty et al, 2001), noun phrase chunking (Shaand Pereira, 2003) and extraction of tabular data(Pinto et al, 2003), among other tasks.CRFs are usually estimated using gradient-basedmethods such as limited memory variable metric(LMVM).
However, even with these efficientmethods, training can be slow.
Consequently, mostof the tasks to which CRFs have been applied arerelatively small scale, having only a small numberof training examples and small label sets.
Formuch larger tasks, with hundreds of labels andmillions of examples, current training methodsprove intractable.
Although training can potentiallybe parallelised and thus run more quickly on largeclusters of computers, this in itself is not a solutionto the problem: tasks can reasonably be expectedto increase in size and complexity much fasterthan any increase in computing power.
In order toprovide scalability, the factors which most affect theresource usage and runtime of the training method10must be addressed directly ?
ideally the dependenceon the number of labels should be reduced.This paper presents an approach which enablesCRFs to be used on larger tasks, with a significantreduction in the time and resources needed fortraining.
This reduction does not come at the costof performance ?
the results obtained on benchmarknatural language problems compare favourably,and sometimes exceed, the results produced fromregular CRF training.
Error correcting outputcodes (ECOC) (Dietterich and Bakiri, 1995) areused to train a community of CRFs on binarytasks, with each discriminating between a subsetof the labels and its complement.
Inference isperformed by applying these ?weak?
models to anunknown example, with each component modelremoving some ambiguity when predicting the labelsequence.
Given a sufficient number of binarymodels predicting suitably diverse label subsets, thelabel sequence can be inferred while being robustto a number of individual errors from the weakmodels.
As each of these weak models are binary,individually they can be efficiently trained, evenon large problems.
The number of weak learnersrequired to achieve good performance is shown tobe relatively small on practical tasks, such that theoverall complexity of error-correcting CRF trainingis found to be much less than that of regular CRFtraining methods.We have evaluated the error-correcting CRF onthe CoNLL 2003 named entity recognition (NER)task (Sang and Meulder, 2003), where we showthat the method yields similar generalisation perfor-mance to standardly formulated CRFs, while requir-ing only a fraction of the resources, and no increasein training time.
We have also shown how the error-correcting CRF scales when applied to the largertask of POS tagging the Penn Treebank and alsothe even larger task of simultaneously noun phrasechunking (NPC) and POS tagging using the CoNLL2000 data-set (Sang and Buchholz, 2000).2 Conditional random fieldsCRFs are undirected graphical models used to spec-ify the conditional probability of an assignment ofoutput labels given a set of input observations.
Weconsider only the case where the output labels of themodel are connected by edges to form a linear chain.The joint distribution of the label sequence, y, giventhe input observation sequence, x, is given byp(y|x) = 1Z(x) expT+1?t=1?k?kfk(t,yt?1,yt,x)where T is the length of both sequences and ?k arethe parameters of the model.
The functions fk arefeature functions which map properties of the obser-vation and the labelling into a scalar value.
Z(x)is the partition function which ensures that p is aprobability distribution.A number of algorithms can be used to find theoptimal parameter values by maximising the log-likelihood of the training data.
Assuming that thetraining sequences are drawn IID from the popula-tion, the conditional log likelihood L is given byL =?ilog p(y(i)|x(i))=?i??
?T (i)+1?t=1?k?kfk(t,y(i)t?1,y(i)t ,x(i))?
log Z(x(i))}where x(i) and y(i) are the ith observation and labelsequence.
Note that a prior is often included in theL formulation; it has been excluded here for clar-ity of exposition.
CRF estimation methods includegeneralised iterative scaling (GIS), improved itera-tive scaling (IIS) and a variety of gradient basedmethods.
In recent empirical studies on maximumentropy models and CRFs, limited memory variablemetric (LMVM) has proven to be the most efficientmethod (Malouf, 2002; Wallach, 2002); accord-ingly, we have used LMVM for CRF estimation.Every iteration of LMVM training requires thecomputation of the log-likelihood and its deriva-tive with respect to each parameter.
The partitionfunction Z(x) can be calculated efficiently usingdynamic programming with the forward algorithm.Z(x) is given by?y ?T (y) where ?
are the forwardvalues, defined recursively as?t+1(y) =?y??t(y?)
exp?k?kfk(t + 1, y?, y,x)11The derivative of the log-likelihood is given by?L??k=?i??
?T (i)+1?t=1fk(t,y(i)t?1,y(i)t ,x(i))?
?yp(y|x(i))T (i)+1?t=1fk(t,yt?1,yt,x(i))??
?The first term is the empirical count of feature k,and the second is the expected count of the featureunder the model.
When the derivative equals zero ?at convergence ?
these two terms are equal.
Evalu-ating the first term of the derivative is quite simple.However, the sum over all possible labellings in thesecond term poses more difficulties.
This term canbe factorised, yielding?t?y?,yp(Yt?1 = y?, Yt = y|x(i))fk(t, y?, y,x(i))This term uses the marginal distribution over pairs oflabels, which can be efficiently computed from theforward and backward values as?t?1(y?)
exp?k ?kfk(t, y?, y,x(i))?t(y)Z(x(i))The backward probabilities ?
are defined by therecursive relation?t(y) =?y??t+1(y?)
exp?k?kfk(t + 1, y, y?,x)Typically CRF training using LMVM requiresmany hundreds or thousands of iterations, each ofwhich involves calculating of the log-likelihoodand its derivative.
The time complexity of a singleiteration is O(L2NTF ) where L is the numberof labels, N is the number of sequences, T isthe average length of the sequences, and F isthe average number of activated features of eachlabelled clique.
It is not currently possible to stateprecise bounds on the number of iterations requiredfor certain problems; however, problems with alarge number of sequences often require many moreiterations to converge than problems with fewersequences.
Note that efficient CRF implementationscache the feature values for every possible cliquelabelling of the training data, which leads to amemory requirement with the same complexity ofO(L2NTF ) ?
quite demanding even for currentcomputer hardware.3 Error Correcting Output CodesSince the time and space complexity of CRFestimation is dominated by the square of the numberof labels, it follows that reducing the numberof labels will significantly reduce the complexity.Error-correcting coding is an approach which recastsmultiple label problems into a set of binary labelproblems, each of which is of lesser complexity thanthe full multiclass problem.
Interestingly, training aset of binary CRF classifiers is overall much moreefficient than training a full multi-label model.
Thisis because error-correcting CRF training reducesthe L2 complexity term to a constant.
Decodingproceeds by predicting these binary labels and thenrecovering the encoded actual label.Error-correcting output codes have been used fortext classification, as in Berger (1999), on which thefollowing is based.
Begin by assigning to each of them labels a unique n-bit string Ci, which we will callthe code for this label.
Now train n binary classi-fiers, one for each column of the coding matrix (con-structed by taking the labels?
codes as rows).
The j thclassifier, ?j , takes as positive instances those withlabel i where Cij = 1.
In this way, each classifierlearns a different concept, discriminating betweendifferent subsets of the labels.We denote the set of binary classifiers as?
= {?1, ?2, .
.
.
, ?n}, which can be used forprediction as follows.
Classify a novel instance xwith each of the binary classifiers, yielding a n-bitvector ?
(x) = {?1(x), ?2(x), .
.
.
, ?n(x)}.
Nowcompare this vector to the codes for each label.
Thevector may not exactly match any of the labels dueto errors in the individual classifiers, and thus wechose the actual label which minimises the distanceargmini?(?
(x), Ci).
Typically the Hammingdistance is used, which simply measures the numberof differing bit positions.
In this manner, predictionis resilient to a number of prediction errors by thebinary classifiers, provided the codes for the labelsare sufficiently diverse.3.1 Error-correcting CRF trainingError-correcting codes can also be applied tosequence labellers, such as CRFs, which are capableof multiclass labelling.
ECOCs can be used withCRFs in a similar manner to that given above for12classifiers.
A series of CRFs are trained, eachon a relabelled variant of the training data.
Therelabelling for each binary CRF maps the labelsinto binary space using the relevant column of thecoding matrix, such that label i is taken as a positivefor the jth model example if Cij = 1.Training with a binary label set reduces the timeand space complexity for each training iteration toO(NTF ); the L2 term is now a constant.
Pro-vided the code is relatively short (i.e.
there arefew binary models, or weak learners), this translatesinto considerable time and space savings.
Codingtheory doesn?t offer any insights into the optimalcode length (i.e.
the number of weak learners).When using a very short code, the error-correctingCRF will not adequately model the decision bound-aries between all classes.
However, using a longcode will lead to a higher degree of dependencybetween pairs of classifiers, where both model simi-lar concepts.
The generalisation performance shouldimprove quickly as the number of weak learners(code length) increases, but these gains will diminishas the inter-classifier dependence increases.3.2 Error-correcting CRF decodingWhile training of error-correcting CRFs is simplya logical extension of the ECOC classifier methodto sequence labellers, decoding is a different mat-ter.
We have applied three decoding different strate-gies.
The Standalone method requires each binaryCRF to find the Viterbi path for a given sequence,yielding a string of 0s and 1s for each model.
Foreach position t in the sequence, the tth bit fromeach model is taken, and the resultant bit stringcompared to each of the label codes.
The labelwith the minimum Hamming distance is then cho-sen as the predicted label for that site.
This methodallows for error correction to occur at each site, how-ever it discards information about the uncertainty ofeach weak learner, instead only considering the mostprobable paths.The Marginals method of decoding uses themarginal probability distribution at each positionin the sequence instead of the Viterbi paths.
Thisdistribution is easily computed using the forwardbackward algorithm.
The decoding proceeds asbefore, however instead of a bit string we have avector of probabilities.
This vector is comparedto each of the label codes using the L1 distance,and the closest label is chosen.
While this methodincorporates the uncertainty of the binary models, itdoes so at the expense of the path information in thesequence.Neither of these decoding methods allow themodels to interact, although each individual weaklearner may benefit from the predictions of theother weak learners.
The Product decoding methodaddresses this problem.
It treats each weak modelas an independent predictor of the label sequence,such that the probability of the label sequence giventhe observations can be re-expressed as the productof the probabilities assigned by each weak model.A given labelling y is projected into a bit string foreach weak learner, such that the ith entry in thestring is Ckj for the jth weak learner, where k isthe index of label yi.
The weak learners can thenestimate the probability of the bit string; these arethen combined into a global product to give theprobability of the label sequencep(y|x) = 1Z ?
(x)?jpj(bj(y)|x)where pj(q|x) is the predicted probability of q givenx by the jth weak learner, bj(y) is the bit stringrepresenting y for the jth weak learner and Z ?
(x)is the partition function.
The log probability is?j{Fj(bj(y), x) ?
?j ?
log Zj(x)} ?
log Z ?
(x)where Fj(y, x) = ?T+1t=1 fj(t,yt?1,yt,x).
This logprobability can then be maximised using the Viterbialgorithm as before, noting that the two log terms areconstant with respect to y and thus need not be eval-uated.
Note that this decoding is an equivalent for-mulation to a uniformly weighted logarithmic opin-ion pool, as described in Smith et al (2005).Of the three decoding methods, Standalonehas the lowest complexity, requiring only a binaryViterbi decoding for each weak learner.
Marginalsis slightly more complex, requiring the forwardand backward values.
Product, however, requiresViterbi decoding with the full label set, and manyfeatures ?
the union of the features of each weaklearner ?
which can be quite computationallydemanding.133.3 Choice of codeThe accuracy of ECOC methods are highly depen-dent on the quality of the code.
The ideal codehas diverse rows, yielding a high error-correctingcapability, and diverse columns such that the weaklearners model highly independent concepts.
Whenthe number of labels, k, is small, an exhaustivecode with every unique column is reasonable, giventhere are 2k?1 ?
1 unique columns.
With largerlabel sets, columns must be selected with care tomaximise the inter-row and inter-column separation.This can be done by randomly sampling the columnspace, in which case the probability of poor separa-tion diminishes quickly as the number of columnsincreases (Berger, 1999).
Algebraic codes, such asBCH codes, are an alternative coding scheme whichcan provide near-optimal error-correcting capabil-ity (MacWilliams and Sloane, 1977), however thesecodes provide no guarantee of good column separa-tion.4 ExperimentsOur experiments show that error-correcting CRFsare highly accurate on benchmark problems withsmall label sets, as well as on larger problems withmany more labels, which would be otherwise proveintractable for traditional CRFs.
Moreover, with agood code, the time and resources required for train-ing and decoding can be much less than that of thestandardly formulated CRF.4.1 Named entity recognitionCRFs have been used with strong results on theCoNLL 2003 NER task (McCallum, 2003) and thusthis task is included here as a benchmark.
This dataset consists of a 14,987 training sentences (204,567tokens) drawn from news articles, tagged for per-son, location, organisation and miscellaneous enti-ties.
There are 8 IOB-2 style labels.A multiclass (standardly formulated) CRF wastrained on these data using features covering wordidentity, word prefix and suffix, orthographic testsfor digits, case and internal punctuation, wordlength, POS tag and POS tag bigrams before andafter the current word.
Only features seen at leastonce in the training data were included in the model,resulting in 450,345 binary features.
The model wasModel Decoding MLE RegularisedMulticlass 88.04 89.78Coded standalone 88.23?
88.67?marginals 88.23?
89.19product 88.69?
89.69Table 1: F1 scores on NER task.trained without regularisation and with a Gaussianprior.
An exhaustive code was created with all127 unique columns.
All of the weak learnerswere trained with the same feature set, each havingaround 315,000 features.
The performance of thestandard and error-correcting models are shown inTable 1.
We tested for statistical significance usingthe matched pairs test (Gillick and Cox, 1989) atp < 0.001.
Those results which are significantlybetter than the corresponding multiclass MLE orregularised model are flagged with a ?, and thosewhich are significantly worse with a ?.These results show that error-correcting CRFtraining achieves quite similar performance to themulticlass CRF on the task (which incidentallyexceeds McCallum (2003)?s result of 89.0 usingfeature induction).
Product decoding was thebetter of the three methods, giving the bestperformance both with and without regularisation,although this difference was only statisticallysignificant between the regularised standalone andthe regularised product decoding.
The unregularisederror-correcting CRF significantly outperformedthe multiclass CRF with all decoding strategies,suggesting that the method already provides someregularisation, or corrects some inherent bias in themodel.Using such a large number of weak learners iscostly, in this case taking roughly ten times longerto train than the multiclass CRF.
However, muchshorter codes can also achieve similar results.
Thesimplest code, where each weak learner predictsonly a single label (a.k.a.
one-vs-all), achieved anF score of 89.56, while only requiring 8 weak learn-ers and less than half the training time as the multi-class CRF.
This code has no error correcting capa-bility, suggesting that the code?s column separation(and thus interdependence between weak learners)is more important than its row separation.14An exhaustive code was used in this experimentsimply for illustrative purposes: many columnsin this code were unnecessary, yielding only aslight gain in performance over much simplercodes while incurring a very large increase intraining time.
Therefore, by selecting a good subsetof the exhaustive code, it should be possible toreduce the training time while preserving the stronggeneralisation performance.
One approach is toincorporate skew in the label distribution in ourchoice of code ?
the code should minimise theconfusability of commonly occurring labels moreso than that of rare labels.
Assuming that errorsmade by the weak learners are independent, theprobability of a single error, q, as a function of thecode length n can be bounded byq(n) ?
1 ?
?lp(l)bhl?12 c?i=0(ni)p?i(1 ?
p?
)n?iwhere p(l) is the marginal probability of the label l,hl is the minimum Hamming distance between l andany other label, and p?
is the maximum probabilityof an error by a weak learner.
The performanceachieved by selecting the code with the minimumloss bound from a large random sample of codesis shown in Figure 1, using standalone decoding,where p?
was estimated on the development set.
Forcomparison, randomly sampled codes and a greedyoracle are shown.
The two random sampled codesshow those samples where no column is repeated,and where duplicate columns are permitted (randomwith replacement).
The oracle repeatedly adds to thecode the column which most improves its F1 score.The minimum loss bound method allows the per-formance plateau to be reached more quickly thanrandom sampling; i.e.
shorter codes can be used,thus allowing more efficient training and decoding.Note also that multiclass CRF training required830Mb of memory, while error-correcting trainingrequired only 380Mb.
Decoding of the test set(51,362 tokens) with the error-correcting model(exhaustive, MLE) took between 150 seconds forstandalone decoding and 173 seconds for integrateddecoding.
The multiclass CRF was much faster,taking only 31 seconds, however this time differencecould be reduced with suitable optimisations.838485868788899010  15  20  25  30  35  40  45  50F1scorecode lengthrandomrandom with replacementminimum loss boundoracleMLE multiclass CRFRegularised multiclass CRFFigure 1: NER F1 scores for standalone decodingwith random codes, a minimum loss code and agreedy oracle.Coding Decoding MLE RegularisedMulticlass 95.69 95.78Coded - 200 standalone 95.63 96.03marginals 95.68 96.03One-vs-all product 94.90 96.57Table 2: POS tagging accuracy.4.2 Part-of-speech TaggingCRFs have been applied to POS tagging, howeveronly with a very simple feature set and small trainingsample (Lafferty et al, 2001).
We used the PennTreebank Wall Street Journal articles, training onsections 2?21 and testing on section 24.
In thistask there are 45,110 training sentences, a total of1,023,863 tokens and 45 labels.The features used included word identity, prefixand suffix, whether the word contains a number,uppercase letter or a hyphen, and the words oneand two positions before and after the current word.A random code of 200 columns was used for thistask.
These results are shown in Table 2, along withthose of a multiclass CRF and an alternative one-vs-all coding.
As for the NER experiment, the decod-ing performance levelled off after 100 bits, beyondwhich the improvements from longer codes wereonly very slight.
This is a very encouraging char-acteristic, as only a small number of weak learnersare required for good performance.15The random code of 200 bits required 1,300Mbof RAM, taking a total of 293 hours to train and3 hours to decode (54,397 tokens) on similarmachines to those used before.
We do not havefigures regarding the resources used by Lafferty etal.
?s CRF for the POS tagging task and our attemptsto train a multiclass CRF for full-scale POS taggingwere thwarted due to lack of sufficient availablecomputing resources.
Instead we trained on a10,000 sentence subset of the training data, whichrequired approximately 17Gb of RAM and 208hours to train.Our best result on the task was achieved usinga one-vs-all code, which reduced the trainingtime to 25 hours, as it only required training 45binary models.
This result exceeds Lafferty et al?saccuracy of 95.73% using a CRF but falls short ofToutanova et al (2003)?s state-of-the-art 97.24%.This is most probably due to our only using afirst-order Markov model and a fairly simple featureset, where Tuotanova et al include a richer set offeatures in a third order model.4.3 Part-of-speech Tagging and Noun PhraseSegmentationThe joint task of simultaneously POS tagging andnoun phrase chunking (NPC) was included in orderto demonstrate the scalability of error-correctingCRFs.
The data was taken from the CoNLL 2000NPC shared task, with the model predicting both thechunk tags and the POS tags.
The training corpusconsisted of 8,936 sentences, with 47,377 tokensand 118 labels.A 200-bit random code was used, with the follow-ing features: word identity within a window, pre-fix and suffix of the current word and the presenceof a digit, hyphen or upper case letter in the cur-rent word.
This resulted in about 420,000 featuresfor each weak learner.
A joint tagging accuracy of90.78% was achieved using MLE training and stan-dalone decoding.
Despite the large increase in thenumber of labels in comparison to the earlier tasks,the performance also began to plateau at around 100bits.
This task required 220Mb of RAM and took atotal of 30 minutes to train each of the 200 binaryCRFs, this time on Pentium 4 machines with 1GbRAM.
Decoding of the 47,377 test tokens took 9,748seconds and 9,870 seconds for the standalone andmarginals methods respectively.Sutton et al (2004) applied a variant of the CRF,the dynamic CRF (DCRF), to the same task, mod-elling the data with two interconnected chains whereone chain predicted NPC tags and the other POStags.
They achieved better performance and train-ing times than our model; however, this is not afair comparison, as the two approaches are orthogo-nal.
Indeed, applying the error-correcting CRF algo-rithms to DCRF models could feasibly decrease thecomplexity of the DCRF, allowing the method to beapplied to larger tasks with richer graphical struc-tures and larger label sets.In all three experiments, error-correcting CRFshave achieved consistently good generalisation per-formance.
The number of weak learners requiredto achieve these results was shown to be relativelysmall, even for tasks with large label sets.
The timeand space requirements were lower than those of atraditional CRF for the larger tasks and, most impor-tantly, did not increase substantially when the num-ber of labels was increased.5 Related workMost recent work on improving CRF performancehas focused on feature selection.
McCallum (2003)describes a technique for greedily adding thosefeature conjuncts to a CRF which significantlyimprove the model?s log-likelihood.
His experi-mental results show that feature induction yields alarge increase in performance, however our resultsshow that standardly formulated CRFs can performwell above their reported 73.3%, casting doubton the magnitude of the possible improvement.Roark et al (2004) have also employed featureselection to the huge task of language modellingwith a CRF, by partially training a voted perceptronthen removing all features that the are ignoredby the perceptron.
The act of automatic featureselection can be quite time consuming in itself,while the performance and runtime gains are oftenmodest.
Even with a reduced number of features,tasks with a very large label space are likely toremain intractable.166 ConclusionStandard training methods for CRFs suffer greatlyfrom their dependency on the number of labels,making tasks with large label sets either difficultor impossible.
As CRFs are deployed more widelyto tasks with larger label sets this problem willbecome more evident.
The current ?solutions?
tothese scaling problems ?
namely feature selection,and the use of large clusters ?
don?t address theheart of the problem: the dependence on the squareof number of labels.Error-correcting CRF training allows CRFs to beapplied to larger problems and those with largerlabel sets than were previously possible, withoutrequiring computationally demanding methods suchas feature selection.
On standard tasks we haveshown that error-correcting CRFs provide compa-rable or better performance than the standardly for-mulated CRF, while requiring less time and space totrain.
Only a small number of weak learners wererequired to obtain good performance on the taskswith large label sets, demonstrating that the methodprovides efficient scalability to the CRF framework.Error-correction codes could be applied toother sequence labelling methods, such as thevoted perceptron (Roark et al, 2004).
This mayyield an increase in performance and efficiencyof the method, as its runtime is also heavilydependent on the number of labels.
We plan toapply error-correcting coding to dynamic CRFs,which should result in better modelling of naturallylayered tasks, while increasing the efficiency andscalability of the method.
We also plan to develophigher order CRFs, using error-correcting codes tocurb the increase in complexity.7 AcknowledgementsThis work was supported in part by a PORES travel-ling scholarship from the University of Melbourne,allowing Trevor Cohn to travel to Edinburgh.ReferencesAdam Berger.
1999.
Error-correcting output coding fortext classification.
In Proceedings of IJCAI: Workshop onmachine learning for information filtering.Thomas G. Dietterich and Ghulum Bakiri.
1995.
Solving mul-ticlass learning problems via error-correcting output codes.Journal of Artificial Intelligence Reseach, 2:263?286.L.
Gillick and Stephen Cox.
1989.
Some statistical issues inthe comparison of speech recognition algorithms.
In Pro-ceedings of the IEEE Conference on Acoustics, Speech andSignal Processing, pages 532?535, Glasgow, Scotland.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.Conditional random fields: Probabilistic models for seg-menting and labelling sequence data.
In Proceedings ofICML 2001, pages 282?289.Florence MacWilliams and Neil Sloane.
1977.
The theory oferror-correcting codes.
North Holland, Amsterdam.Robert Malouf.
2002.
A comparison of algorithms for max-imum entropy parameter estimation.
In Proceedings ofCoNLL 2002, pages 49?55.Andrew McCallum and Wei Li.
2003.
Early results for namedentity recognition with conditional random fields, featureinduction and web-enhanced lexicons.
In Proceedings ofCoNLL 2003, pages 188?191.Andrew McCallum.
2003.
Efficiently inducing features ofconditional random fields.
In Proceedings of UAI 2003,pages 403?410.David Pinto, Andrew McCallum, Xing Wei, and Bruce Croft.2003.
Table extraction using conditional random fields.In Proceedings of the Annual International ACM SIGIRConference on Research and Development in InformationRetrieval, pages 235?242.Brian Roark, Murat Saraclar, Michael Collins, and Mark John-son.
2004.
Discriminative language modeling with condi-tional random fields and the perceptron algorithm.
In Pro-ceedings of ACL 2004, pages 48?55.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.
Introduc-tion to the CoNLL-2000 shared task: Chunking.
In Proceed-ings of CoNLL 2000 and LLL 2000, pages 127?132.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
Introduc-tion to the CoNLL-2003 shared task: Language-independentnamed entity recognition.
In Proceedings of CoNLL 2003,pages 142?147, Edmonton, Canada.Fei Sha and Fernando Pereira.
2003.
Shallow parsing withconditional random fields.
In Proceedings of HLT-NAACL2003, pages 213?220.Andrew Smith, Trevor Cohn, and Miles Osborne.
2005.
Loga-rithmic opinion pools for conditional random fields.
In Pro-ceedings of ACL 2005.Charles Sutton, Khashayar Rohanimanesh, and Andrew McCal-lum.
2004.
Dynamic conditional random fields: Factorizedprobabilistic models for labelling and segmenting sequencedata.
In Proceedings of the ICML 2004.Kristina Toutanova, Dan Klein, Christopher Manning, andYoram Singer.
2003.
Feature rich part-of-speech taggingwith a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, pages 252?259.Hanna Wallach.
2002.
Efficient training of conditional randomfields.
Master?s thesis, University of Edinburgh.17
