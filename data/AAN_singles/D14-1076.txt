Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 691?701,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsImproving Multi-documents Summarization by Sentence Compressionbased on Expanded Constituent Parse TreesChen Li1, Yang Liu1, Fei Liu2, Lin Zhao3, Fuliang Weng31 Computer Science Department, The University of Texas at DallasRichardson, TX 75080, USA2 School of Computer Science, Carnegie Mellon UniversityPittsburgh, PA 15213, USA3 Research and Technology Center, Robert Bosch LLCPalo Alto, California 94304, USA{chenli,yangl@hlt.utdallas.edu}{feiliu@cs.cmu.edu}{lin.zhao,fuliang.weng@us.bosch.com}AbstractIn this paper, we focus on the problemof using sentence compression techniquesto improve multi-document summariza-tion.
We propose an innovative sentencecompression method by considering everynode in the constituent parse tree and de-ciding its status ?
remove or retain.
In-teger liner programming with discrimina-tive training is used to solve the problem.Under this model, we incorporate variousconstraints to improve the linguistic qual-ity of the compressed sentences.
Then weutilize a pipeline summarization frame-work where sentences are first compressedby our proposed compression model to ob-tain top-n candidates and then a sentenceselection module is used to generate thefinal summary.
Compared with state-of-the-art algorithms, our model has simi-lar ROUGE-2 scores but better linguisticquality on TAC data.1 IntroductionAutomatic summarization can be broadly dividedinto two categories: extractive and abstractivesummarization.
Extractive summarization focuseson selecting salient sentences from the documentcollection and concatenating them to form a sum-mary; while abstractive summarization is gener-ally considered more difficult, involving sophisti-cated techniques for meaning representation, con-tent planning, surface realization, etc.There has been a surge of interest in recent yearson generating compressed document summaries asa viable step towards abstractive summarization.These compressive summaries often contain moreinformation than sentence-based extractive sum-maries since they can remove insignificant sen-tence constituents and make space for more salientinformation that is otherwise dropped due to thesummary length constraint.
Two general strate-gies have been used for compressive summariza-tion.
One is a pipeline approach, where sentence-based extractive summarization is followed or pro-ceeded by sentence compression (Lin, 2003; Zajicet al., 2007; Vanderwende et al., 2007; Wang et al.,2013).
Another line of work uses joint compres-sion and summarization.
Such methods have beenshown to achieve promising performance (Daume?,2006; Chali and Hasan, 2012; Almeida and Mar-tins, 2013; Qian and Liu, 2013), but they are typi-cally computationally expensive.In this study, we propose an innovative sen-tence compression model based on expanded con-stituent parse trees.
Our model uses integer lin-ear programming (ILP) to search the entire spaceof compression, and is discriminatively trained.It is built based on the discriminative sentencecompression model from (McDonald, 2006) and(Clarke and Lapata, 2008), but our method usesan expanded constituent parse tree rather than onlythe leaf nodes in previous work.
Therefore wecan extract rich features for every node in the con-stituent parser tree.
This is an advantage of tree-based compression technique (Knight and Marcu,2000; Galley and McKeown, 2007; Wang et al.,2013).
Similar to (Li et al., 2013a), we use apipeline summarization framework where multi-ple compression candidates are generated for eachpre-selected important sentence, and then an ILP-691based summarization model is used to select thefinal compressed sentences.
We evaluate our pro-posed method on the TAC 2008 and 2011 datasets using the standard ROUGE metric (Lin, 2004)and human evaluation of the linguistic quality.Our results show that using our proposed sentencecompression model in the summarization systemcan yield significant performance gain in linguis-tic quality, without losing much performance onthe ROUGE metric.2 Related WorkSummarization research has seen great develop-ment over the last fifty years (Nenkova and McKe-own, 2011).
Compared to the abstractive counter-part, extractive summarization has received con-siderable attention due to its clear problem for-mulation: to extract a set of salient and non-redundant sentences from the given documentset.
Both unsupervised and supervised approacheshave been explored for sentence selection.
Su-pervised approaches include the Bayesian classi-fier (Kupiec et al., 1995), maximum entropy (Os-borne, 2002), skip-chain CRF (Galley, 2006), dis-criminative reranking (Aker et al., 2010), amongothers.
The extractive summary sentence selec-tion problem can also be formulated in an opti-mization framework.
Previous methods includeusing integer linear programming (ILP) and sub-modular functions to solve the optimization prob-lem (Gillick et al., 2009; Li et al., 2013b; Lin andBilmes, 2010).Compressive summarization receives increas-ing attention in recent years, since it offers a vi-able step towards abstractive summarization.
Thecompressed summaries can be generated through ajoint model of the sentence selection and compres-sion processes, or through a pipeline approach thatintegrates a sentence compression model with asummary sentence pre-selection or post-selectionstep.Many studies have explored the joint sentencecompression and selection setting.
Martins andSmith (2009) jointly performed sentence extrac-tion and compression by solving an ILP prob-lem.
Berg-Kirkpatrick et al.
(2011) proposed anapproach to score the candidate summaries ac-cording to a combined linear model of extrac-tive sentence selection and compression.
Theytrained the model using a margin-based objec-tive whose loss captures the final summary qual-ity.
Woodsend and Lapata (2012) presented an-other method where the summary?s informative-ness, succinctness, and grammaticality are learnedseparately from data but optimized jointly using anILP setup.
Yoshikawa et al.
(2012) incorporatedsemantic role information in the ILP model.Our work is closely related with the pipelineapproach, where sentence-based extractive sum-marization is followed or proceeded by sentencecompression.
There have been many studies onsentence compression, independent of the summa-rization task.
McDonald (2006) firstly introduceda discriminative sentence compression model todirectly optimize the quality of the compressedsentences produced.
Clarke and Lapata (2008)improved the above discriminative model by us-ing ILP in decoding, making it convenient toadd constraints to preserve grammatical structure.Nomoto (2007) treated the compression task asa sequence labeling problem and used CRF forit.
Thadani and McKeown (2013) presented anapproach for discriminative sentence compressionthat jointly produces sequential and syntactic rep-resentations for output text.
Filippova and Altun(2013) presented a method to automatically builda sentence compression corpus with hundreds ofthousands of instances on which deletion-basedcompression algorithms can be trained.In addition to the work on sentence compres-sion as a stand-alone task, prior studies have alsoinvestigated compression for the summarizationtask.
Knight and Marcu (2000) utilized the noisychannel and decision tree method to perform sen-tence compression in the summarization task.
Lin(2003) showed that pure syntactic-based compres-sion may not significantly improve the summariza-tion performance.
Zajic et al.
(2007) comparedtwo sentence compression approaches for multi-document summarization, including a ?parse-and-trim?
and a noisy-channel approach.
Galanis andAndroutsopoulos (2010) used the maximum en-tropy model to generate the candidate compres-sions by removing branches from the source sen-tences.
Woodsend and Lapata (2010) presented ajoint content selection and compression model forsingle-document summarization.
They operatedover a phrase-based representation of the sourcedocument which they obtained by merging infor-mation from PCFG parse trees and dependencygraphs.
Liu and Liu (2013) adopted the CRF-based sentence compression approach for summa-692rizing spoken documents.
Unlike the word-basedoperation, some of these models e.g (Knight andMarcu, 2000; Siddharthan et al., 2004; Turnerand Charniak, 2005; Galanis and Androutsopou-los, 2010; Wang et al., 2013), are tree-based ap-proaches that operate on the parse trees and thusthe compression decision can be made for a con-stituent, instead of a single word.3 Sentence Compression MethodSentence compression is a task of producing asummary for a single sentence.
The compressedsentence should be shorter, contain important con-tent from the original sentence, and be grammat-ical.
In some sense, sentence compression canbe described as a ?scaled down version of thetext summarization problem?
(Knight and Marcu,2002).
Here similar to much previous work onsentence compression, we just focus on how to re-move/select words in the original sentence withoutusing operation like rewriting sentence.3.1 Discriminative Compression Model byILPMcDonald (2006) presented a discriminative com-pression model, and Clarke and Lapata (2008) im-proved it by using ILP for decoding.
Since ourproposed method is based upon this model, inthe following we briefly describe it first.
Detailscan be found in (Clarke and Lapata, 2008).
Inthis model, the following score function is usedto evaluate each compression candidate:s(x, y) =|y|?j=2s(x, L(yj?1), L(yj)) (1)where x = x1x2, ..., xnrepresents an original sen-tence and y = y1y2, ..., ymdenotes a compressedsentence.
Because the sentence compression prob-lem is defined as a word deletion task, yjmust oc-cur in x.
Function L(yi) ?
[1...n] maps word yiinthe compression to the word index in the originalsentence x.
Note that L(yi) < L(yi+1) is required,that is, each word in x can only occur at mostonce in compression y.
In this model, a first or-der Markov assumption is used for the score func-tion.
Decoding this model is to find the combina-tion of bigrams that maximizes the score functionin Eq (1).
Clarke and Lapata (2008) introduced thefollowing variables and used ILP to solve it:?i={1 if xiis in the compression0 otherwise?i ?
[1..n]?i={1 if xistarts the compression0 otherwise?i ?
[1..n]?i={1 if xiends the compression0 otherwise?i ?
[1..n]?ij={1 if xi, xjare in the compression0 otherwise?i ?
[1..n ?
1]?j ?
[i + 1..n]Using these variables, the objective function canbe defined as:max z =n?i=1?i?
s(x, 0, i)+n?1?i=1n?j=i+1?ij?
s(x, i, j)+n?i=1?i?
s(x, i, n + 1) (2)The following four basic constraints are used tomake the compressed result reasonable:n?i=1?i= 1 (3)?j?
?j?j?i=1?ij= 0 ?j ?
[1..n] (4)?i?n?j=i+1?ij?
?i= 0 ?i ?
[1..n] (5)n?i=1?i= 1 (6)Formula (3) and (6) denote that exactly oneword can begin or end a sentence.
Formula (4)means if a word is in the compressed sentence, itmust either start the compression or follow anotherword; formula (5) represents if a word is in the693compressed sentence, it must either end the sen-tence or be followed by another word.Furthermore, discriminative models are used forthe score function:s(x, y) =|y|?j=2w ?
f(x, L(yj?1), L(yj)) (7)High dimensional features are used and their cor-responding weights are trained discriminatively.Above is the basic supervised ILP formula-tion for sentence compression.
Linguistically andsemantically motivated constraints can be addedin the ILP model to ensure the correct grammarstructure in the compressed sentence.
For exam-ple, Clarke and Lapata (2008) forced the introduc-ing term of prepositional phrases and subordinateclauses to be included in the compression if anyword from within that syntactic constituent is alsoincluded, and vice versa.3.2 Compression Model based on ExpandedConstituent Parse TreeIn the above ILP model, variables are defined foreach word in the sentence, and the task is to pre-dict each word?s status.
In this paper, we proposeto adopt the above ILP framework, but operate di-rectly on the nodes in the constituent parse tree,rather than just the words (leaf nodes in the tree).This way we can remove or retain a chunk of thesentence rather than isolated words, which we ex-pect can improve the readability and grammar cor-rectness of the compressed sentences.The top part of Fig1 is a standard constituentparse tree.
For some levels of the tree, the nodesat that same level can not represent a sentence.
Weextend the parse tree by duplicating non-POS con-stituents so that leaf nodes (words and their corre-sponding POS tags) are aligned at the bottom levelas shown in bottom of as Fig1.
In the example tree,the solid lines represent relationship of nodes fromthe original parse tree, the long dot lines denote theextension of the duplication nodes from the up-per level to the lower level, and the nodes at thesame level are connected (arrowed lines) to repre-sent that is a sequence.
Based on this expandedconstituent parse tree, we can consider every levelas a ?sentence?
and the tokens are POS tags andparse tree labels.
We apply the above compressionmodel in Section 3.1 on every level to decide everynode?s status in the final compressed sentence.
Inorder to make the compressed parsed tree reason-able, we model the relationship of nodes betweenPRP/IVBP/amDT/aNN/workerIN/fromNNP/USANP INPRPPRPPRPDT/theNNP/USAIN/fromNN/workerPPNPPRP/IDT/theSVP NPVBP NPNP PPDT NNVBPVBPSVP NPVBP/amNPNPDT/aFigure 1: A regular constituent parse tree and itsExpanded constituent tree.adjacent levels as following: if the parent node islabeled as removed, all of its children will be re-moved; one node will retain if at least one of itschildren is kept.Therefore, the objective function in the new ILPformulation is:max z =height?l=1(nl?i=1?li?
s(x, 0, li)+nl?1?i=1nl?j=i+1?lij?
s(x, li, lj)+nl?i=1?li?
s(x, li, nl+ 1) ) (8)where height is the depth for a parse tree (startingfrom level 1 for the tree), and nlmeans the lengthof level l (for example, n5= 6 in the examplein Fig1).
Then every level will have a set of pa-rameters ?li, ?li, ?li, and ?lij, and the correspondingconstraints as shown in Formula (3) to (6).
The re-lationship between nodes from adjacent levels canbe expressed as:?li?
?(l+1)j(9)?li???
(l+1)j(10)in which node j at level (l+1) is the child of node694i at level l. In addition, 1 ?
l ?
height ?
1,1 ?
i ?
nland 1 ?
j ?
nl+1.3.3 Linguistically Motivated ConstraintsIn our proposed model, we can jointly decide thestatus of every node in the constituent parse treeat the same time.
One advantage is that we canadd constraints based on internal nodes or rela-tionship in the parse tree, rather than only usingthe relationship based on words.
In addition tothe constraints proposed in (Clarke and Lapata,2008), we introduce more linguistically motivatedconstraints to keep the compressed sentence moregrammatically correct.
The following describesthe constraints we used based on the constituentparse tree.?
If a node?s label is ?SBAR?, its parent?s labelis ?NP?
and its first child?s label is ?WHNP?
or?WHPP?
or ?IN?, then if we can find a nounin the left siblings of ?SBAR?, this subordi-nate clause could be an attributive clause orappositive clause.
Therefore the found nounnode should be included in the compressionif the ?SBAR?
is also included, because thenode ?SBAR?
decorates the noun.
For exam-ple, the top part of Fig 2 is part of expandedconstituent parse tree of sentence ?Those whoknew David were all dead.?
The nodes in el-lipse should share the same status.?
If a node?s label is ?SBAR?, its parent?s labelis ?VP?
and its first child?s label is ?WHNP?,then if we can find a verb in the left siblingsof ?SBAR?, this subordinate clause could bean objective clause.
Therefore, the foundverb node should be included in the compres-sion if the ?SBAR?
node is also included, be-cause the node ?SBAR?
is the object of thatverb.
An example is shown in the bottom partof Fig 2.
The nodes in ellipse should share thesame status.?
If a node?s label is ?SBAR?, its parent?slabel is ?VP?
and its first child?s label is?WHADVP?, then if the first leaf for this nodeis a wh-word (e.g., ?where, when, why?)
or?how?, this clause may be an objective clause(when the word is ?why, how, where?)
or at-tributive clause (when the word is ?where?)
oradverbial clause (when the word is ?when?
).Therefore, similar to above, if a verb or nounis found in the left siblings of ?SBAR?, theVBD/knewNNP/DavidNPDTDTDTVPWPWP/whoDT/ThoseVBDSNPPRP/hePRP/IVBP/believePRP VBPWP/whatWHNP S PRPVBPVBD/saidSBARVP NPNP VP WP PRPVBPNPSBARWHNPWHNPSFigure 2: Expanded constituent parse tree for ex-amples.found verb or noun node should be includedin the compression if the ?SBAR?
node is alsoincluded.?
If a node?s label is ?SBAR?
and its parent?s la-bel is ?ADJP?, then if we can find a ?JJ?, ?JJR?,or ?JJS?
in the left siblings of ?SBAR?, the?SBAR?
node should be included in the com-pression if the found ?JJ?, ?JJR?
or ?JJS?
nodeis also included because the node ?SBAR?
isdecorated by the adjective.?
The node with a label of ?PRN?
can be re-moved without other constraints.We also include some other constraints based onthe Stanford dependency parse tree.
Table 1 liststhe dependency relations we considered.?
For type I relations, the parent and child nodewith those relationships should have the samevalue in the compressed result (both are keptor removed).?
For type II relations, if the child node inthose relations is retained in the compressedsentence, the parent node should be also re-tained.695Dependency Relation Exampleprt: phrase verb particle They shut down the station.
prt(shut,down)prep: prepositional modifier He lives in a small village.
prep(lives,in)I pobj: object of a preposition I sat on the chair.
pobj(on,chair)nsubj: nominal subject The boy is cute.
nsubj(cute,boy)cop: copula Bill is big.
cop(big,is)partmod: participial modifier Truffles picked during the spring are tasty.
partmod(truffles,picked)II nn: noun compound modifier Oil price futures.
nn(futures,oil)acomp: adjectival complement She looks very beautiful.
acomp(looks,beautiful)pcomp: prepositional complement He felt sad after learning that tragedy.
pcomp(after,learning)III ccomp: clausal complement I am certain that he did it.
ccomp(certain,did)tmod: temporal modifier Last night I swam in the pool.
tmod(swam,night)Table 1: Some dependency relations used for extra constraints.
All the examples are from (Marneffe andManning, 2002)?
For type III relations, if the parent node inthese relations is retained, the child nodeshould be kept as well.3.4 FeaturesSo far we have defined the decoding processand related constraints used in decoding.
Theseall rely on the score function s(x, y) = w ?f(x, L(yj?1), L(yj)) for every level in the con-stituent parse tree.
We included all the features in-troduced in (Clarke and Lapata, 2008) (those fea-tures are designed for leaves).
Table 2 lists theadditional features we used in our system.General Features for Every Node1.
individual node label and concatenation of a pair ofnodes2.
distance of two nodes at the same level3.
is the node at beginning or end at that level?4.
do the two nodes have the same parent?5.
if two nodes do not have the same parent, then is the leftnode the rightmost child of its parent?
is the right node theleftmost child of its parent?6.
combination of parent label if the node pair are notunder the same parent7.
number of node?s children: 1/0/>18.
depth of nodes in the parse treeExtra Features for Leaf nodes1.
word itself and concatenation of two words2.
POS and concatenation of two words?
POS3.
whether the word is a stopword4.
node?s named entity tag5.
dependency relationship between two leavesTable 2: Features used in our system besides thoseused in (Clarke and Lapata, 2008).3.5 LearningTo learn the feature weights during training, weperform ILP decoding on every sentence in thetraining set, to find the best hypothesis for eachnode in the expanded constituent parse tree.
Ifthe hypothesis is incorrect, we update the featureweights using the structured perceptron learningstrategy (Collins, 2002).
The reference label forevery node in the expanded constituent parse treeis obtained automatically from the bottom to thetop of the tree.
Since every leaf node (word) ishuman annotated (removed or retain), we annotatethe internal nodes as removed if all of its childrenare removed.
Otherwise, the node is annotated asretained.During perceptron training, a fixed learning rateis used and parameters are averaged to preventoverfitting.
In our experiment, we observe sta-ble convergence using the held-out developmentcorpus, with best performance usually obtainedaround 10-20 epochs.4 Summarization SystemSimilar to (Li et al., 2013a), our summarizationsystem is , which consists of three key compo-nents: an initial sentence pre-selection moduleto select some important sentence candidates; theabove compression model to generate n-best com-pressions for each sentence; and then an ILP sum-marization method to select the best summary sen-tences from the multiple compressed sentences.The sentence pre-selection model is a simple su-pervised support vector regression (SVR) modelthat predicts a salience score for each sentence andselects the top ranked sentences for further pro-cessing (compression and summarization).
Thetarget value for each sentence during training isthe ROUGE-2 score between the sentence and thehuman written abstracts.
We use three commonfeatures: (1) sentence position in the document;(2) sentence length; and (3) interpolated n-gramdocument frequency as introduced in (Ng et al.,2012).The final sentence selection process follows the696ILP method introduced in (Gillick et al., 2009).Word bi-grams are used as concepts, and their doc-ument frequency is used as weights.
Since we usemultiple compressions for one sentence, an addi-tional constraint is used: for each sentence, onlyone of its n-best compressions may be included inthe summary.For the compression module, using the ILPmethod described above only finds the best com-pression result for a given sentence.
To generaten-best compression candidates, we use an iterativeapproach ?
we add one more constraints to preventit from generating the same answer every time af-ter getting one solution.5 Experimental Results5.1 Experimental SetupSummarization Data For summarization experi-ments, we use the standard TAC data sets1, whichhave been used in the NIST competitions.
In par-ticular, we used the TAC 2010 data set as train-ing data for the SVR sentence pre-selection model,TAC 2009 data set as development set for parame-ter tuning, and the TAC 2008 and 2011 data as thetest set for reporting the final summarization re-sults.
The training data for the sentence compres-sion module in the summarization system is sum-mary guided compression corpus annotated by (Liet al., 2013a) using TAC2010 data.
In the com-pression module, for each word we also used itsdocument level feature.2Compression Data We also evaluate our com-pression model using the data set from (Clarkeand Lapata, 2008).
It includes 82 newswire arti-cles with manually produced compression for eachsentence.
We use the same partitions as (Martinsand Smith, 2009), i.e., 1,188 sentences for trainingand 441 for testing.Data Processing We use Stanford CoreNLPtoolkit3 to tokenize the sentences, extract name en-tity tags, and generate the dependency parse tree.Berkeley Parser (Petrov et al., 2006) is adoptedto obtain the constituent parse tree for every sen-tence and POS tag for every token.
We use Pocket1http://www.nist.gov/tac/data/index.html2Document level features for a word include informationsuch as the word?s document frequency in a topic.
Thesefeatures cannot be extracted from a single sentence, as in thestandard sentence compression task, and are related to thedocument summarization task.3http://nlp.stanford.edu/software/corenlp.shtmlCRF4 to implement the CRF sentence compres-sion model.
SVMlight5 is used for the summarysentence pre-selection model.
Gurobi ILP solver6does all ILP decoding.5.2 Summarization ResultsWe compare our summarization system againstfour recent studies, which have reported some ofthe highest published results on this task.
Berg-Kirkpatrick et al.
(2011) introduced a joint modelfor sentence extraction and compression.
Wood-send and Lapata (2012) learned individual sum-mary aspects from data, e.g., informativeness, suc-cinctness, grammaticalness, stylistic writing con-ventions, and jointly optimized the outcome inan ILP framework.
Ng et al.
(2012) exploitedcategory-specific information for multi-documentsummarization.
Almeida and Martins (2013) pro-posed compressive summarization method by dualdecomposition and multi-task learning.
Our sum-marization framework is the same as (Li et al.,2013a), except they used a CRF-based compres-sion model.
In addition to the four previous stud-ies, we also report the best achieved results in theTAC competitions.Table 3 shows the summarization results of ourmethod and others.
The top part contains the re-sults for TAC 2008 data and bottom part is forTAC 2011 data.
We use the ROUGE evaluationmetrics (Lin, 2004), with R-2 measuring the bi-gram overlap between the system and referencesummaries and R-SU4 measuring the skip-bigramwith the maximum gap length of 4.
In addition,we evaluate the linguistic quality (LQ) of the sum-maries for our system and (Li et al., 2013a).7 Thelinguistic quality consists of two parts.
One eval-uates the grammar quality within a sentence.
Forthis, annotators marked if a compressed sentenceis grammatically correct.
Typical grammar errorsinclude lack of verb or subordinate clause.
Theother evaluates the coherence between sentences,including the order of sentences and irrelevant sen-tences.
We invited 3 English native speakers to dothis evaluation.
They gave every compressed sen-tence a grammar score and a coherence score for4http://sourceforge.net/projects/pocket-crf-1/5http://svmlight.joachims.org/6http://www.gurobi.com7We chose to evaluate the linguistic quality for this systembecause of two reasons: one is that we have an implementa-tion of that method; the other more important one is that ithas the highest reported ROUGE results among the comparedmethods.697System R-2 R-SU4 Gram CohereTAC?08 Best System 11.03 13.96 n/a n/a(Berg-Kirk et al., 2011) 11.70 14.38 n/a n/a(Woodsend et al., 2012) 11.37 14.47 n/a n/a(Almeida et al.,2013) 12.30 15.18 n/a n/a(Li et al., 2013a) 12.35 15.27 3.81 3.41Our System 12.23 15.47 4.29 4.11TAC?11 Best System 13.44 16.51 n/a n/a(Ng et al., 2012) 13.93 16.83 n/a n/a(Li et al., 2013a) 14.40 16.89 3.67 3.32Our System 14.04 16.67 4.18 4.07Table 3: Summarization results on the TAC 2008and 2011 data sets.each topic.
The score is scaled and ranges from 1(bad) to 5 (good).
Therefore, in table 3, the gram-mar score is the average score for each sentenceand coherence score is the average for each topic.We measure annotators?
agreement in the follow-ing way: we consider the scores from each anno-tator as a distribution and we find that these threedistributions are not statistically significantly dif-ferent each other (p > 0.05 based on paired t-test).We can see from the table that in general, oursystem achieves better ROUGE results than mostprevious work except (Li et al., 2013a) on bothTAC 2008 and TAC 2011 data.
However, oursystem?s linguistic quality is better than (Li etal., 2013a).
The CRF-based compression modelused in (Li et al., 2013a) can not well model thegrammar.
Particularly, our results (ROUGE-2) arestatistically significantly (p < 0.05) higher thanTAC08 Best system, but are not statistically signif-icant compared with (Li et al., 2013a) (p > 0.05).The pattern is similar in TAC 2011 data.
Our result(R-2) is statistically significantly (p < 0.05) betterthan TAC11 Best system, but not statistically (p >0.05) significantly different from (Li et al., 2013a).However, for the grammar and coherence score,our results are statistically significantly (p < 0.05)than (Li et al., 2013a).
All the above statistics arebased on paired t-test.5.3 Compression ResultsThe results above show that our summarizationsystem is competitive.
In this section we focuson the evaluation of our proposed compressionmethod.
We compare our compression systemagainst four other models.
HedgeTrimmer in Dorret al.
(2003) applied a variety of linguistically-motivated heuristics to guide the sentences com-System C Rate (%) Uni-F1 Rel-F1HedgeTrimmer 57.64 0.64 0.50McDonald (2006) 70.95 0.77 0.55Martins (2009) 71.35 0.77 0.56Wang (2013) 68.06 0.79 0.59Our System 71.19 0.77 0.58Table 4: Sentence compression results.
The hu-man compression rate of the test set is 69%.pression; McDonald (2006) used the output of twoparsers as features in a discriminative model thatdecomposes over pairs of consecutive words; Mar-tins and Smith (2009) built the compression modelin the dependency parse and utilized the relation-ship between the head and modifier to preserve thegrammar relationship; Wang et al.
(2013) devel-oped a novel beam search decoder using the tree-based compression model on the constituent parsetree, which could find the most probable compres-sion efficiently.Table 4 shows the compression results of vari-ous systems, along with the compression ratio (CRate) of the system output.
We adopt the com-pression metrics as used in (Martins and Smith,2009) that measures the macro F-measure for theretained unigrams (Uni-F1), and the one usedin (Clarke and Lapata, 2008) that calculates theF1 score of the grammatical relations labeled by(Briscoe and Carroll, 2002) (Rel-F1).
We can seethat our proposed compression method performswell, similar to the state-of-the-art systems.To evaluate the power of using the expandedparse tree in our model, we conducted another ex-periment where we only consider the bottom levelof the constituent parse tree.
In some sense, thiscould be considered as the system in (Clarke andLapata, 2008).
Furthermore, we use two differ-ent setups: one uses the lexical features (about thewords) and the other does not.
Table 5 shows theresults using the data in (Clarke and Lapata, 2008).For a comparison, we also include the results us-ing the CRF-based compression model (the oneused in (Nomoto, 2007; Li et al., 2013a)).
Wereport results using both the automatically calcu-lated compression metrics and the linguistic qual-ity score.
Three English native speaker annotatorswere asked to judge two aspects of the compressedsentence compared with the gold result: one is thecontent that looks at whether the important wordsare kept and the other is the grammar score whichevaluates the sentence?s readability.
Each of these698two scores ranges from 1(bad) to 5(good).Table 5 shows that when using lexical features,our system has statistically significantly (p < 0.05)higher Grammar value and content importancevalue than the CRF and the leaves only system.When no lexical features are used, default systemcan achieve statistically significantly (p < 0.01)higher results than the CRF and the leaves onlysystem.We can see that using the expanded parse treeperforms better than using the leaves only, espe-cially when lexical features are not used.
In ad-dition, we observe that our proposed compressionmethod is more generalizable than the CRF-basedmodel.
When our system does not use lexicalfeatures in the leaves, it achieves better perfor-mance than the CRF-based model.
This is impor-tant since such a model is more robust and may beused in multiple domains, whereas a model rely-ing on lexical information may suffer more fromdomain mismatch.
From the table we can see ourproposed tree based compression method consis-tently has better linguistic quality.
On the otherhand, the CRF compression model is the mostcomputationally efficient one among these threecompression methods.
It is about 200 times fasterthan our model using the expanded parse tree.
Ta-ble 6 shows some examples using different meth-ods.System C Rate(%) Uni-F1 Rel-F1 Gram ImpUsing lexical featuresCRF 79.98 0.80 0.51 3.9 4.0ILP(I) 80.54 0.79 0.57 4.0 4.2ILP(II) 79.90 0.80 0.57 4.2 4.4No lexical featuresCRF 77.75 0.78 0.51 3.35 3.5ILP(I) 77.77 0.78 0.56 3.7 3.9ILP(II) 77.78 0.80 0.58 4.1 4.2Table 5: Sentence compression results: effect oflexical features and expanded parse tree.
ILP(I)represents the system using only bottom nodes inconstituent parse tree.
ILP(II) is our system.
Impmeans the content importance value.6 ConclusionIn this paper, we propose a discriminative ILP sen-tence compression model based on the expandedconstituent parse tree, which aims to improve thelinguistic quality of the compressed sentences inthe summarization task.
Linguistically motivatedconstraints are incorporated to improve the sen-tence quality.
We conduct experiments on the TACUsing lexical featuresSource:Apart from drugs, detectives believe money is laun-dered from a variety of black market deals involvingarms and high technology.Human compress:detectives believe money is laundered from a variety ofblack market deals.CRF result :Apart from drugs detectives believe money is launderedfrom a black market deals involving arms and technol-ogy.ILP(I) Result:detectives believe money is laundered from a variety ofblack deals involving arms.ILP(II) Result:detectives believe money is laundered from black mar-ket deals.No lexical featuresSource:Mrs Allan?s son disappeared in May 1989, after a partyduring his back packing trip across North America.Human compress:Mrs Allan?s son disappeared in 1989, after a party dur-ing his trip across North America.CRF result :Mrs Allan?s son disappeared May 1989, after during hispacking trip across North America.ILP(I) Result:Mrs Allan?s son disappeared in May, 1989, after a partyduring his packing trip across North America .ILP(II) Result:Mrs Allan?s son disappeared in May 1989, after a partyduring his trip.Table 6: Examples of original sentences and theircompressed sentences from different systems.2008 and 2011 summarization data sets and showthat by incorporating this sentence compressionmodel, our summarization system can yield signif-icant performance gain in linguistic quality with-out losing much ROUGE results.
The analysisof the compression module also demonstrates itscompetitiveness, in particular the better linguisticquality and less reliance on lexical cues.AcknowledgmentsWe thank the anonymous reviewers for their de-tailed and insightful comments on earlier draftsof this paper.
The work is also partially sup-ported by NSF award IIS-0845484 and DARPAContract No.
FA8750-13-2-0041.
Any opinions,findings, and conclusions or recommendations ex-pressed are those of the authors and do not neces-sarily reflect the views of the funding agencies.699ReferencesAhmet Aker, Trevor Cohn, and Robert Gaizauskas.2010.
Multi-document summarization using a*search and discriminative training.
In Proceedingsof EMNLP.Miguel B. Almeida and Andre F. T. Martins.
2013.Fast and robust compressive summarization withdual decomposition and multi-task learning.
In Pro-ceedings of ACL.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of ACL.Ted Briscoe and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceedingsof LREC.Yllias Chali and Sadid A. Hasan.
2012.
On the effec-tiveness of using sentence compression models forquery-focused multi-document summarization.
InProceedings of COLING.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression an integer linearprogramming approach.
Journal of Artificial Intelli-gence Research.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP.Hal Daume?.
2006.
Practical structured learning tech-niques for natural language processing.
Ph.D. the-sis, University of Southern California.Bonnie Dorr, David Zajic, and Richard Schwartz.2003.
Hedge trimmer: A parse-and-trim approachto headline generation.
In Proceedings of NAACL.Katja Filippova and Yasemin Altun.
2013.
Overcom-ing the lack of parallel data in sentence compression.In Proceedings of EMNLP.Dimitrios Galanis and Ion Androutsopoulos.
2010.
Anextractive supervised two-stage method for sentencecompression.
In Proceedings of NAACL.Michel Galley and Kathleen McKeown.
2007.
Lexi-calized markov grammars for sentence compression.In Processings of NAACL.Michel Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.In Proceedings of EMNLP.Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, BerndtBohnet, Yang Liu, and Shasha Xie.
2009.
Theicsi/utd summarization system at tac 2009.
In Pro-ceedings of TAC.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In Proceedings of AAAI.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139(1):91?107.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In Proceedingsof SIGIR.Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013a.Document summarization via guided sentence com-pression.
In Proceedings of the EMNLP.Chen Li, Xian Qian, and Yang Liu.
2013b.
Using su-pervised bigram-based ilp for extractive summariza-tion.
In Proceedings of ACL.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submodu-lar functions.
In Proceedings of NAACL.Chin-Yew Lin.
2003.
Improving summarization per-formance by sentence compression - A pilot study.In Proceeding of the Sixth International Workshopon Information Retrieval with Asian Language.Chin-Yew Lin.
2004.
Rouge: a package for automaticevaluation of summaries.
In Proceedings of ACL.Fei Liu and Yang Liu.
2013.
Towards abstractivespeech summarization: Exploring unsupervised andsupervised approaches for spoken utterance com-pression.
IEEE Transactions on Audio, Speech, andLanguage Processing.Marie-Catherine de Marneffe and Christopher D Man-ning.
2002.
Stanford typed dependencies manual.Andre F. T. Martins and Noah A. Smith.
2009.
Sum-marization with a joint model for sentence extrac-tion and compression.
In Proceedings of the ACLWorkshop on Integer Linear Programming for Natu-ral Language Processing.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceed-ings of EACL.Ani Nenkova and Kathleen McKeown.
2011.
Auto-matic summarization.
Foundations and Trends inInformation Retrieval.Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-YenKan, and Chew-Lim Tan.
2012.
Exploitingcategory-specific information for multi-documentsummarization.
In Proceedings of COLING.Tadashi Nomoto.
2007.
Discriminative sentence com-pression with conditional random fields.
Informa-tion Processing and Management.Miles Osborne.
2002.
Using maximum entropy forsentence extraction.
In Proceedings of the ACL-02Workshop on Automatic Summarization.700Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofCOLING-ACL.Xian Qian and Yang Liu.
2013.
Fast joint compressionand summarization via graph cuts.
In Proceedingsof EMNLP.Advaith Siddharthan, Ani Nenkova, and KathleenMcKeown.
2004.
Syntactic simplification for im-proving content selection in multi-document sum-marization.
In Proceedings of Coling.Kapil Thadani and Kathleen McKeown.
2013.
Sen-tence compression with joint structural inference.
InProceedings of CoNLL.Jenine Turner and Eugene Charniak.
2005.
Super-vised and unsupervised learning for sentence com-pression.
In Proceedings of ACL.Lucy Vanderwende, Hisami Suzuki, Chris Brockett,and Ani Nenkova.
2007.
Beyond sumbasic: Task-focused summarization with sentence simplificationand lexical expansion.
Information Processing &Management.Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-rian, and Claire Cardie.
2013.
A sentence com-pression based framework to query-focused multi-document summarization.
In Proceedings of ACL.Kristian Woodsend and Mirella Lapata.
2010.
Auto-matic generation of story highlights.
In Proceedingsof ACL.Kristian Woodsend and Mirella Lapata.
2012.
Mul-tiple aspect summarization using integer linear pro-gramming.
In Proceedings of EMNLP-CoNLL.Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, andManabu Okumura.
2012.
Sentence compressionwith semantic role constraints.
In Proceedings ofACL.David Zajic, Bonnie J. Dorr, Jimmy Lin, and RichardSchwartz.
2007.
Multi-candidate reduction: Sen-tence compression as a tool for document summa-rization tasks.
In Information Processing and Man-agement.701
