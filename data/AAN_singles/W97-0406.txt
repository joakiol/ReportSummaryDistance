Deal ing wi th  Mult i l ingual i ty in a Spoken Language Query" TranslatorPascale Fung, Bertram Shi, Dekai Wu, Lain Wai Bun, Wong Shuen KongDept.
of Electrical and Electronic Engineering, Dept.
of Computer ScienceUniversity of Science & Technology (HKUST)Clear Water Bay, Hong Kong{pascale, eebert }tee.
ust.
hk, dekai?cs, ust.
hkAbstractRobustness i an important issue for mul-tilingual speech interfaces for spoken lan-guage translation systems.
We have stud-ied three aspects of robustness in such asystem: accent differences, mixed languageinput, and the use of common feature setsfor HMM-based speech recognizers for En-glish and Cantonese.
The results of ourpreliminary experiments show that accentdifferences cause recognizer performance todegrade.
A rather surprising finding isthat for mixed language input, a straightforward implementation of a mixed lan-guage model-based speech recognizer per-forms less well than the concatenation ofpure language recognizers.
Our experimen-tal results also show that a common fea-ture set, parameter set, and common algo-rithm lead to different performance outputfor Cantonese and English speech recogni-tion modules.1 In t roduct ionIn the past few decades, automatic speech recog-nition (ASR) and machine translation (MT) haveboth undergone rapid technical progress.
Spokenlanguage translation has emerged as a new field com-bining the advances in ASR and MT(Levin et al,1995; Mayfield et al, 1995; Lavie et al, 1995; Vi-lar et al, 1996).
Robustness i a critical issue whichmust be addressed for this technology to be useful inreal applications.
There are several robustness i suesarising from the multilingual characteristics ofmanyspoken language translation systems which have notstudied by the speech recognition community sincethe latter tends to focus on monolingual recognitionsystems.One problem in a multilingual system is accentvariability.
It is frequently assumed that the speak-ers using a system are native speakers belongingto the same accent group.
However, this is notgenerally true.
For example, in Hong Kong, al-though many people can speak English, one encoun-ters a large variety of different accents ince in addi-tion to Hong Kong's large population of Cantonesespeakers, there are also many Mandarin speakersand many Indian, British, American and AustralianHong Kong residents.Another problem with multilinguality is mixedlanguage recognition.
Although the official lan-guages of Hong Kong are English, spoken Cantoneseand written Mandarin, most Hong Kongers speaka hybrid of English and Cantonese.
In fact, sincemany native Cantonese speakers do not know theChinese translations of many English terms, forcingthem to speak in pure Cantonese is impractical andunrealistic.A third problem is the complexity of the design ofrecognizers for multiple languages.
Many large mul-tilingual spoken language translation systems uchas JANUS (Lavie et al, 1995) and the C-STAR Con-sortium decouple the development of speech recog-nition interfaces for different languages.
However,for developers of a multilingual system at one singlesite, it would be more efficient if the speech interfacesfor the different languages shared a common enginewith one set of features, one set of parameters, onerecognition algorithm and one system architecture,but differed in the parameter values used.We are studying the issues raised above in thedomain of a traveling business-person's query trans-lation system (Figure 1).
This translator is a sym-metrical query/response ystem.
Both ends of thesystem recognize input speech from human througha common recognition engine comprising of either aconcatenated or a mixed language recognizer.
Af-ter the speech is decoded into text, the translatorconverts one language to another.
Both ends of thesystem have a speech synthesizer for output speech.The domain of our system is restricted to pointsof interest to a traveling business-person, such asnames and directions of business districts, confer-ence centers, hotels, money exchange, restaurants.We are currently implementing such a system withCantonese and English as the main languages.
We40use HMM-based, isolated word recognition systemas the recognition engine, and a statistical transla-tor for the translation engine.2 Does  accent  a f fec t  speechrecogn izer  per fo rmance?We have performed a set of experiments ocomparetile effect of different accents.
We train two setsof models: an English model using native AmericanEnglish speakers as reference and a Cantonese modelusing native Cantonese speakers as references.
Wordmodels of 34 (17 English and 17 Cantonese) simplecommands were trained using 6 utterances of eachcommand per speaker.
The models were evaluatedusing a separate set of native Cantonese and nativeAmerican English speakers.
The recognition resultsare shown in Figure 2.Our experimental results support the claim thatrecognition accuracy degrades in the presence of anunmodelled accent.
In order to bring the recognizerperformance for the non-native speaker to that ofthe native speaker, we need to improve the mod-els in the recognizer.
An  obvious solution seemsto train the model on different accents.
However,it is quite a daunting task to train every languagewith every type of accent.
One approximation isto train the system with a mixture of separate lan-guages so that the model parameters would capturethe spectral characteristics of more than one lan-guage.
A mechanism for gradual accent adaptationmight potentially increase recognition accuracies ofthe speech recognizers of both source and target lan-guages.3 How to  dea l  w i th  mixed  languagerecogn i t ion?Consider two possible ways to implement a mixedlanguage recognizer--(1) Use two pure monolingualrecognizers to recognize different parts of the mixedlanguage separately; (2) Use a single mixed languagemodel where the word network allows words in bothlanguages.
Method (1) requires ome sort of lan-guage identification toswitch between two recogniz-ers whereas method (2) seems to be more flexibleand efficient.We compared the recognition accuracies of a purelanguage recognizer with a mixed language recog-nizer.
In the pure language recognizer, the wordcandidates are all from a single language dictio-nary, whereas the mixed language dictionary con-tains words from two dictionaries.
See Figure 3.
Inthe concatenation model, we assume a priori knowl-edge (possibly from a language identifier) of the lan-guage ID of the words.
The expected recognition rateof the concatenation model is the product of the ac-curacies of the pure language model.From this preliminary experiment, we discoverthat although a mired language model offers greaterflexibility to the speaker, it has a considerably owerperformance than that of the concatenation of twopure language models.
The reason for such a per-formance degradation ofa mixed model is not diffi-cult to deduce--the dictionary of a mixed model hasmore candidates.
Consequently, the search result isless accurate.
If the recognizer knows a priori whichdictionary (English or Chinese) it should search fora particular word, it would make less error.This is therefore a potentially interesting prob-lem.
Should we incorporate a language identifier inparallel to the recognizers or should we accept ileloss in recognition rate but enjoy the flexibility ofa mixed language recognizer?
We will implement alanguage identifier and carry out more experimentsto compare the output from the recognizers.4 Can  the  source  and  ta rgetlanguages  share  the  samerecogn i t ion  eng ine?One important issue for multilinguality in a spokenlanguage translator is the complexity of implement-ing more than one recognizer in the system.
An effi-cient approach is to use recognizers which are iden-tical except for parameter values.
Will this enablerobust recognizers?The word-based HMM recognizers for English andCantonese use identical features (Nine MFCCs andnine delta MFCCs.)
The same microphone was usedto record both languages.
The same initializationprocedure was used to initialize the recognizer forboth languages.
For English, the number of HMMstates is deduced from spectrograms.
For Cantonese,it is deduced from phoneme numbers for each word.The recognizers were evaluated using native Englishand Cantonese speakers who were not in the trainingset.In general, the English recognizer is more robustthan our Cantonese recognizer even though identi-cal parameter set, training and testing mechanismsare used.
Rather than jumping to the conclusionthat a different feature set is needed for Cantonese,we would like to find out what other factors couldcause a lower performance of the Cantonese recog-nizer.
For example, we would like to perform exper-iments on a larger number of speakers to determinewhether training and test speaker mismatch causedsuch a performance degradation.5 Conc lus ion  and  fu ture  workIn this paper, we have examined three issues con-cerning the robustness of multilingual speech inter-faces for spoken language translation systems: ac-cent differences, mixed language input, and the useof common feature sets for HMM-based speech rec-ognizers for English and Cantonese.
From the re-41\[ l ' ouest  deof/*w*'t/~ ~~Figure 1: A symmetrical system as traveling business-person's query translatorFigure 2: Speech recognizers perform better on native speakersNative speaker English model Cantonese modelEnglish 94% 77%Cantonese 86% 90%Average 90% 83%Figure 3: Speech recognizers perform better with concatenated pure language model than with mixed lan-guage modelNative speakerEnglishCantoneseSpeechEnglishCantoneseMixedEnglishCantoneseMixedMixed model92%59%64%86%75%68%English only94%86%Cantonese only77%90%Concatenate ( xpected)66%(72%)78%(77%)42suits of our preliminary experiments, we find that ac-cent difference causes recognizers performance tode-grade.
For mixed language input, we found out thata straight forward implementation f a mixed lan-guage model-based speech recognizer performs lesswell than the concatenation f pure language recog-uizers due to the increase in recognition candidateuumbers.
Finally, our experimental results showthat the Cantonese recognizer has a lower recogni-tion rate on the average than the English recognizerdespite a common feature set, parameter set, andcommon algorithm.
We will perform more expri-tnents using larger training and test sets to verifyour results.ReferencesA.
Lavie, L. Levin, A. Waibel, D. Gates, M. Gavalda,and L. Mayfield.
1995.
JANUS: Multi-lingualtranslation of spontaneous speech in a limited do-main.
In Proceedings of the Second Conferenceof the Association for Machine Translation in theAmericas, pages 252-255, Montreal, Quebec, Oc-tober.L.
Levin, O. Glickman, Y. Qu, C. P. Rose, D. Gates,A.
Lavie, A. Waibel, and C. Van Ess-Dykema.1995.
Using context in machine translation of spo-ken language.
In  Proceedings of the Sixth Interna-tional Conference on Theoretical and Methodologi-cal Issues in Machine Translation, pages 173-187,Leuven, Belgium, July.L.
J. Mayfield, M. Gavalda, Y.-H. Seo, B. Suhm,W.
Ward, and A. Waibel.
1995.
Concept-basedparsing for speech translation.
In Proceedings ofthe Sizth International Conference on Theoreti-cal and Methodological Issues in Machine Trans-lation, pages 196-187, Leuven, Belgium, July.J.
M. Vilar, A. Castellanos, J. M Jimenez, J. A.Sanchez, E. Vidal, J. Oneina, and H. Rulot.
1996.Spoken-language machine translation in limiteddomains: Can it be achieved by finite-state mod-els?
In Proceedings of the Sixth InternationalConference on Theoretical and Methodological Is-sues in Machine Translation, pages 326-333, Leu-ven, Belgium, July.43
