Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 47?50,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics?How was your day??
An architecture for multimodal ECA systemsRa?l Santos de laC?maraTelef?nica I+DC/ Emilio Vargas 628043 Madrid, Spaine.rsai@tid.esMarkku TurunenUniv.
of TampereKanslerinrinne 1FI-33014, Finlandmturunen@cs.uta.fiJaakko HakulinenUniv.
of TampereKanslerinrinne 1FI-33014, Finlandjh@cs.uta.fiDebora FieldComputer ScienceUniv.
of  SheffieldS1 4DP, UKd.field@shef.ac.ukAbstractMultimodal conversational dialogue sys-tems consisting of numerous softwarecomponents create challenges for the un-derlying software architecture and devel-opment practices.
Typically, such sys-tems are built on separate, often pre-existing components developed by dif-ferent organizations and integrated in ahighly iterative way.
The traditional dia-logue system pipeline is not flexibleenough to address the needs of highly in-teractive systems, which include parallelprocessing of multimodal input and out-put.
We present an architectural solutionfor a multimodal conversational socialdialogue system.1 IntroductionMultimodal conversational dialogue applica-tions with embodied conversational agents(ECas) are complex software systems consistingof multiple software components.
They requiremuch of architectural solutions and developmentapproaches compared to traditional spoken dia-logue systems.
These systems are mostly assem-bled from separate, often pre-existing compo-nents developed by different organizations.
Forsuch systems, the simple pipeline architecture isnot a viable choice.
When multimodal systemsare built, software architecture should be flexibleenough to enable the system to support naturalinteraction with features such as continuous andtimely multimodal feedback and interruptions byboth participants.
Such features require parallelprocessing components and flexible communica-tion between the components.
Furthermore, thearchitecture should provide an open sandbox,where the components can be efficiently com-bined and experimented with during the iterativedevelopment process.The HWYD (?How was your day??)
Compan-ion system is a multimodal virtual companioncapable of affective social dialogue and forwhich we have developed a custom novel archi-tecture.
The application features an ECA whichexhibits facial expressions and bodily move-ments and gestures.
The system is rendered on aHD screen with the avatar being presented asroughly life-size.
The user converses with theECA using a wireless microphone.
A demonstra-tion video of the virtual companion in action isavailable online1.The application is capable of long social con-versations about events that take place during auser?s working day.
The system monitors theuser?s emotional state on acoustic and linguisticlevels, generates affective spoken responses, andattempts to positively influence the user?s emo-tional state.
The system allows for user initiative,it asks questions, makes comments and sugges-tions, gives warnings, and offers advice.2 Communications frameworkThe HWYD Companion system architecture em-ploys Inamode, a loosely coupled multi-hubframework which facilitates a loose, non-hierarchical connection between any number ofcomponents.
Every component in the system isconnected to a repeating hub which broadcastsall messages sent to it to all connected compo-nents.
The hub and the components connected toit form a single domain.
Facilitators are used toforward messages between different domainsaccording to filtering rules.
During development,we have experimented with a number of Facilita-tors to create efficient and simple domains toovercome problems associated with single-hubsystems.
For example, multiple hubs allow the1http://www.youtube.com/watch?v=BmDMNguQUmM47reduction of broadcast messages, which is forexample used in the audio processing pipeline,where a dedicated hub allows very rapid messagebroadcast (nearly 100 messages per second areexchanged) without compromising the stabilityof the system by flooding the common pipeline.For communication between components, alightweight communication protocol is used tosupport components implemented in variousprogramming languages.
A common XML mes-sage ?envelope?
specifies the basic format ofmessage headers as seen in Figure 1.Figure 1: System message XML format.Mandatory elements in the envelope (topblock) are necessary so other modules can iden-tify the purpose of the message and its contentsupon a shallow inspection.
These include thesender component and a unique message id.
Ad-ditional envelope fields elements include: mes-sage type, turn id, dialogue segment identifier,recipient identifier, and a list of message identi-fiers corresponding to the previous messages inthe current processing sequence.For system-wide and persistent knowledgemanagement, a central XML-database allows thesystem to have inter-session and intra-session?memory?
of past events and dialogues.
This da-tabase (KB) includes information such the userand dialogue models, processing status of mod-ules, and other system-wide information.3 Data flow in the architectureTo maximize the naturalness of the ECA?s inter-action, the system implements parallel process-ing paths.
It also makes use of a special module,the Interruption Manager (IM), to controlcomponents in situations where regular process-ing procedure must be deviated from.
In addi-tion, there are ?long?
and ?short?
processing  se-quences from user input to system output.
Both?loops?
operate simultaneously.
The Main Dia-logue (?long?)
Loop, which is the normal proc-essing path, is indicated by the bold arrows inFig.
2, and includes all system components ex-cept the IM.
The dotted arrows signal the devia-tions to this main path that are introduced by theNatural LanguageUnderstanding (NLU)AcousticAnalyzer (AA)Automatic SpeechRecognition (ASR)Acoustic Emotion Classifier (AEC)SentimentAnalyzer (SA)Dialogue ActTagger (DAT)DialogueManager(DM)Affective Strategy(ASM)Multimodal FissionManager (MFM)Text-to-Speech(TTS)Avatar(ECA)Acoustic Turn-Taking (ATT)InterruptionManager(IM)EmotionalModel (EM)Natural LanguageGeneration (NLG)KnowledgeBase & UMFigure 2:HWYD Companion main modulesinterruption management and feedback loops.The system has an activity detector in the inputsubsystem that is active permanently and analy-ses user input in real-time.
If there is a detectionof user input at the same time as the ECA is talk-ing, this module triggers a signal that is capturedby the IM.
The IM, which tracks the activity ofthe rest of the modules in the system, has a set ofheuristics that are examined each time this trig-gering signal is detected.
If any heuristicmatches, the system decides there has been aproper user interruption and decides upon a se-ries of actions to recover from the interruption.4 Module Processing ProcedureThe first stage in the processing is the acousticprocessing.
User speech is processed by theAcoustic Analyzer, the Automatic Speech Rec-ognizer, and the Acoustic Emotion Classifiersimultaneously for maximum responsiveness.The Acoustic Analyzer (AA) extracts low-level features (pitch, intensity and the probabilitythat the input was from voiced speech) from theacoustic signal at frequent time intervals (typi-cally 10 milliseconds).
Features are passed to theAcoustic Turn-Taking Detector in larger buffers(a few hundred milliseconds) together with time-stamps.
AA is implemented in TCL using Snacktoolkit (http://www.speech.kth.se/snack/).The Acoustic Turn-Taking detector (ATT)is a Java module, which estimates when the userhas finished a turn by comparing intensity pauselengths and pitch information of user speech toconfigurable empirical thresholds.
ATT also de-cides whether the user has interrupted the system48(?barge-in?
), while ignoring shorter backchannel-ling phrases (Crook et al (2010)).
Interruptionmessages are passed to the Interruption Manager.ATT receives a message from the ECA modulewhen the system starts or stops speaking.Dragon Naturally Speaking AutomaticSpeech Recognition (ASR) system is used toprovide real-time large vocabulary speech recog-nition.
Per-user acoustic adaptation is used toimprove recognition rates.
ASR provides N-bestlists, confidence scores, and phrase hypotheses.The Acoustic Emotion Classifier (AEC)component (EmoVoice (Vogt et al (2008)) cate-gorizes segments of user speech into five va-lence+arousal categories, also applying a confi-dence score.
The Interruption Manager monitorsthe messages of the AEC to include emotion-related information into feedback loop messagessent to the ECA subsystem.
This allows rapidreactions to the user mood.The Sentiment Analyzer (SA) labels ASRoutput strings with sentiment information atword and sentence levels using valence catego-ries positive, neutral and negative.
The SA usesthe AFFECTiS Sentiment Server, which is a gen-eral purpose .NET SOAP XML service foranalysis and scoring of author sentiment.The Emotional Model (EM), written in Lisp,fuses information from the AEC and SA.
Itstores a globally accessible emotional representa-tion of the user for other system modules tomake use of.
Affective fusion is rule-based, pre-fers the SA?s valence information, and outputsthe same five valence+arousal categories as usedin the AEC.
The EM can also serve as a basis fortemporal integration (mood representation) aspart of the affective content of the User Model.
Italso combines the potentially different segmenta-tions by the ASR and AEC.The User Model (UM) stores facts about theuser as objects and associated attributes.
The in-formation contained in the User Model is used byother system modules, in particular by DialogueManager and Affective Strategy Module.The Dialogue Act Tagger and Segmenter(DAT), written in C under Linux, uses the ATTresults to compile all ASR results correspondingto each user turn.
DAT then segments the com-bined results into semantic units and labels eachwith a dialogue act (DA) tag (from a subset ofSWBD-DAMSL (Jurafsky et al (2001)).
A Sto-chastic Machine Learning model combiningHidden Markov Model (HMM) and N-grams isused in a manner analogous to Mart?nez-Hinarejos et al (2006).
The N-grams yield theprobability of a possible DA tag given the previ-ous ones.
The Viterbi algorithm is used to findthe most likely sequence of DA tags.The Natural Language Understanding(NLU) component, implemented in Prolog, pro-duces a logical form representing the semanticmeaning of a user turn.
The NLU consists of apart-of-speech tagger, a Noun Phrase and VerbGroup chunker, a named-entity classificationcomponent (rule-based), and a set of pattern-matching rules which recognize major gram-matical relationships (subject, direct object, etc.
)The resulting shallow-parsed text is further proc-essed using pattern-matching rules.
These recog-nize configurations of entity and relation relevantto the templates needed by the Dialogue Man-ager, the EM, and the Affective Strategy Module.The Dialogue Manager (DM), written in Javaand Prolog, combines the SA and NLU results,decides on the system's next utterance and identi-fies salient objects for the Affective StrategyModule.
The DM maintains an information statecontaining information about concepts under dis-cussion, as well as the system's agenda of currentconversational goals.One of the main features of the HWYD Com-panion is its ability to positively influence theuser?s mood through its Affective StrategyModule (ASM).
This module appraises theuser?s situation, considering the events reportedin the user turn and its (bi-modal) affective ele-ments.
From this appraisal, the ASM generates along multi-utterance turn.
Each utterance imple-ments communicative acts constitutive of thestrategy.
ASM generates influence operatorswhich are passed to the Natural Language Gen-eration module.
ASM output is triggered whenthe system has learned enough about a particularevent to warrant affective influence.
As input,ASM takes information extraction templates de-scribing events, together with the emotional dataattached.
ASM is a Hierarchical Task Network(HTN) Planner implemented in Lisp.The Natural Language Generator (NLG),written in Lisp, produces linguistic surface formsfrom influence operators produced by the ASM.These operators correspond to communicativeactions taking the form of performatives.
NLGuses specific rhetorical structures and constructsassociated with humour, and uses emotional TTSexpressions through specific lexical choice.495 Multimodal ECA ControlMultimodal control of the ECA, which consistsof a tightly-synchronized naturalistic avatar andaffective Text-To-Speech (TTS) generation, ishighly challenging from an architectural view-point, since the coordinating component needs tobe properly synchronized with the rest of the sys-tem, including both the main dialogue loop andthe feedback and interruption loops.The system Avatar is in charge of generating athree-dimensional, human-like character to serveas the system?s ?face?.
The avatar is connected tothe TTS, and the speech is synchronized with thelip movements.
The prototype is currently usingthe HaptekTM 3D avatar engine running inside aweb browser.
The Haptek engine provides a talk-ing head and torso along with a low level API tocontrol its interaction with any SAPI-compliantTTS subsystem, and also allows some manipula-tion of the character animation.
An intermediatelayer consisting of a Java applet and Javascriptcode embeds the rendered avatar in a web pageand provides connectivity with the MultimodalFission Manager.
We intend to replace the cur-rent avatar with a photorealistic avatar under de-velopment within the project consortium.LoquendoTM TTS SAPI synthesizer is used tovocalize system turns.
The TTS engine works inclose connection with the ECA software usingthe SAPI interface.
TTS includes custom para-linguistic events for producing expressivespeech.
TTS is based on the concatenative tech-nique with variable length acoustic units.The Multimodal Fission Manager (MFM) con-trols the Avatar and the TTS engine, enabling thesystem to construct complex communicative actsthat chain together series of utterances and ges-tures.
It offers FML-standard-based syntax tomake the avatar perform a series of body andfacial gestures.The system features a template-based inputmode in which a module can call ECA to per-form actions without having to build a full FML-based XML message.
This is intended to be usedin the feedback loops, for example, to convey theimpression that the ECA is paying attention.6 ConclusionsWe have presented an advanced multimodal dia-logue system that challenges the usual pipeline-based implementation.
To do so, it leverages onan architecture that provides the means for aflexible component interconnection, that can ac-comodate the needs of a system using more thanone processing path for its data.
We have shownhow this has enabled us to implement complexbehavior such as interrupt and short loop han-dling.
We are currently expanding coverage andwill carry out an evaluation with real users thisSeptember.AcknowledgementsThis work was funded by Companions, a Euro-pean Commission Sixth Framework ProgrammeInformation Society Technologies IntegratedProject (IST-34434).ReferencesVogt, T., Andr?, E. and Bee, N. 2008.
EmoVoice ?
Aframework for online recognition of emotions fromvoice.
In: Proc.
Workshop on Perception and In-teractive Technologies for Speech-Based Systems,Springer, Kloster Irsee, Germany.Cavazza, M., Smith, C., Charlton, D., Crook, N.,Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-tos de la Camara, R., Turunen, M. 2010 PersuasiveDialogue based on a Narrative Theory: an ECAImplementation, Proc.
5th Int.
Conf.
on PersuasiveTechnology (to appear).Hern?ndez, A., L?pez, B., Pardo, D., Santos, R.,Hern?ndez, L., Rela?o Gil, J. and Rodr?guez, M.C.2008 Modular definition of multimodal ECAcommunication acts to improve dialogue robust-ness and depth of intention.
In: Heylen, D., Kopp,S., Marsella, S., Pelachaud, C., and Vilhj?lmsson,H.
(Eds.
), AAMAS 2008 Workshop on FunctionalMarkup Language.Crook, N., Smith, C., Cavazza, M., Pulman, S.,Moore, R., and Boye, J.
2010 Handling User Inter-ruptions in an Embodied Conversational Agent.
InProc.
AAMAS 2010.Wagner J., Andr?, E., and Jung, F. 2009 Smart sensorintegration: A framework for multimodal emotionrecognition in real-time.
In Affective Computingand Intelligent Interaction 2009.Cavazza, M., Pizzi, D., Charles, F., Vogt, T. Andr?,E.
2009 Emotional input for character?based in-teractive storytelling AAMAS (1) 2009: 313-320.Jurafsky, D. Shriberg, E., Biasca, D. 2001Switchboard swbd?damsl shallow?
discourse?function annotation coders manual.
Tech.
Rep. 97?01, University of Colorado Institute of CognitiveScienceMart?nez?Hinarejos, C.D., Granell, R., Bened?, J.M.2006.
Segmented and unsegmented dialogue?actannotation with statistical dialogue models.
Proc.COLING/ACL Sydney, Australia, pp.
563?570.50
