Proceedings of ACL-08: HLT, pages 755?762,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsDistributed Word Clustering for Large Scale Class-BasedLanguage Modeling in Machine TranslationJakob Uszkoreit?
Thorsten BrantsGoogle, Inc.1600 Amphitheatre ParkwayMountain View, CA 94303, USA{uszkoreit,brants}@google.comAbstractIn statistical language modeling, one techniqueto reduce the problematic effects of data spar-sity is to partition the vocabulary into equiva-lence classes.
In this paper we investigate theeffects of applying such a technique to higher-order n-gram models trained on large corpora.We introduce a modification of the exchangeclustering algorithm with improved efficiencyfor certain partially class-based models and adistributed version of this algorithm to effi-ciently obtain automatic word classificationsfor large vocabularies (>1 million words) us-ing such large training corpora (>30 billion to-kens).
The resulting clusterings are then usedin training partially class-based language mod-els.
We show that combining them with word-based n-gram models in the log-linear modelof a state-of-the-art statistical machine trans-lation system leads to improvements in trans-lation quality as indicated by the BLEU score.1 IntroductionA statistical language model assigns a probabilityP (w) to any given string of words wm1 = w1, ..., wm.In the case of n-gram language models this is doneby factoring the probability:P (wm1 ) =m?i=1P (wi|wi?11 )and making a Markov assumption by approximatingthis by:m?i=1P (wi|wi?11 ) ?m?i=1p(wi|wi?1i?n+1)Even after making the Markov assumption and thustreating all strings of preceding words as equal which?
Parts of this research were conducted while the authorstudied at the Berlin Institute of Technologydo not differ in the last n?
1 words, one problem n-gram language models suffer from is that the trainingdata is too sparse to reliably estimate all conditionalprobabilities P (wi|wi?11 ).Class-based n-gram models are intended to helpovercome this data sparsity problem by groupingwords into equivalence classes rather than treatingthem as distinct words and thus reducing the num-ber of parameters of the model (Brown et al, 1990).They have often been shown to improve the per-formance of speech recognition systems when com-bined with word-based language models (Martin etal., 1998; Whittaker and Woodland, 2001).
However,in the area of statistical machine translation, espe-cially in the context of large training corpora, fewerexperiments with class-based n-gram models havebeen performed with mixed success (Raab, 2006).Class-based n-gram models have also been shownto benefit from their reduced number of parameterswhen scaling to higher-order n-grams (Goodman andGao, 2000), and even despite the increasing size anddecreasing sparsity of language model training cor-pora (Brants et al, 2007), class-based n-gram mod-els might lead to improvements when increasing then-gram order.When training class-based n-gram models on largecorpora and large vocabularies, one of the prob-lems arising is the scalability of the typical cluster-ing algorithms used for obtaining the word classifi-cation.
Most often, variants of the exchange algo-rithm (Kneser and Ney, 1993; Martin et al, 1998)or the agglomerative clustering algorithm presentedin (Brown et al, 1990) are used, both of which haveprohibitive runtimes when clustering large vocabu-laries on the basis of large training corpora with asufficiently high number of classes.In this paper we introduce a modification of the ex-change algorithm with improved efficiency and thenpresent a distributed version of the modified algo-rithm, which makes it feasible to obtain word clas-755sifications using billions of tokens of training data.We then show that using partially class-based lan-guage models trained using the resulting classifica-tions together with word-based language models ina state-of-the-art statistical machine translation sys-tem yields improvements despite the very large sizeof the word-based models used.2 Class-Based Language ModelingBy partitioning all Nv words of the vocabulary intoNc sets, with c(w) mapping a word onto its equiva-lence class and c(wji ) mapping a sequence of wordsonto the sequence of their respective equivalenceclasses, a typical class-based n-gram model approxi-mates P (wi|wi?11 ) with the two following componentprobabilities:P (wi|wi?11 ) ?
p0(wi|c(wi)) ?
p1(c(wi)|c(wi?1i?n+1))(1)thus greatly reducing the number of parameters inthe model, since usually Nc is much smaller thanNv.In the following, we will call this type of model atwo-sided class-based model, as both the history ofeach n-gram, the sequence of words conditioned on,as well as the predicted word are replaced by theirclass.Once a partition of the words in the vocabulary isobtained, two-sided class-based models can be builtjust like word-based n-gram models using existinginfrastructure.
In addition, the size of the model isusually greatly reduced.2.1 One-Sided Class-Based ModelsTwo-sided class-based models received most atten-tion in the literature.
However, several differenttypes of mixed word and class models have beenproposed for the purpose of improving the perfor-mance of the model (Goodman, 2000), reducing itssize (Goodman and Gao, 2000) as well as lower-ing the complexity of related clustering algorithms(Whittaker and Woodland, 2001).In (Emami and Jelinek, 2005) a clustering algo-rithm is introduced which outputs a separate clus-tering for each word position in a trigram model.
Inthe experimental evaluation, the authors observe thelargest improvements using a specific clustering forthe last word of each trigram but no clustering atall for the first two word positions.
Generalizing thisleads to arbitrary order class-based n-gram modelsof the form:P (wi|wi?11 ) ?
p0(wi|c(wi)) ?
p1(c(wi)|wi?1i?n+1) (2)which we will call predictive class-based models in thefollowing sections.3 Exchange ClusteringOne of the frequently used algorithms for automat-ically obtaining partitions of the vocabulary is theexchange algorithm (Kneser and Ney, 1993; Martinet al, 1998).
Beginning with an initial clustering,the algorithm greedily maximizes the log likelihoodof a two-sided class bigram or trigram model as de-scribed in Eq.
(1) on the training data.
Let V bethe set of words in the vocabulary and C the set ofclasses.
This then leads to the following optimizationcriterion, where N(w) and N(c) denote the numberof occurrences of a word w or a class c in the trainingdata and N(c, d) denotes the number of occurrencesof some word in class c followed by a word in class din the training data:C?
= argmaxC?w?VN(w) ?
logN(w) ++?c?C,d?CN(c, d) ?
logN(c, d)?
?2 ?
?c?CN(c) ?
logN(c) (3)The algorithm iterates over all words in the vo-cabulary and tentatively moves each word to eachcluster.
The change in the optimization criterion iscomputed for each of these tentative moves and theexchange leading to the highest increase in the opti-mization criterion (3) is performed.
This procedureis then repeated until the algorithm reaches a localoptimum.To be able to efficiently calculate the changes inthe optimization criterion when exchanging a word,the counts in Eq.
(3) are computed once for the ini-tial clustering, stored, and afterwards updated whena word is exchanged.Often only a limited number of iterations are per-formed, as letting the algorithm terminate in a localoptimum can be computationally impractical.3.1 ComplexityThe implementation described in (Martin et al,1998) uses a memory saving technique introducinga binary search into the complexity estimation.
Forthe sake of simplicity, we omit this detail in the fol-lowing complexity analysis.
We also do not employthis optimization in our implementation.The worst case complexity of the exchange algo-rithm is quadratic in the number of classes.
However,756Input: The fixed number of clusters NcCompute initial clusteringwhile clustering changed in last iteration doforall w ?
V doforall c ?
C domove word w tentatively to clusterccompute updated optimizationcriterionmove word w to cluster maximizingoptimization criterionAlgorithm 1: Exchange Algorithm Outlinethe average case complexity can be reduced by up-dating only the counts which are actually affected bymoving a word from one cluster to another.
This canbe done by considering only those sets of clusters forwhich N(w, c) > 0 or N(c, w) > 0 for a word w aboutto be exchanged, both of which can be calculated ef-ficiently when exchanging a word.
The algorithmscales linearly in the size of the vocabulary.With Nprec and Nsucc denoting the average numberof clusters preceding and succeeding another cluster,B denoting the number of distinct bigrams in thetraining corpus, and I denoting the number of itera-tions, the worst case complexity of the algorithm isin:O(I ?
(2 ?B +Nv ?Nc ?
(Nprec +Nsucc )))When using large corpora with large numbers ofbigrams the number of required updates can increasetowards the quadratic upper bound as Nprec andNsucc approach Nc.
For a more detailed descriptionand further analysis of the complexity, the reader isreferred to (Martin et al, 1998).4 Predictive Exchange ClusteringModifying the exchange algorithm in order to opti-mize the log likelihood of a predictive class bigrammodel, leads to substantial performance improve-ments, similar to those previously reported for an-other type of one-sided class model in (Whittakerand Woodland, 2001).We use a predictive class bigram model as givenin Eq.
(2), for which the maximum-likelihood prob-ability estimates for the n-grams are given by theirrelative frequencies:P (wi|wi?11 ) ?
p0(wi|c(wi)) ?
p1(c(wi)|wi?1)(4)=N(wi)N(c(wi))?N(wi?1, c(wi))N(wi?1)(5)whereN(w) again denotes the number of occurrencesof the word w in the training corpus and N(v, c)the number of occurrences of the word v followed bysome word in class c. Then the following optimiza-tion criterion can be derived, with F (C) being thelog likelihood function of the predictive class bigrammodel given a clustering C:F (C) =?w?VN(w) ?
log p(w|c(w))+?v?V,c?CN(v, c) ?
log p(c|v) (6)=?w?VN(w) ?
logN(w)N(c(w))+?v?V,c?CN(v, c) ?
logN(v, c)N(v)(7)=?w?VN(w) ?
logN(w)?
?w?VN(w) ?
logN(c(w))+?v?V,c?CN(v, c) ?
logN(v, c)?
?v?V,c?CN(v, c) ?
logN(v) (8)The very last summation of Eq.
(8) now effectivelysums over all occurrences of all words and thus can-cels out with the first summation of (8) which leadsto:F (C) =?v?V,c?CN(v, c) ?
logN(v, c)?
?w?VN(w) ?
logN(c(w)) (9)In the first summation of Eq.
(9), for a given word vonly the set of classes which contain at least one wordw for whichN(v, w) > 0 must be considered, denotedby suc(v).
The second summation is equivalent to?c?C N(c) ?
logN(c).
Thus the further simplifiedcriterion is:F (C) =?v?V,c?suc(v)N(v, c) ?
logN(v, c)?
?c?CN(c) ?
logN(c) (10)When exchanging a word w between two classes cand c?, only two summands of the second summationof Eq.
(10) are affected.
The first summation can beupdated by iterating over all bigrams ending in theexchanged word.
Throughout one iteration of thealgorithm, in which for each word in the vocabularyeach possible move to another class is evaluated, this757amounts to the number of distinct bigrams in thetraining corpus B, times the number of clusters Nc.Thus the worst case complexity using the modifiedoptimization criterion is in:O(I ?Nc ?
(B +Nv))Using this optimization criterion has two effectson the complexity of the algorithm.
The first dif-ference is that in contrast to the exchange algorithmusing a two sided class-based bigram model in its op-timization criterion, only two clusters are affected bymoving a word.
Thus the algorithm scales linearlyin the number of classes.
The second difference isthat B dominates the term B+Nv for most corporaand scales far less than linearly with the vocabularysize, providing a significant performance advantageover the other optimization criterion, especially whenlarge vocabularies are used (Whittaker and Wood-land, 2001).For efficiency reasons, an exchange of a word be-tween two clusters is separated into a remove and amove procedure.
In each iteration the remove proce-dure only has to be called once for each word, whilefor a given word move is called once for every clus-ter to compute the consequences of the tentative ex-changes.
An outline of the move procedure is givenbelow.
The remove procedure is similar.Input: A word w, and a destination cluster cResult: The change in the optimizationcriterion when moving w to cluster cdelta?
N(c) ?
logN(c)N ?(c)?
N(c)?N(w)delta?
delta?N ?
(c) ?
logN ?
(c)if not a tentative move thenN(c)?
N ?
(c)forall v ?
suc(w) dodelta?
delta?N(v, c) ?
logN(v, c)N ?
(v, c)?
N(v, c)?N(v, w)delta?
delta+N ?
(v, c) ?
logN ?
(v, c)if not a tentative move thenN(v, c)?
N ?
(v, c)return deltaProcedure MoveWord5 Distributed ClusteringWhen training on large corpora, even the modifiedexchange algorithm would still require several daysif not weeks of CPU time for a sufficient number ofiterations.To overcome this we introduce a novel distributedexchange algorithm, based on the modified exchangealgorithm described in the previous section.
The vo-cabulary is randomly partitioned into sets of roughlyequal size.
With each word w in one of these sets, allwords v preceding w in the corpus are stored withthe respective bigram count N(v, w).The clusterings generated in each iteration as wellas the initial clustering are stored as the set of wordsin each cluster, the total number of occurrences ofeach cluster in the training corpus, and the list ofwords preceeding each cluster.
For each word w inthe predecessor list of a given cluster c, the numberof times w occurs in the training corpus before anyword in c, N(w, c), is also stored.Together with the counts stored with the vocab-ulary partitions, this allows for efficient updating ofthe terms in Eq.
(10).The initial clustering together with all the requiredcounts is created in an initial iteration by assigningthe n-th most frequent word to cluster n mod Nc.While (Martin et al, 1998) and (Emami and Je-linek, 2005) observe that the initial clustering doesnot seem to have a noticeable effect on the qualityof the resulting clustering or the convergence rate,the intuition behind this method of initialization isthat it is unlikely for the most frequent words to beclustered together due to their high numbers of oc-currences.In each subsequent iteration each one of a num-ber of workers is assigned one of the partitions ofthe words in the vocabulary.
After loading the cur-rent clustering, it then randomly chooses a subsetof these words of a fixed size.
For each of the se-lected words the worker then determines to whichcluster the word is to be moved in order to maxi-mize the increase in log likelihood, using the countupdating procedures described in the previous sec-tion.
All changes a worker makes to the clusteringare accumulated locally in delta data structures.
Atthe end of the iteration all deltas are merged andapplied to the previous clustering, resulting in thecomplete clustering loaded in the next iteration.This algorithm fits well into the MapReduce pro-gramming model (Dean and Ghemawat, 2004) thatwe used for our implementation.5.1 ConvergenceWhile the greedy non-distributed exchange algo-rithm is guaranteed to converge as each exchangeincreases the log likelihood of the assumed bigrammodel, this is not necessarily true for the distributedexchange algorithm.
This stems from the fact thatthe change in log likelihood is calculated by eachworker under the assumption that no other changesto the clustering are performed by other workers in758this iteration.
However, if in each iteration only arather small and randomly chosen subset of all wordsare considered for exchange, the intuition is that theremaining words still define the parameters of eachcluster well enough for the algorithm to converge.In (Emami and Jelinek, 2005) the authors observethat only considering a subset of the vocabulary ofhalf the size of the complete vocabulary in each it-eration does not affect the time required by the ex-change algorithm to converge.
Yet each iteration issped up by approximately a factor of two.
The qual-ity of class-based models trained using the result-ing clusterings did not differ noticeably from thosetrained using clusterings for which the full vocabu-lary was considered in each iteration.
Our experi-ments showed that this also seems to be the case forthe distributed exchange algorithm.
While consider-ing very large subsets of the vocabulary in each iter-ation can cause the algorithm to not converge at all,considering only a very small fraction of the wordsfor exchange will increase the number of iterationsrequired to converge.
In experiments we empiricallydetermined that choosing a subset of roughly a thirdof the size of the full vocabulary is a good balance inthis trade-off.
We did not observe the algorithm tonot converge unless we used fractions above half ofthe vocabulary size.We typically ran the clustering for 20 to 30 itera-tions after which the number of words exchanged ineach iteration starts to stabilize at less than 5 per-cent of the vocabulary size.
Figure 1 shows the num-ber of words exchanged in each of 34 iterations whenclustering the approximately 300,000 word vocabu-lary of the Arabic side of the English-Arabic paralleltraining data into 512 and 2,048 clusters.Despite a steady reduction in the number of wordsexchanged per iteration, we observed the conver-gence in regards to log-likelihood to be far frommonotone.
In our experiments we were able toachieve significantly more monotone and faster con-vergence by employing the following heuristic.
Asdescribed in Section 5, we start out the first itera-tion with a random partition of the vocabulary intosubsets each assigned to a specific worker.
However,instead of keeping this assignment constant through-out all iterations, after each iteration the vocabu-lary is partitioned anew so that all words from anygiven cluster are considered by the same worker inthe next iteration.
The intuition behind this heuris-tic is that as the clustering becomes more coherent,the information each worker has about groups of sim-ilar words is becoming increasingly accurate.
In ourexperiments this heuristic lead to almost monotoneconvergence in log-likelihood.
It also reduced the01000020000300004000050000600007000080000900001000000  5  10  15  20  25  30  35wordsexchangediteration512 clusters2048 clustersFigure 1: Number of words exchanged per iterationwhen clustering the vocabulary of the Arabic side ofthe English-Arabic parallel training data (347 million to-kens).number of iterations required to converge by up to afactor of three.5.2 Resource RequirementsThe runtime of the distributed exchange algorithmdepends highly on the number of distinct bigrams inthe training corpus.
When clustering the approxi-mately 1.5 million word vocabulary of a 405 milliontoken English corpus into 1,000 clusters, one itera-tion takes approximately 5 minutes using 50 workersbased on standard hardware running the Linux oper-ating system.
When clustering the 0.5 million mostfrequent words in the vocabulary of an English cor-pus with 31 billion tokens into 1,000 clusters, one it-eration takes approximately 30 minutes on 200 work-ers.When scaling up the vocabulary and corpus sizes,the current bottleneck of our implementation is load-ing the current clustering into memory.
While thememory requirements decrease with each iteration,during the first few iterations a worker typically stillneeds approximately 2 GB of memory to load theclustering generated in the previous iteration whentraining 1,000 clusters on the 31 billion token corpus.6 ExperimentsWe trained a number of predictive class-based lan-guage models on different Arabic and English cor-pora using clusterings trained on the complete dataof the same corpus.
We use the distributed trainingand application infrastructure described in (Brantset al, 2007) with modifications to allow the trainingof predictive class-based models and their applicationin the decoder of the machine translation system.759For all models used in our experiments, both word-and class-based, the smoothing method used wasStupid Backoff (Brants et al, 2007).
Models withStupid Backoff return scores rather than normalizedprobabilities, thus perplexities cannot be calculatedfor these models.
Instead we report BLEU scores(Papineni et al, 2002) of the machine translation sys-tem using different combinations of word- and class-based models for translation tasks from English toArabic and Arabic to English.6.1 Training DataFor English we used three different training data sets:en target: The English side of Arabic-English andChinese-English parallel data provided by LDC (405million tokens).en ldcnews: Consists of several English news datasets provided by LDC (5 billion tokens).en webnews: Consists of data collected up to De-cember 2005 from web pages containing primarilyEnglish news articles (31 billion tokens).A fourth data set, en web, was used together withthe other three data sets to train the large word-based model used in the second machine translationexperiment.
This set consists of general web datacollected in January 2006 (2 trillion tokens).For Arabic we used the following two differenttraining data sets:ar gigaword: Consists of several Arabic news datasets provided by LDC (629 million tokens).ar webnews: Consists of data collected up toDecember 2005 from web pages containing primarilyArabic news articles (approximately 600 milliontokens).6.2 Machine Translation ResultsGiven a sentence f in the source language, the ma-chine translation problem is to automatically pro-duce a translation e?
in the target language.
In thesubsequent experiments, we use a phrase-based sta-tistical machine translation system based on the log-linear formulation of the problem described in (Ochand Ney, 2002):e?
= argmaxep(e|f)= argmaxeM?m=1?mhm(e, f) (11)where {hm(e, f)} is a set of M feature functions and{?m} a set of weights.
We use each predictive class-based language model as well as a word-based modelas separate feature functions in the log-linear com-bination in Eq.
(11).
The weights are trained usingminimum error rate training (Och, 2003) with BLEUscore as the objective function.The dev and test data sets contain parts of the2003, 2004 and 2005 Arabic NIST MT evaluationsets among other parallel data.
The blind test dataused is the ?NIST?
part of the 2006 Arabic-EnglishNIST MT evaluation set, and is not included in thetraining data.For the first experiment we trained predictiveclass-based 5-gram models using clusterings with 64,128, 256 and 512 clusters1 on the en target data.
Wethen added these models as additional features tothe log linear model of the Arabic-English machinetranslation system.
The word-based language modelused by the system in these experiments is a 5-grammodel also trained on the en target data set.Table 1 shows the BLEU scores reached by thetranslation system when combining the differentclass-based models with the word-based model incomparison to the BLEU scores by a system usingonly the word-based model on the Arabic-Englishtranslation task.dev test nist06word-based only 0.4085 0.3498 0.508864 clusters 0.4122 0.3514 0.5114128 clusters 0.4142 0.3530 0.5109256 clusters 0.4141 0.3536 0.5076512 clusters 0.4120 0.3504 0.5140Table 1: BLEU scores of the Arabic English system usingmodels trained on the English en target data setAdding the class-based models leads to small im-provements in BLEU score, with the highest im-provements for both dev and nist06 being statisti-cally significant 2.In the next experiment we used two predictiveclass-based models, a 5-gram model with 512 clusterstrained on the en target data set and a 6-gram modelalso using 512 clusters trained on the en ldcnewsdata set.
We used these models in addition toa word-based 6-gram model created by combiningmodels trained on all four English data sets.Table 2 shows the BLEU scores of the machinetranslation system using only this word-based model,the scores after adding the class-based model trainedon the en target data set and when using all threemodels.1The beginning of sentence, end of sentence and unkownword tokens were each treated as separate clusters2Differences of more than 0.0051 are statistically significantat the 0.05 level using bootstrap resampling (Noreen, 1989;Koehn, 2004)760dev test nist06word-based only 0.4677 0.4007 0.5672with en target 0.4682 0.4022 0.5707all three models 0.4690 0.4059 0.5748Table 2: BLEU scores of the Arabic English system usingmodels trained on various data setsFor our experiment with the English Arabic trans-lation task we trained two 5 -gram predictive class-based models with 512 clusters on the Arabicar gigaword and ar webnews data sets.
The word-based Arabic 5-gram model we used was createdby combining models trained on the Arabic side ofthe parallel training data (347 million tokens), thear gigaword and ar webnews data sets, and addi-tional Arabic web data.dev test nist06word-based only 0.2207 0.2174 0.3033with ar webnews 0.2237 0.2136 0.3045all three models 0.2257 0.2260 0.3318Table 3: BLEU scores of the English Arabic system usingmodels trained on various data setsAs shown in Table 3, adding the predictive class-based model trained on the ar webnews data setleads to small improvements in dev and nist06scores but causes the test score to decrease.
How-ever, adding the class-based model trained on thear gigaword data set to the other class-based and theword-based model results in further improvement ofthe dev score, but also in large improvements of thetest and nist06 scores.We performed experiments to eliminate the pos-sibility of data overlap between the training dataand the machine translation test data as cause forthe large improvements.
In addition, our experi-ments showed that when there is overlap betweenthe training and test data, the class-based modelslead to lower scores as long as they are trained onlyon data also used for training the word-based model.One explanation could be that the domain of thear gigaword corpus is much closer to the domain ofthe test data than that of other training data setsused.
However, further investigation is required toexplain the improvements.6.3 ClustersThe clusters produced by the distributed algorithmvary in their size and number of occurrences.
Ina clustering of the en target data set with 1,024clusters, the cluster sizes follow a typical long-tailed distribution with the smallest cluster contain-Bai Bi Bu Cai Cao Chang Chen Cheng Chou Chuang Cui DaiDeng Ding Du Duan Fan Fu Gao Ge Geng Gong Gu GuanHan Hou Hsiao Hsieh Hsu Hu Huang Huo Jiang Jiao JuanKang Kuang Kuo Li Liang Liao Lin Liu Lu Luo Mao MeetsMeng Mi Miao Mu Niu Pang Pi Pu Qian Qiao Qiu Qu RenRun Shan Shang Shen Si Song Su Sui Sun Tan Tang Tian TuWang Wu Xie Xiong Xu Yang Yao Ye Yin Zeng Zhang ZhaoZheng Zhou Zhu Zhuang Zou% PERCENT cents percentapprovals bonus cash concessions cooperatives credit disburse-ments dividends donations earnings emoluments entitlementsexpenditure expenditures fund funding funds grants incomeincomes inflation lending liquidity loan loans mortgage mort-gages overhead payroll pension pensions portfolio profits pro-tectionism quotas receipts receivables remittances remunera-tion rent rents returns revenue revenues salaries salary savingsspending subscription subsidies subsidy surplus surpluses taxtaxation taxes tonnage tuition turnover wage wagesAbby Abigail Agnes Alexandra Alice Amanda Amy AndreaAngela Ann Anna Anne Annette Becky Beth Betsy BonnieBrenda Carla Carol Carole Caroline Carolyn Carrie CatherineCathy Cheryl Christina Christine Cindy Claire Clare ClaudiaColleen Cristina Cynthia Danielle Daphne Dawn Debbie Deb-orah Denise Diane Dina Dolores Donna Doris Edna EileenElaine Eleanor Elena Elisabeth Ellen Emily Erica Erin EstherEvelyn Felicia Felicity Flora Frances Gail Gertrude GillianGina Ginger Gladys Gloria Gwen Harriet Heather Helen Hi-lary Irene Isabel Jane Janice Jeanne Jennifer Jenny JessicaJo Joan Joanna Joanne Jodie Josie Judith Judy Julia JulieKaren Kate Katherine Kathleen Kathryn Kathy Katie Kim-berly Kirsten Kristen Kristin Laura Laurie Leah Lena Lil-lian Linda Lisa Liz Liza Lois Loretta Lori Lorraine LouiseLynne Marcia Margaret Maria Marian Marianne Marilyn Mar-jorie Marsha Mary Maureen Meg Melanie Melinda MelissaMerle Michele Michelle Miriam Molly Nan Nancy Naomi Na-talie Nina Nora Norma Olivia Pam Pamela Patricia PattiPaula Pauline Peggy Phyllis Rachel Rebecca Regina ReneeRita Roberta Rosemary Sabrina Sally Samantha Sarah SelenaSheila Shelley Sherry Shirley Sonia Stacy Stephanie Sue Su-sanne Suzanne Suzy Sylvia Tammy Teresa Teri Terri TheresaTina Toni Tracey Ursula Valerie Vanessa Veronica Vicki Vi-vian Wendy Yolanda Yvonnealmonds apple apples asparagus avocado bacon bananas bar-ley basil bean beans beets berries berry boneless broccolicabbage carrot carrots celery cherries cherry chile chiles chilichilies chives cilantro citrus cranberries cranberry cucumbercucumbers dill doughnuts egg eggplant eggs elk evergreen fen-nel figs flowers fruit fruits garlic ginger grapefruit grasses herbherbs jalapeno Jell-O lemon lemons lettuce lime lions mac-aroni mango maple melon mint mozzarella mushrooms oakoaks olives onion onions orange oranges orchids oregano oys-ter parsley pasta pastries pea peach peaches peanuts pearpears peas pecan pecans perennials pickles pine pineapplepines plum pumpkin pumpkins raspberries raspberry rice rose-mary roses sage salsa scallions scallops seasonings seaweedshallots shrimp shrubs spaghetti spices spinach strawberriesstrawberry thyme tomato tomatoes truffles tulips turtles wal-nut walnuts watermelon wildflowers zucchinimid-April mid-August mid-December mid-February mid-January mid-July mid-June mid-March mid-May mid-November mid-October mid-September mid-afternoonmidafternoon midmorning midsummerTable 4: Examples of clusters761ing 13 words and the largest cluster containing 20,396words.
Table 4 shows some examples of the gener-ated clusters.
For each cluster we list all words oc-curring more than 1,000 times in the corpus.7 ConclusionIn this paper, we have introduced an efficient, dis-tributed clustering algorithm for obtaining word clas-sifications for predictive class-based language modelswith which we were able to use billions of tokens oftraining data to obtain classifications for millions ofwords in relatively short amounts of time.The experiments presented show that predictiveclass-based models trained using the obtained wordclassifications can improve the quality of a state-of-the-art machine translation system as indicated bythe BLEU score in both translation tasks.
Whenusing predictive class-based models in combinationwith a word-based language model trained on verylarge amounts of data, the improvements continue tobe statistically significant on the test and nist06 sets.We conclude that even despite the large amounts ofdata used to train the large word-based model inour second experiment, class-based language modelsare still an effective tool to ease the effects of datasparsity.We furthermore expect to be able to increase thegains resulting from using class-based models byusing more sophisticated techniques for combiningthem with word-based models such as linear inter-polations of word- and class-based models with coef-ficients depending on the frequency of the history.Another interesting direction of further research isto evaluate the use of the presented clustering tech-nique for language model size reduction.ReferencesThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language modelsin machine translation.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing and on Computational Natural LanguageLearning (EMNLP-CoNLL), pages 858?867, Prague,Czech Republic.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer.
1990.Class-based n-gram models of natural language.
Com-putational Linguistics, 18(4):467?479.Jeffrey Dean and Sanjay Ghemawat.
2004.
MapReduce:Simplified data processing on large clusters.
In Pro-ceedings of the Sixth Symposium on Operating SystemDesign and Implementation (OSDI-04), San Francisco,CA, USA.Ahmad Emami and Frederick Jelinek.
2005.
Ran-dom clusterings for language modeling.
In Proceedingsof the IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), Philadelphia,PA, USA.Joshua Goodman and Jianfeng Gao.
2000.
Languagemodel size reduction by pruning and clustering.
InProceedings of the IEEE International Conference onSpoken Language Processing (ICSLP), Beijing, China.Joshua Goodman.
2000.
A bit of progress in languagemodeling.
Technical report, Microsoft Research.Reinherd Kneser and Hermann Ney.
1993.
Improvedclustering techniques for class-based statistical lan-guage modelling.
In Proceedings of the 3rd EuropeanConference on Speech Communication and Technology,pages 973?976, Berlin, Germany.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of theConference on Empirical Methods in Natural LanguageProcessing (EMNLP), Barcelona, Spain.Sven Martin, Jo?rg Liermann, and Hermann Ney.
1998.Algorithms for bigram and trigram word clustering.Speech Communication, 24:19?37.Eric W. Noreen.
1989.
Computer-Intensive Methods forTesting Hypotheses.
John Wiley & Sons, New York.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 295?302, Philadelphia, PA,USA.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 160?167, Sapporo,Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 311?318, Philadelphia,PA, USA.Martin Raab.
2006.
Language model techniques in ma-chine translation.
Master?s thesis, Universita?t Karl-sruhe / Carnegie Mellon University.E.
W. D. Whittaker and P. C. Woodland.
2001.
Effi-cient class-based language modelling for very large vo-cabularies.
In Proceedings of the IEEE InternationalConference on Acoustics, Speech and Signal Processing(ICASSP), pages 545?548, Salt Lake City, UT, USA.762
