Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 821?831, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsType-Supervised Hidden Markov Models for Part-of-Speech Taggingwith Incomplete Tag DictionariesDan GarretteDepartment of Computer ScienceThe University of Texas at Austindhg@cs.utexas.eduJason BaldridgeDepartment of LinguisticsThe University of Texas at Austinjbaldrid@utexas.eduAbstractPast work on learning part-of-speech taggersfrom tag dictionaries and raw data has re-ported good results, but the assumptions madeabout those dictionaries are often unrealistic:due to historical precedents, they assume ac-cess to information about labels in the rawand test sets.
Here, we demonstrate ways tolearn hidden Markov model taggers from in-complete tag dictionaries.
Taking the MIN-GREEDY algorithm (Ravi et al2010) as astarting point, we improve it with several intu-itive heuristics.
We also define a simple HMMemission initialization that takes advantage ofthe tag dictionary and raw data to capture boththe openness of a given tag and its estimatedprevalence in the raw data.
Altogether, ouraugmentations produce improvements to per-formance over the original MIN-GREEDY al-gorithm for both English and Italian data.1 IntroductionLearning accurate part-of-speech (POS) taggersbased on plentiful labeled training material is gener-ally considered a solved problem.
The best taggersobtain accuracies of over 97% for English newswiretext in the Penn Treebank, which can be consid-ered as an upper-bound that matches human perfor-mance on the same task (Manning, 2011).
How-ever, as Manning notes, this story changes as soonas one is working with different assumptions anddata, including having less training data, differentkinds of training data, other languages, and otherdomains.
Such POS tagging work has been plen-tiful and includes efforts to induce POS tags withoutlabels (Christodoulopoulos et al2010); learn fromPOS-tag dictionaries (Ravi et al2010), incom-plete dictionaries (Hasan and Ng, 2009) and human-constructed dictionaries (Goldberg et al2008);bootstrap taggers for a language based on knowl-edge about other languages (Das and Petrov, 2011),and creating supervised taggers for new, challengingdomains such as Twitter (Gimpel et al2011).Here, we focus on learning from tag dictionar-ies.
This is often characterized as unsupervised orweakly supervised training.
We adopt the terminol-ogy type-supervised training to distinguish it fromunsupervised training from raw text and supervisedtraining from word tokens labeled with their parts-of-speech.
Work on type-supervision goes back to(Merialdo, 1994), who introduced the still standardprocedure of using a bigram Hidden Markov Model(HMM) trained via Expectation Maximization.Early research appeared to show that learningfrom types works nearly as well as learning fromtokens, with researchers in the 1990s obtaining ac-curacies up to 96% on English (e.g.
Kupiec (1992)).However, the tag dictionaries in these cases were ob-tained from labeled tokens.
While replicating earlierexperiments, Banko and Moore (2004) discoveredthat performance was highly dependent on clean-ing tag dictionaries using statistics gleaned from thetokens.
This greatly simplifies the job of a type-supervised HMM: it no longer must entertain entriesfor uncommon word-tag pairs (or mistaken pairsdue to annotation errors), which otherwise stand onequal footing with the common ones.
When thefull, noisy tag dictionary was employed, Banko andMoore found accuracies dropped from 96% to 77%.Banko and Moore?s observations spurred a newline of research that sought to improve performancein the face of full, noisy dictionaries; see Ravi and821Knight (2009) for an overview.
The highest accu-racy achieved to date under these assumptions is91.6% (Ravi et al2010).
However, as is oftennoted (including by the authors themselves), manypapers that work on learning taggers from tag dic-tionaries make unrealistic assumptions about the tagdictionaries they use as input (Toutanova and John-son, 2008; Ravi and Knight, 2009; Hasan and Ng,2009).
For example, tag dictionaries are typicallyconstructed with every token-tag pair in the data, in-cluding those that appear only in the test set.
Thismeans that the evaluation of these taggers does notmeasure how they perform on sentences that containunseen words or unseen word-tag pairs, a likely oc-currence in real use of a trained tagger.We show that it is possible to achieve good tag-ging accuracy using a noisy and incomplete tag dic-tionary that has no access to the tags of the raw andtest data and no access to the tag frequency infor-mation of the labeled training data from which thedictionary is drawn.
We build on Ravi et al (2010)model minimization approach, which reduces dic-tionary noise by greedily approximating the mini-mum set of tag bigrams needed to cover the raw dataand exploits that information as a constraint on theinitialization of the model before running EM.
Weextend their method in four distinct ways.1.
Enable the algorithm to be used with incompletedictionaries by exploiting the type-based infor-mation provided by the tag dictionary and rawtext to initialize EM, and by training a standardsupervised HMM on the output of EM.2.
Improve the greedy procedure to find a betterminimized set of tag-tag bigrams.3.
Modify the method to return only the set of bi-grams required to tag sentences instead of keep-ing all bigrams chosen by minimization.4.
Exploit the paths found during minimization as adirect initialization for EM.Together, these improvements make it possible touse model minimization in a realistic context, andobtain higher performance: on English, results gofrom 63.5% for a vanilla HMM to 82.1% for anHMM that uses strategies to deal with unknowns,then to 85.0% with Ravi and Knight?s minimizationand finally to 88.5% with our enhancements.2 Supervision for HMMsHidden Markov Models (HMMs) are well-knowngenerative probabilistic sequence models commonlyused for POS-tagging.
The probability of a tag se-quence given a word sequence is determined fromthe product of emission and transition probabilities:P (t|w) ?N?i=1P (wi|ti) ?
P (ti|ti?1)HMMs can be trained directly from labeled data bycalculating maximum likelihood estimates or fromincomplete data using Expectation Maximization(EM) (Dempster et al1977).
We use both strate-gies in this work: EM is used to estimate modelsthat can automatically label raw tokens, and then anew HMM is estimated from that auto-labeled data.2.1 Token-supervised trainingWe use a simple but effective smoothing regime toaccount for unknown words and unseen tag-tag tran-sitions.
For emissions:P (wi|ti) =C(ti, wi) + ?
(ti)Puni(wi)C(ti) + ?
(ti)where Puni(wi) is the unigram probability of wi,and ?
(ti) is a tag specific amount of mass forsmoothing.
We use one-count smoothing (Chen andGoodman, 1996), where ?
(ti) is based on the num-ber of words that occur with ti once:?
(ti) = |wi : C(ti, wi) = 1|Since open-class tags occur more frequently withwords that appear once, they will reserve more massfor unknown words than closed-class tags will.
Thetransition distributions are smoothed in a similarfashion:P (ti|ti?1) =C(ti?1, ti) + ?
(ti?1)Puni(ti)C(ti?1) + ?(ti?1)?
(ti?1) = |ti : C(ti?1, ti) = 1|This simple scheme is quite effective: an HMMtrained on the Penn Treebank sections 0-18 and eval-uated on sections 19-21 and smoothed in this wayobtains 96.5% accuracy.
We do not use gold stan-dard labels elsewhere for this paper, but do use thismodel on the output of type-supervised HMMs.8222.2 Type-supervised trainingWe are primarily interested in learning taggers fromtag dictionaries combined with unlabeled text.
As isstandard, we use EM to iteratively estimate the tran-sition and emission probability parameters to maxi-mize the likelihood of unlabeled data.
It is known,however, that EM has particular problems learning agood HMM for POS tagging (Johnson, 2007; Raviand Knight, 2009).
One reason is that EM gener-ally tries to learn probability distributions that arefairly uniform while POS tag frequencies are quiteskewed.
For example, ?a?
appears in the trainingdata with seven different tags, but 99.9% of ?a?
to-kens are determiners.
Thus, the accuracy of anythingapproaching a uniform distribution for ?a?
tags willsuffer greatly.
In the context of unsupervised POStagging models, modeling this distinction greatlyimproves results (Moon et al2010).
Here, we cansimply exploit the tag dictionary and raw data.An initial set of parameters for the transitionsand emissions must be supplied as the input to EM.Given just a tag dictionary, the simplest initializa-tion is to set alag transitions to be uniform, rang-ing over all tag continuations, while for emissions, auniform distribution over all words that occur withthe tag is assigned.
This may be appropriate when acomplete tag dictionary is available, including com-plete information for words that appear only in thetest data.
This is because there will never be any un-known words during model estimation or inference.Likewise, there will never be a situation where thetag dictionary rules out all possible tag transitionsbetween two adjacent tokens in training or testing.As a result, no smoothing is needed in this scenario.The problem with this is that estimating a modelbased on type-supervision requires raw text, and ifwe have an incomplete tag dictionary, some of thewords in that text will be missing from the tag dic-tionary.
In a Bayesian setting, priors provide massfor such tokens; models are estimated using eitherGibbs sampling or variational inference (Johnson,2007).
However, we use vanilla EM here; as a con-sequence, once a parameter is zero, it is always zero.We thus need to ensure that mass is reserved forwords outside the tag dictionary at the start of EM.
(For transitions, uniform distributions are sufficientsince the set of tags is closed.
)2.3 Emission probability initializationThe simplest way to initialize the emission distribu-tions is to assign a count of one to every entry in thetag dictionary, and one count for unknowns.
Then,during each iteration of EM, the expectation step isable to estimate new non-zero counts for all possibleemissions encountered in the raw corpus.
This basicstrategy allows one to train an HMM with EM us-ing only an incomplete tag dictionary and raw text.However, this basic approach for emission proba-bilities produces bad unknown-word probabilities.Specifically, if for each tag we simply assume onecount for each entry in the tag dictionary and onecount for unknowns and then normalize, the proba-bility of an unknown word having a specific tag isinversely correlated with the number of word typesassociated with the tag in the tag dictionary.
In otherwords, a tag that appears with a smaller number ofdistinct words will be seen by the HMM as being abetter candidate tag for an unknown word.
Unfor-tunately this is the opposite behavior we want sinceclosed-class tags like determiner and preposition arebad candidates for tagging novel words.For type-supervised training, we can do much bet-ter.
Note that C(w, t) comes in two varieties: wis either found in the tag dictionary (known wordtypes), or it is not (unknown word types).
We referto the later as td-unknown: these are words that oc-cur in the raw word sequence used for EM but whichdo not occur in the tag dictionary.
These are thusdifferent unknowns from words have not been ob-served in the dictionary or in the raw set but whichmay be encountered at test time.
Computing the fullC(w, t) is necessary since we want P (w|t) to coverknown and td-unknown words.
We must thus deter-mine both Cknown(w, t) and Cunktd(w, t).First, we focus on calculating Cknown(w, t).
If aword w appears C(w) times in the raw corpus, andis seen with |TD(w)| tags in the tag dictionary, thenassume for each t in TD(w):Cknown(w, t) = C(w) / |TD(w)|andCknown(w, t) = 0 for all other t. In other words,we split C(w), the count of w tokens in the corpus,evenly among each of w?s possible tags.
This pro-vides us with an estimate of the true C(w, t) by ap-proximating the portion of the counts of each word823type that may be associated with that tag.
Note thatwhile this will give us zeros for any words that don?tappear in the raw corpus, this is not a problem be-cause EM training is based only on that corpus.Second, we look at td-unknown word types: thosein the raw data that are not found in the tag dic-tionary.
Given the value P (unktd|t) for the like-lihood of an unknown word given a tag t, we cancompute estimated counts Cunktd(w, t) for a td-unknown word w usingCunktd(w, t) = C(w) ?
P (unktd|t)where C(w), again, comes from the raw corpus.This has the effect of spreading C(w), the count oftokens of that unknown word w, across all of thepossible tags, with each tag receiving a proportionof the total count as determined by P (unktd|t).The challenge, then, is to compute P (unktd|t).For this, we have two potential sources of knowl-edge, the tag dictionary and the raw token sequence,each telling us complementary information.First, the tag dictionary tells us about the opennessof a tag?the likelihood that an unseen word willhave that label?based on our previously-discussedintuition that we are more likely to see a new wordwith a tag that is known to be associated with manywords already.
Thus, we can estimate Ptd(unktd|t)by simply normalizing the |TD(t)| values:Ptd(unktd|t) =|TD(t)|2?t?
?Tags |TD(t?
)|2We exaggerate the differences between tags bysquaring the |TD(t)| terms to draw an even largerdistinction between open and closed class types.Unfortunately, if we calculate an estimated wordcount directly from this using Cunktd(w, t) =C(w) ?
Ptd(unktd|t), the Cunktd(w, t) values wouldbe taken without any regard to the overall like-lihood of tag t. Since Cknown(NN) is veryhigh, Cunktd(NN) will seem very low by compar-ison.
Likewise, since Cknown(RB) is much lower,Cunktd(RB) will seem very high by comparison.P (unktd|t) must account for the overall likeli-hood of t so that the Cunktd(w, t) values will bescaled appropriately according to the overall likeli-hood of t. For this, we use our second knowledgesource: the raw data.
Based on the Cknown(w, t)values as given above, the raw data tells us about theoverall expectation of a word having a particular tag.From this, we can estimate the tag distribution forknown words: Cknown(t) =?w?
?V Cknown(w?, t)and then normalize to get Pknown(t).Finally, we need to combine Ptd(unktd|t) andPknown(t) into a single P (unktd|t) that accountsfor both the openness of a tag and its overall preva-lence.
We would like this combination to use thehigh Pknown(NN) to boost P (unktd|NN) and thelow Pknown(RB) to dampen P (unktd|RB).
So, wecompute and normalize:P (unktd|t) ?|TD(t)|2?t?
?Tags |TD(t?)|2?
Pknown(t)2.4 Auto-supervised post-EM smoothingThe initialization accounting for td-unknown wordsgiven above allows EM to be run on the raw tokensequence, but it provides no probability for wordsthat are truly unseen (in either the tag dictionary orthe raw data).
Consequently, any novel words in thetest set will have zero emission probabilities, leadingto extremely low unknown-word accuracies.To overcome this problem, we perform a sim-ple post-processing step after EM, which we referto as auto-supervised training.
We take the HMMtrained by EM and use it to label the raw corpus.This gives us an automatically-labeled corpus thatcan be used for standard supervised training (with-out EM) to produce a new HMM.
The effect of thispost-processing step is to smooth the counts learnedfrom EM onto any new words encountered duringtesting.
This procedure significantly improves theability of the HMM to label unknown words.As a final note, it would of course be possible touse other models at this stage, such as a ConditionalRandom Field (Lafferty et al2001).3 Enhancing MIN-GREEDYAs was discussed above, one of the major prob-lems for type-supervised POS-tagger training withEM is a tag dictionary with low-frequency entriessuch as the word ?a?
being associated with the for-eign word tag when nearly all of its instances areas a determiner.
To avoid the need for manuallypruning the tag dictionary, Ravi and Knight (2009)824?b?
The boy sees a dog ?\b?
?b?2%%DT1&&DT1%%NN&&NN3V B&&BBFW?\b?Figure 1: MIN-GREEDY graph showing a state in thefirst phase.
Numbered, solid arrows: order of chosenbigrams; dotted: potential choices.?b?
The boy sees a dog ?\b?
?b?%%DT&&DT%%NN&&NNV B&&BBFWBB?\b?Figure 2: Start of the second MIN-GREEDY phase.proposed that low-probability tags might be auto-matically filtered from the tag dictionary through amodel minimization procedure applied to the rawtext and constrained by the full tag dictionary.
Raviet al2010) develop a faster approach for modelminimization using a greedy algorithm that they callMIN-GREEDY.
It is this algorithm that we extend.3.1 The original MIN-GREEDY algorithmThe MIN-GREEDY algorithm starts by initializing agraph with a vertex for each possible tag of each to-ken in the raw data.
The set of possible tags for eachtoken is the set of tags associated with that wordin the tag dictionary.Special sentence start and sen-tence end vertices are added to the graph for eachsentence to mark its beginning and end.
Unlike Raviet al2010), we allow for an incomplete tag dic-tionary, meaning that our scenario has the additionalcomplication that the tag set for some raw-corpus?b?
The boy sees a dog ?\b?
?b?%%DT&&DT%%NN&&NNV B&&BBFW?\b?Figure 3: Potential MIN-GREEDY conclusion.words will not be known.
For these words, the fullset of tags is used.
Note that this increases the ambi-guity and overall number of edges in the graph.The MIN-GREEDY algorithm works in threephases: Greedy Set Cover, Greedy Path Comple-tion, and Iterative Model-Fitting.
In the first twophases, the algorithm chooses tag bigrams that formthe edges of the graph.
The goal of these phases is toselect a set of edges that is sufficient to allow a paththrough every sentence in the raw corpus.
The al-gorithm greedily selects these edges in an attempt toquickly approximate the minimal set of tag bigramsneeded to accomplish this goal.
In the final phase,the algorithm runs several iterations of EM in orderto fit the bigram set to the raw data.In the first phase, Greedy Set Cover, the algorithmselects tag bigrams in an effort to cover all of theword tokens.
A word token is considered coveredif there is at least one tag bigram edge connectedto at least one of its vertices.
At each iteration, thealgorithm examines the entire graph, across all sen-tences, to find the tag bigram that, if added, wouldmaximize the number of newly covered words.Consider the graph in Figure 1.
Assume, for theexample, that this sentence comprises the entire rawcorpus.
At the start of the first phase, no tag bigramsare selected.
On the first iteration, the algorithmchooses the tag bigram DT?NN because this tagbigram describes two edges for a total of four wordsnewly covered: The, boy, a, and dog.
On the seconditeration, there are only three word tokens left un-covered: the start symbol, sees, and the end symbol.At this point, as the figure shows, there are five tagbigrams that would each result in covering one addi-825tional token.
Since there are no tag bigrams whosechoosing would result in covering more than one ad-ditional token, the algorithm randomly chooses oneof these five.
The algorithm iterates like this until allwords are covered, as in, for example, Figure 2.The second phase of the MIN-GREEDY algorithm,Greedy Path Completion, seeks to fill holes in thetag paths found in the graph.
A hole is a poten-tial edge that, if added, would connect two existingedges.
At each iteration, the algorithm finds the tagbigram that, if selected, would maximize the numberof holes that would be filled across all raw sentences.The example graph in Figure 2 shows a potentialstart of the second phase.
At this point, there arethree tag bigrams that each fill one hole if selected,and the algorithm randomly selects one.
Iterationcontinues until there is a complete tag path througheach sentence in the raw corpus.
One potential reso-lution for the example is given in Figure 3.Once a set of tag bigrams has been generated thatallows for a complete tag path through every sen-tence of the raw corpus, MIN-GREEDY begins itsfinal phase: Iterative Model-Fitting.
In this phase,the algorithm trains a succession of type-supervisedHMM models.
Each iteration trains an HMM andthen uses it to tag the raw corpus, the result of whichis used to prepare inputs for the next iteration.Iterative Model-Fitting begins with the minimizedset of bigrams returned from the second phase ofMIN-GREEDY.
This set is used as a hard constrainton the allowable tag bigrams during type-supervisedHMM training.
While EM is running, the only tagtransitions that are counted are those that fall into theminimized tag bigram set; all other transition countsare ignored.
Once an HMM has been trained, it isimmediately used to tag the raw corpus, producing aset of auto-labeled sentences.
For the second itera-tion of the phase, we extract a constrained tag dictio-nary from the auto-labeled corpus by simply takingevery word/tag pair appearing in the data.
This newtag dictionary is a subset of the original, full, tagdictionary, and hopefully has fewer low-frequencyentries that would cause problems for EM.We use this constrained tag dictionary to againperform type-supervised HMM training, but withoutany constraints on the allowable tag bigrams.
Thisproduces our third HMM.
Using this HMM, we can,again, tag the raw corpus, producing another set ofauto-labeled sentences.
We can then extract the setof tag bigrams appearing in this data to produce anew set of tag transition constraints, similar to whatwas returned by the second phase.
With this set oftag transition constraints, and the full tag dictionary,we can perform another round of type-supervisedHMM training, and repeat the entire process.The third MIN-GREEDY phase continues iterating,alternating between training an HMM using a con-strained set of tag transitions and training one usinga constrained tag dictionary.
The size of the set ofconstrained tag bigrams produced is tracked on eachiteration, and the algorithm is considered to haveconverged when this value changes by less than fivepercent.
The final result of the MIN-GREEDY algo-rithm is a trained HMM.The evaluation of the MIN-GREEDY algorithm, asdescribed in Ravi et al2010), was performed onlyfor scenarios with a complete tag dictionary (includ-ing all raw and test word types).
As such, no tech-niques were described for handling unknown words.Because we are interested in the more realistic sce-nario of an incomplete tag dictionary, we augmentthe original MIN-GREEDY setup with the smoothingtechniques described above.3.2 Improving tag bigram selectionOne of the major problems with the MIN-GREEDYalgorithm is that its heuristics for choosing the nexttag bigram frequently result in many-way ties.
In thefirst two phases of MIN-GREEDY, the greedy pro-cedure looks for the tag bigram that will have themost positive impact.
In the Greedy Set Cover phasethis means choosing the tag bigram that would coverthe most new tokens, and in the Greedy Path Com-pletion phase this means choosing the tag bigramthat would fill the most holes.
However, it is fre-quently the case that there are many distinct tag bi-grams that would cover the most new tokens or fillthe most holes, leaving the MIN-GREEDY algorithmwith no choice but to randomly select from theseoptions.
Since there are frequently cases of havingmany dozens of options, it is clear that some of thosechoices must be better than others, even though MIN-GREEDY does not make a distinction and considersthem all to be equally good choices.Consider the example in Figure 1 representing apossible state of the minimization graph.
To have826reached this stage, tag bigram DT?NN would havebeen chosen since it covered the highest number oftokens: four.
Additionally, ?b?
?DT and NN?
?\b?could have been chosen as the second and third tagbigrams since they tied for the most new tokens cov-ered: one.
For the state shown in this figure, thereis only one uncovered token, sees, but three tag bi-grams that cover it.
Since each of these tag bigramscovers exactly one new word, they are all consideredby MIN-GREEDY to be equally good choices as thenext tag bigram for inclusion, and the algorithm willchoose one at random.
However, it should be clearthat the VB?FW tag bigram is wrong while theother two would lead to a correct answer.
As such,we would like for the algorithm to avoid choosingVB?FW, and to pick one of the others.In order to push the algorithm into choosing theright tag bigrams in these otherwise ambiguous sit-uations, we have added an additional criterion to thebigram-choosing heuristic: after narrowing downthe set of tag bigrams to those that cover the mostnew tokens, we further narrow the choice of bigramsby minimizing the number of new word-type/tagpairs that would be added to the result.
Considerour example.
If we choose the tag bigram NN?VBor VB?DT, then exactly one new word-type/tagpair would be added to our result: sees/VB (sinceboy/NN and a/DT would already have been addedby the incorporation of previous selected tag bi-grams).
By contrast if we choose the tag bigramVB?FW then two new word-type/tag pairs wouldbe added: sees/VB and a/FW.Minimizing the number of new word/tag pairsadded by the algorithm has two main advantages.First, it keeps the selected bigrams focused on thesame vertices, which results in fewer holes that theGreedy Path Completion phase must deal with.
Sec-ondly, it keeps the selected bigrams focused on morecommon tags for each word type, such as a/DT, andkeeps it away from rare tags, such as a/FW.3.3 Only tag bigrams on minimization pathsAs was described above, the output of MIN-GREEDY?s second stage is a minimized set of tagbigrams which is used as a constraint on the firstiteration of the third stage, Iterative Model-Fitting.However, in order to determine when to stop addingnew bigrams during the first two phases, the MIN-GREEDY algorithm must try to find complete tagpaths through each sentence in the raw corpus, stop-ping once a tag path has been found for each one.While the algorithm is trying to select only the tagbigrams that are necessary for a complete tagging, ithappens frequently that bigrams are selected that arenot actually used on any tag path.Consider the example shown in Figure 3.
Thegraph has a complete path through the sentence, butalso contains an extraneous edge, VB?FW, that isnot used on the path.
Assuming that this tag bigramis not used on the tag path of any other sentence, itcan safely be removed from the resultant set to pro-duce a smaller set of tag bigrams, getting us evencloser to the minimized set that we desire.To find the set of tag bigrams excluding these ex-traneous edges, we modify the MIN-GREEDY algo-rithm.
During the first and second phases of the al-gorithm, we check all raw data sentences for a com-pleted path after each tag bigram is selected.
If acompleted path is found for a sentence, we store thatpath immediately.
Once a path is found for everysentence, we extract the set of bigrams used on thesepaths, and pass that set, instead of the full set of se-lected bigrams, to the third phase of the algorithm.Note that it is important that we store the com-pleted paths as soon as they are completed.
Sincesentences are completed at different stages, andmore tag bigrams are selected after some of thesesentences are complete, it is inevitable that somesentences will end up with multiple complete tagpaths by the end of the second phase.
However, weseek only the first such path.
Tag bigrams are se-lected in order of their impact, so bigrams selectedearlier are better and should be preferred.
Consid-ering again the example in Figure 3, based on thefrequency of the tags, it is likely that, given thepresence of other sentences in the raw corpus, thetag path including bigrams VB?DT and DT?NNwould be found before the one including VB?FWand FW?NN.
Since they are more frequent bi-grams, we would want to keep the first path evenif the second is completed at a later time.The result of this improvement is a smaller,cleaner minimized tag bigram set to be delivered tothe third phase of MIN-GREEDY.827Scenario Total Known Unk.0.
Random baseline (choose tag randomly from tag dictionary) 63.53 65.49 2.381.
HMM baseline (simple EM with tag dictionary and raw text) 69.20 71.42 0.272.
HMM baseline + auto-supervised training 82.33 83.67 40.463.
HMM baseline + auto-supervised training + emission initialization 82.05 83.27 44.314.
MIN-GREEDY (Ravi et al2010) with add-one smoothing 74.79 77.17 0.455.
MIN-GREEDY with add-one smoothing + auto-supervised 86.10 87.59 39.746.
MIN-GREEDY with add-one smoothing + auto-supervised + emission init 85.02 86.33 44.287.
6 + enhanced tag bigram choice heuristic 86.71 88.08 43.938.
6 + restrict tag bigrams to tag paths of minimization-tagged output 87.01 88.40 43.749.
6 + HMM initialization from minimization-tagged output 88.52 89.92 44.8010.
6 + 7 + 8 + 9 88.51 89.92 44.80Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary.
Known word typesare those appearing in the tag dictionary.3.4 EM initialization with minimization outputAs a final improvement to MIN-GREEDY, we tookthe set of completed tag paths returned from the sec-ond phase of the algorithm, as described in the pre-vious section, and used them as labeled data to ini-tialize an HMM for EM training.Since we modified MIN-GREEDY to produce a setof completed tag paths for sentences, we can takethis to be a complete set of labels for the raw cor-pus.
Furthermore, since we were careful about stor-ing paths as soon as they become completed by theminimization process, and the tag bigrams are cho-sen in order of frequency, there will be more high-frequency bigrams than low-frequency.
As a result,this labeling will contain good tag transitions andtoken labelings.
As such, the labeled data producedby the second phase provides useful information be-yond a simple set of sufficient bigrams: it containslegitimate frequency information that can be usedto initialize the HMM.
We, therefore, initialize anHMM directly from this data to start EM.4 Evaluation1English data.
We evaluate on the Penn Treebank(Marcus et al1993).
In all cases we use the first47,996 tokens of section 16 as our raw data, sections19?21 as our development set, and perform the finalevaluation on sections 22?24.1Source code, scripts, and data to reproduce the results pre-sented here can be found at github.com/dhgarrette/type-supervised-tagging-2012emnlpWe evaluate two differently sized tag dictionaries.The first is extracted directly from sections 00?15(751,059 tokens) and the second from sections 00?07 (379,908 tokens).
The former contains 39,087word types, 45,331 word/tag entries, a per-type am-biguity of 1.16 and yields a per-token ambiguity of2.21 on the raw corpus (treating unknown wordsas having all 45 possible tags).
The latter contains26,652 word types, 30,662 word/tag entries, a per-type ambiguity of 1.15 and yields a per-token ambi-guity of 2.03 on the raw corpus.
In both cases, everyword/tag pair found in the relevant sections was usedin the tag dictionary: no pruning was performed.Italian data.
As a second evaluation, we use theTUT corpus (Bosco et al2000).
To verify that ourapproach is language-independent without the needfor specific tuning, we executed our tests on the Ital-ian data without any trial runs, parameter modifica-tions, or other changes.
We divided the TUT data,taking the first half of each of the five sections as in-put to the tag dictionary, the next quarter as raw data,and the last quarter as test data.
All together, the tagdictionary was constructed from 41,000 tokens con-sisting of 7,814 word types, 8,370 word/tag pairs,per-type ambiguity of 1.07 and a per-token ambigu-ity of 1.41 on the raw data.
The raw data consisted of18,574 tokens and the test contained 18,763 tokens.Results We ran eleven experiments for each dataset with results shown in Tables 1 and 2.
All scoresare reported as the percentage of tokens for whichthe correct tag was assigned.
Accuracy is shown as828PTB (00-07) TUTScenario Total Known Unk.
Total Known Unk.0.
Random 64.98 68.04 2.81 62.81 76.10 1.581.
HMM basic 69.32 72.70 0.56 60.70 73.77 0.512.
HMM + auto-super 81.50 83.67 37.46 70.03 80.64 21.123.
HMM + auto-super + init 81.71 83.62 42.89 70.89 80.91 24.744.
MIN-GREEDY + add-1 68.86 72.20 0.92 53.96 65.49 0.845.
MIN-GREEDY + add-1 + auto-super 80.78 82.88 38.02 70.85 82.41 17.606.
MIN-GREEDY + add-1 + auto-super + init 80.92 82.80 42.64 71.52 81.56 25.287.
6 + enhanced bigram choice heuristic 86.69 88.83 43.07 71.48 81.57 24.988.
6 + restrict tag bigrams to tag paths 80.86 82.73 42.84 72.86 83.45 24.089.
6 + HMM init from minimization output 87.61 89.74 44.18 72.00 82.28 24.6510.
6 + 7 + 8 + 9 87.95 90.12 43.74 71.99 82.50 23.57Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary.
Known wordtypes are those appearing in the tag dictionary.
Scenario numbers correspond to Table 1.the Total (all word types), Known (word types foundin the tag dictionary), and Unknown (word types notfound in the tag dictionary).Experiments 1?3 evaluate our smoothing tech-niques applied directly to the task of type-supervisedHMM training with EM, without MIN-GREEDY.The basic HMM consistently beats the baseline ran-dom tagger, the auto-supervision technique makesan enormous improvement for both known and un-known words, and the the emission initializationyields a sizable improvement for unknown words.Experiments 4?6 evaluated our reimplementationof MIN-GREEDY.
We start with the most basic levelof smoothing needed to work in a type-supervisedscenario.
For the smaller PTB tag dictionary andthe TUT data, MIN-GREEDY actually has lower per-formance than the HMM alone.
This indicates thatif the tag dictionary has a low degree of ambigu-ity, then MIN-GREEDY can make the situation worse.However, with our smoothing techniques, we regainsimilar improvements as with the HMM.Finally we performed experiments evaluat-ing combinations of our improvements to MIN-GREEDY.
Scenarios 7?9 show each improvementtaken in turn.
Scenario 10 shows the results for us-ing all three improvements.
For the English data, thebest results are found when all the improvements areused.
When taken individually, the bigram choiceheuristic and HMM initialization from minimizationoutput each consistently outperform the improved-MIN-GREEDY baseline on English.
However, re-stricting the tag bigrams to that in the minimization-tagged output causes problems in the smaller PTBscenario, presumably falling to a local maximumlike MIN-GREEDY that the other improvements areable to help the algorithm avoid.Though the accuracy improvements are less thanfor English, the Italian results show that our MIN-GREEDY enhancements make an appreciable differ-ence for a language and dataset for which the ap-proaches considered were run sight unseen.Error analysis One of the primary goals of modelminimization is to automatically eliminate low-probability entries from the tag dictionary that mightconfuse the EM algorithm (Ravi et al2010).
In or-der to see how well our techniques are able to iden-tify and eliminate these unlikely word/tag pairs, weanalyzed the tagging errors from each experiment.In doing so, we discovered that the two of the mostproblematic words for the EM algorithm are ?a?
and?in?.
We ran further experiments explore what washappening with those words.
The results, using PTBsections 00?07 are shown in Table 3.In PTB sections 00-07 the word ?a?
appears 7630times and with 7 different tags.
This includes 7621occurrences with tag DT, 3 with tag SYM (symbol),and 1 time with LS (list item marker).
As such, wewould want the HMM to lean heavily toward tag DTwhen tagging the token ?a?.
Unfortunately, the raretags confuse the EM procedure and end up with dis-829model tokens tagged by scenariotok output 3 6 7 8 9 10a DT 32 4 4 4 2424 2425LS 1531 0 0 0 0 0SYM 731 2356 2305 2356 0 0in IN 12 15 2024 4 2042 2047FW 1922 1910 0 0 0 0RP 20 27 0 2037 0 0Table 3: Number of times, for the words ?a?
and?in?, the tagger trained by the particular scenario se-lected the given tag.
Experiments used PTB sections00-07 for the initial tag dictionary.
Scenario num-bers correspond to Table 1.proportionately high probabilities.
Our experimenttraining an HMM without minimization (scenario 3)resulted in 1531 ?a?
tokens being tagged LS, 731 asSYM, and only 32 tagged as DT.The situation is similar with the word ?in?, whichappears 6155 times with 5 different tags in the 8sections.
Of these, 6073 occurrences are taggedIN (preposition), 63 are RP (particle), and 1 is FW(foreign word).
Again, EM without minimizationis confused by the rare tokens, assigning FW 1922times and IN 12 times.The minimization procedure attempts to over-come this problem by removing unlikely tags fromthe tag dictionary automatically.
As is show in Table3, MIN-GREEDY without our enhancements is ableto reject the problematic LS as a tag for ?a?, butunable to do so for SYM, resulting in 2356 tokenstagged SYM and only 4 tagged DT.
Similarly, MIN-GREEDY is unable to reject FW as a tag for ?in?.Our enhancements to MIN-GREEDY improve thesituation.
More careful choosing of bigrams duringminimization results in the avoidance of LS and FW(but not SYM) for ?a?
as well as FW and RP for?in?.
Restricting the tag bigrams output from MIN-GREEDY to just those on tag paths avoids LS and FWfor ?a?
and FW for ?in?.
Finally, using the taggedsentences from MIN-GREEDY as noisy supervisionfor EM initialization eliminates all rare tags, as doesthe use of all three enhancements together.5 ConclusionOur results show it is possible to create accuratePOS-taggers using type-supervision with incom-plete tag dictionaries by extending the MIN-GREEDYalgorithm of Ravi et al2010).
The most usefulchange we made to the MIN-GREEDY procedure wasthe implementation of a better heuristic for pickingtag bigrams.
An intuitive and straightforward emis-sion initialization provides the necessary basis to runEM on a given raw token sequence.
Using EM out-put on this raw sequence as auto-labeled materialto a supervised HMM then proves highly effectivefor generalization to new texts containing previouslyunseen word types.Vaswani et al010) explore the use of minimumdescription length principles in a Bayesian model asa way of capturing model minimization, inspired bythe MIN-GREEDY algorithm.
The advantage there isthat only a single objective function needs to be opti-mized, rather than having initialization followed byan iterative back and forth with pruning of tag-tagpairs.
Our own next steps are to move in a similardirection to explore the possibilities for encoding theintuitions we developed for initialization and mini-mization as a single generative model.Goldberg et al2008) note that fixing noisy dic-tionaries by hand is actually quite feasible, and sug-gest that effort should focus on exploiting humanknowledge rather than just algorithmic improve-ments.
We agree; however, our ultimate motivationis to use this work to tackle bootstrapping from verysmall tag dictionaries or dictionaries obtained fromlinguists or resources other than a corpus, and fortag sets that are more ambiguous (e.g., supertaggingfor CCGbank (Hockenmaier and Steedman, 2007)).Such efforts require automatic expansion of tag dic-tionaries, which then need be constrained based onavailable raw token sequences using methods suchas those explored here.
In this respect, the some-what idiosyncratic noise in the corpus-derived dic-tionaries used here make a good test.AcknowledgementsWe thank Yoav Goldberg, Sujith Ravi, and the re-viewers for their feedback.
This work was supportedby the U.S. Department of Defense through the U.S.Army Research Office (grant number W911NF-10-1-0533) and via a National Defense Science and En-gineering Graduate Fellowship for the first author.830ReferencesMichele Banko and Robert C. Moore.
2004.
Part-of-speech tagging in context.
In Proceedings of COLING,pages 556?561, Geneva, Switzerland.Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo, ,and Leonardo Lesmo.
2000.
Building a treebank forItalian: a data-driven annotation schema.
In Proceed-ings of LREC.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of ACL, pages 310?318, SantaCruz, California, USA.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedpos induction: How far have we come?
In Proceed-ings of EMNLP.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of ACL-HLT, pages 600?609,Portland, Oregon, USA.Arthur P. Dempster, Nan M. Laird, and Donald.
B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the Royal StatisticalSociety: Series B (Statistical Methodology), 39:1?22.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging for twit-ter: Annotation, features, and experiments.
In Pro-ceedings of ACL-HLT, pages 42?47, Portland, Oregon,USA.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.EM can find pretty good HMM POS-taggers (whengiven a good start).
In Proceedings ACL, pages 746?754.Kazi Saidul Hasan and Vincent Ng.
2009.
Weakly super-vised part-of-speech tagging for morphologically-rich,resource-scarce languages.
In Proceedings of EACL,pages 363?371, Athens, Greece.Julia Hockenmaier and Mark Steedman.
2007.
Ccgbank:A corpus of ccg derivations and dependency structuresextracted from the penn treebank.
Computational Lin-guistics, 33(3):355?396.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proceedings EMNLP-CoNLL, pages296?305.Julian Kupiec.
1992.
Robust part-of-speech tagging us-ing a hidden markov model.
Computer Speech & Lan-guage, 6(3):225?242.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
InProceedings of ICML, pages 282?289.
Morgan Kauf-mann.Christopher D. Manning.
2011.
Part-of-speech taggingfrom 97% to 100%: Is it time for some linguistics?
InAlexander Gelbukh, editor, Proceedings of CICLing,volume 6608 of Lecture Notes in Computer Science,pages 171?189.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2):155?171.Taesun Moon, Katrin Erk, and Jason Baldridge.
2010.Crouching dirichlet, hidden markov model: Unsu-pervised POS tagging with context local tag genera-tion.
In Proceedings of EMNLP, pages 196?206, Cam-bridge, MA.Sujith Ravi and Kevin Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In Proceed-ings of ACL-AFNLP.Sujith Ravi, Ashish Vaswani, Kevin Knight, and DavidChiang.
2010.
Fast, greedy model minimization forunsupervised tagging.
In Proceedings of COLING,pages 940?948.Kristina Toutanova and Mark Johnson.
2008.
A bayesianlda-based model for semi-supervised part-of-speechtagging.
In Proceedings of NIPS.Ashish Vaswani, Adam Pauls, and David Chiang.
2010.Efficient optimization of an mdl-inspired objectivefunction for unsupervised part-of-speech tagging.
InProceedings of the ACL 2010 Conference Short Pa-pers, pages 209?214, Uppsala, Sweden.831
