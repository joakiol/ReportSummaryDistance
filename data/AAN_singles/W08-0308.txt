Proceedings of the Third Workshop on Statistical Machine Translation, pages 62?69,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsImproved Tree-to-string Transducer for Machine TranslationDing Liu and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractWe propose three enhancements to the tree-to-string (TTS) transducer for machine trans-lation: first-level expansion-based normaliza-tion for TTS templates, a syntactic align-ment framework integrating the insertion ofunaligned target words, and subtree-based n-gram model addressing the tree decomposi-tion probability.
Empirical results show thatthese methods improve the performance of aTTS transducer based on the standard BLEU-4 metric.
We also experiment with semanticlabels in a TTS transducer, and achieve im-provement over our baseline system.1 IntroductionSyntax-based statistical machine translation(SSMT) has achieved significant progress duringrecent years, with two threads developing simul-taneously: the synchronous parsing-based SSMT(Galley et al, 2006; May and Knight, 2007) andthe tree-to-string (TTS) transducer (Liu et al,2006; Huang et al, 2006).
Synchronous SSMThere denotes the systems which accept a sourcesentence as the input and generate the translationand the syntactic structure for both the source andthe translation simultaneously.
Such systems aresometimes also called TTS transducers, but in thispaper, TTS transducer refers to the system whichstarts with the syntax tree of a source sentence andrecursively transforms the tree to the target languagebased on TTS templates.In synchronous SSMT, TTS templates are usedsimilar to the context free grammar used in the stan-dard CYK parser, thus the syntax is part of the outputand can be thought of as a constraint on the transla-tion process.
In the TTS transducer, since the parsetree is given, syntax can be thought of as an addi-tional feature of the input to be used in the transla-tion.
The idea of synchronous SSMT can be tracedback to Wu (1997)?s Stochastic Inversion Transduc-tion Grammars.
A systematic method for extract-ing TTS templates from parallel corpora was pro-posed by Galley et al (2004), and later binarizedby Zhang et al (2006) for high efficiency and ac-curacy.
In the other track, the TTS transducer orig-inated from the tree transducer proposed by Rounds(1970) and Thatcher (1970) independently.
Graehland Knight (2004) generalized the tree transducerto the TTS transducer and introduced an EM al-gorithm to estimate the probability of TTS tem-plates based on a bilingual corpus with one sideparsed.
Liu et al (2006) and Huang et al (2006)then used the TTS transducer on the task of Chinese-to-English and English-to-Chinese translation, re-spectively, and achieved decent performance.Despite the progress SSMT has achieved, it isstill a developing field with many problems un-solved.
For example, the word alignment com-puted by GIZA++ and used as a basis to extractthe TTS templates in most SSMT systems has beenobserved to be a problem for SSMT (DeNero andKlein, 2007; May and Knight, 2007), due to thefact that the word-based alignment models are notaware of the syntactic structure of the sentences andcould produce many syntax-violating word align-ments.
Approaches have been proposed recently to-wards getting better word alignment and thus bet-ter TTS templates, such as encoding syntactic struc-ture information into the HMM-based word align-ment model DeNero and Klein (2007), and build-62ing a syntax-based word alignment model Mayand Knight (2007) with TTS templates.
Unfortu-nately, neither approach reports end-to-end MT per-formance based on the syntactic alignment.
DeN-ero and Klein (2007) focus on alignment and do notpresent MT results, while May and Knight (2007)takes the syntactic re-alignment as an input to an EMalgorithm where the unaligned target words are in-serted into the templates and minimum templates arecombined into bigger templates (Galley et al, 2006).Thus the improvement they reported is rather indi-rect, leading us to wonder how much improvementthe syntactic alignment model can directly bring to aSSMT system.
Some other issues of SSMT not fullyaddressed before are highlighted below:1.
Normalization of TTS templates.
Galley etal.
(2006) mentioned that with only the mini-mum templates extracted from GHKM (Galleyet al, 2004), normalizing the template proba-bility based on its tree pattern ?can become ex-tremely biased?, due to the fact that bigger tem-plates easily get high probabilities.
They in-stead use a joint model where the templates arenormalized based on the root of their tree pat-terns and show empirical results for that.
Thereis no systematic comparison of different nor-malization methods.2.
Decomposition model of a TTS transducer(or syntactic language model in synchronousSSMT).
There is no explicit modeling for thedecomposition of a syntax tree in the TTStransducer (or the probability of the syntactictree in a synchronous SSMT).
Most systemssimply use a uniform model (Liu et al, 2006;Huang et al, 2006) or implicitly consider itwith a joint model producing both syntax treesand the translations (Galley et al, 2006).3.
Use of semantics.
Using semantic features ina SSMT is a natural step along the way to-wards generating more refined models acrosslanguages.
The statistical approach to semanticrole labeling has been well studied (Xue andPalmer, 2004; Ward et al, 2004; Toutanova etal., 2005), but there is no work attempting touse such information in SSMT, to our limitedknowledge.This paper proposes novel methods towards solv-ing these problems.
Specifically, we compare threeways of normalizing the TTS templates based on thetree pattern, the root of the tree pattern, and the first-level expansion of the tree pattern respectively, inthe context of hard counting and EM estimation; wepresent a syntactic alignment framework integratingboth the template re-estimation and insertion of un-aligned target words; we use a subtree-based n-grammodel to address the decomposition of the syntaxtrees in TTS transducer (or the syntactic languagemodel for synchronous SSMT); we use a statisticalclassifier to label the semantic roles defined by Prop-Bank (Palmer et al, 2005) and try different ways ofusing the semantic features in a TTS transducer.We chose the TTS transducer instead of syn-chronous SSMT for two reasons.
First, the decodingalgorithm for the TTS transducer has lower compu-tational complexity, which makes it easier to inte-grate a complex decomposition model.
Second, theTTS Transducer can be easily integrated with se-mantic role features since the syntax tree is present,and it?s not clear how to do this in a synchronousSSMT system.
The remainder of the paper willfocus on introducing the improved TTS transducerand is organized as follows: Section 2 describes theimplementation of a basic TTS transducer; Section3 describes the components of the improved TTStransducer; Section 4 presents the empirical resultsand Section 5 gives the conclusion.2 A Basic Tree-to-string Transducer forMachine TranslationThe TTS transducer, as a generalization to the finitestate transducer, receives a tree structure as its inputand recursively applies TTS templates to generatethe target string.
For simplicity, usually only onestate is used in the TTS transducer, i.e., a TTS tem-plate will always lead to the same outcome wher-ever it is used.
A TTS template is composed of aleft-hand side (LHS) and a right-hand side (RHS),where LHS is a subtree pattern and RHS is a se-quence of the variables and translated words.
Thevariables in the RHS of a template correspond to thebottom level non-terminals in the LHS?s subtree pat-tern, and their relative order indicates the permuta-tion desired at the point where the template is ap-63SQAUX  NP1 RB  VP2  ?3Is            notNP1 ??
VP2  ?3Figure 1: A TTS Template ExampleSQAUX  NP  1R  BP   V2?3SQ 3AUX I?s NPn 31R ot ?
s BP ?
V?
s ?
?
NPn  ??
BP ?
V?NN?
t ??
?V?
?
?BRNot ?
?
IoI??
?
?3NP 3?
?
?
?
?
s 3NN ?
t ?
ss ?
?
?
?3BP BRNns ?
?
BRNn3BRN ?
IoI??
?
?
s ?
?
?
?3V Vs ?
?
???
??
??
?Figure 2: Derivation Exampleplied to translate one language to another.
The vari-ables are further transformed and the recursive pro-cess goes on until there are no variables left.
Theformal description of a TTS transducer is describedin Graehl and Knight (2004), and our baseline ap-proach follows the Extended Tree-to-String Trans-ducer defined in (Huang et al, 2006).
Figure 1 givesan example of the English-to-Chinese TTS template,which shows how to translate a skeleton YES/NOquestion from English to Chinese.
NP 1 and V P 2are the variables whose relative position in the trans-lation are determined by the template while their ac-tual translations are still unknown and dependent onthe subtrees rooted at them; and the English words Isand not are translated into the Chinese word MeiYouin the context of the template.
The superscripts at-tached on the variables are used to distinguish thenon-terminals with identical names (if there is any).Figure 2 shows the steps of transforming the Englishsentence ?Is the job not finished ??
to the corre-sponding Chinese.For a given derivation (decomposition) of a syn-tax tree, the translation probability is computed asthe product of the templates which generate boththe source syntax trees and the target translations.In theory, the translation model should sum overall possible derivations generating the target transla-tion, but in practice, usually only the best derivationis considered:Pr(S|T,D?)
=?t?D?Weight(t)Here, S denotes the target translation, T denotes thesource syntax tree, and D?
denotes the best deriva-tion of T .
The implementation of a TTS trans-ducer can be done either top down with memoiza-tion to the visited subtrees (Huang et al, 2006), orwith a bottom-up dynamic programming (DP) algo-rithm (Liu et al, 2006).
This paper uses the lat-ter approach, and the algorithm is sketched in Fig-ure 3.
For the baseline approach, only the translationmodel and n-gram model for the target language areused:S?
= argmaxSPr(T |S) = argmaxSPr(S)Pr(S|T )Since the n-gram model tends to favor short transla-tions, a penalty is added to the translation templateswith fewer RHS symbols than LHS leaf symbols:Penalty(t) = exp(|t.RHS| ?
|t.LHSLeaf |)where |t.RHS| denotes the number of symbols inthe RHS of t, and |t.LHSLeaf | denotes the num-ber of leaves in the LHS of t. The length penalty isanalogous to the length feature widely used in log-linear models for MT (Huang et al, 2006; Liu et al,2006; Och and Ney, 2004).
Here we distribute thepenalty into TTS templates for the convenience ofDP, so that we don?t have to generate the N -best listand do re-ranking.
To speed up the decoding, stan-dard beam search is used.In Figure 3, BinaryCombine denotes the target-size binarization (Huang et al, 2006) combination.The translation candidates of the template?s vari-ables, as well as its terminals, are combined pair-wise in the order they appear in the RHS of thetemplate.
fi denotes a combined translation, whoseprobability is equal to the product of the probabili-ties of the component translations, the probability ofthe rule, the n-gram probability of connecting thecomponent translations, and the length penalty of64Match(v, t): the descendant tree nodes of v, which match the variables in template tv.sk: the stack associated with tree node vIn(cj , fi): the translation candidate of cj which is chosen to combine fi??????????????????????????????????
?for all tree node v in bottom-up order dofor all template t applicable at v do{c1, c2, ..., cl}=Match(v, t);{f1, f2, ..., fm} = BinaryCombine(c1.sk, c2.sk, ..., cn.sk, t);for i=1:m doPr(fi) =?lj=1Pr(In(cj , fi)) ?
Weight(t)?
?
Lang(v, t, fi)?
?
Penalty(t)?
;Add (fi, P r(fi)) to v.sk;Prune v.sk;Figure 3: Decoding Algorithmthe template.
?, ?
and ?
are the weights of the lengthpenalty, the translation model, and the n-gram lan-guage model, respectively.
Each state in the DPchart denotes the best translation of a tree node witha certain prefix and suffix.
The length of the pre-fix and the suffix is equal to the length of the n-grammodel minus one.
Without the beam pruning, the de-coding algorithm runs in O(N4(n?1)RPQ), whereN is the vocabulary size of the target language, n isthe length of the n-gram model, R is the maximumnumber of templates applicable to one tree node, Pis the maximum number of variables in a template,and Q is the number of tree nodes in the syntax tree.The DP algorithm works for most systems in the pa-per, and only needs to be slightly modified to en-code the subtree-based n-gram model described inSection 3.3.3 Improved Tree-to-string Transducer forMachine Translation3.1 Normalization of TTS TemplatesGiven the story that translations are generated basedon the source syntax trees, the weight of the templateis computed as the probability of the target stringsgiven the source subtree:Weight(t) =#(t)#(t?
: LHS(t?)
= LHS(t))Such normalization, denoted here as TREE, is usedin most tree-to-string template-based MT systems(Liu et al, 2007; Liu et al, 2006; Huang et al,2006).
Galley et al (2006) proposed an alterationin synchronous SSMT which addresses the proba-bility of both the source subtree and the target stringgiven the root of the source subtree:Weight(t) =#(t)#(t?
: root(t?)
= root(t))This method is denoted as ROOT.
Here, we proposeanother modification:Weight(t) =#(t)#(t?
: cfg(t?)
= cfg(t))(1)cfg in Equation 1 denotes the first level expansionof the source subtree and the method is denoted asCFG.
CFG can be thought of as generating both thesource subtree and the target string given the firstlevel expansion of the source subtree.
TREE focuseson the conditional probability of the target stringgiven the source subtree, ROOT focuses on the jointprobability of both the source subtree and the targetstring, while CFG, as something of a compromisebetween TREE and ROOT, hopefully can achieve acombined effect of both of them.
Compared withTREE, CFG favors the one-level context-free gram-mar like templates and gives penalty to the templatesbigger (in terms of the depth of the source subtree)than that.
It makes sense considering that the bigtemplates, due to their sparseness in the corpus, areoften assigned unduly large probabilities by TREE.3.2 Syntactic Word AlignmentThe idea of building a syntax-based word alignmentmodel has been explored byMay and Knight (2007),with an algorithm working from the root tree nodedown to the leaves, recursively replacing the vari-ables in the matched tree-to-string templates untilthere are no such variables left.
The TTS tem-plates they use are initially gathered using GHKM651.
Run GIZA++ to get the initial word alignment, useGHKM to gather translation templates, and com-pute the initial probability as their normalized fre-quency.2.
Collect all the one-level subtrees in the training cor-pus containing only non-terminals and create TTStemplates addressing all the permutations of thesubtrees?
leaves if its spanning factor is not greaterthan four, or only the monotonic translation tem-plate if its spanning factor is greater than four.
Col-lect all the terminal rules in the form of A ?
Bwhere A is one source word, B is the consecutivetarget word sequence up to three words long, andA, B occurs in some sentence pairs.
These extratemplates are assigned a small probability 10?6.3.
Run the EM algorithm described in (Graehl andKnight, 2004) with templates obtained in step 1 andstep 2 to re-estimate their probabilities.4.
Use the templates from step 3 to compute the viterbiword alignment.5.
The templates not occurring in the viterbi deriva-tion are ignored and the probability of the remain-ing ones are re-normalized based on their frequencyin the viterbi derivation.Figure 4: Steps generating the refined TTS templates(Galley et al, 2004) with the word alignment com-puted by GIZA++ and re-estimated using EM, ig-noring the alignment from Giza++.
The refinedword alignment is then fed to the expanded GHKM(Galley et al, 2006), where the TTS templates willbe combined with the unaligned target words andre-estimated in another EM framework.
The syn-tactic alignment proposed here shares the essence ofMay and Knight?s approach, but combines the re-estimation of the TTS templates and insertion of theunaligned target words into a single EM framework.The process is described in Figure 4.
The inser-tion of the unaligned target words is done implicitlyas we include the extra terminal templates in Fig-ure 4, and the extra non-terminal templates ensurethat we can get a complete derivation forest in theEM training.
The last viterbi alignment step mayseem unnecessary given that we already have theEM-estimated templates, but in experiments we findthat it produces better result by cutting off the noisy(usually very big) templates resulting from the pooralignments of GIZA++.3.3 Tree Decomposition ModelA deficiency of the translation model for tree-to-string transducer is that it cannot fully addressthe decomposition probability of the source syntaxtrees.
Though we can say that ROOT/CFG implic-itly includes the decomposition model, a more di-rect and explicit modeling of the decomposition isstill desired.
Here we propose a novel n-gram-likemodel to solve this problem.
The probability of adecomposition (derivation) of a syntax tree is com-puted as the product of the n-gram probability ofthe decomposed subtrees conditioned on their ascen-dant subtrees.
The formal description of the modelis in Equation 2, whereD denotes the derivation andPT (st) denotes the direct parent subtree of st.Pr(D|T ) =?subtreesst?DPr(st|PT (st), PT (PT (st)), ...)(2)Now, with the decomposition model added in, theprobability of the target string given the source syn-tax tree is computed as:Pr(S|T ) = Pr(D?|T )?
Pr(S|T,D?
)To encode this n-gram probability of the subtreesin the decoding process, we need to expand thestate space of the dynamic programming algorithmin Figure 3, so that each state represents not onlythe prefix/suffix of the partial translation, but alsothe decomposition history of a tree node.
For ex-ample, with a bigram tree model, the states shouldinclude the different subtrees in the LHS of the tem-plates used to translate a tree node.
With bigger n-grams, more complex history information should beencoded in the states, and this leads to higher com-putational complexity.
In this paper, we only con-sider the tree n-gram up to size 2.
It is not practi-cal to search the full state space; instead, we mod-ify the beam search algorithm in Figure 3 to encodethe decomposition history information.
The mod-ified algorithm for the tree bigram creates a stackfor each tree pattern occurring in the templates ap-plicable to a tree node.
This ensures that for eachtree node, the decompositions headed with differ-ent subtrees have equal number of translation can-didates surviving to the upper phase.
The function66SQAUXXS NP1URBV21URU?
?V?3I sRnoBt V?
n?U?
VB?
?
?
RU3t V?
n?U?
VB ?
P ?
Q?
VR?
Q2V?
s2VR?U?
?
oU?
V3?
s2V??
RU?
B?U?
Qs?
3?
2V?2V?
s?
nsBQ?
s?
3?
s2V??
?
?
?
Pt V?
n?U?
VB?
sR?
U?QAV?
o?
?
?
RVV?
Q?
RU?Figure 5: Flow graph of the system with all componentsintegratedBinaryCombine is almost the same as in Figure 3,except that the translation candidates (states) of eachtree node are grouped according to their associatedsubtrees.
The bigram probabilities of the subtreescan be easily computed with the viterbi derivation inlast subsection.
Also, a weight should be assignedto this component.
This tree n-gram model can beeasily adapted and used in synchronous SSMT sys-tems such as May and Knight (2007), Galley et al(2006).
The flow graph of the final system with allthe components integrated is shown in Figure 5.3.4 Use of Semantic RolesStatistical approaches to MT have gone throughword-based systems, phrase-based systems, andsyntax-based systems.
The next generation wouldseem to be semantic-based systems.
We use Prop-Bank (Palmer et al, 2005) as the semantic driver inour TTS transducer because it is built upon the samecorpus (the Penn Treebank) used to train the statisti-cal parser, and its shallow semantic roles are moreeasily integrated into a TTS transducer.
A Max-Entropy classifier, with features following Xue andPalmer (2004) andWard et al (2004), is used to gen-erate the semantic roles for each verb in the syntaxtrees.
We then replace the syntactic labels with thesemantic roles so that we have more general tree la-bels, or combine the semantic roles with the syntac-tic labels to generate more refined tree node labels.Though semantic roles are associated with the verbs,it is not feasible to differentiate the roles of differentNP VP VP NP(S NP-agent VP) 0.983 0.017(S NP-patient VP) 0.857 0.143Table 1: The TREE-based weights of the skeleton tem-plates with NP in different rolesverbs due to the data sparseness problem.
If sometree nodes are labeled different roles for differentverbs, those semantic roles will be ignored.A simple example demonstrating the need for se-mantics in the TTS transducer is that in English-Chinese translation, the NP VP skeleton phrase ismore likely to be inverted when NP is in a patientrole than when it is in an agent role.
Table 1 showsthe TREE-based weights of the 4 translation tem-plates, computed based on our training corpus.
Thisshows that the difference caused by the roles of NPis significant.4 ExperimentWe used 74,597 pairs of English and Chinese sen-tences in the FBIS data set as our experimentaldata, which are further divided into 500 test sen-tence pairs, 500 development sentence pairs and73597 training sentence pairs.
The test set and de-velopment set are selected as those sentences hav-ing fewer than 25 words on the Chinese side.
Thetranslation is from English to Chinese, and Char-niak (2000)?s parser, trained on the Penn Treebank,is used to generate the syntax trees for the Englishside.
The weights of the MT components are op-timized based on the development set using a grid-based line search.
The Chinese sentence from the se-lected pair is used as the single reference to tune andevaluate the MT system with word-based BLEU-4(Papineni et al, 2002).
Huang et al (2006) usedcharacter-based BLEU as a way of normalizing in-consistent Chinese word segmentation, but we avoidthis problem as the training, development, and testdata are from the same source.4.1 Syntax-Based SystemThe decoding algorithm described in Figure 3 isused with the different normalization methods de-scribed in Section 3.1 and the results are summa-rized in Table 2.
The TTS templates are extractedusing GHKM based on the many-to-one alignment67Baseline Syntactic Alignment Subtree bigramdev test dev test dev testTREE 12.29 8.90 13.25 9.65 14.84 10.61ROOT 12.41 9.66 13.72 10.16 14.24 10.66CFG 13.27 9.69 14.32 10.29 15.30 10.99PHARAOH 9.04 7.84Table 2: BLEU-4 scores of various systems with the syntactic alignment and subtree bigram improvements addedincrementally.from Chinese to English obtained from GIZA++.We have tried using alignment in the reverse direc-tion and the union of both directions, but neitherof them is better than the Chinese-to-English align-ment.
The reason, based on the empirical result,is simply that the Chinese-to-English alignmentslead to the maximum number of templates usingGHKM.
A modified Kneser-Ney bigram model ofthe Chinese sentence is trained using SRILM (Stol-cke, 2002) using the training set.
For comparison,results for Pharaoh (Koehn, 2004), trained and tunedunder the same condition, are also shown in Table 2.The phrases used in Pharaoh are extracted as the pairof longest continuous spans in English and Chinesebased on the union of the alignments in both direc-tion.
We tried using alignments of different direc-tions with Pharaoh, and find that the union givesthe maximum number of phrase pairs and the bestBLEU scores.
The results show that the TTS trans-ducers all outperform Pharaoh, and among them, theone with CFG normalization works better than theother two.We tried the three normalization methods in thesyntactic alignment process in Figure 4, and foundthat the initialization (step 1) and viterbi alignment(step 3 and 4) based on the least biased modelROOT gave the best performance.
Table 2 showsthe results with the final template probability re-normalized (step 5) using TREE, ROOT and CFGrespectively.
We can see that the syntactic align-ment brings a reasonable improvement for the TTStransducer no matter what normalization method isused.
To test the effect of the subtree-based n-gram model, SRILM is used to compute a modi-fied Kneser-Ney bigram model for the subtree pat-terns used in the viterbi alignment.
The last 3 linesin Table 2 show the improved results by further in-corporating the subtree-based bigram model.
Wecan see that the difference of the three normaliza-tion methods is lessened and TREE, the weakest nor-malization in terms of addressing the decompositionprobability, gets the biggest improvement with thesubtree-based bigram model added in.4.2 Semantic-Based SystemFollowing the standard division, our max-entropybased SRL classifier is trained and tuned using sec-tions 2-21 and section 24 of PropBank, respectively.The F-score we achieved on section 23 is 88.70%.We repeated the experiments in last section withthe semantic labels generated by the SRL classi-fier.
Table 3 shows the results, comparing the non-semantic-based systems with similar systems us-ing the refined and general semantic labels, respec-tively.
Unfortunately, semantic based systems donot always outperform the syntactic based systems.We can see that for the baseline systems based onTREE and ROOT, semantic labels improve the re-sults, while for the other systems, they are not re-ally better than the syntactic labels.
Our approachto semantic roles is preliminary; possible improve-ments include associating role labels with verbs andbacking off to the syntactic-label based models fromsemantic-label based TTS templates.
In light of ourresults, we are optimistic that more sophisticateduse of semantic features can further improve a TTStransducer?s performance.5 ConclusionThis paper first proposes three enhancements to theTTS transducer: first-level expansion-based normal-ization for TTS templates, a syntactic alignmentframework integrating the insertion of unaligned tar-get words, and a subtree-based n-gram model ad-dressing the tree decomposition probability.
The ex-periments show that the first-level expansion-based68No Semantic Labels Refined Labels General LabelsSyntactic Subtree Syntactic Subtree Syntactic SubtreeBaseline Alignment Bigram Baseline Alignment Bigram Baseline Alignment BigramTREE 8.90 9.65 10.61 9.40 10.25 10.42 9.40 10.02 10.47ROOT 9.66 10.16 10.66 9.89 10.32 10.43 9.82 10.17 10.42CFG 9.69 10.29 10.99 9.66 10.16 10.33 9.58 10.25 10.59Table 3: BLEU-4 scores of semantic-based systems on test data.
As in Table 2, the syntactic alignment and subtreebigram improvements are added incrementally within each condition.normalization for TTS templates is better than theroot-based one and the tree-based one; the syntacticalignment framework and the n-gram based tree de-composition model both improve a TTS transducer?sperformance.
Our experiments using PropBank se-mantic roles in the TTS transducer show that the ap-proach has potential, improving on our baseline sys-tem.
However, adding semantic roles does not im-prove our best TTS system.ReferencesEugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In The Proceedings of the North AmericanChapter of the Association for Computational Linguis-tics, pages 132?139.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In Pro-ceedings of ACL-07, pages 17?24.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of NAACL-04.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING/ACL-06, pages 961?968, July.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In Proceedings of NAACL-04.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the 7th BiennialConference of the Association for Machine Translationin the Americas (AMTA), Boston, MA.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In The Sixth Conference of the Association forMachine Translation in the Americas, pages 115?124.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of COLING/ACL-06, Sydney,Australia, July.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007.Forest-to-string statistical translation rules.
In Pro-ceedings of ACL-07, Prague.J.
May and K. Knight.
2007.
Syntactic re-alignmentmodels for machine translation.
In Proceedings ofEMNLP.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4).Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proceedings of ACL-02.William C. Rounds.
1970.
Mappings and grammars ontrees.
Mathematical Systems Theory, 4(3):257?287.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In International Conference on Spo-ken Language Processing, volume 2, pages 901?904.J.
W. Thatcher.
1970.
Generalized2 sequential machinemaps.
J. Comput.
System Sci., 4:339?367.Kristina Toutanova, Aria Haghighi, and ChristopherManning.
2005.
Joint learning improves semantic rolelabeling.
In Proceedings of ACL-05, pages 589?596.Wayne Ward, Kadri Hacioglu, James Martin, , and DanJurafsky.
2004.
Shallow semantic parsing using sup-port vector machines.
In Proceedings of EMNLP.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedings ofEMNLP.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of NAACL-06, pages 256?263.69
