Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 79?83,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsEnglish-to-Hindi system description for WMT 2014:Deep Source-Context Features for MosesMarta R. Costa-juss`a1, Parth Gupta2, Rafael E. Banchs3and Paolo Rosso21Centro de Investigaci?on en Computaci?on, Instituto Polit?ecnico Nacional, Mexico2NLE Lab, PRHLT Research Center, Universitat Polit`ecnica de Val`encia3Human Language Technology, Institute for Infocomm Research, Singapore1marta@nlp.cic.ipn.mx,2{pgupta,prosso}@dsic.upv.es,3rembanchs@i2r.a-star.edu.sgAbstractThis paper describes the IPN-UPV partici-pation on the English-to-Hindi translationtask from WMT 2014 International Evalu-ation Campaign.
The system presented isbased on Moses and enhanced with deeplearning by means of a source-context fea-ture function.
This feature depends on theinput sentence to translate, which makesit more challenging to adapt it into theMoses framework.
This work reports theexperimental details of the system puttingspecial emphasis on: how the feature func-tion is integrated in Moses and how thedeep learning representations are trainedand used.1 IntroductionThis paper describes the joint participation of theInstituto Polit?ecnico Nacional (IPN) and the Uni-versitat Polit`ecnica de Valencia (UPV) in cooper-ation with Institute for Infocomm Research (I2R)on the 9th Workshop on Statistical Machine Trans-lation (WMT 2014).
In particular, our participa-tion was in the English-to-Hindi translation task.Our baseline system is an standard phrase-based SMT system built with Moses (Koehn et al.,2007).
Starting from this system we propose to in-troduce a source-context feature function inspiredby previous works (R. Costa-juss`a and Banchs,2011; Banchs and Costa-juss`a, 2011).
The mainnovelty of this work is that the source-context fea-ture is computed in a new deep representation.The rest of the paper is organized as follows.Section 2 presents the motivation of this seman-tic feature and the description of how the sourcecontext feature function is added to Moses.
Sec-tion 3 explains how both the latent semantic in-dexing and deep representation of sentences areused to better compute similarities among sourcecontexts.
Section 4 details the WMT experimentalframework and results, which proves the relevanceof the technique proposed.
Finally, section 5 re-ports the main conclusions of this system descrip-tion paper.2 Integration of a deep source-contextfeature function in MosesThis section reports the motivation and descrip-tion of the source-context feature function, to-gether with the explanation of how it is integratedin Moses.2.1 Motivation and descriptionSource context information in the phrase-basedsystem is limited to the length of the translationunits (phrases).
Also, all training sentences con-tribute equally to the final translation.We propose a source-context feature func-tion which measures the similarity betweenthe input sentence and all training sen-tences.
In this way, the translation unitshould be extended from source|||target tosource|||target|||trainingsentence, with thetrainingsentence the sentence from whichthe source and target phrases were extracted.The measured similarity is used to favour thosetranslation units that have been extracted fromtraining sentences that are similar to the currentsentence to be translated and to penalize thosetranslation units that have been extracted fromunrelated or dissimilar training sentences asshown in Figure 2.1.In the proposed feature, sentence similarity ismeasured by means of the cosine distance in areduced dimension vector-space model, which isconstructed either by means of standard latent se-mantic analysis or using deep representation as de-cribed in section 3.79S1: we could not book the room in timeT1: hm smy m\ EVkV aArE?fta nhF\ kr sk\S2: I want to write the book in timeT2: m{\ smy m\ EktaAb ElKnA cAhtaA hInput: i am reading a nice bookbook : EktaAbbook : aArE?fta krnAS2S1InputFigure 1: Illustration of the method2.2 Integration in MosesAs defined in the section above and, previously,in (R. Costa-juss`a and Banchs, 2011; Banchsand Costa-juss`a, 2011), the value of the proposedsource context similarity feature depends on eachindividual input sentence to be translated by thesystem.
We are computing the similarity betweenthe source input sentence and all the source train-ing sentences.This definition implies the feature function de-pends on the input sentence to be translated.
Toimplement this requirement, we followed our pre-vious implementation of an off-line version of theproposed methodology, which, although very in-efficient in the practice, allows us to evaluate theimpact of the source-context feature on a state-of-the-art phrase-based translation system.
This prac-tical implementation follows the next procedure:1.
Two sentence similarity matrices are com-puted: one between sentences in the develop-ment and training sets, and the other betweensentences in the test and training datasets.2.
Each matrix entry mijshould contain thesimilarity score between the ithsentence inthe training set and the jthsentence in the de-velopment (or test) set.3.
For each sentence s in the test and develop-ment sets, a phrase pair list LSof all poten-tial phrases that can be used during decodingis extracted from the aligned training set.4.
The corresponding source-context similarityvalues are assigned to each phrase in lists LSaccording to values in the corresponding sim-ilarity matrices.5.
Each phrase list LSis collapsed into a phrasetable TSby removing repetitions (when re-moving repeated entries in the list, the largestvalue of the source-context similarity featureis retained).6.
Each phrase table is completed by addingstandard feature values (which are computedin the standard manner).7.
Moses is used on a sentence-per-sentence ba-sis, using a different translation table for eachdevelopment (or test) sentence.3 Representation of SentencesWe represent the sentences of the source languagein the latent space by means of linear and non-linear dimensionality reduction techniques.
Suchmodels can be seen as topic models where the low-dimensional embedding of the sentences representconditional latent topics.3.1 Deep AutoencodersThe non-linear dimensionality reduction tech-nique we employ is based on the concept of deeplearning, specifically deep autoencoders.
Autoen-coders are three-layer networks (input layer, hid-den layer and output layer) which try to learn anidentity function.
In the neural network represen-tation of autoencoder (Rumelhart et al., 1986), thevisible layer corresponds to the input layer andhidden layer corresponds to the latent features.The autoencoder tries to learn an abstract repre-sentation of the data in the hidden layer in sucha way that minimizes reconstruction error.
Whenthe dimension of the hidden layer is sufficientlysmall, autoencoder is able to generalise and derivepowerful low-dimensional representation of data.We consider bag-of-words representation of textsentences where the visible layer is binary featurevector (v) where vicorresponds to the presenceor absence of ithword.
We use binary restrictedBoltzmann machines to construct an autoencoderas shown in (Hinton et al., 2006).
Latent repre-sentation of the input sentence can be obtained asshown below:p(h|v) = ?
(W ?
v + b) (1)where W is the symmetric weight matrix betweenvisible and hidden layer and b is hidden layerbias vector and ?
(x) is sigmoid logistic function1/(1 + exp(?x)).80Autoencoders with single hidden layer do nothave any advantage over linear methods likePCA (Bourlard and Kamp, 1988), hence weconsider deep autoencoder by stacking multipleRBMs on top of each other (Hinton and Salakhut-dinov, 2006).
The autoencoders have always beendifficult to train through back-propagation untilgreedy layerwise pre-training was found (Hintonand Salakhutdinov, 2006; Hinton et al., 2006; Ben-gio et al., 2006).
The pre-training initialises thenetwork parameters in such a way that fine-tuningthem through back-propagation becomes very ef-fective and efficient (Erhan et al., 2010).3.2 Latent Semantic IndexingLinear dimensionality reduction technique, latentsemantic indexing (LSI) is used to represent sen-tences in abstract space (Deerwester et al., 1990).The term-sentence matrix (X) is created where xijdenotes the occurrence of ithterm in jthsentence.Matrix X is factorized using singular value decom-position (SVD) method to obtain top m principlecomponents and the sentences are represented inthis m dimensional latent space.4 ExperimentsThis section describes the experiments carried outin the context of WMT 2014.
For English-Hindithe parallel training data was collected by CharlesUniversity and consisted of 3.6M English wordsand 3.97M Hindi words.
There was a monolingualcorpus for Hindi comming from different sourceswhich consisted of 790.8M Hindi words.
In ad-dition, there was a development corpus of newsdata translated specifically for the task which con-sisted of 10.3m English words and 10.1m Hindiwords.
For internal experimentation we built atest set extracted from the training set.
We se-lected randomly 429 sentences from the trainingcorpus which appeared only once, removed themfrom training and used them as internal test set.Monolingual Hindi corpus was used to build alarger language model.
The language model wascomputed doing an interpolation of the languagemodel trained on the Hindi part of the bilingualcorpus (3.97M words) and the language modeltrained on the monolingual Hindi corpus (790.8Mwords).
Interpolation was optimised in the de-velopment set provided by the organizers.
Bothlanguage models interpolated were 5-grams usingKneser-Ney smoothing.The preprocessing of the corpus was done withthe standard tools from Moses.
English was low-ercased and tokenized.
Hindi was tokenized withthe simple tokenizer provided by the organizers.We cleaned the corpus using standard parameters(i.e.
we keep sentences between 1 and 80 wordsof length).For training, we used the default Moses op-tions, which include: the grow-diag-final andword alignment symmetrization, the lexicalizedreordering, relative frequencies (conditional andposterior probabilities) with phrase discounting,lexical weights and phrase bonus for the trans-lation model (with phrases up to length 10), alanguage model (see details below) and a wordbonus model.
Optimisation was done using theMERT algorithm available in Moses.
Optimisa-tion is slow because of the way integration of thefeature function is done that it requires one phrasetable for each input sentence.During translation, we dropped unknown wordsand used the option of minimum bayes risk de-coding.
Postprocessing consisted in de-tokenizingHindi using the standard detokenizer of Moses(the English version).4.1 Autoencoder trainingThe architecture of autoencoder we consider wasn-500-128-500-n where n is the vocabulary size.The training sentences were stemmed, stopwordswere removed and also the terms with sentencesfrequency1less than 20 were also removed.
Thisresulted in vocabulary size n=7299.The RBMs were pretrained using ContrastiveDivergence (CD) with step size 1 (Hinton, 2002).After pretraining, the RBMs were stacked on topof each other and unrolled to create deep autoen-coder (Hinton and Salakhutdinov, 2006).
Duringthe fine-tuning stage, we backpropagated the re-construction error to update network parameters.The size of mini-batches during pretraining andfine-tuning were 25 and 100 respectively.
Weightdecay was used to prevent overfitting.
Addition-ally, in order to encourage sparsity in the hid-den units, Kullback-Leibler sparsity regularizationwas used.
We used GPU2based implementation ofautoencoder to train the models which took around4.5 hours for full training.1total number of training sentences in which the term ap-pears2NVIDIA GeForce GTX Titan with Memory 5 GiB and2688 CUDA cores814.2 ResultsTable 1 shows the improvements in terms ofBLEU of adding deep context over the baselinesystem for English-to-Hindi (En2Hi) over devel-opment and test sets.
Adding source-context infor-mation using deep learning outperforms the latentsemantic analysis methodology.En2HiDev Testbaseline 9.42 14.99+lsi 9.83 15.12+deep context 10.40?15.43?Table 1: BLEU scores for En2Hi translation task..?depicts statistical significance (p-value<0.05).Our source-context feature function may bemore discriminative in a task like English-to-Hindiwhere the target language has a larger vocabularythan the source one.Table 2 shows an example of how the translationis improving in terms of lexical semantics which isthe goal of the methodology presented in the pa-per.
The most frequent sense of word cry is ronA,which literally means ?to cry?
while the examplein Table 2 refers to the sense of cry as cFK, whichmeans to scream.
Our method could identify thecontext and hence the source context feature (scf )of the unit cry|||cFK is higher than for the unitscf (cry|||ronA) as shown in Table 3 and for thisparticular input sentence.5 ConclusionThis paper reports the IPN-UPV participation inthe WMT 2014 Evaluation Campaign.
The systemis Moses-based with an additional feature functionbased on deep learning.
This feature function in-troduces source-context information in the stan-dard Moses system by adding the information ofhow similar is the input sentence to the differenttraining sentences.
Significant improvements overSystem TranslationSource soft cry from the depthBaseline ghrAiyo\ s m lAym ron lgta+deep context ghrAiyo\ s m lAym cFKReference ghrAiyo\ s koml cFKTable 2: Manual analysis of a translation output.cp pp scfcry|||ronA 0.23 0.06 0.85cry|||cFK 0.15 0.04 0.90Table 3: Probability values (conditional, cp, andposterior, pp, as standard features in a phrase-based system) for the word cry and two Hinditranslations.the baseline system are reported in the task fromEnglish to Hindi.As further work, we will implement our featurefunction in Moses using suffix arrays in order tomake it more efficient.AcknowledgementsThis work has been supported in part bySpanish Ministerio de Econom?
?a y Compet-itividad, contract TEC2012-38939-C03-02 aswell as from the European Regional Develop-ment Fund (ERDF/FEDER).
The work of thesecond and fourth authors is also supportedby WIQ-EI (IRSES grant n. 269180) andDIANA-APPLICATIONS (TIN2012-38603-C02-01) project and VLC/CAMPUS Microcluster onMultimodal Interaction in Intelligent Systems.ReferencesRafael E. Banchs and Marta R. Costa-juss`a.
2011.
Asemantic feature for statistical machine translation.In Proceedings of the Fifth Workshop on Syntax,Semantics and Structure in Statistical Translation,SSST-5, pages 126?134.Yoshua Bengio, Pascal Lamblin, Dan Popovici, andHugo Larochelle.
2006.
Greedy layer-wise trainingof deep networks.
In NIPS, pages 153?160.Herv?e Bourlard and Yves Kamp.
1988.
Auto-association by multilayer perceptrons and singu-lar value decomposition.
Biological Cybernetics,59(4):291?294, September.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Dumitru Erhan, Yoshua Bengio, Aaron C. Courville,Pierre-Antoine Manzagol, Pascal Vincent, and SamyBengio.
2010.
Why does unsupervised pre-traininghelp deep learning?
Journal of Machine LearningResearch, 11:625?660.82Geoffrey Hinton and Ruslan Salakhutdinov.
2006.
Re-ducing the dimensionality of data with neural net-works.
Science, 313(5786):504 ?
507.Geoffrey E. Hinton, Simon Osindero, and Yee WhyeTeh.
2006.
A fast learning algorithm for deep beliefnets.
Neural Computation, 18(7):1527?1554.Geoffrey E. Hinton.
2002.
Training products of ex-perts by minimizing contrastive divergence.
NeuralComputation, 14(8):1771?1800.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180.Marta R. Costa-juss`a and Rafael E. Banchs.
2011.
Thebm-i2r haitian-cr?eole-to-english translation systemdescription for the wmt 2011 evaluation campaign.In Proceedings of the Sixth Workshop on StatisticalMachine Translation, pages 452?456, Edinburgh,Scotland, July.
Association for Computational Lin-guistics.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning representations by back-propagating errors.
Nature, 323(6088):533?536.83
