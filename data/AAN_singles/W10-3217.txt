Proceedings of the 8th Workshop on Asian Language Resources, pages 129?136,Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language ProcessingA Supervised Learning based Chunking in Thaiusing Categorial GrammarThepchai Supnithi, Peerachet Porkaew,Taneth Ruangrajitpakorn, KanokornTrakultaweekoolHuman Language Technology,National Electronics and ComputerTechnology Center{thepchai.sup, peera-chet.por, taneth.rua, ka-nokorn.tra}@nectec.or.thChanon Onman, Asanee Kaw-trakulDepartment of Computer Engineer-ing, Kasetsart University andNational Electronics and ComputerTechnology Centerchanon.onman@gmail.com,asanee.kaw@nectec.or.thAbstractOne of the challenging problems in ThaiNLP is to manage a problem on a syn-tactical analysis of a long sentence.This paper applies conditional randomfield and categorical grammar to devel-op a chunking method, which can groupwords into larger unit.
Based on the ex-periment, we found the impressive re-sults.
We gain around 74.17% on sen-tence level chunking.
Furthermore wegot a more correct parsed tree based onour technique.
Around 50% of tree canbe added.
Finally, we solved the prob-lem on implicit sentential NP which isone of the difficult Thai language pro-cessing.
58.65% of sentential NP is cor-rectly detected.1 IntroductionRecently, many languages applied chunking, orshallow parsing, using supervised learning ap-proaches.
Basili (1999) utilized clause boundaryrecognition for shallow parsing.
Osborne (2000)and McCallum et al (2000) applied MaximumEntropy tagger for chunking.
Lafferty (2001)proposed Conditional Random Fields for se-quence labeling.
CRF can be recognized as agenerative model that is able to reach globaloptimum while other sequential classifiers focuson making the best local decision.
Sha and Pe-reira (2003) compared CRF to other supervisedlearning in CoNLL task.
They achieved resultsbetter than other approaches.
Molina et al(2002) improved the accuracy of HMM-basedshallow parser by introducing the specializedHMMs.In Thai language processing, many research-es focus on fundamental level of NLP, such asword segmentation, POS tagging.
For example,Kruengkrai et al (2006) introduced CRF forword segmentation and POS tagging trainedover Orchid corpus (Sornlertlamvanich et al,1998.).
However, the number of tagged texts inOrchid is specific on a technical report, which isdifficult to be applied to other domains such asnews, document, etc.
Furthermore, very littleresearches on other fundamental tools, such aschunking, unknown word detection and parser,have been done.
Pengphon et al (2002) ana-lyzed chunks of noun phrase in Thai for infor-mation retrieval task.
All researches assume thatsentence segmentation has been primarily donein corpus.
Since Thai has no explicit sentenceboundary, defining a concrete concept of sen-tence break is extremely difficult.Most sentence segmentation researches con-centrate on "space" and apply to Orchid corpus(Meknavin 1987, Pradit 2002).
Because of am-biguities on using space, the accuracy is not im-pressive when we apply into a real application.Let consider the following paragraph whichis a practical usage from news:129"???????????????????????????????
??????????????????????????????
?????????????????????
| ?????????????????
| ??????????????????????????
"lit: ?The red shirts have put bunkers aroundthe assembly area and put oil and tires.
Thetraffic is opened normally.
?We found that three events are described inthis paragraph.
We found that both the first andsecond event do not contain a subject.
The thirdevent does not semantically relate to the previ-ous two events.
With a literal translation to Eng-lish, the first and second can be combined intoone sentence; however, the third events shouldbe separated.As we survey in BEST corpus (Kosawat2009), a ten-million word Thai segmented cor-pus.
It contains twelve genres.
The number ofword in sentence is varied from one word to2,633 words and the average word per line is40.07 words.
Considering to a News domain,which is the most practical usage in BEST, wefound that the number of words are ranged fromone to 415 words, and the average word lengthin sentence is 53.20.
It is obvious that there is aheavy burden load for parser when these longtexts are applied.Example 1:??
???
???????
?               ??
????????????
?man(n) drive(v)   taxi(n)  find(v)   wallet(n)lit1: A man drove a taxi and found a wallet.lit2: A taxi chauffeur found a wallet.Example 2:???
??
????
??????
?????
?????
?should will must    can    develop(v) country(n)lit: possibly have to develop country.Figure 1.
Examples of compounds in ThaiTwo issues are raised in this paper.
The firstquestion is "How to separate a long paragraphinto a larger unit than word effectively?"
We arelooking at the possibility of combining wordsinto a larger grain size.
It enables the system tounderstand the complicate structure in Thai asexplained in the example.
Chunking approach inthis paper is closely similar to the work of Shaand Pereira (2003).
Second question is "How toanalyze the compound noun structure in Thai?
"Thai allows a compound construction for a nounand its structures can be either a sequence ofnouns or a combination of nouns and verbs.
Thesecond structure is unique since the word orderis as same as a word order of a sentence.
Wecall this compound noun structure as a ?senten-tial NP?.Let us exemplify some Thai examples related tocompound word and serial construction problemin Figure 1.
The example 1 shows a sentencewhich contains a combination of nouns andverbs.
It can be ambiguously represented intotwo structures.
The first alternative is that thissentence shows an evidence of a serial verbconstruction.
The first word serves as a subjectof the two following predicates.
Another alter-native is that the first three word can be formedtogether as a compound noun and they refer to?a taxi driver?
which serve as a subject of thefollowing verb and noun.
The second alternativeis more commonly used in practical language.However, to set the ?N V N?
pattern as a nouncan be very ambiguous since in the example 1can be formed a sentential NP from either thefirst three words or the last three words.From the Example 2, an auxiliary verb serial-ization is represented.
It is a combination ofauxiliary verbs and verb.
The word order isshown in Aux Aux Aux Aux V N sequence.The given examples show complex cases thatrequire chunking to reduce an ambiguity whileThai text is applied into a syntactical analysissuch as parsing.
Moreover, there is more chanceto get a syntactically incorrect result from eitherrule-based parser or statistical parser with a highamount of word per input.This paper is organized as follows.
Section 2explains Thai categorial grammar.
Section 3130illustrates CRF, which is supervised methodapplied in this work.
Section 4 explains themethodology and experiment framework.
Sec-tion 5 shows experiments setting and result.Section 6 shows discussion.
Conclusion andfuture work are illustrated in section 7.2 Linguistic Knowledge2.1 Categorial GrammarCategorial grammar (Aka.
CG or classical cate-gorial grammar) (Ajdukiewicz, 1935; Bar-Hillel, 1953; Carpenter, 1992; Buszkowski,1998; Steedman, 2000) is formalism in naturallanguage syntax motivated by the principle ofconstitutionality and organized according to thesyntactic elements.
The syntactic elements arecategorised in terms of their ability to combinewith one another to form larger constituents asfunctions or according to a function-argumentrelationship.
All syntactic categories in CG aredistinguished by a syntactic category identifyingthem as one of the following two types:1.
Argument: this type is a basic category,such as s (sentence) and np (nounphrase).2.
Functor (or function category): this cat-egory type is a combination of argu-ment and operator(s) '/' and '\'.
Functoris marked to a complex constituent toassist argument to complete sentencesuch as s\np (intransitive verb) requiresnoun phrase from the left side to com-plete a sentence.CG captures the same information by associ-ating a functional type or category with allgrammatical entities.
The notation ?/?
is arightward-combining functor over a domain of ?into a range of ?.
The notation ?\?
is a leftward-combining functor over ?
into ?.
?
and ?
areboth argument syntactic categories(Hockenmaier and Steedman, 2002; Baldridgeand Kruijff, 2003).The basic concept is to find the core of thecombination and replace the grammatical modi-fier and complement with set of categoriesbased on the same concept with fractions.
Forexample, intransitive verb is needed to combinewith a subject to complete a sentence thereforeintransitive verb is written as s\np which meansFigure 2 Example of Thai CG-parsed Tree.it needs a noun phrase from the left side tocomplete a sentence.
If there is a noun phraseexists on the left side, the rule of fraction can-cellation is applied as np*s\np = s. With CG,each constituent is annotated with its own syn-tactic category as its function in text.
Currentlythere are 79 categories in Thai.
An example ofCG derivation from Thai is shown in Figure 2.2.2 CG-SetCG-Set are used as a feature when no CG aretagged to the input.
We aim to apply our chunk-er to a real world application.
Therefore, in casethat we have only sentence without CG tags, wewill use CG-Set instead.Cat-SetIndexCat-Set Member0 np ????????
?2 s\np/pp,s\np/np,s\np/pp/np,s\np ???
?, ????3(np\np)/(np\np),((s\np)\(s\np))/spnum,np,(np\np)\num,np\num,(np\np)/spnum,((s\np)\(s\np))\num????,?????
?62 (s\np)\(s\np),s\s ??
'?, ??
'?, ??
?134 np/(s\np),np/((s\np)/np) ??
?, ???
?Table 1 Example of CG-Set131The concept of CG-Set is to group words thattheir all possible CGs are equivalent to theother.
Therefore every word will be assigned toonly one CG-Set.
By using CG-Set we use thelookup table for tagging the input.
Table 1shows examples of CG-set.
Currently, there are183 CG set.3 Conditional Random Field (CRF)CRF is an undirected graph model in whicheach vertex represents a random variable whosedistribution is to be inferred, and edgerepresents a dependency between two randomvariables.
It is a supervised framework forlabeling a sequence data such as POS taggingand chunking.
Let X  is a random variable ofobserved input sequence, such as sequence ofwords, and Y is a random variable of labelsequence corresponding to X , such as sequenceof POS or CG.
The most probable labelsequence ( y? )
can be obtain by)|(maxarg?
xypy =Where nxxxx ,...,, 21= and nyyyy ,...,, 21=)|( xyp  is the conditional probabilitydistribution of a label sequence given by aninput sequence.
CRF defines )|( xyp as?????
?= ?=niixyFZxyP1),,(exp1)|(where ( )?
?
== y ni ixyFZ 1 ),,(exp  is anormalization factor over all state sequences.
),,( ixyF  is the global feature vector of CRFfor sequence x and y at position i .
),,( ixyFcan be calculated by using summation of localfeatures.??
+= ?jjjiiiii tyxgtyyfixyF ),,(),,(),,( 1 ?
?Each local feature consists of transition featurefunction ),,( 1 tyyf iii ?
and per-state featurefunction ),,( tyxg j .
Where i?
and j?
areweight vectors of transition feature function andper-state feature function respectively.The parameter of CRF can be calculated bymaximizing the likelihood function on thetraining data.
Viterbi algorithm is normallyapplied for searching the most suitable output.4 MethodologyFigure 3 shows the methodology of ourexperiments.
To prepare the training set, westart with our corpus annotated with CG tag.Then, each sentence in the corpus was parsed byFigure 3 Experimental Framework132our Thai CG parser, developed by GLR tech-technique.
However, not all sentences can beparsed successfully due to the complexity of thesentence.
We kept parsable sentences andunparsable sentences separately.
The parsablesentences were selected to be the training set.There are four features ?
surface, CG, CG-setand chunk marker ?
in our experiments.
CRF isapplied using 5-fold cross validation overcombination of these features.
Accuracy in termof averaged precision and recall are reported.We select the best model from the experimentto implement the chunker.
To investigateperformance of the chunker, we feed theunparsable sentences to the chunker andevaluate them manually.After that, the sentences which are correctlychunked will be sent to our Thai CG parser.
Wecalculate the number of successfully-parsedsentences and the number of correct chunks.5 Experiment Settings and Results5.1 Experiment on chunking5.1.1 Experiment settingTo develop chunker, we apply CG Dictionaryand CG tagged corpus as input.
Four featuresare provided to CRF.
Surface is a word surface.CG is a categorial grammar of the word.
CG-setis a combination of CG of the word.
IOBrepresents a method to mark chunk in asentence.
"I" means "inner" which representsthe word within the chunk.
"O" means "outside"which represents the word outside the chunk.
"B" means "boundary" which represents theword as a boundary position.
It accompaniedwith five chunk types.
"NP" stands for nounphrase, "VP" stands for verb phrase, "PP" standsfor preposition phrase, "ADVP" stands foradverb phrase and S-BAR stands forcomplementizer that link two phrases.Surface and CG-set are developed from CGdictionary.
CG is retrieved from CG taggedcorpus.
IOB is developed by parsing tree.
Weapply Thai CG parser to obtain the parsed tree.Figure 4 shows an example of our prepareddata.
We provide 4,201 sentences as a trainingdata in CRF to obtain a chunked model.
In thisexperiment, we use 5-fold cross validation toevaluation the model in term of F-measure.surface cg_set cg chunk_label??
74 s/s/np B-ADVP???
3 np I-ADVP?
? 180 (np\np)/(s\np) I-ADVP??
?
54 (s\np)/(s\np) I-ADVP????
7 s\np I-ADVP????
130 ((s/s)\(s/s))/(s/s) I-ADVP??
74 s/s/np I-ADVP???????
0 np I-ADVP???
0 np B-NP???
8 s\np/np B-VP???'?
0 np B-NP??
148 (s\np)/(s\np) B-VP????????
2 s\np I-VPFigure 4 An example of prepared dataTable 2 Chunking accuracy of each chunk1335.1.2 Experiment resultFrom Table 2, considering on chunk based lev-el, we found that CG gives the best resultamong surface, CG-set, CG and their combina-tion.
The average on three types in terms of F-measure is 86.20.
When we analyze infor-mation in detail, we found that NP, VP and PPshow the same results.
Using CG shows the F-measure for each of them, 81.15, 90.96 and99.56 respectively.From Table 3, considering in both word leveland sentence level, we got the similar results,CG gives the best results.
F-measure is 93.24 inword level and 74.17 in sentence level.
Thisshows the evidence that CG plays an importantrole to improve the accuracy on chunking.5.2 Experiment on parsing5.2.1 Experiment settingWe investigate the improvement of parsing con-sidering unparsable sentences.
There are 14,885unparsable sentences from our CG parser.
Thesesentences are inputted in chunked model to ob-tain a chunked corpus.
We manually evaluatethe results by linguist.
Linguists evaluate thechunked output in three types.
0 means incorrectchunk.
1 means correct chunk and 2 represents aspecial case for Thai NP, a sentential NP.5.2.2 Experiment resultFrom the experiment, we got an impressive re-sult.
We found that 11,698 sentences (78.59%)are changed from unparsable to parsable sen-tence.
Only 3,187 (21.41%) are unparsable.
Wemanually evaluate the parsable sentence by ran-domly select 7,369 sentences.
Linguists found3,689 correct sentences (50.06%).
In addition,we investigate the number of parsable chunkcalculated from the parsable result and found37,743 correct chunks from 47,718 chunks(78.47%).
We also classified chunk into threetypes NN VP and PP and gain the accuracy ineach type 79.14% ,74.66% and 92.57% respec-tively.6 Discussion6.1 Error analysisFrom the experiment results, we found the fol-lowing errors.6.1.1 Chunking Type missingSome chunk missing types are found in experi-ment results.
For example, [PP ??????
(rec-ord)][NP ?????????????????
(character about)].
[PPTable 3 Chunking accuracy based onword and sentence.Figure 4 An Example of sentential NP134??????
(record)] should be defined as VP insteadof PP.6.1.2 Over-groupingIn the sentence ?
[VP ??
?
(Using)][NP(medicine)][VP ?????
(treat) ][NP ???????????'
???????????
(each disease have to)][PP ???
(follow) ][NP ????????????????
?
(doctor?s instruction)] ?, wefound that ?NP ???????????'
???????????
(each diseasehave to) ?
has over-grouping.
IT is necessary tobreakdown to NP ???????????'
?
(each disease)  andVP ??????????
(have to).
The reason of this error isdue to allow the sentential structure NP VP NP,and then NP and VP are combined.6.1.3 Sentential NPWe investigated the number of sentential NP.
Ifthe number of chunk equal to 1, sentence shouldnot be recognized as NP.
Other cases are de-fined as NP.
We found that 929 from 1,584 sen-tences (58.65 % of sentences) are correct sen-tential NP.
This evidence shows the impressiveresults to solve implicit NP in Thai.
Figure 4shows an example of sentential NP.6.1.4 CG-setSince CG-set is another representation of wordand can only detect from CG dictionary.
It isvery easy to develop a tag sequence using CG-set.
We found that CG-set is more powerful thansurface.
It might be another alternative for lesslanguage resource situation.6.2 The Effect of Linguistic Knowledge onchunkingSince CG is formalism in natural language syn-tax motivated by the principle of constitutionali-ty and organised according to the syntactic ele-ments, we would like to find out whether lin-guistic knowledge effects to the model.
Wegrouped 89 categorial grammars into 17 groups,called CG-17.It is categorized into Noun, Prep, NounModifier, Number modifier for noun, Numbermodifier for verb, Number, Clause Marker,Verb with no argument, Verb with 1 argument,Verb with 2 or more arguments, Prefix noun,Prefix predicate, Prefix predicate modifier,Noun linker, Predicate Modification, Predicatelinker, and Sentence Modifier.We found that F-measure is slightly improvedfrom 74.17% to 75.06%.
This shows the evi-dence that if we carefully categorized data basedon linguistics viewpoint, it may improve moreaccuracy.7 Conclusions and Future WorkIn this paper, we stated Thai language problemson the long sentence pattern and find the novelmethod to chunk sentence into smaller unit,which larger than word.
We concluded that us-ing CRF accompanied with categorical grammarshow the impressive results.
The accuracy ofchunking in sentence level is 74.17%.
We arepossible to collect 50% more on correct tree.This technique enables us to solve the implicitsentential NP problem.
With our technique, wefound 58% of implicit sentential NP.
In the fu-ture work, there are several issues to be im-proved.
First, we have to trade-off betweenover-grouping problem and implicit sententialproblem.
Second, we plan to consider ADVP,SBAR, which has a very small size of data.
It isnot adequate to train for a good result.
Finally,we plan to apply more linguistics knowledge toassist more accuracy.ReferencesAbney S., and Tenny C., editors, 1991.
Parsingby chunks, Priciple-based Parsing.
KluwerAcademic Publishers.Awasthi P., Rao D., Ravindram B., 2006.
Partof Speech Tagging and Chunking with HMMand CRF, Proceeding of the NLPAI MachineLearning Competition.Basili R., Pazienza T., and Massio F., 1999.Lexicalizing a shallow parser, Proceedings of135Traitement Automatique du Langage Naturel1999.
Corgese, Corsica.Charoenporn Thatsanee, Sornlertlamvanich Vi-rach,  and Isahara Hitoshi.
1997.
Building ALarge Thai Text Corpus - Part-Of-SpeechTagged Corpus: ORCHID.
Proceedings ofNatural Language Processing Pacific RimSymposium.Kosawat Krit, Boriboon Monthika, ChootrakoolPatcharika, Chotimongkol Ananlada, KlaithinSupon, Kongyoung Sarawoot, KriengketKanyanut, Phaholphinyo Sitthaa, Puroda-kananda Sumonmas,ThanakulwarapasTipraporn, and Wutiwiwatchai Chai.
2009.BEST 2009: Thai Word Segmentation Soft-ware Contest.
The Eigth International Sym-posium on Natural Language Processing  :83-88.Kruengkrai C., Sornlertlumvanich V., Isahara H,2006.
A Conditional Random Field Frame-work for Thai Morphological Analysis, Pro-ceedings of 5th International Conference onLanguage Resources and Evaluation (LREC-2006).Kudo T., and Matsumoto Y., 2001.
Chunkingwith support vector machines, Proceeding ofNAACL.Lafferty J., McCallum A., and Pereira F., 2001.Conditional Random Fields : Probabilisticmodels for segmenting and labeling sequencedata.
In Proceeding of ICML-01, 282-289.McCallum A., Freitag D., and Pereira F. 2000.Maximum entropy markov model for infor-mation extraction and segmentation.
Pro-ceedings of ICML.Molina A., and Pla F., 2002.
Shallow Parsingusing Specialized HMMs, Journal of MachineLearning Research 2,595-613Nguyen L. Minh, Nguyen H. Thao, and NguyenP., Thai.
2009.
An Empirical Study of Viet-namese Noun Phrase Chunking with Discrim-inative Sequence Models, Proceedings of the7th Workshop on Asian Language Resources,ACL-IJCNLP 2009,9-16Osborne M. 2000.
Shallow Parsing as Part-of-Speech Tagging.
Proceedings of CoNLL-2000 and LLL-2000, Lisbon, Portugal.Pengphon N., Kawtrakul A., Suktarachan M.,2002.
Word Formation Approach to NounPhrase Analysis for Thai,  Proceedings ofSNLP2002.Sha F. and Pereira F., 2003.
Shallow Parsingwith Conditional Random Fields, Proceedingof HLT-NAACL.136
