Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 731?738,Sydney, July 2006. c?2006 Association for Computational LinguisticsOn-Demand Information ExtractionSatoshi SekineComputer Science DepartmentNew York University715 Broadway, 7th floorNew York, NY 10003  USAsekine@cs.nyu.eduAbstractAt present, adapting an Information Ex-traction system to new topics is an expen-sive and slow process, requiring someknowledge engineering for each new topic.We propose a new paradigm of Informa-tion Extraction which operates 'on demand'in response to a user's query.
On-demandInformation Extraction (ODIE) aims tocompletely eliminate the customization ef-fort.
Given a user?s query, the system willautomatically create patterns to extract sa-lient relations in the text of the topic, andbuild tables from the extracted informationusing paraphrase discovery technology.
Itrelies on recent advances in pattern dis-covery, paraphrase discovery, and ex-tended named entity tagging.
We report onexperimental results in which the systemcreated useful tables for many topics,demonstrating the feasibility of this ap-proach.1 IntroductionMost of the world?s information is recorded,passed down, and transmitted between people intext form.
Implicit in most types of text are regu-larities of information structure - events whichare reported many times, about different indi-viduals, in different forms, such as layoffs ormergers and acquisitions in news articles.
Thegoal of information extraction (IE) is to extractsuch information:  to make these regular struc-tures explicit, in forms such as tabular databases.Once the information structures are explicit, theycan be processed in many ways: to mine infor-mation, to search for specific information, togenerate graphical displays and other summaries.However, at present, a great deal of knowl-edge for automatic Information Extraction mustbe coded by hand to move a system to a newtopic.
For example, at the later MUC evaluations,system developers spent one month for theknowledge engineering to customize the systemto the given test topic.
Research over the lastdecade has shown how some of this knowledgecan be obtained from annotated corpora, but thisstill requires a large amount of annotation inpreparation for a new task.
Improving portability- being able to adapt to a new topic with minimaleffort ?
is necessary to make Information Extrac-tion technology useful for real users and, we be-lieve, lead to a breakthrough for the applicationof the technology.We propose ?On-demand information extrac-tion (ODIE)?
: a system which automaticallyidentifies the most salient structures and extractsthe information on the topic the user demands.This new IE paradigm becomes feasible due torecent developments in machine learning forNLP, in particular unsupervised learning meth-ods, and it is created on top of a range of basiclanguage analysis tools, including POS taggers,dependency analyzers, and extended Named En-tity taggers.2 OverviewThe basic functionality of the system is the fol-lowing.
The user types a query / topic descriptionin keywords (for example, ?merge?
or ?merger?
).Then tables will be created automatically in sev-eral minutes, rather than in a month of humanlabor.
These tables are expected to show infor-mation about the salient relations for the topic.Figure 1 describes the components and howthis system works.
There are six major compo-nents in the system.
We will briefly describeeach component and how the data is processed;then, in the next section, four important compo-nents will be described in more detail.731Description of task (query)Figure 1.
System overview1) IR system: Based on the query given by theuser, it retrieves relevant documents from thedocument database.
We used a simple TF/IDFIR system we developed.2) Pattern discovery: First, the texts in the re-trieved documents are analyzed using a POStagger, a dependency analyzer and an Ex-tended NE (Named Entity) tagger, which willbe described later.
Then this component ex-tracts sub-trees of dependency trees which arerelatively frequent in the retrieved documentscompared to the entire corpus.
It counts thefrequencies in the retrieved texts of all sub-trees with more than a certain number of nodesand uses TF/IDF methods to score them.
Thetop-ranking sub-trees which contain NEs willbe called patterns, which are expected to indi-cate salient relationships of the topic and willbe used in the later components.3) Paraphrase discovery: In order to find semanticrelationships between patterns, i.e.
to find pat-terns which should be used to build the sametable, we use paraphrase discovery techniques.The paraphrase discovery was conducted off-line and created a paraphrase knowledge base.4) Table construction: In this component, thepatterns created in (2) are linked based on theparaphrase knowledge base created by (3),producing sets of patterns which are semanti-cally equivalent.
Once the sets of patterns arecreated, these patterns are applied to the docu-ments retrieved by the IR system (1).
Thematched patterns pull out the entity instancesand these entities are aligned to build the finaltables.5) Language analyzers: We use a POS tagger anda dependency analyzer to analyze the text.
Theanalyzed texts are used in pattern discoveryand paraphrase discovery.6) Extended NE tagger: Most of the participantsin events are likely to be Named Entities.However, the traditional NE categories are notsufficient to cover most participants of variousevents.
For example, the standard MUC?s 7NE categories (i.e.
person, location, organiza-tion, percent, money, time and date) missproduct names (e.g.
Windows XP, Boeing 747),event names (Olympics, World War II), nu-merical expressions other than monetary ex-pressions, etc.
We used the Extended NEcategories with 140 categories and a taggerbased on the categories.IR systemPattern discovery Paraphrase discoveryRelevantdocumentsPatternsPattern setsTableParaphraseKnowledge baseExtendedNE tagger6) 5) LanguageAnalyzer1)2)4) Table construction3)7323 Details of ComponentsIn this section, four important components will bedescribed in detail.
Prior work related to eachcomponent is explained and the techniques used inour system are presented.3.1 Pattern DiscoveryThe pattern discovery component is responsiblefor discovering salient patterns for the topic.
Thepatterns will be extracted from the documentsrelevant to the topic which are gathered by an IRsystem.Several unsupervised pattern discovery tech-niques have been proposed, e.g.
(Riloff 96),(Agichtein and Gravano 00) and (Yangarber et al00).
Most recently we (Sudo et al 03) proposed amethod which is triggered by a user query to dis-cover important patterns fully automatically.
Inthis work, three different representation modelsfor IE patterns were compared, and the sub-treemodel was found more effective compared to thepredicate-argument model and the chain model.
Inthe sub-tree model, any connected part of a de-pendency tree for a sentence can be considered asa pattern.
As it counts all possible sub-trees fromall sentences in the retrieved documents, the com-putation is very expensive.
This problem wassolved by requiring that the sub-trees contain apredicate (verb) and restricting the number ofnodes.
It was implemented using the sub-treecounting algorithm proposed by (Abe et al 02).The patterns are scored based on the relative fre-quency of the pattern in the retrieved documents(fr) and in the entire corpus (fall).
The formula usesthe TF/IDF idea (Formula 1).
The system ignoresvery frequent patterns, as those patterns are socommon that they are not likely to be important toany particular topic, and also very rare patterns, asmost of those patterns are noise.
))(log()():(ctftfsubtreetscoreallr+=          (1)The scoring function sorts all patterns whichcontain at least one extended NE and the top 100patterns are selected for later processing.
Figure 2shows examples of the discovered patterns for the?merger and acquisition?
topic.
Chunks are shownin brackets and extended NEs are shown in uppercase words.
(COM means ?company?
and MNYmeans ?money?
)<COM1> <agree to buy> <COM2> <for MNY><COM1> <will acquire> <COM2> <for MNY><a MNY merger> <of COM1> <and COM2>Figure 2.
Pattern examples3.2 Paraphrase DiscoveryThe role of the paraphrase discovery component isto link the patterns which mean the same thing forthe task.
Recently there has been a growingamount of research on automatic paraphrase dis-covery.
For example, (Barzilay 01) proposed amethod to extract paraphrases from parallel trans-lations derived from one original document.
Weproposed to find paraphrases from multiple news-papers reporting the same event, using sharedNamed Entities to align the phrases (Shinyama etal.
02).
We also proposed a method to find para-phrases in the context of two Named Entity in-stances in a large un-annotated corpus (Sekine 05).The phrases connecting two NEs are groupedbased on two types of evidence.
One is the iden-tity of the NE instance pairs, as multiple instancesof the same NE pair (e.g.
Yahoo!
and Overture)are likely to refer to the same relationship (e.g.acquisition).
The other type of evidence is thekeywords in the phrase.
If we gather a lot ofphrases connecting NE's of the same two NEtypes (e.g.
company and company), we can clusterthese phrases and find some typical expressions(e.g.
merge, acquisition, buy).
The phrases areclustered based on these two types of evidenceand sets of paraphrases are created.Basically, we used the paraphrases found bythe approach mentioned above.
For example, theexpressions in Figure 2 are identified as para-phrases by this method; so these three patternswill be placed in the same pattern set.733Note that there is an alternative method ofparaphrase discovery, using a hand crafted syno-nym dictionary like WordNet (WordNet Homepage).
However, we found that the coverage ofWordNet for a particular topic is not sufficient.For example, no synset covers any combinationsof the main words in Figure 2, namely ?buy?, ?ac-quire?
and ?merger?.
Furthermore, even if thesewords are found as synonyms, there is the addi-tional task of linking expressions.
For example, ifone of the expressions is ?reject the merger?, itshouldn?t be a paraphrase of ?acquire?.3.3 Extended NE taggingNamed Entities (NE) were first introduced by theMUC evaluations (Grishman and Sundheim 96).As the MUCs concentrated on business and mili-tary topics, the important entity types were limitedto a few classes of names and numerical expres-sions.
However, along with the development ofInformation Extraction and Question Answeringtechnologies, people realized that there should bemore and finer categories for NE.
We proposedone of those extended NE sets (Sekine 02).
It in-cludes 140 hierarchical categories.
For example,the categories include Company, Company group,Military, Government, Political party, and Interna-tional Organization as subcategories of Organiza-tion.
Also, new categories are introduced such asVehicle, Food, Award, Religion, Language, Of-fense, Art and so on as subcategories of Product,as well as Event, Natural Object, Vocation, Unit,Weight, Temperature, Number of people and soon.
We used a rule-based tagger developed to tagthe 140 categories for this experiment.Note that, in the proposed method, the slots ofthe final table will be filled in only with instancesof these extended Named Entities.
Most commonnouns, verbs or sentences can?t be entries in thetable.
This is obviously a limitation of the pro-posed method; however, as the categories are de-signed to provide good coverage for a factoid typeQA system, most interesting types of entities arecovered by the categories.3.4 Table ConstructionBasically the table construction is done by apply-ing the discovered patterns to the original corpus.The discovered patterns are grouped into patternset using discovered paraphrase knowledge.
Oncethe pattern sets are built, a table is created for eachpattern set.
We gather all NE instances matchedby one of the patterns in the set.
These instancesare put in the same column of the table for thepattern set.
When creating tables, we impose somerestrictions in order to reduce the number ofmeaningless tables and to gather the same rela-tions in one table.
We require columns to have atleast three filled instances and delete tables withfewer than three rows.
These thresholds are em-pirically determined using training data.Figure 3.
Table Construction4 Experiments.
Examplesof4.1 Data and ProcessingWe conducted the experiments using the 1995New York Times as the corpus.
The queries usedfor system development and threshold tuning werecreated by the authors, while queries based on theset of event types in the ACE extraction evalua-tions were used for testing.
A total of 31 test que-ries were used; we discarded several querieswhich were ambiguous or uncertain.
The test que-ries were derived from the example sentences foreach event type in the ACE guidelinesqueries are shown in the Appendix.At the moment, the whole process takes about15 minutes on average for each query on a Pen-tium 2.80GHz processor running Linux.
The cor-pus was analyzed in advance by a POS tagger, NEtagger and dependency analyzer.
The processingNews Paper* COM1 agree to buyireCOM1 and COM2COM2 for MNY* COM1 will acquCOM2 for MNY* a MNY merger ofNewspaper Pattern SetArticle1ABC agreed tobuy CDE for $1M?.?????
?Article 2a $20M merger ofFGH and IJKArticle       Company                 Money1        ABC, CDE                 $1M2        FGH, IJK                  $20MC no structed table734and counting of sub-trees takes the majority (morethan 90%) of the time.
We believe we can easilymake it faster by programming techniques, forple, using distributed puting.usually notfulle data, the evaluation data aresele more useful and interestinge information is.sefulnessNumber of topicsexam com4.2 Result and EvaluationOut of 31 queries, the system is unable to buildany tables for 11 queries.
The major reason is thatthe IR component can?t find enough newspaperarticles on the topic.
It retrieved only a few arti-cles for topics like ?born?, ?divorce?
or ?injure?from The New York Times.
For the moment, wewill focus on the 20 queries for which tables werebuilt.
The Appendix shows some examples ofqueries and the generated tables.
In total, 127 ta-bles are created for the 20 topics, with one to thir-teen tables for each topic.
The number of columnsin a table ranges from 2 to 10, including thedocument ID column, and the average number ofcolumns is 3.0.
The number of rows in a tablerange from 3 to 125, and the average number ofrows is 16.9.
The created tables arey filled; the average rate is 20.0%.In order to measure the potential and the use-fulness of the proposed method, we evaluate theresult based on three measures: usefulness, argu-ment role coverage, and correctness.
For the use-fulness evaluation, we manually reviewed thetables to determine whether a useful table is in-cluded or not.
This is inevitably subjective, as theuser does not specify in advance what table rowsand columns are expected.
We asked a subject tojudge usefulness in three grades; A) very useful ?for the query, many people might want to use thistable for the further investigation of the topic, B)useful ?
at least, for some purpose, some peoplemight want to use this table for further investiga-tion and C) not useful ?
no one will be interestedin using this table for further investigation.
Theargument role coverage measures the percentageof the roles specified for each ACE event typewhich appeared as a column in one or more of thecreated tables for that event type.
The correctnesswas measured based on whether a row of a tablereflects the correct information.
As it is impossi-ble to evaluate all thected randomly.Table 1 shows the usefulness evaluation result.Out of 20 topics, two topics are judged very usefuland twelve are judged useful.
The very useful top-ics are ?fine?
(Q4 in the appendix) and ?acquit?
(not shown in the appendix).
Compared to the re-sults in the ?useful?
category, the tables for thesetwo topics have more slots filled and the NE typesof the fillers have fewer mistakes.
The topics inthe ?not useful?
category are ?appeal?, ?execute?,?fired?, ?pardon?, ?release?
and ?trial?.
These areagain topics with very few relevant articles.
Byincreasing the corpus size or improving the IRcomponent, we may be able to improve the per-formance for these topics.
The majority category,?useful?, has 12 topics.
Five of them can be foundin the appendix (all those besides Q4).
For thesetopics, the number of relevant articles in the cor-pus is relatively high and interesting relations arefound.
The examples in the appendix are selectedfrom larger tables with many columns.
Althoughthere are columns that cannot be filled for everyevent instance, we found that the more columnsthat are filled in, ththTable 1.
U  evaluation resultEvaluationVery useful 2Useful 12Not useful 6For the 14 ?very useful?
and ?useful?
topics,the role coverage was measured.
Some of the rolesin the ACE task can be filled by different types ofNamed Entities, for example, the ?defendant?
of a?sentence?
event can be a Person, Organization orGPE.
However, the system creates tables based onNE types; e.g.
for the ?sentence?
event, a Personcolumn is created, in which most of the fillers aredefendants.
In such cases, we regard the columnas covering the role.
Out of 63 roles for the 14event types, 38 are found in the created tables, fora role coverage of 60.3%.
Note that, by loweringthe thresholds, the coverage can be increased to asmuch as 90% (some roles can?t be found becauseof Extended NE limitations or the rare appearanceof roles) but with some sacrifice of precision.Table 2 shows the correctness evaluation re-sults.
We randomly select 100 table rows amongthe topics which were judged ?very useful?
or?useful?, and determine the correctness of the in-formation by reading the newspaper articles theinformation was extracted from.
Out of 100 rows,84 rows have correct information in all slots.
4735rows have some incorrect information in some ofthe columns, and 12 contain wrong information.Most errors are due to NE tagging errors (11 NEerrors out of 16 errors).
These errors include in-stances of people which are tagged as other cate-gories, and so on.
Also, by looking at the actualarticles, we found that co-reference resolutioncould help to fill in more information.
Because theimportant information is repeatedly mentioned innewspaper articles, referential expressions are of-ten used.
For example, in a sentence ?In 1968 hewas elected mayor of Indianapolis.
?, we could notextract ?he?
at the moment.
We plan to addcoreference resolution in the near future.
Other?
e entity is confused, i.e.
victim?query (as both of them?He was sentenced 3ears and fined $1,000?.orrectnessn Numbsources of error include:The role of thand murdererDifferent kinds of events are found in one table,e.g., the victory of Jack Nicklaus was found inthe political electionuse terms like ?win?
)An unrelated but often collocate entity wasincluded.
For example, Year period expres-sions are found in ?fine?
events, as there aremany expressions like ?yTable 2.
C  evaluation resultEvaluatio er of rowsCorrect 84Partially correct 4Incorrect 125 Related WorkAs far as the authors know, there is no systemsimilar to ODIE.
Several methods have been pro-posed to produce IE patterns automatically to fa-cilitate IE knowledge creation, as is described inSection 3.1.
But those are not targeting the fullyautomatic creation of a complete IE system for anewvent detection followthie a countryandial where an ODIE-typesystem can be beneficial.topic.There exists another strategy to extend therange of IE systems.
It involves trying to cover awide variety of topics with a large inventory ofrelations and events.
It is not certain if there areonly a limited number of topics in the world, butthere are a limited number of high-interest topics,so this may be a reasonable solution from an engi-neering point of view.
This line of research wasfirst proposed by (Aone and Ramos-Santacruz 00)and the ACE evaluations of es line (ACE Home Page).An unsupervised learning method has been ap-plied to a more restricted IE task, Relation Dis-covery.
(Hasegawa et al 2004) used large corporaand an Extended Named Entity tagger to findnovel relations and their participants.
However,the results are limited to a pair of participants andbecause of the nature of the procedure, the discov-ered relations are static relations likits presidents rather than events.Topic-oriented summarization, currently pur-sued by the DUC evaluations (DUC Home Page),is also closely related.
The systems are trying tocreate summaries based on the specified topic fora manually prepared set of documents.
In this case,if the result is suitable to present in table format, itcan be handled by ODIE.
Our previous study (Se-kine and Nobata 03) found that about one third ofrandomly constructed similar newspaper articleclusters are well-suited to be presented in tableformat, and another one third of the clusters canbe acceptably expressed in table format.
This sug-gests there is a big potent6 Future WorkWe demonstrated a new paradigm of InformationExtraction technology and showed the potential ofthis method.
However, there are problems to besolved to advance the technology.
One of them isthe coverage of the extracted information.
Al-though we have created useful tables for sometopics, there are event instances which are notfound.
This problem is mostly due to the inade-quate performance of the language analyzers (in-formation retrieval component, dependencyanalyzer or Extended NE tagger) and the lack of acoreference analyzer.
Even though there are pos-sible applications with limited coverage, it will beessential to enhance these components and addcoreference in order to increase coverage.
Also,there are basic domain limitations.
We made thesystem ?on-demand?
for any topic, but currentlyonly within regular news domains.
As configured,the system would not work on other domains suchas a medical, legal, or patent domain, mainly dueto the design of the extended NE hierarchy.While specific hierarchies could be incorporated736for new domains, it will also be desirable to inte-grate bootstrapping techniques for rapid incre-mental additions to the hierarchy.
Also at thewould like to investigate this problem in the future.7 Conclusionand demonstrates the feasibility of this approach.8 Acknowledgementsarily reflect the positionof-suke Shinyama for useful comments, discussion.ACE Home Pag.edu/Projects/aceKed Practice of Knowledge in DatabaseChtural Lan-EuExtracting Relations from Large Plaintext Collec-moment, table column labels are simply ExtendedNE categories, and do not indicate the role.
WeIn this paper, we proposed ?On-demand Informa-tion Extraction (ODIE)?.
It is a system whichautomatically identifies the most salient structuresand extracts the information on whatever topic theuser demands.
It relies on recent advances in NLPtechnologies; unsupervised learning and severaladvanced NLP analyzers.
Although it is at a pre-liminary stage, we developed a prototype systemwhich has created useful tables for many topicsThis research was supported in part by the De-fense Advanced Research Projects Agency underContract HR0011-06-C-0023 and by the NationalScience Foundation under Grant IIS-0325657.This paper does not necessthe U.S. Government.We would like to thank Prof. Ralph Grishman,Dr.
Kiyoshi Sudo, Dr. Chikashi Nobata, Mr. Ta-kaaki Hasegawa, Mr. Koji Murakami and Mr. YuReferencese:http://www.ldc.upennDUC Home Page: http://duc.nist.govWordNet Home Page:  http://wordnet.princeton.edu/nji Abe, Shinji Kawasone, Tatsuya Asai, HirokiArimura and Setsuo Arikawa.
2002.
?OptimizedSubstructure Discovery for Semi-structured Data?.In Proceedings of the 6th European Conference onPrinciples an(PKDD-02)inatsu Aone; Mila Ramos-Santacruz.
2000.
?REES:A Large-Scale Relation and Event Extraction Sys-tem?
In Proceedings of the 6th Applied Naguage Processing Conference (ANLP-00)gene Agichtein and L. Gravano.
2000.
?Snowball:tionss?.
In Proceedings of the 5th ACM InternationalConference on Digital Libraries (DL-00)Regina Barzilay and Kathleen McKeown.
2001.
?Ex-tracting Paraphrases from a Parallel Corpus.
In Pro-ceedings of the Annual Meeting of Association ofComputational Linguistics/ and European Chapterof Association of Computational Linguistics(ACL/EACL-01)Ralph Grishman and Beth Sundheim.1996.
?MessageUnderstanding Conference - 6: A Brief History?, inProceedings of the 16th International Conference onComputational Linguistics (COLING-96)Takaaki Hasegawa, Satoshi Sekine and Ralph Grish-man 2004.
?Discovering Relations among NamedEntities from Large Corpora?, In Proceedings of theAnnual Meeting of the Association of Computa-tional Linguistics (ACL-04)Ellen Riloff.
1996.
?Automatically Generating Extrac-tion Patterns from Untagged Text?.
In Proceedingsof Thirteen National Conference on Artificial Intel-ligence (AAAI-96)Satoshi Sekine, Kiyoshi Sudo and Chikashi Nobata.2002 ?Extended Named Entity Hierarchy?
In Pro-ceefings of the third International Conference onLanguage Resources and Evaluation (LREC-02)Satoshi Sekine and Chikashi Nobata.
2003.
?A surveyfor Multi-Document Summarization?
In the pro-ceedings of Text Summarization Workshop.Satoshi Sekine.
2005.
?Automatic Paraphrase Discov-ery based on Context and Keywords between NEPairs?.
In Proceedings of International Workshop onParaphrase (IWP-05)Yusuke Shinyama, Satoshi Sekine and Kiyoshi Sudo.2002.
?Automatic Paraphrase Acquisition fromNews Articles?.
In Proceedings of the Human Lan-guage Technology Conference (HLT-02)Kiyoshi Sudo, Satsohi Sekine and Ralph Grishman.2003.
?An Improved Extraction Pattern Representa-tion Model for Automatic IE Pattern Acquisition?.In Proceedings of the Annual Meeting of Associa-tion of Computational Linguistics (ACL-03)Roman Yangarber, Ralph Grishman, Pasi Tapanainenand Silja Huttunen.
2000.
?Unsupervised Discoveryof Scenario-Level Patterns for Information Extrac-tion?.
In Proceedings of 18th International Confer-ence on Computational Linguistics (COLING-00)737Appendix: Sample queries and tables(Note that this is only a part of created tables)Q1: acquire, acquisition, merge, merger, buy purchasedocid MONEY COMPANY DATEnyt950714.0324 About $3 billion PNC Bank Corp., Midlantic Corp.nyt950831.0485 $900 million Ceridian Corp., Comdata Holdings Corp. Last weeknyt950909.0449 About $1.6 billion Bank South Corpnyt951010.0389 $3.1 billion CoreStates Financial Corp.nyt951113.0483 $286 million Potash Corp. Last monthnyt951113.0483 $400 million Chemicals Inc. Last yearQ2: convict, guiltydocid PERSON DATE AGEnyt950207.0001 Fleiss Dec. 2 28nyt950327.0402 Gerald_Amirault 1986 41nyt950720.0145 Hedayat_Eslaminia 1988nyt950731.0138 James McNally, James Johnson Bey, Jose Prieto, Pat-terson1993, 1991, thisyear, 1984nyt951229.0525 Kane Last yearQ3: electDocid POSITION TITLE PERSON DATEnyt950404.0197 president Havel Dec. 29, 1989nyt950916.0222 president Ronald Reagan 1980nyt951120.0355 president Aleksander KwasniewskiQ4: fineDocid PERSON MONEY DATEnyt950420.0056 Van Halen $1,000nyt950525.0024 Derek Meredith $300nyt950704.0016 Tarango At least $15,500nyt951025.0501 Hamilton $12,000 This weeknyt951209.0115 Wheatley Approximately $2,000Q5: arrest jail incarcerate imprisonDocid PERSON YEAR PERIODnyt950817.0544 Nguyen Tan Tri Four yearsnyt951018.0762 Wolf Six yearsnyt951218.0091 Carlos Mendoza-Lugo One yearQ6: sentenceDocid PERSON YEAR PERIODnyt950412.0448 Mitchell Antar Four yearsnyt950421.0509 MacDonald 14 yearsnyt950622.0512 Aramony Three yearsnyt950814.0106 Obasanjo 25 years738
