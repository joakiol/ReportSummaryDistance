Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 436?446,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLatent Tree Language ModelToma?s?
Brychc?
?nNTIS ?
New Technologies for the Information Society,Faculty of Applied Sciences, University of West Bohemia,Technicka?
8, 306 14 Plzen?, Czech Republicbrychcin@kiv.zcu.cznlp.kiv.zcu.czAbstractIn this paper we introduce Latent Tree Lan-guage Model (LTLM), a novel approach tolanguage modeling that encodes syntax andsemantics of a given sentence as a tree of wordroles.The learning phase iteratively updates thetrees by moving nodes according to Gibbssampling.
We introduce two algorithms to in-fer a tree for a given sentence.
The first one isbased on Gibbs sampling.
It is fast, but doesnot guarantee to find the most probable tree.The second one is based on dynamic program-ming.
It is slower, but guarantees to find themost probable tree.
We provide comparisonof both algorithms.We combine LTLM with 4-gram ModifiedKneser-Ney language model via linear inter-polation.
Our experiments with English andCzech corpora show significant perplexity re-ductions (up to 46% for English and 49%for Czech) compared with standalone 4-gramModified Kneser-Ney language model.1 IntroductionLanguage modeling is one of the core disciplinesin natural language processing (NLP).
Automaticspeech recognition, machine translation, opticalcharacter recognition, and other tasks strongly de-pend on the language model (LM).
An improve-ment in language modeling often leads to betterperformance of the whole task.
The goal of lan-guage modeling is to determine the joint probabil-ity of a sentence.
Currently, the dominant approachis n-gram language modeling, which decomposesthe joint probability into the product of conditionalprobabilities by using the chain rule.
In traditionaln-gram LMs the words are represented as distinctsymbols.
This leads to an enormous number of wordcombinations.In the last years many researchers have tried tocapture words contextual meaning and incorporateit into the LMs.
Word sequences that have neverbeen seen before receive high probability when theyare made of words that are semantically similar towords forming sentences seen in training data.
Thisability can increase the LM performance because itreduces the data sparsity problem.
In NLP a verycommon paradigm for word meaning representationis the use of the Distributional hypothesis.
It sug-gests that two words are expected to be semanti-cally similar if they occur in similar contexts (theyare similarly distributed in the text) (Harris, 1954).Models based on this assumption are denoted as dis-tributional semantic models (DSMs).Recently, semantically motivated LMs have be-gun to surpass the ordinary n-gram LMs.
The mostcommonly used architectures are neural networkLMs (Bengio et al, 2003; Mikolov et al, 2010;Mikolov et al, 2011) and class-based LMs.
Class-based LMs are more related to this work thus weinvestigate them deeper.Brown et al (1992) introduced class-based LMsof English.
Their unsupervised algorithm searchesclasses consisting of words that are most probablein the given context (one word window in both di-rections).
However, the computational complex-ity of this algorithm is very high.
This approachwas later extended by (Martin et al, 1998; Whit-436taker and Woodland, 2003) to improve the complex-ity and to work with wider context.
Deschacht etal.
(2012) used the same idea and introduced La-tent Words Language Model (LWLM), where wordclasses are latent variables in a graphical model.They apply Gibbs sampling or the expectation max-imization algorithm to discover the word classesthat are most probable in the context of surround-ing word classes.
A similar approach was pre-sented in (Brychc?
?n and Konop?
?k, 2014; Brychc?
?nand Konop?
?k, 2015), where the word clusters de-rived from various semantic spaces were used to im-prove LMs.In above mentioned approaches, the meaning of aword is inferred from the surrounding words inde-pendently of their relation.
An alternative approachis to derive contexts based on the syntactic relationsthe word participates in.
Such syntactic contexts areautomatically produced by dependency parse-trees.Resulting word representations are usually less top-ical and exhibit more functional similarity (they aremore syntactically oriented) as shown in (Pado?
andLapata, 2007; Levy and Goldberg, 2014).Dependency-based methods for syntactic parsinghave become increasingly popular in NLP in the lastyears (Ku?bler et al, 2009).
Popel and Marec?ek(2010) showed that these methods are promisingdirection of improving LMs.
Recently, unsuper-vised algorithms for dependency parsing appearedin (Headden III et al, 2009; Cohen et al, 2009;Spitkovsky et al, 2010; Spitkovsky et al, 2011;Marec?ek and Straka, 2013) offering new possibili-ties even for poorly-resourced languages.In this work we introduce a new DSM that usestree-based context to create word roles.
The wordrole contains the words that are similarly distributedover similar tree-based contexts.
The word roleencodes the semantic and syntactic properties of aword.
We do not rely on parse trees as a prior knowl-edge, but we jointly learn the tree structures andword roles.
Our model is a soft clustering, i.e.
oneword may be present in several roles.
Thus it is the-oretically able to capture the word polysemy.
Thelearned structure is used as a LM, where each wordrole is conditioned on its parent role.
We present theunsupervised algorithm that discovers the tree struc-tures only from the distribution of words in a trainingcorpus (i.e.
no labeled data or external sources of in-formation are needed).
In our work we were inspiredby class-based LMs (Deschacht et al, 2012), unsu-pervised dependency parsing (Marec?ek and Straka,2013), and tree-based DSMs (Levy and Goldberg,2014).This paper is organized as follows.
We start withthe definition of our model (Section 2).
The pro-cess of learning the hidden sentence structures is ex-plained in Section 3.
We introduce two algorithmsfor searching the most probable tree for a given sen-tence (Section 4).
The experimental results on En-glish and Czech corpora are presented in Section 6.We conclude in Section 7 and offer some directionsfor future work.2 Latent Tree Language ModelIn this section we describe Latent Tree LanguageModel (LTLM).
LTLM is a generative statisticalmodel that discovers the tree structures hidden in thetext corpus.Let L be a word vocabulary with total of |L| dis-tinct words.
Assume we have a training corpus wdivided into S sentences.
The goal of LTLM orother LMs is to estimate the probability of a textP (w).
Let Ns denote the number of words in thes-th sentence.
The s-th sentence is a sequence ofwords ws = {ws,i}Nsi=0, where ws,i ?
L is a wordat position i in this sentence and ws,0 = < s > isan artificial symbol that is added at the beginning ofeach sentence.Each sentence s is associated with the dependencygraph Gs.
We define the dependency graph as alabeled directed graph, where nodes correspond tothe words in the sentence and there is a label foreach node that we call role.
Formally, it is a tripleGs = (V s,Es, rs) consisting of:?
The set of nodes V s = {0, 1, ..., Ns}.
Eachtoken ws,i is associated with node i ?
V s.?
The set of edges Es ?
V s ?
V s.?
The sequence of roles rs = {rs,i}Nsi=0, where1 ?
rs,i ?
K for i ?
V s. K is the number ofroles.The artificial word ws,0 = < s > at the beginningof the sentence has always role 1 (rs,0 = 1).
Anal-ogously to w, the sequence of all rs is denoted as rand sequence of allGs asG.437Figure 1: Example of LTLM for the sentence ?Ev-erything has beauty, but not everyone sees it.
?Edge e ?
Es is an ordered pair of nodes (i, j).We say that i is the head or the parent and j is thedependent or the child.
We use the notation i ?
jfor such edge.
The directed path from node i to nodej is denoted as i ??
j.We place a few constraints on the graphGs.?
The graphGs is a tree.
It means it is the acyclicgraph (if i ?
j then not j ??
i), where eachnode has one parent (if i ?
j then not k ?
jfor every k 6= i).?
The graph Gs is projective (there are no crossedges).
For each edge (i, j) and for each k be-tween i and j (i.e.
i < k < j or i > k > j)there must exist the directed path i ??
k.?
The graphGs is always rooted in the node 0.We denote these graphs as the projective depen-dency trees.
Example of such a tree is on Figure 1.For the treeGs we define a functionhs(j) = i, when (i, j) ?
Es (1)that returns the parent for each node except the root.We use graph Gs as a representation of theBayesian network with random variables Es andrs.
The roles rs,i represent the node labels and theedges express the dependences between the roles.The conditional probability of the role at positioni given its parent role is denoted as P (rs,i|rs,hs(i)).The conditional probability of the word at positioni in the sentence given its role rs,i is denoted asP (ws,i|rs,i).We model the distribution over words in the sen-tence s as the mixtureP (ws) = P (ws|rs,0) =Ns?i=1K?k=1P (ws,i|rs,i = k)P (rs,i = k|rs,hs(i)).
(2)The root role is kept fixed for each sentence (rs,0= 1) so P (ws) = P (ws|rs,0).We look at the roles as mixtures over child rolesand simultaneously as mixtures over words.
We canrepresent dependency between roles with a set of Kmultinomial distributions ?
over K roles, such thatP (rs,i|rs,hs(i) = k) = ?
(k)rs,i .
Simultaneously, de-pendency of words on their roles can be representedas a set of K multinomial distributions ?
over |L|words, such that P (ws,i|rs,i = k) = ?
(k)ws,i .
To makepredictions about new sentences, we need to assumea prior distribution on the parameters ?
(k) and ?
(k).We place a Dirichlet prior D with the vector ofK hyper-parameters ?
on a multinomial distribu-tion ?
(k) ?
D(?)
and with the vector of |L| hyper-parameters ?
on a multinomial distribution ?
(k) ?D(?).
In general, D is not restricted to be Dirichletdistribution.
It could be any distribution over dis-crete children, such as logistic normal.
In this paper,we focus only on Dirichlet as a conjugate prior tothe multinomial distribution and derive the learningalgorithm under this assumption.The choice of the child role depends only on itsparent role, i.e.
child roles with the same parent aremutually independent.
This property is especiallyimportant for the learning algorithm (Section 3) andalso for searching the most probable trees (Section4).
We do not place any assumption on the length ofthe sentence Ns or on how many children the parentnode is expected to have.3 Parameter EstimationIn this section we present the learning algorithm forLTLM.
The goal is to estimate ?
and ?
in a waythat maximizes the predictive ability of the model(generates the corpus with maximal joint probabilityP (w)).Let ?k(i,j) be an operation that changes the treeGstoG?s?k(i,j) : Gs ?
G?s, (3)438such that the newly created tree G?
(V ?s,E?s, r?s)consists of:?
V ?s = V s.?
E?s = (Es \ {(hs(i), i)}) ?
{(j, i)}.?
r?s,a ={rs,a for a 6= ik for a = i , where 0 ?
a ?
Ns.It means that we change the role of the selectednode i so that rs,i = k and simultaneously wechange the parent of this node to be j.
We call thisoperation a partial change.The newly created graph G?
must satisfy all con-ditions presented in Section 2, i.e.
it is a projec-tive dependency tree rooted in the node 0.
Thus notall partial changes ?k(i,j) are possible to perform ongraphGs.Clearly, for the sentence s there is at mostNs(1+Ns)2 parent changes1.To estimate the parameters of LTLM we applyGibbs sampling and gradually sample ?k(i,j) for treesGs.
For doing so we need to determine the posteriorpredictive distribution2G?s ?
P (?k(i,j)(Gs)|w,G), (4)from which we will sample partial changes to updatethe trees.
In the equation, G denote the sequence ofall trees for given sentences w and G?s is a result ofone sampling.
In the following text we derive thisequation under assumptions from Section 2.The posterior predictive distribution of Dirichletmultinomial has the form of additive smoothing thatis well known in the context of language modeling.The hyper-parameters of Dirichlet prior determinehow much is the predictive distribution smoothed.Thus the predictive distribution for the word-in-roledistribution can be expressed asP (ws,i|rs,i,w\s,i, r\s,i) =n(ws,i|rs,i)\s,i + ?n(?|rs,i)\s,i + |L|?, (5)1The most parent changes are possible for the special caseof the tree, where each node i has parent i ?
1.
Thus for eachnode i we can change its parent to any node j < i and keep theprojectivity of the tree.
That is Ns(1+Ns)2 possibilities.2The posterior predictive distribution is the distribution ofan unobserved variable conditioned by the observed data, i.e.P (Xn+1|X1, ..., Xn), where Xi are i.i.d.
(independent andidentically distributed random variables).where n(ws,i|rs,i)\s,i is the number of times the rolers,i has been assigned to the word ws,i, exclud-ing the position i in the s-th sentence.
The sym-bol ?
represents any word in the vocabulary so thatn(?|rs,i)\s,i =?l?L n(l|rs,i)\s,i .
We use the symmetricDirichlet distribution for the word-in-role probabili-ties as it could be difficult to estimate the vector ofhyper-parameters ?
for large word vocabulary.
Inthe above mentioned equation, ?
is a scalar.The predictive distribution for the role-by-roledistribution isP(rs,i|rs,hs(i), r\s,i)=n(rs,i|rs,hs(i))\s,i + ?rs,in(?|rs,hs(i))\s,i +K?k=1?k.
(6)Analogously to the previous equation,n(rs,i|rs,hs(i))\s,i denote the number of times therole rs,i has the parent role rs,hs(i), excluding theposition i in the s-th sentence.
The symbol ?represents any possible role to make the probabilitydistribution summing up to 1.
We assume anasymmetric Dirichlet distribution.We can use predictive distributions of above men-tioned Dirichlet multinomials to express the jointprobability that the role at position i is k (rs,i = k)with parent at position j conditioned on current val-ues of all variables, except those in position i in thesentence sP (rs,i = k, j|w, r\s,i) ?P (ws,i|rs,i = k,w\s,i, r\s,i)?
P (rs,i = k|rs,j , r\s,i)?
?a:hs(a)=iP (rs,a|rs,i = k, r\s,i).
(7)The choice of the node i role affects the word thatis produced by this role and also all the child rolesof the node i.
Simultaneously, the role of the nodei depends on its parent j role.
Formula 7 is derivedfrom the joint probability of a sentence s and a treeGs, where all probabilities which do not depend onthe choice of the role at position i are removed andequality is replaced by proportionality (?
).We express the final predictive distribution forsampling partial changes ?k(i,j) as439P (?k(i,j)(Gs)|w,G) ?P (rs,i = k, j|w, r\s,i)P (rs,i, hs(i)|w, r\s,i)(8)that is essentially the fraction between the jointprobability of rs,i and its parent after the partialchange and before the partial change (conditionedon all other variables).
This fraction can be in-terpreted as the necessity to perform this partialchange.We investigate two strategies of sampling partialchanges:?
Per sentence: We sample a single partialchange according to Equation 8 for each sen-tence in the training corpus.
It means duringone pass through the corpus (one iteration) weperform S partial changes.?
Per position: We sample a partial change foreach position in each sentence.
We perform intotalN =?Ss=1Ns partial changes during onepass.
Note that the denominator in Equation 8is constant for this strategy and can be removed.We compare both training strategies in Section 6.After enough training iterations, we can estimate theconditional probabilities ?
(k)l and ?
(p)k from actualsamples as?
(k)l ?n(ws,i=l|rs,i=k) + ?n(?|rs,i=k) + |L|?
(9)?
(p)k ?n(rs,i=k|rs,hs(i)=p) + ?kn(?|rs,hs(i)=p) +K?m=1?m.
(10)These equations are similar to equations 5 and 6, buthere the counts n do not exclude any position in acorpus.Note that in the Gibbs sampling equation, weassume that the Dirichlet parameters ?
and ?
aregiven.
We use a fixed point iteration technique de-scribed in (Minka, 2003) to estimate them.4 InferenceIn this section we present two approaches for search-ing the most probable tree for a given sentence as-suming we have already estimated the parameters ?and ?.
(a) The root has two or more children.
(b) The root has only one child.Figure 2: Searching the most probable subtrees.4.1 Non-deterministic InferenceWe use the same sampling technique as for estimat-ing parameters (Equation 8), i.e.
we iteratively sam-ple the partial changes ?k(i,j).
However, we use equa-tions 9 and 10 for predictive distributions of Dirich-let multinomials instead of 5 and 6.
In fact, theseequations correspond to the predictive distributionsover the newly added wordws,i with the role rs,i intothe corpus, conditioned on w and r. This samplingtechnique rarely finds the best solution, but often itis very near.4.2 Deterministic InferenceHere we present the deterministic algorithm thatguarantees to find the most probable tree for a givensentence.
We were inspired by Cocke-Younger-Kasami (CYK) algorithm (Lange and Lei?, 2009).Let T ns,a,c denote the subtree of Gs (subgraphof Gs that is also a tree) containing subsequenceof nodes {a, a + 1, ..., c}.
The superscript n de-notes the number of children the root of this sub-tree has.
We denote the joint probability of a sub-tree from position a to position c with the cor-responding words conditioned by the root role kas Pn({ws,i}ci=a,T ns,a,c|k).
Our goal is to findthe tree Gs = T 1+s,0,Ns that maximizes probabilityP (ws,Gs) = P 1+({ws,i}Nsi=0,T 1+s,0,Ns |0).Similarly to CYK algorithm, our approach fol-440lows bottom-up direction and goes through all pos-sible subsequences for a sentence (sequence ofwords).
At the beginning, the probabilities for sub-sequences of length 1 (i.e.
single words) are calcu-lated as P 1+({ws,a},T 1+s,a,a|k) = P (ws,a|rs,a = k).Once it has considered subsequences of length 1, itgoes on to subsequences of length 2, and so on.Thanks to mutual independence of roles under thesame parent, we can find the most probable subtreewith the root role k and with at least two root chil-dren according toP 2+({ws,i}ci=a,T 2+s,a,c|k) = maxb:a<b<c[P 1+({ws,i}bi=a,T 1+s,a,b|k)?P 1+({ws,i}ci=b+1,T 1+s,b+1,c|k)].
(11)It means we merge two neighboring subtrees withthe same root role k. This is the reason why the newsubtree has at least two root children.
This formulais visualized on Figure 2a.
Unfortunately, this doesnot cover all subtree cases.
We find the most proba-ble tree with only root child as followsP 1({ws,i}ci=a,T 1s,a,c|k) = maxb,m:a?b?c,1?m?K[P (ws,b|rs,b = m)?
P (rs,b = m|k)?P 1+({ws,i}b?1i=a ,T 1+s,a,b?1|m)?P 1+({ws,i}ci=b+1,T 1+s,b+1,c|m)].
(12)This formula is visualized on Figure 2b.To find the most probable subtree no matter howmany children the root has, we need to take themaximum from both mentioned equations P 1+ =max(P 2+, P 1).The algorithm has complexity O(N3sK2), i.e.
ithas cubic dependence on the length of the sentenceNs.5 Side-dependent LTLMUntil now, we presented LTLM in its simplified ver-sion.
In role-by-role probabilities (role conditionedon its parent role) we did not distinguish whether therole is on the left side or the right side of the parent.However, this position keeps important informationabout the syntax of words (and their roles).We assume separate multinomial distributions ?
?for roles that are on the left and ??
for roles on theright.
Each of them has its own Dirichlet prior withhyper-parameters ??
and ?
?, respectively.
The pro-cess of estimating LTLM parameters is almost thesame.
The only difference is that we need to rede-fine the predictive distribution for the role-by-roledistribution (Equation 6) to include only counts ofroles on the appropriate side.
Also, every time therole-by-role probability is used we need to distin-guish sides:P (rs,i|rs,hs(i)) ={??
(rs,hs(i))rs,i for i < hs(i))??
(rs,hs(i))rs,i for i > hs(i)).
(13)In the following text we always assume the side-dependent LTLM.6 Experimental Results and DiscussionIn this section we present experiments with LTLMon two languages, English (EN) and Czech (CS).As a training corpus we use CzEng 1.0 (Bojaret al, 2012) of the sentence-parallel Czech-Englishcorpus.
We choose this corpus because it containsmultiple domains, it is of reasonable length, and itis parallel so we can easily provide comparison be-tween both languages.
The corpus is divided into100 similarly-sized sections.
We use parts 0?97 fortraining, the part 98 as a development set, and thelast part 99 for testing.We have removed all sentences longer than 30words.
The reason was that the complexity of thelearning phase and the process of searching mostprobable trees depends on the length of sentences.It has led to removing approximately a quarter ofall sentences.
The corpus is available in a tokenizedform so the only preprocessing step we use is lower-casing.
We keep the vocabulary of 100,000 most fre-quent words in the corpus for both languages.
Theless frequent words were replaced by the symbol<unk>.
Statistics for the final corpora are shownin Table 1.We measure the quality of LTLM by perplexitythat is the standard measure used for LMs.
Perplex-ity is a measure of uncertainty.
The lower perplexitymeans the better predictive ability of the LM.441Corpora Sentences Tokens OOV rateEN train 11,530,604 138,034,779 1.30%EN develop.
117,735 1,407,210 1.28%EN test 117,360 1,405,106 1.33%CS train 11,832,388 133,022,572 3.98%CS develop.
120,754 1,353,015 4.00%CS test 120,573 1,357,717 4.03%Table 1: Corpora statistics.
OOV rate denotes theout-of-vocabulary rate.Figure 3: Learning curves of LTLM for both Englishand Czech.
The points in the graphs represent theperplexities in every 100th iteration.During the process of parameter estimation wemeasure the perplexity of joint probability of sen-tences and their trees defined as PPX(P (w,G)) =N?1P (w,G) , where N is the number of all words inthe training data w.As we describe in Section 3, there are two ap-proaches for the parameter estimation of LTLM.During our experiments, we found that the per-position strategy of training has the ability to con-verge faster, but to a worse solution compared to theper-sentence strategy which converges slower, but toa better solution.We train LTLM by 500 iterations of the per-position sampling followed by another 500 iterationsof the per-sentence sampling.
This proves to be effi-Model EN CS2-gram MKN 165.9 272.03-gram MKN 67.7 99.34-gram MKN 46.2 73.5300n RNNLM 51.2 69.44-gram LWLM 52.7 81.5PoS STLM 455.7 747.31000r STLM 113.7 211.01000r det.
LTLM 54.2 111.14-gram MKN + 300n RNNLM 36.8 (-20.4%) 49.5 (-32.7%)4-gram MKN + 4-gram LWLM 41.5 (-10.2%) 62.4 (-15.1%)4-gram MKN + PoS STLM 42.9 (-7.1%) 63.3 (-13.9%)4-gram MKN + 1000r STLM 33.6 (-27.3%) 50.1 (-31.8%)4-gram MKN + 1000r det.
LTLM 24.9 (-43.1%) 37.2 (-49.4%)Table 2: Perplexity results on the test data.
Thenumbers in brackets are the relative improvementscompared with standalone 4-gram MKN LM.cient in both aspects, the reasonable speed of con-vergence and the satisfactory predictive ability ofthe model.
The learning curves are showed on Fig-ure 3.
We present the models with 10, 20, 50, 100,200, 500, and 1000 roles.
The higher role cardinal-ity models were not possible to create because ofthe very high computational requirements.
Similarlyto the training of LTLM, the non-deterministic in-ference uses 100 iterations of per-position samplingfollowed by 100 iterations of per-sentence sampling.In the following experiments we measure howwell LTLM generalizes the learned patterns, i.e.how well it works on the previously unseen data.Again, we measure the perplexity, but of prob-ability P (w) for mutual comparison with differ-ent LMs that are based on different architectures(PPX(P (w)) = N?1P (w) ).To show the strengths of LTLM we compareit with several state-of-the-art LMs.
We experi-ment with Modified Kneser-Ney (MKN) interpola-tion (Chen and Goodman, 1998), with RecurrentNeural Network LM (RNNLM) (Mikolov et al,2010; Mikolov et al, 2011)3, and with LWLM (De-schacht et al, 2012)4.
We have also created syntac-tic dependency tree based LM (denoted as STLM).Syntactic dependency trees for both languages areprovided within CzEng corpus and are based on3Implementation is available at http://rnnlm.org/.Size of the hidden layer was set to 300 in our experiments.
Itwas computationally intractable to use more neurons.4Implementation is available at http://liir.cs.kuleuven.be/software.php.442EN CSModel\roles 10 20 50 100 200 500 1000 10 20 50 100 200 500 1000STLM 408.5 335.2 261.7 212.6 178.9 137.8 113.7 992.7 764.2 556.4 451.0 365.9 265.7 211.0non-det.
LTLM 329.5 215.1 160.4 126.5 105.6 86.7 78.4 851.0 536.6 367.4 292.6 235.2 186.1 157.6det.
LTLM 252.4 166.4 115.3 92.0 75.4 60.9 54.2 708.5 390.2 267.8 213.2 167.9 133.5 111.14-gram MKN + STLM 42.7 41.6 39.9 37.9 36.3 34.9 33.6 67.5 65.1 61.4 58.3 55.5 52.4 50.14-gram MKN + non-det.
LTLM 41.1 38.0 35.2 32.7 30.7 28.9 27.8 65.8 59.4 55.1 51.1 47.5 43.7 41.34-gram MKN + det.
LTLM 39.9 36.4 32.8 30.3 28.1 26.0 24.9 64.4 56.1 51.5 47.3 43.4 39.9 37.2Table 3: Perplexity results on the test data for LTLMs and STLMs with different number of roles.
Deter-ministic inference is denoted as det.
and non-deterministic inference as non-det.MST parser (McDonald et al, 2005).
We use thesame architecture as for LTLM and experiment withtwo approaches to represent the roles.
Firstly, theroles are given by the part-of-speech tag (denoted asPoS STLM).
No training is required, all informationcome from CzEng corpus.
Secondly, we learn theroles using the same algorithm as for LTLM.
Theonly difference is that the trees are kept unchanged.Note that both deterministic and non-deterministicinference perform almost the same in this model sowe do not distinguish between them.We combine baseline 4-gram MKN model withother models via linear combination (in the tablesdenoted by the symbol +) that is simple but very ef-ficient technique to combine LMs.
Final probabilityis then expressed asP (w) =S?s=1Ns?i=1[?P LM1 + (??
1)P LM2].
(14)In the case of MKN the probability PMKN is theprobability of a word ws,i conditioned by 3 previouswords with MKN smoothing.
For LTLM or STLMthis probability is defined asP LTLM(ws,i|rs,hs(i)) =K?k=1P (ws,i|rs,i = k)P (rs,i = k|rs,hs(i)).
(15)We use the expectation maximization algorithm(Dempster et al, 1977) for the maximum likelihoodestimate of ?
parameter on the development part ofthe corpus.
The influence of the number of roleson the perplexity is shown in Table 3 and the final0.10.20.30.40.50.610 20 50 100 200 500 1000Model weightRolesEN det.
LTLMCS det.
LTLMEN non-det.
LTLMCS non-det.
LTLMEN STLMCS STLMFigure 4: Model weights optimized on developmentdata when interpolated with 4-gram MKN LM.results are shown in Table 2.
Note that these per-plexities are not comparable with those on Figure3 (PPX(P (w)) vs. PPX(P (w,G))).
Weights ofLTLM and STLM when interpolated with MKN LMare shown on Figure 4.From the tables we can see several importantfindings.
Standalone LTLM performs worse thanMKN on both languages, however their combi-nation leads to dramatic improvements comparedwith other LMs.
Best results are achieved by 4-gram MKN interpolated with 1000 roles LTLM andthe deterministic inference.
The perplexity wasimproved by approximately 46% on English and49% on Czech compared with standalone MKN.The deterministic inference outperformed the non-deterministic one in all cases.
LTLM also signifi-443everything has beauty , but not everyone sees it .it ?s one , but was he saw him .that is thing ; course it i made it !let was life ?
though not she found her ...there knows name - or this they took them ?something really father ... perhaps that that gave his whatnothing says mother : and the it told me ?everything comes way maybe now who felt a howhere does wife ( although had you thought out whysomeone gets place ?
yet <unk> someone knew that ?god has idea naught except all which heard himself -Table 4: Ten most probable word substitutions on each position in the sentence ?Everything has beauty, butnot everyone sees it.?
produced by 1000 roles LTLM with the deterministic inference.cantly outperformed STLM where the syntactic de-pendency trees were provided as a prior knowledge.The joint learning of syntax and semantics of a sen-tence proved to be more suitable for predicting thewords.An in-depth analysis of semantic and syntacticproperties of LTLM is beyond the scope of this pa-per.
For better insight into the behavior of LTLM,we show the most probable word substitutions forone selected sentence (see Table 4).
We can seethat the original words are often on the front po-sitions.
Also it seems that LTLM is more syntac-tically oriented, which confirms claims from (Levyand Goldberg, 2014; Pado?
and Lapata, 2007), but todraw such conclusions a deeper analysis is required.The properties of the model strongly depends onthe number of distinct roles.
We experimented withmaximally 1000 roles.
To catch the meaning of var-ious words in natural language, more roles may beneeded.
However, with our current implementation,it was intractable to train LTLM with more roles ina reasonable time.
Training 1000 roles LTLM tookup to two weeks on a powerful computational unit.7 Conclusion and Future WorkIn this paper we introduced the Latent Tree Lan-guage Model.
Our model discovers the latent treestructures hidden in natural text and uses them topredict the words in a sentence.
Our experimentswith English and Czech corpora showed dramaticimprovements in the predictive ability comparedwith standalone Modified Kneser-Ney LM.
Our Javaimplementation is available for research purposes athttps://github.com/brychcin/LTLM.It was beyond the scope of this paper to explic-itly test the semantic and syntactic properties of themodel.
As the main direction for future work weplan to investigate these properties for example bycomparison with human-assigned judgments.
Also,we want to test our model in different NLP tasks(e.g.
speech recognition, machine translation, etc.
).We think that the role-by-role distribution shoulddepend on the distance between the parent and thechild, but our preliminary experiments were not metwith success.
We plan to elaborate on this assump-tion.
Another idea we want to explore is to usedifferent distributions as a prior to multinomials.For example, Blei and Lafferty (2006) showed thatthe logistic-normal distribution works well for topicmodeling because it captures the correlations be-tween topics.
The same idea might work for roles.AcknowledgmentsThis publication was supported by the projectLO1506 of the Czech Ministry of Education, Youthand Sports.
Computational resources were providedby the CESNET LM2015042 and the CERIT Sci-entific Cloud LM2015085, provided under the pro-gramme ?Projects of Large Research, Development,and Innovations Infrastructures?.
Lastly, we wouldlike to thank the anonymous reviewers for their in-sightful feedback.ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-444guage model.
Journal of Machine Learning Research,3:1137?1155, March.David M. Blei and John D. Lafferty.
2006.
Correlatedtopic models.
In In Proceedings of the 23rd Interna-tional Conference on Machine Learning, pages 113?120.
MIT Press.Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???Mars??
?k, Michal Nova?k, Martin Popel, and Ales?
Tam-chyna.
2012.
The joy of parallelism with czeng 1.0.In Proceedings of the Eight International Conferenceon Language Resources and Evaluation (LREC?12),Istanbul, Turkey, may.
European Language ResourcesAssociation (ELRA).Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18:467?479.Toma?s?
Brychc?
?n and Miloslav Konop??k.
2014.
Semanticspaces for improving language modeling.
ComputerSpeech & Language, 28(1):192?209.Toma?s?
Brychc?
?n and Miloslav Konop??k.
2015.
Latentsemantics in language models.
Computer Speech &Language, 33(1):88?108.Stanley F. Chen and Joshua T. Goodman.
1998.
Anempirical study of smoothing techniques for languagemodeling.
Technical report, Computer Science Group,Harvard University.Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.2009.
Logistic normal priors for unsupervised prob-abilistic grammar induction.
In Advances in NeuralInformation Processing Systems 21, pages 1?8.Arthur P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the emalgorithm.
Journal of the Royal Statistical Society.
Se-ries B, 39(1):1?38.Koen Deschacht, Jan De Belder, and Marie-FrancineMoens.
2012.
The latent words language model.Computer Speech & Language, 26(5):384?409.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.William P. Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages101?109, Boulder, Colorado, June.
Association forComputational Linguistics.Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency parsing.
Synthesis Lectures on Hu-man Language Technologies, 2(1):1?127.Martin Lange and Hans Lei?.
2009.
To cnf or not tocnf?
an efficient yet presentable version of the cykalgorithm.
Informatica Didactica, 8.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 302?308,Baltimore, Maryland, June.
Association for Computa-tional Linguistics.David Marec?ek and Milan Straka.
2013.
Stop-probability estimates computed on a large corpus im-prove unsupervised dependency parsing.
In Proceed-ings of the 51st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 281?290, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Sven Martin, Jorg Liermann, and Hermann Ney.
1998.Algorithms for bigram and trigram word clustering.Speech Communication, 24(1):19?37.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In Proceedings of theConference on Human Language Technology and Em-pirical Methods in Natural Language Processing, HLT?05, pages 523?530, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Toma?s?
Mikolov, Martin Karafia?t, Luka?s?
Burget, JanC?ernocky?, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In Proceedingsof the 11th Annual Conference of the InternationalSpeech Communication Association (INTERSPEECH2010), volume 2010, pages 1045?1048.
InternationalSpeech Communication Association.Toma?s?
Mikolov, Stefan Kombrink, Luka?s?
Burget, JanC?ernocky?, and Sanjeev Khudanpur.
2011.
Exten-sions of recurrent neural network language model.In Proceedings of the IEEE International Conferenceon Acoustics, Speech, and Signal Processing, pages5528?5531, Prague Congress Center, Prague, CzechRepublic.Thomas P. Minka.
2003.
Estimating a dirichlet distribu-tion.
Technical report.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199, June.Martin Popel and David Marec?ek.
2010.
Perplex-ity of n-gram and dependency language models.
InProceedings of the 13th International Conference onText, Speech and Dialogue, TSD?10, pages 173?180,Berlin, Heidelberg.
Springer-Verlag.Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,and Christopher D. Manning.
2010.
Viterbi training445improves unsupervised dependency parsing.
In Pro-ceedings of the Fourteenth Conference on Computa-tional Natural Language Learning, pages 9?17, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang,and Daniel Jurafsky.
2011.
Unsupervised dependencyparsing without gold part-of-speech tags.
In Proceed-ings of the 2011 Conference on Empirical Methods inNatural Language Processing, pages 1281?1290, Ed-inburgh, Scotland, UK., July.
Association for Compu-tational Linguistics.Edward W. D. Whittaker and Philip C. Woodland.
2003.Language modelling for russian and english usingwords and classes.
Computer Speech & Language,17(1):87?104.446
