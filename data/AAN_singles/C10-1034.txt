Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 295?303,Beijing, August 2010An Empirical Study on Learning to Rank of Tweets1Yajuan Duan* 2Long Jiang 2Tao Qin 2Ming Zhou 2Heung-Yeung Shum1Department of Computer Science and TechnologyUniversity of Science and Technology of China2Microsoft Research Asia{v-yaduan,longj,taoqin,mingzhou,hshum}@microsoft.comAbstractTwitter, as one of the most popularmicro-blogging services, provides largequantities of fresh information includingreal-time news, comments, conversation,pointless babble and advertisements.Twitter presents tweets in chronologicalorder.
Recently, Twitter introduced anew ranking strategy that considerspopularity of tweets in terms of numberof retweets.
This ranking method,however, has not taken into accountcontent relevance or the twitter account.Therefore a large amount of pointlesstweets inevitably flood the relevanttweets.
This paper proposes a newranking strategy which uses not only thecontent relevance of a tweet, but also theaccount authority and tweet-specificfeatures such as whether a URL link isincluded in the tweet.
We employlearning to rank algorithms to determinethe best set of features with a series ofexperiments.
It is demonstrated thatwhether a tweet contains URL or not,length of tweet and account authority arethe best conjunction.11 IntroductionTwitter provides a platform to allow users topost text messages known as tweets to updatetheir followers with their findings, thinking andcomments on some topics (Java et al, 2007).
*The work was done when the first author was intern atMicrosoft Research AsiaThe searched tweets are presented by Twitter inchronological order except the first three, whichare ranked by considering popularity of tweets interms of the number of retweets.This ranking method, however, has not takeninto account the content relevance and twitteraccount; inevitably, a large amount of pointlesstweets (Pear Analytics, 2009) may flood therelevant tweets.
Although this ranking methodcan provide fresh information to tweet users,users frequently expect to search relevant tweetsto the search queries.
For example, considersomeone researching consumer responsestoward the iPad.
He or she would like to findtweets with appropriate comments such as iPadis great or you can find many useful features ofiPad, rather than tweets with irrelevant comment,even if they are most recent or popular.Moreover, neither Twitter?s currentchronological order based ranking nor therecently introduced popularity based ranking canavoid spam.
A developer can accumulatehundreds of thousands of followers in a day orso.
At the same time, it is not difficult forspammers to create large quantities of retweets.By contrast, content relevance ranking caneffectively prevent spammers from cheating.Different from ranking tweets throughchronological order and popularity, a contentrelevance strategy considers manycharacteristics of a tweet to determine itsranking level.
Thus it is difficult for spammersto break the ranking system by simple methodssuch as increasing retweet count or number offollowers.In this paper, we propose a method to rank thetweets which outputs the matched tweets basedon their content relevance to the query.
We295investigate the effects of content features andnon-content features and produce a rankingsystem by a learning to rank approach.With a series of experiments, we determinedthe best set of features and analyzed the effectsof each of individual feature.
We provideempirical evidence supporting the followingclaims,?
Account authority, length of tweet andwhether a tweet contains a URL are the topthree effective features for tweet ranking,where containing a URL is the mosteffective feature.?
We find an effective representation ofaccount authority: the number of times theauthor was listed by other users.
We findthrough experiments that this representationis better than the widely adopted number offollowers.2 Related Work2.1 Real-time SearchAt present, a number of web sites offer theso-called real-time search service which mainlyreturns real-time posts or shared links, videosand images obtained from micro-bloggingsystems or other medium according to the user?squery.
We investigate the ranking method usedby these web sites.
From their self-introductionpage, we find four main criteria for rankingreal-time posts.
They are posting time, accountauthority, topic popularity and contentrelevance.Specifically, Twitter maintains a specializedsearch engine which ranks tweets according toposting time and topic popularity.
In addition,Google, Twazzup2 and Chirrps3 rank real-timetweets by posting time.
While the last one alsoranks tweets by popularity, which is measuredby retweet count.Tweefind4 ranks search result according toauthority of authors which depends on howpopular, relevant, and active the author is.Additionally, Twitority5 rank tweets by authorauthority as well.2 Twazzup: http://www.twazzup.com/3 Chirrps: http://chirrps.com/4 Tweefind: http://www.tweefind.com/5 Twitority: http://www.twitority.com/Bing and CrowdEye6 rank tweets by postingtime or content relevance.
Bing takes authorsauthority, retweet count and freshness intoconsideration while measuring the relevance.
Todetermine the relevance of a tweet, CrowdEyeconsiders a number of factors including contentrelevance and author influence which appears torely heavily on the number of followers anauthor has.
It turns out that the number offollowers is not a very reasonable measure of theinfluence of an account according to ourexperimental results.2.2 Twitter RecommendationBesides tweet search, recently some researchershave focused on twitter recommendation system.Chen et al (2010) presented an approach torecommend URLs on Twitter as a means tobetter direct user attention in informationstreams.
They designed the recommender takingthree separate dimensions into consideration:content source, topic interest and social voting.Sun et al (2009) proposed a diffusion-basedmicro-blogging recommendation frameworkaiming to recommend micro-blogs duringcritical events via optimizing story coverage,reading effort and delay time of a story.
The keypoint of this method is to construct an exactdiffusion graph for micro-blogging, which isdifficult due to the presence of extensiveirrelevant personal messages and spam.2.3 Blog Search and Forum SearchAnother related topic is blog search and forumsearch.
Recently, many approaches for blogsearch and forum search have been developed,which include learning to rank methods andlink-based method.Learning to rank approachXi et al (2004) used features from the threadtrees of forums, authors, and lexical distributionwithin a message thread and then applied LinearRegression and Support Vector Machine (SVM)to train the ranking function.
Fujimura et al(2005) exploited provisioning link andevaluation link between bloggers and blogentries, and scored each blog entry by weightingthe hub and authority scores of the bloggers.Link-Based approach6 CrowdEye: http://www.crowdeye.com/296Kritikopoulos et al (2006) introducedsimilarities among bloggers and blogs into blogranking.
This method enabled the assignment ofa higher score to the blog entry published by ablogger who has already accepted a lot ofattention.
Xu and Ma (2006) built a topichierarchy structure through content similarity.Liu et al (2007) presented a newsgroupstructure-based approach PostRank which builtposting trees according to response relationshipbetween postings.Chen et al (2008) proposed a posting rankalgorithm which built link graphs according toco-replier relationships.
This kind of methodexploits different types of structures amongpostings and improved the performance oftraditional link-based ranking algorithm forforum search.
However, it is difficult to rankpostings which only have a few words simplybased on content by using FGRank algorithm.And PostingRank approach relies too much onreply relations which are more likely to sufferfrom topic excursion.Although approaches proposed above performeffectively in forum search and blog search, theyare not appropriate for twitter search becausetweets are usually shorter and more informalthan blogs.
Furthermore, it does not have theexplicit hierarchy structure of newsgroupmessages on forums.
In addition, tweets possessmany particular characteristics that blog andforum do not have.3 Overview of Our ApproachTo generate a good ranking function whichprovides relevant search results and preventsspammers?
cheating activities, we analyze bothcontent features and authority features of tweetsand determine effective features.
We adoptlearning to rank algorithms which havedemonstrated excellent power in addressingvarious ranking problems of search engines.3.1 Learning to Rank FrameworkLearning to Rank is a data-driven approachwhich integrates a bag of features in the modeleffectively.
Figure 1 shows the paradigm oflearning for tweet ranking.At the first step, we prepare the training andtest corpus as described in Section 5.
Then weextract features from the training corpus.RankSVM algorithm (Joachims Thorsten, 1999) isused to train a ranking model from the trainingcorpus.
Finally, the model is evaluated by thetest corpus.Figure 1.
General Paradigm of Learning forTweets Ranking3.2 Features for Tweets RankingOne of the most important tasks of a learning torank system is the selection of a feature set.
Weexploit three types of features for tweet ranking.1) Content relevance features refer to thosefeatures which describe the contentrelevance between queries and tweets.2) Twitter specific features refer to thosefeatures which represent the particularcharacteristics of tweets, such as retweetcount and URLs shared in tweet.3) Account authority features refer to thosefeatures which represent the influence ofauthors of the tweets in Twitter (Leavitt et al,2009).In the next section, we will describe thesethree types of features in detail.4 Feature Description4.1 Content Relevance FeaturesWe used three content relevance features, OkapiBM25 (Robertson et al, 1998), similarity ofcontents and length of tweet.Okapi BM25 score measures the contentrelevance between query Q and tweet T. Thestandard BM25 weighting function is:(1)297where Length(T) denotes the length of T andrepresents average length of tweet incorpus.
IDF(  ) is Inverse Document Frequency.Similarity of contents estimates thepopularity of documents in the corpus (Song etal., 2008).
In our case, it measures how manytweets of the query are similar in content withthe current tweet.
We calculate a cosinesimilarity score for every pair of tweets, and thefinal similarity score for tweet     in     iscomputed by the following formula:(2)Where     represents the TFIDF vector ofand     refers to tweets collection of query   .Length is measured by the number of wordsthat a tweet contains.
Intuitively, a long sentenceis apt to contain more information than a shortone.
We use length of tweet as a measure of theinformation richness of a tweet.4.2 Twitter?s Specific FeaturesTweets have many special characteristics.
Weexploit these characteristics and extract sixtwitter specific features as listed in Table 1.Feature DescriptionURL Whether the tweet contains a URLURL Count Frequency of URLs in corpusRetweetCountHow many times has this tweet beenretweetedHash tagScoreSum of frequencies of the top-n hash tagsappeared in the tweetReply Is the current tweet a reply tweetOOV Ratio of words out of vocabularyTable 1.
Twitter Specific FeaturesFigure 2.
A Tweet ExampleURL & URL Count: Twitter allows users toinclude URL as a supplement in their tweets.The tweet in Figure 2 contains URLhttp://myloc.me/43tPj which leads to a mapindicating where the publisher located.URL is a binary feature.
It is assigned 1 whena tweet contains at least one URL, otherwise 0.URL Count estimates the number of times thatthe URL appears in the tweet corpus.Retweet Count: Twitter users can forward atweet to his or her followers with or withoutmodification on the forwarded tweets, which iscalled retweet on Twitter.
A retweeted tweetusually includes an RT tag.
Generally, sentencesbefore RT are comments of the retweeter andsentences after RT are the original content,perhaps with some modifications.
Here we onlyconsider tweets including RT with the originalcontent unmodified.
Retweet count is defined asthe number of times a tweet is retweeted.
InFigure 2, original tweet Satu-slank#nowplaying !!
http://myloc.me/43tPj isretweeted once.Hash Tag Score: Publishers are allowed toinsert hash tags into their tweets to indicate thetopic.
In Figure 2, #nowplaying is a hash tag.
Wecollect hash tags appearing in the tweets of everyquery and sort them in descending orderaccording to frequency.
Tag frequency for tweetof query    is computed from normalizedfrequency of top-n tags.
(3)Where    is the normalization factor.represents the frequent of      incorpus.
And       denotes the tag collectionextracted from    .Reply: This is a binary feature.
It is 1 whenthe tweet is a reply and 0 otherwise.
A tweetstarting with a twitter account is regarded as areply tweet in our experiment.
Figure 3 shows anexample.Figure 3.
Reply TweetOOV: This feature is used to roughlyapproximate the language quality of tweets.Words out of vocabulary in Twitter includespelling errors and named entities.
According toa small-scale investigation, spelling errorsaccount for more than 90% of OOVs excludingcapitalized words, tags, mentions of users and298URLs.
We use a dictionary with 0.5 millionentries to compute the ratio of OOVs in a tweet.
(4)4.3 Account Authority FeaturesThere are three important relations betweenusers in Twitter: follow, retweet, and mention.Additionally, users are allowed to classify theirfollowings into several lists based on topics.
Wemeasured the influence of users?
authorities ontweets based on the following assumptions:?
Users who have more followers and havebeen mentioned in more tweets, listed inmore lists and retweeted by more importantusers are thought to be more authoritative.?
A tweet is more likely to be an informativetweet rather than pointless babble if it isposted or retweeted by authoritative users.Figure 4.
PageRank Algorithm for CalculatingPopularity Score for UsersIn order to distinguish the effect of the threerelations, we computed four scores for each userrepresenting the authority independently.?
Follower Score: number of followers a userhas.?
Mention Score: number of times a user isreferred to in tweets.?
List Score: number of lists a user appears in.?
Popularity Score: computed by PageRankalgorithm (Page et al, 1999) based onretweet relations.Following the retweet relationship amongusers, we construct a directed graph G (V, E).
Inour experiments, G is built from a tweetcollection including about 1.1 million tweets.
Vdenotes twitter users that appear in trainingexamples.
E is a set of directed edges.
If authorpublished the tweet   , and authorretweeted    after   , there exists an edge fromto   .
We call    original author andretweeter.
Figure 4 shows the PageRankalgorithm for calculating popularity scores fortwitter users.
In our experiment, damping factore was set to 0.8.
Like Dong et al (2010) did, wedefine three subtypes for each account authorityscore.
Table 2 presents features of accountauthority we use.Feature DescriptionSum_followerSum of follower scores of users whopublished or retweeted the tweetSum_popularitySum of popularity scores of users whopublished or retweeted the tweetSum_mentionSum of mention scores of users whopublished or retweeted the tweetSum_listSum of list scores of users who publishedor retweeted the tweetFirst_followerFollower score of the user who publishedthe tweetFirst_popularityPopularity score of the user who publishedthe tweetFirst_mentionMention score of the user who publishedthe tweetFirst_listList score of the user who published thetweetImportant_followerThe highest follower score of the user whopublished or retweeted the tweetImportant_popularityThe highest popularity score of the userwho published or retweeted the tweetImportant_mentionThe highest mention score of the user whopublished or retweeted the tweetImportant_listThe highest list score of the user whopublished or retweeted the tweetTable 2.
Account Authority Features for tweet5 Experiment Data and EvaluationWe introduce the data we used in experimentand the evaluation metrics in this section.5.1 DataWe analyze 140 hot searches on CrowdEyewithin a week.
They consist of big events,PageRank algorithm for calculating popularity scorefor users.Input: Directed Graph G of retweet relationshipDamping factor e.Output: popularity score for each userProcedure:Step 1: popularity score of all users are initialized as.Step 2: update the popularity score for users.denotes the collection of users whoretweeted   ?s tweet.is the number of times    has beenretweeted by   .is the number of users whose tweetshas retweeted.Step 3: Repeat the second step until all popularityscores will never change.299famous persons, new products, festivals, moviesand so on.
The most frequent types of hotsearches, which account for more than 81% ofall hot searches, are as follows:?
News: news about public figures andnews related to some places.?
Products: character description,promotion information and commentsabout products.?
Entertainment: mainly about movies,including film reviews and introductionsabout plots.We select 20 query terms as shown in Table 3,including 5 persons, 5 locations, 5 products and5 movie names.
Specifically, Locations aresampled from a list of American cities.
Personnames come from the hot search and hot trendsprovided by Twitter and CrowdEye.
Productsare sampled from the popular searches of 35product categories on eBay.
And movies areselected from a collection of recommendedmovies from 2005 to 2010.
We crawl 162,626English tweets for the selected queries betweenMarch 25, 2010 and April 2, 2010 from TwitterSearch.
After removing the repeated ones,159,298 tweets remained.Query type Query termsLocationsNew York, Nashville, Denver,Raleigh, LufkinPersonNamesObama, Bill Clinton, JamesCameron, Sandra Bullock, LeBronJamesproductsCorvette, iPad, Barbie, Harry Potter,Windows 7MoviesThe Dark Knight, up in the air, thehurt locker, Batman Begins, Wall ETable 3.
20 Query TermsRetweets are forwardings of correspondingoriginal tweets, sometimes with comments ofretweeters.
They are supposed to contain nomore information than the original tweets,therefore they drops out of ranking in this paper.We sample 500 tweets for each query from itsoriginal tweets collection and ask a human editorto label them with a relevance grade.
In order toensure the annotation is reasonable, we setmultiple search intentions for each queryreferring to the topics arising in the tweets aboutthe query in the corpus.
Specifically, forLocations, tweets describing news related to thelocation are relevant.
For people, what they havedone and the comments about them are regardedas relevant information.
For products, tweetsincluding feature description, promotion andcomments are considered relevant.
And formovies, tweets about comment on the movies,show time and tickets information are preferred.We apply four judgment grades on query-tweetpairs: excellent, good, fair and bad.
According tothe statistics, about half of the tweets in theexperiment data are labeled as bad.
Table 4presents the distribution for all grades.Grade Excellent Good Fair BadPercentage  20.9% 10.9% 16.9% 51.3%Min 2.4% 1.8% 4.0% 8.0%Max 69.8% 23.2% 54.4% 81.0%Table 4.
Tweet Distribution of Each Grade5.2 Evaluation MetricsThere are several metrics that are often used tomeasure the quality of rankings.
In this paper,we use Normalized Discount Cumulative Gain(NDCG) which can handle multiple levels ofrelevance as the evaluation metrics (Jarvelin andKekalainen, 2002).6 ResultsFive-fold cross-validation was used in ourexperiments.
We choose tweets of sixteenqueries (four from each query type) as thetraining data.
The remaining tweets are dividedinto evaluation data and validation data equally.6.1 Learning to Rank for Tweet RankingWe learn a ranking model by using a RankSVMalgorithm based on all features we extracted,which is denoted as RankSVM_Full.
In theexperiment, a toolkit named svmstruct 7implemented by Thorsten Joachims is used.Figure 5 shows the comparison between ourmethod which integrates three types of featuresand ranking through chronological order,account authority, and content relevanceindividually.In this experiment, Content Relevance ismeasured by BM25 score.
And Account7 SVMstruct: http://svmlight.joachims.org/svm_struct.html300Authority is approximated by the number offollowers of the user.
Figure 5 illustrates thatranking through content relevance is not aseffective as other methods.
This is because ourwork is essentially re-ranking on the result ofTwitter Search.
Hence almost all tweets includethe query term which makes it difficult todistinguish them by BM25 score.
Figure 5 alsoreveals that account authority is useful forranking tweet relevance; it outperforms rankingthrough chronological order and is competitiveto our model trained from all features.
Thisagrees with the assumption we made about theinfluence of user authorities on tweets.Figure 5.
Performance of Four Ranking Methods6.2 Feature SelectionAs the RankSVM_Full underperforms againstsome models trained from subsets of features,we use an advanced greedy feature selectionmethod and find the best feature conjunction toimprove the performance of RankSVM_full.Figure 6 shows the feature selection approach.Although greedy feature selection approach iscommonly used in many problems, it does notwork efficiently in addressing this problempartly for data sparseness.
It is always blockedby a local optimum feature set.
In order toresolve this problem, we first generate severalfeature sets randomly and run the greedyselection algorithm based the best one amongthem.
Finally, we find the best featureconjunction composed by URL, Sum_mention,First_List, Length, and Important_follower,from which a model is learnt denoted asRankSVM_Best.
Figure 7 illustrates that thismodel outperforms RankSVM_Full by about15.3% on NDCG@10.Figure 6.
Advanced Greedy Feature SelectionAlgorithmFigure 7.
Comparison between RankSVM_Fulland RankSVM_BestWe conduct a paired t-test betweenRankSVM_Best and each of other four rankingmethods on NDCG@10 of ten test queries.
Theresults demonstrate that RankSVM_Bestoutperforms ranking through time, accountauthority and content relevance respectivelywith a significance level of 0.01, andRankSVM_Full with a level of 0.05.6.3 Feature AnalysisWe are interested in which features in particularare highly valued by our model for tweet ranking.We evaluate the importance of each feature bythe decrement of performance when removingthe feature measured from RankSVM_Best.Figure 8 reveals the importance of each featurein our model.An advanced greedy feature selection algorithm.Input: All features we extracted.Output: the best feature conjunction BFCProcedure:Step1: Randomly generate 80 feature set F.Step 2: Evaluate every feature set in F and selectthe best one denoted by RBF.Features excluded those in RBF are denoted asEX_RBFStep 3: t = 0,BFC(t)=RBF;RepeatForeach feature in EX_RBFIf  Evaluation(BFC)< Evaluation(BFC, feature)BFC(t+1) = {BFC(t), feature}EX_RBF(t+1) = EX_RBF(t) ?
{feature}While BFC(t+1) ?
BFC(t)Note: Evaluation(BFC) refers to the performance ofranking function trained from features in BFC onvalidation data.301Figure 8.
Importance of Each FeatureWe observe from Figure 8 that URL is veryimportant for our model; without it theperformance declines seriously (with asignificance level of 0.001).
The reason may bethat URLs shared in tweets, which provide moredetailed information beyond the tweet?s 140characters, may be relevant to the query at a highprobability.Another useful feature is the number of liststhat the author of the tweet has been listed in.The performance of ranking decreases with asignificance level of 0.05 when removing it fromthe best feature combination.
However, otherfeatures do not show significant contribution.7 DiscussionOur experiment in section 6.2 demonstrates thatfeatures such as Hash tag Score and RetweetCount are not as effective as expected.
This maybe due to the small size of training data.
Wepresent an approach to learn an effective tweetsranker in a small dataset through featureselection.
However, 20 queries are not sufficientto train a powerful ranker for Twitter.In this study, to minimize the annotationeffort, for each test query, we only annotate thetweets containing the query (returned by TwitterSearch) and then used them for evaluation.
Withthis kind of evaluation, it is hard to completelyevaluate the significance of some features, suchas content relevance features.
In the future, wewill select more queries including both hotsearches and long tail searches, and select tweetsfor annotation directly from the twitter firehose.There is also an opportunity for more accurateretweet relation detection in our work.
Atpresent, we just identify the retweet whoseoriginal tweet has not been modified, whichleaves out a fair amount of retweet information.We would need to develop a more preciseretweet relation detection method.8 ConclusionIn this paper, we study three types of tweetfeatures and propose a tweet ranking strategy byapplying learning to rank algorithm.
We find aset of most effective features for tweet ranking.The results of experiments demonstrate that thesystem using Sum_mention, First_list,Important_follower, length and URL performsbest.
In particular, whether a tweet contains aURL is the most effective feature.
Additionally,we find in the experiments that the number oftimes the account is listed by other users is aneffective representation of account authority andperforms better than the number of followersthat is widely used in previous work.There are many aspects we would like toexplore in the future.
First, this research is basedon the search results returned from Twitterwhich contains the input query.
The tweets notcontaining the queries are not returned.
We willexplore query expansion approaches to improvethe recall of the search results.
We did notconsider spam issues in the ranking process.However, spam filtering is important to all typesof search engines.
We will explore the impactsof spam and work out a spam filtering approach.ReferencesChen Jilin, Rowan Nairn, Les Nelson, MichaelBernstein, and Ed H. Chi.
2010.
Short and Tweet:Experiments on Recommending Content fromInformation Streams.
In the Proceedings of the28th International conference on Human Factorsin Computing Systems, Pages: 1185-1194.Chen Zhi, Li Zhang, Weihua Wang.
2008.PostingRank: Bringing Order to Web ForumPostings.
In the proceedings of the 4th AsiaInformation Retrieval Symposium, Pages: 377-384.Dong Anlei, Ruiqiang Zhang, Pranam Kolari, JingBai, Fernando Diaz, Yi Chang, Zhaohui Zheng,and Hongyuan Zha.
2010.
Time of the essence:improving recency ranking using Twitter data.
Inthe proceedings of the 19th InternationalConference on World Wide Web, Pages: 331-340.302Fujimura Ko, Takafumi Inoue, and MasayukiSugisaki.
2005.
The EigenRumor Algorithm forRanking Blogs.
In the proceedings of the 2ndAnnual Workshop on the Weblogging Ecosystem:Aggregation, Analysis and Dynamics, World WideWeb.Jarvelin Kalervo, and Jaana Kekalainen.
2002.Cumulated gain-based evaluation of IR techniques.ACM Transactions on Information Systems,Volume 20, Pages: 422-446.Java Akshay, Xiaodan Song, Tim Finin, and BelleTseng.
2007.
Why we twitter: UnderstandingMicroblogging Usage and Communities.
In theproceedings of the 9th International Workshop onKnowledge Discovery on the Web and the 1stInternational Workshop on Social NetworksAnalysis.
Pages: 118-138.Joachims Thorsten.
1999.
Making Large-Scale SVMLearning Practical.
Advances in Kernel Methods:Support Vector Learning, Pages: 169-184.Pear Analytics.
2009.
Twitter Study-August 2009.Kritikopoulos Apostolos, Martha Sideri, and IraklisVarlamis.
2006.
BlogRank: Ranking WeblogsBased on Connectivity and Similarity Features.
Inthe proceedings of the 2nd International Workshopon Advanced Architectures and Algorithms forInternet Delivery and Applications.Leavitt Alex, Evan Burchard, David Fisher, and SamGilbert.
2009.
The Influentials: New Approachesfor Analyzing Influence on Twitter.
A publicationof the Web Ecology Project.Liu Hongbo, Jiahai Yang, Jiaxin Wang, Yu Zhang.2007.
A Link-Based Rank of Postings inNewsgroup.
In the proceedings of the 5thInternational Conference on Machine Learningand Data Mining in Pattern Recognition, Pages:392-403.Page Lawrence, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The PageRank CitationRanking: Bring Order to the Web.
Technicalreport, Stanford University.Robertson Stephen E., Steve Walker, and MichelineHancock-Beaulieu.
1998.
Okapi at TREC-7:Automatic Ad Hoc, Filtering, VLC and Interactive.In the Proceedings of the 7th Text RetrievalConference.
Pages: 199-210Song Young-In, Chin-Yew Lin, Yunbo Cao, andHae-Chang Rim.
2008.
Question Utility: A NovelStatic Ranking of Question Search.
In theProceedings of the 23rd AAAI Conference onArtificial Intelligence.
Pages: 1231-1236Sun Aaron R., Jiesi Cheng, and Daniel D. Zeng.
2009.A Novel Recommendation Framework forMicro-blogging based on Information Diffusion.In the proceedings of the 19th Workshop onInformation Technologies and Systems.Xi Wensi, Jesper Lind, and Eric Brill.
2004.
Learningeffective ranking functions for newsgroup search.In the proceedings of the 27th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval, Pages:394-401Xu Gu, and Ma Wei-Ying.
2006.
Building ImplicitLinks from Content for Forum Search.
In theproceedings of the 29th International ACM SIGIRConference on Research and Development inInformation Retrieval.
Pages: 300-307.303
