Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 24?32Manchester, August 2008Native Judgments of Non-Native Usage:Experiments in Preposition Error DetectionJoel R. TetreaultEducational Testing Service660 Rosedale RoadPrinceton, NJ, USAJTetreault@ets.orgMartin ChodorowHunter College of CUNY695 Park AvenueNew York, NY, USAmartin.chodorow@hunter.cuny.eduAbstractEvaluation and annotation are two of thegreatest challenges in developing NLP in-structional or diagnostic tools to markgrammar and usage errors in the writing ofnon-native speakers.
Past approaches havecommonly used only one rater to annotatea corpus of learner errors to compare tosystem output.
In this paper, we show howusing only one rater can skew system eval-uation and then we present a sampling ap-proach that makes it possible to evaluate asystem more efficiently.1 IntroductionIn this paper, we present a series of experimentsthat explore the reliability of human judgmentsin rating preposition usage.
While one tends tothink of annotator disagreements about discourseand semantics as being quite common, our studiesshow that judgments of preposition usage, which islargely lexically driven, can be just as contentious.As a result, this unreliability poses a serious issuefor the development and evaluation of NLP toolsin the task of automatically detecting prepositionusage errors in the writing of non-native speakersof English.To date, single human annotation has typicallybeen the gold standard for grammatical error de-tection, such as in the work of (Izumi et al, 2004),(Han et al, 2006), (Nagata et al, 2006), (Gamon etal., 2008)1.
Although there are several learner cor-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1(Eeg-Olofsson and Knuttson, 2003) had a small evalu-ation of 40 prepositions and it is unclear whether they usedmultiple annotators or not.pora annotated for preposition and determiner er-rors (such as the Cambridge Learners Corpus2andthe Chinese Learner English Corpus3), it is unclearwhich portions of these, if any, were doubly anno-tated.
This previous work has side-stepped the is-sue of annotator reliability, which we address herethrough the following three contributions:?
Judgments of Native Usage To motivate ourwork in non-native usage, we first illustratethe difficulty of preposition selection withtwo experiments: a cloze test and a choicetest, where native speakers judge native texts(section 4).?
Judgments of Non-Native Usage As statedearlier, most computational work in the fieldof error detection tools for non-native speak-ers has relied on a single rater to annotatea gold standard corpus to check a system?soutput.
We conduct an extensive double-annotation evaluation to measure inter-raterreliability and show that using one rater canbe unreliable and may produce misleading re-sults in a system test (section 5).?
Sampling ApproachMultiple annotation canbe very costly and time-consuming, whichmay explain why previous work employedonly one rater.
As an alternative to thestandard exhaustive annotation, we proposea sampling approach in which estimates ofthe rates of hits, false positives, and missesare derived from random samples of the sys-tem?s output, and then precision and recallof the system can be calculated.
We showthat estimates of system performance derived2http://www.cambridge.org/elt3http://langbank.engl.polyu.edu.hk/corpus/clec.html24from the sampling approach are comparableto those derived from an exhaustive annota-tion, but require only a fraction of the effort(section 6).In short, through a battery of experiments weshow how rating preposition usage, in either na-tive or non-native texts, is a task that has sur-prisingly low inter-annotator reliability and thusgreatly impacts system evaluation.
We then de-scribe a method for efficiently annotating non-native texts to make multiple annotation more fea-sible.In section 2, we discuss in more depth the mo-tivation for detecting usage errors in non-nativewriting, as well as the complexities of prepositionusage.
In section 3, we describe a system that au-tomatically detects preposition errors involving in-correct selection and extraneous usage.
In sections4 and 5 respectively, we discuss experiments on thereliability of judging native and non-native prepo-sition usage.
In section 6, we present results of oursystem and results from comparing the samplingapproach with the standard approach of exhaustiveannotation.2 MotivationThe long-term goal of our work is to develop asystem which detects errors in grammar and us-age so that appropriate feedback can be givento non-native English writers, a large and grow-ing segment of the world?s population.
Estimatesare that in China alone as many as 300 millionpeople are currently studying English as a for-eign language.
Even in predominantly English-speaking countries, the proportion of non-nativespeakers can be very substantial.
For example,the US National Center for Educational Statistics(2002) reported that nearly 10% of the students inthe US public school population speak a languageother than English and have limited English pro-ficiency .
At the university level in the US, thereare estimated to be more than half a million for-eign students whose native language is not English(Burghardt, 2002).
Clearly, there is an increasingdemand for tools for instruction in English as aSecond Language (ESL).Some of the most common types of ESL usageerrors involve prepositions, determiners and col-locations.
In the work discussed here, we targetpreposition usage errors, specifically those of in-correct selection (?we arrived to the station?)
andextraneous use (?he went to outside?)4.
Preposi-tion errors account for a substantial proportion ofall ESL usage errors.
For example, (Bitchener etal., 2005) found that preposition errors accountedfor 29% of all the errors made by intermediate toadvanced ESL students.
In addition, such errorsare relatively common.
In our learner corpora, wefound that 6% of all prepositions were incorrectlyused.
Some other estimates are even higher: forexample, (Izumi et al, 2003) reported error ratesthat were as high as 10% in a Japanese learner cor-pus.At least part of the difficulty in mastering prepo-sitions seems to be due to the great variety of lin-guistic functions that they serve.
When a prepo-sition marks the argument of a predicate, such asa verb, an adjective, or a noun, preposition se-lection is constrained by the argument role that itmarks, the noun which fills that role, and the par-ticular predicate.
Many English verbs also displayalternations (Levin, 1993) in which an argumentis sometimes marked by a preposition and some-times not (e.g., ?They loaded the wagon with hay?/ ?They loaded hay on the wagon?).
When prepo-sitions introduce adjuncts, such as those of timeor manner, selection is constrained by the objectof the preposition (?at length?, ?in time?, ?withhaste?).
Finally, the selection of a preposition fora given context also depends upon the intention ofthe writer (?we sat at the beach?, ?on the beach?,?near the beach?, ?by the beach?
).3 Automatically Detecting PrepositionUsage ErrorsIn this section, we give a description of our sys-tem and compare its performance to other sys-tems.
Although the focus of this paper is on hu-man judgments in the task of error detection, wedescribe our system to show that variability in hu-man judgments can impact the evaluation of a sys-tem in this task.
A full description of our systemand its performance can be found in (Tetreault andChodorow, 2008).3.1 SystemOur approach treats preposition error detection asa classification problem: that is, given a context oftwo words before and two words after the writer?spreposition, what is the best preposition to use?4There is a third error type, omission (?we are fond nullbeer?
), that is a topic for our future research.25An error is marked when the system?s sugges-tion differs from the writer?s by a certain thresholdamount.We have used a maximum entropy (ME) clas-sifier (Ratnaparkhi, 1998) to select the most prob-able preposition for a given context from a set of34 common English prepositions.
One advantageof using ME is that there are implementations of itwhich can handle very large models built frommil-lions of training events and consisting of hundredsof thousands of feature-value pairs.
To constructa model, we begin with a training corpus that isPOS-tagged and heuristically chunked into nounphrases and verb phrases5.
For each prepositionthat occurs in the training corpus, a preprocessingprogram extracts a total of 25 features.
These con-sist of words and POS tags in positions adjacent tothe preposition and in the heads of nearby phrases.In addition, we include combination features thatmerge the head features.
We also include featuresrepresenting only the tags to be able to cover casesin testing where the words in the context were notseen in training.In many NLP tasks (parsing, POS-tagging, pro-noun resolution), it is easy to acquire training datathat is similar to the testing data.
However, in thecase of grammatical error detection, one does nothave that luxury because reliable error-annotatedESL corpora that are large enough for training astatistical classifier simply do not exist.
To circum-vent this problem, we have trained our classifier onexamples of prepositions used correctly, as in newstext.3.2 EvaluationBefore evaluating our system on non-native writ-ing, we evaluated how well it does on the task ofpreposition selection in native text, an area wherethere has been relatively little work to date.
In thistask, the system predicts the writer?s prepositionbased on its context.
Its prediction is scored au-tomatically by comparison to what the writer actu-ally wrote.
Most recently, (Gamon et al, 2008) ad-dressed preposition selection by developing a sys-tem that combined a decision tree and a languagemodel.
Besides the difference in algorithms, thereis also a difference in coverage between their sys-tem, which selects among 13 prepositions plus acategory for Other, and the system presented here,5We have avoided parsing because our ultimate test corpusis non-native writing, text that is difficult to parse due to thepresence of numerous errors in spelling and syntax.Prep (Gamon et al, 2008) (Tetreault et al, 2008)in 0.592 0.845for 0.459 0.698of 0.759 0.906on 0.322 0.751to 0.627 0.775with 0.361 0.675at 0.372 0.685by 0.502 0.747as 0.699 0.711from 0.528 0.591about 0.800 0.654Table 1: Comparison of F-measures on En-carta/Reuters Corpuswhich selects among 34 prepositions.
In their sys-tem evaluation, they split a corpus of Reuters Newstext and Microsoft Encarta into two sets: 70% fortraining (3.2M examples), and the remaining 30%for testing (1.4M examples).
For purposes of com-parison, we used the same corpus and evaluationmethod.
While (Gamon et al, 2008) do not presenttheir overall accuracy figures on the Encarta eval-uation, they do present the precision and recallscores for each preposition.
In Table 3.2, we dis-play their results in terms of F-measures and showthe performance of our system for each preposi-tion.
Our model outperforms theirs for 9 out of the10 prepositions that both systems handle.
Over-all accuracy for our system is 77.4% and increasesto 79.0% when 7M more training examples areadded.
For comparison purposes, using a major-ity baseline (always selecting the preposition of) inthis domain results in an accuracy of 27.2%.
(Felice and Pullman, 2007) used perceptronclassifiers for preposition selection in BNC NewsText at 85% accuracy.
For each of the five mostfrequent prepositions, they used a separate binaryclassifier to decide whether that preposition shouldbe used or not.
The classifiers are not combinedinto a unified model.
When we reconfigured oursystem and evaluation to be comparable to (Feliceand Pullman, 2007), our model achieved an accu-racy of 90% on the same five prepositions whentested on Wall Street Journal News, which is simi-lar, though not identical, to BNC News.While systems can perform at close to 80% ac-curacy in the task of preposition selection in nativetexts, this high performance does not transfer tothe end-task of detecting preposition errors in es-says by non-native writers.
For example, (Izumi etal., 2003) reported precision and recall as low as25% and 7% respectively when detecting different26grammar errors (one of which was prepositions)in English essays by non-native writers.
(Gamonet al, 2008) reported precision up to 80% in theirevaluation on the CLEC corpus, but no recall fig-ure was reported.
We have found that our system(the model which performs at 77.4%), also per-forms as high as 80% precision, but recall rangedfrom 12% to 26% depending on the non-native testcorpus.While our recall figures may seem low, espe-cially when compared to other NLP tasks such asparsing and anaphora resolution, this is really a re-flection of how difficult the task is.
In addition, inerror detection tasks, high precision (and thus lowrecall) is favored since one wants to minimize thenumber of false positives a student may see.
Thisis a common practice in grammatical error detec-tion applications, such as in (Han et al, 2006) and(Gamon et al, 2008).4 Human Judgments of Native Usage4.1 Cloze TestWith so many sources of variation in Englishpreposition usage, we wondered if the task of se-lecting a preposition for a given context mightprove challenging even for native speakers.
Toinvestigate this possibility, we randomly selected200 sentences from Microsoft?s Encarta Encyclo-pedia, and, in each sentence, we replaced a ran-domly selected preposition with a blank.
We thenasked two native English speakers to perform acloze task by filling in the blank with the bestpreposition, given the context provided by the restof the sentence.
In addition, we had our systempredict which preposition should fill each blank aswell.
Our results (Table 2) showed only about 76%agreement between the two raters (bottom row),and between 74% and 78% when each rater wascompared individually with the original preposi-tion used in Encarta.
Surprisingly, the systemperformed just as well as the two native raters,when compared with Encarta (third row).
Al-though these results seem very promising, it shouldbe noted that in many cases where the system dis-agreed with Encarta, its prediction was not a goodfit for the context.
But in the cases where theraters disagreed with Encarta, their prepositionswere also licensed by the context, and thus wereacceptable alternatives to the preposition that wasused in the text.Our cloze study shows that even with well-Agreement KappaEncarta vs. Rater 1 0.78 0.73Encarta vs. Rater 2 0.74 0.68Encarta vs. System 0.75 0.68Rater 1 vs. Rater 2 0.76 0.70Table 2: Cloze Experiment on Encartaformed text, native raters can disagree with eachother by 25% in the task of preposition selec-tion.
We can expect even more disagreement whenthe task is preposition error detection in ?noisy?learner texts.4.2 Choice TestThe cloze test presented above was scored by au-tomatically comparing the system?s choice (or therater?s choice) with the preposition that was actu-ally written.
But there are many contexts that li-cense multiple prepositions, and in these cases, re-quiring an exact match is too stringent a scoringcriterion.To investigate how the exact match metric mightunderestimate system performance, and to furthertest the reliability of human judgments in nativetext, we conducted a choice test in which twonative English speakers were presented with 200sentences from Encarta and were asked to selectwhich of two prepositions better fit the context.One was the originally written preposition and theother was the system?s suggestion, displayed inrandom order.
The human raters were also giventhe option of marking both prepositions as equallygood or equally bad.
The results indicated thatboth Rater 1 and Rater 2 considered the system?spreposition equal to or better than the writer?spreposition in 28% of the cases.
This suggeststhat 28% of the mismatched cases in the automaticevaluation are not system errors but rather are in-stances where the context licenses multiple prepo-sitions.
If these mismatches in the automatic eval-uation are actually cases of correct system perfor-mance, then the Encarta/Reuters test which per-forms at 75% accuracy (third row of Table 2), ismore realistically around 82% accuracy (28% ofthe 25% mismatch rate is 7%).5 Annotator ReliabilityIn this section, we address the central problem ofevaluating NLP error detection tools on learnerdata.
As stated earlier, most previous work has re-lied on only one rater to either create an annotated27corpus of learner errors, or to check the system?soutput.
While some grammatical errors, such asnumber disagreement between subject and verb,no doubt show very high reliability, others, such asusage errors involving prepositions or determinersare likely to be much less reliable.
In section 5.1,we describe our efforts in annotating a large cor-pus of student learner essays for preposition us-age errors.
Unlike previous work such as (Izumiet al, 2004) which required the rater to check foralmost 40 different error types, we focus on anno-tating only preposition errors in hopes that havinga single type of target will insure higher reliabil-ity by reducing the cognitive demands on the rater.Section 5.2 asks whether, under these conditions,one rater is acceptable for this task.
In section 6,we describe an approach to efficiently evaluating asystem that does not require the amount of effortneeded in the standard approach to annotation.5.1 Annotation SchemeTo create a gold-standard corpus of error anno-tations for system evaluation, and also to deter-mine whether multiple raters are better than one,we trained two native English speakers to anno-tate preposition errors in ESL text.
Both annota-tors had prior experience in NLP annotation andalso in ESL error detection.
The training was veryextensive: both raters were trained on 2000 prepo-sition contexts and the annotation manual was it-eratively refined as necessary.
To our knowledge,this is the first scheme that specifically targets an-notating preposition errors6.The two raters were shown sentences randomlyselected from student essays, with each preposi-tion highlighted in the sentence.
The raters werealso shown the sentence which preceded the onecontaining the preposition that they rated.
The an-notator was first asked to indicate if there were anyspelling errors within the context of the preposi-tion (?2-word window and the commanding verb).Next the annotator noted determiner or plural er-rors in the context, and then checked if there wereany other grammatical errors (for example, wrongverb form).
The reason for having the annota-tors check spelling and grammar is that other mod-ules in a grammatical error detection system wouldbe responsible for these error types.
For an ex-6(Gamon et al, 2008) did not have a scheme for annotat-ing preposition errors to create a gold standard corpus, but diduse a scheme for the similar problem of verifying a system?soutput in preposition error detection.ample of a sentence with multiple spelling, gram-matical and collocational errors, consider the fol-lowing sentence: ?In consion, for some reasons,museums, particuraly known travel place, get onmany people.?
A spelling error follows the prepo-sition In, and a collocational error surrounds on.
Ifthe contexts are not corrected, it is impossible todiscern if the prepositions are correct.
Of course,there is the chance that by removing these we willscreen out cases where there are multiple interact-ing errors in the context that involve prepositions.When comparing human judgments to the perfor-mance of the preposition module, the latter shouldnot be penalized for other kinds of errors in thecontext.Finally, the annotator judged the writer?s prepo-sition with a rating of ?0-extraneous preposition?,?1-incorrect preposition?, ?2-correct preposition?,or ?e-equally good prepositions?.
If the writerused an incorrect preposition, the rater supplied thebest preposition(s) given the context.
Very often,when the writer?s preposition was correct, severalother prepositions could also have occurred in thesame context.
In these cases, the annotator was in-structed to use the ?e?
category and list the otherequally plausible alternatives.
After judging theuse of the preposition and, if applicable, supplyingalternatives, the annotator indicated her confidencein her judgment on a 2-point scale of ?1-low?
and?2-high?.5.2 Two Raters vs. One?Following training, each annotator judged approxi-mately 18,000 occurrences of preposition use.
An-notation of 500 occurrences took an average of 3 to4 hours.
In order to calculate agreement and kappavalues, we periodically provided identical sets of100 preposition occurrences for both annotators tojudge (totaling 1800 in all).
After removing in-stances where there were spelling or grammar er-rors, and after combining categories ?2?
and ?e?,both of which were judgments of correct usage,we computed the kappa values for the remainingdoubly judged sets.
These ranged from 0.411 to0.786, with an overall combined value of 0.6307.The confusion matrix for the combined set (to-taling 1336 contexts) is shown in Table 3.
Therows represent Rater 1?s (R1) judgments while thecolumns represent Rater 2?s judgments.
As one7When including spelling and grammar annotations,kappa ranged from 0.474 to 0.773.28would expect given the prior reports of prepositionerror rates in non-native writing, the raters?
agree-ment for this task was quite high overall (0.952)due primarily to the large agreement count whereboth annotators rated the usage ?OK?
(1213 totalcontexts).
However there were 42 prepositions thatboth raters marked as a ?Wrong Choice?
and 17 as?Extraneous.?
It is important to note the disagree-ments in judging these errors: for example, Rater1 judged 26 prepositions to be errors that Rater 2judged to be OK, for a disagreement rate of .302(26/86).
Similarly, Rater 2 judged 37 prepositionsto be errors that Rater 1 judged to be OK, for adisagreement rate of .381 (37/97).R1?
; R2?
Extraneous Wrong-Choice OKExtraneous 17 0 6Wrong-Choice 1 42 20OK 4 33 1213Table 3: Confusion MatrixThe kappa of 0.630 and the off-diagonal cellsin the confusion matrix both show the difficultyof this task and also show how two highly trainedraters can produce very different judgments.
Thissuggests that for certain error annotation tasks,such as preposition usage, it may not be appropri-ate to use only one rater and that using two or moreraters to produce an adjudicated gold-standard setis the more acceptable path.As a second test, we used a set of 2,000 prepo-sition contexts from ESL essays (Chodorow et al,2007) that were doubly annotated by native speak-ers with a scheme similar to that described above.We then compared an earlier version of our sys-tem to both raters?
judgments, and found that therewas a 10% difference in precision and a 5% differ-ence in recall between the two system/rater com-parisons.
That means that if one is using only asingle rater as a gold standard, there is the potentialto over- or under-estimate precision by as much as10%.
Clearly this is problematic when evaluatinga system?s performance.
The results are shown inTable 4.Precision RecallSystem vs. Rater 1 0.78 0.26System vs. Rater 2 0.68 0.21Table 4: Rater/System Comparison6 Sampling ApproachIf one uses multiple raters for error annotation,there is the possibility of creating an adjudicatedset, or at least calculating the variability of sys-tem evaluation.
However, annotation with multipleraters has its own disadvantages in that it is muchmore expensive and time-consuming.
Even usingone rater to produce a sizeable evaluation corpusof preposition errors is extremely costly.
For ex-ample, if we assume that 500 prepositions can beannotated in 4 hours using our annotation scheme,and that the error rate for prepositions is 10%, thenit would take at least 80 hours for a rater to findand mark 1000 errors.
In this section, we proposea more efficient annotation approach to circumventthis problem.6.1 MethodologyThe sampling procedure outlined here is inspiredby the one described in (Chodorow and Leacock,2000).
The central idea is to skew the annotationcorpus so that it contains a greater proportion oferrors.
The result is that an annotator checks morepotential errors since he or she is spending lesstime checking prepositions used correctly.Here are the steps in the procedure.
Figure 1 il-lustrates this procedure with a hypothetical corpusof 10,000 preposition examples.1.
Process a test corpus of sentences so that eachpreposition in the corpus is labeled ?OK?
or?Error?
by the system.2.
Divide the processed corpus into two sub-corpora, one consisting of the system?s ?OK?prepositions and the other of the system?s?Error?
prepositions.
For the hypotheticaldata in Figure 1, the ?OK?
sub-corpus con-tains 90% of the prepositions, and the ?Error?sub-corpus contains the remaining 10%.3.
Randomly sample cases from each sub-corpus and combine the samples into an an-notation set that is given to a ?blind?
humanrater.
We generally use a higher samplingrate for the ?Error?
sub-corpus because wewant to ?enrich?
the annotation set with alarger proportion of errors than is found in thetest corpus as a whole.
In Figure 1, 75% ofthe ?Error?
sub-corpus is sampled while only16% of the ?OK?
sub-corpus is sampled.29Figure 1: Sampling Approach (with hypothetical sample calculations)4.
For each case that the human rater judges tobe an error, check to see which sub-corpus itcame from.
If it came from the ?OK?
sub-corpus, then the case is a Miss (an error thatthe system failed to detect).
If it came fromthe ?Error?
sub-corpus, then the case is a Hit(an error that the system detected).
If the raterjudges a case to be a correct usage and it camefrom the ?Error?
sub-corpus, then it is a FalsePositive (FP).5.
Calculate the proportions of Hits and FPs inthe sample from the ?Error?
sub-corpus.
Forthe hypothetical data in Figure 1, these val-ues are 600/750 = 0.80 for Hits, and 150/750= 0.20 for FPs.
Calculate the proportion ofMisses in the sample from the ?OK?
sub-corpus.
For the hypothetical data, this is450/1500 = 0.30 for Misses.6.
The values computed in step 5 are conditionalproportions based on the sub-corpora.
To cal-culate the overall proportions in the test cor-pus, it is necessary to multiply each valueby the relative size of its sub-corpus.
Thisis shown in Table 5, where the proportion ofHits in the ?Error?
sub-corpus (0.80) is mul-tiplied by the relative size of the ?Error?
sub-corpus (0.10) to produce an overall Hit rate(0.08).
Overall rates for FPs and Misses arecalculated in a similar manner.7.
Using the values from step 6, calculate Preci-sion (Hits/(Hits + FP)) and Recall (Hits/(Hits+ Misses)).
These are shown in the last tworows of Table 5.Estimated Overall RatesSample Proportion * Sub-Corpus ProportionHits 0.80 * 0.10 = 0.08FP 0.20 * 0.10 = 0.02Misses 0.30 * 0.90 = 0.27Precision 0.08/(0.08 + 0.02) = 0.80Recall 0.08/(0.08 + 0.27) = 0.23Table 5: Sampling Calculations (Hypothetical)This method is similar in spirit to active learning((Dagan and Engelson, 1995) and (Engelson andDagan, 1996)), which has been used to iterativelybuild up an annotated corpus, but it differs fromactive learning applications in that there are no it-erative loops between the system and the humanannotator(s).
In addition, while our methodologyis used for evaluating a system, active learning iscommonly used for training a system.6.2 ApplicationNext, we tested whether our proposed samplingapproach provides good estimates of a system?sperformance.
For this task, we split a large corpusof ESL essays into two sets: first, a set of 8,269preposition contexts (standard approach corpus) tobe annotated using the scheme in section 5.1, and30second, a set of 22,000 preposition contexts to berated using the sampling approach (sampling cor-pus).
We used two non-overlapping sets becausethe raters were the same for this test of the two ap-proaches.Using the standard approach, the sampling cor-pus of 22,000 prepositions would normally takeseveral weeks for two raters to double annotateand then adjudicate.
After this corpus was di-vided into ?OK?
and ?Error?
sub-corpora, the twosub-corpora were proportionally sampled, result-ing in an annotation set of 750 preposition con-texts (500 contexts from the ?OK?
sub-corpus and250 contexts from the ?Error?
sub-corpus).
Thisrequired roughly 6 hours for annotation, which issubstantially more manageable than the standardapproach.
We had both raters work together tomake judgments for each preposition context.The precision and recall scores for both ap-proaches are shown in Table 6 and are quite simi-lar, thus suggesting that the sampling approach canbe used as an alternative to exhaustive annotation.Precision RecallStandard Approach 0.80 0.12Sampling Approach 0.79 0.14Table 6: Sampling Results6.3 Confidence IntervalsIt is important with the sampling approach to useappropriate sample sizes when drawing from thesub-corpora, because the accuracy of the estimatesof hits and misses will depend upon the propor-tion of errors in each sub-corpus as well as on thesample sizes.
The ?OK?
sub-corpus is expectedto have even fewer errors than the overall baserate, so it is especially important to have a rela-tively large sample from this sub-corpus.
The com-parison study described above used an ?OK?
sub-corpus sample that was twice as large as the Errorsub-corpus sample.One can compute the 95% confidence interval(CI) for the estimated rates of hits, misses and falsepositives by using the formula:CI = p?
1.96?
?pwhere p is the proportion and ?pis the standarderror of the proportion given by:?p=?p(1?
p)Nwhere N is the sample size.For the example in Figure 1, the confidence in-terval for the proportion of Hits from the sample ofthe ?Error?
sub-corpus is:CIhits= 0.80?
1.96??0.8?
(1?
0.80)750which yields an interval of 0.077 and 0.083.
Usingthese values, the confidence interval for precisionis 0.77 to 0.83.
The interval for recall can be com-puted in a similar manner.
Of course, a larger sam-ple size will yield narrower confidence intervals.6.4 SummaryTable 7 summarizes the advantages and disadvan-tages of three methods for evaluating error detec-tion systems.
The standard (or exhaustive) ap-proach refers to the method of annotating the er-rors in a large corpus.
Its advantage is that the an-notated corpus can be reused to evaluate the samesystem or compare multiple systems.
However,it is costly and time-consuming which often pre-cludes the use of multiple raters.
The verificationmethod (as used in (Gamon et al, 2008)), refers tothe method of simply checking the acceptability ofsystem output with respect to the writer?s preposi-tion.
Like the sampling method, it has the advan-tages of efficiency and use of multiple raters (whencompared to the standard method).
But the dis-advantage of verification is that it does not permitestimation of recall.
Both verification and vam-pling methods require re-annotation for system re-testing and comparison.
In terms of system devel-opment, sampling (and to a lesser extent, verifica-tion) allows one to quickly assess system perfor-mance on a new corpus.In short, the sampling approach is intended toalleviate the burden on annotators when faced withthe task of having to rate several thousand errors ofa particular type to produce a sizeable error corpus.7 ConclusionsIn this paper, we showed that the standard ap-proach to evaluating NLP error detection sys-tems (comparing the system?s output with a gold-standard annotation) can greatly skew system re-sults when the annotation is done by only one rater.However, one reason why a single rater is com-monly used is that building a corpus of learner er-rors can be extremely costly and time-consuming.To address this efficiency issue, we presented a31Approach Advantages DisadvantagesStandard Easy to retest system (no re-annotation required) CostlyEasy to compare systems Time-ConsumingMost reliably estimates precision and recall Difficult to use multiple ratersSampling Efficient, especially for low-frequency errors Less reliable estimate of recallPermits estimation of precision and recall Hard to re-test system (re-annotation required)More easily allows use of multiple raters Hard to compare systemsVerification Efficient, especially for low-frequency errors Does not permit estimation of recallMore easily allows use of multiple raters Hard to re-test system (re-annotation required)Hard to compare systemsTable 7: Comparison of Evaluation Methodssampling approach that produces results compa-rable to exhaustive annotation.
This makes usingmultiple raters possible since less time is requiredto assess the system?s performance.
While thework presented here has focused on prepositions,the reasons for using multiple raters and a sam-pling approach apply equally to other error types,such as determiners and collocations.It should be noted that the work here uses tworaters.
For future work, we plan on annotatingpreposition errors with more than two raters to de-rive a range of judgments.
We also plan to look atthe effects of feedback for errors involving prepo-sitions and determiners, on the quality of ESLwrit-ing.The preposition error detection system de-scribed here was recently integrated into Cri-terionSMOnline Writing Evaluation Servicedeveloped by Educational Testing Service.Acknowledgements We would first like tothank our two annotators Sarah Ohls and WaverlyVanWinkle for their hours of hard work.
We wouldalso like to acknowledge the three anonymousreviewers and Derrick Higgins for their helpfulcomments and feedback.ReferencesBitchener, J., S. Young, and D. Cameron.
2005.
The ef-fect of different types of corrective feedback on ESLstudent writing.
Journal of Second Language Writ-ing.Burghardt, L. 2002.
Foreign applications soar at uni-versities.
New York Times, April.Chodorow, M. and C. Leacock.
2000.
An unsupervisedmethod for detecting grammatical errors.
In NAACL.Chodorow, M., J. Tetreault, and N-R. Han.
2007.
De-tection of grammatical errors involving prepositions.In Proceedings of the Fourth ACL-SIGSEM Work-shop on Prepositions.Dagan, I. and S. Engelson.
1995.
Committee-basedsampling for training probabilistic classifiers.
InProceedings of ICML, pages 150?157.Eeg-Olofsson, J. and O. Knuttson.
2003.
Automaticgrammar checking for second language learners - theuse of prepositions.
In Nodalida.Engelson, S. and I. Dagan.
1996.
Minimizing manualannotation cost in supervised training from corpora.In Proceedings of ACL, pages 319?326.Felice, R. De and S. Pullman.
2007.
Automatically ac-quiring models of preposition use.
In Proceedings ofthe Fourth ACL-SIGSEM Workshop on Prepositions.Gamon, M., J. Gao, C. Brockett, A. Klementiev, W. B.Dolan, D. Belenko, and L. Vanderwende.
2008.
Us-ing contextual speller techniques and language mod-eling for esl error correction.
In IJCNLP.Han, N-R., M. Chodorow, and C. Leacock.
2006.
De-tecting errors in English article usage by non-nativespeakers.
Natural Language Engineering, 12:115?129.Izumi, E., K. Uchimoto, T. Saiga, T. Supnithi, andH.
Isahara.
2003.
Automatic error detection in theJapanese leaners?
English spoken data.
In ACL.Izumi, E., K. Uchimoto, and H. Isahara.
2004.
Theoverview of the sst speech corpus of Japanese learnerEnglish and evaluation through the experiment onautomatic detection of learners?
errors.
In LREC.Levin, B.
1993.
English verb classes and alternations:a preliminary investigation.
Univ.
of Chicago Press.Nagata, R., A. Kawai, K. Morihiro, and N. Isu.
2006.A feedback-augmented method for detecting errorsin the writing of learners of English.
In Proceedingsof the ACL/COLING.NCES.
2002.
National center for educational statis-tics: Public school student counts, staff, and graduatecounts by state: School year 2000-2001.Ratnaparkhi, A.
1998.
Maximum Entropy Models fornatural language ambiguity resolution.
Ph.D. thesis,University of Pennsylvania.Tetreault, J. and M. Chodorow.
2008.
The ups anddowns of preposition error detection in ESL writing.In COLING.32
