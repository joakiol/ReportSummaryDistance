RESPONDING TO SEMANTICALLY  I LL -FORMED INPUTRa lph  Gr i shman and  P ing  PengComputer  Sc ience  Depar tmentNew York  Un ivers i ty251 Mercer  S t reetNew York ,  NY  10012Abst rac tOne cause of failure in natural anguage in-terfaces is semantic overshoot; this is re-flected in input sentences which do not cor-respond to any semantic pattern in the sys-tem.
We describe a system which provideshelpful feedback in such cases by identifyingthe "semantically closest" inputs which thesystem would be able to understand.1.
Int roduct ionNatural language interfaces have achieved a lim-ited success in small, well circumscribed omains,such as query systems for simple data bases.
Onetask in constructing such an interface is identi-fying the relationships which exist in a domain,and the possible linguistic expressions of these re-lationships.
As we set our sights on more complexdomains, it will become much harder to developa complete or nearly complete catalog of the rele-vant relationships and linguistic expressions; ub-stantial gaps will be inevitable.
In consequence,many inputs will be rejected because they fail tomatch the semantic/linguistic model we have con-structed for the domain.We are concerned with the following question:what response should we give a user when his in-put cannot be analyzed for the reasons just de-scribed?
The response "please rephrase" gives theuser no clue as to how to rephrase.
This leadsto the well-known "stonewalling" phenomenon,where a user tries repeatedly, without success, torephrase his request in a form the system will un-derstand.
This may seem amusing to the outsideobserver, but it can be terribly frustrating to theuser, and to the system designer watching his sys-tem being used.We propose instead to provide the user withsentences which are semantically close to the orig-inal input (in a sense to be defined below) andare acceptable inputs to the system.
Such feed-back may occasionally be confusing, but we ex-pect that more often it will be helpful in showingthe system's capabilities and suggesting possiblerephrasings.In the remainder of this paper we brieflyreview the prior work on responding to ill-formedness, describe our proposal and its imple-mentation as part of a small question-answeringsystem, and relate our initial experiences with thissystem.2.
BackgroundA.
Re la t ive  and  Abso lu te  I l l - fo rm-ednessWeischedel and Sondheimer (~,Veischedel, 1983)have distinguished two types of ill-formedness: ab-solute ill-formedness and relative ill-formedness.Roughly speaking, an absolutely ill-formed inputis one which does not conform to the syntacticand semantic onstraints of the natural languageor the sublanguage; a relatively ill-formed input isone which is outside the coverage of a particularnatural language interface.
Our concern is pri-marily with relative ill-formedness.
For complexdomains, we believe that it will be difficult to cre-ate complete semantic models, and therefore thatrelatively ill-formed input will be a serious prob-lem - a problem that it will be hard for users toremedy without suitable feedback.B .
Syntact i c  and  Semant ic  I l l -fo rmednessEarlier studies have examined both syntacticallyand semantically ill-formed input.
Among thework on syntactically ill-formed input has beenEPISTLE (Miller 1981), the work of Weischedeland Sondheimer (Weischedel 1980, Kwasney 1981,and Weischedel 1983), and Carbonell and Hayes(Carbonell 1983).
Some of this work has involvedthe relaxation of syntactic onstraints; other (such65as Carbonell and Hayes) a reliance primarily onsemantic structures when syntactic analysis fails.Our system has been primarily motivated by ourconcern about the possiblity of constructing com-plete semantic models, so we have focussed to dateon semantic ill-formedness, but we believe that oursystem will have to be extended in the future tohandle syntactic ill-formedness as well.C .
E r ror  Ident i f i ca t ion  and  Cor -rec t ionFor some applications, it is sufficient that the pointof ill-formedness be identified, and the constraintbe relaxed so that an analysis can be obtained.This was the case in Wilks' early work on "Prefer-ence Semantics"(Wilks 1975), which was used formachine translation applications.
In other appli-cations it is necessary to obtain an analysis con-forming to the system's emantic model in orderfor further processing of the input to take place,in effect "correcting" the user's input.
This isthe case for data base query (our current appli-cation), for command systems (such as MURPHY(Selfridge 1986)), and for message ntry systems(such as NOMAD (Granger 1983) and VOX (Mey-ers 1985)).D .
Sys tem Organ izat ionError correction can be provided either by makingpervasive changes to a set of rules, or by providinguniform correction procedures which work with astandard (non-correcting) set of rules.
In the syn-tactic domain, EPISTLE is an example of the for-mer, the metarule approach (Weischedel 1983) anexample of the latter.
We feel that, particularlyfor semantic orrection, it is important o take the"uniform procedure" approach, since a semanticmodel for a large domain will be difficult enoughto build and maintain without having to take theneeds of a correction mechanism into account.
Itis equally important o have a procedure whichwill operate on a system with separate syntacticand semantic omponents, o that we may reapthe advantages of such an organization (concise-ness, modularity).
The NOMAD system used pro-cedures associated with individual words and sowas very hard to extend (Granger 1983, p. 195);the VOX system remedied some of these defectsbut used a "conceptual grammar" mixing syntac-tic and semantic onstraints (Meyers 1985).
TheMURPHY system (Selfridge 1986) is most simi-lar to our own work in terms of the approach tosemantic onstraint relaxation and user feedback;however, it used a syntactic representation whichwould be difficult to extend, and required weightsin the semantic model for the correction proce-dure.The ill-formedness we are considering mayalso be viewed as one type of violation of the in-tensional constraints of the data base (constraintsin this case on the classes of objects which mayparticipate in particular relations).
Intensionalconstraints have been studied in connection withnatural language query systems by several re-searchers, including Mays (1980) and Gal (1985).In particular, the technique that we have adoptedis similar in general terms to that suggested byMays to handle non-existent relationships.In addition to the shortcomings ofsome of thesystems just described, we felt it important o de-velop and test a system in order to gain experiencein the effectiveness of these correction techniques.Although (as just noted) many techniques havebeen described, the published reports contain vir-tually no evaluation of the different approaches.3.
System OverviewOur feedback mechanism is being evaluated in thecontext of a small question-answering system witha relatively standard structure.
Processing of aquestion begins with two stages of syntax analysis:parsing, using an augmented context-free gram-mar, and syntactic regularization, which convertsthe various types of clauses (active and passive;interrogative, imperative, and declarative; rela-tive and reduced relative; etc.)
into a canonicalform.
In this canonical form, each clause is rep-resented as a list consisting of: tense, aspect, andvoice markers; the verb (root form); and a list ofoperands, each marked by "subject", "object", orthe governing preposition.
For example, "John re-ceived an A in calculus."
would be translated to(past receive (subject John) (object A)(in calculus))Processing continues with semantic analysis,which translates the regularized parse into anextended-predicate-calculus formula.
One aspectof this translation is the determination of quanti-fier scope.
Another aspect is the mapping of eachverb and its operands (subject, objects, and mod-ifiers) into a predicate-argument structure.
Thepredicate calculus formula is then interpreted asa data base retrieval command.
Finally, the re-trieved data is formatted for the user.The translation from verb plus operands topredicate plus arguments is controlled by the67model for the domain.
The domain vocabulary isorganized into a set of verb, noun, adjective, andadverb semantic classes.
The model is a set ofpatterns tated in terms of these semantic lasses.Each pattern represents one combination of verband operands which is valid (meaningful) in thisdomain.
For example, the pattern which wouldmatch the sentence given just above is(v-receive (subject nstudent)(object ngrade) (in ncourse))where v-receive is the class of verbs including re-ceive, get, etc.
; nstudent he class of students;ngrade the class of grades; and ncourse the classof course names.
Associated with each patternis a rule for creating the corresponding predicate-argument structure.4.
The Diagnostic ProcessIn terms of the system just described, the analy-sis failures we are concerned with correspond tothe presence in the input of clauses which do notmatch any pattern in the model.
The essence ofour approach is quite simple: find the patternsin the model which come closest to matching theinput clause, and create sentences using these pat-terns.
Implementation of this basic idea, however,has required the development of several processingsteps, which we now describe.Our first task is to identify the clauses towhich we should apply our diagnostic procedure.Our first impulse might be to trigger the proce-dure as soon as we parse a clause which doesn'tmatch the model.
However, the process of match-ing clause against model serves in our system tocheck selectional constraints.
These constraintsare needed to filter out, from syntactically validanalyses, those which are semantically ill-formed.In a typical query we may have several seman-tically ill-foimed analyses (along with one well-formed one), and thus several occasions of failurein the matching process before we obtain the cor-rect analysis.We must therefore wait until syntax analy-sis is complete and see if there is any syntacticanalysis satisfying all selectional constraints.
Ifthere is no such analysis, we look for an analysisin which all but one clause satisfies the selectionalconstraints; if there is such an analysis, we markthe offending clause as our candidate for diagnos-tic processing.Next we look for patterns in the model which"roughly match" this clause.
As we explainedabove, the regularized clause contains a verb anda set of syntactic ases with case labels and fillers;each model pattern specifies a verb class and aset of cases, with each case slot specifying a la-bel and the semantic lass of its filler.
We definea distance measure between a clause and a pat-tern by assigning a score to each type of mismatch(clause and pattern have the same syntactic casewith different semantic lasses; clause and patterninclude the same semantic class but in differentcases; clause has case not present in pattern; etc.
)and adding the scores.
We then select the pat-tern or patterns which, according to this distancemeasure, are closest o the offending clause.We now must take each of these patterns andbuild from it a sentence or phrase the user can un-derstand.
Each pattern is in effect a syntactic aseframe, with slots whose values have to be filledin.
If the case corresponds to one present in theclause, we copy the value from the clause; if thecase is optional, we delete it.
Othewise we createa slot filler consisting of an indefinite article and anoun describing the semantic lass allowed in thatslot (for example, if the pattern allows members ofthe class of students in a slot, we would generatethe filler "a student").
When all the slots havebeen filled, we have a structure comparable to theregularized clause structure produced by syntacticanalysis.Finally each filled-in pattern must be trans-formed to a syntactic form parallel to that of theoriginal offending clause.
(If we don't do this -if, for example, the input is a yes-no question andthe feedback is a declarative sentence - the systemoutput can be quite confusing.)
We isolate thetense, voice, aspect, and other syntactic featuresof the original clause (this is part of the syntacticregularization process) and transfer these featuresto the generated structure.
If the offending clauseis an embedded clause in the original sentence, wesave the context of the offending clause (the matrixsentence) and insert the "corrected" clause intothis context.
We take the resulting structure andapply a sentence generation procedure.
The gen-eration procedure, guided by the syntactic featuremarkers, applies "forward" transformations whicheventually generate a sentence string.
These sen-tences are presented as the system's uggestionsto the user.5.
ExamplesThe system has been implemented as describedabove, and has been tested as part of a question-answering system for a small "student ranscript"58data base.
The syntactic model currently has pat-terns for 30 combinations of verbs and arguments.While the model has been gradually growing, itstill has sufficient "gaps" to give adequate oppor-tunity for applying the diagnostics.A few examples will serve to clarify the oper-ation of the system.
The system has models(take (subject student) (object course))and(offer (subject school) (object course))but no model of the form(offer (subject student) (object course))Accordingly, if a user typesDid any students offer V l l ?
(where V l l  is the name of a course), the systemwill respondSorry, I don't understand the pattern(students offer courses)and will offer the "suggestions"Did any students take V l l ?andDid some school offer V l l ?Prepositional phrase modifiers are analyzedby inserting a "be" and treating the result as arelative clause.
For example, "students in V l l "would be expanded to "students \[such that\] \[stu-dents\] be in V l l " .
If the resulting clause is notin the semantic model, the usual correction proce-dures are applied.
As part of our policy of limitingthe model for testing purposes, we did not includea pattern of the form(be (subject student) (in course))but there is a pattern of the form(enroll (subject student) (in course))(for sentences uch as "Tom enrolled in Vl l .")
.Therefore if the user typesList the students in Vl l .the system will generate the suggestionsList the students who enroll in Vl l .andList the students.
(the second suggestion arising by deleting themodifier).6.
Cur rent  S ta tusThe system has been operational since the summerof 1986.
Since that time we have been regularlytesting the system on various volunteers and revis-ing the system to improve its design and feedback.We instructed the volunteers to try to use the sys-tem to get various pieces of information, ratherthan setting them a fixed task, so the queries triedhave varied widely among users.The experimental results indicate both thestrength and weakness of the technique we havedescribed.
On the one hand, semantic patternmismatch is not the primary cause of failure; vo-cabulary overshoot (using words not in the dictio-nary) is much more common.
In a series of testsinvolving 375 queries (by 8 users), 199 (53%) weresuccessful, 95 (25%) failed due to missing vocabu-lary, 22 (6%) failed due to semantic pattern mis-match, and 59 (16%) failed for other reasons.
Onthe other hand, in cases of semantic pattern mis-match, the suggestions made by the system usu-ally include an appropriate rephrasing of the query(as well as some extraneous suggestions).
Of the22 failures due to semantic pattern mismatch (inboth series of tests), we judge that in 14 cases thesuggestions included an appropriate rephrasing.7.
AssessmentThese results, while not definitive, suggest thatthe technique described above /s a useful one,but will have to be combined with other tech-niques to forge a general strategy for dealing withproblems encountered in interpreting the input.Extending the syntactic coverage of our system,which at present is quite limited, should reducethe frequency of some types of failure.
To ob-tain further improvement, we will have to extendour technique to deal with input containing un-known words.
It should be possible to do thisin a straightforward way by adding dictionary en-tries for the closed syntactic lasses, guessing frommorphological c ues the syntactic lass(es) of newwords not in the dictionary, obtaining a parse, andthen applying the techniques just described (witha new word treated as a semantic unknown, notbelonging to any class).Our system only offers suggestions; it doesnot aspire to correct the user's input.
That wouldbe an unreasonable expectation for our simple sys-tem, which does not maintain any user or dis-course model.
Our current system typically gen-erates several equally-rated suggestions for an ill-formed input.
For a more sophisticated system69which does maintain a richer model, correctionmay be a feasible goal.
Specifically, we might gen-erate the suggested questions as we do now andthen see if any question corresponds to a plausiblegoal.8.
AcknowledgementsThis report is based upon work supported bythe National Science Foundation under Grant No.DCR-8501843 and the Defense Advanced ResearchProjects Agency under Contract N00014-85-K-0163 from the Office of Naval Research.References\[1\] J. G. Carbonell and P. J. Hayes, 1983, Re-covery Strategies for Parsing Extragrammati-cal Language.
Am.
J. Computational Linguis-tics 9(3-4), pp.
123-146.\[2\] A. Gal and J. Minker, 1985, A Natural Lan-guage Data Base Interface that Provides Coop-erative Answers.
Proc.
Second Conf.
ArtificialIntelligence Applications, IEEE Computer So-ciety, pp.
352-357.\[3\] R. H. Granger 1983 The NOMAD System:Expectation-Based Detection and Correction ofErrors during Understanding of Syntacticallyand Semantically Ill-Formed Text.
Am.
J. Com-"putational Linguistics, 9(3-4), pp.
188-196.\[4\] S. C. Kwasney and N. K. Sondheimer, 1981,Relaxation Techniques for Parsing Ill-FormedInput.
Am.
J. Computational Linguistics, 7, pp.99-108.\[5\] E. Mays, 1980, Failures in Natural LanguageSystems: Applications to Data Base Query Sys-tems.
Proc.
First Nat'l Conf.
Artificial Intelli-gence (AAAI-80), pp.
327-330.\[6\] A. Meyers, 1985, VOX - An Extensible Nat-ural Language Processor.
Proc.
IJCAI-85, LosAngeles, CA, pp.
821-825.\[7\] L. A. Miller, G. E. Heidorn, and K. Jensen,1981, Text-critiquing with the EPISTLE Sys-tem: An Author's Aid to Better Syntax.
InProc.
Nat'l Comp.
Conf., AFIPS Press, Arling-ton, VA, pp.
649-655.\[8\] M. Selfridge, 1986, Integrated Processing Pro-duees Robust Understanding, ComputationalLinguistics, 12(2), pp.
89-106.\[9\] R. M. Weischedel and J. E. Black, 1980, Re-sponding Intelligently to Unparsable Inputs.Am.
J. Computational Linguistics, 6(2), pp.
97-109.\[10\] R. M. Weischedel and N. K. Sondheimer,1983, Meta-rules as a Basis for Processing Ill-Formed Input.
Am.
J. Computational Linguis-tics, 9(3-4), pp.
161-177.\[11\] Y. Wilks, 1975, An Intelligent Analyser andUnderstander of English.
Comm.
ACM 18, pp.264-274.70
