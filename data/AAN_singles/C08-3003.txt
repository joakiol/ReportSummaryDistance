Coling 2008: Companion volume ?
Posters and Demonstrations, pages 153?156Manchester, August 2008A Toolchain for GrammariansBruno GuillaumeLORIAINRIA Nancy Grand-EstBruno.Guillaume@loria.frJoseph Le RouxLORIANancy Universit?eJoseph.Leroux@loria.frJonathan MarchandLORIANancy Universit?eJonathan.Marchand@loria.frGuy PerrierLORIANancy Universit?eGuy.Perrier@loria.frKar?en FortLORIAINRIA Nancy Grand-EstKaren.Fort@loria.frJennifer PlanulLORIANancy Universit?eJennifer.Planul@loria.frAbstractWe present a chain of tools used by gram-marians and computer scientists to developgrammatical and lexical resources fromlinguistic knowledge, for various naturallanguages.
The developed resources areintended to be used in Natural LanguageProcessing (NLP) systems.1 IntroductionWe put ourselves from the point of view of re-searchers who aim at developing formal grammarsand lexicons for NLP systems, starting from lin-guistic knowledge.
Grammars have to represent allcommon linguistic phenomena and lexicons haveto include the most frequent words with their mostfrequent uses.
As everyone knows, building suchresources is a very complex and time consumingtask.When one wants to formalize linguistic knowl-edge, a crucial question arises: which mathemat-ical framework to choose?
Currently, there is noagreement on the choice of a formalism in the sci-entific community.
Each of the most popular for-malisms has its own advantages and drawbacks.
Agood formalism must have three properties, hard toconciliate: it must be sufficiently expressive to rep-resent linguistic generalizations, easily readable bylinguists and computationally tractable.
Guidedby those principles, we advocate a recent formal-ism, Interaction Grammars (IGs) (Perrier, 2003),the goal of which is to synthesize two key ideas,expressed in two kinds of formalisms up to now:using the resource sensitivity of natural languagesc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.as a principle of syntactic composition, which isa characteristic feature of Categorial Grammars(CG) (Retor?e, 2000), and viewing grammars asconstraint systems, which is a feature of unifica-tion grammars such as LFG (Bresnan, 2001) orHPSG (Pollard and Sag, 1994).Researchers who develop large lexicons andgrammars from linguistic knowledge are con-fronted to the contradiction between the necessityto choose a specific grammatical framework andthe cost of developing resources for this frame-work.
One of the most advanced systems de-voted to such a task is LKB (Copestake, 2001).LKB allows grammars and lexicons to be devel-oped for different languages, but only inside theHPSG framework, or at most a typed feature struc-ture framework.
Therefore, all produced resourcesare hardly re-usable for other frameworks.
Ourgoal is to design a toolchain that is as much as pos-sible re-usable for other frameworks than IG.Our toolchain follows the following architecture(see Figure 1):?
First, for building grammars, we use XMG(Section 3.1) which translates the sourcegrammar into an object grammar.?
IGs that we have developed with XMG areall lexicalized.
Therefore, the object grammarhas to be anchored in a lexicon (Section 3.2)in order to produce the anchored grammar.?
Then, when analyzing a sentence, we startwith a lexical disambiguation module (Sec-tion 3.3).?
The resulting lexical selections, presented inthe compact form of an automaton, are finallysent to the LEOPAR parser (Section 3.4).153LEOPARsource grammarXMGobject grammar lexiconsanchoringinput sentencelexical disambiguationoutput parse treesanchored grammarautomatonparsingFigure 1: Toolchain architecture2 Interaction GrammarsIGs (Perrier, 2003) are a grammatical formalismbased on the notion of polarity.
Polarities expressthe resource sensitivity of natural languages bymodeling the distinction between saturated and un-saturated syntactic structures.
Syntactic composi-tion is represented as a chemical reaction guidedby the saturation of polarities.
In a more preciseway, syntactic structures are underspecified treesequipped with polarities expressing their satura-tion state.
They are superposed under the con-trol of polarities in order to saturate them.
InCG, Tree Adjoining Grammars (TAGs) and De-pendency Grammars, syntactic composition canalso be viewed as a mechanism for saturating po-larities, but this mechanism is less expressive be-cause node merging is localized at specific places(root nodes, substitution nodes, foot nodes, ad-junction nodes .
.
.).
In IGs, tree superposition isa more flexible way of realizing syntactic compo-sition.
Therefore, it can express sophisticated con-straints on the environment in which a polarity hasto be saturated.
From this angle, IGs are relatedto Unification Grammars, such as HPSG, becausetree superposition is a kind of unification, but withan important difference: polarities play an essen-tial role in the control of unification.3 Description of the Toolchain3.1 The XMG Grammar CompilerThe first piece of software in our toolchain isXMG1(Duchier et al, 2004), a tool used to de-velop grammars.
XMG addresses the issue of de-signing wide-coverage grammars: it is based on adistinction between source grammar, written by ahuman, and object grammar, used in NLP systems.XMG provides a high level language for writingsource grammars and a compiler which translatesthose grammars into operational object grammars.XMG is particularly adapted to develop lexical-ized grammars.
In those grammars, parsing a sen-tence amounts to combining syntactical items at-tached to words.
In order to have an accurate lan-guage model, it may be necessary to attach a hugenumber of syntactical items to some words (verbsand coordination words, in particular) that describethe various usages of those words.
In this context,a grammar is a collection of items representingsyntactical behaviors.
Those items, although dif-ferent from each other, often share substructures(for instance, almost all verbs have a substruc-ture for subject verb agreement).
That is to say,if a linguist wants to change the way subject-verbagreement is modeled, (s)he would have to mod-ify all the items containing that substructure.
Thisis why designing and maintaining strongly lexical-ized grammars is a difficult task.The idea behind the so-called metagrammati-cal approach is to write only substructures (calledfragments) and then add rules that describe thecombinations (expressed with conjunctions, dis-junctions and unifications) of those fragments toobtain complete items.Fragments may contain syntactic, morpho-syntactic and semantic pieces of information.
Anobject grammar is a set of structures containingsyntactic and semantic information, that can be an-chored using morpho-syntactic information storedin the interface of the structure (see Section 3.2).During development and debugging stages, por-tions of the grammar can be evaluated indepen-dently.
The grammar can be split into various mod-ules that can be shared amongst grammars.
Fi-nally, graphical tools let the users explore the in-heritance hierarchy and the partial structures be-fore complete evaluation.1XMG is freely available under the CeCILL license athttp://sourcesup.cru.fr/xmg154XMG is also used to develop TAGs (Crabb?e,2005) and it can be easily extended to other gram-matical frameworks based on tree representations.3.2 Anchoring the Object Grammar with aLexiconThe tool described in the previous section buildsthe set of elementary trees of the grammar.
Thetoolchain includes a generic anchoring mechanismwhich allows to use formalism independent lin-guistic data for the lexicon part.Each structure produced by XMG comes withan interface (a two-level feature structure) whichdescribes morphological and syntactical con-straints used to select words from the lexicon.
Du-ally, in the lexicon, each inflected form of the natu-ral language is described by a set of two-level fea-ture structures that contain morphological and syn-tactical information.If the interface of an unanchored tree unifieswith some feature structure associated with w inthe lexicon, then an anchored tree is produced forthe word w.The toolchain also contains a modularized lexi-con manager which aims at easing the integrationof external and formalism independent resources.The lexicon manager provides several levels of lin-guistic description to factorize redundant data.
Italso contains a flexible compilation mechanism toimprove anchoring efficiency and to ease lexicondebugging.3.3 Lexical DisambiguationNeutralization of polarities is the key mechanismin the parsing process as it is used to control syn-tactic composition.
This principle can also be usedto filter lexical selections.
For a input sentence, alexical selection is a choice of an elementary treefrom the anchored grammar for each word of thesentence.Indeed, the number of possible lexical selec-tions may present an exponential complexity inthe length of the sentence.
A way of filter-ing them consists in abstracting some informationfrom the initial formalism F to a new formalismFabs.
Then, parsing in Fabsallows to eliminatewrong lexical selections at a minimal cost (Boul-lier, 2003).
(Bonfante et al, 2004) shows that po-larities allow original methods of abstraction.Following this idea, the lexical disambiguationmodule checks the global neutrality of every lex-ical selection for each polarized feature: a set oftrees bearing negative and positive polarities canonly be reduced to a neutral tree if the sum of thenegative polarities for each feature equals the sumof its positive polarities.Counting the sum of positive and negative fea-tures can be done in a compact way by using an au-tomaton.
This automaton structure allows to shareall paths that have the same global polarity bal-ance (Bonfante et al, 2004).3.4 The LEOPAR ParserThe next piece of software in our toolchain is aparser based on the IGs formalism2.
In additionto a command line interface, the parser providesan intuitive graphical user interface.
Parsing canbe highly customized in both modes.
Besides, theprocessed data can be viewed at each stage of theanalysis via the interface so one can easily checkthe behavior of the grammar and the lexicons inthe parsing process.The parsing can also be done manually: one firstchooses a lexical selection of the sentence givenby the lexer and then proceeds to the analysis byneutralizing nodes from the selection.
This way,the syntactic composition can be controlled by theuser.4 ResultsOur toolchain has been used first to produce a largecoverage French IG.
Most of the usual syntacticalconstructions of French are covered.
Some nontrivial constructions covered by the grammar are,for instance: coordination, negation (in French,negation is expressed with two words with com-plex placement rules), long distance dependencies(with island constraints).
The object grammar con-tains 2,074 syntactic structures which are producedby 455 classes in the source grammar.The French grammar has been tested on theFrench TSNLP (Test Suite for the Natural Lan-guage Processing) (Lehmann et al, 1996); this testsuite contains around 1,300 grammatical sentencesand 1,600 ungrammatical ones.
The fact that ourgrammar is based on linguistic knowledge ensuresa good coverage and greatly limits overgeneration:88% of the grammatical sentences are correctlyparsed and 85% of the ungrammatical sentencesare rejected by our grammar.2LEOPAR is freely available under the CeCILL license athttp://www.loria.fr/equipes/calligramme/leopar155A few months ago, we started to build an En-glish IG.
The modularity of the toolchain was anadvantage to build this grammar by abstracting theinitial grammar and then specifying the abstractkernel for English.
The English TSNLP has beenused to test the new grammar: 85% of the gram-matical sentences are correctly parsed and 84%of the ungrammatical sentences are rejected.
It isworth noting that those scores are obtained with agrammar that is still being developed .5 Future workThe toolchain we have presented here aims at pro-ducing grammars and lexicons with large cover-age from linguistic knowledge.
This justifies thechoice of discarding statistical methods in the firststage of the toolchain development: in the twosteps of lexical disambiguation and parsing, wewant to keep all possible solutions without dis-carding even the less probable ones.
Now, in anext future, we have the ambition of using thetoolchain for parsing large raw corpora in differ-ent languages.For French, we have a large grammar and a largelexicon, which are essential for such a task.
The in-troduction of statistics in the two modules of lexi-cal disambiguation and parsing will contribute tocomputational efficiency.
Moreover, we have toenrich our parsing strategies with robustness.
Wealso ambition to integrate semantics into grammarsand lexicons.Our experience with English is a first step to takemulti-linguality into account.
The crucial pointis to make our grammars evolve towards an evenmore multi-lingual architecture with an abstractkernel, common to different languages, and differ-ent specifications of this kernel for different lan-guages, thus following the approach of the Gram-matical Framework (Ranta, 2004).Finally, to make the toolchain evolve towardsmulti-formalism, it is first necessary to extendXMG for more genericity; there is no fundamentalobstacle to this task.
Many widespread formalismscan then benefit from our original methods of lex-ical disambiguation and parsing, based on polari-ties.
(Kahane, 2006) presents the polarization ofseveral formalisms and (Kow, 2007) shows thatthis way is promising.ReferencesBonfante, G., B. Guillaume, and G. Perrier.
2004.
Po-larization and abstraction of grammatical formalismsas methods for lexical disambiguation.
In CoL-ing?2004, 2004, pages 303?309, Geneva, Switzer-land.Boullier, P. 2003.
Supertagging: A non-statisticalparsing-based approach.
In IWPT 03, pages 55?65,Nancy, France.Bresnan, J.
2001.
Lexical-Functional Syntax.
Black-well Publishers, Oxford.Copestake, A.
2001.
Implementing Typed FeatureStructure Grammars.
CSLI Publications.Crabb?e, B.
2005.
Repr?esentation informatique degrammaires fortement lexicalis?ees : application `a lagrammaire d?arbres adjoints.
Phd thesis, Universit?eNancy 2.Duchier, D., J.
Le Roux, and Y. Parmentier.
2004.The metagrammar compiler : A NLP Applicationwith a Multi-paradigm Architecture.
In SecondInternational Mozart/Oz Conference - MOZ 2004,Charleroi, Belgium.Kahane, S. 2006.
Polarized unification grammars.In 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 137?144,Sydney, Australia.Kow, E. 2007.
Surface realisation: ambiguity and de-terminism.
Phd thesis, Universit?e Nancy 2.Lehmann, S., S. Oepen, S. Regnier-Pros, K. Netter,V.
Lux, J. Klein, K. Falkedal, F. Fouvry, D. Estival,E.
Dauphin, H. Compagnion, J. Baur, L. Balkan, andD.
Arnold.
1996.
TSNLP ?
Test Suites for Natu-ral Language Processing.
In CoLing 1996, Kopen-hagen.Perrier, G. 2003.
Les grammaires d?interaction.
Ha-bilitation thesis, Universit?e Nancy 2.Pollard, C.J.
and I.A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.Ranta, A.
2004.
Grammatical Framework: A Type-Theoretical Grammar Formalism.
Journal of Func-tional Programming, 14(2):145?189.Retor?e, C. 2000.
The Logic of Categorial Grammars.ESSLI?2000, Birmingham.156
