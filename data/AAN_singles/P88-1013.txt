PROJECT APRIL - -  A PROGRESS REPORTRobin Haigh, Geoffrey Sampson, Eric AtwellCenlre for Computer Analysis of Language and Speech,University of Leeds,Leeds LS2 9JT, UKABSTRACTParsing techniques based on rules defininggrammaticality are difficult to use with authenticinputs, which are often grammatically messy.Instead, the APRIL system seeks a labelled treesu~cture which maximizes a numerical measureof conformity to statistical norms derived flom asample of parsed text.
No distinction betweenlegal and illegal trees arises: any labelled treehas a value.
Because the search space is largeand has an irregular geometry, APRIL seeks thebest tree using simulated annealing, a stochasticoptimization technique.
Beginning with an arbi-Irary tree, many randomly-generated localmodifications are considered and adopted orrejected according to their effect on tree-value:acceptance decisions are made probabilistically,subject o a bias against advexse moves which isvery weak at the outset but is made to increaseas the random walk through the search spacecontinues.
This enables the system to convergeon the global optimum without getting trappedin local optima.
Performance of an early ver-sion of the APRIL system on authentic inputs isyielding analyses with a mean accuracy of75.3% using a schedule which increases pro-cessing linearly with sentence-length;modifications currently being implementedshould eliminate a high proportion of theremaining errors.INTRODUCTIONProject APRIL (Annealing Parser for ~a l~-tic Input Language) is constructing a softwaresystem that uses the stochastic optimizationtechnique known as "simulated annealing'"(Kirkpatnck et al 1983, van T ~rhoven & Aatts1987) to parse authentic English inputs by seek-ing labelled trce-su~ctures that maximize ameasure of plausibility defined in terms ofempirical statistics on parse-tree configurationsdrawn from a dmahase of mavnolly parsedEnglish toxL This approach is a response to thefact that "real-life" English, such as them~u,Jial in the Lancaster-Oslo/Bergen Corpuson which our research focuses, does not appearto conform to a fixed set of grammatical rules.
(On the LOB Corpus and the research back-ground from which Project APRIL emerged, seeGarside et al (1987).
A crude pilot version ofthe APRIL system was described in Sampson(1986).
)Orthodox computational linguistics isheavily influenced by a concept of languageaccording to which the set of all strings over thevocabulary of the language is partitioned into aclass of grammatical strings, which possess ana-lyses all parts of which conform to a finite setof rules defining the language, and a class ofstrings which are ungrammatical nd for whichthe question of their grammatical stntctureaccordingly does not arise.
Even systems whichset out to handle "deviant" sentences com-monly do so by referring them to particular"non-deviant" sentences of which they aredeemed to be distortions.
In our wcck withauthentic texts, however, we find the "gramma-ticality" concept unhelpful.
It frequendy hap-pens that a word-sequence occurs which violatessome recognized rule of English grammar, yetany reader can understand the passage withoutdifficulty, and it often seems unlikely that mostreaders would notice the violation.
Further-more, a problem which is probably even moretroublesome for the rule-based approach is thatthere is an apparently endless diversity of con-structious that no-one would be likely todescribe as ungrammatical or devianL Impres-sionistically it appears that any attempt to statea finite set of rules covering everything thatoccurs in authentic English text is doomed to goon adding more rules as long as more text isexamined; Sampson (1987) adduced objectiveevidence supporting this impression.Our approach, therefore, is to define a func-tion which associates a figure of merit with any104possible tree having labels drawn from a recog-uized alphabet of grammatical category-symbols; any input sentence is parsed by seek-ing the highest-valued tree possible for that sen-tence.
The analysis process works the sameway, whether the input is impeccably grammati-cal or quite bizarre.
No conwast between legaland illegal labelled trees arises: a tree whichwould ordinarily be described as thoroughly ille-gal is in our terms just a tree whose figure ofmerit is relatively very poor.This conception of parsing as optimizationof a function defined for all inputs seems to usnot implausible as a model of how peopleunderstand language.
But that is not our con-cern; what matters to us is that this modelseems very fimitful for automatic language-processing systems.
It has a theoretical dir,~l-vantage by comparison with rule-basedapproaches: if an input is perfectly granunaticalbut contains many out-of-the-way (i.e.
low fi'e-quency) constructions, the correct analysis maybe assigned a low figure of merit relative tosome alternative analysis which treats the sen-tence as an imperfect approximation to a struc-ture composed of high-frequency onstructions.However, our experience is that, in authenticEnglish, "trick sentences" of this kind tend tobe much rarer than textbooks of theoreticallinguistics might lead one m imagine.
Againstthis drawback our approach balances the advan-tage of robusmess.
No input, no matter howbizarre, can can cause our system simply to failto return any analysis.
Our sponsors, the RoyalSignals and Radar Establishment (an agency ofthe U.K. Ministry of Defence) 1 ar~ principallyinterested in speech analysis, and arguably thisrobusmess hould be even more advantageousfor spoken language, which makes little use ofconstructions that are legitimate but rechercM,while it contains a great dead that is sloppy orincorrecLPARSING SCHEMEAny automatic parser needs some external .standard against which its output is judged.
Our"target" parses are those given by a schemepreviously evolved for analysis of LOB Corpusmaterial, which is sketched in Garside et aLI Proj~t APRIL has hem sponuned since De-cember 1986 under contract MOD2062~I28(RSRE);we me grateful to the Minhmy of Defmce for permis-sion to publish this paper.
(1987, chap.
7) and laid down in minute detailin unpublished documentation.
This schemewas applied in manually parsing sentences total-ling ca 50,000 words drawn from the variousLOB genres: this TreeBank, as we call it, alsoserves as our source of grammatical statistics.A major objective in the definition of the pars-ing scheme and the construction of theTreeBank was consistency: wherever alternativeanalyses of a complex consm~ction might besuggested (as a malxer of analytic style asopposed to genuine ambiguity in sense), thescheme alms to stipulate which of the alterna-fives is to be used.
It is this need to ensure thegreatest possible consistency which sets a practi-cal limit to the size of the available database;producing the TreeBank took most of oneteacher's research time for two years.The parses yielded by the TreeBank schemeare immedlate-cunstituent a alyses of conven-tional type: they were designed so far as possi-ble to be theoretically uncontroversial.
Theywere not designed to be especially convenientfor stochastic parsing, which we had not at thattime thought of.The prior existence of the TreeBank is alsothe reason why we are working with writtenlanguage rather than speech: at present we haveno equivalent resource for spoken English.THE PRINCIPLES OF SIMULATEDANNEALINGTo explain how APRIL works, two chiefissues must be clarified.
One is the simulatedannealing technique used to locate the highest-valued tree in the set of poss~le labelled trees;the other is the function used to evaluate anysuch tree.We will begin by explaining the techniqueof simulated annealing.
This technique usesstochastic (randomizing) methods to locate goodsolutions; it is now widely exploited, in domainswhere combinatorial explosion makes the searchspace too vast for exhaustive examination,where no algorithm is av.aii~ble which leads sys-tematically to the optimal solution, and wherethere is a considerable degree of "fzustration"in the sense of Toulouse (1977), meaning that aseeming improvement in one feature of a solu-tion often at the same time worsens ome otherfeature of the solution, so that the problem can-not be decomposed into small subproblemswhich can each be optimized separately.
(Com-105pare how, in parsing, deciding to attach a con-stiment A as a daughter of a constituent B maybe a relatively attractive way of "using up" A,at the cost of making B a less plm~ible consti-tuent han it would be without A.
)One simple optimization technique, iterafiveimprovement, begins by selecting a solutionarbitrarily and then makes a long series of smallmodifications, drawn from a class ofmodifications which is defined in such a waythat any point in the solution-space can bereached from any other point by a chain ofmodifications each belonging to the class.
Ateach step the value of the solution obtained bymalting some such change is compared with thevalue of the current solution.
The change isaccepted and the new solution becomes currentif it is an improvement; otherwise the change isrejected, the existing solution retained, and analternative modification is tried.
The processterminates on reaching a solution superior toeach of its neighbours, i.e.
when none of theavailable modifications i an improvement.As it stands, such a technique is useless forparsing.
It is too easy for the system to becometrapped at a point which is better than itsimmediate neighbonrs but which is by no meansthe best solution overall, i.e.
at a local but not aglobal optimum.Simulated annealing is a variant which dealswith this difficulty by using a more sophisti-cated rule for deciding whether to accept orreject a modification.
In the variant we use, afavourable step is always accepted; but anunfavonrable step is rejected only if the loss ofmerit resulting from the step exceeds a certainthreshold.
This acceptance threshold is ran-domly generated at each step from a biasseddistribution; it may at any lime be very high orvery low, but its mean value is made todecrease in accordance with some definedschedule as the iteration proceeds, so that ini-tially almost atl moves are accepted, good orbad, but moves which are severely detrimentalsoon start to be rejected, and in the later stagesalmost all detrimental moves are avoided.
Thisscheme was originally devised as a simulationof the thermodynamic processes involved in theslow cooling of certain materials, hence thename "simulated annealing".
Acceptingmodifications which worsen the current ree is atfirst sight a surprising idea, but such movesprevent the system getting stuck and insteedopen up new possibilities; at the same time,there is an inexorable overall trend towardsimprovement.
As a result, the system tends toseek out high-valued areas of the solution spaceinitially in terms of gross features, and later interms of progressively finer detail.
Again, theprocess terminates at a local optimum, but notbefore exploring the possibilities o thoroughlythat this is in general the global optimum.
Withcertain simplifying assumptions, it has beenshown mathematically that the global optimumis always found (Lundy & Mees, 1986): in prac-tice, the procedure appears to work well underrather less stringent conditions than thosedemanded by mathematical treaunents hat haveso far appeared" and our application does in facttake several liberties with the "pure" algorithmas set out in the literature.ANNEALING PARSE-TREESTo apply simulated annealing toa  givenproblem, it is necessary to define (a) a space ofpossible solutions, Co) a class of solutionmodifications which provides a mute from anypoint in the space to any other, and (c) anannealing schedule (i.e.
an initial value for themean acceptance threshold, a specification ofthe rate at which this mean is reduced, and acriterion for terminating the Im3cess).Solution spaceFor us, the solution space for an input son-tence n wc~ls long is the set of all rootedlabelled trees having n leaves, in which the leafnodes are labelled with the word-class codescorresponding to the words of the sentence (fortest inputs drawn from LOB, these are the codesgiven in the Tagged version of the LOB corpus)and the non-terminal nodes have labels drawnfrom the set of grammatical-category labelsspecified in the parsing scheme.
The root nodeof a tree is assigned a fixed label, but any othernon-terminal node may bear any category label.Move setA set of possible parse-tree modificationsallowing any tree to be reached from any othercan be defined as follows.
To generate amodification, pick a non-terminal node of thecurrent ree at random.
Choose at random oneof the move-types Merge or Hive.
If Merge ischosen, delete the chosen node by replacing it,in its mother's dAughter-sequence, with its owndaughter-sequence.
If the move-type is Hive,choose a random continuous ubsequence of the106node's daughter-sequence, and replace thatsubsequence by a new node having the subse-quence as its own daughter-sequence; assign alabel drawn from the non-terminal alphabet othe new node.
R is easy to see that the class ofMerge and Hive moves allows at least one routefrom any u~e to any other tree over the sameleaf-sequence: repeated Merging will ultimatelymm any tree into the "flat tree" in which evea 7leaf is directly dominated by the root, and sinceMerge and Hive moves mirror one another, if itis possible to get from any tree to the flat Iree itis equally possible to get from the flat tree toany tree.
(In reality, there will be numerousalternative mutes between a given pair of trees,most of which will not pass through the flattree.
)New labels for nodes created by Hive movesare chosen randomly, with a bias determined bythe labels of the daughter-sequence.
This biasattempts to increase the frequency with whichcorrect labels are chosen, without limiting thechoice to the label which is best for thedaughter-sequence considered in isolation,which may not of course be the best in context.An early version of APRIL limited itself tojust the Merge and Hive moves.
However, agood move-set for annealing should not onlypermit any solution to be reached from anyother solution, but should also be such thatpaths exist between good trees which do notinvolve passing through much inferior inter-mediate stages.
(See for example the remarkson depth in Lundy & Mees (1986).)
Tostrengthen this tendency in our system it hasproved desirable to add a third class of Re, attachmoves to the move-set.
To generate a Reattachmove, choose randomly any non-root node inthe current tree, eliminate the arc linking thechosen node to its mother, and insert an arclinking it to a node randomly chosen fi'om theset of nodes topologically capable of being itsmother.
Currently, we are exploring the cost-effectiveness of adding a fourth move-type,which relabels a randomly-chosen node withoutchanging the tree shape; a m~lr for the future isto investigate how best to determine the propor-tions in which different move-types are gen-erated.ScheduleThe annealing schedule is ultimately acompromise between processing time and qual-ity of results: although the process can bespeeded up at will, inevitably speeding up toomuch will make the system more likely to con-verge on a false solution when presented with adifficult sentence.
Optimizing the schedule is atopic to which much attention has been paid inthe literature of simulated annealing, but itseems fair to say that the discussion remainsinconclusive.
Since it does not in general bearon the specifically linguistic aspects of our pro-ject' we have deferred etailed consideration ofthis issue.
We intend however to look at thevariation in rate with respect o type of input,exploiting the division of the TreeBank (like itsparent LOB Corpus) into genres: we wouldexpect hat the simple if sometimes messy sen-tences of dialogue in fiction, for instance, can bedealt with more quickly than the precise but tor-tuons grammar of legal prose.At present, then, we reduce the acceptancethreshold at a constant rate which errs on theslow side; we expect hat important advances inefficiency will result from improvements in theschedule, but such improvements may be over-taken by other developments to be described inlater sections.
The rate of decrease of theacceptance threshold is varied inversely with thelength of the sentence, with the consequencethat the run time varies roughly linearly withsentence l ngth.EVALUATING PARSE-TREESThe function of the evaluation system is toassign a value to any labelled tree whatsoever,in such a way that the correct parse-tree for anygiven sentence is the highest-valued tree whichcan be drawn over the sentence, and the valuesof other trees over the same sentence reflecttheir relative merit (though comparisons ofvalues between trees drawn over diffeaent sen-tences axe not required to be meaningful).An advantage of the annealing technique isthat in principle it makes no demands on theform of evaluation: in parfic-lae, we are notconstrained by the nature of the parsing algo-rithm to assume that the grammar of English iscontext-free or has any other special property.Nevertheless, we have found it convenient inour early work to start with a context-freeassumption and work forward from that.With this assumption, a tree can be treatedas a set of productions m~ld2 .
.
.d ,ccm'esponding to the various nodes in the tree,where m is a non-terminul label and each d~ is107either a non-terminal label or a wordtag, and wecan assign to any such production a probabilityrepresenting the frequency of such productions,as a proportion of all productions having m asmother-label; the value assigned to the entiretree will be the product of the probabilities ofits productions.The statistic required for any production,then, is an estimate of its probability ofoccurrence, and this may be derived from itsfrequency in the manually-parsed TreeBank.
(To avoid circularity, sentences in the TreeBank?
which are to be used to test the performance ofthe parser are excluded from the frequencycounts.)
Clearly, with a dam_base of this size,the figures obtained as production probabilitieswill be distorted by sampling effects.
In gen-eral, even quite large sampling errors have littleinfluence on results, since the frequency con-trasts between alternative tree-structures tend tobe of a higher order of magnitude, butdifficulties arise with very low frequency pro-doctions: in particular, as an important specialcase, many quite normal productions wi l l  fail tooccur at all in the TrecBank, and are thus notdistinguished in our raw data from virtually-impossible productions.
But it seems reasonableto infer probability estimates for unobservedproductions from those of similar, observed pro-ductions, and more generally to smooth the rawfrequency observations using statistical tech-niques (see for insmnco Good (1953)).
(Oneconsequence of such smoothing is that no pro-duction is ever assigned a probability of zero.
)A natural response by linguists would be to saythat a relationship of "'similarity" between pro-ductions needs to be defined in terms of subtle,complex theoretical issues.
However, so far wehave been impressed by results obtainable inpractice using very crude similarity ~Intlon-ships.Our current evaluation method is onlyslightly more elaborate than the techniquedescribed in Sampson (1986), whereby the pro-hability of a woducfion was derived exclusivelyfrom the observed frequencies of the variouspairwise transitions between daughter-labelswithin the production (that is, for any produc-tion m--->dodt ...d.d.+t, where do and d.+t areboundary symbols, the estimated probability wasthe product of the observed frequencies of thevarious transitions m-+...d~ di+x... (O~gi ~;n)with zeroes replaced by small positive values).This approach was suggested by the success ofthe CLAWS system for grammatically disambi-gtt~tit~g words in context (Garside et al 1987,chap.
3), which uses an essentially Markovianmodel, and by the success of Markovian tech-niques in automatic spee.~h understandingresearch from the Harpy project onwards (e.g.Lea 1986, Cravero et al 1984).Subsequent versions of APRIL have begunto incorporate an evaluation measure whichmakes limited use of non-Markovian relation-ships.
Each label in the non-terminal lphabetis associated with a transition etwork, each arcof which is assigned a probability as well as a(non-terminal or terminal) label: the probabilityestimate for a node labelled m is the product ofthe probabilities of the consecutive arcs in thetransition etwork for m which carry the labelsof the node's daughter-sequence.
Unlike theFSAs commonly used in computational linguis-tics, ours are required to accept any label-sequence: a "crazy" sequence will be assigneda low but non-zero value.
Indeed our networksmake no attempt o reflect subtle nuances ofgrammaticallty; they diverge from Markoviannetworks only to represent a limited number offundamental issues that are lost in a pure Mar-kovian system.APRIL  IN  ACT IONIt is rather difficult to convey non-mathematically a feel for the way in which thesystem converges from an arbitrary tree to thecorrect ree by a sequence of random moves.
Inthe earliest stages, labelled nodes are beingctented, moved and destroyed at a rapid rate inall regions of the tree, but after a while it startsto become apparent hat certain local featmesare tending to persisL These tend to be themost strongly marked features grammatically,such as constituents comprising a single pro-noun or an attxili.gry verb.
While such a featll~persists, surrounding developments are con-strained by it: other new nodes can be created ifthey are compatible, but new nodes whichwould conflict cannot appear.
Thus the gram-matical words form a skeleton on which thephrases and clauses can start to hang, and wefind there is a perceptible gradually ~creasingtendency for the tree to consist of nodes andsubstructures which fit together well into acoherent whole.
Speaking anthropomorphically.the system tends to make the simplest and mostclear-cut decisions first, and the more subtledecisions later.
But the strength of the system108lies in the fact that no such decision is final:each is constantly being reappraised in the lightof developments in its surroundings.CURRENT PERFORMANCEIn order to assess APRIL's performance weneed an objective way to compare output withtarget parses, i.e.
a measure of similaritybetween pairs of distinct trees over the samesequence of leaf nodes.
We know of no stan-dard measure for this, but we have evolved onethat seems natural and fair.
Fcf each word ofinput we compare the chains of node-labelsbetween leaf and root in the two trees, and com-pute the number of labels which match eachother and occur in the same order in the twochains as a proportion of all labels in bothchains; then we average over the words.
(Weomit discussion of a refinement included inorder to ensure that only fully-identical tree-pairs receive 100% scores.)
With respect o ourparsing technique, this performance measme isconservative, since averaging over words meansthat high-level nodes, dominating many weeds,contribute more than low-level nodes to overallscores, but APRIL tends to discover structure ina broadly bottom-up fashion.At the time of writing, our latest resultswere those of a test run carried out in esxlyFebruary 1988, 14 months into a 36-month pro-ject, over 50 LOB sentences drawn from techni-cal prose and fiction, with mean, minimum, andmaximum lengths of 22.4, 3, and 140 wordsrespectively.
(Note that our parsing scheme,and therefore our word-counts, treat punctuationmarks as separate "words".)
The alphabet ofnon-terminal labels from which APRIL chooseswhen labelling new nodes included virtually allthe distinctions required by our scheme in anadequately parsed output; and it includedseveral of the more significant phrase-subeategory distinctions whose role in thescheme is to guide the parser towards thecorrect output rather than to appear in the out-put (Garside et al 1987, p. 89).
Altogether thenon-terminal alphabet included 113 distinctlabels.For a 22-word sentence, the number of dis-tinct trees with labels drawn from a 113-member alphabet (and obeying the resirictionsour scheme places on the occurrence of nodeswith only single daughters) is about 5?10103 .To put this in perspective, finding a particularlabelled tree in a search space of this size is likefinding a single atom of gold in a solid cube ofgold a thousand million light-years on a side.Mean scoc?
of the 50 output analyses was75.3%.
This is not yet good enough for incor-poration into practical language-processingapplication software, but bearing in mind thepreliminary nature of the current version of thesystem we are heartened by how good thescores already are.
Furthennct'e, above about15 words there appears to be no correlationbetween sentence-length and output score,offering a measure of support fc?
our decisionto use an annealing schedule which increasesprocessing time roughly linearly with inputlength.
Kirkpalrick et al (1983) suggest hatlineax processing is adequate for simulatedannealing in other domains, but orthodox deter-ministic approaches to computational linguisticsdo not permit linear parsing except for highlyartificial well-behaved languages.The parse-trees prodir.~ in this test run typ-ically show a substantially correct overall slruc-ture, with isolated local areas of difficulty wheresome deviant analysis has been preferred, com-monly a constituent wrongly labelled or a con-stituent attached to the surrounding tree at thewrong level An encouraging point is that anumber of these errors relate to debatable gram-matical issues and might not be seen as errors atall.
In the years when our target parsingscheme was being evolved, we worded aboutthe idiomatic onstruction to try and \[do some-thing\]: should try and Verb be grouped as aconstituent equivalent o a single verb?
Wefinally decided not: we chose to analyse suchsequences as co-ordinated clauses.
But, wherethe test sentences include the sequence I want totry and find properties that .... APRIL hasparsed: I want \[Ti to \[VB& try and fred\] proper-ties that...\].--the analysis which we came closeto choosing as correct.A sentence which raises less trivial issues isillustrated (this is from text E23 in the LOBCorpus).
We show the manual parse in theTreeBank (Fig.l), and APRIL's current output(Fig.
2), which contains two errors.
First, thefinal phrase of the human mind should beattached as a posunodifier of mysteries.
At thisstage no distinction was made in word-taggingbetween of and other prepositions: there is how-ever a su'ong tendency (though no absolute rule,of course) for an of phrase following a noun tobe a postmodifier of the noun, and it iscorrespondingly rare for such a phrase to be an109G._z ts~mG."
--ilI.-"--Im.z-~ ~ ~- ; ~ ~,~ ~ ?
~-~ ~;Q.-~ ~ ~ ?
~<-~ ;"i;0e~Q.CDmt- I .b -e,'-"1Z' -- IZ- ~ j -  goE!- ~ ~~ 8Q)e-U,.110immediate constituent of a clause.
Distinguish-ing of from other prepositions will enable theevaluation system to incorlxrate a representa-tion of this piece of statistical evidence in itswansition probabilities, whereupon this errorshould be avoided.Secondly, APRIL has rejected the interpreta-tion of the clause beginning representing.., as aposunedifier of tulle, and has chosen to makethis clause appositional to the clause beginningplacing... (our scheme represents apposition in amanner akin to subordination).
1"his error canbe avoided ff we note the su'ong tendency inEnglish (again, not an absolute rule) thatposlmodifiers of any kind are most oftenattached to the nearest element hat they canlogically postmodify, that is, that the chain-structure typified in Fig.
1 is preferred to theembedding-structure in Fig.
2.
A preliminarystatistical analysis of the TreeBank appears tosupport the conjecture---developed from thehypothesis formulated by Yngve (1960)---that"the greater the depth of a non-terminal consti-tuent, the greater the probability that either (a)this constituent is the last daughter of itsmother, or Co) the next daughter of its mother isa punctuation mark."
(We adapt Yngve'snotion of depth to non-binary trees.)
With thisformulation it is relatively easy to incorporateinto our evaluation system the necessary adjust-ments to our transition probabilities, so thattrees of the more common type will tend to bepreferred; but note that nothing prevents anoverriding local consideration f~m leading theparser to prefer, in any given case, an analysisthat departs from this general principle.
WhenOtis is done, the initial context-free assumptionwill have been abandoned, to the extent thatdepths of constituents are taken into account aswell as their labels, but no change is needed inthe parsing algcxithm.The erroneous parsings in this example floutno rules of syntax that we can formulate andseem to involve no impossible productions, othey could be regarded as valid alternatives in asyntactically ambiguous entence: a generativegmnmar could be expected to generate this sen-tence in several different ways, of whichAPRIL's would be one.
However, as ourmethods improve we find that more and moresentences which are in principle ambiguoushave the same reading selected by purelystatistical-syntactic considerations a is preferredby human readers, who also have access tosemantic and pragmatic onsiderations.FUTURE DEVELOPMENTSApart from improving the evaluation systemas already discussed, we plan in the near futureto adapt APRIL so that it accepts raw text ratherthan sequences of word-class codes as input,choosing tags for grammatically ambiguouswords as part of the same optimization processby which higher struclm'e is discovered.
Theavailability of the (probabilistic but determinis-tic) CLAWS word-tagging system meant thatthis was not seen as an initial priority.
Rawtext input involves a number of problems relat-ing to orthographic matters uch as capitaliza-tion and hyphenated words, but these problemshave essentially been solved by our Lancastercolleagues (Garside et aL, chap.
8).
We alsointend soon to move from the current static sys-tent whose inputs are isolated sentences to adynamic system within which annealing willtake place in a window that scans across con-tinuous text, with the system discoveringsentence-boundaries for itself along with lower-level structure.
(If our system is in due courseadapted to parse spoken rather than writteninput, it is clear that all constituent boundariesincluding those of sentences would need to bediscovered rather than given, and a corollaryappears to be that the processing time neededfor any length of input must increase onlylinearly with input length.)
As adumbrated inSampson (1986), we expect to make thedynamic annealing parser more efficient byexploiting the insight of Marcus (1980) thatback'wacking ~.is rarely needed in naturallanguage parsing: a gradient of processing inten-sity will be imposed on the annealing window,with most processing occuning in the "newest"parts of the current ree where valuable movesare most likely to be found.However, simulated annealing is necessarilycostly in terms of amount of processing needed.
(The schedule used for the run discussed aboveinvolved on the order of 30,000 steps generatedper input word.)
Partic~l~ly with a view toapplications such as re.-time speech analysis, itwould be desirable to find a way of exploitingparallel processing in order to minimize thetime needed for parse-lree optimization.Parallelizing our approach to parsing is not aswaightforward matter, one cannot, for instance,s~nply associate a process with each node of atree, since there is no nalaral identity relation-111ship between odes in different trees within thesolution space for an input.
However, we haveevolved an algorithm for concurrent tree anneal-ing which we believe should be efficient, and aresearch proposal currently under considerationwill implement this algorithm, using a wanspumrarray which is about to be installed by a consm'-tium of Leeds departments.
In view of thewidespread occurrence of hierarchical sm~c~a-esin cognitive science, we hope that a successfulsolution to the problem of l~a'allel tree-optimization should be of interest to workers inother areas, such as image processing, as well asto linguists.Lastly, a reasonable criticism of our work sofar is that our target parses are those defined bya purely "surfacy" parsing scheme.
For somespeech-prvcessing applications surface parsing isadequate, but for many purposes deeperlanguage analyses are needed.
We see no issueof principle hindering the extension of ourmethods to deep parsing, but at present here isa serious practical hindrance: our techniques canonly be applied after a target parsing schemehas been specified in sufficient detail mprescribe unambiguous analyses for allphenomena occurring in authentic English, andthen applied man~mlly to a large enough quan-tity of text to yield usable statistics.
A secondcurrently-pending research proposal plans mconvert the Gothenburg Corpus (Elleg~l 1978),which consists of relatively deep manual pars-ings of 128,000 words of the Brown Corpus ofAmerican English, into a database usable forthis purpose.mESERENCESCravero, M., et al 1984.
"Syntax drivenrecognition of connected words by Markovmodels".
Proceedings of the 1984 IEEE Inter-national Conference on Acoustics, Speech andSignal Processing.Elleg~rd, A.
1978.
The Syntactic Structure ofEnglish Texts.
Gothenburg Studies in English,43.Garside, R. G., et al, eds.
1987.
The Computa-tional Analysis of English.
Longraan.Good, I. J.
1953.
"The population frequenciesof species and the estimation of populationparameters".
Biometrika 40.237-64.Kirkpatrick, S. E., et al 1983.
"Optimizationby Simulated Annealing".
Science 220.671-80.van Laarhoven, P. J. M., & E. H. L. Aar~.1987.
Simulated Annealing: Theory and Appli-cations.
D. Reidel.Lea, R. G., ed.
1980.
Trends in Speech Recog-nition.
Prentice-Hall.Lundy, NL and A. Mees.
1986.
"Convergenceof an annealing algorithm".
Mathematical Pro-gramming 34.111-24.Marcus, M. P. 1980.
A Theory of SyntacticRecognition for Natural Language.
MIT Press.Sampson, G.R.
1986.
"A stochastic approachto parsing".
Proceedings of the llth Interna-tional Conference on Computational Linguistics(COLING '86), pp.
151-5.
\[GRS wishes to takethis opportunity to apologize for the inadvertentnear-coincidence of title between this paper andan important 1984 paper by T. Fujisaki.\]Sampson, G. R. 1987.
"'Evidence against he'grammafical'/'ungrammatical' d stinction".
InW.
Meijs, eeL, Corpus Linguistics and Beyond.Rodopi.Toulouse, G. 1977.
"Theory of the frustrationeffect in spin glasses.
I."
Communications onPhysics, 2.115-119.Yngve, V. 1960.
"A model and an hypothesisfor language structure".
Proceedings of theAmerican Philosophical Society, 104.dd A.-66.112
