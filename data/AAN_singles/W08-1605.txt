Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 33?40Manchester, August 2008Personalized, Interactive Question Answering on the WebSilvia QuarteroniUniversity of TrentoVia Sommarive 1438100 Povo (TN), Italysilviaq@disi.unitn.itAbstractTwo of the current issues of Question Answer-ing (QA) systems are the lack of personaliza-tion to the individual users?
needs, and the lackof interactivity by which at the end of each Q/Asession the context of interaction is lost.We address these issues by designing and im-plementing a model of personalized, interac-tive QA based on a User Modelling componentand on a conversational interface.
Our eval-uation with respect to a baseline QA systemyields encouraging results in both personaliza-tion and interactivity.1 IntroductionInformation overload, i.e.
the presence of an exces-sive amount of data from which to search for relevantinformation, is a common problem to Information Re-trieval (IR) and its subdiscipline of Question Answering(QA), that aims at finding concise answers to questionsin natural language.
In Web-based QA in particular, thisproblem affects the relevance of results with respect tothe users?
needs, as queries can be ambiguous and evenanswers extracted from documents with relevant con-tent but expressed in a difficult language may be ill-received by users.While the need for user personalization has been ad-dressed by the IR community for a long time (Belkinand Croft, 1992), very little effort has been carried outup to now in the QA community in this direction.
In-deed, personalized Question Answering has been ad-vocated in TREC-QA starting from 2003 (Voorhees,2003); however, the issue was solved rather expedi-tiously by designing a scenario where an ?average newsreader?
was imagined to submit the 2003 task?s defini-tion questions.Moreover, a commonly observed behavior in usersof IR systems is that they often issue queries not asstandalone questions but in the context of a wider in-formation need, for instance when researching a spe-cific topic.
Recently, a new research direction hasc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.been proposed, which involves the integration of QAsystems with dialogue interfaces in order to encour-age and accommodate the submission of multiple re-lated questions and handle the user?s requests for clar-ification in a less artificial setting (Maybury, 2002);however, Interactive QA (IQA) systems are still at anearly stage or applied to closed domains (Small et al,2003; Kato et al, 2006).
Also, the ?complex, inter-active QA?
TREC track (www.umiacs.umd.edu/?jimmylin/ciqa/) has been organized, but herethe interactive aspect refers to the evaluators being en-abled to interact with the systems rather than to dia-logue per se.In this paper, we first present an adaptation of UserModelling (Kobsa, 2001) to the design of personalizedQA, and secondly we design and implement an inter-active open-domain QA system, YourQA.
Section 2briefly introduces the baseline architecture of YourQA.In Section 3, we show how a model of the user?s read-ing abilities and personal interests can be used to effi-ciently improve the quality of the information returnedby a QA system.
We provide an extensive evaluationmethodology to assess such efficiency by improving onour previous work in this area (Quarteroni and Manand-har, 2007b).Moreover, we discuss our design of interactive QAin Section 4 and conduct a more rigorous evaluation ofthe interactive version of YourQA by comparing it tothe baseline version on a set of TREC-QA questions,obtaining encouraging results.
Finally, a unified modelof personalized, interactive QA is described in Section5.2 Baseline System ArchitectureThe baseline version of our system, YourQA, is able toextract answers to both factoid and non-factoid ques-tions from the Web.
As most QA systems (Kwok et al,2001), it is organized according to three phases:?
Question Processing: The query is classified andthe two top expected answer types are estimated; itis then submitted to the underlying search engine;?
Document Retrieval: The top n documents areretrieved from the search engine (Google, www.google.com) and split into sentences;33?
Answer Extraction:1.
A sentence-level similarity metric combininglexical, syntactic and semantic criteria is ap-plied to the query and to each retrieved doc-ument sentence to identify candidate answersentences;2.
Candidate answers are ordered by relevanceto the query; the Google rank of the answersource document is used as a tie-breaking cri-terion.3.
The list of top ranked answers is then re-turned to the user in an HTML page.Note that our answers are in the form of sentences withrelevant words or phrases highlighted (as visible in Fig-ure 2) and surrounded by their original passage.
Thisis for two reasons: we believe that providing a con-text to the exact answer is important and we have beenmostly focusing on non-factoids, such as definitions,which it makes sense to provide in the form of a sen-tence.
A thorough evaluation of YourQA is reported ine.g.
(Moschitti et al, 2007); it shows an F1 of 48?.7for non-factoids on Web data, further improved by aSVM-based re-ranker.In the following sections, we describe how the base-line architecture is enhanced to accommodate personal-ization and interactivity.3 User Modelling for PersonalizationOur model of personalization is centered on a UserModel which represents students searching for informa-tion on the Web according to three attributes:1. age range a ?
{7?
10, 11?
16, adult},2. reading level r ?
{basic,medium, advanced};3. profile p, a set of textual documents, bookmarksand Web pages of interest.Users?
age1and browsing history are typical UMcomponents in news recommender systems (Magniniand Strapparava, 2001); personalized search systemssuch as (Teevan et al, 2005) also construct UMs basedon the user?s documents and Web pages of interest.3.1 Reading Level EstimationWe approach reading level estimation as a supervisedlearning task, where representative documents for eachof the three UM reading levels are collected to be la-belled training instances and used to classify previouslyunseen documents.Our training instances consist of about 180 HTMLdocuments from a collection of Web portals2where1Although the reading level can be modelled separatelyfrom the age range, for simplicity we here assume that theseare paired in a reading level component.2Such Web portals include: bbc.co.uk/schools,www.think-energy.com, kids.msfc.nasa.gov.pages are explicitly annotated by the publishers ac-cording to the three reading levels above.
As a learn-ing model, we use unigram language modelling in-troduced in (Collins-Thompson and Callan, 2004) tomodel the reading level of subjects in primary and sec-ondary school.Given a set of documents, a unigram language modelrepresents such a set as the vector of all the words ap-pearing in the component documents associated withtheir corresponding probabilities of occurrence withinthe set.In the test phase of the learning process, for each un-classified document D, a unigram language model isbuilt (as done for the training documents).
The esti-mated reading level of D is the language model lmimaximizing the likelihood that D has been generatedby lmi(In our case, three language models lmiare de-fined, where i ?
{basic,medium, advanced}.).
Suchlikelihood is estimated using the function:L(lmi|D) =?w?DC(w,D) ?
log[P (w|lmi)], (1)where w is a word in the document, C(w, d) representsthe number of occurrences of w in D and P (w|lmi) isthe probability that w occurs in lmi(approximated byits frequency).3.2 Profile EstimationInformation extraction from the user?s documents as ameans of representation of the user?s interests, such ashis/her desktop files, is a well-established technique forpersonalized IR (Teevan et al, 2005).Profile estimation in YourQA is based on key-phraseextraction, a technique previously employed in severalnatural language tasks (Frank et al, 1999).For this purpose, we use Kea (Witten et al, 1999),which splits documents into phrases and chooses someof the phrases as be key-phrases based on two criteria:the first index of their occurrence in the source doc-uments and their TF ?
IDF score3with respect tothe current document collection.
Kea outputs for eachdocument in the set a ranked list where the candidatekey-phrases are in decreasing order; after experiment-ing with several values, we chose to use the top 6 askey-phrases for each document.The profile resulting from the extracted key-phrasesis the base for all the subsequent QA activity: any ques-tion the user submits to the QA system is answered bytaking such profile into account, as illustrated below.3.3 Personalized QA AlgorithmThe interaction between the UM component and thecore QA component modifies the standard QA processat the Answer Extraction phase, which is modified asfollows:3The TF ?
IDF of a term t in document D within a col-lection S is: TF?IDF (t,D, S) = P (t ?
D)?
?logP (t ?[S/D]).341.
The retrieved documents?
reading levels are esti-mated;2.
Documents having a different reading level fromthe user are discarded; if the remaining documentsare insufficient, part of the incompatible docu-ments having a close reading level are kept;3.
From the documents remaining from step 2, key-phrases are extracted using Kea;4.
The remaining documents are split into sentences;5.
Document topics are matched with the topics inthe UM that represent the user?s interests;6.
Candidate answers are extracted from the docu-ments and ordered by relevance to the query;7.
As an additional answer relevance criterion, thedegree of match between the candidate answerdocument topics and the user?s topics of interest isused and a new ranking is computed on the initiallist of candidate answers.Step 7 deserves some deeper explanation.
For eachdocument composing the UM profile and the retrieveddocument set, a ranked list of key-phrases is availablefrom the previous steps.
Both key-phrase sets are rep-resented by YourQA as arrays, where each row corre-sponds to one document and each column correspondsto the rank within such document of the key-phrase inthe corresponding cell.As an illustrative example, a basic user profile, cre-ated from two documents about Italian cuisine and themovie ?Ginger and Fred?, respectively, might result inthe following array:[pizza lasagne tiramisu recipe chef eggfred ginger film music movie review]The arrays of UM profile and retrieved documentkey-phrases are named P and Retr, respectively.
Wecall Retrithe document represented in the i-th row inRetr, and Pnthe one represented in the n-th row of P4.
Given kij, i.e.
the j-th key-phrase extracted fromRetri, and Pn, i.e.
the n-th document in P , we callw(kij, Pn) the relevance of kijwith respect to Pn.
Wedefinew(kij, Pn) ={|Retri|?j|Retri|, kij?
Pn0, otherwise(2)where |Retri| is the number of key-phrases of Retri.The total relevance of document Retriwith respect toP , wP(Retri), is defined as the maximal sum of therelevance of its key-phrases, obtained for all the rowsin P :wP(Retri) = maxn?P?kij?Retriw(kij, Pn).
(3)4Note that, while column index reflects a ranking basedon the relevance of a key-phrase to its source document, rowindex only depends on the name of such document.The personalized answer ranking takes wPinto ac-count as a secondary ranking criterion with respectto the baseline system?s similarity score; as before,Google rank of the source document is used as furthera tie-breaking criterion.Notice that our approach to User Modelling can beseen as a form of implicit (or quasi-implicit) relevancefeedback, i.e.
feedback not explicitly obtained from theuser but inferred from latent information in the user?sdocuments.
Indeed, we take inspiration from (Teevanet al, 2005)?s approach to personalized search, comput-ing the relevance of unseen documents (such as thoseretrieved for a query) as a function of the presence andfrequency of the same terms in a second set of docu-ments on whose relevance the user has provided feed-back.Our approaches to personalization are evaluated inSection 3.4.3.4 Evaluating PersonalizationThe evaluation of our personalized QA algorithms as-sessed the contributions of the reading level attributeand of the profile attribute of the User Model.3.4.1 Reading Level EvaluationReading level estimation was evaluated by first as-sessing the robustness of the unigram language modelsby running 10-fold cross-validation on the set of doc-uments used to create such models, and averaging theratio of correctly classified documents with respect tothe total number of documents for each fold.
Our re-sults gave a very high accuracy, i.e.
94.23% ?
1.98standard deviation.However, this does not prove a direct effect on theuser?s perception of such levels.
For this purpose, wedefined Reading level agreement (Ar) as the percentageof documents rated by the users as suitable to the read-ing level to which they were assigned.
We performeda second experiment with 20 subjects aged between 16and 52 and with a self-assessed good or medium En-glish reading level.
They evaluated the answers re-turned by the system to 24 questions into 3 groups (ba-sic, medium and advanced reading levels), by assessingwhether they agreed that the given answer was assignedto the correct reading level.Our results show that altogether, evaluators found an-swers appropriate for the reading levels to which theywere assigned.
The agreement decreased from 94% forAadvto 85% for Amedto 72% for Abas; this was pre-dictable as it is more constraining to conform to a lowerreading level than to a higher one.3.4.2 Profile EvaluationThe impact of the UM profile was tested by us-ing as a baseline the standard version of YourQA,where the UM component is inactive.
Ten adult par-ticipants from various backgrounds took part in theexperiment; they were invited to form an individualprofile by brainstorming key-phrases for 2-3 topics of35their interest chosen from the Yahoo!
directory (dir.yahoo.com): examples were ?ballet?, ?RPGs?
and?dog health?.For each user, we created the following 3 questionsso that he/she would submit them to the QA system:Qper, related to the user?s profile, for answering whichthe personalized version of YourQA would be used;Qbas, related to the user?s profile, for which the base-line version of the system would be used; and Qunr,unrelated to the user?s profile, hence not affected bypersonalization.
The reason why we handcrafted ques-tions rather than letting users spontaneously interactwith YourQA?s two versions is that we wanted the re-sults of the two versions to be different in order to mea-sure a preference.
After examining the top 5 results toeach question, users had to answer the following ques-tionnaire5:?
For each of the five results separately:TEST1: This result is useful to me:5) Yes, 4) Mostly yes, 3) Maybe, 2) Mostly not, 1)Not at allTEST2: This result is related to my profile:5) Yes, 4) Mostly yes, 3) Maybe, 2) Mostly not, 1)Not at all?
For the five results taken as a whole:TEST3: Finding the info I wanted in the result pagetook:1) Too long, 2) Quite long, 3) Not too long, 4)Quite little, 5) Very littleTEST4: For this query, the system results were sensi-tive to my profile:5) Yes, 4) Mostly yes, 3) Maybe, 2) Mostly not, 1)Not at allThe experiment results are summarized in Table 1.
TheTable 1: Profile evaluation results (avg ?
st.
dev.
)Measurement QrelQbasQunrTEST1 3.6?0.4 2.3?0.3 3.3?0.3TEST2 4.0?0.5 2.2?0.3 1.7?0.1TEST3 3.1?1.1 2.7?1.3 3.4?1.4TEST4 3.9?0.7 2.5?1.1 1.8?1.2first row reports a remarkable difference between theperceived usefulness for question Qrelwith respect toquestion Qbas(answers to TEST1).The results were compared by carrying out a one-way analysis of variance (ANOVA) and performing theFischer test using the usefulness as factor (with the5The adoption of a Likert scale made it possible to com-pute the average and standard deviations of the user commentswith respect to each answer among the top five returned by thesystem.
It was therefore possible to replace the binary mea-surement of perceived usefulness, relatedness and sensitivityused in (Quarteroni and Manandhar, 2007b) in terms of to-tal number of users with a more fine-grained one in terms ofaverage computed over the users.three queries as levels) at a 95% level of confidence.The test revealed an overall significant difference be-tween factors, confirming that users are positively bi-ased towards questions related to their own profile whenit comes to perceived utility.To analyze the answers to TEST2 (Table 1, row 2),which measured the perceived relatedness of each an-swer to the current profile, we used ANOVA again andand obtained an overall significant difference.
Hence,answers obtained without using the users?
profile wereperceived as significantly less related to those obtainedusing their own profile, i.e.
there is a significant differ-ence between Qreland Qbas.
As expected, the differ-ence between Qreland Qunris even more significant.Thirdly, the ANOVA table computed using averageperceived time (TEST3) as variable and the three ques-tions as factors did not give any significance, nor didany of the paired t-tests computed over each result pair.We concluded that apparently, the time spent browsingresults is not directly correlated to the personalizationof results.Finally, the average sensitivity of the five answers al-together (TEST4) computed over the ten participantsfor each query shows an overall significant difference inperceived sensitivity between the answers to questionQrel(3.9?0.7) and those to question Qbas(2.5?1.1)and Qunr(1.8?1.2).To conclude, our experience with profile evaluationshows that personalized QA techniques yield answersthat are indeed perceived as more satisfying to users interms of usefulness and relatedness to their own profile.4 InteractivityMaking a QA system interactive implies maintainingand efficiently using the current dialogue context andthe ability to converse with the user in a natural manner.Our implementation of IQA is guided by the followingconversation scenario:1.
An optional reciprocal greeting, followed by aquestion q from the user;2. q is analyzed to detect whether it is related to pre-vious questions or not;3.
(a) If q is unrelated to the preceding questions, itis submitted to the QA component;(b) If q is related to the preceding questions(follow-up question), it is interpreted by thesystem in the context of previous queries;a revised version of q, q?, is either directlysubmitted to the QA component or a requestfor confirmation (grounding) is issued to theuser; if he/she does not agree, the system asksthe user to reformulate the question until itcan be interpreted by the QA component;4.
As soon as the QA component results are avail-able, an answer a is provided;365.
The system enquires whether the user is interestedin submitting new queries;6.
Whenever the user wants to terminate the interac-tion, a final greeting is exchanged.4.1 Choosing a Dialogue ManagerAmong traditional methods for implementinginformation-seeking dialogue management, Finite-State (FS) approaches are the simplest.
Here, thedialogue manager is represented as a Finite-Statemachine, where each state models a separate phaseof the conversation, and each dialogue move encodesa transition to a subsequent state (Sutton, 1998).However, an issue with FS models is that they allowvery limited freedom in the range of user utterances:since each dialogue move must be pre-encoded in themodels, there is a scalability issue when addressingopen domain dialogue.On the other hand, we believe that other dialogue ap-proaches such as the Information State (IS) (Larsson etal., 2000) are primarily suited to applications requiringa planning component such as closed-domain dialoguesystems and to a lesser extent to open-domain QA.As an alternative approach, we studied conversa-tional agents (?chatbots?)
based on AIML (ArtificialIntelligence Markup Language), such as ALICE6.
Chat-bots are based on the pattern matching technique, whichconsists in matching the last user utterance against arange of dialogue patterns known to the system.
A co-herent answer is created by following a range of ?tem-plate?
responses associated with such patterns.As its primary application is small-talk, chatbot di-alogue appears more natural than in FS and IS sys-tems.
Moreover, since chatbots support a limited no-tion of context, they can handle follow-up recognitionand other dialogue phenomena not easily covered usingstandard FS models.4.2 A Wizard-of-Oz ExperimentTo assess the utility of a chatbot-based dialogue man-ager in an open-domain QA application, we conductedan exploratory Wizard of Oz experiment.Wizard-of-Oz (WOz) experiments are usually de-ployed for natural language systems to obtain initialdata when a full-fledged prototype is not yet available(Dahlbaeck et al, 1993) and consist in ?hiding?
a hu-man operator behind a computer interface to simulate aconversation with the user, who believes to be interact-ing with a fully automated prototype.We designed six tasks reflecting the intended typicalusage of the system (e.g.
: ?Find out who painted Guer-nica and ask the system for more information about theartist?)
to be carried out by 7 users by interacting withan instant messaging platform, which they were told tobe the system interface.6www.alicebot.org/The role of the Wizard was to simulate a limitedrange of utterances and conversational situations han-dled by a chatbot.User feedback was collected mainly by using apost-hoc questionnaire inspired by the experiment in(Munteanu and Boldea, 2000), which consists of ques-tions Q1to Q6in Table 2, col. 1, to be answered usinga scale from 1=?Not at all?
to 5=?Yes, absolutely?.From the WOz results, reported in Table 2, col.?WOz?, users appear to be generally very satisfied withthe system?s performances: Q6obtained an average of4.5?.5.
None of the users had difficulties in reformu-lating their questions when this was requested: Q4ob-tained 3.8?.5.
For the remaining questions, satisfactionlevels were high: users generally thought that the sys-tem understood their information needs (Q2obtained 4)and were able to obtain such information (Q1obtained4.3?.5).The dialogue manager and interface of YourQA wereimplemented based on the dialogue scenario and thesuccessful outcome of the WOz experiment.4.3 Dialogue Management AlgorithmsAs chatbot dialogue follows a pattern-matching ap-proach, it is not constrained by a notion of ?state?
:when a user utterance is issued, the chatbot?s strategy isto look for a pattern matching it and fire the correspond-ing template response.
Our main focus of attentionin terms of dialogue manager design was therefore di-rected to the dialogue tasks invoking external resources,such as handling follow-up questions, and tasks involv-ing the QA component.4.3.1 Handling follow-up questionsFor the detection of follow-up questions, the algo-rithm in (De Boni and Manandhar, 2005) is used, whichuses features such as the presence of pronouns andthe absence of verbs in the current question and wordrepetitions with the n previous questions to determinewhether qiis a follow-up question with respect to thecurrent context.
If the question q is not identified as afollow-up question, it is submitted to the QA compo-nent.
Otherwise, the reference resolution strategy be-low is applied on q, drawing on the stack S of previoususer questions:1.
If q is elliptic (i.e.
contains no verbs), its keywordsare completed with the keywords extracted by theQA component from the previous question in Sfor which there exists an answer.
The completedquery is submitted to the QA component;2.
If q contains pronoun/adjective anaphora, a chun-ker is used to find the most recent compatible an-tecedent in S. This must be a NP compatible innumber with the referent.3.
If q contains NP anaphora, the first NP in S con-taining all the words in the referent is used to re-place the latter in q.
When no antecedent can be37found, a clarification request is issued by the sys-tem until a resolved query can be submitted to theQA component.When the QA process is terminated, a message direct-ing the user to the HTML answer frame (see Figure 1) isreturned and a follow-up proposal or an enquiry aboutuser satisfaction is optionally issued.4.4 ImplementationTo implement the dialogue manager and allow a seam-less integration with our Java-based QA system, we ex-tended the Java-based AIML interpreter Chatterbean7.We started by augmenting the default AIML tag set(including tags such as <srai> and <that>) withtwo tags: <query>, to seamlessly invoke the core QAmodule, and <clarify>, to support follow-up detec-tion and resolution.Moreover, the interpreter allows to instantiate andupdate a set of variables, represented as context prop-erties.
Among others, we defined:a) userID, which is matched against a list of knownuser IDs to select a UM profile for answer extraction(see Section 5);b) the current query, which is used to dynamically up-date the stack of recent user questions used by the clar-ification request detection module to perform referenceresolution;c) the topic of conversation, i.e.
the keywords of thelast question issued by the user which received an an-swer.
The latter is used to clarify elliptic questions, byaugmenting the current query keywords with those inthe topic when ellipsis is detected.Figure 1 illustrates YourQA?s interactive version,which is accessible from the Web.
As in a normal chatapplication, users write in a text field and the currentsession history as well as the interlocutor replies are vi-sualized in a text area.4.5 Interactive QA evaluationFor the evaluation of interactivity, we built on our pre-vious results from a Wizard-of-Oz experiment and aninitial evaluation conducted on a limited set of hand-crafted questions (Quarteroni and Manandhar, 2007a).We chose 9 question series from the TREC-QA 2007campaign8.
Three questions were retained per series tomake each evaluation balanced.
For instance, the threefollowing questions were used to form one task: 266.1:?When was Rafik Hariri born?
?, 266.2: ?To what reli-gion did he belong (including sect)??
and 266.4: ?Atwhat time in the day was he assassinated?
?.Twelve users were invited to find answers to thequestions to one of them by using the standard versionof the system and to the second by using the interactiveversion.
Each series was evaluated at least once usingboth versions of the system.
At the end of the exper-iment, users had to give feedback about both versions7chatterbean.bitoflife.cjb.net.8trec.nist.govTable 2: Interactive QA evaluation results obtained forthe WOz, Standard and Interactive versions of YourQA.Average ?
st. dev.
are reported.Question WOz Stand InteractQ1Did you get al the in-formation you wantedusing YourQA?4.3?.5 4.1?1 4.3?.7Q2Do you think YourQAunderstood what youasked?4.0 3.4?1.3 3.8?1.1Q3How easy was it toobtain the informationyou wanted?4.0?.8 3.9?1.1 3.7?1Q4Was it difficult to re-formulate your ques-tions when requested?3.8?.5 - 3.9?.6Q5Do you think youwould use YourQAagain?4.1?.6 3.3?1.6 3.1?1.4Q6Overall, are you satis-fied with YourQA?4.5?.5 3.7?1.2 3.8?1.2Q7Was the pace of inter-action with YourQAappropriate?- 3.2?1.2 3.3?1.2Q8How often wasYourQA sluggish inreplying to you?- 2.7?1.1 2.5?1.2Q9Which interface didyou prefer?- 41.7% 58.3%of the system by filling in the satisfaction questionnairereported in Table 2.Although the paired t-test conducted to comparequestionnaire replies to the standard and interactive ver-sions did not register statistical significance, we believethat the evidence we collected suggests a few interest-ing interpretations.First, a good overall satisfaction appears with bothversions of the system (Q6), with a slight difference infavor of the interactive version.
The two versions ofthe system seem to offer different advantages: whilethe ease of use of the standard version was rated higher(Q3), probably because the system?s reformulation re-quests added a challenge to users used to search engineinteraction, users felt they obtained more informationusing the interactive version (Q1).Concerning interaction comfort, users seemed to feelthat the interactive version understood better their re-quests than the standard one (Q2); they also found iteasy to reformulate questions when the former askedto (Q6).
However, while the pace of interaction wasjudged slightly more appropriate in the interactive case(Q7), interaction was considered faster when using thestandard version (Q4).
This partly explains the fact thatusers seemed more ready to use again the standard ver-sion of the system (Q5).7 out of 12 users (58.3%) answered the ?preference?question Q9by saying that they preferred the inter-active version.
The reasons given by users in their38Figure 1: YourQA?s interactive interfacecomments were mixed: while some of them were en-thusiastic about the chatbot?s small-talk features, oth-ers clearly said that they felt more comfortable with asearch engine-like interface.
Most of the critical aspectsemerging from our overall satisfactory evaluation de-pend on the specific system we have tested rather thanon the nature of interactive QA, to which none of suchresults appear to be detrimental.We believe that the search-engine-style use and in-terpretation of QA systems are due to the fact that QAis still a very little known technology.
It is a challengefor both developers and the larger public to cooperatein designing and discovering applications that take ad-vantage of the potentials of interactivity.5 A Unified ModelOur research so far has demonstrated the utility of per-sonalization and interactivity in a QA system.
It isthus inevitable to regard the formulation of a unifiedmodel of personalized, interactive QA as a valuable by-product of these two technologies.
In this perspective,we propose the following dialogue scenario:1.
The user interacts with the dialogue interface for-mulating an utterance q;2.
If q is recognized as a question, it is analyzed bythe dialogue manager (DM) to detect and resolvemultiple and follow-up questions;3.
As soon as a resolved version q?of q is available,the DM passes q?to the QA module; the latter pro-cesses q?and retrieves a set Retr(q?)
of relevantdocuments;4.
The QA module exchanges information with theUM component which is responsible of maintain-ing and updating the User Model of the currentuser, u; Based on u, the QA module extracts a listL(q?, u) of personalized results from Retr(q?);5.
The DM produces a reply r, which is returnedalong with L(q?, u) to the user via the dialogue in-terface;6.
Once terminated, the current QA session is loggedinto the dialogue history H(u), that will be usedto update u;Concerning step 4, an efficient strategy for elicitingthe User Model from the user is yet to be specified atthis stage: the current one relies on the definition ofa context variable userID in the dialogue manager,which at the moment corresponds to the user?s name.
Anumber of AIML categories are created are created forYourQA to explicitly ask for the user?s name, whihc isthen assigned to the userID variable.Figure 2 illustrates an example of a personalized, QAsession in YourQA where the user?s name is associatedwith a basic reading level UM.
This affects the docu-ment retrieval phase, where only documents with sim-ple words are retained for answer extraction.6 Conclusions and Future WorkIn this paper, we present an efficient and light-weightmethod to personalize the results of a Web-based QAsystem based on a User Model representing individualusers?
reading level, age range and interests.
Our resultsshow the efficiency of reading level estimation, and a39Figure 2: Screenshot from a personalized, interactive QA session.
Here, the user?s name (?Kid?)
is associated witha UM requiring a basic reading level, hence the candidate answer documents are filtered accordingly.significant improvement in satisfaction when filteringanswers based on the users?
profile with respect to thebaseline version of our system.
Moreover, we introducea dialogue management model for interactive QA basedon a chat interface and evaluate it with optimistic con-clusions.In the future, we plan to study efficient strategies forbootstrapping User Models based on current and pastconversations with the present user.
Another problemto be solved is updating user interests and reading lev-els based on the dialogue history, in order to make thesystem fully adaptive.AcknowledgementsThe research reported here was mainly conducted at the Com-puter Science Department of the University of York, UK, un-der the supervision of Suresh Manandhar.ReferencesBelkin, N. J. and W.B.
Croft.
1992.
Information filter-ing and information retrieval: Two sides of the samecoin?
Comm.
ACM, 35(12):29?38.Collins-Thompson, K. and J. P. Callan.
2004.
A lan-guage modeling approach to predicting reading diffi-culty.
In HLT/NAACL?04.Dahlbaeck, N., A. Jonsson, and L. Ahrenberg.
1993.Wizard of Oz studies: why and how.
In IUI ?93.De Boni, M. and S. Manandhar.
2005.
Implement-ing clarification dialogue in open-domain questionanswering.
JNLE, 11.Frank, E., G. W. Paynter, I. H. Witten, C. Gutwin,and C. G. Nevill-Manning.
1999.
Domain-specifickeyphrase extraction.
In IJCAI ?99.Kato, T., J. Fukumoto, F.Masui, and N. Kando.
2006.Woz simulation of interactive question answering.
InIQA?06.Kobsa, A.
2001.
Generic user modeling systems.UMUAI, 11:49?63.Kwok, C. T., O. Etzioni, and D. S. Weld.
2001.
Scalingquestion answering to the web.
In WWW?01.Larsson, S., P. Ljungl?of, R. Cooper, E. Engdahl, andS.
Ericsson.
2000.
GoDiS?an accommodating di-alogue system.
In ANLP/NAACL?00 WS on Conver-sational Systems.Magnini, B. and C. Strapparava.
2001.
Improving usermodelling with content-based techniques.
In UM?01.Maybury, M. T. 2002.
Towards a question answeringroadmap.
Technical report, MITRE Corporation.Moschitti, A., S. Quarteroni, R. Basili, and S. Man-andhar.
2007.
Exploiting syntactic and shallow se-mantic kernels for question/answer classification.
InACL?07.Munteanu, C. and M. Boldea.
2000.
Mdwoz: A wizardof oz environment for dialog systems development.In LREC?00.Quarteroni, S. and S. Manandhar.
2007a.
A chatbot-based interactive question answering system.
InDECALOG?07, Rovereto, Italy.Quarteroni, S. and S. Manandhar.
2007b.
Usermodelling for personalized question answering.
InAI*IA?07, Rome, Italy.Small, S., T. Liu, N. Shimizu, and T. Strzalkowski.2003.
HITIQA: an interactive question answeringsystem- a preliminary report.
In ACL?03 WS on Mul-tilingual summarization and QA.Sutton, S. 1998.
Universal speech tools: the CSLUtoolkit.
In ICSLP?98.Teevan, J., S. T. Dumais, and E. Horvitz.
2005.
Per-sonalizing search via automated analysis of interestsand activities.
In SIGIR ?05.Voorhees, E. M. 2003.
Overview of the TREC 2003Question Answering Track.
In TREC?03.Witten, I. H., G. W. Paynter, E. Frank, C. Gutwin, andC.
G. Nevill-Manning.
1999.
KEA: Practical auto-matic keyphrase extraction.
In ACM DL.40
