Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 88?95,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsNarrowing the Loop: Integration of Resources andLinguistic Dataset Development with Interactive Machine LearningSeid Muhie YimamFG Language TechnologyDepartment of Computer ScienceTechnische Universit?at Darmstadthttp://www.lt.tu-darmstadt.deyimam@lt.informatik.tu-darmstadt.deAbstractThis thesis proposal sheds light on the role ofinteractive machine learning and implicit userfeedback for manual annotation tasks and se-mantic writing aid applications.
First we fo-cus on the cost-effective annotation of train-ing data using an interactive machine learn-ing approach by conducting an experimentfor sequence tagging of German named en-tity recognition.
To show the effectiveness ofthe approach, we further carry out a sequencetagging task on Amharic part-of-speech andare able to significantly reduce time used forannotation.
The second research directionis to systematically integrate different NLPresources for our new semantic writing aidtool using again an interactive machine learn-ing approach to provide contextual paraphrasesuggestions.
We develop a baseline systemwhere three lexical resources are combined toprovide paraphrasing in context and show thatcombining resources is a promising direction.1 IntroductionMachine learning applications require considerableamounts of annotated data in order to achieve a goodprediction performance (Pustejovsky and Stubbs,2012).
Nevertheless, the development of such an-notated data is labor-intensive and requires a certaindegree of human expertise.
Also, such annotateddata produced by expert annotators has limitations,such as 1) it usually does not scale very well sinceannotation of a very large data set is prohibitively ex-pensive, and 2) for applications which should reflectdynamic changes of data over time, static trainingdata will not serve its purpose.
This issue is com-monly known as concept drift (Kulesza et al, 2014).There has been a lot of effort in automaticallyexpanding training data and lexical resources usingdifferent techniques.
One approach is the use of ac-tive learning (Settles et al, 2008) which aims at re-ducing the amount of labeled training data requiredby selecting most informative data to be annotated.For example it selects the instances from the trainingdataset about which the machine learning model isleast certain how to label (Krithara et al, 2006; Set-tles, 2010; Raghavan et al, 2006; Mozafariy et al,2012).
Another recent approach to alleviate bot-tleneck in collecting training data is the usage ofcrowdsourcing services (Snow et al, 2008; Costaet al, 2011) to collect large amount of annotationsfrom non-expert crowds at comparably low cost.In an interactive machine learning approach, theapplication might start with minimal or no train-ing data.
During runtime, the user provides simplefeedback to the machine learning process interac-tively by correcting suggestions or adding new anno-tations and integrating background knowledge intothe modeling stage (Ware et al, 2002).Similarly, natural language processing (NLP)tasks, such as information retrieval, word sense dis-ambiguation, sentiment analysis and question an-swering require comprehensive external knowledgesources (electronic dictionaries, ontologies, or the-sauri) in order to attain a satisfactory performance(Navigli, 2009).
Lexical resources such as Word-Net, Wordnik, and SUMO (Niles and Pease, 2001)also suffer from the same limitations that the ma-chine learning training data faces.88Figure 1: An online interface for the semantic writing aidapplication.
Paraphrase suggestions are presented from asystematic combination of different NLP resources.This proposal focuses on the development and en-hancement of training data as well as on systematiccombinations of different NLP resources for a se-mantic writing aid application.
More specifically weaddress the following issues: 1) How can we pro-duce annotated data of high quality using an inter-active machine learning approach?
2) How can wesystematically integrate different NLP resources?
3)How can we integrate user interaction and feedbackinto the interactive machine learning system?
More-over, we will explore the different paradigms of in-teractions (when should the machine learning pro-duce a new model, how to provide useful sugges-tions to users, and how to control annotators behav-ior in the automation process ).
To tackle these prob-lems, we will look at two applications, 1) an annota-tion task using a web-based annotation tool and 2) asemantic writing aid application, a tool with an on-line interface that provides users with paraphrase de-tection and prediction capability for a varying writ-ing style.
In principle, the two applications havesimilar nature except that the ultimate goal of theannotation task is to produce a fully annotated datawhereas the semantic writing aid will use the im-proved classifier model instantly.
We have identifieda sequence tagging and a paraphrasing setup to ex-plore the aforementioned applications.Sequence tagging setup: We will employ an an-notation tool similar to WebAnno (Yimam et al,2014) in order to facilitate the automatic acquisitionof training data for machine learning applications.Our goal is to fully annotate documents sequentiallybut interactively using the machine learning supportin contrast to an active learning setup where the sys-tem presents portions of the document at a time.Paraphrasing setup: The semantic writing aidtool is envisioned to improve readability of docu-ments and provide varied writing styles by suggest-ing semantically equivalent paraphrases and removeredundant or overused words or phrases.
Using sev-eral lexical resources, the system will detect and pro-vide alternative contextual paraphrases as shown inFigure 1.
Such paraphrasing will substitute words orphrases in context with appropriate synonyms whenthey form valid collocations with the surroundingwords (Bolshakov and Gelbukh, 2004) based on thelexical resource suggestion or using statistics gath-ered from large corpora.
While the work of Bha-gat and Hovy (2013) shows that there are differentapproaches of paraphrasing or quasi-paraphrasingbased on syntactical analysis, we will also furtherexplore context-aware paraphrasing using distribu-tional semantics (Biemann and Riedl, 2013) and ma-chine learning classifiers for contextual similarity.2 Related WorkThere have been many efforts in the developmentof systems using an adaptive machine learning pro-cess.
Judah et al (2009) developed a system wherethe machine learning and prediction process incor-porates user interaction.
For example, for sensitiveemail detection system, the user is given the oppor-tunity to indicate which features, such as body ortitle of the message, or list of participants, are im-portant for prediction so that the system will accord-ingly learn the classification model based on the userpreference.
Similarly, recommender systems usu-ally provide personalized suggestions of products toconsumers (Desrosiers and Karypis, 2011).
The rec-ommendation problem is similar to an annotationtask as both of them try to predict the correct sug-gestions based on the existing user preference.CueFlik, a system developed to support Web im-age search (Amershi et al, 2011), demonstratesthat active user interactions can significantly impactthe effectiveness of the interactive machine learningprocess.
In this system, users interactively define vi-sual concepts of pictures such as product photos orpictures with quiet scenery, and they train the systemso as to learn and re-rank web image search results.JAAB (Kabra et al, 2013) is an interactive ma-chine learning system that allows biologists to usemachine learning in closed loop without assistance89from machine learning experts to quickly train clas-sifiers for animal behavior.
The system allows usersto start the annotation process with trustworthy ex-amples and train an initial classifier model.
Further-more, the system enables users to correct sugges-tions and annotate unlabeled data that is leveragedin subsequent iteration.Stumpf et al (2007) investigate the impact of userfeedback on a machine learning system.
In additionto simple user feedback such as accepting and re-jecting predictions, complex feedback like selectingthe best features, suggestions for the reweighting offeatures, proposing new features and combining fea-tures significantly improve the system.2.1 Combination and Generation of ResourcesThere are different approaches of using existingNLP resources for an application.
Our approachmainly focuses on a systematic combination of NLPresources for a specific application with the helpof interactive machine learning.
As a side product,we plan to generate an application-specific NLP re-source that can be iteratively enhanced.The work by Lavelli et al (2002) explores howthematic lexical resources can be built using an itera-tive process of learning previously unknown associ-ations between terms and themes.
The research is in-spired by text categorization.
The process starts withminimal manually developed lexicons and learnsnew thematic lexicons from the user interaction.Jonnalagadda et al (2012) demonstrate the use ofsemi-supervised machine learning to build medicalsemantic lexicons.
They demonstrated that a distri-butional semantic method can be used to increase thelexicon size using a large set of unannotated texts.The research conducted by Sinha and Mihalcea(2009) concludes that a combination of several lexi-cal resources generates better sets of candidate syn-onyms where results significantly exceed the perfor-mance obtained with one lexical resource.While most of the existing approaches such asUBY (Gurevych et al, 2012) strive at the construc-tion of a unified resource from several lexical re-sources, our approach focuses on a dynamic and in-teractive approach of resource integration.
Our ap-proach is adaptive in such a way that the resourceintegration depends on the nature of the application.3 Overview of the Problem3.1 Interactive Machine Learning ApproachThe generation of large amounts of high qualitytraining data to train or validate a machine learningsystem at one pass is very difficult and even unde-sirable (Vidulin et al, 2014).
Instead, an interac-tive machine learning approach is more appropriatein order to adapt the machine learning model itera-tively using the train, learn, and evaluate technique.Acquiring new knowledge from newly addedtraining data on top of an existing trained machinelearning model is important for incremental learn-ing (Wen and Lu, 2007).
An important aspect ofsuch incremental and interactive machine learningapproach is, that the system can start with mini-mal or no annotated training data and continuouslypresents documents to a user for annotation.
On theway, the system can learn important features fromthe annotated instances and improve the machinelearning model continuously.
When a project re-quires to annotate the whole dataset, an interactivemachine learning approach can be employed to in-crementally improve the machine learning model.3.2 Paraphrasing and Semantic Writing AidAcquisition and utilization of contextual para-phrases in a semantic writing aid ranges from in-tegration of structured data sources such as ontolo-gies, thesauri, dictionaries, and wordnets over semi-structured data sources such as Wikipedia and ency-clopedia entries to resources based on unstructureddata such as distributional thesauri.
Paraphrasesusing ontologies such as YAGO (Suchanek et al,2007) and SUMO provide particular semantic rela-tions between lexical units.
This approach is domainspecific and limited to some predefined form of se-mantic relations.
Structured data sources such asWordNet support paraphrase suggestions in the formof synonyms.
Structured data sources have lim-ited coverage and they usually do not capture con-textual paraphrases.
Paraphrases from unstructuredsources can be collected using distributional similar-ity techniques from large corpora.
We can also ob-tain paraphrase suggestions from monolingual com-parable corpora, for example, using multiple trans-lations of foreign novels (Ibrahim et al, 2003) ordifferent news articles about the same topics (Wang90and Callison-Burch, 2011).
Moreover, paraphrasescan also be extracted from bilingual parallel cor-pora by ?pivoting?
a shared translation and rankingparaphrases using the translation probabilities fromthe parallel text (Ganitkevitch and Callison-Burch,2014).The research problem on the one hand is the adap-tation of such diverse resources on the target seman-tic writing aid application and on the other hand thecombination of several such resources using interac-tive machine learning to suit the application.4 Methodology: Paraphrasing ComponentThe combinations of lexical resources will be basedon the approach of Sinha and Mihalcea (2009),where candidate synonymous from different re-sources are systematically combined in a machinelearning framework.
Furthermore, lexical resourcesinduced in a data driven way such as distributionalthesauri (DT) (Weeds and Weir, 2005), will be com-bined with the structured lexical resources in an in-teractive machine learning approach, which incre-mentally learns weights through a classifier.
Wewill train a classifier model using features from re-sources, such as n-gram frequencies, co-occurrencestatistics, number of senses from WordNet, dif-ferent feature values from the paraphrase database(PPDB)1(Ganitkevitch and Callison-Burch, 2014),and syntactic features such as part of speech and de-pendency patterns.
Training data will be acquiredwith crowdsourcing by 1) using existing crowd-sourcing frameworks and 2) using an online inter-face specifically developed as a semantic writing aidtool (ref Figure 1).While the way the system provides suggestionsmight be based on many possible conditions, we willparticularly address at least the following ones: 1)non-fitting word detection, 2) detection of too manyrepetitions, and 3) detection of stylistic deviations.Once we have the resource combining componentin place, we employ an interactive machine learn-ing to train a classifier based on implicit user feed-back obtained as 1) users intentionally request para-phrasing and observe their actions (such as which ofthe suggestion they accept, if they ignore all sugges-tions, if the users provide new paraphrase by them-1http://paraphrase.orgselves, and so on), and 2) the system automaticallysuggests candidate paraphrases (as shown in Figure1) and observe how the user interacts.5 Experiments and EvaluationWe now describe several experimental setups thatevaluate the effectiveness of our current system, thequality of training data obtained, and user satisfac-tion in using the system.
We have already conductedsome preliminary experiments and simulated evalu-ations towards some of the tasks.5.1 Annotation TaskAs a preliminary experiment, we have conducted aninteractive machine learning simulation to investi-gate the effectiveness of this approach for namedentity annotation and POS tagging tasks.
For thenamed entity annotation task, we have used thetraining and development dataset from the Ger-mEval 2014 Named Entity Recognition Shared Task(Benikova et al, 2014) and the online machinelearning tool MIRA2(Crammer and Singer, 2003).The training dataset is divided by an increasing size,as shown in Table 1 to train the system where everylarger partition contains sentences from earlier parts.From Figure 2 it is evident that the interactive ma-chine learning approach improves the performanceof the system (increase in recall) as users continuecorrecting the suggestions provided.Sentences precision recall F-score24 80.65 1.12 2.2160 62.08 6.68 12.07425 71.57 35.13 47.13696 70.36 43.02 53.401264 71.35 47.15 56.785685 77.22 56.57 65.308770 77.83 60.16 67.8610 812 78.06 62.72 69.5515 460 78.14 64.96 70.9524 000 80.15 68.82 74.05Table 1: Evaluation result for the German named entityrecognition task using an interactive online learning ap-proach with different sizes of training dataset tested onthe fixed development dataset.2https://code.google.com/p/miralium/91Furthermore, an automation experiment is carriedout for Amharic POS tagging to explore if interac-tive machine learning reduces annotation time.
Inthis experiment, a total of 34 sentences are manu-ally annotated, simulating different levels of preci-sion and recall (ref Table 2) for automatic sugges-tions as shown in Figure 3.
We have conducted thisannotation task several times to measure the savingsin time when using automatic annotation.
When nosuggestion is provided, it took about 67 minutes foran expert annotator to completely annotate the doc-ument.
In contrast to this, the same annotation taskwith suggestions (e.g with recall of 70% and preci-sion of 60%) took only 21 minutes, demonstrating asignificant reduction in annotation cost.recall (%)no Auto.
30 50 70prec (%) no Auto.
67 - - -60 - 53 33 2170 - 45 29 2080 - 42 28 18Table 2: Experimentation of interactive machine learningfor different precision and recall levels for Amharic POStagging task.
The cell with the precision/recall intersec-tion records the total time (in minutes) required to fullyannotate the dataset with the help of interactive automa-tion.
Without automation (no Auto.
), annotation of allsentences took 67 minutes.Figure 2: Learning curve showing the performance of in-teractive automation using different sizes of training data5.2 Evaluation of ParaphrasingFor the semantic writing aid tool, we need to create aparaphrasing component (see Sec.
3.2).
We conductan evaluation by comparing automatic paraphrasesagainst existing paraphrase corpora (Callison-Burchet al, 2008).
The Microsoft Research ParaphraseCorpus (MSRPC) (Dolan et al, 2004) dataset,PPDB, and the DIRT paraphrase collections (Linand Pantel, 2001) will be used for phrase-level eval-uations.
The TWSI dataset (Biemann, 2012) will beused for the word level paraphrase evaluation.
Wewill use precision, recall, and machine translationmetrics BLEU for evaluation.Once the basic paraphrasing system is in placeand evaluated, the next step will be the improvementof the paraphrasing system using syntagmatic andparadigmatic structures of language as features.
Theprocess will incorporate the implementation of dis-tributional similarity based on syntactic structuressuch as POS tagging, dependency parsing, tokenn-grams, and patterns, resulting in a context-awareparaphrasing system, which offers paraphrases incontext.
Furthermore, interactive machine learningcan be employed to train a model that can be used toprovide context-dependent paraphrasing.5.2.1 Preliminary ExperimentsWe have conducted preliminary experiments fora semantic writing aid system, employing the Lan-guageTools (Naber, 2004) user interface to displayparaphrase suggestions.
We have used WordNet,PPDB, and JobimText DT3to provide paraphrase3http://goo.gl/0Z2RcsFigure 3: Amharic POS tagging.
lower pane: suggestionprovided to the user by the interactive classifier, upperpane: annotations by the user.
When (grey) the sugges-tion in the lower pane is correct, the user will click the an-notation and copy it to the upper pane.
Otherwise (shownin red or no suggestion), the user should provide a newannotation in the upper pane.92suggestions.
Paraphrases are first obtained fromeach individual resources and irrelevant or out-of-context paraphrases are discarded by ranking al-ternatives using an n-gram language model.
Para-phrases suggested by most of the underlining re-sources (at least 2 out of 3) are provided as sugges-tions.
Figure 1 shows an online interface displayingparaphrase suggestions based on our approach4.We have conducted experimental evaluation to as-sess the performance of the system using recall as ametric (recall =srwhere s is the number of to-kens in the source (paraphrased) sentence and r isthe number of tokens in the reference sentence).
Wehave used 100 sentences of paraphrase pairs (sourceand reference sentences) from the MSRPC dataset.The baseline result is computed using the originalparaphrase pairs of sentences which gives us a recallof 59%.
We took the source sentence and appliedour paraphrasing technique for words that are not inthe reference sentence and computed recall.
Table3 shows results for different settings, such as takingthe first, top 5, and top 10 suggestions from the can-didate paraphrases which outperforms the baselineresult.
The combination of different resources im-proves the performance of the paraphrasing system.setups Baseline top 1 top 5 top 10WordNet 59.0 60.3 61.4 61.9ppdb 59.0 60.2 62.2 64.6JoBimText 59.0 59.9 60.3 60.42in3 59.0 60.7 65.3 66.2Table 3: Recall values for paraphrasing using differentNLP resources and techniques.
Top 1 is where we con-sider only the best suggestion and compute the score.
top5 and 10 considers the Top 5 and 10 suggestions providedby the system respectively.
The row 2in3 shows the resultwhere we consider a paraphrase suggestion to be a candi-date when it appears at least in two of the three resources.6 Conclusion and Future WorkWe propose to integrate interactive machine learn-ing for an annotation task and semantic writing aidapplication to incrementally train a classifier basedon user feedback and interactions.
While the goalof the annotation task is to produce a quality an-4http://goo.gl/C0YkiAnotated data, the classifier is built into the seman-tic writing aid application to continuously improvethe system.
The proposal addresses the followingmain points: 1) How to develop a quality linguisticdataset using interactive machine learning approachfor a given annotation task.
2) How to systemati-cally combine different NLP resources to generateparaphrase suggestions for a semantic writing aidapplication.
Moreover, how to produce an applica-tion specific NLP resource iteratively using an inter-active machine learning approach.
3) How to inte-grate user interaction and feedback to improve theeffectiveness and quality of the system.We have carried out preliminary experiments forcreating sequence tagging data for German NERand Amharic POS.
Results indicate that integrat-ing interactive machine learning into the annotationtool can substantially reduce the annotation time re-quired for creating a high-quality dataset.Experiments have been conducted for the system-atic integrations of different NLP resources (Word-Net, PPDB, and JoBimText DT) as a paraphras-ing component into a semantic writing aid applica-tion.
Evaluation with the recall metric shows that thecombination of resources yields better performancethan any of the single resources.For further work within the scope of this thesis,we plan the following:?
Integrate an active learning approach for thelinguistic dataset development?
Investigate crowdsourcing techniques for inter-active machine learning applications.?
Integrate more NLP resources for the semanticwriting aid application.?
Investigate different paradigms of interactions,such as when and how the interactive classifiershould produces new model and study how sug-gestions are better provided to annotators.?
Investigate how user interaction and feedbackcan improve the linguistic dataset developmentand the semantic writing aid applications.?
Investigate how to improve the paraphrasingperformance by exploring machine learning forlearning resource combinations, as well as byleveraging user interaction and feedback.93ReferencesSaleema Amershi, James Fogarty, Ashish Kapoor, and DesneyTan.
Effective end-user interaction with machine learning.In Proceedings of the 25th AAAI Conference on Artificial In-telligence, San Francisco, CA, USA, 2011.Darina Benikova, Chris Biemann, and Marc Reznicek.
NoSta-D Named Entity Annotation for German: Guidelines andDataset.
In Proceedings of the Ninth International Confer-ence on Language Resources and Evaluation (LREC-2014),2014.Rahul Bhagat and Eduard Hovy.
What is a paraphrase?
InAssociation for Computational Linguistics.
MIT Press, 2013.Chris Biemann.
Structure Discovery in Natural Language.Springer Berlin Heidelberg, 2012.
ISBN 978-3-642-25922-7.Chris Biemann and Martin Riedl.
Text: now in 2D!
A frame-work for lexical expansion with contextual similarity.
J. Lan-guage Modelling, pages 55?95, 2013.Igor A. Bolshakov and Alexander Gelbukh.
Synonymous para-phrasing using wordnet and internet.
In Farid Meziane andElisabeth M?etais, editors, Natural Language Processing andInformation Systems, volume 3136 of Lecture Notes in Com-puter Science, pages 312?323.
Springer Berlin Heidelberg,2004.Chris Callison-Burch, Trevor Cohn, and Mirella Lapata.
Para-metric: An automatic evaluation metric for paraphrasing.
InProceedings of the 22nd International Conference on Com-putational Linguistics (Coling 2008), pages 97?104, Manch-ester, UK, 2008.
Coling 2008 Organizing Committee.Joana Costa, Catarina Silva, M?ario Antunes, and BernardeteRibeiro.
On using crowdsourcing and active learning to im-prove classification performance.
In Intelligent Systems De-sign and Applications (ISDA), pages 469?474, San Diego,USA, 2011.Koby Crammer and Yoram Singer.
Ultraconservative onlinealgorithms for multiclass problems.
J. Mach.
Learn.
Res.,pages 951?991, 2003.Christian Desrosiers and George Karypis.
A comprehensivesurvey of neighborhood-based recommendation methods.Recommender Systems Handbook, 2011.William Dolan, Chris Quirk, and Chris Brockett.
Unsupervisedconstruction of large paraphrase corpora: Exploiting mas-sively parallel news sources.
International Conference onComputational Linguistics, 2004.Juri Ganitkevitch and Chris Callison-Burch.
The multilingualparaphrase database.
In Proceedings of the Ninth Interna-tional Conference on Language Resources and Evaluation(LREC-2014), Reykjavik, Iceland, May 26-31, 2014., pages4276?4283, 2014.Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann,Michael Matuschek, Christian M. Meyer, and ChristianWirth.
Uby - a large-scale unified lexical-semantic resourcebased on lmf.
In Proceedings of the 13th Conference of theEuropean Chapter of the Association for Computational Lin-guistics (EACL 2012), pages 580?590, 2012.Ali Ibrahim, Boris Katz, and Jimmy Lin.
Extracting structuralparaphrases from aligned monolingual corpora.
In Proceed-ings of the Second International Workshop on Paraphrasing- Volume 16, PARAPHRASE ?03, pages 57?64, 2003.Siddhartha Jonnalagadda, Trevor Cohen, Stephen Wu, and Gra-ciela Gonzalez.
Enhancing clinical concept extraction withdistributional semantics.
In Journal of Biomedical Informat-ics, pages 129?140, San Diego, USA, 2012.Kshitij Judah, Thomas Dietterich, Alan Fern, Jed Irvine,Michael Slater, Prasad Tadepalli, Melinda Gervasio, Christo-pher Ellwood, William Jarrold, Oliver Brdiczka, and JimBlythe.
User initiated learning for adaptive interfaces.
InIJCAI Workshop on Intelligence and Interaction, Pasadena,CA, USA, 2009.Mayank Kabra, Alice A Robie, Marta Rivera-Alba, StevenBranson, and Kristin Branson.
Jaaba: interactive machinelearning for automatic annotation of animal behavior.
In Na-ture Methods, pages 64?67, 2013.Anastasia Krithara, Cyril Goutte, MR Amini, and Jean-MichelRenders.
Reducing the annotation burden in text classifi-cation.
In Proceedings of the 1st International Conferenceon Multidisciplinary Information Sciences and Technologies(InSciT 2006), Merida, Spain, 2006.Todd Kulesza, Saleema Amershi, Rich Caruana, Danyel Fisher,and Denis Charles.
Structured labeling to facilitate conceptevolution in machine learning.
In Proceedings of CHI 2014,Toronto, ON, Canada, 2014.
ACM Press.Alberto Lavelli, Bernardo Magnini, and Fabrizio Sebastiani.Building thematic lexical resources by bootstrapping andmachine learning.
In Proc.
of the workshop ?LinguisticKnowledge Acquisition and Representation: BootstrappingAnnotated Language Data?, wokshop at LREC-2002, 2002.Dekang Lin and Patrick Pantel.
Dirt - discovery of inferencerules from text.
In Proceedings of ACM Conference onKnowledge Discovery and Data Mining (KDD-01), pages323?328, San Francisco, CA, USA, 2001.Barzan Mozafariy, Purnamrita Sarkarz, Michael Franklinz,Michael Jordanz, and Samuel Madden.
Active learningfor crowdsourced databases.
In arXiv:1209.3686. arXiv.orgpreprint, 2012.Daniel Naber.
A rule-based style and grammar checker.diploma thesis, Computer Science - Applied, University ofBielefeld, 2004.Roberto Navigli.
Word sense disambiguation: A survey.
ACMComput.
Surv., pages 10:1?10:69, 2009.
ISSN 0360-0300.Ian Niles and Adam Pease.
Toward a Standard Upper Ontology.In Proceedings of the 2nd International Conference on For-mal Ontology in Information Systems (FOIS-2001), pages 2?9, 2001.James Pustejovsky and Amber Stubbs.
Natural Language An-notation for Machine Learning.
O?Reilly Media, 2012.ISBN 978-1-4493-0666-3.Hema Raghavan, Omid Madani, Rosie Jones, and Pack Kael-bling.
Active learning with feedback on both features andinstances.
Journal of Machine Learning Research, 7, 2006.94Burr Settles.
Active learning literature survey.
Technical report,University of Wisconsin?Madison, 2010.Burr Settles, Mark Craven, and Lewis Friedland.
Active learn-ing with real annotation costs.
In Proceedings of the NIPSWorkshop on Cost-Sensitive Learning, 2008.Ravi Sinha and Rada Mihalcea.
Combining lexical resourcesfor contextual synonym expansion.
In Proceedings ofthe International Conference RANLP-2009, pages 404?410,Borovets, Bulgaria, 2009.
Association for ComputationalLinguistics.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and AndrewNg.
Cheap and fast ?
but is it good?
evaluating non-expertannotations for natural language tasks.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natural Lan-guage Processing, pages 254?263, Honolulu, Hawaii, 2008.Simone Stumpf, Vidya Rajaram, Lida Li, Margaret Burnett,Thomas Dietterich, Erin Sullivan, Russell Drummond, andJonathan Herlocker.
Toward harnessing user feedback formachine learning.
In Proceedings of the 12th Interna-tional Conference on Intelligent User Interfaces, pages 82?91, 2007.Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.Yago: A core of semantic knowledge.
In Proceedings of the16th International Conference on World Wide Web, WWW?07, pages 697?706, 2007.
ISBN 978-1-59593-654-7.Vedrana Vidulin, Marko Bohanec, and Matja?z Gams.
Com-bining human analysis and machine data mining to obtaincredible data relations.
Information Sciences, 288:254?278,2014.Rui Wang and Chris Callison-Burch.
Paraphrase fragment ex-traction from monolingual comparable corpora.
In Proceed-ings of the 4th Workshop on Building and Using Compara-ble Corpora: Comparable Corpora and the Web, BUCC ?11,pages 52?60, 2011.Malcolm Ware, Eibe Frank, Geoffrey Holmes, Mark Hall,and Ian H Witten.
Interactive machine learning: lettingusers build classifiers.
In International Journal of Human-Computer Studies, pages 281?292, 2002.Julie Weeds and David Weir.
Co-occurrence retrieval: A flex-ible framework for lexical distributional similarity.
In As-sociation for Computational Linguistics, volume 31, pages439?475, 2005.Yi-Min Wen and Bao-Liang Lu.
Incremental learning of sup-port vector machines by classifier combining.
In Advancesin Knowledge Discovery and Data Mining, pages 904?911,Heidelberg, Germany, 2007.Seid Muhie Yimam, Richard Eckart de Castilho, IrynaGurevych, and Chris Biemann.
Automatic annotation sug-gestions and custom annotation layers in WebAnno.
In Pro-ceedings of the 52nd Annual Meeting of the Association forComputational Linguistics.
System Demonstrations, pages91?96, Stroudsburg, PA 18360, USA, 2014.
Association forComputational Linguistics.95
