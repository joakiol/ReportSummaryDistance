Proceedings of the Workshop on BioNLP: Shared Task, pages 19?27,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsEvent Extraction from Trimmed Dependency GraphsEkaterina Buyko, Erik Faessler, Joachim Wermter and Udo HahnJena University Language & Information Engineering (JULIE) LabFriedrich-Schiller-Universita?t JenaFu?rstengraben 30, 07743 Jena, Germany{ekaterina.buyko|erik.faessler|joachim.wermter|udo.hahn}@uni-jena.deAbstractWe describe the approach to event extrac-tion which the JULIELab Team from FSUJena (Germany) pursued to solve Task 1 inthe ?BioNLP?09 Shared Task on Event Ex-traction?.
We incorporate manually curateddictionaries and machine learning method-ologies to sort out associated event triggersand arguments on trimmed dependency graphstructures.
Trimming combines pruning ir-relevant lexical material from a dependencygraph and decorating particularly relevant lex-ical material from that graph with more ab-stract conceptual class information.
Giventhat methodological framework, the JULIELabTeam scored on 2nd rank among 24 competingteams, with 45.8% precision, 47.5% recall and46.7% F1-score on all 3,182 events.1 IntroductionSemantic forms of text analytics for the life scienceshave long been equivalent with named entity recog-nition and interpretation, i.e., finding instances of se-mantic classes such as proteins, diseases, or drugs.For a couple of years, this focus has been comple-mented by analytics dealing with relation extraction,i.e., finding instances of relations which link one ormore (usually two) arguments, the latter being in-stances of semantic classes, such as the interactionbetween two proteins (PPIs).PPI extraction is a complex task since cascadesof molecular events are involved which are hard tosort out.
Many different approaches have alreadybeen tried ?
pattern-based ones (e.g., by Blaschkeet al (1999), Hakenberg et al (2005) or Huang etal.
(2004)), rule-based ones (e.g., by Yakushiji et al(2001), ?Saric?
et al (2004) or Fundel et al (2007)),and machine learning-based ones (e.g., by Katrenkoand Adriaans (2006), S?tre et al (2007) or Airola etal.
(2008)), yet without conclusive results.In the following, we present our approach to solveTask 1 within the ?BioNLP?09 Shared Task on EventExtraction?.1 Task 1 ?Event detection and charac-terization?
required to determine the intended rela-tion given a priori supplied protein annotations.
Ourapproach considers dependency graphs as the cen-tral data structure on which various trimming oper-ations are performed involving syntactic simplifica-tion but also, even more important, semantic enrich-ment by conceptual overlays.
A description of thecomponent subtasks is provided in Section 2, whilethe methodologies intended to solve each subtaskare discussed in Section 3.
The system pipeline forevent extraction reflecting the task decomposition isdescribed in Section 4, while Section 5 provides theevaluation results for our approach.2 Event Extraction TaskEvent extraction is a complex task that can be sub-divided into a number of subtasks depending onwhether the focus is on the event itself or on the ar-guments involved:Event trigger identification deals with the largevariety of alternative verbalizations of the sameevent type, i.e., whether the event is expressed in1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/19a verbal or in a nominalized form (e.g., ?A is ex-pressed?
and ?the expression of A?
both refer to thesame event type, viz.
expression(A)).
Since thesame trigger may stand for more than one event type,event trigger ambiguity has to be resolved as well.Event trigger disambiguation selects the correctevent name from the set of alternative event triggers.Event typing, finally, deals with the semanticclassification of a disambiguated event name and theassignment to an event type category.2Argument identification is concerned with find-ing all necessary participants in an event, i.e., thearguments of the relation.Argument typing assigns the correct semanticcategory (entity class) to each of the determined par-ticipants in an event (which can be considered as in-stances of that class).Argument ordering assigns each identified par-ticipant its functional role within the event, mostlyAgent (and Patient/Theme).The sentence ?Regulation of jun and fos gene ex-pression in human monocytes by the macrophagecolony-stimulating factor?, e.g., contains mentionsof two Gene Expression events with respectiveTHEME arguments ?jun?
and ?fos?, triggered in thetext by the literal phrase ?gene expression?.Task 1 of the ?BioNLP?09 Shared Task on EventExtraction?
was defined in such a way as to iden-tify a proper relation (event) name and link it withits type, plus one or more associated arguments de-noting proteins.
To focus on relation extraction onlyno automatic named entity recognition and interpre-tation had to be performed (subtask ?argument typ-ing?
from above); instead candidate proteins werealready pre-tagged.
The complexity of Task 1 wasraised by the condition that not only proteins wereallowed to be arguments but also were events.3 Event Extraction SolutionOur event extraction approach is summarized in Fig-ure 1 and consists of three major streams ?
first, thedetection of lexicalized event triggers (cf.
Section3.1), second, the trimming of dependency graphswhich involves pruning irrelevant and semanticallyenriching relevant lexical material (cf.
Section 3.2),2In our approach, event trigger disambiguation already im-plies event typing.Pre-processingArgument Identi f icat ion withEnsemble of ClassifiersEvent DetectionPost-processingTrimming ofDependency GraphsTyping of PutativeEvent TriggersFigure 1: General Architecture of the Event ExtractionSolution of the JULIELab Team.and, third, the identification of arguments for theevent under scrutiny (cf.
Section 3.3).
Event typ-ing results from proper event trigger identification(see Section 3.1.2), which is interlinked with the out-come of the argument identification.
We talk aboutputative triggers because we consider, in a greedymanner, all relevant lexical items (see Section 3.1.1)as potential event triggers which might represent anevent.
Only those event triggers that can eventuallybe connected to arguments, finally, represent a trueevent.
To achieve this goal we preprocessed both theoriginal training and test data such that we enrich theoriginal training data with automatically predictedevent triggers in order to generate more negative ex-amples for a more effective learning of true events.33.1 Event Trigger IdentificationLooking at the wide variety of potential lexicalizedtriggers for an event, their lacking discriminativepower relative to individual event types and theirinherent potential for ambiguity,4 we decided ona dictionary-based approach whose curation princi-ples are described in Section 3.1.1.
Our disambigua-tion policy for the ambiguous lexicalized event trig-3Although the training data contains cross-sentence eventdescriptions, our approach to event extraction is restricted tothe sentence level only.4Most of the triggers are neither specific for molecular eventdescriptions, in general, nor for a special event type.
?Induc-tion?, e.g., occurs 417 times in the training data.
In 162 of thesecases it acts as a trigger for Positive regulation, 6 times as atrigger for Transcription, 8 instances trigger Gene expression,while 241 occurrences do not trigger an event at all.20gers assembled in this suite of dictionaries, one perevent type, is discussed in Section 3.1.2.3.1.1 Manual Curation of the DictionariesWe started collecting our dictionaries from theoriginal GENIA event corpus (Kim et al, 2008a).The extracted event triggers were then automaticallylemmatized5 and the resulting lemmata were subse-quently ranked by two students of biology accordingto their predictive power to act as a trigger for a par-ticular event type.
This expert assessment led us tofour trigger groups (for each event type these groupswere determined separately):(1) Triggers are important and discriminative fora specific event type.
This group contains event trig-gers such as ?upregulate?
for Positive regulation.
(2) Triggers are important though not fully dis-criminative for a particular event type; yet, this defi-ciency can be overcome by other lexical cues withinthe context of the same sentence.
This group with in-context disambiguators contains lexical items suchas ?proteolyse?
for Protein catabolism.
(3) Triggers are non-discriminative for an eventtype and even cannot be disambiguated by linguisticcues within the context of the same sentence.
Thisgroup contains lexical items such as ?presence?
forLocalization and Gene expression.
(4) Triggers are absolutely non-discriminative foran event.
This group holds general lexical triggerssuch as ?observe?, ?demonstrate?
or ?function?.The final dictionaries used for the detection ofputative event triggers are a union of the first twogroups.
They were further extended by biologistswith additional lexical material of the first group.The dictionaries thus became event type-specific ?they contain all morphological forms of the originallemma, which were automatically generated usingthe Specialist NLP Tools (2008 release).We matched the entries from the final set of dic-tionaries with the shared task data using the Ling-pipe Dictionary Chunker.6 After the matching pro-cess, some cleansing had to be done.75We used the lemmatizer from the Specialist NLP Tools(http://lexsrv3.nlm.nih.gov/SPECIALIST/index.html, 2008 release).6http://alias-i.com/lingpipe/7Event triggers were removed which (1) were found withinsentences without any protein annotations, (2) occurred within3.1.2 Event Trigger DisambiguationPreliminary experiments indicated that the dis-ambiguation of event triggers might be beneficialfor the overall event extraction results since eventstend to be expressed via highly ambiguous triggers.Therefore, we performed a disambiguation step pre-ceding the extraction of any argument structures.It is based on the importance of an event trig-ger ti for a particular event type T as defined byImp(tTi ) := f(tTi )Pi f(tTi ), where f(tTi ) is the frequencyof the event trigger ti of the selected event type Tin a training corpus divided by the total amount ofall event triggers of the selected event type T inthat training corpus.
The frequencies are measuredon stemmed event triggers.
For example, Imp forthe trigger stem ?depend?
amounts to 0.013 for theevent type Positive regulation, while for the eventtype Regulation it yields 0.036 .
If a text span con-tains several event triggers with the same span off-set, the event trigger with max(Imp) is selected andother putative triggers are discarded.
The triggerstem ?depend?
remains thus only for Regulation.3.2 Trimming Dependency GraphsWhen we consider event (relation) extraction as a se-mantic interpretation task, plain dependency graphsas they result from deep syntactic parsing might notbe appropriate to directly extract semantic informa-tion from.
This is due to two reasons - they containa lot of apparently irrelevant lexical nodes (from thesemantic perspective of event extraction) and theyalso contain much too specific lexical nodes thatmight better be grouped and further enriched se-mantically.
Trimming dependency graphs for thepurposes of event extraction, therefore, amounts toeliminate semantically irrelevant and to semanticallyenrich relevant lexical nodes (i.e., overlay with con-cepts).
This way, we influence the final representa-tion for the machine learners we employ (in terms offeatures or kernel-based representations) ?
we mayavoid an overfitting of the feature or kernel spaceswith syntactic and lexical data and thus reduce struc-tural information in a linguistically motivated way.a longer event trigger, (3) overlapped with a longer trigger ofthe same event type, (4) occurred inside an entity mention an-notation.213.2.1 Syntactic PruningPruning targets auxiliary and modal verbs whichgovern the main verb in syntactic structures such aspassives, past or future tense.
We delete the aux-iliars/modals as govenors of the main verbs fromthe dependency graph and propagate the semantics-preserving dependency relations of these nodes di-rectly to the main verbs.
Adhering to the depen-dency tree format and labeling conventions set upfor the 2006 and 2007 CONLL shared tasks on de-pendency parsing main verbs are usually connectedwith the auxiliar by the VC dependency relation (seeFigure 2).
Accordingly, in our example, the verb?activate?
is promoted to the ROOT in the depen-dency graph and governs all nodes that were origi-nally governed by the modal ?may?.Figure 2: Trimming of Dependency Graphs.3.2.2 Conceptual DecorationLexical nodes in the (possibly pruned) depen-dency graphs deemed to be important for argumentextraction were then enriched with semantic classannotations, instead of keeping the original lexical(stem) representation (see Figure 2).
The rationalebehind this decision was to generate more powerfulkernel-based or features representations (see Section3.3.2 and 3.3.1).The whole process is based on a three-tier task-specific semantic hierarchy of named entity classes.The top rank is constituted by the equivalent classesTranscription factor, Binding site, and Promoter.The second rank is occupied by MESH terms, andthe third tier assembles the named entity classesGene and Protein.
Whenever a lexical item is cat-egorized by one of these categories, the associatednode in the dependency graph is overlaid with thatcategory applying the ranking in cases of conflicts.We also enriched the gene name mentions withtheir respective Gene Ontology Annotations fromGOA.8 For this purpose, we first categorized GOterms both from the ?molecular function?
and fromthe ?biological process?
branch with respect totheir matching event type, e.g., Phosphorylationor Positive regulation.
We then mapped all genename mentions which occurred in the text to theirUNIPROT identifier using the gene name normalizerGENO (Wermter et al, 2009).
This identifier links agene with a set of (curated) GO annotations.In addition, we inserted semantic information interms of the event trigger type and the experimen-tal methods.
As far as experimental methods areconcerned, we extracted all instances of them an-notated in the GENIA event corpus.
One studentof biology sorted the experimental methods relativeto the event categories under scrutiny.
For example?affinity chromatography?
was assigned both to theGene expression and to the Binding category.
Forour purposes, we only included those GO annota-tions and experimental methods which matched theevent types to be identified in a sentence.3.3 Argument Identification and OrderingThe argument identification task can be subdividedinto three complexity levels.
Level (1) incorpo-rates five event types (Gene expression, Transcrip-tion, Protein catabolism, Localization, Phosphory-lation) which involve a single participant with aTHEME role only.
Level (2) is concerned with oneevent type (Binding) that provides an n-ary argumentstructure where all arguments occupy the THEME(n)role.
Level (3) comprises three event types (Posi-tive regulation, Negative regulation, or an unspeci-fied Regulation) that represent a regulatory relationbetween the above-mentioned event classes or pro-teins.
These events have usually a binary structure,with a THEME argument and a CAUSE argument.For argument extraction, we built sentence-wisepairs of putative triggers and their putative argu-ment(s), the latter involving ontological informa-tion about the event type.
For Level (1), we builtpairs only with proteins, while for Level (3) we al-8http://www.ebi.ac.uk/GOA22lowed all events as possible arguments.
For Level(2), Binding events, we generated binary (trigger,protein) pairs as well as triples (trigger, protein1,protein2) to adequately represent the binding be-tween two proteins.9 Pairs of mentions not con-nected by a dependency path could not be detected.For the argument extraction we chose two ma-chine learning-based approaches, feature-based anda kernel-based one, as described below.103.3.1 Feature-based ClassifierWe distinguished three groups of features.
First,lexical features (covering lexical items before, af-ter and between both mentions (of the event triggerand an argument) as described by Zhou and Zhang(2007)); second, chunking features (concerned withhead words of the phrases between two mentions asdescribed by Zhou and Zhang (2007)); third, de-pendency parse features (considering both the se-lected dependency levels of the arguments (parentsand least common subsumer) as discussed by Ka-trenko and Adriaans (2006), as well as a shortest de-pendency path structure between the arguments asused by Kim et al (2008b) for walk features).For the feature-based approach, we chose theMaximum Entropy (ME) classifier from MALLET.113.3.2 Graph Kernel ClassifierThe graph kernel uses a converted form of depen-dency graphs in which each dependency node is rep-resented by a set of labels associated with that node.The dependency edges are also represented as nodesin the new graph such that they are connected to thenodes adjacent in the dependency graph.
Subgraphswhich represent, e.g., the linear order of the wordsin the sentence can be added, if required.
The entiregraph is represented in terms of an adjacency matrixwhich is further processed to contain the summedweights of paths connecting two nodes of the graph(see Airola et al (2008) for details).9We did not account for the binding of more than two pro-teins as this would have led to a combinatory explosion of pos-sible classifications.10In our experiments, we used full conceptual overlaying(see Section 3.2) for the kernel-based representation and partialoverlaying for the dependency parse features (only gene/proteinannotation was exploited here).
Graph representations allow formany semantic labels to be associated with a node.11http://mallet.cs.umass.edu/index.php/Main_PageFigure 3: Graph Kernel Representation for a TrimmedDependency Graph ?
(1) original representation, (2)representation without graph dependency edge nodes(weights (0.9, 0.3) taken from Airola et al (2008)).For our experiments, we tried some variants of theoriginal graph kernel.
In the original version eachdependency graph edge is represented as a node.That means that connections between graph tokennodes are expressed through graph dependency edgenodes (see Figure 3; (1)).
To represent the connec-tions between original tokens as direct connectionsin the graph, we removed the edge nodes and eachtoken was assigned the edge label (its dependencylabel; see Figure 3; (2)).
Further variants includedencodings for (1) the shortest dependency path (sp)between two mentions (argument and trigger)12 (2)the complete dependency graph (sp-dep), and (3) thecomplete dependency graph and linear information(sp-dep-lin) (the original configuration from Airolaet al (2008)).For the graph kernel, we chose the LibSVM(Chang and Lin, 2001) Support Vector Machine asclassifier.3.4 PostprocessingThe postprocessing step varies for the three differentLevels (see Section 3.3).
For every event trigger ofLevel (1) (e.g., Gene expression), we generate oneevent per relation comprising a trigger and its argu-ment.
For Level (2) (Binding), we create a Bindingevent with two arguments only for triples (trigger,protein1, protein2).
For the third Level, we createfor each event trigger and its associated argumentse = n ?
m events, for n CAUSE arguments and mTHEME arguments.12For Binding we extracted the shortest path between twoprotein mentions if we encounter a triple (trigger, protein1,protein2).234 PipelineThe event extraction pipeline consists of two ma-jor parts, a pre-processor and the dedicated eventextractor.
As far as pre-processing is concerned,we imported the sentence splitting, tokenization andGDep parsing results (Sagae and Tsujii, 2007) asprepared by the shared task organizers for all datasets (training, development and test).
We processedthis data with the OpenNLP POS tagger and Chun-ker, both re-trained on the GENIA corpus (Buyko etal., 2006).
Additionally, we enhanced the originaltokenization by one which includes hyphenizationof lexical items such as in ?PMA-dependent?.
13The data was further processed with the gene nor-malizer GENO(Wermter et al, 2009) and a num-ber of regex- and dictionary-based entity taggers(covering promoters, binding sites, and transcrip-tion factors).
We also enriched gene name men-tions with their respective Gene Ontology annota-tions (see Section 3.2.2).
The MESH thesaurus (ex-cept chemical and drugs branch) was mapped on thedata using the Lingpipe Dictionary Chunker.14After preprocessing, event extraction was starteddistinguishing between the event trigger recognition(cf.
Section 3.1), the trimming of the dependencygraphs (cf.
Section 3.2), and the argument extrac-tion proper (cf.
Section 3.3).15 We determined inour experiments on the development data the perfor-mance of every classifier type and its variants (forthe graph kernel), and of ensembles of the most per-formant (F-Score) graph kernel variant and an MEmodel.16 We present here the argument extractionconfiguration used for the official run.17 For theprediction of Phosphorylation, Localization, Pro-tein catabolism types we used the graph kernel in13This tokenization is more advantageous for the detectionof additional event triggers as it allows to generate depen-dency relations from hyphenated terms.
For example, in ?PMA-dependent?, ?PMA?
will be a child of ?dependent?
linked bythe AMOD dependency relation, and ?dependent?
receives theoriginal dependency relation of the ?PMA-dependent?
token.14http://alias-i.com/lingpipe/15For the final configurations of the graph kernel, we opti-mized the C parameter in the spectrum between 2?3 and 23 onthe final training data for every event type separately.16In the ensemble configuration we built the union of positiveinstances.17We achieved with this configuration the best performanceon the development set.its ?sp without dependency-edge-nodes?
configura-tion, while for the prediction of Transcription andGene expression events we used an ensemble of thegraph kernel in its ?sp with dependency-edge-nodes?variant, and an ME model.
For the prediction ofBinding we used an ensemble of the graph kernel(?sp-dep with dependency-edge-nodes?)
and an MEmodel.
For the prediction of regulatory events weused ME models for each regulatory type.5 ResultsThe baseline against which we compared our ap-proach can be captured in a single rule.
We extractfor every pair of a putative trigger and a putative ar-gument the shortest dependency path between them.If the shortest dependency path does not contain anydirection change, i.e., the argument is either a directchild or a direct parent of the trigger, and if the pathdoes not contain any other intervening event trig-gers, the argument is taken as the THEME role.We performed evaluations on the shared task de-velopment and test set.
Our baseline achieved com-petitive results of 36.0% precision, 34.0% recall,35.0% F-score on the development set (see Table1), and 30.4% precision, 35.7% recall, 32,8% F-score on the test set (see Table 2).
In particularthe one-argument events, i.e., Gene expression, Pro-tein catabolism, Phosphorylation are effectively ex-tracted with an F-score around 70.0%.
More com-plex events, in particular events of Level (3), i.e.,(Regulation) were less properly dealt with becauseof their strong internal complexity.Event Class gold recall prec.
F-scoreLocalization 53 75.47 30.30 43.24Binding 248 33.47 20.80 25.66Gene expression 356 76.12 75.07 75.59Transcription 82 68.29 40.58 50.91Protein catabolism 21 76.19 66.67 71.11Phosphorylation 47 76.60 72.00 74.23Regulation 169 14.20 15.09 14.63Positive regulation 617 15.40 20.83 17.71Negative regulation 196 11.73 13.22 12.43TOTAL 1789 36.00 34.02 34.98Table 1: Baseline results on the shared task developmentdata.
Approximate Span Matching/Approximate Recur-sive Matching.24Event Class gold recall prec.
F-score gold recall prec.
F-scoreLocalization 174 42.53 44.85 43.66 174 42.53 44.85 43.66Binding 347 32.28 37.09 34.51 398 44.22 58.28 50.29Gene expression 722 61.36 80.55 69.65 722 61.36 80.55 69.65Transcription 137 39.42 35.06 37.11 137 39.42 35.06 37.11Protein catabolism 14 71.43 66.67 68.97 14 71.43 66.67 68.97Phosphorylation 135 65.93 90.82 76.39 135 65.93 90.82 76.39EVT-TOTAL 1529 51.14 60.90 55.60 1580 53.54 65.89 59.08Regulation 291 9.62 11.72 10.57 338 9.17 12.97 10.75Positive regulation 983 10.38 11.33 10.83 1186 14.67 19.33 16.68Negative regulation 379 14.25 19.22 16.36 416 14.18 21.00 16.93REG-TOTAL 1653 11.13 12.96 11.98 1940 13.61 18.59 15.71ALL-TOTAL 3182 30.36 35.72 32.82 3520 31.53 41.05 35.67Table 2: Baseline results on the shared task test data.
Approximate Span Matching/Approximate Recursive Matching(columns 3-5).
Event decomposition, Approximate Span Matching/Approximate Recursive Matching (columns 7-9).The event extraction approach, in its final config-uration (see Section 4), achieved a performance of50.4% recall, 45.8% precision and 48.0% F-score onthe development set (see Table 4), and 45.8% recall,47.5% precision and 46.7% F-score on the test set(see Table 3).
This approach clearly outperformedthe baseline with an increase of 14 percentage pointson the test data.
In particular, the events of Level (2)and (3) were more properly dealt with than by thebaseline.
In the event decomposition mode (argu-ment detection is evaluated in a decomposed event)we achieved a performance of 49.4% recall, 56.2%precision, and 52.6% F-score (see Table 3).Our experiments on the development set showedthat the combination of the feature-based and thegraph kernel-based approach can boost the results upto 6 percentage points F-score (for the Binding eventtype).
It is interesting that the combination for Bind-ing increased recall without dropping precision.
Theoriginal graph kernel approach for Binding eventsperforms with 38.3% recall, 27.9% precision and32.3% F-score on the development set.
The com-bined approach comes with a remarkable increaseof 14 percentage points in recall.
The combinationcould also boost the recall of the Gene expressionand Transcription by 15 percentage points and 5 per-centage points, respectively, without seriously drop-ping the precision (4 points for every type).
Forthe other event types, no improvements were foundwhen we combined both approaches.5.1 Error DiscussionOne expert biologist analyzed 30 abstracts randomlyextracted from the development error data.
We de-termined seven groups of errrors based on this anal-ysis.
The first group contains examples for whichan event should be determined, but a false argumentwas found (e.g., Binding arguments were not prop-erly sorted, or correct and false arguments were de-tected for the same trigger) (44 examples).
The sec-ond group comprised examples where no trigger wasfound (23 examples).
Group (3) stands for caseswhere no events were detected although a triggerwas properly identified (14 examples).
Group (4)holds examples detected in sentences which did notcontain any events (12 examples).
Group (5) lists bi-ologically meaningful analyses, actually very closeto the gold annotation, especially for the cascadedregulatory events (12 examples), while Group (6) in-corporates examples of a detected event with incor-rect type (1 example).
Group (7) gathers misleadinggold annotations (10 examples).This assessment clearly indicates that a majorsource of errors can be traced to the level of argu-ment identification, in particular for Binding events.The second major source has its offspring at thelevel of trigger detection (we ignored, for exam-ple, triggers such as ?in the presence of ?, ?when?,?normal?).
About 10% of the errors are due to aslight difference between extracted events and goldevents.
For example, in the phrase ?role for NF-kappaB in the regulation of FasL expression?
we25Event Class gold recall prec.
F-score gold recall prec.
F-scoreLocalization 174 43.68 77.55 55.88 174 43.68 77.55 55.88Binding 347 49.57 35.25 41.20 398 63.57 54.88 58.91Gene expression 722 64.82 80.27 71.72 722 64.82 80.27 71.72Transcription 137 35.77 62.03 45.37 137 35.77 62.03 45.37Protein catabolism 14 78.57 84.62 81.48 14 78.57 84.62 81.48Phosphorylation 135 76.30 91.15 83.06 135 76.30 91.15 83.06EVT-TOTAL 1529 57.49 63.97 60.56 1580 60.76 71.27 65.60Regulation 291 31.27 30.13 30.69 338 35.21 37.54 36.34Positive regulation 983 34.08 37.18 35.56 1186 40.64 49.33 44.57Negative regulation 379 40.37 31.16 35.17 416 42.31 39.11 40.65REG-TOTAL 1653 35.03 34.18 34.60 1940 40.05 44.55 42.18ALL-TOTAL 3182 45.82 47.52 46.66 3520 49.35 56.20 52.55Table 3: Offical Event Extraction results on the shared task test data of the JULIELab Team.
ApproximateSpan Matching/Approximate Recursive Matching (columns 3-5).
Event decomposition, Approximate Span Match-ing/Approximate Recursive Matching (columns 7-9).could not extract the gold event Regulation of Regu-lation (Gene expression (FasL)) associated with thetrigger ?role?, but we were able to find the (inside)event Regulation (Gene expression (FasL)) associ-ated with the trigger ?regulation?.
Interestingly, thetyping of events is not an error source in spite ofthe simple disambiguation approach.
Still, our dis-ambiguation strategy is not appropriate for the anal-ysis of double-annotated triggers such as ?overex-pression?, ?transfection?, etc., which are annotatedas Gene expression and Positive regulation and area major source of errors in Group (2).
As Group(6) is an insignificant source of errors in our ran-domly selected data, we focused our error analysison the especially ambiguous event type Transcrip-tion.
We found from 34 errors that 14 of them weredue to the disambiguation strategy (in particular fortriggers ?
(gene) expression?
and ?induction?
).6 ConclusionOur approach to event extraction incorporates man-ually curated dictionaries and machine learningmethodologies to sort out associated event triggersand arguments on trimmed dependency graph struc-tures.
Trimming combines pruning irrelevant lexi-cal material from a dependency graph and decorat-ing particularly relevant lexical material from thatgraph with more abstract conceptual class informa-tion.
Given that methodological framework, theJULIELab Team scored on 2nd rank among 24 com-Event Class gold recall prec.
F-scoreLocalization 53 71.70 74.51 73.08Binding 248 52.42 29.08 37.41Gene expression 356 75.28 81.46 78.25Transcription 82 60.98 73.53 66.67Protein catabolism 21 90.48 79.17 84.44Phosphorylation 47 82.98 84.78 83.87Regulation 169 37.87 36.78 37.32Positive regulation 617 34.36 35.99 35.16Negative regulation 196 41.33 33.61 37.07TOTAL 1789 50.36 45.76 47.95Table 4: Event extraction results on the shared taskdevelopment data of the official run of the JULIELabTeam.
Approximate Span Matching/Approximate Recur-sive Matching.peting teams, with 45.8% precision, 47.5% recalland 46.7% F1-score on all 3,182 events.7 AcknowledgmentsWe wish to thank Rico Landefeld for his technicalsupport, Tobias Wagner and Rico Pusch for theirconstant help and great expertise in biological is-sues.
This research was partially funded within theBOOTSTREP project under grant FP6-028099 andthe CALBC project under grant FP7-231727.ReferencesAntti Airola, Sampo Pyysalo, Jari Bjo?rne, TapioPahikkala, Filip Ginter, and Tapio Salakoski.
2008.
A26graph kernel for protein-protein interaction extraction.In Proceedings of the Workshop on Current Trends inBiomedical Natural Language Processing, pages 1?9.Christian Blaschke, Miguel A. Andrade, Christos Ouzou-nis, and Alfonso Valencia.
1999.
Automatic ex-traction of biological information from scientific text:Protein-protein interactions.
In ISMB?99 ?
Proceed-ings of the 7th International Conference on IntelligentSystems for Molecular Biology, pages 60?67.Ekaterina Buyko, Joachim Wermter, Michael Poprat, andUdo Hahn.
2006.
Automatically adapting an NLPcore engine to the biology domain.
In Proceedingsof the Joint BioLINK-Bio-Ontologies Meeting.
A JointMeeting of the ISMB Special Interest Group on Bio-Ontologies and the BioLINK Special Interest Group onText Data M ining in Association with ISMB, pages65?68.
Fortaleza, Brazil, August 5, 2006.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.2007.
Relex-relation extraction using dependencyparse trees.
Bioinformatics, 23(3):365?371.Jo?rg Hakenberg, Ulf Leser, Conrad Plake, Harald Kirsch,and Dietrich Rebholz-Schuhmann.
2005.
LLL?05challenge: Genic interaction extraction - identifica-tion of language patterns based on alignment and finitestate automata.
In Proceedings of the 4th LearningLanguage in Logic Workshop (LLL05), pages 38?45.Minlie Huang, Xiaoyan Zhu, Donald G. Payan, Kun-bin Qu, and Ming Li.
2004.
Discovering patternsto extract protein-protein interactions from full texts.Bioinformatics, 20(18):3604?3612.Sophia Katrenko and Pieter W. Adriaans.
2006.
Learn-ing relations from biomedical corpora using depen-dency trees.
In Karl Tuyls, Ronald L. Westra, YvanSaeys, and Ann Nowe?, editors, KDECB 2006 ?
Knowl-edge Discovery and Emergent Complexity in Bioin-formatics.
Revised Selected Papers of the 1st Inter-national Workshop., volume 4366 of Lecture Notesin Computer Science, pages 61?80.
Ghent, Belgium,May 10, 2006.
Berlin: Springer.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008a.Corpus annotation for mining biomedical events fromliterature.
BMC Bioinformatics, 9(10).Seon-Ho Kim, Juntae Yoon, and Jihoon Yang.
2008b.Kernel approaches for genic interaction extraction.Bioinformatics, 24(1):118?126.Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii.
2007.
Syn-tactic features for protein-protein interaction extrac-tion.
In Christopher J. O. Baker and Jian Su, editors,LBM 2007, volume 319, pages 6.1?6.14.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with LR models and par serensembles.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 1044?1050.Jasmin ?Saric?, Lars J. Jensen, Rossitza Ouzounova, IsabelRojas, and Peer Bork.
2004.
Extracting regulatorygene expression networks from pubmed.
In ACL ?04:Proceedings of the 42nd Annual Meeting on Associa-tion for Computational Linguistics, page 191, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Joachim Wermter, Katrin Tomanek, and Udo Hahn.2009.
High-performance gene name normalizationwith GeNo.
Bioinformatics, 25(6):815?821.Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, andJun?ichi Tsujii.
2001.
Event extraction from biomed-ical papers using a full parser.
In Russ B. Altman,A.
Keith Dunker, Lawrence Hunter, Kevin Lauderdale,and Teri E. Klein, editors, PSB 2001 ?
Proceedingsof the 6th Pacific Symposium on Biocomputing, pages408?419.
Maui, Hawaii, USA.
January 3-7, 2001.
Sin-gapore: World Scientific Publishing.Guodong Zhou and Min Zhang.
2007.
Extracting re-lation information from text documents by exploringvarious types of knowledge.
Information Processing& Management, 43(4):969?982.27
