Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 985?992Manchester, August 2008Extractive Summarization Using Supervised and Semi-supervisedLearningKam-Fai Wong*, Mingli Wu*?
*Department of Systems Engineering andEngineering ManagementThe Chinese University of Hong KongNew Territories, Hong Kong{kfwong,mlwu}@se.cuhk.edu.hkWenjie Li?
?Department of ComputingThe Hong Kong Polytechnic UniversityKowloon, Hong Kongcswjli@comp.polyu.edu.hkAbstractIt is difficult to identify sentence impor-tance from a single point of view.
In thispaper, we propose a learning-based ap-proach to combine various sentence fea-tures.
They are categorized as surface,content, relevance and event features.Surface features are related to extrinsicaspects of a sentence.
Content featuresmeasure a sentence based on content-conveying words.
Event features repre-sent sentences by events they contained.Relevance features evaluate a sentencefrom its relatedness with other sentences.Experiments show that the combined fea-tures improved summarization perform-ance significantly.
Although the evalua-tion results are encouraging, supervisedlearning approach requires much labeleddata.
Therefore we investigate co-trainingby combining labeled and unlabeled data.Experiments show that this semi-supervised learning approach achievescomparable performance to its supervisedcounterpart and saves about half of thelabeling time cost.1 Introduction1 Automatic text summarization involves con-densing a document or a document set to producea human comprehensible summary.
Two kinds ofsummarization approaches were suggested in thepast, i.e., extractive (Radev et al, 2004; Li et al,2006) and abstractive summarization (Dejong,1978).
The abstractive approaches typically need?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.to ?understand?
and then paraphrase the salientconcepts across documents.
Due to the limita-tions in natural language processing technology,abstractive approaches are restricted to specificdomains.
In contrast, extractive approachescommonly select sentences that contain the mostsignificant concepts in the documents.
These ap-proaches tend to be more practical.Recently various effective sentence featureshave been proposed for extractive summarization,such as signature word, event and sentence rele-vance.
Although encouraging results have beenreported, most of these features are investigatedindividually.
We argue that it is ineffective toidentify sentence importance from a single pointof view.
Each sentence feature has its uniquecontribution, and combing them would be advan-tageous.
Therefore we investigate combined sen-tence features for extractive summarization.
Todetermine weights of different features, we em-ploy a supervised learning framework to identifyhow likely a sentence is important.
Some re-searchers explored learning based summarization,but the new emerging features are not concerned,such as event features (Li et.
al, 2006).We investigate the effectiveness of differentsentence features with supervised learning to de-cide which sentences are important for summari-zation.
After feature vectors of sentences are ex-amined, a supervised learning classifier is thenemployed.
Particularly, considering the length offinal summaries is fixed, candidate sentences arere-ranked.
Finally, the top sentences are ex-tracted to compile the final summaries.
Experi-ments show that combined features improvesummarization performance significantly.Our supervised learning approach generatespromising results based on combined features.However, it requires much labeled data.
As thisprocedure is time consuming and costly, we in-vestigate semi-supervised learning to combinelabeled data and unlabeled data.
A semi-985supervised learning classifier is used instead of asupervised one in our extractive summarizationframework.
Two classifiers are co-trained itera-tively to exploit unlabeled data.
In each iterationstep, the unlabeled training examples with topclassifying confidence are included in the labeledtraining set, and the two classifiers are trained onthe new training data.
Experiments show that theperformance of our semi-supervised learningapproach is comparable to its supervised learningcounterpart and it can reduce the labeling timecost by 50%.The remainder of this paper is organized asfollows.
Section 2 gives related work and Section3 describes our learning-based extractive summa-rization framework.
Section 4 outlines the vari-ous sentence features and Section 5 describessupervised/semi-supervised learning approaches.Section 6 presents experiments and results.
Fi-nally, Section 7 concludes the paper.2 Related WorkTraditionally, features for summarization werestudied separately.
Radev et al (2004) reportedthat position and length are useful surface fea-tures.
They observed that sentences located at thedocument head most likely contained importantinformation.
Recently, content features were alsowell studied, including centroid (Radev et al,2004), signature terms (Lin and Hovy, 2000) andhigh frequency words (Nenkova e t al., 2006).Radev et al (2004) defined centroid words asthose whose average tf*idf score were higherthan a threshold.
Lin and Hovy (2000) identifiedsignature terms that were strongly associatedwith documents based on statistics measures.Nenkova et al (2006) later reported that highfrequency words were crucial in reflecting thefocus of the document.Bag of words is somewhat loose and omitsstructural information.
Document structure isanother possible feature for summarization.
Bar-zilay and Elhadad (1997) constructed lexicalchains and extracted strong chains in summaries.Marcu (1997) parsed documents as rhetoricaltrees and identified important sentences based onthe trees.
However, only moderate results werereported.
On the other hand, Dejong (1978) rep-resented documents using predefined templates.The procedure to create and fill the templateswas time consuming and it was hard to adapt themethod to different domains.Recently, semi-structure events (Filatovia andHatzivassiloglou, 2004; Li et al, 2006; Wu, 2006)have been investigated by many researchers asthey balanced document representation withwords and structures.
They defined events asverbs (or action nouns) plus the associatednamed entities.
For instance, given the sentence?Yasser Arafat on Tuesday accused the UnitedStates of threatening to kill PLO officials?, theyfirst identified ?accused?, ?threatening?
and?kill?
as event terms; and ?Yasser Arafat?,?United States?, ?PLO?
and ?Tuesday?
as eventelements.
Encouraging results based on eventswere reported for news stories.From another point of view, sentences in adocument are somehow connected.
Sentencerelevance has been used as an alternative meansto identify important sentences.
Erkan and Radev(2004) and Yoshioka (2004) evaluate the rele-vance (similarity) between any two sentencesfirst.
Then a web analysis approach, PageRank,was used to select important sentences from asentence map built on relevance.
Promising re-sults were reported.
However, the combination ofthese features is not well studied.
Wu et al (2007)conducted preliminary research on this problem,but event features were not considered.Normally labeling procedure in supervisedlearning is very time consuming.
Blum andMitchell (1998) proposed co-training approach toexploit labeled and unlabeled data.
Promisingresults were reported from their experiments onweb page classification.
A number of successfulstudies emerged thereafter for other natural lan-guage processing tasks, such as text classification(Denis and Gilleron, 2003), noun phrase chunk-ing (Pierce and Cardie, 2001), parsing (Sarkar,2001) and reference or relation resolution (Mul-ler et al, 2001; Li et al, 2004).
To our knowl-edge, there is little research in the application ofco-training techniques to extractive summariza-tion.3 The Framework for Extractive Sum-marizationExtractive summarization can be regarded as aclassification problem.
Given the features of asentence, a machine-learning based classificationmodel will judge how likely the sentence is im-portant.
The classification model can be super-vised or semi-supervised learning.
Supervisedapproaches normally perform better, but requiremore labeled training data.
SVMs perform wellin many classification problems.
Thus we em-ploy it for supervised learning.
For semi-supervised learning, we co-trained a probabilistic986SVM and a Na?ve Bayesian classifier to exploitunlabeled data.Figure 1.
Learning-based Extractive Summariza-tion FrameworkThe automatic summarization procedure isshown in Figure 1.
First, each input sentence isexamined by going through the pre-specified fea-ture functions.
The classification model will thenpredict the importance of each sentence accord-ing to its feature values.
A re-ranking algorithmis then used to revise the order.
Finally, the topsentences are included in the summaries until thelength limitation is reached.
The re-ranking algo-rithm is crucial, as more important content areexpected to be contained in the final summarywith fixed length.
Important sentences above athreshold are regarded as candidates.
The onewith less words and located at the beginning partof a document is ranked first.
The re-ranking al-gorithm is described as follows.Ranki = RankPosi + RankLengthiwhere RankPosi is the rank of sentence i accord-ing to its position in a document (i.e.
the sentenceno.)
and RankLengthi is rank of sentence i ac-cording to its length.4 Sentence Features for ExtractiveSummarizationThis section provides a detailed description onthe four types of sentence features, i.e., surface,content, event and relevance features, which willbe examined systematically.4.1 Surface FeaturesSurface features are based on structure ofdocuments or sentences, including sentenceposition in the document, the number of words inthe sentence, and the number of quoted words inthe sentence (see Table 1).Name DescriptionPosition 1/sentence no.Doc_First Whether it is the first sentence of a documentPara_First Whether it is the first sentence of a paragraphLength The number of words in a sentenceQuote The number of quoted words in a sen-tenceTable 1.
Types of surface featuresThe intuition with respect to the importance ofa sentence stems from the following observations:(1) the first sentence in a document or a para-graph is important; (2) the sentences in the ear-lier parts of a document is more important thansentences in later parts; (3) a sentence is impor-tant if the number of words (except stop words)in it is within a certain range; (4) a sentence con-taining too many quoted words is unimportant.4.2 Content FeaturesWe integrate three well-known sentence featuresbased on content-bearing words i.e., centroidwords, signature terms, and high frequencywords.
Both unigram and bigram representationshave been investigated.
Table 2 summarizes thesix content features we studied.Name DescriptionCentroid_Uni The sum of  the weights of cen-troid uni-gramCentroid_Bi The sum of  the weights of cen-troid bi-gramsSigTerm_Uni The number of signature uni-gramsSigTerm_Bi The number of signature bi-gramsFreqWord_Uni The sum of  the weights of fre-quent uni-gramsFreqWord_Bi The sum of  the weights of fre-quent bi-gramsTable 2.
Types of content features4.3 Event FeaturesAn event is comprised of an event term and asso-ciated event elements.
In this study, we chooseverbs (such as ?elect and incorporate?)
and ac-tion nouns (such as ?election and incorporation?
)as event terms that can characterize actions.
Theyrelate to ?did what?.
One or more associatednamed entities are considered as event elements.Four types of named entities are currently under987consideration.
The GATE system (Cunninghamet al, 2002) is used to tag named entities, whichare categorized as <Person>, <Organization>,<Location> and <Date>.
They convey the infor-mation about ?who?, ?whom?, ?when?
and?where?.
A verb or an action noun is deemed anevent term only when it appears at least oncebetween two named entities.Event summarization approaches based on in-stances or concepts are investigated.
An occur-rence of an event term (or event element) in adocument is considered as an instance, while thecollection of the same event terms (or event ele-ments) is considered as a concept.
Given adocument set, instances of event terms and eventelements are identified first.
An event map isthen built based on event instances or concepts(Wu , 2006; Li et al, 2006).
PageRank algorithmis used to assign weight to each node (an instanceor concept) in the event map.
The final weight ofa sentence is the sum of weights of event in-stances contained in the sentence.4.4 Relevance FeaturesRelevance features are incorporated to exploitinter-sentence relationships.
It is assumed that: (1)sentences related to important sentences are im-portant; (2) sentences related to many other sen-tences are important.
The first sentence in adocument or a paragraph is important, and othersentences in a document are compared with theleading ones.
Two types of sentence relevance,FirstRel_Doc and FirstRel_Para (see Table 3),are measured by comparing pairs of sentencesusing word-based cosine similarity.Another way to exploit sentence relevance isto build a sentence map.
Every two sentences areregarded relevant if their similarity is above athreshold.
Every two relevant sentences are con-nected with a unidirectional link.
Based on thismap, PageRank algorithm is applied to evaluatethe importance of a sentence.
These relevancefeatures are shown in Table 3.Name DescriptionFirstRel_Doc Similarity with the first sentence in the documentFirstRel_Para Similarity with the first sentence in the paragraphPageRankRel PageRank value of the sentence based on the sentence mapTable 3.
Types of relevance features5 Supervised/Semi-supervised LearningApproachesTo incorporate features described in Section 4,we investigate supervised and semi-supervisedlearning approaches.
Probabilistic Support Vec-tor Machine (PSVM)  is employed as supervisedlearning (Wu et al, 2004), while the co-trainingof PSVM and Na?ve Bayesian Classifier (NBC)is used for semi-supervised learning.
The twolearning-based classification approaches, PSVMand NBC, are described in following sections.5.1 Probabilistic Support Vector Machine(PSVM)For a set of training examples ( ix , iy ),li ,...,1= , where ix  is an instance and iy  thecorresponding label, basic SVM requires the so-lution of the following optimization problem.
?=+liiTbwCww1,, 21  min ?
?subject to01  ))(( ,??
?+iiiTi bxwy??
?Here the SVM classifier is expected to find ahyper-plane to separate testing examples as posi-tive and negative.
Wu et al (2004) extend thebasic SVM to a probabilistic version.
Its goal isto estimatekixiyppi ,...1 ),|( === .First the pairwise (one-against-one) probabilities) ,or  |( xjiyiyprij ==?
is estimated usingBAfij er ++?
11where A and B are estimated by minimizing thenegative log-likelihood function using trainingdata and their decision values f. Then ip  is ob-tained by solving the following optimizationproblem?
?= ?
?ki ijjjijijipprpr1 :2)(21  minsubject to01  ))(( ,??
?+iiiTi bxwy??
?The problem can be reformulated asQPP TP 21  min988where      ??????
?== ?
?ji ifji ifQ2:ijijjisiissrrrThe problem is convex and the optimality condi-tions a scalar b such that??????=???????????
?1zbP0TeeQwhere e is the vector of all 1s and z is the vectorof all 0s, and b is the Lagrangian multiplier of theequality constraint ?==kiip11 .5.2 Na?ve Bayesian Classier (NBC)Na?ve Bayesian Classier assumes features areindependent.
It learns prior probability and con-ditional probability of each feature, and predictsthe class label by highest posterior probability.Given a feature vector (F1, F2, F3,?, Fn), theclassifier need to decide the label c:),...,,|(maxarg 321 ncFFFFcPc =By applying Bayesian rule, we have),...,,,()|,...,,,()(),...,,,|(321321321nnn FFFFPcFFFFPcPFFFFcP =Since the denominator does not depend on c andthe values of Fi are given, therefore the denomi-nator is a constant and we are only interested inthe numerator.
As features are assumed inde-pendent,?=?=niinncFPcPcFFFFPcPFFFFcP1321321)|()()|,...,,()(),...,,|(where )|( cFP i is estimated with MLE fromtraining data with Laplace Smoothing.5.3 Co-Training (COT)Supervised learning approaches require muchlabeled data and the labeling procedure is verytime-consuming.
Literature (Blum and Mitchell,1998; Collins, 1999) has suggested that unla-beled data can be exploited together with labeleddata by co-training two classifiers.
(Blum andMitchell, 1998) trained two classifiers of sametype on different features, and (Li et al, 2004)trained two classifiers of different types.
In thispaper, as the number of involved features is nottoo many, we train two different classifiers,PSVM and NBC, on the same feature spaces.The co-training algorithm is described as follows.Given:L is the set of labeled training examplesU is the set of unlabeled training examplesLoop: until the unlabeled data is exhaustedTrain the first classifier C1 (PSVM) on LTrain the second classifier C2 (NBC) on LFor each classifier CiCi labels examples from UCi chooses p positive and n negative ex-amples E from U.
These examples havetop classifying confidence.Ci removes examples E from UCi adds examples E with the correspond-ing labels to LEndOutput: label the test examples by the optimalclassifier which is evaluated on training data ac-cording to the classification performance.6 ExperimentsDUC 20012 has been used in our experiments.
Itcontains 30 clusters of relevant documents and308 documents in total.
Each cluster deals with aspecific topic (e.g.
a hurricane) and comes withmodel summaries created by NIST assessors.
50,100, 200 and 400 word summaries are provided.Twenty-five of the thirty document clusters areused as training data and the remaining five areused as testing.
The training/testing configurationis same in experiments of supervised learningand semi-supervised learning, while the differ-ence is that some sentences in training data arenot tagged for semi-supervised learning.An automatic evaluation package, i.e.,ROUGE (Lin and Hovy, 2003) is employed toevaluate the summarization performance.
Itcompares machine-generated summaries withmodel summaries based on the overlap.
Precisionand recall measures are used to evaluate the clas-sification performance.
For comparison, weevaluate our approaches on DUC 2004 data setalso.
It contains 50 clusters of documents.
Only665-character summaries are given by assessorsfor each cluster.6.1 Experiments on Supervised LearningApproachWe use LibSVM3 as our classification model forSVM classifiers normally perform better.
Typesof features presented in previous section areevaluated individually first.
Precision measures2 http://duc.nist.gov/3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/989the percentage of true important sentencesamong all important sentences labeled by theclassifier.
Recall measures the percentage of trueimportant sentences labeled by the classifieramong all true important sentences.Table 4 shows the precisions and recalls ofdifferent feature groups under the PSVM classi-fier.
Table 5 records the ROUGE evaluation re-sults ?
ROUGE-1, ROUGE-2 and ROUGE-L.They evaluate the overlap between machine-generated summaries and model summariesbased on unigram, bigram and long distance re-spectively.
The summary length is limited to 200words here.Feature Precision RecallSur 0.488 0.146Con 0.407 0.167Rel 0.488 0.146Event 0.344 0.146Sur+Con 0.575 0.160Sur+Rel 0.488 0.146Con+Rel 0.588 0.139Sur+Event 0.600 0.125Con+Event 0.384 0.194Rel+Event 0.543 0.132Sur+Con+Event 0.595 0.153Sur+Rel+Event 0.553 0.146Con+Rel+Event 0.581 0.125Sur+Con+Rel 0.595 0.174Sur+Con+Rel+Event 0.579 0.153Table 4.
Classification performance based ondifferent feature groupsFeature Rouge-1Rouge-2Rouge-LSur 0.373 0.103 0.356Con 0.352 0.074 0.334Rel 0.373 0.103 0.356Event 0.344 0.064 0.325Sur+Con 0.380 0.109 0.363Sur+Rel 0.373 0.103 0.356Con+Rel 0.375 0.103 0.358Sur+Event 0.348 0.091 0.332Con+Event 0.344 0.071 0.330Rel+Event 0.349 0.089 0.356Sur+Con+Event 0.379 0.106 0.363Sur+Rel+Event 0.371 0.101 0.353Sur+Con+Rel 0.396 0.116 0.358Sur+Con+Rel+Event 0.375 0.106 0.359Table 5.
ROUGE evaluation results for differ-ent feature groupsFrom Table 4, we can see the most useful fea-ture groups are ?surface?
and ?relevance?, i.e.the external characteristics of a sentence in thedocument and the relationships of a sentencewith other sentences in a cluster.
The evaluationscores from surface features and relevance fea-tures are the same.
We found that the reason isthat the dominating feature in each feature groupis about whether a sentence is the first sentencein a document.
The influence of event features isnot very positive.
Based on our analysis the rea-son is that not all clusters contain enough eventterms/elements to build a good event map.From Table 5, it can be seen that the combina-tion of multiple features or multiple featuregroups outperforms individual feature or featuregroups.
When surface, content and relevance fea-tures are employed, the best performance isachieved, i.e., ROUGE-1 and ROUGE-2 scoreare 0.396 and 0.116 respectively.
In our prelimi-nary experiments, we find ROUGE-1 score of amodel summary is 0.422 (without stemming andfiltering stop words).
Therefore summaries gen-erated by our supervised learning approach re-ceived comparable performance with modelsummaries when evaluated by ROUGE.
Al-though ROUGE is not perfect at this time, it isautomatic and good complement to subjectiveevaluations.We also find that the Rouge scores are similarfor variations on the feature set.
Sentences fromoriginal documents are selected to build the finalsummaries.
Normally, only four to six sentencesare contained in one 200-word summary in ourexperiments, i.e., few sentences will be kept in asummary.
As variations of the feature set onlyinduce little change of the order of most impor-tant sentences, the ROUGE scores change little.6.2 Experiments on Semi-supervised Learn-ing ApproachSupervised learning approaches normallyachieve good performance but require manuallylabeled data.
Recent literature (Blum andMitchell, 1998; Collins, 199) has suggested thatco-training techniques reduce the amount of la-beled data.
They trained two homogeneous clas-sifiers based on different feature spaces.
How-ever this method is unsuitable for our applicationas the number of required features in our case isnot too many.
Therefore we develop a co-training approach to train different classifiersbased on same feature space.
PSVM and NBCare applied to the combination of surface, contentand relevance features.The capability of different learning approachesto identify important sentences is shown in Fig-990ure 2.
The ?x?
axis shows the number of labeledsentences employed.
The remained training sen-tences in DUC 2001 are employed as unlabeledtraining data.
The y axis shows f-measures ofimportant sentences identified from the test set.The size of the training seed set is investigated.For each size, three different seed sets which arechose randomly are used.
The average evaluationscores are used as the final performance.
Thisprocedure avoids the variance of the final evalua-tion results.
The ROUGE evaluation results ofthese supervised learning approaches and semi-supervised learning approaches are shown in Ta-ble 6 (2000 labeled sentences).
It can be seen thatthe ROUGE performance of co-trained classifiersis better than that of individual classifiers.00.10.20.30.450 100 200 500 1000 2000Number of Labeled SentencesF-Measure CotrainBayesSvmFigure 2.
Performance of supervised learningand semi-supervised learning approachesLearningApproaches Rouge-1 Rouge-2 Rouge-LPSVM 0.358 0.082 0.323NBC 0.353 0.061 0.317COT 0.366 0.090 0.329Table 6.
ROUGE evaluation results of supervisedlearning and semi-supervised learning6.3 Experiments on Summary LengthIn DUC 2001 dataset, 50, 100, 200 and 400-wordsummaries are provided to evaluate summarieswith different length.
Our supervised approach,which generates the best performance in previousexperiments, is employed.
The ROUGE scores ofevaluations on different summary length areshown in Table 7.
Our summaries consist of ex-tracted sentences.
It can be seen that these sum-maries achieve lower ROUGE scores when thelength of summary is reduced.
The reason is thatwhen people try to write a more concise sum-mary, condensed contents are included in thesummaries, which may not use the original con-tents directly.
Therefore the word-overlappingtest tool in ROUGE generates lower scores.We then tested the same classifier and samefeatures on DUC 2004.
The length of summariesis only 665 characters (about 100 words).ROUGE-1 and ROUGE-2 are 0.329 and 0.073respectively.
It confirms that the performance ofour approach is sensitive to the length of thesummary.Sum_length Rouge-1 Rouge-2 Rouge-L50 0.241 0.036 0.205100 0.309 0.085 0.277200 0.396 0.116 0.358400 0.423 0.118 0.402Table 7.
ROUGE evaluation results for differ-ent summary length7 Conclusions and Future WorkWe explore surface, content, event, relevancefeatures and their combinations for extractivesummarization with supervised learning ap-proach.
Experiments show that the combinationof surface, content and relevance features per-form best.
The highest ROUGE-1, ROUGE-2scores are 0.396 and 0.116 respectively.
TheRouge-1 score of manually generated summariesis 0.422.
This shows the ROUGE performance ofour supervised learning approach is comparableto that of manually generated summaries.
TheROUGE-1 scores of extractive summarizationbased on centroid, signature word, high fre-quency word and event individually are 0.319,0.356, 0.371 and 0.374 respectively.
It can beseen that our summarization approach based oncombination of features improves the perform-ance obviously.Although the results of supervised learningapproach are encouraging, it required much la-beled data.
To reduce labeling cost, we apply co-training to combine labeled and unlabeled data.Experiments show that compare with supervisedlearning, semi-supervised learning approachsaves half of the labeling cost and maintainscomparable performance (0.366 vs 0.396).
Wealso find that our extractive summarization issensitive to length of the summary.
When thelength is extended, the ROUGE scores of samesummarization method are improved.
In the fu-ture, we plan to investigate sentence compressionto improve performance of our summarizationapproaches on short summaries.AcknowledgementThe research described in this paper is partiallysupported by Research Grants Council of HongKong (RGC: PolyU5217/07E), CUHK StrategicGrant Scheme (No: 4410001) and Direct GrantScheme (No: 2050417).991ReferencesRegina Barzilay, and Michael Elhadad.
1997.
Usinglexical chains for text summarization.
In Proceed-ings of the 35th Annual Meeting of the Associationfor Computational Linguistics Workshop on Intel-ligent Scalable Text Summarization, pages 10-17.Avrim Blum and Tom Mitchell.
1998.
Combininglabeled and unlabeled data with co-training.
InProceedings of the 11th Annual Conference onComputational Learning Theory, pages 92-100.Hamish Cunningham, Diana Maynard, KalinaBontcheva, Valentin Tablan.
2002.
GATE: aframework and graphical development environ-ment for robust NLP tools and applications.
InProceedings of the 40th Annual Meeting of the As-sociation for computational Linguistics.Francois Denis and Remi Gilleron.
2003.
Text classi-fication and co-training from positive and unla-beled examples.
In Proceedings of the 20th Inter-national Conference on Machine Learning Work-shop: the Continuum from Labeled Data to Unla-beled Data in Machine Learning and Data Mining.Gunes Erkan and Dragomir R. Radev.
2004.
LexPag-eRank: prestige in multi-document text summariza-tion.
In Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Process-ing, pages 365-371.Elena Filatova and Vasileios Hatzivassiloglou.
Event-based extractive summarization.
2004.
In Proceed-ings of the 42nd Annual Meeting of the Associationfor Computational Linguistics Workshop, pages104-111.Gerald Francis DeJong.
1978.
Fast skimming of newsstories: the FRUMP system.
Ph.D. thesis, YaleUniversity.Wenjie Li, Guihong Cao, Kam-Fai Wong and ChunfaYuan.
2004.
Applying machine learning to Chinesetemporal relation resolution.
In Proceedings of the42nd Annual Meeting of the Association for Com-putational Linguistics, pages 583-589.Wenjie Li, Wei Xu, Mingli Wu, Chunfa Yuan, Qin Lu.2006.
Extractive summarization using inter- and in-tra- event relevance.
In proceedings of Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages369-376.Chin-Yew Lin; Eduard Hovy.
2000.
The automatedacquisition of topic signatures for text summariza-tion.
In Proceedings of the 18th International Con-ference on Computational Linguistics, pages 495-501.Chin-Yew Lin and Eduard Hovy.
2003.
Automaticevaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Human Language Technology Conference of theNorth American Chapter of the Association forComputational Linguistics, Edmonton, Canada.Daniel Marcu.
1997.
The rhetorical parsing of naturallanguage texts.
In Proceedings of the 35th AnnualMeeting of the Association for computational Lin-guistics, pages 96-103.Christoph Muller, Stefan Rapp and Michael Strube.2001.
Applying co-training to reference resolution.In Proceedings of the 40th Annual Meeting on As-sociation for Computational Linguistics.Ani Nenkova, Lucy Vanderwende and KathleenMcKeown.
2006.
A compositional context sensi-tive multi-document summarizer: exploring thefactors that influence summarization.
In Proceed-ings of the 29th Annual International ACM SIGIRConference on Research and Development in In-formation Retrieval.David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Process-ing, pages 1-9.Dragomir R. Radev, Timothy Allison, et al 2004.MEAD - a platform for multidocument multilin-gual text summarization.
In Proceedings of 4th In-ternational Conference on Language Resourcesand Evaluation.Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In Proceedings of 2nd Meetingof the North American Chapter of the Associationfor Computational Linguistics on Language Tech-nologies.Mingli Wu.
2006.
Investigations on event-basedsummarization.
In proceedings of the 21st Interna-tional Conference on Computational Linguisticsand 44th Annual Meeting of the Association forComputational Linguistics Student Research Work-shop, pages 37-42.Mingli Wu, Wenjie Li, Furu Wei, Qin Lu and Kam-Fai Wong.
2007.
Exploiting surface, content andrelevance features for learning-based extractivesummarization.
In Proceedings of 2007 IEEE In-ternational Conference on Natural LanguageProcessing and Knowledge Engineering.Ting-Fan Wu, Chih-Jen Lin and Ruby C. Weng.
2004.Probability estimates for multi-class classificationby pairwise coupling.
Journal of Machine LearningResearch, 5:975-1005.Masaharu Yoshioka and Makoto Haraguchi.
2004.Multiple news articles summarization based onevent reference information.
In Working Notes ofthe Fourth NTCIR Workshop Meeting, National In-stitute of Informatics.992
