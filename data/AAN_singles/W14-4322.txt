Proceedings of the SIGDIAL 2014 Conference, pages 161?170,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsIn-depth Exploitation of Noun and Verb Semanticsto Identify Causation in Verb-Noun PairsMehwish Riaz and Roxana GirjuDepartment of Computer Science and Beckman InstituteUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801, USA{mriaz2,girju}@illinois.eduAbstractRecognition of causality is important toachieve natural language discourse under-standing.
Previous approaches rely onshallow linguistic features.
In this work,we propose to identify causality in verb-noun pairs by exploiting deeper seman-tics of nouns and verbs.
Particularly, weacquire and employ three novel types ofknowledge: (1) semantic classes of nounswith a high and low tendency to encodecausality along with information regard-ing metonymies, (2) data-driven seman-tic classes of verbal events with the leasttendency to encode causality, and (3) ten-dencies of verb frames to encode causal-ity.
Using these knowledge sources, weachieve around 15% improvement in F-score over a supervised classifier trainedusing linguistic features.1 IntroductionThe identification of cause-effect relations is crit-ical to achieve natural language discourse under-standing.
Causal relations are encoded in text us-ing various linguistic constructions e.g., betweentwo verbs, a verb and a noun, two discourse seg-ments, etc.
In this research, we focus on identify-ing causality encoded between a verb and a noun(or noun phrase).
For example, consider the fol-lowing example:1.
At least 1,833 people died in the hurricane.In example (1), the verb-noun phrase pair?died?-?the hurricane?
encodes causality whereevent ?died?
is the effect of ?hurricane?
event.Previously several approaches have been pro-posed to identify causality between two verbs(Bethard and Martin, 2008; Riaz and Girju, 2010;Do et al., 2011; Riaz and Girju, 2013) and dis-course segments (Sporleder and Lascarides, 2008;Pitler and Nenkova, 2009; Pitler et al., 2009).However, the problem of identifying causality inverb-noun pairs has not received a considerableattention.
For example, Do et al.
(2011) havestudied this task but they worked only with a listof predefined nouns representing events.
In thiswork, we focus on the linguistic construction ofverb-noun (or noun phrase) pairs where noun canbe of any semantic type.Traditional approaches for identifying causal-ity mainly employ linguistic features (e.g., lexicalitems, part-of-speech tags of words, etc.)
in theframework of supervised learning (Girju, 2003;Sporleder and Lascarides, 2008; Bethard and Mar-tin, 2008; Pitler and Nenkova, 2009; Pitler et al.,2009) and do not involve deeper semantics of lan-guage.
Analysis of such approaches by Sporlederand Lascarides (2008) have revealed that the lin-guistic features are not always sufficient to achievea good performance on the task of identifying se-mantic relations including causality.
In this work,we propose a model that deeply processes andacquires the specific semantic information aboutthe participants of a verb-noun phrase (v-np) pair(i.e., noun and verb semantics) to identify causal-ity with a better performance over the baselinemodel depending merely on shallow linguistic fea-tures.The work in this paper builds on our recent workreported in Riaz and Girju (2014).
In that previ-ous model, we identified the semantic classes ofnouns and verbs with a high and low tendency toencode causation.
For example, a named entitysuch as LOCATION may have the least tendencyto encode causation.
We leveraged such informa-tion about nouns to filter false positives.
Sim-ilarly, we utilized the TimeBank?s (Pustejovskyet al., 2006) classification of verbal events (i.e.,Occurrence, Perception, Aspectual, State, I State,I Action and Reporting) and their definitions toclaim that the reporting events (e.g., say, tell, etc.
)161just describe and narrate other events instead ofencoding causality with them.
We proposed an In-teger Linear Programming (ILP) model (Roth andYih, 2004; Do et al., 2011) to combine noun andverb semantics with the decisions of a supervisedclassifier which only relies on linguistic features.In this paper, we extend our previous model byacquiring and exploiting the following three noveltypes of knowledge:1.
We learn the information about tendencies ofvarious verb frames to encode causation.
Forexample, our model identifies if the subject ofverb ?destroy?
(?occur?)
has a high (low) ten-dency to encode causation.
Such informationhelps gain performance by exploiting causalsemantics of each verb frame separately.
Wealso learn and incorporate information aboutthe verb frames in general e.g., how likely it isfor the subject of any verb to encode causationwith its verb.2.
In Riaz and Girju (2014), we utilized the Time-Bank?s definition of reporting events to arguethat such events have the least tendency to en-code causation.
Instead of relying on humanjudgment we now introduce a data intensive ap-proach to identify the TimeBank?s classes ofevents with the least tendency to encode cau-sation.3.
Although, information about the nouns withthe least tendency to encode causation helps tofilter false positives it can lead to false nega-tives when metonymic readings are associatedwith such nouns.
Therefore, we introduce ametonymy resolver on top of our current modelto avoid false negatives.We provide details of our previously proposedmodel in section 3.
We introduce new model anddiscuss its performance in sections 4 and 5.
Sec-tion 6 concludes the current research.2 Relevant WorkIn Natural Language Processing (NLP), re-searchers are showing lots of interest in the taskof identifying causality due to its various applica-tions e.g., question answering (Girju, 2003), sum-marization (Chklovski and Pantel, 2004), futureprediction (Radinsky and Horvitz, 2013), etc.Several approaches have been proposed to iden-tify causality in pairs of verbal events (Bethardand Martin, 2008; Riaz and Girju, 2010; Do etal., 2011; Riaz and Girju, 2013) and discoursesegments (Sporleder and Lascarides, 2008; Pitlerand Nenkova, 2009; Pitler et al., 2009).
Howevercausality a pervasive relation of language can beencoded via various linguistic constructions.
Forexample, verbs and nouns are the key componentsof language to represent events.
Therefore in thiswork we focus on identifying causality in verb-noun pairs.Previously researchers have followed the pathof utilizing linguistic features in the frameworkof supervised learning (Girju, 2003; Bethard andMartin, 2008; Sporleder and Lascarides, 2008;Pitler and Nenkova, 2009; Pitler et al., 2009).Though linguistic features are important but othersources of knowledge are also critically requiredto achieve progress on the current task.In recent years, researchers have proposed un-supervised metrics to identify causality betweenevents (Riaz and Girju, 2010; Do et al., 2011).For example, Riaz and Girju (2010) and Do etal.
(2011) introduced unsupervised metrics tolearn causal dependencies between events.
Thesemetrics mainly depend on probabilities of co-occurrences of events and do not distinguish wellcausality from any other types of correlation (Riazand Girju, 2013).
In order to overcome this prob-lem Riaz and Girju (2013) proposed some ad-vanced metrics which combine probabilities of co-occurrences of events with the supervised esti-mates of cause and non-cause relations.Considering the importance of employing richsources of knowledge other than linguistic featuresfor the current task, we have recently proposed amodel that incorporates semantic classes of nounsand verbs with a high and low tendency to encodecausation (Riaz and Girju, 2014).
In this work, weexploit information about verb frames, data-drivenverb semantics and metonymies to achieve moreprogress on our recent work.3 Model for Recognizing CausalityIn this section we provide an overview of our pre-vious model (Riaz and Girju, 2014) for identifyingcausality in v-np pairs where v (np) stands for verb(noun phrase).
This model works in the followingtwo stages: (1) A supervised classifier is used tomake binary predictions (i.e., the label cause (C)or non-cause (?C)) employing linguistic features,and (2) noun and verb semantics are then com-bined with the predictions of supervised classifierin the ILP framework to identify causality.1623.1 Supervised ClassifierTo the best of our knowledge, there is no dataset of v-np pairs with the labels C and ?C avail-able to us.
For the current task we employ someheuristics to extract a training corpus of v-np pairsusing FrameNet (Baker et al., 1998).
FrameNetprovides frame elements for the verbs and handannotated examples (aka annotations) of theseframe elements.
Consider the following annota-tion from FrameNet ?They died [Causefrom shot-gun wounds]?
where the frame element ?Cause?is given for the verb ?died?.
We remove the prepo-sition ?from?
from the above annotation of frameelement to acquire an instance of v-np (i.e., died-shotgun wounds) pair.
We extract all annotationsfor verbs from FrameNet in which a frame elementmust contain at least one noun and no verb in it.We found such annotations for 729 distinct frameelements.
We manually assigned the labels C and?C to these frame elements.
Cause, Purpose, Rea-son, Result, Explanation are some examples of theframe elements to which we assigned the label C.Using the above mentioned assignments of labelsC and ?C to frame elements, we have acquireda training corpus of 4, 141 (77, 119) C (?C) in-stances from FrameNet.
In order to avoid class im-balance while training we employ an equal num-ber of instances of both labels.Due to space constraints, we refer the reader toAppendix A for the details of linguistic featuresto build the supervised classifier.
We employ bothNaive Bayes (NB) and Maximum Entropy (Max-Ent) algorithms to acquire predictions and prob-abilities of assignments of labels.
We set up thefollowing ILP using these probabilities:Z1= max?v-np?I?l?L1x1(v-np, l)P (v-np, l) (1)?l?L1x1(v-np, l) = 1 ?
v-np ?
I (2)x1(v-np, l) ?
{0, 1} ?
v-np ?
I ?l ?
L1(3)Here, L1= {C,?C}, I is the set of all v-np pairs.x1(v-np, l) is a binary decision variable set to 1only if the label l ?
L1is assigned to a v-np pairand only one label out of |L1| choices can be as-signed to a v-np pair (see constraints 2 and 3).
Inparticular, we maximize the objective function Z1(1) assigning the labels l ?
{L1} to v-np pairs de-pending on the probabilities of assignments (i.e.,P (v-np, l)) obtained through the supervised clas-sifier.3.2 Noun and Verb SemanticsWe automatically acquire and employ semanticclasses of nouns and verbs with a high and lowtendency to encode causation.
Such informationhelps to reduce errors in predictions of the super-vised classifier.We derive two semantic classes of nouns for ourpurpose i.e., Cnpand ?Cnpwhere the class Cnp(?Cnp) represents the noun phrases with a high(low) tendency to encode causation.
For exam-ple, a noun phrase expression for a location hasthe least tendency to encode causation unless ametonymic reading is associated with it.
In or-der to acquire these classes, we extract annotationsof 936 distinct frame elements from FrameNetin which a frame element must contain at leastone noun and no verb in it.
These annotationsof frame elements roughly represent instances ofnoun phrases (np).
We manually assigned the la-bels Cnpand ?Cnpto the frame elements.
Forexample, we assign the label ?Cnpto the frameelement ?Place?
which represents a location (seeAppendix B for some examples of the frame ele-ments with labels Cnpand ?Cnp).
We also fol-low the approach similar to Girju and Moldovan(2002) to employ WordNet senses of nouns to ac-quire more instances of the classes Cnpand ?Cnp(see Appendix B for the details).
We have ac-quired a total of 280, 212 instances of np (50%for each of the two classes i.e., Cnpand ?Cnp)using both FrameNet and WordNet.
Using theseinstances, we build a supervised classifier to iden-tify the semantic class of np (see Appendix B forthe details of features to build the classifier).
Weincorporate the knowledge of semantic classes ofnouns by making the following additions to ILP:Z2= Z1+?v-np?I?M?l?L2x2(fnp(v-np), l)P (fnp(v-np), l)(4)?l?L2x2(fnp(v-np), l) = 1 ?
v-np ?
I ?M (5)x2(fnp(v-np), l) ?
{0, 1} ?
v-np ?
I ?M (6)?l ?
L2x1(v-np,?C)?
x2(fnp(v-np),?Cnp) ?
0 (7)?
v-np ?
I ?MHere L2= {Cnp,?Cnp}.
fnp(v-np) is a func-tion which returns np of a v-np pair.
M is the setof v-np pairs with metonymic readings associatedwith np.
Currently, this set is empty and in sec-tion 4.3 we introduce a metonymy resolver to pop-163ulate this set.
x2(fnp(v-np), l) is a binary decisionvariable set to 1 only if the label l ?
L2is assignedto np and only one label out of |L2| choices canbe assigned to np (see constraints 5 and 6).
Con-straint 7 enforces that if an np belongs to the class?Cnpthen its corresponding v-np pair is assignedthe label ?C.
In particular, we maximize the ob-jective function Z2(4) subject to the constraintsintroduced till now.
For each v-np pair, we predictthe semantic class of np using our supervised clas-sifier for the labels l ?
L2and set the probabilities?
i.e., P (fnp(v-np), l) = 1, P (fnp(v-np), {L2} ?
{l}) = 0 if the label l ?
L2is assigned to np.
Alsobefore running our supervised classifier, we run anamed entity recognizer (Finkel et al., 2005) andassign the label ?Cnpto all noun phrases identi-fied as named entities.
We also determine associa-tion of metonymies with the noun phrases identi-fied as named entities.For the current task we also acquire two seman-tic classes of verbs i.e., Cevand ?Cevwhere theclass Cev(?Cev) contains the verbal events with ahigh (low) tendency to encode causation.
In orderto derive these two classes we exploit the Time-Bank corpus (Pustejovsky et al., 2003) which pro-vides seven semantic classes of verbal events ?
i.e.,Occurrence, Perception, Aspectual, State, I State,I Action and Reporting.
According to the defini-tions of these classes, we claim that the report-ing events (e.g., say, tell, etc.)
just describe andnarrate other events instead of encoding causalitywith them.
Using this claim, we consider that allinstances of reporting verbal events of TimeBankbelong to the class ?Cevand the rest of instancesof verbal events lie in the class Cev.
After ac-quiring instances of the classes Cevand ?Cev, webuild a supervised classifier for these two classes.We use the features introduced by Bethard andMartin (2006) to build this classifier (see Bethardand Martin (2006) for the details).
Employing pre-dictions and probabilities of assignments of the la-bels Cevand ?Cevwe add the following two con-straints to ILP: (1) if the event represented by vbelongs to ?Cevthen the corresponding v-np pairmust be labeled with ?C and (2) if a v-np pair isa causal pair then the event represented by v mustbe labeled with Cev.4 Enriched Verb and Noun SemanticsThis section describes the novel contributions ofthis work i.e., identification of semantics of verbframes, semantic classes of verbal events via a dataintensive approach and association of metonymicreadings with noun phrases to identify causalitywith a better performance.4.1 Verb FramesWe introduce a method to acquire tendencies ofvarious verb frames to encode causation.
Considerthe following two examples to understand the ten-dencies of verb frames of form {v, gr} to encodecausation where v is the verb and gr is the gram-matical relation of np with the verb v.1.
The Great Storm of October 1987 almost totally de-stroyed the eighty year old pinetum at Nymans Gardenin Sussex.
(Cause (C))2.
The explosion occurred in the city?s main business area.
(Non-Cause (?C))In above two examples the nps ?The GreatStorm of October 1987?
and ?The explosion?
havethe grammatical relations of subject with the verbs?destroyed?
and ?died?.
In examples (1) and (2)the verb frames {destroy, subject} and {occur,subject} encode cause and non-cause relations.These examples reveal that each verb frame hasits own tendency to encode causation.
This typeof knowledge helps gain performance by exploit-ing the semantics of each verb frame separately.We leverage FrameNet annotations to acquiresuch type of knowledge.
We collect all annota-tions of verbs from FrameNet and assign the la-bels C and ?C to the frame elements as discussedin section 3.1.
In FrameNet, example (1) is givenas follows:3.
[CauseThe Great Storm of October 1987] [Degreealmosttotally] destroyed [Undergoerthe eighty year old pinetumat Nymans Garden in Sussex].According to our assignments of labels C and?C to the frame elements, example (1) is givenas ?
[CThe Great Storm of October 1987] [?Cal-most totally] destroyed [?Cthe eighty year oldpinetum at Nymans Garden in Sussex].?.
After ac-quiring instances of the labels C and ?C from ex-ample (1), we populate the fields of a knowledgebase of verb frames (see Table 1).
Fields of thisknowledge base are {v, gr}, count({v, gr},C) andcount({v, gr},?C).
gr is the dependency relationof the frame element with the verb v. We use Stan-ford?s dependency parser (Marneffe et al., 2006)to collect dependency relations.
count({v, gr},C)(count({v, gr},?C)) is the count of the label C(?C) of the frame {v, gr}.
As shown in Table 1,164for the frame element ?The Great Storm of Octo-ber 1987?, the word ?Storm?
has the dependencyrelation of ?nsubj?
with the verb ?destroy?.
Ifthere exists more than one dependency relationsbetween the frame element and its verb then wechoose the very first relation in the text order.
Ac-cording to the counts given in Table 1, {destroy,nsubj} has more tendency to encode a cause re-lation than the non-cause one.
We have acquired7,156 and 114,898 instances of the labels C and?C from FrameNet for populating the knowledgebase of verb frames.
We compute tendencies ofverb frames to encode causality using the follow-ing scores:S({v, gr}, l) = S1({v, gr}, l)?
S2({*, gr}, l) (8)S1({v, gr}, l) =count({v,gr},l)count({v,gr},l)+count({v,gr},L1?
{l})S2({*, gr}, l) =count({*,gr},l)count({*,gr},l)+count({*,gr},L1?
{l})Counts of first component (S1) can be takenfrom the knowledge base of verb frames of form{v, gr}.
The second component (S2) with countscount({*, gr}, l) and count({*, gr}, L1?
{l})captures tendencies of verb frames in general.For example, what is the tendency of any subjectto encode causality with its verb i.e., the scoreS2({?, nsubj},C).
We populate the knowledgebase of Table 1 with equal number of C and ?Cinstances to calculate counts for S2.
We make thefollowing additions to ILP to incorporate informa-tion about verb frames:Z3= Z2+?v-np?I?g(v-np)?KB?fnp(v-np)?Cnp?l?L1x3(g(v-np), l)S(g(v-np), l)(9)?l?L1x3(g(v-np), l) = 1 ?v-np?I?g(v-np)?KB?fnp(v-np)?Cnp(10)x3(g(v-np), l) ?
{0, 1} ?l ?
L1,?v-np?I?g(v-np)?KB?fnp(v-np)?Cnp(11)x3(g(v-np), l) ?
x1(v-np, l) ?l ?
L1, (12)?v-np?I?g(v-np)?KB?
fnp(v-np)?Cnpx1(v-np, l) ?
x3(g(v-np), l) ?l ?
L1, (13)?v-np?I?g(v-np)?KB?fnp(v-np)?CnpHere, KB is the knowledge base of verb framesand g(v-np) is the function which returns the verbframe i.e., {v, gr}.
This function returns NULLvalue if there is no grammatical relation betweenv and np in an instance.
The above changes inILP are only applicable for the v-np pairs with{v, gr} count({v, gr},C) count({v, gr},?C){destroy,nsubj} 1 0{destroy,advmod} 0 1{destroy,dobj} 0 1[CThe Great Storm of October 1987] [?Calmost totally] de-stroyed [?Cthe eighty year old pinetum at Nymans Garden inSussex].Table 1: A knowledge base of verb frames.
Thisknowledge base is populated using the instancesof C and ?C labels given in this table.g(v-np) ?
KB and np identified as of class Cnpbecause we have already filtered the cases of np ?
?Cnpin section 3.2. x3(g(v-np), l) is a binary de-cision variable set to 1 only if the label l ?
L1is assigned to g(v-np) and only one label out of|L1| choices can be assigned to g(v-np) (see con-straints 10 and 11).
We add information about verbframes using constraints 12 and 13.
These con-straints enforce the predictions of the supervisedclassifier of causality (section 3.1) to be consis-tent with the predictions using tendencies of verbframes (i.e., score S({v, gr}, l)).
We maximizeobjective function (9) subject to the above con-straints.
We remove those {v, gr} from KB whichhave count({v, gr},C)+count({v, gr},?C) < 5to avoid wrong predictions based on the smallcounts of verb frames.4.2 Data-driven Verb SemanticsIn section 3.2 we considered that reporting eventsbelong to the class ?Cevwith the least tendencyto encode causation using the definition of theseevents in the TimeBank corpus.
Instead of re-lying on definitions of events we now introducea data intensive approach to automatically iden-tify the class ?Cevof verbal events.
In orderto identify this class we extract training instancesof verbal events encoding C and ?C relations.Verbal events encode cause-effect relations usingverb-verb (e.g., Five shoppers were killed when acar blew up.)
and verb-noun linguistic construc-tions.
Therefore for the current purpose we usethe following two types of training instances: (A)a training corpus of 240K instances of verb-verb(vi-vj) pairs encoding C and ?C relations (namedas Trainingvi-vj) (we refer the reader to Riaz andGirju (2013) for the details of this training corpus)and (B) the training corpus v-np instances intro-duced in section 3.1 (named as Trainingv-np).Following is the procedure to derive V?C?V where V={Occurrence, Perception, Aspectual,State, I State, I Action, Reporting} and the setV?Ccontains the TimeBank?s semantic classes165with the least tendency to encode a cause relation.1.
Input: Training corpus, V2.
Output: Set V?C3.
For each training instance k employ the supervised clas-sifier of Bethard and Martin (2006) to do the following:(a) if k ?
Trainingvi-vjthen identify the semantic class(sc) of both events represented by both verbs viandvjand add this information to a set i.e., T = T ?
(kvi, scvi, l) ?
(kvj, scvj, l) where scviis the se-mantic class of event of the verb viof instance kand l ?
{C,?C}.
(b) Else if k ?
Trainingv-npthen identify the semanticclass (sc) of event represented by the verb v and setT = T ?
(kv, scv, l).4.
Using results of step 3, calculate tendency of each se-mantic class sc ?
V to encode non-causality (i.e.,score(sc,?C)) as follows:score(sc,?C) = score1(sc,?C)?
score2(sc,?C)score1(sc,?C) = (count(sc,?C)count(sc)?count(sc,C)count(sc))score2(sc,?C) = (count(sc,?C)count(?C)?count(sc,C)count(C))where count(m, n) is the number of instances of verbalevents with the labels m and n and count(m) is the numberof instances of verbal events with the label m.5.
Acquire a ranked list of semantic classes listsc= [sc1, sc2,.
.
.
scm] s.t.
score(sci,?C) ?
score(sci+1,?C).
Fromthis list we remove the class sciif either score1(sci, ?c)< 0 or score2(sci, ?c) < 0..
The following steps are used to determine the cutoffclass sci?
listscs.t.
the semantic classes {sc1, sc2, .
.
.,sci-1} have the least tendency to encode causation.6.
resultsc?1= 0 and resultsc0= 0.7.
Remove scifrom the front of listscand do the following:(c) Predict the label (l) ?C for all tuples of form(m, sc, l) ?
T if sc ?
{sc1, sc2, .
.
., sci} and pre-dict C for the rest of the tuples.
(d) Using predictions from step (c), calculate theresultsci= F1-score ?
accuracy for the label l ?{C,?C}.
(e) If resultsci?resultsci-1< resultsci-1?resultsci-2then output {sc1, sc2, .
.
., sci?1}(f) Else go to step 7.Using the above procedure, we obtain thesets {Aspectual} and {Reporting, I State} withTrainingvi-vjand Trainingv-npcorpora.
We con-sider that the Aspectual, Reporting and I Stateevents of the TimeBank corpus belong to the class?Cevand rest of the events lie in Cev.
Using thesesemantic classes we apply the constraints intro-duced in section 3.2.4.3 Metonymy Resolution:Metonymy resolution is the task to determine if aliteral or non-literal reading is associated with a{v, gr} count({v, gr},Cnp) count({v, gr},?Cnp){kill,nsubj} 1 0{kill,dobj} 0 1[CnpPissed off Angelus] just kills [?Cnpme]Table 2: A knowledge base of verb frames.
Thisknowledge base is populated using the instancesof Cnpand ?Cnplabels given in this table.natural language expression (Markert and Nissim,2009).
Consider the following example:4.
The United States has killed Osama bin Laden and hascustody of his body.
(Cause (C))In example (4) ?The United States?
refers to anon-literal reading i.e., the event of ?raid in Ab-bottabad on May 2, 2011 by the United States?rather than merely referring to a literal sense i.e.,a country.
The association of non-literal readingwith ?The United States?
results in killing event.Previously, researchers have worked with hand-annotated selectional restrictions violation for thistask (Markert and Nissim, 2009).
In the exam-ple (4) a country cannot ?kill?
someone and thusa metonymic reading is associated with it.
In thiswork we identify association of metonymies withnoun phrases via verb frames and prepositions asexplained below in this section.In the first part of our approach we employviolations of tendencies of verb frames to iden-tify if a non-literal reading is associated with anoun phrases.
Particularly, we build a knowledgebase of verb frames using Cnpand ?Cnpclassesas discussed in section 4.1.
Consider the knowl-edge base given in Table 2 populated using thefollowing FrameNet annotations ?
[StimulusPissedoff Angelus] just kills [Experiencerme].?
with as-signments of labels Cnpand ?Cnpto the frameelements.
We populate the knowledge base usingonly those FrameNet annotations in which a frameelement does not contain a verb.Now we introduce our method to identify theassociation of non-literal reading with the ?TheUnited States?
in example (4).
The supervisedclassifier predicts the class ?Cnpfor the np ?TheUnited States?.
However, in the current stateof knowledge base (Table 2) P({destroy, nsubj},Cnp) > P({destroy, nsubj}, ?Cnp) where P isthe probability.
The prediction of ?Cnpfor ?TheUnited States?
violates the above probabilities.Considering this violation, we predict the associa-tion of metonymy with np.In the second part of our approach we iden-tify tendencies of prepositions to encode causation166and use violation of these tendencies to identifymetonymies.
For this purpose, we use the trainingcorpus of v-np pairs with 4, 141 C and 77, 119 ?Ctraining instances (see section 3.1).
We employonly those training instances in which a preposi-tion appears between v and np and there appearsno verb between them.
From these instances, weacquire a set of prepositions that appear betweenv and np.
Using this set of prepositions (PR) asinput to the following procedure, we acquire a setof prepositions (PRC) with the highest tendencyto encode causation:1.
Input: Training Corpus of v-np pairs, PR2.
Output: PRC3.
Calculate tendency of each preposition pr ?
PR to en-code causality (i.e., score(pr,C)) as follows:score(pr,C) = score1(pr,C)?
score2(pr,C)score1(pr,C) = (count(pr,C)count(pr)?count(pr,?C)count(pr))score2(pr,C) = (count(pr,C)count(C)?count(pr,?C)count(?C))4.
Acquire a ranked list of prepositions listpr= [pr1, pr2, .
.
.prm] s.t.
score(pri,C) ?
score(pri+1,C).
From thislist we remove priif either score1(pri, C) or score2(pri,C) < 0.5. resultpr?1= 0, resultpr0= 06.
Remove prifrom the front of the listprand do the follow-ing:(a) Predict the label C for all v-np training instanceswith pr ?
{pr1, pr2, .
.
., pri} and assign the label?C to the rest of the instances.
(b) Using predictions from step (a) calculate theresultpri= F1-score ?
accuracy.
(c) If resultpri-resultpri-1< resultpri-1-resultpri-2thenoutput {pr1, pr2, .
.
., pri?1}.
(d) Else go to step 6.The above procedure outputs the set PRC={for, by}.
Now we introduce method to identifyassociation of non-literal reading for the example?All weapon sites in Iraq were destroyed by theUnited States?
where ?the United States?
?
?Cnpas identified by the supervised classifier.
However,the preposition ?by?
has a high tendency to en-code causation and thus ?the United States?
mayencode causation.
Therefore, there is a possibil-ity that this noun phrase has a non-literal senseattached to it which results in encoding causality.Using this method, we predict metonymies onlyfor the v-np instances where preposition appearsbetween v and np and there appears no verb be-tween them.
If any of two methods of metonymyresolution predicts the association of metonymywith np then we add v-np to the set M used inILP (see section 3.2).5 Evaluation and DiscussionIn this section we present experiments and discus-sion on the performance achieved for the currenttask.
In order to evaluate our model, we generateda test set of instances of v-np pairs.
For this pur-pose, we collected three wiki articles on the topicsof Hurricane Katrina, Iraq War and Egyptian Rev-olution of 2011.
We apply a part-of-speech taggerand a dependency parser on all sentences of thesethree articles (Toutanova et al., 2003; Marneffe etal., 2006).
We extracted all v-np pairs from eachsentence of these articles.
For each of the thesethree articles, we selected first 500 instances of v-np pairs.
Two annotators were asked to provide thelabels C and ?C to the instances of v-np pairs us-ing the annotation guidelines from Riaz and Girju(2010).
We have achieved a 0.64 kappa score forthe human inter-annotator agreement on a total of1,500 v-np instances.
This results in a total of1,365 instances of v-np pairs with 11.86% C pairs.In this section, we present performance of thefollowing models (see Table 3):1.
Baseline: NB and MaxEnt (McCallum, 2002)supervised classifiers using only the shallowlinguistic features (see section 3.1).2.
Basic noun and verb semantics: ILP withthe addition of semantic classes of nounswithout metonymy (denoted by +N!M) andthe addition of semantic classes of verbswhere ?Cev={(R)eporting events} (denoted by+N!M+V{R}).
These models represent thework proposed in Riaz and Girju (2014) (sec-tion 3).3.
Noun semantics with metonymies: ILPwith the addition of noun semantics involv-ing metonymies resolved via verb frames (de-noted by +NM1), metonymies resolved via verbframes {v, gr} where gr ?
GR = {csubj, csub-jpass, nsubj, nsubjpass, xsubj, dobj, iobj, pobj,agent} a set of core dependency relations ofsubjects and objects (denoted by +NM1GR) andmetonymies resolved via both verb frames andprepositions (denoted by +NM1GR+M2).4.
Verb frames and data-driven verb seman-tics: ILP with the addition of information aboutverb frames (denoted by +NM+VF where M =M1GR+M2), data-driven verb semantics i.e.,?Cev={(A)spectual, (R)eporting, (I) (S)tateevents} (denoted by +NM+V{A,R,IS}) and bothverb frames and data-driven verb semantics(denoted by +NM+VF+V{A,R,IS})167S B +N!M+N!M+V{R}+NM1+NM1GR+NM1GR+M2+NM+VF +NM+V{A,R,IS}+NM+VF +V{A,R,IS}A 28.86 71.86 73.40 71.35 71.42 71.64 72.96 75.16 76.19P 13.52 26.18 27.21 26.29 26.34 27.54 28.39 29.93 30.82R 92.59 75.30 74.07 78.39 78.39 85.18 83.95 81.48 80.86F 23.60 38.85 39.80 39.37 39.44 41.62 42.43 43.78 44.63A 61.46 80.73 81.17 80.65 80.73 81.02 81.39 81.75 82.05P 19.46 32.02 32.72 32.41 32.52 34.09 34.66 35.25 35.64R 71.60 55.55 55.55 58.02 58.24 64.19 64.19 64.19 63.58F 30.60 40.63 41.18 41.59 41.68 44.53 45.02 45.51 45.67Table 3: Performance of (B)aseline, +N!M, +N!M+V{R}, +NM1, +NM1GR, +NM1GR+M2, +NM+VF,+NM+V{A,R,IS}and +NM+VF+V{A,R,IS}(see text for details) in terms of (S)cores of (A)ccuracy,(P)recision, (R)ecall, (F)-score.
The row 1 (2) of this table presents results over NB (MaxEnt) base-line supervised classifier, respectively.Table 3 shows that MaxEnt gives a very high ac-curacy and F-score as compared with NB.
Model+N!M+V{R}with basic noun and verb seman-tics introduced in section 3.2 results in more than10% improvement in F-score over NB and Max-Ent classifiers relying only on shallow linguis-tic features.
Model +NM+VF+V{A,R,IS}with en-riched verb and noun semantics brings more than4% improvement in F-score over +N!M+V{R}with MaxEnt as baseline.
We perform statis-tical significance test using bootstrap samplingmethod given in Berg-Kirkpatrick et al.
(2012)(see Berg-Kirkpatrick et al.
(2012) for the de-tails).
+NM+VF+V{A,R,IS}brings significant im-provement in F-score over +N!M+V{R}with p-value 0.0.Though +N!Mgives significantly better F-scoreover baseline, it drops recall by more than 16%.Metonymy resolution helps perform quite bet-ter by recovering more than 8% recall with+NM1GR+M2over +N!M.
+NM1GR+M2also re-sults in 3.9% improvement in F-score over +N!Mwith MaxEnt as baseline model (significant im-provement with p-value 0.0).
Metonymies re-solved via verb frames with all and core grammat-ical relations (i.e., set GR) recover more than 2%recall and slightly improve F-score.Model with the addition of information of verbframes (i.e.,+NM+VF) brings 0.49% improve-ment in F-score over +NM1GR+M2using Max-Ent as baseline model (significant improvementwith p-value 0.027).
Model with the addition ofdata-driven verb semantics (i.e., +NM+V{A,R,IS})results in 0.98% improvement in F-score over+NM1GR+M2using MaxEnt as baseline model(significant improvement with p-value 0.0021).Overall the model +NM+VF+V{A,R, IS} yieldsmore than 16% (20%) F-score (accuracy) over thebaseline models build via NB and MaxEnt.5.1 Error AnalysisWe performed error analysis for the model+NM+VF+V{A,R,IS}by randomly selecting 50False Positives (FP) and 50 False Negatives (FN).For 32% FP instances information about verbframes is not available in the knowledge base ofverb frames.
To avoid this problem researchersshould exploit some abstractions e.g., {semanticsense of v, gr} frames.
Our model fails to iden-tify the class ?Cnpfor noun phrases of 29% FPinstances due to the lack of enough training datafor the semantic classes of nouns.
In 21% FPinstances v and np are not even relevant to eachother.
Our model first needs to determine rele-vance between v and np before identifying causal-ity.
Remaining 18% instances have v and np intemporal only sense, comparison relation or bothrepresent parts of same event.
There is need to ex-tract more knowledge sources to better distinguishcausality from any other type of relation.77% FN instances are classified as non-causaldue to the lack of enough v-np training data andrequire more sources of knowledge e.g., back-ground knowledge.
On remaining 23% FN in-stances our model fails to identify Cnpclass dueto the lack of enough training data for the seman-tic classes of nouns.6 ConclusionThis work has revealed that enriched semanticsof nouns and verbs help gain significant improve-ment in performance over a baseline relying onlyon shallow linguistic features.
Through empiri-cal evaluation and error analysis of our model wehave highlighted strengths and weaknesses of ourmodel for the current task.
Our work has provideda novel direction to exploit semantics of partici-pants of causal relations to solve the challenge ofidentifying causality.168ReferencesCollin F. Baker, Charles J. Fillmore and John B. Lowe.1998.
The Berkeley FrameNet project.
In proceed-ings of COLING-ACL.
Montreal, Canada.Taylor Berg-Kirkpatrick, David Burkett, and DanKlein.
2012.
An empirical investigation of statisti-cal significance in NLP.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL)..Steven Bethard and James H. Martin.
2006.
Identifica-tion of Event Mentions and their Semantic Class.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Steven Bethard and James H. Martin.
2008.
LearningSemantic Links from a Corpus of Parallel Temporaland Causal Relations.
In proceedings of ACL-08:HLT.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained SemanticVerb Relations.
In proceedings of Conference onEmpirical Methods in Natural Language Processing(EMNLP-04).
Barcelona, Spain.Quang X.
Do, Yee S. Chen and Dan Roth.
2011.
Min-imally Supervised Event Causality Identication.
Inproceedings of EMNLP-2011.Jenny R. Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
In Proceedings of the Association forComputational Linguistics (ACL).Roxana Girju and Dan Moldovan.
2002.
Mining An-swers for Causation Questions.
In American Asso-ciations of Artificial Intelligence (AAAI), 2002 Sym-posium.Roxana Girju.
2003.
Automatic detection of causalrelations for Question Answering.
Association forComputational Linguistics ACL, Workshop on Mul-tilingual Summarization and Question AnsweringMachine Learning and Beyond 2003.Katja Markert and Malvina Nissim 2009.
Data andmodels for metonymy resolution.
Language Re-sources and Evaluation Volume 43 Issue 2, Pages123?138.Marie-Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.In Proceedings of the International Conference onLanguage Resources and Evaluation (LREC).Andrew K. McCallum.
2002.
MALLET:A Machine Learning for Language Toolkit.http://mallet.cs.umass.edu.Emily Pitler, Annie Louis and Ani Nenkova.
2009.Automatic Sense Prediction for Implicit DiscourseRelations in Text.
In proceedings of ACL-IJCNLP,2009.Emily Pitler and Ani Nenkova.
2009.
Using Syntaxto Disambiguate Explicit Discourse Connectives inText.
In proceedings of ACL-IJCNLP, 2009.James Pustejovsky, Patrick Hanks, Roser Saur, AndrewSee, Robert Gaizauskas, Andrea Setzer, DragomirRadev, Beth Sundheim, David Day, Lisa Ferro andMarcia Lazo.
2003.
The TIMEBANK Corpus.
InProceedings of Corpus Linguistics.Kira Radinsky and Eric Horvitz.
2013.
Mining theWeb to Predict Future Events.
In proceedings ofsixth ACM international conference on Web searchand data mining, WSDM ?13.Mehwish Riaz and Roxana Girju.
2010.
Another Lookat Causality: Discovering Scenario-Specific Contin-gency Relationships with No Supervision.
In pro-ceedings of the IEEE 4th International Conferenceon Semantic Computing (ICSC).Mehwish Riaz and Roxana Girju 2013.
Towarda Better Understanding of Causality between Ver-bal Events: Extraction and Analysis of the CausalPower of Verb-Verb Associations.
Proceedings ofthe annual SIGdial Meeting on Discourse and Dia-logue (SIGDIAL).Mehwish Riaz and Roxana Girju 2014.
RecognizingCausality in Verb-Noun Pairs via Noun and Verb Se-mantics.
Proceedings of the Workshop on Computa-tional Approaches to Causality in Language EACL,2014.Dan Roth and Wen-tau Yih 2004.
A Linear Program-ming Formulation for Global Inference in NaturalLanguage Tasks.
In Proceedings of the Annual Con-ference on Computational Natural Language Learn-ing (CoNLL).Caroline Sporleder and Alex Lascarides.
2008.
Usingautomatically labelled examples to classify rhetor-ical relations: An assessment.
Journal of NaturalLanguage Engineering Volume 14 Issue 3, July 2008Pages 369?416.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In Proceedings of Human Language Technol-ogy and North American Chapter of the Associationfor Computational Linguistics (HLT-NAACL).169Appendix A.
Supervised ClassifierIn this appendix, we provide a set of linguistic fea-tures taken from Riaz and Girju (2014) to iden-tify causality in v-np pairs employing a supervisedclassifier (see section 3.1 for the details).?
Lexical Features: verb, lemma of verb,noun phrase, lemmas of all words ofnoun phrase, head noun of noun phrase,lemmas of all words between verb andnoun phrase.?
Syntatic Features: part-of-speech tags of verband head noun of noun phrase.?
Semantic Features: We adopted this fea-ture from Girju (2003) to capture semanticsof nouns.
The 9 noun hierarchies of Word-Net i.e., entity, psychological feature, abstrac-tion, state, event, act, group, possession, phe-nomenon are used as this feature.
Each of thesehierarchies is set to 1 if any sense of head nounof noun phrase lies in that hierarchy, otherwiseset to 0.?
Structural Features: This feature is appliedby considering both subject (i.e., sub in np)and object (i.e., obj in np) of verb (v).
For ex-ample, for the pair v-np the variable sub in npis set to 1 if the subject ?
np, set to 0 if thesubject 6?
np and set to -1 if the subject is notavailable in an instance.
The subject and objectof a verb are its core arguments and may some-time be part of an event represented by a verb.Therefore, these arguments may have high ten-dency to encode non-causation with their verb.?
Pairs: The following pairs (verb, head nounof noun phrase), (subjectverb, head noun ofnoun phrase) and (objectverb, head noun ofnoun phrase) are used to capture relations.Appendix B. Noun SemanticsIn this appendix, some examples of the frame ele-ments of FrameNet and the WordNet senses be-longing to the classes Cnpand ?Cnpare givenin Tables 4 and 5 (see section 3.2 for the de-tails).
We employ training instances acquired us-ing the FrameNet annotations and WordNet sensesfor building a supervised classifier for the classesCnpand ?Cnp.
Following is the list of features weuse for this supervised classifier:?
Lexical Features: All words of noun phrase,lemmas of all words of noun phrase, head nounof noun phrase, first two (three) (four) lettersSC FrameNet LabelscnpEvent, Goal, Purpose, Cause, Internal cause, Externalcause, Result, Means, Reason, Phenomena, Coordi-nated event, Action, Activity, Circumstances, Desiredgoal, Explanation?cnpArtist, Performer, Duration, Time, Place, Distributor,Area, Path, Direction, Sub-region Frequency, Bodypart, Area, Degree, Angle, Fixed location, Path shape,Addressee, IntervalTable 4: Some examples of the frame elements ofFrameNet to which we assign the semantic classesCnpand ?Cnp.SC WordNet Sensescnp{act, deed, human action, human activity},{phenomenon}, {state}, {psychological feature},{event}, {causal agent, cause, causal agency}?cnp{time period, period of time, period}, {measure,quantity, amount}, {group, grouping}, {organization,organisation}, {time unit, unit of time}, {clock time,time}Table 5: This table shows our selected Word-Net senses of nouns belonging to classes Cnpand?Cnp.
For example, using the information pro-vided in this table we assume that any noun con-cept whose all senses of WordNet lie in the seman-tic hierarchy of the sense {time period, period oftime, period} is of class ?Cnp.
We use EnglishGigaword corpus to collect instances of noun (ornoun phrases) and label them with Cnpand ?Cnpaccording to their senses in WordNet.of head noun of noun phrase, last two, (three)(four) letters of head noun of noun phrase.?
Word Class Features: part-of-speech tags ofall words of noun phrase and head noun ofnoun phrase.?
Semantic Features: Frequent sense of headnoun of noun phrase.170
