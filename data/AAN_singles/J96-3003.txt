Efficient MultilingualPhoneme-to-Grapheme ConversionBased on HMMPanagiot is  A. Rentzepopou los*University of PatrasGeorge  K. Kokkinakis*University of PatrasGrapheme-to-phoneme conversion (GTPC) has been achieved in most European languagesbydictionary look-up or using rules.
The application of these methods, however, in the reverse pro-cess, (i.e., in phoneme-to-grapheme conversion \[PTGC\]) creates erious problems, especially ininflectionally rich languages.
In this paper the PTGC problem is approached from a completelydifferent point of view.
Instead of rules or a dictionary, the statistics of language connecting pro-nunciation to spelling are exploited.
The novelty lies in modeling the natural anguage intrawordfeatures using the theory of hidden Markov models (HMM) and performing the conversion usingthe Viterbi algorithm.
The PTGC system has been established and tested on various multilingualcorpora.
Initially, the first-order HMM and the common Viterbi algorithm were used to obtain asingle transcription for each word.
Afterwards, the second-order HMM and the N-best algorithmadapted to PTGC were implemented to provide one or more transcriptions for each word input(homophones).
This system gave an average score of more than 99% correctly transcribed words(overall success in the first four candidates)for most of the seven languages it was tested on(Dutch, English, French, German, Greek, Italian, and Spanish).
The system can be adapted toalmost any language with little effort and can be implemented in hardware to serve in real-timespeech recognition systems.1.
IntroductionPhoneme-based speech recognition systems incorporate a phoneme-to-grapheme con-version (PTGC) module to produce orthographically correct output.
Many approacheshave been used, most of which compare the phonemic strings to a (usually application-specific) dictionary containing both the phonemic and the graphemic form of everyword the system can handle (Laface, Micca, and Pieraccini 1987; Levinson et al 1989,etc.).
Considering the effort and cost required to create such a dictionary, this is a seri-ous limitation, especially for inflectionally rich languages such as Greek and German.Another very important issue when searching for words in a dictionary is the numberof candidates resulting from each phonemic input.
Depending on the language and theerrors of the recognizer, this number may be very large, rendering the disambiguationof the words by a subsequent language model a time-consuming and unreliable task.The domain of application is another factor that strongly influences conversionperformance; a general dictionary can omit the specialized words of specific domains(e.g., legal, engineering, or medical terminology) and vice versa.
Finally, applicationsthat must handle a large number of proper names (e.g., directory service applications)generally cannot include all the possible names.
The only remedy in such situations?
Wire Communication Laboratory, University ofPatras, Patras, GR 26500 Greece(~) 1996 Association for Computational LinguisticsComputational Linguistics Volume 22, Number 3would be to increase the size of the reference dictionary, so that every possible inputword is included.
A final consideration is the type of errors a dictionary-based PTGCsystem introduces when it encounters a word that is not contained in the dictionary:the system will produce the closest existing word (in its dictionary) as the best can-didate, which may give a completely incomprehensible (if not wrong) meaning to theinput phrase.Another approach to phoneme-to-grapheme conversion is the use of linguisticand/or heuristic rules (Kerkhoff and Wester 1987).
This method works on a phonemeor syllable basis and can give adequate results in languages where the spelling is verysimilar to the pronunciation (such as Italian).
Nevertheless, languages with diphthongsor double letters cannot benefit from this method, since it creates long lists of homo-phonic candidates that are all correct (in the sense that they are pronounced as theinput word) but that do not exist in the language.
In Greek, for example, where thephoneme/ i / i s  the sound of five different graphemes (~, z, v, ~, o~) and the phoneme/1/can come from ~ and ;~;~, the phonemic fo rm/m' i la /wou ld  produce a list contain-ing the following 10 transcriptions: #i)~c~, #~&c~, #(;Ae~, l~i&o~, #oi&~, #i)~&c~, #~&&c~,#~)~&a, #ci&o~, and #oi&&e~ all having the same pronunciation.
From this list, only tworepresent existing orthographically correct words; "#i),c~" 'speak!'
and "#~)~c~" 'apples.
'Previous work has shown that an average of 30 graphernic andidates i produced bythis transcription for every input phonemic word (Rentzepopoulos 1988).To overcome the disadvantages of the above mentioned methods, anovel statisticalapproach to the problem of PTGC, which is based on hidden Markov models (HMM),has been investigated and is presented in this paper.
Although statistical approacheshave already been widely applied in several fields of natural anguage processing, theyhave not been considered for PTGC.
The proposed method is language independent,does not use a dictionary, and can be applied with only minimal inguistic knowledge,thus reducing the cost of system development.
Initially, the first-order HMM and thecommon Viterbi algorithm were used to provide a simple transcription for each inputword.
In its current version, the method is based on second-order HMM and on amodified Viterbi algorithm, which can provide more than one graphemic output foreach phonemic input, in descending order of probability.
The multiple outputs makeit possible to apply a language model in sentence level for disambiguation ata subse-quent stage.
This version of the algorithm raised the number of correctly transcribedphonemes to 97%-100% for most of the languages the system was tested on.
The pro-posed system assumes that the word boundaries are known; that is, it is a subsequentstage in an isolated-word speech recognition system.
The PTGC method can work asa stand-alone module or in co-operation with a look-up module with a small to mod-erate size dictionary containing the most common words of the language.
In the lattercase, the look-up module employs a distance threshold: when the difference betweenthe input and the words in the dictionary is greater than this threshold, control ispassed to the HMM system, which converts the input phoneme string to graphemes.The basic theory, the pilot implementation, and the proposed final system arepresented in Section 2.
The evaluation procedure and the error-measure methodologyare described in Section 3.
In Section 4, the experimental results of the system arepresented and the nature of the errors is discussed.
The multilingual aspects of thealgorithm and experimental results for seven languages are also given in this section.Finally, some conclusions are drawn about the system and topics for further researchand hardware implementation are discussed in Section 5.352Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMM2.
Description of the SystemBefore the presentation of the proposed system, a brief overview of the theory usedand the issues addressed in its application are given.
These include the basic hiddenMarkov model theory, the Viterbi algorithm, the N-best algorithm and the solutionsused to make the PTGC system fast and efficient, adequate for real-time applications.2.1 The First Order Hidden Markov ModelAn HMM can model any real-world process that changes tates in time, providedthat the state changes are more or less time independent (Hannaford and Lee 1990;Rabiner 1989).
An HMM is used to describe statistical phenomena that can be consid-ered sequences of hidden (i.e., not directly observable) states that produce observablesymbols (Lee 1989).
These phenomena are called hidden Markov processes.
A hiddenMarkov process is described by a model ,~ that consists of three matrices A, B, and ~-.Matrix A contains the transition probabilities of the hidden states, matrix B containsthe probability of occurrence of an observation symbol given the hidden state, andvector 7r contains the initial probabilities of the hidden state.
In mathematical terms:A = {oqy: i=  1 .
.
.
N , j  = 1...  N), ~ij = P(qt = Sj \[qt-1 ~-  Si) (1)B = {fly(m) : j=  l .
.
.N ,m = l .
.
.M},  fly(m) = P(Ot = vm \] qt = Sj) (2)~r = Or/: i=  1...  N}, 7ri = P(ql = Si) (3)where N is the number of possible hidden states and M is the number of all theobservable vents.
Obviously the dimension of matrix A is N x N, that of matrix B isN x M, and ~ is a vector of N elements.In equations (1)-(3), qt is the hidden state of the system at time t, Si is the /thpossible hidden state of the system, Ot is the observation symbol at time t, and Vm isthe m th possible observable symbol.For the application of HMM theory to PTGC, the correspondence of the naturallanguage intraword features to an HMM can be found on the following basis:The natural anguage pronunciation can be considered as the output(observation) ofa system that uses as input (hidden state sequence) thespelling of the language (Rentzepopoulos, Tsopanoglou, and Kokki-nakis 1991).In this formulation, the sequence of phonemes produced by the system can beseen as the observation-symbol sequence of an HMM that uses the graphemic formsas a hidden-state s quence.
With this statement, the PTGC problem can be restated asfollows:Given the observation-symbol sequence O(t) (phonemes) and the HMMA, find the hidden-state s quence Q(t) (graphemes) that maximizes theprobability P(O I Q, ,~).A formal technique for finding the single best state sequence is based on dynamicprogramming and is the well-known Viterbi algorithm (Forney 1973; Viterbi 1967).In a word-level implementation, the algorithm ust find the hidden-state s quence(i.e., word in its orthographic form) with the best score, given the model & and theobservation sequence O (i.e., word in its phonemic form).
This algorithm proceeds353Computational Linguistics Volume 22, Number 3recursively from the beginning to the end of the word calculating for any time (in thecase of PTGC, time is the position of a phoneme/grapheme in the word) the score ofthe best path in all possible hidden-state sequences that end at the current state.The model's parameters can be estimated using the definition formulas, since boththe hidden-state and the observation-symbol sequences are known during the trainingphase of the conversion system.
Thus there is no need of a special estimation procedurelike the Baum-Welsh algorithm (Rabiner 1989), which is used when the hidden-statesequence is not known.
In general:nS(qt-1 ---- Si, qt = Sj) (4)aij = n(qt = S/)n'(qt = SpOt  = vm)bj(m) = n(qt = Sj) (5)n'(ql  = Si) (6)~i  - -  n(ql)where n(x)  is the number of occurrences of x in the training corpus and n'(x) is anestimation of the number of occurrences of x in the application corpus.
The size ofthe training corpus and the sparseness of the resulting matrices can lead to differentapproaches in the definition of the estimation function n' (x).
If a reasonably large textis available for training, then nS(x) ~- n(x) .
On the other hand, if the training data areinsufficient (something that would result in a very sparse transition matrix) then asmoothing technique should be used for the estimation function n' (x) (Katz 1987; Neyand Essen 1991).2.2 Pilot SystemTo implement he above algorithm in PTGC, some decisions had to be made aboutthe states, observation symbols, and transition probabilities.
These decisions are listedbelow.a.b.Every hidden state should produce one observation symbol.
To achievethis, all the possible graphemic transcriptions of phonemes were codedas separate graphemic symbols (e.g., 7r and 7rTr are two differentgraphemic symbols even though they are both pronounced/p/ ) .The transition probability matrix (A) should be biased to contain at leastone occurrence for every transition and no zero elements.Consider first (a).
According to the physical meaning given to the hidden statesand the observation symbols of the HMM used, there cannot be hidden states (graph-emes) that do not produce an observable symbol (phoneme).
This is only partiallycorrect for natural languages including mute letters and diphthongs.
To overcomethis problem, the hidden-state alphabet and the observation-symbol alphabet shouldcontain not only single characters (single graphemes or phonemes respectively) butalso clusters.
This way, it is guaranteed that there will be no case where a sequence ofgraphemes produces a sequence of phonemes of a different length.
The rules for thesegmentation of a phoneme string to a sequence of symbols conforming to the abovecondition are manually defined off-line according to the procedure presented belowin an informal algorithmic language (Figure 1).354Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMMLet  G = {gl, g2, ... gM} be  the  set  o f  phonemes  andP = {Pl, P2, ... PN} the  set  o f  g raphemes  o f  the  languagerepeatPt = Zfor i= l  to  MPt = Pt  phoneme_t ranscr ip t ions_o fend forP = PU PtGt = Zfor i= l  to  NGt = Gt U grapheme_t ranscr ip t ions_o fend forG = G u Gtunt i l  (Pt == Z and Gt == Z)(gi)(Pi)Figure 1Segmentation rule development algorithm.The meaning of this algorithm is the following: If a pair of phonemes is writtenas either a single grapheme or a pair of graphemes, then this pair is considered asingle state.
The same holds for the reverse procedure when a pair of graphemes ipronounced as either a single phoneme or a pair of phonemes.
For example (in Greek):grapheme ~ is pronounced/ks/e.g.
,  ?4&-ksfdi 'vinegar'grapheme ~ is pronounced/k/e .g .
,  ~aA6-kald 'good'grapheme rTis pronounced/s /e .g .
,  c~c~O~-saff 'lucid'graphemes ~cr are pronounced/ks/e.g.
,  ~?~-c~cr~-4kstasi 'ecstasy'In this example the pair of phonemes/ks / i s  considered a single phonemic sym-bol.
Accordingly, the pair "~cr" is also considered a single graphemic state since itis pronounced as /ks/ .
As can be seen, in order to disambiguate the case of ~cr thephonemic symbo l /ks /and  the graphemic state ~cr must be introduced.This algorithm is the only language-specific part of the PTGC system and its for-mulation requires only familiarity with the spelling of the language and not sophisti-cated linguistic knowledge.
The rules are incorporated in the PTGC system using anautomated procedure as a separate input function that parses the input strings intostates.Now consider (b), concerning the transition probability matrix.
Matrix A is es-tablished according to formula (4) through training in appropriate corpora using asn'(x) = max(n(x), 1).
The bias described in (b) is necessary so that the algorithm doesnot discard a new transition but instead assigns a bad score to it (Rabiner 1989).
Thebias is one occurrence for each transition that has never occurred in the training corpus355Computational Linguistics Volume 22, Number 3and the model is normalized so that it fulfills the statistical constraints, i.e.
:o~ij >_ O, ~ o~ij = 1 (7)iThis estimation is allowed since the training corpora are reasonably large and thebias of one occurrence per transition has no significant effect on the validity of theactually nonzero matrix elements.Initially, a system based on a first-order HMM was implemented, and the resultsof its evaluation, detailed in Section 3, were promising.
For Greek, this system gave anaverage score of 78% correctly transcribed words, while at the phoneme level the scorereached 95.5% (Rentzepopoulos and Kokkinakis 1991).
Similar rates were achievedin four other languages (English, French, German, and Italian) (Rentzepopoulos andKokkinakis 1992).The model implemented as above showed some disadvantages:?
It did not have enough detail.?
It could not produce more than one solution (homophones).Therefore, a higher-order HMM and a multiple-output conversion algorithm wereemployed in order to overcome these disadvantages and achieve better esults.2.3 Second Order  HMMSince the results of the first order HMM system were encouraging, we decided todevelop an improved version of the system.
Two areas were selected for possibleadvancement: first, to make the system contain more detail in the modeling of thelanguage, and second, to use a system that could produce more than one outputsolution for each phonemic input (homophones).
This would offer a choice betweenalternatives, making it possible to find the best solution at a following stage.The first improvement was accomplished using a second-order HMM.
This is amodel that contains conditional probabilities of the form:o~ijk = e(qt  = sk \[ qt_l  = sj, qt_2 = si) (8)i.e., the probability of occurrence of state Sk when the two previous tates are Si and Sjat t - 2 and t - 1, respectively.
The complete model needs a new matrix of conditionalprobabilities that contains the probability of state-pairs in time t = {1, 2}:p= {pij:i= l .
.
.N , j=  l .
.
.N}, pij=P(ql=Silq2=Sj) (9)So the complete model 3~ consists of {A, B, 7r, p}.
The second-order HMM can betranslated into a first-order HMM with an extended state space, in which state pairsare used as single states.To use the above model, a new version of the Viterbi algorithm should be em-ployed, one which can recursively calculate the intermediate values of the probabilitymeasure d using the second-order HMM.
A second-order HMM has been introducedbefore (Kriouile, Mari, and Haton 1990) for other problems in the field of patternanalysis and speech recognition.
In He (1988) the Viterbi algorithm is presented fora second-order HMM using the transformation of the model to a first-order with ex-tended state space.
The algorithm that was developed here uses the features of theViterbi algorithm in a slightly different way, tailored to the needs of the PTGC problemas described in Section 2.5.356Rentzepopoulos and Kokkinakis Phoneme-to-Grapherne Using HMM2.4 Multiple-Output (N-best) Conversion AlgorithmThe Viterbi algorithm produces the overall best state sequence by maximizing the over-all probability P(O I Q).
If the N best state sequences are needed, then the algorithmmust be modified to keep the N best state' sequences from ql through qt.
Schwartz andAustin (1991) and Schwartz and Chow (1990) present he N-best algorithm in detail.The following consideration is the basis of the multiple-output conversion algorithm:Let QE = {Ql(t), Q2(t),..., QE(t)} be the globally E best hidden-statesequences that end at state qt = Si at a given time t. By "best" wemean, as usual, those sequences having the highest probability.
If oneof the globally E best hidden-state s quences that starts at t = 1 andends at t = T passes from state Si at time t then it must have one ofthe members of Qt E as part of the path from time 1 to t.To prove this, only the following assumptions are required: Qx(t) is a state se-quence that ends at time t at state Si; Qx(t) ?~ QE; and Qx(t) is part of one of theE globally best hidden-state sequences.
Clearly Qx(t) ~ QE ~=~ P(Qx(t)) < P(Qi(t)),Vi E 1...  E. The probability of the complete state sequence Qm(T) (1 < m _< E) whichcontains Qx(t) would be:P(Qm(T))TT=2t T"~ 71"ql/~ql (O l ) "  H OLq'-\]qT~'(OT)" 1-'I OZq'r -- \]q'r ~q" (OT)r=2 r=t+lT= P(QI(t)).
I-I {2qr--lqrflq'r(OT)~-=t+l(lO)Since qt = Si, the underlined part of (10) is independent from P(Qx(t)).
ButP(Qx(t)) < P(Qi(t)), Vi E 1...E. This means that there are at least E more pathsleading to state Si at time t that are more probable than Qm (T), which is a path amongthe first E most probable paths; a contradictory statement.Summarizing, we have shown that we only need to keep the locally (at any timet in 1...  T) E best paths as we go along the possible state sequences for every pos-sible state.
When we arrive at the end, we only need to keep the E globally highestprobabilities and trace back the states that resulted in these.2.5 Final systemThe final version of the conversion system uses the previously mentioned methods,i.e., the second-order HMM and the N-best version of the Viterbi algorithm along witha transformation that is necessary to speed up the execution of the conversion.The algorithm as described previously has many disadvantages fora PTGC systemfrom the implementation point of view.
The values of the parameters of the model arein the range of 100.
This implies that, considering storage, we need to keep in memory100 x 100 x 100 double precision floating point numbers for matrix A along with theother data of the model and the algorithm.
To be exact we need for:A: N 3B: NxMdouble precision floating point numbersdouble precision floating point numbers357Computational Linguistics Volume 22, Number 37r: Np: N 26: N2xExT~:N2xExTdouble precision floating point numbersdouble precision floating point numbersdouble precision floating point numbersshort integers(for the meaning of 6 and ~b see the algorithm presented in theAppendix).In the above, if we substitute the values of the Greek PTGC system (N=140, M=70,T=30, E=4) for the symbols, we can see that we need no less than 45,708,320 bytes forstoring these data.
Aside from the problem of storage, the computer has to executethe inner part of the second-order-multiple-output conversion algorithm N 3 x E x T/2times (the average length of a word is about T/2), i.e., 219,520,000 times per word.As is clear from the presentation of the algorithm, this part contains a rather time-consuming sorting procedure plus a floating point multiplication.
It is obvious thatthis is an unacceptable time delay for real-time applications.To decrease the algorithm execution time and storage needs we introduced thefollowing improvements:a.b.C.Taking advantage of the relative sparseness of matrix B, we firstdetermine if Bj(t) is nonzero and only then does the algorithm proceedto the rest of the processing.
This has decreased the execution time of theconversion by nearly 100 times.We do the same for matrix A.
This means that if the indices (i,j, k)indicate a zero transition probability then the algorithm proceedswithout trying to calculate the overall probability, thus eliminating afloating point multiplication.Since at every time point the intermediate variable d(t) is calculated onlyfrom d(t - 1) we keep only two copies of dij, one for t and one for t - 1.Finally, the fact that only multiplications are involved in the processing of theconversion algorithm led us to transform the algorithm to use only additions.
In theAppendix, the algorithm we implemented is presented.3.
TestingThe proposed system has been tested and evaluated in two separate procedures: thetraining process and the conversion process.
The training process has been performedusing dictionaries that contain both the graphemic form and the phonemic form ofwords along with the frequency of occurrence of the words in the corpora that wereused for the creation of the dictionary.
The output of the training process was a filecontaining the model parameters (transition matrix, initial state probabilities, etc.).
Theconversion process has been performed using various portions of texts not included inthe training texts, for which the phonemic form and the graphemic form were known.The phonemic form was converted into orthographic (graphemic) form using the al-gorithm and then compared with the original.
To thoroughly test the performance ofthe system, a series of experiments was conducted.
These experiments were designedso that the following set of factors could be examined:358Rentzep?p?ul?s and Kokkinakis Phoneme-to-Grapheme Using HMM?
training and testing material domain: general, specialized, namedirectory?
type of the phonetic form: correct phonemic strings, corrupted speech?
language: Dutch, English, French, German, Greek, Italian, Spanish?
conversion algorithrn version: first-/second-order HMMFor each experiment the following figures were measured:?
number of words converted correctly?
number of phonemes converted correctly?
rank of the correct word (for the multiple-output versions of theconversion algorithm)?
time response?
memory requirementsDetails about the factors presented above and the quantities that were measuredare given below.3.1 Evaluation factorsFor all languages tested, the models were created using full-form dictionaries et upduring the EEC ESPRIT project 291/860 "Linguistic Analysis of the European Lan-guages" (ESPRIT 1987) from corpora of about 300,000 words.
These dictionaries coverthree domains: office environment, newspapers, and law.
The input to the conversionprocess was separate 10,000 word texts not included in the training dictionaries.
Thetesting material was taken from the above domains.
Furthermore, for Greek, two ad-ditional dictionaries of proper names provided by the ONOMASTICA (LRE 61004)project were used for training and testing the algorithm in a name directory environ-ment.
This was done to get a more accurate indication of the system's performance inapplications where a complete dictionary can never be available.A second set of training and testing material was created from the above usinga phoneme confusion matrix that simulated the output of a speech recognizer.
Theconfusion matrix relates the input (correct) with the output (corrupted) phonemesemploying probabilities of the form mq = P(Oout  = Pj I Oin  ~- Pi).
The texts used for thetraining and testing phase were corrupted according to these probabilities.
Differentconfusion matrices were applied to show the degradation of the performance as afunction of the input phonemic orruption.
In Section 4.2, the two sets of results arepresented and compared.Finally, one more experiment per language was performed using a first-orderHMM, so that the ambiguity in each of the languages tested could be revealed and acomparison of the performance of the two HMM models could be made.3.2 Performance CriteriaFor every experiment carried out, the success rate of the conversion algorithm wasmeasured for each output candidate (the system was asked to produce a maximumof four candidates if available) in two levels:a.
Errors at word (state sequence) level: The system counts one error forevery phonemic word not converted correctly to its graphemic form.359Computational Linguistics Volume 22, Number 3b.
Errors at symbol (state) level: The system counts one error for everygraphemic symbol (unit grapheme) that does not match with thecorresponding symbol of the correct graphemic transcription.As an example, if instead of the correct ranscription ofthe Hellenic word/trap~zi/'table' to the graphemic form 9-pc~lrg~, the system produces 9-pc~Tr~/, this counts forone error per one word (i.e., 100% error) and for one error per seven symbols (or14.3% error).
This distinction was made because the first error type (word error) ismore important from the user's point of view, while the second type (symbol error)is a more objective measure of the performance of the system.In addition to these rror types, the average symbol errors per incorrect word werecounted.
This is a measure of the quality of the system output since it shows whetheran incorrect word is easily comprehensible or not.Another very important feature was also measured!
the position in the output listof the correct candidate.
The distribution of this variable is very important, so thatdecisions about the trade-off between speed and accuracy can be made.
Finally, foreach experiment, he amount of time and computer memory needed were counted,to get a measure of the applicability of the algorithm in real-time applications whenusing general- or special-purpose hardware.4.
Results4.1 Explanation of tables and chartsIn all tables and charts ome symbols have been used to designate the different param-eters of the experiments.
More precisely, Exp n (n = 1, 2, 3) designates the experimenttype as follows:Exp 1: uses a first-order HMM with correct phonemic representation fthe inputExp 2: is like Exp 1 but uses a second order HMMExp 3: is like Exp 2 but with corrupted phonemic representationsimulating the output of a speech recognizerThe letter combinations El, E2, and NE show the domain of the experiment: E1experiments use the office environment corpora for training and assessment, E2 thelaw corpora, NE the newspaper corpora.
For the name corpus (Table 8) N1 showsexperiments u ing a corpus of surnames and OD experiments u ing a corpus of streetnames.
It must be noted that in all experiments he testing material was not includedin the training of the model although it may belong to the same domain.In Tables 1 through 8 the model parameters for all the models created for theexperiments mentioned above are presented.
The columns show the density (i.e.,the number of nonzero elements) of the respective model parameters (initial hidden-state probability vector ~r, initial hidden-state pair probability vector p, observation-symbol probability matrix B, and hidden-state ransition probability matrix A).
Thevalues are percentages.
The matrix density is a way of measuring the saturation ofthe model, that is, whether the model is sufficiently objective or is too dependenton the nature of the training material.
One can see from these tables the impor-tant differences between the languages on which the experiments were performed.360Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMMTable I Table 2Model parameters for Dutch.
Model parameters for English.lr p B A rr p B AE1 53.21 4.29 2.38 0.52 E1 61.46 5.02 6.05 0.50E2 39.45 2.48 1.46 0.22 E2 61.46 5.75 7.08 0.58NE 52.29 4.88 2.28 0.58 NE 65.63 8.72 9.41 1.19Table 3 Table 4Model parameters for French.
Model parameters for German.rc p B A ~r p B AE1 57.79 4.43 3.11 0.21 E1 59.26 3.56 2.16 0.31E2 60.42 4.51 3.29 0.22 E2 62.96 4.00 2.34 0.37NE 64.02 5.02 3.36 0.32 NE 61.48 4.86 2.78 0.47Table 5 Table 6Model parameters for Greek.
Model parameters for Italian.rr p B A 7r p B AE1 48.89 2.51 1.56 0.25 E1 82.00 10.00 3.10 2.38E2 55.56 3.28 1.61 0.33 E2 86.00 13.88 3.10 3.32NE 60.74 5.09 1.80 0.52 NE 70.00 9.84 2.78 1.99Table 7 Table 8Model parameters for Spanish.
Model parameters for names.rc p B A rr p B AE1 64.286 5.329 1.83 0.661 N1 49.47 5.25 2.12 0.60E2 72.619 6.76 1.863 0.771 OD 64.21 10.15 2.87 1.78NE 65.476 6.08 1.775 0.706In Tables 9 to 17, a summary  of the conversion results is presented for the threesets of experiments carried out.
The columns have the following meaning:d:l(s):l(w):1-2, etc:Is/lw x 100% where lw is the size of a word in error (incharacters), Is is the number  of incorrect characters in the word,and Is/lw x 100% is the mean value estimated over all wrongwords.
This number  is a measure of the similarity of wrongwords with the corresponding correct words (percentage).
Asmall percentage indicates a high similarity.symbol conversion success rate for the first position (percentage).word conversion success rate for the first position (percentage).word conversion success rate accounting for all the referencedpositions (percentage).Figures 2 through 10 give an analytic overview of the results in each language.The legends of these figures have the form cc/n where cc is a two letter code for thecorpus domain (E1 /E2 /NE/N1/OD,  as described in the beginning of this section) andn is either 1 for a first-order model or 2 for a second-order model.
For example, thelegend E1/1 means that text of the domain E1 (office environment) was used with afirst-order HMM for the experiment.361Computational Linguistics Volume 22, Number 3Table 9Conversion results for Dutch.d l(s) 1 1-2 1-3 1-4Exp 1 21.82 93.12 68.49 84.67 89.26 91.92Exp 2 18.80 98.00 87.62 96.29 97.27 97.60Exp 3 26.32 78.40 76.23 86.66 90.46 92.72Table 10Conversion results for English.d l(s) 1 1-2 1-3 1--4Exp 1 19.69 95.51 62.95 75.56 81.84 83.51Exp 2 16.96 97.60 74.53 86.50 88.10 89.14Exp 3 23.74 78.08 64.84 77.85 81.93 84.68Table 11Conversion results for French.d l(s) 1 1-2 1-3 1-4Exp 1 16.99 96.02 64.23 79.24 82.25 83.70Exp 2 16.04 97.42 76.36 85.59 87.24 88.31Exp 3 22.46 77.94 66.43 77.03 81.13 83.90Table 12Conversion results for German.d l(s) 1 1-2 1-3 1-4Exp 1 17.90 95.01 69.94 83.95 90.93 92.79Exp 2 15.42 97.34 82.81 96.11 97.89 99.05Exp 3 21.58 77.87 72.04 86.50 91.04 94.09Table 13Conversion results for German with no capital etters.d l(s) 1 1-2 1-3 1-4Exp 1 17.40 95.27 72.77 86.93 91.50 93.27Exp 2 15.50 99.20 95.00 99.20 99.70 99.90Exp 3 21.70 79.36 82.65 89.28 92.72 94.91Table 14Conversion results for Greek.d l(s) 1 1-2 1-3 1-4Exp 1 15.17 96.45 72.17 89.03 92.41 94.05Exp 2 14.32 97.70 85.80 96.17 98.02 99.23Exp 3 20.05 78.16 74.65 86.55 91.16 94.27362Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMMTable 15Conversion results for Italian.d l(s) 1 1-2 1-3 1-4Exp 1 13.74 98.94 92.30 99.54 99.91 99.95Exp 2 13.67 99.83 98.30 99.95 99.99 100.00Exp 3 19.14 79.86 85.52 89.96 92.99 95.00Table 16Conversion results for Spanish.d l(s) 1 1-2 1-3 1--4Exp 1 15.49 97.89 86.39 96.84 98.66 99.31Exp 2 14.37 98.98 93.03 96.59 99.90 99.99Exp 3 20.11 79.18 80.94 86.93 92.91 94.99Table 17Conversion results for names.d l(s) 1 1-2 1-3 1-4Exp 1 12.30 96.24 69.36 83.16 89.39 92.33Exp 2 11.88 97.83 81.44 93.60 96.86 98.28Exp 3 16.63 78.26 70.85 84.24 90.08 93.37In Figures 11 to 13, a summary for each type of experiment is shown in order tocompare the performance between the languages.
In Figure 14 the average number oftimes an output position is occupied is given for all the languages.
Finally, in Figure 15,the degradation of performance as a function of the corrupted input words is shown.The differences in performance between the languages and the types of domains andmodels used are discussed in the following section.4.2 Comments on the Performance of the Proposed SystemOne can initially observe the number of times the algorithm produced a word ineach position 1 to 4 (Figure 14).
This number decreases very fast from the first tothe last position for most of the languages, which shows that the system does notproduce extreme spellings of the input words (even though these may be allowedby the language).
The second very interesting feature revealed in Tables 9 to 17 isthat the improvement in the system's performance decreases rapidly from the first tothe last position of the output, which means that the majority of correct suggestions isincluded in the first two positions.
Column l(s) shows that the percentage oferroneoussymbols is very small indeed, while column d shows that even though a word may beincorrect, only a small percentage of its symbols may be wrong (about 15% on averagein Exp 2), which proves that the output of the algorithm is very easily human-readableeven when it contains errors.The performance of the algorithm varied widely, depending on the language beingtested.
This is due to the differences in spelling in each language and, consequently,to the training the model required.
As described in Section 3.1, the available materialfor training were 300k-word corpora for all languages.
This amount was sufficientfor some languages (Dutch, German, Italian, Greek, and Spanish) but insufficient for363Computational Linguistics Volume 22, Number 3lOO ?
m i90 t .
.
.
.8s i -" "" i ' " " ' "  - " IX / ?
?z/80757065605550 I I I I1 1-2 1-3 1-4XX-I-eXX+El/E1/1El/El/2El/E2/1El/E2/2E1/NE/1E1/NE/2E2/E1/1E2/E1/2E2/E2/1E21E2/2E2/NE/1E2/NE/2NE/ElllNE/EI/2NE/E2/1NE/E2/2NE/NE/1NE/NE/2Exp IFigure 2Overview of results for Dutch.100958580 .
/ j  .
.
- "  ?
X75 .
X7065605550t //tm!XXII I I ~" I1 1-2 1-3 1-4?
E11E1/1?
El/El/2?
El/E2/2X EI/NE/2X E21E1/1?
E2/E1/2+ E21E2/2?
E2/NE/2- -  NE/EI/1?
NE/EI/2?
NE/E2/2?
NE/NE/2X E21E211X NE/NE/1.
.
.
.
.
ExplExp 2Figure 3Overview of results for English.364Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMM10095 I8580/ 7570 / /60I5550 i ~ = u1 1-2 1-3 1-4?
El/El/2?
El/E2/2?
El/NEllX E2/E1/2I E2/E2/1?
E2/E2/2+ E2/NE/1- NE/EI/2-- NE/E2/2?
NE/NE/1?
El/El/1?
NE/NE/2.
.
.
.
.
Exp lExp 2Figure 4Overview of results for French.10095908580757065605550/ /  x?
*Xm// .
/  q\]~X ,- X J ~/ / /?Xm1 1-2 1-3 1-4?
El/El/1?
El/El/2?
El/E2/2X EI/NE/2E2/E1/1?
E2/E1/2+ E2/E2/2- E2/NE/2- -  NE/EI/1~, NE/EI/2NE/E2/2?
NE/NE/2X E2/E2/1X NE/NE/1.
.
.
.
.
Exp lExp 2Figure 5Overview of results for German.365Computational Linguistics Volume 22, Number 310095 i908580757065605550?
IIm __-i?
.w .
.
.
.
t//:i.pi i I i1 1-2 1-3 1~?
E2/E1/1?
El/E1/1?
NE/EI/1X E2/E2/1X El/E2/1?
NE/E2/1-I- E2/NE/1?
El/NEll- -  NE/NE/1?
E2/E1/2?
El/El/2.
.
.
.
.
Exp lExp 2Figure 6Overview of results for German (no capitals).10095908580757065605550//l1i1 1-2 1-3 1-4?
El /El l2?
El/E2/2?
El/NEllX E2/E1/2X E2/E2/1?
E2/E2/2+ E21NE/1- NE/EI/2-- NE/E2/2?
NE/NE/1?
El/El/1?
NE/NE/2- - -  Exp lExp 2Figure 7Overview of results for Greek.366Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMM1009590858O75706s l60 ~55501 1-2 1-3 1-4?
El/El/1I I  E l /E l /2?
El/E2/1X El/E2/2;K EI/NE/1?
EI/NE/2?
.I- E2/E1/1?
E2/El/2?
- E2/E2/1?
E21E212D E2/NE/1& E2/NE/2X NE/EI/1X NE/EI/2?
NE/E2/1+ NE/E2/2?
, NE/NE/1- .
NE/NE/2- - -  Exp1Figure 8Overview of results for Italian.'?
?1 .
.
.
.
.
.
.
i95ii I 85 ~'"  80/?
, "  ?706560 t55501 1-2 1-3 1-4XX+XX+El/El/1E l /E l /2El/E2/1El/E2/2El /NEl lEI/NE/2E2/El/1E2/E1/2E21E211E2/E2/2E2/NE/1E2/NE/2NE/EI/1NE/EI/2NE/E2/1NE/E2/2NE/NE/1NE/NE/2Exp 1Figure 9Overview of results for Spanish.367Computational Linguistics Volume 22, Number 3?
," ?
x ," x X/ ~,  X4xI P I I1 1-2 1-3 1-4?
Nl/N1/1am N1/N1/2?
NI/OD/1X NI/OD/2X ODIN1/1?
ODIN1/24- OD/ODI1o OD/OD/2.
.
.
.
.
ExplExp 2Figure 10Overview of results for names.others (English and French).
A more detailed presentation of the algorithm's behaviorin the languages tested follows.For Dutch, the model gives relatively good results (97.6% for four output candi-dates).
Spelling in Dutch is rather straightforward for etymologically Dutch words, butwords of foreign origin are usually spelled as in the lauguage of their origin.
Thesewords are responsible for most of the errors encountered.The model performed worse for English than for the other languages mainly be-cause the relationship between pronunciation and spelling is less regular.
This resultedin fewer grapheme transitions in the training corpus and meant hat the standard train-ing period was insufficient.
Another problem is that compound words usually keepthe initial pronunciation of their components (e.g., in words such as "whatsoever","therefore", etc.
); this leads to many errors for an algorithm like the one proposedhere, which has no information about the origin and etymology of each word.
Simi-lar work (Parfitt and Sharman 1991) shows the same problems in a slightly differentcontext.
Of course, more training of the model would improve performance.With French, there is a special problem, which does not occur with other languages:there exist many homophones that are distinguished only by the presence or absenceof various mute letters at the ends of the words.
This feature significantly increases thenumber of states that have to be defined.
Consequently, the available training materialwas inadequate for the creation of a correct model, and led to poor performance.The model performed well with German.
The only drawback was the decisionabout the type of the first letter (uppercase or lowercase); nouns always start witha capital letter while other words do not.
This is the primary cause of the errorsintroduced in the experiments with German.
Experiments ignoring this ambiguitysignificantly improved the German results as can be seen from a comparison of Figure 5368Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMM100 ,90807570656055504.4.XxtI Ii1 1-2 1-3 14?
Dutch?
English?
FrenchX GermanX Hellenic?
Italian+ Spanish- NamesFigure 11Summary of performance: Experiment 1.10095908580757065605550?x |X| m1 1-2 1-3 1-4?
Dutch?
English?
FrenchxGermanX Hellenic?
Italian+ Spanish-NamesFigure 12Summary of performance: Experiment 2.369Computational Linguistics Volume 22, Number 310095908580757065605550 I?
$.=i+ ||iI i1 1-2 1-3 1-4?
Dutch?
English?
FrenchxGermanX Hellenic?
Italian4- Spanish-NamesFigure 13Summary of performance: Experiment 3.10090807060% 504030201001 2 3 4--41,.- Dutch--m-- EnglishFrenchX GermanX Hellenic- -e - -  ItalianI Spanish?
NamesFigure 14Occupation of output candidate positions.370Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMM.
.
.
.
.
,mw?Ju)100 .
.
.
.
.
_ _90807060500 20 40 60 80 100Input Corruption (% of words)Figure 15Degradation of output vs input corruption.and Figure 6.With Greek, the model behaved quite well, reaching more than 99% success for thesecond-order HMM experiments with up to four output candidates.
Figure 7 illustratesthe difference between the performance of Exp 1 and Exp 2 (order of model) in thefirst output position.
These results are the consequence of two contradictory featuresof the Greek language:a.b.every grapheme is usually pronounced in the same way (i.e.,corresponds to one phoneme), andevery phoneme usually has more than one possible spellings regardlessof its neighboring phonemes.As an example, the phoneme/ i / can  be transcribed as z, 7/, v, ?z, and oz in almostany context (Petrounias 1984; Setatos 1974).
Other problems arise from the consonants,which can be either single or double without any change in the pronunciation.Finally, the model gave extremely good results with Italian and Spanish, reachingmore than 99% success for the second order model and up to two or three outputcandidates for known and unknown text experiments, respectively.
This is becausethere is usually a one-to-one correspondence b tween phonemes and graphemes inthese languages.Another dimension of the analysis of the results is the domain of the experiment.The model behaved best in experiments hat used the newspaper corpora, which aremore casual in style and richer in vocabulary than the other domains.
These corporausually contain more grapheme transitions, which give greater detail about he spellingmechanism of the language, and provide the most efficient training possible.
The371Computational Linguistics Volume 22, Number 3experiments on the Name corpora resulted in lower scores than the correspondingexperiments on the Hellenic general corpora reaching 92.3% for Exp 1, 98.3% for Exp2, and 93.4% for Exp 3 for four output candidates.
The main difference in the successrate (Table 17) is due to the size of the training corpora (the training, especially withthe street names, was inadequate) and to the fact that names are usually spelled orpronounced in a more arbitrary way than other words.Finally, as expected, the model performed worse in experiments using as input asimulation of a speech recognizer output (distorted speech) than in the correspondingexperiments using a correct phonemic representation f the words.
However, by mea-suring the ambiguity introduced by the speech recognizer output, it can be seen thatthe PTGC system in fact improved the performance of the overall system (recognizersimulator and PTGC).
This was also expected, since in the training phase the modelis trained using the correct graphemic form of the words, which is later reproducedin the conversion experiments.
Evidently, the performance of the algorithm dependson the amount of distortion introduced in the input phonemic string.
Figure 15 showsthe degradation of the success rate of the algorithm as a function of the corruption ofthe input stream.
The dashed lines refer to a first-order HMM experiment, while thesolid lines refer to a second-order HMM experiment.
The input degradation does notaffect the overall system performance very much (in any of the four output positions)even when more than 85% of the input words have at least one incorrect phoneme.
Itmust be noted that, in this case, about 30% of the input symbols (unit phonemes) havebeen replaced by erroneous ones but still the score of the first four positions remainsabove 98%.5.
Conc lus ionWe have presented a system for phoneme-to-grapheme conversion (PTGC) at the wordlevel that uses the principles of hidden Markov models to statistically correlate thegraphemic forms to the phonemic forms of a natural anguage.
A first- and second-order HMM have been created and the Viterbi and N-best algorithm have been usedfor the conversion.
In the latter case, experimentation showed that no more than twosolutions (output candidates) are necessary to produce the correct output with anaccuracy higher than 96% for most of the languages the system was tested on.
If fouroutput candidates are allowed, then this rate reaches 97% to 100%.
Moreover, it mustbe noted that the success rate of the system, although already good enough, can befurther improved by better training on a larger corpus of selected texts.An important advantage of the system presented here, in comparison to rule-based or dictionary look-up systems, is that it produces only one (or at least very few)graphemic suggestions for each phonemic word.
In the first case (one suggestion), nolanguage model is needed to disambiguate potential homophones at sentence level.
Inthe second case (a few suggestions), the execution speed of the system is substantiallyhigher than in rule-based or dictionary-based systems, due to the small number ofsuggestions per word.
The prototype system, which was implemented on a 486-basedpersonal computer, responded at an average rate of one word per second for Exp 2(second-order HMM) and about ten times faster for Exp 1 (first-order model).
The factthat the algorithm scans the input word linearly (once from the beginning to the end)means that it can work in parallel with other modules of speech recognition systemsand produce output with a very short delay after the end of the input.Another advantage of this system is that it can work in any language in whichthe pronunciation of the words is statistically dependent only on their spelling.
Theonly language-specific part of the system, i.e., the algorithm for the segmentation rule372Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMMdefinition, is straightforward and does not need any special inguistic knowledge butonly familiarity with the target language to be processed.The system is not limited by any dictionary.
This is a significant advantage invery large or unlimited dictionary applications.
An implication of this property isthat the system does not try to match the input utterance to the closest word (bysome measure of distance) contained in the dictionary but rather tries to find its mostprobable spelling.
In this sense, the output of the PTGC system never misleads thefinal human user about what the input was.Note also that the system is symmetric between the two forms of a natural an-guage: graphemic and phonemic.
This implies that without any modification, the al-gorithm can be used in the reverse order (i.e., for a grapheme-to-phoneme conversionsystem, widely used in text-to-speech \[or speech synthesis\] systems) by just inter-changing the phonemic with the graphemic data of the training procedure.Last but not least, the fact that the system is not rule based but uses an algorithmbased on probabilities makes it possible to implement the system in hardware, result-ing in a system adaptable to any real-time speech recognition system.
As can be seenin the equations of the appendix the algorithm is highly parallel since the values ofdjk(t) are independently computed from the values of dij(t - 1); this means that thesecalculations can be performed concurrently.
In this manner, the response time of thecomplete algorithm can be proportional to N 2 rather than to N 3, yielding a systemthat can serve as a module for any real-time speech recognition system.In conclusion, the proposed method has the following advantages:?
It is language independent, making it adaptable it to any language withlittle effort.?
It does not need a dictionary and thus is free of any restrictions.?
It gives only one or very few transcriptions per word.?
It can be implemented in hardware and serve in real-time speechrecognition systems.Appendix: Implementation NotesThe fact that only multiplications are involved in the processing of the conversionalgorithm led us to convert the algorithm to use only additions.
Instead of usingprobabilities, we used their negative logarithm, thus yielding distances.
This trans-formation offers two advantages: First, a four-byte integer representation is used foreach number instead of a ten-byte floating point representation, without any loss ofaccuracy, thus reducing memory requirements.
Second, a substantial increase in pro-cessing speed is achieved, since the fixed point addition is faster than floating pointmultiplication.Clearly, since the probability P is a number between 0 and 1, - log(P)  is a numberin the range 0. .
.
ec.
In order to reduce computation, one of the two library-suppliedlogarithm functions had to be used, i.e., log10 or log e. It can easily be seen that if a > b,then - loga(P ) < -lOgb(P ).
For this reason the natural ogarithms (base e = 2.71828)were chosen instead of decimal ogarithms.To benefit from the above transformation, a fixed point arithmetic should be used(floating point addition is as troublesome as floating point multiplication if not more).At this point, we had to make decisions taking into account implementation-specificparameters.
The system was implemented using the C programming language on373Computational Linguistics Volume 22, Number 3a 486-based computer in protected mode, thus exploiting its full 32-bit architecture.The probabilities are first calculated using the greatest available floating point rep-resentation, which is 80 bits for a long double floating point number.
The small-est nonzero value of P in this representation (and effectively its best resolution) is3.4 x 10 -4932 corresponding to the greatest value of - log(P), which is 11,355.126.
Anunsigned long integer has a 32-bit dynamic range, which results in a maximum valueof 2 32 = 4, 294, 967, 295.
Since for every state we need to add two distances, one frommatrix A and one from matrix B, we must be sure that there will be no overflowafter all the additions that must be made for each word.
The system as tested uses amaximum number of 30 states per word, a constant that has not yet been surpassedby any word in all the languages in which it was tested.
This means that the max-imum distance value must be 232/60 = 71,582, 788, which results in a scaling factorf = 71,582,788/11,355.126 = 3, 040.
By multiplying every distance with this factorand truncating it to its integral part, it is guaranteed that there will be no overflowin the execution of the Viterbi algorithm.
This fact allows the elimination of codethat would check for overflow during the algorithm, resulting in a much faster code.For reference, the complete algorithm converted to work with logarithms (as it wasimplemented) is presented below:Let a t, fl!, 7r' and p' be the HMM parameters after the above transformation andnormalization (e.g., alj k = f .
Lloge(aijk)J where f (=  63, 040) is the factor that was used tofacilitate fixed point arithmetic).
Then we inductively compute the locally minimumdistance 6!
and the path ~ as follows:Initialization!
!
6'~(2) = 7r i + Pij + fl;(O1) q- fl;(02)" 1 <_ i, j  <_ N (11)6'e(2~ijk, = 232, 1 < i, j < N (12) 2<e<Ew/B ,~/''et2~ = 0, 1 _< i, j _< N (13) 2<e<ERecursion (2 < t < T):!
6;k(t + 1) = m/in *(6;j(t) + aljk) + fl~k(Ot+l) 1 <_ i, j  <_ N (14)~;k(t + 1) = arg rain *(6\[j(t) + aljk)ISequence backtracking:1 < i,j <_ N (15)- .
!
d*=mm 6ij(T ), l <_ i,j < N (16)zj(qT--l,qv) = arg min *dlj(T ), 1 G i,j G N (17)q,e I 1 < t < T - 2 qe t = ~b q,+lq,+2 1 K e < E (18)Kwhere d* is a vector containing the E minimum distance values that correspond tothe E state sequences Qe = {qt}, t = 1. .
.
T, e = 1. .
.
E, which are returned as the best(most probable) state sequences.374Rentzepopoulos and Kokkinakis Phoneme-to-Grapheme Using HMMAcknowledgmentsThe text corpora used for the training andassessment of the system were gathered andtranscribed to their phonemic form duringthe EEC ESPRIT-1 Project 860 "LinguisticAnalysis of the European Languages."
Thename directory corpora were obtained fromthe LRE-61004 ONOMASTICA project.
Theauthors wish to thank Dr. AnastassiosTsopanoglou and Dr. Evaggelos Dermatasfor their comments on the paper and theirvaluable assistance in the final preparationof the manuscript.ReferencesESPRIT Project 291/860.
1987.
Linguisticanalysis of the European languages.Technical Annex.
European CommissionFramework Programme "EuropeanStrategic Programme for Research inInformation Technology" (ESPRIT).Forney, G. David Jr. 1973.
The Viterbialgorithm.
In Proceedings o/the IEEE, 61(3).Hannaford, Blake and Paul Lee.
1990.Multi-dimensional hidden Markov modelof telemanipulation tasks with varyingoutcomes.
In Proceedings o/the IEEEInternational Conference on Systems, Man &Cybernetics, pages 127-133.He, Yang.
1988.
Extended Viterbi algorithmfor second order hidden Markov process.In Proceedings ofthe IEEE 9th InternationalConference on Pattern Recognition, pages718-720.Katz, Slava M. 1987.
Estimation ofprobabilities from sparse data for thelanguage model component of a speechrecognizer.
IEEE Transactions on Acoustics,Speech and Signal Processing, 35(3).Kerkhoff, J. and J. Wester.
1987.
FONPARS1User Manual.
Internal publication291/860, ESPRIT project.Kriouile, Abdelaziz, Jean-Francois Mari, andJean-Paul Haton.
1990.
Someimprovements in speech recognitionalgorithms based on HMM.
In Proceedingso/the International Conference on Acoustics,Speech and Signal Processing, pages545-548.
ICASSP.Laface P., G. Micca, and R. Pieraccini.
1987.Experimental results on a large lexiconaccess task.
In Proceedings oftheInternational Conference on Acoustics, Speechand Signal Processing, pages 809-812.ICASSP.Lee, Kai-Fu.
1989.
Hidden Markov models:Past, present and future.
In Proceedings ofEurospeech, 1:148-155.Levinson, S. E., M. Y. Liberman, A. Ljolje,and L. G. Miller.
1989.
Speakerindependent phonetic transcription offluent speech for large vocabulary speechrecognition.
In Proceedings oftheInternational Conference on Acoustics, Speechand Signal Processing, pages 441-444.ICASSP.LRE-61004, Multi-language PronunciationDictionary of Proper Names and PlaceNames.
ONOMASTICA Technical Annex.European Commission FrameworkProgramme "Linguistic Research andEngineering.
"Ney, Hermann and Ute Essen.
1991.
Onsmoothing techniques for bigram-basednatural anguage modelling.
InProceedings ofthe International Conference onAcoustics, Speech and Signal Processing,pages 825-828.
ICASSP.Parfitt, S. H. and R. A. Sharman.
1991.
Abi-directional model of Englishpronunciation.
In Proceedings ofEurospeech,2:801-804.Petrounias, Evaggelos.
1984.
Modern GreekGrammar and Comparative Analysis (inGreek).
University Studio Press,Thessaloniki, Greece.Rabiner, Lawrence R. 1989.
A tutorial onhidden Markov models and selectedapplications in speech recognition.
InProceedings ofthe IEEE, 77(2).Rentzepopoulos, Panagiotis.
1988.
Phonemeto Grapheme Conversion Using Rules (inGreek).
Electrical Engineering Diplomathesis.
University of Patras, Patras, Greece.Rentzepopoulos, Panagiotis and GeorgeKokkinakis.
1991.
Phoneme to graphemeconversion using HMM.
In Proceedings ofEurospeech "91: 797-800.Rentzepopoulos, Panagiotis and GeorgeKokkinakis.
1992.
Multilingual phonemeto grapheme conversion system based onHMM.
In Proceedings ofthe InternationalConference on Spoken Language Processing2:1191-1194.
ICSLP.Rentzepopoulos, Panagiotis, AnastassiosTsopanoglou, and George Kokkinakis.1991.
A statistical approach for phonemeto grapheme conversion.
In Proceedings ofthe 1st Quantitative Linguistics ConferenceQUALICO.Schwartz, Richard and Steve Austin.
1991.A comparison of several approximatealgorithms for finding multiple (N-BEST)sentence hypotheses.
In Proceedings oftheInternational Conference on Acoustics, Speechand Signal Processing, pages 701-704.ICASSP.Schwartz, Richard and Yen-Lu Chow.
1990.375Computational Linguistics Volume 22, Number 3The N-best algorithm: An efficient andexact procedure for finding the N mostlikely sentence hypotheses.
In Proceedingsof the International Conference on Acoustics,Speech and Signal Processing, pages 81-84.ICASSP.Setatos, M. 1974.
Phonology of the ModernGreek Koini (in Greek).
Edited by Papazisis(editor).
Athens, Greece.Viterbi, A. J.
1967.
Error bound forconvolutional codes and anasymptotically optimum decodingalgorithm.
IEEE Transactions in InformationTheory, 13(2).376
