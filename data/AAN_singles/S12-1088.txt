First Joint Conference on Lexical and Computational Semantics (*SEM), pages 597?602,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUNITOR: Combining Semantic Text Similarity functionsthrough SV RegressionDanilo Croce, Paolo Annesi, Valerio Storch and Roberto BasiliDepartment of Enterprise EngineeringUniversity of Roma, Tor Vergata00133 Roma, Italy{croce,annesi,storch,basili}@info.uniroma2.itAbstractThis paper presents the UNITOR system thatparticipated to the SemEval 2012 Task 6: Se-mantic Textual Similarity (STS).
The task ishere modeled as a Support Vector (SV) regres-sion problem, where a similarity scoring func-tion between text pairs is acquired from exam-ples.
The semantic relatedness between sen-tences is modeled in an unsupervised fashionthrough different similarity functions, eachcapturing a specific semantic aspect of theSTS, e.g.
syntactic vs. lexical or topical vs.paradigmatic similarity.
The SV regressor ef-fectively combines the different models, learn-ing a scoring function that weights individualscores in a unique resulting STS.
It provides ahighly portable method as it does not dependon any manually built resource (e.g.
WordNet)nor controlled, e.g.
aligned, corpus.1 IntroductionSemantic Textual Similarity (STS) measures the de-gree of semantic equivalence between two phrasesor texts.
An effective method to compute similar-ity between short texts or sentences has many appli-cations in Natural Language Processing (Mihalceaet al, 2006) and related areas such as InformationRetrieval, e.g.
to improve the effectiveness of a se-mantic search engine (Sahami and Heilman, 2006),or databases, where text similarity can be used inschema matching to solve semantic heterogeneity(Islam and Inkpen, 2008).STS is here modeled as a Support Vector (SV) re-gression problem, where a SV regressor learns thesimilarity function over text pairs.
Regression learn-ing has been already applied to different NLP tasks.In (Pang and Lee, 2005) it is applied to OpinionMining, in particular to the rating-inference prob-lem, wherein one must determine an author evalua-tion with respect to a multi-point scale.
In (Albrechtand Hwa, 2007) a method is proposed for develop-ing sentence-level MT evaluation metrics using re-gression learning without directly relying on humanreference translations.
In (Biadsy et al, 2008) it hasbeen used to rank candidate sentences for the taskof producing biographies from Wikipedia.
Finally,in (Becker et al, 2011) SV regressor has been usedto rank questions within their context in the multi-modal tutorial dialogue problem.In this paper, the semantic relatedness betweentwo sentences is modeled as a combination of dif-ferent similarity functions, each describing the anal-ogy between the two texts according to a specificsemantic perspective: in this way, we aim at captur-ing syntactic and lexical equivalences between sen-tences and exploiting either topical relatedness orparadigmatic similarity between individual words.The variety of semantic evidences that a system canemploy here grows quickly, according to the genreand complexity of the targeted sentences.
We thuspropose to combine such a body of evidence to learna comprehensive scoring function y = f(~x) over in-dividual measures from labeled data through SV re-gression: y is the gold similarity score (provided byhuman annotators), while ~x is the vector of the dif-ferent individual scores, provided by the chosen sim-ilarity functions.
The regressor objective is to learnthe proper combination of different functions redun-dantly applied in an unsupervised fashion, withoutinvolving any in-depth description of the target do-main or prior knowledge.
The resulting function se-lects and filters the most useful information and it597is a highly portable method.
In fact, it does not de-pend on manually built resources (e.g.
WordNet),but mainly exploits distributional analysis of unla-beled corpora.In Section 2, the employed similarity functionsare described and the application of SV regressionis presented.
Finally, Section 3 discusses results onthe SemEval 2012 - Task 6.2 Combining different similarity functionthrough SV regressionThis section describes the UNITOR systems partic-ipating to the SemEval 2012 Task 6: in Section 2.1the different similarity functions between sentencepairs are discussed, while Section 2.2 describes howthe SV regression learning is applied.2.1 STS functionsEach STS depends on a variety of linguistic aspectsin data, e.g.
syntactic or lexical information.
Whiletheir supervised combination can be derived throughSV regression, different unsupervised estimators ofSTS exist.Lexical Overlap (LO).
A basic similarity functionis first employed as the lexical overlap between sen-tences, i.e.
the cardinality of the set of words occur-ring in both sentences.Document-oriented similarity based on LatentSemantic Analysis (LSA).
This function captureslatent semantic topics through LSA.
The adjacencyterms-by-documents matrix is first acquired throughthe distributional analysis of a corpus and reducedthrough the application of Singular Value Decom-position (SVD), as described in (Landauer and Du-mais, 1997).
In this work, the individual sentencesare assumed as pseudo documents and representedby vectors in the lower dimensional LSA space.
Thecosine similarity between vectors of a sentence pairis the metric hereafter referred to as topical similar-ity.Compositional Distributional Semantics (CDS).Lexical similarity can also be extended to accountfor syntactic compositions between words.
Thismakes sentence similarity to depend on the set of in-dividual compounds, e.g.
subject-verb relationshipinstances.
While basic lexical information can stillbe obtained by distributional analysis, phrase levelFigure 1: Example of dependency graphsimilarity can be here modeled as a specific func-tion of the co-occurring words, i.e.
a complex alge-braic composition of their corresponding word vec-tors.
Differently from the document-oriented caseused in the LSA function, base lexical vectors arehere derived from co-occurrence counts in a wordspace, built according to the method discussed in(Sahlgren, 2006; Croce and Previtali, 2010).
In or-der to keep dimensionality as low as possible, SVDis also applied here (Annesi et al, 2012).
The resultis that every noun, verb, adjective and adverb is thenprojected in the reduced word space and then dif-ferent composition functions can be applied as dis-cussed in (Mitchell and Lapata, 2010) or (Annesi etal., 2012).Convolution kernel-based similarity.
The similar-ity function is here the Smoothed Partial Tree Ker-nel (SPTK) proposed in (Croce et al, 2011).
Thisconvolution kernel estimates the similarity betweensentences, according to the syntactic and lexical in-formation in both sentences.
Syntactic representa-tion of a sentence like ?A man is riding a bicycle?
isderived from the dependency parse tree, as shownin Fig.
1.
It allows to define different tree struc-tures over which the SPTK operates.
First, a treeincluding only lexemes, where edges encode theirdependencies, is generated and called Lexical OnlyCentered Tree (LOCT), see Fig.
2.
Then, we addto each lexical node two leftmost children, encod-ing the grammatical function and the POS-Tag re-spectively: it is the so-called Lexical Centered Tree(LCT), see Fig.
3.
Finally, we generate the Gram-matical Relation Centered Tree (GRCT), see Fig.4, by setting grammatical relation as non-terminalnodes, while PoS-Tags are pre-terminals and fathersof their associated lexemes.
Each tree representationprovides a different kernel function so that three dif-ferent SPTK similarity scores, i.e.
LOCT, LCT andGRCT, are here obtained.598be::vride::vbicycle::na::dman::na::dFigure 2: Lexical Only Centered Tree (LOCT)be::vVBZROOTride::vVBGVCbicycle::nNNOBJa::dDTNMODman::nNNSBJa::dDTNMODFigure 3: Lexical Centered Tree (LCT)ROOTVCOBJNNbicycle::nNMODDTa::dVBGride::vVBZbe::vSBJNNman::nNMODDTa::dFigure 4: Grammatical Relation Centered Tree (GRCT)2.2 Combining STSs with SV RegressionThe similarity functions described above providescores capturing different linguistic aspects and aneffective way to combine such information is madeavailable by Support Vector (SV) regression, de-scribed in (Smola and Scho?lkopf, 2004).
The ideais to learn a higher level model by weighting scoresaccording to specific needs implicit in training data.Given similarity scores ~xi for the i-th sentence pair,the regressor learns a function yi = f(~xi), where yiis the score provided by human annotators.The ?-SV regression (Vapnik, 1995) algorithm al-lows to define the best f approximating the train-ing data, i.e.
the function that has at most ?
de-viation from the actually obtained targets yi forall the training data.
Given a training dataset{(~x1, y1), .
.
.
, (~xl, yl)} ?
X ?
R, where X is thespace of the input patterns, i.e.
the original similar-ity scores, we can acquire a linear functionf(~x) = ?~w, ~x?+ b with ~w ?
X, b ?
Rby solving the following optimization problem:minimize12||~w||2subject to{yi ?
?~w, ~xi?
?
b ?
?
?~w, ~xi?+ b?
yi ?
?Since the function f approximating all pairs(~xi, yi) with ?
precision, may not exist, i.e.
the con-vex optimization problem is infeasible, slack vari-ables ?i, ?
?i are introduced:minimize12||~w||2 + Cl?i=1(?i + ?
?i )subject to????
?yi ?
?~w, ~xi?
?
b ?
?+ ?i?~w, ~xi?+ b?
yi ?
?+ ?
?i?i, ?
?i ?
0where ?i, ?
?i measure the error introduced by trainingdata with a deviation higher than ?
and the constantC > 0 determines the trade-off between the norm?~w?
and the amount up to which deviations largerthan ?
are tolerated.3 Experimental EvaluationThis section describes results obtained in the Se-mEval 2012 Task 6: STS.
First, the experimentalsetup of different similarity functions is described.Then, results obtained over training datasets are re-ported.
Finally, results achieved in the competitionare discussed.3.1 Experimental setupIn order to estimate the Latent Semantic Analysis(LSA) based similarity function, the distributionalanalysis of the English version of the Europarl Cor-pus (Koehn, 2002) has been carried out.
It is thesame source corpus of the SMTeuroparl dataset andit allows to acquire a semantic space capturing thesame topics characterizing this dataset.
A word-by-sentence matrix models the sentence representationspace.
The entire corpus has been split so that eachvector represents a sentence: the number of differentsentences is about 1.8 million and the matrix cellscontain tf-idf scores between words and sentences.The SVD is applied and the space dimensionality599is reduced to k = 250.
Novel sentences are im-mersed in the reduced space, as described in (Lan-dauer and Dumais, 1997) and the LSA-based simi-larity between two sentences is estimated accordingthe cosine similarity.To estimate the Compositional Distributional Se-mantics (CDS) based function, a co-occurrenceWord Space is first acquired through the distribu-tional analysis of the UKWaC corpus (Baroni et al,2009), i.e.
a Web document collection made ofabout 2 billion tokens.
UKWaC is larger than theEuroparl corpus and we expect it makes availablea more general lexical representation suited for alldatasets.
An approach similar to the one described in(Croce and Previtali, 2010) has been adopted for theacquisition of the word space.
First, all words occur-ring more than 200 times (i.e.
the targets) are rep-resented through vectors.
The original space dimen-sions are generated from the set of the 20,000 mostfrequent words (i.e.
features) in the UKWaC cor-pus.
One dimension describes the Pointwise MutualInformation score between one feature as it occurson a left or right window of 3 tokens around a target.Left contexts of targets are treated differently fromthe right ones, in order to also capture asymmetricsyntactic behaviors (e.g., useful for verbs): 40,000dimensional vectors are thus derived for each target.The particularly small window size allows to bettercapture paradigmatic relations between targets, e.g.hyponymy or synonymy.
Again, the SVD reductionis applied to the original matrix with a k = 250.Once lexical vectors are available, a compositionalsimilarity measure can be obtained by combiningthe word vectors according to a CDS operator, e.g.
(Mitchell and Lapata, 2010) or (Annesi et al, 2012).In this work, the adopted compositional representa-tion is the additive operator between lexical vectors,as described in (Mitchell and Lapata, 2010) and thesimilarity function between two sentences is the co-sine similarity between their corresponding compo-sitional vectors.
Moreover, two additive operatorsthat only sum over nouns and verbs are also adopted,denoted by CDSV and CDSN , respectively.The estimation of the semantically Smoothed Par-tial Tree Kernel (SPTK) is made available by an ex-tended version of SVM-LightTK software1 (Mos-1http://disi.unitn.it/moschitti/Tree-Kernel.htmchitti, 2006) implementing the smooth matching be-tween tree nodes.
The tree representation describedin Sec.
2.1 allows to define 3 different kernels, i.e.SPTKLOCT , SPTKLCT and SPTKGRCT .
Similaritybetween lexical nodes is estimated as the cosine sim-ilarity in the co-occurrence Word Space describedabove, as in (Croce et al, 2011).In all corpus analysis and experiments, sentencesare processed with the LTH dependency parser, de-scribed in (Johansson and Nugues, 2007), for Part-of-speech tagging and lemmatization.
Dependencyparsing of datasets is required for the SPTK appli-cation.
Finally, SVM-LightTK is employed for theSV regression learning to combine specific similar-ity functions.3.2 Evaluating the impact of unsupervisedmodelsTable 1 compares the Pearson Correlation of differ-ent similarity functions described in Section 2.1, i.e.mainly the results of the unsupervised approaches,against the challenge training data.
Regarding toMSRvid dataset, the topical similarity (LSA func-tion) achieves the best result, i.e.
0.748.
Paradig-matic lexical information as in CDS, CDSN and LOprovides also good results, confirming the impact oflexical generalization.
However, only nouns seemto contribute significantly, as for the poor results ofCDSV suggest.
As the dataset is characterized byshort sentences with negligible syntactic differences,SPTK-based kernels are not discriminant.
On thecontrary, the SPTKLCT achieves the best result inthe MSRpar dataset, where paraphrasing phenom-ena are peculiar.
Notice that the other SPTK kernelsare not equivalently performant, in line with previ-ous results on question classification and semanticrole labeling (Croce et al, 2011).
Lexical informa-tion provides a crucial contribution also for LO, al-though the contribution of topical or paradigmaticgeneralization seems negligible over MSRpar.
Fi-nally, in the SMTeuroparl, longer sentences are thenorm and length seems to compromise the perfor-mance of LO.
The best results seem to require thelexical and syntactic information provided by CDSand SPTK.600ModelsDatasetMSRvid MSRpar SMTeuroparlCDS .652 .393 .681CDSN .630 .234 .485CDSV .219 .317 .264LSA .748 .344 .477SPTKLOCT .300 .251 .611SPTKLCT .297 .464 .622SPTKGRCT .278 .255 .626LO .560 .446 .248Table 1: Unsupervised results over the training dataset3.3 Evaluating the role of SV regressionThe SV regressors have been trained over a featurespace that enumerates the different similarity func-tions: one feature is provided by the LSA function,three by the CDS, i.e.
CDS, CDSN and CDSV ,three by SPTK, i.e.
SPTKLOCT , SPTKLCT andSPTKGRCT and one by LO, i.e.
the number ofwords in common.
Two more features are obtainedby the sentence lengths of a pair, i.e.
the numberof words in the first and second sentence, respec-tively.
Table 2 shows Pearson Correlation resultswhen the regressor is trained according a 10-foldcross validation schema.
First, all possible featurecombinations are attempted for the SV regression,so that every subset of the 10 features is evaluated.Results of the best feature combination are shown incolumn bestfeat: for MSRvid, the best performanceis achieved when all 10 features are considered; inMSRpar, SPTK combined with LO is sufficient; fi-nally, in the SMTeuroparl the combination is LO,CDS and SPTK.
In column allfeat results achievedby considering all features are reported.
Last col-umn specifies the performance increase with respectto the corresponding best results in the unsupervisedsettings.Results of the regressors are always higher withrespect to the unsupervised settings, with up to a35% improvement for the MSRpar, i.e.
the mostcomplex domain.
Moreover, differences when bestand all features are employed are negligible.
Itmeans that SV regressor allows to automaticallycombine and select the most informative similarityaspects, confirming the applicability of the proposedredundant approach to STS.DatasetExperimentsGainbestfeat allfeatMSRvid .789 .789 5,0%MSRpar .615 .612 32,4%SMTeuroparl .692 .691 1,6%Table 2: SV regressor results over the training dataset3.4 Results over the SemEval Task 6According to the above evidence, we participated tothe SemEval challenge with three different systems.Sys1 - Best Features.
Scores between pairs from aspecific dataset are obtained by applying a regressortrained over pairs from the same dataset.
It meansthat, for example, the test pairs from the MSRviddataset are processed with a regressor trained overthe MSRvid training data.
Moreover, the most rep-resentative similarity function estimated for the col-lection is employed: the feature combination provid-ing the best correlation results over training pairs isadopted for the test.
The same is applied to MSRparand SMTeuroparl.
No selection is adopted for theSurprise data and training data for all the domainsare used, as described in Sys3.Sys2 - All Features.
Relatedness scores betweenpairs from a specific dataset are obtained using a re-gressor trained using pairs from the same dataset.Differently from the Sys1, the similarity functionhere is employed within the SV regressors trainedover all 10 similarity functions (i.e.
all features).Sys3 - All features and All domains.
The SV re-gressor is trained using training pairs from all col-lections and over all 10 features.
It means that onesingle model is trained and employed to score alltest data.
This approach is also used for the Surprisedata, i.e.
the OnWN and SMTnews datasets.Table 3 reports the general outcome for the UN-ITOR systems.
Rank of the individual scores withrespect to the other systems participating to the chal-lenge is reported in parenthesis.
This allows to drawsome conclusions.
First, the proposed system ranksaround the 12 and 13 system positions (out of 89systems), and the 6th group.
The adoption of all pro-posed features suggests that more evidence is better,as it can be properly modeled by regression.
It seemsgenerally better suited for the variety of semanticphenomena observed in the tests.
Regressors seem601DatasetResultsBL Sys1 Sys2 Sys3MSRvid .299 .821 .821 .802MSRpar .433 .569 .576 .468SMTeuroparl .454 .516 .510 .457surp.OnWN .586 .659surp.SMTnews .390 .471ALL .311 .747 (13) .747 (12) .628 (40)ALLnrm .673 .829 (12) .830 (11) .815 (21)Mean .436 .632 (10) .632 ( 9) .594 (28)Table 3: Results over the challenge test datasetto be robust enough to select the proper features andmake the feature selection step (through collectionspecific cross-validation) useless.
Collection spe-cific training seems useful, as Sys3 achieves lowerresults, basically due to the significant stylistic dif-ferences across the collections.
However, the goodlevel of accuracy achieved over the surprise data sets(between 11% and 17% performance gain with re-spect to the baselines) confirms the large applica-bility of the overall technique: our system in factdoes not depend on any manually coded resource(e.g.
WordNet) nor on any controlled (e.g.
parallelor aligned) corpus.
Future work includes the studyof the learning rate and its correlation with differ-ent and richer similarity functions, e.g.
CDS as in(Annesi et al, 2012).Acknowledgements This research is partiallysupported by the European Community?s Sev-enth Framework Programme (FP7/2007-2013) un-der grant numbers 262491 (INSEARCH).
Manythanks to the reviewers for their valuable sugges-tions.ReferencesJoshua Albrecht and Rebecca Hwa.
2007.
Regression forsentence-level mt evaluation with pseudo references.In Proceedings of ACL, pages 296?303, Prague, CzechRepublic, June.Paolo Annesi, Valerio Storch, and Roberto Basili.
2012.Space projections as distributional models for seman-tic composition.
In CICLing (1), Lecture Notes inComputer Science, pages 323?335.
Springer.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Lee Becker, Martha Palmer, Sarel van Vuuren, andWayne Ward.
2011.
Evaluating questions in context.Fadi Biadsy, Julia Hirschberg, and Elena Filatova.
2008.An unsupervised approach to biography productionusing wikipedia.
In ACL, pages 807?815.Danilo Croce and Daniele Previtali.
2010.
Manifoldlearning for the semi-supervised induction of framenetpredicates: An empirical investigation.
In Proceed-ings of the GEMS 2010 Workshop, pages 7?16, Upp-sala, Sweden.Danilo Croce, Alessandro Moschitti, and Roberto Basili.2011.
Structured lexical similarity via convolutionkernels on dependency trees.
In Proceedings ofEMNLP, Edinburgh, Scotland, UK.Aminul Islam and Diana Inkpen.
2008.
Semantictext similarity using corpus-based word similarity andstring similarity.
ACM Trans.
Knowl.
Discov.
Data,2:10:1?10:25, July.Richard Johansson and Pierre Nugues.
2007.
Semanticstructure extraction using nonprojective dependencytrees.
In Proceedings of SemEval-2007, Prague, CzechRepublic, June 23-24.P.
Koehn.
2002.
Europarl: A multilingual corpus forevaluation of machine translation.
Draft.Thomas K Landauer and Susan T. Dumais.
1997.
A so-lution to platos problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological review, pages 211?240.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In In AAAI06.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InProceedings of ECML?06, pages 318?329.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the ACL.Mehran Sahami and Timothy D. Heilman.
2006.
A web-based kernel function for measuring the similarity ofshort text snippets.
In Proceedings of the 15th inter-national conference on World Wide Web, WWW ?06,pages 377?386, New York, NY, USA.
ACM.Magnus Sahlgren.
2006.
The Word-Space Model.
Ph.D.thesis, Stockholm University.Alex J. Smola and Bernhard Scho?lkopf.
2004.
A tutorialon support vector regression.
Statistics and Comput-ing, 14(3):199?222, August.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer?Verlag, New York.602
