Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 44?54,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsCross-Framework Evaluation for Statistical ParsingReut Tsarfaty Joakim Nivre Evelina AnderssonUppsala University, Box 635, 75126 Uppsala, Swedentsarfaty@stp.lingfil.uu.se,{joakim.nivre,evelina.andersson}@lingfil.uu.seAbstractA serious bottleneck of comparative parserevaluation is the fact that different parserssubscribe to different formal frameworksand theoretical assumptions.
Convertingoutputs from one framework to another isless than optimal as it easily introducesnoise into the process.
Here we present aprincipled protocol for evaluating parsingresults across frameworks based on func-tion trees, tree generalization and edit dis-tance metrics.
This extends a previouslyproposed framework for cross-theory eval-uation and allows us to compare a widerclass of parsers.
We demonstrate the useful-ness and language independence of our pro-cedure by evaluating constituency and de-pendency parsers on English and Swedish.1 IntroductionThe goal of statistical parsers is to recover a for-mal representation of the grammatical relationsthat constitute the argument structure of naturallanguage sentences.
The argument structure en-compasses grammatical relationships between el-ements such as subject, predicate, object, etc.,which are useful for further (e.g., semantic) pro-cessing.
The parses yielded by different parsingframeworks typically obey different formal andtheoretical assumptions concerning how to rep-resent the grammatical relationships in the data(Rambow, 2010).
For example, grammatical rela-tions may be encoded on top of dependency arcsin a dependency tree (Mel?c?uk, 1988), they maydecorate nodes in a phrase-structure tree (Marcuset al 1993; Maamouri et al 2004; Sima?an etal., 2001), or they may be read off of positions ina phrase-structure tree using hard-coded conver-sion procedures (de Marneffe et al 2006).
Thisdiversity poses a challenge to cross-experimentalparser evaluation, namely: How can we evaluatethe performance of these different parsers relativeto one another?Current evaluation practices assume a set ofcorrectly annotated test data (or gold standard)for evaluation.
Typically, every parser is eval-uated with respect to its own formal representa-tion type and the underlying theory which it wastrained to recover.
Therefore, numerical scoresof parses across experiments are incomparable.When comparing parses that belong to differentformal frameworks, the notion of a single goldstandard becomes problematic, and there are twodifferent questions we have to answer.
First, whatis an appropriate gold standard for cross-parserevaluation?
And secondly, how can we alle-viate the differences between formal representa-tion types and theoretical assumptions in order tomake our comparison sound ?
that is, to make surethat we are not comparing apples and oranges?A popular way to address this has been topick one of the frameworks and convert allparser outputs to its formal type.
When com-paring constituency-based and dependency-basedparsers, for instance, the output of constituencyparsers has often been converted to dependencystructures prior to evaluation (Cer et al 2010;Nivre et al 2010).
This solution has vari-ous drawbacks.
First, it demands a conversionscript that maps one representation type to anotherwhen some theoretical assumptions in one frame-work may be incompatible with the other one.In the constituency-to-dependency case, someconstituency-based structures (e.g., coordination44and ellipsis) do not comply with the single headassumption of dependency treebanks.
Secondly,these scripts may be labor intensive to create, andare available mostly for English.
So the evalua-tion protocol becomes language-dependent.In Tsarfaty et al(2011) we proposed a gen-eral protocol for handling annotation discrepan-cies when comparing parses across different de-pendency theories.
The protocol consists of threephases: converting all structures into functiontrees, for each sentence, generalizing the differentgold standard function trees to get their commondenominator, and employing an evaluation mea-sure based on tree edit distance (TED) which dis-cards edit operations that recover theory-specificstructures.
Although the protocol is potentiallyapplicable to a wide class of syntactic represen-tation types, formal restrictions in the procedureseffectively limit its applicability only to represen-tations that are isomorphic to dependency trees.The present paper breaks new ground in theability to soundly compare the accuracy of differ-ent parsers relative to one another given that theyemploy different formal representation types andobey different theoretical assumptions.
Our solu-tion generally confines with the protocol proposedin Tsarfaty et al(2011) but is re-formalized toallow for arbitrary linearly ordered labeled trees,thus encompassing constituency-based as well asdependency-based representations.
The frame-work in Tsarfaty et al(2011) assumes structuresthat are isomorphic to dependency trees, bypass-ing the problem of arbitrary branching.
Here welift this restriction, and define a protocol whichis based on generalization and TED measures tosoundly compare the output of different parsers.We demonstrate the utility of this protocol bycomparing the performance of different parsersfor English and Swedish.
For English, ourparser evaluation across representation types al-lows us to analyze and precisely quantify previ-ously encountered performance tendencies.
ForSwedish we show the first ever evaluation be-tween dependency-based and constituency-basedparsing models, all trained on the Swedish tree-bank data.
All in all we show that our ex-tended protocol, which can handle linearly-ordered labeled trees with arbitrary branch-ing, can soundly compare parsing results acrossframeworks in a representation-independent andlanguage-independent fashion.2 Preliminaries: Relational Schemes forCross-Framework Parse EvaluationTraditionally, different statistical parsers havebeen evaluated using specially designated evalu-ation measures that are designed to fit their repre-sentation types.
Dependency trees are evaluatedusing attachment scores (Buchholz and Marsi,2006), phrase-structure trees are evaluated usingParsEval (Black et al 1991), LFG-based parserspostulate an evaluation procedure based on f-structures (Cahill et al 2008), and so on.
From adownstream application point of view, there is nosignificance as to which formalism was used forgenerating the representation and which learningmethods have been utilized.
The bottom line issimply which parsing framework most accuratelyrecovers a useful representation that helps to un-ravel the human-perceived interpretation.Relational schemes, that is, schemes that en-code the set of grammatical relations that con-stitute the predicate-argument structures of sen-tences, provide an interface to semantic interpre-tation.
They are more intuitively understood than,say, phrase-structure trees, and thus they are alsomore useful for practical applications.
For thesereasons, relational schemes have been repeatedlysingled out as an appropriate level of representa-tion for the evaluation of statistical parsers (Lin,1995; Carroll et al 1998; Cer et al 2010).The annotated data which statistical parsers aretrained on encode these grammatical relationshipsin different ways.
Dependency treebanks providea ready-made representation of grammatical rela-tions on top of arcs connecting the words in thesentence (Ku?bler et al 2009).
The Penn Tree-bank and phrase-structure annotated resources en-code partial information about grammatical rela-tions as dash-features decorating phrase structurenodes (Marcus et al 1993).
Treebanks like Tigerfor German (Brants et al 2002) and Talbankenfor Swedish (Nivre and Megyesi, 2007) explic-itly map phrase structures onto grammatical rela-tions using dedicated edge labels.
The Relational-Realizational structures of Tsarfaty and Sima?an(2008) encode relational networks (sets of rela-tions) projected and realized by syntactic cate-gories on top of ordinary phrase-structure nodes.Function trees, as defined in Tsarfaty et al(2011), are linearly ordered labeled trees in whichevery node is labeled with the grammatical func-45(a) -ROOT- John loves Marysbj objroot ?
rootsbjJohnhdlovesobjMary(b) S-rootNP-sbjNN-hdJohnVP-prdV-hdlovesNP-objNN-hdMary?
rootsbjhdJohnprdhdlovesobjhdMary(c) S{sbj,prd,obj}sbjNP{hd}hdNNJohnprdVlovesobjNP{hd}hdNNMary?
rootsbjhdJohnprdlovesobjhdMaryFigure 1: Deterministic conversion into function trees.The algorithm for extracting a function tree from a de-pendency tree as in (a) is provided in Tsarfaty et al(2011).
For a phrase-structure tree as in (b) we can re-place each node label with its function (dash-feature).In a relational-realizational structure like (c) we can re-move the projection nodes (sets) and realization nodes(phrase labels), which leaves the function nodes intact.tion of the dominated span.
Function trees ben-efit from the same advantages as other relationalschemes, namely that they are intuitive to under-stand, they provide the interface for semantic in-terpretation, and thus may be useful for down-stream applications.
Yet they do not suffer fromformal restrictions inherent in dependency struc-tures, for instance, the single head assumption.For many formal representation types there ex-ists a fully deterministic, heuristics-free, proce-dure mapping them to function trees.
In Figure 1we illustrate some such procedures for a simpletransitive sentence.
Now, while all the structuresat the right hand side of Figure 1 are of the sameformal type (function trees), they have differenttree structures due to different theoretical assump-tions underlying the original formal frameworks.
(t1) rootf1f2w(t2) rootf2f1w(t3) root{f1,f2}wFigure 2: Unary chains in function treesOnce we have converted framework-specificrepresentations into function trees, the problem ofcross-framework evaluation can potentially be re-duced to a cross-theory evaluation following Tsar-faty et al(2011).
The main idea is that onceall structures have been converted into functiontrees, one can perform a formal operation calledgeneralization in order to harmonize the differ-ences between theories, and measure accuratelythe distance of parse hypotheses from the gener-alized gold.
The generalization operation definedin Tsarfaty et al(2011), however, cannot handletrees that may contain unary chains, and thereforecannot be used for arbitrary function trees.Consider for instance (t1) and (t2) in Figure 2.According to the definition of subsumption inTsarfaty et al(2011), (t1) is subsumed by (t2)and vice versa, so the two trees should be identi-cal ?
but they are not.
The interpretation we wishto give to a function tree such as (t1) is that theword w has both the grammatical function f1 andthe grammatical function f2.
This can be graphi-cally represented as a set of labels dominating w,as in (t3).
We call structures such as (t3) multi-function trees.
In the next section we formally de-fine multi-function trees, and then use them to de-velop our protocol for cross-framework and cross-theory evaluation.3 The Proposal: Cross-FrameworkEvaluation with Multi-Function TreesOur proposal is a three-phase evaluation proto-col in the spirit of Tsarfaty et al(2011).
First,we obtain a formal common ground for all frame-works in terms of multi-function trees.
Then weobtain a theoretical common ground by meansof tree-generalization on gold trees.
Finally, wecalculate TED-based scores that discard the costof annotation-specific edits.
In this section, wedefine multi-function trees and update the tree-generalization and TED-based metrics to handlemulti-function trees that reflect different theories.46Figure 3: The Evaluation Protocol.
Different formal frameworks yield different parse and gold formal types.All types are transformed into multi-function trees.
All gold trees enter generalization to yield a new gold foreach sentence.
The different ?
arcs represent the different edit scripts used for calculating the TED-based scores.3.1 Defining Multi-Function TreesAn ordinary function tree is a linearly ordered treeT = (V,A) with yield w1, ..., wn, where internalnodes are labeled with grammatical function la-bels drawn from some set L. We use span(v)and label(v) to denote the yield and label, respec-tively, of an internal node v. A multi-function treeis a linearly ordered tree T = (V,A) with yieldw1, ..., wn, where internal nodes are labeled withsets of grammatical function labels drawn from Land where v 6= v?
implies span(v) 6= span(v?
)for all internal nodes v, v?.
We use labels(v) todenote the label set of an internal node v.We interpret multi-function trees as encodingsets of functional constraints over spans in func-tion trees.
Each node v in a multi-function treerepresents a constraint of the form: for eachl ?
labels(v), there should be a node v?
in thefunction tree such that span(v) = span(v?)
andlabel(v?)
= l. Whenever we have a conversion forfunction trees, we can efficiently collapse theminto multi-function trees with no unary produc-tions, and with label sets labeling their nodes.Thus, trees (t1) and (t2) in Figure 2 would bothbe mapped to tree (t3), which encodes the func-tional constraints encoded in either of them.For dependency trees, we assume the conver-sion to function trees defined in Tsarfaty et al(2011), where head daughters always get the la-bel ?hd?.
For PTB style phrase-structure trees, wereplace the phrase-structure labels with functionaldash-features.
In relational-realization structureswe remove projection and realization nodes.
De-terministic conversions exist also for Tiger styletreebanks and frameworks such as LFG, but wedo not discuss them here.11All the conversions we use are deterministic and aredefined in graph-theoretic and language-independent terms.We make them available at http://stp.lingfil.uu.se/?tsarfaty/unipar/index.html.3.2 Generalizing Multi-Function TreesOnce we obtain multi-function trees for all thedifferent gold standard representations in the sys-tem, we feed them to a generalization operationas shown in Figure 3.
The goal of this opera-tion is to provide a consensus gold standard thatcaptures the linguistic structure that the differentgold theories agree on.
The generalization struc-tures are later used as the basis for the TED-basedevaluation.
Generalization is defined by means ofsubsumption.
A multi-function tree subsumes an-other one if and only if all the constraints definedby the first tree are also defined by the second tree.So, instead of demanding equality of labels as inTsarfaty et al(2011), we demand set inclusion:T-Subsumption, denoted vt, is a relationbetween multi-function trees that indicatesthat a tree pi1 is consistent with and moregeneral than tree pi2.
Formally: pi1 vt pi2iff for every node n ?
pi1 there exists a nodem ?
pi2 such that span(n) = span(m) andlabels(n) ?
labels(m).T-Unification, denoted unionsqt, is an operationthat returns the most general tree structurethat contains the information from both inputtrees, and fails if such a tree does not exist.Formally: pi1 unionsqt pi2 = pi3 iff pi1 vt pi3 andpi2 vt pi3, and for all pi4 such that pi1 vt pi4and pi2 vt pi4 it holds that pi3 vt pi4.T-Generalization, denoted ut, is an opera-tion that returns the most specific tree thatis more general than both trees.
Formally,pi1utpi2 = pi3 iff pi3 vt pi1 and pi3 vt pi2, andfor every pi4 such that pi4 vt pi1 and pi4 vt pi2it holds that pi4 vt pi3.The generalization tree contains all nodes that ex-ist in both trees, and for each node it is labeled by47the intersection of the label sets dominating thesame span in both trees.
The unification tree con-tains nodes that exist in one tree or another, andfor each span it is labeled by the union of all labelsets for this span in either tree.
If we generalizetwo trees and one tree has no specification for la-bels over a span, it does not share anything withthe label set dominating the same span in the othertree, and the label set dominating this span in thegeneralized tree is empty.
If the trees do not agreeon any label for a particular span, the respectivenode is similarly labeled with an empty set.
Whenwe wish to unify theories, then an empty set overa span is unified with any other set dominating thesame span in the other tree, without altering it.Digression: Using Unification to Merge Infor-mation From Different Treebanks In Tsarfatyet al(2011), only the generalization operationwas used, providing the common denominator ofall the gold structures and serving as a commonground for evaluation.
The unification operationis useful for other NLP tasks, for instance, com-bining information from two different annotationschemes or enriching one annotation scheme withinformation from a different one.
In particular,we can take advantage of the new framework toenrich the node structure reflected in one theorywith grammatical functions reflected in an anno-tation scheme that follows a different theory.
Todo so, we define the Tree-Labeling-Unificationoperation on multi-function trees.TL-Unification, denoted unionsqtl, is an opera-tion that returns a tree that retains the struc-ture of the first tree and adds labels that ex-ist over its spans in the second tree.
For-mally: pi1 unionsqtl pi2 = pi3 iff for every noden ?
pi1 there exists a node m ?
pi3 suchthat span(m) = span(n) and labels(m) =labels(n) ?
labels(pi2, span(n)).Where labels(pi2, span(n)) is the set of labels ofthe node with yield span(n) in pi2 if such a nodeexists and ?
otherwise.
We further discuss the TL-Unification and its use for data preparation in ?4.3.3 TED Measures for Multi-Function TreesThe result of the generalization operation pro-vides us with multi-function trees for each of thesentences in the test set representing sets of con-straints on which the different gold theories agree.We would now like to use distance-based met-rics in order to measure the gap between the goldand predicted theories.
The idea behind distance-based evaluation in Tsarfaty et al(2011) is thatrecording the edit operations between the nativegold and the generalized gold allows one to dis-card their cost when computing the cost of a parsehypothesis turned into the generalized gold.
Thismakes sure that different parsers do not get penal-ized, or favored, due to annotation specific deci-sions that are not shared by other frameworks.The problem is now that TED is undefined withrespect to multi-function trees because it cannothandle complex labels.
To overcome this, weconvert multi-function trees into sorted functiontrees, which are simply function trees in whichany label set is represented as a unary chain ofsingle-labeled nodes, and the nodes are sorted ac-cording to the canonical order of their labels.2 Incase of an empty set, a 0-length chain is created,that is, no node is created over this span.
Sortedfunction trees prevent reordering nodes in a chainin one tree to fit the order in another tree, since itwould violate the idea that the set of constraintsover a span in a multi-function tree is unordered.The edit operations we assume are add-node(l, i, j) and delete-node(l, i, j) where l ?
Lis a grammatical function label and i < j definethe span of a node in the tree.
Insertion into aunary chain must confine with the canonical orderof the labels.
Every operation is assigned a cost.An edit script is a sequence of edit operations thatturns a function tree pi1 into pi2, that is:ES(pi1, pi2) = ?e1, .
.
.
, ek?Since all operations are anchored in spans, the se-quence can be determined to have a unique orderof traversing the tree (say, DFS).
Different editscripts then only differ in their set of operationson spans.
The edit distance problem is finding theminimal cost script, that is, one needs to solve:ES?
(pi1, pi2) = minES(pi1,pi2)?e?ES(pi1,pi2)cost(e)In the current setting, when using only add anddelete operations on spans, there is only one editscript that corresponds to the minimal edit cost.So, finding the minimal edit script entails findinga single set of operations turning pi1 into pi2.2The ordering can be alphabetic, thematic, etc.48We can now define ?
for the ith framework, asthe error of parsei relative to its native gold stan-dard goldi and to the generalized gold gen. Thisis the edit cost minus the cost of the script turningparsei into gen intersected with the script turninggoldi into gen.
The underlying intuition is thatif an operation that was used to turn parsei intogen is used to discard theory-specific informationfrom goldi, its cost should not be counted as error.?
(parsei, goldi, gen) = cost(ES?
(parsei, gen))?cost(ES?
(parsei, gen) ?
ES?
(goldi, gen))In order to turn distance measures into parse-scores we now normalize the error relative to thesize of the trees and subtract it from a unity.
Sothe Sentence Score for parsing with framework iis:score(parsei, goldi, gen) =1??
(parsei, goldi,gen)|parsei|+ |gen|Finally, Test-Set Average is defined by macro-avaraging over all sentences in the test-set:1?
?|testset|j=1 ?
(parseij , goldij , genj)?|testset|j=1 |parseij |+ |genj |This last formula represents the TEDEVAL metricthat we use in our experiments.A Note on System Complexity Conversion ofa dependency or a constituency tree into a func-tion tree is linear in the size of the tree.
Ourimplementation of the generalization and unifica-tion operation is an exact, greedy, chart-based al-gorithm that runs in polynomial time (O(n2) inn the number of terminals).
The TED softwarethat we utilize builds on the TED efficient algo-rithm of Zhang and Shasha (1989) which runs inO(|T1||T2|min(d1, n1)min(d2, n2)) time wheredi is the tree degree (depth) and ni is the numberof terminals in the respective tree (Bille, 2005).4 ExperimentsWe validate our cross-framework evaluation pro-cedure on two languages, English and Swedish.For English, we compare the performance oftwo dependency parsers, MaltParser (Nivre et al2006) and MSTParser (McDonald et al 2005),and two constituency-based parsers, the Berkeleyparser (Petrov et al 2006) and the Brown parser(Charniak and Johnson, 2005).
All experimentsuse Penn Treebank (PTB) data.
For Swedish,we compare MaltParser and MSTParser with twovariants of the Berkeley parser, one trained onphrase structure trees, and one trained on a vari-ant of the Relational-Realizational representationof Tsarfaty and Sima?an (2008).
All experimentsuse the Talbanken Swedish Treebank (STB) data.4.1 English Cross-Framework EvaluationWe use sections 02?21 of the WSJ Penn Tree-bank for training and section 00 for evaluation andanalysis.
We use two different native gold stan-dards subscribing to different theories of encodinggrammatical relations in tree structures:?
THE DEPENDENCY-BASED THEORY is thetheory encoded in the basic Stanford Depen-dencies (SD) scheme.
We obtain the set ofbasic stanford dependency trees using thesoftware of de Marneffe et al(2006) andtrain the dependency parsers directly on it.?
THE CONSTITUENCY-BASED THEORY isthe theory reflected in the phrase-structurerepresentation of the PTB (Marcus et al1993) enriched with function labels compat-ible with the Stanford Dependencies (SD)scheme.
We obtain trees that reflect thistheory by TL-Unification of the PTB multi-function trees with the SD multi-functiontrees (PTBunionsqtlSD) as illustrated in Figure 4.The theory encoded in the multi-function treescorresponding to SD is different from the oneobtained by our TL-Unification, as may be seenfrom the difference between the flat SD multi-function tree and the result of the PTBunionsqtlSD inFigure 4.
Another difference concerns coordina-tion structures, encoded as binary branching treesin SD and as flat productions in the PTBunionsqtlSD.Such differences are not only observable but alsoquantifiable, and using our redefined TED metricthe cross-theory overlap is 0.8571.The two dependency parsers were trained usingthe same settings as in Tsarfaty et al(2011), usingSVMTool (Gime?nez and Ma`rquez, 2004) to pre-dict part-of-speech tags at parsing time.
The twoconstituency parsers were used with default set-tings and were allowed to predict their own part-of-speech tags.
We report three different evalua-tion metrics for the different experiments:49(PTB) SNPNNJohnVPVlovesNPNNMary?John lovesMary?
??John?
?loves?Mary(SD) -ROOT- John loves Marysbj objroot ?
rootsbjJohnhdlovesobjMary?
{root}{sbj}John{hd}loves{obj}Mary(PTB) unionsqtl (SD) = {root}{sbj}John?
{hd}loves{obj}MaryFigure 4: Conversion of PTB and SD tree to multi-function trees, followed by TL-Unification of the trees.Note that some PTB nodes remain without an SD label.?
LAS/UAS (Buchholz and Marsi, 2006)?
PARSEVAL (Black et al 1991)?
TEDEVAL as defined in Section 3We use LAS/UAS for dependency parsers thatwere trained on the same dependency theory.
Weuse ParseEval to evaluate phrase-structure parsersthat were trained on PTB trees in which dash-features and empty traces are removed.
Weuse our implementation of TEDEVAL to evaluateparsing results across all frameworks under twodifferent scenarios:3 TEDEVAL SINGLE evalu-ates against the native gold multi-function trees.TEDEVAL MULTIPLE evaluates against the gen-eralized (cross-theory) multi-function trees.
Un-labeled TEDEVAL scores are obtained by sim-ply removing all labels from the multi-functionnodes, and using unlabeled edit operations.
Wecalculate pairwise statistical significance using ashuffling test with 10K iterations (Cohen, 1995).Tables 1 and 2 present the results of our cross-framework evaluation for English Parsing.
In theleft column of Table 1 we report ParsEval scoresfor constituency-based parsers.
As expected, F-Scores for the Brown parser are higher than theF-Scores of the Berkeley parser.
F-Scores arehowever not applicable across frameworks.
Inthe rightmost column of Table 1 we report theLAS/UAS results for all parsers.
If a parser yields3Our TedEval software can be downloaded athttp://stp.lingfil.uu.se/?tsarfaty/unipar/download.html.a constituency tree, it is converted to and evalu-ated on SD.
Here we see that MST outperformsMalt, though the differences for labeled depen-dencies are insignificant.
We also observe here afamiliar pattern from Cer et al(2010) and others,where the constituency parsers significantly out-perform the dependency parsers after conversionof their output into dependencies.The conversion to SD allows one to compareresults across formal frameworks, but not with-out a cost.
The conversion introduces a set of an-notation specific decisions which may introducea bias into the evaluation.
In the middle columnof Table 1 we report the TEDEVAL metrics mea-sured against the generalized gold standard for allparsing frameworks.
We can now confirm thatthe constituency-based parsers significantly out-perform the dependency parsers, and that this isnot due to specific theoretical decisions which areseen to affect LAS/UAS metrics (Schwartz et al2011).
For the dependency parsers we now seethat Malt outperforms MST on labeled dependen-cies slightly, but the difference is insignificant.The fact that the discrepancy in theoretical as-sumptions between different frameworks indeedaffects the conversion-based evaluation procedureis reflected in the results we report in Table 2.Here the leftmost and rightmost columns reportTEDEVAL scores against the own native gold(SINGLE) and the middle column against the gen-eralized gold (MULTIPLE).
Had the theoriesfor SD and PTBunionsqtlSD been identical, TEDEVALSINGLE and TEDEVAL MULTIPLE would havebeen equal in each line.
Because of theoreticaldiscrepancies, we see small gaps in parser perfor-mance between these cases.
Our protocol ensuresthat such discrepancies do not bias the results.4.2 Cross-Framework Swedish ParsingWe use the standard training and test sets of theSwedish Treebank (Nivre and Megyesi, 2007)with two gold standards presupposing differenttheories:?
THE DEPENDENCY-BASED THEORY is thedependency version of the Swedish Tree-bank.
All trees are projectivized (STB-Dep).?
THE CONSTITUENCY-BASED THEORY isthe standard Swedish Treebank with gram-matical function labels on the edges of con-stituency structures (STB).50Formalism PS Trees MF Trees Dep TreesTheory PTB unionsqlt SD (PTB unionsqlt SD) SDut SDMetrics PARSEVAL TEDEVAL ATTSCORESMALT N/A U: 0.9525L: 0.9088U: 0.8962L: 0.8772MST N/A U: 0.9549L: 0.9049U: 0.9059L: 0.8795BERKELEY F-Scores0.9096U: 0.9677L: 0.9227U: 0.9254L: 0.9031BROWN F-Scores0.9129U: 0.9702L: 0.9264U: 0.9289L: 0.9057Table 1: English cross-framework evaluation: Threemeasures as applicable to the different schemes.
Bold-face scores are highest in their column.
Italic scoresare the highest for dependency parsers in their column.Formalism PS Trees MF Trees Dep TreesTheory PTB unionsqlt SD (PTB unionsqlt SD) SDut SDMetrics TEDEVAL TEDEVAL TEDEVALSINGLE MULTIPLE SINGLEMALT N/A U: 0.9525L: 0.9088U: 0.9524L: 0.9186MST N/A U: 0.9549L: 0.9049U: 0.9548L: 0.9149BERKELEY U: 0.9645L: 0.9271U: 0.9677L: 0.9227U: 0.9649L: 0.9324BROWN U: 0.9667L: 0.9301U: 9702L: 9264U: 0.9679L: 0.9362Table 2: English cross-framework evaluation: TEDE-VAL scores against gold and generalized gold.
Bold-face scores are highest in their column.
Italic scoresare highest for dependency parsers in their column.Because there are no parsers that can out-put the complete STB representation includingedge labels, we experiment with two variants ofthis theory, one which is obtained by simply re-moving the edge labels and keeping only thephrase-structure labels (STB-PS) and one whichis loosely based on the Relational-Realizationalscheme of Tsarfaty and Sima?an (2008) but ex-cludes the projection set nodes (STB-RR).
RRtrees only add function nodes to PS trees, andit holds that STB-PSutSTB-RR=STB-PS.
Theoverlap between the theories expressed in multi-function trees originating from STB-Dep andSTB-RR is 0.7559.
Our evaluation protocol takesinto account such discrepancies while avoidingbiases that may be caused due to these differences.We evaluate MaltParser, MSTParser and twoversions of the Berkeley parser, one trained onSTB-PS and one trained on STB-RR.
We usepredicted part-of-speech tags for the dependencyFormalism PS Trees MF Trees Dep TreesTheory STB STB ut Dep DepMetrics PARSEVAL TEDEVAL ATTSCOREMALT N/A U: 0.9266L: 0.8225U: 0.8298L: 0.7782MST N/A U: 0.9275L: 0.8121U: 0.8438L: 0.7824BKLY/STB-RR F-Score0.7914U: 0.9281L: 0.7861 N/ABKLY/STB-PS F-Score0.7855 N/A N/ATable 3: Swedish cross-framework evaluation: Threemeasures as applicable to the different schemes.
Bold-face scores are the highest in their column.Formalism PS Trees MF Trees Dep TreesTheory STB STB ut Dep DepMetrics TEDEVAL TEDEVAL TEDEVALSINGLE MULTIPLE SINGLEMALT N/A U: 0.9266L: 0.8225U: 0.9264L: 0.8372MST N/A U: 0.9275L: 0.8121U: 0.9272L: 0.8275BKLY-STB-RR U: 0.9239L: 0.7946U: 0.9281L: 0.7861 N/ATable 4: Swedish cross-framework evaluation: TEDE-VAL scores against the native gold and the generalizedgold.
Boldface scores are the highest in their column.parsers, using the HunPoS tagger (Megyesi,2009), but let the Berkeley parser predict its owntags.
We use the same evaluation metrics and pro-cedures as before.
Prior to evaluating RR treesusing ParsEval we strip off the added functionnodes.
Prior to evaluating them using TedEval westrip off the phrase-structure nodes.Tables 3 and 4 summarize the parsing resultsfor the different Swedish parsers.
In the leftmostcolumn of table 3 we present the constituency-based evaluation measures.
Interestingly, theBerkeley RR instantiation performs better thanwhen training the Berkeley parser on PS trees.These constituency-based scores however have alimited applicability, and we cannot use them tocompare constituency and dependency parsers.
Inthe rightmost column of Table 3 we report theLAS/UAS results for the two dependency parsers.Here we see higher performance demonstrated byMST on both labeled and unlabeled dependen-cies, but the differences on labeled dependenciesare insignificant.
Since there is no automatic pro-cedure for converting bare-bone phrase-structureSwedish trees to dependency trees, we cannot use51LAS/UAS to compare across frameworks, and weuse TEDEVAL for cross-framework evaluation.Training the Berkeley parser on RR trees whichencode a mapping of PS nodes to grammaticalfunctions allows us to compare parse results fortrees belonging to the STB theory with trees obey-ing the STB-Dep theory.
For unlabeled TEDE-VAL scores, the dependency parsers perform at thesame level as the constituency parser, though thedifference is insignificant.
For labeled TEDEVALthe dependency parsers significantly outperformthe constituency parser.
When considering onlythe dependency parsers, there is a small advantagefor Malt on labeled dependencies, and an advan-tage for MST on unlabeled dependencies, but thelatter is insignificant.
This effect is replicated inTable 4 where we evaluate dependency parsers us-ing TEDEVAL against their own gold theories.
Ta-ble 4 further confirms that there is a gap betweenthe STB and the STB-Dep theories, reflected inthe scores against the native and generalized gold.5 DiscussionWe presented a formal protocol for evaluatingparsers across frameworks and used it to soundlycompare parsing results for English and Swedish.Our approach follows the three-phase protocol ofTsarfaty et al(2011), namely: (i) obtaining a for-mal common ground for the different representa-tion types, (ii) computing the theoretical commonground for each test sentence, and (iii) countingonly what counts, that is, measuring the distancebetween the common ground and the parse treewhile discarding annotation-specific edits.A pre-condition for applying our protocol is theavailability of a relational interpretation of trees inthe different frameworks.
For dependency frame-works this is straightforward, as these relationsare encoded on top of dependency arcs.
For con-stituency trees with an inherent mapping of nodesonto grammatical relations (Merlo and Musillo,2005; Gabbard et al 2006; Tsarfaty and Sima?an,2008), a procedure for reading relational schemesoff of the trees is trivial to implement.For parsers that are trained on and parse intobare-bones phrase-structure trees this is not so.Reading off the relational structure may be morecostly and require interjection of additional theo-retical assumptions via manually written scripts.Scripts that read off grammatical relations basedon tree positions work well for configurationallanguages such as English (de Marneffe et al2006) but since grammatical relations are re-flected differently in different languages (Postaland Perlmutter, 1977; Bresnan, 2000), a proce-dure to read off these relations in a language-independent fashion from phrase-structure treesdoes not, and should not, exist (Rambow, 2010).The crucial point is that even when using ex-ternal scripts for recovering a relational schemefor phrase-structure trees, our protocol has a clearadvantage over simply scoring converted trees.Manually created conversion scripts alter the the-oretical assumptions inherent in the trees and thusmay bias the results.
Our generalization operationand three-way TED make sure that theory-specificidiosyncrasies injected through such scripts donot lead to over-penalizing or over-creditingtheory-specific structural variations.Certain linguistic structures cannot yet be eval-uated with our protocol because of the strict as-sumption that the labeled spans in a parse form atree.
In the future we plan to extend the protocolfor evaluating structures that go beyond linearly-ordered trees in order to allow for non-projectivetrees and directed acyclic graphs.
In addition, weplan to lift the restriction that the parse yield isknown in advance, in order to allow for evalua-tion of joint parse-segmentation hypotheses.6 ConclusionWe developed a protocol for comparing parsingresults across different theories and representa-tion types which is framework-independent in thesense that it can accommodate any formal syntac-tic framework that encodes grammatical relations,and it is language-independent in the sense thatthere is no language specific knowledge encodedin the procedure.
As such, this protocol is ad-equate for parser evaluation in cross-frameworkand cross-language tasks and parsing competi-tions, and using it across the board is expectedto open new horizons in our understanding of thestrengths and weaknesses of different parsers inthe face of different theories and different data.Acknowledgments We thank David McClosky,Marco Khulmann, Yoav Goldberg and threeanonymous reviewers for useful comments.
Wefurther thank Jennifer Foster for the Brown parsesand parameter files.
This research is partly fundedby the Swedish National Science Foundation.52ReferencesPhilip Bille.
2005.
A survey on tree edit distance andrelated.
problems.
Theoretical Computer Science,337:217?239.Ezra Black, Steven P. Abney, D. Flickenger, Clau-dia Gdaniec, Ralph Grishman, P. Harrison, Don-ald Hindle, Robert Ingria, Frederick Jelinek, Ju-dith L. Klavans, Mark Liberman, Mitchell P. Mar-cus, Salim Roukos, Beatrice Santorini, and TomekStrzalkowski.
1991.
A procedure for quantitativelycomparing the syntactic coverage of English gram-mars.
In Proceedings of the DARPA Workshop onSpeech and Natural Language, pages 306?311.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The Tigertreebank.
In Proceedings of TLT.Joan Bresnan.
2000.
Lexical-Functional Syntax.Blackwell.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL-X, pages 149?164.Aoife Cahill, Michael Burke, Ruth O?Donovan, StefanRiezler, Josef van Genabith, and Andy Way.
2008.Wide-coverage deep statistical parsing using auto-matic dependency structure annotation.
Computa-tional Linguistics, 34(1):81?124.John Carroll, Edward Briscoe, and Antonio Sanfilippo.1998.
Parser evaluation: A survey and a new pro-posal.
In Proceedings of LREC, pages 447?454.Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-rafsky, and Christopher D. Manning.
2010.
Pars-ing to Stanford Dependencies: Trade-offs betweenspeed and accuracy.
In Proceedings of LREC.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of ACL.Paul Cohen.
1995.
Empirical Methods for ArtificialIntelligence.
The MIT Press.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC, pages 449?454.Ryan Gabbard, Mitchell Marcus, and Seth Kulick.2006.
Fully parsing the Penn treebank.
In Proceed-ing of HLT-NAACL, pages 184?191.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool:A general POS tagger generator based on supportvector machines.
In Proceedings of LREC.Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Number 2 in SynthesisLectures on Human Language Technologies.
Mor-gan & Claypool Publishers.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedingsof IJCAI-95, pages 1420?1425.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic treebank:Building a large-scale annotated Arabic corpus.
InProceedings of NEMLAR International Conferenceon Arabic Language Resources and Tools.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19:313?330.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In HLT ?05:Proceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, pages 523?530, Morristown, NJ,USA.
Association for Computational Linguistics.Beata Megyesi.
2009.
The open source tagger Hun-PoS for Swedish.
In Proceedings of the 17th NordicConference of Computational Linguistics (NODAL-IDA), pages 239?241.Igor Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.Paola Merlo and Gabriele Musillo.
2005.
Accuratefunction parsing.
In Proceedings of EMNLP, pages620?627.Joakim Nivre and Beata Megyesi.
2007.
Bootstrap-ping a Swedish Treebank using cross-corpus har-monization and annotation projection.
In Proceed-ings of TLT.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: A data-driven parser-generator for de-pendency parsing.
In Proceedings of LREC, pages2216?2219.Joakim Nivre, Laura Rimell, Ryan McDonald, andCarlos Go?mez-Rodr??guez.
2010.
Evaluation of de-pendency parsers on unbounded dependencies.
InProceedings of COLING, pages 813?821.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of ACL.Paul M. Postal and David M. Perlmutter.
1977.
To-ward a universal characterization of passivization.In Proceedings of the 3rd Annual Meeting of theBerkeley Linguistics Society, pages 394?417.Owen Rambow.
2010.
The simple truth about de-pendency and phrase structure representations: Anopinion piece.
In Proceedings of HLT-ACL, pages337?340.Roy Schwartz, Omri Abend, Roi Reichart, and AriRappoport.
2011.
Neutralizing linguistically prob-lematic annotations in unsupervised dependencyparsing evaluation.
In Proceedings of ACL, pages663?672.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,and Noa Nativ.
2001.
Building a Tree-Bank forModern Hebrew Text.
In Traitement Automatiquedes Langues.53Reut Tsarfaty and Khalil Sima?an.
2008.
Relational-Realizational parsing.
In Proceedings of CoLing.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2011.
Evaluating dependency parsing: Robust andheuristics-free cross-framework evaluation.
In Pro-ceedings of EMNLP.Kaizhong Zhang and Dennis Shasha.
1989.
Sim-ple fast algorithms for the editing distance betweentrees and related problems.
In SIAM Journal ofComputing, volume 18, pages 1245?1262.54
