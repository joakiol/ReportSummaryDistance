Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 74?80,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsImplicatures and Nested Beliefs in Approximate Decentralized-POMDPsAdam Vogel, Christopher Potts, and Dan JurafskyStanford UniversityStanford, CA, USA{acvogel,cgpotts,jurafsky}@stanford.eduAbstractConversational implicatures involve rea-soning about multiply nested belief struc-tures.
This complexity poses significantchallenges for computational models ofconversation and cognition.
We show thatagents in the multi-agent Decentralized-POMDP reach implicature-rich interpreta-tions simply as a by-product of the waythey reason about each other to maxi-mize joint utility.
Our simulations involvea reference game of the sort studied inpsychology and linguistics as well as adynamic, interactional scenario involvingimplemented artificial agents.1 IntroductionGricean conversational implicatures (Grice, 1975)are inferences that listeners make in order toreconcile the speaker?s linguistic behavior withthe assumption that the speaker is cooperative.As Grice conceived of them, implicatures cru-cially involve reasoning about multiply-nested be-lief structures: roughly, for p to count as an impli-cature, the speaker must believe that the listenerwill infer that the speaker believes p. This com-plexity makes implicatures an important testingground for models of conversation and cognition.Implicatures have received considerable atten-tion in the context of simple reference games inwhich the listener uses the speaker?s utteranceto try to identify the speaker?s intended referent(Rosenberg and Cohen, 1964; Clark and Wilkes-Gibbs, 1986; Dale and Reiter, 1995; DeVault andStone, 2007; Krahmer and van Deemter, 2012).Many implicature patterns can be embedded inthese games using specific combinations of poten-tial referents and message sets.
The paradigm hasproven fruitful not only for evaluating computa-tional models (Golland et al, 2010; Degen andFranke, 2012; Frank and Goodman, 2012; Rohdeet al, 2012; Bergen et al, 2012) but also for study-ing children?s pragmatic abilities without implic-itly assuming they have mastered challenging lin-guistic structures (Stiller et al, 2011).In this paper, we extend these results beyondsimple reference games to full decision-problemsin which the agents reason about language and ac-tion together over time.
To do this, we use the De-centralized Partially Observable Markov DecisionProcess (Dec-POMDP) to implement agents thatare capable of manipulating the multiply-nestedbelief structures required for implicature calcula-tion.
Optimal decision making in Dec-POMDPsis NEXP complete, so we employ the single-agentPOMDP approximation of Vogel et al (2013).We show that agents in the Dec-POMDP reachimplicature-rich interpretations simply as a by-product of the way they reason about each otherto maximize joint utility.
Our simulations involvea reference game and a dynamic, interactional sce-nario involving implemented artificial agents.2 Decision-Theoretic CommunicationThe Decentralized Partially Observable MarkovDecision Process (Dec-POMDP) (Bernstein etal., 2002) is a multi-agent generalization of thePOMDP, where agents act to maximize a sharedutility function.
Formally, a Dec-POMDP con-sists of a tuple (S,A,O,R, T,?, b0, ?).
S is afinite set of states, A is the set of actions, O isthe set of observations, and T (s?|a1, a2, s) is thetransition distribution which determines what ef-fect the joint action (a1, a2) has on the state of theworld.
The true state s ?
S is not observable tothe agents, who must utilize observations o ?
O,which are emitted after each action according tothe observation distribution ?
(o1, o2|s?, a).
Thereward functionR(s, a1, a2) represents the goal ofthe agents, who act to maximize expected reward.Lastly, b0 ?
?
(S) is the initial belief state and74?
?
[0, 1) is the discount factor.The true state of the world s ?
S is not ob-servable to either agent.
In single-agent POMDPs,agents maintain a belief state b(s) ?
?
(S), whichis a distribution over states.
Agents acting in Dec-POMDPs must take into account not only theirbeliefs about the state of the world, but also thebeliefs of their partners, leading to nested beliefstates.
In the model presented here, our agentmodels the other agent?s beliefs about the state ofthe world, and assumes that the other agent doesnot take into account our own beliefs, a commonapproach (Gmytrasiewicz and Doshi, 2005).Agents make decisions according toa policy pii : ?
(S) ?
A which max-imizes the discounted expected reward?
?t=0 ?tE[R(st, at1, at2)|b0, pi1, pi2].
Usingthe assumption that the other agent tracks one lesslevel of belief, we can solve for the other agent?spolicy p?i, which allows us to estimate his actionsand beliefs over time.
To construct policies,we use Perseus (Spaan and Vlassis, 2005), apoint-based value iteration algorithm.Even tracking just one level of nested beliefsquickly leads to a combinatorial explosion in thenumber of belief states the other agent might have.This causes decision making in Dec-POMDPs tobe NEXP complete, limiting their application toproblems with only a handful of states (Bernsteinet al, 2002).
To ameliorate this difficulty, weuse the method of Vogel et al (2013), which cre-ates a single-agent approximation to the full Dec-POMDP.
To form this single-agent POMDP, weaugment the state space to be S ?
S, where thesecond set of state variables allows us to modelthe other agent?s beliefs.
We maintain a pointestimate b?
of the other agent?s beliefs, whichis formed by summing out observations O thatthe other player might have received.
To ac-complish this, we factor the transition distribu-tion into two terms: T ((s?, s??
)|a, p?i(s?
), (s, s?))
=T?
(s?
?|s?, a, p?i(s?
), (s, s?
))T (s?|a, p?i(s?
), (s, s?)).
Thisobservation marginalization can be folded into thetransition distribution T?
(s?
?|s?, a, p?i(s?
), (s, s?)):T?
(s?
?| s?, a, p?i(s?
), (s, s?))
= Pr(s?
?|s?, a, p?i(s?
), (s, s?))=?o?
?O( ?(o?|s?
?, a, p?i(s?
))T (s?
?|a, p?i(s?
), s?)?s???
?(o?|s??
?, a, p?i(s?
))T (s??
?|a, p?i(s?
), s?)?
?
(o?|s?, a, p?i(s?
)))(1)Communication is treated as another type of ob-servation, with messages coming from a finite setM .
Each message m ?
M has the semanticsPr(s|m), which represents the probability that theworld is in state s ?
S given that m is true.
Mes-sages m received from a partner are combinedwith perceptual observations o ?
O, to form ajoint observation (m, o).A literal listener, denoted L, interprets mes-sages according to this semantics, without takinginto account the beliefs of the speaker.
L assumesthat the perceptual observations and messages areconditionally independent given the state of theworld.
Using Bayes?
rule, the literal listener?s jointobservation/message distribution isPr((o,m)|s, s?, a) = ?
(o|s?, a) Pr(m|s)= ?
(o|s?, a) Pr(s|m) Pr(m)?m?
?M Pr(s|m?)
Pr(m?
)(2)The Pr(m) prior over messages can be estimatedfrom corpus data, but we use a uniform prior forsimplicity.A literal speaker, denoted S, produces mes-sages according to the most descriptive term:piS(s) = arg maxm?Mp(s|m).
(3)The literal speaker does not model the beliefs ofthe listener.To interpret implicatures, a level-one lis-tener, denoted L(S), models the beliefs a literalspeaker must have had to produce an utterance:Pr(m|s) = 1[p?iS(s) = m], where p?iS is the level-one listener?s estimate of the speaker?s policy.
Inthis setting, we denote the level-one listener?s es-timate of the speaker?s belief as s?, yielding the be-lief update equationPr((o,m)|(s, s?
), (s?, s??
), a, p?iS(s?))
=?
(o|s?, a)1[p?iS(s?)
= m] (4)The literal semantics of messages is not explicitlyincluded in the level-one listener?s belief update.Instead, when he solves for the literal speaker?spolicy p?iS , the meaning of a message is the set ofbeliefs that would lead the literal speaker to pro-duce the utterance.A level-one speaker, S(L), produces utterancesto influence a literal listener, and a level-two lis-tener, L(S(L)), uses two levels of belief nesting tointerpret utterances as the beliefs that a level-onespeaker might have to produce that utterance.
Ateach level of nesting, we apply the marginalized75r1 0 0 1r2 0 1 1r3 1 1 0hatglassesmustacher1 r2 r3(a) Scenario.Message r1 r2 r3moustache 12 12 0glasses 0 12 12hat 0 0 1(b) Literal interpretations.Message r1 r2 r3moustache 1 0 0glasses 0 1 0hat 0 0 1(c) Implicature-rich interpretations.Figure 1: A simple reference game.
The matricesgive distributions Pr(t = ri|utterance)belief-state approach of (Vogel et al, 2013), aug-menting the state space with another copy of theunderlying world state space, where the new copyrepresents the next level of belief.
For instance, theL(S(L)) agent will make decisions in the S?S?Sspace.
For an L(S(L)) state (s, s?, s?
), s is the truestate of the world, s?
is the speaker?s belief of thestate of the world, and s?
is the speaker?s belief ofthe listener?s beliefs.
In the next two sections weshow how a level-one and level-two listener inferimplicatures.3 Reference Game ImplicaturesFig.
1a is the scenario for a reference game of thesort pioneered by Rosenberg and Cohen (1964)and Dale and Reiter (1995).
The potential refer-ents are r1, r2, and r3.
Speakers use a restrictedvocabulary consisting of three messages: ?mous-tache?, ?glasses?, and ?hat?.
The speaker is as-signed a referent ri (hidden from the listener) andproduces a message on that basis.
The speaker andlistener share the goal of having the listener iden-tify the speaker?s intended referent ri.Fig.
1b depicts the literal interpretations forthis game.
It looks like the listener?s chancesof success are low.
Only ?hat?
refers unambigu-ously.
However, the language and scenario fa-cilitate scalar implicature (Horn, 1972; Harnish,1979; Gazdar, 1979).
Briefly, the scalar implica-ture pattern is that a speaker who is knowledgeableabout the relevant domain will choose a commu-nicatively weak utterance U over a communica-tively stronger utterance U ?
iff U ?
is false (assum-ing U and U ?
are relevant).
The required sense ofcommunicative strength encompasses logical en-tailments as well as more particularized pragmaticpartial orders (Hirschberg, 1985).In our scenario, ?hat?
is stronger than ?glasses?
:the referents wearing a hat are a proper subsetof those wearing glasses.
Thus, given the play-ers?
goal, if the speaker says ?glasses?, the lis-tener should draw the scalar implicature that ?hat?is false.
Thus, ?glasses?
comes to unambiguouslyrefer to r2 (Fig.
1c, line 2).
Similarly, though?moustache?
and ?glasses?
do not literally stand inthe specific?general relationship needed for scalarimplicature, they do with ?glasses?
pragmaticallyassociated with r2 (Fig.
1c, line 1).Our implementation of these games as Dec-POMDPs mirrors their intuitive description andtheir treatment in iterated best response models(Ja?ger, 2007; Ja?ger, 2012; Franke, 2009; Frankand Goodman, 2012).
The state space S encodesthe attributes of the referents (e.g., hat(r2) = T,glasses(r1) = F) and includes a target variable tidentifying the speaker?s referent (hidden from thelistener).
The speaker has three speech actions,identified with the three messages.
The listenerhas four actions: ?listen?
plus a ?choose?
action cifor each referent ri.
The set of observations O isjust the set of messages (construed as utterances).The agents receive a positive reward iff the listeneraction ci corresponds to the speaker?s target t. Be-cause this is a one-step reference game, the transi-tion distribution T is the identity distribution.The literal listener L interprets utterances asa truth-conditional speaker would produce them(Fig.
1b).
The level-one speaker S(L) augmentsthe state space with a variable ?listener target?
andmodels L?s beliefs b?
using the approximate meth-ods of Sec.
2.
Crucially, the optimal speaker pol-icy piS(L) is such that piS(L)(t=r3) = ?hat?
andpiS(L)(t=r1) = ?moustache?.
The level-two lis-tener L(S(L)) models S(L) via an estimate of the?listener target?
variable.
For each speech actionm, L(S(L)) considers all values of t and the likeli-76hood that S(L) would have produced m:Pr(t=ri|m) ?
1[p?iS(L)(t=ri) = m]Since S(L) uses ?hat?
to describe r3 and?moustache?
to describe r1, L(S(L)) correctly in-fers that ?glasses?
refers to r2, completing Fig.
1c?sfull implicature-rich pattern of mutual exclusivity(Clark, 1987; Frank et al, 2009).This basic pattern is robustly attested empiri-cally in human data.
The experimental data are,of course, invariably less crisp than our idealizedmodel predicts, but many important sources ofvariation could be brought into our model, withthe addition of strong salience priors (Frank andGoodman, 2012; Stiller et al, 2011), assumptionsabout bounded rationality (Camerer et al, 2004;Franke, 2009), and a ?soft-max?
view of the lis-tener (Frank et al, 2009).4 Cards World ImplicaturesThe Cards corpus1 contains 1266 metadata-richtranscripts from a two-player chat-based game.The world is a simple maze in which a deck ofcards has been distributed.
The players?
goal is tofind specific subsets of the cards, subject to a vari-ety of constraints on what they can see and do.
TheDec-POMDP-based agents of Vogel et al (2013)play a simplified version in which the goal is to beco-located with a single card.
Vogel et al showthat their agents?
linguistic behavior is broadlyGricean.
However, their agents?
language is toosimple to reveal implicatures.
The present sectionremedies this shortcoming.
Implicature-rich inter-pretations are an immediate consequence.We implement the simplified Cards tasks as fol-lows.
The state space S is composed of the loca-tion of each player and the location of the card.The transition distribution T (s?|s, a1, a2) encodesthe outcome of movement actions.
Agents receiveone of two sensor observations, indicating whetherthe card is at their current location.
The players arerewarded when they are both located on the card.Each player begins knowing his own location, butnot the location of the other player nor of the card.The players have four movement actions (?up?,?down?, ?left?, ?right?)
and nine speech actions in-terpreted as identifying card locations.
Fig.
2 de-picts these utterances as a partial order determinedby entailment.
These general-to-specific relation-1http://cardscorpus.christopherpotts.nettop right top left bottom right bottom lefttop right left bottom middle,,,,\\\\SSSS!!!!!!!!
!SSSS""""""Figure 2: Cards world utterance actions.top left (5.75) top (6.68) top right (5.57)left (6.81) middle (7.16) right (6.86)bottom left (6.11) bottom (6.37) bottom right (5.42)Figure 3: Literal interpretations derived from theCards corpus.
The entropy of each distribution isincluded in parentheses.
Each term is estimatedfrom all tokens that contain it, which washesout implicature-rich usage, thereby providing ourmodel with an empirically-grounded literal start.ships show that the language can support scalarconversational implicatures.2Fig.
2 is not entirely appropriate in our setting,however.
Our expressions are vague; there is nosharp boundary between, e.g., ?top?
and ?bottom?,nor is it clear where ?top right?
begins.
To modelthis vagueness, we analyze each message m asdenoting a conditional distribution Pr(x|m) overgrid squares x in the gameboard.
These distribu-tions are derived from human?human Cards inter-actions using the data and methods of Potts (2012).Of course, there is a tension here: our model as-sumes that we begin with literal interpretations,but human?human data will reflect pragmatically-enriched usage.
To get around this, we approxi-mate literal interpretations by deriving each term?sdistribution from all the corpus tokens that con-tain it.
For example, the distribution for ?top?
is2Our agents cannot produce modified versions of ?mid-dle?
like ?middle right?.
These would be synonymous withimplicature-enriched general terms.
We work with a simplecost-function that treats all forms alike, but future versions ofthis work will incorporate more realistic form-based costs.77top left (5.17) top (3.46) top right (5.04)left (3.91) middle (2.35) right (3.58)bottom left (4.81) bottom (3.70) bottom right (5.04)Figure 4: Implicature-rich interpretations, derivedusing the level-one listener L(S).estimated not only from ?top?
but also from ?topright?, ?middle right?, and so forth.
The denotationfor ?top right?
excludes simple ?top?
and ?right?utterances but includes expressions like ?very topright?.
This semantics washes out any implicaturepatterns, thereby giving us a proper literal startingpoint.
Fig.
3 shows these denotations for the fullset of expressions.
The entailment relations fromFig.
2 are (fuzzily) evident.
For example, the areasof high probability for ?right?
properly contain theareas of high probability for ?top right?.To show how the Dec-POMDP model deliversimplicatures, we begin with a literal speaker Swho does not consider the location of the otherplayer and instead searches the board until he findsthe card.
After finding it, he communicates the re-ferring expression with highest literal probabilityfor his location, using the distributions from Fig.
3.We denote the literal speaker?s policy by piS.
Thelevel-one listener L(S) tracks an estimate of S?s lo-cation and beliefs about the card location.
Usingthe approximation defined in Sec.
2, L(S) inter-prets an utterancem as Pr(m|s) = 1[p?iS(s) = m].Thus, the meaning of each m is the set of be-liefs that S might have to produce this utterance.Fig.
4 shows how L(S) interprets each message.The meaning of general terms like ?top?
and ?right?now exclude their modified counterparts.
Thisis evident in the lack of overlap between high-probability areas and in the lower entropy values.Direct evaluation of this result against the cor-pus data is not possible, because the corpus doesnot encode interpretations.
However, we expecttop left (5.82) top (5.74) top right (5.49)left (6.15) middle (6.14) right (6.57)bottom left (5.29) bottom (5.43) bottom right (5.44)Figure 5: Distributions reflecting human speakers?aggregate referential intentions .
Each term is es-timated only from tokens that exactly match it.listener interpretations to align with speaker in-tentions, and we can gain insight into (aggregate)speaker intentions using our method for ground-ing referential terms.
Whereas the literal inter-pretation for message m is obtained from all thetokens that contain it (Fig.
3), the speaker?s in-tended interpretation for m is obtained from allof the tokens that exactly match it.
For instance,the meaning of ?top?
now excludes tokens like ?topleft?.
Fig.
5 shows these denotations, which mirrorthe distributions predicted by our model (Fig.
4).Thus, the L(S) model correctly infers the prag-matic meaning of referring expressions as used byhuman speakers, albeit in an idealized manner.5 Future WorkWe showed that implicatures arise in cooperativecontexts from nested belief models.
Our listener-centric implicatures must be combined with ratio-nal speaker behavior (Vogel et al, 2013) to pro-duce general dialog agents.
The computationalcomplexity of Dec-POMDPs is prohibitive, andour approximations can be problematic for deepbelief nesting.
Future work will explore sampling-based approaches to belief update and decisionmaking (Doshi and Gmytrasiewicz, 2009) to over-come these problems.
These steps will move uscloser to a computationally effective, unified the-ory of pragmatic enrichment and decision making.Acknowledgements This research was supported inpart by ONR grants N00014-10-1-0109 and N00014-13-1-0287 and ARO grant W911NF-07-1-0216.78ReferencesLeon Bergen, Noah D. Goodman, and Roger Levy.2012.
That?s what she (could have) said: How alter-native utterances affect language use.
In Proceed-ings of the Thirty-Fourth Annual Conference of theCognitive Science Society.Daniel S. Bernstein, Robert Givan, Neil Immerman,and Shlomo Zilberstein.
2002.
The complexity ofdecentralized control of Markov decision processes.Mathematics of Operations Research, 27(4):819?840.Colin F. Camerer, Teck-Hua Ho, and Juin-Kuan Chong.2004.
A cognitive hierarchy model of games.
TheQuarterly Journal of Economics, 119(3):861?898,August.Herbert H. Clark and Deanna Wilkes-Gibbs.
1986.Referring as a collaborative process.
Cognition,22(1):1?39.Eve V. Clark.
1987.
The principle of contrast: A con-straint on language acquisition.
In Brian MacWhin-ney, editor, Mechanisms of Language Acquisition,pages 1?33.
Erlbaum, Hillsdale, NJ.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,19(2):233?263.Judith Degen and Michael Franke.
2012.
Optimal rea-soning about referential expressions.
In Proceed-ings of SemDIAL 2012, Paris, September.David DeVault and Matthew Stone.
2007.
Manag-ing ambiguities across utterances in dialogue.
InRon Artstein and Laure Vieu, editors, Proceedingsof DECALOG 2007: Workshop on the Semanticsand Pragmatics of Dialogue.Prashant Doshi and Piotr J. Gmytrasiewicz.
2009.Monte carlo sampling methods for approximatinginteractive pomdps.
J. Artif.
Int.
Res., 34(1):297?337, March.Michael C. Frank and Noah D. Goodman.
2012.
Pre-dicting pragmatic reasoning in language games.
Sci-ence, 336(6084):998.Michael C. Frank, Noah D. Goodman, and Joshua B.Tenenbaum.
2009.
Using speakers?
referential in-tentions to model early cross-situational word learn-ing.
Psychological Science, 20(5):579?585.Michael Franke.
2009.
Signal to Act: Game Theoryin Pragmatics.
ILLC Dissertation Series.
Institutefor Logic, Language and Computation, Universityof Amsterdam.Gerald Gazdar.
1979.
Pragmatics: Implicature, Pre-supposition and Logical Form.
Academic Press,New York.Piotr J. Gmytrasiewicz and Prashant Doshi.
2005.
Aframework for sequential planning in multi-agentsettings.
Journal of Artificial Intelligence Research,24:24?49.Dave Golland, Percy Liang, and Dan Klein.
2010.A game-theoretic approach to generating spatial de-scriptions.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 410?419, Cambridge, MA, October.ACL.H.
Paul Grice.
1975.
Logic and conversation.
In Pe-ter Cole and Jerry Morgan, editors, Syntax and Se-mantics, volume 3: Speech Acts, pages 43?58.
Aca-demic Press, New York.Robert M. Harnish.
1979.
Logical form and implica-ture.
In Linguistic Communication and Speech Acts,pages 313?391.
MIT Press, Cambridge, MA.Julia Hirschberg.
1985.
A Theory of Scalar Implica-ture.
Ph.D. thesis, University of Pennsylvania.Laurence R Horn.
1972.
On the Semantic Properties ofLogical Operators in English.
Ph.D. thesis, UCLA,Los Angeles.Gerhard Ja?ger.
2007.
Game dynamics connects se-mantics and pragmatics.
In Ahti-Veikko Pietarinen,editor, Game Theory and Linguistic Meaning, pages89?102.
Elsevier, Amsterdam.Gerhard Ja?ger.
2012.
Game theory in semantics andpragmatics.
In Maienborn et al (Maienborn et al,2012).Emiel Krahmer and Kees van Deemter.
2012.
Compu-tational generation of referring expressions: A sur-vey.
Computational Linguistics, 38(1):173?218.Claudia Maienborn, Klaus von Heusinger, and PaulPortner, editors.
2012.
Semantics: An InternationalHandbook of Natural Language Meaning, volume 3.Mouton de Gruyter, Berlin.Christopher Potts.
2012.
Goal-driven answers in theCards dialogue corpus.
In Nathan Arnett and RyanBennett, editors, Proceedings of the 30th West CoastConference on Formal Linguistics, Somerville, MA.Cascadilla Press.Hannah Rohde, Scott Seyfarth, Brady Clark, GerhardJa?ger, and Stefan Kaufmann.
2012.
Communicat-ing with cost-based implicature: A game-theoreticapproach to ambiguity.
In The 16th Workshop onthe Semantics and Pragmatics of Dialogue, Paris,September.Seymour Rosenberg and Bertram D. Cohen.
1964.Speakers?
and listeners?
processes in a word com-munication task.
Science, 145:1201?1203.Matthijs T. J. Spaan and Nikos Vlassis.
2005.Perseus: Randomized point-based value iterationfor POMDPs.
Journal of Artificial Intelligence Re-search, 24(1):195?220, August.79Alex Stiller, Noah D. Goodman, and Michael C. Frank.2011.
Ad-hoc scalar implicature in adults and chil-dren.
In Proceedings of the 33rd Annual Meeting ofthe Cognitive Science Society, Boston, July.Adam Vogel, Max Bodoia, Dan Jurafsky, and Christo-pher Potts.
2013.
Emergence of Gricean max-ims from multi-agent decision theory.
In HumanLanguage Technologies: The 2013 Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics, Atlanta, Geor-gia, June.
Association for Computational Linguis-tics.80
