Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 36?44,Avignon, France, April 23 2012. c?2012 Association for Computational LinguisticsAn Unsupervised and Data-Driven Approach for Spell Checking inVietnamese OCR-scanned TextsCong Duy Vu HOANG & Ai Ti AWDepartment of Human Language Technology (HLT)Institute for Infocomm Research (I2R)A*STAR, Singapore{cdvhoang,aaiti}@i2r.a-star.edu.sgAbstractOCR (Optical Character Recognition) scan-ners do not always produce 100% accuracyin recognizing text documents, leading tospelling errors that make the texts hard toprocess further.
This paper presents an in-vestigation for the task of spell checkingfor OCR-scanned text documents.
First, weconduct a detailed analysis on characteris-tics of spelling errors given by an OCRscanner.
Then, we propose a fully auto-matic approach combining both error detec-tion and correction phases within a uniquescheme.
The scheme is designed in an un-supervised & data-driven manner, suitablefor resource-poor languages.
Based on theevaluation on real dataset in Vietnameselanguage, our approach gives an acceptableperformance (detection accuracy 86%, cor-rection accuracy 71%).
In addition, we alsogive a result analysis to show how accurateour approach can achieve.1 Introduction and Related WorkDocuments that are only available in print re-quire scanning from OCR devices for retrievalor e-archiving purposes (Tseng, 2002; Magdyand Darwish, 2008).
However, OCR scannersdo not always produce 100% accuracy in rec-ognizing text documents, leading to spelling er-rors that make the texts texts hard to process fur-ther.
Some factors may cause those errors.
Forinstance, shape or visual similarity forces OCRscanners to misunderstand some characters; or in-put text documents do not have good quality, caus-ing noises in resulting scanned texts.
The task ofspell checking for OCR-scanned text documentsproposed aims to solve the above situation.Researchers in the literature used to approachthis task for various languages such as: English(Tong and Evans, 1996; Taghva and Stofsky,2001; Kolak and Resnik, 2002), Chinese (Zhuanget al, 2004), Japanese (Nagata, 1996; Nagata,1998), Arabic (Magdy and Darwish, 2006), andThai (Meknavin et al, 1998).The most common approach is to involve usersfor their intervention with computer support.Taghva and Stofsky (2001) designed an interac-tive system (called OCRSpell) that assists users asmany interactive features as possible during theircorrection, such as: choose word boundary, mem-orize user-corrected words for future correction,provide specific prior knowledge about typical er-rors.
For certain applications requiring automa-tion, the interactive scheme may not work.Unlike (Taghva and Stofsky, 2001), non-interactive (or fully automatic) approaches havebeen investigated.
Such approaches need pre-specified lexicons & confusion resources (Tongand Evans, 1996), language-specific knowledge(Meknavin et al, 1998) or manually-created pho-netic transformation rules (Hodge and Austin,2003) to assist correction process.Other approaches used supervised mecha-nisms for OCR error correction, such as: statis-tical language models (Nagata, 1996; Zhuang etal., 2004; Magdy and Darwish, 2006), noisy chan-nel model (Kolak and Resnik, 2002).
These ap-proaches performed well but are limited due torequiring large annotated training data specific toOCR spell checking in languages that are veryhard to obtain.Further, research in spell checking forVietnamese language has been understudied.36Hunspell?spellcheck?vn1 & Aspell2 are inter-active spell checking tools that work based onpre-defined dictionaries.According to our best knowledge, there is nowork in the literature reported the task of spellchecking for Vietnamese OCR-scanned text doc-uments.
In this paper, we approach this task interms of 1) fully automatic scheme; 2) without us-ing any annotated corpora; 3) capable of solvingboth non-word & real-word spelling errors simul-taneously.
Such an approach will be beneficial fora poor-resource language like Vietnamese.2 Error CharacteristicsFirst of all, we would like to observe and analysethe characteristics of OCR-induced errors in com-pared with typographical errors in a real dataset.2.1 Data OverviewWe used a total of 24 samples of VietnameseOCR-scanned text documents for our analysis.Each sample contains real & OCR texts, referringto texts without & with spelling errors, respec-tively.
Our manual sentence segmentation givesa result of totally 283 sentences for the above 24samples, with 103 (good, no errors) and 180 (bad,errors existed) sentences.
Also, the number of syl-lables3 in real &OCR sentences (over all samples)are 2392 & 2551, respectively.2.2 Error ClassificationWe carried out an in-depth analysis on spellingerrors, identified existing errors, and then man-ually classified them into three pre-defined errorclasses.
For each class, we also figured out howan error is formed.As a result, we classified OCR-induced spellingerrors into three classes:Typographic or Non-syllable Errors (Class 1):refer to incorrect syllables (not includedin a standard dictionary).
Normally, atleast one character of a syllable is expectedmisspelled.1http://code.google.com/p/hunspell-spellcheck-vi/2http://en.wikipedia.org/wiki/GNU_Aspell/3In Vietnamese language, we will use the word ?sylla-ble?
instead of ?token?
to mention a unit that is separated byspaces.Real-syllable or Context-based Errors (Class 2):refer to syllables that are correct in terms oftheir existence in a standard dictionary butincorrect in terms of their meaning in thecontext of given sentence.Unexpected Errors (Class 3): are accidentallyformed by unknown operators, such as:insert non-alphabet characters, do incorrectupper-/lower- case, split/merge/removesyllable(s), change syllable orders, .
.
.Note that errors in Class 1 & 2 can be formedby applying one of 4 operators4 (Insertion, Dele-tion, Substitution, Transposition).
Class 3 is ex-clusive, formed by unexpected operators.
Table 1gives some examples of 3 error classes.An important note is that an erroneous syllablecan contain errors across different classes.
Class3 can appear with Class 1 or Class 2 but Class 1never appears with Class 2.
For example:?
ho?n (correct) || H?an (incorrect) (Class 3 & 1)?
b?t (correct) || b?t?
(incorrect) (Class 3 & 2)Figure 1: Distribution of operators used in Class1 (left) & Class 2 (right).2.3 Error DistributionOur analysis reveals that there are totally 551 rec-ognized errors over all 283 sentences.
Each erroris classified into three wide classes (Class 1, Class2, Class 3).
Specifically, we also tried to identifyoperators used in Class 1 & Class 2.
As a result,we have totally 9 more fine-grained error classes(1A..1D, 2A..2D, 3)5.We explored the distribution of 3 error classesin our analysis.
Class 1 distributed the most, fol-lowing by Class 3 (slightly less) and Class 2.4Their definitions can be found in (Damerau, 1964).5A, B, C, and D represent for Insertion, Deletion, Sub-stitution, and Transposition, respectively.
For instance, 1Ameans Insertion in Class 1.37Class Insertion Deletion Substitution TranspositionaClass 1 ?p (correct) || ?ip (in-correct) (?i?
inserted)kh?ng (correct) || kh(incorrect).
(??
?, ?n?,and ?g?
deleted)y?u (correct) || ??u(incorrect).
(?y?
sub-stituted by ???
)N.A.Class 2 l?n (correct) || li?n(contextually incor-rect).
(?i?
inserted)tr?nh (correct) ||t?nh (contextuallyincorrect).
(?r?deleted)ngay (correct) || ng?y(contextually incor-rect).
(?a?
substitutedby ???
)N.A.Class 3 x?c nh?n l?
(correct) || x||nha0a (incorrect).
3 syllables were misspelled & accidentally merged.aOur analysis reveals no examples for this operator.Table 1: Examples of error classes.Generally, each class contributed a certain quan-tity of errors (38%, 37%, & 25%), making thecorrection process of errors more challenging.
Inaddition, there are totally 613 counts for 9 fine-grained classes (over 551 errors of 283 sentences),yielding an average & standard deviation 3.41 &2.78, respectively.
Also, one erroneous syllable isable to contain the number of (fine-grained) errorclasses as follows: 1(492), 2(56), 3(3), 4(0) ((N)is count of cases).We can also observe more about the distribu-tion of operators that were used within each errorclass in Figure 1.
The Substitution operator wasused the most in both Class 1 & Class 2, holding81% & 97%, respectively.
Only a few other oper-ators (Insertion, Deletion) were used.
Specially,the Transposition operator were not used in bothClass 1&Class 2.
This justifies the fact that OCRscanners normally have ambiguity in recognizingsimilar characters.3 Proposed ApproachThe architecture of our proposed approach(namely (VOSE)) is outlined in Figure 2.
Our pur-pose is to develop VOSE as an unsupervised data-driven approach.
It means VOSE will only usetextual data (un-annotated) to induce the detection& correction strategies.
This makes VOSE uniqueand generic to adapt to other languages easily.In VOSE, potential errors will be detected lo-cally within each error class and will then be cor-rected globally under a ranking scheme.
Specif-ically, VOSE implements two different detectors(Non-syllable Detector & Real-syllable Detec-tor) for two error groups of Class 1/3 & Class2, respectively.
Then, a corrector combines theoutputs from two above detectors based on rank-ing scheme to produce the final output.
Currently,VOSE implements two different correctors, aContextual Corrector and a Weighting-basedCorrector.
Contextual Corrector employs lan-guage modelling to rank a list of potential can-didates in the scope of whole sentence whereasWeighting-based Corrector chooses the bestcandidate for each syllable that has the highestweights.
The following will give detailed descrip-tions for all components developed in VOSE.3.1 Pre-processorPre-processor will take in the input text, dotokenization & normalization steps.
Tokeniza-tion in Vietnamese is similar to one in En-glish.
Normalization step includes: normal-ize Vietnamese tone & vowel (e.g.
h?a ?>ho?
), standardize upper-/lower- cases, find num-bers/punctuations/abbreviations, remove noisecharacters, .
.
.This step also extracts unigrams.
Each of themwill then be checked whether they exist in a pre-built list of unigrams (from large raw text data).Unigrams that do not exist in the list will be re-garded as Potential Class 1 & 3 errors and thenturned into Non-syllable Detector.
Other uni-grams will be regarded as Potential Class 2 er-rors passed into Real-syllable Detector.3.2 Non-syllable DetectorNon-syllable Detector is to detect errors that donot exist in a pre-built combined dictionary (Class1 & 3) and then generate a top-k list of poten-tial candidates for replacement.
A pre-built com-bined dictionary includes all syllables (unigrams)extracted from large raw text data.In VOSE, we propose a novel approach thatuses pattern retrieval technique forNon-syllable38Figure 2: Proposed architecture of our approachDetector.
This approach aims to retrieve all n-gram patterns (n can be 2,3) from textual data,check approximate similarity with original erro-neous syllables, and then produce a top list of po-tential candidates for replacement.We believe that this approach will be able tonot only handle errors with arbitrary changes onsyllables but also utilize contexts (within 2/3 win-dow size), making possible replacement candi-dates more reliable, and more semantically tosome extent.This idea will be implemented in the N-gramEngine component.3.3 Real-syllable DetectorReal-syllable Detector is to detect all possiblereal-syllable errors (Class 2) and then producethe top-K list of potential candidates for replace-ment.
The core idea of Real-syllable Detector isto measure the cohesion of contexts surrounding atarget syllable to check whether it is possibly erro-neous or not.
The cohesion is measured by counts& probabilities estimated from textual data.Assume that a K-size contextual window with atarget syllable at central position is chosen.s1 s2 ?
?
?
[sc] ?
?
?
sK?1 sK (K syllables, sc tobe checked, K is an experimental odd value (canbe 3, 5, 7, 9).
)The cohesion of a sequence of syllables sK1 bi-ased to central syllable sc can be measured by oneof three following formulas:Formula 1:cohesion1(sK1 ) = log(P (sK1 ))= log(P (sc) ?K?i 6=c,i=1P (si|sc))(1)Formula 2:cohesion2(sK1 ) = countexist?
(sc?2sc?1sc,sc?1scsc+1, scsc+1sc+2, sc?1sc, scsc+1)(2)Formula 3:cohesion3(sK1 ) = countexist?
(sc?2 ?
sc,sc?1sc, sc ?
sc+2, scsc+1)(3)where:?
cohesion(sK1 ) is cohesion measure of sequence sK1 .39?
P (sc) is estimated from large raw text data com-puted by c(sc)C , whereas c(sc) is unigram count and Cis total count of all unigrams from data.?
P (si|sc) is computed by:P (si|sc) =P (si, sc)P (sc)=c(si, sc, |i?
c|)c(sc)(4)where:?
c(si, sc, |i?
c|) is a distance-sensitive count of twounigrams si and sc co-occurred and the gap betweenthem is |i?
c| unigrams.For Formula 1, if cohesion(sK1 ) < Tc withTc is a pre-defined threshold, the target syllable ispossibly erroneous.For Formula 2, instead of probabilities as inFormula 1, we use counting on existence of n-grams within a context.
It?s maximum value is 5.Formula 3 is a generalized version of Formula 2(the wild-card ?*?
means any syllable).
It?s maxi-mum value is 4.N-gram Engine.
The N-gram Engine compo-nent is very important in VOSE.
All detectors &correctors use it.Data Structure.
It is worthy noting that in or-der to compute probabilities like c(si, sc, |i?
c|)or query the patterns from data, an efficient datastructure needs to be designed carefully.
It MUSTsatisfy two criteria: 1) space to suit memory re-quirements 2) speed to suit real-time speed re-quirement.
In this work,N-gram Engine employsinverted index (Zobel and Moffat, 2006), a well-known data structure used in text search engines.Pattern Retrieval.
After detecting poten-tial errors, both Non-syllable Detector andReal-syllable Detector use N-gram Engine tofind a set of possible replacement syllables byquerying the textual data using 3-gram patterns(sc?2sc?1[s?c], sc?1[s?c]sc+1, and [s?c]sc+1sc+2) or2-gram patterns (sc?1 [s?c], [s?c]sc+1), where [s?c] isa potential candidate.
To rank a list of top candi-dates, we compute the weight for each candidateusing the following formula:weight(si) = ?
?Sim(si, s?c)+(1??)?Freq(si)(5)where:?
Sim(si, s?c) is the string similarity between candi-date syllable si and erroneous syllable s?c .?
Freq(si) is normalized frequency of si over a re-trieved list of possible candidates.?
?
is a value to control the weight biased to stringsimilarity or frequency.In order to compute the string similarity, wefollowed a combined weighted string similarity(CWSS) computation in (Islam and Inkpen, 2009)as follows:Sim(si, s?c) = ?1 ?NLCS(si, s?c)+?2 ?NCLCS1(si, s?c) + ?3 ?NCLCSn(si, s?c)+?4 ?NCLCSz(si, s?c)(6)where:?
?1, ?2, ?3, and ?4 are pre-defined weights for eachsimilarity computation.
Initially, all ?
are set equal to1/4.?
NLCS(si, s?c) is normalized length of longestcommon subsequence between si and s?c .?
NCLCS1(si, s?c), NCLCSn(si, s?c), andNCLCSz(si, s?c) is normalized length of maximalconsecutive longest common subsequence betweensi and s?c starting from the first character, from anycharacter, and from the last character, respectively.?
Sim(si, s?c) has its value in range of [0, 1].We believe that the CWSS method will ob-tain better performance than standard meth-ods (e.g.
Levenshtein-based String Matching(Navarro, 2001) or n-gram based similarity (Lin,1998)) because it can exactly capture more infor-mation (beginning, body, ending) of incompletesyllables caused by OCR errors.
As a result, thisstep will produce a ranked top-k list of potentialcandidates for possibly erroneous syllables.
In ad-dition, N-gram Engine also stores computationutilities relating the language models which arethen provided to Contextual Corrector.3.4 CorrectorIn VOSE, we propose two possible correctors:Weighting-based CorrectorGiven a ranked top-K list of potential can-didates from Non-syllable Detector and Real-syllable Detector, Weighting-based Correctorsimply chooses the best candidates based on theirweights (Equation 5) to produce the final output.Contextual CorrectorGiven a ranked top-K list of potential can-didates from Non-syllable Detector and Real-syllable Detector, Contextual Corrector glob-ally ranks the best candidate combination usinglanguage modelling scheme.40Specifically, Contextual Corrector employsthe language modelling based scheme whichchooses the combination of candidates (sn1 )?
thatmakes PP ((sn1 )?)
maximized over all combina-tions as follows:(sn1 )?best = argmax(sn1 )?
PP ((sn1 )?)
(7)where: PP (.)
is a language modelling score or per-plexity (Jurafsky and Martin, 2008; Koehn, 2010).In our current implementation, we used Depth-First Traversal (DFS) strategy to examine over allcombinations.
The weakness of DFS strategy isthe explosion of combinations if the number ofnodes (syllables in our case) grows more than 10.In this case, the speed of DFS-based ContextualCorrector is getting slow.
Future work can con-sider beam search decoding idea in StatisticalMachine Translation (Koehn, 2010) to adapt forContextual Corrector.3.5 Prior Language-specific KnowledgeSinceVOSE is an unsupervised & data-driven ap-proach, its performance depends on the qualityand quantity of raw textual data.
VOSE?s cur-rent design allows us to integrate prior language-specific knowledge easily.Some possible sources of prior knowledgecould be utilized as follows:?
Vietnamese Character Fuzzy Matching - InVietnamese language, some characters look verysimilar, forcing OCR scanners mis-recognition.Thus, we created a manual list of highly similarcharacters (as shown in Table 2) and then inte-grate this into VOSE.
Note that this integrationtakes place in the process of string similarity com-putation.?
English Words & Vietnamese Abbrevia-tions Filtering - In some cases, there exist En-glish words or Vietnamese abbreviations.
VOSEmay suggest wrong replacements for those cases.Thus, a syllable in either English words or Viet-namese abbreviations will be ignored in VOSE.4 Experiments4.1 Baseline SystemsAccording to our best knowledge, previous sys-tems that are able to simultaneously handle bothnon-syllable and real-syllable errors do not exist,especially apply for Vietnamese language.
We be-lieve that VOSE is the first one to do that.No.
Character Similar Characters1 a {?
?
?
?
?
?
?
?
}2 e {?
?
?
?}
+ {c}3 i {?
?}
+ {l}4 o {?
?
?
?
?
}5 u {?
?
?
?
?
}6 y {?
?
}7 d {?
}Table 2: Vietnamese similar characters.4.2 N-gram Extraction DataIn VOSE, we extracted ngrams from the raw tex-tual data.
Table 3 shows data statistics used in ourexperiments.4.3 Evaluation MeasureWe used the following measure to evaluate theperformance of VOSE:- For Detection:DF =2?DR?DPDR+DP(8)Where:?
DR (Detection Recall) = the fraction of errorscorrectly detected.?
DP (Detection Precision) = the fraction of de-tected errors that are correct.?
DF (Detection F-Measure) = the combinationof detection recall and precision.- For Correction:CF =2?
CR?
CPCR+ CP(9)Where:?
CR (Correction Recall) = the fraction of errorscorrectly amended.?
CP (Correction Precision) = the fraction ofamended errors that are correct.?
CF (Correction F-Measure) = the combinationof correction recall and precision.4.4 ResultsWe carried out our evaluation based on the realdataset as described in Section 2.
In our evalua-tion, we intend:?
To evaluate whether VOSE can benefit from ad-dition of more data, meaning that VOSE is actu-ally a data-driven system.?
To evaluate the effectiveness of language mod-elling based corrector in compared to weighing41N-gramsNo Dataset NumOfSents Vocabulary 2-gram 3-gram 4-gram 5-gram1 DS1 1,328,506 102,945 1,567,045 8,515,894 17,767,103 24,700,8152 DS2a 2,012,066 169,488 2,175,454 12,610,281 27,961,302 40,295,8883 DS3b 283 1,546 6,956 9,030 9,671 9,9464 DS4c 344 1,755 6,583 7,877 8,232 8,383aincludes DS1 and morebannotated test data (not included in DS1 & DS2) as described in Section 2cweb contexts data (not included in others) crawled from the InternetTable 3: Ngram extraction data statistics.based corrector.?
To evaluate whether prior knowledge specificto Vietnamese language can help VOSE.The overall evaluation result (in terms of detec-tion & correction accuracy) is shown in Table 4.In our experiments, all VOSE(s) except of VOSE6 used contextual corrector (Section 3.4).
Also,Real-syllable Detector (Section 3.3) used Equa-tion 3 which revealed the best result in our pre-evaluation (we do not show the results becausespaces do not permit).We noticed the tone & vowel normalizationstep in Pre-processormodule.
This step is impor-tant specific to Vietnamese language.
VOSE 2a inTable 4 shows that VOSE using that step gives asignificant improvement (vs. VOSE 1) in both de-tection & correction.We also tried to assess the impact of languagemodelling order factor in VOSE.
VOSE using 3-gram language modelling gives the best result(VOSE 2a vs. VOSE 2b & 2c).
Because of this,we chose 3-gram for next VOSE set-ups.We experiment how data addition affectsVOSE.
First, we used bigger data (DS2) for ngramextraction and found the significant improvement(VOSE 3a vs. VOSE 2a).
Second, we tried aninteresting set-up in which VOSE utilized ngramextraction data with annotated test data (DatasetDS3) only in order to observe the recall abilityof VOSE.
Resulting VOSE (VOSE 3b) performedextremely well.As discussed in Section 3.5, VOSE allows in-tegrated prior language-specific knowledge thathelps improve the performance (VOSE 4).
Thisjustifies that statistical method in combined withsuch prior knowledge is very effective.Specifically, for each error in test data, wecrawled the web sentences containing contexts inwhich that error occurs (called web contexts).
Weadded such web contexts into ngram extractiondata.
With this strategy, we can improve the per-formance of VOSE significantly (VOSE 5), ob-taining the best result.
Again, we?ve proved thatmore data VOSE has, more accurate it performs.The result of VOSE 6 is to show the superiorityof VOSE using contextual corrector in comparedwith using weighting-based corrector (VOSE 6 vs.VOSE 4).
However, weighting-based correctorhas much faster speed in correction than contex-tual corrector which is limited due to DFS traver-sal & language modelling ranking.Based on the above observations, we have twofollowing important claims:?
First, the addition of more data in ngram ex-traction process is really useful for VOSE.?
Second, prior knowledge specific to Viet-namese language helps to improve the perfor-mance of VOSE.?
Third, contextual corrector with language mod-elling is superior than weighting-based correctorin terms of the accuracy.4.5 Result AnalysisBased on the best results produced by our ap-proach (VOSE), we recognize & categorize casesthat VOSE is currently unlikely to detect & cor-rect properly.Consecutive Cases (Category 1)When there are 2 or 3 consecutive errors, theircontexts are limited or lost.
This issue will af-fect the algorithm implemented in VOSE utilizingthe contexts to predict the potential replacements.VOSE can handle such errors to limited extent.Merging Cases (Category 2)In this case, two or more erroneous syllablesare accidentally merged.
Currently, VOSE cannot42Detection Accuracy Correction AccuracySet-up Recall Precision F1 Recall Precision F1 RemarkVOSE 1 0.8782 0.5954 0.7097 0.6849 0.4644 0.5535 w/o TVN + 3-LM + DS1VOSE 2a 0.8782 0.6552 0.7504 0.6807 0.5078 0.5817 w/ TVN + 3-LM + DS1VOSE 2b 0.8782 0.6552 0.7504 0.6744 0.5031 0.5763 w/ TVN + 4-LM + DS1VOSE 2c 0.8782 0.6552 0.7504 0.6765 0.5047 0.5781 w/ TVN + 5-LM + DS1VOSE 3a 0.8584 0.7342 0.7914 0.6829 0.5841 0.6296 w/ TVN + 3-LM + DS2VOSE 3b 0.9727 0.9830 0.9778 0.9223 0.9321 0.9271 w/ TVN + 3-LM + DS3VOSE 4 0.8695 0.7988 0.8327 0.7095 0.6518 0.6794 VOSE 3a + PKVOSE 5 0.8674 0.8460 0.8565 0.7200 0.7023 0.7110 VOSE 4 + DS4VOSE 6 0.8695 0.7988 0.8327 0.6337 0.5822 0.6069 VOSE 4 but uses WCTable 4: Evaluation results.
Abbreviations: TVN (Tone & Vowel Normalization); N-LM (N-orderLanguage Modelling); DS (Dataset); PK (Prior Knowledge); WC (Weighting-based Corrector).handle such cases.
We aim to investigate this inour future work.Proper Noun/Abbreviation/Number Cases(both in English, Vietnamese) (Category 3)Abbreviations or proper nouns or numbers areunknown (for VOSE) because they do not appearin ngram extraction data.
If VOSE marks them aserrors, it could not correct them properly.Ambiguous Cases (Category 4)Ambiguity can happen in:?
cases in which punctuation marks (e.g.
comma,dot, dash, .
.
. )
are accidentally added between twodifferent syllable or within one syllable.?
cases never seen in ngram extraction data.?
cases relating to semantics in Vietnamese.?
cases where one Vietnamese syllable that ischanged incorrectly becomes an English word.Lost Cases (Category 5)This case happens when a syllable which is ac-cidentally lost most of its characters or too shortbecomes extremely hard to correct.Additionally, we conducted to observe the dis-tribution of the above categories (Figure 3).
Ascan be seen, Category 4 dominates more than 70%cases that VOSE has troubles for detection & cor-rection.5 Conclusion & Future WorkIn this paper, we?ve proposed & developed a newapproach for spell checking task (both detectionand correction) for Vietnamese OCR-scanned textdocuments.
The approach is designed in an un-supervised & data-driven manner.
Also, it allowsFigure 3: Distribution of categories in the resultof VOSE 4 (left) & VOSE 5 (right).to integrate the prior language-specific knowledgeeasily.Based on the evaluation on a real dataset,the system currently offers an acceptable perfor-mance (best result: detection accuracy 86%, cor-rection accuracy 71%).
With just an amountof small n-gram extraction data, the obtained re-sult is very promising.
Also, the detailed erroranalysis in previous section reveals that cases thatcurrent system VOSE cannot solve are extremelyhard, referring to the problem of semantics-related ambiguity in Vietnamese language.Further remarkable point of proposed approachis that it can perform the detection & correctionprocesses in real-time manner.Future works include some directions.
First, weshould crawl and add more textual data for n-gramextraction to improve the performance of currentsystem.
More data VOSE has, more accurate itperforms.
Second, we should investigate more oncategories (as discussed earlier) that VOSE couldnot resolve well.
Last, we also adapt this work foranother language (like English) to assess the gen-eralization and efficiency of proposed approach.43ReferencesFred J. Damerau.
1964.
A technique for computer de-tection and correction of spelling errors.
Commun.ACM, 7:171?176, March.Victoria J. Hodge and Jim Austin.
2003.
A com-parison of standard spell checking algorithms anda novel binary neural approach.
IEEE Trans.
onKnowl.
and Data Eng., 15(5):1073?1081, Septem-ber.Aminul Islam and Diana Inkpen.
2009.
Real-wordspelling correction using google web it 3-grams.In Proceedings of the 2009 Conference on Empir-ical Methods in Natural Language Processing: Vol-ume 3 - Volume 3, EMNLP ?09, pages 1241?1249,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Daniel Jurafsky and James H. Martin.
2008.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Computational Linguis-tics and Speech Recognition.
Prentice Hall, secondedition, February.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Okan Kolak and Philip Resnik.
2002.
Ocr errorcorrection using a noisy channel model.
In Pro-ceedings of the second international conference onHuman Language Technology Research, HLT ?02,pages 257?262, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.Dekang Lin.
1998.
An information-theoretic def-inition of similarity.
In Proceedings of the Fif-teenth International Conference on Machine Learn-ing, ICML ?98, pages 296?304, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Walid Magdy and Kareem Darwish.
2006.
Arabic ocrerror correction using character segment correction,language modeling, and shallow morphology.
InProceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?06, pages 408?414, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Walid Magdy and Kareem Darwish.
2008.
Effect ofocr error correction on arabic retrieval.
Inf.
Retr.,11:405?425, October.Surapant Meknavin, Boonserm Kijsirikul, AnanladaChotimongkol, and Cholwich Nuttee.
1998.
Com-bining trigram and winnow in thai ocr error cor-rection.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguis-tics and 17th International Conference on Compu-tational Linguistics - Volume 2, ACL ?98, pages836?842, Stroudsburg, PA, USA.
Association forComputational Linguistics.Masaaki Nagata.
1996.
Context-based spelling cor-rection for japanese ocr.
In Proceedings of the 16thconference on Computational linguistics - Volume2, COLING ?96, pages 806?811, Stroudsburg, PA,USA.
Association for Computational Linguistics.Masaaki Nagata.
1998.
Japanese ocr error correctionusing character shape similarity and statistical lan-guage model.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics - Volume 2, ACL ?98, pages922?928, Stroudsburg, PA, USA.
Association forComputational Linguistics.Gonzalo Navarro.
2001.
A guided tour to approximatestring matching.
ACM Comput.
Surv., 33(1):31?88,March.Kazem Taghva and Eric Stofsky.
2001.
Ocrspell: aninteractive spelling correction system for ocr errorsin text.
International Journal of Document Analysisand Recognition, 3:2001.Xian Tong and David A. Evans.
1996.
A statisticalapproach to automatic ocr error correction in con-text.
In Proceedings of the Fourth Workshop onVery Large Corpora (WVLC-4, pages 88?100.Yuen-Hsien Tseng.
2002.
Error correction in a chi-nese ocr test collection.
In Proceedings of the 25thannual international ACM SIGIR conference on Re-search and development in information retrieval,SIGIR ?02, pages 429?430, New York, NY, USA.ACM.Li Zhuang, Ta Bao, Xioyan Zhu, Chunheng Wang,and S. Naoi.
2004.
A chinese ocr spelling checkapproach based on statistical language models.
InSystems, Man and Cybernetics, 2004 IEEE Interna-tional Conference on, volume 5, pages 4727 ?
4732vol.5.Justin Zobel and Alistair Moffat.
2006.
Inverted filesfor text search engines.
ACM Comput.
Surv., 38,July.44
