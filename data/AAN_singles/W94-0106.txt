DO WE NEED LINGUISTICS WHEN WE HAVE STATISTICS?
ACOMPARATIVE ANALYSIS OF THE CONTRIBUTIONS OF LINGUISTICCUES TO A STATISTICAL WORD GROUPING SYSTEMVasileios HatzivassiloglouDeparlrnent of Computer Science450 Computer Science BuildingColumbia UniversityNew York, N.Y. 10027vh@cs.columbia.eduABSTRACTWe present a comparative analysis of the performance ofa statistics-based system for the formation of semantic groupsof adjectives when various sources of linguistic knowledgeare introduced.
We identify four different ypes of slufllowlinguistic knowledge that are applicable to this system, andwe quantify the performance gained by incorporating eachsuch knowledge module, We perform experiments for dif-ferent corpus sizes and different inputs (sets of adjectives togroup), collect clam on the usaful.ness of each linguisticmodule, assess the statistical significance of the results, andcompare the contributions of the linguistic knowledge sourcesagainst each other.
We also assess the overall effect linguisticknowledge has in our system.
Our results how that linguisticknowledge causes asignificant increase in the performance ofthe system.
We conclude by discussing how these positiverestdts can be generalized to other problems in statisticalNLP.1.
INTRODUCTIONThe idea of integrating statistical andknowledge-based approaches for natural anguageproblems, has been.
recently.
.
gainin$ ground, in.
thecomputational lingmsucs commumty, as it is ex-pected that a combined approach will offer sig-nificantly better performance over eithermethodology alone.
This paper supplements hisintuitive belief with actual evaluatzon data, ob-tained when several linguistics-based moduleswere integrated in a statistical system.We used a system we previously developed forthe separation of adjectives into semantic groups\[Hatzivassiloglou and McKeown, 1993\] as thebasis for our comparative analysis.
We identifiedseveral different types of shallow linguisticknowledge that can be efficiently introduced intoour system.
We evaluated the system with and43without each such feature, obtaining an estimate ofeach feature's positive or negative contribution tothe overall performance.
By matching cases whereall system parameters are the same except for oneteature, we assess the statistical significance of thedifferences found.
Also, a statistical model of thesystem's performance in terms of the active fea-tures for each run offers a view of the contribu-tions of features from a different angle, contrastingthe significance of linguistic features (or othermodeled system parameters) against each other.Our analysis of the experimental resultsshowed that many forms of li%.uistic knowledgehave a significant positive conmbution to the per-formance of the system.
We attribute to the com-bined effect of the linguistic knowledge modulesthe ability of our system to perform fine-tunedclassification of adjectives into semantic lasses.Other statistical systems that address word clas-sification probleans do not emphasize the use oflinguistic knowledge and do not deal with aspecific word class\[Brown et al, 1992\], or do notexploit as much linguistic knowledge as we do\[Pereira et al, 1993\].
As a result, a coarser clas-sification is usually produced.
In contrast, bylimiting the system's input to adjectives, we cantake advantage of specific syntactic relationshipsand additional faltering procedures that apply onlyto ,particular word classes.
These sources of lin-gmstic knowledge provide in turn the extra eedgcfor discriminating among the adjectives at thesemantic level.Our" adjective grouping system can be used forapplications uch as natural ansuage generation(where knowledge of the semanuc groups and ofthe ordering of the elements within them allowsthe precise lexiealization of semantic concepts\[Elhadad, 1991\]) and computational lexicography(by automatically eompifing domain-dependentlists of synonyms and antonyms).
The producedgroups can also help correct erroneous usage ofmultiple qualifiers that are superfluc~ts or con-tradict each other, a phenomenon that has been ob-served in medical reports 1.
But in addition to theimmediate applications of word classification,many other sfatistical NLP applications can be castin a similar framework.
Therefore, the positive f-fects of linguistic knowledge on our system in-dicate that the incorpo/'ation of linguisticknowledge will probably result in similar b~efitsfor other applications as well.In what follows, we briefly review our adjec-tive grouping system, and then present the !ingui.
's -tic features we explored and the alternatives toreach of them.
In Section 5 we give the results ofour evaluation on different combinations of fea-tures and we analyze their significance.
We also~rresent these results in a predictor-responseamework, and we conclude by discussing the ap-plicability of our results to other NLP problems.2.
AN OVERVIEW OF THE ADJECT IVEGROUPING SYSTEMOur adjective grouping system \[Hatzi-vassilogiou and McKeown, 1993\] starts with a setof adjectives to be clustered into semantically re-lated groups.
Ideally, we want highly relatedwords such as synonyms, antonyms, andhyponyms to be the only ones placed in the samegroup.
The system is given ~e number of groupsto form as an input parameter 2, and has access to atext corpus.
No semantic information about theadjectives is available to the system.
The systemoperates by extracting pairs of modified nouns foreach adjective, and, optionally, pairs of adjectivesthat we can expect o be semantically uurelated onlinguistic grounds 3.
From the estimated istribu-tion of modified nouns for each adjective, asimilarity sccxe is assigned to each possible pair ofadjectives.
This is based on KendaLl's x, a non-parametric, robust estimator of correlation\[Kendall, 193811 Using the similarity scores and,optionally, the established relationships of non-relatedness, a non-hierarchical clustermg method\[Spath, 1985\] assigns the adjectives to groups in away that maximizes the within-group similarity(and therefore also maximizes the between-groupdissimilarity).IWe thank Johanna Moore for pointing out this applicationto us.2Determining this number from the data is probably thehardest problem in cluster analysis in general; see \[Kaufmanand Rousseeuw.
1990\].J~hese are adjectives that either modify the same noun inthe same NP (e.g.
big white house) or one of them modifiesthe other (e.g.
light blue coat); see \[Hatzivassiloglou andMcKeown.
1993\] for a detailed analysis.441.
deadly fatal2.
capitalist socialist?
3. clean dirty dumb4.
hazardous toxic5.
insufficient scant6.
generous outrageous nreasonable7.
endless protracted&plain9.
hostile unfi~endlyI0.
delicate fragile unstable1 I. affluent impoverished prosperous12.
brilliant clever energetic smart stupid13.
communist leftist14.
astonishing meager vigorous15.
catastrophic disasm3ns harmful16.
dry exotic wet17.
chaotic turbulent18.
confusing misleading19.
dismal gloomy20.
dual multiple pleasant21.
fat slim22.
affordable inexpensive23.
abrupt gradual stunning24.
flexible lenient rigid strict stringentFigure 1: Example clustering found by thesystem using all linguistic modules.To evaluate our system, we have developedextended versions of the standard informationretrieval measures precision, recell, and fallout.These extended versions score the groupingproduced .by the system against a set of modelgroupings (mstead of just one) for the same adjec-fives, supplied by humans.
In the experimentsreported m this paper, we employ 8 or 9 human-constructed models for each adjective set.
Webase our comparisons on and report he F-measurescores \[Van Rijsbergen, 1979\] which combineprecision and recall in a single number.
In ad-dition, since the  correct number of groupings issomething that the system cannot yet determine(and, incidentally, something that humanevaluators disagree abou0, we run the system forthe five ceases m the range -2 to +2 around theaverage number of clusters employed by thehumans and average the results.
This smoothingoperation prevents an accidental hilgh or low scorebeing reported when a small variauon in the num-ber of clusters produces very different scores.It should be noted here that the scores reportedshould not be interpreted as linear percentages.
Inother words, a score of 40 is not just twice as goodas a score of 20, and going from 30 to 40 is muchharder than goin$ from 20to 30.
The latter is truefor most applicanons, but the problem of interpret-ing the scores is exacerbated in our context be-cause of the structural constraints imposed by theclustering and the presence of multiple models.Furthermore, ven the best clustering that could beproduced would not receive a score of 100, be-cause of disagreement among humans on what isthe correct answer.
To e l ,~  the meaning of thescores, we accompany them with lower and _upperbounds for each adjective set we examine.
Thesebounds are obtained by the performance of a sys-tem that creates random groupings (averaged overmany runs) and by the average score of thehuman-produced partitions when evaluated againstthe other human models respectively.Figure 1 shows an example clusteringproduced by our system for one of the adjectivesets analyzed in this paper.3.
THE L INGUIST IC  FEATURES BEINGTESTEDWe have identified several sources of linguis-tic knowledge that can be incorporated in our sys-tem, augmenting the statistical component.
Eachsuch source represents a parameter of the system,i.e.
a Ieature that can be present or absent or moregenerally take a value from a predefined set.
Weselected features that can be effficienfly computedin a completely automatic way for unrestricted textand do not require extensive amounts ofknowledge to be available to the system.
Almostall of these features can be ~generalized to other ap-plications as well, as we discuss in Section 6.
Inthis section we discuss first one of theseparameters that can take several values, namelythe method of extracting data from the corpus, andthen several other bina~-valued features.453.1 Extracting data from the corpusOur adjective clustering system determines thedistribution of related (modified) nouns for eachadjective and eventuaUy the similarity between ad-jectives from pairs of the form (adjective.modified noun)observed in the corpus.
Direct in-formation about incompatible adjectives (in theform of appropriate adjective-adjective pairs) cart~lso be collected from the corpus.
Therefore, atWst parameter of the st, stem and a possible dimen-sion for comparisons t the method employed toidentify such pairs in free text.
This is hardly aunigue feature of our system: all word-basedstatistical systems must fwst collect data from thecorpus about the words of interest, on which thesubsequent statistics operate 4 .There are several alternate models for this taskof data collection, with different degees of lin-guistic sophistication.
A first model Is to use nolinguistic knowledge at all: we collect for each ad-jective of interest all words that fall within a win-dow of some predetermined size.
Naturally.
nonegative data (adjective-adjective pairs) can becollected with this method.
However, the methodcan be implemented easily and does not requirethe identification of any linguistic onstraints so itis completely general.
It has been used for diverseproblems uch as machine translation and sensedisambiguation \[Gale t al., 1992, Schiltze, 1992\].A second model is to restrict he words col-lected to the same sentence as the adjective of in-terest and to word elass(es) that we expect on lin-guistic grounds to be relevant o adjectives.
Forour application, we collect all nouns in the vicinityof an adjective without leaving the current sen-tence.
We assume that these nouns have somerelationship with the adjective and that seman-tically different adjectives will exhibit differentcollections of such nouns.
This model requiresonly part-of-speech information (to identifynouns) and a method of detecting sentence boun-daries.
It uses a window of fixed length to definethe neighborhood f each adjective.
Such a modelincorporates minimal linguistic knowledge.namely in determining what constitutes the infor-mative class(es) of words collected (nouns in ourproblem).
Again, negative knowledge such as in-compatible adjective pairs cannot be collectedwith this model.
Nevertheless, it has also beenwidely used, e.g.
for collocation extraction\[Sm~ja, 1993\] and sense disambiguation \[Liddyand Park, 1992\].A third model uses a simple linguistic rule toidentify pairs of interest that is even more restric-tive and informative than the "nouns in vicinity"4Although frequently details of the statistical modelemployed receive more consideration.approach.
Since we are interested in .nounsmodified by adjectives, uch a rule is to correct anoun immediately following an adjective, assum-ing that this implies a modification relationship.Pairs of consecutive adjectives Can also be col-lected.Up to this point we have successivelyrestricted the collected pairs on linguistic grounds,so that less but cleaner data is collected.
For thefourth model, we extend the simple rule givenabove, using linguistic information to catch morevalid pairs without sacrificing accuracy.
Weemploy a pattern matcher that retrieves any se-quence of one or more adjectives followed by anysequence of zero or more nouns.
These sequencesare then analyzed with heuristics based on linguis-tics to obtain pairs.
For example, it can be shownthat all adjectives in such a sequence must besemantically unrelated, and that it is best to attachall the adjectives to the final noun.The regular expression and pattern matchingrules of the previous model can be extended fur-ther, forming a grammar for the constructs of in-terest.
This approach can detect more pairs, and atthe same time address known problematic casesnot detected by the previous models.We imp.lemented the above five data extractionmodels, using typical window sizes for the firsttwo methods (50 and 5 on each side of the windowrespectively) which have been found useful inother problems before.
For the fifth model, wedeveloped a finite-state grammar for NPs which isable to handle both predicative and attributivemodification of nouns, conjunctions of adjectives,adverbial modification of adjectives, quantifiers,and apposition of adjectives to nouns or otheradjectives 5.
Unfortunately, the resources requiredto perform our tests for the first model were toogreat (e.g.
12,287,320 pairs in a 151 MB file wereextracted for the 21 adjectives in our smallest testset) so we dropped that model from further con-sideration and we use the second model as thebaseline of minimal inguistic knowledge.
Otherresearchers have also reported similar problems ofexcessive resource demands with the "collect allneighbors" model \[Gale t al., 1992\].3.2 Other l inguistic featuresIn addition to the data extraction method, weidentified three other areas where linguisticknowledge can be introduced in our system.
F'trst,we can employ morphology to convert plural5For efficiency reasons we did not consider a more power-ful formalism.antitrust newbig oldeconomic politicalfinancial potentialforeign realglobal seriousinternational severelegal staggeringlittle technicalmajor unexpectedmechanicalFigure 2: Test set 1; from an earlier corpus.nouns to the corresponding singular ones and ad-jectives in comparative or superlative degree totheir base form.
Almost all adjectives and nounsthat appear in multiple forms have no semanticdifference from their base form except for thenumber or deg'e  feature.
This conversion com-bines counts of similar pairs, thus raising the ex-pected and estimated fr~,quencies of each pair inany statistical model.
We develoI.~zl a m0rphol-ogy component that produces the singular fohn ofnouns nsmg rules pins a large table of exceptions.For adjectives, a set of rules is _again employed butbecause of the vowed in the suffix -er or -est, manybase forms look plausible without a lexicon (e.g.bigger could have been produced from big, bi~g,or bigge).
We solve this problem ~ counting meoccurrences of each candidate form in our corpusand selecting the one with non-zero frequency.Another potential application of linguisticknowledge is the use o f  it spell-checking proce-dure, combined with a word list, to eliminatetypographical errors from the corpus.
Such en'orscan produce wrong estimates for the frequenciesof modified nouns for an adjective, but most im-portant!y introduce "unique" nouns appea~'ingonly with one adjective, skewing the comparisonof noun distributions.
We implemented this com-ponent using the Unix spell program and as-sociated word list, with extensions for hyphenatedcompounds.
Uni~ortm~tedy, since a fixed anddomgiu independent lemcon is used for thisprocess, some valid but overspecialized wordsmay be discarded too.Finally, we can use additional sources ofknowledge which supplement the primarysimilarity relationships and are justified on linguis-tic grounds.
We identified several potentialsources of additional knowledge that can be ex-tracted from the corpus (e.g.
conjunctions of ad-jectives).
In this comparison study we im-plemented and consider the significance of one ofthese knowledge sources, r~medy the negative x-amples offered by adjective-adjective pairs.464.
THE COMPARISON EXPERIMENTSIn the previous section we identified fourparameters of the system, the effects of which wewant to analyze.
But in addition to theseparameters that can be directly varied and havepredetermined possible values, several other vari-ables can affect he performance of the system.First, the performance of the system dependsnaturally on the adjective set that is to be clus-tered.
Presumably variations in the adjective setcan be modeled by several parameters, uch assize of the set, number of semantic groups in it,and strength of semantic relatedness among itsmembers, plus several parameters describing theproperties of the adjectives in the set in isolation,such as frequency, specificity, etc.A second variable that affects the clustering isthe corpus that is used as the main knowledgesource, through the observed cooeeurrence pat-terns.
Again the effects of different corpora can beseparatecl into several factors, e.g.
the size of thecorpus, its generality, the genre of  the texts, etc.Since in this paper we are interested in quan-tifying the effect of the linguistic knowlcdse in oursystem, or more precisely of the linguisticknowledge that we can explicitly control throughthe four parameters discussed above, we did notattempt o model in detail the various factorsentering the system as a result of the choice of ad-annual negativebig netchief newcommercial nextcurrent olddaily pastdifferent positivedifficult possibleeasy pre-taxfinal previousfuture privatehard publichigh quarterlyimportant recentinitial regionalinternational seniorlikely significantlocal similarlow smallmilitary strongmodest weaknationalFigure 3: Test set 2; high frequency words..~ective set and corpus.
However, we are interestedin measuring the effects of the linguisticparameters in a wide range of contexts, and m cor-relating these effects with variables originatingfrom the choice of corpus and adjective set.
Forexample, we would want to be able to detect hatthe linguistic parameter "morphology" is sig-nificant for small corpora but not for large ones, ifthat were the cease.
Therefore, we included in ourmodel two additional parameters, representing thecorpus and the adjective set used.We used the Wall Street Journal articles fromthe ACL-DCI as our corpus.
We selected four sub-corpora of decreasing size to study the relationshipof corpus size with linguistic feature effects: allthe 1987 articles (21 million words), every third ofthese articles (7 million words), every twenty-first(1 million words), and articles no.
50 and 100(330,000 words).
Since we use subsets of the samecorpus, we are essentially modeling the corpussize parameter only.abrupt hazardousaffluent hostileaffordable impoverishedastonishing inexpensivebrilliant insufficientcapitalist leftistcatastrophic lenientchaotic meagerclean misleadingclever multiplecommunist outrageousconfusing plaindeadly pleasantdelicate prosperousdirty protracteddisastrous rigiddismal scantdry slimdual smartdumb socialistendless strictenergetic stringentexotic stunningfat stupidfatal toxicflexible turbulentfragile unfriendlygenerous unreasonablegloomy unstablegradual vigorousharmful wetFigure 4: Test set 3; low frequency words.47ParameterExtraction Model ParsingMorphologySpell-checkingUse of negativeknowledgeValue i score30.29Pattern Matching 28.88Observed Pairs 27.87Nouns in Vicinity 22.36Yes 28.60No 27.53Yes 28.12No 28.00Yes 29.40No 28.63Table 1: Average scores when only one featureis changed.For each corpus, we analyzed three differentsets of adjectives, listed in figures 2-4.
The first ofthem was selected from a similar corpus, contains21 frequent and ambiguous words that all as-sociate strongly with a particular noun (problem),and was analyzed in \[Hatzivassiloglou andMcKeown, 1993\].
The second set (43 adjectives)was saected with the constraint that it containhigh frequency adjectives (more than 1,000 occur-rences in the 21 million word corpus).
The thirdset (62 adjec.fives) satisfies the opposite constraintcontaining adjectives of relatively low frequency(between 50 and 250).
Figure 1 shows a typical uping found by our system for the third set of a jectives, when the full corpus and all linguisticmodules were used.These three sets of adjectives represent variouscharacteristics of the adjective sets that the systemmay be c,~led to.
duster.
First, they explicitlyrepresent increasing sizes of the groupingproblem.
The second and third sets also contrastthe independent frequencies of their member ad-JfreCtives.
Furthermore, we have found that the lessequent adjectives of the third set tend to be morespecific than the more frequent ones.
The humanevaluators reported that the task of classificationwas easier for the third set, and their models ex-hibited about he same degree of agreement for thesecond and third sets although the third set is sig-nificantly larger.
We plan to investigate thegenerality of this inverse correlation between fre-quency and specificity in the future.By including the parameters "corpus size"and "adjective set", we have six parameters thatwe can vary in our experiments.
Any remainingtactors affecting the performance of our systemare modeled as random noise, so staUsticalmethods are used to evaluate the effects of theselected parameters.
The six chosen parameters48are completely orthogonal, with the exception thatparameter "negative knowledge" must have thevalue "not used" when parameter "extractionmodel" has the value "nouns in vicinity".
In or-der to avoid introducing imbalance in our experi-ment, we constructed a complete designed experi-merit \[Hicks, 1973\] for all their (4x2- l )x2x2x4 x 3 = 336 valid combinations 6.5.
RESULTS5.1 Average effect of each linguisticparameterSpace limitations do not allow us to present thescores for every one of the 336 individual experi-ments performed, corresponding to all valid com-binations of the six modeled parameters.
Insteadwe present several summary measures.
Wemeasured the effect of eachparticular setting ofeach linguistic parameter of Section 3 by averag-ing the scores obtained in all experiments wheremat particular parameter had thatparticular value.In this way, Table 1 summarizes the differences inthe performance of the system caused by eachparameter.
Because of the complete design of theexperiment, each value in Table 1 is obtained inruns that are identical to the runs used for estimat-ing the other values of the same parameter xceptfor the difference in the parameter itself 7.Table I shows that there is indeed improve-.ment with the introduction of any of the proposedlinguistic teatures, or with the use of a lingnis-ticfilly more sophisticated xtraction model.
To as-sess the statistical significance of these dif-ferences, we compared each run for a particularvalue of a parameter tothe corresponding identicalvalueo '(exc ptflOr that parameter) run for a differentu me parameter.
Each pair of values for aparan~eter p oduces~ in this way a set of paired ob-servations, on eacn of these sets, we performed asign test \[Gibbons and Chakrahorti, 1992\] of thenull hypoth~is that there is no real difference inthe system s performance between the two values,i.e.
that any observed ifference is due to chance.We counted the number of times that the first ofthe two compared values led to superior perfor-mance relative to the second, distributing tiesequally between the two cases.
Under the nullhypothesis, the number of times that the first value6RecaU that a designed experiment is complete when atleast one trial, or run.
is performed for every valid combina-tion of the modeled predictors.7The slight asymmetry in parameters "extraction model"and "negative knowledge" is accounted for by leaving outnon-matching runs.Parameter testedExtraction modelMo*hologySpell.checkingNegative knowledgel~irst ValueParsingTestS nd Value'Pattern matchingParsing Observed pairs!
Parsing Nouns in vicinRyPattern matching Observed pairsPattern matchingObserved pairsUsedUsedUsedNouns in vicinityNouns in vicinityNot usedNot usedNot usedComparisons First value better Probabilitythan second96 64 0'.001496 66 0.000348 42 10 -796 61 0.01044848168168144413610794976.24:'10-70.00070.0005O.
14253.756-10 -5Table 2:performs better follows the binomial distributionwith parameter p=0.5.
Table 2 gives the results ofthese tests along with the probabilities that thesame or more extreme results would be encoun-tered by chance.
We can see from the table that alltypos of linguistic knowledge except spell-checking have a beneficial effect that is statis-tically stgnificant at, or below, the 1% level.Statistical tests of the difference in performance offered by each linguistic feature.5.2 Comparison among the linguisticfeaturesIn order to measure the significance of the con-tribufion of each linguistic feature relative to theother linguistic features, we fitted a linear regres-sion model \[Draper and Smith, 1981\] to the data.We use the six parameters of our experiments asthe predictors, and the measured F-score of thecorresponding clustering as the response variable.In such a model the response Y is assumed to be alinear function of the predictors, i.e.Y=bo+bl.Xl+b2.X2+... +bn'X n (1)where X i is the i-th predictor and bi is its ccor-responding weight s .
The weights found by the fit-ring process (Table 3) indicate by their absolutemagriitude and sign how important each predictoris and whether it contributes positively or nega-tively to the final result.
Numerical values such asSSueh a model is appropriate for comparative purposes,although extrapolating response values for prediction outsidethe range of predictor values used in the fitting may giveincorrect results.
For example, the coefficients in Table 3earmot be used to predict the score when the corpus is fig-niticanfly smaller than 0.33 Mbytes or larger than 21 Mbytes.the corpus size enter formula (1) directly aspredictors, so Table 3 indicates that each ad-ditional megabyte of text increases the perfor-mance of the system by 0.9417 on the average.For binary features, the weights in Table 3 indicatethe increase in the system's performance when thefeature is present, so introduction of morphologyimproves the system's performance by 0.5371 onthe average.
For the categorical variables "extrac-tion model" and "adjective set", the weightsshow the change in score for the indicated value incontrast to the base case (minimal linguisticknowledse represented by extraction model"nouns m vicinity" and adjective set 1 respec-tively).
For example, using the finite-state parserinstead of the "nouns in vicinity" model improvesVariable WeightIntercept 18.7997Corpus size (in megabytes) 0.9417Extraction method (Pairs) 5.1307Extraction method (Sequences) 6.1418Extraction method (Parser) 7.5423Morphology 0.5371Spelling 0.0589Adjective Set (2) 2.5996Adjective Set (3) - 11.4882Use of negative knowledge 0.3838Table 3: Fitted coefficients for linear egressionmodel.49Adjective Set 1 Adjective Set 2 Adjective Set 3Random partitions 9.66 6.21 3.80No linguistic omponents active 24.51 38.51 33.21All linguistic omponents active 39.06 44.73 46.17 IHumans 53.98 64.27 63.07Table 4: Performance of a random classifier, of the system on the 21 million word corpus,and of the humans.the score by 7.5423 on the average, while goingfrom adjecuve set 2 to adjective set 3 decreasesthe score by -(-2.5996-11.4882) = 14.0878 on theaverage.
Finally the intercept b0 gives a baseline erformance of a minimal system that uses the ase case for each pararneter; the effects of corpussize are to be added to this system.From Table 3 we can see that the data extrac-tion model has a significant effect on the quality ofthe produced clustering, and among the fingutsticparameters i the most important one.
Increasingthe size of the corpus also significantly increasesthe score.
The adjective set that is clustered alsohas a major influence on the score, with rarer ad-jectives leading to worse clusterings.
The two lin-guistic feat~u~e s "morphology" and "negativeknowledge' have less pronounced although stillsil~nificant effects, while spell-cbecking offersminimal improvement that probably does not jus-tify the effort of implementing the module and thecost of activating it at run-time.5.3 Overall effect of linguistic knowledgeUp to this point we have described averages ofscores, taken over many combinations of featuresthat are orthogonal to the one studied.
Theseaverages are good for describing the existence of adifference caused by the different values of eachfeature, across all possible combinations of theother features.
They are not, however, repre-sentative of the performance of the system m aparticular setting of parameters, nor are theysuitable for describing the difference in featuresquantitatively, since they are averages taken overwidely differing settings of the system'sparameters.
In particular, the inclusion of verysmall corpora drives the average scores down, aswe have confirmed in a more detailed analysiswhere averages were computed separately for eachvalue of the corpus size parameter.
To gtve a feel-ing of how important the introduction of linguisticknowledge is quantitatively, we compare in Table4 the results obtained for the full corpus of 21 mil-lion words for the two cases of having all or noneof the linguistic omponents active.
The scores ob-tained by a random system that produces partitions50of the adjectives with no knowledge xcept thenumber of groups are included as a lower bound.These estimates are obtained after averaging thescores of 20,000 such random partitions for eachadjective set.
The average scores that each humanmodel receives when compared to all the otherhuman models are also included, since theyovide an estimate of the maximum score that canachieved by any system.
That maximumdepends on the disagreement between models foreach adjective set.
For these measurements weusea smaller smoothing window of size 3 instead of 5,which is fairer to the system when its performanceis compared to the humans.
We also give in Figure5 the grouping produced by the system witlaoutusing any of the linguistic modules for adjectiveset 3; this is to be contrasted with Figure I.6.
GENERAL IZ ING TO OTHERAPPL ICAT IONSIn the previous ection, we showed that the in-troduction of linguistic knowledge in our systemproduces a performance difference, which is notonly statistically observable but also quantitativelysignificant (cf.
Table 4).
We believe that thesepositive results should also apply to other corpus-based NLP systems that employ statisticalmethods.
Many of the linguistic components ofour system, including the extraction model thatwas shown to be the most important linguisticparameter, are not specific to the word groupingproblem.
They can thus be directly incorporated insystems designed for other problems but essen-tially following the same basic architecture asours.Many statistical approaches hare the samebasic methodology with our system: a set of wordsis preselected, related words are identified in acorpus, the frequencies of words and of pairs o frelated words are estimated, and a statistical modelis used to make predictions for the original words.Across appfications, there are differences in whatwords are selected, how related words are defined,and what kind of predictions i  made.
Neverthe-less, the basic components stay the same.
For ex-ample, in our appfication the original words are1.
catastrophic harmful2.
dry wet3.
lenient rigid strict stringent4.
communist leftist5.
flexible hostile protracted unfriendly6.
abrupt chaotic disastrous gradualturbulent vigorous7.
affluent affordable inexpensiveprosperous8.
outrageous9.
capitalist socialistlO.
dismal gloomy pleasant11.
generous insufficient meager scantslim12.
delicate fragile13.
brilliant energetic14.
dual multiple stupid15.
hazardous toxic unreasonableunstable16.
plain17.
confusing18.
clever19.
endless20.
clean dirty impoverished21.
deadly fatal22.
astonishing misleading stunning?
23. dumb fat smart24.
exoticFigure 5: Example clustering found by thesystem using no linguistic modules.the adjectives and the predictions are their groups:in machine translation, the predictions are thetranslations of the words in the source languag~text; in sense disambiguation, the predictions arethe senses assigned to the words of interest; inpart-of-speech tagging or in classification thepredictions are the t,~s or classes assigned to eachword.
Because of this underlying similarity, thecomparative analysis presented m the paper isrelevant to all these problems.For a concrete xample, we examine the caseof collocation extraction that has been addressedwith statistical methods in the past.
Smadja\[1993\] describes a system that imtially uses the"nouns in vicinity" extraction model to collectcooccurrence information about words, and thenidentifies collocations on the basis of distributionalcriteria.
A later component falters the retrieved col-locations, removing the ones where the participat-ing words are not used consistently in the samesyntactic relationship.
This post-processing stagedoubles the precision of the system.
We believethat using from the start a more sophisticated ex-traction model to collect these pairs of relatedwords will have similar positive effects.
Other lin-guistic components, such as a morphology modulethat combines frequency counts, should also im-prove the performance of the system, In this way,we can benefit from linguistic knowledge withouthaving to use a separate filtering process after ex-pending the effort o coUeet he collocations.Similarly, the sense disambiguation problem istypically attacked by comparing the distribution ofthe neighbors of a word's occurrence toprototypi-cal distributions associated with each of theword's senses \[Gale et al, 1992, Schtltze, 1992\].Usually, no explicit linguistic knowledge is usedin defining these neighbors, which are taken as allwords appearing within a window of fixed widthcentered-at the word being disambiguated.
Manywords unrelated to the word of interest are col-lected in this way.
In contrast, identifying ap-propriate word classes that can be expected on lin-guistic grounds to convey significant informationabout he original word should increase the perfor-mance of the disambiguation system.
Such classesmight be modified nouns for adjectives, nouns in asubject or object position for verbs, etc.
As wehave showed in Section 5, less but cleaner infor-mation increases the quality of the results.An interesting topic is the identification ofparallels of our fingulstic modules for these ap-plications, at least for those modules which, unlikemorphology, are not ubiquitous.
Negativeknowledge for example improves the performanceof our system, supplementing the positive infor-mation provided by adjective-noun pairs.
It couldbe useful for other systems as well if an ap-propriate application-dependent me hod of extract-mg such information is identified.517.
CONCLUSIONS AND FUTURE WORKWe have showed that all lin$uistic featuresconsidered in this study had a posiuve contributionto the performance of the system.
Except for ~spell-checking, all these contributions were both statis-tically significant and large enough to make a dif-ferencce in practical situations.
Furthermore, theresults can be expected to generalize to a widevariety of corpus-based systems for different ap-plications.The cost of incorporating the linguistics-basedmodules in the system is not prohibitive.
The ef-fort needed to implement all the linguistic moduleswas about 5 person-months, in contrast with 7person-months needed to develop the basic statis-tical system.
Furthermore, the run-time overheadcausedby the linguistic modules is not significant.Each takes from l to  7 minutes on a Sun SparcSta-tion 10 to process a million entries (words orpairs) and all except the negative knowledgemodule need process a corpus only once, reusingthe same information for different adjective sets.This should be compared to the approximately 15minutes needed by the statistical component forgrouping about 40 adjectives.In the future, we plan to extend the results dis-cussed in this paper by an analysis of the depen-dence of the effects of each parameter on thevalues of the other parameters.
We are currentlystratifying the experimental data obtained to studytrends in the magnitude of parameter effects asother parameters vary in a controlled manner, andwe will examine the interactions with corpus sizeand specificity of clustered adjectives.
We arealso interested in providing similar quantitativeresults for other applications, to corroborate ourbelief in the generality of the importance of easilyobtainable linguistic knowledge for statistical sys-tems.ACKNOWLEDGEMENTSThis work was supported jointly by ARPA andONR under contract N00014-89-J-1782, by NSFGER-90-24069, and by New York State center forAdvanced Technology contract NYSSTF-CAT(91)-053.
I wish to thank Kathy McKeown,Jacques Robin, and the workshop organizers forproviding useful comments on earlier versions ofthe paper.REFERENCESBrown P.. Della Pietra V., deSouza P., Lai J., and Mer-cer R. (1992).
Class-based n-gram Models ofNatural Language.
Computational Linguistics.18"4.467-479.Draper, H, R. and Smith, H. (1981).
Applied Regres-sion Analysis (2rid ed.).
New York: Wiley.Elhadad, Michael.
(1991).
Generating Adjectives toExpress the Speaker's Argumentative inure.Proceedings of the 9th National Conference onArttficial Intelligence (AAAI 91).
Annheim.Gale, W. A., Clmrch.
K. W., and Yarowsky, D. (1992).Work on Statistical Methods for Word Sense Dis-ambiguation.
Probabilistic Approaches toNatural Language: Papers from the 1992 FallSymposium.
AAAI.Gibbons, Jean Dickinson and ChAIn'aborti, Subhabrata.(1992).
Nonparametric Statistical Inference (3rded.).
New York: Marcel Deld~er.Hatzivassiloglou, Vasileios and MeKeown, Kathleen.
(Iune 1993).
Towards the Automatic Ideatifica-tion of Adjectival Scales: Clustering AdjectivesAecor'dlnoe to MeJmin~l Proceedings of the 31stAnnual Meeting of the ACL.
Columbus, Ohio:Association for Computational Linguistics.Hicks, C.R.
(1973).
Fundamental Concepts in theDesign of Experiments.
New York: Holt.Rinehart, and Wilson.Kaufinan, L. and Rousseeuw, PJ.
(1990).
FindingGroups in Data: An Introduction to ClusterAnalysis.
New Yedl:: Wiley.Kendall, M.G.
(1938).
A New Measure of Rank Cor-relation.
Biometrika, 30, 81-93.Liddy, Elizabeth D. and Paik; Woojin (1992).Statistically-Guided Word Sense Disambiguadon.Probabilistic Approaches to Natural Language:Papers from the 1992 Fall Symposium.
AAAI.Pereira F., Tishby N.0 and Lee L. (June 1993).
Dis-tributional Clustexing of l~Jn~li~h Words.Proceedings of the 31st Conference of the ACL.Columbus, Ohio: Associatiea for ComputationalLi uisti .Sehtttze, Him'ieh.
(July 1992).
Word Sense Dis-ambiguation With Sublexical Representations.Proceedings of the AAAI-92 Workshop onStatistically-Based NILP Techniques.
AAAI.Smadja, Frank.
(March 1993).
Retrieving Collocatims~om Text: Xtract.
Computational Linguistics.19:1.
143-177.Spath, Helmuth.
(1985).
Cluster Dissection andAnalysis: Theory, FORTRAN Programs,Examples.
Cldchester, West Sussex.
England:Ellis Horwood.Van Rijsbergen, C.J.
(1979).
Information Retrieval(2rid ed.).
London: Butterwoths.52
