Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 783?793, Dublin, Ireland, August 23-29 2014.Soft Cross-lingual Syntax Projection for Dependency ParsingZhenghua Li , Min Zhang?, Wenliang ChenProvincial Key Laboratory for Computer Information Processing TechnologySoochow University{zhli13,minzhang,wlchen}@suda.edu.cnAbstractThis paper proposes a simple yet effective framework of soft cross-lingual syntax projection totransfer syntactic structures from source language to target language using monolingual treebanksand large-scale bilingual parallel text.
Here, soft means that we only project reliable dependenciesto compose high-quality target structures.
The projected instances are then used as additionaltraining data to improve the performance of supervised parsers.
The major issues for thisidea are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsicsyntactic non-isomorphism between languages; 3) incomplete parse trees after projection.
Tohandle the first two issues, we propose to use a probabilistic dependency parser trained on thetarget-language treebank, and prune out unlikely projected dependencies that have low marginalprobabilities.
To make use of the incomplete projected syntactic structures, we adopt a newlearning technique based on ambiguous labelings.
For a word that has no head words afterprojection, we enrich the projected structure with all other words as its candidate heads as longas the newly-added dependency does not cross any projected dependencies.
In this way, thesyntactic structure of a sentence becomes a parse forest (ambiguous labels) instead of a singleparse tree.
During training, the objective is to maximize the mixed likelihood of manually labeledinstances and projected instances with ambiguous labelings.
Experimental results on benchmarkdata show that our method significantly outperforms a strong baseline supervised parser andprevious syntax projection methods.1 IntroductionDuring the past decade, supervised dependency parsing has made great progress.
However, due tothe limitation of scale and genre coverage of labeled data, it is very difficult to further improve theperformance of supervised parsers.
On the other hand, it is very time-consuming and labor-intensive tomanually construct treebanks.
Therefore, lots of recent work has been devoted to get help from bilingualconstraints.
The motivation behind are two-fold.
First, a difficult syntactic ambiguity in one languagemay be very easy to resolve in another language.
Second, a more accurate parser on one language mayhelp an inferior parser on another language, where the performance difference may be due to the intrinsiccomplexity of languages or the scale of accessible labeled resources.Following the above research line, much effort has been done recently to explore bilingual constraintsfor parsing.
Burkett and Klein (2008) propose a reranking based method for joint constituent parsingof bitext, which can make use of structural correspondence features in both languages.
Their methodneeds bilingual treebanks with manually labeled syntactic trees on both sides for training.
Huang etal.
(2009) compose useful parsing features based on word reordering information in source-languagesentences.
Chen et al.
(2010a) derive bilingual subtree constraints with auto-parsed source-languagesentences.
During training, both Huang et al.
(2009) and Chen et al.
(2010a) require bilingual text withtarget-language gold-standard dependency trees.
All above work shows significant performance gain?Correspondence authorThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/783over monolingual counterparts.
However, one potential disadvantage is that bilingual treebanks andbitext with one-side annotation are difficult to obtain.
Therefore, They usually conduct experiments ontreebanks with a few thousand sentences.
To break this constraint, Chen et al.
(2011) extend their workin Chen et al.
(2010a) and translate text of monolingual treebanks to obtain bilingual treebanks with astatistical machine translation system.This paper explores another line of research and aims to boost the state-of-the-art parsing accuracyvia syntax projection.
Syntax projection typically works as follows.
First, we train a parser on source-language treebank, called a source parser.
Then, we use the source parser to produce automatic syntacticstructures on the source side of bitext.
Next, with the help of automatic word alignments, we project thesource-side syntactic structures into the target side.
Finally, the target-side structures are used as gold-standard to train new parsing models of target language.
Previous work on syntax projection mostlyfocuses on unsupervised grammar induction where no labeled data exists for target language (Hwa et al.,2005; Spreyer and Kuhn, 2009; Ganchev et al., 2009; Liu et al., 2013).
Smith and Eisner (2009) proposequasi-synchronous grammar for cross-lingual parser projection and assume the existence of hundredsof target language annotated sentences.
Similar to our work in this paper, Jiang et al.
(2010) try toexplore projected structures to further improve the performance of statistical parsers trained on full-scalemonolingual treebanks (see Section 4.4 for performance comparison).The major issues for syntax projection are 1) errors from the source-language parser and unsupervisedword aligner; 2) intrinsic syntactic non-isomorphism between languages; 3) incomplete parse trees afterprojection.
Hwa et al.
(2005) propose a simple projection algorithm based on the direct correspondenceassumption (DCA).
They apply post-editing to the projected structures with a set of hand-crafted heuristicrules, in order to handle some typical cross-lingual syntactic divergences.
Similarly, Ganchev et al.
(2009) manually design several language-specific constrains during projection, and use projected partialstructures as soft supervision during training based on posterior regularization (Ganchev et al., 2010).To make use of projected instances with incomplete trees, Spreyer and Kuhn (2009) propose a heuristicmethod to adapt training procedures of dependency parsing.
Instead of directly using incomplete treesto train dependency parsers, Jiang et al.
(2010) train a local dependency/non-dependency classifier onprojected syntactic structures, and use outputs of the classifier as auxiliary features to help supervisedparsers.
One potential common drawback of above work is the lack of a systematic way to handleprojection errors and incomplete trees.Different from previous work, this paper proposes a simple yet effective framework of soft syntaxprojection for dependency parsing, and provides a more elegant and systematic way to handle theabove issues.
First, we propose to use a probabilistic parser trained on target-language treebank, andprune unlikely projected dependencies which have very low marginal probabilities.
Second, we adopta new learning technique based on ambiguous labelings to make use of projected incomplete treesfor training.
For a word that has no head words after projection, we enrich the projected structureby adding all possible words as its heads as long as the newly-added dependency does not cross anyprojected dependencies.
In this way, the syntactic structure of a sentence becomes a parse forest(ambiguous labelings) instead of a single parse tree.
During training, the objective is to maximizethe mixed likelihood of manually labeled instances and projected instances with ambiguous labelings.Experimental results on benchmark data show that our method significantly outperforms a strong baselinesupervised parser and previous syntactic projection methods.2 Syntax ProjectionGiven an input sentence x = w0w1...wn, a dependency tree is d = {(h,m) : 0 ?
h ?
n, 0 < m ?
n},where (h,m) indicates a directed arc from the head word whto the modifier wm, and w0is an artificialnode linking to the root of the sentence.Syntax projection aims to project the dependency tree ds of a source-language sentence xs into thedependency structure of its target-language translation x via word alignments a, where a word alignmentai= z means the target-side word wiis aligned into the source-side word wsz, as depicted in Figure1(a) and Figure 1(b).
For simplicity, we avoid one-to-many alignments by keeping the one with highest784w0things1I2did3w01Z2?3?4?5(a) Source tree and word alignmentsw01Z2?3?4?5(b) Projected incomplete treew01Z2?3?4?5(c) Forest (ambiguous labelings)Figure 1: Illustration of syntax projection from English to Chinese with a sentence fragment.
The twoChinese auxiliary words, ??3?
(past tense marker) and ??4?
(relative clause marker), are not aligned toany English words.marginal probability when the target word is aligned to multiple source words.
We first introduce asimple syntax projection approach based on DCA (Hwa et al., 2005), and then propose two extensionsto handle parsing and aligning errors and cross-lingual syntactic divergences.Projection with DCA.
If two target words wiand wjare aligned to two different source words wsaiandwsaj, and the two words compose a dependency in the source tree (ai, aj) ?
ds, then add a dependency(i, j) into the projected syntactic structure.
For example, as shown in Figure 1(a), the two Chinesewords ?Z2?
and ??5?
are aligned to the two English words ?did3?
and ?things1?, and the dependency?things1ydid3?
is included in the source tree.
Therefore, we project the dependency into the target sideand add a dependency ?Z2x?5?
into the projected structure, as shown in Figure 1(b).
An obviousdrawback of DCA is that it may produce many wrong dependencies due to the errors in the automaticsource-language parse trees and word alignments.
Even with manual parse trees and word alignments,syntactic divergences between languages can also lead to projection errors.Pruned with target-side marginals.
To overcome the weakness of DCA, we propose to use target-side marginal probabilities to constrain the projection process and prune obviously bad projections.
Wetrain a probabilistic parser on an existing target-side treebank.
For each projected dependency, wecompute its marginal probability with the target parser, and prune it off the projected structure if theprobability is below a pruning threshold ?p.
Our study shows that dependencies with very low marginalprobabilities are mostly wrong (Figure 2).Supplemented with target-side marginals.
To further improve the quality of projected structures, weadd dependencies with high marginal probabilities according to the target parser.
Specifically, if a targetword wjobtain a head word wiafter projection, and if another word wkhas higher marginal probabilitythan a supplement threshold ?sto be the head word of wj, then we also add the dependency (k, j) intothe projected structure.
In other words, we allow one word to have multiple heads so that the projectedstructure can cover more correct dependencies.From incomplete tree to forest.
Some words in the target sentence may not obtain any head wordsafter projection due to incomplete word alignments or the pruning process, which leads to incompleteparse trees after projection.
Also, some words may have multiple head words resulting from thesupplement process.
To handle these issues, we first convert the projected structures into parse forests,and then propose a generalized training technique based on ambiguous labelings to make use of theprojected instances.
Specifically, if a word does not have head words after projection, we simplyadd into the projected structure all possible words as its candidate heads as long as the newly-addeddependency does not cross any projected dependencies, as illustrated in Figure 1(c).
We introduce threenew dependencies to compose candidate heads for the unattached word ??3?.
Note that it is illegal toadd the dependency ?1y?3?
since it would cross the projected dependency ?Z2x?5?.7853 Dependency Parsing with Ambiguous LabelingsIn parsing community, two mainstream methods tackle the dependency parsing problem from differentperspectives but achieve comparable accuracy on a variety of languages.
Graph-based methods viewthe problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005;McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methodstry to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada andMatsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011).3.1 Graph-based Dependency Parser (GParser)We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilisticparser, which is required to compute the marginal probabilities of dependencies and likelihood of bothmanually labeled data and unannotated bitext with ambiguous labelings.
The graph-based method factorsthe score of a dependency tree into scores of small subtrees p.Score(x,d;w) = w ?
f(x,d) =?p?dScore(x,p;w) (1)We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algorithm,1which defines the score of a dependency tree as:Score(x,d;w) =?{(h,m)}?dwdep?
fdep(x, h,m) +?{(h,s),(h,m)}?dwsib?
fsib(x, h, s,m) (2)where fdep(x, h,m) and fsib(x, h, s,m) are feature vectors corresponding to two kinds of subtree;wdep/sibare the feature weight vectors; the dot product gives the scores contributed by the correspondingsubtrees.
We adopt the state-of-the-art syntactic features proposed in Bohnet (2010).3.2 Probabilistic CRF-based GParserPrevious work on dependency parsing mostly adopts linear models and online perceptron training, whichlack probabilistic explanations of dependency trees and likelihood of the training data.
Instead, we builda log-linear CRF-based probabilistic dependency parser, which defines the probability of a dependencytree as:p(d|x;w) =exp{Score(x,d;w)}Z(x;w); Z(x;w) =?d??Y(x)exp{Score(x,d?
;w)} (3)where Z(x) is the normalization factor and Y(x) is the set of all legal dependency trees for x.3.3 Likelihood and Gradient of Training Data with Ambiguous LabelingsTraditional CRF models assume one gold-standard label for each training instance, which means eachsentence is labeled with a single parse tree in the case of parsing.
To make use of projected instanceswith ambiguous labelings, we propose to use a generalized training framework which allows a sentenceto have multiple parse trees (forest) as its gold-standard reference (Ta?ckstro?m et al., 2013).
The goalof the training procedure is to maximize the likelihood of the training data, and the model is updated toimprove the probabilities of parse forests, instead of single parse trees.
In other words, the model hasthe flexibility to distribute the probability mass among the parse trees inside the forest, as long as theprobability of the forest improves.
In this generalized framework, a traditional instance labeled with asingle parse tree can be regarded as a special case that the forest contains only one parse tree.The probability of a sentence x with ambiguous labelings F is defined as the sum of probabilities ofall parse tree d contained in the forest F :p(F|x;w) =?d?Fp(d|x;w) (4)1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve a little bit higher accuracy, but suffer fromhigher time cost of O(n4) and system complexity.
Our method is applicable to the third-order model.786Train Dev TestPTB 39,832 1,346 2416CTB5 16,091 803 1,910CTB5X 18,104 352 348Bitext 0.9M ?
?Table 1: Data sets (in sentence number).Suppose the training data set is D = {(xi,Fi)}Ni=1.
Then the log likelihood of D is:L(D;w) =N?i=1log p(Fi|xi;w) (5)Then we can derive the partial derivative of the log likelihood with respect to w:?L(D;w)?w=N?i=1(?d?Fip?
(d|xi,Fi;w)f(xi,d) ?
?d?Y(xi)p(d|xi;w)f(xi,d))(6)where p?
(d|xi,Fi;w) is the probability of d under the space constrained by the parse forest Fi:p?
(d|xi,Fi;w) =exp{Score(xi,d;w)}Z(xi,Fi;w); Z(xi,Fi;w) =?d?Fiexp{Score(xi,d;w)} (7)The first term in Eq.
(6) is the model expectations in the search space constrained by Fi, and the secondterm is the model expectations in the complete search space Y(xi).
Since Y(xi) contains exponentiallymany legal dependency trees, direct calculation of the second term is prohibitive.
Instead, we can use theclassic Inside-Outside algorithm to efficiently compute the second term within O(n3) time complexity,where n is the length of the input sentence.
Similarly, the first term can be solved by running the Inside-Outside algorithm in the constrained search space Fi.3.4 Stochastic Gradient Descent (SGD) TrainingWith the likelihood gradients, we apply L2-norm regularized SGD training to iteratively learn the featureweights w for our CRF-based baseline and bitext-enhanced parsers.
We follow the implementationin CRFsuite.2 At each step, the algorithm approximates a gradient with a small subset of trainingexamples, and then updates the feature weights.
Finkel et al.
(2008) show that SGD achieves optimaltest performance with far fewer iterations than other optimization routines such as L-BFGS.
Moreover,it is very convenient to parallel SGD since computation among examples in the same batch is mutuallyindependent.Once the feature weights w are learnt, we can parse the test data and try to find the optimal parse treewith the Viterbi decoding algorithm in O(n3) parsing time (Eisner, 2000; McDonald and Pereira, 2006).d?= arg maxd?Y(x)p(d|x;w) (8)4 Experiments and AnalysisTo verify the effectiveness of our proposed method, we carry out experiments on English-to-Chinesesyntax projection, and aim to enhance our baseline Chinese parser with additional training instancesprojected from automatic English parse trees on bitext.
For monolingual treebanks, we use PennEnglish Treebank (PTB) and Penn Chinese Treebank 5.1 (CTB5).
For English, we follow the standardpractice to split the data into training (sec 02-21), development (sec 22), and test (sec 23).
For CTB5, weadopt the data split of (Duan et al., 2007).
We convert the original bracketed structures into dependency2http://www.chokkan.org/software/crfsuite/78701020304050607080901000  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1UAS&Percent (%)Marginal Probability IntervalPercentUASFigure 2: Distribution (Percent) and accuracy (UAS) of dependencies under different marginalprobability interval for Chinese baseline parser on CTB5 development set.
For example, 0.8 at x-axismeans the interval [0.8, 0.9).structures using Penn2Malt with its default head-finding rules.
We build a CRF-based bigram part-of-speech (POS) tagger with the features described in (Li et al., 2012b), and produce POS tags forall train/development/test datasets and bitext (10-way jackknifing for training datasets).
The taggingaccuracy on test sets is 97.3% on English and 94.0% on Chinese.To compare with the recent work on syntax projection of Jiang et al.
(2010) who use a smaller testdataset, we follow their data split of CTB5 and use gold-standard POS tags during training and test.
Werefer to this setting as CTB5X.For bitext, we collect a parallel corpus from FBIS news (LDC03E14, 0.25M sentence pairs), UnitedNations (LDC04E12, 0.62M), IWSLT2008 (0.04M), and PKU-863 (0.2M).
After corpus cleaning, weobtain a large-scale bilingual parallel corpus containing 0.9M sentence pairs.
We run the unsupervisedBerkeleyAligner3 (Liang et al., 2006) for 4 iterations to obtain word alignments.
Besides hardalignments, we also make use of posterior probabilities to simplify one-to-many alignments to one-to-oneas discussed in Section 2.
Table 1 shows the data statistics.For training both the baseline and bitext-enhanced parsers, we set the batch size to 100 and run SGDuntil a maximum iteration number of 50 is met or the change on likelihood of training data becomes toosmall.
Since the number of projected sentences is much more than that of manually labeled instances(0.9M vs. 16K), it is likely that the projected data may overwhelm manually labeled data during training.Therefore, we adopt a simple corpus-weighting strategy.
Before each iteration, we randomly sample 50Kprojected sentences and 15K manually labeled sentences from all training data, and run SGD to trainfeature weights using the sampled data.
To speed up training, we adopt multi-thread implementation ofgradient computations in the same batch.
It takes about 1 day to train our bitext-enhanced parser for oneiteration using a single CPU core, while using 24 CPU cores only needs about 2 hours.We measure parsing performance using unlabeled attachment score (UAS, percent of words withcorrect heads), excluding punctuation marks.
For significance test, we adopt Dan Bikel?s randomizedparsing evaluation comparator (Noreen, 1989).44.1 Analysis on Marginal ProbabilitiesIn order to gain insights for parameter settings of syntax projection, we analyse the distribution andaccuracy of dependencies under different marginal probability interval.
We train the baseline Chineseparser on CTB5 train set, and use the parser to produce the marginal probabilities of all dependenciesfor sentences in CTB5 development set.
We discard all dependencies that have a marginal probabilityless than 0.0001 for better illustration.
Figure 2 shows the results, where we can see that UAS is roughlyproportional to marginal probabilities.
In other word, dependencies with higher marginal probabilitiesare more accurate.
For example, dependencies with probabilities under interval [0.8, 0.9) has a 80%chance to be correct.
From another aspect, we can see that 50% of dependencies fall in probability3http://code.google.com/p/berkeleyaligner/4http://www.cis.upenn.edu/?dbikel/software.html78877787980818283841  6  11  16  21  26  31  36  41  46UAS(%)Iteration NumberSupervisedDCA (0.0 1.0)DCA Pruned (0.1 1.0)DCA Pruned (0.5 1.0)(a) Parameters for DCA and DCA Pruned77787980818283841  6  11  16  21  26  31  36  41  46UAS(%)Iteration NumberSupervised(0.1 0.5)(0.1 0.6)(0.1 0.8)(b) Parameters for DCA Pruned & SupplementedFigure 3: Performance with different parameter settings of (?p?s) on CTB5 development set.interval [0, 0.1), and such dependencies have very low accuracy (4%).
These observations are helpful forour parameter selection and methodology study during syntax projection.4.2 Results of Syntax Projection on Development DatasetWe apply the syntax projection methods described in Section 2 to the bilingual text, and use the projectedsentences with ambiguous labelings as additional training instances to train new Chinese parsers based onthe framework described in Section 3.
Figure 3 shows the UAS curves on development set with differentparameters settings.
The pruning threshold ?p(see Section 2) balances the quality and coverage ofprojection.
Larger ?pleads to more accurate but fewer projections.
The supplement threshold ?s(seeSection 2) balances the size and oracle score of the projected forest.
Smaller ?scan increase the oraclescore of the forest by adding more dependencies with lower marginal probabilities, but takes the risk ofmaking the resulted forest too ambiguous and weak to properly supervise the model during training.
5The DCA method corresponds to the results with ?p= 0.0 and ?s= 1.0.
We can see that DCAlargely decreases UAS compared with the baseline CRF-based parser.
The reason is that although DCAprojects many source-language dependencies to the target side (44% of target-language words obtainhead words), it also introduces a lot of noise during projection.DCA pruned with target-side marginals corresponds to the results with ?p> 0.0 and ?s= 1.0.Pruning with target-side marginals can clearly improve the projection quality by pruning out badprojections.
When ?p= 0.1, 31% of target-language words obtain head words, and the modeloutperforms the baseline parser by 0.6% at peak UAS.
When ?p= 0.5, the projection ratio decreases to26% and the improvement is 0.3%.
Based on the results, we choose ?p= 0.1 in later experiments.Figure 3(b) presents the results of DCA pruned & supplemented with different ?s.
The supplementprocess adds a small amount of dependencies of high probabilities into the projected forest and thereforeincreases the oracle score, which provides the model with flexibility to distribute the probability mass tomore preferable parse trees.
We can see that although the peak UAS does not increase much, the trainingcurve is more smooth and stable than that without supplement.
Based on the results, we choose ?s= 0.6in later experiments.4.3 Final Results and Comparisons on Test DatasetTable 2 presents the final results on CTB5 test set.
For each parser, we choose the parameterscorresponding to the iteration number with highest UAS on development set.
To further verify theusefulness of syntax projection, we also conduct experiments with self-training, which is known as atypical semi-supervised method.
For the standard self-training, we use Chinese-side bitext with self-predicted parse trees produced by the baseline parser as additional training instances, which turns outto be hurtful to parsing performance.
This is consistent with earlier results (Spreyer and Kuhn, 2009).5Please note when ?p+?s>= 1, ?sbecomes useless.
The reason is that if the probability of a projected dependency (i, j)is larger ?p, then no other word beside wican have a probability larger than ?sof being the head word of wj.789UASBaseline Supervised Parser 81.04Standard Self-training 80.51 (-0.53)Self-training with Ambiguous Labelings 81.09 (+0.05)DCA 78.70 (-2.34)DCA Pruned 81.46 (+0.42 ?
)DCA Pruned & Supplemented 81.71 (+0.67 ?
)Table 2: UAS on CTB5 test set.
?
indicate statistical significance at confidence level of p < 0.01.Supervised Bitext-enhancedJiang et al.
(2010) 87.15 87.65 (+0.50)This work 89.62 90.50 (+0.88 ?
)Table 3: UAS on CTB5X test set.
?
indicate statistical significance at confidence level of p < 0.01.Then, we try a variant of self-training with ambiguous labelings following the practice in Ta?ckstro?met al.
(2013), and use a parse forest composed of dependencies of high probabilities as the syntacticstructure of an instance.
We can see that ambiguous labelings help traditional self-training, but still haveno significant improvement over the baseline parser.
Results in Table 2 indicate that our syntax projectionmethod is able to project useful knowledge from source-language parse trees to the target-side forest, andthen helps the target parser to learn effective features.4.4 Comparisons with Previous Results on Syntax Projection on CTB5XTo make comparison with the recent work of Jiang et al.
(2010), We rerun the process of syntax projectionwith CTB5X as the target treebank with the DCA pruned & supplemented method (?p= 0.1 and ?s=0.6).6 Table 3 shows the results.
Jiang et al.
(2010) employ the second-order MSTParser of McDonaldand Pereira (2006) with a basic feature set as their base parser.
We can see that our baseline parser ismuch stronger than theirs.
Even though, our approach leads to larger UAS improvement.This work is different from theirs in a few aspects.
First, the purpose of syntax projection in theirwork is to produce dependency/non-dependency instances which are used to train local classifiers toproduce auxiliary features for MSTParser.
In contrast, the outputs of syntax projection in our workare partial trees/forests where only reliable dependencies are kept and some words may receive morethan one candidate heads.
We directly use these partial structures as extra training data to learn modelparameters.
Second, their work measures the reliability of a projected dependencies only from theperspective of alignment probability, while we adopt a probabilistic parsing model and use target-sidemarginal probabilities to throw away bad projections, which turns out effective in handling syntacticnon-isomorphism and errors in word alignments and source-side parses.5 Related workCross-lingual annotation projection has been applied to many different NLP tasks to help processingresource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das andPetrov, 2011) and named entity recognition (NER) (Fu et al., 2011).
In another direction, much previouswork explores bitext to improve monolingual NER performance based on bilingual constraints (Chen etal., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013).Based on a universal POS tag set (Petrov et al., 2011), McDonald et al.
(2011) propose to traindelexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext(Zeman and Resnik, 2008; Cohen et al., 2011; S?gaard, 2011).
Ta?ckstro?m et al.
(2012) derive cross-lingual clusters from bitext to help delexicalized parser transfer.
Naseem et al.
(2012) propose selectivelysharing to better explore multi-source transfer information.6In the previous draft of this paper, we directly use the projected data with in previous subsection for simplicity, and findthat UAS can reach 91.39% (+1.77).
The reason is that the CTB5X test is overlapped with CTB5 train.
We correct this mistakein this version.790Our idea of training with ambiguous labelings is originally inspired by the work of Ta?ckstro?m et al.
(2013) on multilingual parser transfer for unsupervised dependency parsing.
They use a delexicalizedparser trained on source-language treebank to obtain parse forests for target-language sentences, and re-train a lexicalized target parser using the sentences with ambiguous labelings.
Similar ideas of learningwith ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002) andsequence labeling problems (Dredze et al., 2009).6 ConclusionsThis paper proposes a simple yet effective framework of soft cross-lingual syntax projection.
Wemake use of large-scale projected structures as additional training instances to boost performance ofsupervised parsing models trained on full-set manually labeled treebank.
Compared with previous work,we make two innovative contributions: 1) using the marginal probabilities of a target-side supervisedparser to control the projection quality with the existence of parsing and aligning errors and cross-lingualsyntax divergences; 2) adopting a new learning technique based ambiguous labelings to make use ofprojected incomplete dependency trees for model training.
Experimental results on two Chinese datasetsdemonstrate the effectiveness of the proposed framework, and show that the bitext-enhanced parsersignificantly outperforms all baselines, including supervised parsers, semi-supervised parsers based onself-training, and previous syntax projection methods.Our anonymous reviewers present many great comments, especially on the experimental section.
Wewill improve this work accordingly and release an extended version of this paper at the homepage ofthe first author.
Such extensions include: 1) further exploring source-language parsing probabilities andalignment probabilities to help syntax projection; 2) studying the effect of the scale of source/targettreebank and bilingual text.AcknowledgmentsThe authors would like to thank Wanxiang Che and Jiang Guo for sharing their bilingual data, and ouranonymous reviewers for their critical and insightful comments, which will certainly help our futurework.
This work was supported by National Natural Science Foundation of China (Grant No.
61373095,61203314, 61373097).ReferencesBernd Bohnet.
2010.
Top accuracy and fast dependency parsing is not a contradiction.
In Proceedings of COLING,pages 89?97.David Burkett and Dan Klein.
2008.
Two languages are better than one (for syntactic parsing).
In ProceedingsEMNLP, pages 877?886.David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010.
Learning better monolingual models withunannotated bilingual text.
In Proceedings of CoNLL 2010, pages 46?54.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In Proceedings ofEMNLP/CoNLL, pages 141?150.Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu.
2013.
Named entity recognition withbilingual constraints.
In Proceedings of NAACL 2013.Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010a.
Bitext dependency parsing with bilingual subtreeconstraints.
In Proceedings of ACL, pages 21?29.Yufeng Chen, Chengqing Zong, and Keh-Yih Su.
2010b.
On jointly recognizing and aligning bilingual namedentities.
In Proceedings of ACL 2010.Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshimasa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro Torisawa,and Haizhou Li.
2011.
SMT helps bitext dependency parsing.
In EMNLP.791Shay B. Cohen, Dipanjan Das, , and Noah A. Smith.
2011.
Unsupervised structure prediction with non-parallelmultilingual guidance.
In Proceedings of EMNLP.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projections.In Proceedings of ACL-HLT 2011, pages 600?609.Mark Dredze, Partha Pratim Talukdar, and Koby Crammer.
2009.
Sequence learning from data with multiplelabels.
In ECML/PKDD Workshop on Learning from Multi-Label Data.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Probabilistic parsing action models for multi-lingual dependencyparsing.
In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 940?946.Jason Eisner.
2000.
Bilexical grammars and their cubic-time parsing algorithms.
In Advances in Probabilistic andOther Parsing Technologies, pages 29?62.Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In Proceedings of ACL, pages 959?967.Ruiji Fu, Bing Qin, and Ting Liu.
2011.
Generating chinese named entity data from a parallel corpus.
InProceedings of IJCNLP 2011, pages 264?272.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009.
Dependency grammar induction via bitextprojection constraints.
In Proceedings of ACL-IJCNLP 2009, pages 369?377.Kuzman Ganchev, Jo ao Graca, Jennifer Gillenwater, and Ben Taskar.
2010.
Posterior regularization for structuredlatent variable models.
Journal of Artifical Intellignece Research.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.
Bilingually-constrained (monolingual) shift-reduce parsing.
InProceedings of EMNLP, pages 1222?1231.Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak.
2005.
Boostrapping parsers viasyntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.Wenbin Jiang, , and Qun Liu.
2010.
Dependency parsing and projection based on word-pair classification.
InACL, pages 897?904.Rong Jin and Zoubin Ghahramani.
2002.
Learning with multiple labels.
In Proceedings of NIPS.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In ACL, pages 1?11.Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zeng, and Fei Huang.
2012a.
Joint bilingual name tagging for parallelcorpora.
In Proceedings of CIKM 2012.Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu.
2012b.
A separately passive-aggressive training algorithmfor joint POS tagging and dependency parsing.
In COLING 2012, pages 1681?1698.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Alignment by agreement.
In HLT-NAACL.Kai Liu, Yajuan Lu?, Wenbin Jiang, and Qun Liu.
2013.
Bilingually-guided monolingual dependency grammarinduction.
In Proceedings of ACL.Ryan McDonald and Fernando Pereira.
2006.
Online learning of approximate dependency parsing algorithms.
InProceedings of EACL, pages 81?88.Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL, pages 91?98.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.
Multi-source transfer of delexicalized dependency parsers.In Proceedings of EMNLP.Tahira Naseem, Benjamin Snyder, Jacob Eisentein, and Regina Barzilay.
2009.
Multilingual part-of-speechtagging: two unsupervised approaches.
Journal of Artifical Intellignece Research, 36(1):341?385.Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012.
Selective sharing for multilingual dependencyparsing.
In Proceedings of ACL.Joakim Nivre.
2003.
An efficient algorithm for projective dependency parsing.
In Proceedings of IWPT, pages149?160.792Eric W. Noreen.
1989.
Computer-intensive methods for testing hypotheses: An introduction.
John Wiley & Sons,Inc., New York.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.
A universal part-of-speech tagset.
In ArXiv:1104.2086.David A. Smith and Jason Eisner.
2009.
Parser adaptation and projection with quasi-synchronous grammarfeatures.
In Proceedings of EMNLP, pages 822?831.Anders S?gaard.
2011.
Data point selection for cross-language adaptation of dependency parsers.
In Proceedingsof ACL 2011, pages 682?686.Kathrin Spreyer and Jonas Kuhn.
2009.
Data-driven dependency parsing of new languages using incomplete andnoisy training data.
In CoNLL, pages 12?20.Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012.
Cross-lingual word clusters for direct transfer oflinguistic structure.
In Proceedings of NAACL-HLT.Oscar Ta?ckstro?m, Ryan McDonald, and Joakim Nivre.
2013.
Target language adaptation of discriminative transferparsers.
In Proceedings of NAACL, pages 1061?1071.Mengqiu Wang, Wanxiang Che, and Christopher D. Manning.
2013.
Joint word alignment and bilingual namedentity recognition using dual decomposition.
In Proceedings of ACL 2013.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statistical dependency analysis with support vector machines.
InProceedings of IWPT, pages 195?206.David Yarowsky and Grace Ngai.
2001.
Inducing multilingual pos taggers and np bracketers via robust projectionacross aligned corpora.
In Proceedings of NAACL 2001.Daniel Zeman and Philip Resnik.
2008.
Cross-language parser adaptation between related languages.
InProceedings of IJCNLP 2008.Yue Zhang and Joakim Nivre.
2011.
Transition-based dependency parsing with rich non-local features.
InProceedings of ACL, pages 188?193.793
