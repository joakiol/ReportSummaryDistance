Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 65?73,Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational LinguisticsPOSTECH Grammatical Error Correction System in the CoNLL-2014 Shared TaskKyusong Lee, Gary Geunbae LeeDepartment of Computer Science and EngineeringPohang University of Science and TechnologyPohang, Korea{Kyusonglee,gblee}@postech.ac.krAbstractThis paper describes the POSTECH gram-matical error correction system.
Variousmethods are proposed to correct errorssuch as rule-based, probability n-gramvector approaches and router-based ap-proach.
Google N-gram count corpus isused mainly as the correction resource.Correction candidates are extracted fromNUCLE training data and each candidateis evaluated with development data to ex-tract high precision rules and n-gramframes.
Out of 13 participating teams, oursystem is ranked 4th on both the originaland revised annotation.1 IntroductionAutomatic grammar error correction (GEC) iswidely used by learners of English as a secondlanguage (ESL) in written tasks.
Many methodshave been proposed to correct grammatical errors;these include methods based on rules (Naber,2003), on statistical machine translation (Brockettet al., 2006), on machine learning, and on n-grams(Alam et al., 2006).
Early research (Han et al.,2006; De Felice, 2008; Knight & Chander, 1994;Nagata et al., 2006) on error correction for non-native text was based on well-formed corpora.Most recent work (Cahill et al., 2013;Rozovskaya & Roth, 2011; Wu & Ng, 2013) hasused machine learning methods that rely on a GE-tagged corpus such as NUCLE, Japanese EnglishLearner corpus, and Cambridge Learner Corpus(Dahlmeier et al., 2013; Izumi et al., 2005;Nicholls, 2003), because well-formed and GE-tagged approaches are closely related to eachother, can be synergistically combined.
Therefore,research using both types of data has also beenconducted (Dahlmeier & Ng, 2011).
Moreover, ameta-classification method using several GE-tagged corpora and a native corpus has been pro-posed to correct the grammatical errors (Seo et al.,2012).
A meta-classifier approach has been pro-posed to combine a language model and error-spe-cific classification for correction of article andpreposition errors (Gamon, 2010).
Web-scalewell-formed corpora have been successfully ap-plied to grammar error correction tasks instead ofusing error-tagged data (Bergsma et al., 2009;Gamon et al., 2009; Hermet et al., 2008).
Espe-cially in the CoNLL-2013 grammar error correc-tion shared task, many of the high-ranked teams(Kao et al., 2013; Mark & Roth, 2013; Xing et al.,2013) exploited the Google Web-1T n-gram cor-pus.
The major advantage of using these web-scale corpora is that extremely large quantities ofdata are publicly available at no additional costs;thus fewer data sparseness problems arise com-pared to previous approaches based on error-tagged corpora.We also use the Google Web-1T n-gram corpus.We extract the candidate pairs (original erroneoustext and its correction) from NUCLE training data.We use a router to choose the best frame to com-pare the n-gram score difference between the orig-inal and replacement in a given candidate pair.The intuition of our grammar error correctionmethod is the following: First, if the uni-gramcount is less than some threshold, we assume thatthe word is erroneous.
Second, if the replacementword n-gram has more frequent than the originalword n-gram, it presents strong evidence for cor-rection.
Third, depending on the candidate pair,tailored n-gram frames help to correct errors ac-curately.
Fourth, only high precision method andrules are applied.
If correction precision on a can-didate pair is less than 30% in development data,65we do not make a correction for the candidate pairat runtime.In the CoNLL-Shared Task, objectives werepresented yearly.
In 2012, the objective was tocorrect article and preposition errors; in 2013, itwas to correct article, preposition, noun number,verb form, and subject-verb agreement errors.This year, the objective is to correct all errors.Thus, our method should also correct prepro-cessing and spelling errors.
Detailed descriptionof the shared task set up, data set, and evaluationabout the CoNLL-2014 Shared Task is explainedin (Ng et al., 2014)2 Data and RecourseThe Google Web-1T corpus contains 1012 wordsof running text and the counts for all 109 five-wordsequences that appear > 40 times (Brants & Franz,2006).
We used the NUS Corpus of Learner Eng-lish (NUCLE) training data to extract the candi-date pairs and CoNLL-2013 Official Shard Tasktest data as development data.
We used the Stan-ford parser (De Marneffe & Manning, 2008) toextract part-of-speech, dependency, and constitu-ency trees.3 Method3.1 Overall ProcessWe correct the errors in the following order:Tokenizing ?
spelling error correction ?
punc-tuation error correction ?
N-gram Vector Ap-proach for Noun number (Nn) ?
Router-based1 http://abisource.com/projects/enchant/Correction (Deletion Correction ?
Insertion Cor-rection ?
Replacement) for various error types ?Rule-based method for verb errors.
Between eachpair of step, we parse, tag, and tokenize again us-ing the Stanford parser because the previous cor-rection affects parsing, tagging, and tokenizing re-sults.3.2 PreprocessingBecause the correction task is no longer restrictedto five error types, tokenizing and spelling errorcorrection have become critical for error correc-tion.
To detect tokenizing error such as ?civiliza-tions.It?, a re-tokenzing process is necessary.
If aword contains a comma, punctuation (e.g., ?,?
or?.?)
and the word count in Google n-gram is lessthan some threshold (here, 1000), we tokenize theword, e.g., as ?civilizations .
It?.
We also correctspelling errors by referring to the Google n-gramword count.
If the word uni-gram count is lessthan a threshold (here, 60000) and the part-of-speech (POS) tag is not NNP or NNPS, we assumethat the word has o  ne or more errors.
The thresh-old is set based on the development set.
We usethe Enchant Python Library to correct the spellingerrors1.
However, using only one best result is notvery accurate.
Thus, among the best results in theEnchant Python Library, we select the one bestword, i.e.
that word with the highest frequency inthe Google n-gram corpus.
Using NUCLE train-ing data, rules are constructed for comma, punc-tuation, and other errors (Table 3).Figure 1.
Overall Process of Router-based Correction663.3 Candidate GenerationSelecting appropriate correction candidates is crit-ical for the precision of the method.
In article andnoun number correction, the number of candidatesis small: ?a?,?an?,?the?
in article correction, ?plural?or ?singular?
in noun number correction.
However,the number of correction candidates can be unlim-ited in wrong collocation/idiom errors.
Reducingthe number of candidates is important in the gram-mar error correction task.Nn Correction Candidate: noun number correc-tion has just one replacement candidate.
If theword is plural, its correction candidate is singular,and vice versa.
The language tool2 can performthese changes.Other Correction Candidate: for correctionsother than noun number, candidates are selectedfrom the GE-tagged corpus.
A total of 4206 pairswere extracted.
We use the notation of candidatepair (o?r), which links the original word (o) andits correction candidate (r).
In the deletion correc-tion step, we determine whether or not the wordshould be deleted.
In the insertion correction step,we select the insertion position in a sentence as aspace between two words.
If o is ?, insertion cor-rection is required; if r is ?, the pair deletion cor-rection is required.
We use the Stanford constitu-ency parser (De Marneffe & Manning, 2008) toextract a noun phrase; if it does not contain a de-terminer or article, we insert one in front of thenoun phrase; if the noun in the noun phrase is sin-gular, ?the?, ?a?, and, ?an?
are selected an insertioncandidates; if the noun is plural, only ?the?
is se-lected as an insertion candidate.
We only apply in-sertion correction at ArtOrDet, comma errors, andpreposition; we skip insertion correction for othererror types because selecting an insertion positionis difficult and if every position is selected as in-sertion position, precision decrease.4 N-gram ApproachWe used the following notation.N(o) n-gram vector in original sentenceN(r) n-gram vector in replacement sen-tencen(o)i i th element in N(o)?(?)?
i th element in N(r)N[i:j] n-gram vector from i th element toj th element2http://www.languagetool.orgWeb-scale data have also been used successfullyin many other research areas, such as lexical dis-ambiguation (Bergsma et al., 2009).
Most NLPsystems resolve ambiguities with the help of alarge corpus of text, e.g.:?
The system tried to decide {among, between}the two confusable words.Disambiguation accuracy increases with the sizeof the corpus.
Many systems incorporate the webcount into their selection process.
For the aboveexample, a typical web-based system would querya search engine with the sequences ?decide amongthe?
and ?decide between the?
and select the can-didate that returns the most hits.
Unfortunately,this approach would fail when disambiguation re-quires additional context.
Bergsma (2009) sug-gested using the context of samples of variouslengths and positions.
For example, from theabove the example sentence, the following 5-grampatterns can be extracted:?
system tried to decide {among, between}?
tried to decide {among, between} the?
to decide {among, between} the two?
decide {among, between} the two confusable?
{among, between} the two confusable wordsSimilarly, four 4-gram patterns, three 3-gram pat-terns and two 2-gram patterns are extracted byspanning the target.
A score for each pattern is cal-culated by summing the log-counts.
This methodwas successfully applied in lexical disambigua-tion.
Web-scale data were used with the count in-formation specified as features.
Kao et al.
(2013)used a ?moving window (MW)?
:???,?
(w) = {????
, ?
, ????+(?
?1), ?
= 0, ?
?
1}  (1)where ?
denotes the position of the word, k thewindow size and w the original or replacementword at position ?.
The window size is set to 2 to5 words.
MW is the same concept as the SUMLM:??,?(?)
= ?
?????(?????)?????????(?
)(2)Both approaches apply the sum of all MWs in (1).Our approach is based on the MW method.
Thedifference is that instead of summing all the MWs,we consider only one best MW which is referredto here as a frame.
The following sentences67demonstrate the case when the following wordsare the crucial features to correct errors:?
I will do it (in?at) home.?
We need (an??)
equipment to solve problems.However, following sentences demonstrate thecase when preceding words is the crucial featureto correct errors:?
One (are?is) deemed to death at a later stage .?
But data that (shows?show) the rising of lifeexpectanciesWe investigated which frame is the best based onthe development set, then router is trained to de-cide on the frame depending on the candidate pair.4.1 Router-based N-gram CorrectionA frame is a sequence of words around the targetposition.
A frame is divided into a precedingframe and a following frame.
The target positioncan be either a position of a target word (Figure2a) or a position in which a candidate word isjudged to be necessary (Figure 2b).
Once the size(i.e., number of words) of frames is chosen, sev-eral forms of frames (n; m) with different sizes ofpreceding (n) and following (m) words are possi-ble.Figure 2.
Frame for n-gramThe router is designed to take care of two stages(training, run-time) error correction.
During train-ing, the router selects the best frame for each can-didate pair.
By testing each candidate pair witheach frame in the development data; the framewith the best precision is selected as the bestframe among (1;1), (1;2), (1;3), (2;1),(2:2), etc.At the end of the training stage, the router hasa list of pairs (x) which matches the best frame (y)associated with it (Table 1) as a result of compar-ing each candidate pair with one in the develop-ment corpus.During runtime, the router assigns each candi-date pair to the best frame to produce the outputsentence (Figure 1).
For example, for a sentence?This ability is not seen 40 years back where thetechnology advances were not as good as now .
?the candidate pair for correction (back?
ago) issuggested.
The best frame assigned by the routerfor this pair (1;1), which is ?years back where?.The best candidate frame for this is ?year agowhere?.
At this point, we query the count of?years back where?
and ?years ago where?
fromthe Google N-gram Count Corpus; these countsare 46 and 1815 respectively.
Because the countof ?years ago where?
is greater than that of ?yearsback where?, the former is selected as the correctform.
As a result, the sentence ?This ability is notseen 40 years back where the technology ad-vances were not as good as now.?
is corrected to?This ability is not seen 40 years ago where thetechnology advances were not as good as now.
?Some words are allowed to have multiple bestframes; in all the best frames, if a candidate wordsequence is more frequent than an original wordsequence in the Google count, then correction ismade.
The multiple frames are also trained fromthe development data set.4.2 Probability n-gram VectorWe use the probability n-gram Vector approach tocorrect Nn.
Most errors are corrected using therouter-based method; however, training the routerfor every noun is difficult because the number ofnouns is extremely large.
Moreover, for nounnumber, we found that rather than considering onedirection or one frame of n-gram, every directionof n-gram should be considered for better perfor-mance such as forward, backward, and two-way.Thus, the probability n-gram vector algorithm isapplied only in the noun number error correction.We propose the probability n-gram vector methodto correct grammatical errors to consider both di-rections, forward and backward.
In a forward n-gram, the probability of each word is estimatedTable 1.
Example of Trained Routerx (o?r) y(another?other) (1;3)(less?fewer) (1;3)(rise?raise) (1;2)(back?ago) (1;1)(could?can) (2;1)(well?good) (2;1)(near??)
No correction68depending on the preceding word.
On the otherhand, in a backward n-gram the probability ofeach word is estimated depending on the follow-ing words.
When the probability of a candidateword is higher than original word, we replace theoriginal with the candidate word in the correctionstep.Probability n-gram vectors are generated from theoriginal word and a candidate word (Figure 3).Rather than using a single sequence of n-gramprobability, we apply contexts of various lengthsand positions.
We applied the probability infor-mation using the Google n-gram count infor-mation as in the following equation:P(??|??
?2, ??
?1) =?(???2,???1??)?(???2,??
?1)Moreover, rather than calculating one word?sprobability given n words such asP(??|??
?1, ??
?2, ??
?3), our model calculates theprobability of m words given an n word sequence.The following is an example 4-gram with forwardprobability:?
m = 3, n = 1 P(??
?2, ???1??|???3)?
m = 2, n = 2 P(??
?1, ??|??
?3, ???2)?
m = 1, n = 3 P(??|??
?3, ??
?2, ??
?1).We construct a 40-dimensional probability vectorwith forward and backward probabilities consid-ering of twenty 5-grams, twelve 4-grams, six 3-gram, and two 2-gram.
Additionally, the elementsof the n-gram vector are detailed in Table 2.Back-Off Model: A high-order n-gram is moreeffective than a low-order n-gram.
Thus, we ap-plied back-off methods (Katz, 1987) to assignhigher priority to higher order probabilities.
If allelements in 5-gram vectors are 0 for both the orig-inal and candidate sentence, which means?
{?(?)?
+ ?(?)?}
= 019?=0 , we consider 4-gramvectors (N[20:31]).
If 4-gram vectors are 0, we con-sider 3-gram vectors.
Moreover, when the pro-posed method calculates each of the forward,backward and two-way probabilities, the back-offmethod is used to get each score.Correction: Here, we explain the process of errorcorrection using n-gram vectors.
First, we gener-ate Nn error candidates.
Second, we construct then-gram probability vector for each candidate.
Theback-off method is applied in N(o)+N(r), The vec-tor contains various directions and ranges of prob-abilities of words given a sample sentence.
Wethen calculate forward n-gram score by summingeven elements in the vector.
We calculate thebackward n-gram by summing odd elements inTable 2.
Next, the two-way n-gram is calculatedby summing all elements for both directions n-gram.
If forward, backward, and two-way n-grams have higher probabilities for the candidateword, we select the candidate as corrected word(Figure 3).Table 2: The elements of n-gram vector5-GRAM?0 = ?(??|??+1??+2??+3?
?+4) backward?1 = ?(??|???4???3???2??
?1) forward?2 = ?(????+1|??+2??+3?
?+4) backward?
?..4-GRAM?20 = ?(??|??+1??+2?
?+3) backward?21 = ?(??|???3???2??
?1) forward?
?..3-GRAM?32 = ?(??|??+1?
?+2) backward?33 = ?(??|???2??
?1) forward?34 = ?(????+1|?
?+2) backward?35 = ?(???1??|??
?2) forward?36 = ?(???1??|?
?+1) backward?37 = ?(????+1|??
?1) forward2-GRAM?38 = ?(??|?
?+1) backward?39 = ?(??|??
?1) forwardFigure 3.
Overall process of Nn Correction695 Verb Correction (Rule-based)There are several types of verb errors in non-na-tive text such as verb tense, verb modal, missingverb, verb form, and subject-verb-agreement(SVA).
Among these errors, we attempt to correctSVA errors using rule-based methods (Table 3).In non-native text, parsing and tagging errors areinevitable, and it may cause false alarm.
Thus, in-stead of dependency parsing to find subject andverb, we consider the preceding five words be-cause erroneous sentences often contain depend-ency errors.
Moreover, in erroneous sentences,POS tagging accuracy is lower than native text.Thus, NN and VB are misclassified, as are VBZand NNS.
A rule is used that encodes the relevantlinguistic knowledge that these words or POSsshould not occur in the five positions precedingthe VBZ: ?NN?, ?this?, ?it?
,?one?, ?VBG?.
Moreover,words that preceded and follow ?which?
shouldagree in verb form, as indicated in Rule3 andRule4.6 ExperimentThe CoNLL-2014 training data consist of 1,397articles together with gold-standard annotation.Algorithm Rule1-Comma1: function rule1( toksent,  tokpos)2: for i ?
0 ?
len(toksent) do3: if  toksent[i] in [ However?, ?Therefore?, ?Thus?]
and not  toksent[i + 1] == ?,?
then4: toksent[i]= toksent[i] + ?
,?Algorithm Rule2-preposition1: function rule2( toksent,  tokpos)2: for i ?
0 ?
len(toksent) do3: if  toksent[i] = ?according?
and not  toksent [i+1] = ?to?4:  toksent [i+1] = ?to ?+  toksent [i+1]Algorithm Rule3-Subject Verb Agreement1: function rule3( toksent,  tokpos)2: for i ?
0 ?
len(toksent) do3: if  toksent[i] is ?which?4: if  tokpos[i ?
1] == ?NNS?
and  tokpos[i + 1] == ?VBZ?
then5: toksent[i + 1]= changeWordForm (toksent[i + 1], ?VBP?
)6: else if  tokpos[i ?
1]  == ?NNS?
and  tokpos[i + 1] == ?NNS?
then7: toksent[i + 1]=  =changeWordForm(toksent[i + 1], ?VBP?
)8: else if  tokpos[i ?
1] == ?NN?
and  tokpos[i + 1]== ?are?
then9: toksent[i + 1]=  = is10: else if  tokpos[i ?
1] == ?NN?
and  tokpos[i + 1] in [?VBP?,?VB?,?NN?]
then11: toksent[i + 1]=  = makePlural(toksent[i + 1])Algorithm Rule4-Subject Verb Agreement1: function rule4( toksent,  tokpos)2: for i ?
0 ?
len(toksent) do3: if not ( tokpos[i]is ?VBZ?
and [?NN?,?this?,?it?,?one?,?VBG?]
in  tokpos[i ?
5: i]) then4:  tokcand?changeWordForm( tokword[i], ?VBP?
)5: else if not ( tokpos[i]is ?VBP?
and [?I?,?we?,?they?,?and?]
in  toksent[i ?
5: i]) then6:  tokcand ?changeWordForm( tokword[i], ?VBZ?
)7: else if not ( tokpos[i]is ?NN?
and [?be?,?ing?]
in  toksent[i ?
5: i]) then8:  tokcand?changeWordForm( tokword[i], ?VBN?
)9:         original = ngramCount( toksent), candidate =ngramCount(tokcand)10: If original < candidate then11: Return tokcandTable 3.
Examples of Rules70The documents are a subset of the NUS Corpus ofLearner English (NUCLE).
We use the Max-Match (M2) scorer provided by the CoNLL-2014Shared Task.
The M2 scorer works by using theset that maximally matches the set of gold-stand-ard edits specified by the annotator as being equalto the set of system edits that are automaticallycomputed and used in scoring (Dahlmeier & Ng,2012).
The official evaluation metric is F0.5,weighting precision twice as much as recall.
Weachieve F0.5 of 30.88; precision of 34.51; recallof 21.73 in the original annotation (Table 4).
Afteroriginal official annotations announced by organ-izers (i.e., only based on the annotations of the twoannotators), another set of annotations is offeredbased on including the additional answers pro-posed by the 3 teams (CAMB, CUUI, UMC).
Theimprovement gap between the original annotationand the revised annotation of our team (POST) is5.89%.
We obtain the highest improvement rateexcept for the 3 proposed teams (Figure 4), F0.5of 36.77; precision of 41.28; recall of 25.59 in therevised annotation.
Our system achieves the 4thhighest scores of 13 participating teams based onboth the original and revised annotations.
To ana-lyze the scores of each of the error types and mod-ules, we apply the method of n-gram vector (Nn),rule-based (Verb, Mec), and router-based (others)separately in both the original and the revised an-notation of all error types.
We achieve high preci-sion by rules at the Mec which indicates punctua-tion, capitalization, spelling, and typos errors.
Ad-ditionally, the Nn type has the highest improve-ment gap between the original and revised anno-tation (17% ?
24.31 of F0.5).
In order for ourteam to improve the high precision in the rule-based approach, we tested potential rules on thedevelopment data and kept a rule only if its preci-sion on that data set was 30% or greater.
When wetrained router, the same strategy was conducted.If a frame could not achieve 30% precision, weassigned the candidate pair as ?no correction?
inthe router.
These constraints achieve precision of30 % in most error types.7 DiscussionAlthough preposition errors are frequently com-mitted in non-native text, we mostly skip the cor-rection of preposition error.
This is because as-signing prepositions correctly is extremely diffi-cult, because (1) the preposition used can vary(e.g., Canada: ?on the weekend?
vs. Britain ?at theweekend?
); (2) in a given location, more than onepreposition may be possible, and the choice af-fects the meaning (e.g., ?on the wall?, vs. ?at thewall?).
Verb errors can consist of many multi-Figure 4.
Improvement gap between the original annotation and revised annotation of each team0246810Table 4.
Performance on each error typeOriginal annotation  Revised annotationPrecision Recall F0.5  Precision Recall F0.5N-gram (Nn) 31.0 6.55 17.75  42.28 9.0 24.31Rule (Verb) 28.95 1.12 4.86  31.17 1.29 5.52Rule (Mec) 49.34 5.47 18.94  52.16 6.17 20.93Router (Others) 28.11 12.49 22.49  35.29 15.45 28.08All 34.51 21.73 30.88  41.28 25.59 36.7771word errors due to errors of usages of passive andactive voice.
(e.g.
release?be released).
Our cur-rent system cannot correct these multi-words er-rors, for three reasons.
First, if the original exam-ple consists of one word and the optimal replace-ment consists of two words, n-gram scores cannotbe applied easily to compare probabilities be-tween them.
Second, the n-gram approach alsofails if the distance between subject and verb ismore than 5.
Third, multiply dependent errors arecritical for verb error correction.
For example,noun number, determiner, and subject verb agree-ment are often dependent upon each other: e.g.
?And once this happens, privacy does not existany more and people's (life?lives) (is?are) un-der great threaten.?
The correction order will beimportant when all error type must be correctedsimultaneously.Grammar error correction is a challengingproblem.
In CoNNL-2013, more than half of therelated teams obtained F-score < 10.0.
This lowperformance in the grammar error correction canbe explained by several reasons, which indicatethe present limitations of grammar correction sys-tems.Among a total of 4206 pairs, we only use smallamount of candidate pairs, 215 pairs are used forcandidate pairs.
The other 3991 pairs are dis-carded in the router training step because thesepairs cannot be corrected by the n-gram approach.Various classification methods and statistical ma-chine translation based methods will be investi-gated in the router-based approach to find the tai-lored methods for the given word.
A demonstra-tion and progress of our grammar error correctionsystem is available to the public3.8 ConclusionWe have described the POSTECH grammaticalerror correction system.
We use the Google N-gram count corpus to detect spelling errors, punc-tuation, and comma errors.
A rule-based methodis used to correct verb, punctuation, comma errorsand preposition errors.
The Google corpus is alsoused for an n-gram vector approach and a router-based approaches.
Currently we use the router toselect the best frame.
In the future, we will train arouter to select the best method among classifica-tion, n-gram approach, statistical machine transla-3 http://isoft.postech.ac.kr/grammartion-based method and pattern matching ap-proaches.
A machine learning method will be usedto train the router with various features.AcknowledgementsThis research was supported by the MSIP(The Ministryof Science, ICT and Future Planning), Korea and Mi-crosoft Research, under IT/SW Creative research pro-gram supervised by the NIPA(National IT IndustryPromotion Agency) (NIPA-2013- H0503-13-1006)and this research was supported by the Basic ScienceResearch Program through the National ResearchFoundation of Korea(NRF) funded by the Ministry ofEducation, Science and Technology(2010-0019523).ReferencesHan, Na-Rae, Chodorow, Martin, & Leacock, Claudia.(2006).
Detecting errors in English articleusage by non-native speakers.Alam, Md Jahangir, UzZaman, Naushad, & Khan,Mumit.
(2006).
N-gram based statisticalgrammar checker for Bangla and English.Bergsma, Shane, Lin, Dekang, & Goebel, Randy.(2009).
Web-Scale N-gram Models forLexical Disambiguation.
Paper presented atthe IJCAI.Brants, Thorsten, & Franz, Alex.
(2006).
The GoogleWeb 1T 5-gram corpus version 1.1.LDC2006T13.Brockett, Chris, Dolan, William B, & Gamon, Michael.(2006).
Correcting ESL errors using phrasalSMT techniques.
Paper presented at theProceedings of the 21st InternationalConference on Computational Linguistics andthe 44th annual meeting of the Association forComputational Linguistics.Cahill, Aoife, Madnani, Nitin, Tetreault, Joel, &Napolitano, Diane.
(2013).
Robust Systemsfor Preposition Error Correction UsingWikipedia Revisions.
Paper presented at theProceedings of NAACL-HLT.Dahlmeier, Daniel, & Ng, Hwee Tou.
(2011).Grammatical error correction withalternating structure optimization.
Paperpresented at the Proceedings of the 49thAnnual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies-Volume 1.Dahlmeier, Daniel, & Ng, Hwee Tou.
(2012).
Betterevaluation for grammatical error correction.Paper presented at the Proceedings of the2012 Conference of the North AmericanChapter of the Association for ComputationalLinguistics: Human Language Technologies.72Dahlmeier, Daniel, Ng, Hwee Tou, & Wu, Siew Mei.(2013).
Building a large annotated corpus oflearner English: The NUS corpus of learnerEnglish.
Paper presented at the Proceedings ofthe Eighth Workshop on Innovative Use ofNLP for Building Educational Applications.De Felice, Rachele.
(2008).
Automatic error detectionin non-native English.
University of Oxford.De Marneffe, Marie-Catherine, & Manning,Christopher D. (2008).
The Stanford typeddependencies representation.
Paper presentedat the Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-Domain Parser Evaluation.Gamon, Michael.
(2010).
Using mostly native data tocorrect errors in learners' writing: a meta-classifier approach.
Paper presented at theHuman Language Technologies: The 2010Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics.Gamon, Michael, Leacock, Claudia, Brockett, Chris,Dolan, William B, Gao, Jianfeng, Belenko,Dmitriy, & Klementiev, Alexandre.
(2009).Using statistical techniques and web search tocorrect ESL errors.
Calico Journal, 26(3),491-511.Hermet, Matthieu, D?silets, Alain, & Szpakowicz, Stan.(2008).
Using the web as a linguistic resourceto automatically correct lexico-syntacticerrors.Izumi, Emi, Uchimoto, Kiyotaka, & Isahara, Hitoshi.(2005).
Error annotation for corpus ofJapanese learner English.
Paper presented atthe Proceedings of the Sixth InternationalWorkshop on Linguistically InterpretedCorpora.Kao, Ting-Hui, Chang, Yu-Wei, Chiu, Hsun-Wen, &Yen, Tzu-Hsi.
(2013).
CoNLL-2013 SharedTask: Grammatical Error Correction NTHUSystem Description.
CoNLL-2013, 20.Katz, Slava.
(1987).
Estimation of probabilities fromsparse data for the language modelcomponent of a speech recognizer.
Acoustics,Speech and Signal Processing, IEEETransactions on, 35(3), 400-401.Knight, Kevin, & Chander, Ishwar.
(1994).
Automatedpostediting of documents.
Paper presented atthe AAAI.Mark, Alla Rozovskaya Kai-Wei Chang, & Roth,Sammons Dan.
(2013).
The University ofIllinois System in the CoNLL-2013 SharedTask.
CoNLL-2013, 51, 13.Naber, Daniel.
(2003).
A rule-based style and grammarchecker.
Diploma ThesisNagata, Ryo, Morihiro, Koichiro, Kawai, Atsuo, & Isu,Naoki.
(2006).
A feedback-augmented methodfor detecting errors in the writing of learnersof English.
Paper presented at the Proceedingsof the 21st International Conference onComputational Linguistics and the 44thannual meeting of the Association forComputational Linguistics.Ng, Hwee Tou , Wu, Siew Mei , Briscoe, Ted ,Hadiwinoto, Christian , Susanto, RaymondHendy, & Bryant, Christopher (2014).
TheCoNLL-2014 Shared Task on GrammaticalError Correction.
Paper presented at the theEighteenth Conference on ComputationalNatural Language Learning: Shared Task(CoNLL-2014 Shared Task), Baltimore,Maryland, USA.Nicholls, Diane.
(2003).
The Cambridge LearnerCorpus: Error coding and analysis forlexicography and ELT.
Paper presented at theProceedings of the Corpus Linguistics 2003conference.Rozovskaya, Alla, & Roth, Dan.
(2011).
Algorithmselection and model adaptation for ESLcorrection tasks.
Urbana, 51, 61801.Seo, Hongsuck, Lee, Jonghoon, Kim, Seokhwan, Lee,Kyusong, Kang, Sechun, & Lee, GaryGeunbae.
(2012).
A meta learning approachto grammatical error correction.
Paperpresented at the Proceedings of the 50thAnnual Meeting of the Association forComputational Linguistics: Short Papers-Volume 2.Wu, Yuanbin, & Ng, Hwee Tou.
(2013).
Grammaticalerror correction using integer linearprogramming.
Paper presented at theProceedings of the 51st Annual Meeting ofthe Association for ComputationalLinguistics.Xing, Junwen, Wang, Longyue, Wong, Derek F, Chao,Lidia S, & Zeng, Xiaodong.
(2013).
UM-Checker: A Hybrid System for EnglishGrammatical Error Cor-rection.
CoNLL-2013,34.Yannakoudakis, Helen, Briscoe, Ted, & Medlock, Ben.(2011).
A New Dataset and Method forAutomatically Grading ESOL Texts.
Paperpresented at the ACL.73
