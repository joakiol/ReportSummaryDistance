Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 495?503,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPDistributional Representations for Handling Sparsity in SupervisedSequence-LabelingFei HuangTemple University1805 N. Broad St.Wachman Hall 324tub58431@temple.eduAlexander YatesTemple University1805 N. Broad St.Wachman Hall 324yates@temple.eduAbstractSupervised sequence-labeling systems innatural language processing often sufferfrom data sparsity because they use wordtypes as features in their prediction tasks.Consequently, they have difficulty estimat-ing parameters for types which appear inthe test set, but seldom (or never) ap-pear in the training set.
We demonstratethat distributional representations of wordtypes, trained on unannotated text, canbe used to improve performance on rarewords.
We incorporate aspects of theserepresentations into the feature space ofour sequence-labeling systems.
In an ex-periment on a standard chunking dataset,our best technique improves a chunkerfrom 0.76 F1 to 0.86 F1 on chunks begin-ning with rare words.
On the same dataset,it improves our part-of-speech tagger from74% to 80% accuracy on rare words.
Fur-thermore, our system improves signifi-cantly over a baseline system when ap-plied to text from a different domain, andit reduces the sample complexity of se-quence labeling.1 IntroductionData sparsity and high dimensionality are the twincurses of statistical natural language processing(NLP).
In many traditional supervised NLP sys-tems, the feature space includes dimensions foreach word type in the data, or perhaps even combi-nations of word types.
Since vocabularies can beextremely large, this leads to an explosion in thenumber of parameters.
To make matters worse,language is Zipf-distributed, so that a large frac-tion of any training data set will be hapax legom-ena, very many word types will appear only a fewtimes, and many word types will be left out ofthe training set alogether.
As a consequence, formany word types supervised NLP systems havevery few, or even zero, labeled examples fromwhich to estimate parameters.The negative effects of data sparsity have beenwell-documented in the NLP literature.
The per-formance of state-of-the-art, supervised NLP sys-tems like part-of-speech (POS) taggers degradessignificantly on words that do not appear in thetraining data, or out-of-vocabulary (OOV) words(Lafferty et al, 2001).
Performance also degradeswhen the domain of the test set differs from the do-main of the training set, in part because the test setincludes more OOV words and words that appearonly a few times in the training set (henceforth,rare words) (Blitzer et al, 2006; Daume?
III andMarcu, 2006; Chelba and Acero, 2004).We investigate the use of distributional repre-sentations, which model the probability distribu-tion of a word?s context, as techniques for find-ing smoothed representations of word sequences.That is, we use the distributional representationsto share information across unannotated examplesof the same word type.
We then compute featuresof the distributional representations, and providethem as input to our supervised sequence label-ers.
Our technique is particularly well-suited tohandling data sparsity because it is possible to im-prove performance on rare words by supplement-ing the training data with additional unannotatedtext containing more examples of the rare words.We provide empirical evidence that shows howdistributional representations improve sequence-labeling in the face of data sparsity.Specifically, we investigate empirically theeffects of our smoothing techniques on twosequence-labeling tasks, POS tagging and chunk-ing, to answer the following:1.
What is the effect of smoothing on sequence-labeling accuracy for rare word types?
Our bestsmoothing technique improves a POS tagger by11% on OOV words, and a chunker by an impres-sive 21% on OOV words.4952.
Can smoothing improve adaptability to new do-mains?
After training our chunker on newswiretext, we apply it to biomedical texts.
Remark-ably, we find that the smoothed chunker achievesa higher F1 on the new domain than the baselinechunker achieves on a test set from the originalnewswire domain.3.
How does our smoothing technique affect sam-ple complexity?
We show that smoothing drasti-cally reduces sample complexity: our smoothedchunker requires under 100 labeled samples toreach 85% accuracy, whereas the unsmoothedchunker requires 3500 samples to reach the samelevel of performance.The remainder of this paper is organized as fol-lows.
Section 2 discusses the smoothing problemfor word sequences, and introduces three smooth-ing techniques.
Section 3 presents our empiricalstudy of the effects of smoothing on two sequence-labeling tasks.
Section 4 describes related work,and Section 5 concludes and suggests items for fu-ture work.2 Smoothing Natural LanguageSequencesTo smooth a dataset is to find an approximation ofit that retains the important patterns of the origi-nal data while hiding the noise or other compli-cating factors.
Formally, we define the smoothingtask as follows: let D = {(x, z)|x is a word se-quence, z is a label sequence} be a labeled datasetof word sequences, and let M be a machine learn-ing algorithm that will learn a function f to pre-dict the correct labels.
The smoothing task is tofind a function g such that when M is applied toD?
= {(g(x), z)|(x, z) ?
D}, it produces a func-tion f ?
that is more accurate than f .For supervised sequence-labeling problems inNLP, the most important ?complicating factor?that we seek to avoid through smoothing is thedata sparsity associated with word-based represen-tations.
Thus, the task is to find g such that forevery word x, g(x) is much less sparse, but stillretains the essential features of x that are usefulfor predicting its label.As an example, consider the string ?Researcherstest reformulated gasolines on newer engines.?
Ina common dataset for NP chunking, the word ?re-formulated?
never appears in the training data, butappears four times in the test set as part of theNP ?reformulated gasolines.?
Thus, a learning al-gorithm supplied with word-level features wouldhave a difficult time determining that ?reformu-lated?
is the start of a NP.
Character-level featuresare of little help as well, since the ?-ed?
suffix ismore commonly associated with verb phrases.
Fi-nally, context may be of some help, but ?test?
isambiguous between a noun and verb, and ?gaso-lines?
is only seen once in the training data, sothere is no guarantee that context is sufficient tomake a correct judgment.On the other hand, some of the other contextsin which ?reformulated?
appears in the test set,such as ?testing of reformulated gasolines,?
pro-vide strong evidence that it can start a NP, since?of?
is a highly reliable indicator that a NP is tofollow.
This example provides the intuition for ourapproach to smoothing: we seek to share informa-tion about the contexts of a word across multipleinstances of the word, in order to provide more in-formation about words that are rarely or never seenin training.
In particular, we seek to represent eachword by a distribution over its contexts, and thenprovide the learning algorithm with features com-puted from this distribution.
Importantly, we seekdistributional representations that will provide fea-tures that are common in both training and testdata, to avoid data sparsity.
In the next three sec-tions, we develop three techniques for smoothingtext using distributional representations.2.1 Multinomial RepresentationIn its simplest form, the context of a word may berepresented as a multinomial distribution over theterms that appear on either side of the word.
If V isthe vocabulary, or the set of word types, and X is asequence of random variables over V , the left andright context of Xi = v may each be representedas a probability distribution over V: P (Xi?1|Xi =v) and P (Xi+1|X = v) respectively.We learn these distributions from unlabeledtexts in two different ways.
The first method com-putes word count vectors for the left and right con-texts of each word type in the vocabulary of thetraining and test texts.
We also use a large col-lection of additional text to determine the vectors.We then normalize each vector to form a proba-bility distribution.
The second technique first ap-plies TF-IDF weighting to each vector, where thecontext words of each word type constitute a doc-ument, before applying normalization.
This givesgreater weight to words with more idiosyncraticdistributions and may improve the informativenessof a distributional representation.
We refer to thesetechniques as TF and TF-IDF.496To supply a sequence-labeling algorithm withinformation from these distributional representa-tions, we compute real-valued features of the con-text distributions.
In particular, for every wordxi in a sequence, we provide the sequence labelerwith a set of features of the left and right contextsindexed by v ?
V: F leftv (xi) = P (Xi?1 = v|xi)and F rightv (xi) = P (Xi+1 = v|xi).
For exam-ple, the left context for ?reformulated?
in our ex-ample above would contain a nonzero probabilityfor the word ?of.?
Using the features F(xi), a se-quence labeler can learn patterns such as, if xi hasa high probability of following ?of,?
it is a goodcandidate for the start of a noun phrase.
Thesefeatures provide smoothing by aggregating infor-mation across multiple unannotated examples ofthe same word.2.2 LSA ModelOne drawback of the multinomial representationis that it does not handle sparsity well enough,because the multinomial distributions themselvesare so high-dimensional.
For example, the twophrases ?red lamp?
and ?magenta tablecloth?share no words in common.
If ?magenta?
is neverobserved in training, the fact that ?tablecloth?
ap-pears in its right context is of no help in connectingit with the phrase ?red lamp.?
But if we can groupsimilar context words together, putting ?lamp?
and?tablecloth?
into a category for household items,say, then these two adjectives will share that cat-egory in their context distributions.
Any pat-terns learned for the more common ?red lamp?will then also apply to the less common ?magentatablecloth.?
Our second distributional represen-tation aggregates information from multiple con-text words by grouping together the distributionsP (xi?1 = v|xi = w) and P (xi?1 = v?|xi = w)if v and v?
appear together with many of the samewords w. Aggregating counts in this way smoothsour representations even further, by supplying bet-ter estimates when the data is too sparse to esti-mate P (xi?1|xi) accurately.Latent Semantic Analysis (LSA) (Deerwester etal., 1990) is a widely-used technique for comput-ing dimensionality-reduced representations from abag-of-words model.
We apply LSA to the set ofright context vectors and the set of left context vec-tors separately, to find compact versions of eachvector, where each dimension represents a com-bination of several context word types.
We nor-malize each vector, and then calculate features asabove.
After experimenting with different choicesfor the number of dimensions to reduce our vec-tors to, we choose a value of 10 dimensions as theone that maximizes the performance of our super-vised sequence labelers on held-out data.2.3 Latent Variable Language ModelRepresentationTo take smoothing one step further, we presenta technique that aggregates context distributionsboth for similar context words xi?1 = v and v?,and for similar words xi = w and w?.
Latentvariable language models (LVLMs) can be used toproduce just such a distributional representation.We use Hidden Markov Models (HMMs) as themain example in the discussion and as the LVLMsin our experiments, but the smoothing techniquecan be generalized to other forms of LVLMs, suchas factorial HMMs and latent variable maximumentropy models (Ghahramani and Jordan, 1997;Smith and Eisner, 2005).An HMM is a generative probabilistic modelthat generates each word xi in the corpus con-ditioned on a latent variable Yi.
Each Yi in themodel takes on integral values from 1 to S, andeach one is generated by the latent variable for thepreceding word, Yi?1.
The distribution for a cor-pus x = (x1, .
.
.
, xN ) given a set of state vectorsy = (y1, .
.
.
, yN ) is given by:P (x|y) =?iP (xi|yi)P (yi|yi?1)Using Expectation-Maximization (Dempster etal., 1977), it is possible to estimate the distribu-tions for P (xi|yi) and P (yi|yi?1) from unlabeleddata.
We use a trained HMM to determine the op-timal sequence of latent states y?i using the well-known Viterbi algorithm (Rabiner, 1989).
Theoutput of this process is an integer (ranging from 1to S) for every word xi in the corpus; we include anew boolean feature for each possible value of yiin our sequence labelers.To compare our models, note that in the multi-nomial representation we directly model the prob-ability that a word v appears before a word w:P (xi?1 = v|xi = w)).
In our LSA model, we findlatent categories of context words z, and model theprobability that a category appears before the cur-rent word w: P (xi?1 = z|xi = w).
The HMMfinds (probabilistic) categories Y for both the cur-rent word xi and the context word xi?1, and mod-els the probability that one category follows the497other: P (Yi|Yi?1).
Thus the HMM is our mostextreme smoothing model, as it aggregates infor-mation over the greatest number of examples: fora given consecutive pair of words xi?1, xi in thetest set, it aggregates over all pairs of consecutivewords x?i?1, x?i where x?i?1 is similar to xi?1 andx?i is similar to xi.3 ExperimentsWe tested the following hypotheses in our experi-ments:1.
Smoothing can improve the performance ofa supervised sequence labeling system on wordsthat are rare or nonexistent in the training data.2.
A supervised sequence labeler achieves greateraccuracy on new domains with smoothing.3.
A supervised sequence labeler has a better sam-ple complexity with smoothing.3.1 Experimental SetupWe investigate the use of smoothing in two testsystems, conditional random field (CRF) modelsfor POS tagging and chunking.
To incorporatesmoothing into our models, we follow the follow-ing general procedure: first, we collect a set ofunannotated text from the same domain as the testdata set.
Second, we train a smoothing model onthe text of the training data, the test data, and theadditional collection.
We then automatically an-notate both the training and test data with featurescalculated from the distributional representation.Finally, we train the CRF model on the annotatedtraining set and apply it to the test set.We use an open source CRF software packagedesigned by Sunita Sajarwal and William W. Co-hen to implement our CRF models.1 We use a setof boolean features listed in Table 1.Our baseline CRF system for POS tagging fol-lows the model described by Lafferty et al(2001).We include transition features between pairs ofconsecutive tag variables, features between tagvariables and words, and a set of orthographic fea-tures that Lafferty et al found helpful for perfor-mance on OOV words.
Our smoothed models addfeatures computed from the distributional repre-sentations, as discussed above.Our chunker follows the system described bySha and Pereira (2003).
In addition to the tran-sition, word-level, and orthographic features, weinclude features relating automatically-generatedPOS tags and the chunk labels.
Unlike Sha and1Available from http://sourceforge.net/projects/crf/CRF Feature SetTransition zi=zzi=z and zi?1=z?Word xi=w and zi=zPOS ti=t and zi=zOrthography for every s ?
{-ing, -ogy, -ed, -s, -ly, -ion, -tion, -ity},suffix(xi)= s and zi=zxi is capitalized and zi = zxi has a digit and zi = zTF, TF-IDF, andLSA featuresfor every context type v,F leftv (xi) and F rightv (xi)HMM features yi=y and zi = zTable 1: Features used in our CRF systems.
zi vari-ables represent labels to be predicted, ti represent tags (forthe chunker), and xi represent word tokens.
All features areboolean except for the TF, TF-IDF, and LSA features.Pereira, we exclude features relating consecutivepairs of words and a chunk label, or features re-lating consecutive tag labels and a chunk label,in order to expedite our experiments.
We foundthat including such features does improve chunk-ing F1 by approximately 2%, but it also signifi-cantly slows down CRF training.3.2 Rare Word AccuracyFor these experiments, we use the Wall StreetJournal portion of the Penn Treebank (Marcus etal., 1993).
Following the CoNLL shared task from2000, we use sections 15-18 of the Penn Treebankfor our labeled training data for the supervisedsequence labeler in all experiments (Tjong et al,2000).
For the tagging experiments, we train andtest using the gold standard POS tags contained inthe Penn Treebank.
For the chunking experiments,we train and test with POS tags that are automati-cally generated by a standard tagger (Brill, 1994).We tested the accuracy of our models for chunkingand POS tagging on section 20 of the Penn Tree-bank, which corresponds to the test set from theCoNLL 2000 task.Our distributional representations are trained onsections 2-22 of the Penn Treebank.
Because weinclude the text from the train and test sets in ourtraining data for the distributional representations,we do not need to worry about smoothing them?
when they are decoded on the test set, they498Freq: 0 1 2 0-2 all#Samples 438 508 588 1534 46661Baseline .62 .77 .81 .74 .93TF .76 .72 .77 .75 .92TF-IDF .82 .75 .76 .78 .94LSA .78 .80 .77 .78 .94HMM .73 .81 .86 .80 .94Table 2: POS tagging accuracy: our HMM-smoothedtagger outperforms the baseline tagger by 6% on rarewords.
Differences between the baseline and the HMM arestatistically significant at p < 0.01 for the OOV, 0-2, and allcases using the two-tailed Chi-squared test with 1 degree offreedom.will not encounter any previously unseen words.However, to speed up training during our exper-iments and, in some cases, to avoid running outof memory, we replaced words appearing twice orfewer times in the data with the special symbol*UNKNOWN*.
In addition, all numbers were re-placed with another special symbol.
For the LSAmodel, we had to use a more drastic cutoff to fitthe singular value decomposition computation intomemory: we replaced words appearing 10 times orfewer with the *UNKNOWN* symbol.
We initial-ize our HMMs randomly.
We run EM ten timesand take the model with the best cross-entropy ona held-out set.
After experimenting with differ-ent variations of HMM models, we settled on amodel with 80 latent states as a good compromisebetween accuracy and efficiency.For our POS tagging experiments, we measuredthe accuracy of the tagger on ?rare?
words, orwords that appear at most twice in the trainingdata.
For our chunking experiments, we focus onchunks that begin with rare words, as we foundthat those were the most difficult for the chunkerto identify correctly.
So we define ?rare?
chunksas those that begin with words appearing at mosttwice in training data.
To ensure that our smooth-ing models have enough training data for our testset, we further narrow our focus to those wordsthat appear rarely in the labeled training data, butappear at least ten times in sections 2-22.
Tables 2and 3 show the accuracy of our smoothed modelsand the baseline model on tagging and chunking,respectively.
The line for ?all?
in both tables indi-cates results on the complete test set.Both our baseline tagger and chunker achieverespectable results on their respective tasks forall words, and the results were good enough forFreq: 0 1 2 0-2 all#Samples 133 199 231 563 21900Baseline .69 .75 .81 .76 .90TF .70 .82 .79 .77 .89TF-IDF .77 .77 .80 .78 .90LSA .84 .82 .83 .84 .90HMM .90 .85 .85 .86 .93Table 3: Chunking F1: our HMM-smoothed chunkeroutperforms the baseline CRF chunker by 0.21 on chunksthat begin with OOV words, and 0.10 on chunks that be-gin with rare words.us to be satisfied that performance on rare wordsclosely follows how a state-of-the-art supervisedsequence-labeler behaves.
The chunker?s accuracyis roughly in the middle of the range of results forthe original CoNLL 2000 shared task (Tjong etal., 2000) .
While several systems have achievedslightly higher accuracy on supervised POS tag-ging, they are usually trained on larger trainingsets.As expected, the drop-off in the baseline sys-tem?s performance from all words to rare wordsis impressive for both tasks.
Comparing perfor-mance on all terms and OOV terms, the baselinetagger?s accuracy drops by 0.31, and the baselinechunker?s F1 drops by 0.21.
Comparing perfor-mance on all terms and rare terms, the drop is lesssevere but still dramatic: 0.19 for tagging and 0.15for chunking.Our hypothesis that smoothing would improveperformance on rare terms is validated by these ex-periments.
In fact, the more aggregation a smooth-ing model performs, the better it appears to be atsmoothing.
The HMM-smoothed system outper-forms all other systems in all categories excepttagging on OOV words, where TF-IDF performsbest.
And in most cases, the clear trend is forHMM smoothing to outperform LSA, which inturn outperforms TF and TF-IDF.
HMM taggingperformance on OOV terms improves by 11%, andchunking performance by 21%.
Tagging perfor-mance on all of the rare terms improves by 6%,and chunking by 10%.
In chunking, there is aclear trend toward larger increases in performanceas words become rarer in the labeled data set, froma 0.02 improvement on words of frequency 2, to animprovement of 0.21 on OOV words.Because the test data for this experiment isdrawn from the same domain (newswire) as the499training data, the rare terms make up a relativelysmall portion of the overall dataset (approximately4% of both the tagged words and the chunks).Still, the increased performance by the HMM-smoothed model on the rare-word subset con-tributes in part to an increase in performance onthe overall dataset of 1% for tagging and 3% forchunking.
In our next experiment, we considera common scenario where rare terms make up amuch larger fraction of the test data.3.3 Domain AdaptationFor our experiment on domain adaptation, we fo-cus on NP chunking and POS tagging, and weuse the labeled training data from the CoNLL2000 shared task as before.
For NP chunking, weuse 198 sentences from the biochemistry domainin the Open American National Corpus (OANC)(Reppen et al, 2005) as or our test set.
We man-ually tagged the test set with POS tags and NPchunk boundaries.
The test set contains 5330words and a total of 1258 NP chunks.
We usedsections 15-18 of the Penn Treebank as our labeledtraining set, including the gold standard POS tags.We use our best-performing smoothing model, theHMM, and train it on sections 13 through 19 ofthe Penn Treebank, plus the written portion ofthe OANC that contains journal articles from bio-chemistry (40,727 sentences).
We focus on chunksthat begin with words appearing 0-2 times in thelabeled training data, and appearing at least tentimes in the HMM?s training data.
Table 4 con-tains our results.
For our POS tagging experi-ments, we use 561 MEDLINE sentences (9576words) from the Penn BioIE project (PennBioIE,2005), a test set previously used by Blitzer etal.(2006).
We use the same experimental setup asBlitzer et al: 40,000 manually tagged sentencesfrom the Penn Treebank for our labeled trainingdata, and all of the unlabeled text from the PennTreebank plus their MEDLINE corpus of 71,306sentences to train our HMM.
We report on taggingaccuracy for all words and OOV words in Table5.
This table also includes results for two previoussystems as reported by Blitzer et al (2006): thesemi-supervised Alternating Structural Optimiza-tion (ASO) technique and the Structural Corre-spondence Learning (SCL) technique for domainadaptation.Note that this test set for NP chunking con-tains a much higher proportion of rare and OOVwords: 23% of chunks begin with an OOV word,and 29% begin with a rare word, as compared withBaseline HMMFreq.
# R P F1 R P F10 284 .74 .70 .72 .80 .89 .841 39 .85 .87 .86 .92 .88 .902 39 .79 .86 .83 .92 .90 .910-2 362 .75 .73 .74 .82 .89 .85all 1258 .86 .87 .86 .91 .90 .91Table 4: On biochemistry journal data from the OANC,our HMM-smoothed NP chunker outperforms the base-line CRF chunker by 0.12 (F1) on chunks that begin withOOV words, and by 0.05 (F1) on all chunks.
Results inbold are statistically significantly different from the baselineresults at p < 0.05 using the two-tailed Fisher?s exact test.We did not perform significance tests for F1.All UnknownModel words wordsBaseline 88.3 67.3ASO 88.4 70.9SCL 88.9 72.0HMM 90.5 75.2Table 5: On biomedical data from the Penn BioIEproject, our HMM-smoothed tagger outperforms theSCL tagger by 3% (accuracy) on OOV words, and by1.6% (accuracy) on all words.
Differences between thesmoothed tagger and the SCL tagger are significant at p <.001 for all words and for OOV words, using the Chi-squaredtest with 1 degree of freedom.1% and 4%, respectively, for NP chunks in the testset from the original domain.
The test set for tag-ging also contains a much higher proportion: 23%OOV words, as compared with 1% in the originaldomain.
Because of the increase in the number ofrare words, the baseline chunker?s overall perfor-mance drops by 4% compared with performanceon WSJ data, and the baseline tagger?s overall per-formance drops by 5% in the new domain.The performance improvements for both thesmoothed NP chunker and tagger are again im-pressive: there is a 12% improvement on OOVwords, and a 10% overall improvement on rarewords for chunking; the tagger shows an 8% im-provement on OOV words compared to out base-line and a 3% improvement on OOV words com-pared to the SCL model.
The resulting perfor-mance of the smoothed NP chunker is almost iden-tical to its performance on the WSJ data.
Throughsmoothing, the chunker not only improves by 5%500in F1 over the baseline system on all words, it infact outperforms our baseline NP chunker on theWSJ data.
60% of this improvement comes fromimproved accuracy on rare words.The performance of our HMM-smoothed chun-ker caused us to wonder how well the chunkercould work without some of its other features.
Weremoved all tag features and all features for wordtypes that appear fewer than 20 times in training.This chunker achieves 0.91 F1 on OANC data, and0.93 F1 on WSJ data, outperforming the baselinesystem in both cases.
It has only 20% as many fea-tures as the baseline chunker, greatly improvingits training time.
Thus our smoothing features aremore valuable to the chunker than features fromPOS tags and features for all but the most commonwords.
Our results point to the exciting possibil-ity that with smoothing, we may be able to train asequence-labeling system on a small labeled sam-ple, and have it apply generally to other domains.Exactly what size training set we need is a ques-tion that we address next.3.4 Sample ComplexityOur complete system consists of two learned com-ponents, a supervised CRF system and an unsu-pervised smoothing model.
We measure the sam-ple complexity of each component separately.
Tomeasure the sample complexity of the supervisedCRF, we use the same experimental setup as inthe chunking experiment on WSJ text, but we varythe amount of labeled data available to the CRF.We take ten random samples of a fixed size fromthe labeled training set, train a chunking model oneach subset, and graph the F1 on the labeled testset, averaged over the ten runs, in Figure 1.
Tomeasure the sample complexity of our HMM withrespect to unlabeled text, we use the full labeledtraining set and vary the amount of unlabeled textavailable to the HMM.
At minimum, we use thetext available in the labeled training and test sets,and then add random subsets of the Penn Tree-bank, sections 2-22.
For each subset size, we taketen random samples of the unlabeled text, train anHMM and then a chunking model, and graph theF1 on the labeled test set averaged over the tenruns in Figure 2.The results from our labeled sample complex-ity experiment indicate that sample complexity isdrastically reduced by HMM smoothing.
On rarechunks, the smoothed system reaches 0.78 F1 us-ing only 87 labeled training sentences, a level thatthe baseline system never reaches, even with 6933baseline (all)HMM (all)HMM (rare)0.60.70.80.91F1(Chunking)Labeled Sample Complexitybaseline (rare)0.20.30.40.51 10 100 1000 10000F1(Chunking)Number of Labeled Sentences (log scale)Figure 1: The smoothed NP chunker requires less than10% of the samples needed by the baseline chunker toachieve .83 F1, and the same for .88 F1.Baseline (all)HMM (all) HMM (rare)0.800.850.900.95F1(Chunking)Unlabeled Sample ComplexityBaseline (rare)0.700.750 10000 20000 30000 40000F1(Chunking)Number of Unannotated SentencesFigure 2: By leveraging plentiful unannotated text, thesmoothed chunker soon outperforms the baseline.labeled sentences.
On the overall data set, thesmoothed system reaches 0.83 F1 with 50 labeledsentences, which the baseline does not reach un-til it has 867 labeled sentences.
With 434 labeledsentences, the smoothed system reaches 0.88 F1,which the baseline system does not reach until ithas 5200 labeled samples.Our unlabeled sample complexity results showthat even with access to a small amount of unla-beled text, 6000 sentences more than what appearsin the training and test sets, smoothing using theHMM yields 0.78 F1 on rare chunks.
However, thesmoothed system requires 25,000 more sentencesbefore it outperforms the baseline system on allchunks.
No peak in performance is reached, sofurther improvements are possible with more unla-beled data.
Thus smoothing is optimizing perfor-mance for the case where unlabeled data is plenti-ful and labeled data is scarce, as we would hope.4 Related WorkTo our knowledge, only one previous system ?the REALM system for sparse information extrac-501tion ?
has used HMMs as a feature represen-tation for other applications.
REALM uses anHMM trained on a large corpus to help determinewhether the arguments of a candidate relation areof the appropriate type (Downey et al, 2007).
Weextend and generalize this smoothing techniqueand apply it to common NLP applications involv-ing supervised sequence-labeling, and we providean in-depth empirical analysis of its performance.Several researchers have previously studiedmethods for using unlabeled data for tagging andchunking, either alone or as a supplement to la-beled data.
Ando and Zhang develop a semi-supervised chunker that outperforms purely su-pervised approaches on the CoNLL 2000 dataset(Ando and Zhang, 2005).
Recent projects in semi-supervised (Toutanova and Johnson, 2007) and un-supervised (Biemann et al, 2007; Smith and Eis-ner, 2005) tagging also show significant progress.Unlike these systems, our efforts are aimed at us-ing unlabeled data to find distributional represen-tations that work well on rare terms, making thesupervised systems more applicable to other do-mains and decreasing their sample complexity.HMMs have been used many times for POStagging and chunking, in supervised, semi-supervised, and in unsupervised settings (Bankoand Moore, 2004; Goldwater and Griffiths, 2007;Johnson, 2007; Zhou, 2004).
We take a novel per-spective on the use of HMMs by using them tocompute features of each token in the data thatrepresent the distribution over that token?s con-texts.
Our technique lets the HMM find param-eters that maximize cross-entropy, and then useslabeled data to learn the best mapping from theHMM categories to the POS categories.Smoothing in NLP usually refers to the prob-lem of smoothing n-gram models.
Sophisticatedsmoothing techniques like modified Kneser-Neyand Katz smoothing (Chen and Goodman, 1996)smooth together the predictions of unigram, bi-gram, trigram, and potentially higher n-gram se-quences to obtain accurate probability estimates inthe face of data sparsity.
Our task differs in that weare primarily concerned with the case where eventhe unigram model (single word) is rarely or neverobserved in the labeled training data.Sparsity for low-order contexts has recentlyspurred interest in using latent variables to repre-sent distributions over contexts in language mod-els.
While n-gram models have traditionally dom-inated in language modeling, two recent efforts de-velop latent-variable probabilistic models that ri-val and even surpass n-gram models in accuracy(Blitzer et al, 2005; Mnih and Hinton, 2007).Several authors investigate neural network mod-els that learn not just one latent state, but rather avector of latent variables, to represent each wordin a language model (Bengio et al, 2003; Emamiet al, 2003; Morin and Bengio, 2005).One of the benefits of our smoothing techniqueis that it allows for domain adaptation, a topicthat has received a great deal of attention fromthe NLP community recently.
Unlike our tech-nique, in most cases researchers have focused onthe scenario where labeled training data is avail-able in both the source and the target domain(e.g., (Daume?
III, 2007; Chelba and Acero, 2004;Daume?
III and Marcu, 2006)).
Our technique usesunlabeled training data from the target domain,and is thus applicable more generally, includingin web processing, where the domain and vocab-ulary is highly variable, and it is extremely diffi-cult to obtain labeled data that is representative ofthe test distribution.
When labeled target-domaindata is available, instance weighting and similartechniques can be used in combination with oursmoothing technique to improve our results fur-ther, although this has not yet been demonstratedempirically.
HMM-smoothing improves on themost closely related work, the Structural Corre-spondence Learning technique for domain adap-tation (Blitzer et al, 2006), in experiments.5 Conclusion and Future WorkOur study of smoothing techniques demonstratesthat by aggregating information across manyunannotated examples, it is possible to find ac-curate distributional representations that can pro-vide highly informative features to supervised se-quence labelers.
These features help improve se-quence labeling performance on rare word types,on domains that differ from the training set, andon smaller training sets.Further experiments are of course necessaryto investigate distributional representations assmoothing techniques.
One particularly promis-ing area for further study is the combination ofsmoothing and instance weighting techniques fordomain adaptation.
Whether the current tech-niques are applicable to structured predictiontasks, like parsing and relation extraction, also de-serves future attention.502ReferencesRie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method fortext chunking.
In ACL.Michele Banko and Robert C. Moore.
2004.
Part ofspeech tagging in context.
In COLING.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.C.
Biemann, C. Giuliano, and A. Gliozzo.
2007.
Un-supervised pos tagging supporting supervised meth-ods.
Proceeding of RANLP-07.J.
Blitzer, A. Globerson, and F. Pereira.
2005.
Dis-tributed latent variable models of lexical cooccur-rences.
In Proceedings of the Tenth InternationalWorkshop on Artificial Intelligence and Statistics.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.E.
Brill.
1994.
Some Advances in Rule-Based Part ofSpeech Tagging.
In AAAI, pages 722?727, Seattle,Washington.Ciprian Chelba and Alex Acero.
2004.
Adaptation ofmaximum entropy classifier: Little data can help alot.
In EMNLP.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th annual meet-ing on Association for Computational Linguistics,pages 310?318, Morristown, NJ, USA.
Associationfor Computational Linguistics.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In ACL.S.
C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.Furnas, and R. A. Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of the AmericanSociety of Information Science, 41(6):391?407.Arthur Dempster, Nan Laird, and Donald Rubin.
1977.Likelihood from incomplete data via the EM algo-rithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.Doug Downey, Stefan Schoenmackers, and Oren Et-zioni.
2007.
Sparse information extraction: Unsu-pervised language models to the rescue.
In ACL.A.
Emami, P. Xu, and F. Jelinek.
2003.
Using aconnectionist model in a syntactical based languagemodel.
In Proceedings of the International Confer-ence on Spoken Language Processing, pages 372?375.Zoubin Ghahramani and Michael I. Jordan.
1997.
Fac-torial hidden markov models.
Machine Learning,29(2-3):245?273.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully bayesian approach to unsupervised part-of-speech tagging.
In ACL.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers.
In EMNLP.J.
Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.In Proceedings of the International Conference onMachine Learning.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.
Com-putational Linguistics, 19(2):313?330.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of the 24th International Conferenceon Machine Learning, pages 641?648, New York,NY, USA.
ACM.F.
Morin and Y. Bengio.
2005.
Hierarchical probabilis-tic neural network language model.
In Proceedingsof the International Workshop on Artificial Intelli-gence and Statistics, pages 246?252.PennBioIE.
2005.
Mining the bibliome project.http://bioie.ldc.upenn.edu/.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?285.Randi Reppen, Nancy Ide, and Keith Suderman.
2005.American national corpus (ANC) second release.Linguistic Data Consortium.F.
Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings ofHuman Language Technology - NAACL.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 354?362, Ann Arbor, Michigan, June.Erik F. Tjong, Kim Sang, and Sabine Buchholz.2000.
Introduction to the CoNLL-2000 shared task:Chunking.
In Proceedings of the 4th Conference onComputational Natural Language Learning, pages127?132.Kristina Toutanova and Mark Johnson.
2007.
Abayesian LDA-based model for semi-supervisedpart-of-speech tagging.
In NIPS.GuoDong Zhou.
2004.
Discriminative hidden Markovmodeling with long state dependence using a kNNensemble.
In COLING.503
