COMBINATORIAL  DISAMBIGUATIONP.
S. NewmanIBM Los Angeles Scientific Center11601 Wilshire BoulevardLos Angeles, CA 90025-1738AbstractThe disambiguation of sentences i a combinatorialproblem.
This paper describes a method for treatingit as such, directly, by adapting standard combinatorialsearch optimizations.
Traditional disambiguation heu-ristics are applied but, instead of being embedded inindividual decision procedures for specific types ofambiguities, they contribute to numerical weights thatare considered by a single global optimizer.
The resultis increased power and simpler code.
The method isbeing implemented for a machine translation projecl,but could be adapted to any natural language system.1.
IntroductionThe disambiguation of sentences i a combinatorialproblem.
Identification of one word sense interactswith the identification of other word senses,( I )  l ie addressed the chairand with constituent attachment,(2) He shot some bucks with a rifleMoreover, the attachment of one constituent interactswith the attachment of other constituents:(3) She put the vase on the table in the living roomThis paper describes a method of addressing the prob-lem directly, by adapting standard searctl optimizationtechniques, in the first section we describe the core ofthe method, which applies a version of best-.first searchto a uniform representation f the set of possibilities.In the second section we relate the work to otherapproaches to preference-based disambiguation.
"l"hefinal sections describe how the representation may heobtained from a lexicon.2.
The Search MethodIn the machine translation project for which this tech-nique is being developed, disambiguation begins aftera parser, specifically the PLNI.P English Grammarby Jensen (1986), has identified one or more parsesbased primarily on syntactic onsiderations (includingsubcategorization).
Words are disambiguated up topart-of-speech, but word senses are not identified.
In-dividual parses may indicate alternative attachmentsof many kinds of constituents including prepositionalphrases and relative clauses.Beginning disambiguation o ly after a general parsehas the advantage of making clear what all the possi-bilities are, thus allowing their investigation i an ef-ficient order.
Perlbrming a full parse beforedisamblguatlon eed not consume an inordinateamount of space or time; techniques uch as thoseused by Jensen (default rightmost prepositional phraseattachment), and "l'omita (1985) (parse forests) ade-quately control resource requirements.The parser output is first transfi)rmed so that it isrepresented asa set of of semantic hoice points.
Eachchoice point generally represents a constituent, it con-rains a group of weighted semantic alternatives thatrepresent the different ways in which word-senses oftile constituent head can he associated semanticallywith word-senses of a higher level head.
This allowsword-sense and attachment alternatives to be treateduniformly.Combinatorial disamhiguation then selects the consis-tent combination of alternatives, one from cat'h choicepoint, that yields tile highest otal weight.
"1,~ illustratethe method, we use tile exlension of the classic examplementioned above:(2) lie shot some bucks with a rifleA decomposition of tile sentence into choice points el,c2, and c3 is shown in Figure I.
(The illustration243assumes that "shot" and "bucks" have two meaningseach, and ignores determiners.)
Choice point "cl"gives the alternative syntactic and semantic possibilitiesfor the attachment of the constituent "he" to its onlypossible head "shot'.
Alternative l l  is that "he" isthe subject of %hootl ' ,  with the semantic function"agent', and is given (for reasons which will be dis-cussed later) the weight "3".
Alternative c12 is similar,but the meaning used for "shoot" is "shoot2".
Similaralternatives are used for the attachment (c2) of theobject "some bucks" to its only possible head, "shoot'.Alternative c23 represents the unlikely combinations.Choice point c3 reflects the different possible attach-ments of 'with a rifle'; the highest v, eight (3) is givento its attachment as an instrumental modifier of"shootl ' .
The other possibilities range from barelyplausible to implausible and are weighted accordingly.I laving obtained this repesentation (whose constructionis described in later sections), the next step is to es-tablish the single-alternative choice points as given andto propagate any associated word-sense constraints tonarrow or eliminate other alternatives.
(l'his does notoccur in our example.
)Then combinations of the remaining choices aresearched using a variation of the A* best-first searchmethod.
See Nilsson (1980) for a thorough descriptionof A*.
Briefly, A* views combinatorial search as aproblem of finding an optimal path from the root toa leaf of a tree whose nodes are the weighted alterna-tives to be combined.
At any point in the process, thenext node n to be added is one for which the potentialF(n) = G(n) + If(n)is maximal over all possible additions.
G(n) representsthe value of the path up to and including n. I I(n)represents an estimated upper bound for the value ofthe path below n (i.e., for the additional value whichcan be obtained) ~.When a complete path is found bythis method, it must be optimal.
The efficiency of themethod, i.e., the speed of finding an optimal path,depends on the accuracy of tile estimates ll(n).To apply the method in our context, tile search treeis defined in terms of levels, with each level corre-sponding to a choice point.
Choice points are assignedto levels so that those which would probably be re-sponsible for the greatest difference between estimatedand actual l l(n) in an arhitrary assignment are exam-ined first.
Looked at in another way, the assignmentof choice points to levels is made so that those whichwill best differenliale among polential path scores areexamined firsl.This is done by (partially) ordering the choice pointsin descending order of their difference potential De,the difference in weight between their highest weightedalternative and the next allernalive.
If the highestweight is represented by two different alternatives, Dc= 0.
Within this ordering the choice points are furtherordered by weight differences between tile 2rid and3rd highest weighted alternatives, etc.
For our examplethis results in choice point c3 ('with a rifle') beingassigned to the highest level in the tree, followed bychoice points c2 and then el.c l .
Hec l l  agt (he shoot l )  3c lZ agt the shootZ) 3eZ.
some buckscZ l  goa l  !buck1 shoot1)  3c22 goa l  ( t~ck2 shoot2)  3c23 goa l  !
buck1 shoot2 ), ( buck2 shoo i l  )c3 .~ i*h  a r i f l ec31 ins*  ( r i f l e  shoo i l )  3c32 i ns t  t r i f l e  shooi2)  2c33 ~og~ t r i f l e  (buck l ,buck2) )  0c3~ accm t r i f l e  (shoo~l ,shoo~2))  0t i .e .
,  f i red -a t  )t i .e .
,  ~rasted )l i .
e .
,  male deer )t i .e .
,  do l la r )0( i .e .
,  together -~ i th )( i .e .
,  ~ccompanied-by)Figure 1: Choice points and alternatives244We also associate with each level=choice point thevalue lie, which is the sum of the maximum weightsthat can be added by contributions below that choicepoint.
This is needed by the algorithm to estimatepotential path scores below a given node.
For thisexample, the l ie values are:HO: ~op=9 H3: r i f l e=6HZ: buck=3 H I :  he=0Then the best-first search is carried out.
At each pointin the search, the next node to be added is that which(a) is consistent in word-sense selection with previouschoices, and (b) has the highest potential.
The potentialis calculated as the accumulated weight down to (andincluding) that node plus tic for that level.
A diagramof the procedure, as applied to the example, is shownin Figure 2.
The first node to be added is "with rifleshootl' ,  which has the highest potential.
At that point,the highest weighted consistent alternative is c21, etc.While the set of choice points implies that there are(4 x 3 x 2) = 24 paths to be searched, only one ispursued to any distance.
Thus while the approachtakes a combinatorial view of the problem, it does sowithout loss of efficiency.When a full path is found, it is examined for semanticconsistency (beyond word-sense consistency).
Thechecks made include: (a) ensuring that the interpreta-tion includes all required semantic functions for aword-sense (specified in the lexicon), and (b) ensuringthat non-repeatable functions (e.g., the goal of an ac-tion) are not duplicated.Even if the full path is found to be consistent, hesearch does not terminate immediately, but continuesuntil it is certain that no other path can have aft equalscore.
This will be true whenever the maximum po-tential for open nodes is less than the score for analready-found path.
A more precise description of thealgorithm is given in the Appendix.When more than one path is found with the samehigh score, additional tests are applied.
These testsinclude comparisons of surface proximity and, as thiswork is situated within a multi-target translation sys-tem, user queries in the source language, as outlinedby Tomita (1985).An extended version of the method is used in com-paring alternate parses which differ in constltucnt com-position, and thus are more easily analyzed as differentparse trees, each with its own set of choice points.
Anexample is the classic:(4 )  T ime f l ies like an arrow(where the main verb can he arty one of tile first threewords).
In such cases, one set of choice points isconstructed per parse tree.
In general, the searchalternates among trees, with the next node to be addedbeing that with tile greatest potential across trees.
Ifsuch trees always had Ihe same numhe,- of choicepoints, this would he the only revision needcd, llow-ever, the number of choice poinls may differ, for onething because the parser may have detected and con-densed non-compositional compounds (e.g., "all thesame ~) in one parse but not in another.
For thisreason the algorithm changes to evaluate paths not bytotal scores, but by average scores (i.e., the total scoresdivided by the number of choice points in the particularparse).
"kop"rifle" c31"buck"  cZ l  cZZ ( incons is tent  )"he" cllFigure 2: Red.ced Tree Search!
The basic A* algorithm is usually described as *expanding" (i.e., adding all immediate successors ol) the most promising node.
The variantdescribed here, which is more appropriate to our situation (and also mentioned by Nilsson), adds a single node at each step.2453.
Related WorkThere seems to be little work which directly addressesthe combinatorial problem.
First, there is considerablework in preference-related disambiguation that as-sumes, at least for purposes of discussion, that indi-vidual disambiguation problems can be addressed inisolation.
For example, treatments of prepositionalphrase attachment by by Dahigren and McDowell(1986) and Wilks el.
al.
(1985) propose methods offinding the "best ~ resolution of a single attachmentproblem by finding the first preference which is satisfiedin some recommended order ofrule application.
Othertypes of ambiguity, and other instances of the sametype, are assumed to have been resolved.
This typeof work contributes interesting insights, but cannot beused directly.One type of more realistic treatment, which might becalled the deferred decision approach, is exemplifiedby ilirst (19831.
When, in the course of a parse, animmediate decision about a word sense or attachmentcannot be made, a set of alternative possibilities isdeveloped.
The possibility sets are gradually narrowedby propagating the implications of both decisions andnarrowings of other possibility sets.This approach as a number of problems.
First, it issusceptible to error in semantic "garden path" situa-lions, as early decisions may not be justifiable in thecontext.
For example, in processing(5) He shot a hundred bucks with one rifle.a particular expert might decide on "dollars" for bucks,because of the modifier "hundred', before the prepo-sitional phrase is processed.
Also, it is difficult to seehow versions of this method could be extended to dealwith comparing alternate parses where the alternativesare not just ones of attaching constituents, but of de-ciding what the constituents are in the first place.A full-scale deferred-decision approach also has thepotential of significant design complexity (the il irstversion is explicitly limited), as each type of decisionprocedure (for words and for different kinds of attach-ments) must be able to understand and process theimplications of the results of other kinds of decisionprocedures.Underlying these problems is the lack of quantificationof alternatives, which allows for comparison of com-binations.There are, however, early and more recent approacheswhich do apply numeric weights to sentence analysis.Early approaches using weights applied them primarilyto judge alternative parses.
Syntactically-oriented ap-proaches in this vein attached weights to phrase struc-ture grammar ules (Robinson 1975, 1980) or ATNarcs (Bates 1976).
Some approaches of this periodfocussing on semantic expectations were those of Wilks(19751 and Maxwell and Tuggle (1975), which em-ployed scores expressing the number of expected e-pendents present in an interpretation.
An ambitiousapproach combining different kinds of weights andcumulative scores, described by Walker and Paxtonet.
al.
(19771, included heuristics to select the mostpromising sublrees for earlier development, o avoidrunning out of space before a "best" tree can be found.llowever, except for tiffs type of provision, none ofthe early approaches using weights seem to addressthe combinatorial problem.
~A contemporary approach for thorough syntactic andsemantic disambigualion using weights is described byL.
Schubert (1986).
During a left-to-right parse, in-dividual attachments are weighted based on a list ofconsiderations including preference, relative sententialposition, frames/scripL~, and salience in the currentcontext.
The multiple evolving parse trees are ratedby summing their contained weights, and the combi-natorial problem is controlled by retaining only thetwo highest scoring parses of any complete pltrases.This approach is interesting, although some details arevague 3. l\[owevcr, the post-parse application of A*described in this paper obtains the benefits of such awithin-parse approach without its deficiences in that:(a) combinatorial computations of weighCs and word-sense consistencies are avoided except when warrantedby total sentence informalion, and (b) there is no pos-sibility of early erroneous discarding of allerrmtives.Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisionsbased on syntactic onsiderations only.No examples are given, so it is unclear whether a parse for a phrase or part thereof represents only one interpretation, or all interpretationshaving the same structure, scored by the most likely interpretation.
The former is obviously inadequate (c.g., for highly ambiguous ubject NPslike *The stands'), while the latter seems to require either the calculation of all alternative cumulative scores, or recalculation of scores if aninterpretation fails.246he buckssub~ he shoo( obj buck shootwith  a r i f l ew i th  r i f l e  shootwith r i f l e  buckFigure 3:Syntactic Choice Poin~One other parser-based work should be noted, that ofWittenburg (1986), as it is explicitly based on A*.
Theintent and content of the method is quite different fromthat described here.
It is situated within a chart-parserfor a categorial grammar, and the intent is to limitparsing expense by selecting that rule for next appli-cation which has the least likelihood of leading to anincomplete tree.
While selectional preference is men-tioned in passing as a possible heuristic, the heuristicsdiscussed in any depth are grammar oriented, andoperate on the immediate facts of the situation, ratherthan on informed estimates of total parse scores.It should be also be mentioned that the representationof alternatives in schemes which combine syntacticand semantic disambiguation is rarely discussed, al-though maintaining a consistent representation of therelationships among word-sense and attachment alter-natives is fundamental to a systematic treatment of theproblem.
An exception is the discussion by K.Schubert 0986), who describes a representation foralternatives with some affinities to that described here 4.The information limitations of disambiguation duringparsing are not found in spreading-activation ap-proaches, exemplified by Charniak 0986), Cottrelland Small 0984), and Waltz and Pollack (1985).These approaches are still in the experimental stage,and are primarily intended for parallel hardware, whilethe A* algorithm used in this paper is designed forconventional serial hardware.
But, in a sense, theseapproaches reinforce the main point of this paper:they argue for a single global technique for optimizedcombinatorial disambiguation based on all availableinformation.4.
Preparing Semantic Choicesl laving described how the choice points are used, weaddress their development.
Two steps are involved:(1) the development of syntactic hoice points, and (2)the development of semantic hoice points.
The firststep transforms the parse-level syntactic functions intoa form appropriate to the second step, which is theapplication of the lexicon to those fimctions to obtainthe semantic alternatives.In our example, the first step is a simple one.
Syntacticrelationships among conslituents are transformed intosyntactic relationships among head words, and thesyntactic relationships arc refined, so that "ppmod" isreplaced by the actual preposilions used.
The resultof this step is shown in Figure 3.
The development ofsyntactic hoice poinls for some other types of con-stituents is more complex.
Before discussing thesesituations, we discuss slep 2: application of the lexiconto the syntactic hoice points Io obtain Ihe semanticchoice points, i.e., those shown in Figure I.4.1 The LexiconThe lexicon conlains entries for word stems (distin-guished by part-of-speech), linked to word-sense n-tries, which are lhe lowest level "conccptsL Conceptentries are linked Io superordinale concept entries,forming a lattice.
Concepl entries include a set ofconcept features (e.g.
a list of snperordinate concepts),and one or more rules for each syntactic functionassociated with the concept.
The more relevant partsof the lexicon entries fbr the concepts used in theongoing example are shown in Figure 4.
The "classes"are lisls of superordinale concepts.
The synlactic func-tion rules have the form:synfun dependent head semfun weightThus tile first rule under "shootl" indicates that word-senses falling into tile class "animate" are its preferredobjects, with tile weight 3, and the combination isgiven tile semantic fimclion "goal'.
The concept entry4 However, the weighting scheme is different, and rather interesting.
The reference does not discuss the selection of a particular combination falternatives in any detail, but it appears tobe based on the presence in a combination f one (or more?)
highly weighted alternative (or alternatives.'?
).247shoo~l classes I humanac~, ~ransv)(ob~ec~ animate shoo{1 goal 3)(wi{h f i rearm shoo~1 ins{ 3)humanac~ classes (ac~.
.
.
)(sub~ human humanac{ ag{ 3)tw i ih  human hunmnact accm 31buck1 classes (an imate .
.
.
)but:k2 classes (money..)shoo~2 classes Ihumanac~, {ransv)l ob jec~ money shoo{2 goal 3){ransvlobjec~ ~hing ~ransv goal O)acttwi{h too l  ac t  ins~ 21r i f l e  classes ( f i rearm.
.
.
)f i rearm classes I~oo l .
.
.
)Figure 4: Izxieon Entries"humanact" contains other rules applicable to shootland other verb-senses taking human actors.Weights are actually assigned symbolically, to allowexperimentation.
Current settings are as follows:?
Rules for idioms (e.g., kick the bucket), 4.?
RUles for more general selectional preferences, 3.?
Rules for acceptable but not preferred alternatives(e.g., locative prepositional phrases attached to ar-bitrary actions), 2.?
Very general attachments (e.g., "nounmod noun lnoun2), 0.
These allow for uninterpreted metaphoricusage.
5One major objective in assigning weights to ensurethat combinations of very high (idiom) weights togetherwith very low weights do not ouLscore more balancedcombinations.
Thus, for:(6) He kicked the ugly bucketweights such as:subj he k ickedl  $oh\] bucke{1 k ickedlad\]m ugly buc.ke~ 1 0subj he kicked2 3ob~ bue.ke{2 kicked2 3adam ugly bucke{2 2provide the necessary balance.
( l lere kickedl is theidiomatic interpretation, and bucketl is a word-senseof bucket used only in that interpretation.
)By convention, rules for syntactic functions are as-signed, by class, to entries for specific kinds of concepts.Thus rules for verb-attached arguments or preposi-tional phrases are stored with verbs or verb classes.Adjective-noun rules are generally associated with ad-jectives, and noun-north rules with the right-handnouns (rite next section discusses the treatment ofnoun-phrase choice points irt somewhat greater detail).Lexicon entries also cot|lain additional information.First, a list of required syntactic functions is generallyassociated with word-senses.
Also, syntactic functionrules may contain additional conditions limiting theirapplicability.
For example, a combination "nounl  INnoun2* might be limited to apply only if the secondconcept denotes an object larger than the first.4.2 Lexicon ApplicationGiven these lexicon entries, the set of semantic alter-natives corresponding to each syntactic alternative"synfun wordl word2" may be derived.
The goal ofthe derivation process is to account for all possiblecombinations of word-senses for wordl and word2related by tile syntactic fimction "synfim".
To do this,all concept entries containing potentially applicablerules are searched.
For each satisfied rule fotmd, analternative is created of the form:senran{ic-rela~ion sensepairs weigh{where "sensepairs" is a list of pairs.
F.ach pair is inthe form ((di,dj,...) (hnl,hn,...)), where the di's aresenses of the dependenl participant of the function,Obtaining the necessary lexicon information is of course a major problem.
But there is significant contemporary work in the automatic orsemi-automatic derivation ofthat information.
For example, the aproached described by Jcnsen and Binot (19~,6) obtains attachment preferenceinformation by parsing dictionary definitions.248and tile hi's are senses of the headword.
The semanticrelationship is stated to apply to all combinations ofword-senses in the cross-products of those lists.
Forthe example sentence, this process would obtain es-sentially the alternatives shown in Figure 1, exceptthat alternative c23 would first be expressed as:ob j  I t buek l  , l~ckZ ) ( shoo~l ,  shoot2  ) ) 0The last step in the process reduces this result.
Ifsome of the word-sense combinations are also foundin an alternative of higher weight, the "dominated"combinations are deleted.
And if all word sense com-binations are so dominated, the alternative is deleted.In this way alternative c23 is reduced to its final form.After the semantic hoice point list is completed, thesearch algorithm is applied as described above.5.
Preparing Syntactic ChoicesIn the example above, the preparation of syntacticchoice points from parser output was very simple.Assuming an input choice point for a constituent tobe a list of (one or more) parser-provided alternativerelationships with an immediately containing constitu-ent, the process consisted of obtaining the headwordsof each constituent, and of substituting literal prepo-sitions for the general syntactic function "ppmod'.i iowever, in other cases this step is a more significantone.
In the lexicon, selectional preferences are ex-pressed in terms of the syntactic functions of somebasic constituent types.
For example, verb preferencesare expressed in terms of the syntactic functions ofactive-voice, declarative main clauses, with dependentsin unmarked positions.
Adjective preferences are ex-pressed in terms of classes of nouns occurring in therelationship "adjective noun'.
But there are manyother syntactic relationships whose dlsambiguation de-pends on this information.
The major function of thesyntactic hoice identification step is to re-express, or"normalize" input syntacfic relationsips in terms of therelationships assumed by the lexicon.
For example,passive constructions such as:(7) The bucks were shot with a rifleare normalized by replacing the choice "subj buckshoot" with "object buck shoot'.
(A lexicon conditionbarring or lowering preference for the "gambling" in-terpretation i the passive voice is also needed here.
)In ditransitive cases both indirect and direct objectfunctions are used as allernatives.Thus the sequence of deriving semantic hoice pointsconsists of two siguilicant steps, which may be depictedin terms of results as shown in Figure 5.The transformation f input syntactic hoice points tonormalized synlactic hoice poinls is governed by de-clarative specifications indicaling, for each syntacticfunction, how its choice points are to be transformed.The changes are specified as replacements for one ormore positions of the choice triple.
For example, someof the "subj" rules are:t sub j( ~es~ tvo icepass ive!sy~fun  'ob j  ) )!
t :es~ (not  (vo ice f )ass ive) )synfun  ' sub j l  .
.
.slating that for tile input fimction "subj', if the specifiedtest (voicepassive) succeeds, then "obj" is used for thesynfun part of tile normalized choice.
The additionalrule is used to ensure that tile ~subject" fimclion isretained only for ttle aclive voice.Additional applications of these transformations in-clude those for modifiers of nomlnalized verbs, attrib-utive clauses, and relative clauses.I nput  Syn Chp~l  Normal i zed  Syn Chp~l.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Choice  C l l  Cho ice  C l l lSeman~ ic  Chp~lCho ice  C1111Cho ice  Cl112Cho ice  C l lZ  Cho ice  C1121Cho ice  Cl12ZFigure 5: Steps in Semantic Choice Point Derivation249Noun phrases whose heads are nominalized verbs areaddressed by adding choice points corresponding toverb arguments.
Thus for(8) 1"he bucks' shooting .....the alternative "nounmod bucks shooting" is expandedto include-the alternatives "subj bucks shooting" and"obj bucks shooting'.
Then, during lexical processing,rules for word-senses of the noun "shooting" havingan associated verb are understood as expanded toinclude the expected verb arguments.Attributive clauses such as:(9) The bucks were handsome.are transformed to allow the application of adjectiveinformation, t lere 'ob j  handsome were" is transformedto *adjmod handsome bucks'.For relative clauses, the core of the transformationexpresses alternative attachments of the relative clauseas alternative connections between the head of therelative clause, and the possible fillers of the gap po-sition.
(Relative clauses with multiple gaps are gen-erally handled in separate parses.)
Thus for:(10) The rifle above the mantle that the bucks wereshot with...transformations produce the alternatives "with shootrifle" and "with shoot mantle'.The handling of relative clauses, however, is morecomplicated than this, as it is desirable to also useinformation from the relative pronoun (if present) forthe disambiguation.
Two initial choice points are in-volved, one attaching the relative clause to its higherlevel head, and one attaching the gap to its head.
Thefirst is expanded to to obtain relationships *relp thatrifle", "relp that mantle*, and the other to obtain the"with .... " relationships.
And an additional consistencycheck is made during the tree search, beyond word-sense consistency, to keep the choices consistent.It should be noted that the transformation rules forsyntactic hoice points also include "fixup specifica-tions" (not shown above) indicating how result semanticfunctions and attachments are to be modified if thetransformed alternatives are used in the final interpre-tation.
For example, to "fixup" the results of trans-forming attributive clauses, the noun-modifier semanticrole is replaced with one suitable to a direct role inthe clause.6.
Concluding RemarksThis paper has summarized a three-step method foroptimized combinatorial preference baseddisambiguadon:1. obtaining syntactic hoice points, with alternativesstated as syntactic functions relating words.2.
transformation i to semantic hoice points withalternatives stated as weighted semantic functionsrelating word-senses, via lexicon search.3.
application of A* to search alternative combina-tions.This method, currently being implemented in the con-text of a multi-target machine translation system, ismore powerful and systematic than approaches usingisolated or interacting decision procedures, and thusis easier to modify and extend with new heuristics asdesired.The method is applicable to any post-parsedisambigualion situation, and can be taken as an ar-gument for that approach.
To demonstrate this, wefirst note that aspects of the method are useful forwithin-parse disambigualion.
In any realistic scheme,decisions must of\[en be deferred, making two aspectsof the method relevant: (a) the unified way of repre-senting word sense and attachment alternatives andtheir interdependency, and (b) the explicit, additiveweights.
F.xplicit additive weights substitute for elab-orate, case-specific rules, and also make possible asystematic treatment of alternative parses which differin more than word-senses and attachments.!
lowever, using weighted attachments for within-parsedisambiguation requires calculating the summedweights of, and examining the consistency of, all com-binations encountered whose elements cannot be dis-carded (assuming some good criteria for discardingcan be found).
Deferring disambiguation until afterthe parse allows for optimized searching of alternatives,as described above, to significantly limit tile numberof combinations examined.Future work in this direction will include refining tileweighting criteria, extending the method It deal withanaphoric references (using consideralions developedby, for example, Jensen and Zadrozny (1987), andintegrating a treatment of non-frozen metaphor.2507.Acknowledgements!
thank John Sowa, Maria Fuenmayor, and ShelleySmith for their careful reviews and many helpful sug-gestions.
Also, I thank Peter Woon for his patientmanagerial support of this project.8.
References1.
Bates, Madeleine 1976.
"Syntax in AutomaticSpeech Understanding', Am.
J. Comp.
Ling.
Mi-crofiche 45.2.
Charniak, Eugene 1986.
"A Neat Theory ofMarker Passing" Proc.
AAAI-86 584-5883.
Cottrell, Garrison W. and Steven L. Small 1984.
"Viewing Parsing as Word Sense Discrimination:A Connectlonist Approach', in B.G.
Bara and G.Guida (eds), Computation Models of Natural Lan-guage Processing, Elsevier Science Publishers B.V.4.
Dahlgren, Kathleen and Joyce McDowell 1986.
"Using Commonsense Knowledge toDisambiguate Prepositional Phrase Modifiers",Proc.
AAAI-86, 589-5935.
Heidorn, George 1982.
"Experience with an EasilyComputed Metric for Ranking Alternative Parses"Proc.
20th Annual Meeting of the ACL, June 19826.
Hirst, Graeme 1983.
"Semantic InterpretationAgainst Ambiguity', Technical Report CS-83-25,Brown University, December 19837.
Jensen, Karen 1986.
"Parsing Strategies in aBroad-coverage Grammar of English', IBM Re-search Report RC 12147, 19868.
Jensen, Karen and Jean-Louis Binot 1987.
"ASemantic Expert Using an Online Standard Dic-tionary', Proc.
HCAI-87, 709-7149.
Jensen, Karen and WIodzimierz Zadrozny 1987.
"The Semantics of Paragraphs', presented at Logicand Linguistics, Stanford, July 1987.10.
Maxwell, B.D and F. D. Tuggle 1975.
"Towarda Natural Language Question Answering Facility',Am.
J. Comp.
Ling., Microfiche 61.11.
Nilsson, Nils J.
1980.
Principles of Artificial In-telligence, Tioga Publishing Co.12.
Robinson, Jane J.
1982.
"DIAGRAM: A Gram-mar for Dialogues', Comm.
ACM Vol 25 No 1,27-4713.
Schubert, Klaus 1986.
"Linguistic and Extra-Linguistic Knowledge" Computers and TranslationVol 1, No 3, July-September 198614.
Schubert, Lenhart K. 1986.
"Are There PreferenceTrade-oirs in Attachment Decisions', Proc.AAAI-86, 601-60515.
Tomita, Masaru 1984.
"Disambiguating Gram-matically Ambiguous Sentences by Asking", Proc.COLING 8416.
Tomita, Masaru 1985.
"An Emcient Context-FreeParsing Algorithm for Natural Languages', Proc.IJCAI-85,756-76317.
Walker, Donald E. and William 11.
Paxton withGary G. Ilendrix, Ann E. Robinson, Jane J. Rob-inson, Jonathan Slocum 1977.
"Procedures forIntegrating Knowledge in a Speech UnderstandingSystem", SRI Technical Note 143.18.
Waltz, David L. and J.
B. Pollack 1985.
"Mas-sively Parallel Parsing: A Strongly InteractiveModel of Natural I.anguage Interpretation', Cog-nitive Science Vol 9,No I, January-March 1985,51-7419.
Wilks, Yorick, Xiuming lluang, and Dan Fass1985.
"Syntax, Preference and Right Attachment',Proc.
IJCAI-85 779-79420.
Wilks, Yorick 1975.
"An Intelligent Analyzer andUnderstander of English', Comm.
ACM, Vol 18No 5, 264-27421.
Wittenburg, Kent 1986.
"A Parser for PortableNi.
Interfaces Using Graph-Unification-BasedGrammars", Proc.
AAAI-86, 1053-10589.
Appendix: Search AlgorithmFigure 6 describes Ihe step by step application of A*to searching semantic hoices.251Assume an "open l i s t "  conta in ing ,  fo r  each  node n w i th  an unexaminedch i ld ,  the  fo l low ing  in fo rmat ion :1.
The l i s t  o f  choices made on the path  up to  and inc lud ing  n.2.
The set  o f  const ra in ts  on word senses imposed by nodes on the path .3.
The index o f  the h ighest  we ighted  unexamined ch i ld(cho ice  at  next  leve l )  o f  n,  w l~re  choices w i th in  leve ls  are sor ted  bydescending we ight .~.
The potent ia l  Fc = An + Hc + Hc fo r  that  ch i ld ,  ~ere  An is  theaccumulated weight  on the  path up to  and i nc lud ing  n, Nc i s  theNeight  of  the ch i ld ,  and Hc i s  the upper bound on the cumulat ivepotent ia l  fop paths below the ch i ld .Then the fo l low ing  a lgor i thm is  used to  search the t ree .1.
Put an ent ry  fo r  the dummy " top"  node in  the open l i s t ,w i th  path=se l f ,  index o f  f i r s t  ch i ld ,  and i t s  potent ia l  Fc.2.
Find the node n in  the "open l i s t "  Nhich has the h ighestpotent ia l  Fc fo r  a ch i ld  node.3.
I f  a fu l l  path has a l ready  been found, and Fc is  lower  than theto ta l  ~leight fo r  that  path ,  the search is  over .4.
Otherwise check the cons is tency  of  the des ignated  ch i ld  inent ry  n. I f  i t  i s  cons is tent ,  add an open l i s t  ent ry  fo r  the ch i ld ,i nc lud ing  a ne~, more const ra ined  cons is tency  requ i rement .~;.
Nhether o r  not  the des ignated ch i ld  i s  cons is tent ,  determinei f  there  are any unexamined ch i ld ren  of  node n. I f  so,  modifyent ry  n accord ing ly .
Otherwise remove ent ry  n from the open l i s t .6.
I f  there  i s  a new ent ry ,  and i t  represents  a completed path ,remove i t  from the open l i s t  and per form add i t iona l  cons is tencychecks.
I f  the checks fa i l ,  ignore  the new path .
I f  they succeed,record  the path and i t s  score as a competing a l te rnat ive .7.
Return to  Step Z.Figure 6: Search Algorithm252
