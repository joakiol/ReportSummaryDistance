Exploiting Unlabeled Text to Extract New Words of DifferentSemantic Transparency for Chinese Word SegmentationRichard Tzong-Han Tsai??
and Hsi-Chuan Hung?
?Department of Computer Science and Engineering,Yuan Ze University, Chung-Li, Taoyuan, Taiwan?Department of Computer Science and Information Engineering,National Taiwan University, Taipei, Taiwanthtsai@saturn.yzu.edu.tw yabthung@gmail.com?corresponding authorAbstractThis paper exploits unlabeled text datato improve new word identification andChinese word segmentation performance.Our contributions are twofold.
First,for new words that lack semantic trans-parency, such as person, location, ortransliteration names, we calculate as-sociation metrics of adjacent charactersegments on unlabeled data and encodethis information as features.
Second, weconstruct an internal dictionary by usingan initial model to extract words fromboth the unlabeled training and test setto maintain balanced coverage on thetraining and test set.
In comparisonto the baseline model which only usesn-gram features, our approach increasesnew word recall up to 6.0%.
Addition-ally, our approaches reduce segmenta-tion errors up to 32.3%.
Our systemachieves state-of-the-art performance forboth the closed and open tasks of the2006 SIGHAN bakeoff.1 IntroductionMany Asian languages do not delimit words byspaces.
Word segmentation is therefore a keystep for language processing tasks in these lan-guages.
Chinese word segmentation (CWS) sys-tems can be built by supervised learning from alabeled data set.
However, labeled data sets areexpensive to prepare as it involves manual an-notation efforts.
Therefore, exploiting unlabeleddata to improve CWS performance becomes animportant research goal.
In addition, new wordidentification (NWI) is also very important be-cause they represent the latest information, suchas new product names.This paper explores methods of extractinginformation from both internal and externalunlabeled data to augment NWI and CWS.According to (Tseng and Chen, 2002), newwords can be divided into two major cate-gories: Words with high or low semantic trans-parency (ST), which describes the correlation ofsemantic meanings between a word and its mor-phemes.
We designed effective strategies towardthe identification of these two new word types.One is based on transductive learning and theother is based on association metrics.2 The Model2.1 FormulationWe convert the manually segmented words intotagged character sequences.
We tag each char-acter with either B, if it begins a word, or I, ifit is inside or at the end of a word.2.2 Conditional Random FieldsCRFs are undirected graphical models trainedto maximize a conditional probability (Laffertyet al, 2001).
A linear-chain CRF with parame-ters ?
= ?1, ?2, .
.
.
defines a conditional proba-bility for a state sequence y = y1 .
.
.
yT given aninput sequence x = x1.
.
.
xT to beP?
(y|x) =1fxexp( T?t=1?k?kfk(yt?1, yt, x, t))where Zx is the normalization that makes theprobability of all state sequences sum to one;fk(yt?1, yt, x, t) is often a binary-valued featurefunction and ?k is its weight.
The feature func-tions can measure any aspect of a state transi-tion, yt?1 ?
yt, and the entire observation se-quence, x, centered at the current position, t.For example, one feature function might havevalue 1 when yt?1 is the state B, yt is the stateI, and is the character ?
?.
Large positive val-ues for ?k indicate a preference for such an event;large negative values make the event unlikely.In our CRF model, each binary feature is mul-tiplied with all states (yt) or all state transitions(yt?1yt).
For simplicity, we omit them in the fol-lowing discussion.
In addition, we use C0 ratherthan xt to denote the current character.9313 Baseline n-gram FeaturesCharacter n-gram features have proven their ef-fectiveness in ML-based CWS (Xue and Shen,2003).
We use 4 types of unigram feature func-tions: C0, C1 (next character), C?1 (previouscharacter), C?2 (character preceding C?1).
Fur-thermore, 6 types of bigram features are used,and are designated here as conjunctions of thepreviously specified unigram features, C?2C?1,C?1C0, C0C1, C?3C?1, C?2C0, and C?1C1.4 New Word IdentificationWe mainly focus on improving new word iden-tification (NWI) using unlabeled text.
Wordswith high and low ST are discussed separatelydue to the disparity in their morphological char-acteristics.
However, it is unnecessary for oursystem to classify words as high- or low-ST be-cause our strategies for dealing with these twoclasses are employed synchronously.4.1 High-ST wordsFor a high-ST word, its meaning can be easilyderived from those of its morphemes.
A word?ssemantic meaning correlates to its tendency ofbeing affixed to longer words.
This behaviorcan be recorded by the baseline n-gram model.When the baseline model is used to segment asentence containing a high-ST word, since thistendency is consistent with that is recorded inthe baseline model, this word tends to be suc-cessfully segmented.
For example, supposeW?
zhi-nan-che (compass chariot) is in the train-ing set.
The baseline n-gram model will recordthe tendency of W zhi-nan (guide) that ittends to be a prefix of a longer word.
Whentagging a sentence that contains another highST word also containing W, such as W?zhi-nan-zhen (compass), this word can be cor-rectly identified.Using only n-gram features may prevent someoccurrences of high-ST words from being iden-tified due to the ambiguity of neighboring n-grams.
To rectify this problem, we introducethe transductive dictionary (TD) feature, whichis similar to the traditional dictionary featurethat indicates if a sequence of characters in asentence matches a word w in an existing dic-tionary.
The difference is that the TD not onlycomprises words in the training set, but con-tains words extracted from the unlabeled testset.
The transductive dictionary is so named be-cause it is generated following general conceptsof transductive learning.
We believe adding TDfeatures can boost recall of high-ST words.
Moredetails on the TD are found in Section 5.
TheTD features that identify high-ST words are de-tailed in Section 6.1.4.2 Low-ST wordsOn the contrary, new words lack of ST, suchas transliteration names, are more likely tobe missed by the baseline n-gram model, be-cause their morphemes?
morphological tenden-cies are not guaranteed to be consistent withthose recorded by n-gram features.
For instance,suppose )s tian-ping (libra) only appears asindividual words in the training set.
The base-line model cannot identify ?
)s xiong-tian-ping (a singer?s name) because ?
)s is a low-ST word and the morphological tendency of )s is not consistent with the recorded one.In English, there is a similar phenomenoncalled multi-word expressions (MWEs).
(Choueka, 1988) regarded MWE as connectedcollocations: a sequence of neighboring words?whose exact meaning cannot be derived fromthe meaning or connotation of its components?,which means that MWEs also have low ST.As some pioneers provide MWE identificationmethods which are based on association metrics(AM), such as likelihood ratio (Dunning, 1993).The methods of identifying low-ST words canbe divided into two: filtering and merging.
Theformer uses AM to measure the likelihood that acandidate is actually a whole word that cannotbe divided.
Candidates with AMs lower thanthe threshold are filtered out.
The latter strat-egy merges character segments in a bottom-upfashion.
AMs are employed to suggest the nextcandidates for merging.
Both methods sufferfrom two main drawbacks of AM: dependencyon segment length and inability to use relationalinformation between context and tags.
In thefirst case, applying AMs to ranking charactersegment pairs, it is difficult to normalize the val-ues calculated from pairs of character segmentsof various lengths.
Secondly, AMs ignore the re-lationships among n-grams (or other contextualinformation) and labels, which are abundant inannotated corpora, and they only use annota-tion data to determine thresholds.
In Section6.2, we illustrate how encoding AMs as featurescan avoid the above weaknesses.5 Balanced Transductive DictionaryThe simplest TD is composed of words in thetraining set and words extracted from the unla-beled test set.
The main problem of such TDis the disparity in training and test set cov-erage.
During training, since its coverage is100%, the enabled dictionary features will be as-signed very high weights while n-gram features932will be assigned low weights.
During testing,when coverage is approximately 80-90%, mosttags are decided by dictionary features enabledby IV words, while n-gram features have lit-tle influence.
As a result, it is likely that onlyIV words are correctly segmented, while OOVwords are over-segmented.
Loosely speaking, adictionary?s coverage of the training set is linkedto the degree of reliance placed by the CRFmodel on the corresponding dictionary features.Therefore, the dictionary should be made morebalanced in order to avoid the potential problemof overfitting.
Here a dictionary is said to bemore balanced if its coverage of the training setapproximates its coverage of the test set whilemaximizing the latter.
Afterward we name theTD composed of words from gold training setand tagged test set and as Na?
?ve TD (NTD) forits unbalanced coverage in training and test set.Our TD is constructed as follows.
Given ini-tial features, we use the model trained on thewhole training set with these features to labelthe test set and add all words into our TD.The next step is to balance our TD?s cover-age of the training and test sets.
Since coverageof the test set cannot reach 100%, the only wayto achieve this goal is by slightly lowering thedictionary?s coverage on the training set.
Weapply n-fold cross-tagging to label the trainingset data: Each fold that has 1/n of the train-ing set is tagged by the model trained on theother n ?
1 folds with initial features.
All thewords identified by this cross-tagging process arethen added to our TD.
The difference betweenthe NTD and our TD is that the NTD extractswords from the gold training set, but our TD ex-tracts words from the cross-tagged training set.Finally, our TD is used to generate dictionaryfeatures to train the final model.
Since the TDconstructed from cross-tagging training set andtagged test set exists more balanced coverageof the training and test set, we call such a TD?balanced TD?, shorted as BTD.6 Our NWI Features6.1 Transductive Dictionary FeaturesIf a sequence of characters in a sentence matchesa word w in an existing dictionary, it may indi-cate that the sequence of characters should besegmented as one word.
The traditional wayis to encode this information as binary wordmatch features.
To distinguish the matches withthe same position and length, we propose a newword match feature that contains frequency in-formation to replace the original binary wordmatch feature.
Since over 90% of words are fouror fewer characters in length, we only considerwords of one to four characters.
In the followingsections, we use D to denote the dictionary.6.1.1 Word Match Features (WM)This feature indicates if there is a sequence ofneighboring characters around C0 that match aword inD.
Features of this type are identified bytheir positions relative to C0 and their lengths.Word match features are defined as:WM(w = C?pos .
.
.
C?pos+len?1)={1 if w ?
D0 otherwisewhere len ?
[1..4] is w?s length and pos ?
[0..len]is C0?s zero-based relative position in w (whenpos = len, the previous len characters form aword found in D).
If C0 is ??
and ???is found in D, WM(C?2 .
.
.
C0) is enabled.6.1.2 Word Match with WordFrequency (WMWF)Given two different words that have the sameposition and length, WM features cannot dif-ferentiate which should have the greater weight.This could cause problems when two matchedwords of same length overlap.
(Chen and Bai,1998) solved this conflict by selecting the wordwith higher (frequency ?
length).
We utilizethis idea to reform the WM features into ourWMWF features:WMWFq(w = C?pos .
.
.
C?pos+len?1)={1 if w ?
D and log feq(w) = q0 otherwisewhere the word frequency is discretized into10 bins in a logarithmic scale:log feq(w) =min(dlog2 w?s frequency + 1e, 10)thus q[0..10] is the discretized log frequencyof w. In this formulation, matching words withhigher log frequencies are more likely to be thecorrect segmentation.
Following the above ex-ample, if the frequency of ???
is 15, thenthe feature WMWF4(C?2 .
.
.
C0) is enabled.6.1.3 Discretization v.s.
Zipf?s LawSince current implementations of CRF modelsonly allow discrete features, the word frequencymust be discretized.
There are two commonlyused discretization methods: equal-width inter-val and equal-frequency interval, where the lat-ter is shown to be more suitable for data fol-lowing highly skewed distribution (Ismail andCiesielski, 2003).
The word frequency distribu-tion is the case: Zipf?s law (Zipf, 1949) statesthat the word frequency is inversely proportionalto its rank (Adamic and Huberman, 2002):f(x) ?
z?
?933where f(x) is x?s frequency, z is its rank in thefrequency table, and ?
is empirically found to beclose to unity.
Obviously this distribution is farfrom flat uniform.
Hence the equal-frequencybinning turns out to be our choice.Ideally, we would like each bin to have equalexpected number of values rather than followingempirical distribution.
Therefore, we attemptto discretize according to their underlying Zip-fian distribution.Adamic & Huberman (2002) shows that Zipf?slaw is equivalent to the power law, which de-scribes Zipf?s law in a unranked form:fX(x) ?
x?(1+(1/?
)),where X is the random variable denoting theword frequency and fX(x) is its probability den-sity function.
Approximated by integration, theexpected number of values in the bin [a, b] canbe calculated as?a?x?bx ?
Pr [X = x] ??
bax ?
fX(d)dx??
bax ?
x?(1+(1/?
))dx ?
lnx|ba = ln(b/a)(?
?
?
1)Thus each bin has equal number of values withinit if and only if b/a is a constant, which is in a logscale.
This shows that our strategy to discretizethe WMWF and WMNF features in a log scaleis not only a conventional heuristic but also hastheoretical support.6.2 Association Metric Features (AM)In this section, we describe how to formulatethe association metrics as features to avoid theweakness stated in Section 4.2.
Our idea is toenumerate all possible character segment pairsbefore and after the segmentation point andtreat their association metrics as feature values.Each possible pair corresponds to an individualfeature.
For computational feasibility, only pairswith total length shorter than five characters areselected.
All the enumerated segment pairs arelisted in the following table:Feature x,y Feature x,yAM1+1 c?1, c0 AM2+1 c?2c?1, c0AM1+2 c?1, c0c1 AM2+2 c?2c?1, c0c1AM1+3 c?1, c0c1c2 AM3+1 c?3c?2c?1, c0We use Dunning?s method (Dunning, 1993)because it does not depend on the assumption ofnormality and it allows comparisons to be madebetween the significance of the occurrences ofboth rare and common phenomenon.
The like-lihood ratio test is applied as follows:LR(x, y) =2?
(logl(p1, k1, n1) + logl(p2, k2, n2)?
logl(p, k1, n1)?
logl(p, k2, n2))where logl(P,K,M) = K?
lnP +(M ?K)?
ln(1?P ), k1 = freq(x, y); k2 = f(x,?y) = freq(x)?k1;n1 = freq(y); n2 = N ?
n1; p1 = p(x|y) = k1/n1;p2 = p(x|y) = k2/n2; p = p(x) = (k1 + k2)/N ;N is the number of words in corpus.An important property of likelihood ratio isthat ?2LR is asymptotically x21 distributed.Hence we can directly compute its p-value.
Wethen discretize the p-value into several bins, eachbin is defined by two significance levels 2?
(q+1)and 2?q.
Thus, our AM feature is defined as:AMq(x, y) =??
?1 if the p-value ofLR(x, y) ?
[2?
(q+1), 2?q]0 otherwiseSince we have a constraint 0 ?
q ?
10, thus,the last interval is [0, 2?10].
We can think thatlarger q implies higher tendency of current char-acter to be labeled as ?I ?.7 External Dictionary Features7.1 Word Match with NgramFrequency (WMNF)In addition to internal dictionaries extractedfrom the training and test data, external dic-tionaries can also be used.
Unlike with internaldictionaries, the true frequency of words in ex-ternal dictionaries cannot be acquired.
We musttreat each external dictionary word as an n-gramand calculate its frequency in the entire unseg-mented (training plus test) set as follows:WMNFq(w = C?pos .
.
.
C?pos+len?1)={1 if w ?
D log ngram freq(w) = q0 otherwisewhere the frequencies are discretized into 10 binsby the same way describing in previous section.In this formulation, matching n-grams withhigher log frequencies are more likely to repre-sent correct segmentations.8 Experiments and Results8.1 Data and Evaluation MetricsWe use two datasets in SIGHAN Bakeoff 2006:one Simplified Chinese provided by Univ.
ofPennsylvania (UPUC) and one Traditional Chi-nese provided by the City Univ.
of HK(CITYU), as shown in Table 1.Two unlabeled text data used in our exper-iments.
For the CITYU dataset, we use partof the CIRB40 corpus1 (134M).
For the UPUCdataset, we use the Contemporary Chinese Cor-pus at PKU2 (73M).1http://clqa.jpn.org/2006/04/corpus.html2http://icl.pku.edu.cn/icl_res/934UPUC CITYUF +/- Roov +/- Riv NC NCRR F +/- Roov +/- Riv NC NCRRclosed1 N-grams 93.0 n/a 71.1 n/a 95.7 14094 n/a 96.6 n/a 78.8 n/a 97.3 9642 n/a2 (1) + AM (int raw) 94.3 +1.3 76.4 +5.3 96.5 11655 +17.3 97.3 +0.7 80.3 +1.5 97.9 7890 +18.23 (1) + WM, NTD(1) 93.4 +0.4 74.8 +3.7 95.4 13182 +6.5 97.0 +0.4 81.6 +2.8 97.3 8597 +10.84 (1) + WMWF, NTD(1) 93.7 +0.7 75.0 +3.9 95.8 12719 +9.7 97.2 +0.6 82.0 +3.2 97.6 8029 +16.75 (1) + WMWF, BTD(1) 94.0 +1.0 73.4 +2.3 96.7 12218 +13.3 97.4 +0.8 79.2 +0.4 98.3 7429 +23.06 (1) + WMWF, BTD(2)+ AM (int raw)94.5 +1.5 76.6 +5.5 96.7 11173 +20.7 97.5 +0.9 80.3 +1.5 98.2 7377 +23.57 Rank 1 in Closed 93.3 n/a 70.7 n/a 96.3 n/a n/a 97.2 n/a 78.7 n/a 98.1 n/a n/aopen8 (1) + AM (ext raw) 94.3 +1.3 75.9 +4.8 96.6 11695 +17.0 97.3 +0.7 81.9 +3.1 97.9 7747 +19.79 (1) + WMWF, BTD(8)+ AM (ext raw)94.7 +1.7 77.1 +6.0 96.9 10844 +23.1 97.8 +1.2 82.2 +3.4 98.5 6531 +32.310 (9) + WMNF 95.0 +2.0 78.7 +7.6 97.1 10326 +26.7 97.9 +1.3 84.0 +5.2 98.5 6117 +36.611 Rank 1 in Open 94.4 n/a 76.8 n/a 96.6 n/a n/a 97.7 n/a 84.0 n/a 98.4 n/a n/aTable 2: Comparison scores for UPUC and CITYUSource Training(Wds/Types)Test(Wds/Types)UPUC 509K/37K 155K/17KCITYU 1.6M/76K 220K/23KTable 1: An overview of corpus statisticsWe use SIGHAN?s evaluation script to scoreall segmentation results.
This script pro-vides three basic metrics: Precision (P), Re-call (R), and F-Measure (F).
In addition, italso provides three detailed metrics: ROOVstands for the recall rate of the OOV words.RIV stands for the recall rate of the IVwords, and NC stands for NChanges (inser-tion+deletion+substitution) (Sproat and Emer-son, 2003).
In addition, we also compare theNChange reduction rate (NCRR) because theCWS?s state-of-the art F-measure is over 90%.Here, the NCRR of any system s is calculated:NCRR(s) = NChangebaseline ?NChangesNChangebaseline8.2 ResultsOur system uses the n-gram features describedin Section 3 as our baseline features, denotedas n-grams.
We then sequentially add other fea-tures and show the results in Table 2.
Each con-figuration is labeled with the features and theresources used in it.
For instance, AM(int raw)means AM features computed from the inter-nal raw data, including the unlabeled trainingand test set, and WM, NTD(1) stands for WMfeatures based on the NTD employing config.1?sfeature as its initial features.Our experiments are conducted in the follow-ing order: starting from baseline model, we thengradually add AM features (config.2) and TDfeatures (config.4 & 5) and combined them asour final setting (config.6) for the closed task.
Inthe open task, we sequentially add AM features(config.8), TD features (config.9), which only ex-ploit internal and unlabeled data.
Finally, thelast setting (config.10) employs external dictio-naries besides all above features.Association Metric At first, we comparethe effects after adding AM which is computedbased on the internal raw data (config.2).
Wecan see that adding AM can significantly im-prove the performance on both datasets.
Also,the OOV-recall is improved 5.3% and 1.5% onUPUC and CITYU respectively.Transductive Dictionary Without lost ofgenerality, we firstly use the WM features in-troduced in Section 6.1.1 to represent dictio-nary features which is denoted as config.
3 inTables 3.
We can see that the configurationwith WM features outperforms that with N-grams (config.1).
It is worth mentioning thateven though N-grams achieve satisfactory OOVrecall (0.788 and 0.711) in CITYU and UPUC,config.
3 achieves higher OOV recall.Frequency Information and BTD To showthe effectiveness of frequency, we compare WMwith WMWF features.
In Table 2, we cansee that WMWF features (config.4) outperformWM features (config.3) on both datasets interms of F-Measure and RIV.
In addition,switching the NTD (config.4) with BTD (con-fig.5) can further improve RIV and F-score whileROOV slightly decreases.
This is not surpris-ing.
In a BTD, most incorrectly segmentedwords appear infrequently.
Unfortunately, thenew words detected by the baseline model alsohave comparatively low frequencies.
Therefore,these words will be assigned into the same sev-eral bins corresponding to infrequent words asthe incorrectly segmented words and share lowweights with them.Combined Effects In config.6, we use themodel with N-gram plus AM features as initialfeatures to construct the BTD.
In Table 2, wecan see that the increase of ROOV?s can recoverthe loss brought by using BTD and further raise935the F-measure to the level of the state-of-the-artopen task performance.In comparison of the baseline n-gram model,our approach reduces the errors by an significantnumber of 20.7% and 23.5% in the UPUC andCITYU datasets, respectively.
The OOV recallof our approach increases 5.5% and 1.5% on theUPUC and CITYU datasets, respectively.
As-tonishingly, in the UPUC dataset, with limitedinformation provided by training corpus and un-labeled test data, our system still outperformsthe best SIGHAN open CWS system that areallowed to use unlimited external resources.8.2.1 Using External Unlabeled DataIn config.9, we also use the ngrams plus AM asinitial features to generate the BTD, but exter-nal unlabeled data are used along with internaldata to calculate values of AM features.
Com-paring with config.6, we can see that ROOV,RIV, and F-score are further improved, espe-cially ROOV.
Notably, this configuration canreduce NChanges by 2.4% in comparison of thebest closed configuration.8.2.2 Using External DictionariesTo demonstrate that our approach can beexpandable by installing external dictionaries,we add WMNF features based on the externaldictionaries into the config.9, and denote thisto be our config.10.
We use the Grammati-cal Knowledge-Base of Contemporary Chinese(GKBCC) (Yu et al, 2003) and Chinese Elec-tronic Dictionary for the UPUC and CITYUdataset, respectively.As shown in Table 2, all metrics of config.10are better than config.9, especially ROOV.
Thisis because most of the new words do not exist inexternal dictionaries; therefore, using externaldictionaries can complement our results.9 ConclusionThis paper presents how to exploit unlabeleddata to improve both NWI and CWS perfor-mance.
For new high-ST words, since theycan be decomposed into semantically relevantatomic parts, they could be identified by the n-gram models.
Using the property, we constructan internal dictionary by using this model toextract words from both the unlabeled trainingand test set to maintain balanced coverage onthem, which makes the weights of the internaldictionary features more accurate.
Also, fre-quency is initiatively considered in dictionaryfeatures and shows its effectiveness.For low-ST words, we employ AMs, whichis frequently used in English MWE extractionto enhance the baseline n-gram model.
Weshow that this idea effectively extract muchmore unknown person, location, and transliter-ation names which are not found by the baselinemodel.The experiment results demonstrate thatadopting our two strategies generally benefi-cial to NWI and CWS on both traditionaland simplified Chinese datasets.
Our sys-tem achieves state-of-the-art closed task perfor-mance on SIGHAN bakeoff 2006 datasets.
Un-der such most stringent constraints defined inthe closed task, our performances are even com-parable to open task performance.
Moreover,with only external unlabeled data, our systemalso achieves state-of-the-art open task perfor-mance on SIGHAN bakeoff 2006 datasets.ReferencesL.A.
Adamic and B.A.
Huberman.
2002.
Zipf?s lawand the internet.
Glottometrics, 3:143?150.K.
J. Chen and M. H. Bai.
1998.
Unknown worddetection for chinese by a corpus-based learningmethod.
Computational Linguistics and ChineseLanguage Processing, 3(1):27?44.Y.
Choueka.
1988.
Looking for needles in a haystackor locating interesting collocation expressions inlarge textual databases.
In RIAO.T.
Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):65?74.Michael K. Ismail and Vic Ciesielski.
2003.
An em-pirical investigation of the impact of discretizationon common data distributions.
In Design and Ap-plication of Hybrid Intelligent Systems.
IOS Press,Amsterdam, Netherlands.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In ICML-01, pages 282?289.Richard Sproat and Thomas Emerson.
2003.The first international chinese word segmentationbakeoff.
In SIGHAN-03.Huihsin Tseng and Keh-Jiann Chen.
2002.
Design ofchinese morphological analyzer.
In SIGHAN-02.Nianwen Xue and Libin Shen.
2003.
Chinese wordsegmentation as lmr tagging.
In SIGHAN-03.G.K.
Zipf.
1949.
Human Behavior and the Principleof Least Effort.
Addison-Wesley, Cambridge, MA.936
