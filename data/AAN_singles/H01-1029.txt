Fine-Grained Hidden Markov Modeling for Broadcast-News Story SegmentationWarren Greiff, Alex Morgan, Randall Fish, Marc Richards, Amlan Kundu,MITRE Corporation202 Burlington RoadBedford, MA 01730-1420(greiff, amorgan, fishr, marc, akundu)@mitre.orgABSTRACTWe present the design and development of a Hidden MarkovModel for the division of news broadcasts into story segments.Model topology, and the textual features used, are discussed,together with the non-parametric estimation techniques that wereemployed for obtaining estimates for both transition andobservation probabilities.
Visualization methods developed forthe analysis of system performance are also presented.1.
INTRODUCTIONCurrent technology makes the automated capture, storage,indexing, and categorization of broadcast news feasible allowingfor the development of computational systems that provide for theintelligent browsing and retrieval of news stories [Maybury,Merlino & Morey ?97; Kubula, et al, ?00].
To be effective, suchsystems must be able to partition the undifferentiated input signalinto the appropriate sequence of news-story segments.In this paper we discuss an approach to segmentation based on theuse of a fine-grained Hidden Markov Model [Rabiner, `89] tomodel the generation of the words produced during a newsprogram.
We present the model topology, and the textual featuresused.
Critical to this approach is the application of non-parametricestimation techniques, employed to obtain robust estimates forboth transition and observation probabilities.
Visualizationmethods developed for the analysis of system performance arealso presented.Typically, approaches to news-story segmentation have beenbased on extracting features of the input stream that are likely tobe different at boundaries between stories from what is observedwithin the span of individual stories.
In [Beeferman, Berger, &Lafferty ?99], boundary decisions are based on how wellpredictions made by a long-range exponential language modelcompare to those made by a short range trigram model.
[Ponte andCroft, ?97] utilize Local Context Analysis [Xu, J. and Croft, ?96]to enrich each sentence with related words, and then use dynamicprogramming to find an optimal boundary sequence based on ameasure of word-occurrence similarity between pairs of enrichedsentences.
In [Greiff, Hurwitz & Merlino, `99], a na?ve Bayesclassifier is used to make a boundary decision at each word of thetranscript.
In [Yamron, et al, ?98], a fully connected HiddenMarkov Model is based on automatically induced topic clusters,with one node for each topic.
Observation probabilities for eachnode are estimated using smoothed unigram statistics.The approach reported in this paper goes further along the lines offind-grained modeling in two respects: 1) differences in featurepatterns likely to be observed at different points in thedevelopment of a news story are exploited, in contrast toapproaches that focus on boudary/no-boundary differences; and 2)a more detailed modeling of the story-length distribution profile,unique to each news source (for example, see the histogram ofstory lengths for ABC World News Tonight shown in the topgraph of Figure 3, below).2.
GENERATIVE MODELWe model the generation of news stories as a 251 state HiddenMarkov Model, with the topology shown in Figure 1.
Stateslabeled, 1 to 250, correspond to each of the first 250 words of astory.
One extra state, labeled 251, is included to model theproduction of all words at the end of stories exceeding 250 wordsin length.Several other models were considered, but this model isparticularly suited to the features used, as it allows one to modelfeatures that vary with depth into the story (Section 3.1), whilesimultaneously, by delaying certain features.
It also allows one tomodel features that occur in specific regions the boundaries(Section 3.3).
This is possible because all states can feed into theinitial state, i.e.
all stories end by going into the first word of anew story.1 2 3 250 251Figure 1:  Current HMM TopologyFor example, the original model involved a series of beginningand then end states, with a single middle state that could be cycledthrough (Figure 2).
This proved to be a problem because the endsof long stories were being mixed with the ends of short storieswhich led to problems with our spaced coherence feature (Section3.1).
Another possibility involved splitting the model into twomain paths, one to model the shorter stories, and one to model thelonger as there is something of a bimodal distribution in storylengths (Figure 4).
However, the fine-grained nature of our modelwould suffer from splitting the data in this manner, and a choiceabout at which length to fork the model would be somewhatartificial.3.
FEATURESAssociated with the model is a set of features.
For each state, themodel assigns a probability distribution over all possiblecombinations of values the features may take on.
The probabilityassigned to value combinations is assumed to be independent ofthe state/observation history, conditioned on the state.
We furtherassume that the value of any one feature is independent of allothers, once the current state is known.
Features have beenexplicitly designed with this assumption in mind.
Threecategories of features have been used, which we refer to ascoherence features, x-duration feature, and the trigger features.3WshwdoapofsttrrafeCOHER-4  (Figures 3b, c & d) correspond to similar features; forthese, however, the buffer is separated by 50, 100, and 150 words,respectively, from the current word.
Interestingly, the COHER-4feature actually caused a reduction in performance, and was notused in the final evaluation.3.2.
X-durationThis feature is based on indications given by the speech recognizerthat it was unable to transcribe a portion of the audio signal.
Theexistence of an untranscribable section prior to the word gives anon-zero X-DURATION value based on the extent of the section.Empirically this is an excellent predictor of boundaries in that anuntranscribable event has uniform likelihood of occurringanywhere in a news story, except prior to the first word of a story,where it is extremely likely to occur.3.3.
TriggersTrigger features correspond to small regions at the beginning andend of stories, and exploit the fact that some words are far morelikely to occur in these positions than in other parts of a newssegment.
One region, for example, is restricted to the first word ofthe story.
In ABC?s World News Tonight, for example, the word?finally?
is far more likely to occur in the first word of a story thanwould be expected by its general rate of occurrence in the trainingdata.
For a word, w, appearing in the input stream, the value ofthe feature is an estimate of how likely it is for w to appear in theregion of interest.
The estimate used is given by:( )RwRwfnnRwp /11)(?++=?
?where Rwn ?
is the number of times w appeared in R in the trainingdata; wn  is the total number of occurrences of w; and Rf  is thefraction of all tokens of w that occurred in the region.
Thisestimate can be viewed as Bayesian estimate with a beta prior.The beta prior is equivalent to a uniform prior and the observationof one occurrence of the word in the region out of ( )Rf/1  totaloccurrences.
This estimate was chosen so that: 1) the priorprobability would not be greatly affected for words observed onlya few times in the training data; 2) it would be pushed stronglytowards the empirical probability of the word appearing in theregion for words that were encountered in R; 3) it has a priorprobability, Rf , equal to the expectation for a randomly selectedword.
The regions used for the submission were restricted to theone-word regions for: first word, second word, last word, and1 2 500 501Figure 3: Original Topology.1.
Coherenceabce have used four coherence features.
The COHER-1 feature,own schematically in Figure 2a, is based on a buffer of 50ords immediately prior to the current word.
If the current wordes not appear in the buffer, the value of COHER-1 is 0.
If it doespear in the buffer, the value is -log(sw/s), where sw is the numberstories in which the word appears, and s is the total number ofories, in the training data.
Words that did not appear in theaining data, are treated as having appeared once.
In this way,re words get high feature values, and common words get lowature values.
Three other features: COHER-2, COHER-3, andnext-to-last word.
Limited experimentation with multi-stateregions, was not fruitful.
For example, including the regions,{3,4,?,10} and {-10,-9,?,-3}, where ?i is interpreted as i wordsprior to the end of the story, did not improve segmentationperformance.Since, as described, the current HMM topology does not modelend-of-story words (earlier versions of the topology did modelthese states directly), trigger features for end-of-story regions aredelayed.
That means that a trigger related to the last word in astory would be delayed by a one word buffer.
In this way, it islinked to the first word in the next story.
For example, the word?Jennings?
(the name of the main anchorperson) is stronglydFigure 2: Coherence Featurescorrelated with the last word in news stories in the ABC WorldNews Tonight corpus.
The estimated probability of it being thelast word of the story in which it appears is .235 (obtained by theaforementioned method).
The trained model associates a highlikelihood of seeing the value .235 at state = 1; the intuitiveinterpretation being, "a word highly likely to appear at the lastword of a story, occurred 1-word ago".4.
PARAMETER ESTIMATIONThe Hidden Markov Model requires the estimation of transitionand conditional observation probabilities.
There are 251 transitionprobabilities to be estimated.
Much more of a problem are theobservation probabilities, there being 9 features in the model, foreach of which a probability distribution over as many as 100values must be estimated, for each of 251 states.
With the goal ofdeveloping methods for robust estimation in the context of storysegmentation, we have applied non-parametric kernel estimationtechniques, using the LOCFIT library [Loader, ?99] of the R open-source statistical analysis package, which is based on the S-plussystem [Venables & Ripley,`99; Chambers & Hastie, `92, Becker, Chambers & Wilks, `88].For the transition probabilities, it is assumed that the underlyingprobability distribution over story length is smooth, allowing theempirical histogram, shown at the top of Figure 4, to betransformed to the probability density estimate shown at thebottom.
From this probability distribution over story lengths, theconditional transition probabilities can be estimated directly.Conditional observation probabilities are also deduced from anestimate of the joint probability distribution.
First, observationvalues were binned.
Binning limits were set in an attempt to 1) belarge enough to obtain sufficient counts for the production ofrobust probability estimates, and yet, 2) be constrained enough sothat important distinctions in the probabilities for different featurevalues will be reflected in the model.
For each bin, theobservation counts are smoothed by performing a non-parametricregression of the observation counts as a function of state.
Thesmoothed observations counts corresponding to the regression arethen normalized so as to sum to the total observation count for thebin.
The result is a conditional probability distribution over statesfor a given binned feature value,  p(State=s|Feature=fv).
Oncethis is done for all bin values, each conditional probability ismultiplied by the marginal probability, p(State=s), of being in agiven state, resulting in a joint distribution, p(fv,s), over the entirespace of (Feature,State) values.
From this joint distribution, thenecessary conditional probabilities, p(Feature=fv|State=s), can bededuced directly.Figure 5 shows the conditional probability estimates, p(fv | s), forthe feature value COHER-3=20, across all states, confirming theintuition that, while the probability of seeing a value of 20 is smallfor all states, the likelihood of seeing it is much higher in latterparts of a story than it is in early-story states.5.
SEGMENTATIONOnce parameters for the HMM have been determined,segmentation is straightforward.
The Viterbi algorithm [Rabiner,`89], is employed to determine the sequence of states most likelyto have produced the observation sequence associated with thebroadcast.
A boundary is then associated with each wordproduced from State 1 for the maximum likelihood state sequence.The version of the Viterbi algorithm we have implementedprovides for the specification of ?state-penalty?
parameters, whichwe have used for the ?boundary state?, state 1.
In effect, theprobability for each path in consideration is multiplied by thevalue of this parameter (which can be less than, equal to, orgreater than, 1) for each time the path passes through the boundarystate.
Variation of the parameter effectively controls the?aggressiveness?
of segmentation, allowing for tuning systembehavior in the context of the evaluation metric.6.
RESULTSPreliminary test results of this approach are encouraging.
Aftertraining on all but 15 of the ABC World News Tonight programsfrom the TDT-2 corpus [Nist, ?00], a test on the remaining 15produced a false-alarm (boundary predicted incorrectly)probability of .11, with a corresponding miss (true boundary notpredicted) probability of .14, equal to the best performancereported to date, for this news source.A more intuitive appreciation for the quality of performance canbe garnered from the graphs in Figure 6, which contrast thesegmentation produced by the system (middle) with ground truth(the top graph), for a typical member of the ABC test set.
The x-axis corresponds to time (in units of word tokens); i.e., the indexof the word produced by the speech recognizer, and the y-axisFigure 4: Histograms of story lengths (up to 250 words)-- raw and smoothed --Figure 5: Likelihood of COHER-3=2 over all statescorresponds to the state of the HMM model.
A path passingthrough the point (301, 65), for example, corresponds to a paththrough the network that produced the 65th word from state 301.Returns to state=1 correspond to boundaries between stories.
Thebottom graph shows the superposition of the two to help illustratethe agreement between the path chosen by the system and the pathcorresponding to perfect segmentation..7.
VISUALIZATIONThe evolution of the segmentation algorithm was driven byanalysis of the behavior of the system, which was supported byvisualization routines developed using the graphing capability ofthe R package.
Figure 7 gives an example of the kind of graphicaldisplays that were used for analysis of the segmentation of aspecific broadcast news program; in this case, analysis of the roleof the X-DURATION feature.
This graphical display allows forthe comparison of the maximum likelihood path produced by theHMM to the path through the HMM that would be produced by aperfect system ?
one privy to ground-truth.TshgrcopohafrthhiloTligesyDtrVstXt gativep n thatr e truep deling.T systemp of theeFigure 6: Perfohe true state than from the predicted state.
Strongly neoints are a major component of the probability calculatioesulted in the system preferring the path it chose over thath.
These points suggest potential deficiencies in the moheir identification directs the focus of analysis so thaterformance can be improved by correcting weaknessesxisting model.rmanceheiesogfulutedisheithtalueceodhe top graph corresponds to the bottom graph of Figure 6,owing the states traversed by the two systems.
The secondaph shows the value of the X-DURATION featurerresponding to each word of the broadcast.
So, the plotting of aint at (301, 3) corresponds to an X-DURATION value of 3ving been observed at time, 301.
One thing that can be seenom this graph is that being at a story boundary (low-points one thicker-darker line of the top graph) is more frequent whengher values of the X-DURATION cue are observed, than whenwer values are observed, as could be expected.he third graph shows, on a log scale, how many times morekely it is that the observed X-DURATION value would benerated from the true state than from the state predicted by thestem.
Most points are close to 0, indicating that the X-URATION value observed was as likely to have come from theue state as it is to have come from the state predicted by theiterbi algorithm.
Of course, this is the case wherever the trueate has been correctly predicted.
Negative points indicate that the-DURATION value observed is less likely to be produced fromThe final graph shows the cumulative sum of the values from tgraph above it.
(Note that the sum of the logs of the probabilitis equivalent to the cumulative product of probabilities on a lscale.)
The graphing of the cumulative sum can be very usewhen the system is performing poorly due to a small bconsistent preference for the observations having been producby the state sequence chosen by the system.
This phenomenonmade evident by a steady downward trend in the graph of tcumulative sum.
This is in contrast to an overall level trend woccasional downward dips.
Note, that a similar graph for the toprobability (equal to the product of all the individual feature valprobabilities) will always have an overall downward trend, sinthe maximum likelihood path will always have a likelihoFigure 7: Visualization for x-duration featuregreater than the likelihood of any other path.Aside from supporting the detailed analysis of specific features,the productions of these graphs for each of the features, togetherwith the corresponding graph for the total observation probability,allowed us to quickly asses which of the features was mostproblematic at any given stage of model development.8.
FURTHER WORKIt should be kept in mind that experimentation with this approachhas been based on relatively primitive features ?
our focus, to thispoint, having been on the development of the core segmentationmechanism.
Features based on more sophisticated extractiontechniques, which have been reported in the literature ?
forexample, the use of exponential models for determining triggercues used in [Beeferman, Berger, & Lafferty ?99] ?
can easily beincorporated into this general framework.
Integration of suchtechniques can be expected to result in significant furtherimprovement in segmentation quality.To date, the binning method described has given much betterresults than two dimensional kernel density estimation techniqueswhich we also attempted to employ.
One of the main difficultieswith using traditional kernel density estimation techniques is thatthey tend to inaccurately estimate the density at areas ofdiscontinuity, such as state=1 in our model and our triggerfeatures.
Preliminary work with boundary kernels [Scott, ?92] isvery promising.
It is certainly an area worthy of more in-depthinvestigation.Work done by another group [Liu, ?00] to segment documentariesbased on video cues alone has been moderately successful in thepast.
We engineered a neural network in an attempt to identifyvideo frames containing an anchorperson, a logo, and blankframes, with a belief that these are all features that would containinformation about story boundaries.
Preliminary work was alsodone to extract features directly from the audio signal, such astrying to identify speaker change.
Initial work with the audio andvideo has been unable to aid in segmentation, but we feel this isalso an area worth continuing to pursue.9.
REFERENCES1.
[Becker, Chambers & Wilks, `88] Becker, Richard A.,Chambers, John M., and Wilks, Allan R.  The New SLanguage.
Wadsworth & Brooks/Cole, Pacific Grove, Cal.2.
[Beeferman, Berger, & Lafferty ?99] D. Beeferman, D., A.Berger, A. and Lafferty, J.
Statistical models for textsegmentation.
Machine Learning, vol.
34, pp.
1-34, 1999.3.
[Chambers & Hastie, `88] Chambers, John M. and Hastie,Trevor, J.
Statistical Models in S.  Wadsworth &Brooks/Cole, Pacific Grove, Cal., 1988.4.
[Greiff, Hurwitz & Merlino, `99] Greiff, Warren, Hurwitz,Laurie, and Merlino, Andrew.
MITRE TDT-3 segmentationsystem.
TDT-3 Topic Detection and Tracking Conference,Gathersburg, Md, February, 2000.5.
[Kubula, et al, ?00] Kubula, F., Colbath, S.,  Liu, D.,Srivastava, A. and Makhoul, J.
Integrated technologies forindexing spoken language, Communication of the ACM, vol.43, no.
2, Feb., 2000.6.
[Liu, ?00] Liu, Tiecheng and Kender, John R.  A hiddenMarkov model approach to the structure of documentaries.Proceedings of the IEEE Workshop on Content-basedAccess of Image and Video Libraries, 2000.7.
[Loader, `99] Loader, C.  Local Regression and Likelihood.Springer, Murray Hill, N.J., 1999.8.
[Maybury, Merlino & Morey ?97] Maybury, M., Merlino, A.Morey, D.  Broadcast news navigation using storysegments.
Proceedings of the ACM InternationalMultimedia Conference, Seattle, WA, Nov., 1997.9.
[Nist, ?00] Topic Detection and Tracking (TDT-3) EvaluationProject.
http://www.nist.gov/speech/tests/tdt/tdt99/.10.
[Ponte and Croft, ?97] Ponte, J.M.
and Croft, W.B.
Textsegmentation by topic, Proceedings of the First EuropeanConference on Research and Advanced Technology forDigital Libraries, pp.
120--129, 1997.11.
[Rabiner, `89] L. R. Rabiner, A tutorial on hidden Markovmodels and selected applications in speech recognition.Proceedings of the IEEE, vol.
37, no.
2, pp.
257-86,February, 1989.12.
[Scott, ?92] David W. Scorr, Boundary kernels, MultivariateDensity Estimation: Theory and Practice, pp 146-149, 1992.13.
[Venables & Ripley, `99]  Venables, W. N. and Ripley, B. D.Modern Applied Statistics with S-PLUS.
Springer, MurrayHill, N.J., 1999.14.
[Xu, J. and Croft, ?96] Xu, J. and Croft, W.B., Queryexpansion using local and global document analysis,Proceedings of the Nineteenth Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, pp.
4--11, 199615.
[Yamron, et al, ?98] Yamron, J. P., Carp, I., Gillick, L., Lowe,S.
and van Mulbregt, P.  A Hidden Markov Model approachto text segmentation and event tracking.
ProceedingsICASSP-98, Seattle, WA.
May, 1998.
