Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2063?2072,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsHidden Softmax Sequence Model for Dialogue Structure AnalysisZhiyang He1, Xien Liu2, Ping Lv2, Ji Wu11Department of Electronic Engineering, Tsinghua University, Beijing, China2Tsinghua-iFlytek Joint Laboratory for Speech Technology, Beijing, China{zyhe ts, xeliu, luping ts, wuji ee}@mail.tsinghua.edu.cnAbstractWe propose a new unsupervised learningmodel, hidden softmax sequence model(HSSM), based on Boltzmann machine fordialogue structure analysis.
The modelemploys three types of units in the hiddenlayer to discovery dialogue latent struc-tures: softmax units which represent latentstates of utterances; binary units whichrepresent latent topics specified by dia-logues; and a binary unit that representsthe global general topic shared across thewhole dialogue corpus.
In addition, themodel contains extra connections betweenadjacent hidden softmax units to formu-late the dependency between latent states.Two different kinds of real world dialoguecorpora, Twitter-Post and AirTicketBook-ing, are utilized for extensive comparingexperiments, and the results illustrate thatthe proposed model outperforms sate-of-the-art popular approaches.1 IntroductionDialogue structure analysis is an important andfundamental task in the natural language process-ing domain.
The technology provides essentialclues for solving real-world problems, such as pro-ducing dialogue summaries (Murray et al, 2006;Liu et al, 2010), controlling conversational agents(Wilks, 2006), and designing interactive dialoguesystems (Young, 2006; Allen et al, 2007) etc.The study of modeling dialogues always assumesthat for each dialogue there exists an unique latentstructure (namely dialogue structure), which con-sists of a series of latent states.11Also called dialogue acts or speech acts in some pastwork.
In this paper, for simplicity we will only use the term?latent state?
to describe the sequential dialogue structure.Some past works mainly rely on supervised orsemi-supervised learning, which always involveextensive human efforts to manually construct la-tent state inventory and to label training samples.Cohen et al (2004) developed an inventory of la-tent states specific to E-mail in an office domainby inspecting a large corpus of e-mail.
Jeong etal.
(2009) employed semi-supervised learning totransfer latent states from labeled speech corporato the Internet media and e-mail.
Involving exten-sive human efforts constrains scaling the trainingsample size (which is essential to supervised learn-ing) and application domains.In recent years, there has been some workon modeling dialogues with unsupervised learn-ing methods which operate only on unlabeled ob-served data.
Crook et al (2009) employed Dirich-let process mixture clustering models to recog-nize latent states for each utterance in dialoguesfrom a travel-planning domain, but they do notinspect dialogues?
sequential structure.
Choti-mongkol (2008) proposed a hidden Markov model(HMM) based dialogue analysis model to studystructures of task-oriented conversations from in-domain dialogue corpus.
More recently, Ritter etal.
(2010) extended the HMM based conversa-tion model by introducing additional word sourcesfor topic learning process.
Zhai et al (2014)assumed words in an utterance are emitted fromtopic models under HMM framework, and topicswere shared across all latent states.
All these dia-logue structure analysis models are directed gener-ative models, in which the HMMs, language mod-els and topic models are combined together.In this study, we attempt to develop a Boltz-mann machine based undirected generativemodel for dialogue structure analysis.
As forthe document modeling using undirected gener-ative model, Hinton and Salakhutdinov (2009)proposed a general framework, replicated soft-2063max model (RSM), for topic modeling basedon restricted Boltzmann machine (RBM).
Themodel focuses on the document-level topic anal-ysis, it cannot be applied for the structure analy-sis.
We propose a hidden softmax sequence model(HSSM) for the dialogue modeling and structureanalysis.
HSSM is a two-layer special Boltzmannmachine.
The visible layer contains softmax unitsused to model words in a dialogue, which are thesame with the visible layer in RSM (Hinton andSalakhutdinov, 2009).
However, the hidden layerhas completely different design.
There are threekinds of hidden units: softmax hidden units, whichis utilized for representing latent states of dia-logues; binary units used for representing dialoguespecific topics; and a special binary unit used forrepresenting the general topic of the dialogue cor-pus.
Moreover, unlike RSM whose hidden binaryunits are conditionally independent when visibleunits are given, HSSM has extra connections uti-lized to formulate the dependency between adja-cent softmax units in the hidden layer.
The con-nections are the latent states of two adjacent utter-ances.
Therefore, HSSM can be considered as aspecial Boltzmann machine.The remainder of this paper is organized as fol-lows.
Section 2 introduces two real world dia-logue corpora utilized in our experiments.
Section3 describes the proposed hidden softmax sequencemodel.
Experimental results and discussions arepresented in Section 4.
Finally, Section 5 presentsour conclusions.2 Data SetTwo different datasets are utilized to test the ef-fectiveness of our proposed model: a corpus ofpost conversations drawn from Twitter (Twitter-Post), and a corpus of task-oriented human-humandialogues in the airline ticket booking domain(AirTicketBooking).2.1 Twitter-PostConversations in Twitter are carried out by re-plying or responding to specific posts with short140-character messages.
The post length restric-tion makes Twitter keep more chat-like interac-tions than blog posts.
The style of writing usedon Twitter is widely varied, highly ungrammatical,and often with spelling errors.
For example, theterms ?be4?, ?b4?, and ?bef4?
are always appearedin the Twitter posts to represent the word ?before?.Here, we totally collected about 900, 000 rawTwitter dialogue sessions.
The majority of conver-sation sessions are very short; and the frequenciesof conversation session lengths follow a power lawrelationship as described in (Ritter et al, 2010).For simplicity , in the data preprocessing stagenon-English sentences were dropped; and non-English characters, punctuation marks, and somenon-meaning tokens (such as ?&?)
were also fil-tered from dialogues.
We filtered short Twitter di-alogue sessions and randomly sampled 5,000 di-alogues (the numbers of utterances in dialoguesrang from 5 to 25) to build the Twitter-Post dataset.2.2 AirTicketBookingThe AirTicketBooking corpus consists of a set oftask-oriented human-human mandarin dialoguesfrom an airline ticket booking service center.
Themanual transcripts of the speech dialogues are uti-lized in our experiments.
In the dataset, there isalways a relative clear structure underlying eachdialogue.
A dialogue often begins with a cus-tomer?s request about airline ticket issues.
Andthe service agent always firstly checks the client?spersonal information, such as name, phone num-ber and credit card numberm, etc.
Then the agentstarts to deal with the client?s request.
We totallycollected 1,890 text-based dialogue sessions ob-taining about 40,000 conversation utterances withlength ranging from 15 to 100.3 Dialogue Structure Analysis3.1 Model DesignFigure 1: Hidden layer that consists of differenttypes of latent variablesWe design an undirected generative modelbased on Boltzmann machine.
As we known, di-alogue structure analysis models are always basedon an underlying assumption: each utterance inthe dialogues is generated from one latent state,which has a causal effect on the words.
For in-stance, an utterance in AirTicketBooking dataset,?Tomorrow afternoon, about 3 o?clock?
corre-2064sponds to the latent state ?Time Information?.However, by carefully examining words in dia-logues we can observe that not all words are gener-ated from the latent states (Ritter et al, 2010; Zhaiand Williams, 2014).
There are some words rele-vant to a global or background topic shared acrossdialogues.
For example, ?about?
and ?that?
be-long to a global (general English) topic.
Someother words in a dialogue may be strongly re-lated to the dialogue specific topic.
For exam-ple, ?cake?, ?toast?
and ?pizza?
may appear in aTwitter dialogue with respect to a specific topic,?food?.
From the perspective of generative model,we can also consider that words in a dialogue aregenerated by the mixture model of latent states, aglobal/background topic, and a dialogue specifictopic.
Therefore, there are three kinds of unitsin the hidden layer of our proposed model, whichare displayed in Figure 1. h?is a softmax unit,which indicates the latent state for a utterance.
h?and h?represent the general topic, and the dia-logue specific topic, respectively.
For the visiblelayer, we utilize the softmax units to model wordsin each utterance, which is the same with the ap-proach in RSM (Hinton and Salakhutdinov, 2009).In Section 3.2, We propose a basic model based onBoltzmann machine to formulate each word in ut-terances of dialogues.A dialogue can be abstractly viewed as a se-quence of latent states in a certain reasonable or-der.
Therefore, formulating the dependency be-tween latent states is another import issue for dia-logue structure analysis.
In our model, we assumethat each utterance?s latent state is dependent onits two neighbours.
So there exist connections be-tween each pair of adjacent hidden softmax unitsin the hidden layer.
The details of the model willbe presented in Section 3.3.3.2 HSM: Hidden Softmax ModelNotation ExplanationK dictionary sizeJ number of latent statesV observed visibles representing words in dialoguesb bias term of Vh?latent variables representing latent statesh?latent variable representing corpus general topich?latent variables representing dialogue specific topicsa?bias terms of h?a?bias term of h?a?bias terms of h?W?weights connecting h?to VW?weights connecting h?to VW?weights connecting h?to VF, Fs, Feweights between hidden softmax unitsTable 1: Definition of notations.Words of utterance 1... ...
...Words of utterance 2 Words of utterance 3Utterance 1 Utterance 2 Utterance 3Figure 2: Hidden Softmax Model.
The bottomlayer are softmax visible units and the top layerconsists of three types of hidden units: softmaxhidden units used for representing latent states, abinary stochastic hidden unit used for represent-ing the dialogue specific topic, and a special bi-nary stochastic hidden unit used for representingcorpus general topic.
Upper: The model for a di-alogue session containing three utterances.
Con-nection lines in the same color related to a latentstate represent the same weight matrix.
Lower:A different interpretation of the Hidden SoftmaxModel, in which Drvisible softmax units in therthutterance are replaced by one single multino-mial unit which is sampled Drtimes.Table 1 summarizes important notations utilizedin this paper.
Before introducing the ultimatelearning model for dialogue structure analysis, wefirstly discuss a simplified version, Hidden Soft-max Model (HSM), which is based on Boltzmannmachine and assumes that the latent variables areindependent given visible units.
HSM has a two-layer architecture as shown in Figure 2.
The en-ergy of the state {V,h?,h?,h?}
is defined as fol-lows:E(V, h?, h?, h?)
=?E?
(V, h?)
+?E?
(V, h?)+?E?
(V, h?)
+ C(V),(1)where?E?(V,h?),?E?(V,h?)
and?E?(V,h?)
aresub-energy functions related to hidden variablesh?, h?, and h?, respectively.
C(V) is the sharedvisible units bias term.
Suppose K is the dictio-nary size, Dris the rthutterance size (i.e.
the2065number of words in the rthutterance), and R isthe number of utterances in the a dialogue.For each utterance vr(r = 1, .., R) in the dia-logue session we have a hidden variable vector h?r(with size of J ) as a latent state of the utterance,the sub-energy function?E?(V,h?)
is defined by?E?
(V, h?)
=?R?r=1J?j=1Dr?i=1K?k=1h?rjW?rjikvrik?R?r=1J?j=1h?rja?rj,(2)where vrik= 1 means the ithvisible unit vriin therthutterance takes on kthvalue, h?rj= 1 means therthsoftmax hidden units takes on jthvalue, and a?rjis the corresponding bias.
W?rjikis a symmetricinteraction term between visible unit vrithat takeson kthvalue and hidden variable h?rthat takes onjthvalue.The sub-energy function?E?(V,h?
), related tothe global general topic of the corpus, is definedby?E?
(V, h?)
= ?R?r=1Dr?i=1K?k=1h?W?rikvrik?
h?a?.
(3)The sub-energy function?E?(V,h?)
corresponds tothe dialogue specific topic, and is defined by?E?
(V, h?)
= ?R?r=1Dr?i=1K?k=1h?W?rikvrik?
h?a?.
(4)W?rikin Eq.
(3) and W?rikin Eq.
(4) are two sym-metric interaction terms between visible units andthe corresponding hidden units, which are similarto W?rjikin (2); a?and a?are the correspondingbiases.
C(V) is defined byC(V) = ?R?r=1Dr?i=1K?k=1vrikbrik, (5)where brikis the corresponding bias.The probability that the model assigns to a vis-ible binary matrix V = {v1, v2, ..., vD} (whereD =?Rr=1Dris the dialogue session size) isP (V) =1Z?h?, h?,h?exp(?E(V, h?, h?, h?
))Z =?V?h?, h?,h?exp(?E(V, h?, h?, h?
),(6)where Z is known as the partition function or nor-malizing constant.In our proposed model, for each word in thedocument we use a softmax unit to represent it.For the sake of simplicity, assume that the orderof words in an utterance is ignored.
Therefore, allof these softmax units can share the same set ofweights that connect them to hidden units, thus thevisible bias term C(V) and the sub-energy func-tions?E?(V,h?),?E?(V,h?)
and?E?(V,h?)
in Eq.
(1) can be redefined as follows:?E?
(V, h?)
=?R?r=1J?j=1K?k=1h?rjW?jkv?rk?R?r=1(DrJ?j=1h?rja?j)(7)?E?
(V, h?)
= ?K?k=1h?W?kv?k?Dh?a?(8)?E?
(V, h?)
= ?K?k=1h?W?kv?k?Dh?a?
(9)C(V) = ?K?k=1v?kbk, (10)where v?rk=?Dri=1vrikdenotes the count for thekthword in the rthutterance of the dialogue, v?k=?Rr=1v?rkis the count for the kthword in wholedialogue session.
Drand D (D =?Rr=1Dr)are employed as the scaling parameters, which canmake hidden units behave sensibly when dealingwith dialogues of different lengths (Hinton andSalakhutdinov, 2009).The conditional distributions are given by soft-max and logistic functions:P (h?rj= 1|V) =exp(?Kk=1W?jkv?rk+Dra?j)?Jj?=1exp(?Kk=1W?j?kv?rk+Dra?j?
)(11)P (h?= 1|V) = ?(K?k=1W?kv?k+Da?)
(12)P (h?= 1|V) = ?(K?k=1W?kv?k+Da?)
(13)P (vrik= 1|h?, h?, h?)
=exp(?Jj=1h?rjW?jk+ h?W?k+ h?W?k+ bk)?Kk?=1exp(?Jj=1h?rjW?jk?+ h?W?k?+ h?W?k?+ bk?
),(14)where ?
(x) = 1/(1 + exp(?x)) is the logisticfunction.20663.3 HSSM: Hidden Softmax Sequence ModelIn this section, we consider the dependency be-tween the adjacent latent states of utterances,and extend the HSM to hidden softmax sequencemodel (HSSM), which is displayed in Figure 3.We define the energy of the state {V,h?,h?,h?
}in HSSM as follows:E(V, h?, h?, h?)
=?E?
(V, h?)
+?E?
(V, h?)
+?E?
(V, h?
)+ C(V) +?E?
(h?, h?
),(15)where C(V),?E?(V,h?),?E?(V,h?)
and?E?(V,h?)
are the same with that in HSM.The last term?E?(h?,h?)
is utilized to formulatethe dependency between latent variables h?,which is defined as follows:?E?
(h?, h?)
=?J?q=1h?sFsqh?1q?J?q=1h?RqFeqh?e?R?1?r=1J?j=1J?q=1h?rjFjqh?r+1,q,(16)where h?sand h?eare two constant scalar variables(h?s?
1, h?e?
1), which represent the virtualbeginning state unit and ending state unit of a di-alogue.
Fsis a vector with size J , and its ele-ments measure the dependency between h?sandthe latent softmax units of the first utterance.
Fealso contains J elements, and in contrast to Fs,Ferepresents the dependency measure betweenh?eand the latent softmax units of the last utter-ance.
F is a symmetric matrix for formulating de-pendency between each two adjacent hidden unitspair (h?r, h?r+1), r = 1, ..., R?
1.Utterance 1 Utterance 2 Utterance 3Figure 3: Hidden softmax sequence model.
A con-nection between each pair of adjacent hidden soft-max units is added to formulate the dependencybetween the two corresponding latent states.3.4 Parameter LearningExact maximum likelihood learning in the pro-posed model is intractable.
?Contrastive Diver-gence?
(Hinton, 2002) can be used for HSM?slearning, however, it can not be utilized for HSSM,because the hidden-to-hidden interaction term,{F, Fs, Fe}, result in the intractability when ob-taining exact samples from the conditional distri-bution P (h?rj= 1|V), r = [1, R], j ?
[1, J ].We use the mean-field variational inference (Hin-ton and Zemel, 1994; Neal and Hinton, 1998;Jordan et al, 1999) and a stochastic approxima-tion procedure (SAP) (Tieleman, 2008) to esti-mate HSSM?s parameters.
The variational learn-ing is utilized to get the data-dependent expecta-tions, and SAP is utilized to estimate the model?sexpectation.
The log-likelihood of the HSSM hasthe following variational lower bound:logP (V; ?)
?
?hQ(h) logP (V, h; ?)
+H(Q).
(17)Q(h) can be any distribution of h in theory.?
= {W?,W?,W?, F, Fs, Fe} (the bias termsare omitted for clarity) are the model parameters.h = {h?,h?,h?}
represent all the hidden vari-ables.
H(?)
is the entropy functional.
In varia-tional learning, we try to find parameters that min-imize the Kullback-Leibler divergences betweenQ(h) and the true posterior P (h|V; ?).
A naivemean-field approach can be chosen to obtain afully factorized distribution for Q(h):Q(h) =[R?r=1q(h?)]q(h?)
q(h?
),(18)where q(h?rj= 1) = ?
?rj, q(h?= 1) = ?
?,q(h?= 1) = ??.
?
= {?
?, ?
?, ??}
are the pa-rameters of Q(h).
Then the lower bound on thelog-probability logP (V; ?)
has the form:logP (V; ?)
???E?
(V, ??)??E?
(V, ??)??E?
(V, ??)?
C(V)??E?(?
?, ??)?
logZ,(19)where?E?
(V, ??),?E?
(V, ??),?E?
(V, ??
), and?E?(?
?, ??)
have the same forms, by replacing ?with h, as Eqs.
(7), (8), (9), and (16), respectively.We can maximize this lower bound with respectto parameters ?
for fixed ?, and obtain the mean-field fixed-point equations:??rj=exp(?Kk=1W?jkv?rk+Dra?j+Djprev+Djnext?
1)?Jj?=1exp(?Kk=1W?j?kv?rk+Dra?j?+Dj?prev+Dj?next?
1),(20)2067?
?= ?(K?k=1W?kv?k+Da?)
(21)?
?= ?(K?k=1W?kv?k+Da?
), (22)where Djprevand Djnextare two terms relevant tothe derivative of the RHS of Eq.
(19) with respectto ?
?rj, defined byDjprev={Fsj, r = 1?Jq=1?
?r?1,qFqj, r > 1Djnext={?Jq=1Fjq?
?r+1,q, r < R.Fej, r = RThe updating of ?
can be carried out iterativelyuntil convergence.
Then, (V,?)
can be consideredas a special ?state?
of HSSM, thus the SAP can beapplied to update the model?s parameters, ?, forfixed (V,?
).4 Experiments and DiscussionsIt?s not easy to evaluate the performance of a dia-logue structure analysis model.
In this study, weexamined our model via qualitative visualizationand quantitative analysis as done in (Ritter et al,2010; Zhai and Williams, 2014).
We implementedfive conventional models to conduct an extensivecomparing study on the two corpora: Twitter-Postand AirTicketBooking.
Conventional models in-clude: LMHMM (Chotimongkol, 2008), LMH-MMS (Ritter et al, 2010), TMHMM, TMHMMS,and TMHMMSS (Zhai and Williams, 2014).
Inour experiments, for each corpus we randomly se-lect 80% dialogues for training, and use the rest20% for testing.
We select three different num-ber (10, 20 and 30) of latent states to evaluate allthe models.
In TMHMM, TMHMMS and TMH-MMSS, the number of ?topics?
in the latent statesand a dialogue is a hyper-parameter.
We con-ducted a series of experiments with varying num-bers of topics, and the results illustrated that 20is the best choice on the two corpora.
So, for allthe following experimental results of TMHMM,TMHMMS and TMHMMSS, the correspondingtopic configurations are set to 20.The number of estimation iterations for all themodels on training sets is set to 10,000; and onheld-out test sets, the numver of iterations for in-ference is set to 1000.
In order to speed-up thelearning of HSSM, datasets are divided into mini-batches, each has 15 dialogues.
In addition, thelearning rate and momentum are set to 0.1 and 0.9,respectively.4.1 Qualitative EvaluationDialogues in Twitter-Post always begin with threelatent states: broadcasting what they (Twitterusers) are doing now (?Status?
), broadcasting aninteresting link or quote to their followers (?Ref-erence Broadcast?
), or asking a question to theirfollowers (?Question to Followers?
).2We find thatstructures discoverd by HSSM and LMHMMSwith 10 latent states are most reasonable to inter-pret.
For example, after the initiating state (?Sta-tus?, ?Reference Broadcast?, or ?Question to Fol-lowers?
), it was often followed a ?Reaction?
to?Reference Broadcast?
(or ?Status?
), or a ?Com-ment?
to ?Status?, or a ?Question?
to ?Status?
(?Reference Broadcast?, or ?Question to Follow-ers??)
etc.
Compared with LMHMMS, besides ob-taining similar latent states, HSSM exhibits pow-erful ability in learning sequential dependency re-lationship between latent states.
Take the follow-ing simple Twitter dialogue session as an example:: rt i like katy perry lt lt we see tht lol: lol gd morning: lol gd morning how u: i?m gr8 n urself: i?m good gettin ready to head out: oh ok well ur day n up its cold out here...LMHMMS labelled the second utterance (?lol gdmorning ?)
and the third utterance (?lol goodmorning how u ? )
into the same latent state, whileHSSM treats them as two different latent states(Though they both have almost the same words).The result is reasonable: the first ?gd morning?
isa greeting, while the second ?gd morning?
is a re-sponse.For AirTicketBooking dataset, the state-transition diagram generated with our modelunder the setting of 10 latent states is presentedin Figure 4.
And several utterance examplescorresponding to the latent staes are also showedin Table 2.
In general, conversations beginwith sever agent?s short greeting, such as ?Hi,very glad to be of service.
?, and then transit tochecking the passenger?s identity information or2For simplicity and readability in consistent, we followthe same latent state names used in (Ritter et al, 2010)2068inquiring the passenger?s air ticket demand; orit?s directly interrupted by the passenger withbooking demand which is always associated withplace information.
After that, conversations arecarried out with other booking related issues, suchas checking ticket price or flight time.The flowchart produced by HSSM can be rea-sonably interpreted with knowledge of air ticketbooking domain, and it most consistent with theagent?s real workflow of the Ticket Booking Cor-poration3compared with other models.
We noticethat conventional models can not clearly distin-guish some relevant latent states from each other.For example, these baseline models always con-found the latent state ?Price Info?
with the latentstate ?Reservation?, due to certain words assignedlarge weights in the two states, such as ???
(dis-count)?, and ????
(credit card)?
etc.
Further-more, Only HSSM and LMHMMS have dialoguespecific topics, and experimental results illustratethat HSSM can learn much better than LMHMMSwhich always mis-recognize corpus general wordsas belonging to dialogue specific topic (An exam-ple is presented in Table 3).Please WaitingConfirmationInquiryStartPlace Info Price InfoTime InfoPassenger InfoEndReservation0.270.290.100.260.210.360.190.170.26 0.180.310.250.120.110.13Figure 4: Transitions between latent states onAirTicketBooking generated by our HSSM modelunder the setting of J = 10 latent states.
Transi-tion probability cut-off is 0.10.4.2 Quantitative EvaluationFor quantitative evaluation, we examine HSSMand traditional models with log likelihood and anordering task on the held-out test set of Twitter-Post and AirTicketBooking.3We hide the corporation?s real name for privacy reasons.Latent States Utterance Examples Utterance Examples(Chinese) (English Translation)Start ???????????
Hello, very glad to be of service.Inquiry ????????
Do you want to make a flightreservation?Place Info ??????????????
?I want to book an air ticket fromBeijing to Shanghai.Time Info ????10????
Tomorrow morning, about 10o?clock.Price Info ????1300????
The adult ticket is 1300 Yuan.Passenger Info ?
?
?
?
?
?
?
?
?12345?My name is Li Dong, and my IDnumber is 12345.Confirmation ??????
Yes, that?s OK.Please Waiting ??????????
Please wait a moment, I?ll checkfor you.Reservation ??????????????
?Please make a reservation, I wantto use a credit card to pay.End ??????????
Welcome to call next time.
Bye.Table 2: Utterance examples of latent states dis-covered by our model.Model Top WordsHSSM??,??,??,??,?
?, ...ten o?clock, Dong Li (name), Fuzhou (city), Xiamen(city), Shanghai Airlines, ...LMHMMS?,??,?,??,?
?, ...have, ten o?clock, er, Dong Li (name), reserve, ...Table 3: One example of dialogue specific topiclearned on the same dialogue session with HSSMand LMHMMS, respectively.Log Likelihood The likelihood metric mea-sures the probability of generating the test set us-ing a specified model.
The likelihood of LMHMMand TMHMM can be directed computed with theforward algorithm.
However, since likelihoods ofLMHMMS, TMHMMS and TMHMMSS are in-tractable to compute due to the local dependen-cies with respect to certain latent variables, Chib-style estimating algorithms (Wallach et al, 2009)are employed in our experiments.
For HSSM, thepartition function is a key problem for calculatingthe likelihood, and it can be effectively estimatedby Annealed Importance Sampling (AIS) (Neal,2001; Salakhutdinov and Murray, 2008).Figure 5 presents the likelihood of differentmodels on the two held-out datasets.
We can ob-serve that HSSM achieves better performance onlikelihood than all the other models under differentnumber of latent states.
On Twitter-Post datasetour model slightly surpasses LMHMMS, and itperforms much better than all traditional modelson AirTicketBooking dataset.Ordering Test Following previous work(Barzilay and Lee, 2004; Ritter et al, 2010;Zhai and Williams, 2014), we utilize Kendall?s?
(Kendall, 1938) as evaluation metric, whichmeasures the similarity between any two se-quential data and ranges from ?1 (indicating areverse ordering) to +1 (indicating an identical2069J = 10 J = 20 J = 30Figure 5: Negative log likelihood (smaller is better) on held-out datasets of Twitter-Post (upper) andAirTicketBooking (lower) under different number of latent states J .ordering).
This is the basic idea: for each dialoguesession with n utterances in the test set, we firstlygenerate all n!
permutations of the utterances;then evaluate the probability of each permutation,and measure the similarity, i.e.
Kendall?s ?
,between the max-probability permutation andthe original order; finally, we average ?
valuesfor all dialogue sessions as the model?s orderingtest score.
As pointed out by Zhai et al (2014),it?s however infeasible to enumerate all possiblepermutations of dialogue sessions when thenumber of utterances in large.
In experiments,we employ the incrementally adding permutationstrategy, as used by Zhai et al (2014), to buildup the permutation set.
The results of orderingtest are presented in Figure 6.
We can see thatHSSM exhibits better performance than all theother models.
For the conventional models, itis interesting that LMHMMS, TMHMMS andTMHMMSS achieve worse performances thanLMHMM and TMHMM.
This is likely becausethe latter two models allow words to be emittedonly from latent states (Zhai and Williams, 2014),while the former three models allow words tobe generated from additional sources.
Thisalso implies HSSM?s effectiveness of modelingdistinct information uderlying dialogues.4.3 DiscussionThe expermental results illustrate the effective-ness of the proposed undirected dialogue struc-ture analysis model based on Boltzmann machine.The conducted experiments also demonstrate thatundirected models have three main merits for textmodeling, which are also demonstrated by Hintonand Salakhutdinov (2009), Srivastava et al (2013)through other tasks.
Boltzmann machine basedundirected models are able to generalize much bet-ter than traditional directed generative model; andmodel learning is more stable.
Besides, an undi-rected model is more suitable for describing com-plex dependencies between different kinds of vari-ables.We also notice that all the models can, to somedegree, capture the sequential structure in the di-alogues, however, each model has a special char-acteristic which makes itself fit a certain kind ofdataset better.
HSSM and LMHMMS are moreappropriate for modeling the open domain dataset,such as Twitter-Post used in this paper, and thetask-oriented domain dataset with one relativelyconcentrated topic in the corpus and special in-formation for each dialogue, such as AirTicket-Booking.
As we known, dialogue specific top-ics in HSSM or LMHMMS are used and trainedonly within corresponding dialogues.
They arecrucial for absorbing certain words that have im-portant meaning but do not belongs to latent states.In addition, for differet dataset, dialogue specifictopics may have different effect to the model-ing.
Take the Twitter-Post for an example, dia-logue specific topics formulate actual themes ofdialogues, such as a pop song, a sport news.
As forthe AirTicketBooking dataset, dialogue specific2070J = 100 20 40 60 80 10000.20.40.60.81.0average  kendall?s  ?J = 200 20 40 60 80 10000.20.40.60.81.0 J = 300 20 40 60 80 10000.20.40.60.81.00 20 40 60 80 10000.20.40.60.81.0average  kendall?s  ?0 20 40 60 80 10000.20.40.60.81.0# of random permutations0 20 40 60 80 10000.20.40.60.81.00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9HSSM LMHMM LMHMMS TMHMM TMHMMS TMHMMSSFigure 6: Average Kendall?s ?
measure (larger is better) on held-out datasets of Twitter-Post (upper) andAirTicketBooking (lower) under different number of latent states J .topics always represent some special information,such as the personal information, including name,phone number, birthday, etc.
In summary, each di-alogue specific topic reflects special informationwhich is different from other dialogues.The three models, TMHMM, TMHMMS andTMHMMSS, which do not include dialogue spe-cific topics, should be utilized on the task-orienteddomain dataset, in which each dialogue has littlespecial or personnal information.
For example, thethree models perform well on the the BusTime andTechSupport datasets (Zhai and Williams, 2014),in which name entities are all replaced by differentsemantic types (e.g.
phone numbers are replacedby ?<phone>?, E-mail addresses are replaced by?<email>?, etc).5 ConclusionsWe develope an undirected generative model,HSSM, for dialogue structure analysis, and exam-ine the effectiveness of our model on two differentdatasets, Twitter posts occurred in open-domainand task-oriented dialogues from airline ticketbooking domain.
Qualitative evaluations andquantitative experimental results demonstrate thatthe proposed model achieves better performancethan state-of-the-art approaches.
Compared withtraditional models, the proposed HSSM has morepowerful ability of discovering structures of latentstates and modeling different word sources, in-cluding latent states, dialogue specific topics andglobal general topic.According to recent study (Srivastava et al,2013), a deep network model exhibits much ben-efits for latent variable learning.
A dialogue mayactually have a hierarchy structure of latent states,therefore the proposed model can be extended to adeep model to capture more complex structures.Another possible way to extend the model is toconsider modeling long distance dependency be-tween latent states.
This may further improve themodel?s performance.AcknowledgmentsWe are grateful to anonymous reviewers for theirhelpful comments and suggestions.
We would liketo thank Alan Ritter for kindly providing the rawTwitter dataset.This work is supported in part by the Na-tional Natural Science Funds of China under Grant61170197 and 61571266, and in part by the Elec-tronic Information Industry Development Fundunder project ?The R&D and Industrializationon Information Retrieval System Based on Man-Machine Interaction with Natural Speech?.2071ReferencesJames Allen, Nathanael Chambers, George Ferguson,Lucian Galescu, Hyuckchul Jung, Mary Swift, andWilliam Taysom.
2007.
Plow: A collaborative tasklearning agent.
In Proceedings of the National Con-ference on Artificial Intelligence, volume 22, page1514.
Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models with applicationsto generation and summarization.
In proceedings ofHLT-NAACL 2004, pages 113?120.Ananlada Chotimongkol.
2008.
Learning the structureof task-oriented conversations from the corpus of in-domain dialogs.
Ph.D. thesis, SRI International.William W Cohen, Vitor R Carvalho, and Tom MMitchell.
2004.
Learning to classify emailinto?speech acts?.
In EMNLP, pages 309?316.Nigel Crook, Ramon Granell, and Stephen Pulman.2009.
Unsupervised classification of dialogue actsusing a dirichlet process mixture model.
In Proceed-ings of the SIGDIAL 2009 Conference: The 10th An-nual Meeting of the Special Interest Group on Dis-course and Dialogue, pages 341?348.
Associationfor Computational Linguistics.Geoffrey E Hinton and Ruslan R Salakhutdinov.
2009.Replicated softmax: an undirected topic model.
InAdvances in neural information processing systems,pages 1607?1614.Geoffrey E Hinton and Richard S Zemel.
1994.Autoencoders, minimum description length, andhelmholtz free energy.
Advances in neural informa-tion processing systems, pages 3?3.Geoffrey E Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural com-putation, 14(8):1771?1800.Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.2009.
Semi-supervised speech act recognition inemails and forums.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 3-Volume 3, pages 1250?1259.Association for Computational Linguistics.Michael I Jordan, Zoubin Ghahramani, Tommi SJaakkola, and Lawrence K Saul.
1999.
An intro-duction to variational methods for graphical models.Machine learning, 37(2):183?233.Maurice G Kendall.
1938.
A new measure of rankcorrelation.
Biometrika, 30(1/2):81?93.Jingjing Liu, Stephanie Seneff, and Victor Zue.
2010.Dialogue-oriented review summary generation forspoken dialogue recommendation systems.
In Hu-man Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, pages64?72.
Association for Computational Linguistics.Gabriel Murray, Steve Renals, Jean Carletta, and Jo-hanna Moore.
2006.
Incorporating speaker anddiscourse features into speech summarization.
InProceedings of the main conference on Human Lan-guage Technology Conference of the North Amer-ican Chapter of the Association of ComputationalLinguistics, pages 367?374.
Association for Com-putational Linguistics.Radford M Neal and Geoffrey E Hinton.
1998.
Aview of the em algorithm that justifies incremental,sparse, and other variants.
In Learning in graphicalmodels, pages 355?368.
Springer.Radford M Neal.
2001.
Annealed importance sam-pling.
Statistics and Computing, 11(2):125?139.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.Ruslan Salakhutdinov and Iain Murray.
2008.
On thequantitative analysis of deep belief networks.
InProceedings of the 25th international conference onMachine learning, pages 872?879.
ACM.Nitish Srivastava, Ruslan R Salakhutdinov, and Geof-frey E Hinton.
2013.
Modeling documents withdeep boltzmann machines.
UAI.Tijmen Tieleman.
2008.
Training restricted boltz-mann machines using approximations to the likeli-hood gradient.
In Proceedings of the 25th interna-tional conference on Machine learning, pages 1064?1071.
ACM.Hanna M Wallach, Iain Murray, Ruslan Salakhutdinov,and David Mimno.
2009.
Evaluation methods fortopic models.
In Proceedings of the 26th Annual In-ternational Conference on Machine Learning, pages1105?1112.
ACM.Yorick Wilks.
2006.
Artificial companions as a newkind of interface to the future internet.Steve J Young.
2006.
Using pomdps for dialog man-agement.
In SLT, pages 8?13.Ke Zhai and Jason D Williams.
2014.
Discoveringlatent structure in task-oriented dialogues.
In ACL(1), pages 36?46.2072
