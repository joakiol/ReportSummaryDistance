Baseline Evaluation of WSDand Semantic Dependency inOntoSemSergei NirenburgStephen BealeMarjorie McShaneUniversity of Maryland Baltimore County (USA)email: sergei@umbc.eduAbstractThis paper presents the evaluation of a subset of the capabilities of the On-toSem semantic analyzer conducted in the framework of the Shared Taskfor the STEP 2008 workshop.
We very briefly describe OntoSem?s com-ponents and knowledge resources, describe the work preparatory to theevaluation (the creation of gold standard basic text meaning representa-tions) and present OntoSem?s performance on word sense disambiguationand determination of semantic dependencies.
The paper also contains el-ements of a methodological discussion.315316 Nirenburg, Beale, and McShane1 Overview of OntoSemOntoSem, which is the implementation of the theory of Ontological Semantics (Niren-burg and Raskin, 2004), is a text-processing environment that takes as input unre-stricted raw text and carries out preprocessing followed by morphological, syntactic,semantic, and discourse analysis, with the results of analysis represented as a formaltext-meaning representation (TMR) that can then be used as the basis for various ap-plications.
Text analysis relies on several knowledge resources, briefly described inthe subsections below.1.1 The OntoSem OntologyThe OntoSem ontology is a formal, language-independent, unambiguous model ofthe world that provides a metalanguage for describing meaning.
It is a multiple-inheritance hierarchical collection of frames that contains richly interconnected de-scriptions of types of OBJECTs, EVENTs and PROPERTies.
It is a general purposesontology, containing about 9,000 concepts, that has a number of especially well de-veloped domains that reflect past and ongoing application-specific knowledge acqui-sition.
Each OBJECT and EVENT is described by several dozen properties, some prop-erty values being locally specified and others, inherited from ancestors.Selectional restrictions in the ontology are multivalued, with fillers being intro-duced by a facet.
The value facet is rigid and is used less in the ontology than in itssister knowledge base of real-world assertions, the fact repository (see Section 1.3).The facets default (for strongly preferred constraints) and sem (for basic semanticconstraints) are abductively overridable.
The relaxable-to facet indicates possible butatypical restrictions, and not blocks the given type of filler.Event-oriented scripts encode typical sequences of EVENTs and the OBJECTs thatfill their case-roles.
Scripts are used to reason about both language and the world and,in addition to supporting text processing, can support simulation, as in our ongoingMaryland Virtual Patient project (see, e.g.
McShane et al, 2007).The number of concepts in the ontology is far fewer than the number of words orphrases in any language due to the existence of synonyms in language; the possibilityof describing lexical items using a combination of ontological and extra-ontological(e.g., temporal) descriptors; the use of a single concept for each scalar attribute thatdescribes all words on that scale (e.g., gorgeous, pretty, ugly); and the decision not toinclude language-specific concepts in the ontology.As an example of the description of an ontological concept, consider an excerptfrom the description of the concept ESOPHAGUS:ESOPHAGUSIS-A value ANIMAL-ORGANLOCATION sem TRUNK-OF-BODYDISTAL-TO sem PHARYNXPROXIMAL-TO sem STOMACHLENGTH sem 24default-measure CENTIMETERINSTRUMENT-OF sem SWALLOWTHEME-OF sem ESOPHAGEAL-CANCERACHALASIA ...Baseline Evaluation of WSD and Semantic Dependency in OntoSem 317It is the richness of the property-based descriptions that permit the OntoSem ontologyto be used for high-end applications like medical simulation and tutoring.1.2 The OntoSem LexiconEven though we refer to the OntoSem lexicon as a semantic lexicon, it contains morethan just semantic information: it also supports morphological and syntactic analysisand generation.
Semantically, it specifies what concept, concepts, property or proper-ties of concepts defined in the ontology must be instantiated in the TMR to accountfor the meaning of a given lexical unit of input.Lexical entries are written in an extended Lexical-Functional Grammar formalismusing LISP-compatible format.
The lexical entry ?
in OntoSem, it is actually calleda superentry ?
can contain descriptions of several lexical senses; we call the latterentries.
As an example, consider the 2nd sense of take:(take-v2(cat v)(def "to begin to grasp physically")(ex "He took her hand as she got out of the car.
")(syn-struc((subject ((root $var1) (cat n)))(root $var0) (cat v)(directobject ((root $var2) (cat n)))))(sem-struc(HOLD(phase begin)(AGENT (value ?
$var1))(THEME (value ?
$var2)))))The sem-struc says that the meaning of this word sense is the inceptive aspect (?phasebegin?)
of the ontological event HOLD.
The AGENT of HOLD is assigned the meaningof $var1 (the caret indicates ?the meaning of?)
in the input text, and the THEME ofHOLD is assigned the meaning of $var2.
The OntoSem lexicon currently containsapproximately 35,000 senses.
For further information about the lexicon, see, e.g.,McShane et al (2005b).A sister resource to the lexicon is the onomasticon, a lexicon of proper nameslinked to their respective ontological concepts: e.g., IBM is semantically described asCORPORATION.1.3 The Fact RepositoryThe fact repository contains numbered remembered instances of concepts, with thenumbers being used for disambiguation: e.g., HUMAN-FR88 is the 88th human storedin the fact repository ?
e.g., President Clinton.
Some aspects of ?general worldknowledge?
are part of the seed fact repository used for all applications: e.g., Franceis recorded as NATION-FR47, and this information is available to all intelligent agentsin our environment.
This seed fact repository is then dynamically augmented as agiven corpus is being processed.
The fact repository also supports text processing, asfor reference resolution: e.g., President Clinton in any text will be coreferential withHUMAN-FR88.318 Nirenburg, Beale, and McShane2 Text Meaning Representations (TMRs)Section 3 includes an example of a TMR taken from the competition texts as wellas a description of it.
Here we will give a brief overview of the status of TMRs inOntoSem.TMRs represent propositions connected by discourse relations (see Nirenburg andRaskin (2004), Chapter 6 for details).
Propositions are headed by instances of on-tological concepts, parameterized for modality, aspect, proposition time and overallTMR time.Each proposition is related to other instantiated concepts using ontologi-cally defined relations.
Coreference links form an additional layer of linking betweeninstantiated concepts in the TMR as well as stored concept instances in the fact repos-itory.3 The STEP 2008 Shared TaskThis section describes the evaluation of OntoSem results for the shared task at theSTEP 2008 Workshop.
Individual groups were allowed to make their own decisionswith respect to a number of important parameters of the task, including, among others:1. the nature of the metalanguage of semantic description (e.g., whether it relieson uninterpreted clusters of word senses, defined either within a language orcross-linguistically; whether it is based on a language-independent ?interlin-gual?
vocabulary; whether the latter is interpreted by assigning properties tovocabulary elements and constraints on the values of these properties, etc.);2.
the breadth of the coverage of phenomena (e.g., whether to include word sensedisambiguation, semantic dependency determination, reference resolution, cov-erage of modality, aspect, time, quantification, etc.);3.
the depth of coverage of phenomena (e.g., the grain size of the description ofword senses, the size of the inventory of semantic roles and other descriptiveproperties);4. whether the analyzer is tuned to produce a complete result for any input; toproduce partial results for all or some inputs; to produce output only for inputsit knows it can process;5. whether (and how) the analyzer takes into account benign ambiguities, vague-ness and underspecification;6. whether the analyzer creates a semantic and pragmatic context for the inputtexts, thus modeling human ability to activate relevant knowledge not expresslymentioned in the text;7. the practical end application(s) that a particular semantic analyzer aims to sup-port.In working on the shared task, our group has elected to test our system?s perfor-mance on word sense disambiguation (WSD) and semantic dependency determina-tion.
(For an early small-scale evaluation, see Nirenburg et al (2004).)
A prerequisiteBaseline Evaluation of WSD and Semantic Dependency in OntoSem 319for our chosen evaluation experiment was filling the lacunae in lexical coverage.
Twopoints are important to make here: a) we acquired what we consider a complete set ofsenses for each input word absent from our lexicon, not just the sense that was neededfor the text ?
in other words, this was general-purpose acquisition; b) this was a partof routine ongoing work on resource acquisition and improvement in OntoSem.
Theonly difference that these input texts made was with respect to the schedule of whatto acquire first.
Here are some basic statistics about our lexicon work.
The inputtexts contained 270 lemmata, of which 36 (13%) were not originally in the OntoSemlexicon.
44 senses were added to the lexicon for the 36 words (these words were pre-dominantly very specific, single-sense ones).
In 12 cases, a sense was added to anexisting lexicon entry (bringing the average number of senses for these 12 lemmata to10.5).
Finally, 5 word senses were added not because of any lacunae in the lexicon butjust to make the life of the analyzer more difficult.
In the end, the lexicon contained1,168 senses for the 270 lemmata (an average of 4.33 senses per word).Note that OntoSem processes more phenomena than WSD and dependency ?
as-pect, time, speaker attitudes (called modalities in OntoSem), reference resolution andsemantic ellipsis, discourse relations, unexpected input, metonymy, etc.
In broadterms, the overall objective of the OntoSem analyzer, when viewed outside of theneeds of this evaluation experiment, is to generate a significant amount of machine-tractable knowledge from the text, knowledge that includes not just a minimum ofinformation gleaned from the text but also preference information obtained from theontological and fact-repository substrate to be used in disambiguation heuristics andapplications relying on the human ability to reconstruct meanings not overtly men-tioned in the text for the purposes of reasoning and applications like question answer-ing.
For an overview of how TMRs produced by OntoSem can be used in lieu oftraditional annotation schemes, see McShane et al (2005a).A standard example of using world knowledge activated in the process of text anal-ysis is being able to infer (abductively) that once ?virus?
has been resolved to be theorganism rather than the computer program, then if the word ?operation?
appears fur-ther on in the text, it is more probable that it means surgery rather than a computeroperation or a military operation.
The meaning extraction process also has a strongfiltering ability to reject most of the senses of the words in the input as inappropriatefor the particular text.
This ability is not error-proof but the filtering capacity is quitestrong even in the current state of the OntoSem analyzer; also note that the ratio ofselected senses to those filtered away is a good measure of how much the static re-sources were tuned to a particular text or domain ?
the greater the number of sensesper word, the less tuning occurred.OntoSem distinguishes two stages of meaning representation producing, respec-tively, what is called basic and extended TMRs.
The former covers the parts of thesemantic representation that can be derived from the syntactic and lexical-semanticinformation and contains triggers (called ?meaning procedures?)
for a variety of mi-crotheories that require additional heuristics (and usually, as in the case of referenceresolution, use general world knowledge from the ontology and fact repository as wellas a window of text that is wider than a single sentence; for more on meaning proce-dures see McShane et al (2004)).
In this evaluation we constrained ourselves to thelevel of basic TMRs.320 Nirenburg, Beale, and McShaneWe will use the following relatively simple example sentence to demonstrate thescope of work of OntoSem.Researchers have been looking for other cancers that may be caused byviruses.The basic TMR for this sentence is as follows:SEARCH-103AGENT RESEARCHER-102THEME CANCER-104textpointer lookword-num 3from-sense look-v2RESEARCHER-102AGENT-OF SEARCH-103multiple +textpointer researcherword-num 0from-sense researcher-n1CANCER-104THEME-OF SEARCH-103CAUSED-BY VIRUS-DISEASE-108multiple +textpointer cancerword-num 6from-sense cancer-n1MODALITY-105TYPE EPISTEMICSCOPE CAUSED-BY-109VALUE 0.5textpointer mayword-num 8from-sense may-aux1CAUSED-BY-109DOMAIN CANCER-104RANGE VIRUS-DISEASE-108SCOPE-OF MODALITY-105textpointer caused byword-num 10from-sense cause-v1VIRUS-DISEASE-108EFFECT CANCER-104multiple +textpointer virusword-num 12from-sense virus-n1We will describe select aspects of this TMR that apply to all frames.
The head of theTMR is a SEARCH event with the instance number 103.
Its AGENT is RESEARCHER-102 and its THEME is CANCER-104.
Both of these case-role fillers also have their ownframes that show inverse relations as well as other properties: e.g., RESEARCHER-102has the property ?multiple +?, which indicates a set with cardinality > 1.
A textpointeris the word in the text that gives rise to the given concept; its word number is shown,as is the appropriate lexical sense.
The textpointer is included for the benefit of theBaseline Evaluation of WSD and Semantic Dependency in OntoSem 321human users, as an aid in debugging.
It does not have any bearing on the meaningrepresentation, as used by an application.
Ontological concepts are unambiguous,as can be seen by the mapping of virus to the concept VIRUS-DISEASE rather thanCOMPUTER-VIRUS.
Modalities are defined for type, scope and value, with valuesbeing on the abstract scale {0,1}.
They are also defined for their attribution, whichdefaults to the speaker if no person is indicated explicitly in the text.In this TMR, it so happens, there are no overt triggers for further processing ?even though reference resolution is routinely triggered on all objects and events withthe exception of those that are known not to require it (e.g., NPs with an indefinitearticle or universally known single entities, such as the sun).The English gloss of the above TMR is as follows.
There is one main event inthe sentence ?
the searching event (represented by the word look).
The agent of thisevent is a set of researchers and the theme of this event is a set of cancers, understoodas diseases.
This latter set is further qualified to include only those cancers that arecaused by viruses, understood as organisms.
The researchers are not sure at the mo-ment of speech whether particular cancers are indeed caused ?
fully or partially ?by viruses.
It is known to the researchers and the author of the text that some canceror cancers may, in fact, be caused by viruses (this is a contribution of the word otherto the meaning of the sentence; another contribution of that word is posting a meaningprocedure for blocking coreference of the cancer or cancers mentioned in this sentencewith those mentioned in previous text).
The search started before the time of speechand is still ongoing at the time of speech.4 Creating Gold Standard TMRsGold standard TMRs form the basis for evaluating the results of automatic semanticanalysis.
To serve as useful measures, these TMRs must be created on the basis of theavailable static knowledge resources (in the case of OntoSem, mainly the lexicon andthe ontology) and reflect the maximum of what a system could in principle do withthe given resources.Creating gold standard TMRs is similar to manually annotating texts using the met-alanguage of OntoSem.
Text annotation is a difficult and time-consuming (and, there-fore, expensive) task.
The deeper the level of annotation, the less reliable the resultsare.
Even syntactic annotation poses problems and does not yield acceptable kappascores measuring agreement among annotators (and, of course, agreement among an-notators is not a fool-proof measure of the quality of an annotation).
The annotationnecessary for evaluating OntoSem TMRs is quite deep.
Our experience showed thatbuilding gold standard TMRs entirely by hand is a very costly task ?
it requireshighly skilled personnel and involves many searches in the knowledge resources forselecting appropriate TMR components.In view of the above, we decided on semi-automatic production of gold standardTMRs as the most economical way of producing high-quality annotations.
The pro-cess is, briefly, as follows.
OntoSem operation is modularized into stages.
Givenan input, OntoSem runs the first stage and presents its results to the human valida-tor/editor.
The latter corrects any mistakes and submits the resulting structure to thenext stage of OntoSem.
The process repeats until OntoSem?s final stage results arecorrected and approved by the human validator, thus yielding a gold standard TMR.322 Nirenburg, Beale, and McShaneAlthough ?raw?
TMRs can be cumbersome to read, the presentation format shownbelow ?
which is automatically generated from raw TMRs ?
reads rather easily asnon-natural languages go.
This demonstrates how our representation is actually quiteNL-like, its role being not unlike the English ?possibilistic?
sentences of Schubert(2002).
As concerns writing TMRs, people practically never do it ?
the most they dois check and, sometimes, correct the output of the automatic analysis system.Figure 1: The preprocessor editor of DEKADEThe user interfaces supporting the production of gold standard TMRs are incor-porated in DEKADE, OntoSem?s Development, Evaluation, Knowledge Acquisitionand Demonstration Environment.
The editor for preprocessor results is illustrated inFigure 1.The process of gold standard TMR creation has undergone modifications since thefirst version of DEKADE was deployed.
In particular, experience showed that peoplefind it difficult to edit the results of syntactic analysis, since it produces a densely pop-ulated chart of various options.
So, instead of the syntax editor, we introduced a link-ing editor (see Figure 2) that helps to establish the correct linking between syntacticarguments and adjuncts on the one hand and semantic case roles and other ontologi-cal properties on the other.
We will return to editing syntactic dependency structuresonce we devise or import an ergonomically appropriate method for this task.
Figure 3illustrates the editor of basic TMRs.The semi-automatic methodology of creating gold standard TMRs has proved ad-equate.
It takes a well-trained person on average less than a minute to correct pre-processing results for an average sentence of 25 words.
Establishing correct linkingbetween syntactic arguments and semantic case roles can take much longer.
Togetherwith the task of validating word sense selection (because of the peculiarities of theDEKADE editors), this task takes on average about 30 minutes per 25-word sentence.The time for final editing of the basic gold standard TMRs varies depending on howmuch material is present that does not relate to the ?who did what to whom?
compo-nent of meaning.
However, the overall net time needed to create a gold standard TMRBaseline Evaluation of WSD and Semantic Dependency in OntoSem 323Figure 2: The linking editor of DEKADEFigure 3: The TMR editor of DEKADE324 Nirenburg, Beale, and McShanefor a 25-word sentence is on the order of about 40 minutes.Our methodology of semi-automatic creation of gold standard TMRs differs fromthe established rules of the game in the annotation-based evaluation business.
We se-lected this approach because it a) significantly cuts the time needed for TMR produc-tion (we believe that the task would be simply impractical if attempted fully manually?
the amount of annotation required being too extensive); b) simplifies the evaluationbecause it produces the TMR for the one paraphrase that OntoSem will (attempt to)produce automatically.
The main efficiency gain in this semi-automatic production ofgold standard TMRs is in using the OntoSem analyzer as a means of quickly retrievingand easily inspecting and selecting the lexical senses for the words in the input.5 ResultsDepending on a particular setting, OntoSem can seek one result for WSD or depen-dency determination, n-best results or both for each case of ambiguity.
We chose tocreate both types of output.
At the level of basic TMR there can in principle be oneor more correct results, the latter case may signify a situation of underspecification orvagueness, to be resolved in OntoSem by special microtheories leading to the produc-tion of an extended TMR.
In producing gold standard TMRs we always selected thesingle correct word sense, irrespective of whether we had to consult other parts of theinput text or general world knowledge.The results of OntoSem?s performance on word sense disambiguation were evalu-ated against the gold standard TMRs and involved four different scores.
In the case ofeach of the scores, performance on individual instances of word sense disambiguationwas averaged over each sentence and text.Score1 is 1 if the disambiguation was correct and 0 if it was not.Score2 is 1 if the correct result is in the set of best results returned by OntoSem fora particular disambiguation instance such that the quality score generated for them byOntoSem is within 0.03 of the single best score (on the scale from 0 to 1).
This scoreis 0 if the correct result is not within the above set.
This measure was used becausethe significance of a preference at this fine-grain level is minimal.Score3 takes into account the complexity of the disambiguation task by involvingthe number of senses from which the choice is made.
Indeed, selecting out of 13candidates is more difficult than out of, say, just 2.
Thus, returning an incorrect resultwhen there are 2 candidates earns a 0 but if there are more than 2 candidates, insteadof 0 (as in Score1), we list the score of 1?
(2/n), where n is the number of senses.A correct result is given a score of 1.
As usual, cumulative scores were computed assimple averages over the input words.Score4 was calculated using partially corrected results of the preprocessing andsyntactic stages of the analysis process.
This score did not take into account thecomplexity of the disambiguation task, as did Score3.
It was, in fact, Score2 computedover partially corrected preprocessing results.
The purpose of using it is to attempt toassign blame across the various components of the rather complex OntoSem system.Semantic dependency results are scored as 1 when they are correct and 0 whenthey are incorrect.
If an element of TMR is not a part of the semantic dependencystructure but should be then we count that as an error.
The results of the evaluation aresummarized in the table below:Baseline Evaluation of WSD and Semantic Dependency in OntoSem 325Text WSD Score1 WSD Score2 WSD Score3 WSD Score4 Dependencies1 42/55 .78 43/55 .87 49.707/55 .90 48/55 .87 20/34 .592 30/40 .75 31/40 .78 37.030/40 .93 36/40 .90 15/20 .753 22/29 .76 23/29 .79 26.348/29 .91 24/29 .83 18/22 .824 75/114 .66 77/114 .68 96.966/114 .85 87/114 .76 44/84 .525 67/105 .64 67/105 .64 88.440/105 .84 85/105 .81 36/62 .586 82/129 .64 84/129 .65 104.807/129 .79 92/129 .71 45/99 .457 95/133 .71 95/133 .71 114.260/133 .86 101/133 .76 47/88 .53Total 413/605 .68 420/605 .69 517.550/605 .86 474/605 .78 225/410 .556 DiscussionOur goal was not to get the best results for this particular task but rather to test someof the capabilities of the ?raw?
OntoSem analyzer.
We have always advocated hy-bridization of methods, a direct consequence of our group?s belief in task- rather thanmethod-oriented approaches to system building.
We fully expect to take that routewhen we are putting together the next end application.
However, from the scientificpoint of view, it is important to assess the quality and promise of a particular method,even if it is known beforehand that it will be used in practical applications togetherwith other methods.Some practical limitations influenced our results.
This is why we included theword ?baseline?
in the title of this paper.
We intend to eliminate these limitationsover time.
The syntactic support for the system has been recently fully revampedto incorporate the Stanford parser.
The work on deriving full syntactic dependencystructures compatible with the requirements and coverage of the OntoSem syntax-to-semantics linking module from the results provided by the Stanford parser was notcompleted by the time of the evaluation.
This means, among other things, that not allthe diathesis transformations needed have been included.In the current version of DEKADE, the automatic validator of lexicon acquisitiondoes not yet indicate to acquirers when a new lexical sense has the same or verysimilar syntactic and semantic constraints.
As a result, some of the word senses cannotcurrently be disambiguated using the standard selectional restriction-based method.In addition to the above general limitations, there were some challenges specific tothe particular input corpus.
For example, the microtheory of measure has not yet beenfully implemented in OntoSem.We have not yet done enough to determine the contribution of preprocessing, syntaxand the various semantic microtheories to the final result.
We intend to pay moreattention to this blame assignment task.We restricted ourselves to evaluating just WSD and dependency determination be-cause of time and resource limitations.
It is clear that the quality of OntoSem?s outputfor the other microtheories mentioned in Section 3 above, among others, must also beevaluated.In addition to the above, we also plan to run an evaluation of OntoSem?s perfor-mance on treating unexpected input, using the version of the lexicon existing beforethe shared task started.We will also work on modifying the relative importance of heuristics from different326 Nirenburg, Beale, and McShanesources.
In particular, we will work toward reducing the influence of syntactic cluesand thereby moving the center of gravity of the analysis process toward semanticsproper.The process of evaluating TMRs has benefits beyond assessing our progress.
Itfacilitates debugging and enhancing the knowledge resources and processing modulesof the system.
Finally, we believe that the gold standard TMRs required for evaluationcan also be used as an annotated training corpus for machine learning experimentsin semantic analysis.
We believe that the annotation task is quite feasible.
If weestimate the time to create a gold standard basic TMR for a 25-word sentence takesone person-hour, counting the estimated time for acquiring the missing lexicon andontology information, then it should be possible to create a 100,000-word corpus ofgold standard basic TMRs in about two person-years.ReferencesMcShane, M., S. Beale, and S. Nirenburg (2004).
Some meaning procedures of onto-logical semantics.
In Proceedings of LREC-2004.McShane, M., S. Nirenburg, and S. Beale (2005a).
An NLP lexicon as a largelylanguage independent resource.
Machine Translation 19(2), 139?173.McShane, M., S. Nirenburg, and S. Beale (2005b).
Text-meaning representations asrepositories of structured knowledge.
In Proceedings of the Fourth Workshop onTreebanks and Linguistic Theories (TLT 2005).McShane, M., S. Nirenburg, S. Beale, B. Jarrell, and G. Fantry (2007).
Knowledge-based modeling and simulation of diseases with highly differentiated clinical man-ifestations.
In Proceedings of the 11th Conference on Artificial Intelligence inMedicine (AIME 07).Nirenburg, S., S. Beale, and M. McShane (2004).
Evaluating the performance of theOntoSem semantic analyzer.
In Proceedings of the ACLWorkshop on Text MeaningRepresentation.Nirenburg, S. and V. Raskin (2004).
Ontological Semantics.
MIT Press.Schubert, L. (2002).
Can we derive general world knowledge from texts?
In Pro-ceedings of the HLT Conference.
