Multidocument Summarization via Information ExtractionMichael White and Tanya KorelskyCoGenTex, Inc.Ithaca, NYmike,tanya@cogentex.comClaire Cardie, Vincent Ng, David Pierce, andKiri WagstaffDepartment of Computer ScienceCornell University, Ithaca, NYcardie,yung,pierce,wkiri@cs.cornell.eduABSTRACTWe present and evaluate the initial version of RIPTIDES, asystem that combines information extraction, extraction-basedsummarization, and natural language generation to support user-directed multidocument summarization.1.
INTRODUCTIONAlthough recent years has seen increased and successful researchefforts in the areas of single-document summarization,multidocument summarization, and information extraction, veryfew investigations have explored the potential of mergingsummarization and information extraction techniques.
This paperpresents and evaluates the initial version of RIPTIDES, a systemthat combines information extraction (IE), extraction-basedsummarization, and natural language generation to support user-directed multidocument summarization.
(RIPTIDES stands forRapIdly Portable Translingual Information extraction andinteractive multiDocumEnt Summarization.)
Following [10], wehypothesize that IE-supported summarization will enable thegeneration of more accurate and targeted summaries in specificdomains than is possible with current domain-independenttechniques.In the sections below, we describe the initial implementation andevaluation of the RIPTIDES IE-supported summarization system.We conclude with a brief discussion of related and ongoing work.2.
SYSTEM DESIGNFigure 1 depicts the IE-supported summarization system.
Thesystem first requires that the user select (1) a set of documents inwhich to search for information, and (2) one or more scenariotemplates (extraction domains) to activate.
The user optionallyprovides filters and preferences on the scenario template slots,specifying what information s/he wants to be reported in thesummary.
RIPTIDES next applies its Information Extractionsubsystem to generate a database of extracted events for theselected domain and then invokes the Summarizer to generate anatural language summary of the extracted information subject tothe user?s constraints.
In the subsections below, we describe theIE system and the Summarizer in turn.2.1 IE SystemThe domain for the initial IE-supported summarization system andits evaluation is natural disasters.
Very briefly, a top-level naturaldisasters scenario template contains: document-level information(e.g.
docno, date-time); zero or more agent elements denotingeach person, group, and organization in the text; and zero ormore disaster elements.
Agent elements encode standardinformation for named entities (e.g.
name, position, geo-politicalunit).
For the most part, disaster elements also contain standardevent-related fields (e.g.
type, number, date, time, location,damage sub-elements).The final product of the RIPTIDES system, however, is not a setof scenario templates, but a user-directed multidocumentsummary.
This difference in goals influences a number oftemplate design issues.
First, disaster elements must distinguishdifferent reports or views of the same event from multiple sources.As a result, the system creates a separate disaster event for eachsuch account.
Disaster elements should also include the reportingagent, date, time, and location whenever possible.
In addition,damage elements (i.e.
human and physical effects) are bestgrouped according to the reporting event.
Finally, a slightbroadening of the IE task was necessary in that extracted text wasnot constrained to noun phrases.
In particular, adjectival andadverbial phrases that encode reporter confidence, and sentencesand clauses denoting relief effort progress appear beneficial forcreating informed summaries.
Figure 2 shows the scenariotemplate for one of 25 texts tracking the 1998 earthquake inAfghanistan (TDT2 Topic 89).
The texts were also manuallyannotated for noun phrase coreference; any phrase involved in acoreference relation appears underlined in the running text.The RIPTIDES system for the most part employs a traditional IEarchitecture [4].
In addition, we use an in-house implementationof the TIPSTER architecture [8] to manage all linguisticannotations.
A preprocessor first finds sentences and tokens.
Forsyntactic analysis, we currently use the Charniak [5] parser, whichcreates Penn Treebank-style parses [9] rather than the partialparses used in most IE systems.
Output from the parser isconverted automatically into TIPSTER parse and part-of-speechannotations, which are added to the set of linguistic annotationsfor the document.
The extraction phase of the system identifiesdomain-specific relations among relevant entities in the text.
Itrelies on Autoslog-XML, an XSLT implementation of theAutoslog-TS system [12], to acquire extraction patterns.Autoslog-XML is a weakly supervised learning system thatrequires two sets of texts for training ?
one set comprises textsrelevant to the domain of interest and the other, texts not relevantto the domain.
Based on these and a small set of extractionpattern templates, the system finds a ranked list of possibleextraction patterns, which a user then annotates with theappropriate extraction label (e.g.
victim).
Once acquired, thepatterns are applied to new documents to extract slot fillers for thedomain.
Selectional restrictions on allowable slot fillers areimplemented using WordNet [6] and BBN?s Identifinder [3]named entity component.
In the current version of the system, nocoreference resolution is attempted; instead, we rely on a verysimple set of heuristics to guide the creation of output templates.The disaster scenario templates extracted for each text areprovided as input to the summarization component along with alllinguistic annotations accrued in the IE phase.
No relief slots areincluded in the output at present, since there was insufficientannotated data to train a reliable sentence categorizer.2.2 The SummarizerIn order to include relief and other potentially relevantinformation not currently found in the scenario templates, theSummarizer extracts selected sentences from the input articles andadds them to the summaries generated from the scenariotemplates.
The extracted sentences are listed under the headingSelected News Excerpts, as shown in the two sample summariesappearing in Figures 3 and 4, and discussed further in Section2.2.5 below.2.2.1 Summarization StagesThe Summarizer produces each summary in three main stages.
Inthe first stage, the output templates are merged into an event-oriented structure, while keeping track of source information.
Themerge operation currently relies on simple heuristics to groupextracted facts that are comparable; for example, during this phasedamage reports are grouped according to whether they pertain tothe event as a whole, or instead to damage in the same particularlocation.
Heuristics are also used in this stage to determine themost relevant damage reports, taking into account specificity,recency and news source.
Towards the same objective but using amore surface-oriented means, simple word-overlap clustering isused to group sentences from different documents into clustersthat are likely to report similar content.
In the second stage, abase importance score is first assigned to each slot/sentence basedon a combination of document position, document recency andgroup/cluster membership.
The base importance scores are thenadjusted according to user-specified preferences and matchingscenariotemplatesA powerful earthquake struck Afghanistan on May30 at 11:25?DamageVOA (06/02/1998) estimated that 5,000 were killedby the earthquake, whereas AP (APW, 06/02/1998)instead reported ?Relief StatusCNN (06/02/1998): Food, water, medicineand other supplies have started to arrive.[?
]NLG ofsummarycontentselectionmulti-documenttemplatemergingtext collectionIESystemuser informationneedevent-orientedstructureevent-orientedstructure with slotimportance scoressummarySummarizerslot   fillerslot   fillerslot   fillerslot   filler...slot   fillerslot   fillerslot   fillerslot   filler...slot   fillerslot   fillerslot   fillerslot   filler...slot   fillerslot   fillerslot   fillerslot   filler...Figure 1.
RIPTIDES System Designcriteria.
The adjusted scores are used to select the most importantslots/sentences to include in the summary, subject to the user-specified word limit.
In the third and final stage, the summary isgenerated from the resulting content pool using a combination oftop-down, schema-like text building rules and surface-orientedrevisions.
The extracted sentences are simply listed in documentorder, grouped into blocks of adjacent sentences.2.2.2 Specificity of Numeric EstimatesIn order to intelligently merge and summarize scenario templates,we found it necessary to explicitly handle numeric estimates ofvarying specificity.
While we did find specific numbers (such as3,000) in some damage estimates, we also found cases with nonumber phrase at all (e.g.
entire villages).
In between theseextremes, we found vague estimates (thousands) and ranges ofnumbers (anywhere from 2,000 to 5,000).
We also found phrasesthat cannot be easily compared (more than half the region?sresidents).To merge related damage information, we first calculate thenumeric specificity of the estimate as one of the values NONE,VAGUE, RANGE, SPECIFIC, or INCOMPARABLE, based on the presenceof a small set of trigger words and phrases (e.g.
several, as manyas, from ?
to).
Next, we identify the most specific currentestimates by news source, where a later estimate is considered toupdate an earlier estimate if it is at least as specific.
Finally, wedetermine two types of derived information units, namely (1) theminimum and maximum estimates across the news sources, and(2) any intermediate estimates that are lower than the maximumestimate.1In the content determination stage, scores are assigned to thederived information units based on the maximum score of theunderlying units.
In the summary generation stage, a handful oftext planning rules are used to organize the text for these derivedunits, highlighting agreement and disagreement across sources.2.2.3 Improving the Coherence of ExtractedSentencesIn our initial attempt to include extracted sentences, we simplychose the top ranking sentences that would fit within the wordlimit, subject to the constraint that no more than one sentence percluster could be chosen, in order to help avoid redundancy.
Wefound that this approach often yielded summaries with very poorcoherence, as many of the included sentences were difficult tomake sense of in isolation.To improve the coherence of the extracted sentences, we haveexperimented with trying to boost coherence by favoringsentences in the context of the highest-ranking sentences overthose with lower ranking scores, following the hypothesis that it isbetter to cover fewer topics in more depth than to change topicsexcessively.
In particular, we assign a score to a set of sentencesby summing the base scores plus increasing coherence boosts foradjacent sentences, sentences that precede ones with an initial1Less specific estimates such as ?hundreds?
are considered lowerthan more specific numbers such as ?5000?
when they are lowerby more than a factor of 10.Document no.
: ABC19980530.1830.0342Date/time: 05/30/1998 18:35:42.49Disaster Type: earthquake?location: Afghanistan?date: today?magnitude: 6.9?magnitude-confidence: high?epicenter: a remote part of the country?damage:human-effect:victim: Thousands of peoplenumber: Thousandsoutcome: deadconfidence: mediumconfidence-marker: fearedphysical-effect:object: entire villagesoutcome: damagedconfidence: mediumconfidence-marker: Details now hard tocome by / reports sayPAKISTAN MAY BE PREPARINGFOR ANOTHER TESTThousands of people are feared dead following... (voice-over) ...a powerful earthquake that hit Afghanistan today.The quake registered 6.9 on the Richter scale, centered ina remote part of the country.
(on camera) Details nowhard to come by, but reports say entire villages wereburied by the quake.Figure 2.
Example scenario template for the natural disasters domainEarthquake strikes quake-devastated villages innorthern AfghanistanA earthquake struck quake-devastated villages in northernAfghanistan Saturday.
The earthquake had a magnitude of 6.9on the Richter scale on the Richter scale.DamageEstimates of the death toll varied.
CNN (06/02/1998) providedthe highest estimate of 4,000 dead, whereas ABC(06/01/1998) gave the lowest estimate of 140 dead.In capital: Estimates of the number injured varied.Selected News ExcerptsCNN (06/01/98):Thousands are dead and thousands more are still missing.
Redcross officials say the first priority is the injured.
Gettingmedicine to them is difficult due to the remoteness of thevillages affected by the quake.PRI (06/01/98):We spoke to the head of the international red cross there, BobMcCaro on a satellite phone link.
He says it?s difficult toknow the full extent of the damage because the region is soremote.
There?s very little infrastructure.PRI (06/01/98):Bob McCaro is the head of the international red cross in theneighboring country of Pakistan.
He?s been speaking to usfrom there on the line.APW (06/02/98):The United Nations, the Red Cross and other agencies havethree borrowed helicopters to deliver medical aid.Figure 4.
200 word summary of actual IE output, withemphasis on Red Crosspronoun, and sentences that preceded ones with stronglyconnecting discourse markers such as however, nevertheless, etc.We have also softened the constraint on multiple sampling fromthe same cluster, making use of a redundancy penalty in suchcases.
We then perform a randomized local search for a good setof sentences according to these scoring criteria.2.2.4 ImplementationThe Summarizer is implemented using the Apacheimplementation of XSLT [1] and CoGenTex?s ExemplarsFramework [13].
The Apache XSLT implementation hasprovided a convenient way to rapidly develop a prototypeimplementation of the first two processing stages using a series ofXML transformations.
In the first step of the third summarygeneration stage, the text building component of the ExemplarsFramework constructs a ?rough draft?
of the summary text.
Inthis rough draft version, XML markup is used to partially encodethe rhetorical, referential, semantic and morpho-syntactic structureof the text.
In the second generation step, the Exemplars textpolishing component makes use of this markup to trigger surface-Earthquake strikes AfghanistanA powerful earthquake struck Afghanistan last Saturday at11:25.
The earthquake was centered in a remote part of thecountry and had a magnitude of 6.9 on the Richter scale.DamageEstimates of the death toll varied.
VOA (06/02/1998)provided the highest estimate of 5,000 dead.
CNN(05/31/1998) and CNN (06/02/1998) supplied lower estimatesof 3,000 and up to 4,000 dead, whereas APW (06/02/1998)gave the lowest estimate of anywhere from 2,000 to 5,000dead.
People were injured, while thousands more weremissing.
Thousands were homeless.Quake-devastated villages were damaged.
Estimates of thenumber of villages destroyed varied.
CNN (05/31/1998)provided the highest estimate of 50 destroyed, whereas VOA(06/04/1998) gave the lowest estimate of at least 25 destroyed.In Afghanistan, thousands of people were killed.Further DetailsHeavy after shocks shook northern afghanistan.
More homeswere destroyed.
More villages were damaged.Landslides or mud slides hit the area.Another massive quake struck the same region three monthsearlier.
Some 2,300 victims were injured.Selected News ExcerptsABC (05/30/98):PAKISTAN MAY BE PREPARING FOR ANOTHER TESTThousands of people are feared dead following...ABC (06/01/98):RESCUE WORKERS CHALLENGED IN AFGHANISTANThere has been serious death and devastation overseas.
InAfghanistan...CNN (06/02/98):Food, water, medicine and other supplies have started toarrive.
But a U.N. relief coordinator says it?s a "scenario fromhell".Figure 3.
200 word summary of simulated IE output, withemphasis on damageoriented revision rules that smooth the text into a more polishedform.
A distinguishing feature of our text polishing approach isthe use of a bootstrapping tool to partially automate theacquisition of application-specific revision rules from examples.2.2.5 Sample SummariesFigures 3 and 4 show two sample summaries that were included inour evaluation (see Section 3 for details).
The summary in Figure3 was generated from simulated output of the IE system, withpreference given to damage information; the summary in Figure 4was generated from the actual output of the current IE system,with preference given to information including the words RedCross.While the summary in Figure 3 does a reasonable job of reportingthe various current estimates of the death toll, the estimates of thedeath toll shown in Figure 4 are less accurate, because the IEsystem failed to extract some reports, and the Summarizer failedto correctly merge others.
In particular, note that the lowestestimate of 140 dead attributed to ABC is actually a report aboutthe number of school children killed in a particular town.
Sinceno location was given for this estimate by the IE system, theSummarizer?s simple heuristic for localized damaged reports ?namely, to consider a damage report to be localized if a location isgiven that is not in the same sentence as the initial disasterdescription ?
did not work here.
The summary in Figure 3 alsosuffered from some problems with merging:  the inclusion of aparagraph about thousands killed in Afghanistan is due to anincorrect classification of this report as a localized one (owing toan error in sentence boundary detection), and the discussion of thenumber of villages damaged should have included a report of atleast 80 towns or villages damaged.Besides the problems related to slot extraction and mergingmentioned above, the summaries shown in Figures 3 and 4 sufferfrom relatively poor fluency.
In particular, the summaries couldbenefit from better use of descriptive terms from the originalarticles, as well as better methods of sentence combination andrhetorical structuring.
Nevertheless, as will be discussed furtherin Section 4, we suggest that the summaries show the potential forour techniques to intelligently combine information from manyarticles on the same natural disaster.3.
EVALUATION AND INITIAL RESULTSTo evaluate the initial version of the IE-supported summarizationsystem, we used Topic 89 from the TDT2 collection ?
25 textson the 1998 Afghanistan earthquake.
Each document wasannotated manually with the natural disaster scenario templatesthat comprise the desired output of the IE system.
In addition,treebank-style syntactic structure annotations were addedautomatically using the Charniak parser.
Finally, MUC-stylenoun phrase coreference annotations were supplied manually.
Allannotations are in XML.
The manual and automatic annotationswere automatically merged, leading to inaccurate annotationextents in some cases.Next, the Topic 89 texts were split into a development corpus anda test corpus.
The development corpus was used to build thesummarization system; the evaluation summaries were generatedfrom the test corpus.
We report on three different variants of theRIPTIDES system here: in the first variant (RIPTIDES-SIM1), anearlier version of the Summarizer uses the simulated output of theIE system as its input, including the relief annotations; in thesecond variant (RIPTIDES-SIM2), the current version of theSummarizer uses the simulated output of the IE system, withoutthe relief annotations; and in the third variant (RIPTIDES-IE), theSummarizer uses the actual output of the IE system as its input.2Summaries generated by the RIPTIDES variants were comparedto a Baseline system consisting of a simple, sentence-extractionmultidocument summarizer relying only on document position,recency, and word overlap clustering.
(As explained in theprevious section, we have found that word overlap clusteringprovides a bare bones way to help determine what information isrepeated in multiple articles, thereby indicating importance to thedocument set as a whole, as well as to help reduce redundancy inthe resulting summaries.)
In addition, the RIPTIDES andBaseline system summaries were compared against the summariesof two human authors.
All of the summaries were graded withrespect to content, organization, and readability on an A-F scaleby three graduate students, all of whom were unfamiliar with thisproject.
Note that the grades for RIPTIDES-SIM1, the Baselinesystem, and the two human authors were assigned during a firstevaluation in October, 2000, whereas the grades for RIPTIDES-SIM2 and RIPTIDES-IE were assigned by the same graders in anupdate to this evaluation in April, 2001.Each system and author was asked to generate four summaries ofdifferent lengths and emphases: (1) a 100-word summary of theMay 30 and May 31 articles; (2) a 400-word summary of all testarticles, emphasizing specific, factual information; (3) a 200-wordsummary of all test articles, focusing on the damage caused by thequake, and excluding information about relief efforts, and (4) a200-word summary of all test articles, focusing on the reliefefforts, and highlighting the Red Cross?s role in these efforts.The results are shown in Tables 1 and 2.
Table 1 provides theoverall grade for each system or author averaged across all gradersand summaries, where each assigned grade has first beenconverted to a number (with A=4.0 and F=0.0) and the averageconverted back to a letter grade.
Table 2 shows the mean andstandard deviations of the overall, content, organization, andreadability scores for the RIPTIDES and the Baseline systemsaveraged across all graders and summaries.
Where the differencesvs.
the Baseline system are significant according to the t-test, thep-values are shown.Given the amount of development effort that has gone into thesystem to date, we were not surprised that the RIPTIDES variantsfared poorly when compared against the manually writtensummaries, with RIPTIDES-SIM2 receiving an average grade ofC, vs. A- and B+ for the human authors.
Nevertheless, we werepleased to find that RIPTIDES-SIM2 scored a full grade ahead ofthe Baseline summarizer, which received a D, and that2Note that since the summarizers for the second and third variantsdid not have access to the relief sentence categorizations, wedecided to exclude from their input the two articles (onetraining, one test) classified by TDT2 Topic 89 as onlycontaining brief mentions of the event of interest, as otherwisethey would have no means of excluding the largely irrelevantmaterial in these documents.RIPTIDES-IE managed a slightly higher grade of D+, despite theimmature state of the IE system.
As Table 2 shows, thedifferences in the overall scores were significant for all threeRIPTIDES variants, as were the scores for organization andreadability, though not for content in the cases of RIPTIDES-SIM1 and RIPTIDES-IE.4.
RELATED AND ONGOING WORKThe RIPTIDES system is most similar to the SUMMONS systemof Radev and McKeown [10], which summarized the results ofMUC-4 IE systems in the terrorism domain.
As a pioneeringeffort, the SUMMONS system was the first to suggest thepotential of combining IE with NLG in a summarization system,though no evaluation was performed.
In comparison toSUMMONS, RIPTIDES appears to be designed to morecompletely summarize larger input document sets, since it focusesmore on finding the most relevant current information, and sinceit includes extracted sentences to round out the summaries.Another important difference is that SUMMONS sidestepped theproblem of comparing reported numbers of varying specificity(e.g.
several thousand vs. anywhere from 2000 to 5000 vs. up to4000 vs. 5000), whereas we have implemented rules for doing so.Finally, we have begun to address some of the difficult issues thatarise in merging information from multiple documents into acoherent event-oriented view, though considerable challengesremain to be addressed in this area.The sentence extraction part of the RIPTIDES system is similar tothe domain-independent multidocument summarizers of Goldsteinet al [7] and Radev et al [11] in the way it clusters sentencesacross documents to help determine which sentences are central tothe collection, as well as to reduce redundancy amongst sentencesincluded in the summary.
It is simpler than these systems insofaras it does not make use of comparisons to the centroid of thedocument set.
As pointed out in [2], it is difficult in general formultidocument summarizers to produce coherent summaries,since it is less straightforward to rely on the order of sentences inthe underlying documents than in the case of single-documentsummarization.
Having also noted this problem, we have focusedour efforts in this area on attempting to balance coherence andinformativeness in selecting sets of sentences to include in thesummary.In ongoing work, we are investigating techniques for improvingmerging accuracy and summary fluency in the context ofsummarizing the more than 150 news articles we have collectedfrom the web about each of the recent earthquakes in CentralAmerica and India (January, 2001).
We also plan to investigateusing tables and hypertext drill-down as a means to help the userverify the accuracy of the summarized information.By perusing the web collections mentioned above, we can see thattrying to manually extricate the latest damage estimates from 150+news articles from multiple sources on the same natural disasterwould be very tedious.
Although estimates do usually converge,they often change rapidly at first, and then are gradually droppedfrom later articles, and thus simply looking at the latest article isnot satisfactory.
While significant challenges remain, we suggestthat our initial system development and evaluation shows that ourapproach has the potential to accurately summarize damageestimates, as well as identify other key story items using shallowertechniques, and thereby help alleviate information overload inspecific domains.5.
ACKNOWLEDGMENTSWe thank Daryl McCullough for implementing the coherenceboosting randomized local search, and we thank Ted Caldwell,Daryl McCullough, Corien Bakermans, Elizabeth Conrey,Purnima Menon and Betsy Vick for their participation as authorsand graders.
This work has been partially supported by DARPATIDES contract no.
N66001-00-C-8009.6.
REFERENCES[1] The Apache XML Project.
2001.
?Xalan Java.
?http://xml.apache.org/.Table 1Baseline RIPTIDES-SIM1 RIPTIDES-SIM2 RIPTIDES-IE Person 1 Person 2D C/C- C D+ A- B+Table 2Baseline RIPTIDES-SIM1 RIPTIDES-SIM2 RIPTIDES-IEOverall 0.96 +/- 0.37 1.86 +/- 0.56 (p=.005) 2.1 +/- 0.59 (p=.005) 1.21 +/- 0.46 (p=.05)Content 1.44 +/- 1.0 1.78 +/- 0.68 2.2 +/- 0.65 (p=.005) 1.18 +/- 0.6Organization 0.64 +/- 0.46 2.48 +/- 0.56 (p=.005) 2.08 +/- 0.77 (p=.005) 1.08 +/- 0.65 (p=.05)Readability 0.75 +/- 0.6 1.58 +/- 0.61 (p=.005) 2.05 +/- 0.65 (p=.005) 1.18 +/- 0.62 (p=.05)[2] Barzilay, R., Elhadad, N. and McKeown, K.
2001.?Sentence Ordering in Multidocument Summarization.?
InProceedings of HLT 2001.
[3] Bikel, D., Schwartz, R. and Weischedel, R.  1999.
?AnAlgorithm that Learns What's in a Name.?
MachineLearning 34:1-3, 211-231.
[4] Cardie, C. 1997.
?Empirical Methods in InformationExtraction.?
AI Magazine 18(4): 65-79.
[5] Charniak, E.  1999.
?A maximum-entropy-inspired parser.
?Brown University Technical Report CS99-12.
[6] Fellbaum, C.  1998.
WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, MA.
[7] Goldstein, J., Mittal, V., Carbonell, J. and Kantrowitz, M.2000.
?Multi-document summarization by sentenceextraction.?
In Proceedings of the ANLP/NAACL Workshopon Automatic Summarization, Seattle, WA.
[8] Grishman, R.  1996.
?TIPSTER Architecture DesignDocument Version 2.2.?
DARPA, available athttp://www.tipster.org/.
[9] Marcus, M., Marcinkiewicz, M. and Santorini, B.
1993.?Building a Large, Annotated Corpus of English: The PennTreebank.?
Computational Linguistics 19:2, 313-330.
[10] Radev, D. R. and McKeown, K. R.  1998.
?Generatingnatural language summaries from multiple on-line sources.
?Computational Linguistics 24(3):469-500.
[11] Radev, D. R., Jing, H. and Budzikowska, M.
2000.?Summarization of multiple documents: clustering, sentenceextraction, and evaluation.?
In Proceedings of theANLP/NAACL Workshop on Summarization, Seattle, WA.
[12] Riloff, E.  1996.
?Automatically Generating ExtractionPatterns from Untagged Text.?
In Proceedings of theThirteenth National Conference on Artificial Intelligence,Portland, OR, 1044-1049.
AAAI Press / MIT Press.
[13] White, M. and Caldwell, T.  1998.
?EXEMPLARS: APractical, Extensible Framework for Dynamic TextGeneration.?
In Proceedings of the Ninth InternationalWorkshop on Natural Language Generation, Niagara-on-the-Lake, Canada, 266-275.
