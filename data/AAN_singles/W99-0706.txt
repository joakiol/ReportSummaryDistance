Learning Transformation Rules to Find GrammaticalRelations*Lisa Ferro and Marc Vilain and Alexander YehAbstractGrammatical relationships are an important level ofnatural language processing.
We present a trainableapproach to find these relationships through transfor-mation sequences and-error-driven learning.
Our ap-proach finds grammatical relationships between coresyntax groups and bypasses much of the parsing phase.On our training and test set, our procedure achieves63.6% recall and 77.3% precision (f-score = 69.8).IntroductionAn important level of natural language process-ing is the finding of grammatical relationships uchas subject, object, modifier, etc.
Such relation-ships are the objects of study in relational grammar\[Perlmutter, 1983\].
Many systems (e.g., the KERNELsystem \[Palmer et al, 1993\]) use these relationships asan intermediate, form when determining the semanticsof syntactically parsed text.
In the SPARKLE project\[Carroll et al, 1997@ grammatical relations form thelayer above the phrasal-level in a three layer syntaxscheme.
Grammatical relationships are often stored insome type of structure like the F-structures of lexical-functional grammar \[Kaplan, 1994\].Our own interest in grammatical relations is as a se-mantic basis for information extraction in the Alembicsystem.
The extraction approach we are currently in-vestigating exploits grammatical relations as an inter-mediary between surface syntactic phrases and proposi-tional semantic interpretations.
By directly associatingsyntactic heads with their arguments and modifiers, weare hoping that these grammatical relations will providea high degree of generality and reliability to the processof composing semantic representations.
This ability to?
This paper eports on work performed at the MITRECorporation under the support of the MITRE SponsoredResearch Program.
Helpful assistance has been given by"Yuval Krymolowski, Lynette Hirschman and an anonymousreviewer.
Copyright (~)1999 The MITRE Corporation.
Allrights reserved.The MITRE Corporation202 Burlington Rd.Bedford, MA 01730USA{lferro,mbv,asy}@mitre.org"parse" into a semantic representation is according toCharniak \[Charniak, 1997, p. 42\], "the most importanttask to be tackled now.
"In this paper, we describe a system to learn rulesfor finding grammatical relationships when just givena partial parse with entities like names, core noun m~dverb phrases (noun and verb groups) and semi-accurateestimates of the attachments of prepositions and subor-dinate conjunctions.
In our system, the different enti-ties, attachments and relationships are found using rulesequence processors that are cascaded together.
Eachprocessor can be thought of as approximating some as-pect of the underlying rammar by finite-state trans-duction.We present the problem scope of interest to us, as wellas the data annotations required to support our investi-gation.
We also present a decision procedure for findinggrammatical relationships.
In brief, on our training midtest set, our procedure achieves 63.6% recall and 77.3%precision, for an f-score of 69.8.Phrase Structure and GrammaticalRelationsIn standard erivational approaches to syntax, start-ing as early as 1965 \[Chomsky, 1965\], the notion ofgrammatical relationship is typically parasitic on thatof phrase structure.
That is to say, the primm'y vehiclesof syntactic analysis are phrase structure trees: gram-matical relationships, if they are to be considered atall.
are given as a secondary analysis defined in termsof phrase structure.
The surface subject of a sentence,for example, is thus no more than the NP attached bythe production S -+ NP VP; i.e., it is the left-most NPdaughter of an S node.The present paper takes an alternate outlook.
In ourcurrent work, grammatical relationships play a centralrole, to tile extent even of replacing phrase structureas the descriptive vehicle for many syntactic phenom-ena.
To be specific, our approach to syntax operatesat two levels: (1) that of core phrases, which are an-43IIIIIIIIIIIIIIIIIlIalyzed through standard erivational syntax, and (2)that of argument and modifier attachments, which areanalyzed through grammatical relations.
These twolevels roughly correspond to the top and bottom lay-ers of the three layer syntax annotation scheme in theSPARKLE project \[Carroll et al, 1997a\].Core syntactic phrasesIn recent years, a consensus of sorts has emergedthat postulates some core level of phrase analy-sis.
By this we mean the kind of non-recursivesimplifications of the NP  and VP  that in the lit-erature go by names such as noun/verb groups\[Appelt et at., 1993\],.
chunks \[Abney, 1996\], or baseNPs \[Ramshaw and Marcus, 1995\].The common thread between these approaches andours is to approximate full noun phrases or verb phrasesby only parsing their non-recursive core, and thus notattaching modifiers or arguments.
For English nounphrases, this amounts to roughly the span betweenthe determiner and the head noun; for English verbphrases, the span runs roughly from the auxiliary to thehead verb.
We call such simplified syntactic categoriesgroups, and consider in particular, noun, verb, adverb,adjective, and IN  groups, i An IN  group 2 contains apreposition or subordinate conjunction (including wh-words and "that").For example, for "I saw the cat that ran.
", we havethe following core phrase analysis:\[I\],,g \[saw\]vg \[the cat\]ng \[that\], 9 \[ran\]rvwhere \[...\]-9 indicates a noun group, \[.--\]09 a verb group,and (...\],,j an IN group.In English and other languages where core phrases(groups) can be analyzed by head-out (island-like) pars-ing, the group head-words are basically a by-product ofthe core phrase analysis.Distinguishing core syntax groups from traditionalsyntactic phrases (such as NPs) is of interest because itsingles out what is usually thought of as easy to parse,and allows that piece of the parsing problem to be ad-dressed by such comparatively simple means as finite-state machines or transformation sequences.
What isthen left of the parsing problem is the difficult stuff:namely the attachment of prepositional phrases, rela-tive clanses, and other constructs that serve in modifi-cation, adjunctive, or argument-passing roles.ZIn addition, for the noun group, our definition encom-passes the named entity task, familiar from information ex-traction \[Def, 1995\].
Named entities include among othersthe names of people, places, and organizations, as well asdates, expressions of money, and (in an idiosyncratic exten-sion) titles, job descriptions, and honorifics.
"The name comes from the Penn Treebank part-of:speechlabel for prepositions and subordinate conjunctions.Grammat ica l  re la t ionsIn the present work, we encode this hard stuff througha small repertoire of grammatical relations.
These re-lations hold directly between constituents, and as suchdefine a graph, with core constituents as nodes in thegraph, and relations as labeled arcs.
Our previous ex-ample, for instance, generates the following grammati-cal relations graph (head words underlined):SUBJect\[ll \[saw\] \[the cat\] \[that I \[ran___\]t IMODifierOur grammatical relations effectively replace the re-cursive X analysis of traditional phrase structure ga'am-mar.
In this respect, the approach bears resemblanceto a dependency grammar, in that it has no notion ofa spanning S node, or of intermediate constituents cor-responding to argument and modifier attachments.One major point of departure from dependency gram-mar, however, is that these grammatical relation graphscan generally not be reduced to labeled trees.
This hap-pens as a result of argument passing, as in\[F ed\] fpromise \] \[to help/\[John\]where \[Fred\] is both the subject of \[promised\] and.-\[tohelp/.
This also happens as a result of argument-modifier cycles, as in/If \[saw\] \[the cat\]/that\]/ran\]where the relationships between \[the cat\] and \[ran\] forma cycle: \[the cat\] has a subject relationship/dependencyto \[ran\], and \[ran\] has a modifier dependency to\[the cat\], since \[ran\] helps indicate (modifies) which catis seen.There has been some work at making additions toextract grammatical relationships from a dependencytree structure \[BrSker, 1998, Lai and Huang, 1998\] sothat one first produces a surface structure dependencytree with a syntactic parse and then extracts grammat-ical relationships from that tree.
In contrast, we skiptrying to find a surface structure tree and just proceedto more directly finding the grammatical relationships,which are the relationships of interest o us.A reason for skipping the tree stage is that extractinggrammatical relations from a surface structure tree isoften a nontrivial task by itself.
For instance, the pre-cise relationship holding between two constituents in asurface structure tree cannot be derived unambiguouslyfrom their relative attachments.
Contrast.
for example"the attack on the military base" with "the attack onMarch 24".
Both of these have the same underlyingsurface structure (a PP attached to an NP).
but the44former encodes the direct object of a verb nominaliza-tion, while the latter encodes a time modifier.
Also,in a surface structure tree, long-distance dependenciesbetween heads and arguments are not explicitly indi-cated by attachments between the appropriate parts ofthe text.
For instance in "Fred promised to help John",no direct attachment exists between the "Fred" in thetext and the "help" in the text, despite the fact thatthe former is the subject of the latter.For our purposes, we have delineated approximatelya dozen head-to-argument relationships as well as acommensurate number of modification relationships.Among the head-to-argument relationships, we have thedeep subject and object (SUBJ and OBJ respectively),and also include the surface subject and object of cop-ulas (COP-SUBJ and the various COP-OBJ forms).
Inaddition, we include a number of relationships (e.g.,PP-SUBJ, PP-OBJ) for arguments that are mediatedby prepositional phrases.
An example is inP P - ~  OBJectI\[the attack\] \[on\] \[the military base\]where \[the attack\], a noun group with a verb nominal-ization, has its object \[the military base\]passed to it viathe preposition in \[on\].
Among modifier elationships,we designate both generic modification and some spe-cializations like locational and temporal modification.A complete definition of all the grammatical relations isbeyond the scope of thi?
paper, but we give a summaryof usage in Table 1.
An earlier version of the definitionscan be found in our annotation guidelines \[Ferro, 1998\].The appendix shows some examples of grammatical re-lationship labeling from our experiments.Our set of relationships is similar to the setused in the SPARKLE project \[Carroll et al, 1997a\]\[Carroll et al, 1998a I.
One difference is that we makemany semantically-based distinctions between whatSPARKLE calls a modifier, such as time and locationmodifier% and the various arguments of event nouns.Semant ic  in terpreta t ionA major motivation for this approach is that it sup-ports a direct mapping into semantic interpretations.In our framework, semantic interpretations are givenin a neo-Davidsonian 'propositional logic.
Grammati-cal relations are thus interpreted in terms of mappingsand relationships between the constants and variablesof the propositional language.
For instance, the deepsubject relation (SUB J) maps to the first position of apredicate's argument list, the deep object {OBJ) to thesecond such position, and so forth.Our example sentence, "I saw the cat that ran" thustranslates directly to the following:45Propositionsaw(xl x2)I(xl)cat(x2)ran(x2) =e3rood(e3 x2)CommentSUBJ and OBJ' relationsSUBJ relation(e3 is the event variable)MOD relationWe do not have an explicit level for clauses betweenour core phrase and grammatical relations levels.
How-ever, we do have a set of implicit clauses in that eachverb (event) and its arguments can be (teemed a baselevel clause.
In our example "I saw the cat that ran".
wehave two such base level clauses.
"saw" and its argu-ments form the clause "I saw the cat".
"ran" and its ar-gument form the clause "the cat ran".
Each noun with apossible semantic lass of "act" or "process" in Wordnet\[Miller, 1990\] (and that noun's arguments) can likewisebe deemed a base level clause.The  Process ing  Mode lOur system uses transformation-based rror-drivenlearning to automatically earn rules from training ex-amples \[Brill and Resnik, 1994\].One first runs the system on a training set, whichstarts with no grammatical relations marked.
Thistraining run moves in iterations, with each iterationproducing the next rule that yields the best bet gainin the training set (number of matching relationshipsfound minus the number of spurious relationships intro-duced).
On ties, rules with less conditions are favoredover rules with more conditions.
The training run endswhen tile next rule found produces a net gain below agiven threshold.The rules are then run in the same order on the testset to see how well they do.The rules are condition~action pairs that are triedon each syntax group.
The actions in our system arelimited to attaching (or unattaching) a relationship ofa particular type from the group under consideration tothat group's neighbor a certain number of groups awayin a particular direction (left or right).
A sample actionwould be to attach a SUBJ relation from the groupunder consideration to the group two groups away tothe right.A rule only applies to a syntax group when that groupand its neighbors meet the rule's conditions.
Each con-dition tests the group in a particular position relativeto the group under consideration (e.g., two groups awayto the left).
All tests can be negated.
Table 2 showsthe possible tests.A sample rule is when a noun group n's?
immediate group to tile right has some form of theverb "be" as the head-word.IIIIIIIIIIIIIIIIIIIRELATION " EXAMPLE(s) in the formatName I Description , \[source\] --~ \[target\] in "text"subj \[I\] --+objloc-objsubjectsubject of a verb- -  link a copula subject and object- -  link a state with the item in that state- -  link a place with the item movingto or from that placeobject-- object of a verbobject of an adjective- -  surface subject in passives- -  object of a preposition,not for partitives or subsets-- object ofan adverbial clause complementizerlocation object-link a movement verb with a placewhere entities are moving to or fromindobj i indirect objectempty use instead of "subj" relation when subjectis an expletive (existential) "it" or "there"pp-subj genitive functional "of"'suse instead of "subj" relation when thesubject is linked via a preposition,links preposition to its headpp-obj nongenitive functional "of"'suse in place of "obj" relation when theobject is linked via a preposition,links preposition to its headipp-io use in place of "indobj" relation when theindirect object is linked via a preposition.links preposition to its headIcop-subj i surface subject for a copulan-cop-obj , surface nominative object for a copula\[promised\] in "I promised to help"\[I\] ~ \[to help\] in "I promised to help"\[the cat\] ---r \[ran\] in "the cat that ran"\[You\] -+ \[happy\] in "You are happy"\[You\] --r \[a runner\] in "You are a runner"\[you\] ---r \[happy\] in "They made you happy"\[I\] -~ \[home\] in "I went home"\[saw\] ~ \[the caq ill "I saw the cat"\[promised\] <--- \[to help\] ill "I promised to help you"\[happy\] <--- \[to help\] in "I was happy to help"\[I\] ~ \[was seen\] in "I was seen 1)y a cat"\[by\] ~ \[the tree\] in "I was by tile tree"\[After\] e-\[left\] in "After I left, I ate"\[wenq +- \[home\] i l l  "I went home"\[went\] ~ \[in\] ill "I went in the house\[gave\] <-- \[you\] in "I gave you a cake"\[There\] ~ \[trees\] in "There are trees"\[name\] e-- \[of\] in "name of the building"\[was seen\] +-- \[by\] in "I was seen by a cat"\[age\] e- \[o\]\] in "age of 12"\[the attack\] <--- \[on\] in "the attack on the base"\[.qave\] e- \[to\] in "gave a cake to thenf'\[You\] ~ \[are\] in "You are happy"\[is\] e- \[a rock\] in "It is a rock"| ip-cop-obj I surface predicate object for a copula i \[are\] e-- \[happy\] in "You are happy"subset subset \[five\] --4 \[the kids\] in "five of the kids"i i \[the cat\] ~-- \[ran\] in "the cat that ran" rood generic modifier (use whenmodifier does not fit in a case below)location modifiertime modifierpossessive modifierquantity modifier (partitive)identity modifier (names)mod-locrood-timemod-possmod-quantmod-identmod-scalm" scalar modifiero .\[ran\] +-- \[with\] in "I ran with new shoes"\[ate\] ?- \[at\] in "I ate at home"\[ate\] ?- \[at\] in "I ate at midnight"\[Yesterday\] --~ \[ate\] in "Yesterday, I ate"\[the cat\] --~ \[toy\] in "the cat's toy"\[hundreds\] --+ \[people\] in "hundreds of people"\[a cat\] ?-- \[Fuzzy\] in "a cat named Fuzzy"\[the winner\] +-- \[Pat Kay\]in "the winner, Pat Kay, is"\[16 years\] -+ \[ago\] in "16 years ago"Table 1: Summary of grammatical relationships46Test Ty.p.e Example, Sample Value(s)group type noun, verbverb group property passive, infinitival,unconjugated present participleend group in a sentence first, lastpp-attachment Is a preposition or subordinateconjunction attached to thegroup under consideration?group contains a particular lexeme or part-of-speechbetween two groups, there is a particular lexeme or part-of-speechgroup's head (main) word "cat"head word part-of-speech .... common plural nounhead word within a named entity p.erson, organizationhead word subcategorization a d complement categories intransitive verbs(from Comlex \[Wolff et al, 1995\], over 100 categories)head word semantic lasses process, communication(from Wordnet \[Miller, 1990\], 25 noun and 15 verb clas.ses)punctuation or coordinating conjunction exist between two groups?head word in a word list?
list of relative pronouns,list of partitive quantities (e.g., "some")Table 2: Possible tests?
immediate group to the left is not an IN group(preposition, wh-word, etc.)
and?
n's head-word is not an existential "there"make n a SUBJ of the group two groups over to n'sright.When applied to the group \[The at\] (head words areunderlined) in the sentence\[The ~ \[was\] \[very happy.\].this rule makes \[The cat\] a SUBject of \[very happy\].Searching over the space of possible rules is very com-putationally expensive.
Our system has features tomake it easier to perform searching in parallel and tominimize the amount of work that needs to be undoneonce a rule is selected.
With these features, rules that(un)attach different ypes of relationships or relation-ships at different distances can be searched indepen-dently of each other in parallel.One feature is that the action of any rule only affectsthe applicability of rules with either the exact same oropposite action.
For example, selecting and running arule which attaches a MOD relationship to the groupthat is two groups to the right only can affect the ap-plicability of other rules that either attach or unattacha MOD relationship to the group that is two groups tothe right.Another feature is the use of net gain as a prox.vnmasure during training.
The actual measure by whichwe judge the system's performance is called an/-score.This/-score is a type of harmonic mean of the precision(p) and recall (r) and is given by 2pr/ (p + r).
Unfor-tunately, this measure is nonlinear, mid the applicationof a new rule can alter the effects of all other possiblerules on the/-score.
To enable the described parallelsearch to take place, we need a measure in which howa rule affects that measure only depends on other ruleswith either the exact same or opposite action.
The netgain measure has this trait, so we use it as a proxy forthe/-score during training.Another way to increase the learniug speed is to re-strict the number of possible combinations of condi-tions/constraints or actions to search over.
Each ruleis automatically limited to only considering one typeof syntactic group.
Then when searching over possibleconditions to add to that rule, the system only needsto consider the parts-of-speech, semantic classes, etc.applicable to that type of group.Many other restrictions are possible.
One can esti-mate which restrictions to try by making some train-ing and test runs with preliminary data sets and seeingwhat restrictions seem to have no effect on performance,etc.
The restrictions used in our experiments are de-scribed below.47!I1IIIIIIIIII!IIIExper imentsThe Data  ~Our data consists of bodies of some elementary schoolreading comprehension tests.
For our purposes, thesetests have the advantage of having a fairly predictablesize (each body has about 100 relationships and syntaxgroups) and a consistent style of writing.
The tests arealso on a wide range of topics, so we avoid a narrowspecialized vocabulary.
Our training set has 1963 re-lationships (2153 syntax groups, 3299 words) and ourtest set has 748 relationships .
(830 syntax groups, 1151words).We prepared the data by first manually removingthe headers and the questions at the end for eachtest.
We then manually annotated the remainder fornamed entities, syntax groups and relationships.
Asthe system reads in our data, it automatically breaksthe data into lexemes and sentences, tags the lexemesfor part-of-speech and estimates the attachments ofprepositions and subordinate conjunctions.
The part-of-speech tagging uses a high-performance tagger basedon \[Brill, 1993\].
The attachment estimation uses a pro-cedure described in \[Yeh and Vilain, 1998\] when mul-tiple left attachment possibilities exist and four simplerules when no or only one left attachment possibilityexists.
Previous testing indicates that the estimationprocedure is about 75% accurate.Parameter Settings for TrainingAs described earlier, a training run uses many param-eter settings.
Examples include where to look for rela-tionships and to test conditions, the max imum numberof constraints allowed in a rule, etc.Based on the observation that 95% of the relation-ships are to at most three groups away in the trainingset, we decided to limit the search for relationships toat most three groups in length.
To keep the number ofpossible constraints down, we disallowed the negationsof most tests for the presence of a particular lexeme orlexeme stem.To help determine man)" of the settings, we madesome preliminary runs using different subsets of our fi-nal training set as the preliminary training and test sets.This kept the final test set unexamined during develop-ment.
From these preliminary runs, we decided to limita rule to at most three constraints 3 in order to keep thetraining time reasonable.
We found a number of limita-tions that help speed up training and seemed to have noeffect on the preliminary test runs.
A threshold of fourwas set to end a training run.
So training ends when itcan no longer find a rule that produces at least a net3In addition to the constraint on the relationship's sourcegroup type.48gain of four in the score.
Only syntax groups spannedby the relationship being attached or unattached andthose groups' immediate neighbors were allowed to bementioned in a rule's conditions.
Each condition test-ing a head-word had to test a head-word of a differentgroup.
Except for the lexemes "of", "?"
and a fewdeternfiners like "the", tests for single lexemes were re-moved.
Also disallowed were negations of tests for thepresence of a particular part-of-speech anywhere withina syntax group.In our preliminary runs, lowering the thresholdtended to raise recall and lower precision.The  ResultsTraining produced a sequence of 95 rules which had63.6% recall and 77.3% precision for an f-score of 69.8when run on the test set.
In our test set.
the key re-lationships, SUBJ  and OBJ, formed the bulk of therelationships (61%).
Both recall and precision for bothSUBJ  and OBJ  were above 70%, which pleased us.
Be-cause of their relative abundance in the test set, thesetwo relationships also had the most number of errors inabsolute terms.
Combined, the two accounted for 45%of the recall errors asld 66?,o of the precision errors.
Interms of percentages, recall was low for many of the lesscommon relationships, such as generic, time and loca-tion modification relationships.
In addition, the relativeprecision was low for those modification relationships.The appendix shows some examples of our system re-sponding to the test set.To see how well the rules, which were trained onreading comprehension test bodies: would carry overto other texts of non-specialized domains, we examineda set of six broadcast news stories.
This set had 525 re-lationships (585 syntax groups, 1129 words).
By somemeasures, this set was fairly similar to our training andtest sets.
In all three sets, 33-34% of the relationshipswere OBJ  and 26-28% were SUBJ.
The broadcast newsset did tend to have relationships between groups thatwere slightly further apart:Percent of Relations with LengthSet < 1 < 2 < 3training ..... 66% 87% 95%test 68% 89% 96%broadcast news 65% 84% 90%This tendency, plus differences in the relative propor-tions of various modification relationships are probablywhat produced the drop in results when we tested therules against his news set: recall at 54.6%, precision at70.5% (f-score at 61.6%).To estimate how fast the results would improve byadding more training data, we had the system learnrules on a new smaller training set and then testedagainst he regular test set.
Recall dropped to 57.8%,precision to 76.2%.
The smaller training set had 981relationships (50% of the original training set).
So dou-bling the training data here (going from the smaller tothe regular training set) reduced the smaller trainingset's recall error of 42.2% by 14% and the precision er-ror of 23.8% by 5%.
Using the broadcast news set as atest produced similar error reduction results.One complication of our current scoring scheme isthat identifying a modification relationship and mis-typing it is more harshly penalized than not findinga modification relationship at all.
For example, find-ing a modification relationship, but mistakingly callingit a generic modifier instead of a time modifier pro-duces both a missed key error (not finding a time mod-ifier) and a spurious response rror (responding with ageneric modifier where none exists).
Not finding thatmodification relationship at all just produces a missedkey error (not finding a time modifier).
This compli-cation, coupled with the fact that generic, time andlocation modifiers often have a similar surface appear-ance (all are often headed by a preposition or a comple-mentizer) may have been responsible for the low recalland precision scores for these types of modifiers.
Eventhe training scores for these types of modifiers wereparticularly low.
To test how well our system findsthese three types of modification when one does notcare about specifying the sub-type, we reran the origi-nal training and test with the three sub-types mergedinto one sub-type in the annotation.
With the merging,recall of these modification relationships jumped from27.8% to 48.9%.
Precision rose from 52.1% to 67.7%.Since these modification relationships are only about20% of all the relationships, the overall improvement ismore modest.
Recall rises to 67.7%, precision to 78.6%(f-score to 72.6).Taking this one step further, the LOC-OBJ and var-ious PP-x arguments also all have both a low recall(below 35%) in the test and a similar surface structureto that of generic, time and location modifiers.
Whenthese argument types were merged with the three modi-tier types into one combined type, their combined recallwas 60.4% and precision was 81.1%.
The correspondingoverall test recall and precision were 70.7% and 80.5%,respectively.Compar i son  w i th  Other  WorkAt one level, computing rammatical relationships canbe seen as a parsing task, and the question aturallyarises as to how well this approach compares to currentstate-of-the-art parsers.
Direct performance compar-isons, however, are elusive, since parsers are evaluatedon an incommensurate tr e bracketing task.
For exam-pie, the SPARKLE project \[Carroll et al, 1997a\] putstree bracketing and grammatical relations in two dif-ferent layers of syntax.
Even if we disregard the ques-tionable aspects of comparing tree bracketing applesto grammatical relation oranges, an additional compli-cation is the fact that our approach divides the pars-ing task into an easy piece (core phrase boundaries)and a hard one (grammatical relations).
The resultswe have presented here are given solely for this harderpart, which may explain why at roughly 70 points off-score, they are lower than those reported for currentstate-of-the-art parsers (e.g., Collins \[Collins, 1997\]).More comparable to our approach are sonde othergrammatical relation finders.
Some examples for En-glish include the English parser used in tide SPARKLEproject \[Briscoe t al., \] \[Carroll et al, 1997b\]\[Carroll et al, 1998b\] and the finder built with amemory-based approach \[Argamon et aI., 1998\].
Theserelation finders make use of large almotated trainingdata sets and/or manually generated grammars andrules.
Both techniques take much effort and time.
Atfirst glance both of these finders perform better thanour approach.
Except for the object precision score of77% in \[Argamon et al, 1998\], both finders have gram-matical relation recall and precision scores in the 80s.But a closer examination reveals that these results arenot quite comparable with ours.. Each system is recovering a different variation ofgrammatical relations.
As mentioned earlier, onedifference between us and the SPARKLE project isthat the latter ignores many of distinctions that wemake for different ypes of modifiers.
The systemin \[Argamon et al, 1998\] only finds a subset of thesurface subjects and objects..
In addition, the evaluations of these two findersproduced more complications.
In an illustration ofthe time consuming nature of annotating or i'eanno-tating a large corpus, the SPARKLE project orig-inally did not have time to annotate the Englishtest data for modifier elationships..ks a result, theSPARKLE English parser was originally not eval-uated on how well it found modifier relationships\[Carroll et al, 1997b\] [Carroll et al: 1998b\].
The re-ported results as of 1998 only apply to the argument(subject, object, etc.)
relationships.
Later on, a testcorpus with modifier elationship annotation was pro-duced.
Testing the parser against his corpus pro-duced generally lower results, with an overall recall,precision and f-score of 75% \[Carroll et al, 1999\].This is still better than our f-score of 70%, but notby nearly as much.
This comparison ignores the factthat tile results are for different versions of grammat-49ical relationships and for different est corpora.The figures given above were the original (1998) re-sults for the system in \[Argamon et al, 1998\], whichcame from training and testing on data derived fromthe Penn Treebank corpus \[Marcus et al, 1993\] inwhich the added null elements (like null subjects)were left in.
These null elements, which were givena -NONE- part-of-speech, do not appear in raw text.Later (1999 results), the system was re-evaluated onthe data with the added null elements removed.
Thesubject results declined a little.
The object results de-clined more, with the precision now lower than ours(73.6% versus 80.3%) and the f-score not much higher(80.6% versus 77.8%).
This comparison is also be-tween results with different est corpora and slightlydifferent notions of what an object is.Summary ,  D iscuss ion ,  and  Specu la t ionIn this paper, we have presented a system for find-ing grammatical relationships that operates on easy-to-find constructs like noun groups.
The approach isguided by a variety of knowledge sources, such as read-ily available lexica , and relies to some degree on well-understood computational infrastructure: a p-o-s tag-ger and an atta?hment procedure for preposition andsubordinate conjunctions.
In sample text, our systemachieves 63.6% recall and 77.3% precision (f-score =69.8) on our repertory of grammatical relationships.This work is admittedly still in relatively early stages.Our training and test corpora, for instance, are less-than-gargantuan compared to such collections as thePenn Treebank \[Marcus et al, 1993\].
However, the factthat we have obtained an f-score of 70 from such sparsetraining materials is encouraging.
The recent imple-mentation of rapid annotation tools should speed upfurther annotation of our own native corpus.Another task that awaits us is a careful measurementof interannotator agreement on our version the gram-matical relationships.We are also keenly interested in applying a widerrange of learning procedures to the task of identify-ing these grammatical relations.
Indeed, a fine-grainedanalysis of our development test data has identifiedsome recurring errors related to the rule sequence ap-proach.
A hypothesis for further experimentation isthat these errors might productively be addressed byrevisiting the way we exploit and learn rule sequences,or by some hybrid approach blending rules and statisti-cal computations.
In addition, since generic, time andlocation modifiers, and LOC-OBJ and various PP-x ar-guments often have a similar surface appearance, oneaResources to find a word's possible stem(s), semanticclass(es) and subcategorization category(ies).might first just try to locate all such entities and thenin a later phase try to classiC- them by type.Different applications will need to deal with differentstyles of text (e.g., journalistic text versus narratives)and different standards of grammatical relationships.An additional item of experimentation is to use our sys-tem to adapt other systems, including earlier versionsof our system, to these differing styles and standards.Like other Brill transformation rule sys-tems \[Brill and Resnik, 1994\], our system can take inthe output of another system and try to improve on it.This suggests a relatively low expense method to adapta hard-to-alter system that performs well on a slightlydifferent style or standard.
Our training al)proach ac-cepts as a starting point an initial labeling of the data.So fat', we have used an empty labeling.
However, oursystem could just as easily start from a labeling pro-duced as the output of the hard-to-alter system.
Thelearning would then not be reducing the error betweenan empty labeling and the key annotations, but betweenthe hard-to-alter system's output and the key anno-tations.
By using our system in this post-processingmanner, we could use a relatively small retraining setto adapt, for example, the SPARKLE English parser,to our standard of grammatical relationships withouthaving reengineer that parser.
Palmer \[Palmer, 1.997\]used a similar approach to improve on existing wordsegmenters for Chinese.
Trying this suggestion out isalso something for us to do.This discussion of training set size brings up perhapsthe most obvious possible improvement.
Namely, en-larging our very small training set.
As has been men-tioned, we have recently improved our annotation envi-ronment and look forward to working with nmre data.Clearly we have many experiments ahead of us.
Butwe believe that the results obtained so far are a promis-ing start, and the potential rewards of the al)proach arevery significant indeed.Append ix :  Examples  f rom Test  Resu l tsFigure 1 shows some example sentences from the testresults of our main experiment.
'~ :@ marks the relation-ship that our system missed.
* marks the relationshipthat our system wrongly hypothesized.
In these ex-amples, our system handled a number of phenomenacorrectly, including:?
The coordination conjunction of the objects\[cars\] and \[trucks\]5The material came from level 2 of "The 5 W's" writtenby" Linda Miller.
It is available from Remedia Publications,10135 E. Via Linda #D124.
Scottsdale.
AZ 85258.
USA.50SUBJ\[The ship\] \[was carrying\] \[oil\] \[for\] \[cars\] andt OBJ\[trucks\].ISUBJ OBJ\] ~ I\[That\] \[means\] \[the same word\] \[might have\] \[two r three spellings\].IDadOBJ\[He\] \[loves\] \[to work\] \[with\] \[words\].I t t I S UBJ PP- OBJ@SUBJII OBJ I OBJ I I SUBJ* 1\[A man\] \[named\] \[Noah\] \[wrote\] \[this book\].I ~ MODI MOD-IDENTIFigure 1: Example test responses from our system.
@ marks the missed key.
* marks the spurious response.?
The verb group \[might have\] being an object of an-other verb.?
The noun group \[He\] being the subject of two verbs.?
The relationships within the reduced relative clause\[A man\] \[named\] \[Noah\], which makes one noungroup a name or label for another noun group.Our system misses a PP-OBJ relationship, which is alow occurrence r lationship.
Our system also acciden-tally make both \[,4 man\] and \[Noah\] subjects of thegroup \[wrote\] when only the former should be.Re ferences\[Abney, 1996\] S. Abney.
Partial parsing via finite-statecascades.
In Proc.
of ESSLI96 Workshop on RobustParsing, 1996.\[Appelt et al.
1993\] D. Appelt, J. Hobbs, J. Bear.D.
Israel.
and M. Tyson.
Fastus: A finite-state pro-cessor for information extraction.
In 13th Intl.
Conf.On Artificial Intelligence (IJCAL93), 1993.51\[Argamon et hi., 1998\]- S. Argamon, I. Dagan, andY.
Krymolowski.
A memory-based approach to learn-ing shallow natural anguage patterns.
In COLING-ACL'98, pages 67-73, Montr6al, Canada, 1998.
Anexpanded 1999 version will appear in JETAI.\[Brill and Resnik, 1994\] E. Brill and P. R~snik.
A rule-based approach to prepositional phrase attachmentdisambiguation.
I  15th International Co,l?
on Com-putational Linguistics (COLING).
1994.\[Brill, 1993\] E. Brill.
A Co77~us-based App~vach to Lan-guage LeaT~ing.
PhD thesis.
U. Pennsylvania, 1993.\[Briscoe t al., \] T. Briscoe, J. Carroll.
G. Car-roll, S. Federici, G. Grefenstette, S. Montemagni,V.
Pirrelli, I. Prodanof, M. Rooth, and M. Van-nocchi.
Phrasal parser software - deliverable3.1.
Available at h t tp : / /w~.
i l c .p i .
cn I - .
i t / -spark le /spark le ,  htm.\[Brbker, 1998\] N. Brbker.
Separating surface orderand syntactic relations in a dependency gram-mar.
In COLING-ACL'98, pages 174-180, Montr4al,Canada.
1998.\[Carroll et al, 1997a\] J. Carroll, T. Briscoe, N. Cal-zolari, S. Federici, S. Montemagni, V. Pir-relli, G. Grefenstette, A. Sanfilippo, G. Cat'-roll, and M. R.ooth.
Sparkle work pack-age 1, specification of phrasal parsing, final re-port.
Available at h t tp : / /www.
i l c .p i .
cnr .
i t / -spark le /spark le .htm,  November 1997.\[Carroll et al, 1997b\] J. Carroll, T. Briscoe, G. Car-roll, M. Light,D.
Prescher, 1%I.
Rooth, S. Federici, S. Montemagni,V.
Pirrelli, I. Prodanof, and M. Vannocchi.
Sparklework package 3, phrasal parsing software, deliverabled3.2.
Available at http://www.ilc.pi.cnr.it/-sparkle/sparkle, htm, November 1997.\[Carroll et al, 1998a\] J. Carroll, T. Briscoe, andA.
Sanfilippo.
Parser evaluation: a survey and a newproposal.
In 1st Intl.
Con\[.
on Language Resourcesand Evaluation (LREC), pages 447-454, Granada,Spain, 1998.\[Carroll et al, 1998b\] J. Carroll, G. Minnen, andT.
Briscoe.
Can subcategorisation probabilities helpa statistical parser?
In 6th ACL/SIGDAT workshopon Vez~t Large Corpora, Montrdal, Canada, 1998.\[Carroll et al, 1999\] J. Carroll, G. Minnen, andT.
Briscoe.
Corpus annotation for parser evaluation.In To appear in the EACL99 workshop on Linguisti-cally Interpreted Corpora (LINC'g9), 1999.\[Charniak.
1997\] E. Charniak.
Statistical techniquesfor natural anguage parsing.
AI magazine, 18(4):33-43, 1997.\[Chomsky, 1965\] N. Chomsky.
Aspects o\[ the Theory ofSyntax.
Massachusetts Institute of Technology, 1965.\[Collins, 1997\] M. Collins.
Three generative, lexical-ized models for statistical parsing.
In Proceedings ofACL/EACLgZ 1997.\[Def, 1995\] Defense Advanced Research ProjectsAgency.
Proc.
6th Message Understanding Confer-enee (MUC-6), November 1995.\[Ferro, 1998\] L. Ferro.
Guidelines for annotating ram-matical relations.
Unpublished annotation guide-lines, 1998.\[Kaplan, 1994\] R. Kaplan.
The formal architecture oflexical-functional grammar.
In M. Dalrymple, R. Ka-plan, J. Maxwell III, and A. Zaenen, editors.
Formalissues in lexical-\]unctional grammar.
Stanforcl Uni-versity.
1994.\[Lai and Huang, 1998\] T. B.Y.
Lai and C. Huang.Complements and adjuncts in dependency grammarparsing emulated by a constrained context-free gram-mar.
In COLING-ACL'98 Workshop: Processingof Dependency-based Grammars, Montrdal, Canada,1998.\[Marcus et al, 1993\] M. Marcus, B. Santorini, andM.
Marcinkiewicz.
Building a large annotated cor-pus of english: the penn treebank.
ComputationalLinguistics, 19(2), 1993.\[Miller.
1990\] G. Miller.
Wordnet: an on-line lexicaldatabase.
Intl.
J. of Lexicography, 3(4).
1990.\[Palmer et al, 1993\]M. Palmer, R. Passonneau, C. Weir, and T. Finin.The kernel text understanding system.
Artificml In-telligence, 63:17-68, 1993.\[Palmer, 1997\] D. Palmer.
A trainable rule-based al-gorithm for word segmentation.
In Proceedings ofACL/EACL97, 1997.\[Perlmutter, 1983\] D. Perlmutter.
Studies in RelationalGrammar 1.
U. Chicago Press, 1983.\[Ramshaw and Marcus, 1995\] L. Ramshawand M. Marcus.
Text chunking using transformagion-based learning.
In Proc.
of the 3rd Workshop on VeryLarge Corpora, pages 82-94.
Cambridge.
MA.
USA.1995.\[Wolff et al, 1995\] S. Wolff, C. Macleod, and A. Mey-ers.
Comlex Word Classes.
C.S.
Dept., New York U..Feb. 1995. prepared for the Linguistic Data Consor-tium, U. Pennsylvania.\[Yeh and Vilain, 1998\] A. Yeh and M. Vilain.
Someproperties of preposition and subordinate conjunc-tion attachments.
In COLING-ACL'98, pages 1436-1442, Montreal, Canada, 1998.52
