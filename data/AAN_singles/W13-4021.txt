Proceedings of the SIGDIAL 2013 Conference, pages 137?141,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsModelling Human Clarification StrategiesSvetlana Stoyanchev, Alex Liu, Julia HirschbergColumbia University, New York NY 10027sstoyanchev, al3037, julia @cs.columbia.eduAbstractWe model human responses to speech recog-nition errors from a corpus of human clarifi-cation strategies.
We employ learning tech-niques to study 1) the decision to either stopand ask a clarification question or to continuethe dialogue without clarification, and 2) thedecision to ask a targeted clarification questionor a more generic question.
Targeted clarifi-cation questions focus specifically on the partof an utterance that is misrecognized, in con-trast with generic requests to ?please repeat?or ?please rephrase?.
Our goal is to generatetargeted clarification strategies for handling er-rors in spoken dialogue systems, when appro-priate.
Our experiments show that linguis-tic features, in particular the inferred part-of-speech of a misrecognized word are predictiveof human clarification decisions.
A combina-tion of linguistic features predicts a user?s de-cision to continue or stop a dialogue with ac-curacy of 72.8% over a majority baseline accu-racy of 59.1%.
The same set of features predictthe decision to ask a targeted question with ac-curacy of 74.6% compared with the majoritybaseline of 71.8%.11 IntroductionClarification questions are common in human-humandialogue.
They help dialogue participants main-tain dialogue flow and resolve misunderstandings.Purver (2004) finds that in human-human dialoguespeakers most frequently use reprise clarification ques-tions to resolve recognition errors.
Reprise clarificationquestions use portions of the misunderstood utterancewhich are thought to be correctly recognized to targetthe part of an utterance that was misheard or misunder-stood.
In the following example from (Purver, 2004),Speaker B has failed to hear the word toast and so con-structs a clarification question using a portion of thecorrectly understood utterance ?
the word some ?
toquery the portion of the utterance B has failed to under-stand:1This work was partially funded by DARPA HR0011-12-C-0016 as a Columbia University subcontract to SRI Interna-tional.A: Can I have some toast please?B: Some?A: Toast.Unlike human conversational partners, most di-alogue systems today employ generic ?please re-peat/rephrase?
questions asking a speaker to repeat orrephrase an entire utterance.
Our goal is to introducereprise, or targeted, clarifications into an automaticspoken system.
Targeted clarifications can be espe-cially useful for systems accepting unrestricted speech,such as tutoring systems, intelligent agents, and speechtranslation systems.
Using a reprise question, a usercan correct an error by repeating only a portion ofan utterance.
Targeted questions also provide naturalgrounding and implicit confirmation by signalling tothe conversation partner which parts of an utterancehave been recognized.In order to handle a misrecognition, the system mustfirst identify misrecognized words (Stoyanchev et al2012), determine the type of question to ask, and con-struct the question.
In this work, we address two pointsnecessary for determining the type of question to ask:?
Is it appropriate for a system to ask a clarificationquestion when a misrecognized word is detected??
Is it possible to ask a targeted clarification ques-tion for a given sentence and an error segment?To answer these questions, we analyze a corpus of hu-man responses to transcribed utterances with missinginformation which we collected using Amazon Me-chanical Turk (2012).
Although the data collectionwas text-based, we asked annotators to respond as theywould in a dialogue.
In Section 2, we describe relatedwork on error recovery strategies in dialogue systems.In Section 3, we describe the corpus used in this exper-iment.
In Section 4, we describe our experiments onhuman clarification strategy modelling.
We concludein Section 5 with our plan for applying our models inspoken systems.2 Related workTo handle errors in speech recognition, slot-filling di-alogue systems typically use simple rejection (?I?msorry.
I didn?t understand you.?)
when they havelow confidence in a recognition hypothesis and ex-plicit or implicit confirmation when confidence scores137are higher.
Machine learning approaches have beensuccessfully employed to determine dialogue strate-gies (Bohus and Rudnicky, 2005; Bohus et al 2006;Rieser and Lemon, 2006), such as when to providehelp, repeat a previous prompt, or move on to the nextprompt.
Reiser and Lemon (2006) use machine learn-ing to determine an optimal clarification strategy inmultimodal dialogue.
Komatani et al(2006) propose amethod to generate a help message based on perceiveduser expertise.
Corpus studies on human clarificationsin dialogue indicate that users ask task-related ques-tions and provide feedback confirming their hypothesisinstead of giving direct indication of their misunder-standing (Skantze, 2005; Williams and Young, 2004;Koulouri and Lauria, 2009).
In our work, we modelhuman strategies with the goal of building a dialoguesystem which can generate targeted clarification ques-tions for recognition errors that require additional userinput but which can also recover from other errors au-tomatically, as humans do.3 DataIn our experiments, we use a dataset of human re-sponses to missing information, which we collectedwith Amazon Mechanical Turk (AMT).
Each AMT an-notator was given a set of Automatic Speech Recog-nition (ASR) transcriptions of an English utterancewith a single misrecognized segment.
925 such utter-ances were taken from acted dialogues between En-glish and Arabic speakers conversing through SRI?sIraqComm speech-to-speech translation system (Akba-cak et al 2009).
Misrecognized segments were re-placed by ?XXX?
to indicate the missing information,simulating a dialogue system?s automatic detection ofmisrecognized words (Stoyanchev et al 2012).
Foreach sentence, AMT workers were asked to 1) indi-cate whether other information in the sentence madeits meaning clear despite the error, 2) guess the miss-ing word if possible, 3) guess the missing word?s part-of-speech (POS) if possible, and 4) create a targetedclarification question if possible.
Three annotators an-notated each sentence.
Table 1 summarizes the results.In 668 (72%) of the sentences an error segment corre-sponds to a single word while in 276 (28%) of them, anerror segment corresponds to multiple words.
For mul-tiple word error segments, subjects had the option ofguessing multiple words and POS tags.
We scored theirguess correct if any of their guesses matched the syn-tactic head word of an error segment determined froman automatically assigned dependency parse structure.We manually corrected annotators?
POS tags if thehypothesized word was itself correct.
After this post-processing, we see that AMT workers hypothesizedPOS correctly in 57.7% of single-word and 60.2% ofmulti-word error cases.
They guessed words correctlyin 34.9% and 19.3% of single- and multi-word errorcases.
They choose to ask a clarification question in38.3% /47.9% of cases and 76.1%/62.3% of these ques-tions were targeted clarification questions.
These re-Single-word Agree Multi-worderror errorTotal sent 668 (72%) - 276 (28%)Correct POS 57.7% 62% 60.2%Correct word 34.9% 25% 19.3%Ask a question 38.3% 39% 47.9%Targeted question 76.1% 25% 62.3%Table 1: Annotation summary for single-word andmulti-word error cases.
Absolute annotator agreementis shown for single-word error cases.sults indicate that people are often able to guess a POStag and sometimes an actual word.
We observe that 1)in a single-word error segment, subjects are better atguessing an actual word than they are in a multi-worderror segment; and 2) in a multi-word error segment,subjects are more likely to ask a clarification questionand less likely to ask a targeted question.
All three an-notators agree on POS tags in 62% of cases and on hy-pothesized words in 25%.
Annotators?
agreement onresponse type is low ?
not surprising since there ismore than one appropriate and natural way to respondin dialogue.
In 39% of cases, all three annotators agreeon the decision to stop/continue and in only 25% ofcases all three annotators agree on asking a targetedclarification question.
Figure 1 shows the annotatorFigure 1: Distribution of decisions to ask a question orcontinue dialogue without a question.distribution for asking a clarification question vs. con-tinuing the dialogue based on hypothesized POS tag.
Itindicates that annotators are more likely to ask a ques-tion than continue without a question when they hy-pothesize a missing word to be a content word (nounor adjective) or when they are unsure of the POS of themissing word.
They are more likely to continue whenthey believe a missing word is a function word.
How-ever, when they believe a missing word is a verb, theyare more likely to continue, and they are also likely toidentify the missing verb correctly.Figure 2 shows a distribution of annotator decisionsas to the type of question they would ask.
The pro-portion of targeted question types varies with hypoth-esized POS.
It is more prevalent than confirm andgeneric questions combined for all POS tags exceptpreposition and question word, indicating that annota-tors are generally able to construct a targeted clarifica-tion question based on their analysis of the error seg-ment.138Figure 2: Distribution of decisions for targeted, confir-mation, and generic question types.4 ExperimentWe use our AMT annotations to build classifiers for 1)choice of action: stop and engage in clarification vs.continue dialogue; and 2) type of clarification ques-tion (targeted vs. non-targeted) to ask.
For the con-tinue/stop experiment, we aim to determine whether asystem should stop and ask a clarification question.
Forthe targeted vs. non-targeted experiment, we aim to de-termine whether it is possible to ask a targeted clarifi-cation question.2Using the Weka (Witten and Eibe, 2005) machinelearning framework, we build classifiers to predictAMT decisions.
We automatically assign POS tags totranscripts using the Stanford tagger (Toutanova andothers, 2003).
We compare models built with an au-tomatically tagged POS for an error word (POS-auto)with one built with POS guessed by a user (POS-guess).
Although a dialogue manager may not haveaccess to a correct POS, it may simulate this by pre-dicting POS of the error.
We assign dependency tagsusing the AMU dependency parser (Nasr et al 2011)which has been optimized on the Transtac dataset.We hypothesize that a user?s dialogue move dependson the syntactic structure of a sentence as well ason syntactic and semantic information about the er-ror word and its syntactic parent.
To capture sentencestructure, we use features associated with the wholesentence: POS ngram, all pairs of parent-child depen-dency tags in a sentence (Dep-pair), and all semanticroles (Sem-presence) in a sentence.
To capture the syn-tactic and semantic role of a misrecognized word, weuse features associated with this word: POS tag, depen-dency tag (Dep-tag), POS of the parent word (Parent-POS), and semantic role of an error word (Sem-role).We first model individual annotators?
decisions foreach of the three annotation instances.
We measurethe value that each feature adds to a model, using an-notators?
POS guess (POS-guess).
Next, we model ajoint annotators?
decision using the automatically as-signed POS-auto feature.
This model simulates a sys-tem behaviour in a dialogue with a user where a systemchooses a single dialogue move for each situation.
Werun 10-fold cross validation using the Weka J48 Deci-2If any annotators asked a targeted question, we assign apositive label to this instance, and negative otherwise.sion Tree algorithm.Feature DescriptionCountWord-position beginning if a misrecognized word isthe first word in the sentence, end if itis the last word, middle otherwise.Utterance-length number of words in the sentencePart-of-speech (compare)POS-auto POS tag of the misrecognized word au-tomatically assigned on a transcriptPOS-guess POS tag of the misrecognized wordguessed by a userPOS ngramsPOS ngrams all bigrams and trigrams of POS tags ina sentenceSyntactic DependencyDep-tag dependency tag of the misrecognizedword automatically assigned on a tran-scriptDep-pair dependency tags of all (parent, child)pairs in the sentenceParent-POS POS tag of the syntactic parent of themisrecognized wordSemanticSem-role semantic role of the misrecognizedwordSem-presence all semantic roles present in a sentenceTable 2: Features4.1 Stop/Continue ExperimentIn this experiment, we classify each instance in thedataset into a binary continue or stop decision.
Sinceeach instance is annotated by three annotators, we firstpredict individual annotators?
decisions.
The absoluteagreement on continue/stop is 39% which means that61% of sentences are classified into both classes.
Weexplore the role of each feature in predicting these de-cisions.
All features used in this experiment, except forthe POS-guess feature, are extracted from the sentencesautomatically.
Variation in the POS-guess feature mayexplain some of the difference between annotator deci-sions.Features Acc F-measure %DiffMajority baseline 59.1%All features 72.8% ?
0.726 0.0%less utt length 72.9% ?
0.727 +0.1%less POS ngrams 72.8% ?
0.727 +0.1%less Semantic 72.6% ?
0.724 -0.3%less Syn.
Depend.
71.5% ?
0.712 -1.9%less Position 71.2% ?
0.711 -2.0%less POS 67.9% ?
0.677 -6.7%POS only 70.1% ?
0.690 -5.0%Table 3: Stop/Continue experiment predicting individ-ual annotator?s decision with POS-guess.
Accuracy, F-measure and Difference of f-measure from All feature.
?indicates statistically significant difference from themajority baseline (p<.01)Table 3 shows the results of continue/stop classifica-tion.
A majority baseline method predicts the most fre-quent class continue and has 59.1% accuracy.
In com-parison, our classifier, built with all features, achieves72.8% accuracy.139Next, we evaluate the utility of each feature by re-moving it from the feature set and comparing the modelbuilt without it with a model built on all features.
POSis the most useful feature, as we expected: when it isremoved from the feature set, the f-measure decreasesby 6.7%.
A model trained on the POS-guess featurealone outperforms a model trained on all other features.Word position in the sentence is the next most salientfeature, contributing 2% to the f-measure.
The syntac-tic dependency features Syn-Dep, Dep-pair, and ParentPOS together contribute 1.9%.3Next, we predict a majority decision for each sen-tence.
Table 4 shows the accuracy of this prediction.A majority baseline has an accuracy of 59.9%.
Whenwe use a model trained on the POS-auto feature alone,accuracy rises to 66.1%, while a combination of all fea-tures further increases it to 69.2%.Features Acc F-measureMajority baseline 59.9%POS 66.1% ?
0.655All features 69.2% ?
0.687Table 4: Stop/Continue experiment predicting majoritydecision, using POS-auto.
?indicates statistically sig-nificant difference from the majority baseline (p<.01).4.2 Targeted Clarification ExperimentIn this experiment, we classify each instance into tar-geted or not targeted categories.
The targeted categorycomprises the cases in which an annotator chooses tostop and ask a targeted question.
We are interested inidentifying these cases in order to determine whether asystem should try to ask a targeted clarification ques-tion.
Table 5 shows the results of this experiment.
Themajority baseline predicts not targeted and has a 71.8%accuracy because in most cases, no question is asked.A model trained on all features increases accuracy to74.6%.
POS is the most salient feature, contributing3.8% to the f-measure.
All models that use POS fea-ture are significantly different from the baseline.
Thenext most salient features are POS ngram and a com-bination of syntactic dependency features contributing1% and .5% to the f-measure respectively.Table 6 shows system performance in predicting ajoint annotators?
decision of whether a targeted ques-tion can be asked.
A joint decision in this experimentis considered not targeted when none of the annotatorschooses to ask a targeted question.
We aim at identi-fying the cases where position of an error word makesit difficult to ask a clarification question, such as for asentence XXX somebody steal these supplies.
Using theautomatically assigned POS (POS-auto) feature aloneachieves an accuracy of 62.2%, which is almost 10%above the baseline.
A combination of all features, sur-prisingly, lowers the accuracy to 59.4%.
Interestingly, acombination of all features less POS increases accuracy3All trained models are significantly different from thebaseline.
None of the trained models are significantly dif-ferent from each other.Features Acc F-measure %DiffMajority baseline 71.8%All features 74.6% ?
0.734 0.0%All feature (POS guess)less Utt length 74.8% ?
0.736 +0.3%less Position 74.9% ?
0.731 -0.4%less Semantic 74.8% ?
0.737 +0.4%less Syn.
Depend.
74.2% ?
0.730 -0.5%less POS ngram 74.2% ?
0.727 -1.0%less POS 74.0% 0.706 -3.8%POS 74.1% ?
0.731 -0.4%Table 5: Targeted/not experiment predicting individ-ual annotator?s decision with POS-guess.
Accuracy, F-measure and Difference of f-measure from All feature.
?indicates statistically significant difference from themajority baseline (p<.05)above the baseline by 7.6% points to 60.1% accuracy.Features Acc F-measureMajority baseline 52.5%POS only 62.2% ?
0.622All features 59.4% ?
0.594All features less POS 60.1% ?
0.600Table 6: Targeted/not experiment predicting majoritydecision, using POS tag feature POS-auto.
?indicatesstatistically significant difference from the majoritybaseline.5 Conclusions and Future WorkIn this paper we have described experiments modellinghuman strategies in response to ASR errors.
We haveused machine learning techniques on a corpus anno-tated by AMT workers asked to respond to missing in-formation in an utterance.
Although annotation agree-ment in this task is low, we aim to learn natural strate-gies for a dialogue system by combining the judge-ments of several annotators.
In a dialogue, as in othernatural language tasks, there is more than one appro-priate response in each situation.
A user does not judgethe system (or another speaker) by a single response.Over a dialogue session, appropriateness, or lack of itin system actions, becomes evident.
We have shownthat by using linguistic features we can predict the de-cision to either ask a clarification question or continuedialogue with an accuracy of 72.8% in comparison withthe 59.1% baseline.
The same linguistic features pre-dict a targeted clarification question with an accuracyof 74.6% compared to the baseline of 71.8%.In future work, we will apply modelling of a clari-fication choice strategy in a speech-to-speech transla-tion task.
In our related work, we have addressed theproblem of automatic correction of some ASR errorsfor cases when humans believe a dialogue can continuewithout clarification In other work, we have addressedthe creation of targeted clarification questions for han-dling the cases when such questions are appropriate.Combining these research directions, we are develop-ing a clarification component for a speech-to-speechtranslation system that responds naturally to speechrecognition errors.140ReferencesM.
Akbacak, Franco, H., M. Frandsen, S. Hasan,H.
Jameel, A. Kathol, S. Khadivi, X. Lei, A. Man-dal, S. Mansour, K. Precoda, C. Richey, D. Vergyri,W.
Wang, M. Yang, and J. Zheng.
2009.
Recent ad-vances in SRI?s IraqCommtm Iraqi Arabic-Englishspeech-to-speech translation system.
In ICASSP,pages 4809?4812.Amazon Mechanical Turk.
2012.http://aws.amazon.com/mturk/, accessed on 28may, 2012.D.
Bohus and A. I. Rudnicky.
2005.
A principled ap-proach for rejection threshold optimization in spokendialog systems.
In INTERSPEECH, pages 2781?2784.D.
Bohus, B. Langner, A. Raux, A.
Black, M. Eske-nazi, and A. Rudnicky.
2006.
Online supervisedlearning of non-understanding recovery policies.
InProceedings of SLT.Y.
Fukubayashi, K. Komatani, T. Ogata, and H. Okuno.2006.
Dynamic help generation by estimating user?smental model in spoken dialogue systems.
In Pro-ceedings of the International Conference on SpokenLanguage Processing (ICSLP).T.
Koulouri and S. Lauria.
2009.
Exploring miscom-munication and collaborative behaviour in human-robot interaction.
In SIGDIAL Conference, pages111?119.A.
Nasr, F. Be?chet, J.F.
Rey, B. Favre, and J.
Le Roux.2011.
Macaon: an nlp tool suite for processingword lattices.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies: SystemsDemonstrations, pages 86?91.
Association for Com-putational Linguistics.M.
Purver.
2004.
The Theory and Use of ClarificationRequests in Dialogue.
Ph.D. thesis, King?s College,University of London.V.
Rieser and O.
Lemon.
2006.
Using machine learn-ing to explore human multimodal clarification strate-gies.
In ACL.G.
Skantze.
2005.
Exploring human error recov-ery strategies: Implications for spoken dialogue sys-tems.
Speech Communication, 45(2-3):325?341.Svetlana Stoyanchev, Philipp Salletmayr, Jingbo Yang,and Julia Hirschberg.
2012.
Localized detectionof speech recognition errors.
In SLT, pages 25?30.IEEE.K.
Toutanova et al2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1.
Association for Computational Linguistics.J.
D. Williams and S. Young.
2004.
Characterizingtask-oriented dialog using a simulated ASR channel.In Proceedings of the ICSLP, Jeju, South Korea.I.
Witten and F. Eibe.
2005.
Data Mining: Practi-cal machine learning tools and techniques.
MorganKaufmann, San Francisco, 2nd edition.141
