Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 32?33,Vancouver, October 2005.OPINE: Extracting Product Features and Opinions from ReviewsAna-Maria Popescu Bao NguyenDepartment of Computer Science and EngineeringUniversity of WashingtonSeattle, WA 98195-2350{amp,omicron,etzioni}@cs.washington.eduOren EtzioniAbstractConsumers have to often wadethrough a large number of on-line re-views in order to make an informedproduct choice.
We introduce OPINE,an unsupervised, high-precision in-formation extraction system whichmines product reviews in order tobuild a model of product features andtheir evaluation by reviewers.1 IntroductionThe Web contains a wealth of customer reviews - as aresult, the problem of ?review mining?
has seen increas-ing attention over the last few years from (Turney, 2003;Hu and Liu, 2004) and many others.
We decompose theproblem of review mining into the following subtasks:a) Identify product features, b) Identify opinions re-garding product features, c) Determine the polarity ofeach opinion and d) Rank opinions according to theirstrength (e.g., ?abominable?
is stronger than ?bad?
).We introduce OPINE, an unsupervised information ex-traction system that embodies a solution to each of theabove subtasks.
The remainder of this paper is organizedas follows: Section 2 describes OPINE?s components to-gether with their experimental evaluation and Section 3describes the related work.2 OPINE OverviewOPINE is built on top of KNOWITALL, a Web-based,domain-independent information extraction system (Et-zioni et al, 2005).
Given a set of relations of inter-est, KNOWITALL instantiates relation-specific genericextraction patterns into extraction rules which find can-didate facts.
The Assessor module then assigns a proba-bility to each candidate using a form of Point-wise Mu-tual Information (PMI) between phrases that is estimatedfrom Web search engine hit counts (Turney, 2003).
ItInput: product class C, reviews R.Output: set of [feature, ranked opinion list] tuplesR??
parseReviews(R);E?
findExplicitFeatures(R?, C);O?
findOpinions(R?, E);CO?
clusterOpinions(O);I?
findImplicitFeatures(CO, E);RO?
rankOpinions(CO);{(f , oi, ...oj)}?outputTuples(RO, I?E);Figure 1: OPINE Overview.computes the PMI between each fact and discriminatorphrases (e.g., ?is a scanner?
for the isA() relationshipin the context of the Scanner class).
Given fact f anddiscriminator d, the computed PMI score is:PMI(f, d) = Hits(d + f )Hits(d)?Hits(f )The PMI scores are converted to binary features for aNaive Bayes Classifier, which outputs a probability asso-ciated with each fact.Given product class C with instances I and reviews R,OPINE?s goal is to find the set of (feature, opinions) tuples{(f, oi, ...oj)} s.t.
f ?
F and oi, ...oj ?
O, where:a) F is the set of product class features in R.b) O is the set of opinion phrases in R.c) opinions associated with a particular feature areranked based on their strength.OPINE?s solution to this task is outlined in Figure 1.
Inthe following, we describe in detail each step.Explicit Feature Extraction OPINE parses the re-views using the MINIPAR dependency parser (Lin, 1998)and applies a simple pronoun-resolution module to theparsed data.
The system then finds explicitly men-tioned product features (E) using an extended versionof KNOWITALL?s extract-and-assess strategy describedabove.
OPINE extracts the following types of product fea-tures: properties, parts, features of product parts (e.g.,ScannerCoverSize), related concepts (e.g., Image32is related to Scanner) and parts and properties of re-lated concepts (e.g., ImageSize).
When compared onthis task with the most relevant previous review-miningsystem in (Hu and Liu, 2004), OPINE obtains a 22% im-provement in precision with only a 3% reduction in recallon the relevant 5 datasets.
One third of this increase is dueto OPINE?s feature assessment step and the rest is due tothe use of Web PMI statistics.Opinion Phrases OPINE extracts adjective, noun, verband adverb phrases attached to explicit features as poten-tial opinion phrases.
OPINE then collectively assigns pos-itive, negative or neutral semantic orientation (SO) labelsto their respective head words.
This problem is similar tolabeling problems in computer vision and OPINE uses awell-known computer vision technique, relaxation label-ing, as the basis of a 3-step SO label assignment proce-dure.
First, OPINE identifies the average SO label for aword w in the context of the review set.
Second, OPINEidentifies the average SO label for each word w in thecontext of a feature f and of the review set (?hot?
hasa negative connotation in ?hot room?, but a positive onein ?hot water?).
Finally, OPINE identifies the SO label ofword w in the context of feature f and sentence s. For ex-ample, some people like large scanners (?I love this largescanner?)
and some do not (?I hate this large scanner?
).The phrases with non-neutral head words are retained asopinion phrases and their polarity is established accord-ingly.
On the task of opinion phrase extraction, OPINEobtains a precision of 79% and a recall of 76% and on thetask of opinion phrase polarity extraction OPINE obtainsa precision of 86% and a recall of 84%.Implicit Features Opinion phrases refer to properties,which are sometimes implicit (e.g., ?tiny phone?
refers tothe phone size).
In order to extract such properties, OPINEfirst clusters opinion phrases (e.g., tiny and small willbe placed in the same cluster), automatically labels theclusters with property names (e.g., Size) and uses themto build implicit features (e.g., PhoneSize).
Opinionphrases are clustered using a mixture of WordNet infor-mation (e.g., antonyms are placed in the same cluster) andlexical pattern information (e.g., ?clean, almost spotless?suggests that ?clean?
and ?spotless?
are likely to refer tothe same property).
(Hu and Liu, 2004) doesn?t handleimplicit features, so we have evaluated the impact of im-plicit feature extraction on two separate sets of reviewsin the Hotels and Scanners domains.
Extracting implicitfeatures (in addition to explicit features) has resulted in a2% increase in precision and a 6% increase in recall forOPINE on the task of feature extraction.Ranking Opinion Phrases Given an opinion cluster,OPINE uses the final probabilities associated with the SOlabels in order to derive an initial opinion phrase strengthranking (e.g., great > good > average) in the mannerof (Turney, 2003).
OPINE then uses Web-derived con-straints on the relative strength of phrases in order to im-prove this ranking.
Patterns such as ?a1, (*) even a2?
aregood indicators of how strong a1 is relative to a2.
OPINEbootstraps a set of such patterns and instantiates themwith pairs of opinions in order to derive constraints suchas strength(deafening) > strength(loud).
OPINEalso uses synonymy and antonymy-based constraintssuch as strength(clean) = strength(dirty).
The con-straint set induces a constraint satisfaction problemwhose solution is a ranking of the respective cluster opin-ions (the remaining opinions maintain their default rank-ing).
OPINE?s accuracy on the opinion ranking task is87%.
Finally, OPINE outputs a set of (feature, rankedopinions) tuples for each product.3 Related WorkThe previous review-mining systems most relevant toour work are (Hu and Liu, 2004) and (Kobayashi etal., 2004).
The former?s precision on the explicit fea-ture extraction task is 22% lower than OPINE?s whilethe latter employs an iterative semi-automatic approachwhich requires significant human input; neither handlesimplicit features.
Unlike previous research on identifyingthe subjective character and the polarity of phrases andsentences ((Hatzivassiloglou and Wiebe, 2000; Turney,2003) and many others), OPINE identifies the context-sensitive polarity of opinion phrases.
In contrast to super-vised methods which distinguish among strength levelsfor sentences or clauses ((Wilson et al, 2004) and oth-ers), OPINEuses an unsupervised constraint-based opin-ion ranking approach.ReferencesO.
Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.
2005.
Un-supervised named-entity extraction from the web: An exper-imental study.
Artificial Intelligence, 165(1):91?134.V.
Hatzivassiloglou and J. Wiebe.
2000.
Effects of Adjec-tive Orientation and Gradability on Sentence Subjectivity.
InCOLING, pages 299?305.M.
Hu and B. Liu.
2004.
Mining and Summarizing CustomerReviews.
In KDD, pages 168?177, Seattle, WA.N.
Kobayashi, K. Inui, K. Tateishi, and T. Fukushima.
2004.Collecting Evaluative Expressions for Opinion Extraction.In IJCNLP, pages 596?605.D.
Lin.
1998.
Dependency-based evaluation of MINIPAR.
InWorkshop on Evaluation of Parsing Systems at ICLRE.P.
Turney.
2003.
Inference of Semantic Orientation from Asso-ciation.
In CoRR cs.
CL/0309034.T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how mad are you?finding strong and weak opinion clauses.
In AAAI, pages761?769.33
