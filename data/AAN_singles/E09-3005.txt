Proceedings of the EACL 2009 Student Research Workshop, pages 37?45,Athens, Greece, 2 April 2009. c?2009 Association for Computational LinguisticsStructural Correspondence Learning for Parse DisambiguationBarbara PlankAlfa-informaticaUniversity of Groningen, The Netherlandsb.plank@rug.nlAbstractThe paper presents an application ofStructural Correspondence Learning(SCL) (Blitzer et al, 2006) for domainadaptation of a stochastic attribute-valuegrammar (SAVG).
So far, SCL hasbeen applied successfully in NLP forPart-of-Speech tagging and SentimentAnalysis (Blitzer et al, 2006; Blitzeret al, 2007).
An attempt was madein the CoNLL 2007 shared task to ap-ply SCL to non-projective dependencyparsing (Shimizu and Nakagawa, 2007),however, without any clear conclusions.We report on our exploration of applyingSCL to adapt a syntactic disambiguationmodel and show promising initial resultson Wikipedia domains.1 IntroductionMany current, effective natural language process-ing systems are based on supervised MachineLearning techniques.
The parameters of such sys-tems are estimated to best reflect the character-istics of the training data, at the cost of porta-bility: a system will be successful only as longas the training material resembles the input thatthe model gets.
Therefore, whenever we have ac-cess to a large amount of labeled data from some?source?
(out-of-domain), but we would like amodel that performs well on some new ?target?domain (Gildea, 2001; Daume?
III, 2007), we facethe problem of domain adaptation.The need for domain adaptation arises in manyNLP tasks: Part-of-Speech tagging, SentimentAnalysis, Semantic Role Labeling or StatisticalParsing, to name but a few.
For example, the per-formance of a statistical parsing system drops inan appalling way when a model trained on the WallStreet Journal is applied to the more varied Browncorpus (Gildea, 2001).The problem itself has started to get attentiononly recently (Roark and Bacchiani, 2003; Hara etal., 2005; Daume?
III and Marcu, 2006; Daume?
III,2007; Blitzer et al, 2006; McClosky et al, 2006;Dredze et al, 2007).
We distinguish two main ap-proaches to domain adaptation that have been ad-dressed in the literature (Daume?
III, 2007): super-vised and semi-supervised.In supervised domain adaptation (Gildea, 2001;Roark and Bacchiani, 2003; Hara et al, 2005;Daume?
III, 2007), besides the labeled source data,we have access to a comparably small, but labeledamount of target data.
In contrast, semi-superviseddomain adaptation (Blitzer et al, 2006; McCloskyet al, 2006; Dredze et al, 2007) is the scenario inwhich, in addition to the labeled source data, weonly have unlabeled and no labeled target domaindata.
Semi-supervised adaptation is a much morerealistic situation, while at the same time also con-siderably more difficult.Studies on the supervised task have shown thatstraightforward baselines (e.g.
models based onsource only, target only, or the union of the data)achieve a relatively high performance level and are?surprisingly difficult to beat?
(Daume?
III, 2007).Thus, one conclusion from that line of work is thatas soon as there is a reasonable (often even small)amount of labeled target data, it is often more fruit-ful to either just use that, or to apply simple adap-tation techniques (Daume?
III, 2007; Plank and vanNoord, 2008).2 Motivation and Prior WorkWhile several authors have looked at the super-vised adaptation case, there are less (and espe-cially less successful) studies on semi-superviseddomain adaptation (McClosky et al, 2006; Blitzeret al, 2006; Dredze et al, 2007).
Of these, Mc-Closky et al (2006) deal specifically with self-training for data-driven statistical parsing.
Theyshow that together with a re-ranker, improvements37are obtained.
Similarly, Structural Correspon-dence Learning (Blitzer et al, 2006; Blitzer etal., 2007; Blitzer, 2008) has proven to be suc-cessful for the two tasks examined, PoS taggingand Sentiment Classification.
In contrast, Dredzeet al (2007) report on ?frustrating?
results onthe CoNLL 2007 semi-supervised adaptation taskfor dependency parsing, i.e.
?no team was ableto improve target domain performance substan-tially over a state of the art baseline?.
In thesame shared task, an attempt was made to ap-ply SCL to domain adaptation for data-driven de-pendency parsing (Shimizu and Nakagawa, 2007).The system just ended up at rank 7 out of 8 teams.However, based on annotation differences in thedatasets (Dredze et al, 2007) and a bug in theirsystem (Shimizu and Nakagawa, 2007), their re-sults are inconclusive.1 Thus, the effectiveness ofSCL is rather unexplored for parsing.So far, most previous work on domain adapta-tion for parsing has focused on data-driven sys-tems (Gildea, 2001; Roark and Bacchiani, 2003;McClosky et al, 2006; Shimizu and Nakagawa,2007), i.e.
systems employing (constituent or de-pendency based) treebank grammars (Charniak,1996).
Parse selection constitutes an importantpart of many parsing systems (Johnson et al,1999; Hara et al, 2005; van Noord and Malouf,2005; McClosky et al, 2006).
Yet, the adaptationof parse selection models to novel domains is a farless studied area.
This may be motivated by thefact that potential gains for this task are inherentlybounded by the underlying grammar.
The fewstudies on adapting disambiguation models (Haraet al, 2005; Plank and van Noord, 2008) have fo-cused exclusively on the supervised scenario.Therefore, the direction we explore in thisstudy is semi-supervised domain adaptation forparse disambiguation.
We examine the effec-tiveness of Structural Correspondence Learning(SCL) (Blitzer et al, 2006) for this task, a re-cently proposed adaptation technique shown to beeffective for PoS tagging and Sentiment Analy-sis.
The system used in this study is Alpino, awide-coverage Stochastic Attribute Value Gram-mar (SAVG) for Dutch (van Noord and Malouf,2005; van Noord, 2006).
For our empirical eval-1As shown in Dredze et al (2007), the biggest problemfor the shared task was that the provided datasets were an-notated with different annotation guidelines, thus the gen-eral conclusion was that the task was ill-defined (NobuyukiShimizu, personal communication).uation we explore Wikipedia as primary test andtraining collection.In the sequel, we first introduce the parsing sys-tem.
Section 4 reviews Structural CorrespondenceLearning and shows our application of SCL toparse selection, including all our design choices.In Section 5 we present the datasets, introduce theprocess of constructing target domain data fromWikipedia, and discuss interesting initial empiri-cal results of this ongoing study.3 Background: Alpino parserAlpino (van Noord and Malouf, 2005; van Noord,2006) is a robust computational analyzer for Dutchthat implements the conceptual two-stage parsingapproach.
The system consists of approximately800 grammar rules in the tradition of HPSG, anda large hand-crafted lexicon, that together with aleft-corner parser constitutes the generation com-ponent.
For parse selection, Alpino employs a dis-criminative approach based on Maximum Entropy(MaxEnt).
The output of the parser is dependencystructure based on the guidelines of CGN (Oost-dijk, 2000).The Maximum Entropy model (Berger et al,1996; Ratnaparkhi, 1997; Abney, 1997) is a con-ditional model that assigns a probability to everypossible parse ?
for a given sentence s. The modelconsists of a set of m feature functions fj(?)
thatdescribe properties of parses, together with theirassociated weights ?j .
The denominator is a nor-malization term where Y (s) is the set of parseswith yield s:p?
(?|s; ?)
=exp(?mj=1 ?jfj(?
))?y?Y (s) exp(?mj=1 ?jfj(y)))(1)The parameters (weights) ?j can be estimatedefficiently by maximizing the regularized condi-tional likelihood of a training corpus (Johnson etal., 1999; van Noord and Malouf, 2005):??
= arg max?logL(?)
?
?mj=1 ?2j2?2 (2)where L(?)
is the likelihood of the training data.The second term is a regularization term (Gaus-sian prior on the feature weights with mean zeroand variance ?).
The estimated weights determinethe contribution of each feature.
Features appear-ing in correct parses are given increasing (posi-tive) weight, while features in incorrect parses are38given decreasing (negative) weight.
Once a modelis trained, it can be applied to choose the parsewith the highest sum of feature weights.The MaxEnt model consists of a large set offeatures, corresponding to instantiations of featuretemplates that model various properties of parses.For instance, Part-of-Speech tags, dependency re-lations, grammar rule applications, etc.
The cur-rent standard model uses about 11,000 features.We will refer to this set of features as original fea-tures.
They are used to train the baseline model onthe given labeled source data.4 Structural Correspondence LearningSCL (Structural Correspondence Learn-ing) (Blitzer et al, 2006; Blitzer et al, 2007;Blitzer, 2008) is a recently proposed domainadaptation technique which uses unlabeled datafrom both source and target domain to learncorrespondences between features from differentdomains.Before describing the algorithm in detail, let usillustrate the intuition behind SCL with an exam-ple, borrowed from Blitzer et al (2007).
Supposewe have a Sentiment Analysis system trained onbook reviews (domain A), and we would like toadapt it to kitchen appliances (domain B).
Fea-tures such as ?boring?
and ?repetitive?
are com-mon ways to express negative sentiment in A,while ?not working?
or ?defective?
are specific toB.
If there are features across the domains, e.g.
?don?t buy?, with which the domain specific fea-tures are highly correlated with, then we mighttentatively align those features.Therefore, the key idea of SCL is to identify au-tomatically correspondences among features fromdifferent domains by modeling their correlationswith pivot features.
Pivots are features occur-ring frequently and behaving similarly in both do-mains (Blitzer et al, 2006).
They are inspired byauxiliary problems from Ando and Zhang (2005).Non-pivot features that correspond with many ofthe same pivot-features are assumed to corre-spond.
Intuitively, if we are able to find good cor-respondences among features, then the augmentedlabeled source domain data should transfer betterto a target domain (where no labeled data is avail-able) (Blitzer et al, 2006).The outline of the algorithm is given in Figure 1.The first step is to identify m pivot features oc-curring frequently in the unlabeled data of bothInput: - labeled source data {(xs, ys)Nss=1}- unlabeled data from both source andtarget domain xul = xs, xt1.
Select m pivot features2.
Train m binary classifiers (pivot predictors)3.
Create matrix Wn?m of binary predictorweight vectors W = [w1, .., wm], where nis the number of nonpivot features in xul4.
Apply SVD to W : Wn?m =Un?nDn?mV Tm?m where ?
= UT[1:h,:]are the h top left singular vectors of W .5.
Apply projection xs?
and train a predictoron the original and new features obtainedthrough the projection.Figure 1: SCL algorithm (Blitzer et al, 2006).domains.
Then, a binary classifier is trained foreach pivot feature (pivot predictor) of the form:?Does pivot feature l occur in this instance??.
Thepivots are masked in the unlabeled data and theaim is to predict them using non-pivot features.In this way, we obtain a weight vector w for eachpivot predictor.
Positive entries in the weight vec-tor indicate that a non-pivot is highly correlatedwith the respective pivot feature.
Step 3 is to ar-range the m weight vectors in a matrix W , wherea column corresponds to a pivot predictor weightvector.
Applying the projection W Tx (where xis a training instance) would give us m new fea-tures, however, for ?both computational and sta-tistical reasons?
(Blitzer et al, 2006; Ando andZhang, 2005) a low-dimensional approximation ofthe original feature space is computed by applyingSingular Value Decomposition (SVD) on W (step4).
Let ?
= UTh?n be the top h left singular vec-tors of W (with h a dimension parameter and nthe number of non-pivot features).
The resulting ?is a projection onto a lower dimensional space Rh,parameterized by h.The final step of SCL is to train a linear predic-tor on the augmented labeled source data ?x, ?x?.In more detail, the original feature space x is aug-mented with h new features obtained by apply-ing the projection ?x.
In this way, we can learnweights for domain-specific features, which oth-erwise would not have been observed.
If ?
con-tains meaningful correspondences, then the pre-39dictor trained on the augmented data should trans-fer well to the new domain.4.1 SCL for Parse DisambiguationA property of the pivot predictors is that they canbe trained from unlabeled data, as they representproperties of the input.
So far, pivot features on theword level were used (Blitzer et al, 2006; Blitzeret al, 2007; Blitzer, 2008), e.g.
?Does the bigramnot buy occur in this document??
(Blitzer, 2008).Pivot features are the key ingredient for SCL,and they should align well with the NLP task.
ForPoS tagging and Sentiment Analysis, features onthe word level are intuitively well-related to theproblem at hand.
For the task of parse disambigua-tion based on a conditional model this is not thecase.Hence, we actually introduce an additional andnew layer of abstraction, which, we hypothesize,aligns well with the task of parse disambiguation:we first parse the unlabeled data.
In this way weobtain full parses for given sentences as producedby the grammar, allowing access to more abstractrepresentations of the underlying pivot predictortraining data (for reasons of efficiency, we here useonly the first generated parse as training data forthe pivot predictors, rather than n-best).Thus, instead of using word-level features, ourfeatures correspond to properties of the gener-ated parses: application of grammar rules (r1,r2features), dependency relations (dep), PoS tags(f1,f2), syntactic features (s1), precedence (mf ),bilexical preferences (z), apposition (appos) andfurther features for unknown words, temporalphrases, coordination (h,in year and p1, respec-tively).
This allows us to get a possibly noisy,but more abstract representation of the underlyingdata.
The set of features used in Alpino is furtherdescribed in van Noord and Malouf (2005).Selection of pivot features As pivot featuresshould be common across domains, here we re-strict our pivots to be of the type r1,p1,s1 (the mostfrequently occurring feature types).
In more de-tail, r1 indicates which grammar rule applied, p1whether coordination conjuncts are parallel, ands1 whether topicalization or long-distance depen-dencies occurred.
We count how often each fea-ture appears in the parsed source and target do-main data, and select those r1,p1,s1 features aspivot features, whose count is > t, where t is aspecified threshold.
In all our experiments, we sett = 5000.
In this way we obtained on average 360pivot features, on the datasets described in Sec-tion 5.Predictive features As pointed out by Blitzer etal.
(2006), each instance will actually contain fea-tures which are totally predictive of the pivot fea-tures (i.e.
the pivot itself).
In our case, we ad-ditionally have to pay attention to ?more specific?features, e.g.
r2 is a feature that extends r1, in thesense that it incorporates more information thanits parent (i.e.
which grammar rules applied in theconstruction of daughter nodes).
It is crucial to re-move these predictive features when creating thetraining data for the pivot predictors.Matrix and SVD Following Blitzer et al (2006)(which follow Ando and Zhang (2005)), we onlyuse positive entries in the pivot predictors weightvectors to compute the SVD.
Thus, when con-structing the matrix W , we disregard all nega-tive entries in W and compute the SVD (W =UDV T ) on the resulting non-negative sparse ma-trix.
This sparse representation saves both timeand space.4.2 Further practical issues of SCLIn practice, there are more free parameters andmodel choices (Ando and Zhang, 2005; Ando,2006; Blitzer et al, 2006; Blitzer, 2008) besidesthe ones discussed above.Feature normalization and feature scaling.Blitzer et al (2006) found it necessary to normal-ize and scale the new features obtained by the pro-jection ?, in order to ?allow them to receive moreweight from a regularized discriminative learner?.For each of the features, they centered them bysubtracting out the mean and normalized them tounit variance (i.e.
x ?
mean/sd).
They thenrescaled the features by a factor ?
found on held-out data: ?
?x.Restricted Regularization.
When training thesupervised model on the augmented feature space?x, ?x?, Blitzer et al (2006) only regularize theweight vector of the original features, but notthe one for the new low-dimensional features.This was done to encourage the model to usethe new low-dimensional representation ratherthan the higher-dimensional original representa-tion (Blitzer, 2008).Dimensionality reduction by feature type.
Anextension suggested in Ando and Zhang (2005) is40to compute separate SVDs for blocks of the matrixW corresponding to feature types (as illustrated inFigure 2), and then to apply separate projectionfor every type.
Due to the positive results in Ando(2006), Blitzer et al (2006) include this in theirstandard setting of SCL and report results usingblock SVDs only.Figure 2: Illustration of dimensionality reductionby feature type (Ando and Zhang, 2005).
The greyarea corresponds to a feature type (submatrix ofW ) on which the SVD is computed (block SVD);the white area is regarded as fixed to zero matrices.5 Experiments and Results5.1 Experimental designThe base (source domain) disambiguation modelis trained on the Alpino Treebank (van Noord,2006) (newspaper text), which consists of ap-proximately 7,000 sentences and 145,000 tokens.For parameter estimation of the disambiguationmodel, in all reported experiments we use theTADM2 toolkit (toolkit for advanced discrimina-tive training), with a Gaussian prior (?2=1000)and the (default) limited memory variable metricestimation technique (Malouf, 2002).For training the binary pivot predictors, we usethe MegaM3 Optimization Package with the so-called ?bernoulli implicit?
input format.
To com-pute the SVD, we use SVDLIBC.4The output of the parser is dependency struc-ture.
A standard evaluation metric is to measurethe amount of generated dependencies that areidentical to the stored dependencies (correct la-beled dependencies), expressed as f-score.
An al-ternative measure is concept accuracy (CA), whichis similar to f-score, but allows possible discrep-ancy between the number of returned dependen-cies (van Noord, 2006; Plank and van Noord,2http://tadm.sourceforge.net/3http://www.cs.utah.edu/?hal/megam/4http://tedlab.mit.edu/?dr/svdlibc/2008).
CA is usually slightly lower than f-score.Let Dip be the number of dependencies producedby the parser for sentence i.
Dig is the number ofdependencies in the treebank parse, and Dio is thenumber of correct dependencies produced by theparser.
Then,CA = Do?i max(Dig,Dip)If we want to compare the performance of dis-ambiguation models, we can employ the ?
mea-sure (van Noord and Malouf, 2005; van Noord,2007).
Intuitively, it tells us how much of the dis-ambiguation problem has been solved.?
= CA?
baseoracle ?
base ?
100In more detail, the ?
measure incorporates an up-per and lower bound: base measures the accu-racy of a model that simply selects the first parsefor each sentence; oracle represents the accuracyachieved by a model that always selects the bestparse from the set of potential parses (within thecoverage of the parser).
In addition, we also re-port relative error reduction (rel.er), which is therelative difference in ?
scores for two models.As target domain, we consider the Dutch partof Wikipedia as data collection, described in thefollowing.5.2 Wikipedia as resourceIn our experiments, we exploit Wikipedia both astestset and as unlabeled data source.
We assumethat in order to parse data from a very specific do-main, say about the artist Prince, then data relatedto that domain, like information about the NewPower Generation, the Purple rain movie, or otherAmerican singers and artists, should be of help.Thus, we exploit Wikipedia and its category sys-tem to gather domain-specific target data.Construction of target domain data In moredetail, we use the Dutch part of Wikipedia pro-vided by WikiXML,5 a collection of Wikipedia ar-ticles converted to XML format.
As the corpus isencoded in XML, we can exploit general purposeXML Query Languages, such as XQuery, Xslt andXPath, to extract relevant information from theWikipedia corpus.Given a wikipage p, with c ?
categories(p),we can identify pages related to p of various5http://ilps.science.uva.nl/WikiXML/41types of ?relatedness?
: directly related pages (thosethat share a category, i.e.
all p?
where ?c?
?categories(p?)
such that c = c?
), or alterna-tively, pages that share a sub- or supercategoryof p, i.e.
p?
where c?
?
categories(p?)
and c?
?sub categories(p) or c?
?
super categories(p).For example, Figure 3 shows the categories ex-tracted for the Wikipedia article about pope Jo-hannes Paulus II.<wikipage id="6677"><cat t="direct" n="Categorie:Paus"/><cat t="direct" n="Categorie:Pools_theoloog"/><cat t="super" n="Categorie:Religieus leider"/><cat t="super" n="Categorie:Rooms-katholiek persoon"/><cat t="super" n="Categorie:Vaticaanstad"/><cat t="super" n="Categorie:Bisschop"/><cat t="super" n="Categorie:Kerkgeschiedenis"/><cat t="sub" n="Categorie:Tegenpaus"/><cat t="super" n="Categorie:Pools persoon"/></wikipage>Figure 3: Example of extracted Wikipedia cate-gories for a given article (direct, sup- and subcats).To create the set of related pages for a given ar-ticle p, we proceed as follows:1.
Find sub- and supercategories of p2.
Extract all pages that are related to p (throughsharing a direct, sub or super category)3.
Optionally, filter out certain pagesIn our empirical setup, we followed Blitzer et al(2006) and tried to balance the size of source andtarget data.
Thus, depending on the size of the re-sulting target domain dataset, and the ?broadness?of the categories involved in creating it, we mightwish to filter out certain pages.
We implementeda filter mechanism that excludes pages of a cer-tain category (e.g.
a supercategory that is hypoth-esized to be ?too broad?).
Alternatively, we mighthave used a filter mechanism that excludes certainpages directly.In our experiments, we always included pagesthat are directly related to a page of inter-est, and those that shared a subcategory.
Ofcourse, the page itself is not included in thatdataset.
With regard to supercategories, we usu-ally included all pages having a category c ?super categories(p), unless stated otherwise.Test collection Our testset consists of a selectionof Wikipedia articles that have been manually cor-rected in the course of the D-Coi/LASSY project.66Ongoing project, see http://www.let.rug.nl/?vannoord/Lassy/An overview of the testset including size indica-tions is given in Table 1.
Table 2 provides infor-mation on the target domain datasets constructedfrom Wikipedia.Wiki/DCOI ID Title Sents6677/026563 Prince (musician) 3586729/036834 Paus Johannes Paulus II 232182654/041235 Augustus De Morgan 259Table 1: Size of test datasets.Related to Articles Sents Tokens RelationshipPrince 290 9,772 145,504 filtered superPaus 445 8,832 134,451 allDe Morgan 394 8,466 132,948 allTable 2: Size of related unlabeled data; relation-ship indicates whether all related pages are usedor some are filtered out (see section 5.2).5.3 Empirical ResultsFor all reported results, we randomly select n =200 maximum number of parses per sentence forevaluation.Baseline accuracies Table 3 shows the baselineperformance (of the standard Alpino model) on thevarious Wikipedia testsets (CA, f-score).
The thirdand fourth column indicate the upper- and lowerbound measures (defined in section 5.1).Title CA f-score base oraclePrince (musician) 85.03 85.38 71.95 88.70Paus Johannes Paulus II 85.72 86.32 74.30 89.09Augustus De Morgan 80.09 80.61 70.08 83.52Table 3: Baseline results.While the parser normally operates on an accu-racy level of roughly 88-89% (van Noord, 2007)on its own domain (newspaper text), the accu-racy on these subdomains drops to around 85%.The biggest performance decrease (to 80%) wason the article about the British logician and math-ematician De Morgan.
This confirms the intu-ition that this specific subdomain is the ?hardest?,given that mathematical expressions might emergein the data (e.g.
?Wet der distributiviteit : a(b+c)= ab+ac?
- distributivity law).SCL results Table 4 shows the results of our in-stantiation of SCL for parse disambiguation, withvarying h parameter (dimensionality parameter;42h = 25 means that applying the projection x?
re-sulted in adding 25 new features to every sourcedomain instance).CA f-score ?
rel.er.baseline Prince 85.03 85.38 78.06 0.00SCL[+/-], h = 25 85.12 85.46 78.64 2.64SCL[+/-], h = 50 85.29 85.63 79.66 7.29SCL[+/-], h = 100 85.19 85.53 79.04 4.47SCL[+/-], h = 200 85.21 85.54 79.18 5.10baseline Paus 85.72 86.32 77.23 0.00SCL[+/-], h = 25 85.87 86.48 78.26 4.52SCL[+/-], h = 50 85.82 86.43 77.87 2.81SCL[+/-], h = 100 85.87 86.49 78.26 4.52SCL[+/-], h = 200 85.87 86.48 78.26 4.52baseline DeMorgan 80.09 80.61 74.44 0.00SCL[+/-], h = 25 80.15 80.67 74.92 1.88SCL[+/-], h = 50 80.12 80.64 74.68 0.94SCL[+/-], h = 100 80.12 80.64 74.68 0.94SCL[+/-], h = 200 80.15 80.67 74.91 1.88Table 4: Results of our instantiation of SCL (withvarying h parameter and no feature normaliza-tion).The results show a (sometimes) small but con-sistent increase in absolute performance on alltestsets over the baseline system (up to +0.26absolute CA score), as well as an increase in ?measure (absolute error reduction).
This corre-sponds to a relative error reduction of up to 7.29%.Thus, our first instantiation of SCL for parse dis-ambiguation indeed shows promising results.We can confirm that changing the dimensional-ity parameter h has rather little effect (Table 4),which is in line with previous findings (Ando andZhang, 2005; Blitzer et al, 2006).
Thus we mightfix the parameter and prefer smaller dimensionali-ties, which saves space and time.Note that these results were obtained withoutany of the additional normalization, rescaling,feature-specific regularization, or block SVD is-sues, etc.
(discussed in section 4.2).
We used thesame Gaussian regularization term (?2=1000) forall features (original and new features), and didnot perform any feature normalization or rescal-ing.
This means our current instantiation of SCLis an actually simplified version of the originalSCL algorithm, applied to parse disambiguation.Of course, our results are preliminary and, ratherthan warranting many definite conclusions, en-courage further exploration of SCL and relatedsemi-supervised adaptation techniques.5.4 Additional Empirical ResultsIn the following, we describe additional results ob-tained by extensions and/or refinements of our cur-rent SCL instantiation.Feature normalization.
We also tested fea-ture normalization (as described in Section 4.2).While Blitzer et al (2006) found it necessary tonormalize (and scale) the projection features, wedid not observe any improvement by normalizingthem (actually, it slightly degraded performance inour case).
Thus, we found this step unnecessary,and currently did not look at this issue any further.A look at ?
To gain some insight of which kindof correspondences SCL learned in our case, westarted to examine the rows of ?.
Recall that ap-plying a row of the projection matrix ?i to a train-ing instance x gives us a new real-valued fea-ture.
If features from different domains have sim-ilar entries (scores) in the projection row, theyare assumed to correspond (Blitzer, 2008).
Fig-ure 4 shows example of correspondences that SCLfound in the Prince dataset.
The first column rep-resents the score of a feature.
The labels wikiand alp indicate the domain of the features, re-spectively.
For readability, we here grouped thefeatures obtaining similar scores.0.00010248|dep35(?Chaka Khan?,name(?PER?),hd/su,verb,ben)|wiki0.00010248|dep35(de,det,hd/det,adj,?Afro-Amerikaanse?
)|wiki0.00010248|dep35(?Yvette Marie Stevens?,name(?PER?
),hd/app,noun,zangeres)|wiki0.000102772|dep34(leraar,noun,hd/su,verb)|alp0.000161095|dep34(commissie,noun,hd/obj1,prep)|16|alp0.00016113|dep34(?Confessions Tour?,name,hd/obj1,prep)|2|wiki0.000161241|dep34(orgel,noun,hd/obj1,prep)|1|wiki0.000217698|dep34(tournee,noun,hd/su,verb)|1|wiki0.000223301|dep34(regisseur,noun,hd/su,verb)|15|wiki0.000224517|dep34(voorsprong,noun,hd/su,verb)|2|alp0.000224684|dep34(wetenschap,noun,hd/su,verb)|2|alp0.000226617|dep34(pop_rock,noun,hd/su,verb)|1|wiki0.000228918|dep34(plan,noun,hd/su,verb)|9|alpFigure 4: Example projection from ?
(row 2).SCL clustered information about ?Chaka Khan?,an ?Afro-Amerikaanse?
?zangeres?
(afro-americansinger) whose real name is ?Yvette MarieStevens?.
She had close connections to Prince,who even wrote one of her singles.
These featuresgot aligned to the Alpino feature ?leraar?
(teacher).Moreover, SCL finds that ?tournee?, ?regisseur?and ?pop rock?
in the Prince domain behave like?voorsprong?
(advance), ?wetenschap?
(research)and ?plan?
as possible heads in a subject relationin the newspaper domain.
Similarly, correspon-43dences between the direct object features ?Con-fessions Tour?
and ?orgel?
(pipe organ) to ?com-missie?
(commission) are discovered.More unlabeled data In the experiments so far,we balanced the amount of source and target data.We started to examine the effect of more unla-beled target domain data.
For the Prince dataset,we included all supercategories in constructingthe related target domain data.
The so obtaineddataset contains: 859 articles, 29,186 sentencesand 385,289 tokens; hence, the size approximatelytripled (w.r.t.
Table 2).
Table 5 shows the effect ofusing this larger dataset for SCL with h = 25.
Theaccuracy increases (from 85.12 to 85.25).
Thus,there seems to be a positive effect (to be investi-gated further).CA f-score ?
rel.er.baseline Prince 85.03 85.38 78.06 0.00SCL[+/-], h = 25, all 85.25 85.58 79.42 6.20Table 5: First result on increasing unlabeled data.Dimensionality reduction by feature type Wehave started to implement the extension discussedin section 4.2, i.e.
perform separate dimension-ality reductions based on blocks of nonpivot fea-tures.
We clustered nonpivots (see section 4.1 fora description) into 9 types (ordered in terms ofdecreasing cluster size): dep, f1/f2 (pos), r1/r2(rules), appos person, mf, z, h1, in year, dist.
Foreach type, a separate SVD was computed on sub-matrix Wt (illustrated in Figure 2).
Then, sepa-rate projections were applied to every training in-stance.The results of these experiments on the Princedataset are shown in Figure 5.
Applying SCL withdimensionality reduction by feature type (SCLblock) results in a model that performs better (CA85.27, ?
79.52, rel.er.
6.65%) than the model withno feature split (no block SVDs), thus obtaining arelative error reduction of 6.65% over the baseline.The same figure also shows what happens if weremove a specific feature type at a time; the appo-sition features contribute the most on this Princedomain.
As a fact, one third of the sentences inthe Prince testset contain constructions with appo-sitions (e.g.
about film-, album- and song titles).6 Conclusions and Future WorkThe paper presents an application of StructuralCorrespondence Learning (SCL) to parse disam-Figure 5: Results of dimensionality reduction byfeature type, h = 25; block SVD included all 9feature types; the right part shows the accuracywhen one feature type was removed.biguation.
While SCL has been successfullyapplied to PoS tagging and Sentiment Analy-sis (Blitzer et al, 2006; Blitzer et al, 2007), itseffectiveness for parsing was rather unexplored.The empirical results show that our instantiationof SCL to parse disambiguation gives promisinginitial results, even without the many additionalextensions on the feature level as done in Blitzeret al (2006).
We exploited Wikipedia as pri-mary resource, both for collecting unlabeled tar-get domain data, as well as test suite for empiricalevaluation.
On the three examined datasets, SCLslightly but constantly outperformed the baseline.Applying SCL involves many design choices andpractical issues, which we tried to depict here indetail.
A novelty in our application is that wefirst actually parse the unlabeled data from bothdomains.
This allows us to get a possibly noisy,but more abstract representation of the underlyingdata on which the pivot predictors are trained.In the near future, we plan to extend the work onsemi-supervised domain adaptation for parse dis-ambiguation, viz.
(1) further explore/refine SCL(block SVDs, varying amount of target domaindata, other testsets, etc.
), and (2) examine self-training.
Studies on the latter have focused mainlyon generative, constituent based, i.e.
data-drivenparsing systems.
Furthermore, from a machinelearning point of view, it would be interesting toknow a measure of corpus similarity to estimatethe success of porting an NLP system from one do-main to another.
This relates to the general ques-tion of what is meant by domain.44ReferencesSteven P. Abney.
1997.
Stochastic attribute-valuegrammars.
Computational Linguistics, 23:597?618.Rie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from multi-ple tasks and unlabeled data.
Journal of MachineLearning Research, 6:1817?1853.Rie Kubota Ando.
2006.
Applying alternating struc-ture optimization to word sense disambiguation.
InProceedings of the 10th Conference on Computa-tional Natural Language Learning (CoNLL).Adam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguis-tics, 22(1):39?72.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Conference on Empirical Meth-ods in Natural Language Processing, Sydney, Aus-tralia.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Association for Computational Linguis-tics, Prague, Czech Republic.John Blitzer.
2008.
Domain Adaptation of NaturalLanguage Processing Systems.
Ph.D. thesis, Uni-versity of Pennsylvania.Eugene Charniak.
1996.
Tree-bank grammars.
In InProceedings of the Thirteenth National Conferenceon Artificial Intelligence, pages 1031?1036.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26:101?126.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In Conference of the Association for Compu-tational Linguistics (ACL), Prague, Czech Republic.Mark Dredze, John Blitzer, Pratha Pratim Taluk-dar, Kuzman Ganchev, Joao Graca, and FernandoPereira.
2007.
Frustratingly hard domain adap-tation for parsing.
In Proceedings of the CoNLLShared Task Session - Conference on Natural Lan-guage Learning, Prague, Czech Republic.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of the 2001 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP).Tadayoshi Hara, Miyao Yusuke, and Jun?ichi Tsu-jii.
2005.
Adapting a probabilistic disambiguationmodel of an hpsg parser to a new domain.
In Pro-ceedings of the International Joint Conference onNatural Language Processing.Mark Johnson, Stuart Geman, Stephen Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In Pro-ceedings of the 37th Annual Meeting of the ACL.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of the Sixth Conference on Natural Lan-guage Learning (CoNLL-2002), Taipei.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the Human Language TechnologyConference of the NAACL, Main Conference, pages152?159, New York City, USA, June.
Associationfor Computational Linguistics.Nelleke Oostdijk.
2000.
The Spoken Dutch Corpus:Overview and first evaluation.
In Proceedings ofSecond International Conference on Language Re-sources and Evaluation (LREC), pages 887?894.Barbara Plank and Gertjan van Noord.
2008.
Ex-ploring an auxiliary distribution based approach todomain adaptation of a syntactic disambiguationmodel.
In Proceedings of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation(PE), Manchester, August.A.
Ratnaparkhi.
1997.
A simple introduction to max-imum entropy models for natural language process-ing.
Technical report, Institute for Research in Cog-nitive Science, University of Pennsylvania.Brian Roark and Michiel Bacchiani.
2003.
Supervisedand unsupervised pcfg adaptation to novel domains.In In Proceedings of the Human Language Technol-ogy Conference and Meeting of the North AmericanChapter of the Association for Computational Lin-guistics (HLT-NAACL).Nobuyuki Shimizu and Hiroshi Nakagawa.
2007.Structural correspondence learning for dependencyparsing.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007.Gertjan van Noord and Robert Malouf.
2005.Wide coverage parsing with stochastic at-tribute value grammars.
Draft available fromhttp://www.let.rug.nl/?vannoord.
A preliminary ver-sion of this paper was published in the Proceedingsof the IJCNLP workshop Beyond Shallow Analyses,Hainan China, 2004.Gertjan van Noord.
2006.
At Last Parsing Is NowOperational.
In TALN 2006 Verbum Ex Machina,Actes De La 13e Conference sur Le TraitementAutomatique des Langues naturelles, pages 20?42,Leuven.Gertjan van Noord.
2007.
Using self-trained bilexi-cal preferences to improve disambiguation accuracy.In Proceedings of the Tenth International Confer-ence on Parsing Technologies.
IWPT 2007, Prague.,pages 1?10, Prague.45
