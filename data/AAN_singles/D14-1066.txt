Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 610?614,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLexical Substitution for the Medical DomainMartin Riedl1Michael R. Glass2Alfio Gliozzo2(1) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany(2) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USAriedl@cs.tu-darmstadt.de, {mrglass,gliozzo}@us.ibm.comAbstractIn this paper we examine the lexical substitu-tion task for the medical domain.
We adaptthe current best system from the open domain,which trains a single classifier for all instancesusing delexicalized features.
We show sig-nificant improvements over a strong baselinecoming from a distributional thesaurus (DT).Whereas in the open domain system, featuresderived from WordNet show only slight im-provements, we show that its counterpart forthe medical domain (UMLS) shows a signif-icant additional benefit when used for featuregeneration.1 IntroductionThe task of lexical substitution (McCarthy and Navigli,2009) deals with the substitution of a target term withina sentence with words having the same meaning.
Thus,the task divides into two subtasks:?
Identification of substitution candidates, i.e.terms that are, for some contexts, substitutable fora given target term.?
Ranking the substitution candidates according totheir contextSuch a substitution system can help for semantic textsimilarity (B?ar et al., 2012), textual entailment (Daganet al., 2013) or plagiarism detection (Chong and Specia,2011).Datasets provided by McCarthy and Navigli (2009)and Biemann (2012) offer manually annotated substi-tutes for a given set of target words within a context(sentence).
Contrary to these two datasets in Kremer etal.
(2014) a dataset is offered where all words have areannotated with substitutes.
All the datasets are suitedfor the open domain.But a system performing lexical substitution is notonly of interest for the open domain, but also for themedical domain.
Such a system could then be appliedto medical word sense disambiguation, entailment orquestion answering tasks.
Here we introduce a newdataset and adapt the lexical substitution system, pro-vided by Szarvas et al.
(2013), to the medical domain.Additionally, we do not make use of WordNet (Miller,1995) to provide similar terms, but rather employ a Dis-tributional Thesaurus (DT), computed on medical texts.2 Related WorkFor the general domain, the lexical substitution taskwas initiated by a Semeval-2007 Task (McCarthy andNavigli, 2009).
This task was won by an unsupervisedmethod (Giuliano et al., 2007), which uses WordNet forthe substitution candidate generation and then relies onthe Google Web1T n-grams (Brants and Franz, 2006)1to rank the substitutes.The currently best system, to our knowledge, is pro-posed by Szarvas et al.
(2013).
This is a supervised ap-proach, where a single classifier is trained using delex-icalized features for all substitutes and can thus be ap-plied even to previously unseen substitutes.
Althoughthere have been many approaches for solving the taskfor the general domain, only slight effort has been donein adapting it to different domains.3 MethodTo perform lexical substitution, we follow the delex-icalization framework of Szarvas et al.
(2013).
Weautomatically build Distributional Thesauri (DTs) forthe medical domain and use features from the Uni-fied Medical Language System (UMLS) ontology.
Thedataset for supervised lexical substitution consists ofsentences, containing an annotated target word t. Con-sidering the sentence being the context for the targetword, the target word might have different meanings.Thus annotated substitute candidates sg1.
.
.
sgn?
sg,need to be provided for each context.
The negative ex-amples are substitute candidates that either are incor-rect for the target word, do not fit into the context orboth.
We will refer to these substitutes as false substi-tute candidates sf1.
.
.
sfm?
sfwith sf?
sg= ?.For the generation of substitute candidates we do notuse WordNet, as done in previous works (Szarvas et al.,2013), but use only substitutes from a DT.
To train asingle classifier, features that distinguishing the mean-ing of words in different context need to be considered.Such features could be e.g.
n-grams, features from dis-tributional semantics or features which are extracted1http://catalog.ldc.upenn.edu/LDC2006T13610relative to the target word, such as the ratio betweenfrequencies of the substitute candidate and the targetword.
After training, we apply the algorithm to un-seen substitute candidates and rank them according totheir positive probabilities, given by the classifier.
Con-trary to Szarvas et al.
(2013), we do not use any weight-ing in the training if a substitute has been supplied bymany annotators, as we could not observe any improve-ments.
Additionally, we use logistic regression (Fan etal., 2008) as classifier2.4 ResourcesFor the substitutes and for the generation of delexical-ized features, we rely on DTs, the UMLS and GoogleWeb1T.4.1 Distributional thesauri (DTs)We computed two different DTs using the frameworkproposed in Biemann and Riedl (2013)3.The first DT is computed based on Medline4ab-stracts.
This thesaurus uses the left and the right wordas context features.
To include multi-word expressions,we allow the number of tokens that form a term to beup to the length of three.The second DT is based on dependencies as contextfeatures from a English Slot Grammar (ESG) parser(McCord et al., 2012) modified to handle medical data.The ESG parser is also capable of finding multi-wordexpressions.
As input data we use 3.3 GB of textsfrom medical textbooks, encyclopedias and clinical ref-erence material as well as selected journals.
This DT isalso used for the generation of candidates supplied toannotators when creating the gold standard and there-fore is the main resource to provide substitute candi-dates.4.2 UMLSThe Unified Medical Language System (UMLS) is anontology for the medical domain.
In contrast to Szarvaset al.
(2013), which uses WordNet (Miller, 1995) togenerate substitute candidates and also for generatingfeatures, we use UMLS solely for feature generation.4.3 Google Web1TWe use the Google Web1T to generate n-gram featuresas we expect this open domain resource to have consid-erable coverage for most specific domains as well.
Foraccessing the resource, we use JWeb1T5(Giuliano etal., 2007).2We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/?cjlin/liblinear/) availablefrom http://liblinear.bwaldvogel.de/3We use Lexicographer?s Mutual Information (LMI) (Ev-ert, 2005) as significance measure and consider only the top1000 (p = 1000) features per term.4http://www.nlm.nih.gov/bsd/licensee/2014_stats/baseline_med_filecount.html5https://code.google.com/p/jweb1t/5 Lexical Substitution datasetBesides the lexical substitution data sets for the opendomain (McCarthy and Navigli, 2009; Biemann, 2012;Kremer et al., 2014) there is no dataset available thatcan be used for the medical domain.
Therefore, weconstructed an annotation task for the medical domainusing a medical corpus and domain experts.In order to provide the annotators with a clear task,we presented a question, and a passage that containsthe correct answer to the question.
We restricted this toa subset of passages that were previously annotated asjustifying the answer to the question.
This is related toa textual entailment task, essentially the passage entailsthe question with the answer substituted for the focus ofthe question.
We instructed the annotators to first iden-tify the terms that were relevant for the entailment rela-tion.
For each relevant term we randomly extracted 10terms from the ESG-based DT within the top 100 mostsimilar terms.
Using this list of distributionally similarterms, the annotators selected those terms that wouldpreserve the entailment relation if substituted.
This re-sulted in a dataset of 699 target terms with substitutes.On average from the 10 terms 0.846 are annotated ascorrect substitutes.
Thus, the remaining terms can beused as false substitute candidates.The agreement on this task by Fleiss Kappa was0.551 indicating ?moderate agreement?
(Landis andKoch, 1977).
On the metric of pairwise agreement,as defined in the SemEval lexical substitution task, weachieve 0.627.
This number is not directly comparableto the pairwise agreement score of 0.277 for the Se-mEval lexical substitution task (McCarthy and Navigli,2009) since in our task the candidates are given.
How-ever, it shows promise that subjectivity may be reducedby casting lexical substitution into a task of maintain-ing entailment.6 EvaluationFor the evaluation we use a ten-fold cross validationand report P@1 (also called Average Precision (AP) at1) and Mean Average Precision (MAP) (Buckley andVoorhees, 2004) scores.
The P@1 score indicates howoften the first substitute of the system matches the goldstandard.
The MAP score is the mean of all AP from 1to the number of all substitutes.?
Google Web 1T:We use the same Google n-gram features, asused in Giuliano et al.
(2007) and Szarvas et al.(2013).
These are frequencies of n-grams formedby the substitute candidate siand the left and rightwords, taken from the context sentence, normal-ized by the frequency of the same context n-gramwith the target term t. Additionally, we add thesame features, normalized by the frequency sumof all n-grams of the substitute candidates.
An-other feature is generated using the frequencieswhere t and s are listed together using the words611and, or and ?,?
as separator and also add the leftand right words of that phrase as context.
Then wenormalize this frequency by the frequency of thecontext occurring only with t.?
DT features:To characterize if t and sihave similar wordsin common, and therefore are similar, we com-pute the percentage of words their thesauri en-tries share, considering the top n words in eachentry with n = 1, 5, 20, 50, 100, 200.
Duringthe DT calculation we also calculate the signif-icances between each word and its context fea-tures (see Section 4.1).
Using this information,we compute if the words in the sentences alsooccur as context features for the substitute can-didate.
A third feature group relying on DTsis created by the overlapping context featuresfor the top m entries of t and siwith m =1, 5, 20, 50, 100, 1000, which are ranked regard-ing their significance score.
Whereas, the simi-larities between the trigram-based and the ESG-based DT are similar, the context features are dif-ferent.
Both feature types can be applied to thetwo DTs.
Additionally, we extract the thesaurusentry for the target word t and generate a featureindicating whether the substitute siis within thetop k entries with k = 1, 5, 10, 20, 100 entries6.?
Part-of-speech n-grams:To identify the context of the word we use thePOS-tag (only the first letter) of siand t as featureand POS-tag combinations of up to three neigh-boring words.?
UMLS:Considering UMLS we look up all concept uniqueidentifiers (CUIs) for siand t. The first two fea-tures are the number of CUIs for siand t. The nextfeatures compute the number of CUIs that siand tshare, starting from the minimal to the maximumnumber of CUIs.
Additionally, we use a featureindicating that siand t do not share any CUI.6.1 Substitute candidatesThe candidates for the substitution are taken from theESG based DT.
For each target term we use the goldsubstitute candidates as correct instances and add allpossible substitutes for the same target term occurringin a different context and do not have been annotatedas valid in the present context as false instances.7 ResultsRunning the experiment, we get the results as shownin Table 1.
As baseline system we use the ranking of6Whereas in Szarvas et al.
(2013) only k = 100 is used,we gained an improvement in performance when also addingsmaller values of k.the ESG-based DT.
As can be seen, the baseline is al-ready quite high, which can be attributed to the factthat this resource was used to generate substitutes undthus contains all positive instances.
Using the super-vised approach, we can beat the baseline by 0.10 forthe MAP score and by 0.176 for the P@1 score, whichis a significant improvement (p < 0.0001, using a twotailed permutation test).
To get insights of the contri-System MAP P@1Baseline 0.6408 0.5365ALL 0.7048 0.6366w/o DT 0.5798 0.4835w/o UMLS 0.6618 0.5651w/o Ngrams 0.7009 0.6252w/o POS 0.7027 0.6323Table 1: Results for the evaluation using substitute can-didates from the DT.bution of individual feature types, we perform an abla-tion test.
We observe that the most prominent featuresare coming from the two DTs as we only achieve re-sults below the baseline, when removing DT features.We still obtain significant improvements over the base-line when removing other feature groups.
The secondmost important feature comes from the UMLS.
Fea-tures coming from the Google n-grams improve thesystem only slightly.
The lowest improvement is de-rived from the part-of-speech features.
This leads usto summarize that a hybrid approach for feature gen-eration using manually created resources (UMLS) andunsupervised features (DTs) leads to the best result forlexical substitution for the medical domain.8 AnalysisFor a better insight into the lexical substitution we ana-lyzed how often we outperform the baseline, get equalresults or get decreased scores.
According to Table 2 inperformance # of instances Avg.
?
MAPdecline 180 -0.16equal 244 0improvements 275 0.26Table 2: Error analysis for the task respectively to theMAP score.around 26% of the cases we observe a decreased MAPscore, which is on average 0.16 smaller then the scoresachieved with the baseline.
On the other hand, we seeimprovements in around 39% of the cases: an averageimprovements of 0.26, which is much higher then theloss.
For the remaining 25% of cases we observe thesame score.Looking inside the data, the largest error class iscaused by antonyms.
A sub-class of this error aremulti-word expressions having an adjective modifier.This problems might be solved by additional featuresusing the UMLS resource.
An example is shown inFigure 1.612Figure 1: Example sentence for the target term mildthrombocytopenia.
The system returns a wrong rank-ing, as the adjective changes the meaning and turns thefirst ranked term into an antonym.For feature generation, we currently lookup multi-word expressions as one term, both in the DT and theUMLS resource and do not split them into their sin-gle tokens.
This error also suggests considering thesingle words inside the multi-word expression, espe-cially adjectives, and looking them up in a resource(e.g.
UMLS) to detect synonymy and antonymy.Figure 2 shows the case, where the ranking is per-formed correctly, but the precise substitute is not an-notated as a correct one.
The term nail plate might beeven more precise in the context as the manual anno-tated term nail bed.
Due to the missing annotation theFigure 2: Example sentence for the target term nails.Here the ranking from the system is correct, but the firstsubstitute from the system was not annotated as such.baseline gets better scores then the result from the sys-tem.9 ConclusionIn summary, we have examined the lexical substitutiontask for the medical domain and could show that a sys-tem for open domain text data can be applied to themedical domain.
We can show that following a hybridapproach using features from UMLS and distributionalsemantics leads to the best results.
In future work, wewill work on integrating DTs using other context fea-tures, as we could see an impact of using two differentDTs.
Furthermore, we want to incorporate features us-ing n-grams computed on a corpus from the domainand include co-occurrence features.AcknowledgmentsWe thank Adam Lally, Eric Brown, Edward A. Epstein,Chris Biemann and Faisal Chowdhury for their helpfulcomments.ReferencesDaniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
UKP: Computing SemanticTextual Similarity by Combining Multiple ContentSimilarity Measures.
In Proceedings of the 6th In-ternational Workshop on Semantic Evaluation, heldin conjunction with the 1st Joint Conference on Lex-ical and Computational Semantics, pages 435?440,Montreal, Canada.Chris Biemann and Martin Riedl.
2013.
Text: Now in2D!
A Framework for Lexical Expansion with Con-textual Similarity.
Journal of Language Modelling,1(1):55?95.Chris Biemann.
2012.
Turk bootstrap word sense in-ventory 2.0: A large-scale resource for lexical sub-stitution.
In Proceedings of the Eight InternationalConference on Language Resources and Evaluation(LREC?12), Istanbul, Turkey.Thorsten Brants and Alex Franz.
2006.
Web 1t 5-gram corpus version 1.
Technical report, Google Re-search.Chris Buckley and Ellen M. Voorhees.
2004.
Re-trieval evaluation with incomplete information.
InProceedings of the 27th Annual International ACMSIGIR Conference on Research and Developmentin Information Retrieval, SIGIR ?04, pages 25?32,Sheffield, United Kingdom.Miranda Chong and Lucia Specia.
2011.
Lexical gen-eralisation for word-level matching in plagiarism de-tection.
In Recent Advances in Natural LanguageProcessing, pages 704?709, Hissar, Bulgaria.Ido Dagan, Dan Roth, Mark Sammons, and Fabio M.Zanzotto.
2013.
Recognizing Textual Entailment:Models and Applications.
Synthesis Lectures on Hu-man Language Technologies, 6(4):1?220.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences: Word Pairs and Collocations.
Ph.D. thesis,Institut f?ur maschinelle Sprachverarbeitung, Univer-sity of Stuttgart.613Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Claudio Giuliano, Alfio Gliozzo, and Carlo Strappar-ava.
2007.
Fbk-irst: Lexical substitution task ex-ploiting domain and syntagmatic coherence.
In Pro-ceedings of the 4th International Workshop on Se-mantic Evaluations, SemEval ?07, pages 145?148,Prague, Czech Republic.Gerhard Kremer, Katrin Erk, Sebastian Pad?o, and Ste-fan Thater.
2014.
What Substitutes Tell Us - Anal-ysis of an ?All-Words?
Lexical Substitution Corpus.In Proceedings of the 14th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL 2014), pages 540?549, Gothen-burg, Sweden.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33:159?174.Diana McCarthy and Roberto Navigli.
2009.
The En-glish lexical substitution task.
Language Resourcesand Evaluation, 43(2):139?159.Michael C. McCord, J. William Murdock, and Bran-imir K. Boguraev.
2012.
Deep Parsing in Watson.IBM J. Res.
Dev., 56(3):264?278.George A. Miller.
1995.
WordNet: A LexicalDatabase for English.
Communications of the ACM,38:39?41.Gy?orgy Szarvas, Chris Biemann, and Iryna Gurevych.2013.
Supervised All-Words Lexical Substitutionusing Delexicalized Features.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies (NAACL-HLT 2013),pages 1131?1141, Atlanta, GA, USA.614
