Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1032?1039,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsWhat to be?
- Electronic Career Guidance Based on Semantic RelatednessIryna Gurevych, Christof Mu?ller and Torsten ZeschUbiquitous Knowledge Processing GroupTelecooperation, Darmstadt University of TechnologyHochschulstr.
10, 64289 Darmstadt, Germanyhttp://www.ukp.tu-darmstadt.de{gurevych,mueller,zesch}@tk.informatik.tu-darmstadt.deAbstractWe present a study aimed at investigatingthe use of semantic information in a novelNLP application, Electronic Career Guid-ance (ECG), in German.
ECG is formu-lated as an information retrieval (IR) task,whereby textual descriptions of professions(documents) are ranked for their relevanceto natural language descriptions of a per-son?s professional interests (the topic).
Wecompare the performance of two semanticIR models: (IR-1) utilizing semantic relat-edness (SR) measures based on either word-net or Wikipedia and a set of heuristics,and (IR-2) measuring the similarity betweenthe topic and documents based on ExplicitSemantic Analysis (ESA) (Gabrilovich andMarkovitch, 2007).
We evaluate the perfor-mance of SR measures intrinsically on thetasks of (T-1) computing SR, and (T-2) solv-ing Reader?s Digest Word Power (RDWP)questions.1 Electronic Career GuidanceCareer guidance is important both for the person in-volved and for the state.
Not well informed deci-sions may cause people to drop the training programthey are enrolled in, yielding loss of time and finan-cial investments.
However, there is a mismatch bet-ween what people know about existing professionsand the variety of professions, which exist in real-ity.
Some studies report that school leavers typi-cally choose the professions known to them, suchas policeman, nurse, etc.
Many other professions,which can possibly match the interests of the personvery well, are not chosen, as their titles are unknownand people seeking career advice do not know abouttheir existence, e.g.
electronics installer, or chem-ical laboratory worker.
However, people are verygood at describing their professional interests in nat-ural language.
That is why they are even asked towrite a short essay prior to an appointment with acareer guidance expert.Electronic career guidance is, thus, a supplementto career guidance by human experts, helping youngpeople to decide which profession to choose.
Thegoal is to automatically compute a ranked list of pro-fessions according to the user?s interests.
A currentsystem employed by the German Federal LabourOffice (GFLO) in their automatic career guidancefront-end1 is based on vocational trainings, manu-ally annotated using a tagset of 41 keywords.
Theuser must select appropriate keywords according toher interests.
In reply, the system consults a knowl-edge base with professions manually annotated withthe keywords by career guidance experts.
There-after, it outputs a list of the best matching profes-sions to the user.
This approach has two significantdisadvantages.
Firstly, the knowledge base has tobe maintained and steadily updated, as the numberof professions and keywords associated with themis continuously changing.
Secondly, the user has todescribe her interests in a very restricted way.At the same time, GFLO maintains an extensivedatabase with textual descriptions of professions,1http://www.interesse-beruf.de/1032called BERUFEnet.2 Therefore, we cast the prob-lem of ECG as an IR task, trying to remove thedisadvantages of conventional ECG outlined aboveby letting the user describe her interests in a shortnatural language essay, called a professional profile.Example essay translated to EnglishI would like to work with animals, to treat and lookafter them, but I cannot stand the sight of blood andtake too much pity on them.
On the other hand, I liketo work on the computer, can program in C, Python andVB and so I could consider software development as anappropriate profession.
I cannot imagine working in akindergarden, as a social worker or as a teacher, as Iam not very good at asserting myself.Textual descriptions of professions are rankedgiven such an essay by using NLP and IR tech-niques.
As essays and descriptions of professionsdisplay a mismatch between the vocabularies of top-ics and documents and there is lack of contextual in-formation, due to the documents being fairly shortas compared to standard IR scenarios, lexical se-mantic information should be especially beneficialto an IR system.
For example, the profile can con-tain words about some objects or activities related tothe profession, but not directly mentioned in the de-scription, e.g.
oven, cakes in the profile and pastries,baker, or confectioner in the document.
Therefore,we propose to utilize semantic relatedness as a rank-ing function instead of conventional IR techniques,as will be substantiated below.2 System ArchitectureIntegrating lexical semantic knowledge in ECG re-quires the existence of knowledge bases encodingdomain and lexical knowledge.
In this paper, we in-vestigate the utility of two knowledge bases: (i) aGerman wordnet, GermaNet (Kunze, 2004), and (ii)the German portion of Wikipedia.3 A large body ofresearch exists on using wordnets in NLP applica-tions and in particular in IR (Moldovan and Mihal-cea, 2000).
The knowledge in wordnets has beentypically utilized by expanding queries with relatedterms (Vorhees, 1994; Smeaton et al, 1994), con-cept indexing (Gonzalo et al, 1998), or similaritymeasures as ranking functions (Smeaton et al, 1994;Mu?ller and Gurevych, 2006).
Recently, Wikipedia2http://infobub.arbeitsagentur.de/berufe/3http://de.wikipedia.org/has been discovered as a promising lexical seman-tic resource and successfully used in such differentNLP tasks as question answering (Ahn et al, 2004),named entity disambiguation (Bunescu and Pasca,2006), and information retrieval (Katz et al, 2005).Further research (Zesch et al, 2007b) indicates thatGerman wordnet and Wikipedia show different per-formance depending on the task at hand.Departing from this, we first compare two seman-tic relatedness (SR) measures based on the informa-tion either in the German wordnet (Lin, 1998) calledLIN, or in Wikipedia (Gabrilovich and Markovitch,2007) called Explicit Semantic Analysis, or ESA.We evaluate their performance intrinsically on thetasks of (T-1) computing semantic relatedness, and(T-2) solving Reader?s Digest Word Power (RDWP)questions and make conclusions about the ability ofthe measures to model certain aspects of semanticrelatedness and their coverage.
Furthermore, we fol-low the approach by Mu?ller and Gurevych (2006),who proposed to utilize the LIN measure and a setof heuristics as an IR model (IR-1).Additionally, we utilize the ESA measure in asemantic information retrieval model, as this mea-sure is significantly better at vocabulary cover-age and at modelling cross part-of-speech relations(Gabrilovich and Markovitch, 2007).
We comparethe performance of ESA and LINmeasures in a task-based IR evaluation and analyze their strengths andlimitations.
Finally, we apply ESA to directly com-pute text similarities between topics and documents(IR-2) and compare the performance of two seman-tic IR models and a baseline Extended Boolean (EB)model (Salton et al, 1983) with query expansion.4To summarize, the contributions of this paper arethree-fold: (i) we present a novel system, utilizingNLP and IR techniques to perform Electronic CareerGuidance, (ii) we study the properties and intrinsi-cally evaluate two SR measures based on GermaNetand Wikipedia for the tasks of computing seman-tic relatedness and solving Reader?s Digest WordPower Game questions, and (iii) we investigate theperformance of two semantic IR models in a taskbased evaluation.4We also ran experiments with Okapi BM25 model as im-plemented in the Terrier framework, but the results were worsethan those with the EB model.
Therefore, we limit our discus-sion to the latter.10333 Computing Semantic Relatedness3.1 SR MeasuresGermaNet based measures GermaNet is a Ger-man wordnet, which adopted the major propertiesand database technology from Princeton?s Word-Net (Fellbaum, 1998).
However, GermaNet dis-plays some structural differences and content ori-ented modifications.
Its designers relied mainly onlinguistic evidence, such as corpus frequency, ratherthan psycholinguistic motivations.
Also, GermaNetemploys artificial, i.e.
non-lexicalized concepts, andadjectives are structured hierarchically as opposedto WordNet.
Currently, GermaNet includes about40000 synsets with more than 60000 word sensesmodelling nouns, verbs and adjectives.We use the semantic relatedness measure by Lin(1998) (referred to as LIN), as it consistently isamong the best performing wordnet based measures(Gurevych and Niederlich, 2005; Budanitsky andHirst, 2006).
Lin defined semantic similarity using aformula derived from information theory.
This mea-sure is sometimes called a universal semantic sim-ilarity measure as it is supposed to be application,domain, and resource independent.
Lin is computedas:simc1,c2 =2 ?
log p(LCS(c1, c2))log p(c1) + log p(c2)where c1 and c2 are concepts (word senses) corre-sponding to w1 and w2, log p(c) is the informationcontent, andLCS(c1, c2) is the lowest common sub-sumer of the two concepts.
The probability p is com-puted as the relative frequency of words (represent-ing that concept) in the taz5 corpus.Wikipedia based measures Wikipedia is a freeonline encyclopedia that is constructed in a col-laborative effort of voluntary contributors and stillgrows exponentially.
During this process, Wikipediahas probably become the largest collection of freelyavailable knowledge.
Wikipedia shares many ofits properties with other well known lexical seman-tic resources (like dictionaries, thesauri, semanticwordnets or conventional encyclopedias) (Zesch etal., 2007a).
As Wikipedia also models relatednessbetween concepts, it is better suited for computing5http://www.taz.desemantic relatedness than GermaNet (Zesch et al,2007b).In very recent work, Gabrilovich and Markovitch(2007) introduce a SR measure called Explicit Se-mantic Analysis (ESA).
The ESA measure repre-sents the meaning of a term as a high-dimensionalconcept vector.
The concept vector is derived fromWikipedia articles, as each article focuses on a cer-tain topic, and can thus be viewed as expressing aconcept.
The dimension of the concept vector is thenumber of Wikipedia articles.
Each element of thevector is associated with a certain Wikipedia article(or concept).
If the term can be found in this article,the term?s tfidf score (Salton and McGill, 1983) inthis article is assigned to the vector element.
Oth-erwise, 0 is assigned.
As a result, a term?s con-cept vector represents the importance of the term foreach concept.
Semantic relatedness of two terms canthen be easily computed as the cosine of their corre-sponding concept vectors.
If we want to measurethe semantic relatedness of texts instead of terms,we can also use ESA concept vectors.
A text is rep-resented as the average concept vector of its terms?concept vectors.
Then, the relatedness of two textsis computed as the cosine of their average conceptvectors.As ESA uses all textual information in Wikipedia,the measure shows excellent coverage.
Therefore,we select it as the second measure for integrationinto our IR system.3.2 DatasetsSemantic relatedness datasets for German em-ployed in our study are presented in Table 1.Gurevych (2005) conducted experiments with twodatasets: i) a German translation of the Englishdataset by Rubenstein and Goodenough (1965)(Gur65), and ii) a larger dataset containing 350word pairs (Gur350).
Zesch and Gurevych (2006)created a third dataset from domain-specific corporausing a semi-automatic process (ZG222).
Gur65 israther small and contains only noun-noun pairs con-nected by either synonymy or hypernymy.
Gur350contains nouns, verbs and adjectives that are con-nected by classical and non-classical relations (Mor-ris and Hirst, 2004).
However, word pairs forthis dataset are biased towards strong classical rela-tions, as they were manually selected from a corpus.1034CORRELATION rDATASET YEAR LANGUAGE # PAIRS POS SCORES # SUBJECTS INTER INTRAGur65 2005 German 65 N discrete {0,1,2,3,4} 24 .810 -Gur350 2006 German 350 N, V, A discrete {0,1,2,3,4} 8 .690 -ZG222 2006 German 222 N, V, A discrete {0,1,2,3,4} 21 .490 .647Table 1: Comparison of datasets used for evaluating semantic relatedness in German.ZG222 does not have this bias.Following the work by Jarmasz and Szpakow-icz (2003) and Turney (2006), we created a sec-ond dataset containing multiple choice questions.We collected 1072 multiple-choice word analogyquestions from the German Reader?s Digest WordPower Game (RDWP) from January 2001 to De-cember 2005 (Wallace and Wallace, 2005).
We dis-carded 44 questions that had more than one correctanswer, and 20 questions that used a phrase insteadof a single term as query.
The resulting 1008 ques-tions form our evaluation dataset.
An example ques-tion is given below:Muffin (muffin)a) Kleingeba?ck (small cake)b) Spenglerwerkzeug (plumbing tool)c) Miesepeter (killjoy)d) Wildschaf (moufflon)The task is to find the correct choice - ?a)?
in thiscase.This dataset is significantly larger than any of theprevious datasets employed in this type of evalua-tion.
Also, it is not restricted to synonym questions,as in the work by Jarmasz and Szpakowicz (2003),but also includes hypernymy/hyponymy, and fewnon-classical relations.3.3 Analysis of ResultsTable 2 gives the results of evaluation on the taskof correlating the results of an SR measure with hu-man judgments using Pearson correlation.
The Ger-maNet based LIN measure outperforms ESA on theGur65 dataset.
On the other datasets, ESA is betterthan LIN.
This is clearly due to the fact, that Gur65contains only noun-noun word pairs connected byclassical semantic relations, while the other datasetsalso contain cross part-of-speech pairs connected bynon-classical relations.
The Wikipedia based ESAmeasure can better capture such relations.
Addition-ally, Table 3 shows that ESA also covers almost allGUR65 GUR350 ZG222# covered word pairs 53 116 55Upper bound 0.80 0.64 0.44GermaNet Lin 0.73 0.50 0.08Wikipedia ESA 0.56 0.52 0.32Table 2: Pearson correlation r of human judgmentswith SR measures on word pairs covered by Ger-maNet and Wikipedia.COVERED PAIRSDATASET # PAIRS LIN ESAGur65 65 60 65Gur350 350 208 333ZG222 222 88 205Table 3: Number of covered word pairs based on Linor ESA measure on different datasets.word pairs in each dataset, while GermaNet is muchlower for Gur350 and ZG222.
ESA performs evenbetter on the Reader?s Digest task (see Table 4).
Itshows high coverage and near human performanceregarding the relative number of correctly solvedquestions.6 Given the high performance and cover-age of the Wikipedia based ESA measure, we expectit to yield better IR results than LIN.4 Information Retrieval4.1 IR ModelsPreprocessing For creating the search index forIR models, we apply first tokenization and then re-move stop words.
We use a general German stop6Values for human performance are for one subject.
Thus,they only indicate the approximate difficulty of the task.
Weplan to use this dataset with a much larger group of subjects.#ANSWERED #CORRECT RATIOHuman 1008 874 0.87GermaNet Lin 298 153 0.51Wikipedia ESA 789 572 0.72Table 4: Evaluation results on multiple-choice wordanalogy questions.1035word list extended with highly frequent domain spe-cific terms.
Before adding the remaining words tothe index, they are lemmatized.
We finally splitcompounds into their constituents, and add both,constituents and compounds, to the index.EB model Lucene7 is an open source text searchlibrary based on an EB model.
After matching thepreprocessed queries against the index, the docu-ment collection is divided into a set of relevant andirrelevant documents.
The set of relevant documentsis, then, ranked according to the formula given in thefollowing equation:rEB(d, q) =nq?i=1tf(tq, d)?idf(tq)?lengthNorm(d)where nq is the number of terms in the query,tf(tq, d) is the term frequency factor for term tqin document d, idf(tq) is the inverse document fre-quency of the term, and lengthNorm(d) is a nor-malization value of document d, given the numberof terms within the document.
We added a simplequery expansion algorithm using (i) synonyms, and(ii) hyponyms, extracted from GermaNet.IR based on SR For the (IR-1) model, we uti-lize two SR measures and a set of heuristics: (i)the Lin measure based on GermaNet (LIN), and (ii)the ESA measure based on Wikipedia (ESA-Word).This algorithm was applied to the German IR bench-mark with positive results by Mu?ller and Gurevych(2006).
The algorithm computes a SR score for eachquery and document term pair.
Scores above a pre-defined threshold are summed up and weighted bydifferent factors, which boost or lower the scores fordocuments, depending on howmany query terms arecontained exactly or contribute a high enough SRscore.
In order to integrate the strengths of tradi-tional IR models, the inverse document frequencyidf is considered, which measures the general im-portance of a term for predicting the content of adocument.
The final formula of the model is as fol-lows:rSR(d, q) =?ndi=1?nqj=1 idf(tq,j) ?
s(td,i, tq,j)(1 + nnsm) ?
(1 + nnr)7http://lucene.apache.orgwhere nd is the number of tokens in the document,nq the number of tokens in the query, td,i the i-thdocument token, tq,j the j-th query token, s(td,i, tq,j)the SR score for the respective document and queryterm, nnsm the number of query terms not exactlycontained in the document, nnr the number of querytokens, which do not contribute a SR score above thethreshold.For the (IR-2) model, we apply the ESA methodfor directly comparing the query with documents, asdescribed in Section 3.1.4.2 DataThe corpus employed in our experiments was builtbased on a real-life IR scenario in the domain ofECG, as described in Section 1.
The document col-lection is extracted from BERUFEnet,8 a databasecreated by the GFLO.
It contains textual descrip-tions of about 1,800 vocational trainings, and 4,000descriptions of professions.
We restrict the collec-tion to a subset of BERUFEnet documents, consist-ing of 529 descriptions of vocational trainings, dueto the process necessary to obtain relevance judg-ments, as described below.
The documents containnot only details of professions, but also a lot of infor-mation concerning the training and administrativeissues.
We only use those portions of the descrip-tions, which characterize the profession itself.We collected real natural language topics by ask-ing 30 human subjects to write an essay about theirprofessional interests.
The topics contain 130 words,on average.
Making relevance judgments for ECGrequires domain expertise.
Therefore, we applied anautomatic method, which uses the knowledge baseemployed by the GFLO, described in Section 1.
Toobtain relevance judgments, we first annotate eachessay with relevant keywords from the tagset of 41and retrieve a ranked list of professions, which wereassigned one or more keywords by domain experts.To map the ranked list to a set of relevant and ir-relevant professions, we use a threshold of 3, assuggested by career guidance experts.
This settingyields on average 93 relevant documents per topic.The quality of the automatically created gold stan-dard depends on the quality of the applied knowl-edge base.
As the knowledge base was created by8http://berufenet.arbeitsamt.de/1036domain experts and is at the core of the electronic ca-reer guidance system of the GFLO, we assume thatthe quality is adequate to ensure a reliable evalua-tion.4.3 Analysis of ResultsIn Table 5, we summarize the results of the ex-periments applying different IR models on theBERUFEnet data.
We build queries from naturallanguage essays by (QT-1) extracting nouns, verbs,and adjectives, (QT-2) using only nouns, and (QT-3) manually assigning suitable keywords from thetagset with 41 keywords to each topic.
We report theresults with two different thresholds (.85 and .98) forthe Lin model, and with three different thresholds(.11, .13 and .24) for the ESA-Word models.
Theevaluation metrics used are mean average precision(MAP), precision after ten documents (P10), thenumber of relevant returned documents (#RRD).
Wecompute the absolute value of Spearman?s rank cor-relation coefficient (SRCC) by comparing the rele-vance ranking of our system with the relevance rank-ing of the knowledge base employed by the GFLO.Using query expansion for the EB model de-creases the retrieval performance for most configu-rations.
The SR based models outperform the EBmodel in all configurations and evaluation metrics,except for P10 on the keyword based queries.
TheLin model is always outperformed by at least one ofthe ESA models, except for (QT-3).
(IR-2) performsbest on longer queries using nouns, verbs, adjectivesor just nouns.Comparing the number of relevant retrieved doc-uments, we observe that the IR models based on SRare able to return more relevant documents than theEB model.
This supports the claim that semanticknowledge is especially helpful for the vocabularymismatch problem, which cannot be addressed byconventional IR models.
E.g., only SR-based mod-els can find the job information technician for a pro-file which contains the sentence My interests andskills are in the field of languages and IT.
The jobcould only be judged as relevant, as the semanticrelation between IT in the profile and informationtechnology in the professional description could befound.In our analysis of the BERUFEnet results ob-tained on (QT-1), we noticed that many errors weredue to the topics expressed in free natural languageessays.
Some subjects deviated from the given taskto describe their professional interests and describedfacts that are rather irrelevant to the task of ECG,e.g.
It is important to speak different languages inthe growing European Union.
If all content wordsare extracted to build a query, a lot of noise is intro-duced.Therefore, we conducted further experimentswith (QT-2) and (QT-3): building the query usingonly nouns, and using manually assigned keywordsbased on the tagset of 41 keywords.
For example,the following query is built for the professional pro-file given in Section 1.Keywords assigned:care for/nurse/educate/teach; use/program computer;office; outside: outside facilities/naturalenvironment; animals/plantsIR results obtained on (QT-2) and (QT-3) showthat the performance is better for nouns, and sig-nificantly better for the queries built of keywords.This suggests that in order to achieve high IR perfor-mance for the task of Electronic Career Guidance,it is necessary to preprocess the topics by perform-ing information extraction to remove the noise fromfree text essays.
As a result of the preprocessing,natural language essays should be mapped to a setof keywords relevant for describing a person?s in-terests.
Our results suggest that the word-based se-mantic relatedness IR model (IR-1) performs signif-icantly better in this setting.5 ConclusionsWe presented a system for Electronic Career Guid-ance utilizing NLP and IR techniques.
Given a nat-ural language professional profile, relevant profes-sions are computed based on the information aboutsemantic relatedness.
We intrinsically evaluated andanalyzed the properties of two semantic relatednessmeasures utilizing the lexical semantic informationin a German wordnet and Wikipedia on the tasks ofestimating semantic relatedness scores and answer-ing multiple-choice questions.
Furthermore, we ap-plied these measures to an IR task, whereby theywere used either in combination with a set of heuris-tics or the Wikipedia based measure was used to di-rectly compute semantic relatedness of topics and1037MODEL(QT-1) NOUNS, VERBS, ADJ.
(QT-2) NOUNS (QT-3) KEYWORDSMAP P10 #RRD SRCC MAP P10 #RRD SRCC MAP P10 #RRD SRCCEB .39 .58 2581 .306 .38 .58 2297 .335 .54 .76 2755 .497EB+SYN .37 .56 2589 .288 .38 .57 2310 .331 .54 .73 2768 .530EB+HYPO .34 .47 2702 .275 .38 .56 2328 .327 .47 .65 2782 .399Lin .85 .41 .56 2787 .338 .40 .59 2770 .320 .59 .73 2787 .578Lin .98 .41 .61 2753 .326 .42 .59 2677 .341 .58 .74 2783 .563ESA-Word .11 .39 .56 2787 .309 .44 .63 2787 .355 .60 .77 2787 .535ESA-Word .13 .38 .59 2787 .282 .43 .62 2787 .338 .62 .76 2787 .550ESA-Word .24 .40 .60 2787 .259 .43 .60 2699 .306 .54 .73 2772 .482ESA-Text .47 .62 2787 .368 .55 .71 2787 .462 .56 .74 2787 .489Table 5: Information Retrieval performance on the BERUFEnet dataset.documents.
We experimented with three differentquery types, which were built from the topics by:(QT-1) extracting nouns, verbs, adjectives, (QT-2)extracting only nouns, or (QT-3) manually assign-ing several keywords to each topic from a tagset of41 keywords.In an intrinsic evaluation of LIN and ESA mea-sures on the task of computing semantic relatedness,we found that ESA captures the information aboutsemantic relatedness and non-classical semantic re-lations considerably better than LIN, which operateson an is-a hierarchy and, thus, better captures the in-formation about semantic similarity.
On the task ofsolving RDWP questions, the ESA measure signif-icantly outperformed the LIN measure in terms ofcorrectness.
On both tasks, the coverage of ESA ismuch better.
Despite this, the performance of LINand ESA as part of an IR model is only slightlydifferent.
ESA performs better for all lengths ofqueries, but the differences are not as significant asin the intrinsic evaluation.
This indicates that theinformation provided by both measures, based ondifferent knowledge bases, might be complementaryfor the IR task.When ESA is applied to directly compute seman-tic relatedness between topics and documents, it out-performs IR-1 and the baseline EB model by a largemargin for QT-1 and QT-2 queries.
For QT-3, i.e.,the shortest type of query, it performs worse thanIR-1 utilizing ESA and a set of heuristics.
Also,the performance of the baseline EB model is verystrong in this experimental setting.
This result in-dicates that IR-2 utilizing conventional informationretrieval techniques and semantic information fromWikipedia is better suited for longer queries provid-ing enough context.
For shorter queries, soft match-ing techniques utilizing semantic relatedness tend tobe beneficial.It should be born in mind, that the constructionof QT-3 queries involved a manual step of assigningthe keywords to a given essay.
In this experimen-tal setting, all models show the best performance.This indicates that professional profiles contain a lotof noise, so that more sophisticated NLP analysisof topics is required.
This will be improved in ourfuture work, whereby the system will incorporatean information extraction component for automat-ically mapping the professional profile to a set ofkeywords.
We will also integrate a component foranalyzing the sentiment structure of the profiles.
Webelieve that the findings from our work on apply-ing IR techniques to the task of Electronic CareerGuidance generalize to similar application domains,where topics and documents display similar proper-ties (with respect to their length, free-text structureand mismatch of vocabularies) and domain and lex-ical knowledge is required to achieve high levels ofperformance.AcknowledgmentsThis work was supported by the German ResearchFoundation under grant ?Semantic Information Re-trieval from Texts in the Example Domain Elec-tronic Career Guidance?, GU 798/1-2.
We are grate-ful to the Bundesagentur fu?r Arbeit for providingthe BERUFEnet corpus.
We would like to thank theanonymous reviewers for valuable feedback on thispaper.
We would also like to thank Piklu Gupta forhelpful comments.1038ReferencesDavid Ahn, Valentin Jijkoun, Gilad Mishne, KarinMu?ller, Maarten de Rijke, and Stefan Schlobach.2004.
Using Wikipedia at the TREC QA Track.
InProceedings of TREC 2004.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating WordNet-based Measures of Semantic Distance.Computational Linguistics, 32(1).Razvan Bunescu and Marius Pasca.
2006.
Using En-cyclopedic Knowledge for Named Entity Disambigua-tion.
In Proceedings of ACL, pages 9?16, Trento, Italy.Christiane Fellbaum.
1998.
WordNet An Electronic Lex-ical Database.
MIT Press, Cambridge, MA.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting Semantic Relatedness using Wikipedia-basedExplicit Semantic Analysis.
In Proceedings of The20th International Joint Conference on Artificial In-telligence (IJCAI), Hyderabad, India, January.Julio Gonzalo, Felisa Verdejo, Irina Chugur, and JuanCigarran.
1998.
Indexing with WordNet synsets canimprove text retrieval.
In Proceedings of the Coling-ACL ?98 Workshop Usage of WordNet in Natural Lan-guage Processing Systems, Montreal, Canada, August.Iryna Gurevych and Hendrik Niederlich.
2005.
Comput-ing semantic relatedness in german with revised infor-mation content metrics.
In Proceedings of ?OntoLex2005 - Ontologies and Lexical Resources?
IJCNLP?05Workshop, pages 28?33, October 11 ?
13.Iryna Gurevych.
2005.
Using the Structure of a Concep-tual Network in Computing Semantic Relatedness.
InProceedings of the 2nd International Joint Conferenceon Natural Language Processing, pages 767?778, JejuIsland, Republic of Korea.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?s the-saurus and semantic similarity.
In RANLP, pages 111?120.Boris Katz, Gregory Marton, Gary Borchardt, AlexisBrownell, Sue Felshin, Daniel Loreto, Jesse Louis-Rosenberg, Ben Lu, Federico Mora, Stephan Stiller,Ozlem Uzuner, and Angela Wilcox.
2005.
Externalknowledge sources for question answering.
In Pro-ceedings of the 14th Annual Text REtrieval Conference(TREC?2005), November.Claudia Kunze, 2004.
Lexikalisch-semantische Wort-netze, chapter Computerlinguistik und Sprachtech-nologie, pages 423?431.
Spektrum AkademischerVerlag.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th Interna-tional Conference on Machine Learning, pages 296?304.
Morgan Kaufmann, San Francisco, CA.Dan Moldovan and Rada Mihalcea.
2000.
Using Word-Net and lexical operators to improve Internet searches.IEEE Internet Computing, 4(1):34?43.Jane Morris and Graeme Hirst.
2004.
Non-ClassicalLexical Semantic Relations.
In Workshop on Com-putational Lexical Semantics, Human Language Tech-nology Conference of the North American Chapter ofthe ACL, Boston.Christof Mu?ller and Iryna Gurevych.
2006.
Exploringthe Potential of Semantic Relatedness in InformationRetrieval.
In Proceedings of LWA 2006 Lernen - Wis-sensentdeckung - Adaptivita?t: Information Retrieval,pages 126?131, Hildesheim, Germany.
GI-FachgruppeInformation Retrieval.Herbert Rubenstein and John B. Goodenough.
1965.Contextual Correlates of Synonymy.
Communicationsof the ACM, 8(10):627?633.Gerard Salton andMichael J. McGill.
1983.
Introductionto Modern Information Retrieval.
McGraw-Hill, NewYork.Gerard Salton, Edward Fox, and Harry Wu.
1983.
Ex-tended boolean information retrieval.
Communicationof the ACM, 26(11):1022?1036.Alan F. Smeaton, Fergus Kelledy, and Ruari O?Donell.1994.
TREC-4 Experiments at Dublin City Univer-sity: Thresholding posting lists, query expansion withWordNet and POS tagging of Spanish.
In Proceedingsof TREC-4, pages 373?390.Peter D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Ellen Vorhees.
1994.
Query expansion using lexical-semantic relations.
In Proceedings of the 17th An-nual ACM SIGIR Conference on Research and Devel-opment in Information Retrieval, pages 61?69.DeWitt Wallace and Lila Acheson Wallace.
2005.Reader?s Digest, das Beste fu?r Deutschland.
Jan2001?Dec 2005.
Verlag Das Beste, Stuttgart.Torsten Zesch and Iryna Gurevych.
2006.
AutomaticallyCreating Datasets for Measures of Semantic Related-ness.
In Proceedings of the Workshop on LinguisticDistances, pages 16?24, Sydney, Australia, July.
As-sociation for Computational Linguistics.Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.2007a.
Analyzing and Accessing Wikipedia as a Lexi-cal Semantic Resource.
In Biannual Conference of theSociety for Computational Linguistics and LanguageTechnology, pages 213?221, Tuebingen, Germany.Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.2007b.
Comparing Wikipedia and German Word-net by Evaluating Semantic Relatedness on MultipleDatasets.
In Proceedings of NAACL-HLT.1039
