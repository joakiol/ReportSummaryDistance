Proceedings of the BioNLP Shared Task 2013 Workshop, pages 139?143,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsOntology-based semantic annotation: an automatic hybrid rule-basedmethodSondes Bannour, Laurent Audibert and Henry SoldanoLIPN, UMR 7030 CNRSUniversite?
Paris 13, Sorbonne Paris Cite?, F-93430, Villetaneuse, Francefirstname.lastname@lipn.univ-paris13.frAbstractIn the perspective of annotating a text withrespect to an ontology, we have partici-pated in the subtask 1 of the BB BioNLP-ST whose aim is to detect, in the text,Bacteria Habitats and associate to themone or several categories from the Onto-Biotope ontology provided for the task.We have used a rule-based machine learn-ing algorithm (WHISK) combined with arule-based automatic ontology projectionmethod and a rote learning technique.
Thecombination of these three sources of rulesleads to good results with a SER measureclose to the winner and a best F-measure.1 IntroductionOntology-based semantic annotation consists inlinking fragments of a text to elements of a do-main ontology enabling the interpretation and theautomatic exploitation of the texts content.
Manysystems annotate texts with respect to an ontology(Dill et al 2003).
Some of them use machine-learning techniques to automate the annotationprocess (Ciravegna, 2000).On one side, machine-learning techniques de-pend strongly on the amount and quality of pro-vided training data sets and do not use informationavailable in the ontology.
On the other side, usingthe ontology to project its elements onto the textdepends strongly on the richness of the ontologyand may neglect important information availablein texts.Our participation in the subtask 1 (entity de-tection and categorization) of the BB BioNLP-ST leverages the provided OntoBiotope ontologyand the training and development data sets pre-processed using our annotation platform based onUIMA (Ferrucci and Lally, 2004) (section 2).
Wefirst tested, on the development set, a rule-basedmachine-learning algorithm (WHISK (Soderlandet al 1999)) that used training set examples (sec-tion 3).
Its results are limited because of the weak-nesses of training data (section 4).
We, then, com-puted a rule-based automatic ontology projectionmethod consisting in retrieving from the text fieldinformation content provided by the ontology (eg.name of the concept).
Thanks to the wealth ofthe OntoBiotope ontology, this method gave goodresults (section 5) that have been improved byadding a rote learning technique that uses train-ing examples and some filtering techniques (sec-tion 6).
Finally, we combined our method withWHISK results, which slightly improved the F-measure (section 7) on the development data.2 TextMarker and data preprocessingIn a rule-based information extraction or seman-tic annotation system, annotation rules are usuallywritten by a domain expert.
However, these rulescan be learned using a rule-based learning algo-rithm.
The TextRuler system (Kluegl et al 2009)is a framework for semi-automatic developmentof rule-based information extraction applicationsthat contains some implementations of such algo-rithms ((LP)2 (Ciravegna, 2001; Ciravegna, 2003),WHISK (Soderland et al 1999), RAPIER (Califfand Mooney, 2003), BWI (Freitag and Kushmer-ick, 2000) and WIEN (Kushmerick et al 1997)).TextRuler is based on Apache UIMA TextMarkerwhich is a rule-based script language.TextMarker is roughly similar to JAPE (Cun-ningham et al 2000), but based on UIMA (Fer-rucci and Lally, 2004) rather than GATE (Cun-ningham, 2002).
According to some users ex-periences, it is even more complete than JAPE.Here is an example that gives an idea about howto write and use TextMarker rules: Given anUIMA type system that contains the types SPACE(whitespace) and Lemma (with a feature ?lemma?containing the lemmatized form of the matched139word), the following rule can be used to recognizethe term ?human body?
in whatever form it ap-pears in the text (singular, plural, uppercase, low-ercase):Lemma{FEATURE("lemma","human")}SPACE Lemma{FEATURE("lemma","body")--> MARK(Habitat, 1, 2, 3)};This rule allows the creation of an annotationcalled ?Habitat?
that covers the three matched pat-terns of the condition part of the rule.To be able to use TextMarker, we have used ourannotation platform based on UIMA to preprocessdata with:?
Tokenisation, lemmatisation, sentence split-ting and PoS-tagging of input data usingBioC (Smith et al 2004; Liu et al 2012).?
Term extraction using BioYatea (Golik etal., 2013), a term extractor adapted to thebiomedical domain.?
Bacteria Habitat annotation to train learningalgorithms using annotation files provided inthis task (.a2).For simplicity reasons, we do not take into ac-count discontinuous annotations.
We consider adiscontinuous annotation as the smallest segmentthat include all fragments.3 Rule Learning using WHISK?In the subtask 1 of the BB BioNLP-ST, par-ticipants must detect the boundaries of BacteriaHabitat entities and, for each entity, assign oneor several concepts of the OntoBiotope ontology.
?Should we decompose the task into two subtaskslike it is suggested in the task formulation : (1) en-tity detection and (2) categorization ?
To answerthis question, we have conducted two experiments.?
Learning the root concept Habitat without as-signing a Category to matched terms.?
Learning Bacteria Categories directly: eachHabitat Category is learned independently.For the two experiments we considered onlyCategories that have more than two examples inthe training set to train WHISK.
Results are shownin Table 1:Experiment Precision Recall F-measureHabitats learning 76.9% 24.5% 37.2%Categories learning 77.3% 24% 36.6%Table 1: Habitats learning vs Categories learningWHISK gives an acceptable precision but alow recall (the explanation is provided in sec-tion 4) for both experiments.
There is no bigdifference between the two experiments?
results:WHISK doesn?t generalize over Habitats Cate-gories.
Learning Habitat Categories seems to bethe easier and safer way to use WHISK in this task.4 Weaknesses of training examplesexplain poor rule learning resultsTraining Development TotalNb.
Concepts: 333 274 491Nb.
Habitat: 934 611 1545Nb.
Annotation: 948 626 1574Nb.
C. 1 Instance: 182 179 272Nb.
C. 2 Instances: 66 41 86Nb.
C. > 2 Instances: 27 15 133Number of concepts in ontology: 1756Table 2: Figures on provided dataA close look at data samples helps understandwhy the WHISK algorithm did not obtain good re-sults.
Table 2 exhibits some figures on training anddevelopment data:?
158 of the 274 concepts (58%) present in thedevelopment data do not appear in the train-ing data.?
Concepts present in sample data account for19% of the ontology for the training data,16% for the development data and 28% fortheir combination.?
Obviously, it is difficult for a machine learn-ing algorithm to learn (i.e.
generalize) ononly one instance.
This is the case for 55%(272) of the concepts considering both thetraining and the development sample data.?
If we consider that at least 3 instances areneeded to apply a machine learning algo-rithm, only 27% of concepts present in thetraining or development data are concerned.This means that the ontology coverage is lessthan 8%.The conclusion is that training data are toosmall to lead to a high performance recall for amachine learning algorithm based exclusively onthese data.5 The wealth of the ontology helps buildan efficient ontology-based rule setThe BB BioNLP-ST?s subtask 1 provides the On-toBiotope ontology used to tag samples.
For ex-140ample, the information provided by the ontologyfor the concept MBTO:00001516 is[Term]id: MBTO:00001516name: microorganismexact_synonym: "microbe" [TyDI:23602]related_synonym: "microbial" [TyDI:23603]is_a: MBTO:00000297 !
living organismText segments tagged with this concept in ex-amples are : microbe, microbial, microbes,microorganisms, harmless stomach bugs.One can notice that the name, exact synonymand related synonym field information providedby the ontology can help identify these segments.If this strategy works, it will be a very robust onebecause it is not sample dependent and it is ap-plicable for all the 1756 concepts present in theontology.The main idea is to directly search and tag inthe corpus the information provided by the con-tent of fields name, exact synonym and related-synonym of the ontology.
Of course, projectingthem directly on samples raises inflection issues.Our corpus provides two levels of lemmatisationto avoid inflection problems: one from BioC andthe other from BioYaTeA.
Our experiments showthat using the two of them in conjunction with thetoken level (without any normalisation of words)provides the best results.
For example, the rules toproject name field of MBTO:00001516 are:Token{REGEXP("?microorganism$")-> MARKONCE(MBTO:00001516,1)} ;Lemma{FEATURE("lemma","microorganism$")-> MARKONCE(MBTO:00001516,1)} ;Term{FEATURE("lemma","microorganism$")-> MARKONCE(MBTO:00001516,1)} ;Table 3 provides results obtained on develop-ment data.
We have also used training data to gen-erate rote learning rules introduced in the next sec-tion.Rule set name Precision Recall F-measurename: 67.4% 61.2% 64.2%exact synonym: 61.2% 4.2% 7.8%related synonym: 26.6% 5.9% 9.7%rote learning: 63.6% 50.2% 56.1%all together: 58.9% 73.8% 65.5%Table 3: Performances of some sets of rules6 Improving ontology-based rulesRote learning rulesResults obtained for name and exact synonymrules in Table 3 are very encouraging.
We canapply the same strategy of automatic rule genera-tion from training data to text segments covered bytraining examples.
Projection rules are generated,as described in section 5, for each example seg-ment using the associated concept?s name as therule conclusion.
This is a kind of rote learning.Of course, we use an appropriate normalised ver-sion of example segment to produce appropriaterules based on BioC lemmatisation and BioYaTeAlemmatisation1.
For example, rote learning rulesfor the segment harmless stomach bugs taggedas MBTO:00000297 in trainning data are:Token{REGEXP("?harmless$")}Token{REGEXP("?stomach$")}Token{REGEXP("?bugs$")-> MARKONCE(MBTO:00001516,1,3)} ;Lemma{FEATURE("lemma","harmless")}Lemma{FEATURE("lemma","stomach")}Lemma{FEATURE("lemma","bug")-> MARKONCE(MBTO00001516,1,3)} ;Rule sets filteringRule set name Precision Recall F-measurename: 87.6% 55.1% 67.6%exact synonym: 94.4% 2.7% 5.3%related synonym: 71.4% 2.4% 4.6%rote learning: 75.8% 44% 55.8%all together: 80.9% 63.4% 71.1%all together bis: 81.4% 63.4% 71.2%Table 4: Performances of sets of filtered rulesA detailed analysis shows that our strategyworks well on the majority of concepts, but pro-duces poor results for some concepts.
To over-come this limitation, we have adopted a strategyconsisting in filtering (deleting) rules that producelots of erroneous matches.
More precisely, wehave deleted rules that match at least one time andthat conclude on a concept that obtains both a pre-cision less or equal to 0.66 and a F-measure less orequal to 0.66.
This filtering is computed on train-ing data.
Table 4 shows performances on develop-ment data obtained by filtered versions of rules oftable 3.Rule sets combinationOur goal is to maximise the F-measure.
F-measure in table 4 for exact synonym andrelated synonym rules is worse than in table 3 be-cause of the decrease of the recall.
But the com-bination of the four simple rule sets allows to re-cover some of the lost recall.
The significative im-1The information from BioYaTeA exists only for seg-ments identified as a term.141provement of precision finally leads to an overallimprovement of the F-measure (all together in ta-ble 4).
Removing either one of the four sets ofrules that constitute the all together set of rulesfrom table 4 leads systematically to a decrease ofthe F-measure.Embedded rules removingWe have noticed a phenomenon that decreases pre-cision and that can be corrected when combiningontology-based sets of rules with the rote learn-ing set of rules.
To illustrate it, the name of theconcept MBTO:00002027 is plant.
Among exam-ples tagged with this concept, we can find healthyplants.
The name rule set matches on plantsand tags it with MBTO:00002027 (which is a mis-take), while the rote learning rule set matches onhealthy plants and tags it with MBTO:00002027.It is possible to correct this problem by a simplerule that unmarks such embedded rules:MBTO:00002027{ PARTOFNEQ( MBTO:00002027 )-> UNMARK( MBTO:00002027 ) } ;We have generated such a rule systematically forall the concepts of the ontology to remove a fewmistakes (all together bis set of rules in table 4).7 Adding Learned rulesFinally, we have completed the all together bisset of filtered rules with the rules produced by theWHISK algorithm.
The difference between all to-gether bis + whisk set of rules and the submittedset of rules is that, by mistake, the last one did notcontain the related synonym rule set.It is important to mention that all rules may ap-ply simultaneously.
There is also no execution or-der between them except for rules that remove em-bedded ones which must be applied at the end ofthe rules set but before WHISK rules.Rule set name Precision Recall F-measureall together bis: 81.4% 63.4% 71.2%all[...] + whisk: 79.1% 65% 71.4%submitted: 79.3% 64.4% 71.1%Table 5: Performances of final sets of rules on devdataTable 5 summarises performances achieved byour final rule sets.
Precision, Recall and F-measure are computed on the development datawith rules based on the training data.Table 6 summarises performances on test datawith the evaluator?s measures achieved by our fi-nal rule sets based on training plus developmentdata.Rule set name Precision Recall F1 SERall together bis: 66.5% 61.4% 63.9% 42.5%all[...] + WHISK: 61.4% 64.4% 62.9% 46.0%submitted: 60.8% 60.8% 60.8% 48.7%IRISA-TexMex (winner): 48% 72% 57% 46%Table 6: Performances of final sets of rules on testdataThe subtask 1 of the BB BioNLP-ST rankscompetitors using the SER measure that must beas close as possible to 0.
We are quite close to thewinner with a SER of 48.7% against 46%.
OurF-measure (60.8%) is even better than the win-ner?s F-measure (57%).
Without our mistake, wewould have been placed equal first with a far bet-ter F-measure (62.9%).
We can also notice thatthe WHISK rule set contribution is negative whileit was not the case on the developement data.8 Conclusion and perspectivesGiven the wealth of the OntoBiotope ontologyprovided for subtask 1 of the BB BioNLP-ST, wehave decided to use a method that consists in iden-tifying Bacteria Habitats using information avail-able in this ontology.
The method we have used isrule-based and allows the automatic establishmentof a set of rules, written in the TextMarker lan-guage, that match every ontology element (HabitatCategory) with its exact name, exact synonyms orrelated synonyms in the text.
As expected, thismethod has achieved good results improved byadding a rote learning technique based on train-ing examples and filtering techniques that elimi-nate categories that don?t perform well on the de-velopment set.The WHISK algorithm was also used to learnBacteria Habitats Categories.
It gives a good pre-cision but a low recall because of the povertyof training data.
Its combination with the ontol-ogy projection method improves the recall and F-measure in developement data but not in the finaltest data.The combination of these sources of rules leadsto good results with a SER measure close to thewinner and a best F-measure.Actually, due to implementation limitations,WHISK rules are essentially based on the Tokenlevel (inflected form) of the corpus.
Improvementscan be made by ameliorating this implementation142considering the lemmatized form of words, theirpostags and also terms extracted by a term extrac-tor.
There is also another way of improvementthat consists in taking into account the is a rela-tion of the ontology, both on WHISK rule set andon ontology-based projection rules.
Last, a closerlook at false positive and false negative errors canlead to some improvements.AcknowledgmentsThis work was realized as part of the Quaero Pro-gramme funded by OSEO, French State agency forinnovation.ReferencesMary Elaine Califf and Raymond J. Mooney.
2003.Bottom-up relational learning of pattern matchingrules for information extraction.
J. Mach.
Learn.Res., 4:177?210, December.Fabio Ciravegna.
2000.
Learning to tag for infor-mation extraction from text.
In Proceedings of theECAI-2000 Workshop on Machine Learning for In-formation Extraction.Fabio Ciravegna.
2001.
(lp)2, an adaptive algorithmfor information extraction from web-related texts.In In Proceedings of the IJCAI-2001 Workshop onAdaptive Text Extraction and Mining.Fabio Ciravegna.
2003.
(lp)2: Rule induction forinformation extraction using linguistic constraints.Technical report.Hamish Cunningham, Diana Maynard and ValentinTablan.
2000.
JAPE: a Java Annotation Pat-terns Engine (Second Edition).
Technical report, ofSheffield, Department of Computer Science.Hamish Cunningham.
2002.
Gate, a general architec-ture for text engineering.
Computers and the Hu-manities, 36(2):223?254.Stephen Dill, Nadav Eiron, David Gibson, DanielGruhl, R. Guha, Anant Jhingran, Tapas Kanungo,Kevin S. Mccurley, Sridhar Rajagopalan, AndrewTomkins, John A. Tomlin, and Jason Y. Zien.
2003.A case for automated large scale semantic annota-tions.
Journal of Web Semantics, 1:115?132.David Ferrucci and Adam Lally.
2004.
Uima: anarchitectural approach to unstructured informationprocessing in the corporate research environment.Nat.
Lang.
Eng., 10:327?348.Dayne Freitag and Nicholas Kushmerick.
2000.Boosted wrapper induction.
pages 577?583.
AAAIPress.Wiktoria Golik, Robert Bossy, Zorana Ratkovic, andNe?dellec Claire.
2013.
Improving Term Extractionwith Linguistic Analysis in the Biomedical Domain.Proceedings of the 14th International Conference onIntelligent Text Processing and Computational Lin-guistics (CICLing13), Special Issue of the journalResearch in Computing Science, pages 24?30.Peter Kluegl, Martin Atzmueller, Tobias Hermann,and Frank Puppe.
2009.
A framework for semi-automatic development of rule-based informationextraction applications.
In Proc.
LWA 2009 (KDML- Special Track on Knowledge Discovery and Ma-chine Learning), pages 56?59.Nicholas Kushmerick, Daniel S. Weld and RobertDoorenbos.
1997.
Wrapper induction for informa-tion extraction.
In Proc.
Int.
Joint Conf.
ArtificialIntelligence.Haibin Liu, Tom Christiansen, William A. Baumgart-ner, and Karin Verspoor.
2012.
BioLemmatizer: alemmatization tool for morphological processing ofbiomedical text.
Journal of biomedical semantics,3(1):3+.Stephen Soderland, Claire Cardie, and RaymondMooney.
1999.
Learning information extractionrules for semi-structured and free text.
In MachineLearning, pages 233?272.Lawrence H. Smith, Thomas C. Rindflesch and W.John Wilbur.
2004.
MedPost: a part-of-speechtagger for bioMedical text.
Bioinformatics (Oxford,England), 20(14):2320?2321, September.143
