Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 921?928,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Bootstrapping Approach to Unsupervised Detection of Cue PhraseVariantsRashid M. Abdalla and Simone TeufelComputer Laboratory, University of Cambridge15 JJ Thomson Avenue, Cambridge CB3 OFD, UKrma33@cam.ac.uk, sht25@cam.ac.ukAbstractWe investigate the unsupervised detectionof semi-fixed cue phrases such as ?Thispaper proposes a novel approach.
.
.
1?from unseen text, on the basis of only ahandful of seed cue phrases with the de-sired semantics.
The problem, in contrastto bootstrapping approaches for QuestionAnswering and Information Extraction, isthat it is hard to find a constraining contextfor occurrences of semi-fixed cue phrases.Our method uses components of the cuephrase itself, rather than external con-text, to bootstrap.
It successfully excludesphrases which are different from the tar-get semantics, but which look superficiallysimilar.
The method achieves 88% ac-curacy, outperforming standard bootstrap-ping approaches.1 IntroductionCue phrases such as ?This paper proposes a novelapproach to.
.
.
?, ?no method for .
.
.
exists?
or even?you will hear from my lawyer?
are semi-fixed inthat they constitute a formulaic pattern with a clearsemantics, but with syntactic and lexical variationswhich are hard to predict and thus hard to detectin unseen text (e.g.
?a new algorithm for .
.
.
issuggested in the current paper?
or ?I envisage le-gal action?).
In scientific discourse, such meta-discourse (Myers, 1992; Hyland, 1998) aboundsand plays an important role in marking the dis-course structure of the texts.Finding these variants can be useful for manytext understanding tasks because semi-fixed cuephrases act as linguistic markers indicating the im-portance and/or the rhetorical role of some ad-jacent text.
For the summarisation of scientific1In contrast to standard work in discourse linguistics,which mostly considers sentence connectives and adverbialsas cue phrases, our definition includes longer phrases, some-times even entire sentences.papers, cue phrases such as ?Our paper dealswith.
.
.
?
are commonly used as indicators ofextraction-worthiness of sentences (Kupiec et al,1995).
Re-generative (rather than extractive) sum-marisation methods may want to go further thanthat and directly use the knowledge that a certainsentence contains the particular research aim of apaper, or a claimed gap in the literature.
Similarly,in the task of automatic routing of customer emailsand automatic answering of some of these, the de-tection of threats of legal action could be useful.However, systems that use cue phrases usuallyrely on manually compiled lists, the acquisitionof which is time-consuming and error-prone andresults in cue phrases which are genre-specific.Methods for finding cue phrases automatically in-clude Hovy and Lin (1998) (using the ratio ofword frequency counts in summaries and their cor-responding texts), Teufel (1998) (using the mostfrequent n-grams), and Paice (1981) (using a pat-tern matching grammar and a lexicon of manu-ally collected equivalence classes).
The main is-sue with string-based pattern matching techniquesis that they cannot capture syntactic generalisa-tions such as active/passive constructions, differ-ent tenses and modification by adverbial, adjecti-val or prepositional phrases, appositions and otherparenthetical material.For instance, we may be looking for sentencesexpressing the goal or main contribution of a pa-per; Fig.
1 shows candidates of such sentences.Cases a)?e), which do indeed describe the authors?goal, display a wide range of syntactic variation.a) In this paper, we introduce a method for similarity-based estimation of .
.
.b) We introduce and justify a method.
.
.c) A method (described in section 1) is introducedd) The method introduced here is a variation.
.
.e) We wanted to introduce a method.
.
.f) We do not introduce a method.
.
.g) We introduce and adopt the method given in [1].
.
.h) Previously we introduced a similar method.
.
.i) They introduce a similar method.
.
.Figure 1: Goal statements and syntactic variation ?
cor-rect matches (a-e) and incorrect matches (f-i)921Cases f)?i) in contrast are false matches: they donot express the authors?
goals, although they aresuperficially similar to the correct contexts.
Whilestring-based approaches (Paice, 1981; Teufel,1998) are too restrictive to cover the wide varia-tion within the correct contexts, bag-of-words ap-proaches such as Agichtein and Gravano?s (2000)are too permissive and would miss many of thedistinctions between correct and incorrect con-texts.Lisacek et al (2005) address the task of iden-tifying ?paradigm shift?
sentences in the biomed-ical literature, i.e.
statements of thwarted expec-tation.
This task is somewhat similar to ours inits definition by rhetorical context.
Their methodgoes beyond string-based matching: In order for asentence to qualify, the right set of concepts mustbe present in a sentence, with any syntactic re-lationship holding between them.
Each conceptset is encoded as a fixed, manually compiled listsof strings.
Their method covers only one particu-lar context (the paradigm shift one), whereas weare looking for a method where many types of cuephrases can be acquired.
Whereas it relies on man-ually assembled lists, we advocate data-driven ac-quisition of new contexts.
This is generally pre-ferrable to manual definition, as language use ischanging, inventive and hard to predict and asmany of the relevant concepts in a domain may beinfrequent (cf.
the formulation ?be cursed?, whichwas used in our corpus as a way of describing amethod?s problems).
It also allows the acquisitionof cue phrases in new domains, where the exactprevalent meta-discourse might not be known.Riloff?s (1993) method for learning informationextraction (IE) patterns uses a syntactic parse andcorrespondences between the text and filled MUC-style templates to learn context in terms of lexico-semantic patterns.
However, it too requires sub-stantial hand-crafted knowledge: 1500 filled tem-plates as training material, and a lexicon of se-mantic features for roughly 3000 nouns for con-straint checking.
Unsupervised methods for simi-lar tasks include Agichtein and Gravano?s (2000)work, which shows that clusters of vector-space-based patterns can be successfully employed todetect specific IE relationships (companies andtheir headquarters), and Ravichandran and Hovy?s(2002) algorithm for finding patterns for a Ques-tion Answering (QA) task.
Based on training ma-terial in the shape of pairs of question and answerterms ?
e.g., (e.g.
{Mozart, 1756}), they learn thea) In this paper, we introduce a method for similarity-based estimation of .
.
.b) Here, we present a similarity-based approach for esti-mation of.
.
.c) In this paper, we propose an algorithm which is .
.
.d) We will here dene a technique for similarity-based.
.
.Figure 2: Context around cue phrases (lexical variants)semantics holding between these terms (?birthyear?)
via frequent string patterns occurring in thecontext, such as ?A was born in B?, by consider-ing n-grams of all repeated substrings.
What iscommon to these three works is that bootstrappingrelies on constraints between the context externalto the extracted material and the extracted mate-rial itself, and that the target extraction material isdefined by real-world relations.Our task differs in that the cue phrases we ex-tract are based on general rhetorical relations hold-ing in all scientific discourse.
Our approach forfinding semantically similar variants in an unsu-pervised fashion relies on bootstrapping of seedsfrom within the cue phrase.
The assumption is thatevery semi-fixed cue phrase contains at least twomain concepts whose syntax and semantics mutu-ally constrain each other (e.g.
verb and direct ob-ject in phrases such as ?
(we) present an approachfor?).
The expanded cue phrases are recognisedin various syntactic contexts using a parser2.
Gen-eral semantic constraints valid for groups of se-mantically similar cue phrases are then applied tomodel, e.g., the fact that it must be the authors whopresent the method, not somebody else.We demonstrate that such an approach is moreappropriate for our task than IE/QA bootstrappingmechanisms based on cue phrase-external con-text.
Part of the reason for why normal boot-strapping does not work for our phrases is the dif-ficulty of finding negatives contexts, essential inbootstrapping to evaluate the quality of the pat-terns automatically.
IE and QA approaches, dueto uniqueness assumptions of the real-world rela-tions that these methods search for, have an auto-matic definition of negative contexts by hard con-straints (i.e., all contexts involving Mozart and anyother year are by definition of the wrong seman-tics; so are all contexts involving Microsoft anda city other than Redmond).
As our task is notgrounded in real-world relations but in rhetoricalones, constraints found in the context tend to be2Thus, our task shows some parallels to work in para-phrasing (Barzilay and Lee, 2002) and syntactic variant gen-eration (Jacquemin et al, 1997), but the methods are verydifferent.922soft rather than hard (cf.
Fig 2): while it it possiblethat strings such as ?we?
and ?in this paper?
occurmore often in the context of a given cue phrase,they also occur in many other places in the paperwhere the cue phrase is not present.
Thus, it ishard to define clear negative contexts for our task.The novelty of our work is thus the new patternextraction task (finding variants of semi-fixed cuephrases), a task for which it is hard to directly usethe context the patterns appear in, and an iterativeunsupervised bootstrapping algorithm for lexicalvariants, using phrase-internal seeds and rankingsimilar candidates based on relation strength be-tween the seeds.While our method is applicable to general cuephrases, we demonstrate it here with transitiveverb?direct object pairs, namely a) cue phrases in-troducing a new methodology (and thus the mainresearch goal of the scientific article; e.g.
?Inthis paper, we propose a novel algorithm.
.
.
?)
?we call those goal-type cue phrases; and b) cuephrases indicating continuation of previous otherresearch (e.g.
?Therefore, we adopt the approachpresented in [1].
.
.
?)
?
continuation-type cuephrases.2 Lexical Bootstrapping AlgorithmThe task of this module is to find lexical vari-ants of the components of the seed cue phrases.Given the seed phrases ?we introduce a method?and ?we propose a model?, the algorithm startsby finding all direct objects of ?introduce?
in agiven corpus and, using an appropriate similar-ity measure, ranks them according to their dis-tributional similarity to the nouns ?method?
and?model?.
Subsequently, the noun ?method?
is usedto find transitive verbs and rank them according totheir similarity to ?introduce?
and ?propose?.
Inboth cases, the ranking step retains variants thatpreserve the semantics of the cue phrase (e.g.
?de-velop?
and ?approach?)
and filters irrelevant termsthat change the phrase semantics (e.g.
?need?
and?example?
).Stopping at this point would limit us to thoseterms that co-occur with the seed words in thetraining corpus.
Therefore additional iterations us-ing automatically generated verbs and nouns areapplied in order to recover more and more vari-ants.
The full algorithm is given in Fig.
3.The algorithm requires corpus data for the stepsHypothesize (producing a list of potential candi-dates) and Rank (testing them for similarity).
WeInput: Tuples {A1, A2, .
.
.
, Am} and {B1, B2, .
.
.
, Bn}.Initialisation: Set the concept-A reference set to{A1, A2, .
.
.
, Am} and the concept-B reference set to{B1, B2, .
.
.
, Bn}.
Set the concept-A active element to A1and the concept-B active element to B1.Recursion:1.
Concept B retrieval:(i) Hypothesize: Find terms in the corpus whichare in the desired relationship with the concept-Aactive element (e.g.
direct objects of a verb activeelement).
This results in the concept-B candidateset.
(ii) Rank: Rank the concept-B candidate set usinga suitable ranking methodology that may make useof the concept-B reference set.
In this process, eachmember of the candidate set is assigned a score.
(iii) Accumulate: Add the top s items of theconcept-B candidate set to the concept-B accumu-lator list (based on empirical results, s is the rank ofthe candidate set during the initial iteration and 50for the remaining iterations).
If an item is alreadyon the accumulator list, add its ranking score to theexisting item?s score.2.
Concept A retrieval: as above, with concepts A andB swapped.3.
Updating active elements:(i) Set the concept-B active element to the highestranked instance in the concept-B accumulator listwhich has not been used as an active element be-fore.
(ii) Set the concept-A active element to the highestranked instance in the concept-A accumulator listwhich has not been used as an active element be-fore.Repeat steps 1-3 for k iterationsOutput: top M words of concept-A (verb) accumulator listand top N words of concept-B (noun) accumulator listReference set: a set of seed words which define the col-lective semantics of the concept we are looking for in thisiterationActive element: the instance of the concept used in the cur-rent iteration for retrieving instances of the other concept.If we are finding lexical variants of Concept A by exploit-ing relationships between Concepts A and B, then the activeelement is from Concept B.Candidate set: the set of candidate terms for one concept(eg.
Concept A) obtained using an active element from theother concept (eg.
Concept B).
The more semantically simi-lar a term in the candidate set is to the members of the refer-ence set, the higher its ranking should be.
This set containsverbs if the active element is a noun and vice versa.Accumulator list: a sorted list that accumulates the rankedmembers of the candidate set.Figure 3: Lexical variant bootstrapping algorithmestimate frequencies for the Rank step from thewritten portion of the British National Corpus(BNC, Burnard (1995)), 90 Million words.
Forthe Hypothesize step, we experiment with twodata sets: First, the scientific subsection of theBNC (24 Million words), which we parse usingRASP (Briscoe and Carroll, 2002); we then ex-amine the grammatical relations (GRs) for transi-tive verb constructions, both in active and passivevoice.
This method guarantees that we find al-most all transitive verb constructions cleanly; Car-roll et al (1999) report an accuracy of .85 for923DOs, Active: "AGENT STRING AUX active-verb-element DETERMINER * POSTMOD"DOs, Passive: "DETERMINER * AUX active-verb-element element"TVs, Active: "AGENT STRING AUX * DETERMINER active-noun- element POSTMOD"TVs, Passive:"DET active-noun-element AUX * POSTMOD"Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step.newspaper articles for this relation.
Second, inorder to obtain larger coverage and more currentdata we also experiment with Google Scholar3, anautomatic web-based indexer of scientific litera-ture (mainly peer-reviewed papers, technical re-ports, books, pre-prints and abstracts).
GoogleScholar snippets are often incomplete fragmentswhich cannot be parsed.
For practical reasons, wedecided against processing the entire documents,and obtain an approximation to direct objects andtransitive verbs with regular expressions over theresult snippets in both active and passive voice(cf.
Fig.
4), designed to be high-precision4 .
Theamount of data available from BNC and GoogleScholar is not directly comparable: harvestingGoogle Scholar snippets for both active and pas-sive constructions gives around 2000 sentences perseed (Google Scholar returns up to 1000 resultsper query), while the number of BNC sentencescontaining seed words in active and passive formvaries from 11 (?formalism?)
to 5467 (?develop?
)with an average of 1361 sentences for the experi-mental seed pairs.RankingHaving obtained our candidate sets (either fromthe scientific subsection of the BNC or fromGoogle Scholar), the members are ranked usingBNC frequencies.
We investigate two rankingmethodologies: frequency-based and context-based.
Frequency-based ranking simply rankseach member of the candidate set by how manytimes it is retrieved together with the current activeelement.
Context-based ranking uses a similaritymeasure for computing the scores, giving a higherscore to those words that share sufficiently similarcontexts with the members of the reference set.We consider similarity measures in a vector spacedefined either by a fixed window, by the sentencewindow, or by syntactic relationships.
The scoreassigned to each word in the candidate set is thesum of its semantic similarity values computedwith respect to each member in the reference set.3http://scholar.google.com4The capitalised words in these patterns are replaced byactual words (e.g.
AGENT STRING: We/I, DETERMINER:a/ an/our), and the extracted words (indicated by ?*?)
are lem-matised.Syntactic contexts, as opposed to window-basedcontexts, constrain the context of a word to onlythose words that are grammatically related toit.
We use verb-object relations in both activeand passive voice constructions as did Pereira etal.
(1993) and Lee (1999), among others.
Weuse the cosine similarity measure for window-based contexts and the following commonlyused similarity measures for the syntactic vectorspace: Hindle?s (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the ?-Skew diver-gence measure (Lee, 1999), the Jensen-Shannon(JS) divergence measure (Lin, 1991), Jaccard?scoefficient (van Rijsbergen, 1979) and the Con-fusion probability (Essen and Steinbiss, 1992).The Jensen-Shannon measure JS (x1, x2) =?y?Y?x?
{x1,x2}(P (y|x) log(P(y|x)12 (P(y|x1)+P(y|x2))))subsequently performed best for our task.
Wecompare the different ranking methodologiesand data sets with respect to a manually-definedgold standard list of 20 goal-type verbs and 20nouns.
This list was manually assembled fromTeufel (1999); WordNet synonyms and otherplausible verbs and nouns found via Web searcheson scientific articles were added.
We ensured bysearches on the ACL anthology that there is goodevidence that the gold-standard words indeedoccur in the right contexts, i.e.
in goal statementsentences.
As we want to find similarity metricsand data sources which result in accumulator listswith many of these gold members at high ranks,we need a measure that rewards exactly thoselists.
We use non-interpolated Mean AveragePrecision (MAP), a standard measure for eval-uating ranked information retrieval runs, whichcombines precision and recall and ranges from 0to 15.We use 8 pairs of 2-tuples as input (e.g.
[in-troduce, study] & [approach, method]), randomlyselected from the gold standard list.
MAP was cal-5MAP = 1N?Nj=1 APj =1N?Nj=11M?Mi=1 P (gi)where P (gi) = nijrij if gi is retrieved and 0 otherwise, N isthe number of seed combinations, M is the size of the goldenlist, gi is the ith member of the golden list and rij is its rankin the retrieved list of combination j while nij is the numberof golden members found up to and including rank rij .924Ranking scheme BNC Google ScholarFrequency-based 0.123 0.446Sentence-window 0.200 0.344Fixedsize-window 0.184 0.342Hindle 0.293 0.416Weighted Lin 0.358 0.509?-Skew 0.361 0.486Jensen-Shannon 0.404 0.550Jaccard?s coef.
0.301 0.436Confusion prob.
0.171 0.293Figure 5: MAPs after the first iterationculated over the verbs and nouns retrieved usingour algorithm and averaged.
Fig.
5 summarises theMAP scores for the first iteration, where GoogleScholar significantly outperformed the BNC.
Thebest result for this iteration (MAP=.550) wasachieved by combining Google Scholar and theJensen-Shannon measure.
The algorithm stops toiterate when no more improvement can be ob-tained, in this case after 4 iterations, resulting ina final MAP of .619.Although ?-Skew outperforms the simpler mea-sures in ranking nouns, its performance on verbsis worse than the performance of Weighted Lin.While Lee (1999) argues that ?-Skew?s asymme-try can be advantageous for nouns, this probablydoes not hold for verbs: verb hierarchies havemuch shallower structure than noun hierarchieswith most verbs concentrated on one level (Milleret al, 1990).
This would explain why JS, whichis symmetric compared to the ?-Skew metric, per-formed better in our experiments.In the evaluation presented here we thereforeuse Google Scholar data and the JS measure.
Anadditional improvement (MAP=.630) is achievedwhen we incorporate a filter based on the follow-ing hypothesis: goal-type verbs should be morelikely to have their direct objects preceded by in-definite articles rather than definite articles or pos-sessive determiners (because a new method is in-troduced) whereas continuation-type verbs shouldprefer definite articles with their direct objects (asan existing method is involved).3 Syntactic variants and semantic filtersThe syntactic variant extractor takes as its inputthe raw text and the lists of verbs and nouns gen-erated by the lexical bootstrapper.
After RASP-parsing the input text, all instances of the inputverbs are located and, based on the grammaticalrelations output by RASP6, a set of relevant en-6The grammatical relations used are nsubj, dobj, iobj,aux, argmod, detmod, ncmod and mod.The agent of the verb (e.g., ?We adopt.
.
.. .
.
adopted by the author?
), the agent?s determiner andrelated adjectives.The direct object of the verb, the object?s determinerand adjectives, in addition to any post-modifiers (e.g.,?.
.
.
apply a method proposed by [1] .
.
.
?
, ?.
.
.
followan approach of [1] .
.
.
?Auxiliaries of the verb (e.g., ?In a similar manner, wemay propose a .
.
.
?
)Adverbial modification of the verb (e.g., ?We have pre-viously presented a .
.
.
.?
)Prepositional phrases related to the verb (e.g., ?In thispaper we present.
.
.
?, ?.
.
.
adopted from their work?
)Figure 6: Grammatical relations consideredtities and modifiers for each verb is constructed,grouped into five categories (cf.
Fig.
6).Next, semantic filters are applied to each of thepotential candidates (represented by the extractedentities and modifiers), and a fitness score is cal-culated.
These constraints encode semantic princi-ples that will apply to all cue phrases of that rhetor-ical category.
Examples for constraints are: ifwork is referred to as being done in previous ownwork, it is probably not a goal statement; the workin a goal statement must be presented here or in thecurrent paper (the concept of ?here-ness?
); and theagents of a goal statement have to be the authors,not other people.
While these filters are manuallydefined, they are modular, encode general princi-ples, and can be combined to express a wide rangeof rhetorical contexts.
We verified that around 20semantic constraints are enough to cover a largesets of different cue phrases (the 1700 cue phrasesfrom Teufel (1999)), though not all of these areimplemented yet.A nice side-effect of our approach is the simplecharacterisation of a cue phrase (by a syntactic re-lationship, some seed words for each concept, andsome general, reusable semantic constraints).
Thischaracterisation is more informative and specificthan string-based approaches, yet it has the poten-tial for generalisation (useful if the cue phrases areever manually assessed and put into a lexicon).Fig.
7 shows successful extraction examplesfrom our corpus7 , illustrating the difficulty ofthe task: the system correctly identified sen-tences with syntactically complex goal-type andcontinuation-type cue phrases, and correctly re-jected deceptive variants8 .7Numbers after examples give CmpLg archive numbers,followed by sentence numbers according to our preprocess-ing.8The seeds in this example were [analyse, present] & [ar-chitecture, method] (for goal) and [improve, adopt] & [model,method] (for continuation).925Correctly found:Goal-type:What we aim in this paper is to propose a paradigmthat enables partial/local generation through de-compositions and reorganizations of tentative localstructures.
(9411021, S-5)Continuation-type:In this paper we have discussed how the lexico-graphical concept of lexical functions, introducedby Melcuk to describe collocations, can be used asan interlingual device in the machine translation ofsuch structures.
(9410009, S-126)Correctly rejected:Goal-type:Perhaps the method proposed by Pereira et al(1993) is the most relevant in our context.
(9605014, S-76)Continuation-type:Neither Kamp nor Kehler extend their copying/ sub-stitution mechanism to anything besides pronouns,as we have done.
(9502014, S-174)Figure 7: Sentences correctly processed by our system4 Gold standard evaluationWe evaluated the quality of the extracted phrasesin two ways: by comparing our system output togold standard annotation, and by human judge-ment of the quality of the returned sentences.
Inboth cases bootstrapping was done using the seedtuples [analyse, present] & [architecture, method].For the gold standard-evaluation, we ran our sys-tem on a test set of 121 scientific articles drawnfrom the CmpLg corpus (Teufel, 1999) ?
en-tirely different texts from the ones the system wastrained on.
Documents were manually annotatedby the second author for (possibly more than one)goal-type sentence; annotation of that type hasbeen previously shown to be reliable at K=.71(Teufel, 1999).
Our evaluation recorded how oftenthe system?s highest-ranked candidate was indeeda goal-type sentence; as this is a precision-criticaltask, we do not measure recall here.We compared our system against our reimple-mentation of Ravichandran and Hovy?s (2002)paraphrase learning.
The seed words were of theform {goal-verb, goal-noun}, and we submittedeach of the 4 combinations of the seed pair toGoogle Scholar.
From the top 1000 documents foreach query, we harvested 3965 sentences contain-ing both the goal-verb and the goal-noun.
By con-sidering all possible substrings, an extensive list ofcandidate patterns was assembled.
Patterns withsingle occurrences were discarded, leaving a listof 5580 patterns (examples in Fig.
8).
In orderto rank the patterns by precision, the goal-verbswere submitted as queries and the top 1000 doc-uments were downloaded for each.
From these,we <verb> a <noun> forof a new <noun> to <verb> theIn this section , we <verb> the <noun> ofthe <noun> <verb> in this paperis to <verb> the <noun> afterFigure 8: Examples of patterns extracted usingRavichandran and Hovy?s (2002) methodMethod CorrectsentencesOur system with bootstrapping 88 (73%)Ravichandran and Hovy (2002) 58 (48%)Our system, no bootstrapping, WordNet 50 (41%)Our system, no bootstrapping, seeds only 37 (30%)Figure 9: Gold standard evaluation: resultsthe precision of each pattern was calculated by di-viding the number of strings matching the patterninstantiated with both the goal-verb and all Word-Net synonyms of the goal-noun, by the numberof strings matching the patterns instantiated withthe goal-verb only.
An important point here isthat while the tight semantic coupling between thequestion and answer terms in the original methodaccurately identifies all the positive and negativeexamples, we can only approximate this by using asensible synonym set for the seed goal-nouns.
Foreach document in the test set, the sentence contain-ing the pattern with the highest precision (if any)was extracted as the goal sentence.We also compared our system to two baselines.We replaced the lists obtained from the lexicalbootstrapping module with a) just the seed pairand b) the seed pair and all the WordNet synonymsof the components of the seed pair9.The results of these experiments are givenin Fig.
9.
All differences are statisticallysignificant with the ?2 test at p=.01 (exceptthose between Ravichandran/Hovy and our non-bootstrapping/WordNet system).
Our bootstrap-ping system outperforms the Ravichandran andHovy algorithm by 34%.
This is not surprising,because this algorithm was not designed to per-form well in tasks where there is no clear negativecontext.
The results also show that bootstrappingoutperforms a general thesaurus such as WordNet.Out of the 33 articles where our system?sfavourite was not an annotated goal-type sentence,only 15 are due to bootstrapping errors (i.e., toan incorrect ranking of the lexical variants), corre-9Bootstrapping should in principle do better than a the-saurus, as some of our correctly identified variants are nottrue synonyms (e.g., theory vs. method), and as noise throughovergeneration of unrelated senses might occur unless auto-matic word sense diambiguation is performed.926System chose: but should have chosen:derive set compare modelillustrate algorithm present formalisationdiscuss measures present variationsdescribe modificationspropose measuresaccommodate material describe approachexamine material present studyFigure 10: Wrong bootstrapping decisionsCeiling System BaselineExp.
A 3.91 3.08 1.58Exp.B 4.33 3.67 2.50Figure 11: Extrinsic evaluation: judges?
scoressponding to a 88% accuracy of the bootstrappingmodule.
Examples from those 15 error cases aregiven in Fig.
10.
The other errors were due to thecue phrase not being a transitive verb?direct ob-ject pattern (e.g.
we show that, our goal is andwe focus on), so the system could not have foundanything (11 cases, or an 80% accuracy), ungram-matical English or syntactic construction too com-plex, resulting in a lack of RASP detection of thecrucial grammatical relation (2) and failure of thesemantic filter to catch non-goal contexts (5).5 Human evaluationWe next perform two human experiments to in-directly evaluate the quality of the automaticallygenerated cue phrase variants.
Given an abstract ofan article and a sentence extracted from the article,judges are asked to assign a score ranging from 1(low) to 5 (high) depending on how well the sen-tence expresses the goal of that article (Exp.
A),or the continuation of previous work (Exp.
B).Each experiment involves 24 articles drawn ran-domly from a subset of 80 articles in the CmpLgcorpus that contain manual annotation for goal-type and continuation-type sentences.
The experi-ments use three external judges (graduate studentsin computational linguistics), and a Latin Squareexperimental design with three conditions: Base-line (see below), System-generated and Ceiling(extracted from the gold standard annotation usedin Teufel (1999)).
Judges were not told how thesentences were generated, and no judge saw anitem in more than one condition.The baseline for Experiment A was a randomselection of sentences with the highest TF*IDFscores, because goal-type sentences typically con-tain many content-words.
The baseline for ex-periment B (continuation-type) were randomly se-lected sentences containing citations, because theyoften co-occur with statements of continuation.
Inboth cases, the length of the baseline sentence wascontrolled for by the average lengths of the goldstandard and the system-extracted sentences in thedocument.Fig.
11 shows that judges gave an average scoreof 3.08 to system-extracted sentences in Exp.
A,compared with a baseline of 1.58 and a ceiling of3.9110; in Exp.
B, the system scored 3.67, witha higher baseline of 2.50 and a ceiling of 4.33.According to the Wilcoxon signed-ranks test at?
= .01, the system is indistinguishable fromthe gold standard, but significantly different fromthe baseline, in both experiments.
Although thisstudy is on a small scale, it indicates that humansjudged sentences obtained with our method as al-most equally characteristic of their rhetorical func-tion as human-chosen sentences, and much betterthan non-trivial baselines.6 ConclusionIn this paper we have investigated the automaticacquisition of semi-fixed cue phrases as a boot-strapping task which requires very little manualinput for each cue phrase and yet generalises toa wide range of syntactic and lexical variants inrunning text.
Our system takes a few seeds of thetype of cue phrase as input, and bootstraps lex-ical variants from a large corpus.
It filters outmany semantically invalid contexts, and finds cuephrases in various syntactic variants.
The systemachieved 80% precision of goal-type phrases ofthe targeted syntactic shape (88% if only the boot-strapping module is evaluated), and good qualityratings from human judges.
We found GoogleScholar to perform better than BNC as source forfinding hypotheses for lexical variants, which maybe due to the larger amount of data available toGoogle Scholar.
This seems to outweigh the dis-advantage of only being able to use POS patternswith Google Scholar, as opposed to robust parsingwith the BNC.In the experiments reported, we bootstrap onlyfrom one type of cue phrase (transitive verbs anddirect objects).
This type covers a large propor-tion of the cue phrases needed practically, but ouralgorithm should in principle work for any kind ofsemi-fixed cue phrase, as long as they have twocore concepts and a syntactic and semantic10This score seems somewhat low, considering that thesewere the best sentences available as goal descriptions, accord-ing to the gold standard.927CUE PHRASE: ?
(previous) methods fail?
(Subj?Verb)VARIANTS SEED 1: methodology, approach,technique.
.
.VARIANTS SEED 2: be cursed, be incapable of, berestricted to, be troubled, degrade, fall prey to, .
.
.CUE PHRASE: ?advantage over previous methods?
(NP?PP postmod + adj?noun premod.
)VARIANTS SEED 1: benefit, breakthrough, edge,improvement, innovation, success, triumph.
.
.VARIANTS SEED 2: available, better-known,cited, classic, common, conventional, current, cus-tomary, established, existing, extant,.
.
.Figure 12: Cues with other syntactic relationshipsrelation between them.
Examples for such othertypes of phrases are given in Fig.
12; the secondcue phrase involves a complex syntactic relation-ship between the two seeds (or possibly it couldbe considered as a cue phrase with three seeds).We will next investigate if the positive results pre-sented here can be maintained for other syntacticcontexts and for cue phrases with more than twoseeds.The syntactic variant extractor could be en-hanced in various ways, eg.
by resolving anaphorain cue phrases.
A more sophisticated model ofsyntactically weighted vector space (Pado and La-pata, 2003) may help improve the lexical acquisi-tion phase.
Another line for future work is boot-strapping meaning across cue phrases within thesame rhetorical class, e.g.
to learn that we proposea method for X and we aim to do X are equivalent.As some papers will contain both variants of thecue phrase, with very similar material (X) in thevicinity, they could be used as starting point forexperiments to validate cue phrase equivalence.7 AcknowledgementsThis work was funded by the EPSRC projects CIT-RAZ (GR/S27832/01, ?Rhetorical Citation Mapsand Domain-independent Argumentative Zon-ing?)
and SCIBORG (EP/C010035/1, ?Extractingthe Science from Scientific Publications?
).ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball: Ex-tracting relations from large plain-text collections.
In Pro-ceedings of the 5th ACM International Conference on Dig-ital Libraries.Regina Barzilay and Lillian Lee.
2002.
Bootstrapping lex-ical choice via multiple-sequence alignment.
In Proc.
ofEMNLP.Ted Briscoe and John Carroll.
2002.
Robust accurate statis-tical annotation of general text.
In Proc.
of LREC.Lou Burnard, 1995.
Users Reference Guide, British NationalCorpus Version 1.0.
Oxford University, UK.John Carroll, Guido Minnen, and Ted Briscoe.
1999.
Cor-pus annotation for parser evaluation.
In Proceedingsof Linguistically Interpreted Corpora (LINC-99), EACL-workshop.Ute Essen and Volker Steinbiss.
1992.
Co-occurrencesmoothing for stochastic language modelling.
In Proc.
ofICASSP.Donald Hindle.
1990.
Noun classification from predicate-argument structures.
In Proc.
of the ACL.Edvard Hovy and Chin-Yew Lin.
1998.
Automated text sum-marization and the Summarist system.
In Proc.
of the TIP-STER Text Program.Ken Hyland.
1998.
Persuasion and context: The pragmat-ics of academic metadiscourse.
Journal of Pragmatics,30(4):437?455.Christian Jacquemin, Judith Klavans, and Evelyn Tzouker-mann.
1997.
Expansion of multi-word terms for indexingand retrieval using morphology and syntax.
In Proc.
of theACL.Julian Kupiec, Jan O. Pedersen, and Francine Chen.
1995.
Atrainable document summarizer.
In Proc.
of SIGIR-95.Lillian Lee.
1999.
Measures of distributional similarity.
InProc.
of the ACL.Jianhua Lin.
1991.
Divergence measures based on the Shan-non entropy.
IEEE transactions on Information Theory,37(1):145?151.Frederique Lisacek, Christine Chichester, Aaron Kaplan, andSandor Agnes.
2005.
Discovering paradigm shift patternsin biomedical abstracts: Application to neurodegenerativediseases.
In Proc.
of the SMBM.George Miller, Richard Beckwith, Christiane Fellbaum,Derek Gross, and Katherine Miller.
1990.
Five paperson WordNet.
Technical report, Cognitive Science Labora-tory, Princeton University.Greg Myers.
1992.
In this paper we report...?speech actsand scientific facts.
Journal of Pragmatics, 17(4):295?313.Sebastian Pado and Mirella Lapata.
2003.
Constructing se-mantic space models from parsed corpora.
In Proc.
ofACL.Chris D. Paice.
1981.
The automatic generation of lit-erary abstracts: an approach based on the identifica-tion of self-indicating phrases.
In Robert Norman Oddy,Stephen E. Robertson, Cornelis Joost van Rijsbergen, andP.
W. Williams, editors, Information Retrieval Research,Butterworth, London, UK.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.
Dis-tributional clustering of English words.
In Proc.
of theACL.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.
InProc.
of the ACL.Ellen Riloff.
1993.
Automatically constructing a dictionaryfor information extraction tasks.
In Proc.
of AAAI-93.Simone Teufel.
1998.
Meta-discourse markers and problem-structuring in scientific articles.
In Proceedings of theACL-98 Workshop on Discourse Structure and DiscourseMarkers.Simone Teufel.
1999.
Argumentative Zoning: InformationExtraction from Scientific Text.
Ph.D. thesis, School ofCognitive Science, University of Edinburgh, UK.Cornelis Joost van Rijsbergen.
1979.
Information Retrieval.Butterworth, London, UK, 2nd edition.Hua Wu and Ming Zhou.
2003.
Synonymous collocationextraction using translation information.
In Proc.
of theACL.928
