IIIIIIIIIIIIIIIIIIIIA Method of Incorporating Bigram Constraints into an LRTable and Its Effectiveness in Natural Language ProcessingiHiroki Imai and Hozumi TanakaGraduate School of Information Science and TechnologyTokyo Institute of Technology2-12-10-okayama, Meguro, Tokyo 152 Japan{imai ,  tanaka}@cs ,  t i tech,  ac.
jpAbstractIn this paper, we propose a method for con-structing bigram LR tables by way of incor-porating bigram constraints into an LR table.Using a bigram LR table, it is possible for aGLR parser to make use of both big'ram andCFG constraints in natural language process-ing.Applying bigram LR tables to our GLRmethod has the following advantages:(1) Language models utilizing bigzam LR ta-bles have lower perplexity than simple bigramlanguage models, since local constraints (hi-gram) and global constraints (CFG) are com-bined in a single bigram LR table.
(2) Bigram constraints are easily acquired froma given corpus.
Therefore data sparseness isnot likely to arise.
(3) Separation of local and global constraintskeeps down the number of CFG rules.The first advantage leads to a reduction incomplexity, and as the result, better perfor-mance in GLR parsing.Our experiments demonstrate the effectivenessof our method.1 IntroductionIn natural anguage processing, stochastic languagemodels are commonly used for lexical and syntacticdisambiguation (Fujisaki et al, 1991; Franz, 1996).Stochastic language models are also helpful in re-ducing the complexity of speech and language pro-cessing by way of providing probabilistic linguisticconstraints (Lee, 1989).N-gram language models (Jelinek, 1990), includ-ing bigram and trigram models, are the most com-monly used method of applying local probabilis-tic constraints.
However, context-free grammars(CFGs) produce more global linguistic constraintsthan N-gram models.
It seems better to combineboth local and global constraints and use them bothconcurrently in natural language processing.
Thereason why N-gram models are preferred over CFGsis that N-gram constraints are easily acquired froma given corpus.
However, the larger N is, the moreserious the problem of data sparseness becomes.CFGs are commonly employed in syntactic pars-ing as global linguistic constraints, ince many eifi-cient parsing algorithms are available.
GLR (Gen-eralized LR) is one such parsing algorithm that usesan LR table, into which CFG constraints are pre-compiled in advance (Knuth, 1965; Tomita, 1986).Therefore if we can incorporate N-gram constraintsinto an LR table, we can make concurrent use ofboth local and global linguistic constraints in GLRparsing.In the following section, we will propose a methodthat incorporates bigram constraints into an LR ta-ble.
The advantages of the method are summarizedas follows:First, it is expected that this method produces alower perplexity than that for a simple bigram lan-guage model, since it is possible to utilize both local(bigram) and global (CFG) constraints in the LRtable.
We will evidence this reduction in perplexityby considering states in an LR table for the case ofGLR parsing.Second, bigram constraints are easily acquiredfrom smaller-sized corpora.
Accordingly, datasparseness is not likely to arise.Third, the separation of local and global con-straints makes it easy to describe CFG rules, sinceCFG writers need not take into'account tedious de-scriptions of local connection constraints within thCFG I .2 CFG, Connection Matrix and LRtable2.1 Relat ion between CFG and Connect ionConstraintsFigure 1 represents a situation in which ai and bj areadjacent to each other, where a~ belongs to Setl (i =1, .
.
- , I )  and bj belongs to Setj (j = 1, .
.
.
,  J).
Set~l(Tana~ et al, 1997) reported that the separate de-scription of local and global constraints reduced the CFGrules to one sixth of their original number.Imai and Tanaka 225 A Method of Incorporating Bigram ConstraintsHirold Ima/ and Hozumi Tanaka (1998) A Method of Incorporating Bigram Constraints into an LR Table and ItsEffectiveness in Natural Language Processing.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods in LanguageProcessing and Computational Natural Language Learning, ACL, pp 225-233.XY Zb jFigure 1: Connection check by CFGand Sets are defined by last1 (Y) and first1 ( Z) (Ahoet al, 1986), respectively.
If a E Setz and b E Setshappen not to be able to occur in this order, it be-comes a non-trivial task to express this adjacencyrestriction within the framework of a CFG.One solution to this problem is to introduce a newnonterminal symbol Ai for each a~ and a nonterminalsymbol Bj for each hi.
We then add rules of the formA --* Ai and Ai "* ai, and B ~ Bj and B i --* bj.As a result of this rule expansion, the order of thenumber of rules will become I x J in the worst case.The introduction of such new nonterminal symbolsleads to an increase in grammar ules, which notonly makes the LR table very large in size, but alsodiminishes efficiency of the GLR parsing method.The second solution is to augment X ~ Y Z witha procedure that checks the connection between a~and bj.
This solution can avoid the problem of theexpansion of CFG rules, but we have to take care ofthe information flow from the bottom leaves to theupper nodes in the tree, Y, Z, and X.Neither the first nor the second solution are prefer-able, in terms of both efficiency of GLR parsing anddescription of CFG rules.
Additionally, it is a mucheasier task to describe local connection constraintsbetween two adjacent erminal symbols by way ofa connection matrix such as in Figure 2, than toexpress these constraints within the CFG.The connection matrix in Figure 2 is defined as:= { ~ if bj can follow ai Connect( a~, bj ) otherwise (1)The best solution seems to be to develop amethodthat can combine both a CFG and a connection ma-trix, avoiding the expansion of CFG rules.
Conse-quently, the size of the LR table will become smallerand we will get better GLR parsing performance.In the following section, we will propose one suchmethod.
Note that we are considering connectionsbetween preterminals rather than words.
Thus, wewill have Connect(ai, bj) = 0 in the preterminal con-nection matrix similarly to the case of words.a2a~I? "
- -  .
.
.
.
b j  .
- -I aI II iI .
.
.
.
I .
.
.
.!
|o |e I--1 .
.
.
.
0 .
.
.
.| || |I io |i eo ji ef I| |I i| |= |I st || I| || I| iFigure 2: Connection matrix2.2 Relation between the LR Table andConnect ion Matr ixFirst we discuss the relation between the LR tableand a connection matrix.
The action part of an LRtable consists of lookahead symbols and states.
Leta shift action sh m be in state l with the lookaheadsymbol a.
After the GLR parser executes actionsh m, the symbol a is pushed onto the top of thestack and the GLR parser shifts to state m. Sup-pose there is an action A in state m with looka-head b (see Figure 3).
The action A is executableif Connect(a,b) ~ 0 (b can follow a), whereas ifConnect(a, b) = 0 (b cannot follow a), the actionA in state m with lookahead b is not executable andwe can remove it from the LR table as an invalidaction.
Removing such invalid actions enables us toincorporate connection constraints into the LR tablein addition to the implicit CFG constraints.In section 3.2, we will propose a method that in-tegrates both bigram and CFG constraints into anLR table.
After this integration process, we obtaina table called a bigram LR table.3 In tegrat ion  o f  B ig ram and CFGConst ra in ts  in to  an  LR  Tab le3.1 The Definition of a ProbabilisticConnection MatrixA close relation exists between bigrams and connec-tion matrices, in that the bigram probability P(bla )corresponds to the matrix dement of Connect(a, b).A connection matrix incorporating bigram probabil-ities is called a probabilistic onnection matrix, inwhich Connect(a, b) = 0 still means b cannot followa, but instead of connection matrix entries havinga binary value of 0 or 1, a probability is associatedwith each element.
This is then used to construct aprobabilistic LR table.lmai and Tanaka 226 A Method of Incorporating Bigram ConstraintsIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII II II II IIII IIII ImmmLR table?
?
.o???
,I | .
.
.
.
.
.
'm .
.
.
.
.
.
.
.ash m\' =?
?=="  ' b , .
.
.
.
~ .
.
.
.
.
.Acl .
.
.
.
.
.
.
.
.
.
.
.Act is removed if Connect (a,b) = 0Stack Input symbolsFigure 3: LR table and Connection ConstraintsThe N-gram model is the most commonly usedprobabilistic language model, and it assumes that asymbol sequence can be described by a higher orderMarkov process.
The simplest N-gram model withN = 2 is called a bigram model, and approximatesthe probability of a string X = xzx2xa...x,~ as theproduct of conditional probabilities:P(X) = e(xz l#)P(x2lx l ) .
.
.
P(x=lx=-l)P($1x,) (2)In the above expression, "#" indicates the sen-tence beginning marker and "$" indicates the sen-tence ending marker.
The above big-ram model canbe represented in a probabilistic onnection matrixdefined as follows.DEFINITION 1 (probabilistic onnection matriz)Let G = (V~v, Vr, P, S) be a context-free gram-mar.
For Va, b E VT (the set of terminal symbols),the probabilistic onnection matrix named PConnectis defined as follows.PConnect(a, b) = P(bla ) (3)where P(bJa ) is a conditional probability andP(b la)  = 1.PConnect(a,b) = 0 means that a and bcannot occur consecutively in the given order.PConnect(a, b) ~ 0 means b can follow a with prob-ability P(b\[a).3.2 An  a lgor i thm to construct  a b igram LRtableAn algorithm to construct a probabilistic LR ta-ble, combining both bigram and CFG constraints,is given in Algorithm I:Algor i thm 1Input: A CFG G = (Vjv, VT, P, S) and a probabilis-tic connection matrix PConnect.Output: An LR table T with CFG and big-ram con-straints.Method:Step 1 Generate an LR  table To from the givenCFG G.Step 2 Removal of actions:For each shift action shm with lookahead a inthe LR table To, delete actions in the state mwith lookalaead b if PConnect(a, b) = O.Step 3 Constraint Propagation (Tanaka et al,1994):Repeat the following two procedures until nofurther actions can be removed:1.
Remove actions which have no succeedingaction,2.
Remove actions which have no precedingaction.Step 4 Compact he LR table if possible.Step 5 Incorporation of big-ram constraints into theLR table:For each shift action shm with lookahead a inthe LR table To, letNP = ~ PConnect(a, bi)i=1where {hi : i = 1, - .
- ,N} is the set of looka-heads for state m. For each action Aj in statern with lookahead bi, assign a probability p toaction Aj:P(bila ) _ PConnect( a, b~ ) .P= Pxn Pxnwhere n is the number of conflict actions instate m with lookahead bi.
The denominatoris dearly a normalization factor.S tep 6 For each shift action A with lookahead ain state 0, assign A a probability p = p(al#),where "#" is the sentence beginning marker.Step 7 Assign a probability p = 1/n to each ac-tion.
A in state m with lookahead symbol a thathas not been assigned a probability, where n isthe number of conflict actions in state m withlookahead symbol a.Step 8 Return the LR table T produced at thecompletion of Step 7 as the Bi#ram LR table.As explained above, the removal of actions at Step2 corresponds to the operation of incorporating con-nection constraints into an LR table.
We call Step3 Constraint Propagation, by which the size of theLR table is reduced (Tanaka et al, 1994).
As manylmai and Tanaka 227 A Method of Incorporating Bigram Constraints(1) S - - *XY  (6) A- -*a l(2) X ~ A (7) A- - ,  ae(3) X -~AB (S) n -~b l(4) Y- - ,A  (9) B- - ,b2(5) Y- - *b lAFigure 4: Grammar G1al a2 bl b2 $# 0.6 0.4 0.0 0.0 0.0al 0.0 0.0 0.0 1.0 0.0a2 0.0 0.0 0.3 0.0 0.7bl 0.0 0.1 0.9 0.0 0.0b2 0.0 0.0 1.0 0.0 0.0Figure 5: Probabilistic onnection matrix Mzactions are removed from the LR table during Steps2 and 3, it becomes possible to compress the LR ta-ble in Step 4.
We will demonstrate one example ofthis process in the following section.It should be noted that the above algorithm can beapplied to any type of LR table, that is a canonicalLR table, an LALR table, or an SLR table.4 An  Example4.1 Generat ing  a Big-ram LR TableIn this section, we will provide a simple example ofthe generation of a bigram LR table by way of ap-plying Algorithm 1 to both a CFG and a probabilis-tic connection matrix, to create a big'ram LR table.Figure 4 and Figure 5 give a sample CFG Gz and aprobabilistic onnection matrix M1, respectively.Note that grammar G1 in Figure 4 does not ex-plicitly express local connection constraints betweenterminal symbols.
Such local connection constraintsare easily expressed by a matrix M1 as shown inFigure 5.From the CFG given in Figure 4, we can generatean LR table, Table 1, in Step 1 using the conven-tional LR table generation algorithm.Table 2 is the resultant LR table at the comple-tion of Step 2 and Step 3, produced based on Table 1.Actions numbered (2) and (3) in Table 2 are thosewhich are removed by Step 2 and Step 3, respec-tively.In state 1 with a lookahead symbol bl, re6 is car-ried out after executing action shl in state 0, push-ing al onto the stack.
Note that al and bl arenow consecutive, in this order.
However, the proba-bilistic connection matrix (see Figure 5) does notallow such a sequence of terminal symbols, sincePConnect( al , bl ) = O.
Therefore, the action re6in state 1 with lookahead bl is removed from Ta-ble 1 in Step 2, and thus marked as (2) in Table 2.For this same reason, the other re6s in state 1 withlookahead symbols al and a$ are also removed fromTable 1.On the other hand, in the case of re6 in state 1with lookahead symbol b$, as al can be followed byb2 (PConnect(al, b~) ~ 0), action re6 cannot beremoved.
The remaining actions marked as (2} inTable 2 should be self-evident to readers.Next, we would like to consider the reason whyaction sh9 in state 4 with lookahead al is removedfrom Table 1.
In state 9, re6 with lookahead symbol$ has already been removed in Step 2, and there isno succeeding action for shg.
Therefore, action sh9in state 3 is removed in Step 3, and hence markedas(3).Let us consider action re3 in state 8 with looka-head al.
After this action is carried out, the GLRparser goes to state 4 after pushing X onto the stack.However, sh9 in state 4 with lookahead al has al-ready been removed, and there is no succeeding ac-tion for re3.
As a result, re3 in state 8 with looka-head symbol al is removed in Step 3.
Similarly, re9in state 7 with lookahead symbol al is also removedin Step 3.
In this way, the removal of actions prop-agates to other removals.
This chain of removals iscalled Constraint Propagation, and occurs in Step3.
Actions removed in Step 3 are marked as (3) inTable 2.Careful readers will notice that there is now noaction in state 9 and that it is possible to delete thisstate in Step 4.
Table 3 shows the LR table afterStep 4.As a final step, we would like to assign big-ramconstraints o each action in Table 3.
Let us considerthe two tess in state 6, reached after executing sh6in state 4 by pushing a lookahead of bl onto thestack.
In state 6, P is calculated at Step 5 as shownbelow:P = PConnect(bl,a2) +PConnect(bl ,b l )= 0.1+0.9= 1We can assign the following probabilities p to eachre8 in state 6 by way of Step 5:PCon,~ect(bl,ae) for re8 withp?n = ~ = 0.I lookahead a2P = PConnect(bl,bl ) for re8 withP?n = ~ = 0.9 lookahead blAfter assigning a probability to each action in theLR table at Step 5, there remain actions withoutprobabilities.
For example, the two conflict actions(re2/sh6) in state 3 with lookahead bl are not as-signed a probability.
Therefore, each of these ac-tions is assigned the same probability, 0.5, in Step 7.A probability of 1 is assigned to remaining actions,since there is no conflict among them.Table 4 shows the final results of applying Algo-rithm 1 to G, and M,.lmai and Tanaka 228 A Method of Incorporating Bigram ConstraintsIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIstate01234567891011121314.alshlre6re7re2sh9re8re9re3sh9a2shere6re7re2shlOre8re9re3shlOactionblre6re7ree/sh6sh11re8re9re3b2re6re7sh7i$ A312acere6re7~4re1re5Table 1: Initial LR table for G1gotoB X48Y13S5state01234567891011121314.alshlre6(2)reT(e)tee(3)shg(3)re8(2)reg(3)re3(3)shg(3)a2shere6(2)re7(2)re2shlOre8re9(2)re3shlOactionblre7re2/sh6sh11re8re9re3b2 $ A3re6re7(2)sh712acere714re4re1re5Table 2: LR table after Steps 2 and 3gotoB X Y S4 5813lmai and Tanaka 229 A Method of Incorporating Bigram Constraintsstate012345678101112i314alsh'la2sh2re2shlOre8re3sh10actionblre7re2/sh6shllre8re9re3b2re6sh7$ A312accre714re4re1re5Table 3: LR table after Step 4gotoB X4Y13S5state01.23456781011121314alshl0.6a2sh20.4re21.0shlO1.0re80.1re31.0shlO1.0actionbl b2 $re61.0re71.0re2/sh6 Sh 70.5/0.5 1.oshll1.0re80.9re91.0re31.0acc1.0re71.0re41.0tel1.0re51.0A3gotoB X Y812 1314Table 4: The Bigram LR table constructed by Algorithm 1IIIIIIIIIIIII!IIklmai and Tanaka 230 A Method of Incorporating Bigram Constraintsk4.2 Comparison of Language ModelsUsing the bigram LR table as shown in Table 4, theprobability P1 of the string "a2 bl ag' is calculatedas:P1 = P(a2 bl ae)T= E P(Tree0i= l= P(O, ae, she) x P(2, bl,re7)xP(3, bI, re._~2) x P(4, bl, sh11)xP( l l ,  a2, shlO) x P(10, $, re7)xP(14, $, re5) x P(13, $, re1 )?P(5, $, acc)+ P(O, a2, she) x P(2, bl, re7)xP(3, bl, sh_66) x P(6, a2, reS)xP(8, ae, re3) x P(4, ae, shlO)xP(10, $, re7) x P(12, $, re4 )xP(13,$, re1) x P(5, $, acc)= 0.4 x 1.0 x 0.5 x 1.0 x 1.0x 1.0 x 1.0 x 1.0 x 1.0+ 0.4 x 1.0 x 0.5 x 0.1 x 1.0x 1.0 x 1.0 x 1.0 x 1.0 x 1.0= 0.2 + 0.02= 0.22where P(Treei) means the probability of the i-th parsing tree generated by the GLR parser andP(S,L,A) means the probability of an action A instate S with Iookahead L.On the other hand, using only bigram constraints,the probability P2 of the string "ae b1 a,~' is calcu-lated as:P2 = P(a2 bl a2)= P(ael#) ?
P(bllae) x P(aelbl) ?
P($1ae)= x0.3 x0.1 x0.7= 0.0084The reason why P1 > P2 can be explained asfollows.
Consider the beginning symbol a2 of a sen-tence.
In the case of the bigram model, a2 can onlybe followed by either of the two symbols bl and $(see Figure 5).
However, consulting the bigram LRtable reveals that in state 0 with lookahead ae, sheis carried out, entering state 2.
State 2 has onlyone action re7 with lookahead symbol bl.
In otherwords, in state 2, $ is not predicted as a succeedingsymbol of al.
The exclusion of an ungrammaticalprediction in $ makes P1 larger than P2.Perplexity is a measure of the complexity of a lan-guage model.
The larger the probability of the lan-guage model, the smaller the perplexity of the lan-guage model.
The above result (P1 > P2) indicatesI Language model I Perplexity IBigram 6.50BigraJfi LR table 5.99Trigram 4.92Table 5: Perplexity of language modelsthat the bigram LR table model gives smaller per-plexity than the bigram model.
In the next section,we will demonstrate his fact.5 Evaluation of PerplexityPerplexity is a measure of the constraint imposedby the language model.
Test-set perplexity (Jelinek,1990) is commonly used to measure the perplexity ofa language model from a test-set.
Test-set perplexityfor a language model L is simply the geometric meanof probabilities defined by:whereQ(L) = 2//(L)1 MS(L )  =  logP(S,)i= lHere N is the number of terminal symbols in the testset, M is the number of test sentences and P(S,) isthe probability of generating i-th test sentence Si.In the case of the bigram model, P~i(Si) is:Pb~(&) = P (z l , ze , .
.
.
, z .
)= P(zI I#)P(zeIzl) .
.
.P(x~Iz._I)P($1..)And in the case of the trigram model, P~(Si) is:Pt~i(S~) = P (x l , ze , .
.
.
, z , )= P(xll#)P(xel#,xl).. .P(x. Ix.-2,  x.-1)P($lx.-a, x.
)Table 5 shows the test-set perplexity of pretermi-rials for each language model.
Here the preterminalbigram models were trained on a corpus with 20663sentences, containing 230927 preterminals.
The test-set consists of 1320 sentences, which contain 13311preterminals.
The CFG used is a phrase context-free grammar used in speech recognition tasks, andthe number of rules and preterminals is 777 and 407,respectively.As is evident from Table 5, the use of a bigramLR table decreases the test-set perplexity from 6.50to 5.99.
Note that in this experiment, we usedthe LALR table generation algorithm 2 to constructthe bigram LR table.
Despite the disadvantages of2In the case of LALR tables, the sum of the proba-bihties of all the possible parsing trees generated by agiven CFG may be less than 1 (Inui et al, 1997).mai and Tanaka 231 A Method of Incorporating Bigram ConstraintsLALR tables, the bigram LR table has better per-formance than the simple bigram language model,showing the effectiveness of a bigram LR table.On the other hand, the perplexity of the trigramlanguage model is smaller than that of the bigramLR table.
However, with regard to data sparseness,the bigram LR table is better than the trigram lan-guage model because bigram constraints are moreeasily acquired from a given corpus than trigramconstraints.Although the experiment described above isconcerned with natural language processing, ourmethod is also applicable to speech recognition.6 Conc lus ionsIn this paper, we described a method to construct abigram LR table, and then discussed the advantageof our method, comparing our method to the bigramand trigram language models.
The principle advan-tage over the bigram language model is that, in usinga bigram LR table, we can combine both local prob-abilistic connection constraints (bigram constraints)and global constraints (CFG).Our method is applicable not only to natural lan-guage processing but also speech recognition.
Weare currently testing our method using a large-sized grammar containing dictionary rules for speechrecognition.Su et al (Suet al, 1991) and Chiang et al (Chi-ang et al, 1995) have proposed a very interestingcorpus-based natural language processing methodthat takes account not only of lexical, syntactic,and semantic scores concurrently, but also context-sensitivity in the language model.
However, theirmethod seems to suffer from difficulty in acquiringprobabilities from a given corpus.Wright (Wright, 1990) developed a method of dis-tributing the probability of each PCFG rule to eachaction in an LR table.
However, this method onlycalculates syntactic scores of parsing trees based ona context-free framework.Briscoe and Carroll (Briscoe and Carroll., 1993)attempt to incorporate probabilities into an LR ta-ble.
They insist that the resultant probabilisticLR table can include probabilities with context-sensitivity.
Inui et.
al.
(Inni et al, 1997)reportedthat the resultant probabilistic LR table has a defectin terms of the process used to normalize probabili-ties associated with each action in the LR table.?
Finally, we would like to mention that Klavansand Resnik (Klavaus and Resnik, 1996) have ad-vocated a similar approach to ours which combinessymbolic and statistical constraints, CFG and bi-gram constraints.AcknowledgementsWe would like to thank Mr. Toshiyuki Takezawa ndMr.
Junji Etoh for providing us the dialog corpusand the grammar for our experiments.
We wouldalso like to thank Mr. Timothy Baldwin for his helpin writing this paper.Re ferencesA.V.
Aho, S. Ravi, and J.D.
UUman.
1986.
Com-pilers: Principle, Techniques, and Tools.
AddisonWesley.T.
Briscoe and J. Carroll.
1993.
Generalized proba-bilistic LR parsing of natural anguage (corpora)with unification-based grammars.
ComputationalLinguistics, 19(1):25-59.T.H.
Chiang, Y.C.
Lin, and K.Y.
Su.
1995.
Robustlearning, smoothing, and parameter tying on syn-tactic ambiguity resolution.
Computational Lin-guistics, 21(3):321-349.A.
Franz.
1996.
Automatic Ambiguity Resolution iNatural Language Processing.
Springer.T.
Fujisaki, F. Jelinek, J. Cocke, E. Black, andT.
Nishino.
1991.
A probabilistic parsing methodfor sentence disambiguation.
In M. Tomita, edi-tor, Current Issues in Parsing Technologies, pages139-152.
Kluwer Academic Publishers.K.
Inui, V. Sornlertlamvanich, H. Tanaka, andT.
Tokunaga.
1997.
A new formalization of prob-abilistic GLR parsing.
In International Workshopon Parsing Technologies.F.
Jelinek.
1990.
Self-organized language mod-eling for speech recognition.
In A. Waibel andK.F.
Lee, editors, Readings in Speech Recognition,pages 450-506.
Morgan Kauhnann.J.L.
Klavans and P. Resnik.
1996.
The Balanc-ing Act: Combining Symbolic and S~atistial Ap-proaches to Language.
The MIT Press.D.E.
Knuth.
1965.
On the translation of languagesleft to right.
Information and Control, 8(6):607-639.K.F.
Lee.
1989.
Automatic Speech Recognition:The Development of he SPHINX System.
KluwerAcademic Publishers.K.Y.
Su, J.N.
Wang, M.H.
Su, and J.S.
Chang.
1991.GLR parsing with scoring.
In M. Tomita, editor,Generalized LR Parsing.
Kluwer Academic Pub-fishers.H.
Tanaka, H. Li, and T. Tokunaga.
1994.
Incor-poration of phoneme-context-dependence into LRtable through constraints propagation method.
InWorkshop on Integration of Natural Language andSpeech Processing, pages 15-22.H.
Tanaka, T. Takezawa, and J. Etoh.
1997.Japanese grammar for speech recognition consid-ering the MSLR method.
In Information Process-ing Society of Japan, SIG-SLP-15, pages 145-150.
(in Japanese).Imai and Tanaka 232 A Method of lncorporaling Bigram Constraintsl/iI/IM.
Tomita.
1986.
Efficient Parsing for Natural Lan-guage: A Fast Algorithm for Practical Systems.Kluwer' Academic Publishers.J.I-I.
Wright.
1990.
LR parsing of probabilisticgrammars with input uncertainty for speech recog-nition.
Computer Speech and Language, 4(4):297-323.Imai and Tanaka 233 A Method of Incorporating Bigram Constraintsmmmmmmmm
