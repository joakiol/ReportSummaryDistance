TextTree Construction for Parser and Treebank DevelopmentPaula S. Newmannewmanp@acm.orgAbstractTextTrees, introduced in (Newman,2005), are skeletal representationsformed by systematically convertingparser output trees into unlabeledindented strings with minimalbracketing.
Files of TextTrees can beread rapidly to evaluate the results ofparsing long documents, and are easilyedited to allow limited-cost treebankdevelopment.
This paper reviews theTextTree concept, and then describesthe implementation of the almostparser- and grammar-independentTextTree generator, as well as auxiliarymethods for producing parser reviewfiles and inputs to bracket scoring tools.The results of some limitedexperiments in TextTree usage are alsoprovided.1 IntroductionThe TextTree representation was developed tosupport a limited-resource effort to build a newhybrid English parser1.
When the parser reachedsignificant apparent coverage, in terms ofnumbers of sentences receiving some parse, theneed arose to quickly assess the quality of theparses produced, for purposes of detectingcoverage gaps, refining analyses, andmeasurement.
But this was hampered by the useof a detailed parser output representation.The two most common parser-output displaysof constituent structure are: (a) multi-line labeledand bracketed strings, with indentation indicatingdominance, and (b) 2-dimensional trees.
Whilethese displays are indispensable in grammardevelopment, they cannot be scanned quickly.Labels and brackets interfere with reading.
And,1The hybrid combines the chunker part of the fast,robust XIP parser (A?t-Mokhtar et al, 2002) with anATN-style parser operating primarily on the chunks.although relatively flat 2D node + edge trees forshort sentences can be grasped at a glance, forlong sentences this property must becompromised.In contrast, for languages with a relativelyfixed word order, and a tendency to post-modification, TextTrees capture thedependencies found by a parser in a natural,almost self-explanatory way.
For example:Theymust havea clear delineationof[roles,missions,andauthority].Indented elements are usually right-hand post-modifiers or subordinates of the lexical head ofthe nearest preceding less-indented line.
Bracketsare generally used only to delimit coordinations(by [...]), nested clauses (by {?
}), and identifiedmulti-words (by |?|).Reading a TextTree for a correct parse issimilar to reading ordinary text, but reading aTextTree for an incorrect parse is jarring.
Forexample, the following TextTree for a 33-wordsentence exposes several errors made by thehybrid parser:Butby |September 2001|,the executive branchof[the U.S. government,the Congress,the news media,andthe American public]had receivedclear warningthat{Islamist terroristsmeantto killAmericansin high numbers}.TextTrees can be embedded in bulk parseroutput files with arbitrary surroundinginformation.
Figure 1 shows an excerpt fromsuch a file, containing the TextTree-form resultsof parsing the roughly 500-sentence "ExecutiveSummary" of the 9/11 Commission Report(2004) by the hybrid parser, with more detailedresults for each sentence accessible via links.
(Note: the links in  Figure 1 are greyed toindicate that they are not operational in theillustration.
)Such files can also be edited efficiently toproduce limited-function treebanks, because theneeded modifications are easy to identify, labelsare unnecessary, and little attention to bracketingis required.
Edited and unedited TextTree filescan then be mapped into files containing fullybracketed strings (although bracketed differentlythan the original parse trees), and compared bybracket scoring methods derived from Black et al(1991).Section 2 below examines the problemspresented by detailed parse trees for late-stageparser development activities in more detail.Section 3 describes the inputs to and outputsfrom the TextTree generator, and Section 4 thegenerator implementation.
Section 5 discussesthe use of the TextTree generator in producingTextTree files for parser output review andTextTreebank construction, and the use ofTextTreebanks in parser measurement.
Theresults of some limited experiments in TextTreefile use are provided in Section 6.
Section 7discusses related work and Section 8 exploressome directions for further exploitation ofTextTrees.6 (3) We have come together with a unity of purpose because our nation demands it.
best morechunksWehave come togetherwith a unityof purposebecause{our nationdemandsit}.7 (15) September 11 , 2001 , was a day of unprecedented shock and suffering in the history of theUnited States .
best more chunks|September 11 , 2001|,wasa dayof[unprecedented shockandsuffering]in the historyof the United States.8 (1) The nation was unprepared .
best more chunksThe nationwasunprepared.Figure 1.
A TextTree file excerptFigure 2.
An LFG c-structure2   Problems of Detailed Parse TreesThis section examines the readability problemsposed by conventional parse tree representations,and their implications for parser developmentactivities.As noted above, parse trees are usuallydisplayed using either 2-dimensional trees orfully bracketed strings.
Two dimensional treesare intended to provide a clear understanding ofstructure.
Yet because of the level of detailinvolved in many grammars, and the problem ofdealing with long sentences, they often fail inthis regard.
Figure 2 illustrates an LFG c-structure, reproduced from King et al (2000), forthe 7 word sentence "Mary sees the girl with thetelescope."
Similar structures would be obtainedfrom parsers using other popular grammaticalparadigms.
The amount of detail created by thedeep category nesting makes it difficult to graspthe structure by casual inspection, and the tree inthis case is actually wider than the sentence.Grammar-specific transformations have beenused in the LKB system to simplify displays(Oepen et al, 2002).
But there are no trulysatisfactory approaches for dealing with theproblem of tree width, that is, for presenting 2Dtrees so as to provide an "at-a-glance"appreciation of structure for sentences longerthan 25 words, which are very prevalent.2 Themethods currently in use or suggested eitherobscure the structure or require additional actionsby the reader.
Allowing trees to be as wide asthe sentences requires horizontal scrolling.Collapsing some subtrees into single nodes(wrapping represented token sequences underthose nodes) requires repeated expansions toview the entire parse.
Using more of the displayspace by overlapping subtrees verticallyinterferes with comprehension because itobscures dominance and sequence.
In Figure 2,for example, the period at the end of thesequence is the second constituent at the top.
Fora longer sentence, a coordinated constituentmight be placed here as well.
Suchunpredictable arrangements impede reading,because the reader does not know where to look2Casual records of parser results for many Englishnon-fiction documents suggest an average of about 20words per sentence, with a standard deviation of about11.S (NP-SBJ Stokely)(VP says(SBAR  0(S (NP-SBJ stores)(VP revive(NP (NP specials)(PP like(NP (NP three cans)(PP of(NP peas))(PP for(NP 99 cents )))))))))Figure 3.
A Penn Treebank TreeThe other conventional parse treerepresentation is as a fully-bracketed string,usually including category labels and, for displaypurposes, using indentation to indicatedominance.
Figure 3 shows such a tree, drawnfrom a Penn Treebank annotation guide (Bies etal., 1995), shown with somewhat narrowerindentation than the original.
Even though thetree is in the relatively flat form developed foruse by annotators, the brackets, labels, and depthof nesting combine to prohibit rapid scanning.However, it should be noted that this format isthe source of the TextTree concept.
Because byeliminating labels, null elements, and mostbrackets, and further flattening, the morereadable TextTree form emerges:Stokelysays{storesrevivespecialslike three cansof peasfor 99 cents}2.1   ImplicationsThe conventional parse tree representationsdiscussed above can be a bottleneck in parserdevelopment, most importantly in checkingparser accuracy with respect to current focusareas and in extending coverage to new genres ordomains.
For these purposes, one would like toreview the results of analyzing collections ofrelatively long (100+ sentence) documents.
Butunless this can be done quickly, a parserdeveloper or grammar writer is tempted to relyon statistics with respect to the number ofsentences given a parse, and declare victory if ahigh rate of parsing is reported.Another approach to assessing parser qualityis to rely on treebank-based bracket scoringmeasures, if treebanks are available in theparticular genre.
This can also can be a pitfall, asbracket scores tend to be unconsciouslyinterpreted as measures of full-sentence parsecorrectness,  which they are not.On the other hand, treebank-basedmeasurements can play a useful secondary rolein evaluating parser development progress and incomparing parsers.
But if insufficienttreebanked material is available for the relevantdomains and/or genres, a custom treebank mustbe developed.
This process generally consists oftwo phases: (a) a bootstrapping phase in whichan existing parser is applied to the corpus toproduce either a single ''best'' tree, or multiplealternative trees, for each sentence, and then (b) asecond phase in which annotators approve or edita given parse tree, select among alternative trees,or manually create a full parse tree when noparse exists.
All of the second phase alternativesare difficult given conventional parse-treerepresentations.
For example, experiments byMarcus et al (1993) associated with the PennTreebank indicated that for the first annotationpass ''At an average rate of 750 words per hour, ateam of five part-time annotators ?
'', i.e., a bitmore than a page of this text per hour.
Aids toselecting among alternative 2D trees can begiven in the form of differentiating features(Oepen et al, 2002), but their effectiveness inhelping to select among large trees differing inattachment choices is not clear.Another activity impeded by conventionalparse tree representations is regression testing.As a grammar increases in size, it is advisable tofrequently re-apply the grammar to a large testcorpus to determine whether recent changes havehad negative effects on accuracy.
While theexistence of differences in output can be detectedby automatic means, one would like to assess thedifferences by quickly comparing the divergentparses, which is difficult to do using detailedparse displays for long sentences.Finally, an activity that is rarely discussedbut is becoming increasingly important isproviding comprehensible parser demonstrations.A syntactic parser is not an end-in-itself, but abuilding block in larger NLP research anddevelopment efforts.
The criteria for selecting aparser for a project include both the kind ofinformation provided by the parser and parseraccuracy.
However, current parser outputrepresentations are not geared to allowingpotential users to quickly assess accuracy withrespect to document types of interest.3   The TextTree Generator: ExternalsTextTrees are generated by an essentiallygrammar-independent processor from acombination of(a) parser output trees that have been put into astandard form that retains the grammar-specificcategory labels and constituents, but whosedetails conform to the requirements of a parser-independent tool interface, and,(b) a set of grammar-specific <category,directive> pairs, e.g., "<ROOT, Align>".
Eachpair specifies a default formatting forconstituents in the category, specifically whethertheir sub-constituents are to be aligned vertically,or concatenated relative to a marked point, withsubsequent children indented.
These defaults areused in the absence of overriding factors such ascoordination.The directives used are very simple ones,because the simpler the formatting rules, themore likely it is that outputs can be accuratelychecked for correctness, and edited convenientlyIt is assumed that the parser output eitherincludes conventional parse trees, or that suchtrees can be derived from the output.
Thisassumption covers many grammaticalapproaches, such as CG, HPSG, LFG, and TAG.The logical configuration implied by theseinputs is shown in Figure 4.
It includes a parser-specific adaptor to convert parser-output treesinto a standard form.
The adaptor need not bevery large; for the hybrid parser it consists ofabout 75 lines of Java code.The subsections below discuss the standardParseTree form, the directives that have beenidentified to date and their formatting effects,and the treatment of coordination.3.1   Standard ParseTreesA standard ParseTree consists of recursivelyFigure 4: Logical Configuration.nested subtrees, each representing a constituentC, and each indicating: A grammar-specific category label for C. Whether C should be considered the head ofits immediately containing constituent P forformatting purposes.
Generally, this is thecase if C is, or dominates, the lexical head ofP, but might differ from that to obtain somedesired formatting effects.
As heads areidentified by most parsers, this marker canusually be set by parser-specific rulesembedded in the adaptor. Whether C is a coord_dom, that is, whetherit immediately dominates the participants ina coordination. If C is a leaf, the associated token,  and (optionally) whether C is a multi-word.3.2    Formatting DirectivesTo generate TextTrees for a particular grammar,a <category, directive> pair is provided for eachcategory that will appear in a ParseTree for thegrammar.
The directive specifies how to formatthe children of constituents in the category in theabsence of lengthy coordinations.The definitions of the directives make use ofone additional locator for a constituent, itsreal_head.
While we generally want thedirectives to format constituents relative to theirlexical heads, in some grammars, those heads aredeeply nested.
For example, a tree for an NPmight be generated by rules such as:NPz => DET NPyNPy => ADJ* NPxNPx => NOUN PP*where each of the underscored terms are heads,but the real_head of NPz is the head NOUN ofNPx.
More generally, the real_head of aconstituent C is the first constituent found byfollowing the head-marked descendants of Cdownward until either (a) a leaf or headlessconstituent is reached, or (b) a post-modifiedhead-marked constituent is found.Using this definition, the current formattingdirectives are as follows:Align:  Align specifies that all children of theconstituent are vertically aligned.
Thus, forexample, a directive <ROOT, Align>, wouldcause a constituent generated by a rule ''ROOT=> NP, VP'' to be formatted as:formatted NPformatted VPConcatHead: ConcatHead concatenates thetokens of a constituent (with separating blanks)up to and including its real_head (as definedabove), and indents and aligns the post-modifiersof the real_head.
For example, given thedirective ''<NP, ConcatHead>'', a constituentproduced by the rules:NPz => PreMod* NPxNPx => NOUN PostMods*would be formatted as:All-words-in-PreMod* NOUNFormatted PostMod1Formatted PostMod2ConcatCompHead: ConcatCompHeadconcatenates everything up to and including thereal_head, and concatenates with that the resultsof a ConcatHead of the first post-modifier of thereal_head (if any).This directive is motivated by rules such as''PP => P NP'', where the desired formattinggroups the head with words up to and includingthe lexical head of the single complement, e.g.,of the manin the moonConcatSimpleComp: ConcatSimpleCompconcatenates material up to and including thereal_head and, if first post-modifying constituentis a simple token, concatenates that as well, andthen aligns and indents any further post-modifiers of the real_head.
It thus formats nounphrases for languages that routinely use simpleadjective post-modifiers.
For example (Sp.
):La casa blancaque ...ConcatPreHead: This directive concatenatesmaterial, if any, before the real_head, and then ifsuch material exists, increases the indent level.
Itthen aligns the following component.
Thedirective is intended for formatting clauses thatbegin with subordinating conjunctions,complementizers, or relative pronouns, where thegrammar has identified the following sententialcomponent as the head, but it is more readable toindent that head under the initial constituent.
Inpractice, in such cases it is easier to just alter thehead marker within the adaptor.3.3  Treatment of CoordinationThe directives listed in the previous subsectionare defaults that specify the handling ofcategories in the absence of coordination.
A setof coordinated constituents are always indicatedby surrounding square brackets ([ ]).
If thecoordination occurs within a requestedconcatenation, then if the width of coordinationis less than a predesignated size, the non-tokenconstituents of the coordination are bracketed, asin:[{Old men} and women]in the park.However, if the width of the coordinationexceeds that size, the concatenation is convertedto an alignment to avoid line wrap, for example,Hegavesizeable donationsto[the church,the school,andthe new museumof art.
]4   TextTree Generator:  ImplementationTextTrees are produced in two steps.
First, aParseTree is processed to form one or moreInternalTextTrees (ITTs), which are then mappedinto an external TextTree.
Most of the work isaccomplished in the first step; the use of asecond step allows the first to focus on logicalstructure, independent of many details ofindentation, linebreaks, and punctuation.We begin by describing ITTs and theirrelationship to external TextTrees, to motivatethe description of ITT formation.
We thendescribe the mapping from ParseTrees to ITTs.4.1   Transforming ITTs to StringsFigure 5 shows a simple ITT and its associatedexternal TextTree.
The node labels of an ITT arecalled the "headparts" of their associatedsubtrees.
Headparts may be null, simple strings,or references to other ITTs.If the headpart of a subtree is null, like that ofthe outermost subtree of Figure 5, the externalTextTrees for its children are aligned verticallyat the current level of indentation.
Also, if thesubtree is not outermost, the aligned sequence isbracketed.However, if the headpart of a subtree is asimple string, as in the other subtrees of Figure 5ITT, that string is printed at the current level ofindentation, and the external TextTrees for itschildren, if any, are aligned vertically at the nextlevel of indentation.The headpart of a subtree may also referenceanother ITT, as illustrated in Figure 6.
Such areference headpart  signals the bracketedinclusion of the referenced tree, which has a nullheadpart, at the current indentation level.
Thispermits the entire referenced tree to be post-modified.
The brackets used depend on a featureassociated with  the referenced tree, and areeither [ ], if the referenced tree represents acoordination, or { } otherwise.Pseudo-code for the ITT2String function thatproduces external TextTrees from ITTs is shownin Figure 7.
The code omits the treatment ofnon-bracketing punctuation; in general, care istaken to prefix or suffix punctuation to tokensappropriately.4.2    Transforming ParseTrees to ITTsTo transform ParseTrees into ITTs, subtreesare processed recursively top-down using thefunction BuildITT of Figure 8.
Its argumentsare an input subtree p, and a directive override,which may be null.
It returns an ITT for p.BuildITT sets the operational directive d aseither the override argument, if that is non-null,or to the default directive for the category of p.Then, if d is ''Align'', the result ITT has a nullheadpart and, if p is a coord_dom, the isCoordproperty of the ITT is set.
The children of theresult ITT are the ITTs of p's children.However, if d specifies that an initial sequenceof p is to be concatenated, BuildITT uses therecursive function Catenate (Figure 9) to obtainthe initial sequence if possible.However, a simple transformationto TextTrees of  parse treesHowever,a simple transformationof  parse treesto TextTreescan expeditethese activities.can expeditethese activitiesFigure 5: Simple InternalTextTree.However, a simple transformation T1these activitiesT1 (coord)significantlysimplifiesand generallyexpeditesHowever,a simple transformation[significantly simplifiesandgenerally expedites]these activities.Figure 6: ITT with long coordinationFigure 7.
The ITT2String Function              Figure 9.
The Catenate FunctionFigure 8.
The BuildITT FunctionFunction ITT2String(ITT s, String indent)returns String// indent is a string of blanks, eol is end-of-lineSet ls to nullIf s has a null headpart, set nextIndent to indent+ 1 blank  (adds space for [ or { bracket )Otherwise  set nextIndent to indent + N blanks,where N is the constant indent incrementIf s has a headpart referenceset nextIndent to nextIndent + 1 blankIf s has children, set ls to the lines producedby ITT2String (ci, nextIndent)for each child ci of sIf s has a headpart string hsReturn the concatenation ofindent, hs, eol, lsElse if s has a headpart reference to an ITT s2Return the concatenation ofITT2String(s2, indent), lsElse  (s has a null headpart )Remove initial & trailing whitespace from lsIf s is a coordination, set pfx to [ and sfx to ]Else set pfx to {and sfx to }Return the concatenation ofnextIndent ?
1 blank, pfx, ls, sfx, eolFunction Catenate (ParseTree p,ParseTree r)  returns <Code, String>1.
Set result = null, code = incomplete.If p is a leaf, result = the token of p.2.
for each child ci of p,while code ?
complete:If ci = r, set code = complete.Set <ccode, cstring> to result ofCatenate(ci, r)If (ccode = failed) return <failed, null>If p is a coord_dom & cstring not 1 word// indicate coordinated within concatSuffix ''{cstring}'' to result.Otherwise suffix ''cstring'' to resultIf ccode = complete, set code = complete3.
Finally,If p = r, set code = complete.if p is a coord_dom andthe length of  result  > LONG_CONST,return <failed, null>.Else if p is a coord_domReturn <code, ''[result]''>Else if p is a multi-word,return <code, ''|result|''>Otherwise return <code, result>Function BuildITT(ParseTree p,  Directive override)  returns ITTSet  d to override if non-null, otherwise to the default category directive for p.1.
If p is a leaf or a multiword:- return an ITT whose headline concatenates the tokens spanned by p, and which has no children.If p is a multiword, bracket the headline by |.2.
Else if  d is Alignreturn an ITT with a null headpart, and children built by invoking BuildITT(ci, null) for eachchild ci of p. If p is a coord_dom, indicate that the ITT isCoord.3.
Otherwise concatenate:a) Find the nested subtree s that contains the rightmost element r to be concatenated according to d.b)  Set the pair <ccode, cstring> to the result of Catenate (p, r).c) If ccode ?
''failed'',   return ITT with headpart = cstring, and children formed by BuildITT(ci, null)for each child ci of s after r.  .d) Otherwise return a directive-dependent tree aligning the contained coordination, for example:For ConcatHead:  i.
Let p?
be like p but without the right siblings of the real_head of  pii.
Return an ITT with headpart referencing the  results of BuildITT (p?, Align)and with children obtained from BuildITT(ci, null)for each right sibling ci of the  real_head of pFor ConcatCompHead:  Return an ITT obtained by invoking BuildITT(p, ConcatHead)Catenate takes two arguments: a ParseTree pwhose initial sequence is to be concatenated, and aParseTree r, beyond which the concatenation is tostop, based on the particular directive involved.
Itreturns a pair <Code, String>.
The Code indicatesif the concatenation succeeded, failed, or isincomplete.
If the concatenation succeeded,BuildITT creates an ITT with a headpart stringcontaining the concatenated sequence, and childrenconsisting of ITTs of the right-hand siblings of r.Complex aspects of BuildITT and Catenaterelate to coordinations within to-be-concatenatedextents.
The desired effect is to include shortcoordinations within the concatenation, whilebracketing its boundaries and non-leafcomponents, e.g. ''
[{Old men} and women]'', butaligning the elements of longer coordinations.So if Catenate (in step 3) determines that thestring resulting from a coordination is very long, itdirectly or indirectly returns an indicator toBuildITT that the concatenation failed.
BuildITT(step 3d) then returns an ITT structured so that thesub-constituents that were to be concatenated areeventually shown as aligned, using differentmethods dependent on the directive d.For example, if d is ConcatHead orConcatSimpleComp, the result ITT contains:a) a headpart reference to an ITT built byBuildITT(p?,Align), where p?
is like p butwithout the right-hand siblings of itsreal_head, andb) children consisting of the ITTs for thoseright-hand siblings, if any.5   TextTree Files and TextTreebanksPrevious sections focused on the production ofindividual TextTrees by the TextTree generator.This section considers some uses of the generatorand auxiliary methods within parser development.A particularly useful approach for producingparser output review material using the generator issketched in Figure 10.
In that approach, the bestparses for a document are converted to standardParseTrees expressed as XML entities and writtento a file of such entities, interspersed with arbitraryother information.
A separate, parser-independentprocess that includes the TextTree generator thencreates a TextTree file by substituting TextTreesfor the ParseTree entities.Figure 10.
TextTree file creationSuch a process may be used to create theHTML TextTree file of Figure 1, which is astandard output form for the hybrid parser.
TheTextTrees are surrounded by HTML  <pre> and</pre> tags to maintain the spacing and linebreaksproduced by the generator.
The interspersedinformation in this case consists of the sentencetext and links to detailed displays of the best parsefound, other parses with high preference scores, aswell as the initial chunks.Reviewing parse results for a document thenconsists of reading the TextTree file and,depending on circumstances, either simply notingor classifying the errors found for later debugging,or investigating them further via the links to thedetailed displays.5.1   TextTreebanksWhatever the limitations (Carroll et al, 1998) ofthe various treebank-based bracket scoringmeasures derived from the Parseval approach ofBlack et al (1991), they can be useful inmonitoring parser development progress and incomparing the capabilities of different parsers, atleast if there are large differences in scores.But, as noted earlier, obtaining a fully labeledtreebank for a specific domain or genre is generallya very labor-intensive process.
A potentially lesscostly alternative is to create informal treebanksconsisting of TextTree files corrected by manualediting.Both corrected and uncorrected TextTree filescan be converted to files consisting of fully-bracketed strings by a simple script that considersonly the contained TextTrees.
The script bracketsthe TextTrees so as to retain the explicit brackets,and to add brackets around each subtree, i.e.,around each sequence of a line and the lines, ifany, indented beneath it, directly or indirectly.For example, a full bracketing of the TextTreeof Figure 5 would be:{However,}{a simple transformation{of parse trees}{to text trees}}{can expedite {these activities}}The actual bracketed strings produced by thescript are ones acceptable as input to the EVALBbracket scoring program (Sekine and Collins,1997) with all brackets expressed as parentheses,and brackets added around words (apparentlyrequired but subsequently discarded by theprogram).
Also, most punctuation is removed,  toavoid spurious token differences.
Then bracketedfiles deriving from noncorrected and correctedTextTree files can be submited to EVALB toobtain a bracket score.Lest this approach be dismissed as overlysketchy, we note that the resulting brackets aresimilar to those resulting from a proposal byRingger et al (2004) for neutralizing differencesbetween parser outputs and treebanks bybracketing maximal head projections, plus someadditional mechanisms to further minimizebrackets.5.2    Preventing Bracketing ErrorsTo avoid bracketing errors resulting fromimprecise spacing in manually edited trees, theTextTree indentations used are relatively wide.With indentations of five spaces, it is likely that animprecisely positioned line will be placed closer tothe desired level of indentation, so that anintelligent guess can be made as to the intent.
Forexample, in:Line aLine bLine cLine e?
?the misplaced Line e begins at a point closer to thebeginning  of Line a than Line b.
It is thenreasonable to guess that Line e is sibling to Line a.6    ExperimentsThis section describes two limited experiments toassess the efficiency of reviewing parser outputsfor accuracy using TextTrees.
One of theexperiments also measures the efficiency ofTextTreebank creation6.1  First ExperimentThe document used in the first experiment was theroughly 500-sentence "Executive Summary" of the9/11 Commission Report (2004).
After parsing bythe hybrid parser, the expected TextTree file,excerpted in Figure 1, was created, reproducingeach sentence and, for parsed sentences, theTextTree string for the best parse obtained, and alinks to the detailed two-dimensional treerepresentation.Of  the 503 sentences,  averaging 20 words inlength, 93% received a parse.
However, reviewingthe TextTree file revealed that at least 191 of the470 parsed sentences were not parsed correctly,indicating an actual parser accuracy for thedocument of at most 55%.Reviewing the TextTrees required 92 minutes,giving a review rate of 6170 words per hour,including checking detailed parses for sentenceswhere errors might have lain in the TextTreeformatting.That review rate can be compared to the resultsof Marcus et al (1993) for post-annotationproofreading of  relatively flat, indented, but fullylabelled and bracketed trees.
Those resultsindicated that:"... experienced annotators can proofreadpreviously corrected material at very highspeeds.
A parsed subcorpus ?was recentlyproofread at an average speed of approximately4,000 words per annotator per hour.
At thisrate?, annotators are able to find and correctgross errors in parsing, but do not have time tocheck, for example, whether they agree with allprepositional phrase attachments.
"While the two tasks are not exactly comparable,if we assume that little or no editing was requiredin proofreading, the ballpark improvement of 50%is encouraging.6.2  Second ExperimentFor the second experiment, we used the CB01 file3of the Brown Corpus (Francis and Kucera, 1964),and reviewed both the TextTree file and then,separately, the detailed 2D parse trees alsoproduced.While the parser reported that 91 of the 103sentences, or 88%, received a parse, the review ofthe TextTree file determined that at most only 50sentences, or 48.5%, received a fully correct parse.The review of the detailed parse trees revealedthree additional errors.The comparison of review times was lessdecisive in this experiment, with the rate for theTextTree review being 5733 words per hour, andthat for the detailed 2D representation 4410 wordsper hour.However, there were non-quantifiabledifferences in the reviews.
One difference was thatthe TextTree review was a fairly relaxed exercise,while the review of the 2D representations wasdone with a conscious attempt at speed, and wasquite stressful?not something one would like torepeat.
Another difference was that scanning theTextTree file provided a far better cumulativesense of the kinds of problems presented by thedocument/genre, which might be further exploitedby a more interactive format (e.g., using HTMLforms) allowing users to classify erroneous parsesby error type.The experiment was then extended to check theextent to which TextTree files could efficientlyedited for purposes of limited-function treebankcreation.For this purpose, to minimize typing when asentence had no complete parse, the TextTree fileincluded the list of chunks identified by the XIPparser.
A similar strategy could be used withparsers that, when no complete parse is found,return an unrelated sequence of adjacentconstituent parses.
This is done by some statisticaland finite-state-based parsers, as well as by parsersemploying the "fitted parse" method of Jensen etal.
(1983) or the "fragment parse" methoddescribed by Riezler et al (2002).3With part-of-speech tags removedEditing the TextTrees for the 104 sentences,with an average sentence length of 21 words,required 83 minutes, giving a rate of 1518 wordsper hour.
This might be compared with theaverage of 750 words per hour for the initialannotation of parses in the Penn Treebankexperiment (Marcus et al, 1993) mentionedearlier.After the TextTreebank was created, thebracketing script described in section 5.1 wasapplied both to the original TextTree file and to theTextTreebank, and the results were submitted tothe EVALB program, which reported a bracketingrecall of 71%, a bracketing precision of 84%, andan average crossing bracket count of 1.15.4  Twosentences were not processed because of tokenmismatches.
As expected, these scores were muchhigher than the percentage of sentences correctlyparsed.7   Related WorkMost natural language parsers include someprovision for displaying their outputs, includingparse tree representations, and/or other material,such as head-dependent relationships, featurestructures, etc.
These displays are generallyintended for deep review of  parse results, and thusattempt to maximize information contentWork on reducing review effort usually takesplace in the context of developing treebanks byselection among, and/or manual correction of,parser outputs.
In this area,  the most relevantwork may be the experiments of Marcus et al(1993) using bracketed, indented trees.
They foundthat annotator efficiency required eliminatingmany detailed brackets and category labels fromthe parser outputs presented.
Other approachesrest, in whole or in part,  on selecting amongalternative two dimensional parse trees,  such asthe distinguishing-feature-augmented  approach ofOepen et al (2002), discussed in Section 2.
Asdiscussed in that section, however, two-dimensional tree displays are problematical forlarge trees, and it is not clear to what extentdistinguishing feature information can expediteselecting among attachment choices.4Sentences not receiving complete parses weresubmitted to EVALB without any brackets, contributingzero counts to the total # of correct constituents recalled.Other related work deals with reducingmeasured differences between parser outputs andtreebanks due solely to grammar style.
Asdiscussed in section 5, the bracketed material usedin treebank-based measurement by Ringger et al(2004) is similar to the bracketed material thatwould result from systematically bracketingTextTrees.Finally, work by Walker (2001),  intended notfor parser/grammar development, but to facilitatereading and improve retention, produces textformats that bear some similarity to TextTrees, butare more closely attuned to spoken phrasing thansyntactic form.
The method uses complexsegmentation and indentation strategies generallybased on a combination of punctuation and  closed-class words.8   Directions for Further DevelopmentWe have described an implemented method forpresenting parser outputs permitting fast visualinspection of the results of parsing longdocuments, as well as  efficient editing to createinformal treebanks.
Although, because of theirflattened, skeletal nature, TextTrees can hide someparser errors, we strongly believe, based onextended usage,  that the convenience of reviewingTextTree files can contribute significantly to parserdevelopment efforts.However, TextTrees are best suited to languagestending to a fixed word order and post-modification.
To improve results for languagesand language aspects that do not fall into thiscategory, TextTrees  might be augmented withhighlighting and color to indicate syntacticfunctions.
One way this could be done in a parser-and grammar-independent way is by adding astring-valued representation feature to the standardParseTrees, and an additional set of directives tomap the feature values to representationalternatives.
For example, a subtree might beannotated with the feature Rep = "subject", and theadditional  directives might include <"subject","blue">.
Experimentation is needed to determinethe usability of this approach.Another topic to explore is the use of TextTreesin the creation of corpora annotated by deepersyntactic or semantic information.
Because suchinformation is generally expressed in forms thatare even less readable than parse trees, a usefulbootstrapping practice is to allow annotators toapprove, or select among, parser output treesconnected with the deeper information (King et al,2003).
TextTrees might be used to facilitate thisprocess, with annotators either (a) interactivelyselecting among alternative TextTrees or, becausethere may be many alternatives, (b) editing aTextTree file containing at most one parse for eachsentence (possibly chosen arbitrarily) and using theresult for offline selection.
Also, a parser used inthe bootstrapping might refer to bracketedTextTreebanks to avoid pruning away elements ofcorrect parses at intermediate points in parsing.A third direction for further work is in extendingthe TextTree approach to deal with outputs ofdependency-based parsers that do not produceconstituent trees.
While this should be a naturalextension, an alternative system of features anddirectives would seem to be needed.AcknowledgementsThe author is indebted to Tracy Holloway King,John Sowa, and the anonymous reviewers for theirvery helpful comments and suggestions on earlierversions of this paper.ReferencesSalah A?t-Mokhtar, Jean-Pierre Chanod, andClaude Roux.
2002.
Robustness beyondshallowness: incremental deep parsing, NaturalLanguage Engineering 8:121-144 CambridgeUniversity Press.Ann Bies, Mark Ferguson, Karen Katz, and RobertMacIntyre.
1995.
Bracketing Guidelines for theTreebank II Style Penn Treebank Project.E.
Black, S. Abney, D. Flickinger, C. Gdaniec, R.Grishman, P. Harrison, D. Hindle, R. Ingria, F.Jelinek, J. Klavans, M. Liberman, M. Marcus, S.Roukos, B Santorini, and T. Strzalkowski.
1991.A procedure for quantitatively comparing thesyntactic coverage of english grammars.
In Proc1991 DARPA Speech and Natural LanguageWorkshop.
306-311.John Carroll, Ted Briscoe, and Antonio SanFillipo.1998.
Parser Evaluation: a Survey and a NewProposal.
In Rubio et al, editors, Proc FirstInternational Conference on LanguageResources and Evaluation, Granada, Spain, 447-454.W.
Nelson Francis and Henry Kucera.
1964, 1971,1979.
Brown Corpus Manual.
Available athttp://helmer.aksis.uib.no/icame/brown/bcm.html Corpus available at http://nltk.sourceforge.netKaren Jensen, George Heidorn, Lance A. Miller,Yael Ravin.
1983.
Parse fitting and prose fixing:getting a hold on ill-formedness.
ComputationalLinguistics 9(3-4):147-160.Tracy H. King, Stefanie Dipper, Anette Frank,Jonas Kuhn, and John Maxwell.
2000.Ambiguity management in grammar writing.
InErhard Hinrichs, Detmar Meurers, and ShulyWintner, editors, Proc ESSLLI Workshop onLinguistic Theory and GrammarImplementation, 5-19, Birmingham, UK.Tracy H. King, Richard Crouch, Stefan Riezler,Mary Dalrymple, and Ronald M. Kaplan.
2003.The PARC 700 Dependency Bank, Proc.
4thInternational Workshop on LinguisticallyInterpreted Corpora, at the 10th Conf of theEuropean Chapter of the ACL, Budapest.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a LargeAnnotated Corpus of English: The PennTreebank, Computational Linguistics 19(2):313-330.Paula S. Newman.
2005.
Abstract: TextTrees inGrammar Development.
Workshop on GrammarEngineering of the 2005 ScandinavianConference on Linguistics (SCL).
Available athttp://www.hf.ntnu.no/scl/grammar_engineering9/11 Commission.
2004.
Final Report of theNational Commission on Terrorist Attacks uponthe United States, Executive Summary.Available at http://www.gpoaccess.gov/911Stephan Oepen, Dan Flickinger, KristinaToutanova, and Christopher Manning.
2002.LinGO Redwoods: A Rich and DynamicTreebank for HPSG, Proc Workshop onTreebanks and Linguistic Theories (TLT02),Sozopol, BulgariaStephan Riezler, Tracy H. King, Ronald M.Kaplan, Richard Crouch, John T. Maxwell, andMark Johnson.
2002.
Parsing the Wall StreetJournal using a Lexical-Functional Grammarand Discriminative Estimation Techniques.
Proc40th Annual Meeting of the Assoc.
forComputational Linguistics (ACL), Philadelphia,271-278.Eric K. Ringger, Robert C. Moore, LucyVanderwende, Hisam Suzuki and EugeneCharniak.
2004.
Using the Penn Treebank toEvaluate Non-Treebank Parsers, Proc 2004Language Resources and EvaluationConference (LREC), Lisbon, Portugal.Satoshi Sekine and Michael Collins.
1997.
EvalB.Available at http://nlp.cs.nyu.edu/evalbRandall C. Walker.
2001.
Method and Apparatusfor Displaying Text Based Upon AttributesFound Within the Text, U.S. Patent 6279017.
