Proceedings of the 25th International Conference on Computational Linguistics, pages 106?108,Dublin, Ireland, August 23-29 2014.Formulating Queries for Collecting Training Examples in Visual ConceptClassificationRami Albatal, Kevin McGuinness, Feiyan Hu, Alan F. SmeatonInsight Centre for Data AnalyticsDublin City UniversityGlasnevin, Dublin 9, Ireland.
{rami.albatal}@insight-centre.orgAbstractVideo content can be automatically analysed and indexed using trained classifiers which maplow-level features to semantic concepts.
Such classifiers need training data consisting of setsof images which contain such concepts and recently it has been discovered that such trainingdata can be located using text-based search to image databases on the internet.
Formulating thetext queries which locate these training images is the challenge we address here.
In this paperwe present preliminary results on TRECVid data of concept classification using automaticallycrawled images as training data and we compare the results with those obtained from manuallyannotated training sets.1 IntroductionContent-based access to video archives is based on learning the presence of semantic concepts in videocontent by mapping low-level features like colour and texture, to high-level concepts.
Concept clas-sification is typically based on training classifiers on a set of annotated ground truth images (called atraining set) containing positive and negative example images of a given semantic concept.
The man-ual creation of training sets for each concept is a time-consuming and costly task.
An alternative is toautomatically gather training examples using available resources on the Internet.
Several recent papershave demonstrated the effectiveness of such an approach.
(Griffin, 2006) used search engine results togather material for learning the appearance of categories, (Chatfield and Zisserman, 2013) shows thateffective classifiers can be trained on-the-fly at query time using examples collected from Google Imagesearch.
The AXES research search engine (McGuinness et al., 2014) uses a combination of pre-trainedclassifiers and on-the-fly classifiers trained using examples from Google Image search.
(Kordumova etal., 2014) investigate four practices for collecting training negative and positive examples from sociallytagged videos and images.The above work exploits the visual content of the collected example images while the question ofhow to formulate a textual query for collecting the data is not yet considered.
It is important to notehere that current search engines are not use content-based image classifiers, they are based the text fromthe embedding pages, and that is not always accurate or scalable.
This represents a unique relationshipbetween vision (the images used to train a concept classifier) and language (the text used to find thosetraining images).
In this work, we initiate a first step to addressing the problem of formulating textqueries that collect positive example images for classifier training.
This first step is based on queryingweb resources with single-term queries and comparing the classification results with those from manuallyannotated training sets.
The results show the potential of automatic crawling and open the way forenhancing query formulation by adding external lexical or semantic resources.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Licence details: http://creativecommons.org/licenses/by/4.0/1062 Automatic gathering of training examplesOur goal is to create a framework for automatic training of classifiers by gathering training examplesfrom available resources on the Internet.
The steps for formulating a query are straightforward andwidely-used in information retrieval, especially query expansion.
First, an initial query is pre-processedby removing stop words and applying stemming techniques; then external lexical and/or semantic basesare explored in order to enrich the query and add useful terms which help in retrieving relevant trainingexamples and excluding false positives.
The resulting query is then posted to a search engine or imagedatabases and the retrieved images are used as positive examples for a classification algorithm.
Our planis to use the Natural Language Toolkit (Bird et al., 2009) for stemming and stop words removal, andWordNet as external lexical base (Fellbaum, 1998).3 Experiments and resultsExperiments were conducted on the TRECVid (Smeaton et al., 2006) 2014 semantic indexing develop-ment data set.
Single-term queries were posted to two data sources: Google Images, and ImageNet (animage database organized according to the nouns of the WordNet hierarchy where each node is depictedby an average of +500 images).
Unlike results from Google Images, examples gathered from ImageNetare classified by human annotators, and are therefore a ?purer?
source of training images.
To ensure ahigh-quality training set, we first search for the concept in ImageNet; if the concept does not exist in asan ImageNet visual category, we use images retrieved using a search for the term on Google Images.We carried out two experiments to evaluate the performance of classifiers trained on manually anno-tated (internal) data provided by TRECVid versus data gathered from external sources.
These externalsources are search engines that retrieve images using textual queries, as explained in section2.
The firstexperiment used data from the 2013x subset of the TRECVid 2014 development data and the secondused external training data gathered as discussed above in the first paragraph of this section.
Accuracyin both cases was evaluated using inferred average precision (infAP) on the 2013y subset of the develop-ment data.
One-vs-all linear SVM classifiers were used for both experiments, trained on visual featuresextracted using pre-trained deep convolutional neural networks (CNN) using the Caffe software (Jia,2013).Classifiers for 34 of the 60 concepts were trained using data from ImageNet and the remaining usingexamples from Google Images.
All classifiers trained using images from Google Images demonstratedpoorer infAP than those trained on internal data.
Of the 34 classifiers trained on ImageNet, 7 demon-strated improved infAP (airplane, beach, bicycling, classroom, computers, dancing, flowers, highway).In all cases it was possible to find more positive examples on ImageNet than in the internal set.
Internalout-performed ImageNet in the remaining 27 cases.
There were several possible reasons for this.
Inmany cases there were fewer examples from ImageNet than in the internal set (12/27 cases) and in somecases the ImageNet examples were incorrect.
For example, in the case of the concept ?hand?, severalsynsets matching the term consisted entirely of wristwatches.
Finally, in other cases, the concept text(the query) was either ambiguous or insufficiently semantically rich, for example ?greeting?
(greetingcards were retrieved) and ?government leader?
(smaller subset of such leaders in internal training data).4 Hypothesis, challenges, and future workThe experiments indicate that automatically-gathered external training data can, in some cases, outper-form annotated internal training data when it is sufficiently plentiful and of high quality.
Using bothhigh-quality external data and internal examples during training has the potential to improve resultsoverall.
A more sophisticated method of gathering external training examples that takes into accountthe semantics of the concept and of related concepts could provide even higher-quality external data.A significant challenge is in combining such semantic query expansion with visual analysis to ensurethat the additional examples collected are relevant.
This could potentially be achieved by bootstrappinga classifier on internal examples and then using this to classify external examples gathered by iterativesemantic query expansion, updating the classifier model with each batch of accepted training examples.107ReferencesSteven Bird, Ewan Klein, and Edward Loper.
2009.
Natural Language Processing with Python.
O?Reilly Media,Inc., 1st edition.Ken Chatfield and Andrew Zisserman.
2013.
Visor: Towards on-the-fly large-scale object category retrieval.
InComputer Vision ACCV 2012, volume 7725 of Lecture Notes in Computer Science, pages 432?446.Christiane Fellbaum, editor.
1998.
WordNet: an electronic lexical database.
MIT Press.Lewis D Griffin.
2006.
Optimality of the basic colour categories for classification.
Multimedia Tools and Appli-cations, 3.6:71?85.Yangqing Jia.
2013.
Caffe: An open source convolutional architecture for fast feature embedding.
http://caffe.berkeleyvision.org/.S.
Kordumova, X. Li, and C. G. M. Snoek.
2014.
Best practices for learning video concept detectors from socialmedia examples.
Multimedia Tools and Applications, pages 1?25, May.Kevin McGuinness, Robin Aly, Ken Chatfield, Omkar Parkhi, Relja Arandjelovic, Matthijs Douze, Max Kemman,Martijn Kleppe, Peggy Van Der Kreeft, Kay Macquarrie, et al.
2014.
The AXES research video search system.In IEEE International Conference on Acoustics, Speech, and Signal Processing.Alan F Smeaton, Paul Over, and Wessel Kraaij.
2006.
Evaluation campaigns and TRECVid.
In Proceedings ofthe 8th ACM international workshop on Multimedia information retrieval, pages 321?330.
ACM.108
