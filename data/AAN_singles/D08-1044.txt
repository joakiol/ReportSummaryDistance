Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 419?428,Honolulu, October 2008. c?2008 Association for Computational LinguisticsScalable Language Processing Algorithms for the Masses: A Case Study inComputing Word Co-occurrence Matrices with MapReduceJimmy LinThe iSchool, University of MarylandNational Center for Biotechnology Information, National Library of Medicinejimmylin@umd.eduAbstractThis paper explores the challenge of scalingup language processing algorithms to increas-ingly large datasets.
While cluster comput-ing has been available in commercial environ-ments for several years, academic researchershave fallen behind in their ability to work onlarge datasets.
I discuss two barriers contribut-ing to this problem: lack of a suitable pro-gramming model for managing concurrencyand difficulty in obtaining access to hardware.Hadoop, an open-source implementation ofGoogle?s MapReduce framework, provides acompelling solution to both issues.
Its simpleprogramming model hides system-level de-tails from the developer, and its ability to runon commodity hardware puts cluster comput-ing within the reach of many academic re-search groups.
This paper illustrates thesepoints with a case study in building word co-occurrence matrices from large corpora.
I con-clude with an analysis of an alternative com-puting model based on renting instead of buy-ing computer clusters.1 IntroductionOver the past couple of decades, the field of compu-tational linguistics (and more broadly, human lan-guage technologies) has seen the emergence andlater dominance of empirical techniques and data-driven research.
Concomitant with this trend is acoherent research thread that focuses on exploitingincreasingly-large datasets.
Banko and Brill (2001)were among the first to demonstrate the importanceof dataset size as a significant factor governing pre-diction accuracy in a supervised machine learningtask.
In fact, they argued that size of training setwas perhaps more important than the choice of ma-chine learning algorithm itself.
Similarly, exper-iments in question answering have shown the ef-fectiveness of simple pattern-matching techniqueswhen applied to large quantities of data (Brill et al,2001; Dumais et al, 2002).
More recently, thisline of argumentation has been echoed in experi-ments with Web-scale language models.
Brants etal.
(2007) showed that for statistical machine trans-lation, a simple smoothing technique (dubbed StupidBackoff) approaches the quality of the Kneser-Neyalgorithm as the amount of training data increases,and with the simple method one can process signifi-cantly more data.Challenges in scaling algorithms to increasingly-large datasets have become a serious issue for re-searchers.
It is clear that datasets readily availabletoday and the types of analyses that researchers wishto conduct have outgrown the capabilities of individ-ual computers.
The only practical recourse is to dis-tribute the computation across multiple cores, pro-cessors, or machines.
The consequences of failingto scale include misleading generalizations on arti-ficially small datasets and limited practical applica-bility in real-world contexts, both undesirable.This paper focuses on two barriers to develop-ing scalable language processing algorithms: chal-lenges associated with parallel programming andaccess to hardware.
Google?s MapReduce frame-work (Dean and Ghemawat, 2004) provides an at-tractive programming model for developing scal-able algorithms, and with the release of Hadoop,an open-source implementation of MapReduce lead419by Yahoo, cost-effective cluster computing is withinthe reach of most academic research groups.
Itis emphasized that this work focuses on large-data algorithms from the perspective of academia?colleagues in commercial environments have longenjoyed the advantages of cluster computing.
How-ever, it is only recently that such capabilities havebecome practical for academic research groups.These points are illustrated by a case study in build-ing large word co-occurrence matrices, a simple taskthat underlies many NLP algorithms.The remainder of the paper is organized as fol-lows: the next section overviews the MapReduceframework and why it provides a compelling solu-tion to the issues sketched above.
Section 3 intro-duces the task of building word co-occurrence ma-trices, which provides an illustrative case study.
Twoseparate algorithms are presented in Section 4.
Theexperimental setup is described in Section 5, fol-lowed by presentation of results in Section 6.
Im-plications and generalizations are discussed follow-ing that.
Before concluding, I explore an alternativemodel of computing based on renting instead of buy-ing hardware, which makes cluster computing prac-tical for everyone.2 MapReduceThe only practical solution to large-data challengestoday is to distribute the computation across mul-tiple cores, processors, or machines.
The de-velopment of parallel algorithms involves a num-ber of tradeoffs.
First is that of cost: a decisionmust be made between ?exotic?
hardware (e.g.,large shared memory machines, InfiniBand inter-connect) and commodity hardware.
There is signif-icant evidence (Barroso et al, 2003) that solutionsbased on the latter are more cost effective?and forresource-constrained academic NLP groups, com-modity hardware is often the only practical route.Given appropriate hardware, researchers muststill contend with the challenge of developing soft-ware.
Quite simply, parallel programming is diffi-cult.
Due to communication and synchronizationissues, concurrent operations are notoriously chal-lenging to reason about.
Reliability and fault tol-erance become important design considerations onclusters containing large numbers of unreliable com-modity parts.
With traditional parallel programmingmodels (e.g., MPI), the developer shoulders the bur-den of explicitly managing concurrency.
As a result,a significant amount of the programmer?s attentionis devoted to system-level details, leaving less timefor focusing on the actual problem.Recently, MapReduce (Dean and Ghemawat,2004) has emerged as an attractive alternative toexisting parallel programming models.
The Map-Reduce abstraction shields the programmer fromhaving to explicitly worry about system-level is-sues such as synchronization, inter-process commu-nication, and fault tolerance.
The runtime is ableto transparently distribute computations across largeclusters of commodity hardware with good scalingcharacteristics.
This frees the programmer to focuson solving the problem at hand.MapReduce builds on the observation that manyinformation processing tasks have the same basicstructure: a computation is applied over a large num-ber of records (e.g., Web pages, bitext pairs, or nodesin a graph) to generate partial results, which arethen aggregated in some fashion.
Naturally, the per-record computation and aggregation function varyaccording to task, but the basic structure remainsfixed.
Taking inspiration from higher-order func-tions in functional programming, MapReduce pro-vides an abstraction at the point of these two opera-tions.
Specifically, the programmer defines a ?map-per?
and a ?reducer?
with the following signatures:map: (k1, v1)?
[(k2, v2)]reduce: (k2, [v2])?
[(k3, v3)]Key-value pairs form the basic data structure inMapReduce.
The mapper is applied to every inputkey-value pair to generate an arbitrary number of in-termediate key-value pairs ([.
.
.]
is used to denote alist).
The reducer is applied to all values associatedwith the same intermediate key to generate outputkey-value pairs.
This two-stage processing structureis illustrated in Figure 1.Under the framework, a programmer needs onlyto provide implementations of the mapper and re-ducer.
On top of a distributed file system (Ghe-mawat et al, 2003), the runtime transparently han-dles all other aspects of execution, on clusters rang-ing from a few to a few thousand nodes.
The run-time is responsible for scheduling map and reduce420Shuffling: group valuesbykeysmapmapmapmapreducereducereduceinputinputinputinputoutputoutputoutputFigure 1: Illustration of the MapReduce framework: the?mapper?
is applied to all input records, which generatesresults that are aggregated by the ?reducer?.
The runtimegroups together values by keys.workers on commodity hardware assumed to be un-reliable, and thus is tolerant to various faults througha number of error recovery mechanisms.
In the dis-tributed file system, data blocks are stored on thelocal disks of machines in the cluster?the Map-Reduce runtime handles the scheduling of mapperson machines where the necessary data resides.
Italso manages the potentially very large sorting prob-lem between the map and reduce phases whereby in-termediate key-value pairs must be grouped by key.As an optimization, MapReduce supports the useof ?combiners?, which are similar to reducers exceptthat they operate directly on the output of mappers(in memory, before intermediate output is written todisk).
Combiners operate in isolation on each nodein the cluster and cannot use partial results fromother nodes.
Since the output of mappers (i.e., thekey-value pairs) must ultimately be shuffled to theappropriate reducer over a network, combiners al-low a programmer to aggregate partial results, thusreducing network traffic.
In cases where an opera-tion is both associative and commutative, reducerscan directly serve as combiners.Google?s proprietary implementation of Map-Reduce is in C++ and not available to the public.However, the existence of Hadoop, an open-sourceimplementation in Java spearheaded by Yahoo, al-lows anyone to take advantage of MapReduce.
Thegrowing popularity of this technology has stimu-lated a flurry of recent work, on applications in ma-chine learning (Chu et al, 2006), machine transla-tion (Dyer et al, 2008), and document retrieval (El-sayed et al, 2008).3 Word Co-occurrence MatricesTo illustrate the arguments outlined above, I presenta case study using MapReduce to build word co-occurrence matrices from large corpora, a commontask in natural language processing.
Formally, theco-occurrence matrix of a corpus is a square N ?N matrix where N corresponds to the number ofunique words in the corpus.
A cell mij contains thenumber of times word wi co-occurs with word wjwithin a specific context?a natural unit such as asentence or a certain window of m words (where mis an application-dependent parameter).
Note thatthe upper and lower triangles of the matrix are iden-tical since co-occurrence is a symmetric relation.This task is quite common in corpus linguisticsand provides the starting point to many other algo-rithms, e.g., for computing statistics such as point-wise mutual information (Church and Hanks, 1990),for unsupervised sense clustering (Schu?tze, 1998),and more generally, a large body of work in lexi-cal semantics based on distributional profiles, dat-ing back to Firth (1957) and Harris (1968).
Thetask also has applications in information retrieval,e.g., (Schu?tze and Pedersen, 1998; Xu and Croft,1998), and other related fields as well.
More gen-erally, this problem relates to the task of estimatingdistributions of discrete events from a large numberof observations (more on this in Section 7).It is obvious that the space requirement for thisproblem is O(N2), where N is the size of the vocab-ulary, which for real-world English corpora can behundreds of thousands of words.
The computationof the word co-occurrence matrix is quite simple ifthe entire matrix fits into memory?however, in thecase where the matrix is too big to fit in memory,a naive implementation can be very slow as mem-ory is paged to disk.
For large corpora, one needsto optimize disk access and avoid costly seeks.
Asillustrated in the next section, MapReduce handlesexactly these issues transparently, allowing the pro-grammer to express the algorithm in a straightfor-ward manner.A bit more discussion of the task before mov-ing on: in many applications, researchers havediscovered that building the complete word co-occurrence matrix may not be necessary.
For ex-ample, Schu?tze (1998) discusses feature selection421techniques in defining context vectors; Mohammadand Hirst (2006) present evidence that conceptualdistance is better captured via distributional profilesmediated by thesaurus categories.
These objections,however, miss the point?the focus of this paperis on practical cluster computing for academic re-searchers; this particular task serves merely as anillustrative example.
In addition, for rapid proto-typing, it may be useful to start with the completeco-occurrence matrix (especially if it can be built ef-ficiently), and then explore how algorithms can beoptimized for specific applications and tasks.4 MapReduce ImplementationThis section presents two MapReduce algorithmsfor building word co-occurrence matrices for largecorpora.
The goal is to illustrate how the prob-lem can be concisely captured in the MapReduceprogramming model, and how the runtime hidesmany of the system-level details associated with dis-tributed computing.Pseudo-code for the first, more straightforward,algorithm is shown in Figure 2.
Unique documentids and the corresponding texts make up the inputkey-value pairs.
The mapper takes each input doc-ument and emits intermediate key-value pairs witheach co-occurring word pair as the key and the inte-ger one as the value.
In the pseudo-code, EMIT de-notes the creation of an intermediate key-value pairthat is collected (and appropriately sorted) by theMapReduce runtime.
The reducer simply sums upall the values associated with the same co-occurringword pair, arriving at the absolute counts of the jointevent in the corpus (corresponding to each cell in theco-occurrence matrix).For convenience, I refer to this algorithm as the?pairs?
approach.
Since co-occurrence is a symmet-ric relation, it suffices to compute half of the matrix.However, for conceptual clarity and to generalize toinstances where the relation may not be symmetric,the algorithm computes the entire matrix.The Java implementation of this algorithm is quiteconcise?less than fifty lines long.
Notice the Map-Reduce runtime guarantees that all values associatedwith the same key will be gathered together at the re-duce stage.
Thus, the programmer does not need toexplicitly manage the collection and distribution of1: procedure MAP1(n, d)2: for all w ?
d do3: for all u ?
NEIGHBORS(w) do4: EMIT((w, u), 1)1: procedure REDUCE1(p, [v1, v2, .
.
.
])2: for all v ?
[v1, v2, .
.
.]
do3: sum?
sum+ v4: EMIT(p, sum)Figure 2: Pseudo-code for the ?pairs?
approach for com-puting word co-occurrence matrices.1: procedure MAP2(n, d)2: INITIALIZE(H)3: for all w ?
d do4: for all u ?
NEIGHBORS(w) do5: H{u} ?
H{u}+ 16: EMIT(w,H)1: procedure REDUCE2(w, [H1, H2, H3, .
.
.
])2: INITIALIZE(Hf )3: for all H ?
[H1, H2, H3, .
.
.]
do4: MERGE(Hf , H)5: EMIT(w,Hf )Figure 3: Pseudo-code for the ?stripes?
approach forcomputing word co-occurrence matrices.partial results across a cluster.
In addition, the pro-grammer does not need to explicitly partition the in-put data and schedule workers.
This example showsthe extent to which distributed processing can bedominated by system issues, and how an appropriateabstraction can significantly simplify development.It is immediately obvious that Algorithm 1 gen-erates an immense number of key-value pairs.
Al-though this can be mitigated with the use of a com-biner (since addition is commutative and associa-tive), the approach still results in a large amount ofnetwork traffic.
An alternative approach is presentedin Figure 3, first reported in Dyer et al (2008).The major difference is that counts of co-occurringwords are first stored in an associative array (H).The output of the mapper is a number of key-valuepairs with words as keys and the corresponding asso-ciative arrays as the values.
The reducer performs anelement-wise sum of all associative arrays with thesame key (denoted by the function MERGE), thus ac-422cumulating counts that correspond to the same cellin the co-occurrence matrix.
Once again, a com-biner can be used to cut down on the network trafficby merging partial results.
In the final output, eachkey-value pair corresponds to a row in the word co-occurrence matrix.
For convenience, I refer to thisas the ?stripes?
approach.Compared to the ?pairs?
approach, the ?stripes?approach results in far fewer intermediate key-valuepairs, although each is significantly larger (and thereis overhead in serializing and deserializing associa-tive arrays).
A critical assumption of the ?stripes?approach is that at any point in time, each associa-tive array is small enough to fit into memory (other-wise, memory paging may result in a serious loss ofefficiency).
This is true for most corpora, since thesize of the associative array is bounded by the vo-cabulary size.
Section 6 compares the efficiency ofboth algorithms.15 Experimental SetupWork reported in this paper used the English Gi-gaword corpus (version 3),2 which consists ofnewswire documents from six separate sources, to-taling 7.15 million documents (6.8 GB compressed,19.4 GB uncompressed).
Some experiments usedonly documents from the Associated Press World-stream (APW), which contains 2.27 million docu-ments (1.8 GB compressed, 5.7 GB uncompressed).By LDC?s count, the entire collection contains ap-proximately 2.97 billion words.Prior to working with Hadoop, the corpus wasfirst preprocessed.
All XML markup was removed,followed by tokenization and stopword removal us-ing standard tools from the Lucene search engine.All tokens were replaced with unique integers for amore efficient encoding.
The data was then packedinto a Hadoop-specific binary file format.
The entireGigaword corpus took up 4.69 GB in this format; theAPW sub-corpus, 1.32 GB.Initial experiments used Hadoop version 0.16.0running on a 20-machine cluster (1 master, 19slaves).
This cluster was made available to the Uni-1Implementations of both algorithms are included inCloud9, an open source Hadoop library that I have been de-veloping to support research and education, available from myhomepage.2LDC catalog number LDC2007T07versity of Maryland as part of the Google/IBM Aca-demic Cloud Computing Initiative.
Each machinehas two single-core processors (running at either 2.4GHz or 2.8 GHz), 4 GB memory.
The cluster has anaggregate storage capacity of 1.7 TB.
Hadoop ran ontop of a virtualization layer, which has a small butmeasurable impact on performance; see (Barham etal., 2003).
Section 6 reports experimental resultsusing this cluster; Section 8 explores an alternativemodel of computing based on ?renting cycles?.6 ResultsFirst, I compared the running time of the ?pairs?
and?stripes?
approaches discussed in Section 4.
Run-ning times on the 20-machine cluster are shownin Figure 4 for the APW section of the Gigawordcorpus: the x-axis shows different percentages ofthe sub-corpus (arbitrarily selected) and the y-axisshows running time in seconds.
For these experi-ments, the co-occurrence window was set to two,i.e., wi is said to co-occur with wj if they are nomore than two words apart (after tokenization andstopword removal).Results demonstrate that the stripes approach isfar more efficient than the pairs approach: 666 sec-onds (11m 6s) compared to 3758 seconds (62m 38s)for the entire APW sub-corpus (improvement by afactor of 5.7).
On the entire sub-corpus, the map-pers in the pairs approach generated 2.6 billion in-termediate key-value pairs totally 31.2 GB.
After thecombiners, this was reduced to 1.1 billion key-valuepairs, which roughly quantifies the amount of datainvolved in the shuffling and sorting of the keys.
Onthe other hand, the mappers in the stripes approachgenerated 653 million intermediate key-value pairstotally 48.1 GB; after the combiners, only 28.8 mil-lion key-value pairs were left.
The stripes approachprovides more opportunities for combiners to aggre-gate intermediate results, thus greatly reducing net-work traffic in the sort and shuffle phase.Figure 4 also shows that both algorithms exhibithighly desirable scaling characteristics?linear inthe corpus size.
This is confirmed by a linear regres-sion applied to the running time data, which yieldsR2 values close to one.
Given that the stripes algo-rithm is more efficient, it is used in the remainder ofthe experiments.4230 500 1000 15002000 2500 3000 350040000  20  40  60  80  100running time (seconds) percentage of the APW sub-corpora of the English GigawordEfficiency comparison of approaches to computing word co-occurrence matricesR2 = 0.992R2 = 0.999"stripes" approach"pairs" approachFigure 4: Running time of the two algorithms (?stripes?
vs.
?pairs?)
for computing word co-occurrence matrices onthe APW section of the Gigaword corpus.
The cluster used for this experiment contains 20 machines, each with twosingle-core processors.0 1000 20003000 4000 500060001  2  3  4  5  6  7running time (seconds) window size (number of words)Running time for different widow sizesR2 = 0.992Figure 5: Running times for computing word co-occurrence matrices from the entire Gigaword corpus with varyingwindow sizes.
The cluster used for this experiment contains 20 machines, each with two single-core processors.424With a window size of two, computing the wordco-occurrence matrix for the entire Gigaword corpus(7.15 million documents) takes 37m 11s on the 20-machine cluster.
Figure 5 shows the running timeas a function of window size.
With a window ofsix words, running time on the complete Gigawordcorpus rises to 1h 23m 45s.
Once again, the stripesalgorithm exhibits the highly desirable characteris-tic of linear scaling in terms of window size, as con-firmed by the linear regression with an R2 value veryclose to one.7 DiscussionThe elegance of the programming model and goodscaling characteristics of resulting implementationsmake MapReduce a compelling tool for a varietyof natural language processing tasks.
In fact, Map-Reduce excels at a large class of problems in NLPthat involves estimating probability distributions ofdiscrete events from a large number of observationsaccording to the maximum likelihood criterion:PMLE(B|A) =c(A,B)c(A)=c(A,B)?B?
c(A,B?
)(1)In practice, it matters little whether these eventsare words, syntactic categories, word alignmentlinks, or any construct of interest to researchers.
Ab-solute counts in the stripes algorithm presented inSection 4 can be easily converted into conditionalprobabilities by a final normalization step.
Recently,Dyer et al (2008) used this approach for word align-ment and phrase extraction in statistical machinetranslation.
Of course, many applications requiresmoothing of the estimated distributions?this prob-lem also has known solutions in MapReduce (Brantset al, 2007).Synchronization is perhaps the single largest bot-tleneck in distributed computing.
In MapReduce,this is handled in the shuffling and sorting of key-value pairs between the map and reduce phases.
De-velopment of efficient MapReduce algorithms criti-cally depends on careful control of intermediate out-put.
Since the network link between different nodesin a cluster is by far the component with the largestlatency, any reduction in the size of intermediateoutput or a reduction in the number of key-valuepairs will have significant impact on efficiency.8 Computing on DemandThe central theme of this paper is practical clus-ter computing for NLP researchers in the academicenvironment.
I have identified two key aspects ofwhat it means to be ?practical?
: the first is an appro-priate programming model for simplifying concur-rency management; the second is access to hardwareresources.
The Hadoop implementation of Map-Reduce addresses the first point and to a large ex-tent the second point as well.
The cluster used forexperiments in Section 6 is modest by today?s stan-dards and within the capabilities of many academicresearch groups.
It is not even a requirement for thecomputers to be rack-mounted units in a machineroom (although that is clearly preferable); there areplenty of descriptions on the Web about Hadoopclusters built from a handful of desktop machinesconnected by gigabit Ethernet.Even without access to hardware, cluster comput-ing remains within the reach of resource-constrainedacademics.
?Utility computing?
is an emerging con-cept whereby anyone can provision clusters on de-mand from a third-party provider.
Instead of up-front capital investment to acquire a cluster and re-occurring maintenance and administration costs, onecould ?rent?
computing cycles as they are needed?this is not a new idea (Rappa, 2004).
One such ser-vice is provided by Amazon, called Elastic ComputeCloud (EC2).3 With EC2, researchers could dynam-ically create a Hadoop cluster on-the-fly and teardown the cluster once experiments are complete.
Todemonstrate the use of this technology, I replicatedsome of the previous experiments on EC2 to providea case study of this emerging model of computing.Virtualized computation units in EC2 are calledinstances.
At the time of these experiments, the ba-sic instance offers, according to Amazon, 1.7 GBof memory, 1 EC2 Compute Unit (1 virtual corewith 1 EC2 Compute Unit), and 160 GB of instancestorage.
Each instance-hour costs $0.10 (all pricesgiven in USD).
Computational resources are simplycharged by the instance-hour, so that a ten-instancecluster for ten hours costs the same as a hundred-instance cluster for one hour (both $10)?the Ama-zon infrastructure allows one to dynamically provi-sion and release resources as necessary.
This is at-3http://www.amazon.com/ec24250 1000 20003000 4000 500010  20  30  40  50  60  70  80  901x2x3x4x1x 2x 3x 4xrunning time (seconds) relative speedupsize of EC2 cluster (number of slave instances)Computing word co-occurrence matrices on Amazon EC2relative size of EC2 cluster$2.76 $2.92 $2.89 $2.64 $2.69 $2.63 $2.5920-machine cluster R2 = 0.997Figure 6: Running time analysis on Amazon EC2 with various cluster sizes; solid squares are annotated with the costof each experiment.
Alternate axes (circles) plot scaling characteristics in terms increasing cluster size.tractive for researchers, who could on a limited basisallocate clusters much larger than they could other-wise afford if forced to purchase the hardware out-right.
Through virtualization technology, Amazonis able to parcel out allotments of processor cycleswhile maintaining high overall utilization across adata center and exploiting economies of scale.Using EC2, I built word co-occurrence matricesfrom the entire English Gigaword corpus (windowof two) on clusters of various sizes, ranging from20 slave instances all the way up to 80 slave in-stances.
The entire cluster consists of the slave in-stances plus a master controller instance that servesas the job submission queue; the clusters ran Hadoopversion 0.17.0 (the latest release at the time theseexperiments were conducted).
Running times areshown in Figure 6 (solid squares), with varying clus-ter sizes on the x-axis.
Each data point is anno-tated with the cost of running the complete experi-ment.4 Results show that computing the completeword co-occurrence matrix costs, quite literally, acouple of dollars?certainly affordable by any aca-demic researcher without access to hardware.
Forreference, Figure 6 also plots the running time ofthe same experiment on the 20-machine cluster used4Note that Amazon bills in whole instance-hour increments;these figures assume fractional accounting.in Section 6 (which contains 38 worker cores, eachroughly comparable to an instance).The alternate set of axes in Figure 6 shows thescaling characteristics of various cluster sizes.
Thecircles plot the relative size and speedup of theEC2 experiments, with respect to the 20-slave clus-ter.
The results show highly desirable linear scalingcharacteristics.The above figures include only the cost of runningthe instances.
One must additionally pay for band-width when transferring data in and out of EC2.
Atthe time these experiments were conducted, Ama-zon charged $0.10 per GB for data transferred in and$0.17 per GB for data transferred out.
To comple-ment EC2, Amazon offers persistent storage via theSimple Storage Service (S3),5 at a cost of $0.15 perGB per month.
There is no charge for data transfersbetween EC2 and S3.
The availability of this servicemeans that one can choose between paying for datatransfer or paying for persistent storage on a cyclicbasis?the tradeoff naturally depends on the amountof data and its permanence.The cost analysis presented above assumesoptimally-efficient use of Amazon?s services; end-to-end cost might better quantify real-world usageconditions.
In total, the experiments reported in this5http://www.amazon.com/s3426section resulted in a bill of approximately thirty dol-lars.
The figure includes all costs associated with in-stance usage and data transfer costs.
It also includestime taken to learn the Amazon tools (I previouslyhad no experience with either EC2 or S3) and torun preliminary experiments on smaller datasets (be-fore scaling up to the complete corpus).
The lack offractional accounting on instance-hours contributedto the larger-than-expected costs, but such wastagewould naturally be reduced with more experimentsand higher sustained use.
Overall, these cost appearto be very reasonable, considering that the largestcluster in these experiments (1 master + 80 slave in-stances) might be too expensive for most academicresearch groups to own and maintain.Consider another example that illustrates the pos-sibilities of utility computing.
Brants et al (2007)described experiments on building language modelswith increasingly-large corpora using MapReduce.Their paper reported experiments on a corpus con-taining 31 billion tokens (about an order of magni-tude larger than the English Gigaword): on 400 ma-chines, the model estimation took 8 hours.6 WithEC2, such an experiment would cost a few hundreddollars?sufficiently affordable that availability ofdata becomes the limiting factor, not computationalresources themselves.The availability of ?computing-on-demand?
ser-vices and Hadoop make cluster computing practi-cal for academic researchers.
Although Amazon iscurrently the most prominent provider of such ser-vices, they are not the sole player in an emergingmarket?in the future there will be a vibrant marketwith many competing providers.
Considering thetradeoffs between ?buying?
and ?renting?, I wouldrecommend the following model for an academic re-search group: purchase a modest cluster for devel-opment and for running smaller experiments; use acomputing-on-demand service for scaling up and forrunning larger experiments (since it would be moredifficult to economically justify a large cluster if itdoes not receive high sustained utilization).If the concept of utility computing takes hold, itwould have a significant impact on computer sci-ence research in general: the natural implication is6Brants et al were affiliated with Google, so access to hard-ware was not an issue.that algorithms should not only be analyzed in tradi-tional terms such as asymptotic complexity, but alsoin terms of monetary costs, in relationship to datasetand cluster size.
One can argue that cost is a more di-rect and practical measure of algorithmic efficiency.9 ConclusionThis paper address two challenges faced by aca-demic research groups in scaling up natural lan-guage processing algorithms to large corpora: thelack of an appropriate programming model for ex-pressing the problem and the difficulty in getting ac-cess to hardware.
With this case study in buildingword co-occurrence matrices from large corpora, Idemonstrate that MapReduce, via the open sourceHadoop implementation, provides a compelling so-lution.
A large class of algorithms in computa-tional linguistics can be readily expressed in Map-Reduce, and the resulting code can be transparentlydistributed across commodity clusters.
Finally, the?cycle-renting?
model of computing makes accessto large clusters affordable to researchers with lim-ited resources.
Together, these developments dra-matically lower the entry barrier for academic re-searchers who wish to explore large-data issues.AcknowledgmentsThis work was supported by the Intramural ResearchProgram of the NIH, National Library of Medicine;NSF under awards IIS-0705832 and IIS-0836560;DARPA/IPTO Contract No.
HR0011-06-2-0001 un-der the GALE program.
Any opinions, findings,conclusions, or recommendations expressed in thispaper are the author?s and do not necessarily reflectthose of the sponsors.
I would like to thank Ya-hoo!
for leading the development of Hadoop, IBMand Google for hardware support via the AcademicCloud Computing Initiative (ACCI), and Amazonfor EC2/S3 support.
This paper provides a neutralevaluation of EC2 and S3, and should not be inter-preted as endorsement for the commercial servicesoffered by Amazon.
I wish to thank Philip Resnikand Doug Oard for comments on earlier drafts ofthis paper, and Ben Shneiderman for helpful editingsuggestions.
I am, as always, grateful to Esther andKiri for their kind support.427ReferencesMichele Banko and Eric Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.
InProceedings of the 39th Annual Meeting of the As-sociation for Computational Linguistics (ACL 2001),pages 26?33, Toulouse, France.Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand,Tim Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, andAndrew Warfield.
2003.
Xen and the art of virtualiza-tion.
In Proceedings of the 19th ACM Symposium onOperating Systems Principles (SOSP-03), pages 164?177, Bolton Landing, New York.Luiz Andre?
Barroso, Jeffrey Dean, and Urs Ho?lzle.
2003.Web search for a planet: The Google cluster architec-ture.
IEEE Micro, 23(2):22?28.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 858?867, Prague, Czech Re-public.Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,and Andrew Ng.
2001.
Data-intensive question an-swering.
In Proceedings of the Tenth Text REtrievalConference (TREC 2001), pages 393?400, Gaithers-burg, Maryland.Cheng-Tao Chu, Sang Kyun Kim, Yi-An Lin, YuanYuanYu, Gary Bradski, Andrew Ng, and Kunle Olukotun.2006.
Map-Reduce for machine learning on multi-core.
In Advances in Neural Information ProcessingSystems 19 (NIPS 2006), pages 281?288, Vancouver,British Columbia, Canada.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22?29.Jeffrey Dean and Sanjay Ghemawat.
2004.
MapReduce:Simplified data processing on large clusters.
In Pro-ceedings of the 6th Symposium on Operating SystemDesign and Implementation (OSDI 2004), pages 137?150, San Francisco, California.Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,and Andrew Ng.
2002.
Web question answering:Is more always better?
In Proceedings of the 25thAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval (SI-GIR 2002), pages 291?298, Tampere, Finland.Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy Lin.2008.
Fast, easy, and cheap: Construction of statisticalmachine translation models with MapReduce.
In Pro-ceedings of the Third Workshop on Statistical MachineTranslation at ACL 2008, pages 199?207, Columbus,Ohio.Tamer Elsayed, Jimmy Lin, and Douglas Oard.
2008.Pairwise document similarity in large collections withMapReduce.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguis-tics (ACL 2008), Companion Volume, pages 265?268,Columbus, Ohio.John R. Firth.
1957.
A synopsis of linguistic theory1930?55.
In Studies in Linguistic Analysis, SpecialVolume of the Philological Society, pages 1?32.
Black-well, Oxford.Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Le-ung.
2003.
The Google File System.
In Proceedingsof the 19th ACM Symposium on Operating SystemsPrinciples (SOSP-03), pages 29?43, Bolton Landing,New York.Zelig S. Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley, New York.Saif Mohammad and Graeme Hirst.
2006.
Distribu-tional measures of concept-distance: A task-orientedevaluation.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2006), pages 35?43, Sydney, Australia.Michael A. Rappa.
2004.
The utility business modeland the future of computing services.
IBM SystemsJournal, 34(1):32?42.Hinrich Schu?tze and Jan O. Pedersen.
1998.
Acooccurrence-based thesaurus and two applications toinformation retrieval.
Information Processing andManagement, 33(3):307?318.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Jinxi Xu and W. Bruce Croft.
1998.
Corpus-basedstemming using cooccurrence of word variants.
ACMTransactions on Information Systems, 16(1):61?81.428
