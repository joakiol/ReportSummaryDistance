Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 912?919,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA System for Large-Scale Acquisition of Verbal, Nominal and AdjectivalSubcategorization Frames from CorporaJudita Preiss, Ted Briscoe, and Anna KorhonenComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FD, UKJudita.Preiss, Ted.Briscoe, Anna.Korhonen@cl.cam.ac.ukAbstractThis paper describes the first system forlarge-scale acquisition of subcategorizationframes (SCFs) from English corpus datawhich can be used to acquire comprehen-sive lexicons for verbs, nouns and adjectives.The system incorporates an extensive rule-based classifier which identifies 168 verbal,37 adjectival and 31 nominal frames fromgrammatical relations (GRs) output by a ro-bust parser.
The system achieves state-of-the-art performance on all three sets.1 IntroductionResearch into automatic acquisition of lexical in-formation from large repositories of unannotatedtext (such as the web, corpora of published text,etc.)
is starting to produce large scale lexical re-sources which include frequency and usage infor-mation tuned to genres and sublanguages.
Suchresources are critical for natural language process-ing (NLP), both for enhancing the performance ofstate-of-art statistical systems and for improving theportability of these systems between domains.One type of lexical information with particularimportance for NLP is subcategorization.
Accessto an accurate and comprehensive subcategoriza-tion lexicon is vital for the development of success-ful parsing technology (e.g.
(Carroll et al, 1998),important for many NLP tasks (e.g.
automatic verbclassification (Schulte im Walde and Brew, 2002))and useful for any application which can benefitfrom information about predicate-argument struc-ture (e.g.
Information Extraction (IE) ((Surdeanu etal., 2003)).The first systems capable of automatically learn-ing a small number of verbal subcategorizationframes (SCFs) from unannotated English corporaemerged over a decade ago (Brent, 1991; Manning,1993).
Subsequent research has yielded systems forEnglish (Carroll and Rooth, 1998; Briscoe and Car-roll, 1997; Korhonen, 2002) capable of detectingcomprehensive sets of SCFs with promising accu-racy and demonstrated success in application tasks(e.g.
(Carroll et al, 1998; Korhonen et al, 2003)).Recently, a large publicly available subcategoriza-tion lexicon was produced using such technologywhich contains frame and frequency information forover 6,300 English verbs ?
the VALEX lexicon (Ko-rhonen et al, 2006).While there has been considerable work in thearea, most of it has focussed on verbs.
Althoughverbs are the richest words in terms of subcatego-rization and although verb SCF distribution data islikely to offer the greatest boost in parser perfor-mance, accurate and comprehensive knowledge ofthe many noun and adjective SCFs in English couldimprove the accuracy of parsing at several levels(from tagging to syntactic and semantic analysis).Furthermore the selection of the correct analysisfrom the set returned by a parser which does not ini-tially utilize fine-grained lexico-syntactic informa-tion can depend on the interaction of conditionalprobabilities of lemmas of different classes occur-912ring with specific SCFs.
For example, a) and b) be-low indicate the most plausible analyses in which thesentential complement attaches to the noun and verbrespectivelya) Kim (VP believes (NP the evidence (Scomp thatSandy was present)))b) Kim (VP persuaded (NP the judge) (Scomp thatSandy was present))However, both a) and b) consist of an identicalsequence of coarse-grained lexical syntactic cate-gories, so correctly ranking them requires learn-ing that P (NP | believe).P (Scomp | evidence) >P (NP&Scomp | believe).P (None | evidence)and P (NP | persuade).P (Scomp | judge) <P (NP&Scomp | persuade).P (None | judge).
Ifwe acquired frames and frame frequencies for allopen-class predicates taking SCFs using a single sys-tem applied to similar data, we would have a betterchance of modeling such interactions accurately.In this paper we present the first system for large-scale acquisition of SCFs from English corpus datawhich can be used to acquire comprehensive lexi-cons for verbs, nouns and adjectives.
The classifierincorporates 168 verbal, 37 adjectival and 31 nomi-nal SCF distinctions.
An improved acquisition tech-nique is used which expands on the ideas Yallop etal.
(2005) recently explored for a small experimenton adjectival SCF acquisition.
It involves identifyingSCFs on the basis of grammatical relations (GRs) inthe output of the RASP (Robust Accurate StatisticalParsing) system (Briscoe et al, 2006).As detailed later, the system performs better withverbs than previous comparable state-of-art systems,achieving 68.9 F-measure in detecting SCF types.
Itachieves similarly good performance with nouns andadjectives (62.2 and 71.9 F-measure, respectively).Additionally, we have developed a tool for lin-guistic annotation of SCFs in corpus data aimed atalleviating the process of obtaining training and testdata for subcategorization acquisition.
The tool in-corporates an intuitive interface with the ability tosignificantly reduce the number of frames presentedto the user for each sentence.We introduce the new system for SCF acquisitionin section 2.
Details of the experimental evaluationare supplied in section 3.
Section 4 provides discus-sion of our results and future work, and section 5concludes.2 Description of the SystemA common strategy in existing large-scale SCF ac-quisition systems (e.g.
(Briscoe and Carroll, 1997))is to extract SCFs from parse trees, introducing anunnecessary dependence on the details of a particu-lar parser.
In our approach SCFs are extracted fromGRs ?
representations of head-dependent relationswhich are more parser/grammar independent but atthe appropriate level of abstraction for extraction ofSCFs.A similar approach was recently motivated andexplored by Yallop et al (2005).
A decision-treeclassifier was developed for 30 adjectival SCF typeswhich tests for the presence of GRs in the GR out-put of the RASP (Robust Accurate Statistical Pars-ing) system (Briscoe and Carroll, 2002).
The resultsreported with 9 test adjectives were promising (68.9F-measure in detecting SCF types).Our acquisition process consists of four mainsteps: 1) extracting GRs from corpus data, 2) feedingthe GR sets as input to a rule-based classifier whichincrementally matches them with the correspondingSCFs, 3) building lexical entries from the classifieddata, and 4) filtering those entries to obtain a moreaccurate lexicon.
The details of these steps are pro-vided in the subsequent sections.2.1 Obtaining Grammatical RelationsWe obtain the GRs using the recent, second releaseof the RASP toolkit (Briscoe et al, 2006).
RASP is amodular statistical parsing system which includes atokenizer, tagger, lemmatizer, and a wide-coverageunification-based tag-sequence parser.
We use thestandard scripts supplied with RASP to output the setof GRs for the most probable analysis returned by theparser or, in the case of parse failures, the GRs forthe most likely sequence of subanalyses.
The GRsare organized as a subsumption hierarchy as shownin Figure 1.The dependency relationships which the GRs em-body correspond closely to the head-complementstructure which subcategorization acquisition at-tempts to recover, which makes GRs ideal input tothe SCF classifier.
Consider the arguments of easy913dependentta arg mod det aux conjmod argncmod xmod cmod pmodsubj dobjsubj compncsubj xsubj csubj obj pcomp clausaldobj obj2 iobj xcomp ccompFigure 1: The GR hierarchy used by RASP???????
?SUBJECT NP 1 ,ADJ-COMPS?PP[PVAL forNP 3],VP???
?MOOD to-infinitiveSUBJECT 3OMISSION 1????????????
?Figure 2: Feature structure for SCFadj-obj-for-to-inf(|These:1_DD2| |example+s:2_NN2| |of:3_IO||animal:4_JJ| |senses:5_NN2| |be+:6_VBR||relatively:7_RR| |easy:8_JJ| |for:9_IF||we+:10_PPIO2| |to:11_TO| |comprehend:12_VV0|)...xcomp(_ be+[6] easy:[8])xcomp(to[11] be+[6] comprehend:[12])ncsubj(be+[6] example+s[2] _)ncmod(for[9] easy[8] we+[10])ncsubj(comprehend[12] we+[10], _)...Figure 3: GRs from RASP for adj-obj-for-to-infin the sentence: These examples of animal sensesare relatively easy for us to comprehend as they arenot too far removed from our own experience.
Ac-cording to the COMLEX classification, this is an ex-ample of the frame adj-obj-for-to-inf, shown inFigure 2, (using AVM notation in place of COMLEXs-expressions).
Part of the output of RASP for thissentence is shown in Figure 3.Each instantiated GR in Figure 3 corresponds toone or more parts of the feature structure in Fig-ure 2. xcomp( be[6] easy[8]) establishes be[6]as the head of the VP in which easy[8] occurs asa complement.
The first (PP)-complement is for us,as indicated by ncmod(for[9] easy[8] we+[10]),with for as PFORM and we+ (us) as NP.
The sec-ond complement is represented by xcomp(to[11]be+[6] comprehend[12]): a to-infinitive VP.
Thexcomp ?Y : pos=vb,val=be ?X : pos=adjxcomp ?S : val=to ?Y : pos=vb,val=be ?W : pos=VV0ncsubj ?Y : pos=vb,val=be ?Z : pos=nounncmod ?T : val=for ?X : pos=adj ?Y: pos=pronncsubj ?W : pos=VV0 ?V : pos=pronFigure 4: Pattern for frame adj-obj-for-to-infNP headed by examples is marked as the subjectof the frame by ncsubj(be[6] examples[2]), andncsubj(comprehend[12] we+[10]) corresponds tothe coindexation marked by 3 : the subject of theVP is the NP of the PP.
The only part of the featurestructure which is not represented by the GRs is coin-dexation between the omitted direct object 1 of theVP-complement and the subject of the whole clause.2.2 SCF ClassifierSCF FramesThe SCFs recognized by the classifier were ob-tained by manually merging the frames exempli-fied in the COMLEX Syntax (Grishman et al, 1994),ANLT (Boguraev et al, 1987) and/or NOMLEX(Macleod et al, 1997) dictionaries and includingadditional frames found by manual inspection ofunclassifiable examples during development of theclassifier.
These consisted of e.g.
some occurrencesof phrasal verbs with complex complementation andwith flexible ordering of the preposition/particle,some non-passivizable words with a surface directobject, and some rarer combinations of governedpreposition and complementizer combinations.The frames were created so that they abstractover specific lexically-governed particles and prepo-sitions and specific predicate selectional preferences914but include some derived semi-predictable boundeddependency constructions.ClassifierThe classifier operates by attempting to match theset of GRs associated with each sentence against oneor more rules which express the possible mappingsfrom GRs to SCFs.
The rules were manually devel-oped by examining a set of development sentencesto determine which relations were actually emittedby the parser for each SCF.In our rule representation, a GR pattern is a set ofpartially instantiated GRs with variables in place ofheads and dependents, augmented with constraintsthat restrict the possible instantiations of the vari-ables.
A match is successful if the set of GRs fora sentence can be unified with any rule.
Unifica-tion of sentence GRs and a rule GR pattern occurswhen there is a one-to-one correspondence betweensentence elements and rule elements that includes aconsistent mapping from variables to values.A sample pattern for matchingadj-obj-for-to-inf can be seen in Fig-ure 4.
Each element matches either an empty GRslot ( ), a variable with possible constraints on partof speech (pos) and word value (val), or an alreadyinstantiated variable.
Unlike in Yallop?s work (Yal-lop et al, 2005), our rules are declarative rather thanprocedural and these rules, written independentlyof the acquisition system, are expanded by thesystem in a number of ways prior to execution.
Forexample, the verb rules which contain an ncsubjrelation will not contain one inside an embeddedclause.
For verbs, the basic rule set contains 248rules but automatic expansion gives rise to 1088classifier rules for verbs.Numerous approaches were investigated to allowan efficient execution of the system: for example, foreach target word in a sentence, we initially find thenumber of ARGument GRs (see Figure 1) containingit in head position, as the word must appear in ex-actly the same set in a matching rule.
This allowsus to discard all patterns which specify a differentnumber of GRs: for example, for verbs each grouponly contains an average of 109 patterns.For a further increase in speed, both the sentenceGRs and the GRs within the patterns are ordered (ac-cording to frequency) and matching is performed us-ing a backing off strategy allowing us to exploit therelatively low number of possible GRs (comparedto the number of possible rules).
The system exe-cutes on 3500 sentences in approx.
1.5 seconds ofreal time on a machine with a 3.2 GHz Intel Xenonprocessor and 4GB of RAM.Lexicon Creation and FilteringLexical entries are constructed for each word andSCF combination found in the corpus data.
Each lex-ical entry includes the raw and relative frequency ofthe SCF with the word in question, and includes var-ious additional information e.g.
about the syntax ofdetected arguments and the argument heads in dif-ferent argument positions1.Finally the entries are filtered to obtain a moreaccurate lexicon.
A way to maximise the accu-racy of the lexicon would be to smooth (correct) theacquired SCF distributions with back-off estimatesbased on lexical-semantic classes of verbs (Korho-nen, 2002) (see section 4) before filtering them.However, in this first experiment with the new sys-tem we filtered the entries directly so that we couldevaluate the performance of the new classifier with-out any additional modules.
For the same reason, thefiltering was done by using a very simple method:by setting empirically determined thresholds on therelative frequencies of SCFs.3 Experimental Evaluation3.1 DataIn order to test the accuracy of our system, we se-lected a set of 183 verbs, 30 nouns and 30 adjec-tives for experimentation.
The words were selectedat random, subject to the constraint that they exhib-ited multiple complementation patterns and had asufficient number of corpus occurrences (> 150) forexperimentation.
We took the 100M-word BritishNational Corpus (BNC) (Burnard, 1995), and ex-tracted all sentences containing an occurrence of oneof the test words.
The sentences were processed us-ing the SCF acquisition system described in the pre-vious section.
The citations from which entries werederived totaled approximately 744K for verbs and219K for nouns and adjectives, respectively.1The lexical entries are similar to those in the VALEX lexi-con.
See (Korhonen et al, 2006) for a sample entry.9153.2 Gold StandardOur gold standard was based on a manual analysisof some of the test corpus data, supplemented withadditional frames from the ANLT, COMLEX, and/orNOMLEX dictionaries.
The gold standard for verbswas available, but it was extended to include addi-tional SCFs missing from the old system.
For nounsand adjectives the gold standard was created.
Foreach noun and adjective, 100-300 sentences from theBNC (an average of 267 per word) were randomlyextracted.
The resulting c. 16K sentences were thenmanually associated with appropriate SCFs, and theSCF frequency counts were recorded.To alleviate the manual analysis we developeda tool which first uses the RASP parser with someheuristics to reduce the number of SCF presented,and then allows an annotator to select the preferredchoice in a window.
The heuristics reduced the av-erage number of SCFs presented alongside each sen-tence from 52 to 7.
The annotator was also presentedwith an example sentence of each SCF and an intu-itive name for the frame, such as PRED (e.g.
Kimis silly).
The program includes an option to recordthat particular sentences could not (initially) be clas-sified.
A screenshot of the tool is shown in Figure 5.The manual analysis was done by two linguists;one who did the first annotation for the whole data,and another who re-evaluated and corrected some ofthe initial frame assignments, and classified most ofthe data left unclassified by the first annotator2).
Atotal of 27 SCF types were found for the nouns and30 for the adjectives in the annotated data.
The av-erage number of SCFs taken by nouns was 9 (withthe average of 2 added from dictionaries to supple-ment the manual annotation) and by adjectives 11(3 of which were from dictionaries).
The latter arerare and may not be exemplified in the data given theextraction system.3.3 Evaluation MeasuresWe used the standard evaluation metrics to evaluatethe accuracy of the SCF lexicons: type precision (thepercentage of SCF types that the system proposes2The process precluded measurements of inter-annotatoragreement, but this was judged less important than the enhancedaccuracy of the gold standard data.Figure 5: Sample screen of the annotation toolwhich are correct), type recall (the percentage of SCFtypes in the gold standard that the system proposes)and the F-measure which is the harmonic mean oftype precision and recall.We also compared the similarity between the ac-quired unfiltered3 SCF distributions and gold stan-dard SCF distributions using various measures ofdistributional similarity: the Spearman rank corre-lation (RC), Kullback-Leibler distance (KL), Jensen-Shannon divergence (JS), cross entropy (CE), skewdivergence (SD) and intersection (IS).
The details ofthese measures and their application to subcatego-rization acquisition can be found in (Korhonen andKrymolowski, 2002).Finally, we recorded the total number of goldstandard SCFs unseen in the system output, i.e.
thetype of false negatives which were never detectedby the classifier.3.4 ResultsTable 1 includes the average results for the 183verbs.
The first column shows the results for Briscoeand Carroll?s (1997) (B&C) system when this sys-tem is run with the original classifier but a morerecent version of the parser (Briscoe and Carroll,2002) and the same filtering technique as our newsystem (thresholding based on the relative frequen-cies of SCFs).
The classifier of B&C system is com-parable to our classifier in the sense that it targets al-most the same set of verbal SCFs (165 out of the 168;the 3 additional ones are infrequent in language andthus unlikely to affect the comparison).
The secondcolumn shows the results for our new system (New).3No threshold was applied to remove the noisy SCFs fromthe distributions.916Verbs - MethodMeasures B&C NewPrecision (%) 47.3 81.8Recall (%) 40.4 59.5F-measure 43.6 68.9KL 3.24 1.57JS 0.20 0.11CE 4.85 3.10SD 1.39 0.74RC 0.33 0.66IS 0.49 0.76Unseen SCFs 28 17Table 1: Average results for verbsThe figures show that the new system clearly per-forms better than the B&C system.
It yields 68.9 F-measure which is a 25.3 absolute improvement overthe B&C system.
The better performance can be ob-served on all measures, but particularly on SCF typeprecision (81.8% with our system vs. 47.3% with theB&C system) and on measures of distributional sim-ilarity.
The clearly higher IS (0.76 vs. 0.49) and thefewer gold standard SCFs unseen in the output of theclassifier (17 vs. 28) indicate that the new system iscapable of detecting a higher number of SCFs.The main reason for better performance is theability of the new system to detect a number of chal-lenging or complex SCFs which the B&C systemcould not detect4.
The improvement is partly at-tributable to more accurate parses produced by thesecond release of RASP and partly to the improvedSCF classifier developed here.
For example, the newsystem is now able to distinguish predicative PP ar-guments, such as I sent him as a messenger from thewider class of referential PP arguments, supportingdiscrimination of several syntactically similar SCFswith distinct semantics.Running our system on the adjective and noun testdata yielded the results summarized in Table 2.
TheF-measure is lower for nouns (62.2) than for verbs(68.9); for adjectives it is slightly better (71.9).54The results reported here for the B&C system are lowerthan those recently reported in (Korhonen et al, 2006) for thesame set of 183 test verbs.
This is because we use an improvedgold standard.
However, the results for the B&C system re-ported using the less ambitious gold standard are still less ac-curate (58.6 F-measure) than the ones reported here for the newsystem.5The results for different word classes are not directly com-parable because they are affected by the total number of SCFsevaluated for each word class, which is higher for verbs andMeasures Nouns AdjectivesPrecision (%) 91.2 95.5Recall (%) 47.2 57.6F-measure 62.2 71.9KL 0.91 0.69JS 0.09 0.05CE 2.03 2.01SD 0.48 0.36RC 0.70 0.77IS 0.62 0.72Unseen SCFs 15 7Table 2: Average results for nouns and adjectivesThe noun and adjective classifiers yield very highprecision compared to recall.
The lower recall fig-ures are mostly due to the higher number of goldstandard SCFs unseen in the classifier output (ratherthan, for example, the filtering step).
This is par-ticularly evident for nouns for which 15 of the 27frames exemplified in the gold standard are missingin the classifier output.
For adjectives only 7 of the30 gold standard SCFs are unseen, resulting in betterrecall (57.6% vs. 47.2% for nouns).For verbs, subcategorization acquisition perfor-mance often correlates with the size of the inputdata to acquisition (the more data, the better perfor-mance).
When considering the F-measure results forthe individual words shown in Table 3 there appearsto be little such correlation for nouns and adjectives.For example, although there are individual high fre-quency nouns with high performance (e.g.
plan,freq.
5046, F 90.9) and low frequency nouns withlow performance (e.g.
characterisation, freq.
91, F40.0), there are also many nouns which contradictthe trend (compare e.g.
answer, freq.
2510, F 50.0with fondness, freq.
71, F 85.7).6Although the SCF distributions for nouns and ad-jectives appear Zipfian (i.e.
the most frequent framesare highly probable, but most frames are infre-quent), the total number of SCFs per word is typi-cally smaller than for verbs, resulting in better resis-tance to sparse data problems.There is, however, a clear correlation betweenthe performance and the type of gold standard SCFstaken by individual words.
Many of the gold stan-lower for nouns and adjectives.
This particularly applies to thesensitive measures of distributional similarity.6The frequencies here refer to the number of citations suc-cessfully processed by the parser and the classifier.917Noun F Adjective Fabundance 75.0 able 66.7acknowledgement 47.1 angry 62.5answer 50.0 anxious 82.4anxiety 53.3 aware 87.5apology 50.0 certain 73.7appearance 46.2 clear 77.8appointment 66.7 curious 57.1belief 76.9 desperate 83.3call 58.8 difficult 77.8characterisation 40.0 doubtful 63.6communication 40.0 eager 83.3condition 66.7 easy 66.7danger 76.9 generous 57.1decision 70.6 imperative 81.8definition 42.8 important 60.9demand 66.7 impractical 71.4desire 71.4 improbable 54.6doubt 66.7 insistent 80.0evidence 66.7 kind 66.7examination 54.6 likely 66.7experimentation 60.0 practical 88.9fondness 85.7 probable 80.0message 66.7 sure 84.2obsession 54.6 unaware 85.7plan 90.9 uncertain 60.0provision 70.6 unclear 63.2reminder 63.2 unimportant 61.5rumour 61.5 unlikely 69.6temptation 71.4 unspecified 50.0use 60.0 unsure 90.0Table 3: System performance for each test noun andadjectivedard nominal and adjectival SCFs unseen by theclassifier involve complex complementation patternswhich are challenging to extract, e.g.
those exem-plified in The argument of Jo with Kim about Fidosurfaced, Jo?s preference that Kim be sacked sur-faced, and that Sandy came is certain.
In addition,many of these SCFs unseen in the data are also verylow in frequency, and some may even be true nega-tives (recall that the gold standard was supplementedwith additional SCFs from dictionaries, which maynot necessarily appear in the test data).The main problem is that the RASP parser system-atically fails to select the correct analysis for someSCFs with nouns and adjectives regardless of theircontext of occurrence.
In future work, we hope to al-leviate this problem by using the weighted GR outputfrom the top n-ranked parses returned by the parseras input to the SCF classifier.4 DiscussionThe current system needs refinement to alleviate thebias against some SCFs introduced by the parser?sunlexicalized parse selection model.
We plan to in-vestigate using weighted GR output with the clas-sifier rather than just the GR set from the highestranked parse.
Some SCF classes also need to be fur-ther resolved mainly to differentiate control optionswith predicative complementation.
This requires alexico-semantic classification of predicate classes.Experiments with Briscoe and Carroll?s systemhave shown that it is possible to incorporate somesemantic information in the acquisition process us-ing a technique that smooths the acquired SCF dis-tributions using back-off (i.e.
probability) estimatesbased on lexical-semantic classes of verbs (Korho-nen, 2002).
The estimates help to correct the ac-quired SCF distributions and predict SCFs which arerare or unseen e.g.
due to sparse data.
They couldalso form the basis for predicting control of predica-tive complements.We plan to modify and extend this technique forthe new system and use it to improve the perfor-mance further.
The technique has so far been appliedto verbs only, but it can also be applied to nounsand adjectives because they can also be classified onlexical-semantic grounds.
For example, the adjec-tive simple belongs to the class of EASY adjectives,and this knowledge can help to predict that it takessimilar SCFs to the other class members and thatcontrol of ?understood?
arguments will pattern witheasy (e.g.
easy, difficult, convenient): The problemwill be simple for John to solve, For John to solvethe problem will be simple, The problem will be sim-ple to solve, etc.Further research is needed before highly accuratelexicons encoding information also about semanticaspects of subcategorization (e.g.
different predicatesenses, the mapping from syntactic arguments tosemantic representation of argument structure, se-lectional preferences on argument heads, diathesisalternations, etc.)
can be obtained automatically.However, with the extensions suggested above, thesystem presented here is sufficiently accurate forbuilding an extensive SCF lexicon capable of sup-porting various NLP application tasks.
Such a lex-icon will be built and distributed for research pur-918poses along with the gold standard described here.5 ConclusionWe have described the first system for automaticallyacquiring verbal, nominal and adjectival subcat-egorization and associated frequency informationfrom English corpora, which can be used to buildlarge-scale lexicons for NLP purposes.
We havealso described a new annotation tool for producingtraining and test data for the task.
The acquisitionsystem, which is capable of distinguishing 168verbal, 37 adjectival and 31 nominal frames, clas-sifies corpus occurrences to SCFs on the basis ofGRs produced by a robust statistical parser.
Theinformation provided by GRs closely matches thestructure that subcategorization acquisition seeksto recover.
Our experiment shows that the systemachieves state-of-the-art performance with eachword class.
The discussion suggests ways in whichwe could improve the system further before using itto build a large subcategorization lexicon capable ofsupporting various NLP application tasks.AcknowledgementsThis work was supported by the Royal Society andUK EPSRC project ?Accurate and ComprehensiveLexical Classification for Natural Language Pro-cessing Applications?
(ACLEX).
We would like tothank Diane Nicholls for her help during this work.ReferencesB.
Boguraev, J. Carroll, E. J. Briscoe, D. Carter, and C. Grover.1987.
The derivation of a grammatically-indexed lexiconfrom the Longman Dictionary of Contemporary English.
InProc.
of the 25th Annual Meeting of ACL, pages 193?200,Stanford, CA.M.
Brent.
1991.
Automatic acquisition of subcategorizationframes from untagged text.
In Proc.
of the 29th Meeting ofACL, pages 209?214.E.
J. Briscoe and J. Carroll.
1997.
Automatic Extraction ofSubcategorization from Corpora.
In Proc.
of the 5th ANLP,Washington DC, USA.E.
J. Briscoe and J. Carroll.
2002.
Robust accurate statisticalannotation of general text.
In Proc.
of the 3rd LREC, pages1499?1504, Las Palmas, Canary Islands, May.E.
J. Briscoe, J. Carroll, and R. Watson.
2006.
The secondrelease of the rasp system.
In Proc.
of the COLING/ACL2006 Interactive Presentation Sessions, Sydney, Australia.L.
Burnard, 1995.
The BNC Users Reference Guide.
BritishNational Corpus Consortium, Oxford, May.G.
Carroll and M. Rooth.
1998.
Valence induction with a head-lexicalized pcfg.
In Proc.
of the 3rd Conference on EMNLP,Granada, Spain.J.
Carroll, G. Minnen, and E. J. Briscoe.
1998.
Can Subcat-egorisation Probabilities Help a Statistical Parser?
In Pro-ceedings of the 6th ACL/SIGDAT Workshop on Very LargeCorpora, pages 118?126, Montreal, Canada.R.
Grishman, C. Macleod, and A. Meyers.
1994.
COMLEXSyntax: Building a Computational Lexicon.
In COLING,Kyoto.A.
Korhonen and Y. Krymolowski.
2002.
On the Robustnessof Entropy-Based Similarity Measures in Evaluation of Sub-categorization Acquisition Systems.
In Proc.
of the SixthCoNLL, pages 91?97, Taipei, Taiwan.A.
Korhonen, Y. Krymolowski, and Z. Marx.
2003.
ClusteringPolysemic Subcategorization Frame Distributions Semanti-cally.
In Proc.
of the 41st Annual Meeting of ACL, pages64?71, Sapporo, Japan.A.
Korhonen, Y. Krymolowski, and E. J. Briscoe.
2006.
Alarge subcategorization lexicon for natural language process-ing applications.
In Proc.
of the 5th LREC, Genova, Italy.A.
Korhonen.
2002.
Subcategorization acquisition.
Ph.D. the-sis, University of Cambridge Computer Laboratory.C.
Macleod, A. Meyers, R. Grishman, L. Barrett, and R. Reeves.1997.
Designing a dictionary of derived nominals.
In Proc.of RANLP, Tzigov Chark, Bulgaria.C.
Manning.
1993.
Automatic Acquisition of a Large Subcat-egorization Dictionary from Corpora.
In Proc.
of the 31stMeeting of ACL, pages 235?242.S.
Schulte im Walde and C. Brew.
2002.
Inducing german se-mantic verb classes from purely syntactic subcategorisationinformation.
In Proc.
of the 40th Annual Meeting of ACL,Philadephia, USA.M.
Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003.Using predicate-argument structures for information extrac-tion.
In Proc.
of the 41st Annual Meeting of ACL, Sapporo.J.
Yallop, A. Korhonen, and E. J. Briscoe.
2005.
Auto-matic acquisition of adjectival subcategorization from cor-pora.
In Proc.
of the 43rd Annual Meeting of the Associationfor Computational Linguistics, pages 614?621, Ann Arbor,Michigan.919
