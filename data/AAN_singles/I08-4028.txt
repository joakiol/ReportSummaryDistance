An Improved CRF based Chinese Language Processing System for SIGHANBakeoff 2007Xihong Wu, Xiaojun Lin, Xinhao Wang, Chunyao Wu, Yaozhong Zhang and Dianhai YuSpeech and Hearing Research CenterState Key Laboratory of Machine Perception,Peking University, China, 100871{wxh,linxj,wangxh,wucy,zhangyaoz,yudh}@cis.pku.edu.cnAbstractThis paper describes three systems: theChinese word segmentation (WS) system,the named entity recognition (NER) sys-tem and the Part-of-Speech tagging (POS)system, which are submitted to the FourthInternational Chinese Language ProcessingBakeoff.
Here, Conditional Random Fields(CRFs) are employed as the primary mod-els.
For the WS and NER tracks, the n-gram language model is incorporated in ourCRFs based systems in order to take into ac-count the higher level language information.Furthermore, to improve the performancesof our submitted systems, a transformation-based learning (TBL) technique is adoptedfor post-processing.1 IntroductionAmong 24 closed and open tracks in this bakeoff, weparticipated in 23 tracks, except the open NER trackof MSRA.
Our systems are ranked 1st in 6 tracks,and get close to the top level in several other tracks.Recently, Maximum Entropy model(ME) andCRFs (Low et al, 2005)(Tseng et al, 2005) (HaiZhao et al, 2006) turned out to be promising in natu-ral language processing tracks, and obtain excellentperformances on most of the test corpora of Bake-off 2005 and Bakeoff 2006.
Compared to the gen-erative models, like HMM, the primary advantageof CRFs is that it relaxes the independence assump-tions, which makes it able to handle multiple inter-acting features between observation elements (Wal-lach et al, 2004).However, the ME and CRFs emphasize the rela-tion of the basic units of sequence, like the Chinesecharacters in these tracks.
While, the higher levelinformation, like the relationship of the words is ig-nored.
From this point of view, the n-gram languagemodel is incorporated in our CRFs based systems inorder to cover the word level language information.Based on several pilot-experimental results, wefound that the tagging errors always follow somepatterns.
In order to find those error patterns and cor-rect the similar errors, we integrated the TBL post-processor in our systems.
In addition, extra train-ing data, which is transformed from People DailyCorpus (Shiwen Yu et al, 2000) with some auto-extracted transition rules, is used in each corpus forthe open tracks of WS.The remainder of this paper is organized as fol-lows.
The scheme of our three developed systemsare described in section 2, 3 and 4, respectively.
Insection 5, evaluation results based on these systemsare enumerated and discussed.
Finally some conclu-sions are drawn in section 6.2 Word SegmentationThe WS system mainly consists of three compo-nents, CRFs, n-gram language model and post-processing strategies.2.1 Conditional Random FieldsConditional Random Fields, as the statistical se-quence labeling models, achieve great success innatural language processing, such as chunking (FeiSha et al, 2003) and word segmentation (Hai Zhaoet al, 2006).
Different from traditional generative155Sixth SIGHAN Workshop on Chinese Language Processingmodel, CRFs relax the constraint of the indepen-dence assumptions, and therefore turn out to be moresuitable for natural language tasks.CRFs model the conditional distribution p(Y |X)of the labels Y given the observations X directlywith the formulation:P?
(Y |X) = 1Z(X)exp{?c?C?k?kfk(Yc, X, c)}(1)Y is the label sequence, X is the observation se-quence, Z(X) is a normalization term, fk is a fea-ture function, and c is the set of cliques in Graphic.In our tasks, C = {(yi?1, yi)}, X is the Chinesecharacter sequence of a sentence.To label a Chinese character, we need to definethe label tags.
Here we have six types of tags ac-cording to character position in a word (Hai Zhao etal., 2006):tag = {B1, B2, B3, I, E, S}?B1, B2, B3, I, E?
represent the first, second, third,continue, and end character positions in a multi-character word, and ?S?
is the single-character wordtag.The unigram feature templates used here are:Cn (n = ?2,?1, 0, 1, 2)CnCn+1 (n = ?2,?1, 0)CnCn+1Cn+2 (n = ?1)Where C0 refers to the current character andC?n(Cn) is the nth character to the left(right) of thecurrent character.
We also use the basic bigram fea-ture template which denotes the dependency on theprevious tag and current tag.2.2 Multi-Model IntegrationIn order to integrate multi-model information, weuse a log-linear model(Och et al, 2002) to computethe posterior probability:Pr (W |C) = p?M1 (W |C)= exp[?Mm=1 ?mhm(W,C)]?W ?
exp[?Mm=1 ?mhm(W ?, C)](2)Where W is the word sequence, and C is the char-acter sequence.
The decision rule here is:W0 = argmaxW {Pr(W |C)}= argmaxW {M?m=1?mhm(W,C)} (3)The parameters ?M1 of this model can be opti-mized by standard approaches, such as the Mini-mum Error Rate Training used in machine transla-tion (Och, 2003).
In fact, the CRFs approach isa special case of this framework when we defineM = 1 and use the following feature function:h1(W,C) = logP?
(Y |X) (4)In our approach, the logarithms of the scores gen-erated by the two kinds of models are used as featurefunctions:h1(W,C) = logPcrf (W,C)= log?wiP?
(wi|C) (5)h2(W,C) = logPlm(W ) (6)The first feature function(Eq.5) comes from CRFs.Instead of computing the score of the whole la-bel sequence Y with character sequence X throughP?
(Y |X) directly, we try to get the posterior prob-ability of a sub-sequence to be tagged as one wholeword P?(wi|C).
Then we combine all the score ofwords together.
The second feature function(Eq.6)comes from n-gram language model, which aims tocatch the words information.The log-linear model with the feature functionsdescribed above allows the dynamic programmingsearch algorithm for efficient decoding.
The systemgenerates the word lattice with posterior probabilityP?(wi|C).
Then the best word sequence is searchedon the word lattice with the decision rule(Eq.3).Since arbitrary sub-sequence can be viewed as acandidate word in word lattice, we need to deal withthe problem of OOV words.
The unigram of an OOVword is estimated as:Unigram(OOV Word) = pl (7)where p is the minimal value of unigram scores inthe language model; l is the length of the OOVword, which is used as a punishment factor toavoid overemphasizing the long OOV words (Xin-hao Wang et al, 2006).2.3 Post-Processing StrategiesThe division and combination rule, which has beenproved to be useful in our system of Bakeoff 2006(Xinhao Wang et al, 2006), is adopted for the post-processing in the system.156Sixth SIGHAN Workshop on Chinese Language Processing2.4 Training Data TransitionFor the WS open tracks, the unique difference fromclosed tracks is that the additional training data issupplemented for model refinement.For the Simplified Chinese tracks, the additionaltraining data are collected from People Daily Cor-pus with a set of auto-extracted transition rules.
Thisprocess is performed in a heuristic strategy and con-tains five steps as follows:(1) Segment the raw People Daily texts with the cor-responding system for the closed track of each cor-pus.
(2) Compare the result of step 1 with People DailyCorpus to get the conflict pairs.
For example,{pair1: ???
vs.
???
}(Zhemin Jiang){pair2: ???
vs.
???
}(catch with two hands)In each pair, the left phrase follows the People DailyCorpus segmentation guideline, while the right oneis the phrase obtained from step 1.
(3) Divide the pairs into two sets: the first set con-tains the pairs with right phrase appearing in the tar-get training data; the other pairs are in the secondset.
(4) Select sentences which contain the left phrase ofthe pairs in the second set from People Daily Cor-pus.
(5) Transform these selected sentences by replacingtheir phrase in the left side of the pair in the first setto the right one.
This is used as our transition rules.3 Named Entity RecognitionThe named entity recognition track is viewed as acharacter sequence tagging problem in our NER sys-tem and the log-linear model mentioned above isemployed again to integrate multi-model informa-tion.
To find the error patterns and correct them,a TBL strategy is then used in the post-processingmodule.3.1 Model DescriptionIn this NER track, we employe the log-linear modeland use the logarithms of the scores generated by thetwo types of models as feature functions.
BesidesCRFs, another model is the class-based n-gram lan-guage model:h1(Y, X) = logPcrf (Y, X)= logP?
(Y |X) (8)h2(Y, X) = logPclm(Y, X) (9)Y is the label sequence and X is the character se-quence.CRFs are used to generate the N-best tagging re-sults with the scores of whole label sequence Y oncharacter sequence X by P?
(Y |X).
And then, thelog-linear model is used to reorder the N-best tag-ging results by integrating the CRFs score and theclass-based n-gram language model score together.CRFsIn this track, one Chinese character is labeled bya tag of ten classes, which denoting the beginning,continue, ending character of a specified named en-tity or a non-entity character.
There are three typesof named entities in these tracks, including personname, location name and organization name.In CRFs, the basic features used here are:Cn (n = ?2,?1, 0, 1, 2)CnCn+1 (n = ?2,?1, 0, 1)CnCn+2 (n = ?1)Besides basic unigram features, the bigram transi-tion features considering the previous tag is adoptedwith template Cn (n = ?2,?1, 0, 1, 2).Class-Based N-gram Language ModelFor the class-based n-gram language model, wedefine that each character is a single class, whileeach type of named entity is viewed as a single class.With the character sequence and label sequence, theclass sequence can be generated.
Take this sentencefor instance:???????????
(But Ibrahimov is not satisfied)Table 1 shows its class sequence.
Class-based n-gram language model can be trained with class se-quence.3.2 TBLSince the analysis on our experiments shows that thetagging errors always follow some patterns in NERtrack, TBL strategy is adopted in our system to findthese patterns and correct the similar errors.157Sixth SIGHAN Workshop on Chinese Language Processingcharacter sequence ?
?
?
?
?
?
?
?
?
?
?label sequence N Per-B Per-C Per-C Per-C Per-C Per-E N N N Nclass sequence ?
PERSON ?
?
?
?Table 1: A class sequence exampleTransformation-based learning is a symbolic ma-chine learning method, introduced by (Eric Brill,1995).
The main idea in TBL is to generate a set oftransformation rules that can correct tagging errorsproduced by the initial process.There are four main procedures in our TBLframework: An initial state assignment which is op-erated by the system we described above; a set of al-lowable templates for rules, ranging from words ina 3 positions windows and name entity informationin a 3-word window with their combinations consid-ered, and rules which are learned according to thetagging differences between training data and resultsgenerated by our system, at last, those rules are in-troduced to correct similar errors.4 POS TaggingThe POS tagging track is to assign the part-of-speech sequence for the correctly segmented wordsequence.
In our system, for the CTB corpus, theCRFs are adopted; however for the other four cor-pora, considering the limitations of resources andtime, the ME model is adopted.
To improve the per-formance of ME model, the POS tag of the previousword is taken as a feature and the dynamic program-ming strategy is used in decoding.In the closed track, the features include the basicfeatures and their combined features.
Firstly the pre-vious and next words of the current word are takenas the basic features.
Secondly, based on the anal-ysis of the OOV words, the first and last charactersof the current word, as well as the length of the cur-rent word are proven to be effective features for theOOV POS.
Furthermore since the long distance con-straint word may impact the POS of current word(Yan Zhao et al, 2006), in the open track, a Chi-nese parser is imported and the word depended onthe current word is extracted as feature.5 Experiments and ResultsWe have participated in 23 tracks, except the openNER track of MSRA.
CRFs, ME model and n-gramlanguage model are adopted in these systems.
Ourimplementation uses the CRF++ package1 providedby Taku Kudo, the Maximum Entropy Toolkit2 pro-vided by Zhang Le, and the SRILM Toolkit providedby Andreas Stolcke (Andreas Stolcke et al, 2002).5.1 Chinese Word SegmentationIn the closed tracks, CRFs and bigram languagemodel are trained on the given training data for eachcorpus.
In order to integrate these two models, it isnecessary to train the corresponding parameter ?M1with Minimum Error Rate Training approache basedon a development data.
Since the development datais not provided in this bakeoff, a ten-fold cross val-idation approach is employed to implement the pa-rameter training.
A set of parameters can be trainedindependently, and then the mean value is calculatedas the estimation of each parameter.Table 2 gives the results of our WS system forclosed tracks.baseline +LM +LM+PostCTB 94.7 94.7 94.8NCC 92.6 92.4 92.9SXU 94.7 95.7 95.8CITYU 92.9 93.7 93.9CKIP 93.2 93.7 93.7Table 2: Word segmentation performance on F-value with different approach for the closed tracksIn the open tracks, as we do not have enough timeto finish the parameter estimation on the new data,our system adopt the same parameters ?M1 used inclosed tracks.
The unique difference from closed1http://chasen.org/taku/software/CRF++2http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html158Sixth SIGHAN Workshop on Chinese Language Processingtracks is that extra training data is added for eachcorpus to improve the performance.
For the Sim-plified Chinese tracks, additional data comes fromPeople Daily Corpus which is transformed by ourtransition strategy.
At the same time, for the Tra-ditional Chinese tracks, additional data comes fromthe training and testing data used in the early Bake-off.
However, we implement two systems for theCTB open track.
The system (a) takes the trainingand testing data used in the early Bakeoff as addi-tional data, and System (b) takes the translated Peo-ple Daily Corpus as additional data.
Table 3 givesthe results of our open WS system.baseline +LM +LM+PostCTB(a) 99.2 99.2 99.3CTB(b) 95.6 95.1 97.0NCC 93.7 93.0 92.9SXU 96.4 87.0 95.8CITYU 95.8 90.6 91.0CKIP 94.5 94.8 95.1Table 3: Word segmentation performance on F-value with different approach for the open tracksThe result shows that the system performance issensitive to the parameters ?M1 .
Although we trainthe useful parameter for closed tracks, it plays a badrole in open tracks as we do not adapt it for the ad-ditional training data.5.2 Named Entity RecognitionIn the closed NER tracks, CRFs and class-based tri-gram language model are trained on the given train-ing data for each corpus.
The same approach em-ployed in the WS tracks is adopted to train the corre-sponding parameter ?M1 in our NER systems.
Mean-while, the TBL rules trained via five-fold cross val-idation approach are also used in post-processingprocedure.
Table 4 reports the results of our closedNER system.5.3 POS TaggingThe experiments show that the CRFs/ME method issuperior to the TBL method, and the concurrent er-rors for these two methods are less than 60%.
There-fore we adopted TBL to correct the output resultsof CRFs/ME: If the output tags of CRFs/ME andbaseline +LM +LM+PostMSRA 89.3 89.7 89.9CITYU 79.3 80.6 80.5Table 4: Named entity recognition F-value throughdifferent approaches for the closed tracksTBL are not consistent and the output probabilityof CRFs/ME is below a certain threshold, the TBLresults are fixed.
Here the 90% of the training setis taken as the training data and remained 10% isseparated as the development data to get the thresh-old, which is 0.60 for the CRFs, and 0.90 for theME.
In addition, the POS tagged corpus of the Chi-nese Treebank 5.0 from LDC is added to the trainingdata for CTB open track.
In our system, the Berke-ley Parser (Slav Petrov et al, 2006) is adopted toobtain the long distance constraint words.
The per-formance achieved by the methods described aboveon each corpus are reported in Table 5.CRFs/ME CRFs/MECRFs/ME TBL +TBL +TBL+SyntaxCTIYU 88.7 87.7 89.1 89.0CKIP 91.8 91.4 92.2 92.1CTB 94.0 92.7 94.3 96.5NCC 94.6 94.3 94.9 95.0PKU 93.5 93.2 94.0 94.1Table 5: POS tagging performance on total-accuracywith different approach6 ConclusionIn this paper, we have briefly described our systemsparticipating in the Bakeoff 2007.
In the WS andNER systems, the log-linear model is adopted to in-tegrate CRFs and language model, which improvesthe system performances effectively.
At the sametime, system integration approach used in the POSsystem also proves its validity.
In addition, a heuris-tic strategy is imported to generate additional train-ing data for the open WS tracks.
Finally, severalpost-processing strategies are used to further im-prove our systems.159Sixth SIGHAN Workshop on Chinese Language ProcessingReferencesJin Kiat Low, Hwee Tou Ng and Wenyuan Guo.
2005.A Maximum Entropy Approach to Chinese Word Seg-mentation.
Proceedings of the Fourth SIGHAN Work-shop on Chinese Language Processing.
pp.
161-164.Jeju Island, Korea.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, Christopher Manning.
2005.
A ConditionalRandom Field Word Segmenter for Sighan Bakeoff2005.
Proceedings of the Fourth SIGHAN Workshopon Chinese Language Processing.
pp.
168-171.
JejuIsland, Korea.Hai Zhao, Chang-Ning Huang and Mu Li.
2006.
AnImproved Chinese Word Segmentation System withConditional Random Field.
Proceedings of the FifthSIGHAN Workshop on Chinese Language Processing.pp.
162-165.
Sydney, Australia.Hanna M. Wallach.
2004.
Conditional Random Fields:An Introduction.
Technical Report, UPenn CIS TRMS-CIS-04-21.Shiwen Yu, Xuefeng Zhu and Huiming Duan.
2000.Specification of large-scale modern Chinese corpus.Proceedings of ICMLP?2001.
pp.
18-24.
Urumqi,China.Fei Sha and Fernando Pereira.
2003.
Shallow Parsingwith Conditional Random Fields.
Proceedings of Hu-man Language Technology/NAACL.
pp.
213-220.
Ed-monton, Canada.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for sta-tistical machine translation.
Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL).
pp.
295-302.
Philadelphia, PA.Franz Josef Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
Proceedings ofthe 41th Annual Meeting of the Association for Com-putational Linguistics (ACL).
pp.
160-167.
Sapporo,Japan.Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, Xi-hong Wu.
2006.
Chinese Word Segmentation withMaximum Entropy and N-gram Language Model.
theFifth SIGHAN Workshop on Chinese Language Pro-cessing.
pp.
138-141.
Sydney, Australia.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a case studyin Part-of-Speech tagging.
Computational Lingusitics.21(4).Yan Zhao, Xiaolong Wang, Bingquan Liu, and Yi Guan.2006.
Fusion of Clustering Trigger-Pair Features forPOS Tagging Based on Maximum Entropy Model.Journal of Computer Research and Development.43(2).
pp.
268-274.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
Proceedings of InternationalConference on Spoken Language Processing.
pp.
901-904.
Denver, Colorado.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein.
2006.
Learning Accurate, Compact, and Inter-pretable Tree Annotation.
Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th annual meeting of the ACL.
pp.
433-440.Sydney, Australia.160Sixth SIGHAN Workshop on Chinese Language Processing
