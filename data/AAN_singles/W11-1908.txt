Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 61?65,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsMulti-metric optimization for coreference: The UniTN / IITP / Essexsubmission to the 2011 CONLL Shared TaskOlga Uryupina?
Sriparna Saha?
Asif Ekbal?
Massimo Poesio??
?University of Trento?Indian Institute of Technology Patna?
University of Essexuryupina@gmail.com, sriparna@iitp.ac.in,asif@iitp.ac.in, massimo.poesio@unitn.itAbstractBecause there is no generally accepted met-ric for measuring the performance of anaphoraresolution systems, a combination of met-rics was proposed to evaluate submissions tothe 2011 CONLL Shared Task (Pradhan etal., 2011).
We investigate therefore Multi-objective function Optimization (MOO) tech-niques based on Genetic Algorithms to opti-mize models according to multiple metrics si-multaneously.1 IntroductionMany evaluation metrics have been proposed foranaphora resolution (Vilain et al, 1995; Bagga andBaldwin, 1998; Doddington et al, 2000; Luo, 2005;Recasens and Hovy, 2011).
Each of these metricsseems to capture some genuine intuition about thethe task, so that, unlike in other areas of HLT, nonehas really taken over.
This makes it difficult to com-pare systems, as dramatically demonstrated by theresults of the Coreference Task at SEMEVAL 2010(Recasens et al, 2010).
It was therefore wise of theCONLL organizers to use a basket of metrics to as-sess performance instead of a single one.This situation suggests using methods to opti-mize systems according to more than one metricat once.
And as it happens, techniques for doingjust that have been developed in the area of Ge-netic Algorithms?so-called multi-objective opti-mization techniques (MOO) (Deb, 2001).
The keyidea of our submission is to use MOO techniquesto optimize our anaphora resolution system accord-ing to three metrics simultaneously: the MUC scorer(a member of what one might call the ?link-based?cluster of metrics) and the two CEAF metrics (rep-resentative of the ?entity-based?
cluster).
In a pre-vious study (Saha et al, 2011), we show that ourMOO-based approach yields more robust results thansingle-objective optimization.We test two types of optimization: feature se-lection and architecture?whether to learn a singlemodel for all types of anaphors, or to learn sepa-rate models for pronouns and for other nominals.We also discuss how the default mention extractiontechniques of the system we used for this submis-sion, BART (Versley et al, 2008), were modified tohandle the all-mention annotation in the OntoNotescorpus.In this paper, we first briefly provide some back-ground on optimization for anaphora resolution, ongenetic algorithms, and on the method for multi-objective optimization we used, Non-DominatedSorting Genetic Algorithm II (Deb et al, 2002).
Af-ter that we discuss our experiments, and present ourresults.2 Background2.1 Optimization for Anaphora ResolutionThere have only been few attempts at optimizationfor anaphora resolution, and with a few exceptions,this was done by hand.The first systematic attempt at automatic opti-mization of anaphora resolution we are aware of wascarried out by Hoste (2005), who used genetic algo-rithms for automatic optimization of both feature se-lection and of learning parameters, also considering61two different machine learners, TimBL and Ripper.Her results suggest that such techniques yield im-provements on the MUC-6/7 datasets.
Recasens andHovy (2009) carried out an investigation of featureselection for Spanish using the ANCORA corpus.A form of multi-objective optimization was ap-plied to coreference by Munson et al (2005).
Mun-son et al (2005) did not propose to train models soas to simultaneously optimize according to multi-ple metrics; instead, they used ensemble selection tolearn to choose among previously trained models thebest model for each example.
Their general conclu-sion was negative, stating that ?ensemble selectionseems too unreliable for use in NLP?, but they didsee some improvements for coreference.2.2 Genetic AlgorithmsGenetic algorithms (GAs) (Goldberg, 1989) are ran-domized search and optimization techniques guidedby the principles of evolution and natural genetics.In GAs the parameters of the search space are en-coded in the form of strings called chromosomes.
Acollection of such strings is called a population.
Anobjective or fitness function is associated with eachchromosome that represents the degree of goodnessof that chromosome.
A few of the chromosomes areselected on the basis of the principle of survival ofthe fittest, and assigned a number of copies that gointo the mating pool.
Biologically inspired opera-tors like crossover and mutation are applied on thesechromosomes to yield a new generation of strings.The processes of selection, crossover and mutationcontinues for a fixed number of generations or till atermination condition is satisfied.2.3 Multi-objective OptimizationMulti-objective optimization (MOO) can be formallystated as follows (Deb, 2001).
Find the vectorsx?
= [x?1, x?2, .
.
.
, x?n]T of decision variables that si-multaneously optimize the M objective values{f1(x), f2(x), .
.
.
, fM (x)}while satisfying the constraints, if any.An important concept in MOO is that of dom-ination.
In the context of a maximization prob-lem, a solution xi is said to dominate xj if?k ?
1, 2, .
.
.
,M, fk(xi) ?
fk(xj) and ?k ?1, 2, .
.
.
,M, such that fk(xi) > fk(xj).Genetic algorithms are known to be more effec-tive for solving MOO than classical methods such asweighted metrics, goal programming (Deb, 2001),because of their population-based nature.
A particu-larly popular genetic algorithm of this type is NSGA-II (Deb et al, 2002), which we used for our runs.3 Using MOO for Optimization inAnaphora ResolutionWe used multi-objective optimization techniques forfeature selection and for identifying the optimal ar-chitecture for the CONLL data.
In this section webriefly discuss each aspect of the methodology.3.1 The BART SystemFor our experiments, we use BART (Versley et al,2008), a modular toolkit for anaphora resolution thatsupports state-of-the-art statistical approaches to thetask and enables efficient feature engineering.
BARTcomes with a set of already implemented features,along with the possibility to design new ones.
Italso implements different models of anaphora reso-lution, allowing the choice between single and splitclassifiers that we explore in our runs, as well asbetween mention-pair and entity-mention, and be-tween best-first and ranking.
It also has interfacesto different machine learners (MaxEnt, SVM, de-cision trees).
It is thus ideally suited for experi-menting with feature selection and other aspects ofoptimization.
However, considering all the param-eters, it was unfeasible to run an optimization onthe amount of data available on CONLL; we fo-cused therefore on feature selection and the choicebetween single and split classifiers.
We considered42 features, including 7 classifying mention type, 8for string matching of different subparts and differ-ent levels of exactness, 2 for aliasing, 4 for agree-ment, 12 for syntactic information including alsobinding constraints, 3 encoding salience, 1 encod-ing patterns extracted from the Web, 3 for proximity,and 2 for 1st and 2nd person pronouns.
Again be-cause of time considerations, we used decision treesas implemented in Weka as our classification modelinstead of maximum-entropy or SVMs.
Finally, weused a simple mention-pair model without rankingas in (Soon et al, 2001).623.2 Mention detectionBART supports several solutions to the mentiondetection (MD) task.
The users can input pre-computed mentions, thus, experimenting with goldboundaries or system boundaries computed by ex-ternal modules (e.g., CARAFE).
BART also hasa built-in mention extraction module, computingboundaries heuristically from the output of a parser.For the CoNLL shared task, we use the BARTinternal MD module, as it corresponds better tothe mention detection guidelines of the OntoNotesdataset.
We have further adjusted this module to im-prove the MD accuracy.
The process of mention de-tection involves two steps.First, we create a list of candidate mentions bymerging basic NP chunks with named entities.
NPchunks are computed from the parse trees providedin the CoNLL distribution, Named entities are ex-tracted with the Stanford NER tool (Finkel et al,2005).
For each candidate mention, we store it mini-mal and maximal span.
The former is used for com-puting feature values (e.g., for string matching); itcorresponds to either the basic NP chunk or the NE,depending on the mention type.
The latter is usedfor alignment with CoNLL mentions; it is computedby climbing up the parse tree.This procedure, combined with the perfect (gold)coreference resolution, gives us an F-score of91.56% for the mention detection task on theCoNLL development set1.At the second step, we aim at discarding men-tions that are unlikely to participate in corefer-ence chains.
We have identified several groups ofsuch mentions: erroneous (?[uh]?
), (parts of) multi-word expressions (?for [example]?
), web addresses,emails (?[http://conll.bbn.com]?
), time/date expres-sions (?two times [a year]?
), non-referring pronouns(?[there]?,?[nobody]?
), pronouns that are unlikelyto participate in a chain (?
[somebody]?, ?[that]?
),time/date expressions that are unlikely to participatein a chain (?
[this time]?
), and expletive ?it?.Our experiments on the development data showthat the first five groups can be reliably identifiedand safely discarded from the processing: even with1Note that, due to the fact that OntoNotes guidelines excludesingleton mentions, it is impossible to evaluate the MD compo-nent independently from coreference resolution.the perfect resolution, we observe virtually no per-formance loss (the F-score for our MD module withthe gold coreference resolution remains at 91.45%once we discard mentions from groups 1-5).The remaining groups are more problematic:when we eliminate such mentions, we see perfor-mance drops with the gold resolution.
The exact im-pact of discarding those mentions can only be as-sessed once we have trained the classifier.In practice, we have performed our optimizationexperiments, selected the best classifier and thenhave done additional runs to fine-tune the mentiondetection module.3.3 Using NSGA-IIChromosome Representation of Feature and Ar-chitecture Parameters We used chromosomes oflength 43, each binary gene encoding whether or notto use a particular feature in constructing the classi-fier, plus one gene set to 1 to use a split classifier, 0to use a single classifier for all types of anaphors.Fitness Computation and Mutations For fitnesscomputation, the following procedure is executed.1.
Suppose there are N number of featurespresent in a particular chromosome (i.e., thereare total N number of 1?s in that chromosome).2.
Construct the coreference resolution system(i.e., BART) with only these N features.3.
This coreference system is evaluated on the de-velopment data.
The recall, precision and F-measure values of three metrics are calculated.For MOO, the objective functions corresponding toa particular chromosome are F1 = F-measureMUC(for the MUC metric), F2 = F-measure?3 (for CEAFusing the ?3 entity alignment function (Luo, 2005))and F3 = F-measure?4 (for CEAF using the ?3entity alignment function).
The objective is to:max[F1, F2, F3]: i.e., these three objective func-tions are simultaneously optimized using the searchcapability of NSGA-II.We use crowded binary tournament selection asin NSGA-II, followed by conventional crossover andmutation for the MOO based optimization.
Themost characteristic part of NSGA-II is its elitism op-eration, where the non-dominated solutions (Deb,632001) among the parent and child populations arepropagated to the next generation.
The near-Pareto-optimal strings of the last generation provide the dif-ferent solutions to the feature selection problem.Genetic Algorithms Parameters Using theCONLL development set, we set the following pa-rameter values for MOO (i.e., NSGA-II): populationsize=20, number of generations=20, probability ofmutation=0.1 and probability of crossover=0.9.3.4 Running the OptimizationConsidering the size of the OntoNotes corpus, itwould be very time-consuming to run an optimiza-tion experiment on the whole dataset.
We havetherefore split the data into 3 sub-samples and per-formed separate MOO experiments on each one.The MOO approach provides a set of non-dominated solutions on the final Pareto optimalfront.
All the solutions are equally important fromthe algorithmic point of view.
We have collected setsof chromosomes for each sub-sample and evaluatedthem on the whole train/development set, pickingthe solution with the highest FINAL2 score for ourCoNLL submission.4 Results4.1 Development setTable 1 compares the performance level obtainedusing all the features with that of loose re-implementations of the systems proposed by Soonet al (2001) and Ng and Cardie (2002), commonlyused as baselines.
Our reimplementation of the Ng& Cardie model uses only a subset of features.The results in Table 1 show that our system witha rich feature set does not outperform simpler base-lines (and, in fact, yields poorer results).
A similartrend has been observed by Ng and Cardie (2002),where the improvement was only possible after man-ual feature selection.The last line of Table 1 shows the performancelevel of the best chromosome found through theMOO technique.
As it can be seen, it outperformsall the baselines according to all the measures, lead-ing to an improvement of 2-5 percentage points inthe FINAL score.2The FINAL score is an average of FMUC , FB3 andFCEAF E .This suggests that automatic feature selection isessential to improve performance ?
i.e., that an effi-cient coreference resolution system should combinerich linguistic feature sets with automatic feature se-lection mechanisms.4.2 Test setWe have re-trained our best solution on the com-bined train and development set, running it on thetest data.
This system has showed the following per-formance in the official evaluation (open track): theFINAL score of 54.32, FMUC = 57.53%, FB3 =65.18%, FCEAFE = 40.16%.5 ConclusionOur results on the development set suggest that alinguistically-rich system for coreference resolutionmight benefit a lot from feature selection.
In partic-ular, we have investigated Non-Dominated SortingGenetic Algorithm II (Deb et al, 2002) for multi-objective optimization.In subsequent work, we plan to expand the opti-mization technique to consider also learning param-eters optimization, classifier selection, and learningmodel selection.AcknowledgmentsThis work was in part supported by the Provinciadi Trento Grande Progetto LiveMemories, in part byan Erasmus Mundus scholarship for Asif Ekbal andSriparna Saha.64Features FMUC FCEAFE FB3 FINALfollowing Soon et al (2001) 54.12 41.08 66.67 53.42-*-, with splitting 53.81 41.03 66.70 53.31following Ng & Cardie (2002) 52.97 42.40 66.18 53.31-*-, with splitting 53.28 40.46 66.03 52.72All features 50.18 38.54 63.79 50.33-*-, with splitting 50.19 39.47 65.38 51.16Optimized feature set (splitting) 57.05 42.61 67.46 55.15Table 1: Performance on the development setReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In Proc.
of the LREC workshop onLinguistic Coreference, pages 563?566, Granada.Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, andT.
Meyarivan.
2002.
A fast and elitist multiobjectivegenetic algorithm: NSGA-II.
IEEE Transactions onEvolutionary Computation, 6(2):181?197.Kalyanmoy Deb.
2001.
Multi-objective OptimizationUsing Evolutionary Algorithms.
John Wiley and Sons,Ltd, England.G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,S.
Strassell, and R. Weischedel.
2000.
The auto-matic content extraction (ACE) program?tasks, data,and evaluation.
In Proc.
of LREC.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages363?370.D.
E. Goldberg.
1989.
Genetic Algorithms in Search,Optimization and Machine Learning.
Addison-Wesley, New York.Veronique Hoste.
2005.
Optimization Issues in Ma-chine Learning of Coreference Resolution.
Ph.D. the-sis, Antwerp University.X.
Luo.
2005.
On coreference resolution performancemetrics.
In Proc.
NAACL / EMNLP, Vancouver.Art Munson, Claire Cardie, and Rich Caruana.
2005.Optimizing to arbitrary NLP metrics using ensem-ble selection.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP), pages 539?546.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting on Associationfor C omputational Linguistics, pages 104?111.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
Conll-2011 shared task: Modeling unrestrictedcoreference in ontonotes.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2011), Portland, Oregon,June.M.
Recasens and E. Hovy.
2009.
A deeper look into fea-tures for coreference resolution.
In S. Lalitha Devi,A.
Branco, and R. Mitkov, editors, Anaphora Pro-cessing and Applications (DAARC 2009, number 5847in LNAI, pages 29?42, Berlin / Heidelberg.
Springer-Verlag.M.
Recasens and E. Hovy.
2011.
Blanc: Implement-ing the rand index for coreference evaluation.
NaturalLanguage Engineering.M.
Recasens, L. Ma`rquez, E. Sapena, M. A.
Mart??,M.
Taule?, V. Hoste, M. Poesio, and Y. Versley.
2010.Semeval-2010 task 1: Coreference resolution in multi-ple languages.
In Proc.
SEMEVAL 2010, Uppsala.Sriparna Saha, Massimo Poesio, Asif Ekbal, and OlgaUryupina.
2011.
Single and multi-objective optimiza-tion for feature selection in anaphora resolution.
Sub-mitted.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistic, 27(4):521?544.Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-sio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.BART: a modular toolkit for coreference resolution.
InProceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics on Human Lan-guage Technologies, pages 9?12.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In Proc.
of the Sixth Message Under-standing Conference, pages 45?52.65
