Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 185?192, Vancouver, October 2005. c?2005 Association for Computational LinguisticsAlignment Link Projection Using Transformation-Based LearningNecip Fazil Ayan, Bonnie J. Dorr and Christof MonzDepartment of Computer ScienceUniversity of MarylandCollege Park, MD 20742{nfa,bonnie,christof}@umiacs.umd.eduAbstractWe present a new word-alignment ap-proach that learns errors made by ex-isting word alignment systems and cor-rects them.
By adapting transformation-based learning to the problem of wordalignment, we project new alignment linksfrom already existing links, using featuressuch as POS tags.
We show that our align-ment link projection approach yields a sig-nificantly lower alignment error rate thanthat of the best performing alignment sys-tem (22.6% relative reduction on English-Spanish data and 23.2% relative reductionon English-Chinese data).1 IntroductionWord-level alignment is a critical component of awide range of NLP applications, such as construc-tion of bilingual lexicons (Melamed, 2000), wordsense disambiguation (Diab and Resnik, 2002), pro-jection of language resources (Yarowsky et al,2001), and statistical machine translation.
Althoughword-level aligners tend to perform well when thereis enough training data, the quality of word align-ment decreases as the size of training data de-creases.
Moreover, word-alignment systems are of-ten tripped up by many-to-many correspondences,morphological language distinctions, paraphrasedand free translations, and a high percentage of func-tion words (about 50% of the tokens in most texts).At the heart of the matter is a set of assumptionsthat word-alignment algorithms must make in orderto reduce the hypothesis space, since word align-ment is an exponential problem.
Because of theseassumptions, learning algorithms tend to make sim-ilar errors throughout the entire data.This paper presents a new approach?AlignmentLink Projection (ALP)?that learns common align-ment errors made by an alignment system and at-tempts to correct them.
Our approach assumes theinitial alignment system adequately captures certainkinds of word correspondences but fails to handleothers.
ALP starts with an initial alignment and thenfills out (i.e., projects) new word-level alignment re-lations (i.e., links) from existing alignment relations.ALP then deletes certain alignment links associatedwith common errors, thus improving precision andrecall.In our approach, we adapt transformation-basedlearning (TBL) (Brill, 1995; Brill, 1996) to the prob-lem of word alignment.
ALP attempts to find anordered list of transformation rules (within a pre-specified search space) to improve a baseline anno-tation.
The rules decompose the search space intoa set of consecutive words (windows) within whichalignment links are added, to or deleted from, theinitial alignment.
This window-based approach ex-ploits the clustering tendency of alignment links,i.e., when there is a link between two words, thereis frequently another link in close proximity.TBL is an appropriate choice for this problem forthe following reasons:1.
It can be optimized directly with respect to anevaluation metric.2.
It learns rules that improve the initial predic-tion iteratively, so that it is capable of correct-ing previous errors in subsequent iterations.3.
It provides a readable description (or classifi-cation) of errors made by the initial system,thereby enabling alignment refinements.185The rest of the paper is organized as follows: Inthe next section we describe previous work on im-proving word alignments.
Section 3 presents a briefoverview of TBL.
Section 4 describes the adapta-tion of TBL to the word alignment problem.
Sec-tion 5 compares ALP to various alignments andpresents results on English-Spanish and English-Chinese.
We show that ALP yields a significant re-ductions in alignment error rate over that of the bestperforming alignment system.2 Related WorkOne of the major problems with the IBM models(Brown et al, 1993) and the HMM models (Vogel etal., 1996) is that they are restricted to the alignmentof each source-language word to at most one target-language word.
The standard method to overcomethis problem to use the model in both directions(interchanging the source and target languages) andapplying heuristic-based combination techniques toproduce a refined alignment (Och and Ney, 2000;Koehn et al, 2003)?henceforth referred to as ?RA.
?Several researchers have proposed algorithms forimproving word alignment systems by injecting ad-ditional knowledge or combining different align-ment models.
These approaches include an en-hanced HMM alignment model that uses part-of-speech tags (Toutanova et al, 2002), a log-linearcombination of IBM translation models and HMMmodels (Och and Ney, 2003), techniques that relyon dependency relations (Cherry and Lin, 2003),and a log-linear combination of IBM Model 3 align-ment probabilities, POS tags, and bilingual dictio-nary coverage (Liu et al, 2005).
A common themefor these methods is the use of additional featuresfor enriching the alignment process.
These methodsperform better than the IBM models and their vari-ants but still tend to make similar errors because ofthe bias in their alignment modeling.We adopt an approach that post-processes a givenalignment using linguistically-oriented rules.
Theidea is similar to that of Ayan et al (2004), wheremanually-crafted rules are used to correct align-ment links related to language divergences.
Ourapproach differs, however, in that the rules are ex-tracted automatically?not manually?by examin-ing an initial alignment and categorizing the errorsaccording to features of the words.Initial AnnotationCorpusTemplatesRule InstantiationBest Rule SelectionRule ApplicationRulesCorpusAnnotatedGround TruthFigure 1: TBL Architecture3 Transformation-based LearningAs shown in Figure 1, the input to TBL is an unanno-tated corpus that is first passed to an initial annotatorand then iteratively updated through comparison to amanually-annotated reference set (or ground truth).On each iteration, the output of the previous iterationis compared against the ground truth, and an orderedlist of transformation rules is learned that make theprevious annotated data better resemble the groundtruth.A set of rule templates determines the space ofallowable transformation rules.
A rule template hastwo components: a triggering environment (condi-tion of the rule) and a rewrite rule (action taken).
Oneach iteration, these templates are instantiated withfeatures of the constituents of the templates whenthe condition of the rule is satisfied.This process eventually identifies all possible in-stantiated forms of the templates.
Among all thesepossible rules, the transformation whose applicationresults in the best score?according to some objec-tive function?is identified.
This transformation isadded to the ordered list of transformation rules.The learning stops when there is no transformationthat improves the current state of the data or a pre-specified threshold is reached.When presented with new data, the transforma-tion rules are applied in the order that they wereadded to the list of transformations.
The output ofthe system is the annotated data after all transforma-tions are applied to the initial annotation.4 Alignment Link Projection (ALP)ALP is a TBL implementation that projects align-ment links from an initial input alignment.
We in-duce several variations of ALP by setting four pa-rameters in different ways:186eifj fj+1NULL eifj fj+1Figure 2: Graphical Representation of a Template1.
Initial alignment2.
Set of templates3.
Simple or generalized instantiation4.
Best rule selectionWe describe each of these below using the followingdefinitions and notation:?
E = e1, .
.
.
, ei, .
.
.
, et is a sentence in lan-guage L1 and F = f1, .
.
.
, fj , .
.
.
, fs is a sen-tence in language L2.?
An alignment link (i, j) corresponds to a trans-lational equivalence between ei and fj .?
A neighborhood of an alignment link (i, j)?denoted by N(i, j)?consists of 8 possiblealignment links in a 3 ?
3 window with (i, j)in the center of the window.
Each element ofN(i, j) is called a neighboring link of (i, j).?
nullEA(i) is true if and only if ei is notaligned to any word in F in a given alignmentA.
Similarly, nullFA(j) is true if and only iffj is not aligned to any word in E in a givenalignment A.4.1 Initial AlignmentAny existing word-alignment system may be usedfor the initial annotation step of the TBL algo-rithm.
For our experiments, we chose GIZA++ (Ochand Ney, 2000) and the RA approach (Koehn etal., 2003)?
the best known alignment combinationtechnique?
as our initial aligners.14.2 TBL TemplatesOur templates consider consecutive words (of size1, 2 or 3) in both languages.
The condition por-tion of a TBL rule template tests for the existenceof an alignment link between two words.
The ac-tion portion involves the addition or deletion of analignment link.
For example, the rule template inFigure 2 is applicable only when a word (ei) in onelanguage is aligned to the second word (fj+1) of aphrase (fj , fj+1) in the other language, and the first1We treat these initial aligners as black boxes.word (fj) of the phrase is unaligned in the initialalignment.
The action taken by this rule template isto add a link between ei and fj .2ALP employs 3 different sets of templates toproject new alignment links or delete existing linksin a given alignment:1.
Expansion of the initial alignment accordingto another alignment2.
Deletion of spurious alignment links3.
Correction of multi-word (one-to-many ormany-to-one) correspondencesEach of these is described below.4.2.1 Expansion TemplatesExpansion templates are used to extend an initialalignment given another alignment as the validationset.
This approach is similar to the one used in theRA method in that it adds links based on knowl-edge about neighboring links, but it differs in that italso uses features of the words themselves to decidewhich neighboring links to add.Our expansion templates are presented in Table 1.The first 8 templates add a new link to the initialalignmentA if there is a neighboring link in the vali-dation alignment V .
The final two templates enforcethe presence of at least two neighboring links in thevalidation set V before adding a new link.Condition Action(i, j) ?
A, (i?
1, j ?
1) ?
V add (i?
1, j ?
1)(i, j) ?
A, (i?
1, j) ?
V add (i?
1, j)(i, j) ?
A, (i?
1, j + 1) ?
V add (i?
1, j + 1)(i, j) ?
A, (i, j ?
1) ?
V add (i, j ?
1)(i, j) ?
A, (i, j + 1) ?
V add (i, j + 1)(i, j) ?
A, (i+ 1, j ?
1) ?
V add (i+ 1, j ?
1)(i, j) ?
A, (i+ 1, j) ?
V add (i+ 1, j)(i, j) ?
A, (i+ 1, j + 1) ?
V add (i+ 1, j + 1)(i?
1, j ?
1) ?
A, (i+ 1, j + 1) ?
A, add (i, j)(i, j) ?
V(i+ 1, j ?
1) ?
A, (i?
1, j + 1) ?
A, add (i, j)(i, j) ?
VTable 1: Templates for Expanding the Alignment AAccording to a Validation Alignment V4.2.2 Deletion TemplatesExisting alignment algorithms (e.g., GIZA++) arebiased toward aligning some words, especially in-frequent ones, in one language to many words in theother language in order to minimize the number ofunaligned words, even if many incorrect alignment2A thick line indicates an added link.187links are induced.3 Deletion templates are useful foreliminating the resulting spurious links.The basic idea is to remove alignment linksthat do not have a neighboring link if the wordin question has already been aligned to anotherword.
Table 2 lists two simple templates toclean up spurious links.
We define the predicateneighbor existsA(i, j) to denote whether there isan alignment link in the neighborhood of the link(i, j) in a given alignment A.
For example, the firsttemplate deletes spurious links for a particular wordei in E.Condition Action(i, j) ?
A, (i, k) ?
A,neighbor existsA(i, j), del (i, k)not(neighbor existsA(i, k))(i, j) ?
A, (k, j) ?
A,neighbor existsA(i, j), del (e, j)not(neighbor existsA(k, j))Table 2: Templates for Deleting Spurious Links in aGiven Alignment A4.2.3 Multi-Word Correction TemplatesCurrent alignment algorithms produce one-to-oneword correspondences quite successfully.
However,accurate alignment of phrasal constructions (many-to-many correspondences) is still problematic.
Onthe one hand, the ability to provide fully correctphrasal alignments is impaired by the occurrence ofhigh-frequency function words and/or words that arenot exact translations of the words in the other lan-guage.
On the other hand, we have observed thatmost alignment systems are capable of providingpartially correct phrasal alignments.4Our templates for handling multi-word correspon-dences are grounded in the outcome of this finding.That is, we make the (frequently correct) assumptionthat at least one alignment link in a many-to-manycorrespondence is correctly identified in the initial3This is a well-known characteristic of statistical alignmentsystems?motivated by the need to ensure a target-word trans-lation ei for each source word fj while modeling p(F |E) ?fordownstream MT.4Specifically, we conducted a preliminary study using 40manually-aligned English-Spanish sentences from a mixed cor-pus (UN + Bible + FBIS) as our gold standard.
We found that,in most cases where the human annotator aligned one word totwo words, an existing alignment system identified at least oneof the two alignment links correctly.Condition ActionnullFA(j), (i, j + 1) ?
A add (i, j)nullFA(j + 1), (i, j) ?
A add (i, j + 1)(i, j) ?
A, (i, j + 1) ?
A del (i, j)(i, j) ?
A, (i, j + 1) ?
A del (i, j + 1)nullFA(j), nullFA(j + 1) add (i, j),add (i, j + 1)nullEA(i), (i+ 1, j) ?
A add (i, j)nullEA(i+ 1), (i, j) ?
A add (i+ 1, j)(i, j) ?
A, (i+ 1, j) ?
A del (i, j)(i, j) ?
A, (i+ 1, j) ?
A del (i+ 1, j)nullEA(i), nullEA(i+ 1) add (i, j)add (i+ 1, j)(i+ 1, j + 1) ?
A add (i, j)nullEA(i), nullFA(j),(i, j) ?
A, nullEA(i+ 1), add (i+ 1, j + 1)nullFA(j + 1)(i, j) ?
A, (i+ 1, j) ?
A, add (i, j + 1)(i+ 1, j + 1) ?
A(i, j) ?
A, (i, j + 1) ?
A, add (i+ 1, j)(i+ 1, j + 1) ?
A(i?
1, j) ?
A, (i+ 1, j) ?
A add (i, j)nullEA(i)(i, j ?
1) ?
A, (i, j + 1) ?
A add (i, j)nullFA(j)Table 3: Templates for Handling Multi-Word Corre-spondences in a Given Alignment ACondition Action(i, j) ?
A del (i, j)nullEA(i), nullFA(j) add (i, j)Table 4: Templates for Correcting One-to-One Cor-respondences in a Given Alignment Aalignment.
Table 3 lists the templates for correct-ing alignment links in multi-word correspondences.The first five templates handle (ei ?
fjfj+1) cor-respondences, the next five handle (eiei+1 ?
fj)correspondences, the next four handle (eiei+1 ?fjfj+1) correspondences, and the final two handle(ei?1eiei+1 ?
fj) and (ei ?
fj?1fjfj+1) corre-spondences.The alignment rules given above may introduceerrors that require additional cleanup.
Thus, we in-troduce two simple templates (shown in Table 4) toaccommodate the deletion or addition of links be-tween a single pair of words.4.3 Instantiation of TemplatesALP starts with a set of templates and an initialalignment and attempts to instantiate the templatesduring the learning process.
The templates can beinstantiated using two methods: Simple (a word isinstantiated with a specific feature) or Generalized (aword is instantiated using a special keyword any-188thing).ALP requires only a small amount of manuallyaligned data for this process?a major strength ofthe system.
However, if we were to instantiate thetemplates with the actual words of the manual align-ment, the frequency counts (from such a small dataset) would not be high enough to derive reasonablegeneralizations.
Thus, ALP adds new links based onlinguistic features of words, rather than the wordsthemselves.
Using these features is what sets ALPapart from systems like the RA approach.
Specifi-cally, three features are used to instantiate the tem-plates:?
POS tags on both sides: We assign POStags using the MXPOST tagger (Ratnaparkhi,1996) for English and Chinese, and Connexorfor Spanish.?
Dependency relations: ALP utilizes depen-dencies for a better generalization?if a depen-dency parser is available in either language.In our experiments, we used a dependencyparser only in English (a version of the Collinsparser (Collins, 1997) that has been adaptedfor building dependencies) but not in the otherlanguage.?
A set of closed-class words: We use 16 dif-ferent classes, 9 of which are different seman-tic verb classes while the other 7 are functionwords, prepositions, and complementizers.5If both POS tags and dependency relations areavailable, they can be used together to instantiatethe templates.
That is, a word can be instantiatedin a TBL template with: (1) a POS tag (e.g., Noun,Adj); (2) a relation (e.g., Subj, Obj); (3) a parameterclass (e.g., Change of State); or (4) different subsetsof (1)?(3).
We also employ a more generalized formof instantiation, where words in the templates maymatch the keyword anything.4.4 Best Rule SelectionThe rules are selected using two different metrics:The accuracy of the rule or the overall impact of theapplication of the rule on the entire data.Two different mechanisms may be used for select-ing the best rule after generating all possible instan-tiations of templates:5These are based on the parameter classes of (Dorr et al,2002).1.
Rule Accuracy: The goal is to minimize theerrors introduced by the application of a trans-formation rule.
To measure accuracy of a ruler, we use good(r)?2?bad(r), where good(r)is the number of alignment links that are cor-rected by the rule, and bad(r) is the number ofincorrect alignment links produced.2.
Overall impact on the training data: The ac-curacy mechanism (above) is useful for bias-ing the system toward higher precision.
How-ever, if the overall system is evaluated using ametric other than precision (e.g., recall), theaccuracy mechanism may not guarantee thatthe best rule is chosen at each step.
Thus, wechoose the best rule according to the evalua-tion metric to be used for the overall system.5 Experiments and ResultsThis section describes our evaluation of ALP vari-ants using different combinations of settings of thefour parameters described above.
The two languagepairs examined are English-Spanish and English-Chinese.5.1 Evaluation MetricsLet A be the set of alignment links for a set of sen-tences.
We take S to be the set of sure alignmentlinks and P be the set of probable alignment links(in the gold standard) for the same set of sentences.Precision (Pr), recall (Rc) and alignment error rate(AER) are defined as follows:Pr =|A ?
P ||A|Rc =|A ?
S||S|AER = 1?|A ?
S|+ |A ?
P ||A|+ |S|A manually aligned corpus is used as our gold stan-dard.
For English-Spanish data, the manual an-notation was done by a bilingual English-Spanishspeaker.
Every link in the English-Spanish goldstandard is considered a sure alignment link.For English-Chinese, we used 2002 NIST MTevaluation test set, and each sentence pair wasaligned by two native Chinese speakers who are flu-ent in English.
Each alignment link appearing inboth annotations was considered a sure link, and189links appearing in only one set were judged as prob-able.
The annotators were not aware of the specificsof our approach.5.2 Evaluation DataWe evaluated ALP using 5-fold cross validation ontwo different data sets:1.
A set of 199 English-Spanish sentence pairs(nearly 5K words on each side) from a mixedcorpus (UN + Bible + FBIS).2.
A set of 491 English-Chinese sentence pairs(nearly 13K words on each side) from 2002NIST MT evaluation test set.We divided the pairs of sentences randomly into 5groups.
Then, for each fold, we used 4 groups as theground truth (for training), and used the other groupas our gold standard (for evaluation).
This processwas repeated 5 times so that each sentence pair wastested exactly once.
We computed precision, recalland error rate on the entire set for each data set.6For an initial alignment, we used GIZA++ in bothdirections (E-to-F and F -to-E, where F is eitherChinese (C) or Spanish (S)), and also two differentcombined alignments: intersection of E-to-F andF -to-E; and RA using a heuristic combination ap-proach called grow-diag-final (Koehn et al, 2003).For the English-Spanish experiments, GIZA++was trained on 48K sentence pairs from a mixedcorpus (UN + Bible + FBIS), with nearly 1.2M ofwords on each side, using 10 iterations of Model 1,5 iterations of HMM and 5 iterations of Model 4.For the English-Chinese experiments, we used 107Ksentence pairs from FBIS corpus (nearly 4.1M En-glish and 3.3M Chinese words) to train GIZA++, us-ing 5 iterations of Model 1, 5 iterations of HMM, 3iterations of Model 3, and 3 iterations of Model 4.5.3 Results for English-SpanishFor our initial alignments we used: (1) Intersec-tion of GIZA++ English-to-Spanish and Spanish-to-English; (2) GIZA++ English-to-Spanish; (3)GIZA++ Spanish-to-English; and (4) RA.
Of these,RA is the best, with an error rate of 21.2%.
For easeof comparison, the RA score appears in all result ta-bles below.6The number of alignment links varies over each fold.Therefore, we chose to evaluate all data at once instead of eval-uating on each fold and then averaging.Tables 5?7 compare ALP to each of these fouralignments using different settings of 4 parameters:ALP[IA, T, I, BRS], where IA is the initial align-ment, T is the set of templates, I is the instantia-tion method, and BRS is the metric for the best ruleselection at each iteration.
TE is the set of expan-sion templates from Table 1, TD is the set of dele-tion templates from Table 2, and TMW is the set ofmulti-word templates from Table 3 (supplementedwith templates from Table 4).As mentioned in Section 4.3, we use two instanti-ation methods: (1) simple instantiation (sim), wherethe words are instantiated using a specific POS tag,relation, parameter class or combination of those;and (2) generalized instantiation (gen), where thewords can be instantiated using the keyword any-thing.
Two different metrics are used to select thebest rule: The accuracy of the rule (acc) and theAER on the entire training data after applying therule (aer).7We performed statistical significance tests usingtwo-tailed paired t-tests.
Unless otherwise indicated,the differences between ALP and initial alignments(for all ALP variations and all initial alignments)were found to be statistically significant within the95% confidence interval.
Moreover, the differencesamong ALP variations themselves were statisticallysignificant within 95% confidence interval.Using Intersection as Initial Alignment We ranALP using the intersection of GIZA++ (E-to-S)and GIZA++(S-to-E) alignments as the initial align-ment in two different ways: (1) With TE using theunion of the unidirectional GIZA++ alignments asthe validation set, and (2) with TD and TMW appliedone after another.
Table 5 presents the precision, re-call and AER results.Alignments Pr Rc AERIntersection (Int) 98.2 59.6 25.9ALP[Int, TE , gen, aer] 90.9 69.9 21.0ALP[Int, (TD, TMW ), gen, aer] 88.8 72.3 20.3RA 83.8 74.4 21.2Table 5: ALP Results Using GIZA++ Intersection asInitial Alignment for English-SpanishUsing the expansion templates (TE) against a val-7We use only sure alignment links as the ground truth tolearn rules inside ALP.
Therefore, AER here refers to the AERof sure alignment links.190Alignments Pr Rc AERE-to-S 87.0 67.0 24.3ALP[E-to-S,(TD, TMW ), gen, aer] 85.6 76.4 19.3S-to-E 88.0 67.5 23.6ALP[S-to-E,(TD, TMW ), gen, aer] 87.1 76.7 18.4RA 83.8 74.4 21.2Table 6: ALP Results Using GIZA++ (Each Direc-tion) as Initial Alignment for English-Spanishidation set produced results comparable to the RAmethod.
The major difference is that ALP resultedin a much higher precision but in a lower recall be-cause ALP is more selective in adding a new linkduring the expansion stage.
This difference is due tothe additional constraints provided by word features.The version of ALP that applies deletion (TD) andmulti-word (TMW ) templates sequentially achieveslower recall but higher precision than RA.
In the bestcase, ALP achieves a statistically significant rela-tive reduction of 21.6% in AER over the Intersectionalignment.
When compared to RA, ALP achieves alower AER but the difference is not significant.Using Unidirectional GIZA++ Alignments as Ini-tial Alignment In a second set of experiments, weapplied ALP to the unidirectional GIZA++ align-ments, using deletion (TD) and multi-word (TMW )templates, generalized instantiation, and AER forthe best rule selection.
Table 6 presents the preci-sion, recall and AER results.For both directions, ALP achieves a lower preci-sion but much higher recall than that of the initialunidirectional alignment.
Overall, there was a rela-tive reduction of 20.6?22.0% in AER.
When com-pared to RA, the version of ALP that uses unidirec-tional GIZA++ alignments brings about significantreductions in AER: 9.0% relative reduction in onedirection and 13.2% relative reduction in the otherdirection.Using RA as Initial Alignment In a third experi-ment, we compared RA with variations of ALP us-ing RA as the initial alignment.
We used the tem-plates in two different ways: (1) with a combinationof TD and TMW (i.e., TD ?TMW ), and (2) with twoconsecutive runs of ALP, first with TD and then withTMW using the output of the first run as the initialannotation in the second run (i.e., TD, TMW ).
Ta-ble 7 presents precision, recall and AER results, us-ing different methods for template instantiation andAlignments Pr Rc AERALP[RA, (TD, TMW ), sim, acc] 87.8 77.7 17.6ALP[RA, (TD, TMW ), sim, aer] 87.9 79.0 16.8ALP[RA, (TD ?
TMW ), gen, aer] 86.2 80.0 17.0ALP[RA, (TD, TMW ), gen, aer] 86.9 80.5 16.4RA 83.8 74.4 21.2Table 7: ALP Results Using RA as Initial Alignmentfor English-Spanishbest rule selection.The results indicate that using AER is better thanusing accuracy for choosing the best rule.
Usinggeneralized instantiation instead of simple instantia-tion results in a better AER.
Running ALP with dele-tion (TD) templates followed by multi-word (TMW )templates results in a lower AER than running ALPonly once with combined templates.The highest performing variant of ALP, shownin the fourth line of the table, uses RA as the ini-tial alignment, template sets TD, TMW , general-ized instantiation, and AER for best rule selection.This variant is significantly better than RA, with a22.6% relative reduction in AER.
When comparedto the unidirectional alignments (E-to-S and S-to-E) given in Table 6, this variant of ALP yields nearlythe same precision (around 87.0%) but a 19.2% rel-ative improvement in recall.
The overall relative re-duction in AER is 30.5% in the S-to-E direction and32.5% in the E-to-S direction.5.4 Results for English-ChineseOur experiments for English-Chinese were designedwith a similar structure to that of English-Spanish,i.e., the same four initial alignments.
Once again,RA performs the best out of these initial alignments,with an error rate of 29.7%.
The results of the ini-tial alignments, and variations of ALP based on dif-ferent initial alignments are shown in Table 8.
Forbrevity, we include only the ALP parameter settingsresulting in the best configurations from the English-Spanish experiments.
For learning rules from thetemplates, we used only the sure alignment links asthe ground truth while learning rules inside ALP.On the English-Chinese data, ALP yields signif-icantly lower error rates with respect to the initialalignments.
When ALP is run with the intersectionof two GIZA++ alignments, the relative reductionis 5.4% in AER.
When ALP is run with E-to-C asinitial alignment, the relative reduction in AER is13.4%.
For the other direction, ALP produces a rel-191Alignments Pr Rc AERIntersection (Int) 94.8 53.6 31.2ALP[Int, (TD, TMW ), gen, aer] 91.7 56.8 29.5E-to-C 70.4 68.3 30.7ALP[E-to-C,(TD, TMW ), gen, aer] 79.1 68.1 26.6C-to-E 66.0 69.8 32.2ALP[C-to-E,(TD, TMW ), gen, aer] 83.3 66.0 26.2RA 61.9 82.6 29.7ALP[RA,(TD, TMW ), gen, aer] 82.1 72.7 22.8Table 8: ALP Results Using Different Initial Align-ments for English-Chineseative reduction of 18.6% in AER.
Finally, when RAis given to ALP as an initial alignment, ALP resultsin a relative reduction of 23.2% in AER.
When com-pared to RA, all variations of ALP, except the onestarting with the intersection, yield statistically sig-nificantly lower AER.
Another important finding isthat ALP yields significantly higher precision thanthe initial alignments but usually lower recall.6 ConclusionWe have presented ALP, a new approach that re-fines alignments by identifying the types of errorsmade by existing alignment systems and correctingthem.
Our approach adapts TBL to the problem ofword-level alignment by examining word featuresas well as neighboring links.
We use POS tags,closed-class words in both languages, and depen-dency relations in one language to classify the er-rors made by the initial alignment system.
We showthat ALP yields at least a 22.6% relative reductionon English-Spanish data and 23.2% relative reduc-tion on English-Chinese data in alignment error rateover that of the best performing system.We should note that ALP is not a stand-aloneword alignment system but a supervised learning ap-proach to improve already existing alignment sys-tems.
ALP takes advantage of clustering of align-ment links to project new links given a reasonableinitial alignment.
We have shown that ALP is quitesuccessful in projecting alignment links for two dif-ferent languages?Spanish and Chinese.Statistical alignment systems are more successfulwith increasing amount of training data.
WhetherALP improves the statistical alignment systemswhen they are trained on more data is an interestingresearch problem, which we plan to tackle in future.Finally, we will evaluate the improved alignmentsin the context of an end-to-end application, such asmachine translation.Acknowledgments This work has been supported, inpart, by ONR MURI Contract FCPO.810548265, Coopera-tive Agreement DAAD190320020, and NSF ITR Grant IIS-0326553.ReferencesNecip F. Ayan, Bonnie J. Dorr, and Nizar Habash.
2004.
Multi-Align: Combining linguistic and statistical techniques toimprove alignments for adaptable MT.
In Proceedings ofAMTA?2004, pages 17?26.Eric Brill.
1995.
Transformation-based error-driven learningand natural language processing: A case study in part-of-speech tagging.
Computational Linguistics, 21(4):543?565.Eric Brill.
1996.
Learning to parse with transformations.
InRecent Advances in Parsing Technology.
Kluwer AcademicPublishers.Peter F. Brown, Stephan A. Della-Pietra, and Robert L. Mer-cer.
1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguistics,19(2):263?311.Colin Cherry and Dekang Lin.
2003.
A probability modelto improve word alignment.
In Proceedings of ACL?2003,pages 88?95.Micheal Collins.
1997.
Three generative lexicalized models forstatistical parsing.
In Proceedings of ACL?1997.Mona Diab and Philip Resnik.
2002.
An unsupervised methodfor word sense tagging using parallel corpora.
In Proceed-ings of ACL?2002.Bonnie J. Dorr, Lisa Pearl, Rebecca Hwa, and Nizar Habash.2002.
DUSTer: A method for unraveling cross-language di-vergences for statistical word?level alignment.
In Proceed-ings of AMTA?2002.Philip Koehn, Franz Josef Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofNAACL/HLT?2003.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear modelsfor word alignment.
In Proceedings of ACL?2005.I.
Dan Melamed.
2000.
Models of translational equivalenceamong words.
Computational Linguistics, 26(2):221?249.Franz J. Och and Hermann Ney.
2000.
Improved statisticalalignment models.
In Proceedings of ACL?2000, pages 440?447.Franz J. Och and Hermann Ney.
2003.
A systematic compari-son of various statistical alignment models.
ComputationalLinguistics, 29(1):9?51, March.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of EMNLP?1996.Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Man-ning.
2002.
Extensions to HMM-based statistical wordalignment models.
In Proceedings of EMNLP?2002, pages87?94.Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996.HMM-based word alignment in statistical translation.
InProceedings of COLING?1996, pages 836?841.David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001.Inducing multilingual text analysis tools via robust projec-tion across aligned corpora.
In Proceedings of HLT?2001,pages 109?116.192
