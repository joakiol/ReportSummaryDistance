A Multi-Document Multi-Lingual Automatic Summarization SystemMohamad Ali Honarpisheh, Gholamreza Ghassem-Sani, Ghassem MirroshandelSharif University of Technology,Department of Computer Engineering, Tehran, Iran,Honarpisheh@ce.sharif.edu, Sani@sharif.ir, Mirroshandel@ce.sharif.eduAbstractAbstract.
In this paper, a new multi-document multi-lingual text summarizationtechnique, based on singular value decom-position and hierarchical clustering, is pro-posed.
The proposed approach relies ononly two resources for any language: aword segmentation system and a dictionaryof words along with their document fre-quencies.
The summarizer initially takes acollection of related documents, and trans-forms them into a matrix; it then appliessingular value decomposition to the re-sulted matrix.
After using a binary hierar-chical clustering algorithm, the most im-portant sentences of the most importantclusters form the summary.
The appropri-ate place of each chosen sentence is deter-mined by a novel technique.
The systemhas been successfully tested on summariz-ing several Persian document collections.1 IntroductionWith the advent of the Internet, different newspa-pers and news agencies regularly upload their newsin their sites.
This let users to access differentviewpoints and quotes about a single event.
At thesame time of this explosive growth of the amountof textual information, the need of people for quickaccess to information has dramatically increased.The solution proposed for dealing with this hugeamount of information is using Text Summarizers.Several systems have been developed with respectto this solution (McKeown et.
al., 2002; Radev et.al., 2001).Generally in the process of multi-document textsummarization, a collection of input documentsabout a particular subject is received from the userand a coherent summary without redundant infor-mation is generated.
However, several challengesexist in this process the most important of whichare removing redundant information from the inputsentences and ordering them properly in the outputsummary.
In a new approach to multi-documentsummarization proposed in this paper, SingularValue Decomposition (SVD) is used to find themost important dimensions and also to removenoisy ones.
This process makes clustering of simi-lar sentences easer.
In order to determine the levelof importance of different clusters, the generatedsingular values and singular vector of the SVDhave been used in a fashion similar to that(Steinberger and., Je?ek, 2004).
To evaluate gener-ated summaries the SVD-based method proposedin the same paper is used.2 Text Summarization approachesThere are different features with which we canclassify text summarization systems.
In (Sparck-Jones, 1999) these features are divided accordingto the input, purpose, and output of system.
Withrespect to this categorization, our proposed systemis a general multi-document multi-lingual textsummarizer which generates extracts a summaryfrom the input documents.Different approaches to text summarization canbe categorized in different ways according to vari-ous features of text summarization systems.
With733Fig.
1.Singular Value Decompositionrespect to the output of the system, there are twocategories of extracting and abstracting methods.Extraction-based summarization methods are alsodivided into three classes.The first group of Extraction-based methods isstatistical.
These methods statistically assign sig-nificance score to different textual units.
The veryfirst developed summarization methods were ofthis category (Edmundson and Wyllys, 1961).Scoring policy in these systems was based on dif-ferent features, such as term frequency and placeof the sentences.
Vector Space Models (Salton et.al., 1994), compression of sentences with Auto-matic Translation approaches (Knight and Marcu,2000), Hidden Markov Model (Jing and McKe-own, 2000), Topic Signatures based methods (Linand Hovy, 2000, Lacatusu et al, 2006) are amongthe most popular techniques that have been used inthe summarization systems of this category.The second groups of extraction-based methods,shallow understanding approaches use some in-formation about words or textual units and theirdependencies to improve the performance of ex-traction.
The understanding can be induced usingdependencies between words (Barzilay and Elha-dad, 1997), rhetorical relations (Paice and Johns,1993), events (Filatova and Hatzivassiloglou,2004).
In all of these methods, the most focuseddependencies are used as a measure for saliency ofeach textual unit.The third group, knowledge-based approaches,uses particular domain knowledge in discriminat-ing the important parts of input documents.
Thisknowledge is usually taking some assumptionsabout the working domain.
Centrifuger (Elhadad etal., 2005) is a good example of systems in thiscategory, which operates in medical domains.The new approach proposed in this paper usesSVD and hierarchical clustering methods.
It cantherefore be categorized in the statistical basedmethods.3 SVD based methodsMethods that use SVD to find salient informationin the input document are a member of VectorSpace Models.
In such models, each textual unit isrepresented by a vector.
Each component of thisvector is filled with a value which represents boththe local and global importance of each word.The idea of using SVD in summarization wasfirst introduced in (Gong and Liu, 2001).
In thismodel, the input document is transformed into annm?
sparse matrix, A, where m is the number ofwords and n is the number of the sentences of inputdocument.The SVD of this nm?
matrix, with the assump-tion nm f , is defined as follows:VUA ?= , (1)where U=[uij] is an nm?
column-orthogonal ma-trix with columns named as the left singular vec-tors, )...,,,( 21 ndiag ???=?
is an nn ?
diagonalmatrix with non-negative diagonal elements in de-scending order, and V=[vij] is an nn ?
row-orthogonal matrix with rows named as the rightsingular vectors (figure 1 demonstrate applicationof SVD to A).
The number of non-zero elements in?
is equal to the rank, r, of matrix A.734There are two viewpoints about the result of per-forming SVD on sentence by the word matrix ofdocument (Gong and Liu, 2001).
From transforma-tion viewpoint, SVD for each sentence reduces thedimensions from m to r. The salience degree of thereduced dimension decreases from the first to therth dimension.
From the semantic viewpoint, SVDderives the hidden latent structure of the inputdocument.
This structure is represented in r line-arly independent base vectors (i.e.
concepts).
SVDcan capture and model the interrelations betweenconcepts and hidden topics, which can be used tocluster semantically related words and sentences.In SVD-summarization method, for each salientconcept (singular vector) of matrix V, the sentencewith the highest value in that dimension is chosen.This technique helps us to choose the sentences ofthe summary that best represent the most importantconcepts.
For example, the most important sen-tence of a document in this summarization methodis the sentence that has the highest value in the firstrow of V.However, this method faces two significantproblems.
First, the number of dimensions shouldbe less than or at most equal to the number of dis-covered.
Second, in this method just individualconcept is used to determine their saliency, nottheir combinations.
This strategy obviously workspoor when a sentence that is not the most impor-tant of any dimension alone may contain conceptthat in combination make it important enough.These problems led to the introduction of a newsummarization method (Steinberger and Je?ek,2004).
This new approach uses summation of theweighted components of singular vectors instead ofeach individual concept alone.
The weight of eachvector's component of each sentence is its corre-sponding singular value.
The reason for suchweighting is to increase the effect of more impor-tant singular vectors.
Formally, the degree of sali-ence of each sentence can be computed by usingthe following formula:?==riiikk vs122, ..?
(2)where sk is the salience degree of kth sentence inthe modified latent vector space, and r is the num-ber of important dimensions of the new space.
Cor-responding value of each r dimensions is greaterthan half of the first singular value.Both of above strategies for text summarizationwere proposed for single document summarizationonly.
These approaches do not utilize the clusteringpower of SVD in discovering sentences with closemeanings.
On the other hand, their pure reliance onSVD, which does not depend on the characteristicsof any language, makes them appropriate to be ap-plied to any language.4 Multi-Document SVD based Summari-zationIn this paper a new version of the SVD basedsummarization is introduced.
After transformingall documents into sentence by word matrix, SVDis applied to the resultant matrix.
To remove re-dundant sentences from the summary, a hierarchi-cal bottom-up clustering algorithm is applied to ther most important extracted concepts of the inputsentences.
After extracting the clusters, the sali-ency score of each cluster is determined, and themost important clusters are selected.
At the sametime, using a simple ordering method, the appro-priate place of each sentence in the sorted collec-tion is determined.
In the following sections, eachof these processes is described in more details.Matrix Construction and Application of SVDGiven a collection of input documents that are re-lated to a particular subject, the system decom-poses the documents into sentences and their cor-responding words.
In addition, it computes the to-tal number of occurrences of each word in alldocuments as the Term Frequency (TF).The developed system works on Persian lan-guage.
It is assumed that words are separated byspaces.
However, some words in Persian are com-pound words .However this did not cause anyproblem for the developed system; because themost meaningful part of such words is usually lesscommon than others and thus have an InverseDocument Frequency (IDF) that is higher than thatof more common less meaningful parts.
IDF repre-sents the amount of meaning stored in each word.It can be used to reduce the impact of less impor-tant constituents such as stop words, which usuallyhave a high TF but contains little meaning.Theformula for calculating IDF is as follows:))(log()(termNUMDOCNUMDOCtermIDF = , (3)735where NUMDOC represents the total number ofdocument processed to create IDF dictionary andNUMDOC(term) is the number of documents inwhich the term appeared.After decomposition, the input sentences alongwith their corresponding words are arranged as amatrix.
Two different weighting schemes havebeen applied to each element of this matrix: 1) aconstant value and, 2) each word's associatedTF*IDF (Salton and Buckley, 1988).After constructing the Sentence by word matrix,SVD is applied to the resultant matrix.
ApplyingSVD removes the effect of unimportant words andhighlights more salient dimensions.
It also reducesthe number of dimensions for each sentence, re-sulting in an easer clustering process.
This im-proves the performance of sentence clustering bymaking it faster and less sensible to unimportantdifferences between sentences.ClusteringTo cluster reduced dimension sentences, a binaryhierarchical agglomerative clustering algorithmwith average group linkage is used.
In this algo-rithm, at first, each sentence is considered as acluster.
At each step, two closest clusters are com-bined into a single cluster.
The dimension of thisnew cluster is the average dimensions of the twocombining ones.
These steps are repeated until wehave only one cluster.
So the result of this algo-rithm is a binary tree.The question that needs to be answered at thisstep is "how clusters containing similar sentencescan be extracted from this binary tree?"
Two prop-erties are required to propose a sub-tree as a possi-ble cluster of similar sentences:1.
The number of existing sentences at the currentnode (cluster) should be less than or equal tothe total number of input documents; becauseit is assumed that there is not much redundantinformation in each document.
This assump-tion is valid with respect to the news docu-ments in which there might be only little re-dundancy.2.
The distance between two children of the cur-rent cluster should be less than or equal to thedistance between current cluster and its siblingnode.
This condition has been found empiri-cally.Using these two heuristics, similar clusters are ex-tracted from the binary tree.Finding Salient unitsTo select important clusters from the set of ex-tracted clusters, different clusters are scored basedon two diffrent methods.
In the first method, theaverage of TF*IDF of different words in the eachsentence in the current cluster are used.
The secondapproach was the latest SVD-based approachwhich was proposed by (Steinberger and Je?ek,2004) and was described in the section 3.
In thelatter, score of each cluster is found using the fol-lowing formula:||)()(clustersscoreclusterscore clusters?
?= , (4)where |cluster| represent the number of sentencesin the current cluster.Selecting and ordering sentences:In this step, the final representation of the sum-mary will be generated.
To this end, from each im-portant cluster, a sentence should be selected.
Atthe same time, the proper order of selected sen-tences should be determined.
To find this order, aRepresentative Document (RD) is selected.
TheRD is the document which includes most sentencesof the most important clusters.
After selecting RDthe following steps are performed:1.
Starting from the most important cluster, whilethe number of processed words of summarysentences does not exceed form the specifiednumber:a.
If no sentence from the current cluster wasnot added to the summary:i.
If there is a sentence from the RD in thiscluster, choose this sentence;ii.
Otherwise find the most important sen-tence, the current cluster: To find out theplace of the selected sentence in thesummary, a search is performed for clus-ters that contain both sentences from RDand neighbors of the selected sentence.The place of the sentences from RD thatco- clustered with neighbors of the se-lected sentence is chosen as the selectedsentence boundary.iii.
If any place has been found for the se-lected sentence, add it to summary in thespecified location, and mark that clusteras having a sentence in the summary.7362.
If it remains any unplaced sentence whichshould be presented in the summary, go tostep 1 with the remaining number of words.5 Experiments5.1 Testing CollectionThe proposed summarizer is originally developedfor the Persian language.
In Persian like manyother languages there is not a standard test collec-tion, to evaluate the summarizers.
To overcome thelake of a test collection in Persian, an unsupervisedapproach of evaluating summaries is selected (i.e.SVD-based evaluation method proposed in(Steinberger and Je?ek, 2004)).
In addition to anevaluator, a collection of documents was also re-quired.
For this purpose different texts related to asingle event were collected.
The properties of thesecollections are presented in table 15.2 Results and Discussions:To find out which term weighting and distancemeasure causes the highest increase in the SVD-Scores, various combinations of these approacheshas been used in the summarization approach.
Tofind the distance between clusters, Euclidian,Hamming, and Chebyshev distances and to deter-mine the saliency of different clusters, TFIDF andSVD-based methods were used.
The gained SVD-Based score using different configurations are rep-resented in table 2As it can be seen in table 2, TFIDF-based meth-ods score higher than SVD-based methods.
Also,the most promising distance was the hamming dis-tance.
It can also be seen that the performance de-creases substantially when instead of a constantvalue tf*idf scores were used.
It was observed thatusing various distance measure the SVD-Score ofeach collection would be different.
The SVD-Scores are in favor of using the boosting methodsfor classification of sentences with different dis-tance measure for each classifier.
Comparing theseresults with the ones proposed in (Steinberger andJe?ek, 2004), a significant decrease in evaluatedSVD-Based scores is observed.
One of the reasonsfor this phenomenon is that the distinct words ap-pearing in multi-documents are more extensivethan the words appear in a single document.6 ConclusionsThis paper presents a new SVD-based multi-lingual multi-document summarization methodusing agglomerative clustering.
In this method,first, using SVD, the most important concepts rep-resenting sentences are extracted into a word by asentence matrix.
Then, similar sentences are clus-tered using a new heuristic method.
Finally, impor-Average number of distinct words in documents 1474Average number of sentences in documents 32Average number of sentences in subjects 643Maximum Number of distinct words in subjects 2189Minimum Number of distinct words in subjects 485Number of subjects 14Table 1.
Testing Collections ?DetailsEuclidian Hamming ChebyshevAvg Max Min Avg Max Min Avg Max MinTFIDF 0.450 0.572 0.286 0.518 0.596 0.343 0.475 0.605 0.264SVD-based 0.466 0.632 0.313 0.472 0.650 0.322 0.449 0.620 0.309Table 2.
Using a constant value for word-sentence matrixEuclidian Hamming ChebyshevAvg Max Min Avg Max Min Avg Max MinTFIDF 0.364 0.549 0.109 0.269 0.512 0.113 0.406 0.512 0.283SVD-based 0.309 0.134 0.563 0.319 0.499 0.112 0.367 0.518 0.235Table 3.
Using TF-IDF for each element of the matrix737in the summary are extracted from the resultingclusters.
Different weighting schemes, distancemetrics and scoring methods have been experi-mented.
According to our experiments constantweighting scheme along with hamming distance issuperior to other combinations.
Since this methodonly needs determination of words and their in-verse document frequency, it can be applied to anylanguage providing these resources.
We are nowtrying to improve the performance of the proposedalgorithm.
It seems that applying Principle Direc-tion Partitioning (Blei, 2002) algorithm in the clus-tering phase and using Latent Dirichlet Allocationmethod (Boley, 1998) instead of the SVD basedones to model sentences can improve the score ofthe proposed method.ReferencesBarzilay, R. and Elhadad.
M. 1997.
?Using lexicalchains for text summarization.?
In Proceedings of theACL/EACL'97 Summarization Workshop, Madrid,Spain.Blei, D., Ng, A., and Jordan M., Latent Dirichlet aloca-tion.
Journal of Machine Learning Research, 3:993?1022, January 2003.
(A shorter version appeared inNIPS 2002).Boley, D.L.
: Principal Direction Divisive Partitioning.Data Mining and Knowledge Discovery, Vol.2(4):325?344, Dec. 1998.Edmundson, H.P.
and Wyllys, R.E., Automatic abstract-ing and indexing - Survey and recommendations,Communications of the ACM, Vol.
4, (1961) 226-234Elhadad, N., Kan, M.Y., Klavans, J., McKeown, K.:Customization in a unified framework for summariz-ing medical literature, Artificial Intelligence in Medi-cine Vol.
33(2): (2005) 179-198.Filatova, E., Hatzivassiloglou, V.: Event-based Extrac-tive summarization, In: Proceedings of ACL 2004Workshop on Summarization, (2004) 104-111.Gong, Y., Liu, X.: Generic Text Summarization UsingRelevance Measure and Latent Semantic Analysis.Proceedings of the 24th ACM SIGIR conference onResearch and development in information retrieval,New Orleans, Louisiana, United States (2001) 19-25Jing, H., McKeown, K.: Cut and paste based text sum-marization.
Proceedings of the 1st Conference of theNorth American Chapter of the Association forComputational Linguistics (NAACL'00), (2000), Se-attle-WashingtonKnight, K., Marcu, D.: Statistics based summarization.step one: Sentence compression, Proceeding of the17th National Conference of the American Associa-tion for Artificial Intelligence (2000)703-710.Lacatusu, F., Hickl, A., Roberts, K., Shi, Y., Bensley, J.,Rink, B., Wang, P., Taylor, L.: LCC?s GISTexter atDUC 2006: Multi-Strategy Multi-Document Summa-rization, Document Understanding Conference(2006)Lin, C.Y., Hovy, E.: From single to multi-documentsummarization: A prototype system and its evalua-tion.
Proceedings of the ACL, pages 457?464, 2002McKeown, K.R., Barzilay, R., Evans, D., Hatzivassi-loglou, V., Klavans, J.L., Nenkova, A., Sable, C.,Schiffman, B., Sigelman, S.: Tracking and summariz-ing news on a daily basis with Columbia's newsblas-ter.
Proceedings of 2002 Human Language Technol-ogy Conference (HLT), San Diego, CA, 2002Paice, C. D., Johns, A. P.: The identification of impor-tant concepts in highly structured technical papers,In: Proceedings of the 16th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval (1993)Radev, D. R., Blair-Goldensohn, S., Zhang, Z., Ragha-van R.S.
: Newsinessence: A system for domain-independent, real-time news clustering and multi-document summarization.
Proceedings of 2001 Hu-man Language Technology Conference (Demo Ses-sion), San Diego, CA, 2001Salton, G., Allan, J., Buckley, C., Singhal, A.: Auto-matic analysis, theme generation, and summarizationof machine readable texts, Science, Vol.
264(5164),(1994) 1421?1426Salton, G. and Buckley, C..
Term weighting approachesin automatic text retrieval.
Information Processingand Management, (1988), 24(5):513523Sparck-Jones, K.: Automatic summarizing: factors anddirections.
Mani I, Maybury MT, editors.
Advancesin automatic text summarization.
(1999) 10-12 [chap-ter 1]Steinberger, J., Je?ek, K. : Text Summarization andSingular Value Decomposition, Lecture Notes inComputer Science.Advances in Information Systems,Vol.
3261/2004, Springer-Verlag, Berlin HeidelbergNew York (2004) 245-254738
