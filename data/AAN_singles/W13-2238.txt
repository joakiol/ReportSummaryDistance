Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 309?319,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsLength-incremental Phrase Training for SMTJoern Wuebker and Hermann NeyHuman Language Technology and Pattern Recognition GroupRWTH Aachen UniversityAachen, Germany{wuebker,ney}@cs.rwth-aachen.deAbstractWe present an iterative technique to gener-ate phrase tables for SMT, which is basedon force-aligning the training data witha modified translation decoder.
Differ-ent from previous work, we completelyavoid the use of a word alignment orphrase extraction heuristics, moving to-wards a more principled phrase generationand probability estimation.
During train-ing, we allow the decoder to generate newphrases on-the-fly and increment the max-imum phrase length in each iteration.
Ex-periments are carried out on the IWSLT2011 Arabic-English task, where we areable to reach moderate improvements on astate-of-the-art baseline with our trainingmethod.
The resulting phrase table showsonly a small overlap with the heuristicallyextracted one, which demonstrates the re-strictiveness of limiting phrase selectionby a word alignment or heuristics.
Byinterpolating the heuristic and the trainedphrase table, we can improve over thebaseline by 0.5% BLEU and 0.5% TER.1 IntroductionMost state-of-the-art SMT systems get the statis-tics from which the different component modelsare estimated via heuristics using a Viterbi wordalignment.
The word alignment is usually gener-ated with tools like GIZA++ (Och and Ney, 2003),that apply the EM algorithm to estimate the align-ment with the HMM or IBM-4 translation mod-els.
This is also the case for the phrases or ruleswhich serve as translation units for the decoder.All phrases that do not violate the word alignmentare extracted and their probabilities are estimatedas relative frequencies (Koehn et al 2003).A number of different approaches have tried todo away with the heuristics and close this gap be-tween the phrase table generation and translationdecoding.
However, most of these approaches ei-ther fail to achieve state-of-the-art performance orstill make use of the word alignment or the ex-traction heuristics, e.g.
as a prior in discriminativetraining or to initialize a generative or generativelyinspired training procedure and are thus biased bytheir weaknesses.
Here, we aim at moving towardsthe ideal situation, where a unified framework in-duces the phrases based on the same models as indecoding.We train the phrase table without using a wordalignment or the extraction heuristics.
Differentfrom previous work, we are able to generate allpossible phrase pairs on-the-fly during the train-ing procedure.
A further advantage of our pro-posed algorithm is that we use basically the samebeam search as in translation.
This makes it easyto re-implement by modifying any translation de-coder, and makes sure that training and translationare consistent.
In principle, we apply the forceddecoding approach described in (Wuebker et al2010) with cross-validation to prevent over-fitting,but we initialize the phrase table with IBM-1 lex-ical probabilities (Brown et al 1993) instead ofheuristically extracted relative frequencies.
Thealgorithm is extended with the concept of back-off phrases, so that new phrase pairs can be gener-ated at training time.
The size of the newly gener-ated phrases is incremented over the training iter-ations.
By introducing fallback decoding runs, weare able to successfully align the complete trainingdata.
Local language models are used for betterphrase pair pre-selection.309The experiments are carried out on the IWSLT2011 Arabic-English shared task.
We are able toshow that it is possible and feasible to reach state-of-the-art performance without the need to word-align the bilingual training data.
The small over-lap of 18.5% between the trained and the heuristi-cally extracted phrase table demonstrates the limi-tations of previous work, where training is initial-ized by the baseline phrase table or phrase selec-tion is restricted by a word alignment.
With a lin-ear interpolation of phrase tables an improvementof 0.5% BLEU and 0.5% TER over the baselinecan be achieved.
The result in BLEU is statisti-cally significant on the test set with 90% confi-dence.
Further, we can confirm the observationof previous work, that phrases with near-zero en-tropies seem to be a disadvantage for translationquality.
Although we use a phrase-based decoderhere, the principles of our work can be applied toany statistical machine translation paradigm.
Thesoftware used for our experiments is available un-der a non-commercial open source licence.The paper is organized as follows.
We reviewrelated work in Section 2.
The decoder and itsfeatures are described in Section 3 and we givean overview of the training procedure in Section4.
The complete algorithm is described in Section5 and experiments are presented in Section 6.
Weconclude with Section 7.2 Related WorkMarcu and Wong (2002) present a joint probabil-ity model, which is trained with a hill-climbingtechnique based on break, merge, swap and moveoperations.
Due to the computational complexitythey are only able to consider phrases, which ap-pear at least five times in the data.
The model isshown to slightly underperform heuristic extrac-tion in (Koehn et al 2003).
For higher efficiency,it is constrained by a word alignment in (Birch etal., 2006).
DeNero et al(2008) introduce a differ-ent training procedure for this model based on aGibbs sampler.
They make use of the word align-ment for initialization.A generative phrase model trained with theExpectation-Maximization (EM) algorithm isshown in (DeNero et al 2006).
It also does notreach the same top performance as heuristic ex-traction.
The authors identify the hidden segmen-tation variable, which results in over-fitting, as themain problem.Liang et al(2006) present a discriminativetranslation system.
One of the proposed strategiesfor training, which the authors call bold updating,is similar to our training scheme.
They use heuris-tically extracted phrase translation probabilities asblanket features in all setups.Another iteratively-trained phrase model is de-scribed by Moore and Quirk (2007).
Their modelis segmentation-free and, confirming the findingsin (DeNero et al 2006), can close the gap tophrase tables induced from surface heuristics.
Itrelies on word alignment for phrase selection.Mylonakis and Sima?an (2008) present a phrasemodel, whose training procedure uses prior prob-abilities based on Inversion Transduction Gram-mar and smoothing as learning objective to pre-vent over-fitting.
They also rely on the word align-ment to select phrase pairs.Blunsom et al(2009) perform inference overlatent synchronous derivation trees under a non-parametric Bayesian model with a Gibbs sampler.Training is also initialized by extracting rules froma word alignment, but the authors let the samplerdiverge from the initial value for 1000 passes overthe data, before the samples are used.
However,as the model is to weak for actual translation, theusual extraction heuristics are applied on the hier-archical alignments to infer a distribution over ruletables.Wuebker et al(2010) use a forced decodingtraining procedure, which applies a leave-one-outtechnique to prevent over-fitting.
They are able toshow improvements over a heuristically extractedphrase table, which is used for initialization of thetraining.In (Saers and Wu, 2011), the EM algorithm isapplied for principled induction of bilexica basedon linear inversion transduction grammar.
Themodel itself underperforms the baseline, but theauthors show moderate improvements by combin-ing it with the baseline phrase table, which is sim-ilar to our results.
(Neubig et al 2011) also propose a probabilis-tic model based on inversion transduction gram-mar, which allows for direct phrase table extrac-tion from unaligned data.
They show results simi-lar to the heuristic baseline on several tasks.A number of different models that can betrained from forced derivation trees are shown in(Duan et al 2012), including a re-estimated trans-lation model, two reordering models and a rule se-310quence model.
For inference, they optimize theirparameters towards alignment F-score.
The forcedderivations are initialized with the standard heuris-tic extraction scheme.He and Deng (2012) describe a discriminativephrase training procedure, where n-best transla-tions are produced by the decoder on the wholetraining data.
The heuristically extracted relativefrequencies serve as a prior, and the probabili-ties are updated with a maximum BLEU criterionbased on the n-best lists.3 Translation ModelWe use the standard phrase-based translation de-coder from the open source toolkit Jane 2 (Wue-bker et al 2012a) for both the training proce-dure and the translation experiments.
It makes useof the usual features: Translation channel mod-els in both directions, lexical smoothing models inboth directions, an n-gram language model (LM),phrase and word penalty and a jump-distance-based distortion model.
Formally, the best trans-lation e?I?1 as defined by the models hm(eI1, sK1 , fJ1 )can be written as (Och and Ney, 2004)e?I?1 = argmaxI,eI1{ M?m=1?mhm(eI1, sK1 , fJ1 )}, (1)where fJ1 = f1 .
.
.
fJ is the source sentence,eI1 = e1 .
.
.
eI the target sentence and sK1 =s1 .
.
.
sK their phrase segmentation and align-ment.
We define sk := (ik, bk, jk), where ik isthe last position of kth target phrase, and (bk, jk)are the start and end positions of the source phrasealigned to the kth target phrase.
Different frommany standard systems, the lexical smoothingscores are not estimated by extracting counts froma word alignment, but with IBM-1 model scorestrained on the bilingual data with GIZA++.
Theyare computed as (Zens, 2008)hlex(eI1, sK1 , fJ1 ) =K?k=1jk?j=bklog?
?p(fj |e0) +ik?i=ik?1+1p(fj |ei)??
(2)Here, e0 denotes the empty target word.
Thelexical smoothing model for the inverse direc-tion is computed analogously.
The log-linear fea-ture weights ?m are optimized on a developmentdata set with minimum error rate training (MERT)(Och, 2003).
As optimization criterion we useBLEU (Papineni et al 2001).4 Training4.1 OverviewIn this work we employ a training procedure in-spired by the Expectation-Maximization (EM) al-gorithm.The E-step corresponds to force-aligning thetraining data with a modified translation decoder,which yields a distribution over possible phrasalsegmentations and their alignment.
Different fromoriginal EM, we make use of not only the twotranslation channel models that are being learned,but the full log-linear combination of models as intranslation decoding.
Formally, we are searchingfor the best phrase segmentation and alignment forthe given sentence pair, which is defined bys?K?1 = argmaxK,sK1{ M?m=1?mhm(eI1, sK1 , fJ1 )}(3)To force-align the training data, the translationdecoder is constrained to the given target sentence.The translation candidates applicable for each sen-tence pair are selected through a bilingual phrasematching before the actual search.In the M-step, we re-estimate the phrase tablefrom the phrase alignments.
The translation prob-ability of a phrase pair (f?
, e?)
is estimated aspFA(f?
|e?)
=CFA(f?
, e?)?f?
?CFA(f?
?, e?
)(4)where CFA(f?
, e?)
is the count of the phrase pair(f?
, e?)
in the phrase-aligned training data.In contrast to original EM, this is done by tak-ing the phrase counts from a uniformly weightedn-best list.
The limitation to n phrase alignmentshelps keeping the number of considered phrasesreasonably small.
Because the log-linear featureweights have been tuned in a discriminative fash-ion to optimize the ranking of translation hypothe-ses, rather than their probability distribution, pos-terior probabilities received by exponentiation andrenormalization need to be scaled similar to (Wue-bker et al 2012b).
Uniform weights can alle-viate this mismatch between the discriminatively311trained log-linear feature weights and the actualprobability distribution, without having to resortto an arbitrarily chosen global scaling factor.
Thiscorresponds to the count model in (Wuebker et al2010) and was shown by the authors to performsimilar or better than using actual posterior proba-bilities.
In our experiments, we set the size of then-best list to n = 1000.The first iteration of phrase training is initializedwith an empty phrase table.
We use the notion ofbackoff phrases to generate new phrase pairs on-the-fly.
To avoid over-fitting, we apply the cross-validation technique presented in (Wuebker et al2010) with a batch-size of 2000 sentences.
Thismeans that for each batch the phrase and marginalcounts from the full phrase table are reduced bythe statistics taken from the same batch in the pre-vious iteration.
The phrase translation probabili-ties are then estimated from these updated counts.Phrase pairs only appearing in a single batch areassigned a fixed penalty.4.2 Backoff PhrasesBackoff phrases are phrase pairs that are generatedon-the-fly by the decoder at training time.
Whenaligning a sentence pair, for a given maximumphrase length m, the decoder inserts all combi-nations of source ms-grams and target mt-gramsinto the translation options, that are present in thesentence pair and with ms,mt ?
m. Formally,for the sentence pair (fJ1 , eI1), fJ1 = f1 .
.
.
fJ ,eI1 = e1 .
.
.
eI , and maximum length m, we gen-erate all phrase pairs (f?
, e?)
where?ms,mt, j, i :1 ?
ms,mt ?
m ?
1 ?
j ?
J ?ms + 1?
1 ?
i ?
I ?mt + 1?
f?
= f (j+ms?1)j ?
e?
= e(i+mt?1)i .
(5)These generated phrase pairs are given a fixedpenalty penp per phrase, pens per source word andpent per target word, which are summed up andsubstituted for the two channel models.
The lex-ical smoothing scores are computed in the usualway based on an IBM-1 table.
Note that this tableis not extracted from a word alignment, but con-tains the real probabilities trained with the IBM-1model by GIZA++.We use backoff phrases in two different con-texts.
In the first mmax = 6 iterations, they areapplied as a means to generate new phrase pairs onthe fly.
We increase the maximum phrase lengthm in each iteration and always generate all possi-ble backoff phrases before aligning each sentence.Later, when a sufficient number of phrases havebeen generated in the previous iterations, they areused as a last resort in order to avoid alignmentfailures.At the later stage of the length-incrementaltraining, we also make use of a modified version,where we only allow new phrase pairs (f?
, e?)
to begenerated, if no translation candidates exist for f?after the bilingual phrase matching.
However, inthis case, backoff phrases are only used if a firstdecoding run fails and we have to resort to fallbackruns, which are described in the next Section.4.3 Fallback Decoding RunsTo maximize the number of successfully alignedsentences, we allow for fallback decoding runswith slightly altered parameterization, wheneverconstrained decoding fails.
In this work, weonly change the parameterization of the backoffphrases.
After mmax = 6 iterations, we no longergenerate any backoff phrases in the first decodingrun.
If it fails, a second run is performed, wherewe allow to generate backoff phrases for all sourcephrases, which have no target candidates after thebilingual phrase matching.
Finally, if this one alsofails, all possible phrases are generated in the thirdrun.
Here, the maximum backoff phrase length isfixed to m = 1.
We denote the number of fallbackruns with nfb = 2.
In our experiments, the twofallback runs enable us to align every sentence pairof the training data after the sixth iteration.4.4 Local Language ModelsTo make the training procedure feasible, it is par-allelized by splitting the training data into batchesof 2000 sentences.
The batches are aligned inde-pendently.
For each batch, we produce a locallanguage model, which is a unigram LM trainedon the target side of the current batch.
We pre-sort the phrases before search by their log-linearmodel score, which uses the phrase-internal uni-gram LM costs as one feature function.
One ef-fect of this is that the order in which phrase candi-dates are considered is adjusted to the local part ofthe data, which has a positive effect on decodingspeed.
Secondly, we limit the number of transla-tion candidates for each source phrase to the bestscoring 500 before the bilingual phrase matching.3122324252627282  3  4  5  68090100BLEU[%]wordcoverage[%]Iterationwp=-0.1wp=-0.3wp=-0.5wp=-0.7Figure 1: BLEU scores and word coverages ondev over the first 6 training iterations with dif-ferent word penalties (wp).Using the local LM for this means that the pre-selection better suits the current data batch.
As aresult, the number of phrases remaining after thephrase matching is increased as compared to thesame setup without a local language model.4.5 ParameterizationThe training procedure has a number of hyper pa-rameters, most of which do not seem to have astrong impact on the results.
This section de-scribes the parameters that have to be chosen care-fully.
To successfully align a sentence pair, ourdecoder is required to fully cover the source sen-tence.
However, in order to achieve a good suc-cess rate in terms of number of aligned sentencepairs, we allow for incompletely aligned targetsentences.
We denote the percentage of success-fully aligned sentence pairs as sentence coverage.Note that we count a sentence pair as successfullyaligned, even if the target sentence is not fullycovered.
the word penalty (wp) feature weight?wp needs to be adjusted carefully.
A high valueleads to a high sentence coverage, but many oftheir target sides may be incompletely aligned.
A242526272  3  4  5  6020406080100BLEU[%]surplusphrases[%]Iterationpen0=4pen0=3pen0=2pen0=1pen0=0.5Figure 2: BLEU scores and percentage of surplusphrases on dev over the first 6 training iterationswith different backoff phrase penalties pen0.low word penalty can decrease the sentence cover-age, while aligning larger parts of the target sen-tences.
We denote the total percentage of suc-cessfully aligned target words as word coverage.Please note the distincton to the sentence cover-age, which is defined above.
Figure 1 shows theword coverages and BLEU scores for training iter-ations 2 through 6 with different word penalties.
Inthe first iteration, the results are identical, as onlyone-to-one phrases are allowed and the number ofaligned target words is therefore predetermined.For ?wp = ?0.1, the word coverages are continu-ously decreasing with each iteration, although notby much.
For ?wp = ?0.3 to ?wp = ?0.7 theword coverage slightly increases from iteration 2to 3 and then decreases again.
In terms of BLEUscore, ?wp = ?0.3 has a slight advantage over theother values and we decided to continue using thisvalue in all subsequent experiments.The backoff phrase penalties directly affectthe learning rate of the training procedure.
Withlow penalties, only few, very good phrases getan advantage over the ones generated on-the-fly,which corresponds to a slow learning rate.
In-3131.
Initialize with empty phrase table2.
Set backoff phrase penalties topen0 = 3 and m = 13.
Until m = mmax, iterate:?
If iteration > 1: setm = m+ 1?s2t = ?s2t + ?
?t2s = ?t2s + ??
Force-align training data andre-estimate phrase table4.
Set m = 1 and nfb = 25.
Iterate:?
Force-align training data andre-estimate phrase tableFigure 3: The complete training algorithm.creasing the penalties means that a larger per-centage of the phrase pairs generated in the pre-vious iterations will be favored over new back-off phrases, which corresponds to a faster learn-ing rate.
We denote phrase pairs that are moreexpensive than their backoff phrase counterpartsas surplus phrases.
Figure 2 shows the behaviorover the training iterations 2 through 6 with differ-ent penalties pen0 in terms of percentage of sur-plus phrase pairs and BLEU score.
Here we setpens = pent = pen0 and penp = 5pen0.
We cansee that pen0 = 4 yields less than 0.1% surplusphrases through all iterations, whereas pen0 = 0.5starts off with 98.2% surplus phrases and goesdown to 55.9% in iteration 6.
In terms of BLEU, afast learning rate seems to be preferable.
The bestresults are achieved with pen0 = 3, where the rateof surplus phrases starts at 6.8% and decreases to1.7% until iteration 6.
In all subsequent experi-ments, we set pen0 = 3.5 Length-incremental TrainingIn this section we describe the complete trainingalgorithm.
The first training iteration is initial-ized with an empty phrase table.
The phrases usedin alignment are backoff phrases, which are gen-erated on-the-fly.
The maximum backoff phraselength is set to m = 1.
Then the forced alignmentis iterated, increasing m by 1 in each iteration, upto a maximum of mmax = 6.After mmax = 6 iterations, we have created asufficient number of phrase pairs and continue it-erating the training procedure with new parame-Arabic Englishtrain Sentences 305KRunning Words 6.5M 6.5MVocabulary 104K 74Kdev Sentences 934Run.
Words 19K 20KVocabulary 4293 3182OOVs (run.
words) 445 182test Sentences 1664Run.
Words 31K 32KVocabulary 5415 3650OOVs (run.
words) 658 159Table 1: Statistics for the IWSLT 2011 Arabic-English data.
The out-of-vocabulary words are de-noted as OOVs.ters.
Now, we do not allow usage of any back-off phrases in the first decoding run.
If the firstrun fails, we allow a fallback decoding run, wherebackoff phrases are generated only for sourcephrases without translation candidates.
If thisone also fails, in a final fallback run all possiblephrases are generated.
Here we allow a maximumbackoff phrase length of m = 1.The log-linear feature weights ?i used for train-ing are mostly standard values.
Only ?wp forthe word penalty is adjusted as described in Sec-tion 4.5, and ?s2t,?t2s for the two phrasal channelmodels are incremented with each iteration.
Westart off with ?s2t = ?t2s = 0 and increment theweights by ?
= 0.02 in each iteration, until thestandard value ?s2t = ?t2s = 0.1 is reached initeration 6, after which the values are kept fixed.MERT is not part of the training procedure, butonly used afterwards for evaluation.
The full algo-rithm is illustrated in Figure 3.6 Experiments6.1 DataWe carry out our experiments on the IWSLT 2011Arabic-English shared task1.
It focuses on thetranslation of TED talks, a collection of lectureson a variety of topics ranging from science to cul-ture.
Our bilingual training data is composed of allavailable in-domain (TED) data and a selection ofthe out-of-domain MultiUN data provided for theevaluation campaign.
The bilingual data selection1www.iwslt2011.org314161718192021222324252627282  4  6  8  10  12  14  16  18  20BLEU[%]IterationLength-incremental Training (dev)Baseline (dev)Length-incremental Training (test)Baseline (test)Figure 4: BLEU scores on dev and test over 20training iterations.is based on (Axelrod et al 2011).
Data statisticsare given in Table 1.
The language model is a 4-gram LM trained on all provided in-domain mono-lingual data and a selection based on (Moore andLewis, 2010) of the out-of-domain corpora.
To ac-count for statistical variation, all reported resultsare average scores over three independent MERTruns.6.2 ResultsTo build the baseline phrase table, we performthe standard phrase extraction from a symmetrizedword alignment created with the IBM-4 model byGIZA++.
The length of the extracted phrases islimited to a maximum of six words.
The lexicalsmoothing scores are computed from IBM-1 prob-abilities.
We run MERT on the development set(dev) and evaluate on the test set (test).
A sec-ond baseline is the technique described in (Wue-bker et al 2010), which we denote as leave-one-out.
It is initialized with the heuristically extractedtable and run for one iteration, which the authorshave shown to be sufficient.Length-incremental training is performed as de-scribed in Section 5.
After each iteration, we runMERT on dev using the resulting phrase table andevaluate.
The set of models used here is identicalto the baseline.The results in BLEU are plotted in Figure 4.
Wecan see that the performance increases up to it-eration 5, after which only small changes can beobserved.
The performance on dev is similar todev testBLEU TER BLEU TER[%] [%] [%] [%]baseline 27.4 54.0 24.6 57.8leave-one-out 27.3 54.2 24.6 57.7length-increm.
27.5 53.8 24.9 57.4lin.
interp.
27.9 53.5 25.1?
57.3Table 2: BLEU and TER scores of the baseline,phrase training with leave-one-out and length-incremental training after 12 iterations, as well asa linear interpolation of the baseline with length-incremental phrase table.
Results marked with ?are statistically significant with 90% confidence.the baseline, on test the trained phrase tablesare consistently slightly above the baseline.
Theoptimum on dev is reached in iteration 12.
Ex-act BLEU and TER (Snover et al 2006) scores ofthe optimum on dev and the baseline are givenin Table 2.
The phrase table trained with leave-one-out (Wuebker et al 2010) performs simlar tothe heuristic baseline.
Length-incremental train-ing is slightly superior to the baseline, yieldingan improvement of 0.3% BLEU and 0.4% TERon test.
Similar to results observed in (DeN-ero et al 2006) and (Wuebker et al 2010), a lin-ear interpolation with the baseline containing allphrase pairs from either of the two tables yields amoderate improvement of 0.5% BLEU and 0.5%TER both data sets.
The BLEU improvement ontest is statistically significant with 90% confi-dence based on bootstrap resampling as describedby Koehn (2004).6.3 AnalysisIn Figure 5 we plot the number of phrase pairspresent in the phrase tables after each iteration.In the first 6 iterations, we keep generating newphrase pairs via backoff phrases.
The maximumof 14.4M phrase pairs is reached after three itera-tions.
For comparison, the size of the heuristicallyextracted table is 19M phrase pairs.
Afterwards,backoff phrases are only used in fallback decodingruns, which leads to drop in the number of phrasepairs that are being used.
It levels out at 10.4Mphrases.When we take a look at the phrase length distri-butions in both the baseline and the trained phrasetable shown in Figure 6, we can see that in the lat-ter the phrases are generally shorter, which con-3155.0 M10.0 M15.0 M2  4  6  8  10  12  14  16  18  20#phrasepairsIterationFigure 5: Number of generated phrase pairs over20 training iterations.firms observations from previous work.
In thetrained phrase table, phrases of length one and twomake up 47% of all phrases.
In the heuristicallyextracted table it is only 32%.
This is even morepronounced in the intersection of the two tables,where 68% of the phrases are of length one andtwo.Interestingly, the total overlap between the twophrase tables is rather small.
Only about 18.5%of the phrases from the trained table also appear inthe heuristically extracted one.
This shows that, bygenerating phrases on-the-fly without restrictionsbased on a word alignment or a bias from intializa-tion, our training procedure strongly diverges fromthe baseline phrase table.
We conclude that mostprevious work in this area, which adhered to theabove mentioned restrictions, was only able to ex-plore a fraction of the full potential of real phrasetraining.Following (DeNero et al 2006), we computethe entropy of the distributions within the phrasetables to quantify the ?smoothness?
of the distri-bution.
For a given source phrase f?
, it is definedasH(f?)
=?e?p(e?|f?)log(p(e?|f?)).
(6)A flat distribution with a high level of uncer-tainty yields a high entropy, whereas a peaked dis-tribution with little uncertainty produces a low en-tropy.
We analyze the phrase tables filtered to-wards the dev and test sets.
The average en-05101520253035401 2 3 4 5 6#phrasepairs[%]source phrase lengthBaselineLength-incremental trainingIntersectionFigure 6: Histogram of the phrase lengths presentin the phrase tables.tropy, weighted by frequency, is 3.1 for the ta-ble learned with length-incremental training, com-pared to 2.7 for the heuristically extracted one.However, the interpolated table, which has the bestperformance, lies in between with an average en-tropy of 2.9.
When we consider the histogram ofentropies for the phrase tables in Figure 7, we cansee that in the baseline phrase table 3.8% of thephrases haven an entropy below 0.5, compared to0.90% for length-incremental training and 0.16%for the linear interpolation.
Therefore, we canconfirm the observation in (DeNero et al 2006),that phrases with a near-zero entropy are undesir-able for decoding.
The distribution of the higherentropies, however, does not seem to matter fortranslation quality.
This also gives us a handle forunderstanding, why phrase table interpolation of-ten improves results: It largely seems to eliminatenear-zero entropies from either table.6.4 Training timeThe training was not run under controlled condi-tions, so we can only give a rough estimate ofhow the training times between the different meth-ods compare.
Also, some of the steps were par-allelized while others are not.
To account forthe computational resources needed, we report thetrainig times on a single machine by summing thetimes for all parallel and sequential processes.Heuristc phrase extraction from the word align-ment took us about 1.7 hours.
A single itera-tion of standard phrase training (leave-one-out)needs about 24 hours.
The first iteration of length-3160510152025303540450-0.5 0.5-1 1-2 2-3 3-4 >4#phrasepairs[%]entropyBaselineLength-incrementalLinear interpolationFigure 7: Histogram of entropies present in thephrase tables.incremental training as well as all iterations afterthe sixth also took roughly 24 hours.
The itera-tions two through six of length-incremental train-ing are considerably more expensive due to thelarger size of backoff phrases.
Iteration six, witha maximum backoff phrase size of six words onsource and target side, was the slowest with around740 hours.7 ConclusionIn this work we presented a training procedure forphrase or rule tables in statistical machine trans-lation.
It is based on force-aligning the trainingdata with a modified version of the translation de-coder.
Different from previous work, we com-pletely avoid the use of a word alignment on thebilingual training corpus.
Instead, we initialize theprocedure with an empty phrase table and gener-ate all possible phrases on-the-fly through the con-cept of backoff phrases.
Starting with a maximumphrase length of m = 1, we increment m in eachiteration, until we reach mmax.
Then, we con-tinue training in a more conventional fashion, al-lowing creation of new phrases only in fallbackruns.
As additional extensions to previous workwe introduce fallback decoding runs for highercoverage of the data and local language modelsfor better pre-selection of phrases.
The effectsof the most important hyper parameters of ourprocedure are discussed and we show how theywere selected in our setup.
The experiments arecarried out with a phrase-based decoder on theIWSLT 2011 Arabic-English shared task.
Thetrained phrase table slightly outperforms our state-of-the-art baseline and a linear interpolation yieldsan improvement of 0.5% BLEU and 0.5% TER.The BLEU improvement on test is statisticallysignificant with 90% confidence.
The small over-lap of 18.5% between the trained and the heuris-tically extracted phrase table shows how initial-ization or restrictions based on word alignmentswould have biased the training procedure.
We alsoanalyzed the distribution of entropies within thephrase tables, confirming the previous observationthat fewer near-zero entropy phrases are advanta-geous for decoding.
We also showed that, in oursetup, near-zero entropies are largely eliminatedby phrase table interpolation.In future work we plan to apply this techniqueas a more principled way to train a wider range ofmodels similar to (Duan et al 2012).
But evenfor the phrase models, we have only scratched thesurface of its potential.
We hope that by findinga meaningful way to set the hyper parameters ofour training procedure, better and smaller phrasetables can be created.AcknowledgmentsThis work was partially realized as part of theQuaero Programme, funded by OSEO, FrenchState agency for innovation.
The material isalso partially based upon work supported bythe DARPA BOLT project under Contract No.HR0011- 12-C-0015.
Any opinions, findings andconclusions or recommendations expressed in thismaterial are those of the authors and do not neces-sarily reflect the views of DARPA.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 355?362, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.Alexandra Birch, Chris Callison-Burch, Miles Os-borne, and Philipp Koehn.
2006.
Constraining thePhrase-Based, Joint Probability Statistical Transla-tion Model.
In Human Language Technology Conf.
(HLT-NAACL): Proc.
Workshop on Statistical Ma-chine Translation, pages 154?157, New York City,NY, June.Phil Blunsom, Trevor Cohn, Chris Dyer, and MilesOsborne.
2009.
A gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of317the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP:Volume 2 - Volume 2, ACL ?09, pages 782?790,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
Computational Linguistics,19(2):263?311, June.John DeNero, Dan Gillick, James Zhang, and DanKlein.
2006.
Why Generative Phrase Models Un-derperform Surface Heuristics.
In Proceedings ofthe Workshop on Statistical Machine Translation,pages 31?38, New York City, June.John DeNero, Alexandre Buchard-Co?te?, and DanKlein.
2008.
Sampling Alignment Structure undera Bayesian Translation Model.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 314?323, Honolulu,October.Nan Duan, Mu Li, and Ming Zhou.
2012.
Forcedderivation tree based model training to statisticalmachine translation.
In Proceedings of the 2012Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natu-ral Language Learning, pages 445?454, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Xiaodong He and Li Deng.
2012.
Maximum ExpectedBLEU Training of Phrase and Lexicon TranslationModels.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 292?301, Jeju, Republic of Korea, Jul.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisti-cal Phrase-Based Translation.
In Proceedings of the2003 Meeting of the North American chapter of theAssociation for Computational Linguistics (NAACL-03), pages 127?133, Edmonton, Alberta.Philipp Koehn.
2004.
Statistical Significance Testsfor Machine Translation Evaluation.
In Proc.
of theConf.
on Empirical Methods for Natural LanguageProcessing (EMNLP), pages 388?395, Barcelona,Spain, July.Percy Liang, Alexandre Buchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An End-to-End Discrimina-tive Approach to Machine Translation.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th annual meetingof the Association for Computational Linguistics,pages 761?768, Sydney, Australia.Daniel Marcu and William Wong.
2002.
A Phrase-Based, Joint Probability Model for Statistical Ma-chine Translation.
In Proc.
of the Conf.
on Em-pirical Methods for Natural Language Processing(EMNLP), pages 133?139, Philadelphia, PA, July.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 220?224, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Robert C. Moore and Chris Quirk.
2007.
AnIteratively-Trained Segmentation-Free PhraseTranslation Model for Statistical Machine Transla-tion.
In Proceedings of the Second Workshop onStatistical Machine Translation, pages 112?119,Prague, June.Markos Mylonakis and Khalil Sima?an.
2008.Phrase Translation Probabilities with ITG Priors andSmoothing as Learning Objective.
In Proceedingsof the 2008 Conference on Empirical Methods inNatural Language Processing, pages 630?639, Hon-olulu, October.Graham Neubig, Taro Watanabe, Eiichiro Sumita,Shinsuke Mori, and Tatsuya Kawahara.
2011.
Anunsupervised model for joint phrase alignment andextraction.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1,HLT ?11, pages 632?641, Stroudsburg, PA, USA.Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449,December.Franz J. Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proc.
of the41th Annual Meeting of the Association for Compu-tational Linguistics (ACL), Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
IBM ResearchReport RC22176 (W0109-022), IBM Research Di-vision, Thomas J. Watson Research Center, P.O.
Box218, Yorktown Heights, NY 10598, September.Markus Saers and Dekai Wu.
2011.
Principled induc-tion of phrasal bilexica.
In Proceedings of the 15thInternational Conference of the European Associa-tion for Machine Translation, pages 313?320, May.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proceedings of the 7th Conference of the As-sociation for Machine Translation in the Americas,pages 223?231, Cambridge, Massachusetts, USA,August.318Joern Wuebker, Arne Mauser, and Hermann Ney.2010.
Training phrase translation models withleaving-one-out.
In Proceedings of the 48th AnnualMeeting of the Assoc.
for Computational Linguistics,pages 475?484, Uppsala, Sweden, July.Joern Wuebker, Matthias Huck, Stephan Peitz, MalteNuhn, Markus Freitag, Jan-Thorsten Peter, SaabMansour, and Hermann Ney.
2012a.
Jane 2: Opensource phrase-based and hierarchical statistical ma-chine translation.
In International Conference onComputational Linguistics, pages 483?491, Mum-bai, India, December.Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.2012b.
Leave-One-Out Phrase Model Trainingfor Large-Scale Deployment.
In Proceedings ofthe NAACL 2012 Seventh Workshop on Statisti-cal Machine Translation, pages 460?467, Montreal,Canada, June.Richard Zens.
2008.
Phrase-based Statistical MachineTranslation: Models, Search, Training.
Ph.D. the-sis, Computer Science Department, RWTH Aachen?
University of Technology, Germany, February.319
