Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 353?356,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPUpdating a Name Tagger Using Contemporary Unlabeled DataCristina MotaL2F (INESC-ID) & IST & NYURua Alves Redol 91000-029 Lisboa Portugalcmota@ist.utl.ptRalph GrishmanNew York UniversityComputer Science DepartmentNew York NY 10003 USAgrishman@cs.nyu.eduAbstractFor many NLP tasks, including named en-tity tagging, semi-supervised learning hasbeen proposed as a reasonable alternativeto methods that require annotating largeamounts of training data.
In this paper,we address the problem of analyzing newdata given a semi-supervised NE taggertrained on data from an earlier time pe-riod.
We will show that updating the unla-beled data is sufficient to maintain qualityover time, and outperforms updating thelabeled data.
Furthermore, we will alsoshow that augmenting the unlabeled datawith older data in most cases does not re-sult in better performance than simply us-ing a smaller amount of current unlabeleddata.1 IntroductionBrill (2003) observed large gains in performancefor different NLP tasks solely by increasing thesize of unlabeled data, but stressed that for otherNLP tasks, such as named entity recognition(NER), we still need to focus on developing toolsthat help to increase the size of annotated data.This problem is particularly crucial when pro-cessing languages, such as Portuguese, for whichthe labeled data is scarce.
For instance, in the firstNER evaluation for Portuguese, HAREM (San-tos and Cardoso, 2007), only two out of the nineparticipants presented systems based on machinelearning, and they both argued they could haveachieved significantly better results if they hadlarger training sets.Semi-supervised methods are commonly cho-sen as an alternative to overcome the lack of an-notated resources, because they present a goodtrade-off between amount of labeled data neededand performance achieved.
Co-training is one ofthose methods, and has been extensively studied inNLP (Nigam and Ghani, 2000; Pierce and Cardie,2001; Ng and Cardie, 2003; Mota and Grishman,2008).
In particular, we showed that the perfor-mance of a name tagger based on co-training de-cays as the time gap between training data (seedsand unlabeled data) and test data increases (Motaand Grishman, 2008).
Compared to the originalclassifier of Collins and Singer (1999) that usesseven seeds, we used substantially larger seed sets(more than 1000), which raises the question ofwhich of the parameters (seeds or unlabeled data)are causing the performance deterioration.In the present study, we investigated two mainquestions, from the point of view of a developerwho wants to analyze a new data set, given an NEtagger trained with older data.
First, we studiedwhether it was better to update the seeds or theunlabeled data; then, we analyzed whether usinga smaller amount of current unlabeled data couldbe better than increasing the amount of unlabeleddata drawn from older sources.
The experimentsshow that using contemporary unlabeled data isthe best choice, outperforming most experimentswith larger amounts of older unlabeled data andall experiments with contemporary seeds.2 Contemporary labeled data in NLPThe speech community has been defending forsome time now the idea of having similar tem-poral data for training and testing automaticspeech recognition systems for broadcast news.Most works focus on improving out-of-vocabulary(OOV) rates, to which new names contributesignificantly.
For instance, Palmer and Osten-dorf (2005) aiming at reducing the error rate dueto OOV names propose to generate offline namelists from diverse sources, including temporallyrelevant news texts; Federico and Bertoldi (2004),and Martins et al (2006) propose to daily adaptthe statistical language model of a broadcast353news transcription system, exploiting contempo-rary newswire texts available on the web; Auzanneet al (2000) proposed a time-adaptive languagemodel, studying its impact over a period of fivemonths on the reduction of OOV rate, word errorrate and retrieval accuracy on a spoken documentretrieval system.Concerning variations over longer periods oftime, we observed that the performance of a semi-supervised name tagger decays over a period ofeight years, which seems to be directly relatedwith the fact that the texts used to train and test thetagger also show a tendency to become less simi-lar over time (Mota and Grishman, 2008); Batistaet al (2008) also observed a decaying tendency inthe performance of a system for recovering capi-talization over a period of six years, proposing toretrain a MaxEnt model using additional contem-porary written texts.3 Name tagger overviewWe assessed the name tagger described in Motaand Grishman (2008) to recognize names of peo-ple, organizations and locations.
The tagger isbased on the co-training NE classifier proposedby Collins and Singer (1999), and is comprisedof several components organized sequentially (cf.Figure 1).!"#$%$"&$'()*#%+,-./01$"&$23(4"5"6%'()*#%!"&$%7)$8%/5(##)9"6%,-!"&$%7)$8%:1/5(##)9"6%,-;6"1$)9/($)01<5(##)9/($)01'*0=(>($)01?"($:*"%"&$*(/$)01'()*#%+#="55)1>%@"($:*"#.%/01$"&$:(5%@"($:*"#2<0A$*()1)1>B="55)1>%C%/01$"&$:(5%*:5"#B""6#%D15(4"5"6%$"&$!"#$%&'!
()%&%&'Figure 1: NE tagger architecture4 Data setsCETEMPu?blico (Rocha and Santos, 2000) is aPortuguese journalistic corpus with 180 millionwords that spans eight years of news, from 1991to 1998.
The minimum size of epoch (time spanof data set) available for analysis is a six-monthperiod, corresponding either to the first half of theyear or the second.The data sets were created using the first 8256extracts1 within each six-month period of the pol-itics section of the corpus: the first 192 are used tocollect seeds, the next 208 extracts are used as testsets and the remaining 7856 are used to collect theunlabeled examples.
The seeds correspond to thefirst 1150 names occurring in those extracts.
Fromthe list of unlabeled examples obtained after theNE identification stage, only the first 41226 exam-ples of each epoch were used to bootstrap in theclassification stage.5 ExperimentsWe denote by S, U and T , respectively, the seed,unlabeled and test texts, and by (Si, Uj, Tk) atraining-test configuration, where 91a ?
i, j, k ?98b, i.e., epochs i, j and k vary between the firsthalf of 1991 (91a) and the second half of 1998(98b).
For instance, the training-test configuration(Si=91a...98b, Ui=91a...98b, Tj=98b) represents thetraining-test configuration where the test set wasdrawn from epoch 98b, and the tagger was trainedin turn with seeds and unlabeled data drawn fromthe same epoch i that varied from 91a to 98b.5.1 Do we need contemporary labeled data?In order to understand whether it is better to labelexamples falling within the epoch of the test setor to keep using old labeled data while bootstrap-ping with contemporary unlabeled data, we fixedthe test set to be within the last epoch of the inter-val (98b), and performed backward experiments,i.e., we varied the epoch of either the seeds or theunlabeled data backwards.
The choice of fixingthe test within the last epoch of the interval is theone that most approximates a real situation whereone has a tagger trained with old data and wants toprocess a more recent text.Figure 2 shows the results for both experiments,where (Sj=98b, Ui=91a...98b, Tj=98b) represents theexperiment where the test was within the sameepoch as the seeds and the unlabeled data weredrawn from a single, variable, epoch in turn, and(Si=91a...98b, Uj=98b, Tj=98b) represents the exper-iment where the test was within the epoch of the1Extracts are typically two paragraphs.354unlabeled data and the seeds were drawn in turnfrom each of the epochs; the graphic also showsthe baseline backward training (varying the epochof both the seeds and the unlabeled data together).0.740.760.780.800.82Training epochF?measure(i,i,98b)(98b,i,98b)(i,98b,98b)91a91b92a92b93a93b94a94b95a95b96a96b97a97b98a98bFigure 2: F-measure over time for test set 98b withconfigurations: (Si=91a...98b, Ui=91a...98b, Tj=98b),(Sj=98b, Ui=91a...98b, Tj=98b), and (Si=91a...98b,Uj=98b, Tj=98b)As can be seen, there is a small gain in perfor-mance by using seeds within the epoch of the testset, but the decay is still observable as we increasethe time gap between the unlabeled data and thetest set.
On the contrary, if we use unlabeled datawithin the epoch of the test set, we hardly seea degradation trend as the time gap between theepochs of seeds and test set is increased.An examination of the results shows that, forinstance, Sendero Luminoso received the correctclassification of organization when the tagger istrained with unlabeled data drawn from the sameepoch, but is incorretly classified as person whentrained with data that is not contemporary with thetest set.
Even though that name is not a seed in anyof the cases, it occurs twice in good contexts fororganization in unlabeled data contemporary withthe test set (l?
?der do Sendero Luminoso/leader ofthe Shining Path and acc?o?es do Sendero Lumi-noso/actions of the Shining Path), while it doesnot occur in the unlabeled data that is not contem-porary.
Given that both the name spelling and thecontext in the test set, o messianismo do peruanoSendero Luminoso/the messianism of the PeruvianShining Path, are insufficient to assign a correct la-bel, the occurrence of the name in the contempo-rary unlabeled data contributes to its correct clas-sification in the test set.5.2 Is more older unlabeled data better?The second question we addressed was whetherhaving more older unlabeled data could result inbetter performance than less data but within theepoch of the test set.
In this case, we conductedtwo backward experiments, augmenting the un-labeled data backwards with older data than thetest set (98b), starting in the previous epoch (98a):in the first experiment, the seeds were within thesame epoch as the test set, and in the second ex-periment the seeds were within the same epoch asthe unlabeled set being added.
This corresponds toconfigurations (Sj=98b, U ?i=91a...98a, Tj=98b) and(Si=91a...98a, U ?i=91a...98a, Tj=98b), respectively,where U ?i=?98ak=iUk.In Figure 3, we show the result of these con-figurations together with the result of the back-ward experiment corresponding to configuration(Si=91a...98b, Uj=98b, Tj=98b), also represented inFigure 2.
We note that, in the case of the formerexperiments, the size of the unlabeled examples isincreasing in the direction 98a to 91a.0.770.780.790.800.810.820.83Training epochF?measure(i,98b,98b)(i,u[i,...,98a],98b)(98b,u[i,...,98a],98b)91a91b92a92b93a93b94a94b95a95b96a96b97a97b98a98bFigure 3: F-measure for test set 98b withconfigurations (Si=91a...98b, Uj=98b, Tj=98b),(Sj=98b, U ?i=91a...98a, Tj=98b) and (Si=91a...98a,U?i=91a...98a, Tj=98b), where U ?i=?98ak=iUkAs can be observed, increasing the size of theunlabeled data does not necessarily result in bet-ter performance: for both choices of seeds, perfor-mance sometimes improves, sometimes worsens,as the unlabeled data grows (following the curves355from right to left).Furthermore, the tagger trained with more unla-beled data in most cases did not outperform thetagger trained with less unlabeled data selectedfrom the epoch of the test set.6 Discussion and future directionsWe conducted experiments varying the epoch ofseeds and unlabeled data of a named entity taggerbased on co-training.
We observed that the per-formance decay resulting from increasing the timegap between training data (seeds and unlabeled ex-amples) and the test set can be slightly attenuatedby using the seeds contemporary with the test set.The gain is larger if one uses older seeds and con-temporary unlabeled data, a strategy that, in mostof the experiments, results in better performancethan using increasing sizes of older unlabeled data.These results suggest that we may not need tolabel new data nor train our tagger with increasingsizes of data, as long as we are able to train it withunlabeled data time compatible with the test set.In the future, one issue that needs clarification iswhy bootstraping from contemporary labeled datahad so little influence on the performance of co-training, and if other semi-supervised approchesare also sensitive to this question.AcknowledgmentThe first author?s research work was funded byFundac?a?o para a Cie?ncia e a Tecnologia through adoctoral scholarship (ref.
: SFRH/BD/3237/2000).ReferencesCe?dric Auzanne, John S. Garofolo, Jonathan G. Fiscus,and William M. Fisher.
2000.
Automatic languagemodel adaptation for spoken document retrieval.
InProceedings of RIAO 2000 Conference on Content-Based Multimedia Information Access.Fernando Batista, Nuno Mamede, and Isabel Trancoso.2008.
Language dynamics and capitalization usingmaximum entropy.
In Proceedings of ACL-08: HLT,Short Papers, pages 1?4, Columbus, Ohio, June.
As-sociation for Computational Linguistics.Eric Brill.
2003.
Processing natural language with-out natural language processing.
In CICLing, pages360?369.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
InProceedings of the Joint SIGDAT Conference onEMNLP.Marcello Federico and Nicola Bertoldi.
2004.
Broad-cast news lm adaptation over time.
ComputerSpeech & Language, 18(4):417?435.Ciro Martins, Anto?nio Teixeira, and Joa?o Neto.
2006.Dynamic vocabulary adaptation for a daily andreal-time broadcast news transcription system.
InIEEE/ACL Workshop on Spoken Language Technol-ogy, Aruba.Cristina Mota and Ralph Grishman.
2008.
Is this NEtagger getting old?
In Proceedings of the SixthInternational Language Resources and Evaluation(LREC?08), Marrakech, Morocco, may.Vincente Ng and Claire Cardie.
2003.
Weakly super-vised natural language learning without redundantviews.
In NAACL?03: Proceedings of the 2003 Con-ference of the North American Chapter of the As-sociation for Computational Linguistics on HumanLanguage Technology, pages 94?101, Morristown,NJ, USA.
ACL.Kamal Nigam and Rayid Ghani.
2000.
Analyzingthe effectiveness and applicability of co-training.
InProceedings of CIKM, pages 86?93.David D. Palmer and Mari Ostendorf.
2005.
Improv-ing out-of-vocabulary name resolution.
ComputerSpeech & Language, 19(1):107?128.David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2001).Paulo Rocha and Diana Santos.
2000.
Cetempu?blico:Um corpus de grandes dimenso?es de linguagemjornal?
?stica portuguesa.
In Maria das Grac?asVolpe Nunes, editor, Actas do V Encontro para oprocessamento computacional da l?
?ngua portuguesaescrita e falada PROPOR 2000, pages 131?140, At-ibaia, Sa?o Paulo, Brasil.Diana Santos and Nuno Cardoso, editors.
2007.
Re-conhecimento de entidades mencionadas em por-tugue?s: Documentac?a?o e actas do HAREM, aprimeira avaliac?a?o conjunta na a?rea.
Linguateca,12 de Novembro.356
