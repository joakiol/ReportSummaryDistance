The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 157?162,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsA Comparison of Greedy and Optimal Assessment of Natural LanguageStudent Input Using Word-to-Word Similarity MetricsVasile Rus Mihai LinteanDepartment of Computer Science Department of Computer ScienceThe University of Memphis The University of MemphisMemphis, TN 38152 Memphis, TN 38152vrus@memphis.edu mclinten@memphis.eduAbstractWe present in this paper a novel, optimalsemantic similarity approach based onword-to-word similarity metrics to solvethe important task of assessing naturallanguage student input in dialogue-basedintelligent tutoring systems.
The optimalmatching is guaranteed using the sailorassignment problem, also known as the jobassignment problem, a well-knowncombinatorial optimization problem.
Wecompare the optimal matching method witha greedy method as well as with a baselinemethod on data sets from two intelligenttutoring systems, AutoTutor and iSTART.IntroductionWe address in this paper the important task ofassessing natural language student input indialogue-based tutoring systems where the primaryform of interaction is natural language.
Studentsprovide their responses to tutor?s requests bytyping or speaking their responses.
Therefore, indialogue-based tutoring systems understandingstudents?
natural language input becomes a crucialstep towards building an accurate student model,i.e.
assessing the student?s level of understanding,which in turn is important for optimum feedbackand scaffolding and ultimately impacts thetutoring?s effectiveness at inducing learning gainson the student user.We adopt a semantic similarity approach toassess students?
natural language input inintelligent tutoring systems.
The semanticsimilarity approach to language understandingderives the meaning of a target text, e.g.
a studentsentence, by comparing it with another text whosemeaning is known.
If the target text is semanticallysimilar to the known-meaning text then we knowthe target?s meaning as well.Semantic similarity is one of the two majorapproaches to language understanding, a centraltopic in Artificial Intelligence.
The alternativeapproach is full understanding.
The fullunderstanding approach is not scalable due toprohibitive costs to encode world and domainknowledge which are needed for full understandingof natural language.To illustrate the problem of assessing naturallanguage student input in dialogue-based tutoringsystems using a semantic similarity approach, weconsider the example below from experiments withAutoTutor (Graesser et al, 2005), a dialogue-basedtutoring system.Expert Answer: The force of the earth's gravity,being vertically down, has no effect on the object'shorizontal velocityStudent Input: The horizontal component of motionis not affected by vertical forcesIn this example, the student input, also calledcontribution, is highly similar to the correct expertanswer, called expectation, allowing us to concludethat the student contribution is correct.
A correctresponse typically triggers positive feedback fromthe tutor.
The expert answer could also be an157anticipated wrong answer, usually called amisconception.
A student contribution similar to amisconception would trigger a misconceptioncorrection strategy.We model the problem of assessing naturallanguage student input in tutoring systems as aparaphrase identification problem (Dolan et al,2004).
The student input assessment problem hasbeen also modeled as a textual entailment task inthe past (Rus & Graesser, 2006).Our novel method to assess a studentcontribution against an expert-generated answerrelies on the compositionality principle and thesailor assignment algorithm that was proposed tosolve the assignment problem, a well-knowncombinatorial optimization problem.
The sailorassignment algorithm optimally assigns sailors toships based on the fitness of the sailors?
skills tothe ships?
needs [7, 8].
In our case, we would liketo optimally match words in the student input (thesailors) to words in the expert-generated answer(the ships) based on how well the words in studentinput (the sailors) fit the words in the expertanswer (the ships).
The fitness between the wordsis nothing else but their similarity according tosome metric of word similarity.
We use theWordNet word-to-word similarity metrics(Pedersen et al, 2004) and Latent SemanticAnalysis (Landauer et al, 2007).The methods proposed so far that rely on theprinciple of compositionality to compute thesemantic similarity of longer texts have beenprimarily greedy methods (Corley & Mihalcea,2005; Lintean & Rus, 2012).
To the best of ourknowledge, nobody proposed an optimal solutionbased on the principle of compositionality andword-to-word similarity metrics for the studentinput assessment problem.
It is important to notethat the optimal method proposed here is generallyapplicable to compute the similarity of any texts.We provide experimental results on two datasetsprovided to us by researchers developing twoworld-class dialogue-based tutoring systems:AutoTutor (Graesser et al, 2005) and iSTART(McNamara et al, 2004).BackgroundIt is beyond the scope of this work to offer anexhaustive overview of methods proposed so far tohandle the task of assessing natural languagestudent input in intelligent tutoring systems.
Weonly describe next methods that are most relevantto our work.Assessing student?s contributions in dialogue-based tutoring systems has been approached eitheras a paraphrase identification task (Graesser et al,2005), i.e.
the task was to assess how similarstudent contributions were to expert-generatedanswers, or as an entailment task (Rus & Graesser,2006), in which case the task was to assess whetherstudent contributions were entailed by expert-generated answers.
The expert answers wereassumed to be true.
If a correct expert answerentailed a student contribution then thecontribution was deemed to be true as well.Latent Semantic Analysis (LSA; Landauer et al,2007) has been used to evaluate studentcontributions during the dialog between the studentby Graesser and colleagues (2005).
In LSA themeaning of a word is represented by a reduced-dimensionality vector derived by applying analgebraic method, called Singular ValueDecomposition (SVD), to a term-by-documentmatrix built from a large collection of documents.A typical dimensionality of an LSA vector is 300-500 dimensions.
To compute the similarity of twowords the cosine of the word?s corresponding LSAvector is computed, i.e.
the normalized dot-product.
A typical extension of LSA-based wordsimilarity to computing the similarity of twosentences (or even larger texts) is to use vectoralgebra to generate a single vector for each of thesentences (by adding up the individual words?
LSAvectors) and then compute the cosine between theresulting sentence vectors.
Another approachproposed so far to compute similarities betweenindividual words in the two sentences, greedilyselects for each word its best match, and then sumsthe individual word-to-word similarities in order tocompute the overall similarity score for the twosentences (Lintean & Rus, 2012).
We do reportresults with LSA using the latter approach forcomparison purposes.
Another reason is that onlythe latter approach allows the application of theoptimum matching method.Extending word-to-word similarity measures tosentence level and beyond has drawn increasinginterest in the last decade or so in the NaturalLanguage Processing community.
The interest hasbeen driven primarily by the creation ofstandardized data sets and corresponding shared158task evaluation campaigns (STECs) for the majortext-to-text semantic relations of entailment (RTE;Recognizing Textual Entailment corpus by Dagan,Glickman, & Magnini, 2005), paraphrase (MSR;Microsoft Research Paraphrase corpus by Dolan,Quirk, and Brockett, 2004), and more recently forelaboration (ULPC; User Language ParaphraseChallenge by McCarthy & McNamara, 2008).None of the existing methods for assessing thesimilarity of texts based on the compositionalprinciple and word-to-word similarity metrics haveproposed an optimum method.Beyond Word-to-Word Similarity MeasuresBased on the principle of compositionality, whichstates that the meaning of longer texts can becomposed from the meaning of their individualwords (which includes collocations in our casesuch as ?free fall?
), we can extend the word-to-word similarity metrics to compute the similarityof longer texts, e.g.
of sentences.In our work, we use a set of WordNet-basedsimilarity metrics as well as LSA.
We used thefollowing similarity measures implemented in theWordNet::Similarity package and described in(Pedersen et al, 2004): LCH (Leacock andChodorow), RES (Resnik), JCN (Jiang andConrath), LIN (Lin), PATH, and WUP (Wu andPalmer).
Some measures, e.g.
PATH, are path-based, i.e.
use paths of lexico-semantic relationsbetween concepts in WordNet, while some othersare gloss-based, that is, they use the text of thegloss or the definition of a concept in WordNet asthe source of meaning for the underlying concept.One challenge with the WordNet word-to-wordrelatedness measures is that they cannot be directlyapplied to larger texts such as sentences.
Theymust be extended to larger texts, which we did asdescribed later.Another challenge with the WordNet word-to-word similarity metrics is the fact that texts expressmeaning using words and not concepts.
To be ableto use the word-to-word related measures we mustmap words in sentences to concepts in WordNet.Thus, we are faced with a word sensedisambiguation (WSD) problem.
It is beyond thescope of our investigation to fully solve the WSDproblem, one of the hardest in the area of NaturalLanguage Processing.
Instead, we addressed theissue in two ways: (1) mapped the words in thestudent contribution and expert answer onto theconcepts corresponding to their most frequentsense, which is sense #1 in WordNet, and (2) mapthe words onto all the concepts corresponding toall the senses and then take the maximum of therelatedness scores for each pair of senses.
Becausethe ALL (all senses) method offered better resultsand because of space constraints we only reportresults with the ALL method in this paper.Greedy versus Optimal Semantic SimilarityMatchingThis section describes the greedy and optimalmatching methods to assess the similarity of twotexts based on word-to-word similarity metrics.We assume the two texts, T1 and T2, are twosentences and regard them as bags of words(syntactic information is ignored).The Greedy Method.
In the greedy method,each word in text T1 is paired with every word intext T2 and word-to-word similarity scores arecomputed according to some metric.
Themaximum similarity score between words in T1and any word in T2 is greedily retained regardlessof the best matching scores of the other words inT1.
The greedily-obtained scores are added upusing a simple or weighted sum which can then benormalized in different ways, e.g.
by dividing tothe longest text or to the average length of the twotexts.
The formula we used is given in equation 1.As one would notice, this formula is asymmetric,i.e.
score(T1,T2)?score(T2,T1).
The average ofthe two scores provides a symmetric similarityscore, more suitable for a paraphrase task, asshown in Equation 2.
In this paper, we do a simplenon-weighted sum, i.e.
all the words are equally-weighted with a weight of 1.The obvious drawback of the greedy method isthat it does not aim for a global maximumsimilarity score.
The optimal method describednext solves this issue.????
?
?
?11 2)(),(max*)()2,1(TvTv TwvweightwvsimwordvweightTTscoreEquation 1.
Asymmetric semantic similarity scorebetween texts T1 and T2.2)1,2()2,1()2,1( TTscoreTTscoreTTsimScore ?
?Equation 2.
Symmetric semantic similarity scorebetween texts T1 and T2.159Optimal Matching.
The optimal assignmentproblem is one of the fundamental combinatorialoptimization problems and consists of finding amaximum weight matching in a weighted bipartitegraph.Given a weighted complete bipartite graph, where edge  has weight, find a matching M from X to Y withmaximum weight.An application is about assigning a group ofworkers, e.g.
sailors, to a set of jobs (on ships)based on the expertise level, measured by ,of each worker at each job.
By adding dummyworkers or jobs we may assume that X and Y havethe same size, n, and can viewed asand Y = .
In thesemantic similarity case, the workers and jobs arewords from the two sentences to be compared andthe weight  is the word-to-word similaritybetween word x and y in the two sentences,respectively.The assignment problem can be stated as findinga permutation  of {1, 2, 3, ?
, n} for whichis maximum.
Such an assignmentis called optimum assignment.
An algorithm, theKuhn-Munkres method (Kuhn, 1955), has beenproposed that can find a solution to the optimumassignment problem in polynomial time.
For spacereasons, we do not show here the algorithm indetail.To illustrate the difference between the twomethods, we use the two sentence fragmentsshown in Figure 1.
A greedy method would pairmotion with motion (similarity score of 1.00) asthat is the maximum similarity between motion andany word in the opposite sentence and accelerationis paired with speed (similarity score of 0.69) for atotal score of 1.69 (before normalization).
Anoptimal matching would yield an overall score of1.70 by pairing motion in the first sentence withspeed (similarity of 0.75) and acceleration withmotion (similarity of 0.95).Figure 1.
Examples of two sentence fragments andword-to-word similarity scores for each of the wordpairs across sentences.
The bold arrows show optimalpairing.Experimental Setup and ResultsWe present in this section the datasets we usedin our experiments and the results obtained.
As wealready mentioned, we use two datasets containingreal student answers from two dialogue-basedtutoring systems: AutoTutor (Graesser et al, 2005)and iSTART (McNamara et al, 2004).The AutoTutor dataset contains 125 studentcontribution ?
expert answer pairs and the correctparaphrase judgment, TRUE or FALSE, asassigned by human experts.
The target domain isconceptual physics.
One expert physicist rated thedegree to which particular speech acts expressedduring AutoTutor training matched particularexpert answers.
These judgments were made on asample of 25 physics expectations (i.e., correctexpert answers) and 5 randomly sampled studentanswers per expectation, yielding a total of 125pairs of expressions.
The learner answers werealways responses to the first hint for thatexpectation.
The E-S pairs were graded by Physicsexperts on a scale of 1-4 (4 being perfect answer).This rubric could be mapped onto a binary TRUE-FALSE rubric: scores 3 and 4 equal a TRUEdecision and 1 and 2 equal a FALSE decision.
Weended up with 36 FALSE and 89 TRUE entailmentpairs, i.e.
a 28.8% versus 71.2% split (as comparedto the 50-50% split of RTE data).The iSTART data set, also known as the UserLanguage Paraphrase Corpus (McCarty &McNamara, 2008) comprises annotations ofparaphrase relations between student responses andideal answers.
The corpus contains 1998 pairscollected from previous student iSTART sessionsand is divided into training (1499 instances) andtesting (499 instances) subsets.
The training subsetcontains 54% positive instances while testingcontains 55% positive instances.
The iSTARTtexts represent high school students?
attempts toself-explain biology textbook texts.To evaluate the performance of our methods, wecompare the methods?
judgments with the expertjudgments.
The percentage of matching judgmentsprovides the accuracy of the run, i.e.
the fraction ofcorrect responses.
We also report kappa statisticswhich indicate agreement between our methods?output and the human-expert judgments for each1.00speed                  motion0.950.75motion              acceleration    Sentence A:Sentence B:0.69160instance while taking into account chanceagreement.Tables 1, 2, and 3 summarize the results on theoriginal AutoTutor data (from Rus & Graesser,2006; Table 1), the re-annotated AutoTutor data bya second rater with inter-annotator agreement of0.606 (Table 2), and the ULPC test subset (Table3).
For the ULPC corpus the methods have beentrained on the training subset, an optimumthreshold has been learned (such that scores abovethe threshold mean TRUE paraphrases) which isthen used on the test data.
Since the AutoTutordataset is small, we only report results on it as awhole, i.e.
only training.
We report for each corpusa baseline method of guessing all the time thedominant class in the dataset (which is TRUEparaphrase for all three datasets), a pure greedymethod (Greedy label in the first column of thetables), a greedy method applied to the wordspaired by the optimum method (optGreedy), andthe results with the optimum matching method(Optimum).Overall, the optimum method offered betterperformance in terms of accuracy and kappastatistics.
The greedy method yields results that areclose.
In fact, when analyzed as raw scores insteadof binary decisions (as is the case when computingaccuracy) the greedy raw score are on average verysimilar to the optimum scores.
For instance, for theLSA word-to-word similarity metric whichprovided best accuracy results on the ULPCdataset (accuracy=.643 for optimum and .615 forgreedy), the average raw scores are .563 (usingoptimum matching) and .567 (using greedymatching).
One reason for why they are so closedis that in optimum matching we have one-to-oneword matches while in the greedy matching many-to-one matches are possible.
That is, two words vand w from text T1 can be matched to same word yin text T2 in the greedy method.
If we enforce thatonly one-to-one matches are possible in the greedmethod as in the optimum method, then we obtainthe optGreedy method.
The optGreedy methoddoes work better than the pure greedy method(Greedy in the tables).Another reason for why the raw scores are closefor greedy and optimum is the fact that studentinput and expert answers in both the AutoTutorand ULPC corpora are sharing many words incommon (>.50).
This is the case because thedialogue is highly contextualized around a given,e.g.
physics, problem.
In the answer, both studentsand experts refer to the entities and interactions inthe problem statement which leads to highidentical word overlap.
Identical words lead toperfect word-to-word similarity scores (=1.00)increasing the overall similarity score of the twosentences in both the greedy and optimum method.Conclusions and Future WorkOverall, the optimum method offers betterperformance in terms of accuracy and kappastatistics than greedy and baseline methods.The way we modeled the student assessmentproblem in this paper cannot deal with some typeof responses.
For instance, sometimes students?responses are mixed.
Instead of being TRUE orFALSE responses, they contain both a correct partand an incorrect part as illustrated in the examplebelow (Expert Answer provided for reference).Expert Answer: The object continues to have aconstant horizontal velocity component after it isreleased that is the same as the person horizontalvelocity at the time of dropping the object.Student Input: The horizontal velocity will decreasewhile the vertical velocity increases.Such a mixed student input should trigger amixed feedback from the system: ?You arepartially right!
The vertical velocity will increasebut not the horizontal velocity.
Can you explainwhy??
We plan to address this problem in thefuture by proposing a more sophisticated model.We also plan to answer the question of how muchlexical versus world and domain knowledge eachof these measures can capture.
For instance,WordNet can be viewed as capturing some worldknowledge as the concepts?
definitions provideinformation about the world.
However, it might beless rich in capturing domain specific knowledge.Indeed, WordNet seems to capture less domainknowledge at first sight.
For instance, thedefinition of acceleration in WordNet does notlink it to the concept of force but physics laws do,e.g.
Newton?s second law of motion.AcknowledgmentsThis research was supported in part by the Institutefor Education Sciences (award R305A100875).The opinions and findings reported in this paperare solely the authors?.161ID RES LCH JCN LSA Path Lin WUPBaseline .712 .712 .712 .712 .712 .712 .712Greedy .736/.153 .752/.204 .760/.298 .744/.365 .752/.221 .744/.354 .760/.298optGreedy .744/187 .752/.221 .760/.298 .744/.306 .752/.309 .752/.204 .784/.349Optimal .744/.236 .752/.204 .760/.298 .744/.221 .752/.334 .752/.204 .784*/.409*Table 1.
Accuracy/kappa on AutoTutor data (* indicates statistical significance over the baseline method at p<0.005 level).ID RES LCH JCN LSA Path Lin WUPBaseline .568 .568 .568 .568 .568 .568 .568Greedy .616/.137 .608/.117 .624/.214 .632/.256 .624/.161 .608/1.34 .624/.181optGreedy .632/.192 .632/.207 .632/.229 .624/.218 .632*/.177* .624/.165 .648*/.235*Optimal .624*/.153* .624/.169 .640*/.208* .640/.283 .624/.165 .624*/.148 .624/.173Table 2.
Accuracy/kappa on AutoTutor data with user annotations (* indicates statistical significance over the baselinemethod at p<0.005 level).ID RES LCH JCN LSA Path Lin WUPBaseline .547 .547 .547 .547 .547 .547 .547Greedy .619/.196 .619/.201 .629/.208 .615/.183 .635/.221 .629/.214 .621/.201optGreedy .621/.195 .615/.201 .629/.208 .643/.237 .623/.197 .619/.196 .613/.190Optimal .625/.205 .615/.196 .629/.208 .643/.237 .633/.215 .623/.203 .625/.214Table 3.
Accuracy/kappa on ULPC test data (all results are statistically different from the baseline at p<0.005 level).ReferencesCourtney Corley and Rada Mihalcea.
2005.
Measures ofText Semantic Similarity.
In Proceedings of the ACLworkshop on Empirical Modeling of SemanticEquivalence, Ann Arbor, MI, June 2005.Ido Dagan, Oren Glickman, and Bernando Magnini.2005.
The PASCAL recognizing textual entailmentchallenge.
In Proceedings of the PASCALWorkshop.Bill W. Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrasecorpora: Exploiting massively parallel news sources.In Proceedings of the 20th International Conferenceon Computational Linguistics, Geneva, Switzerland.Arthur C. Graesser, Andrew Olney, Brian C. Hayes, andPatrick Chipman.
2005.
Autotutor: A cognitivesystem that simulates a tutor that facilitates learningthrough mixed-initiative dialogue.
In CognitiveSystems: Human Cognitive Models in SystemDesign.
Mahwah: Erlbaum.Harold W. Kuhn.
1955.
"The Hungarian Method for theassignment problem", Naval Research LogisticsQuarterly, 2:83?97, 1955.
Kuhn's originalpublication.Thomas K. Landauer, Danielle S. McNamara, SimonDennis, and Walter Kintsch.
2007.
Handbook ofLatent Semantic Analysis.
Mahwah, NJ: Erlbaum.Mihai Lintean and Vasile Rus.
2012.
MeasuringSemantic Similarity in Short Texts through GreedyPairing and Word Semantics.
To be presented at TheTwenty-Fifth International FLAIRS Conference.Marco Island, Florida.Philip M. McCarty and Danielle S. McNamara.
2008.User-Language Paraphrase Corpus Challenge, online.Danielle S. McNamara, Irwin B. Levinstein, andChutima Boonthum.
2004. iSTART: interactivestrategy training for active reading and thinking.Behavioral Research Methods, Instruments, andComputers, 36(2).Ted Pedersen, Siddharth Patwardhan, and JasonMichelizzi.
2004.
WordNet::Similarity ?
Measuringthe Relatedness of Concepts.
In Proceedings of FifthAnnual Meeting of the North American Chapter ofthe Association for Computational Linguistics(NAACL-2004).Vasile Rus, and Arthur C. Graesser.
2006.
DeeperNatural Language Processing for Evaluating StudentAnswers in Intelligent Tutoring Systems,Proceedings of the Twenty-First National Conferenceon Artificial Intelligence (AAAI-06).162
