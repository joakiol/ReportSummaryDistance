Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 72?75,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPCombining a Two-step Conditional Random Field Model and a JointSource Channel Model for Machine TransliterationDong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku OonishiMasanobu Nakamura and Sadaoki FuruiDepartment of Computer ScienceTokyo Institute of Techonology{raymond,dixonp,thomas,oonishi,masa,furui}@furui.cs.titech.ac.jpAbstractThis paper describes our system for?NEWS 2009 Machine TransliterationShared Task?
(NEWS 2009).
We only par-ticipated in the standard run, which is adirect orthographical mapping (DOP) be-tween two languages without using anyintermediate phonemic mapping.
Wepropose a new two-step conditional ran-dom field (CRF) model for DOP machinetransliteration, in which the first CRF seg-ments a source word into chunks and thesecond CRF maps the chunks to a wordin the target language.
The two-step CRFmodel obtains a slightly lower top-1 ac-curacy when compared to a state-of-the-art n-gram joint source-channel model.The combination of the CRF model withthe joint source-channel leads to improve-ments in all the tasks.
The official re-sult of our system in the NEWS 2009shared task confirms the effectiveness ofour system; where we achieved 0.627 top-1 accuracy for Japanese transliterated toJapanese Kanji(JJ), 0.713 for English-to-Chinese(E2C) and 0.510 for English-to-Japanese Katakana(E2J) .1 IntroductionWith the increasing demand for machine transla-tion, the out-of-vocabulary (OOV) problem causedby named entities is becoming more serious.The translation of named entities from an alpha-betic language (like English, French and Spanish)to a non-alphabetic language (like Chinese andJapanese) is usually performed through transliter-ation, which tries to preserve the pronunciation inthe source language.For example, in Japanese, foreign words im-ported from other languages are usually writtenH a r r i n g t o n ?
?
?
?
?
English-to-JapaneseT i m o t h y ???
English-to-ChineseSource Name       Target Name          Noteti mo   xi                     Chinese Romanized writingha  ri n   to  n Japanese Romanized writingFigure 1: Transliteration examplesin a special syllabary called Katakana; in Chi-nese, foreign words accepted to Chinese are al-ways written by Chinese characters; examples aregiven in Figure 1.An intuitive transliteration method is to firstconvert a source word into phonemes, then find thecorresponding phonemes in the target language,and finally convert to the target language?s writ-ing system (Knight and Graehl, 1998; Oh et al,2006).
One major limitation of this method is thatthe named entities are usually OOVs with diverseorigins and this makes the grapheme-to-phonemeconversion very difficult.DOP is gaining more attention in the transliter-ation research community which is also the stan-dard evaluation of NEWS 2009.The source channel and joint source-channelmodels (Li et al, 2004) have been proposed forDOP, which try to model P (T |S) and P (T, S) re-spectively, where T and S denotes the words inthe target and source languages.
(Ekbal et al,2006) modified the joint source-channel model toincorporate different context information into themodel for the Indian languages.
Here we proposea two-step CRF model for transliteration, and theidea is to make use of the discriminative ability ofCRF.
For example, in E2C transliteration, the firststep is to segment an English name into alphabetchunks and after this step the number of Chinesecharacters is decided.
The second step is to per-form a context-dependent mapping from each En-glish chunk into one Chinese character.
Figure 1shows that this method is applicable to many other72transliteration tasks including E2C and E2J.Our CRF method and the n-gram joint source-channel model use different information in pre-dicting the corresponding Chinese characters andtherefore in combination better results are ex-pected.
We interpolate the two models linearlyand use this as our final system for NEWS 2009.The rest of the paper is organized as follows: Sec-tion 2 introduces our system in detail including thealignment and decoding modules, Section 3 ex-plains our experiments and finally Section 4 de-scribes conclusions and future work.2 System DescriptionOur system starts from a joint source channelalignment to train the CRF segmenter.
The CRFis used to re-segment and align the training data,and from this alignment we create a Weighted Fi-nite State Transducer (WFST) based n-gram jointsource-channel decoder and a CRF E2C converter.The following subsections explain the structure ofour system shown in Figure 2.N-gram joint source-channel AlignmentCRF segmenterN-gram WFST decoder CRF E2C converterEach pair in the training corpusNew AlignmentN-gram WFST decoderCRF E2C converterLinear combinationEach source name in the test corpusCRF segmenterTrainingTestingOutputFigure 2: System structure2.1 Theoretical background2.1.1 Joint source channel modelThe source channel model represents the condi-tional probability of target names given a sourcename P (T |S).
The joint source channel modelcalculates how the source words and target namesare generated simultaneously (Li et al, 2004):P (S, T ) = P (s1, s2, ..., sk, t1, t2, ..., tk)= P (< s, t >1, < s, t >2, ..., < s, t >k)=K?k=1P (< s, t >k | < s, t >k?11 ) (1)where, S = (s1, s2, ..., sk) and T =(t1, t2, ..., tk).2.1.2 CRFA CRF (Lafferty et al, 2001) is an undirectedgraphical model which assigns a probability to alabel sequence L = l1l2 .
.
.
lT , given an input se-quence C = c1c2 .
.
.
cT ,P (L|C) = 1Z(C)exp(T?t=1?k?kfk(lt, lt?1, C, t))(2)For the kth feature, fk denotes the feature functionand ?k is the parameter which controls the weight-ing.
Z(C) is a normalization term that ensure thedistribution sums to one.
CRF training is usuallyperformed through the L-BFGS algorithm (Wal-lach, 2002) and decoding is performed by Viterbialgorithm (Viterbi, 1967).
In this paper, we use anopen source toolkit ?crf++?1.2.2 N-gram joint source-channel alignmentTo calculate the probability in Equation 1, thetraining corpus needs to be aligned first.
We usethe Expectation-Maximization(EM) algorithm tooptimize the alignment A between the source Sand target T pairs, that is:A?
= arg maxAP (S, T,A) (3)The procedure is summarized as follows:1.
Initialize a random alignment2.
E-step: update n-gram probability3.
M-step: apply the n-gram model to realigneach entry in corpus4.
Go to step 2 until the alignment converges2.3 CRF alignment & segmentationThe performance of EM algorithm is often af-fected by the initialization.
Fortunately, we cancorrect mis-alignments by using the discriminativeability of the CRF.
The alignment problem is con-verted into a tagging problem that doesn?t requirethe use of the target words at all.
Figure 3 is anexample of a segmentation and alignment, wherethe labels B and N indicate whether the characteris in the starting position of the chunk or not.In the CRF method the feature function de-scribes a co-occurrence relation, and it is formally1crfpp.sourceforge.net73T i m o t h y ??
?T/B i/N m/B o/N t/B h/N y/NTi/?
mo/?
thy/?Figure 3: An example of the CRF segmenter for-mat and E2C converterdefined as fk(lt, lt?1, C, t) (Eq.
2).
fk is usually abinary function, and takes the value 1 when bothobservation ct and transition lt?1 ?
lt are ob-served.
In our segmentation tool, we use the fol-lowing features?
1.
Unigram features: C?2, C?1, C0, C1, C2?
2.
Bigram features:C?1C0, C0C1Here, C0 is the current character, C?1 and C1 de-note the previous and next characters and C?2 andC2 are the characters two positions to the left andright of C0.In the alignment process, we use the CRF seg-menter to split each English word into chunks.Sometimes a problem occurs in which the num-ber of chunks in the segmented output will not beequal to the number of Chinese characters.
In suchcases our solution is to choose from the n-best listthe top scoring segmentation which contains thecorrect number of chunks.In the testing process, we use the segmenter inthe similar way, but only take top-1 output seg-mented English chunks for use in the followingCRF E2C conversion.2.4 CRF E2C converterSimilar to the CRF segmenter, the CRF E2C con-verter has the format shown in Figure 3.
For thisCRF, we use the following features:?
1.
Unigram features: C?1, C0, C1?
2.
Bigram features:C?1C0, C0C1where C represents the English chunks and thesubscript notation is the same as the CRF seg-menter.2.5 N-gram WFST decoder for joint sourcechannel modelOur decoding approach makes use of WFSTs torepresent the models and simplify the develop-ment by utilizing standard operations such as com-position and shortest path algorithms.After the alignments are generated, the firststep is to build a corpus to train the translit-eration WFST.
Each aligned word is convertedto a sequence of transliteration alignment pairs?s, t?1 , ?s, t?2 , ... ?s, t?k, where each s can be achunk of one or more characters and t is assumedto be a single character.
Each of the pairs istreated as a word and the entire set of alignments isused to train an n-gram language model.
In theseevaluations we used the MITLM toolkit (Hsu andGlass, 2008) to build a trigram model with modi-fied Kneser-Ney smoothing.We then use the procedure described in (Caseiroet al, 2002) and convert the n-gram to a weightedacceptor representation where each input label be-longs to the set of transliteration alignment pairs.Next the pairs labels are broken down into the in-put and output parts and the acceptor is convertedto a transducer M .
To allow transliteration from asequence of individual characters, a second WFSTT is constructed.
T has a single state and for eachs a path is added to allow a mapping from thestring of individual characters.To perform the actual transliteration, the inputword is converted to an acceptor I which has onearc for each of the characters in the word.
I isthen combined with T and M according to O =I ?T ?M where ?
denotes the composition opera-tor.
The n?best paths are extracted from O by pro-jecting the output, removing the epsilon labels andapplying the n-shortest paths algorithm with de-terminization from the OpenFst Toolkit(Allauzenet al, 2007).2.6 Linear combinationWe notice that there is a significant difference be-tween the correct answers of the n-gram WFSTand CRF decoders.
The reason may be due tothe different information utilized in the two de-coding methods.
Since their performance levelsare similar, the overall performance is expectedto be improved by the combination.
From theCRF we compute the probability PCRF (T |S) andfrom the list of scores output from the n-gram de-coder we calculate the conditional probability ofPn?gram(T |S).
These are used in our combina-tion method according to:P (T |S) = ?PCRF (T |S)+(1??
)Pn?gram(T |S)(4)where ?
denotes the interpolation weight (0.3 inthis paper).743 ExperimentsWe use the training and development sets ofNEWS 2009 data in our experiments as detailedin Table 12.
There are several measure metrics inthe shared task and due to limited space in this pa-per we provide the results for top-1 accuracy.Task Training data size Test data sizeE2C 31961 2896E2J 23808 1509Table 1: Corpus introductionn-gram+CRFTask Alignment interpolationWFST CRFE2C 70.3 67.3 71.5E2J 44.9 44.8 46.7Table 2: Top-1 accuracies(%)The results are listed in Table 2.
For E2Ctask the top-1 accuracy of the joint source-channelmodel is 70.3% and 67.3% for the two-step CRFmodel.
After combining the two results togetherthe top-1 accuracy increases to 71.5% correspond-ing to a 1.2% absolute improvement over the state-of-the-art joint source-channel model.
Similarly,we get 1.8% absolute improvement for E2J task.4 Conclusions and future workIn this paper we have presented our new hybridmethod for machine transliteration which com-bines a new two-step CRF model with a state-of-the-art joint source-channel model.
In compari-son to the joint source-channel model the combi-nation approach achieved 1.2% and 1.8% absoluteimprovements for E2C and E2J task respectively.In the first step of the CRF method we onlyuse the top-1 segmentation, which may propagatetransliteration errors to the following step.
In fu-ture work we would like to optimize the 2-stepCRF jointly.
Currently, we are also investigatingminimum classification error (MCE) discriminanttraining as a method to further improve the jointsource channel model.2For the JJ task the submitted resultsare only based on the joint sourcechannel model.
Unfortunately, we wereunable to submit a combination resultbecause the training time for the CRFwas too long.AcknowledgmentsThe corpora used in this paper are from ?NEWS2009 Machine Transliteration Shared Task?
(Li etal., 2004; CJK, website)ReferencesKevin Knight and Jonathan Graehl.
1998.
MachineTransliteration, 1998 Association for Computa-tional Linguistics.Li Haizhou, Zhang Min and Su Jian.
2004.
A jointsource-channel model for machine transliteration,2004 Proceedings of the 42nd Annual Meeting onAssociation for Computational Linguistics.Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandy-opadhyay.
2006.
A modified joint source-channelmodel for transliteration, Proceedings of the COL-ING/ACL, pages 191-198.Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara.2006.
A comparison of different machine transliter-ation models , Journal of Artificial Intelligence Re-search, 27, pages 119-151.John Lafferty, Andrew McCallum, and FernandoPereira 2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data., Proceedings of International Confer-ence on Machine Learning, 2001, pages 282-289.Hanna Wallach 2002.
Efficient Training of Condi-tional Random Fields.
M. Thesis, University of Ed-inburgh, 2002.Andrew J. Viterbi 1967.
Error Bounds for Convolu-tional Codes and an Asymptotically Optimum De-coding Algorithm.
IEEE Transactions on Informa-tion Theory, Volume IT-13, 1967,pages 260-269.Bo-June Hsu and James Glass 2008.
Iterative Lan-guage Model Estimation: Efficient Data Structure& Algorithms.
Proceedings Interspeech, pages 841-844.Diamantino Caseiro, Isabel Trancosoo, Luis Oliveiraand Ceu Viana 2002.
Grapheme-to-phone usingfinite state transducers.
Proceedings 2002 IEEEWorkshop on Speech Synthesis.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut and Mehryar Mohri 2002.
OpenFst: AGeneral and Efficient Weighted Finite-State Trans-ducer Library.
Proceedings of the Ninth Interna-tional Conference on Implementation and Applica-tion of Automata, (CIAA 2007), pages 11-23.http://www.cjk.org75
