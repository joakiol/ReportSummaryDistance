2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 305?314,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUsing Supertags and Encoded Annotation Principles for ImprovedDependency to Phrase Structure ConversionSeth Kulick and Ann Bies and Justin MottLinguistic Data ConsortiumUniversity of PennsylvaniaPhiladelphia, PA 19104{skulick,bies,jmott}@ldc.upenn.eduAbstractWe investigate the problem of automaticallyconverting from a dependency representa-tion to a phrase structure representation, akey aspect of understanding the relationshipbetween these two representations for NLPwork.
We implement a new approach to thisproblem, based on a small number of su-pertags, along with an encoding of some ofthe underlying principles of the Penn Tree-bank guidelines.
The resulting system signifi-cantly outperforms previous work in such au-tomatic conversion.
We also achieve compara-ble results to a system using a phrase-structureparser for the conversion.
A comparison withour system using either the part-of-speech tagsor the supertags provides some indication ofwhat the parser is contributing.1 Introduction and MotivationRecent years have seen a significant increase ininterest in dependency treebanks and dependencyparsing.
Since the standard training and test set forEnglish parsing is a phrase structure (PS) treebank,the Penn Treebank (PTB) (Marcus et al, 1993; Mar-cus et al, 1994), the usual approach is to convert thisto a dependency structure (DS) treebank, by meansof various heuristics for identifying heads in a PStree.
The resulting DS representation is then usedfor training and parsing, with results reported on theDS representation.Our goal in this paper is to go in the reverse di-rection, from the DS to PS representation, by find-ing a minimal DS representation from which we canuse an approximate version of the principles of thePTB guidelines to reconstruct the PS.
Work in thisconversion direction is somewhat less studied (Xiaet al, 2009; Xia and Palmer, 2001), but it is stillan important topic for a number of reasons.
First,because both DS and PS treebanks are of current in-terest, there is an increasing effort made to createmulti-representational treebank resources with bothDS and PS available from the beginning, without aloss of information in either direction (Xia et al,2009).
Second, it is sometimes the case that it isconvenient to do annotation in a dependency repre-sentation (e.g., if the annotators are already famil-iar with such a representation), though the treebankwill in final form be either phrase-structure or multi-representational (Xia et al, 2009).However, our concern is somewhat different.
Weare specifically interested in experimenting with de-pendency parsing of Arabic as a step in the annota-tion of the Arabic Treebank, which is a phrase struc-ture treebank (Maamouri et al, 2011).
Although wecurrently use a phrase structure parser in this annota-tion pipeline, there are advantages to the flexibilityof being able to experiment with advances in pars-ing technology for dependency parsing.
We wouldlike to parse with a dependency representation of thedata, and then convert the parser output to a phrasestructure representation so that it can feed into theannotation pipeline.
Therefore, in order to make useof dependency parsers, we need a conversion fromdependency to phrase structure with very high accu-racy, which is the goal of this paper.While one of our underlying concerns is DS toPS conversion for Arabic, we are first focusing on305a conversion routine for the English PTB because itis so well-established and the results are easier tointerpret.
The intent is then to transfer this conver-sion algorithm work to the Arabic treebank as well.We expect this to be successful because the ATB hassome fundamental similarities to the PTB in spite ofthe language difference (Maamouri and Bies, 2004).As mentioned above, one goal in our DS to PSconversion work is to base it on a minimal DS rep-resentation.
By ?minimal?, we mean that it doesnot include information that is redundant, togetherwith our conversion code, with the implicit informa-tion in the dependency structure itself.
As discussedmore in Section 2.1, we aim to make our dependencyrepresentation simpler than ?hybrid?
representationssuch as Johansson and Nugues (2007).
The rea-son for our interest in this minimal representationis parsing.
We do not want to require the parser torecover such a complex dependency representations,when it is, in fact, unnecessary, as we believe our ap-proach shows.
The benefit of this approach can onlybe seen when this line of work is extended to ex-periments with parsing and Arabic conversion.
Thework described here is just the first step in this pro-cess.A conversion scheme, such as ours, necessarilyrelies on some details of the annotation content inthe DS and PS representations, and so our algorithmis not an algorithm designed to take as input any ar-bitrary DS representation.
However, the fundamen-tals of our dependency representation are not radi-cally different than others - e.g.
we make an auxil-iary verb the child of the main verb, instead of theother way, but such choices can be adjusted for inthe conversion.To evaluate the success of this conversion algo-rithm, we follow the same evaluation procedure asXia et al (2009) and Xia and Palmer (2001).
Weconvert the PTB to a DS, and then use our algorithmto convert the DS back to a PS representation.
Theoriginal PS and the converted-from-DS PS are thencompared, in exactly the same way as parser outputis compared with the original (gold) tree.
We willshow that our results in this area are a significantimprovement above previous efforts.A key aspect of this work is that our DS-to-PSconversion encodes many of the properties of thePTB annotation guidelines (Bies et al, 1995), bothglobally and for specific XP projections.
The PTBguidelines are built upon broad decisions about PSrepresentation that provide an overall frameworkand cohesion for the details of the PS trees.
Toimplement these underlying principles of the guide-lines, we defined a set of 30 ?supertags?
that indi-cate how a lexical item can project in the syntac-tic structure, allowing us to specify these principles.We describe these as supertags because of a concep-tual similarity to the supertagging work in the TreeAdjoining Grammar (TAG) tradition (Bangalore andJoshi, 2010), although ours is far smaller than a typ-ical supertag set, and indeed is actually smaller thanthe PTB POS tag set.Our DS-to-PS code is based on this set of su-pertags, and can be run using either the supertagscreated from the gold POS tags, or using the POStags, together with the dependency structure to first(imperfectly) derive the supertags, and then proceedwith the conversion.
This choice of starting point al-lows us to measure the impact of POS tag complex-ities on the DS-to-PS conversion, which provides aninteresting insight on what a phrase structure parsercontributes in addition to this sort of automated DS-to-PS conversion, as discussed in Section 4.We have chosen this approach of encoding under-lying principles of the PTB guidelines for two rea-sons.
First, these principles are non-statistical, andthus we felt it would let us tease apart the contri-bution of the frequency information relating, e.g.,heads, on the one hand, and the basic notions ofphrase structure on the other.
The second reason isthat it was quite easy to implement these principles.We did not attempt a complete examination of everypossible rule in Bies et al (1995), but rather just se-lected the most obvious ones.
As we will see in Sec-tion 4.2, our results indeed are sometimes hurt bysuch lack of thoroughness, although in future workwe will make this more complete.2 Overview and ExampleFigures 1-4 provide a running example of the foursteps in the process.
Figure 1 is the original treefrom the Penn Treebank.
Figures 2 and 3 illustratethe two-step process of creating the dependency rep-resentation, and Figure 4 shows the conversion backto phrase structure.306SADVPRBAsidePPINfromNPNNPGMNP-SBJJJotherNNcarNNSmakersVPVBDpostedNPADJPRBgenerallyVBNmixedNNSresultsFigure 1: Penn Treebank treepostedVPSAsideADVPfromPPGMNP-OBJmakersNP-SBJother carresultsNP-OBJgenerallymixedADJPFigure 2: Tree Insertion Grammar decomposition of Figure1VBD/P VPpostedRB/P ADVPAsideIN/P PPfromNNP/P NP-OBJGMNNS/P NP-SBJmakersJJ/P ADJPotherNN/P PRENOMcarNNS/P NP-OBJresultsVBN/P ADJPmixedRB/P ADVPgenerallyFigure 3: Dependency representation derived from TIG de-composition in Figure 2SADVPRBAsidePPINfromNP-OBJNNPGMNP-SBJJJotherNNcarNNSmakersVPVBDpostedNP-OBJADJPRBgenerallyVBNmixedNNSresultsFigure 4: Conversion of dependency representation in Fig-ure 3 back to phrase structure.2.1 Creation of Dependency RepresentationThe creation of the dependency representation issimilar in basic aspects to many other approaches, inthat we utilize some basic assumptions about headrelations to decompose the full tree into smallerunits.
However, we first decompose the originaltrees into a Tree Insertion Grammar representation(Chiang, 2003), utilizing tree substitution and sisteradjunction.
We refer the reader to Chiang (2003) fordetails of these operations, and instead focus on thefact that the TIG derivation tree in Figure 2 parti-tions the phrase structure representation in Figure 1into smaller units, called elementary trees.
We leaveout the POS tags in Figure 2 to avoid clutter.The creation of the dependency representation isstructurally a simple rewrite of the TIG derivation,taking the word associated with each elementary treeand using it as a node in the dependency tree.
Inthis way, the dependency representation in Figure 3follows immediately from Figure 2.However, in addition, we utilize the TIG deriva-tion tree and the structures of the elementary trees tocreate a supertag (in the sense discussed in Section1) for each word.
For example, aside heads an ele-mentary tree that projects to ADVP, so it is assignedthe supertag P ADVP in Figure 3, meaning that itprojects to ADVP.
We label each node in Figure 3with both its POS tag and supertag, so in this casethe node for aside has RB/P ADVP.There are two typical cases that are not sostraightforward.
The first concerns elementary treeswith more than one level of projection, such as thatfor the verb, posted, which has two levels of pro-jection, S and VP.
In such cases we base the supertagonly on the immediate parent of the word.
For ex-ample, in this case the supertag for posted is P VP,rather than P S. As will be seen in Section 3.2, ourperspective is that the local context of the depen-dency tree will provide the necessary disambigua-tion as to what node is above the VP.307Projection Type SupertagNP P NPADJP P ADJPADVP P ADVPPP P PP, P WHPPS,SINV,SQ P VPQP,NP,QP-NP,QP-ADJP P QPWHNP P WHNPdefault P WHADVP, P INTJ, P PRT, P LSTnone P AUX, P PRENOM, P DET, P COMMA, P PERIOD, P CC, P COMP,P POS, P PRP$, P BACKDQUOTE, P DQUOTE, P COLON, P DOLLAR,P LRB, P RB, P PDT, P SYM, P FW, P POUNDTable 1: 30 supertags handled by 14 projection types.
The ambiguity in some, such as P VP projecting as S, SINV,SQ is handled by an examination of the dependency structure.The second non-straightforward case1 is that ofdegenerate elementary trees, in which the ?tree?is just the word itself, as for other, car, andgenerally.
In such cases we default the supertagbased on the original POS tag, and in some cases, thetree configuration.
For example, a word with the JJtag, such as other, would get the supertag P ADJP,with the RB tag such as generally the supertagP ADVP.
We assign prenominal nouns such as carhere the tag P PRENOM.Generating supertags in this way is a convenientway to correct some of the POS tag errors in the PTB(Manning, 2011).
For example, if that has the (in-correct) tag DT in the complementizer position, itstill receives the new POS tag P COMP.This procedure results in a set of 30 supertags, andTable 1 shows how they are partitioned into 14 pro-jection types.
These supertags and projection typesare the basis of our DS-to-PS conversion, as dis-cussed further in Section 2.2.We note here a brief comparison with earlier workon ?hybrid?
representations, which encode a PS rep-resentation inside a DS one, in order to convert fromthe latter to the former.
(Hall and Nivre, 2008; Jo-han Hall and Nilsson, 2007; Johansson and Nugues,2007).
Our goal is very different.
Instead of en-1There are other details not discussed here.
For example, wedo not automatically assign a P NP supertag to the head childof an NP, since such a head can legitimately be, e.g, a JJ, inwhich case we make the supertag P ADJP, on the reasoning thatit would be encoding ?too much?
to treat it as P NP.
Instead, werely on the DS and such labels as SBJ or OBJ to determine whento project it as NP in the converted PS.coding the phrase structure in the dependency treevia complex tags such as SBARQ in Johansson andNugues (2007), we use a minimal representation andrely on our encoding of the general principles ofPTB phrase structure to carry much of the weight.While supertags such as P VP may appear to encodesome of the structure, their primary role is as an in-termediate link between the POS tags and the phrasestructure conversion.
The created supertags are notin fact necessary for this conversion.
As we will seein the following sections, we convert from DS to PSusing either just the original POS tags, or with ourcreated supertags.We also include five labels in the dependency rep-resentation: SBJ, OBJ, PRN, COORD CONJ, APP.The example dependency tree in Figure 3 includesinstances of the SBJ and OBJ labels, in italics onthe node instead of the edges, for convenience.
TheSBJ label is of course already a function tag in thePTB.
We process the PTB when creating the TIGdecomposition to add an OBJ tag, as well basing thePRN label on the occurrence of the PRN node.
Wealso use heuristics to identify cases of coordinationand apposition, resulting in the COORD CONJ andAPP tags.
The reasons for including these labels isthat they prove useful in the conversion to phrasestructure, as illustrated in some of the examples be-low.Before moving on to the dependency-to-phrase-stucture conversion, we end this section with a com-ment on the role of function tags and empty cate-gories.
The PTB makes use of function tags to in-308dicate certain syntactic and semantic information,and of empty categories (and co-indexing) for amore complete and accurate syntactic representa-tion.
There is some overlap between the five la-bels we use, as just described, and the PTB func-tion tags, but in general we do not encode the fullrange of function tags in our representation, savingthis for future work.
More significantly, we alsodo not include empty categories and associated co-indexing, which has the consequence that the depen-dency trees are projective.The reason we have not included these aspects inour representation and conversion yet is that we arefocused here first on the evaluation for comparisonwith previous work, and the basis for this previouswork is the usual evalb program (Sekine and Collins,2008), which ignores function tags and empty cate-gories.
We return to this issue in the conclusion.2.2 From Dependency to Phrase StructureThere are two key aspects to the conversion from de-pendency to phrase structure.
(1) We encode generalconventions about annotation that are used through-out the annotation guidelines for the PTB.
A com-mon example is that of the ?single-word?
rule, inwhich a constituent consisting of just a single wordis reduced to just that word, without the constituentbracketing, in many cases.
(2) We use the set of su-pertags as the basis for defining projection-specificrules for how to attach children on the left or right ofthe head, in many cases utilizing the supertag namesthat we include to determine the specific attachment.For example, the leaf GM in Figure 3 has the su-pertag P NP (with the label OBJ), so heading a NPprojection, (NP GM).
Its parent node, from, hasthe supertag P PP, indicating that it heads a PP pro-jection, and so attaches the (NP GM) as a sister offrom.
It does not reduce it down as a single word,because the encoding of the PP projection specifiesthat it does not do so for children on its right.A more substantial case is that of the NP othercar makers.
Here the head noun, makers,has the supertag P NP, and so projects as an NP.Its first child, other, has the supertag P ADJP,and so projects as an ADJP, resulting in (ADJPother).
The second child, car, has the supertagP PRENOM (prenominal), and so does not projectat all.
When the NP projection for makers is as-sembled, it applies the ?single-word?
constraint tochildren on its left (as encoded in the definitionof the NP projection), thus stripping the ADJP offof other, resulting in the desired flat NP othercar makers.
Likewise, the ADVP projection forgenerally is stripped off before it is attached asa left sister of the ADJP projection mixed.
Theencoding of a VP projection specifies that it mustproject above VP if it is the root of the tree, and sothe VP projection for posted projects to S (by de-fault).In this way we can see that encoding some ofthe general characteristics of the annotation guide-lines allows the particular details of the PTB phrase-structure representation to be created from the less-specific dependency representation.3 Some Further Examples3.1 QP Projection or ReductionAs mentioned in Section 2.2, the ?single word?
con-vention is implemented in the conversion to PS, aswas the case with other in the previous section.The projection associated with P QP has a slighttwist to this principle, because of the nature of someof the financialspeak in the PTB.
In particular, thedollar sign is treated as a displaced word and istherefore not counted, in a QP constituent, as a tokenfor purposes of the ?single token?
rule.For example, (1abc) in Figure 5 illustrates a casewhere the QP structure projects to an NP node aswell.
(1a) is the original PTB PS tree, and (1b) isthe DS representation.
Note that billion headsthe about $ 9 billion subtree, with the su-pertag P QP and the label OBJ.2 Because it has morethan one child in addition to the $, it is converted tophrase structure as a QP under an NP, implying theempty *U*, although we do not actually put it in.In contrast, (2abc) is a case in which the QP nodeis not generated.
100 is the head of the phrase $ 100*U* in the PTB PS (a), as shown in the dependencystructure (b).
However, because it only has one childin addition to the $, no additional QP node is cre-ated in the phrase structure representation in (c).
Westress that the presence of the QP in (1a) and its ab-2A good case can be made that in fact $ should be the daugh-ter of to in the dependency tree, although we have not imple-mented this as such.309(1) (A)PPTOtoNPQPINabout$$CD9CDbillion-NONE-*U*(B)P PPtoP QP-OBJbillionP PPaboutP DOLLAR$P QP9(C)PPP PPtoNPQPP PPaboutP DOLLAR$P QP9P QPbillion(2) (A)PPINforNP$$CD100-NONE-*U*(B)P PPforP QP-OBJ100P DOLLAR$(C)PPP PPforNP-OBJP DOLLAR$P QP100Figure 5: Examples of handling of QP in dependency to phrase-structure conversion.sence in (2a) is correct annotation, consistent withthe annotation guidelines.3.2 Refinement of VP ProjectionsAs mentioned above, instead of having separate su-pertags for S, SINV, SQ, SBAR, SBARQ, we useonly the P VP supertag and let the context determinethe specifics of the projection.
Sentences (3ab) inFigure 6 illustrate how the SBJ label is used to treatthe P VP supertag as indicating projection to SINV(or SQ) instead of S. The determination is based onthe children of the P VP node.
For example, if thereis a child with the P AUX supertag which is before achild with the SBJ label, which in turn is before theP VP node itself, then the latter is treated as project-ing to either SINV or SQ, depending on the someadditional factors, primarily whether there is a WHword among the children.
In this example, there isno WH word, so it becomes a SINV.3 We note herethat we also include a simple listing of verbs thattake complements of certain types - such as verbs ofsaying, etc., that take SBAR complements, so that aVP will project not just to S, but SBAR, even if thecomplement is missing.3.3 CoordinationWe represent coordination in the dependency inone of the standard ways, by making the follow-ing conjuncts be children of the head word of3This is not a fully precise implementation of the condi-tions distinguishing SQ and SINV projections, in that it doesnot properly check for whether the clause is a question.
(3) (A)P VPabsorbedP AUXhadP NP-SBJcostP DETtheP VPbeen(B)SINVVBDhadNP-SBJDTtheNNcostVPVBNbeenVPVBNabsorbedFigure 6: (3ab) shows that the local context of the P VPsupertag in the dependency tree results in a SINV struc-ture in the converted phrase structure tree (3b).the first conjunct.
For example, a dependencyrepresentation of ...turn down the volumeand close the curtains is shown in (4a) inFigure 7.
The conjunct close the curtainsis converted as a VP projection projecting to S. How-ever, when the projection for turn is assembled, thecode checks if the conjuncts are missing subjects,and if so, reduces the configuration to standard VPcoordination, as in (4b).
The COORD label is usedto identify such structures for examination.4 Results of Dependency to PhraseStructure ConversionTo evaluate the correctness of conversion from de-pendency to phrase structure, we follow the samestrategy as Xia and Palmer (2001) and Xia et al(2009).
We convert the phrase structure trees in thePTB to dependency structure and convert the depen-dency back to phrase structure.
We then comparethe original PTB trees with the newly-created phrase310(4) (A)P VPturnP PRTdownP NPvolumeP DETtheP CCandP VP-COORDcloseP NP-OBJcurtainsP DETthe(B)VPVPturn PRTdownNP-OBJthe volumeand VPclose NP-OBJthe curtainsFigure 7: (4a) is the dependency representation of a coordination structure, and the resulting phrase structure (4b)shows that the conversion treated it as VP coordination, due to the absence of a subject.Sec System rec prec f00 Xia & Palmer ?01 86.2 88.7 87.5Xia et al ?09 91.8 89.2 90.5USE-POS-UNLABEL 96.6 97.4 97.0USE-POS 94.6 95.4 95.0USE-SUPER 95.9 97.0 96.422 Xia et al ?09 90.7 88.1 89.4USE-POS 95.0 95.5 95.3USE-SUPER 96.4 97.1 96.723 Wang & Zong ?10 95.9 96.3 96.1USE-POS 94.8 95.7 95.3USE-SUPER 96.2 97.3 96.724 USE-POS 94.0 94.7 94.4USE-SUPER 95.9 97.1 96.5Table 2: Results of dependency to phrase structure con-version.
For our system, the results are presented in twoways, using either the gold part-of-speech tags (USE-POS) or our gold supertags (USE-SUPER).
For purposesof comparison with Xia and Palmer (2001) and Xia etal.
(2009), we also present the results for Section 00 us-ing part-of-speech tags, but with an unlabeled evaluation(USE-POS-UNLABEL).structure trees, using the standard evalb scoring code(Sekine and Collins, 2008).
Xia and Palmer (2001)defined three different algorithms for the conversion,utilizing different heuristics for how to build projec-tion chains, and where to attach dependent subtrees.They reported results for their system for Section 00of the PTB, and we include in Table 2 only theirhighest scoring algorithm.
The system of Xia et al(2009) uses conversion rules learned from Section19, and then tested on Sections 00 and Section 22.We developed the algorithm using Section 24, andwe also report results for Sections 00, 22, and 23, forcomparison with previous work.
We ran our systemin two ways.
In one we use the ?gold?
supertagsthat were created as described in Section 2.1 (USE-SUPER), based on the TIG decomposition of theoriginal tree.
In the other (USE-POS) we use thegold POS tags, and not the supertags.
Because ourDS-to-PS algorithm is based on using the supertagsto guide the conversion, the USE-POS runs workby using a few straightforward heuristics to guessthe correct supertag from the POS tag and the de-pendency structure.
For example, if a word x hasthe POS tag ?TO?
and the word y to its immediateright is its parent in the dependency tree and y hasone of the verbal POS tags, then x receives the su-pertag P AUX, and otherwise P PP.
Any word withthe POS tag JJ, JJR, or JJS, receives the supertagP ADJP, and so on.
The results for Xia and Palmer(2001) and Xia et al (2009) were reported using anunlabeled version of evalb, so to compare properlywe also report our results for Section 00 using anunlabeled evaluation of the run using the POS tags(USE-POS-UNLABEL), while all the other resultsuse a labeled evaluation.We also compare our system with that of Wangand Zong (2010).
Unlike the three other systems(including ours), this was not based on an automaticconversion from a gold dependency tree to phrasestructure, but rather used the gold dependency treeas additional input for a phrase structure parser (theBerkeley parser).4.1 AnalysisWhile our system was developed using Section 24,the f-measure results for USE-SUPER are virtuallyidentical across all four sections (96.4, 96.7, 96.7,96.5).
Interestingly, there is more variation in the311USE-POS results (95.0, 95.3, 95.3, 94.4).
We takethis to be an indication of a difference in the sec-tions as to the utility of the POS tags to ?bootstrap?the syntactic structure.
As just mentioned above, theUSE-POS runs work by using heuristics to approxi-mate the gold supertags from the POS tags.The supertags, because they are partially derivedfrom the phrase structure, can obscure a discon-nect between a POS tag and the syntactic structureit projects.
For example, the word accordingin the structure (PP (VBG according) (PP(TO to) ...)) receives the gold supertag P PP,a more explicit representation of the word?s role inthe structure than the ambiguous VBG.
This is whythe USE-POS score is lower than the USE-SUPERscore, since the POS tag and dependency structuredo not always, at least with our simple heuristics,lead to the gold supertag.
For example, in the USE-POS run, according receives the incorrect su-pertag P VP, leading to an incorrect structure, whilein the USE-SUPER run, it is able to use P PP, lead-ing to the correct structure.However, even with the lower performance ofUSE-POS, it is well above the results reported in Xiaet al (2009) for Section 22, and even more so withthe unlabeled evaluation of Section 00 compared toXia and Palmer (2001) and Xia et al (2009).
Thecomparison with Wang and Zong (2010) for Section23 (they did not report results for any other section)shows something very different, however.
Their re-sult, using a gold dependency tree together with theBerkeley parser, is above our USE-POS version andbelow our USE-SUPER version.Our interpretation of this is that it provides anindication of what the parser is providing on topof the gold dependency structure, which is roughlythe same information that we have encoded in ourDS to PS code.
However, because the Wang andZong (2010) system performs better than our USE-POS version, it is likely learning some of the non-straightforward cases of how USE-POS tags canbootstrap the syntactic structure that our USE-POSversion is missing.
However, any conclusions mustbe tentative since our dependency structures are notnecessarily the same as theirs and so it is not an ex-act comparison.Error type countproblem with PTB annotation 8ambiguous ADVP placement 3incorrect use of ?single token rule?
3FRAG/X 2multiple levels of recursion 2other 5Table 3: Analysis of errors in first 50 sentences of USE-SUPER run for Section 244.2 Errors from Dependency Structure withSupertags to Phrase StructureWe stressed in the introduction that we are interestedin understanding better the relationship between theDS and PS representations.
Identifying areas wherethe conversion from DS did not result in a perfect(evalb score) PS is therefore of particular interest.For this analysis, we used our dev section, 24,with the run USE-SUPER.
We use this run becausewe are interested in cases where, even with the goldsupertags, there was still a problem with the conver-sion to the PS.
We examined the first 50 sentences inthe section, with a total of 23 errors.
We recognizethat this is a very small sample.
An eyeball exam-ination of other sentences does not reveal anythingsignificantly different than what we present here asfar as the sorts of errors, although we have only per-formed a rigorous analysis of these 23 errors, whichis why we limit our discussion here to these cases.Table 3 shows a breakdown of these 23 errors.Note that by ?error?
here we mean a difference be-tween the reconstructed PS structure, and the PTBgold PS structure, causing the score for Section 24,USE-SUPER (last row) in Table 2 to be less thanperfect.The most common ?error?
is that in which thePTB annotation is itself in error, while our algo-rithm actually creates a correct phrase structure, inthe sense that it is consistent with the PTB guide-lines.
Three of these eight annotation problems areof the same type, in which a NP is headed by a wordwith the RB tag.
An example is shown in (5) inwhich (5a) shows (a fragment of) the original treein the PTB, and (5b) is the resulting DS, with (5c)the reconstructed PS tree.
The word here receivesthe supertag P ADVP, thus resulting in a different re-312constructed PS, with an ADVP.
There is a mismatchbetween the POS tag and the node label in the origi-nal tree (5a), and in fact in this case the node label inthe PTB tree should have been ADVP-LOC, insteadof NP-LOC.
(5) (A)VPVBDpremieredNP-LOCRBhere(B)P VPpremieredP ADVPhere(C)VPVBDpremieredADVPRBhere(6) (A)SNP-SBJ-NONE-VPADVP-MNRRBfranticallyVBGsellingNPNNSbonds(B)P VPsellingP ADVPfranticallyP NPbonds(C)SADVPP ADVPfranticallyVPVBGsellingNPNNSbondsAn example of the ?ambiguous ADVP place-ment?
error is shown in (6), in which the PTB treehas the adverb frantically inside the VP, infor-mation which is not available in the DS (6b).
Ourconversion code has to choose as to where to putsuch ADVPs, and it puts them outside the VP, as in(6c), which is sometimes correct, but not in this case.5 Conclusion and Future WorkIn this work we have described an approach to auto-matically converting DS to PS with significantly im-proved accuracy over previous efforts, and compara-ble results to that of using a phrase structure parserguided by the dependency structure.Following the motivation discussed in Section 1,the next step is straightforward - to adapt the al-gorithm to work on conversion from a dependencyrepresentation of the Arabic Treebank to the phrasestructure representation necessary for the annotationpipeline.
Following this, we will then experimentwith parsing the Arabic dependency representation,converting to phrase structure, and evaluating the re-sulting phrase structure representation as usual forparsing evaluation.
We will also experiment withdependency parsing for the PTB dependency repre-sentation discussed in this paper.
Habash and Roth(2009) discuss an already-existing dependency rep-resentation of parts of the ATB and it will be inter-esting to compare the conversion accuracy using thedifferent dependency representations, although weexpect that there will not be any major differencesin the representations.One other aspect of future work is to implementthe algorithm in Wang and Zong (2010), using ourown dependency representation, since this would al-low a precise investigation of what the phrase struc-ture parser is contributing as compared to our auto-matic conversion.
We note that this work also ex-perimented with dependency parsing, and then auto-matically converting the results to PS, a further basisof comparison.Finally, we would like to stress that while we haveused evalb for scoring the converting PS because itis the standard evaluation for PS work, it is a veryinsufficient standard for this work.
As discussed atthe end of Section 2, we have not included all thefunction tags or empty categories in our representa-tion, a significant omission.
We would like to ex-pand our dependency representation to allow all thefunction tags and empty categories to be includedin the converted PS.
Our plan is to take our anal-ogy to TAG more seriously (e.g., (Joshi and Ram-bow, 2003)) and use a label akin to adjunction to en-code leftward (non-projective) movement in the tree,also using an appropriate dependency parser as well(Shen and Joshi, 2008).AcknowledgementsThis work was supported in part by the Defense Ad-vanced Research Projects Agency, GALE ProgramGrant No.
HR0011-06-1-0003.
The views, opinionsand/or findings contained in this article/presentationare those of the author/presenter and should not beinterpreted as representing the official views or poli-cies, either expressed or implied, of the Defense Ad-vanced Research Projects Agency or the Departmentof Defense.
We would also like to thank MohamedMaamouri, Colin Warner, Aravind Joshi, and MitchMarcus for valuable conversations and feedback.313ReferencesSrinivas Bangalore and Aravind K. Joshi, editors.
2010.Supertagging: Using Complex Lexical Descriptions inNatural Language Processing.
MIT Press.Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-Intyre.
1995.
Bracketing guidelines for Treebank II-style Penn Treebank project.
Technical Report MS-CIS-95-06, University of Pennsylvania.David Chiang.
2003.
Statistical parsing with an auto-matically extracted Tree Adjoining Grammar.
In DataOriented Parsing.
CSLI.Nizar Habash and Ryan Roth.
2009.
CATiB: TheColumbia Arabic Treebank.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pages221?224, Suntec, Singapore, August.
Association forComputational Linguistics.Johan Hall and Joakim Nivre.
2008.
A dependency-driven parser for German dependency and con-stituency representations.
In Proceedings of the Work-shop on Parsing German, pages 47?54, Columbus,Ohio, June.
Association for Computational Linguis-tics.Joakim Nivre Johan Hall and Jens Nilsson.
2007.
Hy-brid constituency-dependency parser for Swedish.
InProceedings of NODALIDA, Tartu, Estonia.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for english.
InProceedings of NODALIDA, Tartu, Estonia.Aravind Joshi and Owen Rambow.
2003.
A formal-ism for dependency grammar based on Tree Adjoin-ing Grammar.
In Proceedings of the Conference onMeaning-Text Theory, Paris, France.Mohamed Maamouri and Ann Bies.
2004.
Developingan arabic treebank: Methods, guidelines, procedures,and tools.
In Ali Farghaly and Karine Megerdoomian,editors, COLING 2004 Computational Approaches toArabic Script-based Languages, pages 2?9, Geneva,Switzerland, August 28th.
COLING.Mohamed Maamouri, Ann Bies, and Seth Kulick.
2011.Upgrading and enhancing the Penn Arabic Treebank.In Joseph Olive, Caitlin Christianson, and John Mc-Cary, editors, Handbook of Natural Language Pro-cessing and Machine Translation: DARPA Global Au-tonomous Language Exploitation.
Springer.Christopher Manning.
2011.
Part-of-speech taggingfrom 97% to 100%: Is it time for some linguistics?In Alexander Gelbukh, editor, Computational Linguis-tics and Intelligent Text Processing, 12th InternationalConference, CICLing 2011, Proceedings, Part I. Lec-ture Notes in Computer Science 6608.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
Computationallinguistics, 19:313?330.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The Penn Tree-bank: Annotating predicate argument structure.
InProceedings of HLT.Satoshi Sekine and Michael Collins.
2008.
Evalb.http://nlp.cs.nyu.edu/evalb/.Libin Shen and Aravind Joshi.
2008.
LTAG dependencyparsing with bidirectional incremental construction.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 495?504, Honolulu, Hawaii, October.
Association for Com-putational Linguistics.Zhiguo Wang and Chengqing Zong.
2010.
Phrase struc-ture parsing with dependency structure.
In COLING2010: Posters, pages 1292?1300, Beijing, China, Au-gust.Fei Xia and Martha Palmer.
2001.
Converting depen-dency structures to phrase structures.
In HLT-2001.Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,and Dipti Misra Sharma.
2009.
Towards a multi-representational treebank.
In Proceedings of the Work-shop on Treebanks and Linguistic Theories.314
