Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 862?872,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsImproved Lexical Acquisition through DPP-based Verb ClusteringRoi ReichartUniversity of Cambridge, UKrr439@cam.ac.ukAnna KorhonenUniversity of Cambridge, UKalk23@cam.ac.ukAbstractSubcategorization frames (SCFs), selec-tional preferences (SPs) and verb classescapture related aspects of the predicate-argument structure.
We present the firstunified framework for unsupervised learn-ing of these three types of information.We show how to utilize DeterminantalPoint Processes (DPPs), elegant proba-bilistic models that are defined over thepossible subsets of a given dataset andgive higher probability mass to high qual-ity and diverse subsets, for clustering.
Ournovel clustering algorithm constructs ajoint SCF-DPP DPP kernel matrix and uti-lizes the efficient sampling algorithms ofDPPs to cluster together verbs with sim-ilar SCFs and SPs.
We evaluate the in-duced clusters in the context of the threetasks and show results that are superior tostrong baselines for each 1.1 IntroductionVerb classes (VCs), subcategorization frames(SCFs) and selectional preferences (SPs) capturedifferent aspects of predicate-argument structure.SCFs describe the syntactic realization of verbalpredicate-argument structure, SPs capture the se-mantic preferences verbs have for their argumentsand VCs in the Levin (1993) tradition provide ashared level of abstraction for verbs that sharemany aspects of their syntactic and semantic be-havior.These three of types of information have proveduseful for Natural Language Processing (NLP)1The source code of the clustering algorithms and evalu-ation is submitted with this paper and will be made publiclyavailable upon acceptance of the paper.tasks which require information about predicate-argument structure, including parsing (Shi and Mi-halcea, 2005; Cholakov and van Noord, 2010;Zhou et al, 2011), semantic role labeling (Swierand Stevenson, 2004; Dang, 2004; Bharati et al,2005; Moschitti and Basili, 2005; zap, 2008; Zapi-rain et al, 2009), and word sense disambiguation(Dang, 2004; Thater et al, 2010; O?
Se?aghdha andKorhonen, 2011), among many others.Because lexical information is highly sensitiveto domain variation, approaches that can identifyVCs, SCFs and SPs in corpora have become in-creasingly popular, e.g.
(O?Donovan et al, 2005;Schulte im Walde, 2006; Erk, 2007; Preiss et al,2007; Van de Cruys, 2009; Reisinger and Mooney,2011; Sun and Korhonen, 2011; Lippincott et al,2012).The task of SCF induction involves identifyingthe arguments of a verb lemma and generalizingabout the frames (i.e.
SCFs) taken by the verb,where each frame includes a number of argumentsand their syntactic types.
For example, in (1),the verb ?show?
takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausalcomplement).
(1) [A number of SCF acquisition papers]SUBJ[show]VERB [their readers]DOBJ [which fea-tures are most valuable for the acquisitionprocess]CCOMP.SP induction involves identifying and classify-ing the lexical items in a given argument slot.
Insentence (2), for example, the verb ?show?
takesthe frame SUBJ-DOBJ.
The direct object in thisframe is likely to be inanimate.
(2) [Most SCF and SP acquisition papers]SUBJ,862[show]VERB [no evidence to the usefulness ofjoint learning leaning for these tasks]DOBJ.Finally, VC induction involves clustering to-gether verbs with similar meaning, reflected insimilar SCFs and SPs.
For example, ?show?
in theabove examples could get clustered together with?demonstrate?
and ?indicate?.Because these challenging tasks capture com-plementary information about predicate argumentstructure, they should be able to inform and sup-port each other.
Recently, researchers have be-gun to investigate the benefits of their joint learn-ing.
Schulte im Walde et al (2008) integrated SCFand VC acquisition and used it for WordNet-basedSP classification.
O?
Se?aghdha (2010) presented a?dual-topic?
model for SPs that induces also verbclusters.
Both works reported SP evaluation withpromising results.
Lippincott et al (2012) pre-sented a joint model for inducing simple syntac-tic frames and VCs.
They reported high accuracyresults on VCs.
de Cruys et al (2012) introduceda joint model for SCF and SP acquisition.
Theyevaluated both the SCFs and SPs, obtaining rea-sonable result on both tasks.In this paper, we present the first unified frame-work for unsupervised learning of the three typesof information - SCFs, SPs and VCs.
Our frame-work is based on Determinantal Point Processes(DPPs, (Kulesza, 2012; Kulesza and Taskar,2012c)), elegant probabilistic models that are de-fined over the possible subsets of a given datasetand give higher probability mass to high qualityand diverse subsets.We first show how individual-task DPP kernelmatrices can be naturally combined to construct ajoint kernel.
We use this to construct a joint SCF-SP kernel.
We then introduce a novel clusteringalgorithm based on iterative DPP sampling whichcan (contrary to other probabilistic frameworkssuch as Markov random fields) be performed bothaccurately and efficiently.
When defined over thejoint SCF and SP kernel, this new algorithm canbe used to induce VCs that are valuable for bothtasks.We also contribute by evaluating the value ofthe clusters induced by our model for the acquisi-tion of the three information types.
Our evaluationagainst a well-known VC gold standard shows thatour clustering model outperforms the state-of-the-art verb clustering algorithm of Sun and Korhonen(2009), in our setup where no manually createdSCF or SP data is available.
Our evaluation againsta well-known SCF gold standard and in the con-text of SP disambiguation tasks shows results thatare superior to strong baselines, demonstrating thebenefit our approach.2 Previous WorkSCF acquisition Most current works induce SCFsfrom the output of an unlexicalized parser (i.e.a parser trained without SCF annotations) usinghand-written rules (Briscoe and Carroll, 1997; Ko-rhonen, 2002; Preiss et al, 2007) or grammaticalrelation (GR) co-occurrence statistics (O?Donovanet al, 2005; Chesley and Salmon-Alt, 2006; Iencoet al, 2008; Messiant et al, 2008; Lenci et al,2008; Altamirano and Alonso i Alemany, 2010;Kawahara and Kurohashi, 2010).Only a handful of SCF induction works areunsupervised.
Carroll and Rooth (1996) appliedan EM-based approach to a context-free grammarbased model, Dkebowski (2009) used point-wiseco-occurrence of arguments in parsed Polish dataand Lippincott et al (2012) presented a Bayesiannetwork model for syntactic frame induction thatidentifies SPs on argument types.
However, theframes induced by Lippincott et al (2012) do notcapture sets of arguments for verbs so are far sim-pler than traditional SCFs.Current approaches to SCF acquisition sufferfrom lack of semantic information which is neededto guide the purely syntax-driven acquisition pro-cess.
Previous works have showed the benefit ofhand-coded semantic information in SCF acquisi-tion (Korhonen, 2002).
We will address this prob-lem in an unsupervised way: our approach is toconsider SCFs together with semantic SPs throughVCs which generalize over syntactically and se-mantically similar verbs.SP acquisition Considerable research has beenconducted on SP acquisition, with a variety ofunsupervised models proposed for this task thatuse no hand-crafted information during training.The latter approaches include latent variable mod-els (O?
Se?aghdha, 2010; Ritter and Etzioni, 2010;Reisinger and Mooney, 2011), distributional sim-ilarity methods (Bhagat et al, 2007; Basili etal., 2007; Erk, 2007) and methods based onnon-negative tensor factorization (Van de Cruys,2009).
These works use a variety of linguistic fea-tures in the acquisition process but none of them863integrates the three information types covered inour work.Verb clustering A variety of VC approacheshave been proposed in the literature.
These in-clude syntactic, semantic and mixed syntactic-semantic classifications (Grishman et al, 1994;Miller, 1995; Baker et al, 1998; Palmer et al,2005; Schuler, 2006; Hovy et al, 2006).
We fo-cus on Levin style classes (Levin, 1993) whichare defined in terms of diathesis alternations andcapture generalizations over a range of syntacticand semantic properties.
Previous unsupervisedVC acquisition approaches clustered a variety oflinguistic features using different (e.g.
K-meansand spectral) algorithms (Schulte im Walde, 2006;Joanis et al, 2008; Sun et al, 2008; Li and Brew,2008; Korhonen et al, 2008; Sun and Korhonen,2009; Vlachos et al, 2009; Sun and Korhonen,2011).
The linguistic features included SCFs andSPs, but these were induced separately and thenfeeded as features to the clustering algorithm.
Ourframework combines together SCF-motivated andSP-motivated kernel matrices , and uses the jointkernel to induce verb clusters which are likely tobe highly relevant for both tasks.
Importantly, nomanual or automatic system for SCF or SP acqui-sition has been utilized when constructing the ker-nel matrices, we only consider features extractedfrom the output of an unlexicalized parser.
Our ap-proach hence provides a framework for acquiringvaluable information for the three tasks together.Joint Modeling A small number of works haverecently investigated joint approaches to SCFs,SPs and VCs.
Each of them has addressed onlya subset of the tasks and all but one have eval-uated the performance in the context of one taskonly.
O?
Se?aghdha (2010) presented a ?dual-topic?model for SPs that induces VCs, reporting evalua-tion of SPs only.
Lippincott et al (2012) presenteda Bayesian network model for syntactic frame(rather than full SCF) induction that induces VCs.Only VCs are evaluated.
de Cruys et al (2012)presented a joint unsupervised model of SCF andSP acquisition based on non-negative tensor fac-torization.
Both SCFs and SPs were evaluated.
Fi-nally, the model of Schulte im Walde et al (2008)addresses the three types of information but SPparameters are estimated with a WordNet basedmethod and only the SPs are evaluated.
Althoughevaluation of these recent joint models has beenpartial, the results have been encouraging and fur-ther motivate the development of a framework thatacquires the three types of information together.3 The Unified FrameworkIn this section we present our unified framework.Our idea is to utilize DPPs for verb clustering thatinforms both SCF and SP acquisition.
DPPs definea probability distribution over the possible subsetsof a given set.
These models assign higher prob-ability mass to subsets that are both high qualityand diverse.Our novel clustering algorithm makes use ofthree DPP properties that are appealing for ourpurpose: (1) The existence of efficient sam-pling algorithms for these models, which enabletractable sampling of high quality and diverse verbsubsets; (2) Such verb subsets form natural highquality seeds for hierarchical clustering; and (3)Given individual-task DPP kernel matrices thereare various simple and natural ways to combinethem into a new DPP kernel matrix.Individual task DPP kernels represent (i) thequality of a data point (verb) as its average feature-based similarity with the other points in the dataset and (ii) the divergence between a pair of pointsas the inverse similarity between them.
For dif-ferent tasks, different feature sets are used for thekernel construction.
The high quality and diversesubsets sampled from the DPP model are consid-ered good cluster seeds as they are likely to be rel-atively uniformly spread and to provide good cov-erage of the data set.
The algorithm induces anhierarchical clustering, which is particularly suit-able for semantic tasks, where a set of clusters thatshare a parent consists of pure members (i.e.
mostof the points in each cluster member belong to thesame gold cluster) and together provide good cov-erage of the verb space.After a brief description of the DeterminantalPoint Processes (DPP) framework (Section 3.1),we discuss the construction of the joint DPP ker-nel, given a kernel for each individual task, In sec-tion 3.3 we present the DPP-Cluster clustering al-gorithm.3.1 Determinantal Point ProcessesDeterminantal point processes (DPPs) are elegantprobabilistic models of repulsion that offer effi-cient and exact algorithms for sampling, marginal-ization, conditioning, and other inference tasks.Recently (Kulesza, 2012; Kulesza and Taskar,8642012c) introduced them to the machine learningcommunity and demonstrated their usefulness fora variety of tasks including document summariza-tion, image search, modeling non-overlapping hu-man poses in images and video and automati-cally building timelines of important news stories(Kulesza and Taskar, 2010; Kulesza and Taskar,2012a; Gillenwater et al, 2012; Kulesza andTaskar, 2012b).
Below we provide a brief descrip-tion of the framework, a comprehensive surveycan be found in (Kulesza and Taskar, 2012c).Given a set of items Y = {y1, .
.
.
, yN}, a DPPP defines a probability measure on the set of allsubsets of Y , 2Y .
Kulesza and Taskar (2012c) re-stricted their discussion of DDPs to L-ensembles,where the probability of a subsetY ?
Y is definedthrough a positive semi-definite matrix L indexedby the elements of Y:PL(Y = Y ) =det(LY )?Y?Y det(LY )= det(LY )det(L+ I)(1)Where I is the N ?
N identity matrix anddet(L?)
= 1.
Since L is positive semi-definite, itcan be decomposed to L = BTB.
This allows theconstruction of an intuitively interpretable modelwhere each column Bi is the product of a qualityterm qi ?
R+ and a vector of (normalized) diver-sity features ?i ?
RD, ||?i|| = 1.
In this model,qi measures an inherent quality of the i ?
th itemin Y while ?Ti ?j ?
[?1, 1] is a similarity measurebetween items i and j.
With this representation wecan write:Lij = qi?Ti ?jqj (2)Sij = ?Ti ?j =Lij?LiiLjj(3)PL(Y = Y ) ?
(?i?Yq2i )det(SY ) (4)It can be shown that the first term in equation 4 in-creases with the quality of the selected items, andthe second term increases with their diversity.
Asa consequence, this distribution places most of itsweight on sets that are both high quality and di-verse.Although the number of possible realizations ofY is exponential in N , many inference procedurescan be performed accurately and efficiently (i.e.in polynomial time which is very short in prac-tice).
In particular, sampling, which NP-hard foralternative models such as Markov Random Fields(MRFs), is efficient, theoretically and practically,for DPPs.3.2 Constructing a Joint Kernel MatrixDPPs are particularly suitable for joint modelingas they come with various simple and intuitiveways to combine individual model kernel matricesinto a joint kernel.
This stems from the fact thatevery positive-semidefinite matrix forms a legalDPP kernel (equation 1).
Given individual modelDPP kernels, we would therefore like to combinethem into a positive-semidefinite matrix.While there are various ways to construct apositive-semidefinite matrix from two positive-semidefinite matrices ?
for example, by takingtheir sum ?
in this work we are motivated by theproduct of experts approach (Hinton, 2002), rea-soning that high quality assignments according toa product of models have to be of high quality ac-cording to each individual model, and sick for aproduct combination.
2In practice we construct the joint kernel in thefollowing way.
We build on the aforementionedproperty that a matrix L is positive semi-definiteiff L = BTB.
Given two DPPs, PL1 defined byL1 = AT1A1 and PL2 defined by L2 = AT2A2, weconstruct the joint kernel L12:L12 = L1L2L2L1 = CTC (5)Where C = AT2A2AT1A1 and CT =AT1A1AT2A2.3.3 Clustering AlgorithmAlgorithm (1) and Figure (1) provide a pseudo-code of the algorithm and an example output.
Be-low is a detailed description.Features Our algorithm builds two DPP ker-nel matrices (the GenKernelMatrix function),in which the rows and columns correspond to theverbs in the data set, such that the (i, j)-th entrycorresponds to verbs number i and j. Followingequations 2 and 3 one matrix is built for SCF andone for SP, and they are then combined into the2Note that we do not take a product of the individual mod-els but only of their kernel matrices.
Yet, if we construct thejoint matrix by a multiplication then it follows from a simplegeneralization of the Cauchy-Binet formula that its principleminors, which define the subset probabilities (equation 1), area sum of multiplications of the principle minors of the indi-vidual model kernels.
Still, we do not have guarantees thatour choice of kernel combination is the right one.
We leavethis for future research.865joint kernel matrix (the GenJointMat function)following equation 5.
Each kernel matrix requiresa proper feature representation ?
and quality scoreq.In both kernels we represent a verb by thecounts of the grammatical relations (GRs) it par-ticipates in.
In the SCF kernel a GR is representedby the GR type and the POS tags of the verb andits arguments.
In the SP kernels the GRs are rep-resented by the POS tags of the verb and its ar-guments as well as by the argument head word.Based on this feature representation, the similarity(opposite divergence) is encoded to the model byequation 3 as the dot product between the normal-ized feature vectors.
The quality score qi of thei-th verb is the average similarity of this verb withthe other verbs in the dataset.Cluster set construction In its while loop, thealgorithm iteratively generates fixed-size clustersets such that each data point belongs to exactlyone cluster in one set.
These cluster sets formthe leaf level of the tree in Figure (1).
It doesso by extracting the T highest probability K-pointsamples from a set of M subsets, each of whichsampled from the joint DPP model, and cluster-ing them by the cluster procedure.
The samplingis done by the K-DPP sampling process ((Kuleszaand Taskar, 2012c), page 62) 3.The cluster procedure first seeds a K-clusterset with the highest probability sample.
Then, itgradually extends the clusters by iteratively map-ping the samples, in decreasing order of probabil-ity, to the existing clusters (them1Mapping func-tion).
Mapping is done by attaching every pointin the mapped subset to its closet cluster, wherethe distance between a point and the cluster is themaximum over the distances between the pointand each of the points in the cluster.
The map-ping is many-to-one, that is, multiple points in thesubset can be assigned to the same cluster.Based on the DPP properties, the higher theprobability of a sampled subset, the more likely itis to consist of distinct points that provide a goodcoverage of the verb set.
By iteratively extendingthe clusters with high probability subsets, we thusexpect each cluster set to consist of clusters thatdemonstrate these properties.3K-DPP is a DPP conditioned on the sample size.
Asshown in ((Kulesza and Taskar, 2012c), Section 2.4.3) thisconditional distribution is also a DPP.
We could have obtainedsamples of size K by sampling the DPP and rejecting sam-ples of other sizes but this would have been slower.SET 1-2-3-4 (45,K)SET 1-2 (23,K)SET1 (12,K) SET2 (11,K)SET3-4(22,K)SET 3 (12,K) SET4 (10,K)Figure 1: An example output hierarchy of DPP-Cluster for a set of 45 data points.
Each set isaugmented with the number of points (left num-ber) and clusters (right number) it includes.
Theiterative DPP-samples clustering (the While loop)generates the lowest level of the tree, by dividingthe data set into cluster sets, each of which con-sists of K clusters.
Each point in the data set be-longs to exactly one cluster in exactly one set.
Theagglomerative clustering then iteratively combinescluster sets such that in each iteration two sets arecombined to one set with K clusters.Agglomerative Clustering Finally, theAgglomerativeClustering function builds ahierarchy of cluster sets, by iteratively combiningcluster set pairs.
In each iteration it computes thesimilarity between any such pair, defined to be thelowest similarity between their cluster members,which is in turn defined to be the lowest cosinesimilarity between their point members.
The mostsimilar cluster sets are combined such that eachof the clusters in one set is mapped to its mostsimilar cluster in the other set.
In this step thealgorithm generates data partitions at differentgranularity levels from finest (from the iterativesampling step) to the coarsest set (generated bythe last agglomerative clustering iteration andconsisting of exactly K clusters).
This property isuseful as the optimal level of generalization maybe task dependent.4 EvaluationData sets and gold standards We evaluated theSCFs and verb clusters on gold standard datasets.We based our set of the largest available joint setfor SCFs and VCs - that of (de Cruys et al, 2012).It provides SCF annotations for 183 verbs (an av-erage of 12.3 SCF types per verb) obtained byannotating 250 corpus occurrences per verb withthe SCF types of (de Cruys et al, 2012).
Theverbs represent a range of Levin classes at the toplevel of the hierarchy in VerbNet (Kipper-Schuler,2005).
Where a verb has more than one Verb-Net class, we assign it to the one supported by thehighest number of member verbs.
To ensure suf-866|C| = 20, 21.6 |C| = 40, 41 |C| = 60, 58.6 |C| = 69, 77.6 |C| = 89, 97.4Model R P F R P F R P F R P F R P FDPP-cluster 93.1 17.3 29.3 77.9 25.4 38.3 63 31.9 42.3 43.8 33.6 38.1 34.4 40.6 37.2AC 67 17.8 28.2 46.6 24 31.7 40.5 29.4 34 33 34.9 33.9 24.7 41.1 30.9SC 32.1 27.5 29.6 26.6 35.9 30.6 23.7 41.5 30.2 22.8 43.6 29.9 21.6 48.7 29.9Table 1: Verb clustering evaluation for the last five iterations of our DPP-cluster model and the baselineagglomerative clustering algorithm (AC, see text for its description), and for the spectral clustering (SC)algorithm of (Sun and Korhonen, 2009) with the same number of clusters induced by DPP-cluster.
|C| isthe number of clusters for DPP-cluster and SC (first number) and for AC (second number).
The F-scoreperformance of DPP-cluster is superior in 4 out of 5 cases.Arg.
per verb P (DPP) P(AC) P (B) P (NF) R (DPP) R (AC) R (B) R(NF) ERR DPP ERR AC ERR B?
200 (133 verbs) 27.3 23.7 27.3 23.1 9.9 7.6 8 11.3 3.4 0.16 1.55?
600 (205 verbs) 26.5 25 27.3 22.6 14.8 11.5 11.9 16.6 2.3 0.50 1.1?
1000 (238 verbs) 24.6 23.6 25.6 21.1 17.5 13.8 14.7 19.8 1.6 0.42 0.95Table 2: Performance of the Corpus Statistics SP baseline (non-filtered, NF) as well as for three filteringmethods: frequency based (filter-baseline, B), DPP-cluster based (DPP) and AC cluster based (AC).
P(method) and R (method) present the precision and recall of the method respectively.
The error reduc-tion ratio (ERR) is the ratio between the reduction in precision error achieved by each method and theincrease in recall error (each method is compared to the NF baseline).
Ratio greater than 1 means thatthe reduction in precision error is larger than the increase in recall error (see text for exact definition).DPP based filtering provides substantially better ratio.ficient representation of each class, we collectedfrom VerbNet the verbs for which at least one ofthe possible classes is represented in the 183 verbsset by at least one and at most seven verbs.
Thisyielded 101 additional verbs which we added tothe gold standard with the initial 183 verbs.We parsed the BNC corpus with the RASPparser (Briscoe et al, 2006) and used it for featureextraction.
Since 176 out of the 183 initial verbsare represented in this corpus, our final gold stan-dard consists of 34 classes containing 277 verbs,of which 176 have SCF gold standard and has beenevaluated for this task.
We set the parameters ofour algorithm on an held-out data, consisting ofdifferent verbs than those used in our experiments,to be M = 10000, K = 20 and T = 10.Clustering Evaluation We first evaluate thequality of the clusters induced by our algorithm(DPP-cluster) compared to the gold standard VCs(table 1).
To evaluate the importance of the DPPcomponent, we compare to the performance of aversion of our algorithm where everything is keptfixed except from the sampling which is done froma uniform distribution rather than from the DPPjoint kernel (this model is denoted in the tablewith AC for agglomerative clustering) 4.
We alsocompare to the state-of-the-art spectral clusteringmethod of Sun and Korhonen (2009) where our4Importantly, the kernel matrix L used in the agglomera-tive clustering process is also used by AC.kernel matrix is used for the distance between datapoints (SC) 5.We evaluated the unified cluster set induced ineach iteration of our algorithm and of the AC base-line and induced the same number of clusters as ineach iteration of our algorithm using the SC base-line.
Since the number of clusters in each iterationis not an argument for our algorithm or for the ACbaseline, the number of clusters slightly differ be-tween the two.
The AC and SC baseline resultswere averaged over 5 and 100 runs respectively.DPP-cluster has produced identical output acrossruns.The table demonstrates the superiority of theDPP-cluster model.
For four out of five conditionsits F-score performance outperforms the baselinesby 4.2-8.3%.
Moreover, in all conditions its recallperformances are substantially higher than thoseof the baselines (by 9.7-26.1%).
Note that DPP-cluster runs for 17 iterations while the AC baselineperforms only 6.
We therefore evaluated only thelast 5 iterations of each model 6.SCF evaluation For this evaluation, we firstbuilt a baseline SCF lexicon based on the parsed5Sun and Korhonen (2009) report better results than thosewe report for their algorithm (on a different data set).
Note,however, that they used the output of a rule-based SCF sys-tem as a source of features, as opposed to our unsupervisedapproach.6For the additional comparable iteration the result patternis very similar to the (C = 89, 97.4) case in the table, and isnot presented due to space limitations.867Algorithm 1 The DPP-cluster clustering algo-rithm.
K is the size of the sampled subsets, M isthe number of subsets sampled at each iteration, Yis the verb set, T is the number of most probablesamples to be used in each iterationAlgorithm DPP-cluster :Arguments: K,M,Y ,TReturn: cluster sets S = {S1, .
.
.
Sn}i?
1S ?
?while Y 6= ?
do(L1, S1)?
GenKernelMatrix(Y, SCF )(L2, S2)?
GenKernelMatrix(Y, SP )(L12, S12)?
GenJointMat(L1, L2)samples?
sampleDpp(L,K,M)topSamples?
exTop(samples, T )Si ?
cluster(topSamples, L)Y ?
Y ?
elements(Si)S ?
S ?
Sii?
i+ 1end whileAgglomerativeClustering(S)?????????????????????
?Function cluster :Arguments: topSamples,LReturn: SS ?
?, topSample?
?i?
1while (topSample ?
elements(S) = ?)
dotopSample?
topSamples(i)S ?
m1Mapping(topSample, S)i?
i+ 1if (i > size(topSamples)) thenreturn Send ifend whileBNC corpus.
We do this by gathering the GR com-binations for each of the verbs in our gold stan-dard, assuming they are frames and gathering theirfrequencies.
Note that this corpus statistics base-line is a very strong baseline that performs verysimilarly to (de Cruys et al, 2012), the best unsu-pervised SCF model we are aware of, when run ontheir dataset 7.As shown in table 3 the corpus statistics base-line achieves high recall (84%) at the cost oflow precision (52.5%) (similar pattern has been7personal communication with the authors.demonstrated for the system of de Cruys et al(2012)).
On the other extreme, two other com-monly used baselines strongly prefer precision.These are the Most Frequent SCF (O?Donovan etal., 2005) which uniformly assigns to all verbs thetwo most frequent SCFs in general language, tran-sitive (SUBJ-DOBJ) and intransitive (SUBJ) (andresults in poor F-score), and a filtering that re-moves frames with low corpus frequencies (whichresults in low recall even when trying to providethe maximum recall for a given precision level).The task we address is therefore to improve theprecision of the corpus statistics baseline in a waythat does not substantially harm the F-score.To remedy this imbalance, we apply a clusterbased filtering method on top of the maximum-recall frequency filter.
This filter excludes a candi-date frame from a verb?s lexicon only if it meetsthe frequency filter criterion and appears in nomore than N other members of the cluster of theverb in question.
The filter utilizes the clusteringproduced by the seventh to last iteration of DPP-cluster that contains seven clusters with approxi-mately 30 members each.
Such clustering shouldprovide a good generalization level for the task.We report results for moderate as well as ag-gressive filtering (N = 3 andN = 7 respectively).Table 3 clearly demonstrates that cluster based fil-tering (DPP-cluster and AC) is the only methodthat provides a good balance between the recalland the precision of the SCF lexicon.
Moreover,the lexicon induced by this method includes a sub-stantially higher number of frames per verb com-pared to the other filtering methods.
While bothAC and DPP-cluster still prefer recall to precision,DPP-cluster does so to a smaller extent 8.
Thisclearly demonstrates that the clustering serves toprovide SCF acquisition with semantic informa-tion needed for improved performance.SP evaluation We explore a variant of thepseudo-disambiguation task of Rooth et al (1999)which has been applied to SP acquisition by anumber of recent papers (e.g.
(de Cruys et al,2012)).
Rooth et al (1999) proposed to judgewhich of two verbs v and v?
is more likely to take agiven noun n as its argument.
In their experimentsthe model has to choose between a pair (v, n) that8We show results for the maximum recall frequency fil-tering with precision equals to 80 or 90.
When the frequencythreshold is further reduced from 0.03, the same result pat-tern hold.
We do not give a detailed description due to spacelimitations.868Corpus Statistics: [P = 52.5, R = 84, F = 64.6, AF = 12.3]Most Frequent SCF: [P = 86.7, R = 22.5, F = 35.8, AF = 2]Clustering Moderate Clustering AggressiveMaximum Recall Frequency Threshold Model P R F AF P R F AFthreshold = 0.03, Prec.
> 80 DPP-cluster 60.8 68.3 64.3 8.7 64.1 64.2 64.2 7.7[P=88.7,R=52.4,F=65.9,AF=4.5] AC 58 73.2 64.6 9.7 61.3 68.9 64.7 8.6threshold = 0.05, Prec.
> 90 DPP-cluster 60.1 64.6 62.3 8.7 63.3 59.3 61.3 7.2[P=92.3,R=44.4,F=59.9,AF=3.7] AC 57.5 70.6 63.2 9.4 60.7 65.4 62.7 8.3Table 3: SCF Results for the DPP-cluster model compared to the Corpus Statistics baseline, Most Fre-quent SCF baseline, maximum-recall frequency thresholding with the maximum threshold values thatkeep precision above 80 (threshold = 0.03) and above 90 (threshold = 0.05), and the AC clustering base-line.
AF is the average number of frames per verb.
All methods except from cluster based filtering(DPP-cluster and AC) induce lexicons with strong recall/precision imbalance.
Cluster based fil-tering keeps a larger number of frames in the lexicon compared to the frequency thresholdingbaseline, while keeping similar F-score levels.
DPP-cluster provides better recall/precision balancethan AC.appears only in the test corpus and a pair (v?, n)that appears neither in the test nor in the trainingcorpus.
Note, however, that this test only evaluatesthe capability of a model to distinguish a correctunseen verb-argument pair from an incorrect one,but not its capability to identify erroneous pairswhen no alternative pair is presented.
This lastproperty can strongly affect the precision of themodel.We therefore propose to measure both aspectsof the SP task by computing both the recall and theprecision between the list of possible arguments averb can take according to the model and the cor-responding test corpus list 9.We evaluate the value of our clustering for SPacquisition in the particularly challenging scenarioof domain adaptation.
For each of the verbs inour set we induce a list of possible noun direct ob-jects from the BNC corpus and an equivalent listfrom the North American News Text (NANT) cor-pus.
Following previous work (e.g.
(de Cruys etal., 2012)) arguments are identified using a parser(RASP in our case).
Using the verb clusters wecreate a filtered version of the BNC argument lex-icon which includes in the noun argument list ofa verb only those nouns that appear in the BNCas arguments of that verb and of one of its clustermembers.
For each verb we then compare the fil-tered as well as the non-filtered BNC induced lex-icon to the NANT lexicon by computing the aver-age recall and precision between the argument lists9In principle these measures can take into account theprobability assigned by the model to each argument and thecorresponding test corpus frequency.
In this work we com-pute probability-ignorant scores and keep more sophisticatedevaluations for future research.and then report the average scores across all verbs.We compare to a baseline which maintains onlynoun arguments that appear at least twice in BNC10.
As a final measure of performance we computethe ratio between the reduction in precision error(i.e.
pmodel?pbaseline100?pbaseline ) and the increase in recall er-ror ( rbaseline?rmodel100?rmodel ).Table 2 presents the results for verbs with up to200, 600 and 1000 noun arguments in the trainingdata.
In all cases, the relative error reduction of theDPP cluster filter is substantially higher than thatof the frequency baseline.
Note that for this taskthe baseline AC clusters are of low quality whichis reflects by an error reduction ratio of up to 0.5.5 Conclusions and Future WorkIn this paper we have presented the first unifiedframework for the induction of verb clusters, sub-categorization frames and selectional preferencesfrom corpus data.
Our key idea is to cluster to-gether verbs with similar SCFs and SPs and to usethe resulting clusters for SCF and SP induction.
Toimplement our idea we presented a novel methodwhich involves constructing a product DPP modelfor SCFs and SPs and introduced a new algorithmthat utilizes the efficient DPP sampling algorithmsto cluster together verbs with similar SCFs andSPs.
The induced clusters performed well in eval-uation against a VerbNet -based gold standard andproved useful in improving the quality of SCFsand SPs over strong baselines.Our results demonstrate the benefits of a uni-fied framework for acquiring lexical informa-10we experimented with other threshold values for thisbaseline but the recall in those case becomes very low.869tion about different aspects of verbal predicate-argument structure.
Not only the acquisition ofdifferent types information (syntactic and seman-tic) can support and inform each other, but also aunified framework can be useful for NLP tasks andapplications which require rich information aboutpredicate-argument structure.
In future work weplan to apply our approach on larger scale datasets and gold standards and to evaluate it in differ-ent domains, languages and in the context of NLPtasks such as syntactic parsing and SRL.In addition, in our current framework SCF andSP information is used for clustering which is inturn used to improve SCF and SP quality.
At thisstage no further information flows from the SCFand SP models to the clustering model.
A naturalextension of our unified framework is to constructa joint model in which the predictions for all threetasks inform each other at all stages of the predic-tion process.AcknowledgementsThe work in this paper was funded by the RoyalSociety University Research Fellowship (UK).ReferencesIvana Romina Altamirano and Laura Alonso i Ale-many.
2010.
IRASubcat, a highly customizable,language independent tool for the acquisition of ver-bal subcategorization information from corpus.
InProceedings of the NAACL HLT 2010 Young Inves-tigators Workshop on Computational Approaches toLanguages of the Americas.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In COLING-ACL-98.Roberto Basili, Diego De Cao, Paolo Marocco, andMarco Pennacchiotti.
2007.
Learning selectionalpreferences for entailment or paraphrasing rules.
InRANLP 2007, Borovets, Bulgaria.Rahul Bhagat, Patrick Pantel, and Eduard Hovy.
2007.Ledir: An unsupervised algorithm for learning di-rectionality of inference rules.
In EMNLP-07, page161170, Prague, Czech Republic.Akshar Bharati, Sriram Venkatapathy, and PrashanthReddy.
2005.
Inferring semantic roles using sub-categorization frames and maximum entropy model.In CoNLL-05.Ted Briscoe and John Carroll.
1997.
Automatic extrac-tion of subcategorization from corpora.
In ANLP-97.E.J.
Briscoe, J. Carroll, and R. Watson.
2006.
Thesecond relsease of the rasp system.
In COLING/ACLinteractive presentation session.Glenn Carroll and Mats Rooth.
1996.
Valence induc-tion with a head-lexicalized pcfg.
In EMNLP-96.Paula Chesley and Susanne Salmon-Alt.
2006.
Au-tomatic extraction of subcategorization frames forfrench.
In LREC-06.Kostadin Cholakov and Gertjan van Noord.
2010.
Us-ing unknown word techniques to learn known words.In EMNLP-10.Hoa Trang Dang.
2004.
Investigations into the Role ofLexical Semantics in Word Sense Disambiguation.Ph.D.
thesis, CIS, University of Pennsylvania.Tim Van de Cruys, Laura Rimell, Thierry Poibeau,and Anna Korhonen.
2012.
Multi-way tensor fac-torization for unsupervised lexical acquisition.
InCOLING-12.Lukasz Dkebowski.
2009.
Valence extraction us-ing EM selection and co-occurrence matrices.
Lan-guage resources and evaluation, 43(4):301?327.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In ACL 2007, Prague,Czech Republic.J.
Gillenwater, A. Kulesza, and B. Taskar.
2012.
Dis-covering diverse and salient threads in documentcollections.
In EMNLP-12.Ralph Grishman, Catherine Macleod, and Adam Mey-ers.
1994.
Comlex syntax: Building a computa-tional lexicon.
In COLNIG-94.G.E.
Hinton.
2002.
Training products of experts byminimizing contrastive divergence.
Neural Compu-tation, 14:1771?1800.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Porceedings 0f NAACL-HLT-06short papers.Dino Ienco, Serena Villata, and Cristina Bosco.
2008.Automatic extraction of subcategorization framesfor italian.
In LREC-08.Eric Joanis, Suzanne Stevenson, and David James.2008.
A general feature space for automatic verbclassification.
Natural Language Engineering.Daisuke Kawahara and Sadao Kurohashi.
2010.
Ac-quiring reliable predicate-argument structures fromraw corpora for case frame compilation.
In LREC-10.Karin Kipper-Schuler.
2005.
VerbNet: A broad-coverage, comprehensive verb lexicon.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA, June.870Anna Korhonen, Yuval Krymolowski, and Nigel Col-lier.
2008.
The choice of features for classifica-tion of verbs in biomedical texts.
In Proceddingsof COLING-08.Anna Korhonen.
2002.
Semantically motivatedsubcategorization acquisition.
In Proceedings ofthe ACL-02 workshop on Unsupervised lexicalacquisition-Volume 9.A.
Kulesza and B. Taskar.
2010.
Structured determi-nantal point processes.
In NIPS-10.A.
Kulesza and B. Taskar.
2012a.
k-dpps: fixed-sizedeterminantal point processes.
In ICML-11.A.
Kulesza and B. Taskar.
2012b.
Learning determi-nantal point processes.
In UAI-12.Alex Kulesza and Ben Taskar.
2012c.
Determi-nantal point processes for machine learning.
InarXiv:1207.6083.A.
Kulesza.
2012.
Learning with determinantal pointprocesses.
Ph.D. thesis, CIS, University of Pennsyl-vania.Alessandro Lenci, Barbara McGillivray, SimonettaMontemagni, and Vito Pirrelli.
2008.
Unsuper-vised acquisition of verb subcategorization framesfrom shallow-parsed corpora.
In LREC-08.Beth Levin.
1993.
English verb classes and alterna-tions: A preliminary investigation.
Chicago, IL.Jianguo Li and Chris Brew.
2008.
Which are the bestfeatures for automatic verb classification.
In ACL-08.Tom Lippincott, Anna Korhonen, and Diarmuid O?Se?aghdha.
2012.
Learning syntactic verb framesusing graphical models.
In ACL-12, Jeju, Korea.Ce?dric Messiant, Anna Korhonen, and ThierryPoibeau.
2008.
LexSchem: A large subcategoriza-tion lexicon for French verbs.
In LREC-08.George A. Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Alessandro Moschitti and Roberto Basili.
2005.
Verbsubcategorization kernels for automatic semantic la-beling.
In Proceedings of the ACL-SIGLEX Work-shop on Deep Lexical Acquisition.Ruth O?Donovan, Michael Burke, Aoife Cahill, Josefvan Genabith, and Andy Way.
2005.
Large-scaleinduction and evaluation of lexical resources fromthe penn-ii and penn-iii treebanks.
ComputationalLinguistics, 31:328?365.Diarmuid O?
Se?aghdha and Anna Korhonen.
2011.Probabilistic models of similarity in syntactic con-text.
In EMNLP-11, Edinburgh, UK.Diarmuid O?
Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In ACL-10, Uppsala, Swe-den.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Judita Preiss, Ted Briscoe, and Anna Korhonen.
2007.A system for large-scale acquisition of verbal, nom-inal and adjectival subcategorization frames fromcorpora.
In ACL-07.Joseph Reisinger and Raymond Mooney.
2011.
Cross-cutting models of lexical semantics.
In EMNLP-11,Edinburgh, UK.Alan Ritter and Oren Etzioni.
2010.
A latent dirich-let alocation method for selectional preferences.
InACL-10.Mats Rooth, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1999.
Inducing a semanti-cally annotated lexicon via em-based clustering.
InACL-99.Karin Kipper Schuler.
2006.
VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D. the-sis, University of Pennsylvania.S.
Schulte im Walde, C. Hying, C. Scheible, andH.
Schmid.
2008.
Combining EM training and theMDL principle for an automatic verb classificationincorporating selectional preferences.
In ACL-08,pages 496?504.Sabine Schulte im Walde.
2006.
Experiments onthe automatic induction of german semantic verbclasses.
Computational Linguistics, 32(2):159?194.Lei Shi and Rada Mihalcea.
2005.
Putting pieces to-gether: Combining framenet, verbnet and wordnetfor robust semantic parsing.
In CICLING-05.Lin Sun and Anna Korhonen.
2009.
Improving verbclustering with automatically acquired selectionalpreferences.
In EMNLP-09, Singapore.Lin Sun and Anna Korhonen.
2011.
Hierarchical verbclustering using graph factorization.
In EMNLP-11.Lin Sun, Anna Korhonen, and Yuval Krymolowski.2008.
Verb class discovery from rich syntactic data.Lecture Notes in Computer Science, 4919(16).Robert Swier and Suzanne Stevenson.
2004.
Unsuper-vised semantic role labelling.
In EMNLP-04.Stefan Thater, Hagen Furstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In ACL-10, Uppsala, Sweden.Tim Van de Cruys.
2009.
A non-negative tensor factor-ization model for selectional preference induction.In Proceedings of the workshop on Geometric Mod-els for Natural Language Semantics (GEMS).871Andreas Vlachos, Anna Korhonen, and ZoubinGhahramani.
2009.
Unsupervised and constraineddirichlet process mixture models for verb cluster-ing.
In Proceedings of the Workshop on GeometricalModels of Natural Language Semantics.2008.
Robustness and generalization of role sets:PropBank vs. VerbNet.Benat Zapirain, Eneko Agirre, and Lluis Marquex.2009.
Generalizing over lexical features: Selec-tional preferences for semantic role classification.
InACL-IJCNLP-09, Singapore.Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.2011.
Exploiting web-derived selectional prefer-ence to improve statistical dependency parsing.
InACL-11, Portland, OR.872
