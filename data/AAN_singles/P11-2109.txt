Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 620?624,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsGeneralized Interpolation in Decision Tree LMDenis Filimonov??
?Human Language TechnologyCenter of ExcellenceJohns Hopkins Universityden@cs.umd.eduMary Harper?
?Department of Computer ScienceUniversity of Maryland, College Parkmharper@umd.eduAbstractIn the face of sparsity, statistical models areoften interpolated with lower order (backoff)models, particularly in Language Modeling.In this paper, we argue that there is a rela-tion between the higher order and the backoffmodel that must be satisfied in order for theinterpolation to be effective.
We show that inn-gram models, the relation is trivially held,but in models that allow arbitrary clusteringof context (such as decision tree models), thisrelation is generally not satisfied.
Based onthis insight, we also propose a generalizationof linear interpolation which significantly im-proves the performance of a decision tree lan-guage model.1 IntroductionA prominent use case for Language Models (LMs)in NLP applications such as Automatic SpeechRecognition (ASR) and Machine Translation (MT)is selection of the most fluent word sequence amongmultiple hypotheses.
Statistical LMs formulate theproblem as the computation of the model?s proba-bility to generate the word sequencew1w2 .
.
.
wm ?wm1 , assuming that higher probability corresponds tomore fluent hypotheses.
LMs are often representedin the following generative form:p(wm1 ) =m?i=1p(wi|wi?11 )In the following discussion, we will refer to the func-tion p(wi|wi?11 ) as a language model.Note the context space for this function, wi?11is arbitrarily long, necessitating some independenceassumption, which usually consists of reducing therelevant context to n?
1 immediately preceding to-kens:p(wi|wi?11 ) ?
p(wi|wi?1i?n+1)These distributions are typically estimated from ob-served counts of n-grams wii?n+1 in the trainingdata.
The context space is still far too large; there-fore, the models are recursively smoothed usinglower order distributions.
For instance, in a widelyused n-gram LM, the probabilities are estimated asfollows:p?
(wi|wi?1i?n+1) = ?
(wi|wi?1i?n+1) + (1)?
(wi?1i?n+1) ?
p?
(wi|wi?1i?n+2)where ?
is a discounted probability1.In addition to n-gram models, there are manyother ways to estimate probability distributionsp(wi|wi?1i?n+1); in this work, we are particularly in-terested in models involving decision trees (DTs).As in n-gram models, DT models also often uti-lize interpolation with lower order models; however,there are issues concerning the interpolation whicharise from the fact that decision trees permit arbi-trary clustering of context, and these issues are themain subject of this paper.1We refer the reader to (Chen and Goodman, 1999) for asurvey of the discounting methods for n-gram models.6202 Decision TreesThe vast context space in a language model man-dates the use of context clustering in some form.
Inn-gram models, the clustering can be represented asa k-ary decision tree of depth n ?
1, where k is thesize of the vocabulary.
Note that this is a very con-strained form of a decision tree, and is probably sub-optimal.
Indeed, it is likely that some of the clusterspredict very similar distributions of words, and themodel would benefit from merging them.
Therefore,it is reasonable to believe that arbitrary (i.e., uncon-strained) context clustering such as a decision treeshould be able to outperform the n-gram model.A decision tree provides us with a clustering func-tion ?
(wi?1i?n+1) ?
{?1, .
.
.
,?N}, where N is thenumber of clusters (leaves in the DT), and clusters?k are disjoint subsets of the context space; theprobability estimation is approximated as follows:p(wi|wi?1i?n+1) ?
p(wi|?
(wi?1i?n+1)) (2)Methods of DT construction and probability estima-tion used in this work are based on (Filimonov andHarper, 2009); therefore, we refer the reader to thatpaper for details.Another advantage of using decision trees is theease of adding parameters such as syntactic tags:p(wm1 ) =Xt1...tmp(wm1 tm1 ) =Xt1...tmmYi=1p(witi|wi?11 ti?11 )?Xt1...tmmYi=1p(witi|?
(wi?1i?n+1ti?1i?n+1)) (3)In this case, the decision tree would cluster the con-text space wi?1i?n+1ti?1i?n+1 based on information the-oretic metrics, without utilizing heuristics for whichorder the context attributes are to be backed off (cf.Eq.
1).
In subsequent discussion, we will writeequations for word models (Eq.
2), but they areequally applicable to joint models (Eq.
3) with trivialtransformations.3 Backoff PropertyLet us rewrite the interpolation Eq.
1 in a moregeneric way:p?
(wi|wi?11 ) = ?n(wi|?n(wi?11 )) + (4)?
(?n(wi?11 )) ?
p?
(wi|BOn?1(wi?11 ))where, ?n is a discounted distribution, ?n is a clus-tering function of order n, and ?
(?n(wi?11 )) is thebackoff weight chosen to normalize the distribution.BOn?1 is the backoff clustering function of ordern ?
1, representing a reduction of context size.
Inthe case of an n-gram model, ?n(wi?11 ) is the setof word sequences where the last n ?
1 words arewi?1i?n+1, similarly, BOn?1(wi?11 ) is the set of se-quences ending with wi?1i?n+2.
In the case of a de-cision tree model, the same backoff function is typ-ically used, but the clustering function can be arbi-trary.The intuition behind Eq.
4 is that the backoff con-text BOn?1(wi?11 ) allows for more robust (but lessinformed) probability estimation than the contextcluster ?n(wi?11 ).
More precisely:?wi?11 ,W: W ?
?n(wi?11 )?W ?
BOn?1(wi?11 )(5)that is, every word sequence W that belongs to acontext cluster ?n(wi?11 ), belongs to the same back-off cluster BOn?1(wi?11 ) (hence has the same back-off distribution).
For n-gram models, Property 5trivially holds since BOn?1(wi?11 ) and ?n(wi?11 )are defined as sets of sequences ending with wi?1i?n+2and wi?1i?n+1 with the former clearly being a supersetof the latter.
However, when ?
can be arbitrary, e.g.,a decision tree, that is not necessarily so.Let us consider what happens when we havetwo context sequences W and W ?
that belong tothe same cluster ?n(W ) = ?n(W ?)
but differ-ent backoff clusters BOn?1(W ) 6= BOn?1(W ?
).For example: suppose we have ?
(wi?2wi?1) =({on}, {may,june}) and two corresponding backoffclusters: BO?
= ({may}) and BO??
= ({june}).Following on, the word may is likely to be a monthrather than a modal verb, although the latter ismore frequent and will dominate in BO?.
There-fore we have much less faith in p?(wi|BO?)
than inp?(wi|BO??)
and would like a much smaller weight ?assigned to BO?, but it is not possible in the back-off scheme in Eq.
4, thus we will have to settle on acompromise value of ?, resulting in suboptimal per-formance.We would expect this effect to be more pro-nounced in higher order models, because viola-621tions of Property 5 are less frequent in lower or-der models.
Indeed, in a 2-gram model, theproperty is never violated since its backoff, un-igram, contains the entire context in one clus-ter.
The 3-gram example above, ?
(wi?2wi?1) =({on}, {may,june}), although illustrative, is notlikely to occur because may in wi?1 position willlikely be split from june very early on, since it isvery informative about the following word.
How-ever, in a 4-gram model, ?
(wi?3wi?2wi?1) =({on}, {may,june}, {<unk>}) is quite plausible.Thus, arbitrary clustering (an advantage of DTs)leads to violation of Property 5, which, we argue,may lead to a degradation of performance if back-off interpolation Eq.
4 is used.
In the next section,we generalize the interpolation scheme which, as weshow in Section 6, allows us to find a better solutionin the face of the violation of Property 5.4 Linear InterpolationWe use linear interpolation as the baseline, rep-resented recursively, which is similar to Jelinek-Mercer smoothing for n-gram models (Jelinek andMercer, 1980):p?n(wi|wi?1i?n+1) = ?n(?n) ?
pn(wi|?n) + (6)(1?
?n(?n)) ?
p?n?1(wi|wi?1i?n+2)where ?n ?
?n(wi?1i?n+1), and ?n(?n) ?
[0, 1] areassigned to each cluster and are optimized on a held-out set using EM.
pn(wi|?n) is the probability dis-tribution at the cluster ?n in the tree of order n. Thisinterpolation method is particularly useful as, un-like count-based discounting methods (e.g., Kneser-Ney), it can be applied to already smooth distribu-tions pn2.5 Generalized InterpolationWe can unwind the recursion in Eq.
6 and make sub-stitutions:?n(?n) ?
??n(?n)(1?
?n(?n)) ?
?n?1(?n?1) ?
?
?n?1(?n?1)...2In decision trees, the distribution at a cluster (leaf) is oftenrecursively interpolated with its parent node, e.g.
(Bahl et al,1990; Heeman, 1999; Filimonov and Harper, 2009).p?n(wi|wi?1i?n+1) =n?m=1?
?m(?m) ?
pm(wi|?m) (7)n?m=1?
?m(?m) = 1Note that in this parameterization, the weight as-signed to pn?1(wi|?n?1) is limited by (1?
?n(?n)),i.e., the weight assigned to the higher order model.Ideally we should be able to assign a different setof interpolation weights for every eligible combina-tion of clusters ?n, ?n?1, .
.
.
, ?1.
However, not onlyis the number of such combinations extremely large,but many of them will not be observed in the train-ing data, making parameter estimation cumbersome.Therefore, we propose the following parameteriza-tion for the interpolation of decision tree models:p?n(wi|wi?1i?n+1) =?nm=1 ?m(?m) ?
pm(wi|?m)?nm=1 ?m(?m)(8)Note that this parameterization has the same num-ber of parameters as in Eq.
7 (one per cluster in ev-ery tree), but the number of degrees of freedom islarger because the the parameters are not constrainedto sum to 1, hence the denominator.In Eq.
8, there is no explicit distinction betweenhigher order and backoff models.
Indeed, it ac-knowledges that lower order models are not backoffmodels when Property 5 is not satisfied.
However,it can be shown that Eq.
8 reduces to Eq.
6 if Prop-erty 5 holds.
Therefore, the new parameterizationcan be thought of as a generalization of linear inter-polation.
Indeed, suppose we have the parameteri-zation in Eq.
8 and Property 5.
Let us transform thisparameterization into Eq.
7 by induction.
We define:?m ?m?k=1?k ; ?m = ?m + ?m?1where, due to space limitation, we redefine ?m ?
?m(?m) and ?m ?
?m(?m); ?m ?
?m(wi?11 ),i.e., the cluster of model order m, to which the se-quence wi?11 belongs.
The lowest order distributionp1 is not interpolated with anything, hence:?1p?1(wi|?1) = ?1p1(wi|?1)Now the induction step.
From Property 5, it followsthat ?m ?
?m?1, thus, for all sequences in ?wn1 ?622n-gram DT: Eq.
6 (baseline) DT: Eq.
8 (generalized)order Jelinek-Mercer Mod KN word-tree syntactic word-tree syntactic2-gram 270.2 261.0 257.8 214.3 258.1 214.63-gram 186.5 (31.0%) 174.3 (33.2%) 168.7 (34.6%) 156.8 (26.8%) 168.4 (34.8%) 155.3 (27.6%)4-gram 177.1 (5.0%) 161.7 (7.2%) 164.0 (2.8%) 156.5 (0.2%) 155.7 (7.5%) 147.1 (5.3%)Table 1: Perplexity results on PTB WSJ section 23.
Percentage numbers in parentheses denote the reduction of per-plexity relative to the lower order model of the same type.
?Word-tree?
and ?syntactic?
refer to DT models estimatedusing words only (Eq.
2) and words and tags jointly (Eq.
3).
?m, we have the same distribution:?mpm(wi|?m) + ?m?1p?m?1(wi|?m?1) == ?m(?m?mpm(wi|?m) +?m?1?mp?m?1(wi|?m?1))= ?m(?
?mpm(wi|?m) + (1?
?
?m)p?m?1(wi|?m?1))= ?mp?m(wi|?m) ; ?
?m ?
?m?mNote that the last transformation is because ?m ?
?m?1; had it not been the case, p?m would depend onthe combination of ?m and ?m?1 and require multi-ple parameters to be represented on its entire domainwn1 ?
?m.
After n iterations, we have:n?m=1?m(?m)pm(wi|?m) = ?np?n(wi|?n); (cf.
Eq.
8)Thus, we have constructed p?n(wi|?n) using thesame recursive representation as in Eq.
6, whichproves that the standard linear interpolation is a spe-cial case of the new interpolation scheme, which oc-curs when the backoff Property 5 holds.6 Results and DiscussionModels are trained on 35M words of WSJ 94-96from LDC2008T13.
The text was converted intospeech-like form, namely numbers and abbrevia-tions were verbalized, text was downcased, punc-tuation was removed, and contractions and posses-sives were joined with the previous word (i.e., they?ll becomes they?ll).
For syntactic modeling, weused tags comprised of POS tags of the word and itshead, as in (Filimonov and Harper, 2009).
Parsingof the text for tag extraction occurred after verbal-ization of numbers and abbreviations but before anyfurther processing; we used an appropriately trainedlatent variable PCFG parser (Huang and Harper,2009).
For reference, we include n-gram modelswith Jelinek-Mercer and modified interpolated KNdiscounting.
All models use the same vocabulary ofapproximately 50k words.We implemented four decision tree models3: twousing the interpolation method of (Eq.
6) and twobased on the generalized interpolation (Eq.
8).
Pa-rameters ?
were estimated using the L-BFGS tominimize the entropy on a heldout set.
In order toeliminate the influence of all factors other than theinterpolation, we used the same decision trees.
Theperplexity results on WSJ section 23 are presented inTable 1.
As we have predicted, the effect of the newinterpolation becomes apparent at the 4-gram order,when Property 5 is most frequently violated.
Notethat we observe similar patterns for both word-treeand syntactic models, with syntactic models outper-forming their word-tree counterparts.We believe that (Xu and Jelinek, 2004) also suf-fers from violation of Property 5, however, sincethey use a heuristic method4 to set backoff weights,it is difficult to ascertain the extent.7 ConclusionThe main contribution of this paper is the insightthat in the standard recursive backoff there is an im-plied relation between the backoff and the higher or-der models, which is essential for adequate perfor-mance.
When this relation is not satisfied other in-terpolation methods should be employed; hence, wepropose a generalization of linear interpolation thatsignificantly outperforms the standard form in sucha scenario.3We refer the reader to (Filimonov and Harper, 2009) fordetails on the tree construction algorithm.4The higher order model was discounted according to KNdiscounting, while the lower order model could be either a lowerorder DT (forest) model, or a standard n-gram model, with theformer performing slightly better.623ReferencesLalit R. Bahl, Peter F. Brown, Peter V. de Souza, andRobert L. Mercer.
1990.
A tree-based statistical lan-guage model for natural language speech recognition.Readings in speech recognition, pages 507?514.Stanley F. Chen and Joshua Goodman.
1999.
An empir-ical study of smoothing techniques for language mod-eling.
Computer Speech & Language, 13(4):359?393.Denis Filimonov and Mary Harper.
2009.
A joint lan-guage model with fine-grain syntactic tags.
In Pro-ceedings of the EMNLP.Peter A. Heeman.
1999.
POS tags and decision treesfor language modeling.
In Proceedings of the JointSIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora, pages129?137.Zhongqiang Huang and Mary Harper.
2009.
Self-Training PCFG grammars with latent annotationsacross languages.
In Proceedings of the EMNLP 2009.Frederick Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of markov source parameters fromsparse data.
In Proceedings of the Workshop on Pat-tern Recognition in Practice, pages 381?397.Peng Xu and Frederick Jelinek.
2004.
Random forests inlanguage modeling.
In Proceedings of the EMNLP.624
