A. L inear  Observed  T ime Stat is t ica l  Parser  Based  on Max imumEnt ropy  Mode lsAdwait Ratnaparkhi*Dept.
of Computer and Information ScienceUniversity of Pennsylvania200 South 33rd StreetPhiladelphia, PA 19104-6389adwait~unagi, cis.
upenn, eduAbstractThis paper presents a statistical parser fornatural language that obtains a parsingaccuracy--roughly 87% precision and 86%recall--which surpasses the best previouslypublished results on the Wall St. Journaldomain.
The parser itself requires very lit-tle human intervention, since the informa-tion it uses to make parsing decisions isspecified in a concise and simple manner,and is combined in a fully automatic wayunder the maximum entropy framework.The observed running time of the parser ona test sentence is linear with respect to thesentence length.
Furthermore, the parserreturns everal scored parses for a sentence,and this paper shows that a scheme to pickthe best parse from the 20 highest scoringparses could yield a dramatically higher ac-curacy of 93% precision and recall.1 IntroductionThis paper presents a statistical parser for naturallanguage that finds one or more scored syntacticparse trees for a given input sentence.
The parsingaccuracy--roughly 87% precision and 86% recall--surpasses the best previously published results onthe Wall St. Journal domain.
The parser consists ofthe following three conceptually distinct parts:1. h set of procedures that use certain actions toincrementally construct parse trees.2.
A set of maximum entropy models that com-pute probabilities of the above actions, and ef-fectively "score" parse trees.
* The author acknowledges the support of AI:tPAgrant N66001-94C-6043.3.
A search heuristic which attempts to find thehighest scoring parse tree for a given input sen-tence.The maximum entropy models used here are simi-lar in form to those in (Ratnaparkhi, 1996; Berger,Della Pietra, and Della Pietra, 1996; Lau, Rosen-feld, and Roukos, 1993).
The models compute theprobabilities of actions based on certain syntacticcharacteristics, or features, of the current context.The features used here are defined in a concise andsimple manner, and their relative importance is de-termined automatically by applying a training pro-cedure on a corpus of syntactically annotated sen-tences, such as the Penn Treebank (Marcus, San-torini, and Marcinkiewicz, 1994).
Although creat-ing the annotated corpus requires much linguisticexpertise, creating the feature set for the parser it-self requires very little linguistic effort.Also, the search heuristic is very simple, and itsobserved running time on a test sentence is linearwith respect o the sentence length.
Furthermore,the search euristic returns everal scored parses for-a sentence, and this paper shows that a scheme topick the best parse from the 20 highest scoring parsescould yield a dramatically higher accuracy of 93%precision and recall.Sections 2, 3, and 4 describe the tree-buildingprocedures, the maximum entropy models, and thesearch heuristic, respectively.
Section 5 describesexperiments with the Penn Treebank and section6 compares this paper with previously publishedworks.2 Procedures for Building TreesThe parser uses four procedures, TAG, CHUNK,BUILD, and CHECK, that incrementally build parsetrees with their actions.
The procedures are ap-plied in three left-to-right passes over the input sen-tence; the first pass applies TAG, the second pass ap-plies CHUNK, and the third pass applies BUILD andPass Procedure Actions DescriptionFirst Pass TAG A POS tag in tag set Assign POS Tag to wordSecond Pass CHUNK Star t  X, Jo in  X, Other Assign Chunk tag to POS tag andwordThird Pass BUILD Star t  X, Jo in  X, where X is a Assign current ree to start a newconstituent label in label set constituent, or to join the previ-ous oneCHECK Yes, No Decide if current constituent iscompleteTable 1: Tree-Building Procedures of ParserCHECK.
The passes, the procedures they apply, andthe actions of the procedures are summarized in ta-ble 1 and described below.The actions of the procedures are designed so thatany possible complete parse tree T for the input sen-tence corresponds to exactly one sequence of actions;call this sequence the derivation of T. Each proce-dure, when given a derivation d = {a l .
.
.
an}, pre-dicts some action a,+l to create a new derivationd' = {al .
.
.
a ,+l  }.
Typically, the procedures postu-late many different values for a ,+ l ,  which cause theparser to explore many different derivations whenparsing an input sentence.
But for demonstrationpurposes, figures 1-7 trace one possible derivationfor the sentence "I saw the man with the telescope",using the part-of-speech (POS) tag set and con-stituent label set of the Penn treebank.2.1 First PassThe first pass takes an input sentence, shown in fig-ure 1, and uses TAG to assign each word a POS tag.The result of applying TAG to each word is shown infigure 2.2.2 Second PassThe second pass takes the output of the first passand uses CHUNK to determine the "flat" phrasechunks of the sentence, where a phrase is "flat" ifand only if it is a constituent whose children consistsolely of POS tags.
Starting from the left, CHUNKassigns each (word,POS tag) pair a "chunk" tag, ei-ther Start X, Join X, or Other.
Figure 3 shows theresult after the second pass.
The chunk tags are thenused for chunk detection, in which any consecutivesequence of words win.., w, (m _< n) are groupedinto a "flat" chunk X if wm has been assigned Star tX and Wm+l .
.
.w ,  have all been assigned Jo in  X.The result of chunk detection, shown in figure 4, isa forest of trees and serves as the input to the thirdpass.Procedure Actions SimilarShift-ReduceParser ActionCHECK No shiftCHECK Yes reduce c~, wherea is CFGrule of proposedconstituentBUILD Star t  X, Jo in  X Determinesa forsubsequent re-duce operationsTable 2: Comparison of BUILD and CHECK to oper-ations of a shift-reduce parser2.3 Th i rd  PassThe third pass always alternates between the useof  BUILD and CHECK, and completes any remain-ing constituent structure.
BUILD decides whethera tree will start a new constituent or join the in-complete constituent immediately to its left.
Ac-cordingly, it annotates the tree with either S tar t  X,where X is any constituent label, or with Jo in  X,where X matches the label of the incomplete con-stituent to the left.
BUILD always processes theleftmost tree without any Star t  X or Jo in  X an-notation.
Figure 5 shows an application of BUILDin which the action is Jo in  VP.
After BUILD, con-trol passes to CHECK, which finds the most recentlyproposed constituent, and decides if it is complete.The most recently proposed constituent, shown infigure 6, is the rightmost sequence of trees t in .
.
,  tn(m < n) such that tm is annotated with S tar t  Xand tm+l  .
.
?
tn are annotated with Jo in  X.
If CHECKdecides yes, then the proposed constituent takes itsplace in the forest as an actual constituent, on whichBUILD does its work.
Otherwise, the constituent isnot finished and BUILD processes the next tree inthe forest, tn+ 1.
CHECK always answers no if the2I saw the man with the telescopeFigure 1: Initial SentencePRP VBD DT NN IN DT NNI I i i I i II saw the man with the telescopeFigure 2: The result after First PassStart NP Other Start NP Join NP Other Start NP Join NPi i I I i i IPRP VBD DT NN IN DT NNi I I i i i iI saw the man with the telescopeFigure 3: The result after Second PassNP VBD NP INPRP saw DT NN withI I II the manNPDT NNi ithe telescopeFigure 4: The result of chunk detectionStart S Start VP Join VP IN NPI I I INP VBD NP with DT NNI f ~ i iPRP saw DT NN the telescopeI I iI the manFigure 5: An application of BUILD in which Join VP is the actionStart S ?
IN NPNP Start VP Join VP with DT NNI I I I IPRP VBD NP the telescopeI saw DT NNI Ithe manFigure 6: The most recently proposed constituent (shown under ?
)3Start S Start VP Join VP ?I I i INP VBD NP INPRP saw DT NN withI I II the manNPDT NNI Ithe telescopeFigure 7: An application of CHECK in which No is the action, indicating that the proposed constituent infigure 6 is not complete.
BUILD will now process the tree marked with ?proposed constituent is a "fiat" chunk, since suchconstituents must be formed in the second pass.
Fig-ure 7 shows the result when CHECK looks at the pro-posed constituent in figure 6 and decides No.
Thethird pass terminates when CHECK is presented aconstituent that spans the entire sentence.Table 2 compares the actions of BUILD and CHECKto the operations of a standard shift-reduce parser.The No and Yea actions of CHECK correspond to theshift and reduce actions, respectively.
The impor-tant difference is that while a shift-reduce parsercreates a constituent in one step (reduce a), the pro-cedures BUILD and CHECK create it over several stepsin smaller increments.3 P robab i l i ty  Mode lThis paper takes a "history-based" approach (Blacket al, 1993) where each tree-building procedure usesa probability model p(alb), derived from p(a, b), toweight any action a based on the available context,or history, b.
First, we present a few simple cate-gories of contextual predicates that capture any in-formation in b that is useful for predicting a. Next,the predicates are used to extract a set of featuresfrom a corpus of manually parsed sentences.
Finally,those features are combined under the maximum en-tropy framework, yielding p(a, b).3.1 Contextua l  P red icatesContextual predicates are functions that check forthe presence or absence of useful information in acontext b and return true or false accordingly.
Thecomprehensive guidelines, or templates, for the con-textual predicates of each tree building procedureare given in table 3.
The templates use indicesrelative to the tree that is currently being modi-fied.
For example, if the current tree is the 5thtree, cons(-2) looks at the constituent label, headword, and start/ join annotation of the 3rd tree inthe forest.
The actual contextual predicates are gen-erated automatically by scanning the derivations ofthe trees in the manually parsed corpus with thetemplates.
For example, an actual contextual pred-icate based on the template cons(0) might be "Doescons(0) = { NP, he } ?"
Constituent head wordsare found, when necessary, with the algorithm in(Magerman, 1995).Contextual predicates which look at head words,or especially pairs of head words, may not be re-liable predictors for the procedure actions due totheir sparseness in the training sample.
Therefore,for each lexically based contextual predicate, therealso exist one or more corresponding less specific,or "backed-off", contextual predicates which lookat the same context, but omit one or more words.For example, the contexts cons(0, 1"), cons(0*, 1),cons(0*, 1") are the same as cons(0,1) but omit ref-erences to the head word of the 1st tree, the 0thtree, and both the 0th and 1st tree, respectively.The backed-off contextual predicates hould allowthe model to provide reliable probability estimateswhen the words in the history are rare.
Backed-offpredicates are not enumerated in table 3, but theirexistence is indicated with a * and t.3.2 Max imum Ent ropy  F rameworkThe contextual predicates derived from the tem-plates of table 3 are used to create the features nec-essary for the maximum entropy models.
The pred-icates for TAG, CHUNK, BUILD, and CHECK are usedto scan the derivations of the trees in the corpus toform the training samples "~^c, TcMusK, '~U~LD, andTCMECK, respectively.
Each training sample has theform T = ((al, 51), (a2, b2), .
.
.
,  CaN, bN)}, where aiis an action of the corresponding procedure and biis the list of contextual predicates that were t rue  inthe context in which al was decided.The training samples are respectively used to cre-ate the models PT^G, PCHUNK, PBUILD, and PCMECK, all ofwhich have the form:kp(a, b) = I I  _ij(o,b  ~  (1)j----1where a is some action, b is some context, ~" is a nor-4Model Categories Description Templates UsedTAG See (Ratnaparkhi, 1996)CHUNK chunkandpostag(n)*BUILDCHECKchunkandpostag(m, n)*cons(n)cons(re, n)*cons(m, n,p) Tpunctuationcheckcons(n)*checkcons(m,n)*productionsurround(n)*The word, POS tag, and chunk tag of nthleaf.
Chunk tag omitted if n > 0.chunkandpostag(m) & chunkandpostag(n)The head word, constituent (or POS) la-bel, and start/join annotation of the nthtree.
Start/join annotation omitted ifn>0.cons(m) & cons(n)cons(m), cons(n), & cons(p).The constituent we could join (1) containsa "\[" and the current tree is a "\]"; (2)contains a "," and the current ree is a ",";(3) spans the entire sentence and currenttree is ".
"The head word, constituent (or POS) la-bel of the nth tree, and the label of pro-posed constituent, begin and last arefirst and last child (resp.)
of proposedconstituent.checkcons(m) & checkcons(n)Constituent label of parent (X), andconstituent or P0S labels of children(Xz ...  Xn) of proposed constituentPOS tag and word of the nth leaf to theleft of the constituent, if n < 0, or to theright of the constituent, if n > 0chunkandpostag(O),chunkandpostag(-1),chunkandpostag(-2)chunkandpostag(1),chunkandpostag(2)chunkandpostag(-1, 0),chunkandpostag(O, 1)cons(O), cons(-1), cons(-2),cons(l), cons(2)cons(-1, 0), cons(O, 1)cons(O, -1, -2), cons(O, 1, 2),cons(-1, O, 1)bracketsmatch, iscomma,endofsentencecheckcons( last) ,checkcons(begin)checkcons(i, last), begin <i < lastproduction=X --} X1 ... Xnsurround(I), surround(2),surround(-1), surround(-2)Table 3: Contextual Information Used by Probability Models (* = all backed-off contexts are used, t = onlybacked-off contexts that include head word of current ree, i.e., 0th tree, are used)5malization constant, a j  are the model parameters,0 < aj < oo, and fj(a, b) E {0, 1} are called features,j = {1. .
.
k}.
Features encode an action a' as wellas some contextual predicate cp that a tree-buildingprocedure would find useful for predicting the actiona'.
Any contextual predicate cp derived from table 3which occurs 5 or more times in a training samplewith a particular action a' is used to construct afeature f j :1 if cp(b) = t rue  && a = a'f j(a,b)= 0 otherwisefor use in the corresponding model.
Each feature f jcorresponds to a parameter aj ,  which can be viewedas a "weight" that reflects the importance of thefeature.The parameters {a l .
.
.an}  are found automat-ically with Generalized Iterative Scaling (Darrochand Ratcliff, 1972), or GIS.
The GIS procedure, aswell as the maximum entropy and maximum likeli-hood properties of the distribution of form (1), aredescribed in detail in (Ratnaparkhi, 1997).
In gen-eral, the maximum entropy framework puts no lim-itations on the kinds of features in the model; nospecial estimation technique is required to combinefeatures that encode different kinds of contextualpredicates, like punctuation and cons(0, 1, 2).
As aresult, experimenters need only worry about whatfeatures to use, and not how to use them.We then use the models Pr^a, PeHusK, PBUILD, andPongee to define a function score, which the searchprocedure uses to rank derivations of incomplete andcomplete parse trees.
For each model, the corre-sponding conditional probability is defined as usual:p(a, b)P(alb) = Ea'eA p(a', b)For notational convenience, define q as follows\[ PrAa(a\]b) if a is an action from TAGpCStmK(a\[b) if a is an action from CHUNKq(alb) = PBUILD(al b) if a is an action from BUILDPcnEcK(alb ) if a is an action from CHECKLet deriv(T) = {a l , .
.
.
,an} be the derivation of aparse T, where T is not necessarily complete, andwhere each al is an action of some tree-buildingprocedure.
By design, the tree-building proceduresguarantee that {a l , .
.
.
,  an} is the only derivation forthe parse T. Then the score of T is merely the prod-uct of the conditional probabilities of the individualactions in its derivation:score(T) = H q(adbi)a~ Ederiv(T)where bi is the context in which ai was decided.4 SearchThe search heuristic attempts to find the best parseT*, defined as:T* =arg  max score(T)TGtrees(S)where trees(S) are all the complete parses for aninput sentence S.The heuristic employs a breadth-first search(BFS) which does not explore the entire frontier,but rather, explores only at most the top g scor-ing incomplete parses in the frontier, and terminateswhen it has found M complete parses, or when allthe hypotheses have been exhausted.
Furthermore,if {a l .
.
.an}  are the possible actions for a givenprocedure on a derivation with context b, and theyare sorted in decreasing order according to q(ailb),we only consider exploring those actions {al .
.
.
am}that hold most of the probability mass, where m isdefined as follows:mm = m axE  q(a, lb) < Qi=1and where Q is a threshold less than 1.
The searchalso uses a Tag Dictionary constructed from train-ing data, described in (Ratnaparkhi, 1996), that re-duces the number of actions explored by the tag-ging model.
Thus there are three parameters for thesearch heuristic, namely K,M, and Q and all exper-iments reported in this paper use K = 20, M = 20,and Q = .951 Table 4 describes the top K BFS andthe semantics of the supporting functions.It should be emphasized that if K > 1, the parserdoes not commit to a single POS or chunk assign-ment for the input sentence before building con-stituent structure.
All three of the passes describedin section 2 are integrated in the search, i.e., whenparsing a test sentence, the input to the second passconsists of K of the best distinct POS tag assign-ments for the input sentence.
Likewise, the inputto the third pass consists of K of the best distinctchunk and POS tag assignments for the input sen-tence.The top K BFS described above exploits the ob-served property that the individual steps of correctderivations tend to have high probabilities, and thusavoids searching a large fraction of the search space.Since, in practice, it only does a constant amount ofwork to advance ach step in a derivation, and sincederivation lengths are roughly proportional to the1The parameters K,M, and Q were optimized on"held out" data separate from the training and test sets.6advance:  d x Vinser t :  d x hext rac t :  h ) dcompleted:  d> d l .
.
.dm> void) {true,false}M=20K=20Q = .95C = <empty heap>h0 =<input sentence>whi le ( IC l  <M )i f  ( Vi, hi i s  empty )then break/ *  Applies relevant tree building procedure to dand returns list of new derivations whose actionprobabilities pass the threshold Q *//* inserts d in heap h *//* removes and returns derivation in hwith highest score *//* returns true if and only ifd is a complete derivation *//* Heap of completed parses *//* hi contains derivations of length i */i = max{i I hi i s  non-empty}sz = min(g ,  ihii)fo r  j= l  to  szd l .
.
.dp  = advance(ext rac t (h l ) ,  V )fo r  q=l  to pif (completed (dq))then insert (dq, C)else insert(dq, hi+l)Table 4: Top K BFS Search HeuristicSecondsI I I I I, li!, l l!
i l l ,0::it:ii i ,i!.,:.
.
.
: i,!li l i !)))I,)!i'?
??
o ?:'i:!"."
: .
: ' , I -  ","?
i .
, i I  :o:!
!
I I ,| l i :  ' .  "
I | I :  ii~ ' :I?
o ?i ' l  " I I f I I10 20 30 40 50 60 70Sentence LengthFigure 8: Observed running time of top K BFS on Section 23 of Penn Treebank WSJ, using one 167MhzUltraSPARC processor and 256MB RAM of a Sun Ultra Enterprise 4000.sentence length, we would expect it to run in lin-ear observed time with respect o sentence length.Figure 8 confirms our assumptions about the linearobserved running time.5 Exper imentsThe maximum entropy parser was trained on sec-tions 2 through 21 (roughly 40000 sentences) ofthe Penn Treebank Wall St. Journal corpus, release2 (Marcus, Santorini, and Marcinkiewicz, 1994), andtested on section 23 (2416 sentences) for compar-ison with other work.
All trees were stripped oftheir semantic tags (e.g., -LOC, -BNF, etc.
), coref-erence information(e.g., *-1), and quotation marks( "  and ' ' ) for both training and testing.
The PAR-SEVAL (Black and others, 1991) measures comparea proposed parse P with the corresponding correcttreebank parse T as follows:# correct constituents in PRecall = # constituents in T# correct constituents in PPrecision = # constituents in PA constituent in P is "correct" if there exists a con-stituent in T of the same label that spans the samewords.
Table 5 shows results using the PARSEVALmeasures, as well as results using the slightly moreforgiving measures of (Collins, 1996) and (Mager-man, 1995).
Table 5 shows that the maximum en-tropy parser performs better than the parsers pre-sented in (Collins, 1996) and (Magerman, 1995) ~,which have the best previously published parsing ac-curacies on the Wall St. Journal domain.It is often advantageous to produce the top Nparses instead of just the top 1, since additional in-formation can be used in a secondary model that re-orders the top N and hopefully improves the qualityof the top ranked parse.
Suppose there exists a "per-fect" reranking scheme that, for each sentence, magi-cally picks the best parse from the top N parses pro-duced by the maximum entropy parser, where thebest parse has the highest average precision and re-call when compared to the treebank parse.
The per-formance of this "perfect" scheme is then an upperbound on the performance of any reranking schemethat might be used to reorder the top N parses.
Fig-ure 9 shows that the "perfect" scheme would achieveroughly 93% precision and recall, which is a dra-matic increase over the top 1 accuracy of 87% preci-sion and 86% recall.
Figure 10 shows that the "Ex-act Match", which counts the percentage of times2Results for SPATTER on section 23 are reported in(Collins, 1996)Parser PrecisionMaximum Entropy ?
86.8%Maximum Entropy* 87.5%(Collins, 1996)* 85.7%(Magerman, 1995)* 84.3%Recall85.6%86.3%85.3%84.0%Table 5: Results on 2416 sentences of section 23(0 to 100 words in length) of the WSJ Treebank.Evaluations marked with ~ ignore quotation marks.Evaluations marked with * collapse the distinctionbetween ADVP and PRT, and ignore all punctuation.the proposed parse P is identical (excluding POStags) to the treebank parse T, rises substantiallyto about 53% from 30% when the "perfect" schemeis applied.
For this reason, research into rerankingschemes appears to be a promising step towards thegoal of improving parsing accuracy.6 Compar i son  Wi th  P rev ious  WorkThe two parsers which have previously reported thebest accuracies on the Penn Treebank Wall St. Jour-nal are the bigram parser described in (Collins, 1996)and the SPATTER parser described in (Jelinek etal., 1994; Magerman, 1995).
The parser presentedhere outperforms both the bigram parser and theSPATTER parser, and uses different modelling tech-nology and different information to drive its deci-sions.The bigram parser is a statistical CKY-style chartparser, which uses cooccurrence statistics of head-modifier pairs to find the best parse.
The max-imum entropy parser is a statistical shift-reducestyle parser that cannot always access head-modifierpairs.
For example, the checkcons(m,n) predicateof the maximum entropy parser may use two wordssuch that neither is the intended head of the pro-posed consituent that the CHECK procedure mustjudge.
And unlike the bigram parser, the maximumentropy parser cannot use head word informationbesides "flat" chunks in the right context.The bigram parser uses a backed-off estimationscheme that is customized for a particular task,whereas the maximum entropy parser uses a gen-eral purpose modelling technique.
This allows themaximum entropy parser to easily integrate vary-ing kinds of features, such as those for punctua-tion, whereas the bigram parser uses hand-craftedpunctuation rules.
Furthermore, the customized es-timation framework of the bigram parser must useinformation that has been carefully selected for itsvalue, whereas the maximum entropy framework ro-89594939291%Accuracy9089888700 ++0 +0 +++I I IPrecision ORecall +oO oO O O OOOOO O OOO+ + + + + + + + + + ++ +85 L - - - - - - - - - - - - - - - - - -~- - - - - - - - - - - - - - - - - - - - - - - - .
.
- - - - - - - - - - - - .
.
.
- -0 5 10 15 20NFigure 9: Precision & recall of a "perfect" reranking scheme for the top N parses of section 23 of the WSJTreebank, as a function of N. Evaluation ignores quotation marks.55535149474543% Accuracy~37353331<2927250OOOOOOO OIO O O O O O O OOO OI I I5 10 15 20NFigure 10: Exact match of a "perfect" reranking scheme for the top N parses of section 23 of the WSJTreebank, as a function of N. Evaluation ignores quotation marks.bustly integrates any kind of information, obviatingthe need to screen it first.The SPATTER parser is a history-based parserthat uses decision tree models to guide the opera-tions of a few tree building procedures.
It differsfrom the maximum entropy parser in how it buildstrees and more critically, in how its decision treesuse information.
The SPATTER decision trees usepredicates on word classes created with a statisticalclustering technique, whereas the maximum entropyparser uses predicates that contain merely the wordsthemselves, and thus lacks the need for a (typicallyexpensive) word clustering procedure.
Furthermore,the top K BFS search heuristic appears to be muchsimpler than the stack decoder algorithm outlinedin (Magerman, 1995).77 Conc lus ionThe maximum entropy parser presented hereachieves a parsing accuracy which exceeds the bestpreviously published results, and parses a test sen-tence in linear observed time, with respect o thesentence length.
It uses simple and concisely speci-?
fled predicates which can added or modified quicklywith little human effort under the maximum entropyframework.
Lastly, this paper clearly demonstratesthat schemes for reranking the top 20 parses deserveresearch effort since they could yield vastly betteraccuracy results.8 AcknowledgementsMany thanks to Mike Collins and Professor MitchMarcus from the University of Pennsylvania for theirhelpful comments on this work.Re ferencesBerger, Adam, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A Maximum EntropyApproach to Natural Language Processing.
Com-putational Linguistics, 22(1):39-71.Black, Ezra et al 1991.
A Procedure for Quan-titatively Comparing the Syntactic Coverage ofEnglish Grammars.
In Proceedings of the Febru-ary 1991 DARPA Speech and Natural LanguageWorkshop, pages 306-311.Black, Ezra, Fred Jelinek, John Lafferty, David M.Magerman, Robert Mercer, and Salim Roukos.1993.
Towards History-based Grammars: UsingRicher Models for Probabilistic Parsing.
In Pro-ceedings of the 31st Annual Meeting of the ACL,Columbus, Ohio.Collins, Michael John.
1996.
A New StatisticalParser Based on Bigram Lexical Dependencies.In Proceedings of the 34th Annual Meeting of theACL.Darroch, J. N. and D. Ratcliff.
1972.
GeneralizedIterative Scaling for Log-Linear Models.
The An-nals of Mathematical Statistics, 43(5):1470-1480.Jelinek, Fred, John Lafferty, David M. Magerman,Robert Mercer, Adwait Ratnaparkhi, and SalimRoukos.
1994.
Decision Tree Parsing using a Hid-den Derivational Model.
In Proceedings of the Hu-man Language Technology Workshop, pages 272-277.
ARPA.Lau, Ray, Ronald Rosenfeld, and Salim Roukos.1993.
Adaptive Language Modeling Using TheMaximum Entropy Principle.
In Proceedings ofthe Human Language Technology Workshop, pages108-113.
ARPA.Magerman, David M. 1995.
Statistical Decision-Tree Models for Parsing.
In Proceedings of the33rd Annual Meeting of the ACL.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1994.
Building a largeannotated corpus of English: the Penn Treebank.Computational Linguistics, 19(2):313-330.Ratnaparkhi, Adwait.
1996.
A Maximum EntropyPart of Speech Tagger.
In Conference on Em-pirical Methods in Natural Language Processing,University of Pennsylvania, May 17-18.Ratnaparkhi, Adwait.
1997.
A Simple Introductionto Maximum Entropy Models for Natural Lan-guage Processing.
Technical Report 97-08, Insti-tute for Research in Cognitive Science, Universityof Pennsylvania.10
