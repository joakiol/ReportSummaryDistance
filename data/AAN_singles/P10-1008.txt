Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 69?78,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning to Adapt to Unknown Users:Referring Expression Generation in Spoken Dialogue SystemsSrinivasan JanarthanamSchool of InformaticsUniversity of Edinburghs.janarthanam@ed.ac.ukOliver LemonInteraction LabMathematics and Computer Science (MACS)Heriot-Watt Universityo.lemon@hw.ac.ukAbstractWe present a data-driven approach to learnuser-adaptive referring expression gener-ation (REG) policies for spoken dialoguesystems.
Referring expressions can be dif-ficult to understand in technical domainswhere users may not know the techni-cal ?jargon?
names of the domain entities.In such cases, dialogue systems must beable to model the user?s (lexical) domainknowledge and use appropriate referringexpressions.
We present a reinforcementlearning (RL) framework in which the sys-tem learns REG policies which can adaptto unknown users online.
Furthermore,unlike supervised learning methods whichrequire a large corpus of expert adaptivebehaviour to train on, we show that effec-tive adaptive policies can be learned froma small dialogue corpus of non-adaptivehuman-machine interaction, by using a RLframework and a statistical user simula-tion.
We show that in comparison toadaptive hand-coded baseline policies, thelearned policy performs significantly bet-ter, with an 18.6% average increase inadaptation accuracy.
The best learned pol-icy also takes less dialogue time (average1.07 min less) than the best hand-codedpolicy.
This is because the learned poli-cies can adapt online to changing evidenceabout the user?s domain expertise.1 IntroductionWe present a reinforcement learning (Sutton andBarto, 1998) framework to learn user-adaptive re-ferring expression generation policies from data-driven user simulations.
A user-adaptive REG pol-icy allows the system to choose appropriate ex-pressions to refer to domain entities in a dialogueJargon: Please plug one end of the broadbandcable into the broadband filter.Descriptive: Please plug one end of the thinwhite cable with grey ends into thesmall white box.Table 1: Referring expression examples for 2 enti-ties (from the corpus)setting.
For instance, in a technical support con-versation, the system could choose to use moretechnical terms with an expert user, or to use moredescriptive and general expressions with noviceusers, and a mix of the two with intermediate usersof various sorts (see examples in Table 1).In natural human-human conversations, dia-logue partners learn about each other and adapttheir language to suit their domain expertise (Is-sacs and Clark, 1987).
This kind of adaptationis called Alignment through AudienceDesign (Clark and Murphy, 1982; Bell, 1984).We assume that users are mostly unknown tothe system and therefore that a spoken dialoguesystem (SDS) must be capable of observing theuser?s dialogue behaviour, modelling his/her do-main knowledge, and adapting accordingly, justlike human interlocutors.
Rule-based and super-vised learning approaches to user adaptation inSDS have been proposed earlier (Cawsey, 1993;Akiba and Tanaka, 1994).
However, such methodsrequire expensive resources such as domain ex-perts to hand-code the rules, or a corpus of expert-layperson interactions to train on.
In contrast, wepresent a corpus-driven framework using whicha user-adaptive REG policy can be learned usingRL from a small corpus of non-adaptive human-machine interaction.We show that these learned policies performbetter than simple hand-coded adaptive policiesin terms of accuracy of adaptation and dialogue69time.
We also compared the performance of poli-cies learned using a hand-coded rule-based simu-lation and a data-driven statistical simulation andshow that data-driven simulations produce betterpolicies than rule-based ones.In section 2, we present some of the relatedwork.
Section 3 presents the dialogue data thatwe used to train the user simulation.
Section 4 andsection 5 describe the dialogue system frameworkand the user simulation models.
In section 6, wepresent the training and in section 7, we presentthe evaluation for different REG policies.2 Related workThere are several ways in which natural languagegeneration (NLG) systems adapt to users.
Someof them adapt to a user?s goals, preferences, en-vironment and so on.
Our focus in this studyis restricted to the user?s lexical domain exper-tise.
Several NLG systems adapt to the user?s do-main expertise at different levels of generation -text planning (Paris, 1987), complexity of instruc-tions (Dale, 1989), referring expressions (Reiter,1991), and so on.
Some dialogue systems, suchas COMET, have also incorporated NLG modulesthat present appropriate levels of instruction to theuser (McKeown et al, 1993).
However, in all theabove systems, the user?s knowledge is assumed tobe accurately represented in an initial user modelusing which the system adapts its language.
Incontrast to all these systems, our adaptive REGpolicy knows nothing about the user when the con-versation starts.Rule-based and supervised learning approacheshave been proposed to learn and adapt during theconversation dynamically.
Such systems learnedfrom the user at the start and later adapted to thedomain knowledge of the users.
However, they ei-ther require expensive expert knowledge resourcesto hand-code the inference rules (Cawsey, 1993) orlarge corpus of expert-layperson interaction fromwhich adaptive strategies can be learned and mod-elled, using methods such as Bayesian networks(Akiba and Tanaka, 1994).
In contrast, we presentan approach that learns in the absence of these ex-pensive resources.
It is also not clear how super-vised and rule-based approaches choose betweenwhen to seek more information and when to adapt.In this study, we show that using reinforcementlearning this decision is learned automatically.Reinforcement Learning (RL) has been suc-cessfully used for learning dialogue managementpolicies since (Levin et al, 1997).
The learnedpolicies allow the dialogue manager to optimallychoose appropriate dialogue acts such as instruc-tions, confirmation requests, and so on, underuncertain noise or other environment conditions.There have been recent efforts to learn informationpresentation and recommendation strategies usingreinforcement learning (Rieser and Lemon, 2009;Hernandez et al, 2003; Rieser and Lemon, 2010),and joint optimisation of Dialogue Managementand NLG using hierarchical RL has been pro-posed by (Lemon, 2010).
In contrast, we present aframework to learn to choose appropriate referringexpressions based on a user?s domain knowledge.Earlier, we reported a proof-of-concept work us-ing a hand-coded rule-based user simulation (Ja-narthanam and Lemon, 2009c).3 The Wizard-of-Oz CorpusWe use a corpus of technical support dialoguescollected from real human users using a Wizard-of-Oz method (Janarthanam and Lemon, 2009b).The corpus consists of 17 dialogues from userswho were instructed to physically set up a homebroadband connection using objects like a wire-less modem, cables, filters, etc.
They listened tothe instructions from the system and carried themout using the domain objects laid in front of them.The human ?wizard?
played the role of only an in-terpreter who would understand what the user saidand annotate it as a dialogue act.
The set-up ex-amined the effect of using three types of referringexpressions (jargon, descriptive, and tutorial), onthe users.Out of the 17 dialogues, 6 used a jargon strat-egy, 6 used a descriptive strategy, and 5 used atutorial strategy1.
The task had reference to 13domain entities, mentioned repeatedly in the di-alogue.
In total, there are 203 jargon, 202 descrip-tive and 167 tutorial referring expressions.
Inter-estingly, users who weren?t acquainted with thedomain objects requested clarification on some ofthe referring expressions used.
The dialogue ex-changes between the user and system were loggedin the form of dialogue acts and the system?schoices of referring expressions.
Each user?sknowledge of domain entities was recorded bothbefore and after the task and each user?s interac-1The tutorial strategy uses both jargon and descriptive ex-pressions together.70tions with the environment were recorded.
We usethe dialogue data, pre-task knowledge tests, andthe environment interaction data to train a usersimulation model.
Pre and post-task test scoreswere used to model the learning behaviour of theusers during the task (see section 5).The corpus also recorded the time taken to com-plete each dialogue task.
We used these data tobuild a regression model to calculate total dialoguetime for dialogue simulations.
The strategies werenever mixed (with some jargon, some descriptiveand some tutorial expressions) within a single con-versation.
Therefore, please note that the strate-gies used for data collection were not adaptive andthe human ?wizard?
has no role in choosing whichreferring expression to present to the user.
Due tothis fact, no user score regarding adaptation wascollected.
We therefore measure adaptation objec-tively as explained in section 6.1.4 The Dialogue SystemIn this section, we describe the different modulesof the dialogue system.
The interaction betweenthe different modules is shown in figure 1 (inlearning mode).
The dialogue system presents theuser with instructions to setup a broadband con-nection at home.
In the Wizard of Oz setup, thesystem and the user interact using speech.
How-ever, in our machine learning setup, they interact atthe abstract level of dialogue actions and referringexpressions.
Our objective is to learn to choosethe appropriate referring expressions to refer to thedomain entities in the instructions.Figure 1: System User Interaction (learning)4.1 Dialogue ManagerThe dialogue manager identifies the next instruc-tion (dialogue act) to give to the user based on thedialogue management policy pidm.
Since, in thisstudy, we focus only on learning the REG policy,the dialogue management is coded in the form ofa finite state machine.
In this dialogue task, thesystem provides two kinds of instructions - ob-servation and manipulation.
For observation in-structions, users observe the environment and re-port back to the system, and for the manipulationinstructions (such as plugging in a cable in to asocket), they manipulate the domain entities in theenvironment.
When the user carries out an instruc-tion, the system state is updated and the next in-struction is given.
Sometimes, users do not under-stand the referring expressions used by the systemand then ask for clarification.
In such cases, thesystem provides clarification on the referring ex-pression (provide clar), which is information toenable the user to associate the expression withthe intended referent.
The system action As,t (tdenoting turn, s denoting system) is therefore toeither give the user the next instruction or a clarifi-cation.
When the user responds in any other way,the instruction is simply repeated.
The dialoguemanager is also responsible for updating and man-aging the system state Ss,t (see section 4.2).
Thesystem interacts with the user by passing both thesystem action As,t and the referring expressionsRECs,t (see section 4.3).4.2 The dialogue stateThe dialogue state Ss,t is a set of variables thatrepresent the current state of the conversation.
Inour study, in addition to maintaining an overall di-alogue state, the system maintains a user modelUMs,t which records the initial domain knowl-edge of the user.
It is a dynamic model that startswith a state where the system does not have anyidea about the user.
As the conversation pro-gresses, the dialogue manager records the evi-dence presented to it by the user in terms of hisdialogue behaviour, such as asking for clarifica-tion and interpreting jargon.
Since the model isupdated according to the user?s behaviour, it maybe inaccurate if the user?s behaviour is itself uncer-tain.
So, when the user?s behaviour changes (forinstance, from novice to expert), this is reflectedin the user model during the conversation.
Hence,unlike previous studies mentioned in section 2, theuser model used in this system is not always an ac-curate model of the user?s knowledge and reflectsa level of uncertainty about the user.71Each jargon referring expression x is repre-sented by a three valued variable in the dialoguestate: user knows x.
The three values that eachvariable takes are yes, no, not sure.
The vari-ables are updated using a simple user model up-date algorithm.
Initially each variable is set tonot sure.
If the user responds to an instructioncontaining the referring expression x with a clari-fication request, then user knows x is set to no.Similarly, if the user responds with appropriate in-formation to the system?s instruction, the dialoguemanager sets user knows x is set to yes.The dialogue manager updates the variablesconcerning the referring expressions used in thecurrent system utterance appropriately after theuser?s response each turn.
The user may have thecapacity to learn jargon.
However, only the user?sinitial knowledge is recorded.
This is based on theassumption that an estimate of the user?s knowl-edge helps to predict the user?s knowledge of therest of the referring expressions.
Another issueconcerning the state space is its size.
Since, thereare 13 entities and we only model the jargon ex-pressions, the state space size is 313.4.3 REG moduleThe REG module is a part of the NLG modulewhose task is to identify the list of domain enti-ties to be referred to and to choose the appropriatereferring expression for each of the domain enti-ties for each given dialogue act.
In this study, wefocus only on the production of appropriate refer-ring expressions to refer to domain entities men-tioned in the dialogue act.
It chooses between thetwo types of referring expressions - jargon and de-scriptive.
For example, the domain entity broad-band filter can be referred to using the jargon ex-pression ?broadband filter?
or using the descrip-tive expression ?small white box?2.
We call thisthe act of choosing the REG action.
The tutorialstrategy was not investigated here since the corpusanalysis showed tutorial utterances to be very timeconsuming.
In addition, they do not contribute tothe adaptive behaviour of the system.The REG module operates in two modes - learn-ing and evaluation.
In the learning mode, the REGmodule is the learning agent.
The REG mod-ule learns to associate dialogue states with opti-mal REG actions.
This is represented by a REG2We will use italicised forms to represent the domain enti-ties (e.g.
broadband filter) and double quotes to represent thereferring expressions (e.g.
?broadband filter?
).policy pireg : UMs,t ?
RECs,t, which mapsthe states of the dialogue (user model) to optimalREG actions.
The referring expression choicesRECs,t is a set of pairs identifying the refer-ent R and the type of expression T used in thecurrent system utterance.
For instance, the pair(broadband filter, desc) represents the descriptiveexpression ?small white box?.RECs,t = {(R1, T1), ..., (Rn, Tn)}In the evaluation mode, a trained REG policy in-teracts with unknown users.
It consults the learnedpolicy pireg to choose the referring expressionsbased on the current user model.5 User SimulationsIn this section, we present user simulation modelsthat simulate the dialogue behaviour of a real hu-man user.
These external simulation models aredifferent from internal user models used by thedialogue system.
In particular, our model is thefirst to be sensitive to a system?s choices of refer-ring expressions.
The simulation has a statisticaldistribution of in-built knowledge profiles that de-termines the dialogue behaviour of the user beingsimulated.
If the user does not know a referringexpression, then he is more likely to request clar-ification.
If the user is able to interpret the refer-ring expressions and identify the references thenhe is more likely to follow the system?s instruc-tion.
This behaviour is simulated by the action se-lection models described below.Several user simulation models have been pro-posed for use in reinforcement learning of dia-logue policies (Georgila et al, 2005; Schatzmannet al, 2006; Schatzmann et al, 2007; Ai and Lit-man, 2007).
However, they are suited only forlearning dialogue management policies, and notnatural language generation policies.
Earlier, wepresented a two-tier simulation trained on dataprecisely for REG policy learning (Janarthanamand Lemon, 2009a).
However, it is not suited fortraining on small corpus like the one we have atour disposal.
In contrast to the earlier model, wenow condition the clarification requests on the ref-erent class rather than the referent itself to handledata sparsity problem.The user simulation (US) receives the systemaction As,t and its referring expression choicesRECs,t at each turn.
The US responds with auser action Au,t (u denoting user).
This can ei-ther be a clarification request (cr) or an instruction72response (ir).
We used two kinds of action selec-tion models: corpus-driven statistical model andhand-coded rule-based model.5.1 Corpus-driven action selection modelIn the corpus-driven model, the US produces aclarification request cr based on the class of thereferent C(Ri), type of the referring expressionTi, and the current domain knowledge of the userfor the referring expression DKu,t(Ri, Ti).
Do-main entities whose jargon expressions raised clar-ification requests in the corpus were listed andthose that had more than the mean number of clar-ification requests were classified as difficultand others as easy entities (for example, ?poweradaptor?
is easy - all users understood thisexpression, ?broadband filter?
is difficult).Clarification requests are produced using the fol-lowing model.P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))where (Ri, Ti) ?
RECs,tOne should note that the actual literal expres-sion is not used in the transaction.
Only the entitythat it is referring to (Ri) and its type (Ti) are used.However, the above model simulates the processof interpreting and resolving the expression andidentifying the domain entity of interest in the in-struction.
The user identification of the entity issignified when there is no clarification request pro-duced (i.e.
Au,t = none).
When no clarificationrequest is produced, the environment action EAu,tis generated using the following model.P (EAu,t|As,t) if Au,t!
= cr(Ri, Ti)Finally, the user action is an instruction re-sponse which is determined by the system actionAs,t.
Instruction responses can be different in dif-ferent conditions.
For an observe and report in-struction, the user issues a provide info actionand for a manipulation instruction, the user re-sponds with an acknowledgement action and soon.P (Au,t = ir|EAu,t, As,t)All the above models were trained on our cor-pus data using maximum likelihood estimation andsmoothed using a variant of Witten-Bell discount-ing.
According to the data, clarification requestsare much more likely when jargon expressionsare used to refer to the referents that belong tothe difficult class and which the user doesn?tlivebox = 1 power adaptor = 1wall phone socket = 1 broadband filter = 0broadband cable = 0 ethernet cable = 1lb power light = 1 lb power socket = 1lb broadband light = 0 lb ethernet light = 0lb adsl socket = 0 lb ethernet socket = 0pc ethernet socket = 1Table 2: Domain knowledge: an IntermediateUserknow about.
When the system uses expressionsthat the user knows, the user generally respondsto the instruction given by the system.
These usersimulation models have been evaluated and foundto produce behaviour that is very similar to theoriginal corpus data, using the Kullback-Leiblerdivergence metric (Cuayahuitl, 2009).5.2 Rule-based action selection modelWe also built a rule-based simulation using theabove models but where some of the parameterswere set manually instead of estimated from thedata.
The purpose of this simulation is to in-vestigate how learning with a data-driven statisti-cal simulation compares to learning with a simplehand-coded rule-based simulation.
In this simula-tion, the user always asks for a clarification whenhe does not know a jargon expression (regardlessof the class of the referent) and never does thiswhen he knows it.
This enforces a stricter, moreconsistent behaviour for the different knowledgepatterns, which we hypothesise should be easier tolearn to adapt to, but may lead to less robust REGpolicies.5.3 User Domain knowledgeThe user domain knowledge is initially set to oneof several models at the start of every conver-sation.
The models range from novices to ex-perts which were identified from the corpus usingk-means clustering.
The initial knowledge base(DKu,initial) for an intermediate user is shown intable 2.
A novice user knows only ?power adap-tor?, and an expert knows all the jargon expres-sions.
We assume that users can interpret the de-scriptive expressions and resolve their references.Therefore, they are not explicitly represented.
Weonly code the user?s knowledge of jargon expres-sions.
This is represented by a boolean variablefor each domain entity.73Corpus data shows that users can learn jargonexpressions during the conversation.
The user?sdomain knowledge DKu is modelled to be dy-namic and is updated during the conversation.Based on our data, we found that when presentedwith clarification on a jargon expression, users al-ways learned the jargon.if As,t = provide clar(Ri, Ti)DKu,t+1(Ri, Ti) ?
1Users also learn when jargon expressions are re-peatedly presented to them.
Learning by repetitionfollows the pattern of a learning curve - the greaterthe number of repetitions #(Ri, Ti), the higher thelikelihood of learning.
This is modelled stochas-tically based on repetition using the parameter#(Ri, Ti) as follows (where (Ri, Ti) ?
RECs,t) .P (DKu,t+1(Ri, Ti) ?
1|#(Ri, Ti))The final state of the user?s domain knowl-edge (DKu,final) may therefore be different fromthe initial state (DKu,initial) due to the learn-ing effect produced by the system?s use of jar-gon expressions.
In most studies done previously,the user?s domain knowledge is considered to bestatic.
However in real conversation, we found thatthe users nearly always learned jargon expressionsfrom the system?s utterances and clarifications.6 TrainingThe REG module was trained (operated in learn-ing mode) using the above simulations to learnREG policies that select referring expressionsbased on the user expertise in the domain.
Asshown in figure 1, the learning agent (REG mod-ule) is given a reward at the end of every dialogue.During the training session, the learning agent ex-plores different ways to maximize the reward.
Inthis section, we discuss how to code the learningagent?s goals as reward.
We then discuss how thereward function is used to train the learning agent.6.1 Reward functionA reward function generates a numeric reward forthe learning agent?s actions.
It gives high rewardsto the agent when the actions are favourable andlow rewards when they are not.
In short, the re-ward function is a representation of the goal of theagent.
It translates the agent?s actions into a scalarvalue that can be maximized by choosing the rightaction sequences.We designed a reward function for the goal ofadapting to each user?s domain knowledge.
Wepresent the Adaptation Accuracy score AA thatcalculates how accurately the agent chose the ex-pressions for each referent r, with respect to theuser?s knowledge.
Appropriateness of an expres-sion is based on the user?s knowledge of the ex-pression.
So, when the user knows the jargon ex-pression for r, the appropriate expression to use isjargon, and if s/he doesn?t know the jargon, an de-scriptive expression is appropriate.
Although theuser?s domain knowledge is dynamically chang-ing due to learning, we base appropriateness onthe initial state, because our objective is to adapt tothe initial state of the user DKu,initial.
However,in reality, designers might want their system to ac-count for user?s changing knowledge as well.
Wecalculate accuracy per referent RAr as the ratioof number of appropriate expressions to the totalnumber of instances of the referent in the dialogue.We then calculate the overall mean accuracy overall referents as shown below.RAr = #(appropriate expressions(r))#(instances(r))AdaptationAccuracyAA = 1#(r)?rRArNote that this reward is computed at the end ofthe dialogue (it is a ?final?
reward), and is thenback-propagated along the action sequence thatled to that final state.
Thus the reward can be com-puted for each system REG action, without thesystem having access to the user?s initial domainknowledge while it is learning a policy.Since the agent starts the conversation withno knowledge about the user, it may try to usemore exploratory moves to learn about the user,although they may be inappropriate.
However,by measuring accuracy to the initial user state,the agent is encouraged to restrict its exploratorymoves and start predicting the user?s domainknowledge as soon as possible.
The system shouldtherefore ideally explore less and adapt more toincrease accuracy.
The above reward function re-turns 1 when the agent is completely accurate inadapting to the user?s domain knowledge and itreturns 0 if the agent?s REC choices were com-pletely inappropriate.
Usually during learning, thereward value lies between these two extremes andthe agent tries to maximize it to 1.746.2 LearningThe REG module was trained in learning mode us-ing the above reward function using the SHAR-SHA reinforcement learning algorithm (with lin-ear function approximation) (Shapiro and Langley,2002).
This is a hierarchical variant of SARSA,which is an on-policy learning algorithm that up-dates the current behaviour policy (see (Suttonand Barto, 1998)).
The training produced approx.5000 dialogues.
Two types of simulations wereused as described above: Data-driven and Hand-coded.
Both user simulations were calibrated toproduce three types of users: Novice, Int2 (in-termediate) and Expert, randomly but with equalprobability.
Novice users knew just one jargonexpression, Int2 knew seven, and Expert usersknew all thirteen jargon expressions.
There wasan underlying pattern in these knowledge profiles.For example, Intermediate users were those whoknew the commonplace domain entities but notthose specific to broadband connection.
For in-stance, they knew ?ethernet cable?
and ?pc ether-net socket?
but not ?broadband filter?
and ?broad-band cable?.Initially, the REG policy chooses randomly be-tween the referring expression types for each do-main entity in the system utterance, irrespectiveof the user model state.
Once the referring expres-sions are chosen, the system presents the user sim-ulation with both the dialogue act and referring ex-pression choices.
The choice of referring expres-sion affects the user?s dialogue behaviour which inturn makes the dialogue manager update the usermodel.
For instance, choosing a jargon expres-sion could evoke a clarification request from theuser, which in turn prompts the dialogue managerto update the user model with the new informationthat the user is ignorant of the particular expres-sion.
It should be noted that using a jargon expres-sion is an information seeking move which enablesthe REG module to estimate the user?s knowledgelevel.
The same process is repeated for every dia-logue instruction.
At the end of the dialogue, thesystem is rewarded based on its choices of refer-ring expressions.
If the system chooses jargon ex-pressions for novice users or descriptive expres-sions for expert users, penalties are incurred and ifthe system chooses REs appropriately, the rewardis high.
On the one hand, those actions that fetchmore reward are reinforced, and on the other hand,the agent tries out new state-action combinationsto explore the possibility of greater rewards.
Overtime, it stops exploring new state-action combina-tions and exploits those actions that contribute tohigher reward.
The REG module learns to choosethe appropriate referring expressions based on theuser model in order to maximize the overall adap-tation accuracy.Figure 2 shows how the agent learns using thedata-driven (Learned DS) and hand-coded simu-lations (Learned HS) during training.
It can beseen in the figure 2 that towards the end the curveplateaus signifying that learning has converged.Figure 2: Learning curves - Training7 EvaluationIn this section, we present the evaluation metricsused, the baseline policies that were hand-codedfor comparison, and the results of evaluation.7.1 MetricsIn addition to the adaptation accuracy mentionedin section 6.1, we also measure other parame-ters from the conversation in order to show howlearned adaptive policies compare with other poli-cies on other dimensions.
We calculate the timetaken (Time) for the user to complete the dialoguetask.
This is calculated using a regression modelfrom the corpus based on number of words, turns,and mean user response time.
We also measurethe (normalised) learning gain (LG) produced byusing unknown jargon expressions.
This is calcu-lated using the pre and post scores from the userdomain knowledge (DKu) as follows.Learning Gain LG = Post?Pre1?Pre757.2 Baseline REG policiesIn order to compare the performance of the learnedpolicy with hand-coded REG policies, three sim-ple rule-based policies were built.
These werebuilt in the absence of expert domain knowledgeand a expert-layperson corpus.?
Jargon: Uses jargon for all referents by de-fault.
Provides clarifications when requested.?
Descriptive: Uses descriptive expressions forall referents by default.?
Switching: This policy starts with jargonexpressions and continues using them untilthe user requests for clarification.
It thenswitches to descriptive expressions and con-tinues to use them until the user complains.In short, it switches between the two strate-gies based on the user?s responses.All the policies exploit the user model in sub-sequent references after the user?s knowledge ofthe expression has been set to either yes or no.Therefore, although these policies are simple, theydo adapt to a certain extent, and are reasonablebaselines for comparison in the absence of expertknowledge for building more sophisticated base-lines.7.3 ResultsThe policies were run under a testing condition(where there is no policy learning or exploration)using a data-driven simulation calibrated to simu-late 5 different user types.
In addition to the threeusers - Novice, Expert and Int2, from the train-ing simulations, two other intermediate users (Int1and Int3) were added to examine how well eachpolicy handles unseen user types.
The REG mod-ule was operated in evaluation mode to producearound 200 dialogues per policy distributed overthe 5 user groups.Overall performance of the different policies interms of Adaptation Accuracy (AA), Time andLearning Gain (LG) are given in Table 3.
Fig-ure 3 shows how each policy performs in terms ofaccuracy on the 5 types of users.We found that the Learned DS policy (i.e.learned with the data-driven user simulation) isthe most accurate (Mean = 79.70, SD = 10.46)in terms of adaptation to each user?s initial stateof domain knowledge.
Also, it is the only pol-icy that has more or less the same accuracy scoresFigure 3: Evaluation - Adaptation AccuracyPolicies AA Time T LGDescriptive 46.15 7.44 0Jargon 74.54 9.15* 0.97*Switching 62.47 7.48 0.30Learned HS 69.67 7.52 0.33Learned DS 79.70* 8.08* 0.63** Significantly different from all oth-ers (p < 0.05).Table 3: Evaluation on 5 user typesover all different user types (see figure 3).
Itshould also be noted that the it generalised wellover user types (Int1 and Int3) which were un-seen in training.
Learned DS policy outperformsall other policies: Learned HS (Mean = 69.67, SD= 14.18), Switching (Mean = 62.47, SD = 14.18),Jargon (Mean = 74.54, SD = 17.9) and Descrip-tive (Mean = 46.15, SD = 33.29).
The differencesbetween the accuracy (AA) of the Learned DS pol-icy and all other policies were statistically signif-icant with p < 0.05 (using a two-tailed paired t-test).
Although Learned HS policy is similar tothe Learned DS policy, as shown in the learningcurves in figure 2, it does not perform as wellwhen confronted with users types that it did notencounter during training.
The Switching policy,on the other hand, quickly switches its strategy(sometimes erroneously) based on the user?s clar-ification requests but does not adapt appropriatelyto evidence presented later during the conversa-tion.
Sometimes, this policy switches erroneouslybecause of the uncertain user behaviours.
In con-trast, learned policies continuously adapt to newevidence.
The Jargon policy performs better than76the Learned HS and Switching policies.
This be-cause the system can learn more about the userby using more jargon expressions and then usethat knowledge for adaptation for known referents.However, it is not possible for this policy to pre-dict the user?s knowledge of unseen referents.
TheLearned DS policy performs better than the Jargonpolicy, because it is able to accurately predict theuser?s knowledge of referents unseen in the dia-logue so far.The learned policies are a little more time-consuming than the Switching and Descriptivepolicies but compared to the Jargon policy,Learned DS takes 1.07 minutes less time.
This isbecause learned policies use a few jargon expres-sions (giving rise to clarification requests) to learnabout the user.
On the other hand, the Jargon pol-icy produces more user learning gain because ofthe use of more jargon expressions.
Learned poli-cies compensate on time and learning gain in orderto predict and adapt well to the users?
knowledgepatterns.
This is because the training was opti-mized for accuracy of adaptation and not for learn-ing gain or time taken.
The results show that usingour RL framework, REG policies can be learnedusing data-driven simulations, and that such a pol-icy can predict and adapt to a user?s knowledgepattern more accurately than policies trained us-ing hand-coded rule-based simulations and hand-coded baseline policies.7.4 DiscussionThe learned policies explore the user?s expertiseand predict their knowledge patterns, in order tobetter choose expressions for referents unseen inthe dialogue so far.
The system learns to iden-tify the patterns of knowledge in the users witha little exploration (information seeking moves).So, when it is provided with a piece of evidence(e.g.
the user knows ?broadband filter?
), it is ableto accurately estimate unknown facts (e.g.
the usermight know ?broadband cable?).
Sometimes, itschoices are wrong due to incorrect estimation ofthe user?s expertise (due to stochastic behaviourof the users).
In such cases, the incorrect adapta-tion move can be considered to be an informationseeking move.
This helps further adaptation us-ing the new evidence.
By continuously using this?seek-predict-adapt?
approach, the system adaptsdynamically to different users.
Therefore, witha little information seeking and better prediction,the learned policies are able to better adapt to userswith different domain expertise.In addition to adaptation, learned policies learnto identify when to seek information from the userto populate the user model (which is initially setto not sure).
It should be noted that the sys-tem cannot adapt unless it has some informationabout the user and therefore needs to decisivelyseek information by using jargon expressions.
Ifit seeks information all the time, it is not adaptingto the user.
The learned policies therefore learn totrade-off between information seeking moves andadaptive moves in order to maximize the overalladaptation accuracy score.8 ConclusionIn this study, we have shown that user-adaptiveREG policies can be learned from a small cor-pus of non-adaptive dialogues between a dialoguesystem and users with different domain knowl-edge levels.
We have shown that such adaptiveREG policies learned using a RL framework adaptto unknown users better than simple hand-codedpolicies built without much input from domain ex-perts or from a corpus of expert-layperson adap-tive dialogues.
The learned, adaptive REG poli-cies learn to trade off between adaptive moves andinformation seeking moves automatically to max-imize the overall adaptation accuracy.
Learnedpolicies start the conversation with informationseeking moves, learn a little about the user, andstart adapting dynamically as the conversationprogresses.
We have also shown that a data-drivenstatistical user simulation produces better policiesthan a simple hand-coded rule-based simulation,and that the learned policies generalise well to un-seen users.In future work, we will evaluate the learnedpolicies with real users to examine how wellthey adapt, and examine how real users evalu-ate them (subjectively) in comparison to baselines.Whether the learned policies perform better or aswell as a hand-coded policy painstakingly craftedby a domain expert (or learned using supervisedmethods from an expert-layperson corpus) is aninteresting question that needs further exploration.Also, it would also be interesting to make thelearned policy account for the user?s learning be-haviour and adapt accordingly.77AcknowledgementsThe research leading to these results has receivedfunding from the European Community?s SeventhFramework Programme (FP7/2007-2013) undergrant agreement no.
216594 (CLASSiC projectwww.classic-project.org) and from theEPSRC, project no.
EP/G069840/1.ReferencesH.
Ai and D. Litman.
2007.
Knowledge consistentuser simulations for dialog systems.
In Proceedingsof Interspeech 2007, Antwerp, Belgium.T.
Akiba and H. Tanaka.
1994.
A Bayesian approachfor User Modelling in Dialogue Systems.
In Pro-ceedings of the 15th conference on ComputationalLinguistics - Volume 2, Kyoto.A.
Bell.
1984.
Language style as audience design.Language in Society, 13(2):145?204.A.
Cawsey.
1993.
User Modelling in Interactive Ex-planations.
User Modeling and User-Adapted Inter-action, 3(3):221?247.H.
H. Clark and G. L. Murphy.
1982.
Audience de-sign in meaning and reference.
In J. F. LeNy andW.
Kintsch, editors, Language and comprehension.Amsterdam: North-Holland.H.
Cuayahuitl.
2009.
Hierarchical ReinforcementLearning for Spoken Dialogue Systems.
Ph.D. the-sis, University of Edinburgh, UK.R.
Dale.
1989.
Cooking up referring expressions.
InProc.
ACL-1989.K.
Georgila, J. Henderson, and O.
Lemon.
2005.Learning User Simulations for Information StateUpdate Dialogue Systems.
In Proc of Eu-rospeech/Interspeech.F.
Hernandez, E. Gaudioso, and J. G. Boticario.
2003.A Multiagent Approach to Obtain Open and FlexibleUser Models in Adaptive Learning Communities.
InUser Modeling 2003, volume 2702/2003 of LNCS.Springer, Berlin / Heidelberg.E.
A. Issacs and H. H. Clark.
1987.
References inconversations between experts and novices.
Journalof Experimental Psychology: General, 116:26?37.S.
Janarthanam and O.
Lemon.
2009a.
A Two-tierUser Simulation Model for Reinforcement Learningof Adaptive Referring Expression Generation Poli-cies.
In Proc.
SigDial?09.S.
Janarthanam and O.
Lemon.
2009b.
A Wizard-of-Oz environment to study Referring Expression Gen-eration in a Situated Spoken Dialogue Task.
In Proc.ENLG?09.S.
Janarthanam and O.
Lemon.
2009c.
Learning Lexi-cal Alignment Policies for Generating Referring Ex-pressions for Spoken Dialogue Systems.
In Proc.ENLG?09.O.
Lemon.
2010.
Learning what to say and how to sayit: joint optimization of spoken dialogue manage-ment and Natural Language Generation.
ComputerSpeech and Language.
(to appear).E.
Levin, R. Pieraccini, and W. Eckert.
1997.
Learn-ing Dialogue Strategies within the Markov DecisionProcess Framework.
In Proc.
of ASRU97.K.
McKeown, J. Robin, and M. Tanenblatt.
1993.
Tai-loring Lexical Choice to the User?s Vocabulary inMultimedia Explanation Generation.
In Proc.
ACL1993.C.
L. Paris.
1987.
The Use of Explicit User Modelsin Text Generations: Tailoring to a User?s Level ofExpertise.
Ph.D. thesis, Columbia University.E.
Reiter.
1991.
Generating Descriptions that Exploit aUser?s Domain Knowledge.
In R. Dale, C. Mellish,and M. Zock, editors, Current Research in NaturalLanguage Generation, pages 257?285.
AcademicPress.V.
Rieser and O.
Lemon.
2009.
Natural LanguageGeneration as Planning Under Uncertainty for Spo-ken Dialogue Systems.
In Proc.
EACL?09.V.
Rieser and O.
Lemon.
2010.
Optimising informa-tion presentation for spoken dialogue systems.
InProc.
ACL.
(to appear).J.
Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.Young.
2006.
A Survey of Statistical User Sim-ulation Techniques for Reinforcement Learning ofDialogue Management Strategies.
Knowledge Engi-neering Review, pages 97?126.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye,and S. J.
Young.
2007.
Agenda-based User Simula-tion for Bootstrapping a POMDP Dialogue System.In Proc of HLT/NAACL 2007.D.
Shapiro and P. Langley.
2002.
Separating skillsfrom preference: Using learning to program by re-ward.
In Proc.
ICML-02.R.
Sutton and A. Barto.
1998.
Reinforcement Learn-ing.
MIT Press.78
