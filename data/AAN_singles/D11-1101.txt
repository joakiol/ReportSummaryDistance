Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1092?1103,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLearning General Connotation of Words using Graph-based AlgorithmsSong Feng Ritwik Bose Yejin ChoiDepartment of Computer ScienceStony Brook UniversityNY 11794, USAsongfeng, rbose, ychoi@cs.stonybrook.eduAbstractIn this paper, we introduce a connotation lex-icon, a new type of lexicon that lists wordswith connotative polarity, i.e., words with pos-itive connotation (e.g., award, promotion) andwords with negative connotation (e.g., cancer,war).
Connotation lexicons differ from muchstudied sentiment lexicons: the latter concernswords that express sentiment, while the formerconcerns words that evoke or associate witha specific polarity of sentiment.
Understand-ing the connotation of words would seem torequire common sense and world knowledge.However, we demonstrate that much of theconnotative polarity of words can be inferredfrom natural language text in a nearly unsu-pervised manner.
The key linguistic insightbehind our approach is selectional preferenceof connotative predicates.
We present graph-based algorithms using PageRank and HITSthat collectively learn connotation lexicon to-gether with connotative predicates.
Our em-pirical study demonstrates that the resultingconnotation lexicon is of great value for sen-timent analysis complementing existing senti-ment lexicons.1 IntroductionIn this paper, we introduce a connotation lexicon,a new type of lexicon that lists words with conno-tative polarity, i.e., words with positive connotation(e.g., award, promotion) and words with negativeconnotation (e.g., cancer, war).
Connotation lexi-cons differ from sentiment lexicons that are studiedin much of previous research (e.g., Esuli and Sebas-tiani (2006), Wilson et al (2005a)): the latter con-cerns words that express sentiment either explicitlyor implicitly, while the former concerns words thatevoke or even simply associate with a specific polar-ity of sentiment.
To our knowledge, there has beenno previous research that investigates polarized con-notation lexicons.Understanding the connotation of words wouldseem to require common sense and world knowl-edge at first glance, which in turn might seem to re-quire human encoding of knowledge base.
However,we demonstrate that much of the connotative polar-ity of words can be inferred from natural languagetext in a nearly unsupervised manner.The key linguistic insight behind our approach isselectional preference of connotative predicates.
Wedefine a connotative predicate as a predicate thathas selectional preference on the connotative polar-ity of some of its semantic arguments.
For instance,in the case of the connotative predicate ?prevent?,there is strong selectional preference on negativeconnotation with respect to the thematic role (se-mantic role) ?THEME?.
That is, statistically speak-ing, people tend to associate negative connotationwith the THEME of ?prevent?, e.g., ?prevent can-cer?
or ?prevent war?, rather than positive conno-tation, e.g., ?prevent promotion?.
In other words,even though it is perfectly valid to use words withpositive connotation in the THEME role of ?pre-vent?, statistically more dominant connotative po-larity is negative.
Similarly, the THEME of ?con-gratulate?
or ?praise?
has strong selectional prefer-ence on positive connotation.The theoretical concept supporting the selective1092accomplish, achieve, advance, advocate, admire,applaud, appreciate, compliment, congratulate,develop, desire, enhance, enjoy, improve, praise,promote, respect, save, support, winTable 1: Positively Connotative Predicates w.r.t.
THEMEalleviate, accuse, avert, avoid, cause, complain,condemn, criticize, detect, eliminate, eradicate,mitigate, overcome, prevent, prohibit, protest, re-frain, suffer, tolerate, withstandTable 2: Negatively Connotative Predicates w.r.t.
THEMEpreference of connotative predicates is that of se-mantic prosody in corpus linguistics.
Semanticprosody describes how some of the seemingly neu-tral words (e.g., ?cause?)
can be perceived with pos-itive or negative polarity because they tend to col-locate with words with corresponding polarity (e.g.,Sinclair (1991), Louw et al (1993), Stubbs (1995),Stefanowitsch and Gries (2003)).
In this work, wedemonstrate that statistical approaches that exploitthis very concept of semantic prosody can success-fully infer connotative polarity of words.Having described the key linguistic insight, wenow illustrate our graph-based algorithms.
Figure 1depicts the mutually reinforcing relation betweenconnotative predicates (nodes on the left-hand side)and words with connotative polarity (node on theright-hand side).
The thickness of edges representsthe strength of the association between predicatesand arguments.
For brevity, we only consider conno-tation of words that appear in the THEME thematicrole.We expect that words that appear often in theTHEME role of various positively (or negatively)connotative predicates are likely to be words withpositive (or negative) connotation.
Likewise, pred-icates whose THEME contains words with mostlypositive (or negative) connotation are likely to bepositively (or negatively) connotative predicates.
Inshort, we can induce the connotative polarity ofwords using connotative predicates, and inversely,we can learn new connotative predicates based onwords with connotative polarity.We hypothesize that this mutually reinforcing re-PreventAvoidAlleviate CancerIncidentPromotionOvercome TragedyFigure 1: Bipartite graph of connotative predicates andarguments.
Edge weights are proportionate to the associ-ation strength.lation between connotative predicates and their ar-guments can be captured via graph centrality ingraph-based algorithms.
Given a small set of seedwords for connotative predicates, our algorithmscollectively learn connotation lexicon together withconnotative predicates in a nearly unsupervisedmanner.
A number of different graph representa-tions are explored using both PageRank (Page et al,1999) and HITS (Kleinberg, 1999) algorithms.
Em-pirical study demonstrates that our graph based al-gorithms are highly effective in learning both con-notation lexicon and connotative predicates.Finally, we quantify the practical value of ourconnotation lexicon in concrete sentiment analysisapplications, and demonstrate that the connotationlexicon is of great value for sentiment classificationtasks complementing conventional sentiment lexi-cons.2 Connotation Lexicon & ConnotativePredicateIn this section, we define connotation lexicon andconnotative predicates more formally, and contrastthem against words in conventional sentiment lexi-cons.2.1 Connotation LexiconThis lexicon lists words with positive and negativeconnotation, as defined below.?
Words with positive connotation: In thiswork, we define words with positive connota-tion as those that describe physical objects orabstract concepts that people generally value,cherish or care about.
For instance, we regardwords such as ?freedom?, ?life?, or ?health?
as1093words with positive connotation.
Some of thesewords may express subjectivity either explic-itly or implicitly, e.g., ?joy?
or ?satisfaction?.However, a substantial number of words withpositive connotation are purely objective, suchas ?life?, ?health?, ?tenure?, or ?scientific?.?
Words with negative connotation: We definewords with negative connotation as those thatdescribe physical objects or abstract conceptsthat people generally disvalue or avoid.
Sim-ilarly as before, some of these words may ex-press subjectivity (e.g., ?disappointment?, ?hu-miliation?
), while many other are purely objec-tive (e.g., ?bedbug?, ?arthritis, ?funeral?
).Note that this explicit and intentional inclusion ofobjective terms makes connotation lexicons differfrom sentiment lexicons: most conventional senti-ment lexicons have focused on subjective words bydefinition (e.g., Wilson et al (2005b)), as many re-searchers use the term sentiment and subjectivity in-terchangeably (e.g., Wiebe et al (2005)).2.2 Connotative PredicateIn this work, connotative predicates are those thatexhibit selectional preference on the connotative po-larity of some of their arguments.
We emphasize thatthe polarity of connotative predicates does not coin-cide with the polarity of sentiment in conventionalsentiment lexicons, as will be elaborated below.?
Positively connotative predicate: In thiswork, we define positively connotative predi-cates as those that expect positive connotationin some arguments.
For example, ?congratu-late?
or ?save?
are positively connotative pred-icates that expect words with positive conno-tation in the THEME argument: people typi-cally congratulate something positive, and savesomething people care about.
More examplesare shown in Table 1.?
Negatively connotative predicate: In thiswork, we define negatively connotative predi-cates as those that expect negative connotationin some arguments.
For instance, predicatessuch as ?prevent?
or ?suffer?
tend to projectnegative connotation in the THEME argument.More examples are shown in Table 2.Note that positively connotative predicates are notnecessarily positive sentiment words.
For instance?save?
is not a positive sentiment word in thelexicon published by Wilson et al (2005b).
In-versely, (strongly) positive sentiment words are notnecessarily (strongly) positively connotative predi-cates, e.g., ?illuminate?, ?agree?.
Likewise, neg-atively connotative predicates are not necessarilynegative sentiment words.
For instance, predicatessuch as ?prevent?, ?detect?, or ?cause?
are notnegative sentiment words, but they tend to corre-late with negative connotation in the THEME argu-ment.
Inversely, (strongly) negative sentiment wordsare not necessarily (strongly) negatively connotativepredicates, e.g., ?abandon?
(?abandoned [somethingvaluable]?
).3 Graph RepresentationIn this section, we explore the graphical representa-tion of our task.
Figure 1 depicts the key intuition asa bipartite graph, where the nodes on the left-handside correspond to connotative predicates, and thenodes on the right-hand side correspond to words inthe THEME argument.
There is an edge between apredicate p and an argument a, if the argument aappears in the THEME role of the predicate p. Forbrevity, we explore only verbs as the predicates, andwords in the THEME role of the predicates as argu-ments.
Our work can be readily extended to exploitother predicate-argument relations however.Note that there are many sources of noise in theconstruction of graph.
For instance, some of thepredicates might be negated, changing the semanticdynamics between the predicate and the argument.In addition, there might be many unusual combina-tions of predicates and arguments, either due to dataprocessing errors or due to idiosyncratic use of lan-guage.
Some of such combinations can be valid ones(e.g., ?prevent promotion?
), challenging the learningalgorithm with confusing evidence.We hypothesize that by focusing on the importantpart of the graph via centrality analysis , it is possibleto infer connotative polarity of words despite variousnoise introduced in the graph structure.
This impliesthat it is important to construct the graph structure soas to capture important linguistic relations betweenpredicates and arguments.
With this goal in mind,1094we next explore the directionality of the edges anddifferent strategies to assign weights to them.3.1 Undirected (Symmetric) GraphFirst we explore undirected edges.
In this case,we assign weight for each undirected edge betweena predicate p and an argument a.
Intuitively, theweight should correspond to the strength of relat-edness or association between the predicate p andthe argument a.
We use Pointwise Mutual Infor-mation (PMI), as it has been used by many pre-vious research to quantify the association betweentwo words (e.g., Turney (2001), Church and Hanks(1990)).
The PMI score between p and a is definedas follows:w(p?
a) := PMI(p, a) = log P (p, a)P (p)P (a)The log of the ratio is positive when the pair ofwords tends to co-occur and negative when the pres-ence of one word correlates with the absence of theother word.3.2 Directed (Asymmetric) GraphNext we explore directed edges.
That is, for eachconnected pair of a predicate p and an argument a,there are two edges in opposite directions: e(p?
a)and e(a ?
p).
In this case, we explore the useof asymmetric weights using conditional probabil-ity.
In particular, we define weights as follows:w(p?
a) := P (a|p) = P (p, a)P (p)w(a?
p) := P (p|a) = P (p, a)P (a)Having defined the graph structure, next we explorealgorithms that analyze graph centrality via randomwalks.
In particular, we investigate the use of HITSalgorithm (Section 4), and PageRank (Section 5).4 Lexicon Induction using HITSThe graph representation described thus far (Sec-tion 3) captures general semantic relations betweenpredicates and arguments, rather than those specificto connotative predicates and arguments.
Thereforein this section, we explore techniques to augmentthe graph representation so as to bias the centralityof the graph toward connotative predicates and argu-ments.In order to establish a learning bias, we start witha small set of seed words for just connotative predi-cates.
We use 20 words for each polarity, as listed inTable 1 and Table 2.
These seed words act as priorknowledge in our learning.
We explore two differenttechniques to incorporate prior knowledge into ran-dom walk, as will be elaborated in Section 4.2 & 4.3,followed by brief description of HITS in Section 4.1.4.1 Hyperlink-Induced Topic Search (HITS)HITS (Hyperlink-Induced Topic Search) algorithm(Kleinberg, 1999), also known as Hubs and author-ities, is a link analysis algorithm that is particularlysuitable to model mutual reinforcement between twodifferent types of nodes: hubs and authorities.
Thedefinitions of hubs and authorities are given recur-sively.
A (good) hub is a node that points to many(good) authorities, and a (good) authority is a nodepointed by many (good) hubs.Notice that the mutually reinforcing relation-ship is precisely what we intend to model betweenconnotative predicates and arguments.
Let G =(P,A,E) be the bipartite graph, where P is the setof nodes corresponding to connotative predicates, Ais the set of nodes corresponding to arguments, andE is the set of edges among nodes.
(Pi, Aj) ?
Eif and only if the predicate Pi and the argument Aioccur together as a predicate ?
argument pair in thecorpus.
The co-occurrence matrix derived from ourcorpus is denoted as L, whereLij ={w(i, j) if(Pi, Aj) ?
E0 otherwiseThe value of w(i, j) is set to w(i ?
j) as definedin Section 3.1 for undirected graphs, and w(i ?
j)defined in Section 3.2 for directed graphs.Let a(Ai) and h(Ai) be the authority and hubscore respectively, for a given node Ai ?
A. Thenwe compute the authority and hub score recursivelyas follows:a(Ai) =?Pi,Aj?Ew(i, j)h(Aj) +?Pj ,Ai?Eh(Pj)w(j, i)h(Ai) =?Pi,Aj?Ew(i, j)a(Aj) +?Pj ,Ai?Ea(Pj)w(j, i)1095The scores a(Pi) and h(Pi) for Pi ?
P are definedsimilarly as above.In what follows, we describe two different tech-niques to incorporate prior knowledge.
Note that itis possible to apply each of the following techniquesto both directed and undirected graph representa-tions introduced in Section 3.
Also note that for eachtechnique, we construct two separate graphsG+ andG?
corresponding to positive and negative polarityrespectively.
That is, G+ learns positively connota-tive predicates and arguments, while G?
learns neg-atively connotative predicates and arguments.4.2 Prior Knowledge via Truncated GraphFirst we introduce a method based on graph trunca-tion.
In this method, when constructing the bipartitegraph, we limit the set of predicates P to only thosewords in the seed set, instead of including all wordsthat can be predicates.
In a way, the truncated graphrepresentation can be viewed as the query inducedgraph on which the original HITS algorithm was in-vented (Kleinberg, 1999).The truncated graph is very effective in reducingthe level of noise that can be introduced by predi-cates of the opposite polarity.
It may seem like wecannot discover new connotative predicates in thetruncated graph however, as the graph structure islimited only to the seed predicates.
We address thisissue by alternating truncation to different side of thegraph, i.e., left (predicates) or right (arguments), asillustrated in Figure 1, through multiple rounds ofHITS.For instance, we start with the graph G =(P o, A,E(P o)) that is truncated only on the left-hand side, with the seed predicates P o. Here,E(P o)denotes the reduced set of edges discarding thoseedges that connect to predicates not in P o.
Then, weapply HITS algorithm until convergence to discovernew words with connotation, and this completes thefirst round of HITS.Next we begin the second round.
Let Ao be thenew words with connotation that are found in thefirst round.
We now set Ao as seed words for thesecond phase of HITS, where we construct a newgraph G = (P,Ao, E(Ao)) that is truncated onlyon the right-hand side, with full candidate words forpredicates included on the left-hand side.
This al-ternation can be repeated multiple times to discovermany new connotative predicates and arguments.4.3 Prior Knowledge via Focussed GraphIn the truncated graph described above, one poten-tial concern is that the discovery of new words withconnotation is limited to those that happen to corre-late well with the seed predicates.
To mitigate thisproblem, we explore an alternative technique basedon the full graph, which we will name as focussedgraph.In this method, instead of truncating the graph, wesimply emphasize the important portion of the graphvia edge weights.
That is, we assign high weights tothose edges that connect a seed predicate with an ar-gument, while assigning low weights for those edgesthat connect to a predicate outside the seed set.
Thisway, we allow predicates not in the seed set to par-ticipate in hubs and authority scores, but in a muchsuppressed way.
This method can be interpreted asa smoothed version of the truncated graph describedin Section 4.2.More formally, if the node Ai is connected tothe seed predicate Pj , the value of co-occurrencematrix Lij is defined by prior knowledge(e.g.PMI(Ai, Pj) or P (Ai|Pj) ), otherwise a small con-stant  is assigned to the edge.Lij ={w(i, j) ifPj ?
Eo otherwiseSimilarly to the truncated graph, we proceed withmultiple rounds of HITS, focusing different part ofthe bipartite graph alternately.5 Lexicon Induction using PageRankIn this section, we explore the use of another popu-lar approach for link analysis: PageRank (Page etal., 1999).
We first describe PageRank algorithmbriefly in Section 5.1, then introduce two differenttechniques to incorporate prior knowledge in Sec-tion 5.2 and 5.3.5.1 PageRankLet G = (V,E) be the graph, where vi ?
V =P ?
A are nodes (words) for the disjunctive set ofpredicates (P ) and arguments (A), and e(i,j) ?
Eare edges.
Let In(i) be the set of nodes with anedge leading to ni and similarly, Out(i) be the set1096of nodes that ni has an edge leading to.
At a giveniteration of the algorithm, we update the score of nias follows:S(i) = ??j?In(i)S(j)?
w(i, j)|Out(i)| + (1?
?)
(1)where the value ?
is constant damping factor.
Thevalue of ?
is typically set to 0.85.
The value ofw(i, j) is set to w(i?j) as defined in Section 3.1 forundirected graphs, and w(i ?
j) as defined in Sec-tion 3.2 for directed graphs.
As before, we will con-sider two different techniques to incorporate priorknowledge into the graph analysis as follows.5.2 Prior Knowledge via Truncated GraphUnlike HITS, which was originally invented for aquery-induced graph, PageRank is typically appliedto the full graph.
However, we can still apply thetruncation technique introduced in Section 4.2 toPageRank as well.
To do so, when constructing thebipartite graph, we limit the set of predicates P toonly those words in the seed set, instead of includingall words that can be predicates.
Graph truncationeliminates the noise that can be introduced by pred-icates of the opposite polarity.
However, in order tolearn new predicates, we need to perform multiplerounds of PageRank, truncating different side of thebipartite graph alternately.
Refer to Section 4.2 forfuther details.5.3 Prior Knowledge via TeleportationWe next explore what is known as teleportationtechnique for topic sensitive PageRank (Haveliwala,2002).
For this, we use the following equation thatis slightly augmented from Equation 1.S(i) = ??j?In(i)S(j)?
w(i, j)|Out(i)| + (1?
?)
i (2)Here, the new term i is a smoothing factor that pre-vents cliques in the graph from garnering reputationthrough feedback (Bianchini et al (2005)).
In or-der to emphasize important portion of the graph, i.e.,subgraphs connected to the seed set, we assign non-zero  scores to only those important nodes, i.e., theseed set.
Intuitively, this will cause the random walkto restart from the seed set with (1??)
= 0.15 prob-ability for each step.6 The Use of Google Web 1T DataIn order to implement the network of connotativepredicates and arguments, we need a substantiallylarge amount of documents.
The quality of the co-occurrence statistics is expected to be proportionateto the size of corpus, but collecting and process-ing such a large amount of data is not trivial.
Wetherefore resort to the Google Web 1T data (Brantsand Franz., 2006), which consists of Google n-gramcounts (frequency of occurrence of each n-gram) for1 ?
n ?
5.
The use of Web 1T data will lessen thechallenge with respect to data acquisition, while stillallowing us to enjoy the co-occurrence statistics ofweb-scale data.
Because Web 1T data is just n-gramstatistics, rather than a collection of normal docu-ments, it does not provide co-occurrence statistics ofany random word pairs.
However, it provides a niceapproximation to the particular co-occurrence statis-tics we are interested in, which are, predicate ?
ar-gument pairs.
This is because the THEME argumentof a verb predicate is typically on the right hand sideof the predicate, and the argument is within the closerange of the predicate.We now describe how to derive co-occurrencestatistics of each predicate ?
argument pair using theWeb 1T data.
For a given predicate p and an argu-ment a, we add up the count (frequency) of all n-grams (2 ?
n ?
5) that match the following pattern:[p] [?
]n?2 [a]where p must be the first word (head), a must be thelast word (tail), and [?
]n?2 matches any n?
2 num-ber of words between p and a.
Note that this ruleenforces the argument a to be on the right hand sideof the predicate p. To reduce the level of noise, wedo not allow the wildcard [?]
to match any punctu-ation mark, as such n-grams are likely to cross sen-tence boundaries representing invalid predicate ?
ar-gument relations.
We consider a word as a predicateif it is tagged as a verb by a Part-of-Speech tagger(Toutanova and Manning, 2000).
For argument [a],we only consider content-words.The use of web n-gram statistics necessarily in-vites certain kinds of noise.
For instance, some ofthe [p] [?
]n?2 [a] patterns might not correspond toa valid predicate ?
argument relation.
However, weexpect that our graph-based algorithms ?
HITS and1097Lexicon FREQ HITS-sT HITS-aT HITS-sF HITS-aF Page-aT Page-aFTop 100 73.6 67.8 77.7 67.8 48.4 76.3 77.0Top 1000 67.8 60.6 68.8 60.6 38.0 68.4 68.5Top MAX 65.8 57.6 66.5 57.6 39.1 65.5 65.7Table 3: Comparison Result with General Inquirer Lexicon(%)Lexicon FREQ HITS-sT HITS-aT HITS-sF HITS-aF Page-aT Page-aFTop 100 83.0 79.3 86.3 79.3 55.8 86.3 87.2Top 1000 80.3 67.3 81.3 67.3 46.5 80.7 80.3Top MAX 71.5 62.7 72.2 62.7 45.4 71.1 72.3Table 4: Comparison Result with OpinionFinder (%)PageRank ?
will be able to discern valid relationsfrom noise, by focusing on the important part of thegraph.
In other words, we expect that good predi-cates will be supported by good arguments, and viceversa, thereby resulting in a reliable set of predicatesand arguments that are mutually supported by eachother.7 ExperimentsAs a baseline, we use a simple method dubbedFREQ, which uses co-occurrence frequency withrespect to the seed predicates.
Using the pattern[p] [?
]n?2 [a] (see Section 6), we collect two setsof n-gram records: one set using the positive con-notative predicates, and the other using the negativeconnotative predicates.
With respect to each set, wecalculate the following for each word a,?
Given [a], the number of unique [p] as f1?
Given [a], the number of unique phrases [?
]n?2as f2?
The number of occurrences of [a] as f3We then obtain the score ?a+ for positive connota-tion and ?a?
for negative connotation using the fol-lowing equations that take a linear combination off1, f2, and f3 that we computed above with respectto each polarity.
?a+ = ??
?f1+ + ?
?
?f2+ + ?
?
?f3+ (3)?a?
= ??
?f1?
+ ?
?
?f2?
+ ?
?
?f3?
(4)Note that the coefficients ?, ?
and ?
are determinedexperimentally.
We assign positive polarity to theword a, if ?a+ >> ?a?
and vice versa.7.1 Comparison against Sentiment LexiconThe polarity defined in the connotation lexicon dif-fers from that of conventional sentiment lexicons inwhich we aim to recognize more subtle sentimentthat correlates with words.
Nevertheless, we provideagreement statistics between our connotation lexi-con and conventional sentiment lexicons for com-parison purposes.
We collect statistics with respectto the following two resources: General Inquirer(Stone and Hunt, 1963) and Opinion Finder (Wilsonet al, 2005b).For polarity ?
?
{+,?
}, let countsentlex(?)
denotethe total number of words labeled as ?
in a givensentiment lexicon, and let countagreement(?)
denotethe total number of words labeled as ?
by both thegiven sentiment lexicon and our connotation lexi-con.
In addition, let countoverlap(?)
denote the totalnumber of words that are labeled as ?
by our conno-tation lexicon that are also included in the referencelexicon with or without the same polarity.
Then wecompute prec?
as follows:prec?
% =countagreement(?)countoverlap(?)?
100We compare prec?
% for three different segmentsof our lexicon: the top 100, top 1000, and the entirelexicon.
We compare the lexicons provided by theseven variations of our algorithm.
Results are shownin Table 3 & 4.The acronym of each different method is definedas follows: HITS-sT & HITS-aT correspond tothe Symmetric (undirected) and Asymmetric (di-rected) version of the Truncated method respec-tively.
HITS-sF & HITS-aF correspond to the1098Positive: include, offer, obtain, allow, build, in-crease, ensure, contain, pursue, fulfill, maintain,recommend, represent, require, respectNegative: abate, die, condemn, deduce, investi-gate, commit, correct, apologize, debilitate, dis-pel, endure, exacerbate, indicate, induce, mini-mizeTable 5: Examples of newly discovered connotative pred-icatesPositive: boogie, housewarming, persuasiveness,kickoff, playhouse, diploma, intuitively, monu-ment, inaugurate, troubleshooter, accompanistNegative: seasickness, overleap, gangrenous,suppressing, fetishist, unspeakably, doubter,bloodmobile, bureaucratizedTable 6: Examples of newly discovered words with con-notations: these words are treated as neutral in some con-ventional sentiment lexicons.symmetric and asymmetric version of the Focusedmethod.
Finally, Page-aT & Page-aF correspond tothe Truncation and teleportation (Focused) respec-tively.Asymmetric HITS on a directed truncated graph(HITS-aT) and topic-sensitive PageRank (Page-aF)achieve the best performance in most cases, espe-cially for top ranked words which have a higheraverage frequency.
The difference between thesetwo top performers is not large, but statisticallysignificant using wilcoxon test with p < 0.03.Standard PageRank (Page-aT) achieves the thirdbest performance overall.
All these top performingones (HITS-aT, Page-aF, Page-aT) outperform thebaseline approach (FREQ) statistically significantlywith p < 0.001.
For brevity, we omit the PageRankresults based on the undirected graphs, as the perfor-mance of those was not as good as that of directedones.7.2 Extrinsic Evaluation via SentimentAnalysisNext we perform extrinsic evaluation to quantify thepractical value of our connotation lexicon in con-crete sentiment analysis applications.
In particular,we make use of our connotation lexicon for binarysentiment classification tasks in two different ways:?
Unsupervised classification by voting.
We de-fine r as the ratio of positive polarity words tonegative polarity words in the lexicon.
In ourexperiment, penalty is 0 for positive and ?0.5for negative.score(x+) = 1 + penalty+(r,#positive)score(x?)
= ?1 + penalty?(r,#negative)?
Supervised classification using SVM.
We usebag-of-words features for baseline.
In orderto quantify the effect of different lexicons, weadd additional features based on the followingscores as defined below:scoreraw(x) =?wxs(w)scorepurity(x) =scoreraw(x)?wx abs(s(w))The two corpora we use are SemEval2007 (Strap-parava and Mihalcea, 2007) and Sentiment Twitter.1The Twitter dataset consists of tweets containing ei-ther a smiley emoticon (representing positive senti-ment) or a frowny emoticon (representing negativesentiment), we randomly select 50000 smiley tweetsand 50000 frowny tweets.2 We perform a 5-foldcross validation.In Table 8, we find very promising results, partic-ularly for Twitter dataset, which is known to be verynoisy.
Notice that the use of Top 6k words fromour connotation lexicon along with OpinionFinderlexicon boost the performance up to 78.0%, whichis significantly better than than 71.4% using onlythe conventional OpinionFinder lexicon.
This resultshows that our connotation lexicon nicely comple-ments existing sentiment lexicon, improving practi-cal sentiment analysis tasks.1http://www.stanford.edu/?
alecmgo/cs224n/twitterdata.2009.05.25.c.zip2We filter out stop-words and words appearing less than 3times.
For Twitter, we also remove usernames of the format@username occurring within tweet bodies.1099Algorithm 1st Round 2nd RoundAcc.
F-val Acc.
F-valVoting 68.7 65.4 71.0 68.5Bag of Words 69.9 65.1 69.9 65.1(??)
+ OpFinder 74.7 75.0 74.7 75.0BoW + Top 2k 73.3 74.5 73.7 75.4(??)
+ OpFinder 72.8 73.5 75.0 77.6BoW + Top 6k 76.6 77.1 74.5 75.3(??)
+ OpFinder 74.1 73.5 75.2 76.0BoW + Top 10k 74,1 73.5 74.2 73.8(??)
+ OpFinder 73.5 74.3 74.7 75.1Table 7: SemEval Classification Result(%) ?
(??)
denotesthat all features in the previous row are copied over.Algorithm 1st Round 2nd RoundAcc.
F-val Acc.
F-valVoting 60.4 59.1 62.6 61.3Bag of Words 69.9 72.1 69.9 72.1(??)
+ OpFinder 70.3 71.4 70.3 71.4BoW + Top 2k 71.3 65.4 72.7 73.3(??)
+ OpFinder 69.4 63.1 73.1 74.6BoW + Top 6k 77.2 69.0 76.4 77.6(??)
+ OpFinder 76.4 72.0 76.8 78.0BoW + Top 10k 73.3 73.5 73.7 74.1(??)
+ OpFinder 74.1 69.5 73.5 74.2Table 8: Twitter Classification Result(%) ?
(??)
denotesthat all features in the previous row are copied over.7.3 Intrinsic Evaluation via Human JudgmentIn order to measure the quality of the connotationlexicon, we also perform human judgment study ona subset of the lexicon.
Human judges are asked toquantify the degree of connotative polarity of eachgiven word using an integer value between 1 and 5,where 1 and 5 correspond to the most negative andpositive connotation respectively.
When computingthe annotator agreement score or evaluating our con-notation lexicon against human judgment, we con-solidate 1 and 2 into a single negative class and 4and 5 into a single positive class.
The Kappa scorebetween two human annotators is 0.78.As a control set, we also include 100 words takenfrom the General Inquirer lexicon: 50 words withpositive sentiment, and 50 words with negative sen-timent.
These words are included so as to mea-sure the quality of human judgment against a well-established sentiment lexicon.
The words were pre-sented in a random order so that the human judgeswill not know which words are from the General In-quirer lexicon and which are from our connotativelexicon.
For the words in the control set, the anno-tators achieved 94% (97% lenient) accuracy on thepositive set and 97% on the negative set.Note that some words appear in both positive andnegative connotation graphs, while others appear inonly one of them.
For instance, if a given word xappears as an argument for only positive connotativepredicates, but never for negative ones, then xwouldappear only in the positive connotation graph.
Thismeans that for such word, we can assume the conno-tative polarity even without applying the algorithmsfor graph centrality.
Therefore, we first evaluate theaccuracy of the polarity of such words that appearonly in one of the connotation graphs.
We discardwords with low frequency (300 in terms of Googlen-gram frequency), and randomly select 50 wordsfrom each polarity.
The accuracy of such words is88% by strict evaluation and 94.5% by lenient eval-uation, where lenient evaluation counts words in ourpolarized connotation lexicon to be correct if the hu-man judges assign non-conflicting polarities, i.e., ei-ther neutral or identical polarity.For words that appear in both positive and nega-tive connotation graphs, we determine the final po-larity of such words as one with higher scores givenby HITS or PageRank.
We randomly select wordsthat rank at 5% of top 100, top 1000, top 2000, andtop 5000 by each algorithm for human judgment.We only evaluate the top performing algorithms ?HITS-aT and Page-aF ?
and FREQ baseline.
Thestratified performance for each of these methods isgiven in Table 9.8 Related WorkGraph based approaches have been used in manyprevious research for lexicon induction.
A tech-nique named label propagation (Zhu and Ghahra-mani, 2002) has been used by Rao and Ravichan-dran (2009) and Velikovich et al (2010), while ran-dom walk based approaches, PageRank in particular,have been used by Esuli and Sebastiani (2007).
Inour work, we explore the use of both HITS (Klein-berg, 1999) and PageRank (Page et al, 1999) and1100Average Positive NegativeTop # Str.
Len.
Str.
Len.
Str.
Len.FREQ@100 73.5 87.3 72.2 91.1 74.7 83.5@1000 51.8 78.6 44.4 75.6 81.8 90.9@2000 66.9 74.7 73.1 84.2 57.3 60.0@5000 61.5 81.3 61.4 84.1 62.0 70.0HITS-aT@100 61.3 79.8 74.4 93.3 47.0 65.1@1000 39.6 75.5 48.1 77.8 30.8 73.1@2000 57.7 72.1 78.0 86.0 41.0 60.7@5000 55.6 73.5 69.7 85.7 44.3 63.8Page-aF@100 63.0 78.6 74.7 91.2 50.0 64.6@1000 53.7 72.2 54.5 72.7 53.1 71.9@2000 56.5 79.6 67.2 91.8 42.6 63.8@5000 57.1 76.2 75.7 91.0 43.3 65.3Table 9: Human Annotation Accuracies(%) ?
Str.
de-notes strict evaluation & Len.
denotes lenient evaluation.present systematic comparison of various options forgraph representation and encoding of prior knowl-edge.
We are not aware of any previous researchthat made use of HITS algorithm for connotation orsentiment lexicon induction.Much of previous research investigated the use ofdictionary network (e.g., WordNet) for lexicon in-duction (e.g., Kamps et al (2004), Takamura et al(2005), Adreevskaia and Bergler (2006), Esuli andSebastiani (2006), Su and Markert (2009), Moham-mad et al (2009)), while relatively less research in-vestigated the use of web documents (e.g., Kaji andKitsuregawa (2007), Velikovich et al (2010))).Wilson et al (2005b) first introduced the sen-timent lexicon, spawning a great deal of researchthereafter.
At the beginning, sentiment lexiconswere designed to include only those words that ex-press sentiment, that is, subjective words.
Howeverin recent years, sentiment lexicons started expand-ing to include some of those words that simply asso-ciate with sentiment, even if those words are purelyobjective (e.g., Velikovich et al (2010), Baccianellaet al (2010)).
This trend applies even to the most re-cent version of the lexicon of Wilson et al (2005b).We conjecture that this trend of broader coveragesuggests that such lexicons are practically more use-ful than sentiment lexicons that include only thosewords that are strictly subjective.
In this work, wemake this transition more explicit and intentional,by introducing a novel connotation lexicon.Mohammad and Turney (2010) focussed on emo-tion evoked by common words and phrases.
Thespirit of their work shares some similarity with oursin that it aims to find the emotion evoked by words,as opposed to expressed.
Two main differences are:(1) our work aims to discover even more subtle asso-ciation of words with sentiment, and (2) we presenta nearly unsupervised approach, while Mohammadand Turney (2010) explored the use of MechanicalTurk to build the lexicon based on human judgment.In the work of Osgood et al (1957), it has beendiscussed that connotative meaning of words canbe measured in multiple scales of semantic differ-ential, for example, the degree of ?goodness?
and?badness?.
Our work presents statistical approachesthat measure one such semantic differential auto-matically.
Our graph construction to capture word-to-word relation is analogous to that of Collins-Thompson and Callan (2007), where the graph rep-resentation was used to model more general defini-tions of words.9 ConclusionWe introduced the connotation lexicon, a novel lex-icon that list words with connotative polarity, whichwill be made publically available.
We also pre-sented graph-based algorithms for learning conno-tation lexicon together with connotative predicatesin a nearly unsupervised manner.
Our approachesare grounded on the linguistic insight with respect tothe selectional preference of connotative predicates.Empirical study demonstrates the practical value ofthe connotation lexicon for sentiment analysis en-couraging further research in this direction.AcknowledgmentsWe wholeheartedly thank the reviewers for veryhelpful and insightful comments.ReferencesAlina Adreevskaia and Sabine Bergler.
2006.
Miningwordnet for fuzzy sentiment: Sentiment tag extractionfrom wordnet glosses.
In 11th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 209?216.1101Stefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2010.
Sentiwordnet 3.0: An enhancedlexical resource for sentiment analysis and opinionmining.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European LanguageResources Association (ELRA).Monica Bianchini, Marco Gori, and Franco Scarselli.2005.
Inside pagerank.
ACM Trans.
Internet Technol.,5:92?128, February.Thorsten Brants and Alex Franz.
2006.
Web 1t 5-gramversion 1.
In Linguistic Data Consortium, ISBN: 1-58563-397-6, Philadelphia.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-phy.
Comput.
Linguist., 16:22?29, March.K.
Collins-Thompson and J. Callan.
2007.
Automaticand human scoring of word definition responses.
InProceedings of NAACL HLT, pages 476?483.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sentiword-net: A publicly available lexical resource for opinionmining.
In In Proceedings of the 5th Conference onLanguage Resources and Evaluation (LREC06, pages417?422.Andrea Esuli and Fabrizio Sebastiani.
2007.
Pagerank-ing wordnet synsets: An application to opinion min-ing.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 424?431.
Association for Computational Linguistics.Taher H. Haveliwala.
2002.
Topic-sensitive pagerank.
InProceedings of the Eleventh International World WideWeb Conference, Honolulu, Hawaii.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Build-ing lexicon for sentiment analysis from massive collec-tion of HTML documents.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 1075?1083.Jaap Kamps, Maarten Marx, Robert J. Mokken, andMaarten De Rijke.
2004.
Using wordnet to mea-sure semantic orientation of adjectives.
In Proceed-ings of the 4th International Conference on LanguageResources and Evaluation (LREC), pages 1115?1118.Jon M. Kleinberg.
1999.
Authoritative sources in a hy-perlinked environment.
JOURNAL OF THE ACM,46(5):604?632.B.
Louw, M. Baker, G. Francis, and E. Tognini-Bonelli.1993.
Irony in the text or insincerity in the writer?the diagnostic potential of semantic prosodies.
TEXTAND TECHNOLOGY IN HONOUR OF JOHN SIN-CLAIR, pages 157?176.Saif Mohammad and Peter Turney.
2010.
Emotionsevoked by common words and phrases: Using me-chanical turk to create an emotion lexicon.
In Pro-ceedings of the NAACL HLT 2010 Workshop on Com-putational Approaches to Analysis and Generation ofEmotion in Text, pages 26?34, Los Angeles, CA, June.Association for Computational Linguistics.Saif Mohammad, Cody Dunne, and Bonnie Dorr.
2009.Generating high-coverage semantic orientation lexi-cons from overtly marked words and a thesaurus.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages599?608, Singapore, August.
Association for Compu-tational Linguistics.C.
E. Osgood, G. Suci, and P. Tannenbaum.
1957.
Themeasurement of meaning.
University of Illinois Press,Urbana, IL.Lawrence Page, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1999.
The pagerank citation ranking:Bringing order to the web.
Technical Report 1999-66,Stanford InfoLab, November.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In EACL ?09:Proceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 675?682, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.John Sinclair.
1991.
Corpus, concordance, colloca-tion.
Describing English language.
Oxford UniversityPress.A.
Stefanowitsch and S.T.
Gries.
2003.
Collostructions:Investigating the interaction of words and construc-tions.
International Journal of Corpus Linguistics,8(2):209?243.Philip J.
Stone and Earl B.
Hunt.
1963.
A computer ap-proach to content analysis: studies using the generalinquirer system.
In Proceedings of the May 21-23,1963, spring joint computer conference, AFIPS ?63(Spring), pages 241?256, New York, NY, USA.
ACM.Carlo Strapparava and Rada Mihalcea.
2007.
Semeval-2007 task 14: affective text.
In SemEval ?07: Pro-ceedings of the 4th International Workshop on Seman-tic Evaluations, pages 70?74, Morristown, NJ, USA.Association for Computational Linguistics.M.
Stubbs.
1995.
Collocations and semantic profiles:on the cause of the trouble with quantitative studies.Functions of language, 2(1):23?55.Fangzhong Su and Katja Markert.
2009.
Subjectivityrecognition on word senses via semi-supervised min-cuts.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 1?9.
Association for ComputationalLinguistics.1102Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of words usingspin model.
In Proceedings of ACL-05, 43rd AnnualMeeting of the Association for Computational Linguis-tics, Ann Arbor, US.
Association for ComputationalLinguistics.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In In EMNLP/VLC2000, pages 63?70.Peter Turney.
2001.
Mining the web for synonyms: Pmi-ir versus lsa on toefl.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-nan, and Ryan McDonald.
2010.
The viability of web-derived polarity lexicons.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.
Association for Computational Lin-guistics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation (for-merly Computers and the Humanities), 39(2/3):164?210.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.2005a.
Opinionfinder: a system for subjectivity anal-ysis.
In Proceedings of HLT/EMNLP on InteractiveDemonstrations, pages 34?35, Morristown, NJ, USA.Association for Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT ?05: Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing,pages 347?354, Morristown, NJ, USA.
Association forComputational Linguistics.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learn-ing from labeled and unlabeled data with label prop-agation.
In Technical Report CMU-CALD-02-107.CarnegieMellon University.1103
