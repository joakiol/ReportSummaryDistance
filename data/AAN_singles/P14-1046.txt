Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 489?499,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsInterpretable Semantic Vectors from a Joint Model of Brain- and Text-Based MeaningAlona Fyshe1, Partha P. Talukdar1, Brian Murphy2, Tom M. Mitchell11Machine Learning Department, Carnegie Mellon University2School of Electronics, Electrical Engineering and Computer ScienceQueen?s University Belfast[afyshe,partha.talukdar,tom.mitchell]@cs.cmu.edubrian.murphy@qub.ac.ukAbstractVector space models (VSMs) representword meanings as points in a high dimen-sional space.
VSMs are typically createdusing a large text corpora, and so repre-sent word semantics as observed in text.We present a new algorithm (JNNSE) thatcan incorporate a measure of semanticsnot previously used to create VSMs: brainactivation data recorded while people readwords.
The resulting model takes advan-tage of the complementary strengths andweaknesses of corpus and brain activationdata to give a more complete representa-tion of semantics.
Evaluations show thatthe model 1) matches a behavioral mea-sure of semantics more closely, 2) canbe used to predict corpus data for unseenwords and 3) has predictive power thatgeneralizes across brain imaging technolo-gies and across subjects.
We believe thatthe model is thus a more faithful represen-tation of mental vocabularies.1 IntroductionVector Space Models (VSMs) represent lexicalmeaning by assigning each word a point in high di-mensional space.
Beyond their use in NLP appli-cations, they are of interest to cognitive scientistsas an objective and data-driven method to discoverword meanings (Landauer and Dumais, 1997).Typically, VSMs are created by collecting wordusage statistics from large amounts of text data andapplying some dimensionality reduction techniquelike Singular Value Decomposition (SVD).
Thebasic assumption is that semantics drives a per-son?s language production behavior, and as a resultco-occurrence patterns in written text indirectlyencode word meaning.
The raw co-occurrencestatistics are unwieldy, but in the compressedVSM the distance between any two words is con-ceived to represent their mutual semantic similar-ity (Sahlgren, 2006; Turney and Pantel, 2010), asperceived and judged by speakers.
This space thenreflects the ?semantic ground truth?
of shared lex-ical meanings in a language community?s vocab-ulary.
However corpus-based VSMs have beencriticized as being noisy or incomplete representa-tions of meaning (Glenberg and Robertson, 2000).For example, multiple word senses collide in thesame vector, and noise from mis-parsed sentencesor spam documents can interfere with the final se-mantic representation.When a person is reading or writing, the se-mantic content of each word will be necessarilyactivated in the mind, and so in patterns of ac-tivity over individual neurons.
In principle then,brain activity could replace corpus data as inputto a VSM, and contemporary imaging techniquesallow us to attempt this.
Functional Magnetic Res-onance Imaging (fMRI) and Magnetoencephalog-raphy (MEG) are two brain activation recordingtechnologies that measure neuronal activation inaggregate, and have been shown to have a pre-dictive relationship with models of word mean-ing (Mitchell et al, 2008; Palatucci et al, 2009;Sudre et al, 2012; Murphy et al, 2012b).1If brain activation data encodes semantics, wetheorized that including brain data in a model ofsemantics could result in a model more consistentwith semantic ground truth.
However, the inclu-sion of brain data will only improve a text-basedmodel if brain data contains semantic informationnot readily available in the corpus.
In addition,if a semantic test involves another subject?s brainactivation data, performance can improve only ifthe additional semantic information is consistentacross brains.
Of course, brains differ in shape,size and in connectivity, so additional informationencoded in one brain might not translate to an-1For more details on fMRI and MEG, see Section 4.2489other.
Furthermore, different brain imaging tech-nologies measure very different correlates of neu-ronal activity.
Due to these differences, it is possi-ble that one subject?s brain activation data cannotimprove a model?s performance on another sub-ject?s brain data, or for brain data collected usinga different recording technology.
Indeed, inter-subject models of brain activation is an open re-search area (Conroy et al, 2013), as is learning therelationship between recording technologies (En-gell et al, 2012; Hall et al, 2013).
Brain datacan also be corrupted by many types of noise (e.g.recording room interference, movement artifacts),another possible hindrance to the use of brain datain VSMs.VSMs are interesting from both engineeringand scientific standpoints.
In this work we fo-cus on the scientific question: Can the inclusionof brain data improve semantic representationslearned from corpus data?
What can we learn fromsuch a model?
From an engineering perspective,brain activation data will likely never replace textdata.
Brain activation recordings are both expen-sive and time consuming to collect, whereas tex-tual data is vast and much of it is free to download.However, from a scientific perspective, combiningtext and brain data could lead to more consistentsemantic models, in turn leading to a better un-derstanding of semantics and semantic modelinggenerally.In this paper, we leverage both kinds of data tobuild a hybrid VSM using a new matrix factor-ization method (JNNSE).
Our hypothesis is thatthe noise of brain and corpus derived statisticswill be largely orthogonal, and so the two datasources will have complementary strengths as in-put to VSMs.
If this hypothesis is correct, weshould find that the resulting VSM is more suc-cessful in modeling word semantics as encoded inhuman judgements, as well as separate corpus andbrain data that was not used in the derivation of themodel.
We will show that our method:1. creates a VSM that is more correlated to anindependent measure of word semantics.2.
produces word vectors that are more pre-dictable from the brain activity of differentpeople, even when brain data is collectedwith a different recording technology.3.
predicts corpus representations of withheldwords more accurately than a model that doesnot combine data sources.4.
directly maps semantic concepts onto thebrain by jointly learning neural representa-tions.Together, these results suggest that corpus andbrain activation data measure semantics in com-patible and complimentary ways.
Our resultsare evidence that a joint model of brain- andtext-based semantics may be closer to seman-tic ground truth than text-only models.
Ourfindings also indicate that there is additional se-mantic information available in brain activationdata that is not present in corpus data, and thatthere are elements of semantics currently lack-ing in text-based VSMs.
We have made avail-able the top performing VSMs created with brainand text data (http://www.cs.cmu.edu/?afyshe/papers/acl2014/).In the following sections we will review NNSE,and our extension, JNNSE.
We will describe thedata used and the experiments to support our posi-tion that brain data is a valuable source of semanticinformation that compliments text data.2 Non-Negative Sparse EmbeddingNon-Negative Sparse Embedding (NNSE) (Mur-phy et al, 2012a) is an algorithm that producesa latent representation using matrix factorization.Standard NNSE begins with a matrix X ?
Rw?cmade of c corpus statistics for w words.
NNSEsolves the following objective function:argminA,Dw?i=1??Xi,:?Ai,:?D?
?2+ ???A?
?1(1)subject to: Di,:DTi,:?
1,?
1 ?
i ?
` (2)Ai,j?
0, 1 ?
i ?
w, 1 ?
j ?
` (3)The solution will find a matrix A ?
Rw?`that issparse, non-negative, and represents word seman-tics in an `-dimensional latent space.
D ?
R`?cgives the encoding of corpus statistics in the la-tent space.
Together, they factor the original cor-pus statistics matrix X in a way that minimizesthe reconstruction error.
TheL1constraint encour-ages sparsity in A; ?
is a hyperparameter.
Equa-tion 2 constrains D to eliminate solutions whereA is made arbitrarily small by making D arbi-trarily large.
Equation 3 ensures that A is non-negative.
We may increase ` to give more dimen-sional space to represent word semantics, or de-crease ` for more compact representations.490The sparse and non-negative representation inA produces a more interpretable semantic space,where interpretability is quantified with a behav-ioral task (Chang et al, 2009; Murphy et al,2012a).
To illustrate the interpretability of NNSE,we describe a word by selecting the word?s topscoring dimensions, and selecting the top scoringwords in those dimensions.
For example, the wordchair has the following top scoring dimensions:1. chairs, seating, couches;2. mattress, futon, mattresses;3. supervisor, coordinator, advisor.These dimensions cover two of the distinct mean-ings of the word chair (furniture and person ofpower).NNSE?s sparsity constraint dictates that eachword can have a non-zero score in only a few di-mensions, which aligns well to previous featureelicitation experiments in psychology.
In featureelicitation, participants are asked to name the char-acteristics (features) of an object.
The number ofcharacteristics named is usually small (McRae etal., 2005), which supports the requirement of spar-sity in the learned latent space.3 Joint Non-Negative Sparse EmbeddingWe extend NNSEs to incorporate an additionalsource of data for a subset of the words in X ,and call the approach Joint Non-Negative SparseEmbeddings (JNNSEs).
The JNNSE algorithmis general enough to incorporate any new infor-mation about the a word w, but for this studywe will focus on brain activation recordings ofa human subject reading single words.
Wewill incorporate either fMRI or MEG data, andcall the resulting models JNNSE(fMRI+Text) andJNNSE(MEG+Text) and refer to them generallyas JNNSE(Brain+Text).
For clarity, from hereon, we will refer to NNSE as NNSE(Text), orNNSE(Brain) depending on the single source ofinput data used.Let us order the rows of the corpus data X sothat the first 1 .
.
.
w?rows have both corpus statis-tics and brain activation recordings.
Each brainactivation recording is a row in the brain data ma-trix Y ?
Rw?
?vwhere v is the number of featuresderived from the recording.
For MEG recordings,v =sensors ?
time points= 306?
150.
For fMRIv = grey-matter voxels =' 20, 000 depending onthe brain anatomy of each individual subject.
Thenew objective function is:argminA,D(c),D(b)w?i=1??Xi,:?Ai,:?D(c)??2+w??i=1??Yi,:?Ai,:?D(b)?
?2+ ???A?
?1(4)subject to: D(c)i,:D(c)i,:T?
1, ?
1 ?
i ?
` (5)D(b)i,:D(b)i,:T?
1,?
1 ?
i ?
` (6)Ai,j?
0, 1 ?
i ?
w, 1 ?
j ?
`(7)We have introduced an additional constraint on therows 1 .
.
.
w?, requiring that some of the learnedrepresentations in A also reconstruct the brain ac-tivation recordings (Y ) through representations inD(b)?
R`?v.
Let us use A?to refer to the brain-constrained rows of A.
Words that are close in?brain space?
must have similar representations inA?, which can further percolate to affect the rep-resentations of other words in A via closeness in?corpus space?.With A or D fixed, the objective function forNNSE(Text) and JNNSE(Brain+Text) is convex.However, we are solving forA andD, so the prob-lem is non-convex.
To solve for this objective, weuse the online algorithm of Section 3 from Mairalet al (Mairal et al, 2010).
This algorithm isguaranteed to converge, and in practice we foundthat JNNSE(Brain+Text) converged as quickly asNNSE(Text) for the same `.
We used the SPAMSpackage2to solve, and set ?
= 0.025.
This al-gorithm was a very easy extension to NNSE(Text)and required very little additional tuning.We also consider learning shared representa-tions in the case where data X and Y contain theeffects of known disjoint features.
For example,when a person reads a word, the recorded brainactivation data Y will contain the physiologicalresponse to viewing the stimulus, which is unre-lated to the semantics of the word.
These sig-nals can be attributed to, for example, the num-ber of letters in the word and the number of whitepixels on the screen (Sudre et al, 2012).
To ac-count for such effects in the data, we augmentA?with a set of n fixed, manually defined fea-tures (e.g.
word length) to create A?percept?Rw?(`+n).
D(b)?
R(`+n)?vis used withA?percept,2SPAMS Package: http://spams-devel.gforge.inria.fr/491to reconstruct the brain data Y .
More gener-ally, one could instead allocate a certain num-ber of latent features specific to X or Y, both ofwhich could be learned, as explored in some re-lated work (Gupta et al, 2013).
We use 11 per-ceptual features that characterize the non-semanticfeatures of the word stimulus (for a list, see sup-plementary material at http://www.cs.cmu.edu/?afyshe/papers/acl2014/).The JNNSE algorithm is advantageous in thatit can handle partially paired data.
That is, thealgorithm does not require that every row in Xalso have a row in Y .
Fully paired data is a re-quirement of many other approaches (White et al,2012; Jia and Darrell, 2010).
Our approach al-lows us to leverage the semantic information incorpus data even for words without brain activa-tion recordings.JNNSE(Brain+Text) does not require brain datato be mapped to a common average brain, whichis often the case when one wants to generalize be-tween human subjects.
Such mappings can blurand distort data, making it less useful for subse-quent prediction steps.
We avoid these mappings,and instead use the fact that similar words elicitsimilar brain activation within a subject.
In theJNNSE algorithm, it is this closeness in ?brainspace?
that guides the creation of the latent spaceA.
Leveraging intra-subject distance measuresto study inter-subject encodings has been studiedpreviously (Kriegeskorte et al, 2008a; Raizadaand Connolly, 2012), and has even been usedacross species (humans and primates) (Kriegesko-rte et al, 2008b).Though we restrict ourselves to using one sub-ject per JNNSE(Brain+Text) model, the JNNSEalgorithm could easily be extended to includedata from multiple brain imaging experiments byadding a new squared loss term for additionalbrain data.3.1 Related WorkPerhaps the most well known related approachto joining data sources is Canonical CorrelationAnalysis (CCA) (Hotelling, 1936), which has beenapplied to brain activation data in the past (Rus-tandi et al, 2009).
CCA seeks two linear trans-formations that maximally correlate two data setsin the transformed form.
CCA requires that thedata sources be paired (all rows in the corpus datamust have a corresponding brain data), as corre-lation between points is integral to the objective.To apply CCA to our data we would need to dis-card the vast majority of our corpus data, and useonly the 60 rows of X with corresponding rowsin Y.
While CCA holds the input data fixed andmaximally correlates the transformed form, wehold the transformed form fixed and seek a solu-tion that maximally correlates the reconstruction(AD(c)or A?D(b)) with the data (X and Y respec-tively).
This shift in error compensation is whatallows our data to be only partially paired.
Whilea Bayesian formulation of CCA can handle miss-ing data, our model has missing data for> 97% ofthe full w ?
(v + c) brain and corpus data matrix.To our knowledge, this extreme amount of missingdata has not been explored with Bayesian CCA.One could also use a topic model style formula-tion to represent this semantic representation task.Supervised topic models (Blei and McAuliffe,2007) use a latent topic to generate two observedoutputs: words in a document and a categorical la-bel for the document.
The same idea could be ap-plied here: the latent semantic representation gen-erates the observed brain activity and corpus statis-tics.
Generative and discriminative models bothhave their own strengths and weaknesses, gener-ative models being particularly strong when datasources are limited (Ng and Jordan, 2002).
Ourtask is an interesting blend of data-limited anddata-rich problem scenarios.In the past, various pieces of additional informa-tion have been incorporated into semantic models.For example, models with behavioral data (Sil-berer and Lapata, 2012) and models with visualinformation (Bruni et al, 2011; Silberer et al,2013) have both shown to improve semantic rep-resentations.
Other works have correlated VSMsbuilt with text or images with brain activationdata (Murphy et al, 2012b; Anderson et al, 2013).To our knowledge, this work is the first to integratebrain activation data into the construction of theVSM.4 Data4.1 Corpus DataThe corpus statistics used here are the download-able vectors from Fyshe et al (2013)3.
Theyare compiled from a 16 billion word subset ofClueWeb09 (Callan and Hoy, 2009) and containtwo types of corpus features: dependency and doc-ument features, found to be complimentary for3http://www.cs.cmu.edu/?afyshe/papers/conll2013/492most tasks.
Dependency statistics were derivedby dependency parsing the corpus and compil-ing counts for all dependencies incident on theword.
Document statistics are word-documentco-occurrence counts.
Count thresholding wasapplied to reduce noise, and positive pointwise-mutual-information (PPMI) (Church and Hanks,1990) was applied to the counts.
SVD was ap-plied to the document and dependency statisticsand the top 1000 dimensions of each type wereretained.
We selected the rows corresponding tonoun-tagged words (approx.
17000 words).4.2 Brain Activation DataWe have MEG and fMRI data at our disposal.MEG measures the magnetic field caused by manythousands of neurons firing together, and has goodtime resolution (1000 Hz) but poor spatial reso-lution.
fMRI measures the change in blood oxy-genation that results from differential neural ac-tivity, and has good spatial resolution but poortime resolution (0.5-1 Hz).
We have fMRI dataand MEG data for 18 subjects (9 in each imagingmodality) viewing 60 concrete nouns (Mitchell etal., 2008; Sudre et al, 2012).
The 60 words span12 word categories (animals, buildings, tools, in-sects, body parts, furniture, building parts, uten-sils, vehicles, objects, clothing, food).
Each of the60 words was presented with a line drawing, soword ambiguity is not an issue.
For both record-ing modalities, all trials for a particular word wereaveraged together to create one training instanceper word, with 60 training instances in all for eachsubject and imaging modality.
More preprocess-ing details appear in the supplementary material.5 Experimental ResultsHere we explore several variations of JNNSE andNNSE formulations.
For a comparison of themodels used, see Table 1.5.1 Correlation to Behavioral DataTo test if our joint model of Brain+Text is closerto semantic ground truth we compared the latentrepresentation A learned via JNNSE(Brain+Text)or NNSE(Text) to an independent behavioral mea-sure of semantics.
We collected behavioral datafor the 60 nouns in the form of answers to 218semantic questions.
Answers were gathered withMechanical Turk.
The full list of questions ap-pear in the supplementary material.
Some exam-ple questions are:?Is it alive?
?, and ?Can it bend?
?.Mechanical Turk users were asked to respond toeach question for each word on a scale of 1-5.
Atleast 3 respondents answered each question andthe median score was used.
This gives us a se-mantic representation of each of the 60 words ina 218-dimensional behavioral space.
Because werequired answers to each of the questions for allwords, we do not have the problems of sparsitythat exist for feature production norms from otherstudies (McRae et al, 2005).
In addition, our an-swers are ratings, rather than binary yes/no an-swers.For a given value of ` we solve the NNSE(Text)and JNNSE(Brain+Text) objective function as de-tailed in Equation 1 and 4 respectively.
We com-pared JNNSE(Brain+Text) and NNSE(Text) mod-els by measuring the correlation of all pairwisedistances in JNNSE(Brain+Text) and NNSE(Text)space to the pairwise distances in the 218-dimensional semantic space.
Distances werecalculated using normalized Euclidean distance(equivalent in rank-ordering to cosine distance,but more suitable for sparse vectors).
Figure 1shows the results of this correlation test.
The er-ror bars for the JNNSE(Brain+Text) models rep-resent a 95% confidence interval calculated usingthe standard error of the mean (SEM) over the 9person-specific JNNSE(Brain+Text) models.
Be-cause there is only one NNSE(Text) model foreach dimension setting, no SEM can be calculated,but it suffices to show that the NNSE(Text) corre-lation does not fall into the 95% confidence inter-val of the JNNSE(Brain+Text) models.
The SVDmatrix for the original corpus data has correlation0.4279 to the behavioral data, also below the 95%confidence interval for all JNNSE models.
The re-sults show that a model that incorporates brain ac-tivation data is more faithful to a behavioral mea-sure of semantics.5.2 Word Prediction from Brain ActivationWe now show that the JNNSE(Brain+Text) vec-tors are more consistent with independent sam-ples of brain activity collected from different sub-jects, even when recorded using different record-ing technologies.
As previously mentioned, be-cause there is a large degree of variation betweenbrains and because MEG and fMRI measure verydifferent correlates of neuronal activity, this typeof generalization has proven to be very challeng-ing and is an open research question in the neuro-science community.The output A of the JNNSE(Brain+Text) or493Table 1: A Comparison of the models explored in this paper, and the data upon which they operate.Model Name Section(s) Text Data Brain Data Withheld DataNNSE(Text) 2, 5 X x -NNSE(Brain) 2, 5.2.1, 5.3 x X -JNNSE(Brain+Text) 3, 5 X X -JNNSE(Brain+Text): Dropout task 5.2.2 X X subset of brain dataJNNSE(Brain+Text): Predict corpus 5.3 X X subset of text data250 500 10000.40.420.440.460.480.5Correlation of Semantic Question Distances to JNNSE(fMRI)Number of Latent DimensionsCorrelationJNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)Figure 1: Correlation of JNNSE(Brain+Text) andNNSE(Text) models with the distances in a se-mantic space constructed from behavioral data.Error bars indicate SEM.NNSE(Text) algorithm can be used as a VSM,which we use for the task of word prediction fromfMRI or MEG recordings.
A JNNSE(Brain+Text)created with a particular human subject?s data isnever used in the prediction framework with thatsame subject.
For example, if we use fMRI datafrom subject 1 to create a JNNSE(fMRI+Text), wewill test it with the remaining 8 fMRI subjects, butall 9 MEG subjects (fMRI and MEG subjects aredisjoint).Let us call the VSM learned withJNNSE(Brain+Text) or NNSE(Text) the se-mantic vectors.
We can train a weight matrix Wthat predicts the semantic vector a of a word fromthat word?s brain activation vector x: a = Wx.W can be learned with a variety of methods, wewill use L2regularized regression.
One can alsotrain regressors that predict the brain activationdata from the semantic vector: x = Wa, but wehave found this to give lower predictive accuracy.Note that we must re-train our weight matrix Wfor each subject (instead of re-using D(b)fromEquation 4) because testing always occurs on adifferent subject, and the brain activation data isnot inter-subject aligned.We train ` independent L2regularized regres-sors to predict the `-dimensional vectors a ={a1.
.
.
a`}.
The predictions are concatenatedto produce a predicted semantic vector: a?
={a?1, .
.
.
, a?`}.
We assess word prediction perfor-mance by testing if the model can differentiate be-tween two unseen words, a task named 2 vs. 2 pre-diction (Mitchell et al, 2008; Sudre et al, 2012).We choose the assignment of the two held out se-mantic vectors (a(1),a(2)) to predicted semanticvectors (a?
(1), a?
(2)) that minimizes the sum of thetwo normalized Euclidean distances.
2 vs. 2 ac-curacy is the percentage of tests where the correctassignment is chosen.The 60 nouns fall into 12 word categories.Words in the same word category (e.g.
screw-driver and hammer) are closer in semantic spacethan words in different word categories, whichmakes some 2 vs. 2 tests more difficult than oth-ers.
We choose 150 random pairs of words (witheach word represented equally) to estimate the dif-ficulty of a typical word pair, without having totest all(602)word pairs.
The same 150 randompairs are used for all subjects and all VSMs.
Ex-pected chance performance on the 2 vs. 2 test is50%.Results for testing on fMRI data in the2 vs. 2 framework appear in Figure 2.JNNSE(fMRI+Text) data performed on aver-age 6% better than the best NNSE(Text), andexceeding even the original SVD corpus represen-tations while maintaining interpretability.
Theseresults generalize across brain activity recordingtypes; JNNSE(MEG+Text) performs as well asJNNSE(fMRI+Text) when tested on fMRI data.The results are consistent when testing on MEGdata: JNNSE(MEG+Text) or JNNSE(fMRI+Text)outperforms NNSE(Text) (see Figure 3).494250 500 1000646668707274Number of Latent Dimensions2 vs. 2 Accuracy2 vs. 2 Acc.
for JNNSE and NNSE, tested on fMRI dataJNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)Figure 2: Average 2 vs. 2 accuracy forNNSE(Text) and JNNSE(Brain+Text), tested onfMRI data.
Models created with one subject?sfMRI data were not used to compute 2 vs. 2 ac-curacy for that same subject.250 500 1000666870727476788082Number of Latent Dimensions2 vs. 2 Accuracy2 vs. 2 Acc.
for JNNSE and NNSE, tested on MEG dataJNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)Figure 3: Average 2 vs. 2 accuracy forNNSE(Text) and JNNSE(Brain+Text), tested onMEG data.
Models created with one subject?sMEG data were not used to compute 2 vs. 2 ac-curacy for that same subject.NNSE(Text) performance decreases as thenumber of latent dimension increases.
This im-plies that without the regularizing effect of brainactivation data, the extra NNSE(Text) dimensionsare being used to overfit to the corpus data, orpossibly to fit semantic properties not detectablewith current brain imaging technologies.
How-ever, when brain activation data is included, in-creasing the number of latent dimensions strictlyincreases performance for JNNSE(fMRI+Text).JNNSE(MEG+Text) has peak performance with500 latent dimensions, with ?
1% decrease inperformance at 1000 latent dimensions.
In previ-ous work, the ability to decode words from brainactivation data was found to improve with addedlatent dimensions (Murphy et al, 2012a).
Ourresults may differ because our words are POStagged, and we included only nouns for the finalNNSE(Text) model.
We found that with the orig-inal ?
= 0.05 setting from Murphy et al (Mur-phy et al, 2012a) produced vectors that were toosparse; four of the 60 test words had all-zero vec-tors (JNNSE(Brain+Text) models did have any all-zero vectors).
To improve the NNSE(Text) vectorsfor a fair comparison, we reduced ?
= 0.025, un-der which NNSE(Text) did not produce any all-zero vectors for the 60 words.Our results show that brain activation data con-tributes additional information, which leads to anincrease in performance for the task of word pre-diction from brain activation data.
This suggeststhat corpus-only models may not capture all rel-evant semantic information.
This conflicts withprevious studies which found that semantic vec-tors culled from corpus statistics contain all of thesemantic information required to predict brain ac-tivation (Bullinaria and Levy, 2013).5.2.1 Prediction from a Brain-only ModelHow much predictive power does the corpus dataprovide to this word prediction task?
To testthis, we calculated the 2 vs. 2 accuracy for aNNSE(Brain) model trained on brain activationdata only.
We train NNSE(Brain) with one sub-ject?s data and use the resulting vectors to calculate2 vs. 2 accuracy for the remaining subjects.
Wehave brain data for only 60 words, so using ` ?
60latent dimensions leads to an under-constrainedsystem and a degenerate solution wherein only onelatent dimension is active for any word (and wherethe brain data can be perfectly reconstructed).
Thedegenerate solution makes it impossible to gen-eralize across words and leads to performance atchance levels.
An NNSE(MEG) trained on MEGdata gave maximum 2 vs. 2 accuracy of 67% when` = 20.
The reduced performance may be due tothe limited training data and the low SNR of thedata, but could also be attributed to the lack of cor-pus information, which provides another piece ofsemantic information.4955.2.2 Effect on Rows Without Brain DataIt is possible that some JNNSE(Brain+Text) di-mensions are being used exclusively to fit brainactivation data, and not the semantics representedin both brain and corpus data.
If a particulardimension j is solely used for brain data, thesparsity constraint will favor solutions that setsA(i,j)= 0 for i > w?
(no brain data constraint),and A(i,j)> 0 for some 0 ?
i ?
w?
(brain dataconstrained).
We found that there were no suchdimensions in the JNNSE(Brain+Text).
In fact forthe ` = 1000 JNNSE(Brain+Text), all latent di-mensions had greater than ?
25% non-zero en-tries, which implies that all dimensions are beingshared between the two data inputs (corpus andbrain activation), and are used to reconstruct both.To test that the brain activation data is truly in-fluencing rows of A not constrained by brain acti-vation data, we performed a dropout test.
We splitthe original 60 words into two 30 word groups (asevenly as possible across word categories).
Wetrained JNNSE(fMRI+Text) with 30 words, andtested word prediction with the remaining 8 sub-jects and the other 30 words.
Thus, the trainingand testing word sets are disjoint.
Because of thereduced size of the training data, we did see a dropin performance, but JNNSE(fMRI+Text) vectorsstill gave word prediction performance 7% higherthan NNSE(Text) vectors.
Full results appear inthe supplementary material.5.3 Predicting Corpus DataHere we ask: can an accurate latent representa-tion of a word be constructed using only brainactivation data?
This task simulates the scenariowhere there is no reliable corpus representation ofa word, but brain data is available.
This scenariomay occur for seldom-used words that fall belowthe thresholds used for the compilation of corpusstatistics.
It could also be useful for acronym to-kens (lol, omg) found in social media contextswhere the meaning of the token is actually a fullsentence.We trained a JNNSE(fMRI+Text) with braindata for all 60 words, but withhold the corpus datafor 30 of the 60 words (as evenly distributed aspossible amongst the 12 word categories).
Thebrain activation data for the 30 withheld wordswill allow us to create latent representations inA for withheld words.
Simultaneously, we willlearn a mapping from the latent representation tothe corpus data (D(c)).
This task cannot be per-Table 2: Mean rank accuracy over 30 wordsusing corpus representations predicted by aJNNSE(MEG+Text) model trained with somerows of the corpus data withheld.
Significanceis calculated using Fisher?s method to combine p-values for each of the subject-dependent models.Latent Dim size Rank Accuracy p-value250 65.30 < 10?19500 67.37 < 10?241000 63.47 < 10?15formed with a NNSE(Text) model because onecannot learn a latent representation of a word with-out data of some kind.
This further emphasizes theimpact of brain imaging data, which will allow usto generalize to previously unseen words in corpusspace.We use the latent representations in A for eachof the words without corpus data and the mappingto corpus space D(c)to predict the withheld cor-pus data in X .
We then rank the withheld rows ofX by their distance to the predicted row of X andcalculate the mean rank accuracy of the held outwords.
Results in Table 2 show that we can recre-ate the withheld corpus data using brain activationdata.
Peak mean rank accuracy (67.37) is attainedat ` = 500 latent dimensions.
This result showsthat neural semantic representations can create alatent representation that is faithful to unseen cor-pus statistics, providing further evidence that thetwo data sources share a strong common element.How much power is the remaining corpus datasupplying in scenarios where we withhold cor-pus data?
To answer this question, we trained anNNSE(Brain) model on 30 words of brain activa-tion, and then trained a regressor to predict cor-pus data from those latent brain-only representa-tions.
We use the trained regressor to predict thecorpus data for the remaining 30 words.
Peak per-formance is attained at ` = 10 latent dimensions,giving mean rank accuracy of 62.37, significantlyworse than the model that includes both corpusand brain activation data (67.37).5.4 Mapping Semantics onto the BrainBecause our method incorporates brain data intoan interpretable semantic model, we can directlymap semantic concepts onto the brain.
To dothis, we examined the mappings from the latentspace to the brain space via D(b).
We found thatthe most interpretable mappings come from mod-496!
"#$%&'()(a) D(b)matrix, subject P3, dimension with top words bath-room, balcony, kitchen.
MNI coordinates z=-12 (left) and z=-18(right).
Fusiform is associated with shelter words.!"#$%&'$()*+!
(&%&'$()*+(b) D(b)matrix; subject P1; dimension with top words ankle,elbow, knee.
MNI coordinates z=60 (left) and z=54 (right).
Pre-and post-central areas are activated for body part words.!
"#$%&'(#)*+"#,$%(c) D(b)matrix; subject P1; dimension with top scoring wordsbuffet, brunch, lunch.
MNI coordinates z=30 (left) and z=24(right).
Pars opercularis is believed to be part of the gustatorycortex, which responds to food related words.Figure 4: The mappings (D(b)) from latent se-mantic space (A) to brain space (Y ) for fMRI andwords from three semantic categories.
Shown arerepresentations of the fMRI slices such that theback of the head is at the top of the image, thefront of the head is at the bottom.els where the perceptual features had been scaleddown (divided by a constant factor), which en-courages more of the data to be explained bythe semantic features in A.
Figure 4 shows themappings (D(b)) for dimensions related to shel-ter, food and body parts.
The red areas alignwith areas of the brain previously known to beactivated by the corresponding concepts (Mitchellet al, 2008; Just et al, 2010).
Our modelhas learned these mappings in an unsupervisedsetting by relating semantic knowledge gleanedfrom word usage to patterns of activation in thebrain.
This illustrates how the interpretability ofJNNSE can allow one to explore semantics inthe human brain.
The mappings for one subjectare available for download (http://www.cs.cmu.edu/?afyshe/papers/acl2014/).6 Future Work and ConclusionWe are interested in pursuing many future projectsinspired by the success of this model.
We wouldlike to extend the JNNSE algorithm to incorporatedata from multiple subjects, multiple modalitiesand multiple experiments with non-overlappingwords.
Including behavioral data and image datais another possibility.We have explored a model of semantics that in-corporates text and brain activation data.
Thoughthe number of words for which we have brain acti-vation data is comparatively small, we have shownthat including even this small amount of data hasa positive impact on the learned latent representa-tions, including for words without brain data.
Wehave provided evidence that the latent representa-tions are closer to the neural representation of se-mantics, and possibly, closer to semantic groundtruth.
Our results reveal that there are aspects ofsemantics not currently represented in text-basedVSMs, indicating that there may be room for im-provement in either the data or algorithms used tocreate VSMs.
Our findings also indicate that usingthe brain as a semantic test can separate modelsthat capture this additional semantic informationfrom those that do not.
Thus, the brain is an im-portant source of both training and testing data.AcknowledgmentsThis work was supported in part by NIH un-der award 5R01HD075328-02, by DARPA underaward FA8750-13-2-0005, and by a fellowship toAlona Fyshe from the Multimodal Neuroimag-ing Training Program funded by NIH awardsT90DA022761 and R90DA023420.ReferencesAndrew J Anderson, Elia Bruni, Ulisse Bordignon,Massimo Poesio, and Marco Baroni.
2013.
Ofwords , eyes and brains : Correlating image-baseddistributional semantic models with neural represen-tations of concepts.
In Proceedings of the Confer-ence on Empirical Methods on Natural LanguageProcessing.David M Blei and Jon D. McAuliffe.
2007.
Supervisedtopic models.
In Advances in Neural InformationProcessing Systems, pages 1?22.497Elia Bruni, Giang Binh Tran, and Marco Baroni.
2011.Distributional semantics from text and images.
InProceedings of the EMNLP 2011 Geometrical Mod-els for Natural Language Semantics (GEMS).John A Bullinaria and Joseph P Levy.
2013.
Limitingfactors for mapping corpus-based semantic repre-sentations to brain activity.
PloS one, 8(3):e57191,January.Jamie Callan and Mark Hoy.
2009.
The ClueWeb09Dataset.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,Chong Wang, and David M Blei.
2009.
ReadingTea Leaves : How Humans Interpret Topic Models.In Advances in Neural Information Processing Sys-tems, pages 1?9.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational linguistics, 16(1):22?29.Bryan R Conroy, Benjamin D Singer, J Swaroop Gun-tupalli, Peter J Ramadge, and James V Haxby.
2013.Inter-subject alignment of human cortical anatomyusing functional connectivity.
NeuroImage, 81:400?11, November.Andrew D Engell, Scott Huettel, and Gregory Mc-Carthy.
2012.
The fMRI BOLD signal tracks elec-trophysiological spectral perturbations, not event-related potentials.
NeuroImage, 59(3):2600?6,February.Alona Fyshe, Partha Talukdar, Brian Murphy, and TomMitchell.
2013.
Documents and Dependencies : anExploration of Vector Space Models for SemanticComposition.
In Computational Natural LanguageLearning, Sofia, Bulgaria.Arthur M Glenberg and David a Robertson.
2000.Symbol Grounding and Meaning: A Compari-son of High-Dimensional and Embodied Theoriesof Meaning.
Journal of Memory and Language,43(3):379?401, October.Sunil Kumar Gupta, Dinh Phung, Brett Adams, andSvetha Venkatesh.
2013.
Regularized nonnegativeshared subspace learning.
Data Mining and Knowl-edge Discovery, 26(1):57?97.Emma L Hall, Si?an E Robson, Peter G Morris, andMatthew J Brookes.
2013.
The relationship be-tween MEG and fMRI.
NeuroImage, November.Harold Hotelling.
1936.
Relations between two sets ofvariates.
Biometrika, 28(3/4):321?377.Yangqing Jia and Trevor Darrell.
2010.
Factorized La-tent Spaces with Structured Sparsity.
In Advances inNeural Information Processing Systems, volume 23.Marcel Adam Just, Vladimir L Cherkassky, SandeshAryal, and Tom M Mitchell.
2010.
A neuroseman-tic theory of concrete noun representation based onthe underlying brain codes.
PloS one, 5(1):e8622,January.Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-dettini.
2008a.
Representational similarity analysis- connecting the branches of systems neuroscience.Frontiers in systems neuroscience, 2(November):4,January.Nikolaus Kriegeskorte, Marieke Mur, Douglas A Ruff,Roozbeh Kiani, Jerzy Bodurka, Hossein Esteky,Keiji Tanaka, and Peter A Bandettin.
2008b.
Match-ing Categorical Object Representations in InferiorTemporal Cortex of Man and Monkey.
Neuron,60(6):1126?1141.TK Landauer and ST Dumais.
1997.
A solution toPlato?s problem: The latent semantic analysis the-ory of acquisition, induction, and representation ofknowledge.
Psychological review, 1(2):211?240.Julien Mairal, Francis Bach, J Ponce, and GuillermoSapiro.
2010.
Online learning for matrix factor-ization and sparse coding.
The Journal of MachineLearning Research, 11:19?60.Ken McRae, George S Cree, Mark S Seidenberg, andChris McNorgan.
2005.
Semantic feature produc-tion norms for a large set of living and nonlivingthings.
Behavior research methods, 37(4):547?59,November.Tom M Mitchell, Svetlana V Shinkareva, Andrew Carl-son, Kai-Min Chang, Vicente L Malave, Robert AMason, and Marcel Adam Just.
2008.
Pre-dicting human brain activity associated with themeanings of nouns.
Science (New York, N.Y.),320(5880):1191?5, May.Brian Murphy, Partha Talukdar, and Tom Mitchell.2012a.
Learning Effective and Interpretable Se-mantic Models using Non-Negative Sparse Embed-ding.
In Proceedings of Conference on Computa-tional Linguistics (COLING).Brian Murphy, Partha Talukdar, and Tom Mitchell.2012b.
Selecting Corpus-Semantic Models for Neu-rolinguistic Decoding.
In First Joint Conferenceon Lexical and Computational Semantics (*SEM),pages 114?123, Montreal, Quebec, Canada.Andrew Y. Ng and Michael I. Jordan.
2002.
On dis-criminative vs. generative classifiers: A compari-son of logistic regression and naive bayes.
In Ad-vances in neural information processing systems,volume 14.Mark Palatucci, Geoffrey Hinton, Dean Pomerleau,and Tom M Mitchell.
2009.
Zero-Shot Learningwith Semantic Output Codes.
Advances in NeuralInformation Processing Systems, 22:1410?1418.Rajeev D S Raizada and Andrew C Connolly.
2012.What Makes Different People?s RepresentationsAlike : Neural Similarity Space Solves the Problemof Across-subject fMRI Decoding.
Journal of Cog-nitive Neuroscience, 24(4):868?877.498Indrayana Rustandi, Marcel Adam Just, and Tom MMitchell.
2009.
Integrating Multiple-StudyMultiple-Subject fMRI Datasets Using CanonicalCorrelation Analysis.
In MICCAI 2009 Workshop:Statistical modeling and detection issues in intra-and inter-subject functional MRI data analysis.Magnus Sahlgren.
2006.
The Word-Space Model Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words.
Doctorof philosophy, Stockholm University.Carina Silberer and Mirella Lapata.
2012.
Groundedmodels of semantic representation.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1423?1433.Carina Silberer, Vittorio Ferrari, and Mirella Lapata.2013.
Models of Semantic Representation with Vi-sual Attributes.
In Association for ComputationalLinguistics 2013, Sofia, Bulgaria.Gustavo Sudre, Dean Pomerleau, Mark Palatucci, LeilaWehbe, Alona Fyshe, Riitta Salmelin, and TomMitchell.
2012.
Tracking Neural Coding of Per-ceptual and Semantic Features of Concrete Nouns.NeuroImage, 62(1):463?451, May.Peter D Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning : Vector Space Models of Se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Martha White, Yaoliang Yu, Xinhua Zhang, and DaleSchuurmans.
2012.
Convex multi-view subspacelearning.
In Advances in Neural Information Pro-cessing Systems, pages 1?14.499
