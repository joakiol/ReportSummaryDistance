Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 80?83,Columbus, June 2008. c?2008 Association for Computational LinguisticsWhat Are Meeting Summaries?
An Analysis of Human ExtractiveSummaries in Meeting CorpusFei Liu, Yang LiuErik Jonsson School of Engineering and Computer ScienceThe University of Texas at DallasRichardson, TX, USA{feiliu,yangl}@hlt.utdallas.eduAbstractSignificant research efforts have been devoted tospeech summarization, including automatic ap-proaches and evaluation metrics.
However, a fun-damental problem about what summaries are for thespeech data and whether humans agree with eachother remains unclear.
This paper performs an anal-ysis of human annotated extractive summaries us-ing the ICSI meeting corpus with an aim to examinetheir consistency and the factors impacting humanagreement.
In addition to using Kappa statistics andROUGE scores, we also proposed a sentence dis-tance score and divergence distance as a quantitativemeasure.
This study is expected to help better definethe speech summarization problem.1 IntroductionWith the fast development of recording and storage tech-niques in recent years, speech summarization has re-ceived more attention.
A variety of approaches havebeen investigated for speech summarization, for exam-ple, maximum entropy, conditional random fields, latentsemantic analysis, support vector machines, maximummarginal relevance (Maskey and Hirschberg, 2003; Horiet al, 2003; Buist et al, 2005; Galley, 2006; Murray etal., 2005; Zhang et al, 2007; Xie and Liu, 2008).
Thesestudies used different domains, such as broadcast news,lectures, and meetings.
In these approaches, different in-formation sources have been examined from both text andspeech related features (e.g., prosody, speaker activity,turn-taking, discourse).How to evaluate speech summaries has also been stud-ied recently, but so far there is no consensus on eval-uation yet.
Often the goal in evaluation is to developan automatic metric to have a high correlation with hu-man evaluation scores.
Different methods have been usedin the above summarization research to compare systemgenerated summaries with human annotation, such as F-measure, ROUGE, Pyramid, sumACCY (Lin and Hovy,2003; Nenkova and Passonneau, 2004; Hori et al, 2003).Typically multiple reference human summaries are usedin evaluation in order to account for the inconsistencyamong human annotations.While there have been efforts on speech summariza-tion approaches and evaluation, some fundamental prob-lems are still unclear.
For example, what are speech sum-maries?
Do humans agree with each other on summaryextraction?
In this paper, we focus on the meeting do-main, one of the most challenging speech genre, to an-alyze human summary annotation.
Meetings often haveseveral participants.
Its speech is spontaneous, containsdisfluencies, and lacks structure.
These all post new chal-lenges to the consensus of human extracted summaries.Our goal in this study is to investigate the variation ofhuman extractive summaries, and help to better under-stand the gold standard reference summaries for meet-ing summarization.
This paper aims to answer two keyquestions: (1) How much variation is there in human ex-tractive meeting summaries?
(2) What are the factorsthat may impact interannotator agreement?
We use threedifferent metrics to evaluate the variation among humansummaries, including Kappa statistic, ROUGE score, anda new proposed divergence distance score to reflect thecoherence and quality of an annotation.2 Corpus DescriptionWe use the ICSI meeting corpus (Janin et al, 2003) whichcontains 75 naturally-occurred meetings, each about anhour long.
All of them have been transcribed and anno-tated with dialog acts (DA) (Shriberg et al, 2004), top-ics, and abstractive and extractive summaries in the AMIproject (Murray et al, 2005).We selected 27 meetings from this corpus.
Three anno-tators (undergraduate students) were recruited to extractsummary sentences on a topic basis using the topic seg-ments from the AMI annotation.
Each sentence corre-sponds to one DA annotated in the corpus.
The annota-tors were told to use their own judgment to pick summarysentences that are informative and can preserve discus-sion flow.
The recommended percentages for the selectedsummary sentences and words were set to 8.0% and16.0% respectively.
Human subjects were provided withboth the meeting audio files and an annotation Graphi-80cal User Interface, from which they can browse the man-ual transcripts and see the percentage of the currently se-lected summary sentences and words.We refer to the above 27 meetings Data set I in thispaper.
In addition, some of our studies are performedbased on the 6 meeting used in (Murray et al, 2005),for which we have human annotated summaries using 3different guidelines:?
Data set II: summary annotated on a topic basis.
This isa subset of the 27 annotated meetings above.?
Data set III: annotation is done for the entire meetingwithout topic segments.?
Data set IV: the extractive summaries are from the AMIannotation (Murray et al, 2005).3 Analysis Results3.1 Kappa StatisticKappa coefficient (Carletta, 1996) is commonly usedas a standard to reflect inter-annotator agreement.
Ta-ble 1 shows the average Kappa results, calculated foreach meeting using the data sets described in Section 2.Compared to Kappa score on text summarization, whichis reported to be 0.38 by (Mani et al, 2002) on a setof TREC documents, the inter-annotator agreement onmeeting corpus is lower.
This is likely due to the dif-ference between the meeting style and written text.Data Set I II III IVAvg-Kappa 0.261 0.245 0.335 0.290Table 1: Average Kappa scores on different data sets.There are several other observations from Table 1.First, comparing the results for Data Set (II) and (III),both containing six meetings, the agreement is higherfor Data Set (III).
Originally, we expected that by di-viding the transcript into several topics, human subjectscan focus better on each topic discussed during the meet-ing.
However, the result does not support this hypoth-esis.
Moreover, the Kappa result of Data Set (III) alsooutperforms that of Data Set (IV).
The latter data set isfrom the AMI annotation, where they utilized a differentannotation scheme: the annotators were asked to extractdialog acts that are highly relevant to the given abstrac-tive meeting summary.
Contrary to our expectation, theKappa score in this data set is still lower than that of DataSet (III), which used a direct sentence extraction schemeon the whole transcript.
This suggests that even usingthe abstracts as a guidance, people still have a high varia-tion in extracting summary sentences.
We also calculatedthe pairwise Kappa score between annotations in differ-ent data sets.
The inter-group Kappa score is much lowerthan those of the intragroup agreement, most likely dueto the different annotation specifications used in the twodifferent data sets.3.2 Impacting FactorsWe further analyze inter-annotator agreement with re-spect to two factors: topic length and meeting partic-ipants.
All of the following experiments are based onData Set (I) with 27 meetings.We computed Kappa statistic for each topic instead ofthe entire meeting.
The distribution of Kappa score withrespect to the topic length (measured using the number ofDAs) is shown in Figure 1.
When the topic length is lessthan 100, Kappa scores vary greatly, from -0.065 to 1.Among the entire range of different topic lengths, thereseems no obvious relationship between the Kappa scoreand the topic length (a regression from the data pointsdoes not suggest a fit with an interpretable trend).-0.200.20.40.60.811.20 200 400 600 800 1000 1200 1400Topic length-0.200.20.40.60.811.20 200 400 600 800 1000 1200 1400Topic lengthKappascoreFigure 1: Relationship between Kappa score and topic length.Using the same Kappa score for each topic, we also in-vestigated its relationship with the number of speakers inthat topic.
Here we focused on the topic segments longerthan a threshold (with more than 60 DAs) as there seemsto be a wide range of Kappa results when the topic isshort (in Figure 1).
Table 2 shows the average Kappascore for these long topics, using the number of speak-ers in the topic as the variable.
We notice that when thespeaker number varies from 4 to 7, kappa scores grad-ually decrease with the increasing of speaker numbers.This phenomenon is consistent with our intuition.
Gener-ally the more participants are involved in a conversation,the more discussions can take place.
Human annotatorsfeel more ambiguity in selecting summary sentences forthe discussion part.
The pattern does not hold for otherspeaker numbers, namely, 2, 3, and 8.
This might be dueto a lack of enough data points, and we will further ana-lyze this in the future research.# of speakers # of topics Avg Kappa score2 2 0.2043 6 0.1824 26 0.295 26 0.2496 33 0.2267 19 0.2218 7 0.3Table 2: Average Kappa score with respect to the number ofspeakers after removing short topics.3.3 ROUGE ScoreROUGE (Lin and Hovy, 2003) has been adopted asa standard evaluation metric in various summarizationtasks.
It is computed based on the n-gram overlap be-tween a summary and a set of reference summaries.Though the Kappa statistics can measure human agree-ment on sentence selection, it does not account for thefact that different annotators choose different sentences81that are similar in content.
ROUGE measures the wordmatch and thus can compensate this problem of Kappa.Table 3 shows the ROUGE-2 and ROUGE-SU4 F-measure results.
For each annotator, we computedROUGE scores using other annotators?
summaries as ref-erences.
For Data Set (I), we present results for each an-notator, since one of our goals is to evaluate the qual-ity of different annotator?s summary annotation.
The lowROUGE scores suggest the large variation among humanannotations.
We can see from the table that annotator1 has the lowest ROUGE score and thus lowest agree-ment with the other two annotators in Data Set (I).
TheROUGE score for Data Set (III) is higher than the others.This is consistent with the result using Kappa statistic:the more sentences two summaries have in common, themore overlapped n-grams they tend to share.ROUGE-2 ROUGE-SU4Annotator 1 0.407 0.457data (I) Annotator 2 0.421 0.471Annotator 3 0.433 0.483data (III) 2 annotators 0.532 0.564data (IV) 3 annotators 0.447 0.484Table 3: ROUGE F-measure scores for different data sets.3.4 Sentence Distance and Divergence ScoresFrom the annotation, we notice that the summary sen-tences are not uniformly distributed in the transcript, butrather with a clustering or coherence property.
However,neither Kappa coefficient nor ROUGE score can rep-resent such clustering tendency of meeting summaries.This paper attempts to develop an evaluation metric tomeasure this property among different human annotators.For a sentence i selected by one annotator, we define adistance score di to measure its minimal distance to sum-mary sentences selected by other annotators (distance be-tween two sentences is represented using the differenceof their sentence indexes).
di is 0 if more than one anno-tator have extracted the same sentence as summary sen-tence.
Using the annotated summaries for the 27 meet-ings in Data Set (I), we computed the sentence distancescores for each annotator.
Figure 2 shows the distributionof the distance score for the 3 annotators.
We can seethat the distance score distributions for the three annota-tors differ.
Intuitively, small distance scores mean bettercoherence and more consistency with other annotators?results.
We thus propose a mechanism to quantify eachannotator?s summary annotation by using a random vari-able (RV) to represent an annotator?s sentence distancescores.When all the annotators agree with each other, the RVd will take a value of 0 with probability 1.
In general,when the annotators select sentences close to each other,the RV d will have small values with high probabilities.Therefore we create a probability distribution Q for theideal situation where the annotators have high agreement,and use this to quantify the quality of each annotation.
Qis defined as:00.050.10.150.20.250.30.350.40.450.50 1 2 3 4 5 6 7 8 9 10 >10Distance ScorePercentageAnnotator 1 Annotator 2 Annotator 300.050.10.150.20.250.30.350.40.450.50 1 2 3 4 5 6 7 8 9 10 >10Distance ScorePercentageAnnotator 1 Annotator 2 Annotator 3Figure 2: Percentage distribution of the summary sentence dis-tance scores for the 3 annotators in Data Set (I).Q(i) =?????
(dmax ?
i + 1) ?
q i 6= 01 ?
?dmaxi=1 Q(i)= 1 ?
dmax?
(dmax+1)2 ?
q i = 0where dmax denotes the maximum distance score basedon the selected summary sentences from all the annota-tors.
We assign linearly decreasing probabilities Q(i) fordifferent distance values i (i > 0) in order to give morecredit to sentences with small distance scores.
The restof the probability mass is given to Q(0).
The parame-ter q is small, such that the probability distribution Q canapproximate the ideal situation.For each annotator, the probability distribution P is de-fined as:P (i) ={wi?fiPi wi?fii ?
Dp0 otherwisewhere Dp is the set of the possible distance values for thisannotator, fi is the frequency for a distance score i, andwi is the weight assigned to that distance (wi is i wheni 6= 0; w0 is p).
We use parameter p to vary the weightingscale for the distance scores in order to penalize more forthe large distance values.Using the distribution P for each annotator and theideal distribution Q, we compute their KL-divergence,called the Divergence Distance score (DD-score):DD =?iP (i) log P (i)Q(i)We expect that the smaller the score is, the better the sum-mary is.
In the extreme case, if an annotator?s DD-scoreis equal to 0, it means that all of this annotator?s extractedsentences are selected by other annotators.Figure 3 shows the DD-score for each annotator cal-culated using Data Set (I), with varying q parameters.Our experiments showed that the scale parameter p in theannotator?s probability distribution only affects the abso-lute value of the DD-score for the annotators, but doesnot change the ranking of each annotator.
Therefore wesimply set p = 10 when reporting DD-scores.
Figure 3shows that different weight scale q does not impact theranking of the annotators either.
We observe in Figure 3,annotator 1 has the highest DD score to the desirable dis-tribution.
We found this is consistent with the cumulativedistance score obtained from the distance score distribu-tion, where annotator 1 has the least cumulative frequen-cies for all the distance values greater than 0.
This is82also consistent with the ROUGE scores, where annotator1 has the lowest ROUGE score.
These suggest that theDD-score can be used to quantify the consistency of anannotator with others.05101520257 8 9 10 11 12 13 14 15 16 17 18 19 20 21-log(q)DivergenceDistanceScoreAnnotator 1Annotator 2Annotator 305101520257 8 9 10 11 12 13 14 15 16 17 18 19 20 21-log(q)DivergenceDistanceScoreAnnotator 1Annotator 2Annotator 3Figure 3: Divergence distance score when varying parameter qin the ideal distribution Q.We also investigated using the sentence distance scoresto improve the human annotation quality.
Our hypothe-sis is that those selected summary sentences with highdistance scores do not contain crucial information ofthe meeting content and thus can be removed from thereference summary.
To verify this, for each annota-tor, we removed the summary sentences with distancescores greater than some threshold, and then computedthe ROUGE score for the newly generated summary bycomparing to other two summary annotations that arekept unchanged.
The ROUGE-2 scores when varying thethreshold is shown in Figure 4.
No threshold in the X-axis means that no sentence is taken out from the humansummary.
We can see from the figure that the removalof sentences with high distance scores can result in evenbetter F-measure scores.
This suggests that we can deletethe incoherent human selected sentences while maintain-ing the content information in the summary.0.380.390.40.410.420.430.440.450.463 4 5 6 7 8 no thresholdThreshold of Distance ScoreF-scoreAnnotator 1 Annotator 2 Annotator 30.380.390.40.410.420.430.440.450.463 4 5 6 7 8 no thresholdThreshold of Distance ScoreF-scoreAnnotator 1 Annotator 2 Annotator 3Figure 4: ROUGE-2 score after removing summary sentenceswith a distance score greater than a threshold.4 ConclusionIn this paper we conducted an analysis about human an-notated extractive summaries using a subset of the ICSImeeting corpus.
Different measurements have been usedto examine interannotator agreement, including Kappacoefficient, which requires exact same sentence selection;ROUGE, which measures the content similarity using n-gram match; and our proposed sentence distance scoresand divergence, which evaluate the annotation consis-tency based on the sentence position.
We find that thetopic length does not have an impact on the human agree-ment using Kappa, but the number of speakers seems tobe correlated with the agreement.
The ROUGE score andthe divergence distance scores show some consistencyin terms of evaluating human annotation agreement.
Inaddition, using the sentence distance score, we demon-strated that we can remove some poorly chosen sentencesfrom the summary to improve human annotation agree-ment and preserve the information in the summary.
Inour future work, we will explore other factors, such assummary length, and the speaker information for the se-lect summaries.
We will also use a bigger data set for amore reliable conclusion.AcknowledgmentsThe authors thank University of Edinburgh for sharing the an-notation on the ICSI meeting corpus.
This research is supportedby NSF award IIS-0714132.
The views in this paper are thoseof the authors and do not represent the funding agencies.ReferencesA.
H. Buist, W. Kraaij, and S. Raaijmakers.
2005.
Automaticsummarization of meeting data: A feasibility study.
In Proc.of the 15th CLIN conference.J.
Carletta.
1996.
Assessing agreement on classification tasks:the kappa statistic.
Computational Linguistics, 22(2):249?254.M.
Galley.
2006.
A skip-chain conditional random fieldfor ranking meeting utterances by importance.
In Proc.
ofEMNLP, pages 364?372.C.
Hori, T. Hori, and S. Furui.
2003.
Evaluation methods forautomatic speech summarization.
In Proc.
of Eurospeech,pages 2825?2828.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan,B.
Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.2003.
The ICSI meeting corpus.
In Proc.
of ICASSP.C.
Y. Lin and E. Hovy.
2003.
Automatic evaluation of sum-maries using n-gram co-occurrence statistics.
In Proc.
ofHLT?NAACL.I.
Mani, G. Klein, D. House, L. Hirschman, T. Firmin, andB.
Sundheim.
2002.
Summac: a text summarization eval-uation.
Natural Language Engineering, 8:43?68.S.
Maskey and J. Hirschberg.
2003.
Automatic summariza-tion of broadcast news using structural features.
In Proc.
ofEUROSPEECH, pages 1173?1176.G.
Murray, S. Renals, J. Carletta, and J. Moore.
2005.
Evalu-ating automatic summaries of meeting recordings.
In Proc.of the ACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation.A.
Nenkova and R. Passonneau.
2004.
Evaluating content se-lection in summarization: The pyramid method.
In Proc.
ofHLT-NAACL.E.
Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004.The ICSI meeting recorder dialog act (MRDA) corpus.
InProc.
of 5th SIGDial Workshop, pages 97?100.S.
Xie and Y. Liu.
2008.
Using corpus and knowledge-basedsimilarity measure in maximum marginal relevance for meet-ing summarization.
In Proc.
of ICASSP.J.
Zhang, H. Chan, P. Fung, and L. Cao.
2007.
A compara-tive study on speech summarization of broadcast news andlecture speech.
In Proc.
of Interspeech.83
