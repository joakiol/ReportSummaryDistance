Proceedings of the Second Workshop on Statistical Machine Translation, pages 181?184,Prague, June 2007. c?2007 Association for Computational LinguisticsGetting to know Moses:Initial experiments on German?English factored translationMaria Holmqvist, Sara Stymne, and Lars AhrenbergDepartment of Computer and Information ScienceLink?pings universitet, Sweden{marho,sarst,lah}@ida.liu.seAbstractWe present results and experiences fromour experiments with phrase-based statisti-cal machine translation using Moses.
Thepaper is based on the idea of using an off-the-shelf parser to supply linguistic infor-mation to a factored translation model andcompare the results of German?Englishtranslation to the shared task baseline sys-tem based on word form.
We report partialresults for this model and results for twosimplified setups.
Our best setup takes ad-vantage of the parser?s lemmatization anddecompounding.
A qualitative analysis ofcompound translation shows that decom-pounding improves translation quality.1 IntroductionOne of the stated goals for the shared task of thisworkshop is ?to offer newcomers a smooth startwith hands-on experience in state-of-the-art statis-tical machine translation methods?.
As our previ-ous research in machine translation has beenmainly concerned with rule-based methods, wejumped at this offer.We chose to work on German-to-English trans-lation for two reasons.
Our primary practical inter-est lies with translation between Swedish and Eng-lish, and of the languages offered for the sharedtask, German is the one closest in structure toSwedish.
While there are differences in word orderand morphology between Swedish and German,there are also similarities, e.g., that both languagesrepresent nominal compounds as single ortho-graphic words.
We chose the direction from Ger-man to English because our knowledge of Englishis better than our knowledge of German, making iteasier to judge the quality of translation output.Experiments were performed on the Europarl data.With factored statistical machine translation,different levels of linguistic information can betaken into account during training of a statisticaltranslation system and decoding.
In our experi-ments we combined syntactic and morphologicalfactors from an off-the-shelf parser with the fac-tored translation framework in Moses (Moses,2007).
We wanted to test the following hypotheses:?
Translation models based on lemmas will im-prove translation quality (Popovi?
and Ney,2004)?
Decompounding German nominal compoundswill improve translation quality (Koehn andKnight, 2003)?
Re-ordering models based on word forms andparts-of-speech will improve translation qual-ity (Zens and Ney, 2006).2 The parserThe parser, Machinese Syntax, is a commerciallyavailable dependency parser from Connexor Oy 1.It provides each word with lemma, part-of-speech,morphological features and dependency relations(see Figure 1).
In addition, the lemmas of com-pounds are marked by a ?#?
separating the twoparts of the compound.
For the shared task we onlyused shallow linguistic information: lemma, part-of-speech and morphology.
The compound bound-ary identification was used to split noun com-1 Connexor Oy, http://www.connexor.com.181pounds to make the German input more similar toEnglish text.1 Mit   mit   pm>2    @PREMARK PREP2 Blick blick advl>10 @NH N MSC SG DAT3 auf   auf   pm>5    @PREMARK PREPFigure 1.
Example of parser outputWe used the parser?s tokenization as given.
Somecommon multiword units, such as ?at all?
and ?vonheute?, are treated as single words by the parser(cf.
Niessen and Ney, 2004).
The German parseralso splits contracted prepositions and determinerslike ?zum?
?
?zu dem?
(?to the?
).3 System descriptionFor our experiments with Moses we basically fol-lowed the shared task baseline system setup totrain our factored translation models.
After traininga statistical model, minimum error-rate tuning wasperformed to tune the model parameters.
All ex-periments were performed on an AMD 64 Athlon4000+ processor with 4 Gb of RAM and 32 bitLinux (Ubuntu).Since time as well as computer resources werelimited we designed a model that we hoped wouldmake the best use of all available factors.
Thismodel turned out to be too complex for our ma-chine and in later experiments we abandoned it fora simpler model.3.1 Pre-processingIn the pre-processing step we used the standardpre-processing of the shared task baseline system,parsed the German and English texts and processedthe output to obtain four factors: word form,lemma, part-of-speech and morphology.
Missingvalues for lemma, part-of-speech and morphologywere replaced with default values.Noun compounds are very frequent in German,2.9% of all tokens in the tuning corpus were identi-fied by the parser as noun compounds.
Compoundstend to lead to sparse data problems and splittingthem has been shown to improve German-Englishtranslation (Koehn and Knight, 2003).
Thus wedecided to decompund German noun compoundsidentified as such by our parser.We used a simple strategy to remove fillers andto correct some obvious mistakes.
We removed thefiller ?-s?
that appear before a marked split unless itwas one of ?-ss?, ?-urs?, ?-eis?
or ?-us?.
This appliedto 35% of the noun compounds in the tuning cor-pus.
The fillers were removed both in the wordform and the lemma (see Figure 2).There were some mistakes made by the parser,for instance on compounds containing the word?nahmen?
which was incorrectly split as ?stel-lungn#ahmen?
instead of ?stellung#nahmen?(?statement?).
These splits were corrected by mov-ing the ?n?
to the right side of the split.We then split noun-lemmas on hyphens unlessthere were numbers on either side of it and on theplaces marked by ?#?.
Word forms were split in thecorresponding places as the lemmas.The part-of-speech and morphology of the lastword in the compound is the same as for the wholecompound.
For the other parts we hypothesizedthat part-of-speech is Noun and the morphology isunknown, marked by the tag UNK.Parser output:unionsl?nder unions#land N NEU PL ACCFactored output:union|union|N|UNKl?nder|land|N|NEU_PL_ACCFigure 2.
Compound splitting for ?unionsl?nder?
(?countries in the union?
)These strategies are quite crude and could be fur-ther refined by studying the parser output thor-oughly to pinpoint more problems.3.2 Training translation models with linguis-tic factorsAfter pre-processing, the German?English Eu-roparl training data contains four factors: 0: wordform, 1: lemma, 2: part-of-speech, 3: morphology.As a first step in training our translation models weperformed word alignment on lemmas as this couldpotentially improve word alignment.3.2.1 First setupFactored translation requires a number of decodingsteps, which are either mapping steps mapping asource factor to a target factor or generation stepsgenerating a target factor from other target factors.Our first setup contained three mapping steps, T0?T2, and one generation step, G0.182T0: 0-0 (word ?
word)T1: 1-1 (lemma ?
lemma)T2: 2,3-2,3  (pos+morph ?
pos+morph)G0:  1,2,3-0  (lemma+pos+morph ?
word)With the generation step, word forms that did notappear in the training data may still get translatedif the lemma, part-of-speech and morphology canbe translated separately and the target word formcan be generated from these factors.Word order varies a great deal between Germanand English.
This is especially true for the place-ment of verbs.
To model word order changes weincluded part-of-speech information and createdtwo reordering models, one based on word form(0), the other on part-of-speech (2):0-0.msd-bidirectional-fe2-2.msd-bidirectional-feThe decoding times for this setup turned out to beunmanageable.
In the first iteration of parametertuning, decoding times were approx.
6min/sentence.
In the second iteration decodingtime increased to approx.
30 min/sentence.
Re-moving one of the reordering models did not resultin a significant change in decoding time.
Just trans-lating the 2000 sentences of test data with untunedparameters would take several days.
We inter-rupted the tuning and abandoned this setup.3.2.2 Second setupBecause of the excessive decoding times of thefirst factored setup we resorted to a simpler systemthat only used the word form factor for the transla-tion and reordering models.
This setup differs fromthe shared task baseline in the following ways:First, it uses the tokenization provided by theparser.
Second, alignment was performed on thelemma factor.
Third, German compounds weresplit using the method described above.
To speedup tuning and decoding, we only used the first 200sentences of development data (dev2006) for tun-ing and reduced stack size to 50.T0: 0-0 (word ?
word)R:  0-0.msd-bidirectional-fe3.2.3 Third setupTo test our hypothesis that word reordering wouldbenefit from part-of-speech information we createdanother simpler model.
This setup has two map-ping steps, T0 and T1, and a reordering modelbased on part-of-speech.T0: 0-0 (word ?
word)T1: 2,3-2,3 (pos+morph ?
pos+morph)R: 2-2.msd-bidirectional-fe4 ResultsWe compared our systems to a baseline systemwith the same setup as the WMT2007 shared taskbaseline system but tuned with our system?s sim-plified tuning settings (200 instead of 2000 tuningsentences, stack size 50).
Table 1 shows the Bleuimprovement on the 200 sentences developmentdata from the first and last iteration of tuning.Dev2006 (200) System1st iteration Last iterationBaseline 19.56 27.07First 21.68 -Second 20.43 27.16Third 20.72 24.72Table 1.
Bleu scores on 200 sentences of tuningdata before and after tuningThe final test of our systems was performed on thedevelopment test corpus (devtest2006) using stacksize 50.
The results are shown in Table 2.
The lowBleu score for the third setup implies that reorder-ing on part-of-speech is not enough on its own.The second setup performed best with a slightlyhigher Bleu score than the baseline.
We used thesecond setup to translate test data for our submis-sion to the shared task.System Devtest2006 (NIST/Bleu)Baseline 6.7415 / 25.94First -Second  6.8036 / 26.04Third 6.5504 / 24.57Table 2.
NIST and Bleu scores on developmenttest data4.1 DecompoundingWe have evaluated the decompounding strategy byanalyzing how the first 75 identified noun com-pounds of the devtest corpus were translated by oursecond setup compared to the baseline.
The sample183excluded doubles and compounds that had no cleartranslation in the reference corpus.Out of these 75 compounds 74 were nouns thatwere correctly split and 1 was an adjective that wassplit incorrectly: ?allumfass#ende?.
Despite that itwas incorrectly identified and split it was trans-lated satisfyingly to ?comprehensive?.The translations were grouped into the catego-ries shown in Table 3.
The 75 compounds wereclassified into these categories for our second sys-tem and the baseline system, as shown in Table 4.As can be seen the compounds were handled betterby our system, which had 62 acceptable transla-tions (C or V) compared to 48 for the baseline anddid not leave any noun compounds untranslated.Table 3.
Classification scheme with examples forcompound translationsTable 4.
Classification of 75 compounds from oursecond system and the baseline systemDecompounding of nouns reduced the numberof untranslated words, but there were still someleft.
Among these were cases that can be handledsuch as separable prefix verbs like ?aufzeigten?
(?pointed out?)
(Niessen and Ney, 2000) or adjec-tive compounds such as ?multidimensionale?
(?multi dimensional?).
There were also some nouncompounds left which indicates that we might needa better decompounding strategy than the one usedby the parser (see e.g.
Koehn and Knight, 2003).4.2 Experiences and future plansWith the computer equipment at our disposal,training of the models and tuning of the parametersturned out to be a very time-consuming task.
Forthis reason, the number of system setups we couldtest was small, and much fewer than we had hopedfor.
Thus it is too early to draw any conclusions asregards our hypotheses, but we plan to performmore tests in the future, also on Swedish?Englishdata.
The parser's ability to identify compoundsthat can be split before training seems to give adefinite improvement, however, and is a featurethat can likely be exploited also for Swedish-to-English translation with Moses.ReferencesKoehn, Philipp and Kevin Knight, 2003.
Empiricalmethods for compound splitting.
In Proceedings ofEACL 2003, 187-194.
Budapest, Hungary.Moses ?
a factored phrase-based beam-search decoderfor machine translation.
13 April 2007,  URL:http://www.statmt.org/moses/ .Niessen, Sonja and Hermann Ney, 2004.
Statistical ma-chine translation with scarce resources using mor-pho-syntactic information.
Computational Linguis-tics, 181-204 .Niessen, Sonja and Hermann Ney, 2000.
ImprovingSMT Quality with Morpho-syntactic Analysis.
InProceedings of Coling 2000.
1081-1085.
Saar-br?cken, Germany.Popovi?, Maja and Hermann Ney, 2004.
ImprovingWord Alignment Quality using Morpho-Syntactic In-formation.
In Proceedings of Coling 2004, 310-314,Geneva, Switzerland.Zens, Richard and Hermann Ney, 2006.
DiscriminativeReordering Models for Statistical Machine Transla-tion.
In HLT-NAACL: Proceedings of the Workshopon Statistical Machine Translation, 55-63, New YorkCity, NY.Category ExampleC-correct RegelungsentwurfDraft regulationRef: Draft regulationV-variant Schlachth?fenAbattoirsRef: Slaughter housesP-partly correct AnpassungsdruckPressureRef: Pressure for adaptionF-wrong form L?nderberichteCountry reportRef: Country reportsW-wrong ErbonkelUncle dnaRef: Sugar daddyU-untranslated SchlussentwurfSchlussentwurfRef: Final draftBaseline systemC V P W U F TotC 36 1 3  3 1 44V 1 9 2 1 5  18P   3  2  5W    1 2  3U       0F 1     4 5 SecondsystemTot 38 10 8 2 12 5 75184
