Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 87?95,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsMorpho Challenge competition 2005-2010: Evaluations and resultsMikko Kurimo, Sami Virpioja, Ville Turunen, Krista LagusAdaptive Informatics Research CentreAalto University, Espoo, FinlandFirstname.Lastname@tkk.fiAbstractMorpho Challenge is an annual evalu-ation campaign for unsupervised mor-pheme analysis.
In morpheme analysis,words are segmented into smaller mean-ingful units.
This is an essential part inprocessing complex word forms in manylarge-scale natural language processingapplications, such as speech recognition,information retrieval, and machine trans-lation.
The discovery of morphemes isparticularly important for morphologicallyrich languages where inflection, deriva-tion and composition can produce a hugeamount of different word forms.
MorphoChallenge aims at language-independentunsupervised learning algorithms that candiscover useful morpheme-like units fromraw text material.
In this paper we de-fine the challenge, review proposed algo-rithms, evaluations and results so far, andpoint out the questions that are still open.1 IntroductionMany large-scale natural language processing(NLP) applications, such as speech recognition,information retrieval and machine translation, re-quire that complex word forms are analyzed intosmaller, meaningful units.
The discovery of theseunits called morphemes is particularly importantfor morphologically rich languages where the in-flection, derivation and composition makes it im-possible to even list all the word forms that areused.
Various tools have been developed for mor-pheme analysis of word forms, but they are mostlybased on language-specific rules that are not eas-ily ported to other languages.
Recently, the per-formance of tools based on language-independentunsupervised learning from raw text material hasimproved significantly and rivaled the language-specific tools in many applications.The unsupervised algorithms proposed so far inMorpho Challenge typically first generate variousalternative morphemes for each word and then se-lect the best ones based on relevant criteria.
Thestatistical letter successor variation (LSV) analy-sis (Harris, 1955) and its variations are quite com-monly used as generation methods.
LSV is basedon the observation that the segment borders be-tween the sub-word units often co-occur with thepeaks of variation for the next letter.
One popu-lar selection approach is to minimize a cost func-tion that balances between the size of the corpuswhen coded by the morphemes and the size ofthe morpheme codebook needed.
Selection cri-teria that produce results resembling the linguis-tic morpheme segmentation include, for example,the Minimum Description Length (MDL) princi-ple and maximum a posteriori (MAP) probabilityoptimization (de Marcken, 1996; Creutz and La-gus, 2005).The Morpho Challenge competition waslaunched in 2005 to encourage the machinelearning people, linguists and specialists in NLPapplications to study this field and come togetherto compare their best algorithms against eachother.
The organizers selected evaluation tasks,data and metric and performed all the evaluations.Thus, participation was made easy for peoplewho were not specialists in the chosen NLPapplications.
Participation was open to everybodywith no charge.
The competition became popularright from the beginning and has gained newparticipants every year.Although not all the authors of relevant mor-pheme analysis algorithms have yet submittedtheir algorithms for this evaluation campaign,more than 50 algorithms have already been eval-uated.
After the first five years of Morpho Chal-lenge, a lot has been learned on the various pos-sible ways to solve the problem and how the dif-ferent methods work in various NLP tasks.
How-87ever, there are still open questions such as: how tofind meaning for the obtained unsupervised mor-phemes, how to disambiguate among the alterna-tive analyses of one word, and how to use contextin the analysis.
Another recently emerged ques-tion that is the special topic in 2010 competitionis how to utilize small amounts of labeled dataand semi-supervised learning to further improvethe analysis.2 Definition of the challenge2.1 Morphemes and their evaluationGenerally, the morphemes are defined as thesmallest meaningful units of language.
Ratherthan trying to directly specify which units aremeaningful, the Morpho Challenge aims at find-ing units that would be useful for various practicalNLP applications.
The goal is to find automaticmethods that can discover suitable units using un-supervised learning directly on raw text data.
Themethods should also not be restricted to certainlanguages or include many language and applica-tion dependent parameters that needed to be handtuned for each task separately.
The following threegoals have been defined as the main scientific ob-jectives for the challenge: (1) To learn of the phe-nomena underlying word construction in naturallanguages.
(2) To discover approaches suitable fora wide range of languages.
(3) To advance ma-chine learning methodology.The evaluation tasks, metrics and languageshave been designed based on the scientific objec-tives of the challenge.
It can not be directly ver-ified how well an obtained analysis reflects theword construction in natural languages, but intu-itively, the methods that split everything into let-ters or pre-specified letter n-grams, or leave theword forms unanalyzed, would not be very in-teresting solutions.
An interesting thing that canbe evaluated, however, is how close the obtainedanalysis is to the linguistic gold standard mor-phemes that can be obtained from CELEX orvarious language-dependent rule-based analyzers.The exact definition of the morphemes, tags, orfeatures available in the gold standard to be uti-lized in the comparison should be decided andfixed for each language separately.To verify that a proposed algorithm works invarious languages would, ideally, require runningthe evaluations on a large number of languagesthat would be somehow representative of variousimportant language families.
However, the re-sources available for both computing and evalu-ating the analysis in various applications and lan-guages are limited.
The suggested and applicablecompromise is to select morphologically rich lan-guages where the morpheme analysis is most use-ful and those languages where interesting state-of-the-art evaluation tasks are available.
By includingGerman, Turkish, Finnish and Arabic, many inter-esting aspects of concatenative morphology havealready been covered.While the comparison against the linguisticgold standard morphemes is an interesting sub-goal, the main interest in running the MorphoChallenge is to find out how useful the proposedmorpheme analyses are for various practical NLPapplications.
Naturally, this is best evaluatedby performing evaluations in several state-of-the-art application tasks.
Due to the limitations ofthe resources, the applications have been selectedbased on the importance of the morpheme analy-sis for the application, on the availability of openstate-of-the-art evaluation tasks, and on the effortneeded to run the actual evaluations.2.2 Unsupervised and semi-supervisedlearningUnsupervised learning is the task of learning with-out labeled data.
In the context of morphology dis-covery, it means learning without knowing wheremorpheme borders are, or which morphemes existin which words.
Unsupervised learning methodshave many attractive features for morphologicalmodeling, such as language-independence, inde-pendence of any particular linguistic theory, andeasy portability to a new language.Semi-supervised learning can be approachedfrom two research directions, namely unsuper-vised and supervised learning.
In an essentiallyunsupervised learning task there may exist somelabeled (classified) data, or some known links be-tween data items, which might be utilized by the(typically generative) learning algorithms.
Turnedaround, an essentially supervised learning task,such as classification or prediction, may benefitalso from unlabeled data which is typically moreabundantly available.In morphology modeling one might considerthe former setup to be the case: the learning taskis essentially that of unsupervised modeling, andmorpheme labels can be thought of as known links88between various inflected word forms.Until 2010 the Morpho Challenge has been de-fined only as an unsupervised learning task.
How-ever, since small samples of morphologically la-beled data can be provided already for quite manylanguages, also the semi-supervised learning taskhas become of interest.Moreover, while there exists a fair amount ofresearch and now even books on semi-supervisedlearning (Zhu, 2005; Abney, 2007; Zhu, 2010),it has not been as widely studied for structuredclassification problems like sequence segmenta-tion and labeling (cf.
e.g.
(Jiao et al, 2006)).
Thesemi-supervised learning challenge introduced forMorpho Challenge 2010 can thus be viewed as anopportunity to strengthen research in both mor-phology modeling as well as in semi-supervisedlearning for sequence segmentation and labelingin general.3 Review of Morpho Challengecompetitions so far3.1 Evaluation tasks, metrics, and languagesThe evaluation tasks and languages selected forMorpho Challenge evaluations are shown in Fig-ure 1.
The languages where evaluations have beenprepared are Finnish (FIN), Turkish (TUR), En-glish (ENG), German (GER), and Arabic (ARA).First the morphemes are compared to linguis-tic gold standards in direct morpheme segmen-tation (2005) and full morpheme analysis (since2007).
The practical NLP application based eval-uations are automatic speech recognition (ASR),information retrieval (IR) and statistical machinetranslation (SMT).
Morphemes obtained by semi-supervised learning can be evaluated in parallelwith the unsupervised morphemes.
For IR, eval-uation has also been extended for full sentences,where the morpheme analysis can based on con-text.
The various suggested and tested evaluationsare defined in this section.year new languages new tasks2005 FIN, TUR, ENG segmentation, ASR2007 GER full analysis, IR2008 ARA context IR2009 - SMT2010 - semi-supervisedTable 1: The evolution of the evaluations.
Theacronyms are explained in section 3.1.3.1.1 Comparisons to linguistic gold standardThe first Morpho Challenge in 2005 (Kurimo etal., 2006) considered unsupervised segmentationof words into morphemes.
The evaluation wasbased on comparing the segmentation boundariesgiven by the competitor?s algorithm to the bound-aries obtained from a gold standard analysis.From 2007 onwards, the task was changed tofull morpheme analysis, that is, the algorithmshould not only locate the surface forms (i.e., wordsegments) of the morphemes, but find also whichsurface forms are realizations (allomorphs) of thesame underlying morpheme.
This generalizes thetask for finding more meaningful units than justthe realizations of morphemes that may be just in-dividual letters or even empty strings.
In applica-tions this is useful when it is important to identifywhich units carry the same meaning even if theyhave different realizations in different words.As an unsupervised algorithm cannot find themorpheme labels that would equal to the labels inthe gold standard, the evaluation has to be basedon what word forms share the same morphemes.The evaluation procedure samples a large num-ber of word pairs, such that both words in thepair have at least one morpheme in common, fromboth the proposed analysis and the gold standard.The first version of the method was applied in2007 (Kurimo et al, 2008) and 2008 (Kurimo etal., 2009a), and minor modifications were done in2009 (Kurimo et al, 2009b).
However, the orga-nizers have reported the evaluation results of the2007 and 2008 submissions also with the new ver-sion, thus allowing a direct comparison betweenthem.
A summary of these results for English,Finnish, German and Turkish for the best algo-rithms is presented in Table 2.
The evaluationsin 2008 and 2009 were also performed on Arabic,but these results and not comparable, because thedatabase and the gold standard was changed be-tween the years.
The exact annual results for allparticipants as well as the details of the evaluationin each year can be reviewed in the annual evalu-ation reports (Kurimo et al, 2006; Kurimo et al,2008; Kurimo et al, 2009a; Kurimo et al, 2009b).Already the linguistic evaluation of MorphoChallenge 2005 applied some principles that havebeen used thereafter: (1) The evaluation is basedon a subset of the word forms given as trainingdata.
This not only makes the evaluation proce-dure lighter, but also allows changing the set when89English FinnishMethod P R F Method P R F2009 2009Allomorfessor 68.98 56.82 62.31 Monson PMU 47.89 50.98 49.39Monson PMU 55.68 62.33 58.82 Monson PMM 51.75 45.42 48.38Lignos 83.49 45.00 58.48 Spiegler PROMODES C 41.20 48.22 44.442008 2008Monson P+M 69.59 65.57 67.52 Monson P+M 65.21 50.43 56.87Monson ParaMor 63.32 51.96 57.08 Monson ParaMor 49.97 37.64 42.93Zeman 1 67.13 46.67 55.06 Monson Morfessor 79.76 24.95 38.022007 2007Monson P+M 70.09 67.38 68.71 Bernhard 2 63.92 44.48 52.45Bernhard 2 67.42 65.11 66.24 Bernhard 1 78.11 29.39 42.71Bernhard 1 75.61 57.87 65.56 Bordag 5a 72.45 27.21 39.56German TurkishMethod P R F Method P R F2009 2009Monson PMU 52.53 60.27 56.14 Monson PMM 48.07 60.39 53.53Monson PMM 51.07 57.79 54.22 Monson PMU 47.25 60.01 52.88Monson PM 50.81 47.68 49.20 Monson PM 49.54 54.77 52.022008 2008Monson P+M 64.06 61.52 62.76 Monson P+M 66.78 57.97 62.07Monson Morfessor 70.73 38.82 50.13 Monson ParaMor 57.35 45.75 50.90Monson ParaMor 56.98 42.10 48.42 Monson Morfessor 77.36 33.47 46.732007 2007Monson P+M 69.96 55.42 61.85 Bordag 5a 81.06 23.51 36.45Bernhard 2 54.02 60.77 57.20 Bordag 5 81.19 23.44 36.38Bernhard 1 66.82 42.48 51.94 Zeman 77.48 22.71 35.13Table 2: The summary of the best three submitted methods for years 2009, 2008 and 2007 using thelinguistic evaluation of Morpho Challenge 2009.
The complete results tables by the organizers are avail-able from http://www.cis.hut.fi/morphochallenge2009/.
The three columns numbersare precision (P), recall (R), and F-measure (F).
The best F-measure for each language is in boldface,and the best result that is not based on a direct combination of two other methods is underlined.the old one is considered to be ?overlearned?.
(2)The frequency of the word form plays no role inevaluation; rare and common forms are equallylikely to be selected, and have equal weight tothe score.
(3) The evaluation score is balanced F-measure, the harmonic mean of precision and re-call.
Precision measures how many of the choicesmade by the algorithm are matched in gold stan-dard; recall measures how many of the choicesin the gold standard are matched in the proposedanalysis.
(4) If the linguistic gold standard hasseveral alternative analysis for one word, for fullprecision, it is enough that one of the alternativesis equivalent to the proposed analysis.
The sameholds the other way around for recall.All of the principles can be also criticized.
Forexample, evaluation based on the full set wouldprovide more trustworthy estimates, and commonword forms are more significant in any practicalapplication.
However, the third and the fourthprinciple have problems that can be considered tobe more serious.Balanced F-measure favors methods that areable to get near-to-equal precision and recall.
Asmany algorithms can be tuned to give either moreor less morphemes per word than in the defaultcase, this encourages using developments sets tooptimize the respective parameters.
The winningmethods in Challenge 2009?Monson?s ParaMor-Morfessor Union (PMU) and ParaMor-Morfessor90Mimic (PMM) (Monson et al, 2009), and Al-lomorfessor (Virpioja and Kohonen, 2009)?didthis, more or less explicitly.1 Moreover, it canbe argued that the precision would be more im-portant than recall in many applications, or, moregenerally, that the optimal balance between preci-sion and recall is application dependent.
We seetwo solutions for this: Either the optimization forF-measure should be allowed with a public devel-opment set, which means moving towards semi-supervised direction, or precision-recall curvesshould be compared, which means more complexevaluations.The fourth principle causes problems, if theevaluated algorithms are allowed to have alterna-tive analyses for each word.
If several alternativeanalyses are provided, the obtained precision isabout the average over the individual analyses, butthe recall is based on the best of the alternatives.This property have been exploited in Challenges2007 and 2008 by combining the results of twoalgorithms as alternative analyses.
The method,Monson?s ParaMor+Morfessor (P+M) holds stillthe best position measured in F-measures in alllanguages.
Combining even better-performingmethods in a similar manner would increase thescores further.
To fix this problem, either the eval-uation metric should require matching number ofalternative analyses to get the full points, or thesymmetry of the precision and recall measures hasto be removed.Excluding the methods that combine the anal-yses of two other methods as alternative ones, wesee that the best F-measure (underlined in Table 2)is held by Monson?s ParaMor-Morfessor Mimicfrom 2009 (Monson et al, 2009) in Turkish andBernhard?s method 2 from 2007 (Bernhard, 2006)in all the other three languages.
This means thatexcept for Turkish, there is no improvement in theresults over the three years.
Furthermore, bothof the methods are based purely on segmentation,and so are all the other top methods presentedin Table 2 except for Bordag?s methods (Bordag,2006) and Allomorfessor (Virpioja and Kohonen,2009).3.1.2 Speech recognitionA key factor in the success of large-vocabularycontinuous speech recognition is the system?s abil-1Allomorfessor was trained with a pruned data to obtaina higher recall, whereas ParaMor-Morfessor is explicitly op-timized for F-measure with a separate Hungarian data set.ity to limit the search space using a statistical lan-guage model.
The language model provides theprobability of different recognition hypothesis byusing a model of the co-occurence of its wordsand morphemes.
A properly smoothed n-gram isthe most conventional model.
The n-gram shouldconsist of modeling units that are suitable for thelanguage, typically words or morphemes.In Morpho Challenge state-of-the-art large-vocabulary speech recognizers have been built forevaluations in Finnish and Turkish (Kurimo et al,2006).
The various morpheme analysis algorithmshave been compared by measuring the recogni-tion accuracy with different language models eachtrained and optimized based on units from one ofthe algorithms.
The best results were quite nearto each other, but Bernhard (Bernhard, 2006) andMorfessor Categories MAP were at the top forboth languages.3.1.3 Information retrievalIn the information retrieval task, the algorithmswere tested by using the morpheme segmentationsfor text retrieval.
To return all relevant documents,it is important to match the words in the queries tothe words in the documents irrespective of whichword forms are used.
Typically, a stemming al-gorithm or a morphological analyzer is used to re-duce the inflected forms to their stem or base form.The problem with these methods is that specificrules need to be crafted for each language.
How-ever, these approaches were also tested for com-parison purposes.
The IR experiments were car-ried out by replacing the words in the corpora andqueries by the suggested morpheme segmenta-tions.
Test corpora, queries and relevance assess-ments were provided by Cross-Language Evalua-tion Forum (CLEF) (Agirre et al, 2008).To test the effect of the morpheme segmen-tation, the number of other variables will haveto be minimized, which poses some challenges.For example, the term weighting method will af-fect the results and different morpheme analyz-ers may perform optimally with different weight-ing approaches.
TFIDF and Okapi BM25 termweighting methods have been tested.
In the 2007Challenge, it was noted that Okapi BM25 suffersgreatly if the corpus contains a lot of frequentterms.
These terms are often introduced when thealgorithms segment suffixes from stems.
To over-come this problem, a method for automaticallygenerating stop lists of frequent terms was intro-91duced.
Any term that occurs more times in the cor-pus than a certain threshold is added to the stop listand excluded from indexing.
The method is quitesimple, but it treats all morpheme analysis meth-ods equally as it does not require the algorithmto tag which morphemes are stems and which aresuffixes.
The generated stoplists are also reason-able sized and the results are robust with respectto the stop list cutoff parameter.
With a stop list,Okapi BM25 clearly outperformed TFIDF rank-ing method for all algorithms.
However, the prob-lem of choosing the term weighting approach thattreats all algorithms in an optimal way remainsopen.Another challenge is analyzing the results as itis hard to achieve statistically significant resultswith the limited number of queries (50-60) thatwere available.
In fact, in each language 11-17 ofthe best algorithms belonged to the ?top group?,that is, had no statistically different result to thetop performer of the language.
To improve thesignificance of the results, the number of queriesshould be increased.
This is a known problem inthe field of IR.
However, it is important to test themethods in a real life application and if an algo-rithm gives good results across languages, there isevidence that it is doing something useful.Some conclusions can be drawn from the re-sults.
The language specific reference methods(Porter stemming for English, two-layer morpho-logical analysis for Finnish and German) give thebest results, but the best unsupervised algorithmsare almost at par and the differences are not signif-icant.
For German and Finnish, the best unsuper-vised methods can also beat in a statistically sig-nificant way the baseline of not doing any segmen-tation or stemming.
The best algorithms that per-formed well across languages are ParaMor (Mon-son et al, 2008), Bernhard (Bernhard, 2006), Mor-fessor Baseline, andMcNamee (McNamee, 2008).Comparing the results to the linguistic evalua-tion (section 3.1.1), it seems that methods that per-form well at the IR task tend to have good preci-sion in the linguistic task, with exceptions.
Thus,in the IR task it seems important not to overseg-ment words.
One exception is the method (Mc-Namee, 2008) which simply splits the words intoequal length letter n-grams.
The method gives sur-prisingly good results in the IR task, given the sim-plicity, but suffers from low precision in the lin-guistic task.3.1.4 Machine translationIn phrase-based statistical machine translationprocess there are two stages where morphemeanalysis and segmentation of the words into mean-ingful sub-word units is needed.
The first stageis the alignment of the parallel sentences in thesource and target language for training the transla-tion model.
The second one is training a statisticallanguage model for the production of fluent sen-tences in a morphologically rich target language.In the machine translation tasks used in theMorpho Challenge, the focus has so far been inthe alignment problem.
In the evaluation tasks in-troduced in 2009 the language-pairs were Finnish-English and German-English.
To obtain state-of-the-art results, the evaluation consists of minimumBayes risk (MBR) combination of two transla-tion systems trained on the same data, one us-ing words and the other morphemes as the ba-sic modeling units (de Gispert et al, 2009).
Thevarious morpheme analysis algorithms are com-pared by measuring the translation performancefor different two-model combinations where theword-based model is always the same, but themorpheme-based model is trained based on unitsfrom each of the algorithms in turns.Because the machine translation evaluation hasyet been tried only in 2009, it is difficult to drawconclusions about the results yet.
However, theMorfessor Baseline algorithm seems to be partic-ularly difficult to beat both in Finnish-German andGerman-English task.
The differences betweenthe best results are small, but the ranking in bothtasks was the same: 1.
Morfessor Baseline, 2.
Al-lomorfessor, 3.
The linguistic gold standard mor-phemes (Kurimo et al, 2009b).3.2 Evaluated algorithmsThis section attempts to describe very briefly someof the individual morpheme analysis algorithmsthat have been most successful in the evaluations.Morfessor Baseline (Creutz and Lagus, 2002):This is a public baseline algorithm based on jointlyminimizing the size of the morph codebook andthe encoded size of the all the word forms usingthe minimum description length MDL cost func-tion.
The performance is above average for allevaluated tasks in most languages.Allomorfessor (Kohonen et al, 2009; Virpi-oja and Kohonen, 2009): The development ofthis method was based on the observation that the92Finnish German English0.250.30.350.40.450.50.55Morfessor baseline2007 Bernhard2008 McNamee 4?gram2008 Monson P+M2009 Monson PMU2009 Lignos2009 AllomorfessorFigure 1: Mean Average Precision (MAP) values for some of the best algorithms over the years in the IRtask.
The upper horizontal line shows the ?goal level?
for each language, i.e.
the performance of the bestlanguage specific reference method.
The lower line shows the baseline reference of doing no stemmingor analysis.morph level surface forms of one morpheme areoften very similar and the differences occur closeto the morpheme boundary.
Thus, the allomor-phemes could be modeled by simple mutations.It has been implemented on top of the MorfessorBaseline using maximum a posteriori (MAP) opti-mization.
This model slightly improves the perfor-mance in the linguistic evaluation in all languages(Kurimo et al, 2009b), but in IR and SMT there isno improvement yet.Morfessor Categories MAP (Creutz and La-gus, 2005): In this method hidden Markov modelsare used to incorporate morphotactic categories fortheMorfessor Baseline.
The structure is optimizedby MAP and yields slight improvements in the lin-guistic evaluation for most languages, but not forIR or SMT tasks.Bernhard (Bernhard, 2006): This has been oneof the best performing algorithms in Finnish, En-glish and German linguistic evaluation and in IR(Kurimo et al, 2008).
First a list of the most likelyprefixes and suffixes is extracted and alternativesegmentations are generated for the word forms.Then the best ones are selected based on cost func-tions that favour most frequent analysis and somebasic morphotactics.Bordag (Bordag, 2006): This method appliesiterative LSV and clustering of morphs into mor-phemes.
The performance in the linguistic eval-uation is quite well for Turkish and decent forFinnish (Kurimo et al, 2008).ParaMor (Monson et al, 2008): This methodapplies an unsupervised model for inflection rulesand suffixation for the stems by building linguisti-cally motivated paradigms.
It has obtained one ofthe top performances for all languages when com-bined with the Morfessor Baseline (Kurimo et al,2009a).
Various combination methods have beentested: union, weighted probabilistic average andproposing both the analyses (Monson et al, 2009).Lignos (Lignos et al, 2009): This method isbased on the observation that the derivation ofthe inflected forms can be modeled as transfor-mations.
The best transformations can be foundby optimizing the simplicity and frequency.
Thismethod performs much better in English than inthe other languages (Kurimo et al, 2009b).Promodes (Spiegler et al, 2009): This methodpresents a probabilistic generative model that ap-plies LSV and combines multiple analysis using acommittee.
It seems to generate a large amountof short morphemes, which is difficult for manyof the practical applications.
However, it obtainedthe best performance for the linguistic evaluationin Arabic 2009 (Kurimo et al, 2009b), but did notsurvive as well in other languages, and particularlynot in the IR application.4 Open questions and challengesAlthough more than 50 algorithms have alreadybeen tested in the Morpho Challenge evaluationsand many lessons have been learned from the re-sults and discussions, many challenges are stillopen and untouched.
In fact, the attempts to solvethe problem have perhaps produced even moreopen questions than there were in the beginning.93The main new and open challenges are describedin this section.What is the best analysis algorithm?
Someof the suggested algorithms have produced goodtest results and some even in several tasks and lan-guages, such as Bernhard (Bernhard, 2006), Mon-son ParaMor+Morfessor (Monson et al, 2008)and Allomorfessor (Virpioja and Kohonen, 2009).However, none of the methods perform really wellin all the evaluation tasks and languages and theirmutual performance differences are often rathersmall, even though the morphemes and the al-gorithmic principles are totally different.
Thus,no dominant morpheme analysis algorithm havebeen found.
Furthermore, reaching the perfor-mance level that rivals, or even sometimes domi-nates, the rule-based and language-dependent ref-erence methods does not mean that the solutionsare sufficient.
Often the limited coverage or un-suitable level of details in the analysis for the taskin the reference methods just indicates that theyare not sufficient either and better solutions areneeded.
Another observation which complicatesthe finding and determination of the best algorithmis that in some tasks, such as statistical languagemodels for speech recognition, very different al-gorithms can reach the same performance, becauseadvanced modelling methods can compensate forunsuitable morpheme analysis.What is the meaning of the morphemes?
Insome of the fundamental applications of mor-pheme analysis, such as text understanding, mor-pheme segmentation alone is only part of the solu-tion.
Even more important is to find the meaningfor the obtained morphemes.
The extension of thesegmentation of words into smaller units to iden-tification of the units that correspond to the samemorpheme is a step taken to this direction, but thequestion of the meaning of the morpheme is stillopen.
However, in the unsupervised way of learn-ing, solutions to this may be so tightly tied to theapplications that much more complex evaluationswould be needed.How to evaluate the alternative analyses?
Itis clear that when a word form is separated fromthe sentence context where it was used, the mor-pheme analysis easily becomes ambiguous.
In theMorpho Challenge evaluations this has been takeninto account by allowing multiple alternative anal-yses.
However, in some evaluations, for exam-ple, in the measurement of the recall of the goldstandard morphemes, this leads to unwanted re-sults and may favour methods that always providea large number of alternative analysis.How to improve the analysis using context?A natural way to disambiguate the analysis in-volves taking the sentence context into account.Some of the Morpho Challenge evaluations, forexample, the information retrieval, allow this op-tion when the source texts and queries are given.However, this has not been widely tried yet bythe participants, probably because of the increasedcomputational complexity of the modelling task.How to effectively apply semi-supervisedlearning?
In semi-supervised learning, a small setof labeled data in the form of gold standard anal-ysis for the word forms are provided.
This datacan be used for improving the unsupervised solu-tions based on unlabeled data in several ways: (1)The labeled data is used for tuning some learningparameters, followed by an unsupervised learningprocess for the unlabeled data.
(2) The labeledmorphemes are used as an ideal starting pointto bootstrap the learning on the unlabeled words(self-training).
(3) Using the EM algorithm for es-timating a generative model, the unlabeled casescan be treated as missing data.The best and most practical way of using thepartly labeled data will be determined in futurewhen the semi-supervised task has been evaluatedin the future Morpho Challenge evaluations.
Forthe first time this task will be evaluated in the on-going Morpho Challenge 2010.AcknowledgmentsWe are grateful to the University of Leipzig,University of Leeds, Computational LinguisticsGroup at University of Haifa, Stefan Bordag,Ebru Arisoy, Nizar Habash, Majdi Sawalha, EricAtwell, and Mathias Creutz for making the dataand gold standards in various languages availableto the Challenge.
This work was supported by theAcademy of Finland in the project Adaptive In-formatics, the graduate schools in Language Tech-nology and Computational Methods of Informa-tion Technology, in part by the GALE program ofthe Defense Advanced Research Projects Agency,Contract No.
HR0011-06-C-0022, and in part bythe IST Programme of the European Community,under the FP7 project EMIME (213845) and PAS-CAL Network of Excellence.94ReferencesSteven Abney.
2007.
Semisupervised Learningfor Computational Linguistics.
Chapman andHall/CRC.Eneko Agirre, Giorgio M. Di Nunzio, Nicola Ferro,Thomas Mandl, and Carol Peters.
2008.
CLEF2008: Ad hoc track overview.
In Working Notes forthe CLEF 2008 Workshop.Delphine Bernhard.
2006.
Unsupervised morpholog-ical segmentation based on segment predictabilityand word segments alignment.
In Proc.
PASCALChallenge Workshop on Unsupervised segmentationof words into morphemes, Venice, Italy.
PASCALEuropean Network of Excellence.Stefan Bordag.
2006.
Two-step approach to unsuper-vised morpheme segmentation.
In Proc.
of the PAS-CAL Challenge Workshop on Unsupervised segmen-tation of words into morphemes, Venice, Italy.
PAS-CAL European Network of Excellence.Mathias Creutz and Krista Lagus.
2002.
Unsu-pervised discovery of morphemes.
In Proc.
SIG-PHON/ACL?02, pages 21?30.Mathias Creutz and Krista Lagus.
2005.
Inducing themorphological lexicon of a natural language fromunannotated text.
In Proc.
AKRR?05, pages 106?113.Adria de Gispert, Sami Virpioja, Mikko Kurimo, andWilliam Byrne.
2009.
Minimum bayes riskcombination of translation hypothesis from alter-native morphological decompositions.
In Proc.NAACL?09, pages 73-76.C.
G. de Marcken.
1996.
Unsupervised Language Ac-quisition.
Ph.D. thesis, MIT.Zellig S. Harris.
1955.
From phoneme to morpheme.Language, 31(2):190?222.
Reprinted 1970 in Pa-pers in Structural and Transformational Linguistics,Reidel Publishing Company, Dordrecht, Holland.Feng Jiao, Shaojun Wang, Chi-Hoon Lee, RussellGreiner, and Dale Schuurmans.
2006.
Semi-supervised conditional random fields for improvedsequence segmentation and labeling.
In Proc.ACL?06, pages 209?216.Oskar Kohonen, Sami Virpioja, and Mikaela Klami.2009.
Allomorfessor: Towards unsupervised mor-pheme analysis.
In Evaluating systems for Mul-tilingual and MultiModal Information Access, 9thWorkshop of the Cross-Language Evaluation Forum,CLEF 2008, Revised Selected Papers, Lecture Notesin Computer Science , Vol.
5706.
Springer.Mikko Kurimo, Mathias Creutz, and Krista Lagus.2006.
Unsupervised segmentation of words intomorphemes - challenge 2005, an introduction andevaluation report.
In Proc.
PASCAL ChallengeWorkshop on Unsupervised segmentation of wordsinto morphemes, Venice, Italy.
PASCAL EuropeanNetwork of Excellence.Mikko Kurimo, Mathias Creutz, and Matti Varjokallio.2008.
Morpho Challenge evaluation using a linguis-tic Gold Standard.
In Advances in Multilingual andMultiModal Information Retrieval, 8th Workshop ofthe Cross-Language Evaluation Forum, CLEF 2007,Revised Selected Papers, Lecture Notes in ComputerScience , Vol.
5152, pages 864?873.
Springer.Mikko Kurimo, Ville Turunen, and Matti Varjokallio.2009a.
Overview of Morpho Challenge 2008.In Evaluating systems for Multilingual and Mul-tiModal Information Access, 9th Workshop of theCross-Language Evaluation Forum, CLEF 2008,Revised Selected Papers, Lecture Notes in ComputerScience , Vol.
5706.
Springer.Mikko Kurimo, Sami Virpioja, Ville T. Turunen,Graeme W. Blackwood, and William Byrne.
2009b.Overview and results of Morpho Challenge 2009.
InWorking Notes for the CLEF 2009 Workshop, Corfu,Greece.Constantine Lignos, Erwin Chan, Mitchell P. Marcus,and Charles Yang.
2009.
A rule-based unsupervisedmorphology learning framework.
In Working Notesfor the CLEF 2009 Workshop, Corfu, Greece.Paul McNamee.
2008.
Retrieval experiments at mor-pho challenge 2008.
InWorking Notes for the CLEF2008 Workshop, Aarhus, Denmark, September.Christian Monson, Jaime Carbonell, Alon Lavie, andLori Levin.
2008.
ParaMor: Finding paradigmsacross morphology.
In Advances in Multilingualand MultiModal Information Retrieval, 8th Work-shop of the Cross-Language Evaluation Forum,CLEF 2007, Revised Selected Papers, Lecture Notesin Computer Science , Vol.
5152.
Springer.Christian Monson, Kristy Hollingshead, and BrianRoard.
2009.
Probabilistic paraMor.
In WorkingNotes for the CLEF 2009 Workshop, Corfu, Greece,September.Sebastian Spiegler, Bruno Golenia, and Peter Flach.2009.
PROMODES: A probabilistic generativemodel for word decomposition.
In Working Notesfor the CLEF 2009 Workshop, Corfu, Greece,September.Sami Virpioja and Oskar Kohonen.
2009.
Unsuper-vised morpheme discovery with Allomorfessor.
InWorking Notes for the CLEF 2009 Workshop, Corfu,Greece, September.Xiaojin Zhu.
2005.
Semi-supervised learning litera-ture survey.
Technical Report 1530, Computer Sci-ences, University of Wisconsin-Madison.Xiaojin Zhu.
2010.
Semi-supervised learning.
In En-cyclopedia of Machine Learning.
To appear.95
