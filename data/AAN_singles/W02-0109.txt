NLTK: The Natural Language ToolkitEdward Loper and Steven BirdDepartment of Computer and Information ScienceUniversity of Pennsylvania, Philadelphia, PA 19104-6389, USAAbstractNLTK, the Natural Language Toolkit,is a suite of open source programmodules, tutorials and problem sets,providing ready-to-use computationallinguistics courseware.
NLTK coverssymbolic and statistical natural lan-guage processing, and is interfaced toannotated corpora.
Students augmentand replace existing components, learnstructured programming by example,and manipulate sophisticated modelsfrom the outset.1 IntroductionTeachers of introductory courses on compu-tational linguistics are often faced with thechallenge of setting up a practical programmingcomponent for student assignments andprojects.
This is a difficult task becausedifferent computational linguistics domainsrequire a variety of different data structuresand functions, and because a diverse range oftopics may need to be included in the syllabus.A widespread practice is to employ multipleprogramming languages, where each languageprovides native data structures and functionsthat are a good fit for the task at hand.
Forexample, a course might use Prolog for pars-ing, Perl for corpus processing, and a finite-statetoolkit for morphological analysis.
By relyingon the built-in features of various languages, theteacher avoids having to develop a lot of softwareinfrastructure.An unfortunate consequence is that asignificant part of such courses must be devotedto teaching programming languages.
Further,many interesting projects span a variety ofdomains, and would require that multiplelanguages be bridged.
For example, a studentproject that involved syntactic parsing of corpusdata from a morphologically rich language mightinvolve all three of the languages mentionedabove: Perl for string processing; a finite statetoolkit for morphological analysis; and Prologfor parsing.
It is clear that these considerableoverheads and shortcomings warrant a freshapproach.Apart from the practical component, compu-tational linguistics courses may also depend onsoftware for in-class demonstrations.
This con-text calls for highly interactive graphical userinterfaces, making it possible to view programstate (e.g.
the chart of a chart parser), observeprogram execution step-by-step (e.g.
execu-tion of a finite-state machine), and even makeminor modifications to programs in response to?what if?
questions from the class.
Becauseof these difficulties it is common to avoid livedemonstrations, and keep classes for theoreti-cal presentations only.
Apart from being dull,this approach leaves students to solve importantpractical problems on their own, or to deal withthem less efficiently in office hours.In this paper we introduce a new approach tothe above challenges, a streamlined and flexibleway of organizing the practical componentof an introductory computational linguisticscourse.
We describe NLTK, the NaturalLanguage Toolkit, which we have developed inconjunction with a course we have taught atthe University of Pennsylvania.The Natural Language Toolkit is avail-able under an open source license fromhttp://nltk.sf.net/.
NLTK runs on allplatforms supported by Python, includingWindows, OS X, Linux, and Unix.2 Choice of Programming LanguageThe most basic step in setting up a practicalcomponent is choosing a suitable programminglanguage.
A number of considerationsinfluenced our choice.
First, the language musthave a shallow learning curve, so that noviceprogrammers get immediate rewards for theirefforts.
Second, the language must supportrapid prototyping and a short develop/testcycle; an obligatory compilation step is aserious detraction.
Third, the code should beself-documenting, with a transparent syntax andsemantics.
Fourth, it should be easy to writestructured programs, ideally object-oriented butwithout the burden associated with languageslike C++.
Finally, the language must havean easy-to-use graphics library to support thedevelopment of graphical user interfaces.In surveying the available languages, webelieve that Python offers an especially goodfit to the above requirements.
Python is anobject-oriented scripting language developedby Guido van Rossum and available on allplatforms (www.python.org).
Python offersa shallow learning curve; it was designed tobe easily learnt by children (van Rossum,1999).
As an interpreted language, Python issuitable for rapid prototyping.
Python code isexceptionally readable, and it has been praisedas ?executable pseudocode.?
Python is anobject-oriented language, but not punitivelyso, and it is easy to encapsulate data andmethods inside Python classes.
Finally, Pythonhas an interface to the Tk graphics toolkit(Lundh, 1999), and writing graphical interfacesis straightforward.3 Design CriteriaSeveral criteria were considered in the designand implementation of the toolkit.
These designcriteria are listed in the order of their impor-tance.
It was also important to decide whatgoals the toolkit would not attempt to accom-plish; we therefore include an explicit set of non-requirements, which the toolkit is not expectedto satisfy.3.1 RequirementsEase of Use.
The primary purpose of thetoolkit is to allow students to concentrate onbuilding natural language processing (NLP) sys-tems.
The more time students must spend learn-ing to use the toolkit, the less useful it is.Consistency.
The toolkit should use consis-tent data structures and interfaces.Extensibility.
The toolkit should easilyaccommodate new components, whether thosecomponents replicate or extend the toolkit?sexisting functionality.
The toolkit shouldbe structured in such a way that it is obviouswhere new extensions would fit into the toolkit?sinfrastructure.Documentation.
The toolkit, its datastructures, and its implementation all need tobe carefully and thoroughly documented.
Allnomenclature must be carefully chosen andconsistently used.Simplicity.
The toolkit should structure thecomplexities of building NLP systems, not hidethem.
Therefore, each class defined by thetoolkit should be simple enough that a studentcould implement it by the time they finish anintroductory course in computational linguis-tics.Modularity.
The interaction between differ-ent components of the toolkit should be keptto a minimum, using simple, well-defined inter-faces.
In particular, it should be possible tocomplete individual projects using small partsof the toolkit, without worrying about how theyinteract with the rest of the toolkit.
This allowsstudents to learn how to use the toolkit incre-mentally throughout a course.
Modularity alsomakes it easier to change and extend the toolkit.3.2 Non-RequirementsComprehensiveness.
The toolkit is notintended to provide a comprehensive set oftools.
Indeed, there should be a wide variety ofways in which students can extend the toolkit.Efficiency.
The toolkit does not need tobe highly optimized for runtime performance.However, it should be efficient enough thatstudents can use their NLP systems to performreal tasks.Cleverness.
Clear designs and implementa-tions are far preferable to ingenious yet inde-cipherable ones.4 ModulesThe toolkit is implemented as a collection ofindependent modules, each of which defines aspecific data structure or task.A set of core modules defines basic datatypes and processing systems that are usedthroughout the toolkit.
The token moduleprovides basic classes for processing individualelements of text, such as words or sentences.The tree module defines data structures forrepresenting tree structures over text, suchas syntax trees and morphological trees.
Theprobability module implements classes thatencode frequency distributions and probabilitydistributions, including a variety of statisticalsmoothing techniques.The remaining modules define data structuresand interfaces for performing specific NLP tasks.This list of modules will grow over time, as weadd new tasks and algorithms to the toolkit.Parsing ModulesThe parser module defines a high-level inter-face for producing trees that represent the struc-tures of texts.
The chunkparser module definesa sub-interface for parsers that identify non-overlapping linguistic groups (such as base nounphrases) in unrestricted text.Four modules provide implementationsfor these abstract interfaces.
The srparsermodule implements a simple shift-reduceparser.
The chartparser module defines aflexible parser that uses a chart to recordhypotheses about syntactic constituents.
Thepcfgparser module provides a variety ofdifferent parsers for probabilistic grammars.And the rechunkparser module defines atransformational regular-expression basedimplementation of the chunk parser interface.Tagging ModulesThe tagger module defines a standard interfacefor augmenting each token of a text with supple-mentary information, such as its part of speechor its WordNet synset tag; and provides severaldifferent implementations for this interface.Finite State AutomataThe fsa module defines a data type for encod-ing finite state automata; and an interface forcreating automata from regular expressions.Type CheckingDebugging time is an important factor in thetoolkit?s ease of use.
To reduce the amount oftime students must spend debugging their code,we provide a type checking module, which canbe used to ensure that functions are given validarguments.
The type checking module is usedby all of the basic data types and processingclasses.Since type checking is done explicitly, it canslow the toolkit down.
However, when efficiencyis an issue, type checking can be easily turnedoff; and with type checking is disabled, there isno performance penalty.VisualizationVisualization modules define graphicalinterfaces for viewing and manipulatingdata structures, and graphical tools forexperimenting with NLP tasks.
The draw.treemodule provides a simple graphical inter-face for displaying tree structures.
Thedraw.tree edit module provides an interfacefor building and modifying tree structures.The draw.plot graph module can be used tograph mathematical functions.
The draw.fsamodule provides a graphical tool for displayingand simulating finite state automata.
Thedraw.chart module provides an interactivegraphical tool for experimenting with chartparsers.The visualization modules provide interfacesfor interaction and experimentation; they donot directly implement NLP data structures ortasks.
Simplicity of implementation is thereforeless of an issue for the visualization modulesthan it is for the rest of the toolkit.Text ClassificationThe classifier module defines a standardinterface for classifying texts into categories.This interface is currently implemented by twomodules.
The classifier.naivebayes moduledefines a text classifier based on the Naive Bayesassumption.
The classifier.maxent moduledefines the maximum entropy model for textclassification, and implements two algorithmsfor training the model: Generalized IterativeScaling and Improved Iterative Scaling.The classifier.feature module providesa standard encoding for the information thatis used to make decisions for a particularclassification task.
This standard encodingallows students to experiment with thedifferences between different text classificationalgorithms, using identical feature sets.The classifier.featureselection moduledefines a standard interface for choosing whichfeatures are relevant for a particular classifica-tion task.
Good feature selection can signifi-cantly improve classification performance.5 DocumentationThe toolkit is accompanied by extensivedocumentation that explains the toolkit, anddescribes how to use and extend it.
Thisdocumentation is divided into three primarycategories:Tutorials teach students how to use thetoolkit, in the context of performing specifictasks.
Each tutorial focuses on a single domain,such as tagging, probabilistic systems, or textclassification.
The tutorials include a high-leveldiscussion that explains and motivates thedomain, followed by a detailed walk-throughthat uses examples to show how NLTK can beused to perform specific tasks.Reference Documentation provides precisedefinitions for every module, interface, class,method, function, and variable in the toolkit.
Itis automatically extracted from docstring com-ments in the Python source code, using Epydoc(Loper, 2002).Technical Reports explain and justify thetoolkit?s design and implementation.
They areused by the developers of the toolkit to guideand document the toolkit?s construction.
Stu-dents can also consult these reports if they wouldlike further information about how the toolkit isdesigned, and why it is designed that way.6 Uses of NLTK6.1 AssignmentsNLTK can be used to create student assign-ments of varying difficulty and scope.
In thesimplest assignments, students experiment withan existing module.
The wide variety of existingmodules provide many opportunities for creat-ing these simple assignments.
Once studentsbecome more familiar with the toolkit, they canbe asked to make minor changes or extensions toan existing module.
A more challenging task isto develop a new module.
Here, NLTK providessome useful starting points: predefined inter-faces and data structures, and existing modulesthat implement the same interface.Example: Chunk ParsingAs an example of a moderately difficultassignment, we asked students to constructa chunk parser that correctly identifies basenoun phrase chunks in a given text, bydefining a cascade of transformational chunkingrules.
The NLTK rechunkparser moduleprovides a variety of regular-expressionbased rule types, which the students caninstantiate to construct complete rules.For example, ChunkRule(?<NN.*>?)
buildschunks from sequences of consecutive nouns;ChinkRule(?<VB.>?)
excises verbs fromexisting chunks; SplitRule(?<NN>?, ?<DT>?
)splits any existing chunk that contains asingular noun followed by determiner intotwo pieces; and MergeRule(?<JJ>?, ?<JJ>?
)combines two adjacent chunks where the firstchunk ends and the second chunk starts withadjectives.The chunking tutorial motivates chunk pars-ing, describes each rule type, and provides allthe necessary code for the assignment.
The pro-vided code is responsible for loading the chun-ked, part-of-speech tagged text using an existingtokenizer, creating an unchunked version of thetext, applying the chunk rules to the unchunkedtext, and scoring the result.
Students focus onthe NLP task only ?
providing a rule set withthe best coverage.In the remainder of this section we reproducesome of the cascades created by the students.The first example illustrates a combination ofseveral rule types:cascade = [ChunkRule(?<DT><NN.*><VB.><NN.*>?),ChunkRule(?<DT><VB.><NN.*>?),ChunkRule(?<.*>?),UnChunkRule(?<IN|VB.*|CC|MD|RB.*>?),UnChunkRule("<,|\\.|??|??>"),MergeRule(?<NN.*|DT|JJ.*|CD>?,?<NN.*|DT|JJ.*|CD>?),SplitRule(?<NN.
*>?, ?<DT|JJ>?
)]The next example illustrates a brute-force sta-tistical approach.
The student calculated howoften each part-of-speech tag was included ina noun phrase.
They then constructed chunksfrom any sequence of tags that occurred in anoun phrase more than 50% of the time.cascade = [ChunkRule(?<\\$|CD|DT|EX|PDT|PRP.*|WP.*|\\#|FW|JJ.*|NN.*|POS|RBS|WDT>*?
)]In the third example, the student constructeda single chunk containing the entire text, andthen excised all elements that did not belong.cascade = [ChunkRule(?<.*>+?)ChinkRule(?<VB.*|IN|CC|R.*|MD|WRB|TO|.|,>+?
)]6.2 Class demonstrationsNLTK provides graphical tools that can be usedin class demonstrations to help explain basicNLP concepts and algorithms.
These interactivetools can be used to display relevant data struc-tures and to show the step-by-step execution ofalgorithms.
Both data structures and controlflow can be easily modified during the demon-stration, in response to questions from the class.Since these graphical tools are included withthe toolkit, they can also be used by students.This allows students to experiment at home withthe algorithms that they have seen presented inclass.Example: The Chart Parsing ToolThe chart parsing tool is an example of agraphical tool provided by NLTK.
This tool canbe used to explain the basic concepts behindchart parsing, and to show how the algorithmworks.
Chart parsing is a flexible parsing algo-rithm that uses a data structure called a chart torecord hypotheses about syntactic constituents.Each hypothesis is represented by a single edgeon the chart.
A set of rules determine when newedges can be added to the chart.
This set of rulescontrols the overall behavior of the parser (e.g.,whether it parses top-down or bottom-up).The chart parsing tool demonstrates the pro-cess of parsing a single sentence, with a givengrammar and lexicon.
Its display is divided intothree sections: the bottom section displays thechart; the middle section displays the sentence;and the top section displays the partial syntaxtree corresponding to the selected edge.
But-tons along the bottom of the window are usedto control the execution of the algorithm.
Themain display window for the chart parsing toolis shown in Figure 1.This tool can be used to explain several dif-ferent aspects of chart parsing.
First, it can beused to explain the basic chart data structure,and to show how edges can represent hypothe-ses about syntactic constituents.
It can thenbe used to demonstrate and explain the indi-vidual rules that the chart parser uses to createnew edges.
Finally, it can be used to show howFigure 1: Chart Parsing Toolthese individual rules combine to find a completeparse for a given sentence.To reduce the overhead of setting up demon-strations during lecture, the user can define alist of preset charts.
The tool can then be resetto any one of these charts at any time.The chart parsing tool allows for flexible con-trol of the parsing algorithm.
At each step ofthe algorithm, the user can select which rule orstrategy they wish to apply.
This allows the userto experiment with mixing different strategies(e.g., top-down and bottom-up).
The user canexercise fine-grained control over the algorithmby selecting which edge they wish to apply a ruleto.
This flexibility allows lecturers to use thetool to respond to a wide variety of questions;and allows students to experiment with differentvariations on the chart parsing algorithm.6.3 Advanced ProjectsNLTK provides students with a flexible frame-work for advanced projects.
Typical projectsinvolve the development of entirely new func-tionality for a previously unsupported NLP task,or the development of a complete system out ofexisting and new modules.The toolkit?s broad coverage allows studentsto explore a wide variety of topics.
In our intro-ductory computational linguistics course, topicsfor student projects included text generation,word sense disambiguation, collocation analysis,and morphological analysis.NLTK eliminates the tedious infrastructure-building that is typically associated withadvanced student projects by providingstudents with the basic data structures, tools,and interfaces that they need.
This allows thestudents to concentrate on the problems thatinterest them.The collaborative, open-source nature of thetoolkit can provide students with a sense thattheir projects are meaningful contributions, andnot just exercises.
Several of the students in ourcourse have expressed interest in incorporatingtheir projects into the toolkit.Finally, many of the modules included in thetoolkit provide students with good examplesof what projects should look like, with wellthought-out interfaces, clean code structure, andthorough documentation.Example: Probabilistic ParsingThe probabilistic parsing module was createdas a class project for a statistical NLP course.The toolkit provided the basic data types andinterfaces for parsing.
The project extendedthese, adding a new probabilistic parsing inter-face, and using subclasses to create a prob-abilistic version of the context free grammardata structure.
These new components wereused in conjunction with several existing compo-nents, such as the chart data structure, to definetwo implementations of the probabilistic parsinginterface.
Finally, a tutorial was written thatexplained the basic motivations and conceptsbehind probabilistic parsing, and described thenew interfaces, data structures, and parsers.7 EvaluationWe used NLTK as a basis for the assignmentsand student projects in CIS-530, an introduc-tory computational linguistics class taught atthe University of Pennsylvania.
CIS-530 is agraduate level class, although some advancedundergraduates were also enrolled.
Most stu-dents had a background in either computer sci-ence or linguistics (and occasionally both).
Stu-dents were required to complete five assign-ments, two exams, and a final project.
All classmaterials are available from the course websitehttp://www.cis.upenn.edu/~cis530/.The experience of using NLTK was very pos-itive, both for us and for the students.
Thestudents liked the fact that they could do inter-esting projects from the outset.
They also likedbeing able to run everything on their computerat home.
The students found the extensive doc-umentation very helpful for learning to use thetoolkit.
They found the interfaces defined byNLTK intuitive, and appreciated the ease withwhich they could combine different componentsto create complete NLP systems.We did encounter a few difficulties during thesemester.
One problem was finding large cleancorpora that the students could use for theirassignments.
Several of the students neededassistance finding suitable corpora for theirfinal projects.
Another issue was the fact thatwe were actively developing NLTK during thesemester; some modules were only completedone or two weeks before the students usedthem.
As a result, students who worked athome needed to download new versions of thetoolkit several times throughout the semester.Luckily, Python has extensive support forinstallation scripts, which made these upgradessimple.
The students encountered a couple ofbugs in the toolkit, but none were serious, andall were quickly corrected.8 Other ApproachesThe computational component of computationallinguistics courses takes many forms.
In this sec-tion we briefly review a selection of approaches,classified according to the (original) target audi-ence.Linguistics Students.
Various books intro-duce programming or computing to linguists.These are elementary on the computational side,providing a gentle introduction to students hav-ing no prior experience in computer science.Examples of such books are: Using Computersin Linguistics (Lawler and Dry, 1998), and Pro-gramming for Linguistics: Java Technology forLanguage Researchers (Hammond, 2002).Grammar Developers.
Infrastructurefor grammar development has a long historyin unification-based (or constraint-based)grammar frameworks, from DCG (Pereiraand Warren, 1980) to HPSG (Pollard andSag, 1994).
Recent work includes (Copestake,2000; Baldridge et al, 2002a).
A concurrentdevelopment has been the finite state toolkits,such as the Xerox toolkit (Beesley andKarttunen, 2002).
This work has foundwidespread pedagogical application.Other Researchers and Developers.A variety of toolkits have been created forresearch or R&D purposes.
Examples includethe CMU-Cambridge Statistical LanguageModeling Toolkit (Clarkson and Rosenfeld,1997), the EMU Speech Database System(Harrington and Cassidy, 1999), the GeneralArchitecture for Text Engineering (Bontchevaet al, 2002), the Maxent Package for MaximumEntropy Models (Baldridge et al, 2002b), andthe Annotation Graph Toolkit (Maeda et al,2002).
Although not originally motivated bypedagogical needs, all of these toolkits havepedagogical applications and many have alreadybeen used in teaching.9 Conclusions and Future WorkNLTK provides a simple, extensible, uniformframework for assignments, projects, and classdemonstrations.
It is well documented, easy tolearn, and simple to use.
We hope that NLTKwill allow computational linguistics classes toinclude more hands-on experience with usingand building NLP components and systems.NLTK is unique in its combination of threefactors.
First, it was deliberately designed ascourseware and gives pedagogical goals primarystatus.
Second, its target audience consists ofboth linguists and computer scientists, and itis accessible and challenging at many levels ofprior computational skill.
Finally, it is based onan object-oriented scripting language support-ing rapid prototyping and literate programming.We plan to continue extending the breadthof materials covered by the toolkit.
We arecurrently working on NLTK modules for HiddenMarkov Models, language modeling, and treeadjoining grammars.
We also plan to increasethe number of algorithms implemented by someexisting modules, such as the text classificationmodule.Finding suitable corpora is a prerequisite formany student assignments and projects.
We aretherefore putting together a collection of corporacontaining data appropriate for every moduledefined by the toolkit.NLTK is an open source project, and we wel-come any contributions.
Readers who are inter-ested in contributing to NLTK, or who havesuggestions for improvements, are encouraged tocontact the authors.10 AcknowledgmentsWe are indebted to our students for feedbackon the toolkit, and to anonymous reviewers, JeeBang, and the workshop organizers for com-ments on an earlier version of this paper.
We aregrateful to Mitch Marcus and the Department ofComputer and Information Science at the Uni-versity of Pennsylvania for sponsoring the workreported here.ReferencesJason Baldridge, John Dowding, and Susana Early.2002a.
Leo: an architecture for sharing resourcesfor unification-based grammars.
In Proceedingsof the Third Language Resources and EvaluationConference.
Paris: European Language ResourcesAssociation.http://www.iccs.informatics.ed.ac.uk/~jmb/leo-lrec.ps.gz.Jason Baldridge, Thomas Morton, and GannBierner.
2002b.
The MaxEnt project.http://maxent.sourceforge.net/.Kenneth R. Beesley and Lauri Karttunen.
2002.Finite-State Morphology: Xerox Tools and Tech-niques.
Studies in Natural Language Processing.Cambridge University Press.Kalina Bontcheva, Hamish Cunningham, ValentinTablan, Diana Maynard, and Oana Hamza.
2002.Using GATE as an environment for teaching NLP.In Proceedings of the ACL Workshop on EffectiveTools and Methodologies for Teaching NLP andCL.
Somerset, NJ: Association for ComputationalLinguistics.Philip R. Clarkson and Ronald Rosenfeld.1997.
Statistical language modeling usingthe CMU-Cambridge Toolkit.
In Proceedingsof the 5th European Conference on SpeechCommunication and Technology (EUROSPEECH?97).
http://svr-www.eng.cam.ac.uk/~prc14/eurospeech97.ps.Ann Copestake.
2000.
The (new) LKB system.http://www-csli.stanford.edu/~aac/doc5-2.pdf.Michael Hammond.
2002.
Programming for Linguis-tics: Java Technology for Language Researchers.Oxford: Blackwell.
In press.Jonathan Harrington and Steve Cassidy.
1999.
Tech-niques in Speech Acoustics.
Kluwer.John M. Lawler and Helen Aristar Dry, editors.1998.
Using Computers in Linguistics.
London:Routledge.Edward Loper.
2002.
Epydoc.http://epydoc.sourceforge.net/.Fredrik Lundh.
1999.
An introduction to tkinter.http://www.pythonware.com/library/tkinter/introduction/index.htm.Kazuaki Maeda, Steven Bird, Xiaoyi Ma, and Hae-joong Lee.
2002.
Creating annotation tools withthe annotation graph toolkit.
In Proceedings ofthe Third International Conference on LanguageResources and Evaluation.
http://arXiv.org/abs/cs/0204005.Fernando C. N. Pereira and David H. D. Warren.1980.
Definite clause grammars for language anal-ysis ?
a survey of the formalism and a comparisonwith augmented transition grammars.
ArtificialIntelligence, 13:231?78.Carl Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
Chicago UniversityPress.Guido van Rossum.
1999.
Computer program-ming for everybody.
Technical report, Corpo-ration for National Research Initiatives.
http://www.python.org/doc/essays/cp4e.html.
