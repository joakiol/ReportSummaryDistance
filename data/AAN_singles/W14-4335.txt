Proceedings of the SIGDIAL 2014 Conference, pages 257?259,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsMVA: The Multimodal Virtual AssistantMichael Johnston1, John Chen1, Patrick Ehlen2, Hyuckchul Jung1, Jay Lieske2, Aarthi Reddy1,Ethan Selfridge1, Svetlana Stoyanchev1, Brant Vasilieff2, Jay Wilpon1AT&T Labs Research1, AT&T2{johnston,jchen,ehlen,hjung,jlieske,aarthi,ethan,sveta,vasilieff,jgw}@research.att.comAbstractThe Multimodal Virtual Assistant (MVA)is an application that enables users to planan outing through an interactive multi-modal dialog with a mobile device.
MVAdemonstrates how a cloud-based multi-modal language processing infrastructurecan support mobile multimodal interac-tion.
This demonstration will highlight in-cremental recognition, multimodal speechand gesture input, contextually-aware lan-guage understanding, and the targetedclarification of potentially incorrect seg-ments within user input.1 IntroductionWith the recent launch of virtual assistant appli-cations such as Siri, Google Now, S-Voice, andVlingo, spoken access to information and serviceson mobile devices has become commonplace.
TheMultimodal Virtual Assistant (MVA) project ex-plores the application of multimodal dialog tech-nology in the virtual assistant landscape.
MVA de-parts from the existing paradigm for dialog-basedmobile virtual assistants that display the unfold-ing dialog as a chat display.
Instead, the MVAprototype situates the interaction directly within atouch-based interface that combines a map withvisual information displays.
Users can interactusing combinations of speech and gesture inputs,and the interpretation of user commands dependson both map and GUI display manipulation andthe physical location of the device.MVA is a mobile application that allows usersto plan a day or evening out with friends usingnatural language and gesture input.
Users cansearch and browse over multiple interconnecteddomains, including music events, movie show-ings, and places to eat.
They can specify multi-ple parameters in natural language, such as ?Jazzconcerts around San Francisco next Saturday?.
Asusers find interesting events and places, they canbe collected together into plans and shared withothers.
The central components of the graph-ical user interface are a dynamic map showingbusiness and event locations, and an informationdisplay showing the current recognition, systemprompts, search result listing, or plans (Figure 1).Figure 1: MVA User InterfaceSpoken input begins when the user taps a micro-phone button on the display.
As the user speaks,incremental speech recognition results appear.
Inaddition to enabling voice input, the microphonebutton also activates the map as a drawing can-vas, and enables the user to combine speech withdrawing in coordinated multimodal commands.For example, a user might say, ?Movies playingtonight in this area?
while simultaneously outlin-ing a relevant area on the map.
Or a user may say,?Restaurants?
while drawing a line down a spe-cific street.
MVA determines the intent and dis-ambiguates concepts in the input in order to re-turn relevant results.
MVA then responds to userinput multimodally, by updating the display andusing speech synthesis to summarize results, pro-vide feedback, or make requests for clarificationand additional information.2572 Sample InteractionIn Figure 2 we present a sample of interactionfrom MVA that illustrates some of its capabilities.The user starts with a spoken natural languagequery where they specify some constraints: thetype of music (jazz), location (San Francisco), andtime (tomorrow).
The system gets low confidenceon the location, so it constructs a targeted clarifi-cation for clarifying only that constraint.
The userrepeats the location, and then the system searchesfor events meeting the user?s constraints.
The userthen reviews the results, and follows on with arefinement: ?What about blues??.
Even thoughmany parameters in this query are underspecified,the system applies contextually-aware natural lan-guage understanding and interprets this as ?Bluesconcerts near San Francisco tomorrow?.
Afterselecting a concert, the user then searches for arestaurant nearby.
The location of the concert re-mains salient.
The user follows up with a mul-timodal query combining speech and gesture tosearch for similar restaurants in an adjoining area.U: ?Jazz concerts near San Francisco tomorrow.
?S: ?Where did you want to see jazz tomorrow?
?U: ?San Francisco.
?S: ?I found 20 jazz concerts in SanFrancisco tomorrow.?
[Zooms map to San Francisco and displayspins on map and list of results]U: ?What about blues?
?S: ?I found 20 blues concerts inSan Francisco tomorrow.
?U: [Clicks on a concert listing and adds itto the plan]U: ?Sushi restaurants near there.
?S: ?I found 10 sushi restaurants.
?U: ?What about here??
[Circles adjoining area on map]S: ?I found 5 sushi restaurants inthe area you indicated?Figure 2: Sample Interaction3 System ArchitectureFigure 3 shows the underlying multimodal assis-tant architecture supporting the MVA app.
Theuser interacts with a native iOS client.
When theuser taps the microphone icon, this initiates theflow of audio interleaved with gesture and contextinformation streamed over a WebSocket connec-tion to the platform.This stream of interleaved data is handled atthe server by a multimodal natural language pro-cessing pipeline.
This fields incoming packets ofFigure 3: MVA Multimodal assistant Architecturedata from the client, demuxes the incoming datastream, and sends audio, ink traces, and contextinformation to three modules that operate in par-allel.
The audio is processed using the AT&TWatsonSMspeech recognition engine (Goffin etal., 2005).
Recognition is performed using a dy-namic hierarchical language model (Gilbert et al.,2011) that combines a statistical N-gram languagemodel with weighted sub-grammars.
Ink tracesare classified into gestures using a linear classi-fier.
Speech recognition results serve as input totwo NLUmodules.
A discriminative stochastic se-quence tagger assigns tags to phrases within theinput, and then the overall string with tags is as-signed by a statistical intent classifier to one ofa number of intents handled by the system e.g.search(music event), refine(location).The NLU results are passed along with gesturerecognition results and the GUI and device contextto a multimodal dialog manager.
The contextualresolution component determines if the input is aquery refinement or correction.
In either case, itretrieves the previous command from a user con-text store and combines the new content with thecontext through destructive unification (Ehlen andJohnston, 2012).
A location salience componentthen applies to handle cases where a location isnot specified verbally.
This component uses a su-pervised classifier to select from among a seriesof candidate locations, including the gesture (ifpresent), the current device location, or the currentmap location (Ehlen and Johnston, 2010).The resolved semantic interpretation of the ut-terance is then passed to a Localized Error Detec-tion (LED) module (Stoyanchev et al., 2012).
TheLEDmodule contains two maximum entropy clas-sifiers that independently predict whether a con-258cept is present in the input, and whether a con-cept?s current interpretation is correct.
These clas-sifiers use word scores, segment length, confu-sion networks and other recognition and contextfeatures.
The LED module uses these classifiersto produce two probability distributions; one forpresence and one for correctness.
These distri-butions are then used by a Targeted Clarificationcomponent (TC) to either accept the input as is,reject all of the input, or ask a targeted clarifica-tion question (Stoyanchev et al., 2013).
These de-cisions are currently made using thresholds tunedmanually based on an initial corpus of user inter-action withMVA.
In the targeted clarification case,the input is passed to the natural language gen-eration component for surface realization, and aprompt is passed back to the client for playbackto the user.
Critically, the TC component decideswhat to attempt to add to the common groundby explicit or implicit confirmation, and what toexplicitly query from the user; e.g.
?Where didyou want to see jazz concerts??.
The TC com-ponent also updates the context so that incomingresponses from the user can be interpreted with re-spect to the context set up by the clarification.Once a command is accepted by the multimodaldialog manager, it is passed to the Semantic Ab-straction Layer (SAL) for execution.
The SAL in-sulates natural language dialog capabilities fromthe specifics of any underlying external APIs thatthe system may use in order to respond to queries.A general purpose time normalization componentprojects relative time expressions like ?tomorrownight?
or ?next week?
onto a reference timeframeprovided by the client context and estimates theintended time interval.
A general purpose locationresolution component maps from natural languageexpressions of locations such as city names andneighborhoods to specific geographic coordinates.These functions are handled by SAL?rather thanrelying on any time and location handling in theunderlying information APIs?to provide consis-tency across application domains.SAL also includes mechanisms for categorymapping; the NLU component tags a portionof the utterance as a concept (e.g., a mu-sic genre or a cuisine) and SAL leveragesthis information to map a word sequence togeneric domain-independent ontological represen-tations/categories that are reusable across differentbackend APIs.
Wrappers in SAL map from thesecategories, time, and location values to the spe-cific query language syntax and values for eachspecific underlying API.
In some cases, a singlenatural language query to MVA may require mul-tiple API calls to complete, and this is capturedin the wrapper.
SAL also handles API format dif-ferences by mapping all API responses into a uni-fied format.
This unified format is then passed toour natural language generation component to beaugmented with prompts, display text, and instruc-tions to the client for updating the GUI.
This com-bined specification of a multimodal presentation ispassed to the interaction manager and routed backto the client to be presented to the user.In addition to testing the capabilities of our mul-timodal assistant platform, MVA is designed as atestbed for running experiments with real users.Among other topics, we are planning experimentswith MVA to evaluate methods of multimodal in-formation presentation and natural language gen-eration, error detection and error recovery.AcknowledgementsThanks to Mike Kai and to Deepak Talesra fortheir work on the MVA project.ReferencesPatrick Ehlen and Michael Johnston.
2010.
Locationgrounding in multimodal local search.
In Proceed-ings of ICMI-MLMI, pages 32?39.Patrick Ehlen and Michael Johnston.
2012.
Multi-modal dialogue in mobile local search.
In Proceed-ings of ICMI, pages 303?304.Mazin Gilbert, Iker Arizmendi, Enrico Bocchieri, Dia-mantino Caseiro, Vincent Goffin, Andrej Ljolje,Mike Phillips, Chao Wang, and Jay G. Wilpon.2011.
Your mobile virtual assistant just got smarter!In Proceedings of INTERSPEECH, pages 1101?1104.
ISCA.Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,Dilek Hakkani-Tur, Andrej Ljolje, S. Parthasarathy,Mazim Rahim, Giuseppe Riccardi, and Murat Sar-aclar.
2005.
The AT&T WATSON speech recog-nizer.
In Proceedings of ICASSP, pages 1033?1036,Philadelphia, PA, USA.Svetlana Stoyanchev, Philipp Salletmayer, JingboYang, and Julia Hirschberg.
2012.
Localized de-tection of speech recognition errors.
In Proceedingsof SLT, pages 25?30.Svetlana Stoyanchev, Alex Liu, and Julia Hirschberg.2013.
Modelling human clarification strategies.
InProceedings of SIGDIAL 2013, pages 137?141.259
