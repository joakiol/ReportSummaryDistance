NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 16?22,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsCapitalization Cues Improve Dependency Grammar InductionValentin I. SpitkovskyStanford University and Google Inc.valentin@cs.stanford.eduHiyan AlshawiGoogle Inc., Mountain View, CA, 94043hiyan@google.comDaniel JurafskyStanford University, Stanford, CA, 94305jurafsky@stanford.eduAbstractWe show that orthographic cues can be helpfulfor unsupervised parsing.
In the Penn Tree-bank, transitions between upper- and lower-case tokens tend to align with the boundariesof base (English) noun phrases.
Such signalscan be used as partial bracketing constraints totrain a grammar inducer: in our experiments,directed dependency accuracy increased by2.2% (average over 14 languages having caseinformation).
Combining capitalization withpunctuation-induced constraints in inferencefurther improved parsing performance, attain-ing state-of-the-art levels for many languages.1 IntroductionDependency grammar induction and related prob-lems of unsupervised syntactic structure discoveryare attracting increasing attention (Rasooli and Faili,2012; Marec?ek and Zabokrtsky?, 2011, inter alia).Since sentence structure is underdetermined by rawtext, there have been efforts to simplify the task, via(i) pooling features of syntax across languages (Co-hen et al, 2011; McDonald et al, 2011; Cohenand Smith, 2009); as well as (ii) identifying uni-versal rules (Naseem et al, 2010) ?
such as verbo-centricity (Gimpel and Smith, 2011) ?
that need notbe learned at all.
Unfortunately most of these tech-niques do not apply to plain text, because they re-quire knowing, for example, which words are verbs.As standard practice shifts away from relying ongold part-of-speech (POS) tags (Seginer, 2007; Pon-vert et al, 2010; S?gaard, 2011b; Spitkovsky et al,2011c, inter alia), lighter cues to inducing linguisticstructure become more important.
Examples of use-ful POS-agnostic clues include punctuation bound-aries (Ponvert et al, 2011; Spitkovsky et al, 2011b;Briscoe, 1994) and various kinds of bracketing con-straints (Naseem and Barzilay, 2011; Spitkovsky etal., 2010b; Pereira and Schabes, 1992).
We proposeadding capitalization to this growing list of sourcesof partial bracketings.
Our intuition stems from En-glish, where (maximal) spans of capitalized words?
such as Apple II, World War I, Mayor William H.Hudnut III, International Business Machines Corp. andAlexandria, Va ?
tend to demarcate proper nouns.Consider a motivating example (all of our exam-ples are from WSJ) without punctuation, in which all(eight) capitalized word clumps and uncased numer-als match base noun phrase constituent boundaries:[NP Jay Stevens] of [NP Dean Witter] actually cut hisper-share earnings estimate to [NP $9] from [NP $9.50]for [NP 1989] and to [NP $9.50] from [NP $10.35]in [NP 1990] because he decided sales would be evenweaker than he had expected.and another (whose first word happens to be a leaf),where capitalization complements punctuation cues:[NP Jurors] in [NP U.S. District Court] in [NP Miami]cleared [NP Harold Hershhenson], a former executivevice president; [NP John Pagones], a former vice presi-dent; and [NP Stephen Vadas] and [NP Dean Ciporkin],who had been engineers with [NP Cordis].Could such chunks help bootstrap grammar induc-tion and/or improve the accuracy of already-trainedunsupervised parsers?
In answering these questions,we will focus predominantly on sentence-internalcapitalization.
But we will also show that first words?
those capitalized by convention ?
and uncasedsegments ?
whose characters are not even drawnfrom an alphabet ?
could play a useful role as well.2 English Capitalization from a TreebankWe began our study by consulting the 51,558 parsedsentences of the WSJ corpus (Marcus et al, 1993):30,691 (59.5%) of them contain non-trivially capi-talized fragments ?
maximal (non-empty and not16Count POS Sequence Frac Cum1 27,524 NNP 44.6%2 17,222 NNP NNP 27.9 72.53 4,598 NNP NNP NNP 7.5 79.94 2,973 JJ 4.8 84.85 1,716 NNP NNP NNP NNP 2.8 87.56 1,037 NN 1.7 89.27 932 PRP 1.5 90.78 846 NNPS 1.4 92.19 604 NNP NNPS 1.0 93.110 526 NNP NNP NNP NNP NNP 0.9 93.9WSJ +3,753 more with Count ?
498 6.1%Table 1: Top 10 fragments of POS tag sequences in WSJ.sentence-initial) consecutive sequences of wordsthat each differs from its own lower-cased form.Nearly all ?
59,388 (96.2%) ?
of the 61,731 frag-ments are dominated by noun phrases; slightly lessthan half ?
27,005 (43.8%) ?
perfectly align withconstituent boundaries in the treebank; and about asmany ?
27,230 (44.1%) are multi-token.
Table 1shows the top POS sequences comprising fragments.3 Analytical Experiments with Gold TreesWe gauged the suitability of capitalization-inducedfragments for guiding dependency grammar induc-tion by assessing accuracy, in WSJ,1 of parsing con-straints derived from their end-points.
Following thesuite of increasingly-restrictive constraints on howdependencies may interact with fragments, intro-duced by Spitkovsky et al (2011b, ?2.2), we testedseveral such heuristics.
The most lenient constraint,thread, only asks that no dependency path from theroot to a leaf enter the fragment twice; tear requiresany incoming arcs to come from the same side ofthe fragment; sprawl demands that there be exactlyone incoming arc; loose further constrains any out-going arcs to be from the fragment?s head; and strict?
the most stringent constraint ?
bans externaldependents.
Since only strict is binding for singlewords, we experimented also with strict?
: applyingstrict solely to multi-token fragments (ignoring sin-gletons).
In sum, we explored six ways in whichdependency parse trees can be constrained by frag-ments whose end-points could be defined by capital-ization (or in other various ways, e.g., semantic an-1We converted labeled constituents into unlabeled depen-dencies using deterministic ?head-percolation?
rules (Collins,1999), discarding any empty nodes, etc., as is standard practice.markup punct.
capital initial uncasedthread 98.5 95.0 99.5 98.4 99.2tear 97.9 94.7 98.6 98.4 98.5sprawl 95.1 92.9 98.2 97.9 96.4loose 87.5 74.0 97.9 96.9 96.4strict?
32.7 35.6 38.7 40.3 55.6strict 35.6 39.2 59.3 66.9 61.1Table 2: Several sources of fragments?
end-points and%-correctness of their derived constraints (for English).notations, punctuation or HTML tags in web pages).For example, in the sentence about Cordis, thestrict hypothesis would be wrong about five of theeight fragments: Jurors attaches in; Court takes thesecond in; Hershhenson and Pagones derive their ti-tles, president; and (at least in our reference) Vadasattaches and, Ciporkin and who.
Based on this, wewould consider strict to be 37.5%-accurate.
Butloose ?
and the rest of the more relaxed constraints?
would get perfect scores.
(And strict?
would re-tract the mistake about Jurors but also the correctguesses about Miami and Cordis, scoring only 20%.
)Table 2 (capital) shows scores averaged over theentire treebank.
Columns markup (Spitkovsky et al,2010b) and punct (Spitkovsky et al, 2011b) indicatethat capitalization yields across-the-board more ac-curate constraints (for English) compared with frag-ments derived from punctuation or markup (i.e., an-chor text, bold, italics and underline tags in HTML),for which such constraints were originally intended.4 Pilot Experiments on Supervised ParsingTo further test the potential of capitalization-inducedconstraints, we applied them in the Viterbi-decodingphase of a simple (unlexicalized) supervised depen-dency parser ?
an instance of DBM-1 (Spitkovskyet al, 2012, ?2.1), trained on WSJ sentences with uppunct.
: thread tear sprawl loosenone: 71.8 74.3 74.4 74.5 73.3capital:thread 72.3 74.6 74.7 74.9 73.6tear 72.4 74.7 74.7 74.9 73.6sprawl 72.4 74.7 74.7 74.9 73.4loose 72.4 74.8 74.7 74.9 73.3strict?
71.4 73.7 73.7 73.9 72.7strict 71.0 73.1 73.1 73.2 72.1Table 3: Supervised (directed) accuracy on Section 23of WSJ using capitalization-induced constraints (vertical)jointly with punctuation (horizontal) in Viterbi-decoding.17CoNLL Year Filtered Training Directed Accuracies with Initial Constraints Fragments& Language Tokens / Sentences none thread tear sprawl loose strict?
strict Multi SingleGerman 2006 139,333 12,296 36.3 36.3 36.3 39.1 36.2 36.3 30.1 3,287 30,435Czech ?6 187,505 20,378 51.3 51.3 51.3 51.3 52.5 52.5 51.4 1,831 6,722English ?7 74,023 5,087 29.2 28.5 28.3 29.0 29.3 28.3 27.7 1,135 2,218Bulgarian ?6 46,599 5,241 59.4 59.3 59.3 59.4 59.1 59.3 59.5 184 1,506Danish ?6 14,150 1,599 21.3 17.7 22.7 21.5 21.4 31.4 27.9 113 317Greek ?7 11,943 842 28.1 46.1 46.3 46.3 46.4 31.1 31.0 113 456Dutch ?6 72,043 7,107 45.9 45.8 45.9 45.8 45.8 45.7 29.6 89 4,335Italian ?7 9,142 921 41.7 52.6 52.7 52.6 44.2 52.6 45.8 41 296Catalan ?7 62,811 4,082 61.3 61.3 61.3 61.3 61.3 61.3 36.5 28 2,828Turkish ?6 17,610 2,835 32.9 32.9 32.2 33.0 33.0 33.6 33.9 27 590Portuguese ?6 24,494 2,042 68.9 67.1 69.1 69.2 68.9 68.9 38.5 9 953Hungarian ?7 10,343 1,258 43.2 43.2 43.1 43.2 43.2 43.7 25.5 7 277Swedish ?6 41,918 4,105 48.6 48.6 48.6 48.5 48.5 48.5 48.8 3 296Slovenian ?6 3,627 477 30.4 30.5 30.5 30.4 30.5 30.5 30.8 1 63Median: 42.5 46.0 46.1 46.0 45.0 44.7 32.5Mean: 42.8 44.4 44.8 45.0 44.3 44.6 36.9Table 4: Parsing performance for grammar inducers trained with capitalization-based initial constraints, tested against14 held-out sets from 2006/7 CoNLL shared tasks, and ordered by number of multi-token fragments in training data.to 45 words (excluding Section 23).
Table 3 showsevaluation results on held-out data (all sentences),using ?add-one?
smoothing.
All constraints otherthan strict improve accuracy by about a half-a-point,from 71.8 to 72.4%, suggesting that capitalizationis informative of certain regularities not captured byDBM grammars; moreover, it still continues to beuseful when punctuation-based constraints are alsoenforced, boosting accuracy from 74.5 to 74.9%.5 Multi-Lingual Grammar InductionSo far, we showed only that capitalization informa-tion can be helpful in parsing a very specific genreof English.
Next, we tested its ability to generallyaid dependency grammar induction, focusing on sit-uations when other bracketing cues are unavailable.We experimented with 14 languages from 2006/7CoNLL shared tasks (Buchholz and Marsi, 2006;Nivre et al, 2007), excluding Arabic, Chinese andJapanese (which lack case), as well as Basque andSpanish (which are pre-processed in a way that losesrelevant capitalization information).
For all remain-ing languages we trained only on simple sentences?
those lacking sentence-internal punctuation ?from the relevant training sets (for blind evaluation).Restricting our attention to a subset of the avail-able training data serves a dual purpose.
First, it al-lows us to estimate capitalization?s impact where noother (known or obvious) cues could also be used.Otherwise, unconstrained baselines would not yieldthe strongest possible alternative, and hence not themost interesting comparison.
Second, to the extentthat presence of punctuation may correlate with sen-tence complexity (Frank, 2000), there are benefits to?starting small?
(Elman, 1993): e.g., relegating fulldata to later stages helps training (Spitkovsky et al,2010a; Cohn et al, 2011; Tu and Honavar, 2011).Our base systems induced DBM-1, starting fromuniformly-at-random chosen parse trees (Cohen andSmith, 2010) of each sentence, followed by inside-outside re-estimation (Baker, 1979) with ?add-one?smoothing.2 Capitalization-constrained systems dif-fered from controls in exactly one way: each learnergot a slight nudge towards more promising struc-tures by choosing initial seed trees satisfying an ap-propriate constraint (but otherwise still uniformly).Table 4 contains the stats for all 14 training sets,ordered by number of multi-token fragments.
Fi-nal accuracies on respective (disjoint, full) evalua-tion sets are improved by all constraints other thanstrict, with the highest average performance result-ing from sprawl: 45.0% directed dependency accu-racy,3 on average.
This increase of about two pointsover the base system?s 42.8% is driven primarily byimprovements in two languages (Greek and Italian).2We used ?early-stopping lateen EM?
(Spitkovsky et al,2011a, ?2.3) instead of thresholding or waiting for convergence.3Starting from five parse trees for each sentence (using con-straints thread through strict?)
was no better, at 44.8% accuracy.186 Capitalizing on Punctuation in InferenceUntil now we avoided using punctuation in grammarinduction, except to filter data.
Yet our pilot exper-iments indicated that both kinds of information arehelpful in the decoding stage of a supervised system.We took trained models obtained using the sprawlnudge (from ?5) and proceeded to again apply con-straints in inference (as in ?4).
Capitalization aloneincreased parsing accuracy only slightly, from 45.0to 45.1%, on average.
Using punctuation constraintsinstead led to more improved performance: 46.5%.Combining both types of constraints again resultedin slightly higher accuracies: 46.7%.
Table 5 breaksdown our last average performance number by lan-guage and shows the combined approach to be com-petitive with state-of-the-art.
We suspect that furtherimprovements could be attained by also incorporat-ing both constraints in training and with full data.7 Discussion and A Few Post-Hoc AnalysesOur discussion, thus far, has been English-centric.Nevertheless, languages differ in how they use capi-talization (and even the rules governing a given lan-guage tend to change over time ?
generally towardshaving fewer capitalized terms).
For instance, adjec-tives derived from proper nouns are not capitalizedin French, German, Polish, Spanish or Swedish, un-like in English (see Table 1: JJ).
And while Englishforces capitalization of the first-person pronoun inthe nominative case, I (see Table 1: PRP), in Danishit is the plural second-person pronoun (also I) thatis capitalized; further, formal pronouns (and theircase-forms) are capitalized in German (Sie and Ihre,Ihres...), Italian, Slovenian, Russian and Bulgarian.In contrast to pronouns, single-word proper nouns?
including personal names ?
are capitalized innearly all European languages.
Such shortest brack-etings are not particularly useful for constrainingsets of possible parse trees in grammar induction,however, compared to multi-word expressions; fromthis perspective, German appears less helpful thanmost cased languages, because of noun compound-ing, despite prescribing capitalization of all nouns.Another problem with longer word-strings in manylanguages is that, e.g., in French (as in English)lower-case prepositions may be mixed in with con-tiguous groups of proper nouns: even in surnames,CoNLL Year this State-of-the-Art Systems: POS-& Language Work (i) Agnostic (ii) IdentifiedBulgarian 2006 64.5 44.3 SCAJ5 70.3 SptCatalan ?7 61.5 63.8 SCAJ5 56.3 MZNRCzech ?6 53.5 50.5 SCAJ5 33.3?
MZNRDanish ?6 20.6 46.0 RF 56.5 SarDutch ?6 46.7 32.5 SCAJ5 62.1 MPHelEnglish ?7 29.2 50.3 SAJ 45.7 MPHelGerman ?6 42.6 33.5 SCAJ5 55.8 MPHnlGreek ?7 49.3 39.0 MZ 63.9 MPHenHungarian ?7 53.7 48.0 MZ 48.1 MZNRItalian ?7 50.5 57.5 MZ 69.1 MPHptPortuguese ?6 72.4 43.2 MZ 76.9 SbgSlovenian ?6 34.8 33.6 SCAJ5 34.6 MZNRSwedish ?6 50.5 50.0 SCAJ6 66.8 MPHptTurkish ?6 34.4 40.9 SAJ 61.3 RFH1Median: 48.5 45.2 58.9Mean: 46.7 45.2 57.2?Table 5: Unsupervised parsing with both capitalization-and punctuation-induced constraints in inference, testedagainst the 14 held-out sets from 2006/7 CoNLL sharedtasks, and state-of-the-art results (all sentence lengths) forsystems that: (i) are also POS-agnostic and monolingual,including SCAJ (Spitkovsky et al, 2011a, Tables 5?6)and SAJ (Spitkovsky et al, 2011b); and (ii) rely on goldPOS-tag identities to (a) discourage noun roots (Marec?ekand Zabokrtsky?, 2011, MZ), (b) encourage verbs (Ra-sooli and Faili, 2012, RF), or (c) transfer delexicalizedparsers (S?gaard, 2011a, S) from resource-rich languageswith parallel translations (McDonald et al, 2011, MPH).the German particle von is not capitalized, althoughthe Dutch van is, unless preceded by a given name orinitial ?
hence Van Gogh, yet Vincent van Gogh.7.1 Constraint Accuracies Across LanguagesSince even related languages (e.g., Flemish, Dutch,German and English) can have quite different con-ventions regarding capitalization, one would not ex-pect the same simple strategy to be uniformly useful?
or useful in the same way ?
across disparate lan-guages.
To get a better sense of how universal ourconstraints may be, we tabulated their accuracies forthe full training sets of the CoNLL data, after allgrammar induction experiments had been executed.Table 6 shows that the less-strict capitalization-induced constraints all fall within narrow (yet high)bands of accuracies of just a few percentage points:99?100% in the case of thread, 98?100% for tear,95?99% for sprawl and 94?99% for loose.
By con-trast, the ranges for punctuation-induced constraintsare all at least 10%.
We do not see anything partic-19CoNLL Year Total Training Capitalization-Induced Constraints Punctuation-Induced Constraints& Language Tokens / Sentences thr-d tear spr-l loose str.?
strict thr-d tear spr-l loose str.?
strictArabic 2006 52,752 1,460 ?
?
?
?
?
?
89.6 89.5 81.9 61.2 29.7 33.4?7 102,375 2,912 ?
?
?
?
?
?
90.9 90.6 83.1 61.2 29.5 35.2Basque ?7 41,013 3,190 ?
?
?
?
?
?
96.2 95.7 92.3 81.9 42.8 50.6Bulgarian ?6 162,985 12,823 99.8 99.5 96.6 96.4 51.8 81.0 97.6 97.2 96.1 74.7 36.7 41.2Catalan ?7 380,525 14,958 100 99.5 95.0 94.6 15.8 57.9 96.1 95.5 94.6 73.7 36.0 42.6Chinese ?6 337,162 56,957 ?
?
?
?
?
?
?
?
?
?
?
?
?7 337,175 56,957 ?
?
?
?
?
?
?
?
?
?
?
?Czech ?6 1,063,413 72,703 99.7 98.3 96.2 95.4 42.4 68.0 89.4 89.2 87.7 68.9 37.2 41.7?7 368,624 25,364 99.7 98.3 96.1 95.4 42.6 67.6 89.5 89.3 87.8 69.3 37.4 41.9Danish ?6 80,743 5,190 99.9 99.4 98.3 97.0 59.0 69.7 96.9 96.9 95.2 68.3 39.6 40.9Dutch ?6 172,958 13,349 99.9 99.1 98.4 96.6 16.6 46.3 89.6 89.5 86.4 69.6 42.5 46.2English ?7 395,139 18,577 99.3 98.7 98.0 96.0 17.5 24.8 91.5 91.4 90.6 76.5 39.6 42.3German ?6 605,337 39,216 99.6 98.0 96.7 96.4 41.7 57.1 94.5 93.9 90.7 71.1 37.2 40.7Greek ?7 58,766 2,705 99.9 99.3 98.5 96.6 13.6 50.1 91.3 91.0 89.8 75.7 43.7 47.0Hungarian ?7 111,464 6,034 99.9 98.1 95.7 94.4 46.6 62.0 96.1 94.0 89.0 77.1 28.9 32.6Italian ?7 60,653 3,110 99.9 99.6 99.0 98.8 12.8 68.2 97.1 96.8 96.0 77.8 44.7 47.9Japanese ?6 133,927 17,044 ?
?
?
?
?
?
100 100 95.4 89.0 48.9 63.5Portuguese ?6 177,581 9,071 100 99.0 97.6 97.0 14.4 37.7 96.0 95.8 94.9 74.5 40.3 45.0Slovenian ?6 23,779 1,534 100 99.8 98.9 98.9 52.0 84.7 93.3 93.3 92.6 72.7 42.7 45.8Spanish ?6 78,068 3,306 ?
?
?
?
?
?
96.5 96.0 95.2 75.4 33.4 40.9Swedish ?6 163,301 11,042 99.8 99.6 99.0 97.0 24.7 58.4 90.8 90.4 87.4 66.8 31.1 33.9Turkish ?6 48,373 4,997 100 99.8 96.2 94.0 22.8 42.8 99.8 99.7 95.1 76.9 37.7 42.0?7 54,761 5,635 100 99.9 96.1 94.2 21.6 42.9 99.8 99.7 94.6 76.7 38.2 42.8Max: 100 99.9 99.0 98.9 59.0 84.7 100 100 96.1 89.0 48.9 63.5Mean: 99.8 99.1 97.4 96.4 30.8 57.7 94.6 94.2 91.7 74.0 38.5 43.3Min: 99.3 98.0 95.0 94.0 12.8 24.8 89.4 89.2 81.9 61.2 28.9 32.6Table 6: Accuracies for capitalization- and punctuation-induced constraints on all (full) 2006/7 CoNLL training sets.ularly special about Greek or Italian in these sum-maries that could explain their substantial improve-ments (18 and 11%, respectively ?
see Table 4),though Italian does appear to mesh best with thesprawl constraint (not by much, closely followed bySwedish).
And English ?
the language from whichwe drew our inspiration ?
barely improved withcapitalization-induced constraints (see Table 4) andcaused the lowest accuracies of thread and strict.These outcomes are not entirely surprising: somebest- and worst-performing results are due to noise,since learning via non-convex optimization can bechaotic: e.g., in the case of Greek, applying 113 con-straints to initial parse trees could have a significantimpact on the first grammar estimated in training ?and consequently also on a learner?s final, convergedmodel instance.
We expect the averages (i.e., meansand medians) ?
computed over many data sets ?to be more stable and meaningful than the outliers.7.2 Immediate Impact from CapitalizationNext, we considered two settings that are less af-fected by training noise: grammar inducers immedi-ately after an initial step of constrained Viterbi EMand supervised DBM parsers (trained on sentenceswith up to 45 words), for various languages in theCoNLL sets.
Table 7 shows effects of capitalizationto be exceedingly mild, both if applied alone and intandem with punctuation.
Exploring better ways ofincorporating this informative resource ?
perhapsas soft features, rather than as hard constraints ?and in combination with punctuation- and markup-induced bracketings could be a fruitful direction.7.3 Odds and EndsOur earlier analysis excluded sentence-initial wordsbecause their capitalization is, in a way, trivial.
Butfor completeness, we also tested constraints derivedfrom this source, separately (see Table 2: initials).As expected, the new constraints scored worse (de-spite many automatically-correct single-word frag-ments) except for strict, whose binding constraintsover singletons drove up accuracy.
It turns out, mostfirst words in WSJ are leaves ?
possibly due to adearth of imperatives (or just English?s determiners).We broadened our investigation of the ?first leaf?20CoNLL Year Evaluation Bracketings Unsupervised Training Supervised Parsing& Language Tokens / Sents capital.
punct.
init.
1-step constrained none capital.
punct.
bothArabic 2006 5,215 146 ?
101 18.4 20.6 ?
?
59.8 ?
?
?
?7 4,537 130 ?
311 19.0 23.5 ?
?
63.5 ?
?
?Basque ?7 4,511 334 ?
547 17.4 22.4 ?
?
58.4 ?
?
?Bulgarian ?6 5,032 398 44 552 19.4 28.9 28.4 -0.5 76.7 76.8 78.1 78.2Catalan ?7 4,478 167 24 398 18.0 25.1 25.4 +0.3 78.1 78.3 78.6 78.9Chinese ?6 5,012 867 ?
?
23.5 27.2 ?
?
83.7 ?
?
?
?7 5,161 690 ?
?
19.4 25.0 ?
?
81.0 ?
?
?Czech ?6 5,000 365 48 549 18.6 19.7 19.8 +0.1 64.9 64.8 67.0 66.9?7 4,029 286 57 466 18.0 21.7 ?
?
62.8 ?
?
?Danish ?6 4,978 322 85 590 19.5 27.4 26.0 -1.3 71.9 72.0 74.2 74.3Dutch ?6 4,989 386 28 318 18.7 17.9 17.7 -0.1 60.9 60.9 62.7 62.8English ?7 4,386 214 151 423 17.6 24.0 21.9 -2.1 65.2 65.6 68.5 68.4German ?6 4,886 357 135 523 16.4 23.0 23.7 +0.7 70.7 70.7 71.5 71.4Greek ?7 4,307 197 47 372 17.1 17.1 16.6 -0.5 71.3 71.6 73.5 73.7Hungarian ?7 6,090 390 28 893 17.1 18.5 18.6 +0.1 67.3 67.2 69.8 69.6Italian ?7 4,360 249 71 505 18.6 32.5 34.2 +1.7 66.0 65.9 67.0 66.8Japanese ?6 5,005 709 ?
0 26.5 36.8 ?
?
85.1 ?
?
?Portuguese ?6 5,009 288 29 559 19.3 24.2 24.0 -0.1 80.5 80.5 81.6 81.6Slovenian ?6 5,004 402 7 785 18.3 22.5 22.4 -0.1 67.5 67.4 70.9 70.9Spanish ?6 4,991 206 ?
453 18.0 19.3 ?
?
69.5 ?
?
?Swedish ?6 4,873 389 14 417 20.2 31.4 31.4 +0.0 74.9 74.9 74.7 74.6Turkish ?6 6,288 623 18 683 20.4 26.4 26.7 +0.3 66.1 66.0 66.9 66.7?7 3,983 300 4 305 20.3 24.8 ?
?
67.3 ?
?
?Max: 20.4 32.5 34.2 +1.7 80.5 80.5 81.6 81.6(aggregated as in Tables 4 and 5) Mean: 18.5 24.2 24.1 -0.1 70.1 70.2 71.8 71.8Min: 16.4 17.1 16.6 -2.1 60.9 60.9 62.7 62.8Table 7: Unsupervised accuracies for uniform-at-random projective parse trees (init), also after a step of Viterbi EM,and supervised performance with induced constraints, on 2006/7 CoNLL evaluation sets (sentences under 145 tokens).phenomenon and found that in 16 of the 19 CoNLLlanguages first words are more likely to be leavesthan other words without dependents on the left;4last words, by contrast, are more likely to take de-pendents than expected.
These propensities may berelated to the functional tendency of languages toplace old information before new (Ward and Birner,2001) and could also help bias grammar induction.Lastly, capitalization points to yet another class ofwords: those with identical upper- and lower-caseforms.
Their constraints too tend to be accurate (seeTable 2: uncased), but the underlying text is not par-ticularly interesting.
In WSJ, caseless multi-tokenfragments are almost exclusively percentages (e.g.,the two tokens of 10%), fractions (e.g., 1 1/4) or both.Such boundaries could be useful in dealing with fi-nancial data, as well as for breaking up text in lan-guages without capitalization (e.g., Arabic, Chinese4Arabic, Basque, Bulgarian, Catalan, Chinese, Danish,Dutch, English, German, Greek, Hungarian, Italian, Japanese,Portuguese, Spanish, Swedish vs. Czech, Slovenian, Turkish.and Japanese).
More generally, transitions betweendifferent fonts and scripts should be informative too.8 ConclusionOrthography provides valuable syntactic cues.
Weshowed that bounding boxes signaled by capitaliza-tion changes can help guide grammar induction andboost unsupervised parsing performance.
As withpunctuation-delimited segments and tags from webmarkup, it is profitable to assume only that a singleword derives the rest, in such text fragments, withoutfurther restricting relations to external words ?
pos-sibly a useful feature for supervised parsing models.Our results should be regarded with some cau-tion, however, since improvements due to capitaliza-tion in grammar induction experiments came mainlyfrom two languages, Greek and Italian.
Further re-search is clearly needed to understand the ways thatcapitalization can continue to improve parsing.21AcknowledgmentsFunded, in part, by Defense Advanced Research Projects Age-ncy (DARPA) Machine Reading Program under Air Force Re-search Laboratory (AFRL) prime contract FA8750-09-C-0181.Any opinions, findings, and conclusions or recommendationsexpressed in this material are those of the authors and do notnecessarily reflect the views of DARPA, AFRL, or the US gov-ernment.
We also thank Ryan McDonald and the anonymousreviewers for helpful comments on draft versions of this paper.ReferencesJ.
K. Baker.
1979.
Trainable grammars for speech recognition.In Speech Communication Papers for the 97th Meeting of theAcoustical Society of America.E.
J. Briscoe.
1994.
Parsing (with) punctuation, etc.
Technicalreport, Xerox European Research Laboratory.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared task onmultilingual dependency parsing.
In CoNLL.S.
B. Cohen and N. A. Smith.
2009.
Shared logistic normal dis-tributions for soft parameter tying in unsupervised grammarinduction.
In NAACL-HLT.S.
B. Cohen and N. A. Smith.
2010.
Viterbi training for PCFGs:Hardness results and competitiveness of uniform initializa-tion.
In ACL.S.
B. Cohen, D. Das, and N. A. Smith.
2011.
Unsupervisedstructure prediction with non-parallel multilingual guidance.In EMNLP.T.
Cohn, P. Blunsom, and S. Goldwater.
2011.
Inducing tree-substitution grammars.
Journal of Machine Learning Re-search.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.J.
L. Elman.
1993.
Learning and development in neural net-works: The importance of starting small.
Cognition, 48.R.
Frank.
2000.
From regular to context-free to mildly context-sensitive tree rewriting systems: The path of child languageacquisition.
In A. Abeille?
and O. Rambow, editors, TreeAdjoining Grammars: Formalisms, Linguistic Analysis andProcessing.
CSLI Publications.K.
Gimpel and N. A. Smith.
2011.
Concavity and initializationfor unsupervised dependency grammar induction.
Technicalreport, CMU.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19.D.
Marec?ek and Z. Zabokrtsky?.
2011.
Gibbs sampling withtreeness constraint in unsupervised dependency parsing.
InROBUS.R.
McDonald, S. Petrov, and K. Hall.
2011.
Multi-source trans-fer of delexicalized dependency parsers.
In EMNLP.T.
Naseem and R. Barzilay.
2011.
Using semantic cues to learnsyntax.
In AAAI.T.
Naseem, H. Chen, R. Barzilay, and M. Johnson.
2010.
Usinguniversal linguistic knowledge to guide grammar induction.In EMNLP.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,and D. Yuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In EMNLP-CoNLL.F.
Pereira and Y. Schabes.
1992.
Inside-outside reestimationfrom partially bracketed corpora.
In ACL.E.
Ponvert, J. Baldridge, and K. Erk.
2010.
Simple unsuper-vised identification of low-level constituents.
In ICSC.E.
Ponvert, J. Baldridge, and K. Erk.
2011.
Simple unsuper-vised grammar induction from raw text with cascaded finitestate models.
In ACL-HLT.M.
S. Rasooli and H. Faili.
2012.
Fast unsupervised depen-dency parsing with arc-standard transitions.
In ROBUS-UNSUP.Y.
Seginer.
2007.
Fast unsupervised incremental parsing.
InACL.A.
S?gaard.
2011a.
Data point selection for cross-languageadaptation of dependency parsers.
In ACL-HLT.A.
S?gaard.
2011b.
From ranked words to dependency trees:two-stage unsupervised non-projective dependency parsing.In TextGraphs.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2010a.
FromBaby Steps to Leapfrog: How ?Less is More?
in unsuper-vised dependency parsing.
In NAACL-HLT.V.
I. Spitkovsky, D. Jurafsky, and H. Alshawi.
2010b.
Profitingfrom mark-up: Hyper-text annotations for guided parsing.
InACL.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011a.
LateenEM: Unsupervised training with multiple objectives, appliedto dependency grammar induction.
In EMNLP.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011b.
Punctu-ation: Making a point in unsupervised dependency parsing.In CoNLL.V.
I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky.2011c.
Unsupervised dependency parsing without gold part-of-speech tags.
In EMNLP.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2012.
Threedependency-and-boundary models for grammar induction.In submission to EMNLP.K.
Tu and V. Honavar.
2011.
On the utility of curricula inunsupervised learning of probabilistic grammars.
In IJCAI.G.
Ward and B. J. Birner.
2001.
Discourse and informationstructure.
In D. Schiffrin, D. Tannen, and H. Hamilton, edi-tors, Handbook of Discourse Analysis.
Oxford: Basil Black-well.22
