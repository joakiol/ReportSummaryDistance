PLANNING COHERENTMULT ISENTENTIAL  TEXTEduard H. HovyUSC/Information Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292-6695, U.S.A.HOVY~VAXA.ISI.EDUAbstractThough most text generators are capable of sim-ply stringing together more than one sentence,they cannot determine which order will ensurea coherent paragraph.
A paragraph is coherentwhen the information in successive sentences fol-lows some pattern of inference or of knowledgewith which the hearer is familiar.
To signal suchinferences, speakers usually use relations that llnksuccessive sentences in fixed ways.
A set of 20relations that span most of what people usuallysay in English is proposed in the Rhetorical Struc-ture Theory of Mann and Thompson.
This paperdescribes the formalization of these relations andtheir use in a prototype text planner that struc-tures input elements into coherent paragraphs.1 The  Prob lem o f  CoherenceThe example texts in this paper are generatedby Penman, a systemic grammar-based genera-tor with larger coverage than probably any otherexisting text generator.
Penman was developedat ISI (see \[Mann & Matthiessen 831, \[Mann 831,\[Matthiessen 84\]).
The input to Penman is pro-duced by PEA (Programming Enhancement Ad-visor; see \[Moore 87\]), a program that inspects auser's LISP program and suggests enhancements.PEA is being developed to interact with the userin order to answer his or her questions about thesuggested enhancements.
Its theoretical focus isthe production of explanations over extended in-teractions in ways that are superior to the simplegoal-tree traversal of systems uch as TYRESIAS(\[Davis 76\]) and MYCIN (\[Shortliffe 76\]).Supported by DARPA contract MDAg03 81 C0~5.In answer to the question how does the systemenhance a program~, the following text (not gen-erated by Penman) is not satisfactory:(a).
The system performs the enhance-ment.
Before *hat, the system resolvesconficts.
First, the system asks theuser to tell Jt the characteristic of theprogram to be enhanced.
The systemapp//es transformations to the program./t confrms the enhancement with theuser.
It scans the program in order tofind opportunities to apply transfarma-tions to the program.. .
.
because you have to work too hard to makesense of it.
In contrast, using the same propo-sitions (now rearranged and linked with appro-priate connectives), paragraph (b) (generated byPenman) is far easier to understand:(b).
The system as/ca ~he user to tellit the characteristic of the program tobe enhanced.
Then the system appliestransformations to the program.
In par-ticular, the system scans the programin order to ~nd opportunities to applytransformations to the program.
Thenthe system resolves contlicts.
It con~rmsthe enhancement with the user.
Fina//y,it performs the enhancement.Clearly, you do not get coherent ext simply bystringing together sentences, even if they are re-lated - -  note especially the underlined text in (b)and its corresponding three propositions in (a).The goal of this paper is to describe a method ofplanning paragraphs to be coherent while avoidingunintended spurious effects that result from thejuxtaposition of unrelated pieces of text.1632 Text Structur ingThis planning work, which can be called teztsiructuring, must obviously be clone before theactual generating of language can begin.
Textstructuring is one of a number of pre-generationtext planning tasks.
For some of the other tasksPenman has special-purpose domain-specific solu-tions.
They include:?
aggregat ion:  determining, for input ele-ments, the appropriate level of detail (see\[Hovy 87\]), the scoping of sentences, and theuse of connectives?
reference:  determining appropriate ways ofreferring to items (see \[Appelt 87a, 87b\])?
hypothet ica ls :  determining the introduc-tion, scope, and closing of hypothesis contexts(spans of text in which some values are as-sumed, as in air you want to go to the game,then ...  ~)The problem of text coherence can be character-ized in specific terms as follows.
Assuming that in-put elements are sentence- or clause-sized chunksof representation, the permutation set of the inputelements defines the space of possible paragraphs.A simplistic, brute-force way to achieve coherenttext would be to search this space and pick outthe coherent paragraphs.
This search would befactorlally expensive.
For example, in paragraph(b) above, the 7 input clusters received from PEAprovide 7!
---- 5,040 candidate paragraphs.
How-ever, by utilizing the constraints imposed by co-herence, one can formulate operators that guidethe search and significantly limit the search to amanageable size.
In the example, the operatorsdescribed below produced only 3 candidate para-graphs.
Then, from this set of remaining candi-dates, the best paragraph can be found by apply-ing a relatively simple evaluation metric.The contention of this paper is that, exercis-ing proper care, the coherence relations that holdbetween successive pieces of text can be formu-lated as the abovementioned search operators andused in a hierarchical-expanslon planner to limitthe search and to produce structures describingthe coherent paragraphs.The illustrate this contention, the Penman textstructurer is a simplified top-down planner (as de-scribed first by \[Sacerdoti 77\]).
It uses a formal-ized version of the relations of Rhetorical Struc-ture Theory (see immediately below) as plans.
Itsoutput is one (or more) tree(s) that describe thestructure(s) of coherent paragraphs built from theinput elements.
Input elements are the leaves ofthe tree(s); they are sent to the Penman generator .to be transformed into sentences.3 Previous ApproachesThe heart of the problem is obviously coherence.Coherent ext can be defined as text in which thehearer knows how each part of the text relates tothe whole; i.e., (a) the hearer knows why it is said,and (b) the hearer can relate the semantics of eachpart to a. single overarching framework.In 1978, Hobhs (\[Hobhs 78, 79, 82\]) recognizedthat in coherent text successive pieces of text arerelated in a specified set of ways.
He produceda set of relations organised into four categories,which he postulated as the four types of phenom-ena that occur during conversation.
His argument,unfortunately, contains a number of shortcomings;not only is the categorization not well-motivated,but the llst of relations is incomplete.In her thesis work, McKeown took a differentapproach (\[McKeown 82\]).
She defined a set ofrelatively static schemas that represent the struc-ture of stereotypical paragraphs for describing ob-jects.
In essence, these schemas are paragraphtemplates; coherence is enforced by the correctnesting and 6\]llng.in of templates.
No explicit he-ory of coherence was offered.Mann and Thompson, after a wide-rangingstudy involving hundreds of paragraphs, proposedthat a set of 20 relations uffice to represent therelations that hold within the texts that normallyoccur in  English (\[Mann & Thompson 87, 86,83\]).
These relations, called RST (rhetorical struc-ture theory), are used recursively; the assumption(never explicitly stated) is that a paragraph is onlycoherent if all its parts can eventually be made tofit into one overarching relation.
The enterprisewas completely descriptive; no formal definitionof the relations or justification for their complete-ness were given.
However, the relations do includemost of Hobbs's relations and support McKeown'sschemas.A number of similar descriptions exist.
The de-scription of how parts of purposive text can re-late goes back at least to Aristotle (\[Aristotle 54 D.Both Grimes and Shepherd categorize typical in-tersentential relations (\[(\]rimes 75\] and \[Shepherd26\]).
Hovy (\[Hovy 86\]) describes a program thatuses some relations to slant text.1644 Formal i z ing  RST  Re la t ionsAs defined by Mann and Thompson, RST rela-tions hold between two successive pieces of text(at the lowest level, between two clauses; at thehighest level, between two parts that make upa paragraph} 1.
Therefore, each relation has twoparts, a aucle~ and a satell~te.
To determine theapplicability of the relation, each part has a setof constraints on the entities that can be related.Relations may also have requirements on the com-bination of the two parts.
In addition, each rela-tion has an effect field, which is intended to denotethe conditions which the speaker is attempting toachieve.In formalizing these relations and using themgeneratively to plan paragraphs, rather than ana-lytically to describe paragraph structure, a shift offocus is required.
Relations must be seen as plansthe operators that guide the search through thepermutation space.
The nucleus and satellite con-straints become requirements that must be met byany piece of text before it can be used in the re-lation (i.e., before it can be coherently juxtaposedwith the preceding text}.
The effect field containsa description of the intended effect of the relation(i.e., the goal that the plan achieves, if properlyexecuted}.
Since the goals in generation are com-municative, the intended effect must be seen asthe inferences that the speaker is licensed to makeabout the bearer's knowledge after the successfulcompletion of the relation.Since the relations are used as plans~ and sincetheir satellite and nucleus constraints must be re-formulated as subgoais to the structurer, theseconstraints are best represented in terms of thecommunicative intent of the speaker.
That is, theyare best represented in terms of what the hearerwill know - -  i.e., what inferences the hearer wouldrun - -  upon being told the nucleus or satellitefiller.As it turns out, suitable terms for this purposeare provided by the formal theory of rational inter-action currently being developed by, among oth-ers, Cohen, Levesque, and Perrault.
For example,in ICohen ~z Levesque 851, Cohen and Levesquepresent a proof that the indirect speech act of re-questing can be derived from the following baskmodal operators?
(BEL  x p) -- p follows from x's  beliefs1This is not str ict ly true; a small  number of relations,such as Seqt lence ,  relate more than two pieces of text.However, for ease of use, they have been implemented asbinary relations in the structurer.?
(BMB x y p) -- p follows from x's beliefsabout what x and y mutually believe?
(GOAL x p) - -  p follows from x 's  goals?
( .AFTER a p) - -  p is true in all courses ofevents after action aas well as from a few other operators such as ANDand OR.
They then define suture,ties as, essen-tiaUy, speech act operators with activating condi-tious (g~tes) and e~ectz.
These summaries closelyresemble, in structure, the RST plans describedhere, with gates corresponding to satellite and nu-cleus constraints and effects to intended effects.5 An  ExampleThe RST relation Purpose  expresses the relationbetween an action and its intended result:= Pro.poseNucleus Constraintsz1.
(BMB S H (ACTION ?act-l))2.
(BMB S H (ACTOR ?act-1 ?agt-1))Satellite Constraintsz1.
(BMB S H (STATE ?state-l))2.
(BMB S H (GOAL ?a~-I  ?state-l))s. (B~ S H (RESULT Zact-1 ?~t-2))4.
(BMB S H (OBJ ?act-2 ?state-I))Intended EEectss1.
(BMB S H (BEL ?ag~-I (RESULT ?act-1 ?state-l)))2.
(BMB S H (PURPOSE ?act-I ?state-l))For example, when used to produce the sentenceThe system scans the program in order to find op-portunltJes to apply ~ansformatlons to t~e pro-gram, this relation is instantiated asI:~I3UL'pO|6Nucleus Coustraints-I.
(B~m S H (ACTION SCA~-I)iThe program k scanned2.
(BMB S H (ACTOR SCAN-I SYS-I})The system scans i tSatellite Constraints:1.
(BMB S H (STATE oee-1))Opportunities to apply transformations exkt2.
(BMB S H (GOAL SYS-10PP-1))The system =wants" to find them3.
(BMB S H (RESULT SCAN-1 FIND-I))Scanning wil/result; in findlng4.
(BMB S H (OBJ FIND-10PP-1))the opportunitiesIntended Effects:1.
(BMB S H (BEL SYS-1(RESULT SCAN-10PP-1}))The system ~believes = that scanningwill disclose the opportunities2.
(BMB S H (PURPOSE SCAN-10PP-I))This is the purpose of the scanning15S?
/SRTELL.IrTE_SEQUEttCE~qTELL~TE-,(YHPUTREC with  (P3)=' (~)SRTELL~TE--SEQUEtlCI~ I'OJCL?US--<IrlPUTREC ,A'lth (C2 f14) * (~%rlUCLEUS--<Ir(PUTREC vlt.h (R1 C4)) ~P-)( ,~I'ELLI T E-- SE OUEtICE/tJ ~ , /SRTELL'II'E--('rltPUTREC u4th (FI KS)*  (~)/SATELLITE--ELROORRTIO~ " tNUCLEUS--PURPOS%NUCLEUS--?IttPUTREC v, th (S2) * Co)S~QUEHC~ I=I'tt,ICLEUS-.
<ZHPUTREC utth (R2) ?
~ ~)ttUCL?US--(IHPUTRgC vlth (RI P4 E6) )~Figure 1: Paragraph Structure ~reeThe elements SCAN-l, OPP-1, etc., are partof a network provided to the Penman structurerby PEA.
These elements are defined as propo-sitions in a property-inheritance network of theusual kind written in NIKL (\[Schmolze & Lipkis83\], \[Kaczmarek et aL 86\]), a descendant of KL-ONE (\[Brachman 78\]).
Some input for this exam-ple sentence is:(PEA-SYST~4 SYS-I) " (OPPORTUNITY OPP-I)(PROGRAM PROG-I) (EHABL~4ENT ENAB-S)(SCAN SCAN-I) (DOMAIN F~-S  OPP-I)(ACTOR SCAN-I &",'S-l) (RANGE EN)3-S APPLY-3)(OBJ SCAN-I PROG-I) (APPLY APPLY-3)(RESULT SCAN-1-FIND-l) (ACTOR APPLY-3 SYS-1)(FIND FIND-I) (OBJ APPLY-S TKANS-2)(ACTOR FI~)-I  SYS-I) (RZCIP APPLY-3 PROG-1)(OBJ FIND-I OPP-I) (TRANSFORMATION TRANS-2)The relations are used as plans; their intendedeffects are interpreted as the goals they achieve.In other words, in order to bring about the statein which both speaker and hearer know that OPP-1is the purpose of SCAN-I (and know that they bothknow it, etc.
), the structurer uses Purpose  as aplan and tries to satisfy its constraints.In this system, constraints and goals are inter-changable; for example, in the event that (RESULTSCAN-I FIND-I) is believed not known by thehearer, satellite constraint 3 of the Purpose  re=lation simply becomes the goal to achieve (BHB SH (RESULT SCAN-I FIND-I)).
Similarly, the propo-sitions (B~ S H (RESULT SCAN-1 ?ACT-2)) (BMB SH (0BJ ?ACT-2 0PP-I)) are interpreted as the goalto find some element that could legitimately takethe place of ?ACT-2.In order to enable the relations to nest recur-sively, some relations' nucleuses and satellites con-taln requirements that specify additional relations,such as examples, contrasts, etc.
Of course, theseadditional requirements may only be included ffsuch material can coherently follow the content ofthe nucleus or satellite.
The question of orderingsuch additional constituents i  still under investi-gation.
The question of whether such additionalmaterial should be included at all is not addressed;the structure," tries to say everything it is given.The structurer produces all coherent paragraphs(that is, coherent as defined by the relations) thatsatisfy the given goal(s) for any set of input ele-ments.
For example, paragraph (b) is produced tosatiny the initial goal (BMB S e (SEQUENCE ASK-1?l~E~r)).
This goal is produced by PEA, to-gether with the appropriate representation ele-ments (ASK-1.
SCAM-I, etc.)
in response to thequestion hoto a~oes ~e system enhance a progr~m~.Di~erent initial goals will result in di~erent pars-graphs.Each paragraph is represented asa tree in whichbranch points are RST relations and leaves areinput elements.
Figure 1 is the tree for para-graph (b).
It cont~n, the relations Sequence(signalled by "then" and "finally'i, E laborat ion('in particular'), and Purpose ('in order to').In the corresponding paragraph produced by Pen-man, the relations' characteristic words or phrases(boldfaced below) appear between the blocks oftext they relate:\[The system asks the user to tell itthe character~stlc of the program to beenhanced.l(6) Then \[the system appliestransformations to the program.\](b) Inpart icular ,  \[the system scans the pro-gram\](c) in order  to \[f~nd opportu-nitlea to apply ~ranaformations to theprogram.\]{a) Then \[the system resolvesconflicts.\](e) lit confu'ms the enhance-meng with the user.\](/) Finally, \[it per-forms the enhancement.\](g)166iIinputupdate agendaget next budexpand budgrow treeH\]Ichoose final planRST relationssentencegeneratorFigure 2: Hierarchical Planning Structurer6 ....
The  St ructurerAs stated above, the structurer is a simplifiedtop-down hierarchical expansion planner (see Fig-ure 2).
It operates as follows: given one or morecommunicative goals, it find s RST relations whoseintended effects match (some of) these goals; itthen inspects which of the input elements matchthe nucleus and subgoal constraints for each re-lation.
Unmatched constraints become subgoalswhich are posted on an agenda for the next levelof planning.
The tree can be expanded in eitherdepth-first or breadth-first fashion.
Eventually,the structuring process bottoms out when either:(a) all input elements have been used and unsatis-fied subgoais remain (in which case the structurercould request more input with desired propertiesfrom the encapsulating system); or (b) all goalsaxe satisfied.
If more than one plan (i.e., para.graph tree structure) is produced, the results axeordered by preferring trees with the min imum un-used number of input elements and the min imumnumber of remaining unsatisfied subgoals.
Thebest tree is then traversed in left-to-right order;leaves provide input to Penman to be generatedin English and relations at branch points providetypical interclausal relation words or phrases.
Inthis way the structurer performs top-down goal re-finement clown to the level of the input elements.7 Shor tcomings  and  Fur therWorkThis work is also being tested in a completely sep-arate domain: the generation of text in a multi-media system that answers database queries.
Pen-man produces the following description of the shipKnox (where CTG 070.10 designates a group ofships):(c).
Knox is en route in order to ren-denvous with CTG 070.10, arriving inPearl Harbor on 4/24, for port visit until4~so.In this text, each clause (en route, rendezvous,arrive, visit) is a separate input element; thestructurer linked them using the relations Se-quence and Purpose (the same Purpose asshown above; it is signalled by ~in order toN).However, Penman can also be made to produce(d).
Knox is en route in order to ren-dezvous with CJTG 070.10.
It  w~11 arrivein Pearl Harbor on 4/24.
It will be onport visit until 4/30.The problem is clear: how should sentences inthe paragraph be scoped?
At present, avoidingany claims about a theory, the structurer can feed167Penman either extreme: make everything one sen-tence, or make each input element a separate sen-tence.
However, neither extreme is satisfactory;as is clear from paragraph (b), ashort" spans oftext can be linked and "long" ones left separate.A simple way to implement this is to count thenumber of leaves under each branch (nucleus orsatellite) in the paragraph structure tree.Another shortcoming is the treatment of inputelements as indivisible entities.
This shortcomingis a result of factoring out the problem of aggre-gation as a separate text planning task.
Chunkingtogether input elements (to eliminate detail) ortaking them apart (to be more detailed) has re-ceived scant mention -- see \[Hovy 87\], and for therelated problem of paraphrase see \[Schank 75\] --but this task should interact with text structur-ing in order to provide text that is both optimallydetailed and coherent.At the present time, only about 20~ of the RSTrelations have been formalized to the extent thatthey can be used by the structurer.
This formal-ization process is di~cult, because it goes hand-in-hand with the development of terms with whichto characterize the relations' goals/constra?uts.Though the formalization can never be completelyfinalized -- who  can hope to represent somethinglike motivation or justification complete with allramifications?
-- the hope is that, by having therequirements stated in rather basic terms, the re-lations will be easily adaptable to any new repre-sentation scheme and domain.
(It should be noted,of course, that, to be useful, these formalizationsneed only be as specific and as detailed as the do-m~in model and representation requires.)
In ad-dition, the availability of a set of communicativegoals more detailed than just say or ask (for ex-ample), should make it easier for programs thatrequire output text to interface with the gener-ator.
This is one focus of current text planningwork at ISL8 AcknowledgmentsFor help with Penman, Robert Albano, John Bate-man, Bob Kasper, Christian Matthiessen, LynnPoulton, and Richard Whitney.
For help with theinput, Bill Mann and Johanna Moore.
For generalcomments, all the above, and Cecile Paris, StuartShapiro, and Norm Sondheimer.91.2.ReferencesAppelt, D.E., 1987a.A Computational Model of Referring, SRITechnical Note 409.Appelt, D.E., 1987b.Towards a Plan-Based Theory of ReferringActions, in Natural Language Generation:Recent Advances in Artificial Intelligence,Psyclwlogy, and Linguistic8, Kempen, G.(ed), (Kluwer Academic Publishers, Boston)63-70.3.4.Aristotle, 1954.The Rhetoric, in The l~,eto~c and the Po-etics of Ar~to~e, W. Rhys Roberts (Pans),(Random House, New York).Brachman, R.J., 1987.A Structural Paradigm for RepresentingKnowledge, Ph.D. dissertation, Harvard Uni-versity; also BBN Research Report 3605.5.
Cohen, P.R.
& Levesque, H.J., 1985.Speech Acts and Rationality, Proceedings ofthe A CL Conference, Chicago (49-59).6.
Davis, R., 1976.Applications of Meta-Level Knowledge tothe Constructions, Maintenance, and Use ofLarge Knowledge Bases, Ph.D. dissertation,Stanford University.7.
Grimes, J.E., 1975.The Thread of D/~courseHague).
(Mouton, The8.
Hobbs, J.R., 1978.Why is Discourse Coherent?., SRI TechnicalNote 176.9.10.Hobbs, J.R., 1979.Coherence and Coreference, in Cognitive Sci-ence 3(1), 67-90.Hobbs, J.R., 1982.Coherence in Discourse, in Strategies for Nat-ural Language Processing, Lehnert, W.G.
&Ringle, M.H.
(eds), (Lawrence Erlbaum As-sociates, \]:\[HI.dale N J) 223-243.11.
Hovy, E.H., 1986.Putting Affect into Text, Proceedings ofthe Cognitive Science Society Conference,Amherst (669-671).16812.
Hovy, E.H., 1987.Interpretation i Generation, Proceedings ofthe AAAI Conference, Seattle (545-549).13.
Kaczmarek, T.S., Bates, R. & Robins, G.,1986.Recent Developments in NIKL, Proceedingsof the AAAI  Conference, Philadelphia (978-985).14.
Mann, W.C., 1983.An Overview of the Nigel Text GenerationGrammar, USC/Information Sciences Insti-tute Research Report RR-83-113.15.
Mann, W.C. & Matthiessen, C.M.I.M., 1983.Nigeh A Systemic Grammar for Text Gen-eration, USC/Information Sciences InstituteResearch Report RR-83-I05.16.
Mann, W.C. & Thompson, S.A., 1983.Relational Propositions in Discourse, USC/-Information Sciences Institute Research Re-port RR-83-115.17.
Mann, W.C. & Thompson, S.A., 1986.Rhetorical Structure Theory: Descriptionand Construction of Text Structures, in Nat-ural Language Generation: Nero Results inArtificial Intelligence, Psychology, and L~n-guistics, Kempen, G. (ed), (Kluwer AcademicPublishers, Dordrecht, Boston MA) 279-300.18.
Mann, W.C. & Thompson, S.A., 1987.Rhetorical Structure Theory: A Theory ofText Organization, USC/Information Sci-ences Institute Research Report RR-87-190.19.
Matthiessen, C.M.I.M., 1984.Systemic Grammar in Computation: theNigel Case, USC/Information Sciences Insti-tute Research Report RR-84-121.20.
McKeown, K.R., 1982.Generating Natural Language Text in Re-sponse to Questions about Database Queries,Ph.D.
dissertation, University Of Pennsylva-nia.21.
Moore, J.D., 1988.Enhanced Explanations in Expert andAdvice-Giving Systems, USC/InformationSciences Institute Research Report (forth-coming).22.
Sacerdoti, E., 1977.A Structure for Plans and B?l~avior (North-Holland, Amsterdam).23.
Schank, R.C., 1975.Conceptual Information Processing, (North-Holland, Amsterdam).24.
Schmolze, J.G.
& Lipkis, T.A., 1983.Classification in the KL-ONE KnowledgeRepresentation System, Proceeding8 of the IJ-CAI Conference, Karisruhe (330-332).25.
Shepherd, H.R., 1926.The Fine Art of Writing, (The Macmillan Co,New York).26.
Shortliffe, E.H., 1976.Computer-Based Medical Consultations:MYCIN.169
