Deve lopment  and Use of a Gold-Standard Data  Set forSubject iv i ty  Classif icationsJ anyce  M.  Wiebet  and Rebecca  F. Bruce:\[: and Thomas  P. O 'HarattDepartment  of Computer Science and Computing Research LaboratoryNew Mexico State University, Las Cruces, NM 88003:~Department of Computer ScienceUniversity of North Carolina at AshevilleAsheville, NC 28804-8511wiebe, tomohara@cs, nmsu.
edu, bruce@cs, unca.
eduAbst ractThis paper presents a case study of analyzingand improving intercoder reliability in discoursetagging using statistical techniques.
Bias-corrected tags are formulated and successfullyused to guide a revision of the coding manualand develop an automatic lassifier.1 In t roduct ionThis paper presents a case study of analyz-ing and improving intercoder reliability in dis-course tagging using the statistical techniquespresented in (Bruce and Wiebe, 1998; Bruceand Wiebe, to appear).
Our approach is datadriven: we refine our understanding and pre-sentation of the classification scheme guided bythe results of the intercoder analysis.
We alsopresent the results of a probabilistic lassifierdeveloped on the resulting annotations.Much research in discourse processing hasfocused on task-oriented and instructional di-alogs.
The task addressed here comes to thefore in other genres, especially news reporting.The task is to distinguish sentences used to ob-jectively present factual information from sen-tences used to present opinions and evaluations.There are many applications for which this dis-tinction promises to be important, includingtext categorization and summarization.
Thisresearch takes a large step toward developinga reliably annotated gold standard to supportexperimenting with such applications.This research is also a case study of ana-lyzing and improving manual tagging that isapplicable to any tagging task.
We performa statistical analysis that provides informationthat complements he information provided byCohen's Kappa (Cohen, 1960; Carletta, 1996).In particular, we analyze patterns of agreementto identify systematic disagreements that resultfrom relative bias among judges, because theycan potentially be corrected automatically.
Thecorrected tags serve two purposes in this work.They are used to guide the revision of the cod-ing manual, resulting in improved Kappa scores,and they serve as a gold standard for developinga probabilistic lassifier.
Using bias-correctedtags as gold-standard tags is one way to definea single best tag when there are multiple judgeswho disagree.The coding manual and data from our exper-iments are available at:http://www.cs.nmsu.edu/~wiebe/projects.In the remainder of this paper, we describethe classification being performed (in section 2),the statistical tools used to analyze the data andproduce the bias-corrected tags (in section 3),the case study of improving intercoder agree-ment (in section 4), and the results of the clas-sifter for automatic subjectivity tagging (in sec-tion 5).2 The  Subjective and ObjectiveCategor iesWe address evidentiality in text (Chafe, 1986),which concerns issues uch as what is the sourceof information, and whether information is be-ing presented as fact or opinion.
These ques-tions are particularly important in news report-ing, in which segments presenting opinions andverbal reactions are mixed with segments pre-senting objective fact (van Dijk, 1988; Kan etal., 1998).The definitions of the categories in our cod-246ing manual are intention-based: "If the primaryintention of a sentence is objective presentationof material that is factual to the reporter, thesentence is objective.
Otherwise, the sentence issubjective."
1We focus on sentences about private states,such as belief, knowledge, emotions, etc.
(Quirket al, 1985), and sentences about speech events,such as speaking and writing.
Such sentencesmay be either subjective or objective.
Fromthe coding manual: "Subjective speech-event(and private-state) sentences are used to com-municate the speaker's evaluations, opinions,emotions, and speculations.
The primary in-tention of objective speech-event (and private-state) sentences, on the other hand, is to ob-jectively communicate material that is factualto the reporter.
The speaker, in these cases, isbeing used as a reliable source of information.
"Following are examples of subjective and ob-jective sentences:1.
At several different levels, it's a fascinatingtale.
Subjective sentence.2.
Bell Industries Inc. increased its quarterlyto 10 cents from seven cents a share.
Ob-jective sentence.3.
Northwest Airlines settled the remaininglawsuits filed on behalf of 156 people killedin a 1987 crash, but claims against thejetliner's maker axe being pursued, a fed-eral judge said.
Objective speech-event sen-tence.4.
The South African Broadcasting Corp.said the song "Freedom Now" was "un-desirable for broadcasting."
Subjectivespeech-event sentence.In sentence 4, there is no uncertainty or eval-uation expressed toward the speaking event.Thus, from one point of view, one might haveconsidered this sentence to be objective.
How-ever, the object of the sentence is not presentedas material that is factual to the reporter, sothe sentence is classified as subjective.Linguistic categorizations usually do notcover all instances perfectly.
For example, sen-1 The category specifications in the coding manual axebased on our previous work on tracking point of view(Wiebe, 1994), which builds on Banfield's (1982) linguis-tic theory of subjectivity.tences may fall on the borderline between twocategories.
To allow for uncertainty in the an-notation process, the specific tags used in thiswork include certainty ratings, ranging from 0,for least certain, to 3, for most certain.
As dis-cussed below in section 3.2, the certainty ratingsallow us to investigate whether a model positingadditional categories provides a better descrip-tion of the judges' annotations than a binarymodel does.Subjective and objective categories are poten-tially important for many text processing ap-plications, such as information extraction andinformation retrieval, where the evidential sta-tus of information is important.
In generationand machine translation, it is desirable to gener-ate text that is appropriately subjective or ob-jective (Hovy, 1987).
In summarization, sub-jectivity judgments could be included in doc-ument profiles, to augment automatically pro-duced document summaries, and to help theuser make relevance judgments when using asearch engine.
In addition, they would be usefulin text categorization.
In related work (Wiebeet al, in preparation), we found that articletypes, such as announcement and opinion piece,are significantly correlated with the subjectiveand objective classification.Our subjective category is related to but dif-fers from the statement-opinion category ofthe Switchboard-DAMSL discourse annotationproject (Jurafsky et al, 1997), as well as thegives opinion category of Bale's (1950) modelof small-group interaction.
All involve expres-sions of opinion, but while our category spec-ifications focus on evidentiality in text, theirsfocus on how conversational participants inter-act with one another in dialog.3 Stat is t ica l  ToolsTable 1 presents data for two judges.
The rowscorrespond to the tags assigned by judge 1 andthe columns correspond to the tags assigned byjudge 2.
Let nij denote the number of sentencesthat judge 1 classifies as i and judge 2 classi-fies as j, and let/~ij be the probability that arandomly selected sentence is categorized as iby judge 1 and j by judge 2.
Then, the max-imum likelihood estimate of 15ij is ~ wheren_l_ + ,n++ = ~i j  nij = 504.Table 1 shows a four-category data configu-247Judge 1= DSub j2,3SubjojObjo,1Obj2,3Judge 2 = JSub j2,3 Subjoa Objoa Obj2,3n13 = 15 n14 = 4 rill = 158 n12 = 43n21 =0 n22 =0 n23 =0 n24 =0n31 = 3 n32 = 2 n33 = 2 n34 = 0n41 = 38 n42 -- 48 n43 = 49 n44 = 142n+z = 199 n+2 = 93 n+3 = 66 n+4 = 146nl+ = 220n2+ = 0n3+ = 7n4+ = 277n++ = 504Table 1: Four-Category Contingency Tableration, in which certainty ratings 0 and 1 arecombined and ratings 2 and 3 are combined.Note that the analyses described in this sectioncannot be performed on the two-category dataconfiguration (in which the certainty ratings arenot considered), due to insufficient degrees offreedom (Bishop et al, 1975).Evidence of confusion among the classifica-tions in Table 1 can be found in the marginaltotals, ni+ and n+j.
We see that judge 1 has arelative preference, or bias, for objective, whilejudge 2 has a bias for subjective.
Relative biasis one aspect of agreement among judges.
Asecond is whether the judges' disagreements aresystematic, that is, correlated.
One pattern ofsystematic disagreement is symmetric disagree-ment.
When disagreement is symmetric, thedifferences between the actual counts, and thecounts expected if the judges' decisions were notcorrelated, are symmetric; that is, 5n~j = 5n~ifor i ~ j, where 5ni~ is the difference from inde-pendence.Our goal is to correct correlated disagree-ments automatically.
We are particularly in-terested in systematic disagreements resultingfrom relative bias.
We test for evidence ofsuch correlations by fitting probability modelsto the data.
Specifically, we study bias usingthe model for marginal homogeneity, and sym-metric disagreement using the model for quasi-symmetry.
When there is such evidence, wepropose using the latent class model to correctthe disagreements; this model posits an unob-served (latent) variable to explain the correla-tions among the judges' observations.The remainder of this section describes thesemodels in more detail.
All models can be eval-uated using the freeware package CoCo, whichwas developed by Badsberg (1995) and is avail-able at:http: / /web.math.auc.dk/- jhb/CoCo.3.1 Patterns of D isagreementA probability model enforces constraints on thecounts in the data.
The degree to which thecounts in the data conform to the constraints icalled the fit of the model.
In this work, modelfit is reported in terms of the likelihood ra-tio statistic, G 2, and its significance (Read andCressie, 1988; Dunning, 1993).
The higher theG 2 value, the poorer the fit.
We will considermodel fit to be acceptable if its reference sig-nificance level is greater than 0.01 (i.e., if thereis greater than a 0.01 probability that the datasample was randomly selected from a popula-tion described by the model).Bias of one judge relative to another is evi-denced as a discrepancy between the marginaltotals for the two judges (i.e., ni+ and n+j inTable 1).
Bias is measured by testing the fit ofthe model for marginal homogeneity: ~i+ = P+ifor all i.
The larger the G 2 value, the greaterthe bias.
The fit of the model can be evaluatedas described on pages 293-294 of Bishop et al(1975).Judges who show a relative bias do not al-ways agree, but their judgments may still becorrelated.
As an extreme example, judge 1may assign the subjective tag whenever judge2 assigns the objective tag.
In this example,there is a kind of symmetry in the judges' re-sponses, but their agreement would be low.
Pat-terns of symmetric disagreement can be identi-fied using the model for quasi-symmetry.
Thismodel constrains the off-diagonal counts, i.e.,the counts that correspond to disagreement.
Istates that these counts are the product of a248table for independence and a symmetric table,nij = hi+ ?
)~+j ?/~ij, such that /kij = )~ji.
Inthis formula, )~i+ ?
,k+j is the model for inde-pendence and ),ij is the symmetric interactionterm.
Intuitively, /~ij represents the differencebetween the actual counts and those predictedby independence.
This model can be evaluatedusing CoCo as described on pages 289-290 ofBishop et al (1975).3.2 Producing Bias-Corrected TagsWe use the latent class model to correct sym-metric disagreements that appear to result frombias.
The latent class model was first intro-duced by Lazarsfeld (1966) and was later madecomputationally efficient by Goodman (1974).Goodman's procedure is a specialization of theEM algorithm (Dempster et al, 1977), whichis implemented in the freeware program CoCo(Badsberg, 1995).
Since its development, helatent class model has been widely applied, andis the underlying model in various unsupervisedmachine learning algorithms, including Auto-Class (Cheeseman and Stutz, 1996).The form of the latent class model is that ofnaive Bayes: the observed variables are all con-ditionally independent ofone another, given thevalue of the latent variable.
The latent variablerepresents he true state of the object, and is thesource of the correlations among the observedvariables.As applied here, the observed variables arethe classifications assigned by the judges.
LetB, D, J, and M be these variables, and let Lbe the latent variable.
Then, the latent classmodel is:p(b,d, j ,m, l )  = p(bll)p(dll)p(jll)p(mll)p(l )(by C.I.
assumptions)p( b, l )p( d, l )p(j , l )p( m, l)p(t)3(by definition)The parameters of the modelare {p(b, l),p(d, l),p(j, l),p(m, l)p(l)}.
Once es-t imates  of these parameters are obtained, eachclause can be assigned the most probable latentcategory given the tags assigned by the judges.The EM algorithm takes as input the numberof latent categories hypothesized, i.e., the num-ber of values of L, and produces estimates of theparameters.
For a description of this process,see Goodman (1974), Dawid & Skene (1979), orPedersen & Bruce (1998).Three versions of the latent class model areconsidered in this study, one with two latentcategories, one with three latent categories, andone with four.
We apply these models to threedata configurations: one with two categories(subjective and objective with no certainty rat-ings), one with four categories (subjective andobjective with coarse-grained certainty ratings,as shown in Table 1), and one with eight cate-gories (subjective and objective with fine-grainedcertainty ratings).
All combinations of modeland data configuration are evaluated, except hefour-category latent class model with the two-category data configuration, due to insufficientdegrees of freedom.In all cases, the models fit the data well, asmeasured by G 2.
The model chosen as finalis the one for which the agreement among thelatent categories assigned to the three data con-figurations is highest, that is, the model that ismost consistent across the three data configura-tions.4 Improving Agreement inDiscourse TaggingOur annotation project consists of the followingsteps: 21.
A first draft of the coding instructions isdeveloped.2.
Four judges annotate a corpus accordingto the first coding manual, each spendingabout four hours.3.
The annotated corpus is statistically ana-lyzed using the methods presented in sec-tion 3, and bias-corrected tags are pro-duced.4.
The judges are given lists of sentencesfor which their tags differ from the bias-corrected tags.
Judges M, D, and J par-ticipate in interactive discussions centeredaround the differences.
In addition, afterreviewing his or her list of differences, eachjudge provides feedback, agreeing with the2The results of the first three steps are reported in(Bruce and Wiebe, to appear).249bias-corrected tag in many cases, but argu-ing for his or her own tag in some cases.Based on the judges' feedback, 22 of the504 bias-corrected tags are changed, and asecond draft of the coding manual is writ-ten.5.
A second corpus is annotated by the samefour judges according to the new codingmanual.
Each spends about five hours.6.
The results of the second tagging experi-ment are analyzed using the methods de-scribed in section 3, and bias-corrected tagsare produced for the second data set.Two disjoint corpora are used in steps 2 and5, both consisting of complete articles takenfrom the Wall Street Journal Treebank Corpus(Marcus et al, 1993).
In both corpora, judgesassign tags to each non-compound sentence andto each conjunct of each compound sentence,504 in the first corpus and 500 in the second.The segmentation of compound sentences wasperformed manually before the judges receivedthe data.Judges J and B, the first two authors of thispaper, are NLP researchers.
Judge M is anundergraduate computer science student, andjudge D has no background in computer scienceor linguistics.
Judge J, with help from M, devel-oped the original coding instructions, and JudgeJ directed the process in step 4.The analysis performed in step 3 revealsstrong evidence of relative bias among thejudges.
Each pairwise comparison of judges alsoshows a strong pattern of symmetric disagree-ment.
The two-category latent class model pro-duces the most consistent clusters across thedata configurations.
It, therefore, is used to de-fine the bias-corrected tags.In step 4, judge B was excluded from the in-teractive discussion for logistical reasons.
Dis-cussion is apparently important, because, al-though B's Kappa values for the first study areon par with the others, B's Kappa values foragreement with the other judges change verylittle from the first to the second study (thisis true across the range of certainty values).
Incontrast, agreement among the other judges no-ticeably improves.
Because judge B's poor Per-formance in the second tagging experiment islinked to a difference in procedure, judge B'sStudy 1 Study 2%of  ~ %ofcorpus corpuscovered coveredCertainty Values 0,1,2 or 3M&DM&JD&JB&JB&MB&D0.60 1000.63 1000.57 1000.62 1000.60 1000.58 1000.76 1000.67 1000.65 1000.64 1000.59 1000.59 100Certainty Values 1,2 or 3M&D 0.62 96 0.84 92M & J 0.78 81 0.81 81D & J 0.67 84 0.72 82Certainty Values 2 or 3M&DM&JD&J0.67 890.88 640.76 680.89 810.87 670.88 62Table 2: Palrwise Kappa (a) Scorestags are excluded from our subsequent analysisof the data gathered uring the second taggingexperiment.Table 2 shows the changes, from study 1 tostudy 2, in the Kappa values for pairwise agree-ment among the judges.
The best results areclearly for the two who are not authors of thispaper (D and M).
The Kappa value for theagreement between D and M considering all cer-tainty ratings reaches .76, which allows tenta-tive conclusions on Krippendorf's cale (1980).If we exclude the sentences with certainty rat-ing 0, the Kappa values for pairwise agreementbetween M and D and between J and M areboth over .8, which allows definite conclusionson Krippendorf's scale.
Finally, if we only con-sider sentences with certainty 2 or 3, the pair-wise agreements among M, D, and J all havehigh Kappa values, 0.87 and over.We are aware of only one previous projectreporting intercoder agreement results for simi-lar categories, the switchboard-DAMSL pro jectmentioned above.
While their Kappa results arevery good for other tags, the opinion-statementtagging was not very successful: "The distinc-tion was very hard to make by labelers, and250Test DIJM.H.
:G 2 104.912Sig.
0.000Q.S.
:G 2 0.054Sig.
0.997DIM JIM17.343 136.6600.001 0.0000.128 0.3500.998 0.95Table 3: Tests for Patterns of Agreementaccounted for a large proportion of our interla-beler error" (Jurafsky et al, 1997).In step 6, as in step 3, there is strong evi-dence of relative bias among judges D, J and M.Each pairwise comparison of judges also shows astrong pattern of symmetric disagreement.
Theresults of this analysis are presented in Table3.
3 Also as in step 3, the two-category latentclass model produces the most consistent clus-ters across the data configurations.
Thus, it isused to define the bias-corrected tags for thesecond data set as well.5 Mach ine  Learn ing  Resu l tsRecently, there have been many successful ap-plications of machine learning to discourse pro-cessing, such as (Litman, 1996; Samuel et al,1998).
In this section, we report the resultsof machine learning experiments, in which wedevelop robablistic lassifiers to automaticallyperform the subjective and objective classifica-tion.
In the method we use for developing clas-sifters (Bruce and Wiebe, 1999), a search is per-formed to find a probability model that cap-tures important interdependencies among fea-tures.
Because features can be dropped andadded during search, the method also performsfeature selection.In these experiments, the system considersnaive Bayes, full independence, full interdepen-dence, and models generated from those usingforward and backward search.
The model se-lected is the one with the highest accuracy on aheld-out portion of the training data.10-fold cross validation is performed.
Thedata is partitioned randomly into 10 differentSFor the analysis in Table 3, certainty ratings 0 and 1,and 2 and 3 are combined.
Similar esults are obtainedwhen all ratings are treated as distinct.sets.
On each fold, one set is used for testing,and the other nine are used for training.
Fea-ture selection, model selection, and parameterestimation are performed anew on each fold.The following are the potential features con-sidered on each fold.
A binary feature is in-cluded for each of the following: the presencein the sentence of a pronoun, an adjective, acardinal number, a modal other than will, andan adverb other than not.
We also include abinary feature representing whether or not thesentence begins a new paragraph.
Finally, a fea-ture is included representing co-occurrence ofword tokens and punctuation marks with thesubjective and objective classification.
4 Thereare many other features to investigate in futurework, such as features based on tags assignedto previous utterances ( ee, e.g., (Wiebe et al,1997; Samuel et al, 1998)), and features basedon semantic lasses, such as positive and neg-ative polarity adjectives (Hatzivassiloglou andMcKeown, 1997) and reporting verbs (Bergler,1992).The data consists of the concatenation f thetwo corpora annotated with bias-corrected tagsas described above.
The baseline accuracy, i.e.,the frequency of the more frequent class, is only51%.The results of the experiments are verypromising.
The average accuracy across allfolds is 72.17%, more than 20 percentage pointshigher than the baseline accuracy.
Interestingly,the system performs better on the sentences forwhich the judges are certain.
In a post hoc anal-ysis, we consider the sentences from the seconddata set for which judges M, J, and D rate theircertainty as 2 or 3.
There are 299/500 such sen-tences.
For each fold, we calculate the system'saccuracy on the subset of the test set consistingof such sentences.
The average accuracy of thesubsets across folds is 81.5%.Taking human performance as an upperbound, the system has room for improvement.The average pairwise percentage agreement be-tween D, J, and M and the bias-corrected tags inthe entire data set is 89.5%, while the system'spercentage agreement with the bias-correctedtags (i.e., its accuracy) is 72.17%.aThe per-class enumerated feature representationfrom (Wiebe et ai., 1998) is used, with 60% as the con-ditional independence utoff threshold.2516 Conclus ionThis paper demonstrates a procedure for auto-matically formulating a single best tag whenthere are multiple judges who disagree.
Theprocedure is applicable to any tagging task inwhich the judges exhibit symmetric disagree-ment resulting from bias.
We successfully usebias-corrected tags for two purposes: to guidea revision of the coding manual, and to developan automatic classifier.
The revision of the cod-ing manual results in as much as a 16 point im-provement in pairwise Kappa values, and raisesthe average agreement among the judges to aKappa value of over 0.87 for the sentences thatcan be tagged with certainty.Using only simple features, the classifierachieves an average accuracy 21 percentagepoints higher than the baseline, in 10-fold crossvalidation experiments.
In addition, the aver-age accuracy of the classifier is 81.5% on thesentences the judges tagged with certainty.
Thestrong performance of the classifier and its con-sistency with the judges demonstrate the valueof this approach to developing old-standardtags.7 AcknowledgementsThis research was supported in part by theOffice of Naval Research under grant numberN00014-95-1-0776.
We are grateful to MatthewT.
Bell and Richard A. Wiebe for participatingin the annotation study, and to the anonymousreviewers for their comments and suggestions.ReferencesJ.
Badsberg.
1995.
An Environment for Graph-ical Models.
Ph.D. thesis, Aalborg University.R.
F. Bales.
1950.
Interaction Process Analysis.University of Chicago Press, Chicago, ILL.Ann Banfield.
1982.
Unspeakable Sentences:Narration and Representation i  the Lan-guage of Fiction.
Routledge & Kegan Paul,Boston.S.
Bergler.
1992.
Evidential Analysis o.f Re-ported Speech.
Ph.D. thesis, Brandeis Univer-sity.Y.M.
Bishop, S. Fienberg, and P. Holland.1975.
Discrete Multivariate Analysis: Theoryand Practice.
The MIT Press, Cambridge.R.
Bruce and J. Wiebe.
1998.
Word sense dis-tinguishability and inter-coder agreement.
In252Proc.
3rd Conference on Empirical Methodsin Natural Language Processing (EMNLP-98), pages 53-60, Granada, Spain, June.
ACLSIGDAT.R.
Bruce and J. Wiebe.
1999.
Decompos-able modeling in natural language processing.Computational Linguistics, 25(2).R.
Bruce and J. Wiebe.
to appear.
Recognizingsubjectivity: A case study of manual tagging.Natural Language Engineering.J.
Carletta.
1996.
Assessing agreement on clas-sification tasks: The kappa statistic.
Compu-tational Linguistics, 22(2):249-254.W.
Chafe.
1986.
Evidentiality in English con-versation and academic writing.
In WallaceChafe and Johanna Nichols, editors, Eviden-tiality: The Linguistic Coding of Epistemol-ogy, pages 261-272.
Ablex, Norwood, NJ.P.
Cheeseman and J. Stutz.
1996.
Bayesianclassification (AutoClass): Theory and re-sults.
In Fayyad, Piatetsky-Shapiro, Smyth,and Uthurusamy, editors, Advances inKnowledge Discovery and Data Mining.AAAI Press/MIT Press.J.
Cohen.
1960.
A coefficient of agreement fornominal scales.
Educational and Psychologi-cal Meas., 20:37-46.A.
P. Dawid and A. M. Skene.
1979.
Maximumlikelihood estimation of observer error-ratesusing the EM algorithm.
Applied Statistics,28:20-28.A.
Dempster, N. Laird, and D. Rubin.
1977.Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the RoyalStatistical Society, 39 (Series B):1-38.T.
Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Com-putational Linguistics, 19(1):75-102.L.
Goodman.
1974.
Exploratory latent struc-ture analysis using both identifiable andunidentifiable models.
Biometrika, 61:2:215-231.V.
Hatzivassiloglou and K. McKeown.
1997.Predicting the semantic orientation of adjec-tives.
In ACL-EACL 1997, pages 174-181,Madrid, Spain, July.Eduard Hovy.
1987.
Generating Natural Lan-guage under Pragmatic Constraints.
Ph.D.thesis, Yale University.D.
Jurafsky, E. Shriberg, and D. Biasca.1997.
Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual,draft 13.
Technical Report 97-01, Universityof Colorado Institute of Cognitive Science.M.-Y.
Kan, J. L. Klavans, and K. R. McKe-own.
1998.
Linear segmentation a d segmentsignificance.
In Proc.
6th Workshop on VeryLarge Corpora (WVLC-98), pages 197-205,Montreal, Canada, August.
ACL SIGDAT.K.
Krippendorf.
1980.
Content Analysis: AnIntroduction to its Methodology.
Sage Publi-cations, Beverly Hills.P.
Lazarsfeld.
1966.
Latent structure analy-sis.
In S. A. Stouffer, L. Guttman, E. Such-man, P.Lazarfeld, S. Star, and J. Claussen,editors, Measurement and Prediction.
Wiley,New York.D.
Litman.
1996.
Cue phrase classification us-ing machine learning.
Journal of ArtificialIntelligence Research, 5:53-94.M.
Marcus,Santorini, B., and M. Marcinkiewicz.
1993.Building a large annotated corpus of English:The penn treebank.
Computational Linguis-tics, 19(2):313-330.Ted Pedersen and Rebecca Bruce.
1998.Knowledge lean word-sense disambiguation.In Proc.
of the 15th National Conference onArtificial Intelligence (AAAI-98), Madison,Wisconsin, July.R.
Quirk, S. Greenbaum, G. Leech, andJ.
Svartvik.
1985.
A Comprehensive Gram-mar of the English Language.
Longman, NewYork.T.
Read and N. Cressie.
1988.
Goodness-of-fit Statistics for Discrete Multivariate Data.Springer-Verlag Inc., New York, NY.K.
Samuel, S. Carberry, and K. Vijay-Shanker.
1998.
Dialogue act tagging withtransformation-based l arning.
In Proc.COLING-ACL 1998, pages 1150-1156, Mon-treal, Canada, August.T.A.
van Dijk.
1988.
News as Discourse.Lawrence Erlbaum, Hillsdale, NJ.J.
Wiebe, R. Bruce, and L. Duan.
1997.Probabilistic event categorization.
In Proc.Recent Advances in Natural Language Pro-cessing (RANLP-97), pages 163-170, TsigovChark, Bulgaria, September.J.
Wiebe, K. McKeever, and R. Bruce.
1998.Mapping collocational properties into ma-chine learning features.
In Proc.
6th Work-253shop on Very Large Corpora (WVLC-98),pages 225-233, Montreal, Canada, August.ACL SIGDAT.J.
Wiebe, J. Klavans, and M.Y.
Kan. in prepa-ration.
Verb profiles for subjectivity judg-ments and text classification.J.
Wiebe.
1994.
Tracking point of viewin narrative.
Computational Linguistics,20(2):233-287.
