AN EXPERIMENT ON " " " L b,\.RNINC APPROPRIATESELECTIONAL RESTRICTIONS I?I{OM A PARSED CORPUS1,'r~mccsc l \ [ ibas  l?ramis*Depar tament  de L lenguatges  i Sist ;emes Inform~tt ics ,  Un ivers i ta t ;  Po l i tScn ica  de  Cata lunyaPau  Garga l lo  5, 08082 Barce lona ,  SPA IN .
eqna ih  r ibasQls i .upc .esAbstractWe present a methodology to extract SelectionalRestrictions at a variable level of abstraction fromphrasally analyzed corpora.
The method relays ia theuse of a wide-coverage noun taxonomy and a statis-tical measure of the co-occurrence of linguistic items.Some experimental results about the performance ofthe method are provided.Keywords: large text corpora, computationallexicons1 INT I~ODUCTIONThese last years there has been a common agreementin the natural anguage processing research communityon the importance of having an extensive coverage ofthe surface lexical semantics of the domain to workwith, (specially, typical contexts of use).
'iFhis knowl-edge may be expressed at different levels of abstractiondepending on the phenomena involved: selecLional re~strictions (SR~), lexical preferences, eel-locations, etc.We are specially interested on SIhs, which can be ex-.pressed as semantic type constraints t/tat a word se~seimposes on the words with which it combines in theprocess of semantic interpretation.
SRs nmst iachlcleinformation on the syntactic position of the words thai;are being restricted semantically.
For instance, one ofthe senses of the verb drink restricts its subject ~o bean animal and its object to be a liq'uid.SILs may help a parser to prefer some parses amongseveral grammatical ones \[WFB90\].
Furthermore, S\]{smay help the parser when deciding tilt s(-mantic roleplayed by a syntactic complement.
Lexicography isalso interested in the acquisition of SlCs.
On the onehand, SRs are an interesting inform~xtion to be im:Imiedin dictionaries (defining in co~dezt approach).
Oa th(;other band, ;m \[ClI90\] remark, the e\[lbrt involved ht all-*This research as been supported by a grant conceded by theGeneralitat de Catahmya, 91-1)OGC-1,191.
Much of the work re-ported here wa-n carried out, during a visit, at.
the Cl)tllptt\[el' ,ab-oratory, University of Cambridge.
11 am grateful t,o Ted Briscoeand t\[oraclo Rodriguez by their vMuab\[e eo~tHlleltt.s.alyzing and cl~ussifying all tile linguistic material pro-vided by concordances of use of a word can be ex-tremely labor-intensiw.
~.
If it was possible to representroughly the Sits of the word being studied, it could bepossible to clmssify roughly the concordances automati-cally in the different word uses before the lexicographeranalysis.The possible sources of Sits are: introspection bylexicographers, machine-readable dictionaries, ~nd on--line corpora.
'l'he main advantage of the latter is thatthey provide experimental evidence of words uses.
tl.e--cently, several approaches on acquiring different kindsof lexical information from corpora have been devel-oped \[BPV92, CG\[III91, CH90, Res92\].
This paperis interested in exploring the amenabil ity of using amethod f(~r extracting SI{~ from textual data, in theline of these works.
The aim of the proposed tech-nique is to learn the Sl~,s that a word is imposing, fromthe analysis of the examples of use of that  word con-taim'd in the corpus.
An i l lustration of such a learn-ing is shown in Figure l, where the system, depart-ing from the three examples of use, and knowing thatprosec'utor, buyer attd lawmaker are nouns belongingto the semantic lass <pera-m~, individual >, and thati~zdictme~d, assura~zce and legislation are members of< legal_in.strttmeuZ >, should induce that the verb see/.
'imposes SILs that constraint he subject to be a nmm-.bet of the semantic type <peraon, individval>, andthe object to be a kind of < legaLiustrurnent >.
Con-eluding, the system should extract for each word (withcontplemeut,..~) having enough number occurrences ofuse in the corpus and for each of its syntactic eomple-melttS, a li:;t of the alternative Sl~s that  this word isimposing.In order to detect the SRs that a word imposesin il;s coutext hy means of statistical techniques twodistiuct approaches haw~, been proposed: word-based\[CC, HIIgl\], and class..based \[Bpv92, ~tes92\].
Word-.based al~proach infers SRs as the collection of wordsthat co-occur significantly in the syntactic context ofthe studi,'.d word.
The clmss--based techniques gatherthe; dillhrene nouns by means of semantic l,'uqses.
Theadvantages of the latter are clear.
On the one hand,sl.atist.ically meaningful data can be gathered From (tel--769Figure 1: Example of the acquisition of Slgs for theverb seek from three examples of use?
Three  examples  of  useprosecutors may soon seek an indictment on rack-eteering and securities fraud charges.In the recent past, bond buyers didn't seek suchassurance.Some lawmakers may seek legislation to limitoverly restrictive insurance policies.?
The  ext rac ted  SILs(seek, subject, <person, individual>)(seek, object, < legal_instrument >)atively) small corpora,and not only for the most fre-quent words.
On the other hand, SRs are generalizedto new examples not present in the training set.
Fi-nMly, the acquired SRs are more independent of thelexical choices made in the training corpus.We have developed and implemented a method forautomatically extracting class-based SRs from on-linecorpora.
In section 2 we describe it while discussingother approaches.
In section 3 we analyze some dataabout the performance of an experiment run in a Unixmachine, on a corpus of 800,000 words.
Finally, in sec-tion 4 we discuss the performance achieved, and sug-gest further refinements of the technique in order tosolve some remaining problems.2 THE METHOD OF ACQUIS IT IONSRs have been used to express semantic constraintsholding in different syntactic and functional configu-rations.
However, in this paper we focus only in se-lectional restrictions holding between verbs and theircomplements.
The method can be easily exported toother configurations.
We won't distinguish the SiTs im-posed by verbs on arguments and adjuncts.
We believethat few adjuncts are going to provide enough evidencein the corpus for creating SRs.
In the following para-graphs we describe the functional specification of thesystem.Tra in ing  set The input to the learning process isa list of co-occurrence triples codifying the co-occurrence of verbs and complement heads in thecorpus: (verb, syntaclic relationship, noun).
Verband noun are the lemrnas of the inflected forms ap-pearing in text.
Syntactic relationship codes thekind of complement: 0subject, I object , or prepo-sition in case it is a PP.
A method to draw theco-occurrence triples from corpus is proposed insubsection 2.1.Output  The result of the learning process is a setof syntactic SPas, (verb, syntactic relationship, se-mantic class}.
Semantic classes are representedextensionally as sets of nouns.
SRa are only ac-quired if there are enough cases in the corpus asto gather statistical evidence.
As long as distinctuses of the same verb can have different SRs, wepermit to extract more than one class for the samesyntactic position.
Nevertheless, they must beumtually disjoint, i.e.
not related by hyperonymy.P rev ious  knowledge  used In the process of learn-ing SRs, the system needs to know how words areclustered in semantic classes, and how semanticclasses are hierarchically organized.
Ambiguouswords must be represented ms having different hy-peronym classes.
In subsection 2.2 we defend theuse of a b'road-coverage taxonomy.Learn ing  process The computational process is di-vided in three stages: (1) Guessing the possiblesemantic lasses, i.e.
creation of the space of can-didates.
In principle, all the hyperonyms (at alllevels) of the nouns appearing in the training setare candidates.
(2) Evaluation of the appropriate~hess of the candidates.
In order to compare thedifferent candidates, art statistical measure sum-ma.rizing the relevance of the occurrence of each ofthe candidate classes is used.
(3) Selection of themost appropriate subset of the candidate space toconvey the SILs, taking into account hat the finalclasses must be mutually disjoint.
While in sub-section 2.3 an statistical measure to flflfill stage 2is presented, stages 1 and 3 are discussed in 2.4thoroughly.2.1 Ext rac t ing  Co-occur rence  Tr ip lesIn any process of learning from examples the accuracyof the training set is the base for the system to makecorrect predictions.
In our case, where the semanticclasses are hypothesized not univoquely from the ex~staples, accuracy becomes fundamental.Different approaches to obtain lexical co-occurren-ces have been proposed in the literature \[BPV92,CGHH91, CH90\].
These approaches eem inappro-priate for tackling our needs, either because they de-tect only local co-occurrences\[CGHtI9i, CtI90\], or be-cause they extract many spurious co-occurrence triples\[BPV92, Clt90\].
On the one hand, our system in-tends to learn SRs on any kind of verb's complements.On the other hand, the fact that these approaches ex-tract co-occurrences without reliability on being verb-complements violates accuracy requirements.However, if the co-occurrences were extracted froma corpus annotated with structural syntactic informa-tion (i.e., part of speech and "skeletal" trees), the re-sults would have considerably higher degrees of accu-770racy and representativity.
In this way, it would be easyto detect all tile relationships between verb and con>plements, and few non-related co-occurrences would beextracted.
'rile most serions objection to this approachis that the task of producing syntactic analyzed cor-pora is very expensive.
Nevertheless, lately there hasbeen a growing interest o produce skeletally analyzedcorpora 1A parser, with some simple heuristics, wonI(1 beenough to meet the requirements of representativenessand accuracy introduced above?
On the other band, itcould be useful to represent he co-occurrence triplesas holding between lemmas, in order to gather ~ muchevidence as possible.
A simple morphological nalyzerthat could get the lemma for a big percentage of tilewords appearing in the corpus would suffice.2 .2  Semant ic  Knowledge  UsedOf the two class-based approaches presented in section1, \[Res92\]'s technique uses a wide-coverage semantictaxonomy , whereas \[BPV92\] consists in hand-taggingwith a fixed set of semantic labels .
The advantagesand drawbacks of both approaches are diverse.
Onthe one hand, in \[BPV92\] approach, semantic lassearelevant o the domain are chosen, and consequently,the adjustment of the classes to the corpus is quite nice.Nevertheless, \[I'~es92\]'s sy tem is less constr~ined and isable to induce a most appropriate l vel for the SRs.
Onthe other hand, while \[BPV92\] implies hand-coding allthe relevant words with semantic tags, \[\[{.es92\] needs abroad semantic taxonomy.
IIowever, there is alreadyan available taxonomy, WordNet 2.
We take ires92\]approach because of the better results obtained, andthe lower cost involved.2 .3  C lass  appropr ia teness :  the  Assoc i -a t ion  ScoreWhen trying to choose a measure of the appropriate-.hess of a semantic lass, we have to consider the fea-tures of the problem: (1) robustness in front of noise,and (2) conservatism in order to be able to general-ize only front positive examples, without having thetendency to over-generalize.Several statistical measures that accomplish theserequirements have been proposed in the literature\[BPV92, CGIItI91, l{es92\].
We adopt \[Res92\]'s ap-proach, which qnantifies tile statistical associationXFor instance, Penn Treebank Corpus, which is belug col-lected and ana|yzed by the University of Penl~sylwmia (sec\[MSM93\]).
The material  is available on request, from the l,in-guistic Data Consort ium, (email) ldc@unagi.cis.upenn.ed, tt=WordNet is a lexieal datab.~e developed with psycholitt-guistlc aims.
it represents lexiea\[ scnlantics infl)t'Inatlon aboutnouns, verbs, adjectives and adverbs such as hypevonyms,meronyms, ... it presently contains information on about 83,000lemtn,'~q.
See \[MBF + 90\]between verbs and classes of nouns from their co-occurrenee.
IIowever we adapt it taking into accounttile syntactic position of the relationship.
Letv = {,,  .
.
.
.
.
~,,}, :?
- -  {,u .
.
.
.
.
,,,,,},a = {0,1,,0, ~t .
.
.
.
}, ~, .
t  c = {~1?
c N}be tim sets of all verbs, nouns, syntactic positions,and possible noun classes, respectively.
Given v E V,s E {'; and c G C, Association Score, Assoc, between vand e in a syntactic position s is defined to beA~.~oc(~, ., ~) =_ P(e/ , , ,  ~)~(~; ~/~)= 1"(cI,,, s) log= e(,,, c/s)Where conditional probabilities are estimated bycounting the number of observations of tile joint eventand dividing by the frequency of the given event, e.g~,,ec eo,.~t(., s, ,,)The two terms of Assoc try to capture different prop-erties of the SI{ expressed by the candidate class.
Mu-tual information, \[(v;c/s), measures the strength ofthe statistical association between the given verb vand the candidate class c in the given syntactic po-sition s. If there is a real relationship, then hopefullyl(v,c/s) >> 0.
On the other hand, the conditionalprobability, P(e/v,s), favors those classes that havemore occurrences of norms.2 .4  Se lec t ing  the  best  c lassesThe existence of noise in the training set introducesclasses in tile candidate space that can't be consideredas expressing SILs.
A common technique used for ignor-ing ,as far ms possible this noise is to consider only thoseevents that have a higher mnnber of occurrences thana certain threshold.
Ilowever, some erroneous classesmay persist because they exceed the threshold.
How-ever, if candidate classes were ordered by the signif-icance of their Assoc with the verb, it is likely thatless appropriate classes (introduced by noise) wouldbe ranked in the last positions of the candidate llst.
'\]'he algorithm to learn SIts is based in a searchthrough all the ckmses with more instances in the train-ing set than the given threshold.
In different iterationsover ~hese candidate classes, two operations are per-fol:med: first, the class, c, having the best Assoc (bestclass), is extracted for tile final result; and second, theremaining candidate classes are filtered from classes be-ing hyper/hyponyms to the best class.
This last step ismade becanse the definitive classes must be mutuallydisjoint.
The iterations are repeated until the candi-date space has been run otlt.771Table 1: SRs acquired for the subject of seekAcquired SR - ~ s o c  #n #s<cognition> Senses I -0.04< activity > Senses I ~0.01< status > Senses 0.087<social_control> Senses 0.111< administrative_district > Senses 0.14<city> Senses 0.15<radical> Senses 0.16< person, individual > Ok 0.23< legal_action > Ok 0.28< gro~p > ~}Abs.
0.35< suit > Senses 0.40< suit_of_clothes > Senses 0.41<suit,suing> Senses 0.415 16 15 06 036 036 05 061 387 664 467 07 07 0Examples of nouns in Treebankconcern, leadership, provision, scienceadministration, leadership, provisiongovernment, leadershipadministration, governmentproper_nameproper_namegroupadvocate, buyer, carrier, client, company, ...suitadministration, agency, bank, ..., group, ...suitsuitsuit\[Res92\] performed a similar learning process, butwhile he was only looking for the preferred class of ob-ject nouns, we are interested in all the possible closes(SRs).
He performed a best-first search on the can-didate space.
Itowever, if tile function to maximizedoesn't have a monotone behavior (as it is the c~e ofAssoc) the best-first search doesn't guarantee globaloptimals, but only local ones.
This fact made us todecide for a global search, specially because the candi-date space is not so big.3 EXPERIMENTAL  I{ESU LTSIn order to experiment the methodology presented, weimplemented a system in a Unix machine.
The cor-pus used for extracting co-occurrence triples is a frag-ment of parsed material from the Penn Treebank Cor-pus (about 880,000 words and 35,000 sentences), con-sisting of articles of the Wall Street Journal, that hasbeen tagged and parsed.
We used Wordnet ~ the verband noun lexicons for the lemmatizer, and also as thesemantic taxonomy for clustering nouns in semanticclasses.
In this section we evaluate the performance ofthe methodology implemented: (1) looking at the per-formance of the techniques used for extracting triples,(2) considering the coverage of the WordNet taxonomyregarding the noun senses appearing in Treebank, and(3) analyzing the performance of the learning process.Tile total number of co-occurrence triples extractedamounts to 190,766.
Many of these triples (68,800,36.1%) were discarded before tile lemmatizing pro~tess because the surface NP head wasn't a noun.The remaining 121,966 triples were processed throughthe lemmatizer.
113,583 (93.1%) could be correctlymapped into their corresponding lemma \[\)rm.in addition, we analyzed manually the results ob-tained for a subset of tile extracted triples, lookingat the sentences in the corpus where they occurred.The subset contains 2,658 examples of four averagecommon verbs in the Treebank: rise, report, seek andpresent (from now on, tile testing sample).
On the onehand, 235 (8.8%) of these triples were considered tobe extracted erroneously because of the parser, and 51(1.9%) because of the lemmatizer.
Summarizing, 2,372(89.2%) of the triples in the testing set were consideredto be correctly extracted and lemmatized.When analyzing the coverage of WordNet taxono-my a we considered two different ratios.
On the onehand, how many of the noun occurrences have one ormore senses included in the taxonomy: 113,583 of the117,215 extracted triples (96.9%).
On the other hand,how many of the noun occurrences in the testing sam-ple have the correct sense introduced in the taxonomy:2,615 of the 2372 well-extracted triples (8.7%).
Thesefigures give a positive evaluation of the coverage ofWordNet.In order to evaluate the performance of the learn-ing process we inspected manually the SRs acquiredon the testing-sample, a.ssessing if they correspondedto the actual Sits imposed.
A first way of evaluationis by means of meazuring precision and recall ratios inthe testing sample.
In our e~e, we define precision asthe proportion of triples appearing in syntactic posi-tions with acquired SRs, which effectively fififill one ofthose SRs.
Precision amounts to 79.2%.
The remain-ing 20.8% triples didn't belong to any of the classesinduced for their syntactic positions.
Some of thembecause they didn't have the correct sense included inthe WordNet taxonomy, and others because tile cor-rect class had not been induced because there wasn't3The informat ion of proper nouns in WordNet is poor.
Forthis reason we assign four predel\]ned classes to them:< person, individual >, < organizat ion :>,< adm{'t~iatrgtive_di.strict 2> etll(I <: city :>.772enough evidence.
On the other hand, we dellne re-call as the proportion of triples which fnlfill one of tileSRs acquired for their corresponding syntactic posi-tions.
Recall anrounts to 75.7%.A second way of evaluating the performance of t, heabstraction process is to manually diagnose the reasonsthat have made the system to deduce the SRs obtained.Table 1 shows the SILs corresponding to the subjectposition of the verb seek.
Type indicates the diagnosticabout the class appropriateness.
Assoc, the value ofthe association score.
"# n', tile number of nounsappearing in the corpus that are contained in the clmss.Finally, "~ s" "indicates the number of actual nounsenses used in the corpus which are coutained in theclass.
In this table we can see some examples of thefive types of manual diagnostic:Ok The acquired SR. is correct according to the nounsenses contained in the corpus.~Abs  The best level for stating the SI{ is not the oneinduced, but a lower one.
It happens because r-roneous senses, i r l e to l ly In ies ,  ...j accumulate vi-dence for the higher class.gAbs  Some of the Slks could be best gathered in aunique class.
We didn't lind any such case.Senses  The class has cropped up because it accu-mulates enough evidence, provide.d by erroneoussenses.No ise  The class accumulates enough evidence pro-vided by erroneously extracted triples.Table 2 shows the incidence of the diagnostic typesin the testing sample.
Each row shows: the type ofdiagnostic, the numher and l)ercerttage of classes thataccomplish it, and the nmnl)er and percentage of nounoccurrences contained by these classes in tile testingsample 4.
Aualyzing the results obtained from thetesting sample (some of which are shown in tables 1and 2) we draw some positive (a, e) and some negativeconclusions (b, c, d and /):a.
Almost one correct semantic lass tbr each syntac.tic position in the sample is acquired.
The tech-nique aehicves a good coverage, even with few co--occurrence triples.b.
Although many of the classes acquired result Doratile accumulation of incorrect senses (73.3%), it;seems that their size.
tends to be smaller thancl~usses in other categories, an they only containa 51,4% of tim senses .'
lthls total  doesn ' t  equal the numher  of tr iples in the test ingsample because tile same 1t(2,1|11 iilgty belong to IllOFQ thali  olll}class in the fin;d SI lsTable 2: Summary of the Sits acquiredDiagnostic # Clas~'es ~-- -~-- -~-n~--- -~--Ok 45l}Abs 7liAbs 0Senses 176Noise 12Term 2402.9 362 I 6.80.0 0 I 0.073.3 2,7401 51.45.0 130 I 2.4\[00.
0e.
There doesn't seem to be a clear co-relation betweenAsset and the manual diagnostic.
Specifically, theclasses considered to be correct sometimes aren'tranked in t;he higher positions of tile Asset (e.g.,Table l).(t.
Tim over-generalization seems to be produced be-cause of little difference in the nouns included inthe rival closes.
Nevertheless this situation israre.e.
The impact of noise provided by erroneous e?trac-tion of cc~-occurrence triples, iu the acquisition ofwrong semantic lasses, seems to be very moderoate.(.
Since diflhrcnt verb senses occur in the corpus, theSI{~ acquired appear mixed.4 \]?
'U f \ [TH EK  V~r 0 Pd(Although perfornmnce of thc technique presented ispretty good, some of the detected problems could posesibly be solved.
Specifically, there are various ways toexplore ill order to re.dace tile problems tated in pointsb and c above:1.2.To measure the Assoe by means of Mutual lntbr-marion between the pair v-s and c. In this way,tim syntactic position also wouhl provide iutbrma-lion (statistical evidence) for measuring the mostappropriate classes.To modify tim Asset in such a way that it wasbased in a likelihood ratio test \[Dun93\].
It seemsthat this kind of tests have a better performancethan nmtual inl'ormation when the counts aresma.ll, ~m it is the case.3.
To estimate the probabilities of classes, not di+rectly from the frequencies of their liouu men>bets, but correcting this evidence by the numberof senses of those nouns, e.g#~ert Jes ~,t ~C l'(~ls) ~ .
)-'"co co.~,uv,.
~, m~,.
+ -,-.v,J---"--'~773In this way, the estimated function would bea probability distribution, and more interesting,nouns would provide vidence on the occurrence oftheir hyperonyms, inversely proportional to theirdegree of ambiguity.4.
To collect a bigger number of examples for eachverbal complement, projecting the complementsin the internal arguments, using diathesis ub-categorization rules.
Hopefully, Assoc would havea better performance if it was estimated on a big-ger population.
On the other hand, in this wayit Would be possible to detect he SRs holding oninternal arguments.In order to solve point d above, we have foreseen twopossibilities:1.
To take into consideration the statistical signifi-cance of the alternatives involved, before doing ageneralization step, climbing upwards,2.
To use the PPs that in the corpus are attachedto other complements and not to the main verbas a source of "implicit negative xamples", insuch a way that they would constrain the over-generalization.Finally, It would be interesting to investigate he so-lution to point fl One possible way would be to disam-biguate the senses of the verbs appering in the corpus,using the SRs already acquired and gathering evidenceof the patterns corresponding toeach sense by meansof a technique similar to that used by \[Yar92\].
There-fore, once disambiguated the verb senses it would bepossible to split the set of SRs acquired.\[BPV92\]\[CGHH91\]\[cH9o\]\[Dun93\]ReferencesP~.
Basili,.M.T.
Pazienza, and P. Velardi.Computationallexicons: the neat examplesand the odd exemplars.
In Proc.
of lhe 3rdANLP, 1992.K.W.
Church, W. Gale, P. Hanks, andD.
Itindle.
Using statistics in lexicai anal-ysis.
In U. Zernik, editor, Lexicat Acqui-sition: Exploiting On-Line Resources toBuild a Lexicon.
Lawrence Erlbaum, 1991.K.W.
Church and P. Hanks.
Word as-sociation norms, mutual information andlexicography.
Computational Linguistics,16(1), 1990.T.
Dunning.
Accurate methods for thestatistics of surprise and coincidence.
Com-putational Linguistics, 19(1), 1993.\[MBF+90\]\[MSM93\]\[1%es92\]\[WFB90\]\[Yar92\]G. Miller, R. Beckwith, C. Fellbaum,D.
Gross, and K. Miller.
Five papers onwordnet.
Technical report, CSL, PrincetonUniversity, 1990.Mitchell P. Marcus, Beatrice Santorini,and Mary Ann Marcinkiewicz.
Buildinga large annotated corpus of english: thePenn Treebank.
Computational Linguis-tics, 19(2), 1993.P.
Resnik.
Wordnet and distributionalanalysis: A class-based approach to lexi-cal discovery.
In Proc.
of AAAI  Workshopon Statistical Methods in NLP, 1992.G.
Whittemore, K. Ferrara, and II.
Brun-net.
Empirical study of predictive pow-ers of simple attachment schemes for post-modifier prepositional phrases.
In Proc.
ofthe 28th ACL, 1990.David Yarowsky.
Word-sense disambigua-tion using statistical models of Roger's cat-egories trained on large corpora.
In Pro-ceedings of COL\[NG-92, Nantes, France,1992.774
