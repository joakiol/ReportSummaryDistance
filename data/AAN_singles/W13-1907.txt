Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 54?62,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsInterpreting Consumer Health Questions: The Role of Anaphora andEllipsisHalil Kilicoglu, Marcelo Fiszman, Dina Demner-FushmanLister Hill National Center for Biomedical CommunicationsNational Library of MedicineBethesda, MD, USA{kilicogluh, fiszmanm, ddemner}@mail.nih.govAbstractWhile interest in biomedical question answer-ing has been growing, research in consumerhealth question answering remains relativelysparse.
In this paper, we focus on the task ofconsumer health question understanding.
Wepresent a rule-based methodology that relieson lexical and syntactic information as well asanaphora/ellipsis resolution to construct struc-tured representations of questions (frames).Our results indicate the viability of our ap-proach and demonstrate the important roleplayed by anaphora and ellipsis in interpretingconsumer health questions.1 IntroductionQuestion understanding is a major challenge inautomatic question answering.
An array of ap-proaches has been developed for this task in thecourse of TREC Question Answering evaluations(see Prager (2006) for an overview).
These col-lectively developed approaches to question un-derstanding were successfully applied and ex-panded upon in IBM?s Watson system (Lally etal., 2012).
Currently, Watson is being retargetedtowards biomedical question answering, joiningthe ongoing research in domain-specific questionanswering (for a review, see Simpson andDemner-Fushman, 2012).Much research in automatic question answer-ing has focused on answering well-formed fac-toid questions.
However, real-life questions thatneed to be handled by such systems are oftenposed by lay people and are not necessarily well-formed or explicit.
This is particularly evident inquestions involving health issues.
Zhang (2010),focusing on health-related questions submitted toYahoo Answers, found that these questions pri-marily described diseases and symptoms (ac-companied by some demographic information),were fairly long, dense (incorporating more thanone question), and contained many abbreviationsand misspellings.
For example, consider the fol-lowing question posed by a consumer:(1) my question is this: I was born w/a esopha-gus atresia w/dextrocardia.
While the hearthasn't caused problems,the other has.
I getfood caught all the time.
My question is...isthere anything that can fix it cause I can't eatanything lately without getting it caught.
Ineed help or will starve!It is clear that the person asking this questionis mainly interested in learning about treatmentoptions for his/her disease, in particular with re-spect to his/her esophagus.
Most of the textualcontent is not particularly relevant in understand-ing the question (I need help or will starve!
or Iget food caught all the time).
In addition, notethe presence of anaphora (it referring to esopha-gus atresia) and ellipsis (the other has [caused prob-lems]), which should be resolved in order to auto-matically interpret the question.
Finally, note theinformal fix instead of the more formal treat, andcause instead of because.The National Library of Medicine?
(NLM?
)receives questions from consumers on a varietyof health-related topics.
These questions are cur-rently manually answered by customer supportservices.
The overall goal of our work is to assistthe customer support services by automaticallyinterpreting these questions, using informationretrieval techniques to find relevant documentsand passages, and presenting the information inconcise form for their assessment.In this paper, we specifically focus on ques-tion understanding, rather than information re-54trieval aspects of our ongoing work.
Our goal inquestion understanding is to capture the core as-pects of the question in a structured representa-tion (question frame), which can then be used toform a query for the search engine.
In the currentwork, we primarily investigate and evaluate therole of anaphora and ellipsis resolution in under-standing the questions.
Our results confirm theviability of rule-based question understandingbased on exploiting lexico-syntactic patterns andclearly demonstrate that anaphora and ellipsisresolution are beneficial for this task.2 BackgroundDespite the growing interest to biomedical ques-tion answering (Cairns et al 2012; Ni et al2012; Bauer and Berleant, 2012), consumerhealth question answering remains a fairly un-derstudied area of research.
The initial researchhas focused on the analysis of consumer lan-guage (McCray et al 1999) and the types ofquestions they asked.
Spink et al(2004) foundthat health-related queries submitted to three websearch engines in 2001 were often advice seekingand personalized, and fell into five major catego-ries: general health, weight issues, reproductivehealth and puberty, pregnancy/obstetrics, andhuman relationships.
Observing that health que-ries constituted no more than 9.5% of all queriesand declined over time, they concluded that theusers turn more to the specialized resources forthe answers to health-related questions.
Similarto the findings of Zhang (2010), Beloborodov etal.
(2013) found that diseases and symptomswere the most popular topics in a resource simi-lar to Yahoo Answers, Otvety@Mail.Ru.
Theyanalyzed Otvety@Mail.Ru questions by mappingquestions to body parts and organs, applying La-tent Dirichlet Allocation method with Gibbssampling to discover topics, and using aknowledge-based method to classify questions asevidence-directed or hypothesis-directed.First efforts in automated consumer healthquestion processing were to classify the ques-tions using machine learning techniques.
In onestudy, frequently asked questions about diabeteswere classified according to two somewhat or-thogonal taxonomies: according to the ?medicaltype of the question?
(Causes, Diagnostic, Pre-vention, Symptoms, Treatment, etc.)
and accord-ing to the ?expected answer type?
(Boolean,Causal, Definition, Factoid, Person, Place, etc.
)(Cruchet et al 2008).
Support Vector Machine(SVM) classification achieved an F-score in low80s in classifying English questions to the ex-pected answer type.
The results for French andmedical type classification in both languageswere much lower.
Liu et al(2011) found thatSVM trained to distinguish questions asked byconsumers from those posed by healthcare pro-fessionals achieve F-scores in the high 80s - low90s.
One of distinguishing characteristics of theconsumer questions in Liu et als study was thesignificantly higher use of personal pronouns(compared to professional questions).
This fea-ture was found to be useful for machine learning;however, the abundance of pronouns in the longdense questions is also a potential source of fail-ure in understanding the question.Vicedo and Ferr?ndez (2000) have shown thatpronominal anaphora resolution improves severalaspects of the QA systems?
performance.
Thisobservation was supported by Harabagiu et al(2005) who have manually resolved coreferenceand ellipsis for 14 of the 25 scenarios in theTREC 2005 evaluation.
Hickl et al(2006) haveincorporated into their question answering sys-tem a heuristic based question coreference mod-ule that resolved referring expressions in thequestion series to antecedents mentioned in pre-vious questions or in the target description.
Toour knowledge, coreference and ellipsis resolu-tion has not been previously attempted in con-sumer health question understanding.Another essential aspect in processing con-sumer questions is defining a formal representa-tion capable of capturing all important pointsneeded for further processing in automatic querygeneration (in the systems that use documentpassage retrieval to find a set of potential an-swers) and answer extraction and unification.Ontologies provide effective representationmechanisms for concepts, whereas relations arebetter captured in frame-like or event-relatedstructures (Hunter and Cohen, 2006).
Frame-based representation of extracted knowledge hasa long-standing tradition in the biomedical do-main, for example, in MedLEE (Friedman et al1994).
Demner-Fushman et al(2011) showedthat frame-based representation of clinical ques-tions improve identification of patients eligiblefor cohort inclusion.
Demner-Fushman and Ab-hyankar (2012) extracted frames in four steps: 1)identification of domain concepts, 2) extractionof patient demographics (e.g., age, gender) andsocial history, 3) establishing dependencies be-tween the concepts using the Stanford dependen-cy parser (de Marneffe et al 2006), and 4) add-ing concepts not involved in the relations to the55frame as a list of keywords.
Event-based repre-sentations have also seen increasing use in recentyears in biomedical text mining, with the availa-bility of biological event corpora, includingGENIA event (Kim et al 2008) and GREC(Thompson et al 2009), and shared task chal-lenges (Kim et al 2012).
Most state-of-the-artsystems address the event extraction task byadopting machine learning techniques, such asdual composition-based models (Riedel andMcCallum, 2011), stacking-based model integra-tion (McClosky et al 2012), and domain adapta-tion (Miwa et al 2012).
Good performance hasalso been reported with some rule-based systems(Kilicoglu and Bergler, 2012).
Syntactic depend-ency parsing has been a key component in allstate-of-the-art event extraction systems, as well.The role of coreference resolution in event ex-traction has recently been acknowledged (Kim etal., 2012), even though efforts in integrating co-reference resolution into event extraction pipe-lines have generally resulted in only modest im-provements (Yoshikawa et al 2011; Miwa etal., 2012; Kilicoglu and Bergler, 2012).Coreference resolution has also been tackledin open domain natural language processing.State-of-the-art systems often employ a combina-tion of lexical, syntactic, shallow semantic anddiscourse information (e.g., speaker identifica-tion) with deterministic rules (Lee et al 2011).Interestingly, coreference resolution is one re-search area, in which deterministic frameworksgenerally outperform machine learning models(Haghighi and Klein, 2009; Lee et al 2011).In contrast to coreference resolution, ellipsisresolution remains an understudied NLP prob-lem.
One type of ellipsis that received some at-tention is null instantiation (Fillmore and Baker,2001), whereby the goal is to recover the refer-ents for an uninstantiated semantic role of a tar-get predicate from the wider discourse context.
Asemantic evaluation challenge that focused onnull instantiation was proposed, although partici-pation was limited (Ruppenhofer et al 2010).Gerber and Chai (2012) focused on implicit ar-gumentation (i.e., null instantiation) for nominalpredicates.
They annotated a corpus of implicitarguments for a small number of nominal predi-cates and trained a discriminative model basedon syntactic, semantic and discourse featurescollected from various linguistic resources.
Fo-cusing on a different type of ellipsis, Bos andSpenader (2011) annotated a corpus of verbphrase ellipsis; however, so far there have beenlittle work in verb phrase ellipsis resolution.
Weare also not aware of any work in ellipsis resolu-tion in biomedical NLP.3 MethodsWe use a pipeline model for question analysis,which results in frame annotations that capturethe content of the question.
Our rule-based meth-od begins with identifying terms (named enti-ties/triggers) in question text.
Next, we recognizeanaphoric mentions and, if any, perform anapho-ra resolution.
The next step is to link frame trig-gers with their theme and question cue by ex-ploiting syntactic dependency relations.
Finally,if frames with implicit arguments exist (that is,frames in which theme or question cue was notinstantiated), we attempt to recover these argu-ments by ellipsis resolution.
In this section, wefirst describe our data selection.
Then, we ex-plain the steps in our pipeline, with particularemphasis on anaphora and ellipsis.
The pipelinediagram is illustrated in Figure 1.Figure 1.
The system pipeline diagram3.1 Data Selection and AnnotationIn this study, we focused on questions about ge-netic diseases, due to their increasing prevalence.Since the majority of the consumers?
questionssubmitted to NLM are about treatment and prog-nosis, we selected mainly these types of ques-tions for our training set.
Note that while thesequestions mostly focused on treatment and prog-nosis, some of them also include other types ofquestions, asking for general information orabout diagnosis, etiology, and susceptibility(thus, confirming the finding of Zhang (2010)).The majority of selected questions were asked byreal consumers in 2012.
Due to our interest ingenetics questions, we augmented this set with56some frequently asked questions from the Genet-ic and Rare Disease Information Center(GARD) 1 .
Our selection yielded 32 treatmentand 22 prognosis questions.
An example treat-ment question was provided earlier (1).
The fol-lowing is a training question on prognosis:(2) They have diagnosed my niece with Salladisease.
I understand that this is a very raredisease and that its main origin is Finland.Can you please let me know what to expect?My niece is 7 years old.
It has taken them 6years to finally come up with this diagnosis.We used training questions to gain linguisticinsights into the problem, to develop and refineour methodology, and as the basis of a trig-ger/question cue dictionary.After the system was developed, we selected29 previously unseen treatment-focused ques-tions posed to GARD for testing.
We annotatedthem with target frames (41 instances) using bratannotation tool (Stenetorp et al 2012) and eval-uated our system results against these frames.
29of the target frames were treatment frames.
Addi-tionally, there were 1 etiology, 6 general infor-mation, 2 diagnosis, and 3 prognosis frames.3.2 Syntactic Dependency ParsingOur question analysis module uses typed de-pendency relations as the basis of syntactic in-formation.
We extract syntactic dependenciesusing Stanford Parser (de Marneffe et al 2006)and use its collapsed dependency format.
Werely on Stanford Parser for tokenization, lemma-tization, and part-of-speech tagging, as well.3.3 Named Entity/Trigger DetectionWe use simple dictionary lookup to map entitymentions in text to UMLS Metathesaurus con-cepts (Lindberg, 1993).
So far, we have focusedon recognizing three mention categories: prob-lems, interventions, and patients.
Based onUMLS 2007AC release, we constructed a dic-tionary of string/concept pairs.
We limited thedictionary to concepts with predefined semantictypes.
For example, all problems in the diction-ary have a semantic type that belongs to the Dis-orders semantic group (McCray et al 2001),such as Neoplastic Process and Congenital Ab-normality.
Currently our dictionary contains ap-proximately 260K string/concept pairs.Dictionary lookup is also used to detect trig-gers and question cues.
We constructed a trigger1 https://rarediseases.info.nih.gov/GARD/and question cue dictionary based on trainingdata and limited expansion.
The dictionary cur-rently contains 117 triggers and 14 question cues.3.4 Recognizing Anaphoric MentionsWe focus on identifying two types of anaphoricphenomena: pronominal anaphora (includinganaphora of personal and demonstrative pro-nouns) and sortal anaphora.
The following ex-amples from the training questions illustratethese types.
Anaphoric mentions are underlinedand their antecedents are in bold.?
Personal pronominal anaphora: My daughterhas just been diagnosed with Meier-Gorlinsyndrome.
I would like to learn more aboutit ??
Demonstrative pronominal anaphora: We justfound out that our grandson has 48,XXYYsyndrome.
?
I was wondering if you couldgive us some information on what to expectand the prognosis for this and ..?
Sortal anaphora: I have a 24-month-old niecewho has the following symptoms of Cohensyndrome: ?
I would like seek your help inlearning more about this condition.To recognize mentions of personal pronominaland sortal anaphora, we mainly adapted the rule-based techniques outlined in Kilicoglu and Ber-gler (2012), itself based on the deterministic co-reference resolution approach described inHaghighi and Klein (2009).
While Kilicoglu andBergler (2012) focused on anaphora involvinggene/protein terms, our adaptation focuses onthose involving problems and patients.
In addi-tion, we expanded their work by developing rulesto recognize demonstrative pronominal anapho-ra.3.4.1 Personal PronounsKilicoglu and Bergler (2012) focused on onlyresolving it and they, since, in scientific articlegenre, resolving other third person pronouns (he,she) was less relevant.
We currently recognizethese two pronouns, as well.
For personal pro-nouns, we merely tag the word as a pronominalanaphor if it is tagged as a pronoun and is inthird person (i.e., she, he, it, they).3.4.2 Demonstrative PronounsWe rely on typed syntactic dependencies as wellas part-of-speech tags to recognize demonstrativepronominal anaphora.
A word is tagged asdemonstrative pronominal anaphor if it is one ofthis, that, those, or these and if it is not the de-57pendent in a det (determiner) dependency (inother words, it is not a pronominal modifier).Furthermore, we ensure that the pronoun thatdoes not act as a complementizer, requiring thatit not be the dependent in a complm (complemen-tizer) dependency.3.4.3 Sortal AnaphoraIn the current work, we limited sortal anaphorato problem terms.
As in Kilicoglu and Bergler(2012), we require that the anaphoric nounphrases not include any named entity terms.Thus, we allow the syndrome as an anaphoricmention, while blocking the Stickler syndrome.To recognize sortal anaphora, we look for thepresence of det dependency, where the depend-ent is one of this, that, these, those, or the.Once the named entities, question cues, trig-gers, and anaphoric mentions are identified in asentence, we collapse the syntactic dependenciesfrom the sentence to simplify further processing.This is illustrated in Table 1 for the sentence in(3).
(3) My partner is a carrier for Simpson-Golabi-Behmel syndrome and her son was diag-nosed with this rare condition.Dependencies before Dependencies afteramod (syndrome, simpson-golabi-behmel)prep_for(carrier, simpson-golabi-behmel syndrome)prep_for(carrier,syndrome)det(condition,this)prep_with (diagnosed, this rare condition) amod(condition, rare)prep_with(diagnosed, condition)Table 1: Syntactic dependency transformations3.5 Anaphora  ResolutionAnaphora resolution is the task of finding theantecedent for an anaphoric mention in prior dis-course.
Our anaphora resolution method is againbased on the work of Kilicoglu and Bergler(2012).
However, we made simplifying assump-tions based on our examination of the trainingquestions.
First observation is that each questionis mainly about one salient topic (problem) andanaphoric mentions are highly likely to refer tothis topic.
Secondly, the salient topic often ap-pears as the first named entity in the question.Based on these observations, we did not attemptto use the relatively complex, semantic graph-based resolution strategies (e.g., graph distance)outlined in that work.
Furthermore, we have notattempted to address set-instance anaphora orevent anaphora in this work, since we did not seeexamples of these in the training data.Anaphora resolution begins with identifyingthe candidate antecedents (problems, patients) inprior discourse, which are then evaluated for syn-tactic and semantic compatibility.
For pronomi-nal anaphora, compatibility involves person andnumber agreement between the anaphoric men-tion and the antecedent.
For sortal anaphora,number agreement as well as satisfying one ofthe following constraints is required:?
Head word constraint: The head of the ana-phoric NP and the antecedent NP match.This constraint allows Wolf-Hirschhorn Syn-drome as an antecedent for this syndrome,matching on the word syndrome.?
Hypernymy constraint: The head of the ana-phoric NP is a problem hypernym and theantecedent is a problem term.
Similar togene/protein hypernym list in Kilicoglu andBergler (2012), we used a small list of prob-lem hypernym words, including disease, dis-order, illness, syndrome, condition, andproblem.
This constraint allows Simpson-Golabi-Behmel syndrome as an antecedentfor this rare condition in example (3).We expanded number agreement test to in-clude singular mass nouns, so that plural anapho-ra (e.g., they) can refer to mass nouns such asfamily, group, population.
In addition, we de-fined lists of gendered nouns (e.g., son, father,nephew, etc.
for male and wife, daughter, niece,etc.
for female) and required gender agreementfor pronominal anaphora.After the candidate antecedents are identified,we assign them salience scores based on the or-der in which they appear in the question and theirfrequency in the question.
The terms that appearearlier in the question and occur more frequentlyreceive higher scores.
The most salient anteced-ent is then taken to be the coreferent.3.6 Frame ConstructionWe adapted the frame extraction process basedon lexico-syntactic information outlined inDemner-Fushman et al(2012) and somewhat58modified the frames to accommodate consumerhealth questions.
For each question posed, weaim to construct a frame which consists of thefollowing elements: type, theme, and questioncue: theme refers to the topic of the question(problem name, etc.
), while type refers to theaspect of the theme that the question is about(treatment, prognosis, etc.)
and question cue tothe question words (what, how, are there, etc.
).Theme element is semantically typed and is re-stricted to the UMLS semantic group Disorders.From the question in (1), the following frameshould be extracted:Treatment fixTheme Esophageal atresia(Disease or Syndrome)QCue Is thereTable 2: Frame exampleWe rely on syntactic dependencies to link frameindicators to their themes and question cues.
Wecurrently search for the following types of syn-tactic dependencies between the indicator men-tion and the argument mentions: dobj (direct ob-ject), nsubjpass (passive nominal subject), nn(noun compound modifier), rcmod (relativeclause modifier), xcomp (open clausal comple-ment), acomp (adjectival complement), prep_of,prep_to, prep_for, prep_on, prep_from,prep_with, prep_regarding, prep_about (prepo-sitional modifier cued by of, to, for, on, from,with, regarding, about, respectively).
Two spe-cial rules address the following cases:?
If the dependency exists between a trigger oftype T and another of type General Infor-mation, the General Information trigger be-comes a question cue for the frame type T.This handles cases such as ?Is there infor-mation regarding prognosis..?
where there isa prep_regarding dependency between theGeneral Information trigger ?information?and the Prognosis trigger ?prognosis?.
Thisresults in ?information?
becoming the ques-tion cue for the Prognosis frame.?
If a dependency exists between a trigger Tand a patient term P and another between thepatient term P and a potential theme argu-ment A, the potential theme argument A isassigned as the theme of the frame indicatedby T. This handles cases such as ?What is thelife expectancy for a child with Dravet syn-drome??
whereby Dravet syndrome is as-signed the Theme role for the Prognosisframe indicated by life expectancy.3.6.1 Ellipsis ResolutionThe frame construction step may result in frameswith uninstantiated themes or question cues.
If aconstructed frame includes a question cue but notheme, we attempt to recover the theme argumentfrom prior discourse by ellipsis processing.
Con-sider the question in (4) and the frame in Table 3extracted from it in previous steps:(4) They have diagnosed my niece with Salladisease.
?Can you please let me know whatto expect?
?Prognosis expectTheme -QCue whatTable 3: Frame with uninstantiated Theme roleIn the context of consumer health questions,the main difficulty with resolving such cases isrecognizing whether it is indeed a legitimate caseof ellipsis.
We use the following dependency-based heuristics to determine the presence of el-lipsis:?
Check for the presence of a syntactic de-pendency of one of the types listed in Sec-tion 3.5, in which the frame trigger appearsas an element.
If such a dependency does notexist, consider it a case of ellipsis.?
Otherwise, consider the other element of thedependency:o If the other element does not corre-spond to a term, we cannot make adecision regarding ellipsis, since wedo not know the semantics of thisother element.o If it corresponds to an element thathas already been used in creating theframe, the dependency is accountedfor.?
If all the dependencies involving the frametrigger are accounted for, consider it a caseof ellipsis.In example (4), the trigger expect is found tobe in an xcomp dependency with the question cueknow, which has already been used in the frame.Therefore this dependency is accounted for, andwe consider this a case of ellipsis.
On the otherhand, consider the example:(5) My child has been diagnosed with pachgyria.What can I expect for my child?s future?As in the previous example, the Theme role ofthe Prognosis frame indicated by expect is unin-stantiated.
However, it is not considered an ellip-59tical case, since there is a prep_for dependencybetween expect and future, a word that is seman-tically unresolved.Once the presence of ellipsis is ensured, wefill the Theme role of the frame with the mostsalient term in the question text, as in anaphoraresolution.In rare cases, the frame may include a themebut not a question cue.
This may be due to a lackof explicit question expression (such as in thequestion ?treatment for Von Hippel-Lindau syn-drome.?)
or due to shortcomings in dependency-based linking of frame triggers to question cues.If no fully instantiated frame was extracted fromthe question, as a last resort, we construct aframe without the question cue in an effort toincrease recall.4 Results and DiscussionWe extracted frames from the test questions andcompared the results with the annotated targetframes.
As evaluation metrics, we calculatedprecision, recall, and F-score.
To assess the ef-fect of various components of the system, weevaluated several scenarios:?
Frame extraction without anaphora/ellipsisresolution (indicated as A in Table 4 below)?
Frame extraction with anaphora/ellipsis reso-lution (B)?
Frame extraction without anaphora/ellipsisresolution but with gold triggers/named enti-ties (C)?
Frame extraction with anaphora/ellipsis reso-lution and gold triggers/named entities (D)The evaluation results are provided in Table 4.
Inthe second column, the numbers in parenthesescorrespond to the numbers of correctly identifiedframes.# of frames Recall Precision F-scoreA 14 (13) 0.32 0.93 0.48B 26 (22) 0.54 0.85 0.66C 17 (16) 0.39 0.84 0.55D 35 (33) 0.80 0.94 0.86Table 4: Evaluation resultsThe evaluation results show that the depend-ency-based frame extraction method with dic-tionary lookup is generally effective; it is precisein identifying frames, even though it missesmany relevant frames, typical of most rule-basedsystems.
On the other hand, anaphora/ellipsisresolution helps a great deal in recovering therelevant frames and only has a minor negativeeffect on precision of the frames, the overall ef-fect being significantly positive.
Note also thatthe increase in recall without gold triggers/namedentities is about 40%, while that with gold trig-gers/named entities is more than double, indicat-ing that accurate term recognition contributes tobetter anaphora/ellipsis resolution and, in turn, tobetter question understanding.The dictionary-based named entity/trigger/question cue detection is relatively simple, andwhile it yields good precision, the lack of termsin the corresponding dictionary causes recall er-rors.
An example is given in (6).
The named enti-ty Reed syndrome was not recognized due to itsabsence in the dictionary, causing two falsenegative errors.
(6) A friend of mine was just told she has Reedsyndrome?
I was wondering if you could letme know where I can find more informationon this topic.
I am wondering what treat-ments there are for this, ?Similarly, dependency-based frame construc-tion is straightforward in that it mostly requiresdirect dependency relations between the triggerand the arguments.
While the two additionalrules we implemented redress the shortcomingsof this straightforward approach, there are casesin which dependency-based mechanism is stilllacking.
An example is given in (7).
The lack ofa direct dependency between treatments and thiscondition causes a recall error.
A more sophisti-cated mechanism based on dependency chainscould recover such frames; however, such chainswould also increase the likelihood of precisionerrors.
(7) Are people with Lebers hereditary optic neu-ropathy partially blind for a long period oftime ?.
?Are there any surgical treatmentsavailable to alter this condition or is it per-manent for life?Anaphora/ellipsis processing clearly benefitedour question understanding system.
However, wenoted several errors due to shortcomings in thisprocessing.
For example, from the sentence in(8), the system constructed a General Infor-mation frame with the trigger wonder and theTheme argument central core disease, whichcaused a false positive error.
(8) After 34 years of living with central core dis-ease, ?.
My lower back doesn't seem towork, and I wonder if I will ever be able towalk up stairs or run.60The system recognized that the trigger wonderhad an uninstantiated theme argument, which itattempted to recover by ellipsis processing.However, this processing misidentified the caseas legitimate ellipsis due to the dependency rela-tions wonder is involved in.
A more sophisticat-ed approach would take into account specificselectional restrictions of predicates like wonder;however, the overall utility of such linguisticknowledge in the context of consumer healthquestions, which are often ungrammatical andnot particularly well-written, remains uncertain.Our anaphora resolution method was unable toresolve some cases of anaphora.
For example,consider the question in (6).
The anaphoric men-tion this topic corefers with Reed syndrome.However, we miss this anaphora since we did notconsider topic as a problem hypernym in scenar-io D, in which gold named entities are used.5 Conclusions and Future WorkWe presented a rule-based approach to consumerhealth question understanding which relies onlexico-syntactic information and anapho-ra/ellipsis resolution.
We showed that lexico-syntactic information provides a good baseline inunderstanding such questions and that resolvinganaphora and ellipsis has a significant impact onthis task.With regard to question understanding, futurework includes generalization of the system toquestions on topics other than genetic disorders(e.g., drugs) and aspects (such as complications,prevention, ingredients, location information,etc.)
and broader evaluation.
We also plan to au-tomate dictionary development to some extentand address misspellings and acronyms in ques-tions.
We have been extending our frames to in-clude ancillary keywords (named entities ex-tracted from the question) that are expected toassist the search engine in pinpointing the rele-vant answer passages, similar to Demner-Fushman and Abhyankar (2012).
We will alsocontinue to develop our anaphora/ellipsis pro-cessing module, addressing the issues revealedby our evaluation as well as other anaphoric phe-nomena, such as recognition of pleonastic it.AcknowledgmentsThis research was supported by the IntramuralResearch Program of the NIH, National Libraryof Medicine.ReferencesMichael A. Bauer and Daniel Berleant.
2012.
Usabil-ity survey of biomedical question answering sys-tems.
Human Genomics, 6(1):17.Alexander Beloborodov, Artem Kuznetsov, PavelBraslavski.
2013.
Characterizing Health-RelatedCommunity Question Answering.
In Advances inInformation Retrieval, 680-683.Brian L. Cairns, Rodney D. Nielsen, James J. Masanz,James H. Martin, Martha S. Palmer, Wayne H.Ward, Guergana K. Savova.
2011.
The MiPACQclinical question answering system.
In AMIA An-nual Symposium Proceedings, pages 171-180.Sarah Cruchet, Arnaud Gaudinat, C?lia Boyer.
2008.Supervised approach to recognize question type ina QA system for health.
Studies in Health Technol-ogy and Informatics, 136:407-412.Marie-Catherine de Marneffe, Bill MacCartney,Christopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation, pages 449-454.Dina Demner-Fushman, Swapna Abhyankar, AntonioJimeno-Yepes, Russell F. Loane, Bastien Rance,Fran?ois-Michel Lang, Nicholas C. Ide, EmiliaApostolova, Alan R. Aronson.
2011.
AKnowledge-Based Approach to Medical RecordsRetrieval.
In Proceedings of Text Retrieval Confer-ence 2011.Dina Demner-Fushman and Swapna Abhyankar.2012.
Syntactic-Semantic Frames for Clinical Co-hort Identification Queries.
Lecture Notes in Com-puter Science, 7348:100-112.Charles J. Fillmore and Collin F. Baker.
2001.
Framesemantics for text understanding.
In Proceedings ofthe NAACL?01 Workshop on WordNet and OtherLexical Resources.Carol Friedman, Philip O. Alderson, John HM Austin,James J. Cimino, and Stephen B. Johnson.
1994.
Ageneral natural-language text processor for clinicalradiology.
Journal of the American Medical Infor-matics Association, 1(2): 161-174.Matthew S. Gerber and Joyce Y. Chai.
2012.
Seman-tic Role Labeling of Implicit Arguments for Nomi-nal Predicates.
Computational Linguistics, 38(4):755-798.Aria Haghighi and Dan Klein.
2009.
Simple corefer-ence resolution with rich syntactic and semanticfeatures.
In Proceedings of EMNLP 2009, pages1152-1161.Sanda Harabagiu, Dan Moldovan, Christine Clark,Mitchell Bowden, Andrew Hickl, Patrick Wang.2005.
Employing two question answering systemsin TREC-2005.
In Proceedings of Text RetrievalConference  2005.61Andrew Hickl, John Williams, Jeremy Bensley, KirkRoberts, Ying Shi, Bryan Rink.
2006.
QuestionAnswering with LCC?s CHAUCER at TREC 2006.In Proceedings of Text Retrieval Conference  2006.Lawrence Hunter and Kevin B. Cohen.
2006.
Bio-medical language processing: what's beyond Pub-Med?
Molecular Cell.
21(5):589-94.Halil Kilicoglu and Sabine Bergler 2012.
BiologicalEvent Composition.
BMC Bioinformatics, 13(Supplement 11):S7.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.2008.
Corpus annotation for mining biomedicalevents from literature.
BMC Bioinformatics, 9:10.Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichiTsujii, Toshihisa Takagi, Akinori Yonezawa.
2012.The Genia Event and Protein Coreference tasks ofthe BioNLP Shared Task 2011.
BMC Bioinformat-ics, 13(Supplement 11):S1.Adam Lally, John M. Prager, Michael C. McCord,Branimir Boguraev, Siddharth Patwardhan, JamesFan, Paul Fodor, Jennifer Chu-Carroll.
2012.
Ques-tion analysis: How Watson reads a clue.
IBM Jour-nal of Research and Development, 56(3):2.Heeyoung Lee, Yves Peirsman, Angel Chang, Na-thanael Chambers, Mihai Surdeanu, Dan Jurafsky.2011.
Stanford's Multi-Pass Sieve CoreferenceResolution System at the CoNLL-2011 SharedTask.
In Proceedings of the CoNLL-2011 SharedTask, pages 28-34.Donald A.B.
Lindberg, Betsy L. Humphreys, Alexa T.McCray.
1993.
The Unified Medical LanguageSystem.
Methods of information in medicine,32(4): 281-291.Feifan Liu, Lamont D. Antieau, Hong Yu.
2011.
To-ward automated consumer question answering: au-tomatically separating consumer questions fromprofessional questions in the healthcare domain.Journal of Biomedical Informatics, 44(6): 1032-1038.David McClosky, Sebastian Riedel, Mihai Surdeanu,Andrew McCallum, Christopher Manning.
2012.Combining joint models for biomedical event ex-traction.
BMC Bioinformatics, 13 (Supplement 11):S9.Alexa McCray, Russell Loane, Allen Browne, Anan-tha Bangalore.
1999.
Terminology issues in useraccess to Web-based medical information.
InAMIA Annual Symposium Proceedings, pages 107?111.Alexa McCray, Anita Burgun, Olivier Bodenreider.2001.
Aggregating UMLS semantic types for re-ducing conceptual complexity.
In Proceedings ofMedinfo, 10(Pt1): 216-220.Makoto Miwa, Paul Thompson, Sophia Ananiadou.2012.
Boosting automatic event extraction from theliterature using domain adaptation and coreferenceresolution.
Bioinformatics, 28(13):1759-1765.Yuan Ni, Huijia Zhu, Peng Cai, Lei Zhang, ZhaomingQui, Feng Cao.
2012.
CliniQA: highly reliableclinical question answering system.
Studies inHealth Technology and Information, 180:215-219.John M. Prager.
2006.
Open-domain question answer-ing.
Foundations and Trends in Information Re-trieval, 1(2):91-231.Sebastian Riedel and Andrew McCallum.
2011.
Fastand robust joint models for biomedical event ex-traction.
In Proceedings of EMNLP 2011, pages 1-12.Josef Ruppenhofer, Caroline Sporleder, Roser Mo-rante, Collin Baker, Martha Palmer.
2010.SemEval-2010 Task 10: Linking Events and TheirParticipants in Discourse.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation,pages 45-50.Matthew S. Simpson and Dina Demner-Fushman.2012.
Biomedical Text Mining: a Survey of RecentProgress.
Mining Text Data 2012:465-517.Amanda Spink, Yin Yang, Jim Jansen, PirrkoNykanen, Daniel P. Lorence, Seda Ozmutlu, H.Cenk Ozmutlu.
2004.
A study of medical andhealth queries to web search engines.
Health In-formation & Libraries Journal.
21(1):44-51.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?,Tomoko Ohta, Sophia Ananiadou, Jun?ichi Tsujii.2012.
Brat: a Web-based Tool for NLP-AssistedText Annotation.
In Proceedings of the Demon-strations Sessions at EACL 2012, pages 102-107.Paul Thompson, Syed A. Iqbal, John McNaught, So-phia Ananiadou.
2009.
Construction of an annotat-ed corpus to support biomedical information ex-traction.
BMC Bioinformatics, 10:349.Jos?
L. Vicedo and Antonio Ferr?ndez.
2000.
Im-portance of pronominal anaphora resolution inquestion answering systems.
In Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, pages 555-562.Katsumasa Yoshikawa, Sebastian Riedel, TsutomuHirao, Masayuki Asahara, Yuji Matsumoto.
2011.Coreference Based Event-Argument Relation Ex-traction on Biomedical Text.
Journal of Biomedi-cal Semantics, 2 (Supplement 5):S6.Yan Zhang.
2010.
Contextualizing Consumer HealthInformation Searching: an Analysis of Questions ina Social Q&A Community.
In Proceedings of the1st ACM International Health Informatics Sympo-sium (IHI?10), pages 210-219.62
