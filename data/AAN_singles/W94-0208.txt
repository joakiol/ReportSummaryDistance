SEGMENTING SPEECH WITHOUT A LEX ICON:THE ROLES OF PHONOTACTICS  AND SPEECH SOURCETimothy Andrew Cartwright and Michael R. BrentDepar tment  of Cognit ive ScienceThe  Johns Hopkins ( \]nivcrsity3400 North  Charles StreetBa.ltinlore, MI) 21218, l ISAInl,crncl,: cat?laa:i.1, cog .
jhu .
eduAbstractInfants face the diMcult problem of segmentingcontinnous peech into words without the bene-lit of a fully dew,loped lexicon.
Several sourcesof information in speech might help infants solve(.his probhml, including prosody, semantic orre-lations an(I I)honotacl, ics.
lt,(~search to date hasfilciiscd i)ll ~h,l,(,rluining to which (if these SOllrCcshlfanl.s lu ight bc sciisil,ivc, bnt l i t t le  work lli~s beendone I.o deternihie l,he potent ial  usefulness of eachSOllrce.
'l'll~ (~ouipnteF siinulations reported herearc it Ih'st a t tcn i l i t  to iiica.,lllre I.he IlSefllhless ofI istr i l iut iol l i l ,  I am l iihouol,acl, ic i l i rornlat ion in seg-lil~qil,ilig l iholiCll l l  , si,qllCli('(,s. The algorithl l lS hy--in~l,licsiz~ ' ~lill'~'r('nl.
scgiuelital.hiliS ill" the hi ln i t  illl,ow~io'ds il.lld .~ch,el, I.\[ie best Ilyliol.l icsis ac(:ol'diilg tot,lil, m i l l i l l l i l l l l  I)escripl,ion I,eugl,h l ir inciplc.
Oul"i'csull.s hldical,c t l iat  while I.here is SOlii~, usefulhiforuial.
iol i  ill both phoneme distr ibut ions aridlihtmol,acl.ic ruh's, i,he coul l) ination of bol,h SOllrcesis inost us(.J'lll.INTRODUCTIONI l ifants lUllSt Icarni I,o recognize ccrtain sound se-qllellCl'.s ;IS I)(~illg words; this is a dillicult i)rob-Icn~ I)ecausc norll ial speech contains no obviousacoustic divisions between words.
Two sourcesof h i fornlat ion that l i i ighl, aid Sl)coch segnierlta-l,ion arc: disl.ribull ion I,h?
I)holienic s(;qilCli~;e in<'.l alillC:u's frcqli(~jil,ly in scw'ral contcxl,s includ-i l lg Ihc~'al, cats all i l  ('fl/ll(li?J, wlicl'(~a~ I.li(~ s(~lillCll(~ein i'(ilu is rti.l'l, i l l l l l l alillCal's ill rl,stricl,ed Colll.l'xts;: i l i l l  llli(ui~ll,:l,cl.i~'s cal is rill acl'(qil,al)h~ syllabl(~ inI'hilxJish, wh<,l'Clis p<'lll is not.
Whi le evidcnc(' (~x--isl,s I.li~d.
infanl.s a, rc scnsitiv(' to I.hcsc il i\['ornlal;iolis~ili'l:(.s, wl, kll(~w of iio Illea.SllrelllelltS of I, he i r  IIS?;-I'uhi~'ss.
In this paper, we a t tempt  to quant i fy  theils~flllli~ss OF distr ibution and phonotactics in seg-li l(, l iti l lg spe(,ell.
W(' found thai, each source pro-vi(Icd Solue IlSCfllJ information for speech seginen-I,atioli, bi l l  I, li(' colirbii iation of sources providedsubsl,anl,ial hiforl i iation.
Wc also fonnd that child-dir~cl,('d Slmech was Uulch ea.~icr to soglnenl, thanadult-directed speech when using both sources.
'Fo date, psychologists have focused on two as-pects of the speech segmentation problem.
Thefirst is the problem of parsing continuous peechinto words given a developed lexicon to which in-coming sounds can be matched; both psycholo-gists (e.g., Cutler & Carter, 1987; Cutler & But-terliel(I, 1992) and designers of speech-recognitionsystems (e.g., (\]hur(:h, 1987) have examined I~hisproblem.
However, the problem we examinedis dilferent---we want to know how infants seg-ment speech before knowing which phonemic se-qllelW,('s form words.
'1'he second aspect psycholo-gists liaw~ focnsed (ill is the lirobleln of dcternihi-ilig the ill\['Orluatioll SOllr(:(~s t() which il ifants areSCllSil,ive.
Pri luarily, I.wo sotircos haw~ ll(~ell (~x--ainine~l: prosody and word stress.
II,enults sug-gest I.hal, parents (~xaggcrate prosody in child-directed speech to highlight iniportant words (Fer-nahl & Mazzie, 1991; Aslin, Woodward, LaMen-dola & Bever, in press) and that infants are sen-sitive to prosody (e.g., Hirsh-Pasek et al, 1987).Word stress in English fairly accurately predictsthe location of word beginnings (Cutler & Norris,1988; Cutler & Butterfield, 1992); Jusczyk, Cutlerand II,edanz (1993) demonstrated that 9-month-ohls (but not 6-month-olds) are sensitive to thecommon strong/weak word stress pattern in En-glish.
Sensitivity to native-language phonotacticsin 9-month-olds was re(:ently reported by Jusczyk,I,'riedcrici, Wessels, Swmkerud and Jusczyk (1993).
'i'lles~ sl, udi(~s deruoilstratcd infants' perceptiveabilil.il's wil,hout deiilonsl.ral,hig t lw usefuhicss ofhli'alil,s > ll(~rcel)l,ioils.I low do childl'(,n coiubine l,li(: iiiforiii;d,ion I,heyi)crc~,iw; froln dilrerenl, SOlll'l;es'.
?
Aslil i  el, al.
Sl)(~c-Illate that infants first learn words heard in isola-tion, then use distribution and prosody to refineand expand their w)cabulary; however, Jusczyk(1(,)93) sliggests that sound sequences learned inisolation dill~r too greatly from those in contexi.to bc useful.
He goes on to say, "just how far in-forniation in the sound structure of the input can83pies, we see that Hypothesis 1 uses 48 charactersand Hypothesis 2 uses 75.
However, this simplis-tic method is inefficient; for instance, the length oflexical indices are arbitrary with respect o prop-erties of the words themselves (e.g., in Hypothesis2, there is no reason why/ ju l /was  assigned tile in-dex '10'-- length two--instead of '9'-- length one).Our system improves upon this simple size metri(:I)y coml)uting sizes based on ;t ('Onll)act rel)rcs(,n-tat.ion motivated I)y informati(m theory.W(: inmginc hypothes(:s r(qu'(~sented ;~a stringof ones and zeros.
This binary string must r(,p-resent not only the lexical entries, their indices(called code words) and the coded sample, butalso overhead information specifying the numberof items coded and their arrangement in the string(information implicitly given by spacing and sl)a-tial placement in the introductory cxamples).
Fur-therrnore, the string and its components must beself-delimiting, so that a decoder could identify theendpoints of components by itself.
The next sec-tion describes the binary representation and thelength formulm derived from it in detail; readerssatisfied with the intuitive descriptions presentedso far should skip ahead to the Phonotactics sub-section.Representat ion  and  Length  FormulaeThe representation scheme described below ixI);~scd on information theory (for more examplesof coding systems, see, e.g., Li L: VitKnyi, 1993and Quinlan & Rivest, 1989).
From this repre-sentation, we can derive a formula describing itslength in bits.
However, the discrete form of theformula would not work well in practice for oursimulations.
Instead, we use a continuous approx-imation of the discrete formula; this approxima-tion typically involves dropping the ceiling func-tion from length computations.
For example, wesometimes use a self-delimiting representation forintegers (as described in Li & VitS.nyi, pp.
74-75).In this representation, the number of bits neededto code an integer x is given bye(~)(=) = i+ \[log~(= + I)1 +2 \[log~ \[logs(= + I)11lIowever, we use the following approximation:e (~) (x) = 1.5+log2(x +1) +2 log2(log 2(x +2) +0.5)Using the discrete formula, the dilference I)etwc(,ng(21(126) and g(2)(127) is zero, while the differ-ence between e(~)(127) and g(21(128) is one bit; us-ing the continuous formula, the difference between~(~)(126) and g(2)(127) is 0.0156, while the differ-(m(:e I)ct.wecn g~)( 1271 and g(2)(128) is 0.0155.
Wef(mn(I it easier to inl.m'l)ret im results using a con-t.imu)us fun(:ti(m, s,) in t,lw J'~dh)witlg(liscussion, w('will only i)r('s(.ut.
I h(.
a.i)l)roxim;d.(~ fi rmuh,~.
'rite lexicon lists words (represented asphoneme sequences) paired With I,Imir codewords 1 .
For (,xample:Word Code WordOa \[the\]k~;t \[cat\]klti \[kitty\]si \[see\]I l l the hhm, ry relu'esentation , the two rohmms a, rerepresented separately, one ;ffter the other; timfirst column is called the word inventory  col-umn;  the second column is called the code wordinventory  co lumn.In the word inventory colunul (see Figure lafor a schematic), the list of lexical items is rel)r('-sented as a continuous tring of i)honemes, withoutseparators between words (e.g., ~;)kmtkltisi...).To mark tile boundaries between lexical items, thephoneme string is preceded by a list of integersrepresenting the lengths (in phonemes) of eachword.
Each length is represented am a. lixcd-length,zero-padded binary number, l'rceeding this list isa single integer denoting the length of each lengthfield; this integer is represented in unary, so thatits length need not be known in adwmce.
Pre-ceding the entire column is the numl)er of h,xica.Ientries n codc(I as a self-dclimiting integer.The length of the representation f I.he integern is given by the fimctiont(-~)(,,) (I)We define len(wi) to be the mmlber ofphonemes ill word wi.
If there are p total uniquephonemes used in tile sample, titan wc representeach phoneme as a fixed-length bit string of lengthlen(p) = log 2 p. So, the length of the representa-tion of a word wi in tile lexicon is the mnnberof phonemes in the word times the length of aphoneme: len(p), len(wi).
The total length of allthe words in the lexicon is tile sum of this formulaover all lexical items:t/, T1lien(p), len(wi)) = len(p) Z lc,,(w,) (2)i=1 i=1As stated al)ovc, the length liehls used to di-vide the phoneme string are lixe(Mcugth, lu e;u'hfield is an integer I)etween one an(I the munl)er ofphonemes in the longest word.
Since repres(mtitlgintegers between one and x takes log2 x bits, timlength of each field is:tog~(,;!
?,~ t.,(,,,,))I( ',ode words ;tl'e I'elWeSelfl,ed I)y Sqllitl'4, br;wi(qq,s, so\[:v\] means  %he (:ode won'd coro'eSl)C,lldintl4 I,o :r'.TI.
)otsl.ral) I.hc acquisition of other levels \[of linguis-l.ic organization\] remaius to be determined."
Inthis paper, we measure the potential roles of dis-I,ribution, phonotactics and their combination us-ing a computer-sitnulated l arning algorithm; thesimulation is based on a bootstrapping model inwhich phonotactic knowledge is used to constrainthe distributional analysis of speech samples.While our work is in part motivated by theabove research, other developmental research sup-ports certain ;assumptions we make.
The inputto our system is represented as a sequence ofi)houenms, o we implicitly assume that infants areaisle I.o ,'ouv('rl, from acoustic inl)ut to phoneme se-quem:es; research i)y Kuhl (e.g., Gricser & Kuhl,1989) suggests tha.t this assmnl)tion is remson-al)h,.
Since sentence I)oundaries provide informa-l.ion ahout word I)oumlaries (the end of a sentenceis also the end of a word), our input containssentence I~oumhu'ik~s; several studies (13ernstein-II.atm'r, 1985; Ilirsh-lh~sek et al, 1987; KemlerNels~m, I lirsh-I'asek, ,lusczyk & Wright C;msidy,1989; ,I usczyk et al, 1992) have shown that infimtscan perceive senl,cncc I)oundarics using prosodiccues.
Ih)wever, FiSher and 'lbkura (in press) foundm) evidence that prosody can accurately predictword boundaries, .so the task of finding words re-mains.
Finally, one might question whether in-Ikmts have the ability we are trying to model--thatis, whether they can identify words embedded insentences; Jusczyk and Aslin (submitted) foundthat 7 I/2-month-olds can do so.The Mode lTo gain an intuitive understanding of our model,consider the f()llowing speech sample (transcrip-ti{,u is in IPA):Orthogral)hy: I)o you see tim kitty?Se(' the kitty?I)o you like the kil,t,y'.~Trauscril)l,ioil: (luj usiiS;~kl tisi(iokltidu.iulalk&)kltiThere are many differeut ways to break this sam-pie into I)utative words (each particular segmen-l, ation is called a segmentation hypothesis).
Twosucll hypotheses a~re:Segmentation 1: du ju si 59 kltisi 5~ kltidu ju lalk 5,3 kltiS<:gnmnfl,ation 2: duj us i5 ~)klt isic) ~)k nti(lu jul alk ()ok Itilasting I, he wor(Is used I)y each segmentation hy-i,othcsis yMds the Ibllowing two lexicons:85Segmentation I1 du 3 klti 5 si2 59 4 lalk 6 juSegmentation 21 alk 5 ok 9 Iti2du 60kIt 10jul3 duj 7 i 11 sis45ok 8i5 12 usNote that Segmentation 1, the correct hypothesis,yields a compact lexicon of frequent words whereasSegmentation 2 yields a much larger lexicon of in-frequent words.
Also note that a lexicon containsonly the words used in the sample--no words areknown I.o tim system a priori, nor are any carriedow;r from one hypothesis to the next.
Given a lexi-con, tim saml)le can I)e encoded by ret)lacing wordswith their respective indices into the lexicon:Encoded Sample l: 1, 6, 5, 2, 3;5, 2, 3;l, 6, 4, 2, 3;Encoded Saml)le 2: 2, 11_2, 6, 4, 5;11, 3, 8;1, 9, 10, 7, 8;Our simulation attempts to find the hypothesisthat minimizes the combined sizes of the lexiconand encoded sample.
This approach is called theMinimum Description Length (MDL) paradigmand has been used recently in other domains toanalyze distributional information (Li & Vitgnyi,1993; Rissanen, 1978; Ellison, 1992, 1994; Brent,1993).
For reasons explained in the next section,the system converts these character-based repre-sentations to compact binary representations, us-ing the number of bits in the binary string as aIne~u re of size.I)imnotac(.ic rules can I)e used to restricttim s(wnenl,al,ion hyl)ol, hesis Sl)ace I)y prevent-ing word I)ountlari(,s a.t certain places; for in-stance, /ka,l,sp:)z/ (",:at's paws") has six i,,ternals(~gmental.ion I)oints (k ;~l,Sl):)z, ka: t.sl):)z, el.c),only two of which are I)honotactically allowed(ka:t Sl):)z and kmts 1)3z).
'17o evaluate the use-fuhmss of phonotactic knowledge, we comparedresults between phonotactically constrained andunconstrained simulations.S IMULAT ION DETAILS' Ib use the MDL principle, as introduced above,we search for the smallest-sized hypothesis.
Wemust have some well-defined method of measur-ing hypothesis izes for this method to work.
Asilnllle, intuitive way of measuing the size of a hy-pothesis is to count the numl)er of characters usedto rcl)resent it.
\[:or example, counting the charac-ters (cxclu(ling spaces) in the introductory exam-Figure 1: Schematic diagrams for components of the representation'e,,.
(t,,,l)\] I 'en(t'nl) I(a)(b)(c)' lb be fully self-delinfiting, the width of a fieldmust be represented in a self-delinfiting way; weuse a unary representation--i.e., write an extrafield consisting of only '1' bits followed by a termi-nating '0'.
There are n fields (one for each word),plus the unary prefix, so the combined length ofi,hc fields plus prefix (plus terminating zero) is:1 + (,~ + 1) log.,(max m~(,.~)) (:~)" 1. .
.nThe total length of the word inventory column rep-resentation is the sum of the terms in (1), (2) and(.~).The code word inventory column of the lexicon(see Figure lb for a schematic) has a nearly iden-tical representation asthe previous colmnn exceptthat code words are listed instead of phonemicwords--the length fields and unary prefix servethe same purpose of marking the divisions betweencode words.The sample can be represented most com-pactly by assigning short code words to frequentwords, reserving longer code words for infrequentwords.
To satisfy this property, code words are as-signed so that their lengths ar~ fre(luency-l);l~sed;the lengl.h of tim ,:ode word fi)r a word of I're(Itn~ncyf(',,) will not be greater than:len(\[w\]) = log2 ~in l f (w)  _ log 2 mf(w) f(w)The total length of the code word list is the sumof the code word lengths over all lexieal entries:IZ 7tZlen( \ [w\ ] )  = Z l?g2 m f(wi) (4)i=1 i=1As in the word inventory colmnn (described;d)ove), the length of each code word is representedin a fixed-length field.
Since the least frequentword will have the longest code word (a prol)ertyof the formula for /cn(\[wi\])), the longest possible.code word comes from a word of frequency one:ml?g'2 T : l?g2 mSim'e t, he fields contains integers between one audthis ,mt,d)('r, w," ~lefit,o the length of a \[i('ld I,o I)(':I, ,g..,( I, ,g:~ .,)As above, we represent he width of a lield inunary, so there are a total of n + 1 elements ofthis size (n fields plus the unary representation fthe field width).
The combined length of the fieldsplus prefix (and terminating zero) is:1 + (n + I)Iog2(Iog ~ m) (5)The total length of the code word inventory col-umn representation is tile sum of the l.errus ill {,1)and (5).Finally, the sequence of words which form timsample (see Figure le for a schematic) is repre-sented as the nurrd)er of words in the sample (m)followed by the list of code words.
Since codewords are used as compact indices into the lex-icon, the original sample could I)e re(x)nstructedcompletely by looking up eacil code word in thislist and replacing it with its phoneme sequencefrom the lexicon.
The code words we assigned tolexical items are self-delimiting (once the set ofcodes is known), so there is no need to representthe boundaries between code words.The length of the representation f the iutegerm is given by I.h(~ fimctione(~)(,.)
((i)The length of the representation f the sanq)leis computed by summing the lengths of the codewords used to represent the sample.
We can sim-plify this description by noting that the combinedlength of all occurrences of a particular code word\[wi\] is f (w i ) ,  len(\[wi\]) since there are f(u,i) oc-currences of the code word in the sample.
So, thelength of the encoded sample is the sum of thisformula over all words in the lexicon:: \[ ('"i=1 i=1(7)The total length of the sample is given by addingthe terms in (6) and (7).
The total length of therepresentation of the entire hyl)othesis is the samof the rel)resentation lengt,hs of the word inw,ntory( 'f) l lnlnl,  I,he code  word  i l lV(qltory ~'ohllll l l ;Hid I,hona.mpb'.This systein of ('olnputhig hyl)othesis sizes is('llicielil, in the sense that elenlents ;ire thoughtof a.s being rel)resent;ed compactly and that (:odewords arc assigned based on the relative frequen-cies of words.
'\['he'final evaluation given to a hy-pothesis is an estimate of the minimal number ofbits required to transmit hat hypothesis.
As such,it pernfits direct comparison between competinghypotheses; that is, the shorter the representationof some hypothesis  the more distributional infor-uiation can be extracted and, therefore, the betterthe hypothesis.Phonotact i csI'honotactic knowledge was given to the system asa.
list of licit initial and Iinal consoliant clusters ofEnglish words~; this list was checked against allsix sanlples so tha?
the list was inaxinmlly pcr-in issiv( '  (e.g., I,li(~ under l ined  conson l iu t  clusl,er inexl l lo i 'e cou ld  I)e d ivh led  as ek-splore or eks-plore) .Ih-q\]iose sinnilittions which used the l ) l iouota( : t icknowledge., it word boundary could lrl()t be insertedwhen (Iohlg so would create a word init ial or final(:onsonant chister not on the list or would create aword without a vowel.
For example (from an ac-tual sample--corresponds to the utterance, "Wantme to help baby?
"):Sample: wantmituhclpbebiVaJid Boundaries: want.mi.t.u.help.be.bilit I,he s('('()li(I l i l le,  I, hose word I)ol luda, ries that  a, rel ihti l i i i l ,acl, ic;I .
I ly nel-ial are iiliirk(;d wil, ii dots.
TheI>,,,,,.I;,.,.y I,,.i,w,,,.,, /w /and  /a /  is ilh'gal I,eca,,s,,/w /  I,y itself is ii(fl, a legal wor(I in English; theI,,mmlary I>ctwcen /a /  an(I liil i~ illegal l)ecausc/n tm/  is n()t a va, lkl word inil,\[al (:ons(inant chls-to,'; th(; I)oundary between /m/  and / i /  is illegalI)('ca, llSe / i i t iu /  iS also not a valid word liual COIISO:"nant chml,er; i,h(' 1,01 ndary between/ I ) /and /b / i slegid i:,e(:ause /11)/is a valid word linal (:luster and/I)/ is a valid word initial cluster.
Note that usingthe l)honotactic onstraints reduces the number ofI)otential word boundaries from fifteen to six inthis exaruple.After the system inserts a new word bound-a ry, it updates the list of remaining valid insertionI)oints --adding a point may cause nearby pointsI.o I)cconm unusable clue to the rcstriction that ev-ery wor(I must llave a vowel.
For example (corre-Sl)On(ling to the utterance "green and"):4hi I,lu,n,h,~ical terms, the syllal)h. (,ns(.i.s permit-I.l',l ni  Wl,l',l I,i.giliiiiii.gs l l i l l l  syllaldl' codas lilwniil,l,ed al,W?il'd i.iids.
Si,IIIi!
lailgllit~es ( inchidi l ig l'\]liglish) ha.v(ilil\[iW('lil, slrl.s of OllSCts it l ld c()das toi" word-il l i ,  eri lal audword-liolllldary positions--we use the word-boundaryset.87Before: gri.n.mndAf ter :  grinsendAfter the segmentation o f /gr in /  and /send/, thepotential boundary between / i /  and /n /  becomesinvalid because inserting a word boundary therewould produce a word with no vowel (/n/).Inputs  and  S imulat ionsTwo speech samples from each of three subjectswere used in the simulations in one sample amother was speaking to her daughter and inthe other, the same mother was speaking to theresearcher.
The samples were taken from theCHILDES database (MacWhinney ~ Snow, 1990)from studies reported in Bernstein (1982).
Eachsample was checked for consistent word spellings(e.g., 'ts wits changed to its), then was transcribedinto an ASCll-I)ased I)honemic rel)res(mtation :l.'Fhe transcription sysl, em was based on IPA an(Iused one character for each consonant or vowel;diphthongs, r-colored vowels and syllabic conso-nants were each represented as one character.
Forexample, "boy" was written as bT, "bird" as bRdand "label" as lebL.
For purposes of phonotac-tic constraints, yllabic consonants were treate(!
asvowels.
Sample lengths were selected to make thenmnber of available segmentation points nearlyequal (about 1,350) when no ph0notactic on-straints were applied; child-directed samples had498-536 tokens and 153-166 types, adult-directedsa.ml)les had 443-484 tokens and 196--205 types.I"inMly, Iwl'ore tim saml)les were fi~(I to the sinm-lations, divisions bel,wcell words (but not l)(%w(~ens(HIt(qIcos) wcr(~ reiuovc'.
(LThe sl)ace of l)Ossil)le hyl)oi, hcses is vlmt 4, sosonl(~ nmthod of finding a minimum-length hy-pothesis without considering all hypotheses i nec-essary.
We used the following method: first, evalu-ate the input sample with no segmentation pointsadded; then evaluate all hypotheses obtained byadding one or two segmentation points; take theshortest hypothesis found in the previous tep andevaluate all hypotheses obtained by adding oneor two more segmentation points; continue thisway until the sample has been segmented into thesmallest possible units and report the shortest hy-pothesis ever found.
Two variants of this simu-lation wcre used: (1) DIST-FREE was free of anyphonotactic restrictions on the hypotheses it couldform (DIST refers to the measurement of distri-butionaJ information), whereas (2) DIST-PtloNO,Ise~l I.Iw i)hon~,t;wtic r(,sl.ricl,i(ms (lescril.,I ;,.I,,w,'.3The I,ranisi:riptioli nil%hod CUlSiix'(!d the identh:altrallscripl~ioii of all occurrences of a word.4F'or our samples, unconstrained by phonotactics,there are about 2 lsS?
~ 2.5 ?
104?s hypotheses.I",ach simulation was rim on ('a('h S;I.IIIpl('.
for ;i.
I.
()tal of twelve DIST rllns.Finally, two other simulations were run oneach sample to measure chance performance:.(1.)
RAND-FREE inserted random segmentationpoints and reported the resulting hypothesis,(2) RAND-PItONO inserted random segmentationpoints where permitted by the phonotactic on-straints.
Since the RAND simulations were giventhe number of segmentation points to add (equalto the number of segmentation points needed toI)rodnce the natural English segmentation), theirj)(~rrormance is an upper t)oml(I on chance pcrl'or-Illll.ll(:(;.
hi C#)lltl';i.st, tim I)INT shrlnlat iol lS nnlstdetermine I.im l l l l l i lh(:r  of  SC~lll(~lll,;i.l.i()n poinl.s I.oa d(I using M I)1, ev;iJurtl.ious.
Tim results for eachI'~,ANI) sinlulatiou a.re averages over 1,000 trims Oile:~ch input sample.RESULTSEach simulation was scored for the number of cor-rect segmentation points inserted, as compared tothe natural English segmentation.
From this scor-ing, two values were computed: recall, the per-cent of all correct segmentation points that wereactually found; and accuracy,  the percent of thehypothesized segmentation points that were actu-ally correct.
In terms of hits, False alarms andmisses, we have:hi tsrecal l  :h i ts  + missesh i l saccuracy  :h i ts  + fa l se  a la rmsResults are given in Table 1.
Note that thereis a trade-off between recall and accuracy--if allpossible segmentation points were added, recallwould be 100% but accuracy would be low; like-wise, if only one segmentation point was addedI)ctwccn two words, accuracy would be 100% butrecall would be low.
Since our goal is to correctlys(.gl|lelit speech, accuracy is more important h;mfinding every correct segmentation.
I"or exa.ml)h~,deciding 'littlekitty' is ;~ word is less disastrousthan deciding 'li', 'tle', 'ki' and 'ty' are all words,because assigning meaning to 'littlekitty' is a rea-sonable first try at learning word-meaning pairs,whereas trying to assign separate meanings to 'li'and 'tle' is problematic.T i le  i)erl~)rlnan(:e of  i) IS'I '- I) I IONO Oll ddhl-(lir('clcd Sl)oech shows l, hal, this systenl goes a longway toward solving the segnleutation i)rol)lcm.liowever, comparing the average pertbrmanees ofsimulations is also useful.
The effect of phone-tactic information can be seen by comparing theaverage performances of RAND-FREE and RAND-I'IIONO, since the ?
)nly difference I)etw('(m l.h('mis the addit,ion of phonotactic constra, ints on seg-mentations in the, latl;er.
Clearly I)houol, actic c(m-straints are useful, as both recall an(I accuracy im-prove.
A similar comparison between RAND-IQtEEand DtST-FREE shows that distributional inlbr-mation alone also improves performance.
Note inall the results of D\[ST-FaEE that using distribu-tional information alone favors recall over accu-racy; in fact, the segmentation hypotheses pro-duced by DIST-FREE have most words broken intosingle phoneme units with only a handful of wordsremaining intact.
Two comparisons are nee(tedto show that the cond)ination of disl.rilmtionaland phonotactic information I)erfi)rnm I)(,tter thaneither sour(:e a.lolle: DIST-PI IONO COml)ared toI{,ANI)-i~IIONO, to see tin'.
elrect of a(Idiug disl.ri-butionai analysis to phonotactic constraints, andDIST-PIIONO compared to l)IsT-I,'ltl,:l,;, to see l.heeffect of adding phonotactic constraints to distri--butional analysis.
The former comparison showsthat  the sources corrlbined are more useful thanphonotactic information alone, rl'he latter com-parison is less obvious--the trade-off between re-call and accuracy seems to have reversed, withno clear winner 5.
Data on discovered word typeshelps make this conlparison: DIS'I'-I?IU,:I,~ found12% of the words with 30% ac(:llraey an(1.
Dis'r-PltONO found 33% of the words with 50% accu-racy.
Whereas the segmentation point data are in-conclusive, word type data demonstrate hat com-bining information sources is more usefifl than us-ing distributional information alone.There is no obvious difference in performancebetween child- and adult-directed speech, exceptin DIST-PI1ONO (combined information sources)in which the difference is striking: accuracy re-mains high and recall rate more than tril)les forchild-directed speeclL This difference is again sup-ported by word type data: 14% recall with 30% ac-curacy for adult-directed speech, 56% recall with65% accuracy for chihl-directed Sl)eech.DISCUSSIONOur technique segments continuous speech intowords using only distributional and phonol,ac-tic information more effectively than one mightexpect--up to 66% recall of segmentation pointswith 92% accuracy on one sample, which yields58% recall of word types with 67% accuracy (therelatively low type accuracy is mitigated by thefact that most incorrect words are meaningf,,Iconcatenations of correct words --e.g., 'thekitty').5'Fhe higher accuracy of DIST-I)HoNO is st good.sign.
Furthermore, the minimum of the recall/accu-racy pair is greater in \])IST-PIIONO t| lml ill I)IST-\[?llEl,:and ?,he nmximum of the recall/accuracy pair is alsc~greal, er in I)IST- IqloNO I,hau hi I)IST- I"IIEI,L'l'~d)le l: Results for all simulations aveTugcd over individual speech samplesSimulationTarget Measure RAND-I"ItEE RAND-PIIONO DIST-FREE DIST-PHONOAdult % Recall 25.1 39.5 95.5 22.5% Accuracy 28.9 50.5 36.0 92.0Child % Recall 23.4 40.2 79.9 72.3% Accuracy 26.7 51.7 37.4 88.3Average % Recall 24.3 39.9 88.0 46.4% Accuracy 27.8 51.1 36.6 89.2This linding conlirms the idea that distril)utiona.nd i)houotactics ~irc useful sources of itfforlnationthat infants might use ill diseow~ring words (e.g.,.lusczyk eL al., 19931)).
In fact, it helps explain in-fa.uts' ability to h'a.rn words \['rom parental speech:these two sources alone are useful a\]l(l infant,s havescw,ral oth(~rs, like prosody and word stress pat-terns, available as well.
It also suggests that se-manl.ics and isolated words need not play as cen-tral a role as one Might think (e.g., Jusczyk, 1993,downplayed the utility of words in isolation).
It isdillicult, if not impossible given currently availablemethods, to determine which sources of informa-l.ion are necessary for infants to segment speechand learn words; only this sort of indirect evidenceis availal)lc to us.The results show a difference between adult-a.ml child-directed speech, in that the latter is ea.s-i~l' I,o sc~gm~ilt giv(m both distril)ution ;ul(I l)hono -tactics.
This lends quantitative SUl)l)ort to re-search which suggests that mothercse dilthrs fl'o\]unormal adult speech in ways possibly useful to thelanguage-learning inl'aut (Aslin et al).
In fact, thefactors making motherese more learnable might bechmidated using this technique: coral)are the re-suits of sevcra.I (lilt~rent models, each containinga.
dilrercnt factor or combination of factors, look-ing for those in which a siibstautial i)erformaneedill~,renec exists b(itwecn child- and adult-directedSl~eech.
()ur model uses phonotaetie constraints as al)-solute requirenlent!s on the structure of individualwt~l'tls; this iml)lies th;tt phonot;teties have beenh'arncd prior to ;tttempts at segmentation.
Wemust.
therefore show that phonotactics can indeedhe learned without access to a lexicon--withoutsuch a tlemonsl.raliion, we are trapped in circularrc;Lsoning.
Gafos :and Brent (1994) demonstratethat phonotacties can be learned with high accu-racy from the same unsegmented utterances weused in our simulations.
In general, two meth-89ods exist for combining information sources in ttleMI)I, I)aradign-l: one is to have absolute require-nllHits Oil plausible hyl*otheses (like our i)honota(:-tic co,lsl.rainl,s) -these requirements must I)e in-dCl)C,l?lcntly learnable; the other method of com-bination is to include an information source in theinternal representation of hypotheses (like our dis-trib,ltional information)---all components of therepresentation are learned simultaneously (see EI-l ison, 1992, for an example of multiple componentsin a representation).We would like to extend the system by usinga more detailed transcription system.
We expectthat this would help the system find word bound-aries for reasons detailed in Church (1987)--inbrief, that allophonic variation may be quite use-ful in predicting word boundaries.
Another sim-pler extension of this researeh will be to increasel, he length of I,he speech samples used.
Finally, wewill try the current system on samples from otherlanguages, to make sure this method generalizesapprol)riately.This research program will provide comple-mentary evidence supporting hypotheses aboutthe sources of information infants use in learningtheir natiw~ languages.
Until now, research as fo-cused ou demonstrations of infants' sensitivity tovarious sourees; we have begun to provide quanti-tatiw, memsures of the usefiflness of those sources.ReferencesRichard N. Aslin, Julide Z. Woodward, Nicholas P.I.aMendola, and Thomas G. Bever.
In press.Models of word segmentation i  fluent ma-ternal speech to infants.
In Morgan & De-muth (Eds.
), Signal to Syntax: Bootstrappingfrom Speech to Syntax in Early Acquisition.Erlbaum, Hillsdale, NJ.Nan Bernstein.
1982.
Acoustic study of mothers'speech to language-learning children: An anal-ysis of vowel articulatory chaTueteristies.
Un-published doctoral dissertation, Boston lhfi-versity.Nan Bernstein-Ratner.
1985.
Cues which markclause-boundaries n mother-child speech.
I)a -per presented at the meeting of the AmericanSpeech-Language H aring Association, Wash-ington DC.Michael R. Brent 1993.
Minimal generative x-planations: A middle ground between euronsand triggers.
In Proceedings of the 15th An-nual Conference of the Cognitive Science So-ciety, pages 28-36, Boulder, Colorado.Kenneth Church.
1987.
Phonological parsing andlexical retrieval.
Cognition, 25:53-69.Aline Cutler, and Sally I hlttcrli(;Id.
1,(192.
Ii.hyth-iiiic cues to si)(~cch segnl('iltation: I'\]vid~'lle(~I'l'olll .itlllCl, llr(' niisl)(wcl~l)t,i()ll.
,\]ourntll ofMemory ?.7 I,aligliage, ~f I :218 236.Alllm Cutler, and I)avi(I M. (Jarter.
1987.
Timpredominance of strong initial syllables in timEnglish vocabulary.
Computer Speech andLanguage, 2:133-142.Anne Cutler, and D. G. Norris.
1988.
The roleof strong syllables in segmentation for lexicalaccess.
Journal of Experimental Psychology:Humen Perception and Performance, 14:113-121.T.
Mark Ellison.
1992.
The Machine Learning ofPhonological Structure.
Unpublished octoraldissertation, University of Western Australia.T.
Mark Ellison.
In press.
The iterative learningof phonological rule's.
Com, putalional I,ingais-lies.Aline I:crnal(I, and (Jlaudia Mazzie.
1991.Prosody and focus in speech to infants aridadults.
Developmental Psychology, 27:20'()221.Cindy Fisher, H. Tokura.
In press.
Acousticcues to clause boundaries in speech to infants:Cross-linguistic evidence.
In Morgan ~ De-muth (Eds.
), Signal to Syntax: Bootstrappingfrom Speech to Syntax in Early Acquisition,Erlbaum, Hiltsdale, NJ.Adamantios Gafos, and Michael R. Brent.
1994.l,earning syllable structure without wordboundaries.
Paper presented at the 1994Stanford Child Language R,esearch Forum,Stanford, CA.I )i A IiI1(~ ( lri('s(,r, ;tii(I I)al,ricia K. I{uhl.
1914'().
'l'h~"c;IJ.
(~g\[ll'izati(in (d" Slie(~eh by infalil,s: Siilll)orl,f(ir Sll(,~ch-soliiid lirol,otylicS.
#){'llf'loplll,{'lltalI~s~lehology, 25:577- 588.9OKathy ltirsh-Pasck, Deborah G. Kemh:r Nelson,Peter W. ,iusezyk, K. Wright (-',;~ssi(ty, I:L1)russ, and 1,.
Kennedy.
1987.
('.laus('s ar(,perceptual units \[br young infants.
(:ognilion,26:269-286.Peter W. Jusczyk.
199;I. Discov(,xing sound I)at -terns in the natty(: language.
In Proceedings ~fthe 15th Annual Confelvliee of the CognitiveScience Society, pages 49-60, Boulder, Col-orado.Peter W. Jusczyk, and Richard N. Asliu.
Submit-ted for publication.
Recognition of familiarpatterns in fluent speech by 7 1/2-month-oldinfants.Peter W. Jusczyk, Anne Cutler, and Nancy ,I.Redanz.
1993.
Infants' preference for thepredominant s ress patterns of English words.Child Development, 64:675-687.llel.er W..lusezyk, Angela I).
I,'ri('(h'rici, .h'aliilil' M. W~,ssl'ls, Vigdis Y. Svl,tll~(,rlill, andA.
M. ,lusezyk.
1993. hifanl,s" sl'ilsil.ivil,y I,~the stunid liatterns (if li~d,ivl' latigilag~' Wlll'ds.,Journal of M('.mory \[~'f Lallgnagc, 32:402 420.Peter W. Jusczyk, Kathy Hirsh-Pasek, l)cljo-rah G. Kemler Nelson, Lori J .
Kennedy,Amanda Woodward, and Julie Piwoz.
1992.Perception of acoustic correlates of majorphrasal units by young infants.
Cognitive Psy-chology, 24:252-293.Deborah G. Kemler Nelson, Kathy Hirsh-Pasck,Peter W. Jusczyk, and K. Wright Cassidy.1989.
How the prosodic cues in motheresemight assist language learning.
.Journal ofChild Language, 16:55--68.Miug IA, and Paul VitAnyi.
1993.
Ari lutroda<'-lion I.o I~'olntogorov Complexity and ils Appli-cations., Slu'ing(,r-Verlag, New York, NY.Brian MacWhinut;y, and C. Snow.
I!)'()0.
'l'll~Child Language Data nxchaugo Syst('lil: Allupdate.. Journal of Child Language, 17:457472.J.
R. Quinlan, and R. L. Rivest.
1989. lnffi'.rringdecision trees using the minimum descriptionlength principle.
Information and Computing,80:227-248.J.
Rissanen.
1978.
Modeling by shortest data de-scription.
Automatica, 14:465-471.
