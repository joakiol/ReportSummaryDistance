T5: Variational Inference for Structured NLPModelsDavid Burkett, Dan KleinABSTRACTHistorically, key breakthroughs in structured NLP models, such as chain CRFs orPCFGs, have relied on imposing careful constraints on the locality of features in order topermit efficient dynamic programming for computing expectations or finding the highest-scoring structures.
However, as modern structured models become more complex andseek to incorporate longer-range features, it is more and more often the case thatperforming exact inference is impossible (or at least impractical) and it is necessary toresort to some sort of approximation technique, such as beam search, pruning, orsampling.
In the NLP community, one increasingly popular approach is the use ofvariational methods for computing approximate distributions.The goal of the tutorial is to provide an introduction to variational methods forapproximate inference, particularly mean field approximation and belief propagation.The intuition behind the mathematical derivation of variational methods is fairly simple:instead of trying to directly compute the distribution of interest, first consider someefficiently computable approximation of the original inference problem, then find thesolution of the approximate inference problem that minimizes the distance to the truedistribution.
Though the full derivations can be somewhat tedious, the resultingprocedures are quite straightforward, and typically consist of an iterative process ofindividually updating specific components of the model, conditioned on the rest.Although we will provide some theoretical background, the main goal of the tutorial is toprovide a concrete procedural guide to using these approximate inference techniques,illustrated with detailed walkthroughs of examples from recent NLP literature.Once both variational inference procedures have been described in detail, we'll providea summary comparison of the two, along with some intuition about which approach isappropriate when.
We'll also provide a guide to further exploration of the topic, brieflydiscussing other variational techniques, such as expectation propagation and convexrelaxations, but concentrating mainly on providing pointers to additional resources forthose who wish to learn more.OUTLINE1.
Introduction1.
Approximate inference background2.
Definition of variational inference3.
Structured NLP problem setting, loglinear models4.
Graphical model notation, feature locality2.
Mean Field Approximation1.
General description and theoretical background2.
Derivation of updates for simple two-variable model3.
Structured mean field: extension to joint CRFs4.
Joint parsing and word alignment5.
High level description of other models (Coref, Nonparametric Bayes)3.
Belief Propagation1.
General description and theoretical background2.
Factor graph notation3.
Formulas for messages and beliefs, with joint CRF example4.
Dependency parsing5.
Word alignment4.
Wrap-up1.
Mean Field vs Belief Propagation (i.e.
what to use when)2.
Other variational methods3.
Additional resourcesBIOSDavid BurkettUniversity of California, Berkeleydburkett--AT--cs.berkeley.eduDavid Burkett is a Ph.D. candidate in the Computer Science Division at the University ofCalifornia, Berkeley.
The main focus of his research is on modeling syntactic agreementin bilingual corpora.
His interests are diverse, though, and he has worked on parsing,phrase alignment, language evolution, coreference resolution, and even video game AI.He has worked as an instructional assistant for multiple AI courses at Berkeley and wonthe Outstanding Graduate Student Instructor award in 2009.Dan KleinUniversity of California, Berkeleyklein--AT--cs.berkeley.eduDan Klein is an Associate Professor of Computer Science at the University of California,Berkeley.
His research includes many areas of statistical natural language processing,including grammar induction, parsing, machine translation, information extraction,document summarization, historical linguistics, and speech recognition.
His academicawards include a Sloan Fellowship, a Microsoft Faculty Fellowship, an NSF CAREERAward, the ACM Grace Murray Hopper Award, Best Paper Awards at ACL, EMNLP andNAACL, and the UC Berkeley Distinguished Teaching Award.
