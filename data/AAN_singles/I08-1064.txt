Projection-based Acquisition of a Temporal LabellerKathrin Spreyer?Department of LinguisticsUniversity of PotsdamGermanyspreyer@uni-potsdam.deAnette FrankDept.
of Computational LinguisticsUniversity of HeidelbergGermanyfrank@cl.uni-heidelberg.deAbstractWe present a cross-lingual projection frame-work for temporal annotations.
Auto-matically obtained TimeML annotations inthe English portion of a parallel corpusare transferred to the German translationalong a word alignment.
Direct projectionaugmented with shallow heuristic knowl-edge outperforms the uninformed baselineby 6.64% F1-measure for events, and by17.93% for time expressions.
Subsequenttraining of statistical classifiers on the (im-perfect) projected annotations significantlyboosts precision by up to 31% to 83.95% and89.52%, respectively.1 IntroductionIn recent years, supervised machine learning has be-come the standard approach to obtain robust andwide-coverage NLP tools.
But manually annotatedtraining data is a scarce and expensive resource.
An-notation projection (Yarowsky and Ngai, 2001) aimsat overcoming this resource bottleneck by scalingconceptually monolingual resources and tools to amultilingual level: annotations in existing monolin-gual corpora are transferred to a different languagealong the word alignment to a parallel corpus.In this paper, we present a projection frameworkfor temporal annotations.
The TimeML specifica-tion language (Pustejovsky et al, 2003a) defines anannotation scheme for time expressions (timex for?
The first author was affiliated with Saarland University(Saarbru?cken, Germany) at the time of writing.John [met]eventMary [last night]timex.John [traf]eventMary [gestern Abend]timex.Figure 1: Annotation projection.short) and events, and there are tools for the auto-matic TimeML annotation of English text (Verha-gen et al, 2005).
Similar rule-based systems existfor Spanish and Italian (Saquete et al, 2006).
How-ever, such resources are restricted to a handful oflanguages.We employ the existing TimeML labellers to an-notate the English portion of a parallel corpus, andautomatically project the annotations to the word-aligned German translation.
Fig.
1 shows a simpleexample.
The English sentence contains an eventand a timex annotation.
The event-denoting verb metis aligned with the German traf, hence the latter alsoreceives the event tag.
Likewise, the components ofthe multi-word timex last night align with Germangestern and abend, respectively, and the timex tag istransferred to the expression gestern abend.Projection-based approaches to multilingual an-notation have proven adequate in various domains,including part-of-speech tagging (Yarowsky andNgai, 2001), NP-bracketing (Yarowsky et al, 2001),dependency analysis (Hwa et al, 2005), and role se-mantic analysis (Pado?
and Lapata, 2006).
To ourknowledge, the present proposal is the first to applyprojection algorithms to temporal annotations.489Cross-lingually projected information is typicallynoisy, due to errors in the source annotations aswell as in the word alignment.
Moreover, success-ful projection relies on the direct correspondenceassumption (DCA, Hwa et al (2002)) which de-mands that the annotations in the source text behomomorphous with those in its (literal) transla-tion.
The DCA has been found to hold, to a sub-stantial degree, for the above mentioned domains.The results we report here show that it can alsobe confirmed for temporal annotations in Englishand German.
Yet, we cannot preclude divergencefrom translational correspondence; on the contrary,it occurs routinely and to a certain extent systemat-ically (Dorr, 1994).
We employ two different tech-niques to filter noise.
Firstly, the projection processis equipped with (partly language-specific) knowl-edge for a principled account of typical alignmenterrors and cross-language discrepancies in the reali-sation of events and timexes (section 3.2).
Secondly,we apply aggressive data engineering techniques tothe noisy projections and use them to train statisticalclassifiers which generalise beyond the noise (sec-tion 5).The paper is structured as follows.
Section 2gives an overview of the TimeML specification lan-guage and compatible annotation tools.
Section 3presents our projection models for temporal annota-tions, which are evaluated in section 4.
Section 5describes how we induce temporal labellers for Ger-man from the projected annotations; section 6 con-cludes.2 Temporal Annotation2.1 The TimeML Specification LanguageThe TimeML specification language (Pustejovskyet al, 2003a)1 and annotation framework emergedfrom the TERQAS workshop2 in the context of theARDA AQUAINT programme.
The goal of the pro-gramme is the development of question answering(QA) systems which index content rather than plainkeywords.
Semantic indexing based on the identifi-cation of named entities in free text is an established1A standardised version ISO-TimeML is in preparation, cf.Schiffrin and Bunt (2006).2See http://www.timeml.org/site/terqas/index.htmlmethod in QA and related applications.
Recent yearshave also seen advances in relation extraction, a vari-ant of event identification, albeit restricted in termsof coverage: the majority of systems addressingthe task use a pre-defined set of?typically domain-specific?templates.
In contrast, TimeML modelsevents in a domain-independent manner and pro-vides principled definitions for various event classes.Besides the identification of events, it addresses theirrelative ordering and anchoring in time by integrat-ing timexes in the annotation.
The major contri-bution of TimeML is the explicit representation ofdependencies (so-called links) between timexes andevents.Unlike traditional accounts of events (e.g.,Vendler (1967)), TimeML adopts a very broadnotion of eventualities as ?situations that happenor occur?
and ?states or circumstances in whichsomething obtains or holds true?
(Pustejovsky etal., 2003a); besides verbs, this definition includesevent nominals such as accident, and stative mod-ifiers (prepared, on board).
Events are annotatedwith EVENT tags.
TimeML postulates seven eventclasses: REPORTING, PERCEPTION, ASPECTUAL, I-ACTION, I-STATE, STATE, and OCCURRENCE.
Fordefinitions of the individual classes, the reader is re-ferred to Saur??
et al (2005b).Explicit timexes are marked by the TIMEX3 tag.It is modelled on the basis of Setzer?s (2001) TIMEXtag and the TIDES TIMEX2 annotation (Ferro et al,2005).
Timexes are classified into four types: dates,times, durations, and sets.Events and timexes are interrelated by three kindsof links: temporal, aspectual, and subordinating.Here, we consider only subordinating links (slinks).Slinks explicate event modalities, which are of cru-cial importance when reasoning about the certaintyand factuality of propositions conveyed by event-denoting expressions; they are thus directly rel-evant to QA and information extraction applica-tions.
Slinks relate events in modal, factive, counter-factive, evidential, negative evidential, or condi-tional relationships, and can be triggered by lexicalor structural cues.2.2 Automatic Labellers for EnglishThe basis of any projection architecture are high-quality annotations of the source (English) portion490e ?
E temporal entityl ?
E ?
E (subordination) linkws ?
Ws, wt ?
Wt source/target wordsal ?
Al : Ws ?
Wt word alignmentAs ?
as : E ?
2Ws source annotationAt ?
at : projected target(E ?
As ?
Al) ?
2Wt annotationTable 1: Notational conventions.of the parallel corpus.
However, given that the pro-jected annotations are to provide enough data fortraining a target language labeller (section 5), man-ual annotation is not an option.
Instead, we use theTARSQI tools for automatic TimeML annotation ofEnglish text (Verhagen et al, 2005).
They have beenmodelled and evaluated on the basis of the Time-Bank (Pustejovsky et al, 2003b), yet for the mostpart rely on hand-crafted rules.
To obtain a full tem-poral annotation, the modules are combined in a cas-cade.
We are using the components for timex recog-nition and normalisation (Mani and Wilson, 2000),event extraction (Saur??
et al, 2005a), and identifica-tion of modal contexts (Saur??
et al, 2006).33 Informed Projection3.1 The Core AlgorithmRecall that TimeML represents temporal entitieswith EVENT and TIMEX3 tags which are anchoredto words in the text.
Slinks, on the other hand, arenot anchored in the text directly, but rather relatetemporal entities.
The projection of links is there-fore entirely determined by the projection of the en-tities they are defined on (see Table 1 for the nota-tion used throughout this paper): a link l = (e, e?
)in the source annotation as projects to the target an-notation at iff both e and e?
project to non-emptysequences of words.
The projection of the enti-ties e, e?
themselves, however, is a non-trivial task.3TARSQI also comprises a component that introduces tem-poral links (Mani et al, 2003); we are not using it here becausethe output includes the entire tlink closure.
Although Mani et al(2006) use the links introduced by closure to boost the amountof training data for a tlink classifier, this technique is not suit-able for our learning task since the closure might easily propa-gate errors in the automatic annotations.a.. .
.
[ ws ]e .
.
.
b. .
.
.
[ ws ]e .
.
.. .
.
[ wt ]e .
.
.
.
.
.
[ wtj wtj+1 ]e .
.
.c.
.
.
.
[ wsi wsi+1 ]e .
.
.. .
.
[ wtj wtj+1 wtj+2 ]e .
.
.Figure 2: Projection scenarios: (a) single-word 1-to-1, (b) single-word 1-to-many, (c) multi-word.a.
[ .
.
.
]e b.
[ .
.
.
]e .
.
.
[ .
.
.
]e?wtj?2 wtj?1 wtj wtj+1 wtFigure 3: Problematic projection scenarios: (a) non-contiguous aligned span, (b) rivalling tags.Given a temporal entity e covering a sequence as(e)of tokens in the source annotation, the projectionmodel needs to determine the extent at(e, as, al) ofe in the target annotation, based on the word align-ment al .
Possible projection scenarios are depictedin Fig.
2.
In the simplest case (Fig.
2a), e spans asingle word ws which aligns with exactly one wordwt in the target sentence.
In this case, the modelpredicts e to project to wt.
A single tagged wordwith 1-to-many alignments (as in Fig.
2b) requiresa more thorough inspection of the aligned words.
Ifthey form a contiguous sequence, e can be projectedonto the entire sequence as a multi-word unit.
Thisis problematic in a scenario such as the one shown inFig.
3a, where the aligned words do not form a con-tiguous sequence.
There are various strategies, de-scribed in section 3.2, to deal with non-contiguouscases.
For the moment, we can adopt a conservativeapproach which categorically blocks discontinuousprojections.
Finally, Fig.
2c illustrates the projec-tion of an entity spanning multiple words.
Here, themodel composes the projection span of e from thealignment contribution of each individual word wscovered by e. Again, the final extent of the projectedentity is required to be contiguous.With any of these scenarios, a problem ariseswhen two distinct entities e and e?
in the source an-4911.
project(as, al ):2. at,C = ?3.
for each entity e defined by as:4. at,C(e, as, al) =SCws?as(e) proj(ws, e, as, al)5. for each link l = (e, e?)
defined over as:6. if at,C(e, as, al) 6= ?
and at,C(e?, as, al) 6= ?7.
then define l to hold for at,C8.
return at,Cwhereproj(ws, e, as, al) = {wt ?
Wt | (ws, wt) ?
al ??e?
?
as.
e?
6= e ?
wt 6?
at,C(e?, as, al)}and[CS =?SS :SS is convex?
: otherwiseFigure 4: The projection algorithm.notation have conflicting projection extents, that is,when at(e, as, al) ?
at(e?, as, al ) 6= ?.
This is il-lustrated in Fig.
3b.
The easiest strategy to resolveconflicts like these is to pick an arbitrary entity andprivilege it for projection to the target word(s) wt inquestion.
All other rivalling entities e?
project ontotheir remaining target words at(e?, as, al) \ {wt}.Pseudocode for this word-based projection oftemporal annotations is provided in Fig.
4.3.2 Incorporating Additional KnowledgeThe projection model described so far is extremelysusceptible to errors in the word alignment.
Re-lated efforts (Hwa et al, 2005; Pado?
and Lapata,2006) have already suggested that additional lin-guistic information can have considerable impact onthe quality of the projected annotations.
We there-fore augment the baseline model with several shal-low heuristics encoding linguistic or else topologi-cal constraints for the choice of words to project to.Linguistically motivated filters refer to the part-of-speech (POS) tags of words in the target languagesentence, whereas topological criteria investigate thealignment topology.Linguistic constraints.
Following Pado?
and La-pata (2006), we implement a filter which discardsalignments to non-content words, for two reasons:(i) alignment algorithms are known to performpoorly on non-content words, and (ii) events aswell as timexes are necessarily content-bearing andhence unlikely to be realised by non-content words.This non-content (NC) filter is defined in terms ofPOS tags and affects conjunctions, prepositions andpunctuation.
In the context of temporal annotations,we extend the scope of the filter such that it effec-tively applies to all word classes that we deem un-likely to occur as part of a temporal entity.
There-fore, the NC filter is actually defined stronger forevents than for timexes, in that it further blocksprojection of events to pronouns, whereas pronounsmay be part of a timex such as jeden Freitag ?ev-ery Friday?.
Moreover, events prohibit the projec-tion to adverbs; this restriction is motivated by thefact that events in English are frequently translatedin German as adverbials which lack an event read-ing (cf.
head switching translations like prefer to Xvs.
German lieber X ?rather X?).
We also devise anunknown word filter: it applies to words for whichno lemma could be identified in the preprocessingstage.
Projection to unknown words is prohibitedunless the alignment is supported bidirectionally.The strictness concerning unknown words is due tothe empirical observation that alignments which in-volve such words are frequently incorrect.In order to adhere to the TimeML specification, asimple transformation ensures that articles and con-tracted prepositions such as am ?on the?
are includedin the extent of timexes.
Another heuristics is de-signed to remedy alignment errors involving auxil-iary and modal verbs, which are not to be annotatedas events.
If an event aligns to more than one word,then this filter singles out the main verb or noun anddiscards auxiliaries.Topological constraints.
In section 3.1, we de-scribed a conservative projection principle which re-jects the transfer of annotations to non-contiguoussequences.
That model sets an unnecessarily modestupper bound on recall; but giving up the contiguityrequirement entirely is not sensible either, since it isindeed highly unlikely for temporal entities to be re-alised discontinuously in either source or target lan-guage (noun phrase cohesion, Yarowsky and Ngai(2001)).
Based on these observations, we proposetwo refined models which manipulate the projectedannotation span so as to ensure contiguity.
One492model identifies and discards outlier alignments,which actively violate contiguity; the other one addsmissing alignments, which form gaps.
Technically,both models establish convexity in non-convex sets.Hence, we first have to come up with a backbonemodel which is less restrictive than the baseline, sothat the convexation models will have a basis to op-erate on.
A possible backbone model at,0 is pro-vided in (1).
(1) at,0(e, as, al) =?ws?as(e)proj(ws, e, as, al )This model simply gathers all words aligned withany word covered by e in the source annotation, ir-respective of contiguity in the resulting sequence ofwords.
Discarding outlier alignments is then for-malised as a reduction of at,0?s output to (one of)its greatest convex subset(s) (GCS).
Let us call thismodel at,GCS.
In terms of a linear sequence ofwords, at,GCS chooses the longest contiguous sub-sequence.
The GCS-model thus serves a filteringpurpose similar to the NC filter.
However, whereasthe latter discards single alignment links on linguis-tic grounds, the former is motivated by topologicalproperties of the alignment as a whole.The second model, which fills gaps in the wordalignment, constructs the convex hull of at,0 (cf.Pado?
and Lapata (2005)).
We will refer to this modelas at,CH.
The example in (2) illustrates both models.
(2)[ .
.
.
]e?C : ?GCS : {1, 2}1 2 3 4 5 CH : {1, 2, 3, 4, 5}Here, entity e aligns to the non-contiguous tokensequence [1, 2, 5], or equivalently, the non-convexset {1, 2, 5}(= at,0(e)).
The conservative base-line at,C rejects the projection altogether, whereasat,GCS projects to the tokens 1 and 2.
The additionalpadding introduced by the convex hull (at,CH) fur-ther extends the projected extent to {1, 2, 3, 4, 5}.Alignment selection.
Although bi-alignments areknown to exhibit high precision (Koehn et al, 2003),in the face of sparse annotations we use unidirec-tional alignments as a fallback, as has been proposedin the context of phrase-based machine translation(Koehn et al, 2003; Tillmann, 2003).
Furthermore,we follow Hwa et al (2005) in imposing a limit onthe maximum number of words that a single wordmay align to.4 ExperimentsOur evaluation setup consists of experiments con-ducted on the English-German portion of the Eu-roparl corpus (Koehn, 2005); specifically, we workwith the preprocessed and word-aligned versionused in Pado?
and Lapata (2006): the source-targetand target-source word alignments were automati-cally established by GIZA++ (Och and Ney, 2003),and their intersection achieves a precision of 98.6%and a recall of 52.9% (Pado?, 2007).
The preprocess-ing consisted of automatic POS tagging and lemma-tisation.To assess the quality of the TimeML projec-tions, we put aside and manually annotated a de-velopment set of 101 and a test set of 236 bi-sentences.4 All remaining data (approx.
960K bi-sentences) was used for training (section 5).
Wereport the weighted macro average over all possi-ble subclasses of timexes/events, and consider onlyexact matches.
The TARSQI annotations exhibitan F1-measure of 80.56% (timex), 84.64% (events),and 43.32% (slinks) when evaluated against the En-glish gold standard.In order to assess the usefulness of the linguis-tic and topological parameters presented in section3.2, we determined the best performing combinationof parameters on the development set.
Not surpris-ingly, event and timex models benefit from the var-ious heuristics to different degrees.
While the pro-jection of events can benefit from the NC filter, theprojection of timexes is rather hampered by it.
In-stead, it exploits the flexibility of the GCS convexa-tion model together with a conservative limit of 2 onper-word alignments.
In the underlying data sampleof 101 sentences, the English-to-German alignmentdirection appears to be most accurate for timexes.Table 2 shows the results of evaluating the optimisedmodels on the test set, along with the baseline fromsection 3.1 and a ?full?
model which activates all4The unconventional balance of test and development data isdue to the fact that a large portion of the annotated data becameavailable only after the parameter estimation phase.493events slinks time expressionsmodel prec recall F prec recall F prec recall Ftimex-optimised 48.53 33.73 39.80 30.09 10.71 15.80 71.01 52.76 60.54event-optimised 50.94 44.23 47.34 30.96 14.29 19.55 56.55 42.52 48.54combined 50.98 44.36 47.44 30.96 14.29 19.55 71.75 52.76 60.80baseline 52.26 33.46 40.80 26.98 10.71 15.34 49.53 37.80 42.87full 51.10 40.42 45.14 29.95 13.57 18.68 73.74 54.33 62.56Table 2: Performance of projection models over test data.[.
.
.]
must today decide [.
.
.
]: [.
.
.]
(108723)[.
.
.]
hat heute u?ber1[.
.
.]
zu entscheiden, na?mlich u?ber2[.
.
.
]APPR VVINF APPRFigure 5: Amending alignment errors.heuristics.
The results confirm our initial assump-tion that linguistic and topological knowledge doesindeed improve the quality of the projected annota-tions.
The model which combines the optimal set-tings for timexes and events outperforms the un-informed baseline by 17.93% (timexes) and 6.64%(events) F1-measure.
However, exploration of themodel space on the basis of the (larger and thus pre-sumably more representative) test set shows that theoptimised models do not generalise well.
The testset-optimised model activates all linguistic heuris-tics, and employs at,CH convexation.
For events,projection considers bi-alignments with a fallback tounidirectional alignments, preferably from Englishto German; timex projection considers all alignmentlinks.
This test set-optimised model, which we willuse to project the training instances for the maxi-mum entropy classifier, achieves an F1-measure of48.82% (53.15% precision) for events and 62.04%(73.74% precision) for timexes.5With these settings, our projection model is ca-pable of repairing alignment errors, as shown inFig.
5, where the automatic word alignments are rep-resented as arrows.
The conservative baseline con-sidering only bidirectional alignments discards all5The model actually includes an additional strategy to ad-just event and timex class labels on the basis of designatedFrameNet frames; the reader is referred to Spreyer (2007), ch.4.5 for details.event timexdata prec recall prec recallall 53.15 45.14 73.74 53.54best 75% 54.81 47.06 74.61 62.82Table 3: Correlation between alignment probabilityand projection quality.alignments but the (incorrect) one to u?ber1.
The op-timised model, on the other hand, does not excludeany alignments in the first place; the faulty align-ments to u?ber1and u?ber2are discarded on linguisticgrounds by the NC filter, and only the correct align-ment to entscheiden remains for projection.5 Robust InductionThe projected annotations, although noisy, can beexploited to train a temporal labeller for German.As Yarowsky and Ngai (2001) demonstrate for POStagging, aggressive filtering techniques applied tovast amounts of (potentially noisy) training data arecapable of distilling relatively high-quality data sets,which may then serve as input to machine learn-ing algorithms.
Yarowsky and Ngai (2001) use theModel-3 alignment score as an indicator for thequality of (i) the alignment, and therefore (ii) theprojection.
In the present study, discarding 25% ofthe sentences based on this criterion leads to gainsin both recall and precision (Table 3).
In accor-dance with the TimeML definition, we further re-strict training instances on the basis of POS tags bybasically re-applying the NC filter (section 3.2).
Buteven so, the proportion of positive and negative in-stances remains heavily skewed?an issue which wewill address below by formulating a 2-phase classi-494prec recall F Fmodel event slink1-step 83.48 32.58 46.87 17.011-step unk 83.88 32.19 46.53 16.872-step 83.95 34.44 48.84 19.062-step unk 84.21 34.30 48.75 19.06timex1-step 87.77 49.11 62.981-step unk 87.22 49.55 63.202-step 89.52 51.79 65.622-step unk 88.68 50.89 64.67Table 4: Classifier performance over test data.fication task.The remaining instances6 are converted to featurevectors encoding standard lexical and grammaticalfeatures such as (lower case) lemma, POS, govern-ing prepositions, verbal dependents, etc.7 For slinkinstances, we further encode the syntactic subordi-nation path (if any) between the two events.We trained 4 classifiers,8 with and withoutsmoothing with artificial unknowns (Collins, 2003),and as a 1-step versus a 2-step decision in whichinstances are first discriminated by a binary classi-fier, so that only positive instances are passed on tobe classified for a subclass.
The performance of thevarious classifiers is given in Table 4.
Although theoverall F1-measure does not notably differ from thatachieved by direct projection, we observe a drasticgain in precision, albeit at the cost of recall.
Withalmost 84% and 90% precision, this is an ideal start-ing point for a bootstrapping procedure.6 Discussion and Future WorkClearly, the?essentially unsupervised?projectionframework presented here does not produce state-of-the-art annotations.
But it does provide an inex-6Note that slink instances are constructed for event pairs, asopposed to event and timex instances, which are constructed forindividual words.7The grammatical features have been extracted from analy-ses of the German ParGram LFG grammar (Rohrer and Forst,2006).8We used the opennlp.maxent package,http://maxent.sourceforge.net/.pensive and largely language-independent basis (a)for manual correction, and (b) for bootstrapping al-gorithms.
In the future, we will investigate howweakly supervised machine learning techniques likeco-training (Blum and Mitchell, 1998) could furtherenhance projection, e.g.
taking into account a thirdlanguage in a triangulation setting (Kay, 1997).AcknowledgementsWe would like to thank Sebastian Pado?
for provid-ing us with the aligned Europarl data, Inderjeet Maniand Marc Verhagen for access to the TARSQI tools,and James Pustejovsky for clarification of TimeMLissues.
We would also like to thank the three anony-mous reviewers for helpful comments.ReferencesAvrim Blum and Tom Mitchell.
1998.
Combining La-beled and Unlabeled Data with Co-Training.
In Pro-ceedings of the 1998 Conference on ComputationalLearning Theory, pages 92?100, July.Michael Collins.
2003.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Computational Lin-guistics, 29(4):589?637, December.Bonnie J. Dorr.
1994.
Machine Translation Divergences:A Formal Description and Proposed Solution.
Com-putational Linguistics, 20(4):597?635.Lisa Ferro, Laurie Gerber, Inderjeet Mani, Beth Sund-heim, and George Wilson, 2005.
TIDES 2005 Stan-dard for the Annotation of Temporal Expressions,September.Rebecca Hwa, Philip Resnik, Amy Weinberg, and OkanKolak.
2002.
Evaluating Translational Correspon-dence using Annotation Projection.
In Proceedings ofACL-2002, Philadelphia, PA.R.
Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas,and Okan Kolak.
2005.
Bootstrapping Parsers viaSyntactic Projection across Parallel Texts.
NaturalLanguage Engineering, 11(3):311?325.Martin Kay.
1997.
The Proper Place of Men and Ma-chines in Language Translation.
Machine Translation,12(1-2):3?23.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceedingsof HLT/NAACL 2003, pages 127?133.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings of theMT Summit 2005.495Inderjeet Mani and George Wilson.
2000.
Robust Tem-poral Processing of News.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics (ACL-2000), pages 69?76, Hong Kong.Inderjeet Mani, Barry Schiffman, and Jianping Zhang.2003.
Inferring Temporal Ordering of Events in News.In Proceedings of the Human Language TechnologyConference (HLT-NAACL-2003).
Short paper.Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong MinLee, and James Pustejovsky.
2006.
MachineLearning of Temporal Relations.
In Proceedings ofACL/COLING 2006, pages 753?760, Sydney, Aus-tralia.Franz J. Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Sebastian Pado?
and Mirella Lapata.
2005.
Cross-lingualprojection of role-semantic information.
In Proceed-ings of HLT/EMNLP 2005, Vancouver, BC.Sebastian Pado?
and Mirella.
Lapata.
2006.
Optimal con-stituent alignment with edge covers for semantic pro-jection.
In Proceedings of ACL-COLING 2006, Syd-ney, Australia.Sebastian Pado?.
2007.
Cross-Lingual Annotation Pro-jection Models for Role-Semantic Information.
Ph.D.thesis, Saarland University, Saarbru?cken, Germany.James Pustejovsky, Jose?
Castan?o, Robert Ingria, RoserSaur?
?, Robert Gaizauskas, Andrea Setzer, and GrahamKatz.
2003a.
TimeML: Robust Specification of Eventand Temporal Expressions in Text.
In Proceedingsof the Fifth International Workshop on ComputationalSemantics.James Pustejovsky, Patrick Hanks, Roser Saur?
?, AndrewSee, Robert Gaizauskas, Andrea Setzer, DragomirRadev, Beth Sundheim, David Day, Lisa Ferro, andMarcia Lazo.
2003b.
The TimeBank Corpus.
In Pro-ceedings of Corpus Linguistics, pages 647?656.Christian Rohrer and Martin Forst.
2006.
Improvingcoverage and parsing quality of a large-scale LFG forGerman.
In Proceedings of LREC 2006, pages 2206?2211, Genoa, Italy, May.Estela Saquete, Patricio Mart?
?nez-Barco, Rafael Mun?oz,Matteo Negri, Manuela Speranza, and Rachele Sprug-noli.
2006.
Multilingual Extension of a TemporalExpression Normalizer using Annotated Corpora.
InProceedings of the EACL 2006 Workshop on Cross-Language Knowledge Induction, Trento, Italy, April.Roser Saur?
?, Robert Knippen, Marc Verhagen, andJames Pustejovsky.
2005a.
Evita: A Robust EventRecognizer For QA Systems.
In Proceedings ofHLT/EMNLP 2005, pages 700?707.Roser Saur?
?, Jessica Littman, Bob Knippen, RobertGaizauskas, Andrea Setzer, and James Pustejovsky,2005b.
TimeML Annotation Guidelines Version 1.2.1,October.Roser Saur?
?, Marc Verhagen, and James Pustejovsky.2006.
SlinkET: A Partial Modal Parser for Events.
InProceedings of LREC-2006, Genova, Italy, May.
Toappear.Amanda Schiffrin and Harry Bunt.
2006.
Defining apreliminary set of interoperable semantic descriptors.Technical Report D4.2, INRIA-Loria, Nancy, France,August.Andrea Setzer.
2001.
Temporal Information in NewswireArticles: an Annotation Scheme and Corpus Study.Ph.D.
thesis, University of Sheffield, Sheffield, UK.Kathrin Spreyer.
2007.
Projecting Temporal AnnotationsAcross Languages.
Diploma thesis, Saarland Univer-sity, Saarbru?cken, Germany.Christoph Tillmann.
2003.
A Projection Extension Algo-rithm for Statistical Machine Translation.
In MichaelCollins and Mark Steedman, editors, Proceedings ofthe 2003 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2003), pages 1?8.Zeno Vendler, 1967.
Linguistics in Philosophy, chapterVerbs and Times, pages 97?121.
Cornell UniversityPress, Ithaca, NY.Marc Verhagen, Inderjeet Mani, Roser Sauri, RobertKnippen, Jessica Littman, and James Pustejovsky.2005.
Automating Temporal Annotation withTARSQI.
In Proceedings of the ACL-2005.David Yarowsky and Grace Ngai.
2001.
Inducing Mul-tilingual POS Taggers and NP Bracketers via RobustProjection across Aligned Corpora.
In Proceedings ofNAACL-2001, pages 200?207.David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing Multilingual Text Analysis Tools viaRobust Projection across Aligned Corpora.
In Pro-ceedings of HLT 2001, First International Conferenceon Human Language Technology Research.496
