In: Proceedings of CoNLL-2000 and LLL-2000, pages 136-138, Lisbon, Portugal, 2000.A Context Sensitive Maximum Likelihood Approach to ChunkingChr i s te r  JohanssonElectrotechnical Laboratories Machine Understanding Division1-1-4 Umezono, Tsukuba.
305 Ibaraki, JAPAN1 In t roduct ionIn Brill's (1994) groundbreaking work on parts-of-speech tagging, the starting point was to as-sign each word its most common tag.
An ex-tension to this first step is to utilize the lexicalcontext (i.e., words and punctuation) surround-ing the word.
This approach could obviously beused for ordering tags into higher order units(referred to as chunks) using chunk :labels.This paper will investigate the performanceof simply picking the most likely tag for a givencontext, under the condition that a larger con-text is allowed to override the most likely la-bel of a smaller context.
The results could beextended by secondary error correction as inBrill's tagger, but this exercise is left to thereader to allow us to concentrate on the perfor-mance based on storing and retrieving the mostlikely examples only.More sophisticated methods may' use morethan one stored context to determine the la-bel that best fits the current context (Van denBosch and Daelemans, 1998; Zavrel and Daele-mans, 1997; Skousen, 1989, inter al.).
Themethod of this paper uses only one context odetermine the best label, but may decrease thesize of the context until a full match is found.2 Out l ine  o f  the  procedure2.1 "Training"The training of this mechanism is to determinewhich patterns in the training set are the mostlikely.
Only tag information is used.
A filter toconvert a tag with a context into a chunk-labelis constructed as follows:0) Construct symmetric n-contexts from thetraining corpus.
A 1-context is simply the mostcommon chunk-label for each tag.
A 3-contextis the tag followed by the tag before and afterit, i.e., \[to t-1 t+l\]:label.
Similarly, a 5-context,(i.e., \[t-2 \[t-1 \[to \] t+l\] t+2\]: label (of to)), isrepresented \[to t-1 t+l t-2 t+2\]:label.
Finally, a7-context is represented as \[to t-1 t+l t-2 t+2t-3 t+3\]:label.
It was verified that results do notsignificantly improve using larger contexts than5-contexts.1) For each set of n-contexts, determinethe most frequent label for each occurring n-context.
For example, the tag CC most fre-quently has the label B-NP if the context isPRP CC RP.
The most frequent label for CCwithout extra context is "0".2) To save some storage space, the most fre-quent label in an n-context is only added if itis different from its nearest lower order context.For example, the label B-NP can be added fora 3-context since PRP CC RP gives a differentresult from CC alone.2.2 Test ingTesting is done by constructing the maximumcontext for each tag, and look it up in thedatabase of the most likely patterns.
If thelargest context cannot be found the context isdiminished step-by-step.3) In the test phase we need to form thelongest contexts used in training (e.g., 7-contexts).
The first word to get a chunk labelis 'Rockwell' (Rockwell International Corp. 's)and its corresponding 7-context (without its la-bel) is NNP = NNP = NNP = POS, where '='is a tag for a blank line (i.e., no text tag) sincethis is the very first few words.4) The only rule for chunk-labeling is to lookup the closest surviving n-context and outputits label.
Simply look up \[to t-1 t+l t-2 t+2 t-3t+3\] ... \[to\] in that order until the context isfound.
The \[to\] context alone produces a F~=iof 77.1363 Resu l tsThe evaluation program shows that this simpleprocedure reaches its best result for 5-contexts(table 1) with 92.46% label accuracy and phrasecorrectness measured by FZ=i = 87.23.
How-ever, the improvement from 3-contexts to 5-contexts is insignificant, as 3-contexts reached92.41% accuracy and F~=1=87.09.
The resultsfor 7-contexts is almost identical to 5-contexts(92.44% and FZ=1=87.21).
This is taken as thelimit performance due to the size of the trainingcorpus.In a larger training corpus, the most commonlonger contexts are likely to be useful but in asmall set the longer contexts may occur withvery low frequencies making it hard to deter-mine if the label of such contexts is the bestguess for unseen samples.These results are the best that could be ex-pected without generalization.
In order to dobetter, the method has to generalize to unseencontexts, e.g., by using some notion of closematching contexts (instances), to be able to uselonger context even when some of that contexthas not been previously recorded.
In addition,the tag-structure could be productively utilized.The presented method has treated all labels asarbitrary, atomic and independent symbols.3.1 Computat iona l  complex i tyUsing rule 2 from section 2.1, 45 patterns 'sur-vived' for 1-contexts, and 3225, 71022, 38541for 3-,5- and 7-contexts respectively, i.e., a totalof 45, 3270, 74292, 109563 using all contexts upto and including 1-, 3-, 5- and 7-contexts.
Eachunique context can be retrieved in one logicalstep (i.e., a hash-table lookup).
There are obvi-ously many patterns in the database - but thecomplexity of the task is limited to the numberof look-ups necessary.There is a maximum of four hash-table look-ups for each tag (i.e., when the 7-, 5-, and 3-contexts does not exist in the database the mostlikely label of the current tag will be used).Good performance can be obtained within amaximum of 2 look-ups for each label (i.e., us-ing only 1- and 3-contexts) and the best resultswere obtained with a maximum of 3 look-upsper label.4 D iscuss ionThe memory-based approach seemingly postu-lates innate tags in the processing machinery.The author has found very little discussion onhow the tags are thought o correspond to real-ity, a fact that was also pointed out, not so longago, by Palmeri (1998).
However, a few papersaiming towards automatic 'label', 'feature' or'tag' creation are available (Miikkulainen andDyer, 1991; Johansson, 1999).It is undeniable that, from a practical per-spective, it is possible to reach very high perfor-mance on tasks, such as tagging, that demanda choice from a known set of alternatives byestimating statistical properties (e.g., the mostlikely label) from a large enough training set.This makes the method extremely useful forquick development of tools, which can be usedin practical applications uch as text retrievaland machine translation; but also in linguis-tic research; e.g., finding examples of specificgrammatical constructions in large collectionsof data.A challenge for future research is how tagscould be constructed automatically, and whatkind of information would be necessary to de-tect the relevant ag dimensions for some lin-guistically motivated task.5 Conc lus ionIt was shown that using context made it possibleto improve performance of maximum likelihoodprediction.
It was suggested that the limit ofperformance for this method is implicitly givenby the size of the training set, as this determinesthe significance of larger contexts, and increasesthe chance of finding a matching longer con-text.
In smaller collections, large patterns area) likely to occur at a low frequency with fewcompeting labels and b) likely to not exist inthe test set.
A larger collection will increasethe number of different contexts, as well as thesignificance of picking the best, most frequent,prediction from a set of (identical) competitorswith different labels.The presented method oes not generalize be-yond what is recorded in the training set as themost likely alternative.
However, it is expectedto ?
improve with the size of the training set, asthis makes it feasible to use longer contexts, and?
have a low computational complexity, as the137process is always limited to use a low' number ofhash table look-ups (determined by the largestsize of context).
Training is limited to detectingthe most likely outcome of each context (i.e., asorting operation).ReferencesAntal van den Bosch and Walter Daelemans.
1998.Do not forget: Full memory in memory-basedlearning of word pronunciation.
In D.M.W.
Pow-ers, editor, proceedings of NeMLap3/CoNLL98,pages 195-204, Sydney, Australia.Eric Brill.
1994.
Some advances in rule-based partof speech tagging.
In proceedings ofAAAI.Christer Johansson.
1999.
Noise resistance in pro-cessing center-embedded clauses: A question ofrepresentation?
I  proceedings ofICCS'99, pages253-258, Tokyo.
Waseda University.Risto Miikkulainen and Michael G. Dyer.
1991.Natural anguage processing with modular PDPnetworks and distributed lexicon.
Cognitive Sci-ence, 15(3):343-399.Thomas J. Palmeri.
1998.
Formal models and fea-ture creation.
Behavioral and Brain Sciences,21:33-34.Royal Skousen.
1989.
Analogical Modeling of Lan-guage.
Kluwer Academic, Dordrecht, he Nether-lands.Jakub Zavrel and Walter Daelemans.
1997.
Memorybased learning: using similarity for smoothing.In proceedings of the 35th annual meeting of theAssociation of Computational Lingui.stics (ACL)and the 8th conference of the European Chapterof the ACL, pages 436-443, Madrid, Spain.test dataADJPADVPCONJPINTJLSTNPPPPRTSBARVPprecision58.33%67.98%0.00%33.33%0.00%88.09%88.18%36.14%54.97%88.27%all 86.24% 88.25%recall FZ=i52.74% 55.4071.59% 69.740.00% 0.0050.00% 40.000.00% 0.0090.53% 89.3093.39% 90.7128.30% 31.7533.08% 41.3191.28% 89.7587.23Table 1: Results using at most 5-contexts138
