Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 193?201,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsEffect of small sample size on text categorization with support vectormachinesPawe?
MatykiewiczBiomedical InformaticsCincinnati Children?s Hospital3333 Burnet AveCincinnat, OH 45220, USApawel.matykiewicz@gmail.comJohn PestianBiomedical InformaticsCincinnati Children?s Hospital3333 Burnet AveCincinnat, OH 45220, USAjohn.pestian@cchmc.orgAbstractDatasets that answer difficult clinical ques-tions are expensive in part due to the need formedical expertise and patient informed con-sent.
We investigate the effect of small samplesize on the performance of a text categoriza-tion algorithm.
We show how to determinewhether the dataset is large enough to trainsupport vector machines.
Since it is not pos-sible to cover all aspects of sample size cal-culation in one manuscript, we focus on howcertain types of data relate to certain proper-ties of support vector machines.
We show thatnormal vectors of decision hyperplanes canbe used for assessing reliability and internalcross-validation can be used for assessing sta-bility of small sample data.1 IntroductionEvery patient visit generates data, some on paper,some stored in databases as structured form fields,some as free text.
Regardless of how they arestored, all such data are to be used strictly for pa-tient care and for billing, not for research.
Patienthealth records are maintained securely according tothe provisions of the Health Insurance Portabilityand Accountability Act (HIPAA).
Investigators mustobtain informed consent from patients whose datawill be used for other purposes.
This means defin-ing which data will be used and how they will beused.
In addition to writing protocols and obtain-ing consent from patients, medical experts must ei-ther manually codify important information or teacha machine how to do it.
All of these labor-intensivetasks are expensive.
No one wants to collect moredata than is necessary.Our research focuses on answering difficult neu-ropsychiatric questions such as, ?Who is at higherrisk of dying by suicide??
or ?Who is a goodcandidate for epilepsy surgery evaluation??
Largeamounts of data that might answer these questionsexist in the form of text dictated by clinicians orwritten by patients and thus unavailable.
Parallelto the collection of such data, we explored whethersmall datasets can be used to build reliable methodsof making this information available.
Here, we in-vestigate how text classification training size relatesto certain aspects of linear support vector machines.We hypothesize that a sufficiently large training sub-set will generate stable and reliable performance es-timates of a classifier.
On the other hand, if thedataset is too small, then even small changes tothe training size will change the performance of aclassifier and manifest unstable and unreliable esti-mates.
We introduce quantitive definitions for sta-bility and reliability and give empirical evidence onhow they work.2 BackgroundHow much data is needed for reliable and stableanalysis?
This question has been answered for mostunivariate problems, and a few solutions exist formultivariate problems, but no widely accepted an-swer is available for sparse and high-dimensionaldata.
Nonetheless, we will review the few samplesize calculation methods that have been used for ma-chine learning.193Hsieh et al (1998) described a method for calcu-lating the sample size needed for logistic and lin-ear regression models.
The multivariate problemwas simplified to a series of univariate two-sample t-tests on the input variables.
A variance inflation fac-tor was used to correct for the multi-dimensionalitywhich quantifies the severity of multicollinearity inthe least squares regression: collinearity deflatesand non-collinearity inflates sample size estima-tion.
Computer simulations were done on low-dimensional and continuous data, so it is not knownwhether the method is applicable to text categoriza-tion.Guyon et al (1998) addressed the problem of de-termining what size test set guarantees statisticallysignificant results in a character recognition task, asa function of the expected error rate.
This methoddoes not assume which learner will be used.
Instead,it requires specific parameters that describe hand-writing data collection properties such as between-writers variance and within-writer variance.
Thedownside of this method is that it must assume theworst-case scenario: a large variance in data and alow error rate for the classifier.
For this reason largerdatasets are recommended.Dobbin et al (2008) and Jianhua Hu (2005) fo-cused only on sample size for a classifier that learnsfrom gene expression data.
No assumptions weremade about the classifier, only about the data struc-ture.
All gene expressions were measured on a con-tinuous scale that denotes some luminescence cor-responding to the relative abundance of nucleic acidsequences in the target DNA strand.
The data, re-gardless of size, can be qualified using just one pa-rameter, fold change, which measures changes in theexpression level of a gene under two different con-ditions.
Furthermore, the fold change can be stan-dardized for compatibility with other biological ex-periments: with a lower standardized fold change,more samples are needed, and with more genes,more samples are needed.
There is a strong assump-tion about data makeup, but no assumption is madeabout the classifier.
This solution allows for smallsample sizes but does not generalize to text classifi-cation data.Way et al (2010) evaluated the performance ofvarious classifiers and featured a selection techniquein the presence of different training sample sizes.Experiments were conducted on synthetic data, withtwo classes drawn from multivariate Gaussian dis-tributions with unequal means and either equal orunequal covariance matrices.
The conclusion wasthat support vector machines with a radial kernelperformed slightly better than the LDA when thetraining sample size was small.
Only certain combi-nations of feature selection and classification meth-ods work well with small sample sizes.
We will usesimilar assumptions for sparse and high-dimensionaldata.Most recently, Juckett (2012) developed a methodfor determining the number of documents needed fora gold standard corpus.
The sample size calculationwas based on the concept of capture probabilities.It is defined as the normalized sum of probabilitiesover all words of interest.
For example, if the re-quired capture probability is 0.95 for a set of med-ical words, when using larger corpora that containthese words, it must first be calculated how manydocuments are needed to capture the same probabil-ity in the target corpus.
This method is specific tolinguistic research on annotated corpora, where theprobabilities of individual words in the sought cor-pora must match the probabilities of words in thetarget domain.
This method focuses solely on thedata structure and does not assume an algorithm orthe task that it will serve.
The downside is a highersample size.When reviewing various methods for sample sizecalculation, we found that as more assumptions canbe made, fewer data are needed for meaningful anal-ysis.
Assumptions can be made about data structureand quality, the task the data serve, feature selection,and the classifier.
Our approach exploits a scenariowhere the task, the feature selection, and the classi-fier are known.3 DataWe used four data sets to test our hypothesis: ver-sicolor and virginica samples from the Iris dataset(VV), newswires about corn and wheat from theModApte split of the Reuters-21578 dataset (WCTand WCE), suicide notes reprinted in Shneidmanand Farberow (1957) (SN), and ubiquitous question-naire patient interviews (UQ).
Properties of thesedata are summarized in Table 1.194The first dataset was created by Anderson (1935)and introduced to the world of statistics by Fisher(1936).
Since then it has been used on countless oc-casions to benchmark machine learning algorithms.Each row of data has four variables to describe theshape of an iris calyx: sepal length, sepal width,petal length, and petal width.
The dataset contains50 measurements for each of three subspecies of theiris flower: setosa, versicolor, and virginica.
Allmeasurements of the setosa calyx are separable fromthe rest of the data and thus were not used in our ex-periments.
Instead, we used data corresponding toversicolor and virginica (VV), which is more inter-esting because of a small class overlap.
The noise isintroduced mostly by sepal width and sepal length.The second dataset was created by Lewis andRinguette (1994) and is the one most commonlyused to benchmark text classification algorithms.The collection is composed of 21,578 short newsstories from the Reuters news agency.
Some storieshave manually assigned topics, like ?earn,?
?acq,?
or?money-fx,?
and others do not.
In order to make thedataset comparable across different uses, a ?Modi-fied Apte?
(?ModApte?)
split was proposed by Apte?et al (1994).
It has 9,603 training and 3,299 exter-nal testing documents, a total of 135 distinct topics,with at least one topic per document.
The most fre-quent topic is ?earn,?
which appears in 3,964 docu-ments.
Here, we used only the ?wheat?
and ?corn?categories, which appear 212 and 181 times in thetraining set alng with 71 and 56 cases in the testset.
These topics are semantically related, so it isno surprise that 59 documents in the training setand 22 documents in test set have both labels.
Thisgives a total of 335 unique training instances and105 unique test instances.
Interestingly, it is eas-ier to distinguish ?corn?
news from ?not corn justwheat?
news than it is to distinguish ?wheat?
from?not wheat just corn.?
The latter seems to be a gooddataset for benchmarking sample size calculation.We will refer to the ?wheat?
versus ?not wheat?training set as WCT and the ?wheat?
versus ?notwheat?
external test set as WCE.The third dataset was extracted from the appendixin Shneidman and Farberow (1957).
It contains 66suicide notes (SN) organized into two categories: 33genuine and 33 simulated.
The authors of the noteswere matched in both groups by gender (male), race(white), religion (Protestant), nationality (native-born U.S. citizens), and age (25-59).
Authors of thesimulated suicide notes were screened for personal-ity disorders or tendencies toward morbid thoughtsthat would exclude them from the study.
Individu-als enrolled in the study were asked to write a sui-cide note as if they were going to take their own life.Notes were anonymized, digitized, and prepared fortext processing (Pestian et al, 2010).The fourth dataset was collected in a clinical con-trolled trial at Cincinnati Children?s Hospital Med-ical Center Emergency Department.
Sixty patientswere enrolled, 30 with suicidal behavior and 30 con-trols from the orthopedic service.
The suicidal be-havior group comprised 15 females and 15 maleswith an average age of ?
15.7 years (SD ?
1.15).The control group included 15 females and 15 maleswith an average age of ?
14.3 years (SD ?
1.21).The interview consisted of five open-ended ubiqui-tous questions (UQ): ?Does it hurt emotionally??
?Do you have any fear??
?Are you angry??
?Doyou have any secrets??
and ?Do you have hope?
?The interviews were recorded in an audio format,transcribed by a medical transcriptionist, and pre-pared for analysis by removing the sections of theinterview where the questions were asked.
To pre-serve the UQ structure, n-grams from each of thefive questions were separated (Pestian et al, 2012).VV SN UQ WCT WCESamples (m) 100 66 60 335 105Classes 2 2 2 2 2Class balance 100% 100% 100% 58% 48%Min row freq 100 2 2 3 0Max row freq 100 66 60 335 105Min cell value 1 0 0 0 0Max cell value 7.9 102.045 64 117 892Features (n) 4 60 7,282 7,132 7,132Sparsity 0% 60% 92.3% 97% 98%Table 1: Four very different benchmark data: versicolorand virginica (VV) from iris data, representing a dense,low-dimensional dataset; suicide notes (SN) from Cluesto Suicide (Shneidman and Farberow, 1957), represent-ing a mildly sparse, high-dimensional dataset; ubiquitousquestionnaires, (UQ) representing a sparse, extremelyhigh-dimensional dataset; and ?wheat?
versus ?not wheatjust corn?
(WCT and WCE) from the ?ModApte?
splitof Reuters-21578 data, representing an unbalanced, ex-tremely sparse, high-dimensional dataset.1954 MethodsFeature extraction.
Every text classification algo-rithm starts with feature engineering.
Documentsin the UQ, WCT, and WCE sets were representedby a bag-of-n-grams model (Manning and Schuetze,1999; Manning et al, 2008).
Every document wastokenized, and frequencies of unigrams, bigrams,and trigrams were calculated.
All digit numbersthat appeared in a document were converted to thesame token (?NUMB?).
Documents become rowvectors and n-grams become column vectors in alarge sparse matrix.
Each n-gram has its own dimen-sion, with the exception of UQ data, where n-gramsare represented separately for each of the five ques-tions.
Neither stemming nor a stop word list wereapplied to the textual data.
Suicide notes (SN) werenot represented by n-grams.
In previous studies, wefound that the structure of the note and its emotionalcontent are indicative of suicidality, not its seman-tic content.
Hence, the SN dataset is representedby the frequency of 23 emotions assigned by men-tal health professionals, the frequency of 34 parts ofspeech, and by three readability scores: Flesch, Fog,and Kincaid.Feature weighting.
Term weighting was chosenad hoc.
UQ, WCT, and WCE had a logarithmicterm frequency (log-tf) as local weighting and an in-verse document frequency (idf) as global weightingbut were derived only from the training data (Saltonand Buckley, 1988; Nakov et al, 2001).Feature selection.
To speed up calculations, theleast frequent features were removed from the SN,UQ, WCT, and WCE datasets (see minimum rowfrequency in Table 1).
Further optimization of thefeature space was done using an information gainfilter (Guyon and Elisseeff, 2003; Yang and Peder-sen, 1997).
Depending on the experiment, some ofthe features with the lowest information gain wereremoved.
For example, IG = 0.4 means that 40%of the features, those with a higher information gain,were kept, and the other 60%, those with a lower in-formation gain, were removed.
Lastly, all row vec-tors in UQ, WCT, and WCE were normalized tounit length (Joachims, 1998).Learning algorithm.
We used linear support vec-tor machines (SVM) to learn from the data.
Sup-port vector machines are described in great detail inFigure 1: Normal vector w of a hyperplane.Schlkopf and Smola (2001).
We will focus on justtwo aspects: properties of the normal vector of de-cision hyperplane (see Figure 1) and internal cross-validation (see Figure 2).
SVM is in essence a sim-ple linear classifier:f(x) = sgn(?w,x?+ b) (1)where x is an input vector that needs to be classified,?
?, ??
is the inner product, w is a weight vector withthe same dimensionality as x, and b is a scalar.
Thefunction f outputs +1 if x belongs to the first classor ?1 if x belongs to the second class.
SVM differsfrom other linear classifiers on how w is computed.Contrary to other classifiers, it does not solve w di-rectly.
Instead, it uses convex optimization to findvectors from the training set that can be used for cre-ating the largest margin between training examplesfrom the first and second class.
Hence, the solutionto w is in the form of the linear combination of co-efficients and training vectors:w =m?i=1?iyixi (2)where m is the number of training vectors, ?i ?
0are Lagrange multipliers, yi ?
{?1, 1} are numer-ical codes for class labels, and xi are training rowvectors.
Vector w is perpendicular to the decisionboundary, and its proper name in the context ofSVM is the normal vector of decision hyperplane1(see Figure 1).
One of the properties of SVM is thatoutlying training vectors are not used in w. Thesevectors have the corresponding coefficient ?i = 0.In fact, these vectors can be removed from the train-ing set and the convex optimization procedure will1If R with SVM from the e1071 package is used,the command to obtain the normal vector is w =c(t(model$coefs)% ?%model$SV).196result in exactly the same solution.
We can use thisproperty to probe how reliable training data are forthe classification task.
If we have enough data thatwe can randomly remove some, what is left will re-sult in w?
?
w. On the other hand, if we do nothave enough data, then random removal of trainingdata will result in a very different equation, becausethe decision boundary changes and w?
6= w.Reliability of performance.
The relationship be-tween w?
and w can be measured.
We introduce theSVM reliability index (SRI):SRI(w?,w) = |r(w?,w)| (3)=|?ni=1(w?i ?w?
)(wi ?w)|?
?ni=1(w?i ?w?)2?
?ni=1(wi ?w)2which is the absolute value of the Pearson product-moment correlation coefficient between convex op-timization solution w?
corresponding to a trainingsubset and w corresponding to the full dataset2.Pearson?s correlation coefficient discovers linear de-pendency between two normally distributed randomvariables and has its domain on a continuous seg-ment between ?1 and +1.
In our case, we arelooking for a strong linear dependency between con-stituents of the training weight vector w?i and con-stituents of the full dataset weight vector wi.
Somenumerical implementations of SVM cause the out-put values for the class labels to switch.
We cor-rected for this effect by applying absolute value tothe Pearson?s coefficient, resulting in SRI ?
[0, 1].We did not have a formal proof on how SRI relatesto SVM performance.
Instead, we showed empir-ical evidence for the relationship based on a fewsmall benchmark data.
Stability of performance.SVM generalization performance is usually mea-sured using cross-validation accuracy.
In particu-lar, we use balanced accuracy because it gives bet-ter evidence for a drop in performance when solvingunbalanced problems.
Following Guyon and Elis-seeff (2003) and many others, we divided the datainto three sets: test, training, and validation.
Meantest balanced accuracy aT is estimated using strati-fied Monte Carlo cross-validation (MCCV), where2We experimented with Pearson?s correlation, Spearman?scorrelation, one-way intraclass correlation, Cosine correlation,Cronbach?s coefficient, and Krippendorff?s coefficients andfound that Pearson?s correlation coefficient works well withboth low-dimensional and high-dimensional spaces.Figure 2: Estimation and resampling: mean test balancedaccuracy and mean validation balanced accuracy shouldmatch.
To prevent overfitting, tuning machine learningshould be guided by mean validation accuracy and con-firmed by mean test accuracy.
This procedure requiresthe ?develop?
set to be large enough to give reliable andstable estimates.the proportion of the training set to the test set isvaried between 0.06 and 0.99.
Mean validation bal-anced accuracy aV (MVA) is estimated using K-fold cross-validation (also known as internal cross-validation), where K = m2 and m is the numberof training cases.
In the case of the ?wheat?
versus?not wheat just corn?
dataset, we have, in addition,the external validation set WCE and correspondingmean external balanced accuracy aE .
Correct esti-mation of the learner?s generalization performanceshould result in all three accuracies being equal:aT ?
aV ?
aE .
Furthermore, we want all three ac-curacies to be the same regardless of the amount ofdata.
If we have enough data that we can randomlyremove some, what is left will result in aV??
aV?
?.On the other hand, if we do not have enough data,then random removal of training data will result invery different accuracy estimations: aV?6= aV?
?.Sample size calculation.
We do not have a goodway of predicting how much data will be needed tosolve a problem with a small p-value, but this is amatter of convenience.
Rather than looking to thefuture, we can simply ask if what we have now isenough.
If we can build a classifier that gives re-liable and stable estimates of performance, we canstop collecting data.
Reliability is measured by SRI,while stability is measured by MVA, not as a singlevalue but merely as a function of the training size:SRI(t) = |r(wtm,wm)| and (4)aT (t) = aTtm(5)where t is a proportion of the training data, t ?
(0, 1), m is size of the full dataset, and tm is theactual number of training instances.
To quantify the197ability of the dataset to produce classification mod-els with reliable and stable performance estimates,we need two more measures: sample dispersion ofSRI and sample dispersion of MVA:cSRI(t ?
p) =sSRI(t?p)SRI(t ?
p)and (6)cMVA(t ?
p) =saT (t?p)aT (t ?
p)(7)defined as the coefficient of variation of all SRI orMVA measurements for training data sizes greaterthan pm?.
For example, we want to know if our 10-fold cross-validation (CV) for a dataset that has 400training samples is reliable and stable.
10-fold CVis 0.9 of training data, so we need to measure SRIand MVA for different proportions of training data,t = {0.90, 0.91, .
.
.
, 0.99}, and then calculate dis-persion for cSRI(t ?
0.9) and cMVA(t ?
0.9).
Nu-merical calculations will give us sense of good andbad dispersion across different datasets.5 ResultsDo I have enough data?
The first set of experi-ments was done with untuned algorithms.
We set theSVM parameter to C = 1 and did not use any fea-ture selection.
Figure 3 shows four examples of howSVM performance depends on the training set size.The performance was measured using mean test bal-anced accuracy, MVA, and SRI.
Numerical calcu-lations showed that VV needs at least 30 randomlyselected training examples to produce reliable andstable results with high accuracy.
cSRI(t ?
0.75)is 0.005 and cMVA(t ?
0.75) is 0.016.
SN wasnot encouraging regarding the estimated accuracy;SRI dropped, suggesting that the SVM decision hy-perplanes are unreliable.
Mental health profession-als can distinguish between genuine and simulatednotes about 63% of time.
Machine learning doesit correctly about 73% of time if text structure andemotional content are used.
Even so, the samplesize calculation yields high dispersion (cSRI(t ?0.75) = 0.134 and cMVA(t ?
0.75) = 0.082).UQ is small and high-dimensional, and yet the re-sults were reliable and stable (cSRI(t ?
0.75) =0.015 and cMVA(t ?
0.75) = 0.023).
Patientsenrolled in the UQ study also received the Sui-cide Ideation Questionnaire (Raynolds, 1987) andthe Columbia-Suicide Severity Rating Scale (Pos-ner et al, 2011).
We found that UQ was no dif-ferent from the structured questionnaires.
UQ de-tects suicidality mostly by emotional pain and hope-lessness, which were mildly present in four controlpatients.
Other instruments returned errors becausethe same few teenagers reported risky behavior andmorbid thoughts.
WCT produced reliable and sta-ble accuracy estimates, but no large amounts of datacould be removed (cSRI(t ?
0.75) = 0.010 andcMVA(t ?
0.75) = 0.053).
It seems that WCEis somehow different from WCT, or it might be acase of overfitting, which causes the mean test ac-curacy to diverge from MVA as the training datasetgets smaller.
Algorithm tuning.
No results shouldbe regarded as satisfactory until a thorough param-eter space search has been completed.
Each step ofa text classification algorithm can be improved.
Toattempt a complete description of the dependencyof a minimal viable sample size on text classifica-tion would be both impossible and futile, since newmethods are discovered every day.
However, to startsomewhere, we focused only on the feature selectionand SVM parameter C 3.
Feature selection removesnoise from data.
Parameter C informs the convexoptimization process about the expected noise level.If both parameters are set correctly, we should seean improvement in the reliability and stability ofthe results.
There are several methods for tuningSVM; the most commonly used but computation-ally expensive is internal cross-validation (Duan etal., 2003; Chapelle et al, 2002).
Figure 5 showsthe results of the parameter tuning procedure.
VVand SN are not extremely high-dimensional, so wetuned just parameter C. MVA maxima were foundat C = 0.45 with VV, C = 0.05 with SN, C = 0.4and IG = 0.1584 with UQ, and C = 2.5 andIG = 0.8020 with WCT.
Do I have enough dataafter algorithm tuning?
Internal cross-validation(MVA) did not improve dispersion universally (seeTable 2).
VV improved on reliability but not stabil-ity.
SN scored much better on both measures, butwe do not yet know what the cutoff for having alow enough dispersion is.
UQ did worse on all mea-sures after tuning.
WCT improved greatly on mean3Please note that most SVM implementations do not allowfor simultaneous feature selection and internal cross-validation.198VV SN UQ WCT and WCEFigure 3: SRI index (S), MVA accuracy (V) and mean test accuracy (T) averaged over 120 repetitions and differenttraining data sizes.
Linear SVM with C = 1 and no feature selection.
VV (cSRI(t ?
0.75) = 0.005 and cMVA(t ?0.75) = 0.016), UQ (cSRI(t ?
0.75) = 0.015 and cMVA(t ?
0.75) = 0.023), and WCT (cSRI(t ?
0.75) = 0.010and cMVA(t ?
0.75) = 0.053) gave stable and reliable estimates, but SN did not (cSRI(t ?
0.75) = 0.134 andcMVA(t ?
0.75) = 0.082).VV SN UQ WCTFigure 4: MVA (internal cross-validation) parameter tuning results.
Maxima were found at C = 0.45 with VV,C = 0.05 with SN, C = 0.4 and IG = 0.1584 with UQ, and C = 2.5 and IG = 0.8020 with WCT.VV SN UQ WCT and WCEFigure 5: SRI index (S), MVA accuracy (V), and mean test accuracy (T) averaged over 60 repetitions and differenttraining data sizes.
Tuned classification algorithms: VV with C = 0.45 and no feature selection, SN with C = 0.05and no feature selection, UQ with C = 0.4 and IG = 0.1584, and WCT with C = 2.5 and IG = 0.8020.
Stabilityand reliability: VV had cSRI(t ?
0.75) = 0.003 and cMVA(t ?
0.75) = 0.018), SN had cSRI(t ?
0.75) = 0.085and cMVA(t ?
0.75) = 0.075, UQ had cSRI(t ?
0.75) = 0.025 and cMVA(t ?
0.75) = 0.024, and WCT hadcSRI(t ?
0.75) = 0.025 and cMVA(t ?
0.75) = 0.011.199test accuracy, mean external validation, and stabilitydispersion (see Figure 5).
It would be interesting tosee if improvement on both reliability dispersion andstability dispersion would bring mean test accuracyand mean external validation even closer together.aT (t ?
0.75) cSRI(t ?
0.75) cMVA(t ?
0.75)VV no tuning 0.965 0.005 0.016SN no tuning 0.744 0.134 0.082UQ no tuning 0.946 0.015 0.023WCT no tuning 0.862 0.010 0.053VV with tuning 0.970 0.003 0.018SN with tuning 0.755 0.085 0.075UQ with tuning 0.941 0.025 0.024WCT with tuning 0.946 0.025 0.011Table 2: Sample size calculation before and after tuningwith internal cross-validation (MVA).
Even though meantest accuracy (aT (t ?
0.75)) improved for VV, SN, andWCT, reliability and stability did not improve univer-sally.
Internal cross-validation alone might not be ade-quate for tuning classification algorithms for all data.6 DiscussionSample size calculation data for a competitionand for problem-solving.
In general, there might betwo conflicting objectives when calculating whetherwhat we have collected is a large enough dataset.
Ifthe objective is to have a shared task with many par-ticipants and, thus, many unknowns, the best courseof action is to assume the weakest classifier: uni-grams with no feature weighting or selection trainedusing the simplest logistic regression.
On the otherhand, if the problem is to be solved with only oneclassifier and the least amount of data, then thestrongest assumptions about the data and the algo-rithm are required.The fallacy of untuned algorithms.
After yearsof working with classification algorithms to solvedifficult patient care problems, we have found thata large amount of data is not needed; usually sam-ples measured in the hundreds will suffice, but thisis only possible when a thorough parameter spacesearch is conducted.
It seems that reliability andstability dispersions are good measures of how wellthe algorithm is tuned to the data without overfitting.Moreover, we now have a new direction for thinkingabout optimizing classification algorithms: insteadof focusing solely on accuracy, we can also measurethe dispersion and see whether this is a better indi-cator of what would happen with unevaluated data.There is a great deal of data available, but very littlethat can be used for training.What to measure?
VC-bound, span-bound, ac-curacy, F1, reliability, and stability dispersions arejust a few examples of indicators of how well ourmodels fit.
What we have outlined here is howone of the many properties of SVM, the propertyof the normal vector, can be used to obtain insightsinto data.
Normal vectors are constructed using La-grangian multipliers and support vectors; accuracyis constructed using a sign function on decision val-ues.
It is feasible that other parts of SVM may bemore suited to algorithm tuning and calculation ofminimum viable training size.7 ConclusionPower and sample size calculations are very impor-tant in any domain that requires extensive expertise.We do not want to collect more data than necessary.There is, however, a scarcity of research in samplesize calculation for machine learning.
Nonetheless,the existing results are consistent: the more that canbe assumed about the data, the problem and the al-gorithm, the fewer data are needed.We proposed two independent measures for eval-uating whether available datasets are sufficientlylarge: reliability and stability dispersions.
Reliabil-ity dispersion measures indirectly whether the deci-sion hyperplane is always similar and how much itvaries, while stability dispersion measures how wellwe are generalizing and how much variability thereis.
If the sample size is large enough, we shouldalways get the same decision hyperplane with thesame generalization accuracy.With little empirical evidence, we can concludethat classifier performance measured by just a singleK in a cross-validation test is not sufficient.
K mustbe be varied, and other measures must be present,such as the SVM reliability index, that support orcontradict the generalization accuracy estimates.
Wesuggest that other measures for sample size calcula-tion and algorithm tuning may exist and there is stillmuch to be learned about the mechanics of supportvector machines.200ReferencesEdgar Anderson.
1935.
The irises of the gaspe peninsula.Bulletin of the American Iris Society, 59:2?5.Chidanand Apte?, Fred Damerau, and Sholom M. Weiss.1994.
Automated learning of decision rules for textcategorization.
ACM Trans.
Inf.
Syst., 12(3):233?251,July.Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, andSayan Mukherjee.
2002.
Choosing multiple parame-ters for support vector machines.
Machine Learning,46:131?159.Kevin K. Dobbin, Yingdong Zhao, and Richard M. Si-mon.
2008.
How large a training set is needed to de-velop a classifier for microarray data?
Clinical cancerresearch : an official journal of the American Associ-ation for Cancer Research, 14(1):108?114, January.Kaibo Duan, S. Sathiya Keerthi, and Aun Neow Poo.2003.
Evaluation of simple performance measuresfor tuning svm hyperparameters.
Neurocomputing,51:41?59.Ronald A. Fisher.
1936.
The use of multiple measure-ments in taxonomic problems.
Annals of Eugenics,7:179?188.Isabelle Guyon and Andre Elisseeff.
2003.
An introduc-tion to variable and feature selection.
J. Mach.
Learn.Res., 3:1157?1182, March.Isabelle Guyon, John Makhoul, Richard Schwartz, andVladimir Vapnik.
1998.
What size test set gives gooderror rate estimates?
Pattern Analysis and MachineIntelligence, IEEE Transactions on, 20(1):52?64, Jan-uary.Fushing Y. Hsieh, Daniel A. Bloch, and Michael D.Larsen.
1998.
A simple method of sample size cal-culation for linear and logistic regression.
Statistics inMedicine, 17(14):1623?1634, December.Fred A. Wright Jianhua Hu, Fei Zou.
2005.
Practical fdr-based sample size calculations in microarray experi-ments.
Bioinformatics, 21(15):3264?3272, August.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
In Claire Ndellec and Cline Rouveirol, ed-itors, Machine Learning: ECML-98, volume 1398,pages 137?142.
Springer-Verlag, Berlin/Heidelberg.David Juckett.
2012.
A method for determining the num-ber of documents needed for a gold standard corpus.Journal of Biomedical Informatics, page In Press, Jan-uary.David D. Lewis and Marc Ringuette.
1994.
A com-parison of two learning algorithms for text categoriza-tion.
In Third Annual Symposium on Document Anal-ysis and Information Retrieval, pages 81?93.Christopher D. Manning and Hinrich Schuetze.
1999.Foundations of Statistical Natural Language Process-ing.
The MIT Press, 1 edition, June.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schtze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, 1 edition, July.Preslav Nakov, Antonia Popova, and Plamen Mateev.2001.
Weight functions impact on lsa performance.In EuroConference RANLP?2001 (Recent Advances inNLP), pages 187?193.John Pestian, Henry Nasrallah, Pawel Matykiewicz, Au-rora Bennett, and Antoon Leenaars.
2010.
Suicidenote classification using natural language processing:A content analysis.
Biomedical Informatics Insights,pages 19?28, August.John Pestian, Jacqueline Grupp-Phelan, PawelMatkiewicz, Linda Richey, Gabriel Meyers,Christina M. Canter, and Michael Sorter.
2012.Suicidal thought markers: A controlled trail exam-ining the language of suicidal adolescents.
To BeDetermined, In Preparation.Kelly Posner, Gregory K. Brown, Barbara Stanley,David A. Brent, Kseniya V. Yershova, Maria A.Oquendo, Glenn W. Currier, Glenn A. Melvin, Lau-rence Greenhill, Sa Shen, and J. John Mann.
2011.The ColumbiaSuicide severity rating scale: Initial va-lidity and internal consistency findings from three mul-tisite studies with adolescents and adults.
The Amer-ican Journal of Psychiatry, 168(12):1266?1277, De-cember.William M. Raynolds, 1987.
Suicidal Ideation Question-naire - Junior.
Odessa, FL: Psychological AssessmentResources.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.
In-formation Processing & Management, 24(5):513?523.Bernhard Schlkopf and Alexander J. Smola.
2001.Learning with Kernels: Support Vector Machines,Regularization, Optimization, and Beyond.
The MITPress, 1st edition, December.Edwin S. Shneidman and Norman Farberow.
1957.Clues to Suicide.
McGraw Hill Paperbacks.Ted W. Way, Berkman Sahiner, Lubomir M. Hadjiiski,and Heang-Ping Chan.
2010.
Effect of finite samplesize on feature selection and classification: a simula-tion study.
Medical Physics, 37(2):907?920, February.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In Proceedings of the Fourteenth International Con-ference on Machine Learning, ICML ?97, pages 412?420, San Francisco, CA, USA.
Morgan KaufmannPublishers Inc.201
