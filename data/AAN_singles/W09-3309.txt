Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 57?62,Suntec, Singapore, 7 August 2009.c?2009 ACL and AFNLPConstructing An Anaphorically Annotated Corpus With Non-Experts:Assessing The Quality Of Collaborative Annotations.Jon ChamberlainUniversity of EssexSchool of Computer Scienceand Electronic Engineeringjchamb@essex.ac.ukUdo KruschwitzUniversity of EssexSchool of Computer Scienceand Electronic Engineeringudo@essex.ac.ukMassimo PoesioUniversity of EssexSchool of Computer Scienceand Electronic Engineeringpoesio@essex.ac.ukAbstractThis paper reports on the ongoing workof Phrase Detectives, an attempt to cre-ate a very large anaphorically annotatedtext corpus.
Annotated corpora of the sizeneeded for modern computational linguis-tics research cannot be created by smallgroups of hand-annotators however theESP game and similar games with a pur-pose have demonstrated how it might bepossible to do this through Web collabora-tion.
We show that this approach could beused to create large, high-quality naturallanguage resources.1 IntroductionThe statistical revolution in natural language pro-cessing (NLP) has resulted in the first NLPsystems and components really usable on alarge scale, from part-of-speech (POS) taggersto parsers (Jurafsky and Martin, 2008).
But ithas also raised the problem of creating the largeamounts of annotated linguistic data needed fortraining and evaluating such systems.This requires trained annotators, which is pro-hibitively expensive both financially and in termsof person-hours (given the number of trained an-notators available) on the scale required.Recently, however, Web collaboration hasstarted to emerge as a viable alternative.Wikipedia and similar initiatives have shownthat a surprising number of individuals are willingto help with resource creation and scientificexperiments.
The goal of the ANAWIKI project1is to experiment with Web collaboration as asolution to the problem of creating large-scalelinguistically annotated corpora.
We do this bydeveloping tools through which members of ourscientific community can participate in corpus1http://www.anawiki.orgcreation and by engaging non-expert volunteerswith a game-like interface.
In this paper wepresent ongoing work on Phrase Detectives2,a game designed to collect judgments aboutanaphoric annotations, and we report a firstanalysis of annotation quality in the game.2 Related WorkLarge-scale annotation of low-level linguistic in-formation (part-of-speech tags) began with theBrown Corpus, in which very low-tech and timeconsuming methods were used.
For the cre-ation of the British National Corpus (BNC), thefirst 100M-word linguistically annotated corpus, afaster methodology was developed using prelimi-nary annotation with automatic methods followedby partial hand-correction (Burnard, 2000).Medium and large-scale semantic annotationprojects (for wordsense or coreference) are a re-cent innovation in Computational Linguistics.
Thesemi-automatic annotation methodology cannotyet be used for this type of annotation, as the qual-ity of, for instance, coreference resolvers is notyet high enough on general text.
Nevertheless thesemantic annotation methodology has made greatprogress with the development, on the one end,of effective quality control methods (Hovy et al,2006) and on the other, of sophisticated annotationtools such as Serengeti (St?uhrenberg et al, 2007).These developments have made it possible tomove from the small-scale semantic annotationprojects, the aim of which was to create resourcesof around 100K words in size (Poesio, 2004b),to the efforts made as part of US initiatives suchas Automatic Context Extraction (ACE), Translin-gual Information Detection, Extraction and Sum-marization (TIDES), and GALE to create 1 mil-lion word corpora.
Such techniques could not beexpected to annotate data on the scale of the BNC.2http://www.phrasedetectives.org572.1 Collaborative Resource CreationCollaborative resource creation on the Web offersa different solution to this problem.
The motiva-tion for this is the observation that a group of in-dividuals can contribute to a collective solution,which has a better performance and is more ro-bust than an individual?s solution as demonstratedin simulations of collective behaviours in self-organizing systems (Johnson et al, 1998).Wikipedia is perhaps the best example of col-laborative resource creation, but it is not an iso-lated case.
The gaming approach to data collec-tion, termed games with a purpose, has receivedincreased attention since the success of the ESPgame (von Ahn, 2006).2.2 Human ComputationHuman computation, as a more general conceptthan games with a purpose, has become popularin numerous research areas.
The underlying as-sumption of learning from a vast user populationhas been largely the same in each approach.
Usersare engaged in different ways to achieve objectivessuch as:?
Assigning labels to items?
Learning to rank?
Acquiring structured knowledgeAn example of the first category is the ESPgame which was a project to label images withtags through a competitive game.
13,500 usersplayed the game, creating 1.3M labels in 3 months(von Ahn, 2006).
Other examples of assigninglables to items include Phetch and Peekaboom(von Ahn et al, 2006).Learning to rank is a very different objective.For example user judgements are collected in thePicture This game (Bennett et al, 2009).
This isa two player game where the user has to selectthe best matching image for a given query froma small set of potential candidates.
The aim isto learn a preference ranking from the user votesto predict the preference of future users.
Severalmethods for modeling the collected preferencesconfirmed the assumption that a consensus rank-ing from one set of users can be used to modelanother.Phrase Detectives is in the third category, i.e.
itaims to acquire structured knowledge, ultimatelyFigure 1: A screenshot of the Annotation Mode.leading to a linguistically annotated corpus.
An-other example of aiming to acquire large amountsof structured knowledge is the Open Mind Com-monsense project, a project to mine commonsenseknowledge to which 14,500 participants con-tributed nearly 700,000 sentences (Singh, 2002).Current efforts in attempting to acquire large-scale world knowledge from Web users includeFreebase3and True Knowledge4.
A slightly dif-ferent approach to the creation of commonsenseknowledge has been pursued in the Semantic Me-diaWiki project (Kr?otzsch et al, 2007), an effort todevelop a ?Wikipedia way to the Semantic Web?
:i.e., to make Wikipedia more useful and to supportimproved search of web pages via semantic anno-tation.3 The Phrase Detectives gamePhrase Detectives offers a simple graphical userinterface for non-expert users to learn how toannotate text and to make annotation decisions(Chamberlain et al, 2008).In order to use Web collaboration to create an-notated data, a number of issues have to be ad-dressed.
First among these is motivation.
For any-body other than a few truly dedicated people, an-notation is a very boring task.
This is where thepromise of the game approach lies.
Provided thata suitably entertaining format can be found, it maybe possible to get people to tag quite a lot of datawithout them even realizing it.3http://www.freebase.com/4http://www.trueknowledge.com/58The second issue is being able to recruit suf-ficient numbers of useful players to make the re-sults robust.
Both of these issues have been ad-dressed in the incentive structures of Phrase De-tectives (Chamberlain et al, 2009).Other problems still remain, most important ofwhich is to ensure the quality of the annotateddata.
We have identified four aspects that need tobe addressed to control annotation quality:?
Ensuring users understand the task?
Attention slips?
Malicious behaviour?
Genuine ambiguity of dataThese issues have been addressed at the designstage of the project (Kruschwitz et al, 2009).The goal of the game is to identify relationshipsbetween words and phrases in a short text.
An ex-ample of a task would be to highlight an anaphor-antecedent relation between the markables (sec-tions of text) ?This parrot?
and ?He?
in ?This parrotis no more!
He has ceased to be!?
Markables areidentified in the text by automatic pre-processing.There are two ways to annotate within the game:by selecting a markable that corefers to anotherone (Annotation Mode); or by validating a deci-sion previously submitted by another player (Vali-dation Mode).Annotation Mode (see Figure 1) is the simplestway of collecting judgments.
The player has to lo-cate the closest antecedent markable of an anaphormarkable, i.e.
an earlier mention of the object.
Bymoving the cursor over the text, markables are re-vealed in a bordered box.
To select it the playerclicks on the bordered box and the markable be-comes highlighted.
They can repeat this process ifthere is more than one antecedent markable (e.g.for plural anaphors such as ?they?).
They submitthe annotation by clicking the Done!
button.The player can also indicate that the highlightedmarkable has not been mentioned before (i.e.
it isnot anaphoric), that it is non-referring (for exam-ple, ?it?
in ?Yeah, well it?s not easy to pad thesePython files out to 150 lines, you know.?)
or thatit is the property of another markable (for exam-ple, ?a lumberjack?
being a property of ?I?
in ?Iwanted to be a lumberjack!?
).In Validation Mode (see Figure 2) the playeris presented with an annotation from a previousFigure 2: A screenshot of the Validation Mode.player.
The anaphor markable is shown with theantecedent markable(s) that the previous playerchose.
The player has to decide if he agrees withthis annotation.
If not he is shown the AnnotationMode to enter a new annotation.In the game groups of players work on the sametask over a period of time as this is likely to leadto a collectively intelligent decision (Surowiecki,2005).
An initial group of players are asked to an-notate a markable.
If all the players agree witheach other then the markable is considered com-plete.However it is likely that the first group of play-ers will not agree with each other (62% of mark-ables are given more than one relationship).
In thiscase each unique relationship for the markable isvalidated by another group of players.
This type ofvalidation has also been proposed elsewhere, e.g.
(Krause and Aras, 2009).When the users register they begin with thetraining phase of the game.
Their answers arecompared with Gold Standard texts to give themfeedback on their decisions and to get a user rat-ing, which is used to determine whether they needmore training.
Contextual instructions are alsoavailable during the game.The corpus used in the game is created fromshort texts including, for example, Wikipedia arti-cles selected from the ?Featured Articles?
and thepage of ?Unusual Articles?
; stories from ProjectGutenberg including Aesop?s Fables, SherlockHolmes and Grimm?s Fairy Tales; and dialoguetexts from Textfile.com.59Expert 1 vs.
Expert 2 Expert 1 vs. Game Expert 2 vs. GameOverall agreement 94.1% 84.5% 83.9%DN agreement 93.9% 96.0% 93.1%DO agreement 93.3% 72.7% 70.0%NR agreement 100.0% 100.0% 100.0%PR agreement 100.0% 0.0% 0.0%Table 1: Agreement figures for overall, discourse-new (DN), discourse-old (DO), non-referring (NR)and property (PR) attributes.4 ResultsThe first public version of Phrase Detectiveswent live in December 2008.
1.1 million wordshave been converted and made ready for annota-tion.
Over 920 players have submitted more than380,000 annotations and validations of anaphoricrelations.
46 documents have been fully anno-tated, meaning that at least 8 players have ex-pressed their judgment on each markable, andeach distinct anaphoric relation that these playersassigned has been checked by four more players.To put this in perspective, the GNOME corpus,produced by traditional methods, included around3,000 annotations of anaphoric relations (Poesio,2004a) whereas OntoNotes53.0, with 1 millionwords, contains around 140,000 annotations.4.1 Agreement on annotationsA set of tools were developed to examine the de-cisions of the players, and address the followingquestions:?
How do the collective annotations producedby the game compare to annotations assignedby an expert annotator??
What is the agreement between two expertsannotating the same texts?The answer to the first question will tell uswhether the game is indeed successful at obtain-ing anaphoric annotations collaboratively withinthe game context.
Anaphoric annotations are how-ever considered much harder than other tasks suchas part-of-speech tagging.
Therefore we ask thesecond question which will give us an upper boundof what can be expected from the game in the bestpossible case.We analysed five completed documents fromthe Wikipedia corpus containing 154 markables.5http://www.ldc.upenn.eduWe first looked at overall agreement and thenbroke it down into individual types of anaphoricrelations.
The following types of relation can beassigned by players:?
DN (discourse-new): this markable has noanaphoric link to any previous markable.?
DO (discourse-old): this markable has ananaphoric link and the player needs to linkit to the most recent antecedent.?
NR (non-referring): this markable does notrefer to anything e.g.
pleonistic ?it?.?
PR (property attribute): this markable repre-sents a property of a previously mentionedmarkable.DN is the most common relation with 70% of allmarkables falling in this category.
20% of mark-ables are DO and form a coreference chain withmarkables previously mentioned.
Less than 1% ofmarkables are non-referring.
The remaining mark-ables have been identified as property attributes.Each document was also manually annotated in-dividually by two experts.
Overall, we observe84.5% agreement between Expert 1 and the gameand 83.9% agreement between Expert 2 and thegame.
In other words, in about 84% of all cases therelation obtained from the majority vote of non-experts was identical to the one assigned by an ex-pert.
Table 1 gives a detailed breakdown of pair-wise agreement values.The agreement between the two experts ishigher than between an expert and the game.
Thison its own is not surprising.
However, an indi-cation of the difficulty of the annotation task is thefact that the experts only agree in 94% of all cases.This can be seen as an upper boundary of what wemight get out of the game.Furthermore, we see that the figures for DN arevery similar for all three comparisons.
This seemsto be the easiest type of relation to be detected.60DO relations appear to be more difficult to de-tect.
However if we relax the DO agreement con-dition and do not check what the antecedent is, weget agreement figures above 90% in all cases: al-most 97% between the two experts and between91% and 93% when comparing an expert with thegame.
A number of these cases which are assignedas DO but with different antecedents are actuallycoreference chains which link to the same object.Extracting coreference chains from the game ispart of the future work.Although non-referring markables are rare, theyare correctly identified in every case.
We additon-ally checked every completed markable identifiedas NR in the corpus and found that there was 100%precision in 54 cases.Property (PR) relations are very hard to identifyand not a single one resulted from the game.4.2 Disagreement on annotationsDisagreements between experts and the gamewere examined to understand whether the gamewas producing a poor quality annotation orwhether the markable was in fact ambiguous.These are cases where the gold standard as cre-ated by an expert is not the interpretation derivedfrom the game.?
In 60% of all cases where the game proposeda relation different from the expert annota-tion, the expert marked this relation to bea possible interpretation as well.
In otherwords, the majority of disagreements are notfalse annotations but alternatives such as am-biguous interpretations or references to othermarkables in the same coreference chain.
Ifwe counted these cases as correct, we get anagreement ratio of above 93%, close to pair-wise expert agreement.?
In cases of disagreement the relation identi-fied by the expert was typically the second orthird highest ranked relation in the game.?
The cumulative score of the expert relation(as calculated by the game) in cases of dis-agreement was 4.5, indicating strong playersupport for the expert relation even though itwasn?t the top answer.
A relation with a scoreof zero would be interpreted as one that hasas many players supporting it as it has playersdisagreeing.4.3 DiscussionThere are very promising results in the agreementbetween an expert and the top answer producedfrom the game.
By ignoring property relations andthe identification of coreference chains, the resultsare close to what is expected from an expert.
Theparticular difficulty uncovered by this analysis isthe correct identification of properties attributes.The analysis of markables with disagreementshow that some heuristics and filtering should beapplied to extract the highest quality decisionsfrom the game.
In many of the cases the gamerecorded plausible interpretations of different re-lations, which is valuable information when ex-ploring more difficult and ambiguous markables.These would also be the markables that automaticanaphora resolution systems would have difficultysolving.The data that was used to generate the resultswas not filtered in any way.
It would be possibleto ignore annotations from users who have a lowrating (judged when players annotate a gold stan-dard text).
Annotation time could also be a factorin filtering the results.
On average an annotationtakes 9 seconds in Annotation Mode and 11 sec-onds in Validation Mode.
Extreme variation fromthis may indicate that a poor quality decision hasbeen made.A different approach could be to identify thoseusers who have shown to provide high quality in-put.
A knowledge source could be created basedon input from these users and ignore everythingelse.
Related work in this area applies ideas fromcitation analysis to identify users of high expertiseand reputation in social networks by, e.g., adoptingKleinberg?s HITS algorithm (Yeun et al, 2009) orGoogle?s PageRank (Luo and Shinaver, 2009).The influence of document type may have a sig-nificant impact on both the distribution of mark-able types as well as agreement between ex-perts and the game.
We have only analysed theWikipedia documents, however discourse textsfrom Gutenberg may provide different results.5 ConclusionsThis first detailed analysis of the annotations col-lected from a collaborative game aiming at a largeanaphorically annotated corpus has demonstratedthat high-quality natural language resources canbe collected from non-expert users.
A game ap-proach can therefore be considered as a possible61alternative to expert annotations.We expect that the finally released corpus willapply certain heuristics to address the cases of dis-agreement between experts and consensus derivedfrom the game.6 Future WorkThis paper has focused on percentage agreementbetween experts and the game output but this isa very simplistic approach.
Various alternativeagreement coefficients have been proposed thatcorrect for chance agreement.
One such measureis Cohen?s ?
(Cohen, 1960) which we are using toperform a more indepth analysis of the data.The main part of our future work remains thecreation of a very large annotated corpus.
Toachieve this we are converting source texts to in-clude them in the game (our aim is a 100M wordcorpus).
We have already started converting textsin different languages to be included in the nextversion of the game.AcknowledgmentsANAWIKI is funded by a grant from the Engineer-ing and Physical Sciences Research Council (EP-SRC), grant number EP/F00575X/1.
Thanks toDaniela Goecke, Nils Diewald, Maik St?uhrenbergand Daniel Jettka (University of Bielefeld), MarkSchellhase (University of Essex) and all the play-ers who have contributed to the projectReferencesP.
N. Bennett, D. M. Chickering, and A. Mitya-gin.
2009.
Learning consensus opinion: min-ing data from a labeling game.
In Proceedings ofthe 18th International World Wide Web Conference(WWW2009), pages 121?130, Madrid.L.
Burnard.
2000.
The British National Corpus Ref-erence guide.
Technical report, Oxford UniversityComputing Services, Oxford.J.
Chamberlain, M. Poesio, and U. Kruschwitz.
2008.Phrase Detectives - A Web-based Collaborative An-notation Game.
In Proceedings of I-Semantics,Graz.J.
Chamberlain, M. Poesio, and U. Kruschwitz.
2009.A new life for a dead parrot: Incentive structures inthe Phrase Detectives game.
In Proceedings of theWebcentives Workshop at WWW?09, Madrid.J.
Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Mea-surement, 20(1):37?46.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
OntoNotes: The 90% Solu-tion.
In Proceedings of HLT-NAACL06.N.
L. Johnson, S. Rasmussen, C. Joslyn, L. Rocha,S.
Smith, and M. Kantor.
1998.
Symbiotic Intel-ligence: Self-Organizing Knowledge on DistributedNetworks Driven by Human Interaction.
In Pro-ceedings of the Sixth International Conference onArtificial Life.
MIT Press.D.
Jurafsky and J. H. Martin.
2008.
Speech and Lan-guage Processing- 2ndedition.
Prentice-Hall.M.
Krause and H. Aras.
2009.
Playful tagging folkson-omy generation using online games.
In Proceedingsof the 18th International World Wide Web Confer-ence (WWW2009), pages 1207?1208, Madrid.M.
Kr?otzsch, D. Vrande`ci?c, M. V?olkel, H. Haller, andR.
Studer.
2007.
Semantic Wikipedia.
Journal ofWeb Semantics, 5:251?261.U.
Kruschwitz, J. Chamberlain, and M. Poesio.
2009.
(Linguistic) Science Through Web Collaboration inthe ANAWIKI Project.
In Proceedings of Web-Sci?09, Athens.X.
Luo and J. Shinaver.
2009.
MultiRank: ReputationRanking for Generic Semantic Social Networks.
InProceedings of the WWW 2009 Workshop on WebIncentives (WEBCENTIVES?09), Madrid.M.
Poesio.
2004a.
Discourse annotation and semanticannotation in the gnome corpus.
In Proceedings ofthe ACL Workshop on Discourse Annotation.M.
Poesio.
2004b.
The MATE/GNOME scheme foranaphoric annotation, revisited.
In Proceedings ofSIGDIAL.P.
Singh.
2002.
The public acquisition of com-monsense knowledge.
In Proceedings of the AAAISpring Symposium on Acquiring (and Using) Lin-guistic (and World) Knowledge for Information Ac-cess, Palo Alto, CA.M.
St?uhrenberg, D. Goecke, N. Diewald, A. Mehler,and I. Cramer.
2007.
Web-based annotation ofanaphoric relations and lexical chains.
In Proceed-ings of the ACL Linguistic Annotation Workshop,pages 140?147.J.
Surowiecki.
2005.
The Wisdom of Crowds.
Anchor.L.
von Ahn, R. Liu, and M. Blum.
2006.
Peekaboom:a game for locating objects in images.
In Proceed-ings of CHI ?06, pages 55?64.L.
von Ahn.
2006.
Games with a purpose.
Computer,39(6):92?94.C.
A. Yeun, M. G. Noll, N. Gibbins, C. Meinel, andN.
Shadbolt.
2009.
On Measuring Expertise in Col-laborative Tagging Systems.
In Proceedings of Web-Sci?09, Athens.62
