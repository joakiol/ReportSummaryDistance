Chinese and Japanese Word Segmentation Using Word-Level andCharacter-Level InformationTetsuji NakagawaCorporate Research and Development CenterOki Electric Industry Co., Ltd.2?5?7 Honmachi, Chuo-ku, Osaka 541-0053, Japannakagawa378@oki.comAbstractIn this paper, we present a hybrid methodfor Chinese and Japanese word segmentation.Word-level information is useful for analysisof known words, while character-level informa-tion is useful for analysis of unknown words,and the method utilizes both these two typesof information in order to effectively handleknown and unknown words.
Experimental re-sults show that this method achieves high over-all accuracy in Chinese and Japanese word seg-mentation.1 IntroductionWord segmentation in Chinese and Japanese isan important and difficult task.
In these lan-guages, words are not separated by explicit delim-iters, and word segmentation must be conductedfirst in most natural language processing applica-tions.
One of the problems which makes word seg-mentation more difficult is existence of unknown(out-of-vocabulary) words.
Unknown words are de-fined as words that do not exist in a system?s dictio-nary.
The word segmentation system has no knowl-edge about these unknown words, and determiningword boundaries for such words is difficult.
Accu-racy of word segmentation for unknown words isusually much lower than that for known words.In this paper, we propose a hybrid method forChinese and Japanese word segmentation, whichutilizes both word-level and character-level infor-mation.
Word-level information is useful for anal-ysis of known words, and character-level informa-tion is useful for analysis of unknown words.
Weuse these two types of information at the same timeto obtain high overall performance.This paper is organized as follows: Section 2describes previous work on Chinese and Japaneseword segmentation on which our method is based.Section 3 introduces the hybrid method which com-bines word-level and character-level processing.Section 4 shows experimental results of Chinese andJapanese word segmentation.
Section 5 discussesrelated work, and Section 6 gives the conclusion.2 Previous Work on Word SegmentationOur method is based on two existing methods forChinese or Japanese word segmentation, and we ex-plain them in this section.2.1 The Markov Model-Based MethodWord-based Markov models are used in Englishpart-of-speech (POS) tagging (Charniak et al,1993; Brants, 2000).
This method identifies POS-tags T = t1, .
.
.
, tn, given a sentence as a word se-quence W = w1, .
.
.
, wn, where n is the numberof words in the sentence.
The method assumes thateach word has a state which is the same as the POSof the word and the sequence of states is a Markovchain.
A state t transits to another state s with prob-ability P (s|t), and outputs a word w with probabil-ity P (w|t).
From such assumptions, the probabilitythat the word sequence W with parts-of-speech T isgenerated isP (W,T ) =n?i=1P (witi|w0t0 .
.
.
wi?1ti?1),'n?i=1P (wi|ti)P (ti|ti?1), (1)where w0(t0) is a special word(part-of-speech) rep-resenting the beginning of the sentence.
Given aword sequence W , its most likely POS sequence T?can be found as follows:T?
= argmaxTP (T |W ),= argmaxTP (W,T )P (W ) ,= argmaxTP (W,T ),' argmaxTn?i=1P (wi|ti)P (ti|ti?1).
(2)The equation above can be solved efficiently by theViterbi algorithm (Rabiner and Juang, 1993).In Chinese and Japanese, the method is usedwith some modifications.
Because each word in aFigure 1: Example of Lattice Used in the Markov Model-Based Methodsentence is not separated explicitly in Chinese andJapanese, both segmentation of words and identifi-cation of the parts-of-speech tags of the words mustbe done simultaneously.
Given a sentence S, itsmost likely word sequence W?
and POS sequenceT?
can be found as follows where W ranges over thepossible segments of S (w1 ?
?
?wn = S):(W?
, T? )
= argmaxW,TP (W,T |S),= argmaxW,TP (W,T, S)P (S) ,= argmaxW,TP (W,T, S),= argmaxW,TP (W,T ),' argmaxW,Tn?i=1P (wi|ti)P (ti|ti?1).
(3)The equation above can be solved using the Viterbialgorithm as well.The possible segments of a given sentence arerepresented by a lattice, and Figure 1 shows an ex-ample.
Given a sentence, this method first con-structs such a lattice using a word dictionary, thenchooses the best path which maximizes Equation(3).This Markov model-based method achieves highaccuracy with low computational cost, and manyJapanese word segmentation systems adopt it(Kurohashi and Nagao, 1998; Matsumoto et al,2001).
However, the Markov model-based methodhas a difficulty in handling unknown words.
In theconstructing process of a lattice, only known wordsare dealt with and unknown words must be handledwith other methods.
Many practical word segmen-tation systems add candidates of unknown words toTag DescriptionB The character is in the beginning of a word.I The character is in the middle of a word.E The character is in the end of a word.S The character is itself a word.Table 1: The ?B, I, E, S?
Tag Setthe lattice.
The candidates of unknown words can begenerated by heuristic rules(Matsumoto et al, 2001)or statistical word models which predict the proba-bilities for any strings to be unknown words (Sproatet al, 1996; Nagata, 1999).
However, such heuris-tic rules or word models must be carefully designedfor a specific language, and it is difficult to properlyprocess a wide variety of unknown words.2.2 The Character Tagging MethodThis method carries out word segmentation by tag-ging each character in a given sentence, and inthis method, the tags indicate word-internal posi-tions of the characters.
We call such tags position-of-character (POC) tags (Xue, 2003) in this paper.Several POC-tag sets have been studied (Sang andVeenstra, 1999; Sekine et al, 1998), and we use the?B, I, E, S?
tag set shown in Table 1 1.Figure 2 shows an example of POC-tagging.
ThePOC-tags can represent word boundaries for anysentences, and the word segmentation task can bereformulated as the POC-tagging task.
The taggingtask can be solved by using general machine learn-ing techniques such as maximum entropy (ME)models (Xue, 2003) and support vector machines(Yoshida et al, 2003; Asahara et al, 2003).1The ?B, I, E, S?
tags are also called ?OP-CN, CN-CN, CN-CL, OP-CL?
tags (Sekine et al, 1998) or ?LL, MM, RR, LR?tags (Xue, 2003).Figure 2: Example of the Character Tagging Method: Word boundaries are indicated by vertical lines (?|?
).This character tagging method can easily han-dle unknown words, because known words and un-known words are treated equally and no other ex-ceptional processing is necessary.
This approach isalso used in base-NP chunking (Ramshaw and Mar-cus, 1995) and named entity recognition (Sekine etal., 1998) as well as word segmentation.3 Word Segmentation Using Word-Leveland Character-Level InformationWe saw the two methods for word segmentationin the previous section.
It is observed that theMarkov model-based method has high overall ac-curacy, however, the accuracy drops for unknownwords, and the character tagging method has highaccuracy for unknown words but lower accuracyfor known words (Yoshida et al, 2003; Xue, 2003;Sproat and Emerson, 2003).
This seems natural be-cause words are used as a processing unit in theMarkov model-based method, and therefore muchinformation about known words (e.g., POS or wordbigram probability) can be used.
However, un-known words cannot be handled directly by thismethod itself.
On the other hand, characters areused as a unit in the character tagging method.
Ingeneral, the number of characters is finite and farfewer than that of words which continuously in-creases.
Thus the character tagging method may berobust for unknown words, but cannot use more de-tailed information than character-level information.Then, we propose a hybrid method which com-bines the Markov model-based method and the char-acter tagging method to make the most of word-level and character-level information, in order toachieve high overall accuracy.3.1 A Hybrid MethodThe hybrid method is mainly based on word-levelMarkov models, but both POC-tags and POS-tagsare used in the same time and word segmentationfor known words and unknown words are conductedsimultaneously.Figure 3 shows an example of the method givena Japanese sentence ?
?,where the word ?
?
(person?s name) is an un-known word.
First, given a sentence, nodes oflattice for known words are made as in the usualMarkov model-based method.
Next, for each char-acter in the sentence, nodes of POC-tags (four nodesfor each character) are made.
Then, the most likelypath is searched (the thick line indicates the correctpath in the example).
Unknown words are identifiedby the nodes with POC-tags.
Note that some transi-tions of states are not allowed (e.g.
from I to B, orfrom any POS-tags to E), and such transitions areignored.Because the basic Markov models in Equation(1) are not expressive enough, we use the followingequation instead to estimate probability of a path ina lattice more precisely:P (W,T ) =n?i=1P (witi|w0t0 .
.
.
wi?1ti?1),'n?i=1{?1P (wi|ti)P (ti)+?2P (wi|ti)P (ti|ti?1)+?3P (wi|ti)P (ti|ti?2ti?1)+?4P (witi|wi?1ti?1)},(?1 + ?2 + ?3 + ?4 = 1).
(4)The probabilities in the equation above are esti-mated from a word segmented and POS-tagged cor-pus using the maximum-likelihood method, for ex-ample,P (wi|ti) =??
?f(wi,ti)?w f(w,ti)(f(wi, ti) > 0),0.5?w f(w,ti)(f(wi, ti) = 0),(5)where f(w, t) is a frequency that the word w withthe tag t occurred in training data.
Unseen eventsin the training data are handled as they occurred 0.5times for smoothing.
?1, ?2, ?3, ?4 are calculatedby deleted interpolation as described in (Brants,2000).
A word dictionary for a Markov model-based system is often constructed from a trainingcorpus, and no unknown words exist in the trainingcorpus in such a case.
Therefore, when the param-eters of the above probabilities are trained from atraining corpus, words that appear only once in thetraining corpus are regarded as unknown words anddecomposed to characters with POC-tags so thatstatistics about unknown words are obtained2.2As described in Equation (5), we used the additive smooth-ing method which is simple and easy to implement.
Althoughthere are other more sophisticated methods such as Good-Turing smoothing, they may not necessarily perform well be-cause the distribution of words is changed by this operation.Figure 3: Example of the Hybrid MethodIn order to handle various character-level fea-tures, we calculate word emission probabilities forPOC-tags by Bayes?
theorem:P (wi|ti)= P (ti|wi, ti ?
TPOC)P (wi, ti ?
TPOC)P (ti) ,= P (ti|wi, ti ?
TPOC)?t?TPOC P (wi, t)P (ti) , (6)where TPOC = {B, I,E,S}, wi is a character andti is a POC-tag.
In the above equation, P (ti) andP (wi, t) are estimated by the maximum-likelihoodmethod, and the probability of a POC tag ti, givena character wi (P (ti|wi, ti ?
TPOC)) is estimatedusing ME models (Berger et al, 1996).
We use thefollowing features for ME models, where cx is thexth character in a sentence, wi = ci?
and yx is thecharacter type of cx (Table 2 shows the definition ofcharacter types we used):(1) Characters (ci?
?2, ci?
?1, ci?
, ci?+1, ci?+2)(2) Pairs of characters (ci??2ci?
?1, ci??1ci?
,ci?
?1ci?+1, ci?ci?+1, ci?+1ci?+2)(3) Character types (yi?
?2, yi?
?1, yi?
, yi?+1, yi?+2)(4) Pairs of character types (yi??2yi?
?1, yi??1yi?
,yi?
?1yi?+1, yi?yi?+1, yi?+1yi?+2)Parameters of ME are trained using all the words intraining data.
We use the Generalized Iterative Scal-ing algorithm (Darroch and Ratcliff, 1972) for pa-rameter estimation, and features that appeared lessthan or equal to 10 times in training data are ignoredin order to avoid overfitting.What our method is doing for unknown wordscan be interpreted as follows: The method exam-ines all possible unknown words in a sentence, andprobability for an unknown word of length k, wi =Character Type DescriptionAlphabet AlphabetsNumeral Arabic and Chinese numeralsSymbol SymbolsKanji Chinese CharactersHiragana Hiragana (Japanese scripts)Katakana Katakana (Japanese scripts)Table 2: Character Typescj ?
?
?
cj+k?1 is calculated as:P (witi|h) (7)=????
?P (cjS|h) (k = 1),P (cjB|h)?j+k?2l=j+1 P (clI|h)P (cj+k?1E|h)(k > 1),where h is a history of the sequence.
In other words,the probability of the unknown word is approxi-mated by the product of the probabilities of the com-posing characters, and this calculation is done in theframework of the word-level Markov model-basedmethod.4 ExperimentsThis section gives experimental results of Chineseand Japanese word segmentation with the hybridmethod.
The following values are used to evaluatethe performance of word segmentation:R : Recall (The number of correctly segmentedwords in system?s output divided by the num-ber of words in test data)P : Precision (The number of correctly segmentedwords in system?s output divided by the num-ber of words in system?s output)F : F-measure (F = 2?R?
P/(R+ P ))Rknown : Recall for known wordsRunknown : Recall for unknown wordsCorpus # of Training Words # of Testing Words # of Words Rate of(known/unknown) in Dictionary Unknown WordsAS 5,806,611 11,985 (11,727/ 258) 146,212 0.0215HK 239,852 34,955 (32,463/2,492) 23,747 0.0713PK 1,121,017 17,194 (16,005/1,189) 55,226 0.0692RWCP 840,879 93,155 (93,085/ 70) 315,602 0.0008Table 3: Statistical Information of Corpora4.1 Experiments of Chinese WordSegmentationWe use three Chinese word-segmented corpora, theAcademia Sinica corpus (AS), the Hong Kong CityUniversity corpus (HK) and the Beijing Universitycorpus (PK), all of which were used in the FirstInternational Chinese Word Segmentation Bake-off (Sproat and Emerson, 2003) at ACL-SIGHAN2003.The three corpora are word-segmented corpora,but POS-tags are not attached, therefore we need toattach a POS-tag (state) which is necessary for theMarkov model-based method to each word.
We at-tached a state for each word using the Baum-Welchalgorithm (Rabiner and Juang, 1993) which is usedfor Hidden Markov Models.
The algorithm findsa locally optimal tag sequence which maximizesEquation (1) in an unsupervised way.
The initialstates are randomly assigned, and the number ofstates is set to 64.We use the following systems for comparison:Bakeoff-1, 2, 3 The top three systems participatedin the SIGHAN Bakeoff (Sproat and Emerson,2003).Maximum Matching A word segmentation sys-tem using the well-known maximum matchingmethod.Character Tagging A word segmentation systemusing the character tagging method.
This sys-tem is almost the same as the one studied byXue (2003).
Features described in Section 3.1(1)?
(4) and the following (5) are used to esti-mate a POC tag of a character ci?
, where tx isa POC-tag of the xth character in a sentence:(5) Unigram and bigram of previous POC-tags (ti?
?1, ti??2ti?
?1)All these systems including ours do not use anyother knowledge or resources than the training data.In this experiments, word dictionaries used by thehybrid method and Maximum Matching are con-structed from all the words in each training corpus.Statistical information of these data is shown in Ta-ble 3.
The calculated values of ?i in Equation (4)are shown in Table 4.Corpus ?1 ?2 ?3 ?4AS 0.037 0.178 0.257 0.528HK 0.048 0.251 0.313 0.388PK 0.055 0.207 0.242 0.495RWCP 0.073 0.105 0.252 0.571Table 4: Calculated Values of ?iThe results are shown in Table 5.
Our systemachieved the best F-measure values for the threecorpora.
Although the hybrid system?s recall val-ues for known words are not high compared to theparticipants of SIGHAN Bakeoff, the recall valuesfor known words and unknown words are relativelywell-balanced.
The results of Maximum Matchingand Character Tagging show the trade-off betweenthe word-based approach and the character-basedapproach which was discussed in Section 3.
Max-imum Matching is word-based and has the higherrecall values for known words than Character Tag-ging on the HK and PK corpus.
Character Taggingis character-based and has the highest recall valuesfor unknown words on the AS, HK and PK corpus.4.2 Experiments of Japanese WordSegmentationWe use the RWCP corpus, which is a Japaneseword-segmented and POS-tagged corpus.We use the following systems for comparison:ChaSen The word segmentation and POS-taggingsystem based on extended Markov models(Asahara and Matsumoto, 2000; Matsumoto etal., 2001).
This system carries out unknownword processing using heuristic rules.Maximum Matching The same system used in theChinese experiments.Character Tagging The same system used in theChinese experiments.As a dictionary for ChaSen, Maximum Matchingand the hybrid method, we use IPADIC (Matsumotoand Asahara, 2001) which is attached to ChaSen.Statistical information of these data is shown in Ta-ble 3.
The calculated values of ?i in Equation (4)are shown in Table 4.Corpus System R P F Rknown RunknownHybrid method 0.973 0.971 0.972 0.979 0.717Bakeoff-1 0.966 0.956 0.961 0.980 0.364AS Bakeoff-2 0.961 0.958 0.959 0.966 0.729Bakeoff-3 0.944 0.945 0.945 0.952 0.574Maximum Matching 0.917 0.912 0.915 0.938 0.000Character Tagging 0.962 0.959 0.960 0.966 0.744Hybrid method 0.951 0.948 0.950 0.969 0.715Bakeoff-1 0.947 0.934 0.940 0.972 0.625HK Bakeoff-2 0.940 0.908 0.924 0.980 0.415Bakeoff-3 0.917 0.915 0.916 0.936 0.670Maximum Matching 0.908 0.830 0.867 0.975 0.037Character Tagging 0.917 0.917 0.917 0.932 0.728Hybrid method 0.957 0.952 0.954 0.970 0.774Bakeoff-1 0.962 0.940 0.951 0.979 0.724PK Bakeoff-2 0.955 0.938 0.947 0.976 0.680Bakeoff-3 0.955 0.938 0.946 0.977 0.647Maximum Matching 0.930 0.883 0.906 0.974 0.020Character Tagging 0.932 0.931 0.931 0.943 0.786Table 5: Performance of Chinese Word SegmentationCorpus System R P F Rknown RunknownHybrid method 0.993 0.994 0.993 0.993 0.586RWCP ChaSen 0.991 0.992 0.991 0.991 0.243Maximum Matching 0.880 0.918 0.898 0.880 0.100Character Tagging 0.972 0.968 0.970 0.972 0.629Table 6: Performance of Japanese Word SegmentationThe results are shown in Table 63.
Compared toChaSen, the hybrid method has the comparable F-measure value and the higher recall value for un-known words (the difference is statistically signif-icant at 95% confidence level).
Character Tagginghas the highest recall value for unknown words asin the Chinese experiments.5 DiscussionSeveral studies have been conducted on word seg-mentation and unknown word processing.
Xue(2003) studied Chinese word segmentation usingthe character tagging method.
As seen in the pre-vious section, this method handles known and un-known words in the same way basing on character-level information.
Our experiments showed that themethod has quite high accuracy for unknown words,but accuracy for known words tends to be lower thanother methods.3In this evaluation, Rknown and Runknown are calculatedconsidering words in the dictionary as known words.
Wordswhich are in the training corpus but not in the dictionary arehandled as unknown words in the calculations.
The number ofknown/unknown words of the RWCP corpus shown in Table 3is also calculated in the same way.Uchimoto et al (2001) studied Japanese wordsegmentation using ME models.
Although theirmethod is word-based, no word dictionaries areused directly and known and unknown words arehandled in a same way.
The method estimates howlikely a string is to be a word using ME.
Given asentence, the method estimates the probabilities forevery substrings in the sentence.
Word segmenta-tion is conducted by finding a division of the sen-tence which maximizes the product of probabilitiesthat each divided substring is a word.
Comparedto our method, their method can handle some typesof features for unknown words such as ?the wordstarts with an alphabet and ends with a numeral?
or?the word consists of four characters?.
Our methodcannot handle such word-level features because un-known words are handled by using a character asa unit.
On the other hand, their method seems tohave a computational cost problem.
In their method,unknown words are processed by using a word asa unit, and the number of candidates for unknownwords in a sentence which consists of n charactersis equal to n(n + 1)/2.
Actually, they did not con-sider every substrings in a sentence, and limited thelength of substrings to be less than or equal to fivecharacters.
In our method, the number of POC-tagged characters which is necessary for unknownword processing is equal to 4n, and there is no lim-itation for the length of unknown words.Asahara et al (2003) studied Chinese word seg-mentation based on a character tagging methodwith support vector machines.
They preprocesseda given sentence using a word segmenter based onMarkov models, and the output is used as featuresfor character tagging.
Their method is a character-based method incorporating word-level informationand that is reverse to our approach.
They did not usesome of the features we used like character types,and our method achieved higher accuracies com-pared to theirs on the AS, HK and PK corpora (Asa-hara et al, 2003).6 ConclusionIn this paper, we presented a hybrid method forword segmentation, which utilizes both word-leveland character-level information to obtain high ac-curacy for known and unknown words.
The methodcombines two existing methods, the Markov model-based method and character tagging method.
Ex-perimental results showed that the method achieveshigh accuracy compared to the other state-of-the-artmethods in both Chinese and Japanese word seg-mentation.
The method can conduct POS taggingfor known words as well as word segmentation, buttagging identified unknown words is left as futurework.AcknowledgementsThis work was supported by a grant from the Na-tional Institute of Information and CommunicationsTechnology of Japan.ReferencesMasayuki Asahara and Yuji Matsumoto.
2000.
Ex-tended Models and Tools for High-performance Part-of-Speech Tagger.
In Proceedings of the 18th Inter-national Conference on Computational Linguistics,pages 21?27.Masayuki Asahara, Chooi Ling Goh, Xiaojie Wang, andYuji Matsumoto.
2003.
Combining Segmenter andChunker for Chinese Word Segmentation.
In Pro-ceedings of the 2nd SIGHAN Workshop on ChineseLanguage Processing, pages 144?147.Adam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A Maximum Entropy Approachto Natural Language Processing.
Computational Lin-guistics, 22(1):39?71.Thorsten Brants.
2000.
TnT ?
A Statistical Part-of-Speech Tagger.
In Proceedings of ANLP-NAACL2000, pages 224?231.Eugene Charniak, Curtis Hendrickson, Neil Jacobson,and Mike Perkowitz.
1993.
Equations for Part-of-Speech Tagging.
In Proceedings of the Eleventh Na-tional Conference on Artificial Intelligence, pages784?789.J.
Darroch and D. Ratcliff.
1972.
Generalized iterativescaling for log-linear models.
The annuals of Mathe-matical Statistics, 43(5):1470?1480.Sadao Kurohashi and Makoto Nagao.
1998.
JapaneseMorphological Analysis System JUMAN version 3.61.Department of Informatics, Kyoto University.
(inJapanese).Yuji Matsumoto and Masayuki Asahara.
2001.
IPADICUser?s Manual version 2.2.4.
Nara Institute of Sci-ence and Technology.
(in Japanese).Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,Yoshitaka Hirano, Hiroshi Matsuda, KazumaTakaoka, and Masayuki Asahara.
2001.
Morpholog-ical Analysis System ChaSen version 2.2.8 Manual.Nara Institute of Science and Technology.Masaki Nagata.
1999.
A Part of Speech EstimationMethod for Japanese Unknown Words using a Statis-tical Model of Morphology and Context.
In Proceed-ings of the 37th Annual Meeting of the Association forComputational Linguistics, pages 227?284.Lawrence R. Rabiner and Biing-Hwang Juang.
1993.Fundamentals of Speech Recognition.
PTR Prentice-Hall.Lance Ramshaw and Mitch Marcus.
1995.
Text Chunk-ing using Transformation-Based Learning.
In Pro-ceedings of the 3rd Workwhop on Very Large Corpora,pages 88?94.Erik F. Tjong Kim Sang and Jorn Veenstra.
1999.
Rep-resenting Text Chunks.
In Proceedings of 9th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 173?179.Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou.1998.
A Decision Tree Method for Finding and Clas-sifying Names in Japanese Texts.
In Proceedings ofthe 6th Workshop on Very Large Corpora, pages 171?177.Richard Sproat and Thomas Emerson.
2003.
The FirstInternational Chinese Word Segmentation Bakeoff.
InProceedings of the Second SIGHAN Workshop on Chi-nese Language Processing, pages 133?143.Richard Sproat, Chilin Shih, William Gale, and NancyChang.
1996.
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese.
ComputationalLinguistics, 22(3):377?404.Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.2001.
The Unknown Word Problem: a MorphologicalAnalysis of Japanese Using Maximum Entropy Aidedby a Dictionary.
In Proceedings of the 2001 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 91?99.Nianwen Xue.
2003.
Chinese Word Segmentation asCharacter Tagging.
International Journal of Compu-tational Linguistics and Chinese, 8(1):29?48.Tatsumi Yoshida, Kiyonori Ohtake, and Kazuhide Ya-mamoto.
2003.
Performance Evaluation of ChineseAnalyzers with Support Vector Machines.
Journalof Natural Language Processing, 10(1):109?131.
(inJapanese).
