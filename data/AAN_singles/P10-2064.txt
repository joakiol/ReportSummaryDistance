Proceedings of the ACL 2010 Conference Short Papers, pages 348?352,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsHierarchical A?
Parsing with Bridge Outside ScoresAdam Pauls and Dan KleinComputer Science DivisionUniversity of California at Berkeley{adpauls,klein}@cs.berkeley.eduAbstractHierarchical A?
(HA?)
uses of a hierarchyof coarse grammars to speed up parsingwithout sacrificing optimality.
HA?
pri-oritizes search in refined grammars usingViterbi outside costs computed in coarsergrammars.
We present Bridge Hierarchi-cal A?
(BHA?
), a modified Hierarchial A?algorithm which computes a novel outsidecost called a bridge outside cost.
Thesebridge costs mix finer outside scores withcoarser inside scores, and thus consti-tute tighter heuristics than entirely coarsescores.
We show that BHA?
substan-tially outperforms HA?
when the hierar-chy contains only very coarse grammars,while achieving comparable performanceon more refined hierarchies.1 IntroductionThe Hierarchical A?
(HA?)
algorithm of Felzen-szwalb and McAllester (2007) allows the use of ahierarchy of coarse grammars to speed up pars-ing without sacrificing optimality.
Pauls andKlein (2009) showed that a hierarchy of coarsegrammars outperforms standard A?
parsing for arange of grammars.
HA?
operates by computingViterbi inside and outside scores in an agenda-based way, using outside scores computed undercoarse grammars as heuristics which guide thesearch in finer grammars.
The outside scores com-puted by HA?
are auxiliary quantities, useful onlybecause they form admissible heuristics for searchin finer grammars.We show that a modification of the HA?
algo-rithm can compute modified bridge outside scoreswhich are tighter bounds on the true outside costsin finer grammars.
These bridge outside scoresmix inside and outside costs from finer grammarswith inside costs from coarser grammars.
Becausethe bridge costs represent tighter estimates of thetrue outside costs, we expect them to reduce thework of computing inside costs in finer grammars.At the same time, because bridge costs mix com-putation from coarser and finer levels of the hier-archy, they are more expensive to compute thanpurely coarse outside costs.
Whether the worksaved by using tighter estimates outweighs the ex-tra computation needed to compute them is an em-pirical question.In this paper, we show that the use of bridge out-side costs substantially outperforms the HA?
al-gorithm when the coarsest levels of the hierarchyare very loose approximations of the target gram-mar.
For hierarchies with tighter estimates, weshow that BHA?
obtains comparable performanceto HA?.
In other words, BHA?
is more robust topoorly constructed hierarchies.2 Previous WorkIn this section, we introduce notation and reviewHA?.
Our presentation closely follows Pauls andKlein (2009), and we refer the reader to that workfor a more detailed presentation.2.1 NotationAssume we have input sentence s0 .
.
.
sn?1 oflength n, and a hierarchy of m weighted context-free grammars G1 .
.
.Gm.
We call the most refinedgrammar Gm the target grammar, and all other(coarser) grammars auxiliary grammars.
Eachgrammar Gt has a set of symbols denoted with cap-ital letters and a subscript indicating the level inthe hierarchy, including a distinguished goal (root)symbol Gt.
Without loss of generality, we assumeChomsky normal form, so each non-terminal ruler in Gt has the form r = At ?
Bt Ct with weightwr.Edges are labeled spans e = (At, i, j).
Theweight of a derivation is the sum of rule weightsin the derivation.
The weight of the best (mini-mum) inside derivation for an edge e is called theViterbi inside score ?
(e), and the weight of the348(a) (b)Gts0s2sn-1VPtGts3s4s5..s0s2sn-1s3s4s5..VPt..
..Figure 1: Representations of the different types of itemsused in parsing and how they depend on each other.
(a)In HA?, the inside item I(VPt, 3, 5) relies on the coarseoutside item O(pit(VPt), 3, 5) for outside estimates.
(b) InBHA?, the same inside item relies on the bridge outside itemO?
(VPt, 3, 5), which mixes coarse and refined outside costs.The coarseness of an item is indicated with dotted lines.best derivation of G ?
s0 .
.
.
si?1 At sj .
.
.
sn?1is called the Viterbi outside score ?(e).
The goalof a 1-best parsing algorithm is to compute theViterbi inside score of the edge (Gm, 0, n); theactual best parse can be reconstructed from back-pointers in the standard way.We assume that each auxiliary grammar Gt?1forms a relaxed projection of Gt.
A grammar Gt?1is a projection of Gt if there exists some many-to-one onto function pit which maps each symbolin Gt to a symbol in Gt?1; hereafter, we will useA?t to represent pit(At).
A projection is relaxedif, for every rule r = At ?
Bt Ct with weightwr the projection r?
= A?t ?
B?t C?t has weightwr?
?
wr in Gt?1.
In other words, the weight of r?is a lower bound on the weight of all rules r in Gtwhich project to r?.2.2 Deduction RulesHA?
and our modification BHA?
can be formu-lated in terms of prioritized weighted deductionrules (Shieber et al, 1995; Felzenszwalb andMcAllester, 2007).
A prioritized weighted deduc-tion rule has the form?1 : w1, .
.
.
, ?n : wnp(w1,...,wn)?????????
?0 : g(w1, .
.
.
, wn)where ?1, .
.
.
, ?n are the antecedent items of thededuction rule and ?0 is the conclusion item.
Adeduction rule states that, given the antecedents?1, .
.
.
, ?n with weights w1, .
.
.
, wn, the conclu-sion ?0 can be formed with weight g(w1, .
.
.
, wn)and priority p(w1, .
.
.
, wn).These deduction rules are ?executed?
withina generic agenda-driven algorithm, which con-structs items in a prioritized fashion.
The algo-rithm maintains an agenda (a priority queue ofitems), as well as a chart of items already pro-cessed.
The fundamental operation of the algo-rithm is to pop the highest priority item ?
fromthe agenda, put it into the chart with its currentweight, and form using deduction rules any itemswhich can be built by combining ?
with items al-ready in the chart.
If new or improved, resultingitems are put on the agenda with priority given byp(?).
Because all antecedents must be constructedbefore a deduction rule is executed, we sometimesrefer to particular conclusion item as ?waiting?
onan other item(s) before it can be built.2.3 HA?HA?
can be formulated in terms of two types ofitems.
Inside items I(At, i, j) represent possiblederivations of the edge (At, i, j), while outsideitems O(At, i, j) represent derivations of G ?s1 .
.
.
si?1 At sj .
.
.
sn rooted at (Gt, 0, n).
SeeFigure 1(a) for a graphical depiction of theseedges.
Inside items are used to compute Viterbi in-side scores under grammar Gt, while outside itemsare used to compute Viterbi outside scores.The deduction rules which construct inside andoutside items are given in Table 1.
The IN deduc-tion rule combines two inside items over smallerspans with a grammar rule to form an inside itemover larger spans.
The weight of the resulting itemis the sum of the weights of the smaller insideitems and the grammar rule.
However, the IN rulealso requires that an outside score in the coarsegrammar1 be computed before an inside item isbuilt.
Once constructed, this coarse outside scoreis added to the weight of the conclusion item toform the priority of the resulting item.
In otherwords, the coarse outside score computed by thealgorithm plays the same role as a heuristic in stan-dard A?
parsing (Klein and Manning, 2003).Outside scores are computed by the OUT-L andOUT-R deduction rules.
These rules combine anoutside item over a large span and inside itemsover smaller spans to form outside items oversmaller spans.
Unlike the IN deduction, the OUTdeductions only involve items from the same levelof the hierarchy.
That is, whereas inside scoreswait on coarse outside scores to be constructed,outside scores wait on inside scores at the samelevel in the hierarchy.Conceptually, these deduction rules operate by1For the coarsest grammar G1, the IN rule builds rulesusing 0 as an outside score.349HA?IN: I(Bt, i, l) : w1 I(Ct, l, j) : w2 O(A?t, i, j) : w3w1+w2+wr+w3???????????
I(At, i, j) : w1 + w2 + wrOUT-L: O(At, i, j) : w1 I(Bt, i, l) : w2 I(Ct, l, j) : w3w1+w3+wr+w2???????????
O(Bt, i, l) : w1 + w3 + wrOUT-R: O(At, i, j) : w1 I(Bt, i, l) : w2 I(Ct, l, j) : w3w1+w2+wr+w3???????????
O(Ct, l, j) : w1 + w2 + wrTable 1: HA?
deduction rules.
Red underline indicates items constructed under the previous grammar in the hierarchy.BHA?B-IN: I(Bt, i, l) : w1 I(Ct, l, j) : w2 O?
(At, i, j) : w3w1+w2+wr+w3???????????
I(At, i, j) : w1 + w2 + wrB-OUT-L: O?
(At, i, j) : w1 I(B?t, i, l) : w2 I(C?t, l, j) : w3w1+wr+w2+w3???????????
O?
(Bt, i, l) : w1 + wr + w3B-OUT-R: O?
(At, i, j) : w1 I(Bt, i, l) : w2 I(C?t, l, j) : w3w1+w2+wr+w3???????????
O?
(Ct, l, j) : w1 + w2 + wrTable 2: BHA?
deduction rules.
Red underline indicates items constructed under the previous grammar in the hierarchy.first computing inside scores bottom-up in thecoarsest grammar, then outside scores top-downin the same grammar, then inside scores in thenext finest grammar, and so on.
However, the cru-cial aspect of HA?
is that items from all levelsof the hierarchy compete on the same queue, in-terleaving the computation of inside and outsidescores at all levels.
The HA?
deduction rules comewith three important guarantees.
The first is amonotonicity guarantee: each item is popped offthe agenda in order of its intrinsic priority p?(?
).For inside items I(e) over edge e, this priorityp?
(I(e)) = ?
(e) + ?(e?)
where e?
is the projec-tion of e. For outside items O(?)
over edge e, thispriority is p?
(O(e)) = ?
(e) + ?
(e).The second is a correctness guarantee: whenan inside/outside item is popped of the agenda, itsweight is its true Viterbi inside/outside cost.
Takentogether, these two imply an efficiency guarantee,which states that only items x whose intrinsic pri-ority p?
(x) is less than or equal to the Viterbi insidescore of the goal are removed from the agenda.2.4 HA?
with Bridge CostsThe outside scores computed by HA?
are use-ful for prioritizing computation in more refinedgrammars.
The key property of these scores isthat they form consistent and admissible heuristiccosts for more refined grammars, but coarse out-side costs are not the only quantity which satisfythis requirement.
As an alternative, we proposea novel ?bridge?
outside cost ??(e).
Intuitively,this cost represents the cost of the best deriva-tion where rules ?above?
and ?left?
of an edge ecome from Gt, and rules ?below?
and ?right?
ofthe e come from Gt?1; see Figure 2 for a graph-ical depiction.
More formally, let the spine ofan edge e = (At, i, j) for some derivation d beVPtNPtXt-1s1s2s3Gts0NNtNPts4s5VPtVPtStXt-1Xt-1 Xt-1NPtXt-1NPtXt-1sn-1Figure 2: A concrete example of a possible bridge outsidederivation for the bridge item O?
(VPt, 1, 4).
This edge isboxed for emphasis.
The spine of the derivation is shownin bold and colored in blue.
Rules from a coarser grammarare shown with dotted lines, and colored in red.
Here we havethe simple projection pit(A) = X , ?A.the sequence of rules between e and the root edge(Gt, 0, n).
A bridge outside derivation of e is aderivation d of G ?
s1 .
.
.
si At sj+1 .
.
.
sn suchthat every rule on or left of the spine comes fromGt, and all other rules come from Gt?1.
The scoreof the best such derivation for e is the bridge out-side cost ??
(e).Like ordinary outside costs, bridge outside costsform consistent and admissible estimates of thetrue Viterbi outside score ?
(e) of an edge e. Be-cause bridge costs mix rules from the finer andcoarser grammar, bridge costs are at least as goodan estimate of the true outside score as entirelycoarse outside costs, and will in general be muchtighter.
That is, we have?(e?)
?
??
(e) ?
?
(e)In particular, note that the bridge costs becomebetter approximations farther right in the sentence,and the bridge cost of the last word in the sentenceis equal to the Viterbi outside cost of that word.To compute bridge outside costs, we introduce350bridge outside items O?
(At, i, j), shown graphi-cally in Figure 1(b).
The deduction rules whichbuild both inside items and bridge outside itemsare shown in Table 2.
The rules are very simi-lar to those which define HA?, but there are twoimportant differences.
First, inside items wait forbridge outside items at the same level, while out-side items wait for inside items from the previouslevel.
Second, the left and right outside deductionsare no longer symmetric ?
bridge outside itemscan extended to the left given two coarse insideitems, but can only be extended to the right givenan exact inside item on the left and coarse insideitem on the right.2.5 GuaranteesThese deduction rules come with guarantees anal-ogous to those of HA?.
The monotonicity guaran-tee ensures that inside and (bridge) outside itemsare processed in order of:p?
(I(e)) = ?
(e) + ??(e)p?(O?
(e)) = ??
(e) + ?(e?
)The correctness guarantee ensures that when anitem is removed from the agenda, its weight willbe equal to ?
(e) for inside items and ??
(e) forbridge items.
The efficiency guarantee remains thesame, though because the intrinsic priorities aredifferent, the set of items processed will be differ-ent from those processed by HA?.A proof of these guarantees is not possibledue to space restrictions.
The proof for BHA?follows the proof for HA?
in Felzenszwalb andMcAllester (2007) with minor modifications.
Thekey property of HA?
needed for these proofs isthat coarse outside costs form consistent and ad-missible heuristics for inside items, and exact in-side costs form consistent and admissible heuris-tics for outside items.
BHA?
also has this prop-erty, with bridge outside costs forming admissi-ble and consistent heuristics for inside items, andcoarse inside costs forming admissible and consis-tent heuristics for outside items.3 ExperimentsThe performance of BHA?
is determined by theefficiency guarantee given in the previous sec-tion.
However, we cannot determine in advancewhether BHA?
will be faster than HA?.
In fact,BHA?
has the potential to be slower ?
BHA?0102030400-split 1-split 2-split 3-split 4-split 5-splitItemsPushed(Billions)BHA*HA*Figure 3: Performance of HA?
and BHA?
as a function ofincreasing refinement of the coarse grammar.
Lower is faster.02.557.5103 3-5 0-5EdgesPushed(billions)Figure 4: Performance of BHA?
on hierarchies of varyingsize.
Lower is faster.
Along the x-axis, we show which coarsegrammars were used in the hierarchy.
For example, 3-5 in-dicates the 3-,4-, and 5-split grammars were used as coarsegrammars.builds both inside and bridge outside items underthe target grammar, where HA?
only builds insideitems.
It is an empirical, grammar- and hierarchy-dependent question whether the increased tight-ness of the outside estimates outweighs the addi-tional cost needed to compute them.
We demon-strate empirically in this section that for hier-archies with very loosely approximating coarsegrammars, BHA?
can outperform HA?, whilefor hierarchies with good approximations, perfor-mance of the two algorithms is comparable.We performed experiments with the grammarsof Petrov et al (2006).
The training procedure forthese grammars produces a hierarchy of increas-ingly refined grammars through state-splitting, soa natural projection function pit is given.
We usedthe Berkeley Parser2 to learn such grammars fromSections 2-21 of the Penn Treebank (Marcus et al,1993).
We trained with 6 split-merge cycles, pro-ducing 7 grammars.
We tested these grammars on300 sentences of length ?
25 of Section 23 of theTreebank.
Our ?target grammar?
was in all casesthe most split grammar.2http://berkeleyparser.googlecode.com351In our first experiment, we construct 2-level hi-erarchies consisting of one coarse grammar andthe target grammar.
By varying the coarse gram-mar from the 0-split (X-bar) through 5-split gram-mars, we can investigate the performance of eachalgorithm as a function of the coarseness of thecoarse grammar.
We follow Pauls and Klein(2009) in using the number of items pushed asa machine- and implementation-independent mea-sure of speed.
In Figure 3, we show the perfor-mance of HA?
and BHA?
as a function of thetotal number of items pushed onto the agenda.We see that for very coarse approximating gram-mars, BHA?
substantially outperforms HA?, butfor more refined approximating grammars the per-formance is comparable, with HA?
slightly out-performing BHA?
on the 3-split grammar.Finally, we verify that BHA?
can benefit frommulti-level hierarchies as HA?
can.
We con-structed two multi-level hierarchies: a 4-level hier-archy consisting of the 3-,4-,5-, and 6- split gram-mars, and 7-level hierarchy consisting of all gram-mars.
In Figure 4, we show the performance ofBHA?
on these multi-level hierarchies, as well asthe best 2-level hierarchy from the previous exper-iment.
Our results echo the results of Pauls andKlein (2009): although the addition of the rea-sonably refined 4- and 5-split grammars producesmodest performance gains, the addition of coarsergrammars can actually hurt overall performance.AcknowledgementsThis project is funded in part by the NSF undergrant 0643742 and an NSERC Postgraduate Fel-lowship.ReferencesP.
Felzenszwalb and D. McAllester.
2007.
The gener-alized A* architecture.
Journal of Artificial Intelli-gence Research.Dan Klein and Christopher D. Manning.
2003.
A*parsing: Fast exact Viterbi parse selection.
InProceedings of the Human Language TechnologyConference and the North American Associationfor Computational Linguistics (HLT-NAACL), pages119?126.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
In Computational Linguistics.Adam Pauls and Dan Klein.
2009.
Hierarchical searchfor parsing.
In Proceedings of The Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL).Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proccedings of theAssociation for Computational Linguistics (ACL).Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and implementation ofdeductive parsing.
Journal of Logic Programming,24:3?36.352
