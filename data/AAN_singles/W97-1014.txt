Word Triggers and the EM A lgor i thmChristoph Tillmann and Hermann NeyLehrstuhl ffir Informatik VIAachen - University of TechnologyD-52074 Aachen, Germany{t illmann, ney}?inf ormat ik.
rwth-aachen, deAbstractIn this paper, we study the use of so-calledword trigger pairs to improve an existinglanguage model, which is typically a tri-gram model in combination with a cachecomponent.
A word trigger pair is de-fined as a long-distance word pair.
Wepresent two methods to select the mostsignificant single word trigger pairs.
Theselected trigger pairs are used in a com-bined model where the interpolation pa-rameters and trigger interaction parame-ters are trained by the EM algorithm.1 IntroductionIn this paper, we study the use of so-called wordtrigger pairs (for short: word triggers) (Bahl et al,1984, Lau and Rosenfeld, 1993, Tillmann and Ney,1996) to improve an existing language model, whichis typically a trigram model in combination with acache component (Ney and Essen, 1994).We use a reference model p(wlh), i.e.
the con-ditional probability of observing the word w for agiven history h. For a trigram model, this historyh includes the two predecessor words of the wordunder consideration, but in general it can be thewhole sequence of the last M predecessor words.The criterion for measuring the quality of a lan-guage model p(Wlh ) is the so-called log-likelihoodcriterion (Ney and Essen, 1994), which for a corpusWl, ..., wn, ...wN is defined by:FN~= y~logp(wn\[hn),rt-~--1According to this definition, the log-likelihood cri-terion measures for each position n how well thelanguage model can predict the next word giventhe knowledge about he preceeding words and com-putes an average over all word positions n. In thecontext of language modeling, the log-likelihood cri-terion F is converted to perplexity PP ,  defined byPP  := -F IN .For applications where the topic-dependence ofthe language model is important, e.g.
text dicta-tion, the history h may reach back several sentencesso that the history length M covers everal hundredwords, say, M = 400 as it is for the cache model.To illustrate what is meant by word triggers, wegive a few examples:airlineconcertoasksneitherwe...... f l ights...... orchestra...... replies.
.
.
.
.
.
nor...... ourselvesThus word trigger pairs can be viewed as long-distance word bigrams.
In this view, we are facedthe problem of finding suitable word trigger pairs.This will be achieved by analysing a large text corpus(i.e.
several millions of running words) and learningthose trigger pairs that are able to improve the base-line language model.
A related approach to captur-ing long-distance d pendencies is based on stochasticvariants of link grammars (Pietra and Pietra, 1994).In several papers (Bahl et al, 1984, Lau andRosenfeld, 1993, Tillmann and Ney, 1996), selectioncriteria for single word trigger pairs were studied.
Inthis paper, this work is extended as follows:?
S ing le -Tr igger  Mode l :  We consider the def-inition of a single word trigger pair.
There aretwo models we consider, namely a backing-offmodel and a linear interpolation model.
For thecase of the backing-off model, there is a closed-form solution for estimating the trigger param-eter by maximum likelihood.
For the linear in-terpolation model, there is no explicit solutionTillmann ~ Ney 117 Word Triggers and EMChristoph Tillmann and Hermann Ney (1997) Word Triggers and the EM Algorithm.
In T.M.
Ellison (ed.
)CoNLL97: Computational Natural Language Learning, ACL pp 117-124.Q 1997 Association for Computational Linguisticsanymore, but this model is better suited for theextension towards a large number of simultane-ous trigger pairs.Mu l t i -T r igger  Model :  In practice, we have totake into account he interaction of many trig-ger pairs.
Here, we introduce a model for thispurpose.
To really use the word triggers for alanguage model, they must be combined withan existing language model.
This is achievedby using linear interpolation between the exist-ing language model and a model for the multi-trigger effects.
The parameters of the resultingmodel, namely the trigger parameters and oneinterpolation parameter, are trained by the EMalgorithm.?
We present experimental results on the WallStreet Journal corpus.
Both the single-triggerapproach and the multi-trigger approach areused to improve the perplexity of a baseline lan-guage model.
We give examples of selected trig-ger pairs with and without using the EM algo-rithm.2 S ing le -Tr igger  Mode lIn this section, we review the basic model definitionfor single word trigger pairs as introduced in (Till-mann and Ney, 1996).We fix one trigger word pair (a --+ b) and define anextended model pab(wlh ) with an trigger interactionparameter q(bla ).
To pave the way for the followingextensions, we consider the asymmetric model ratherthan the symmetric model as originally described in(Tillmann and Ney, 1996).Back ing-Of fAs indicated by the results of several groups (Lauand Rosenfeld, 1993, Rosenfeld, 1994, Tillmann andNey, 1996), the word trigger pairs do not help muchto predict the next word if there is already a goodmodel based on specific contexts like trigram, bi-gram or cache.Therefore, we allow the trigger interaction a ~ bonly if the probability p(blh ) of the reference modelis not sufficiently high, i.e.
if p(blh ) < Po for a cer-tain threshold p0 (note that, by setting P0 := 1.0,the trigger effect is used in all cases).
Thus, we usethe trigger effect only for the following subset of his-tories:H~b := {h : a E hAp(b\[h) <P0}In the experiments, we used P0 := 1.5/W, whereW = 20000 is the vocabulary size.
We definethe model pab(wlh ) as an extension of the referencemodel p(wlh ) by a backing-off technique (Katz 87):p~b(wlh ) =q(bla) if h e H~b, w = b\[1 - q(bla)\].
P(wlh) E p(w'lh)w,~bif h E Hab, w # bp(wlh ) if h ~ HabFor a training corpus wl...WN, we consider thelog-likelihood functions of both the extended modeland the reference model p(wnlh,~), where we definethe history hn:n--1 hn : :  l l)n_ M = Wn-M. .
.Wn-2Wn-1For the difference Fab -- FO in the log-likelihoodsof the extended language model pab(wlh ) and thereference model p(w\[h), we obtain:Fab - Fo =N= ~log  p~b(w"lh")n=l  P (w" lh 'O-- ~ log p,~b(w.lh~)p(w,~lh,~) n: h,~ EH,~b= ~ ~N(h,w)'~ogpab(wlh)p~h: hEHab wE \[N(h,b) log q(bla)h: heHab p(b\[h)+ N(h, b) log 1 - q(bla) \]1 p(blh)J~r(a; b) log q(bla ) + N(a;b)log\[1 - q(b\]a)\]- ~ \[N(h,b) logp(b\[h)h: hEHab+ N(h, b) log\[1 - p(b\[h)\]\]where we have used the usual counts N(h, w):N(h,w) := E 1and two additional counts N(a;b) and N(a;b) de-fined particularly for word trigger modeling:1 /~/(a;b) : :  E N(h,b)= Eh:hE Hab n:h~ EHab,wr~=bN(a;b) : :  E N(h,b)= E 1h:hEHa~ n :hnEHob,w~bTillmann ~t Ney 118 Word Triggers and EMNote that, for the counts N(a; b) and/V(a; b), it doesnot matter how often the triggering word a actuallyoccurred in the history h E Hab.The unknown trigger parameter q(b\[a) is esti-mated using maximum likelihood estimation.
Bytaking the derivative and setting it to zero, we ob-tain the estimate:r(a; b)q(bla ) =At(a; b) +/V(a; b)which can be interpreted as the relative frequency ofthe occurrence of the word trigger (a -+ b).Linear InterpolationAlthough the backing-off method presented resultsin a closed-form solution for the trigger parameterq(b\]a), the disadvantage is that we have to use anexplicit probability threshold P0 to decide whetheror not the trigger effect applies.
Furthermore, theultimate goal is to combine several word trigger pairsinto a single model, and it is not clear how this couldbe done with the backing-off model.Therefore, we replace the backing-off model by thecorresponding model for linear interpolation:pab(wlh ) == ~ \ [1 -  q(bla)\]p(wlh ) + g(w,b)q(bla ) if a e h/ p(wlh ) if a ~ h\ [1 -  q(bla)\]p(blh )-4- q(bla) if a e h, w = b= \[1 q(bla)\]p(wlh ) i fa  e h, w # bP(wlh ) if a ~ hwhere tf(w, v) = 1 if and only if v = w. Note thatthis interpolation model allows a smooth transitionfrom no trigger effect (q(bla) --+ O) to a strong triggereffect (q(bla) -+ 1).For a corpus Wl...Wn...'tON, we have the log-likelihood difference:AFab = ~ log \ [ l -  q(bla)\] +r t :aeh .
,b#w,~q(bla) '~l?g (1-q(bla) + p(blh,~) /rt :aE hn ~b=w n= \[M(a) N(a;b)\].log\[1-q(b\[a)\] +q(bla) "~l?g (1 -  q(bla) + p(blh,~) /n :aE h,,, ,b=w =with the count definition M(a):MCa) := 1n:aEhnThus M(a) counts how many of all positions n(n = 1, .
.
.
,  N) with history hn contain word a andis therefore different from the unigram count N(a).To apply maximum likelihood estimation, we takethe derivative with respect o q(b\[a) and obtain thefollowing implicit equation for q(b\[a) after some ele-mentary manipulations:M(a) = 1 \[1 - q(bla)\] .p(b\[h,~) ?
q(bla)n:aEhn~b=wnNo explicit solution is possible.
However, we cangive bounds for the exact solution (proof omitted):N(a;b)M(a) <p(b)  > N(a;b)< q(bla ) <1-<p(b)> - - M(a) 'with the definition of the average value < p(b) >:1< p(b) > - N(a;b) ~ p(blh")n:aEhn~b=wnand an additional count N(a; b):N(a;b) := ~ 1n:aEh~,b=wr.An improved estimate can be obtained by the EMalgorithm (Dempster and Laird, 1977):q(bla) =1M(a)q(b\[a)- -  ~ \[1 - q(bla)\] .
p(blh,~ ) + q(bla )n:aEh,~,b=w~An example of the full derivation of the iterationformula for the EM algorithm will be given in thenext section for the more general case of a multi-trigger language model.3 Mu l t i -T r igger  Mode lThe trigger pairs are used in combination with aconventional baseline model p(wn \[h,~) (e.g.
m-gram)to define a trigger model pT(Wn \[hn):pT(wnlh ) == (1 -A) .p (w,~ lh , )+~-~(w,~lw, -m)mwith the trigger parameters ot(w\]v) that must benormalized for each v:a(wlv) = 1Tillmann ~4 Ney 119 Word Triggers and EMTo simplify the notation, we have used the conven-tion:m mE.M.with?
.A4,: the set of triggering words for position n?
M,  = I.A4,1: the number of triggering words forposition nUnfortunately, no method is known that producesclosed-form solutions for the maximum-likelihood es-timates.
Therefore, we resort o the EM algorithm inorder to obtain the maximum-likelihood estimates.The framework of the EM algorithm is basedon the so-called Q(#;~) function, where ~ is thenew estimate obtained from the previous estimate/.t (Baum, 1972), (Dempster and Laird, 1977).
Thesymbol # stands for the whole set of parameters tobe estimated.
The Q(#; ~) function is an extensionof the usual log-likelihood function and is for ourmodel:Q(-) = Q({A}, {o4wl~)}; {~(wlv)})Y (1 -- ~)p(w,~lh, ) ?
log\[(1 - X)p(w.lh,O\]=E .=1 pT(w.lh.
)pT(w, lh,)Taking the partial derivatives and solving for ~, weobtain:~' E ,~(wnlw._~ ) 1 N M,  \ ]=  S-"When taking the partial derivatives with respectto ~(wlv), we use the method of Lagrangian multi-pliers for the normalization constraints and obtain:A(w, v) with ~(~1~1- EA(w',v),I/31A(w, v) = ~(wl.)
"5(v, Wn--rn )N M.  ~'E ,~(~,w.)
~' E~(~.I~..-.,-) .=1 (1 - .X)p(w.lh.)
+ .~-Note how the interaction of word triggers is takeninto account by a local weighting effect: For a fixedposition n with wn = w, the contribution of a par-ticular observed istant word pair (v...w) to ~(wlv)depends on the interaction parameters of all otherword pairs (v'...w) with v' e {w~_-~} and the base-line probability p(wlh).Note that the local convergence property stillholds when the length M,  of the history is depen-dent on the word position n, e.g.
if the historyreaches back only to the beginning of the currentparagraph.A remark about the functional form of the multi-trigger model is in order.
The form chosen in thispaper is a sort of linear combination of the triggerpairs.
A different approach is to combine the var-ious trigger pairs in multiplicative way, which re-sults from a Maximum-Entropy approach (Lau andRosenfeld, 1993).4 Exper imenta l  resu l t sLanguage Mode l  T ra in ing  and  CorpusWe first describe the details of the language modelused and of its training.
The trigger pairs were se-lected as described in subsection 2 and were used toextend a baseline language model.
As in many othersystems, the baseline language model used here con-sists of two parts, an m-gram model (here: tri-gram/bigram/unigram) and a cache part(Ney andEssen, 1994).
Since the cache effect is equivalent toself-trigger pairs (a ---+ a), we can expect that thereis some trade-off between the word triggers and thecache, which was confirmed in some initial informalexperiments.For this reason, it is suitable to consider the simul-taneous interpolation of these three language modelparts to define the refined language model.
Thus wehave the following equation for the refined languagemodel p(w \[h,):p(w. lh ,O =AM "pM(w.lh.)
+ AC " pc(w, lh.)
+ AT.
pT(wnwhere pM(w,~ Ih,) is the m-gram model, pc(w, Ih,)is the cache model and pw(wnlhn) is the triggermodel.
The three interpolation parameters must benormalized:AM "1" ~C -I- AT "-~ 1The details of the m-gram model are similar to thosegiven in (Ney and Generet, 1995).
The cache modelPC (Wn n -  1 IWn_M) is defined as:M 1 pc(w. lw~_-lM) = .~ ~ g(w.,w._~)m-~ lTillmann 8J Ney 120 Word Triggers and EMTable 1: Effect of word trigger on test set perplexity (a) and interpolation parameter AM, AC, AT (b).l a) language modeltraining corpus1 Mio 5 Mio 39 Mio \[104.9 \]92.188.5 I87.4trigram with no cache 255.1 168.4trigram with cache 200.0 138.9 I -t- triggers: no EM 183.2 129.8+ triggers: with EM 179.0 127.2b) +triggers: noEM .83 / .11 / .06  .86 / .09 / .05  .89 / .08 / .04+triggers: wi thEM .82 / .10 / .09  .85 / .09 / .07  .86 / .07 / .07where (~(w, v) = 1 if and only if v = w. The triggermodel  PT (Wn Ihn) is defined as:M 1 pT(Wn\[W~I) a(Wn\[Wn--m) - M 2 .
,rn----1There were two me~hods used to compute the triggerparameters:?
method  'no :EM' :  The trigger parameterscr(w\[v) are obtained by renormalization fromthe single trigger parameters q(wlv):q(wlv)~(wlv) - ~q(w' lv)The backing-off method escribed in Section 2.1was used to select the top-K most significantsingle trigger pairs.
In the experiments, we usedK = 1.5 million trigger pairs.?
method  'wi th  EM':  The trigger parameterso~(wlv ) are initialized by the 'no EM' valuesand re-estimated using the EM algorithm as de-scribed in Section 3.
The typical number of it-erations is 10.The experimental tests were performed on theWall Street Journal (WSJ) task (Paul and Baker,1992) for a vocabulary size of 20000 words.
Totrain the m-gram 1,anguage model and the interpo-lation parameters, we used three training corporawith sizes of 1, 5 and 39 million running words.
How-ever, the word trigger pairs were always elected andtrained from the 39=million word training corpus.
Inthe experiments, the history h was defined to startwith the most recent article delimiter.The interpolation parameters are trained by usingthe EM algorithm.
In the case of the 'EM triggers',this is done jointly with the reestimation ofthe trig-ger parameters ~(wlv ).
To avoid the overfitting ofthe interpolation parameters on the training corpus,which was used to train both the m-gram languagemodel and the interpolation parameters, we appliedthe leaving-one-out technique.Examples  of  Tr igger PairsIn Table 2 and Table 3 we present examples of se-lected trigger pairs for the two methods no EM andEM.
For a fixed triggering word v, we show the mostsignificant riggered words w along with the trig-ger interaction parameter c~(wlv ) for both methods.There are 8 triggering words v for each of which weshow the 15 triggered words w with the highest rig-ger parameter ot(wlv ).
The triggered words w aresorted by the ot(wlv ) parameter.
/,From the tableit can be seen that for the no EM trigger pairs thetrigger parameter oL(wlv ) varies only slightly overthe triggered words w. This is different for the EMtriggers, where the trigger parameters o~(wlv ) havea much larger variation.
In addition the probabilitymass of the EM-trained trigger pairs is much moreconcentrated on the first 15 triggered words.Perplexity ResultsThe perplexity was computed on a test corpus of325 000 words from the WSJ task.
The results areshown in Table 1 for each of the three training cor-pora (1,5 and 39 million words).
For comparisonpurposes, the perplexities of the trigram model withand without cache are included.
As can be seen fromthis table, the trigger model is able to improve theperplexities in all conditions, and the EM triggersare consistently (although sometimes only slightly)better than the no EM triggers.
There is an effect ofthe training corpus size: if the trigram model is al-ready well trained, the trigger model does not help asmuch as for a less well trained trigram model.
Thisobservation is confirmed by the part b of Table 1,which shows the EM trained interpolation parame-ters.
As the size of the training corpus decreases therelative weight of the cache and trigger componentincreases.
Furthermore in the last row of Table 1it can be seen that the relative weight of the triggercomponent increases after the EM training which in-dicates that the parameters ofour trigger modell aresuccessfully trained by this EM approach.Tillmann ~ Ney 121 Word Triggers and EMTable 2: Triggered words w along with c~(w\[v) for triggering word v..... decliningaddingBayerischepositive_speculationconcernsfinishedremainingreportingconfusionexcessfallingdisappointingeased, ,equitiesno EMWcompetitorschangingcreativesimplydealscompetinghiringArmonkpersonnelbusinesses"-fasterofficesinventorysuccessfulcolorv = "added" \[ v = "airl ines"I with EM I no EM \[0.0110.0100.0100.0090.0090.0090.0080.0080.0080.0080.0070.0070.0070.0070.007w ot(wlv )declined 0.106asked 0.080estimated 0.070asserted 0.055dropped 0.049concerns 0.036conceded 0.033adding 0.029recommended 0.028contended 0.023confusion 0.023reporting 0.020adequate 0.017referring 0.016contributed 0.016v = "bus iness"\[ with EM0.0040.0040.0040.0040.OO40.0040.0040.0040.0040.0030.0030.0030.0030.0030.003w ot(wlv )corporate 0.146businesses 0.102marketing 0.056customers 0.047computer 0.026executives 0.024working 0.023competitive 0.022manufacturing 0.019product 0.018profits 0.017corporations 0.016started 0.015businessmen 0.014offices 0.011w .
(wlv)pass.engers 0.015careers 0.013passenger 0.013United's 0.013Trans 0.012Continental's 0.011Eastern's 0.010flights 0.010fare 0.009airline 0.009American's 0.009pilots' 0.008airlines' 0.008travel 0.008planes 0.008with EMw ~(wlv )airline 0.296air 0.064Continental 0.056carrier 0.049carriers 0.046passengers 0.037flight 0.035United 0.032flights 0.029Delta 0.026fares 0.024Eastern 0.023carrier's 0.020frequent 0.018passenger 0.018\] v : "buy"no EM IWpurchasesacquiringprivatelydealsspeculativepartlyfinancinghugeimmediatelyaggressivedecliningborrowingcheapcyclicalinvesting0.0050.0050.0050.0040.0040.0040.0040.0040.0040.0040.0040.0040.0040.0040.004with EMpurchasebuying 0.092purchases 0.051well 0.050bought 0.042cash 0.030deal 0.028potential 0.026future 0.025couldn't 0.024giving 0.022buys 0.019together 0.018bid 0.018buyers 0.017Tillmann ~ Ney 122 Word Triggers and EMTable 3: Triggered words w along with c~(w\[v) for triggering word v.no EMWaddi .rig.acquiringpubliclydepressedfinanciallyro.ughlypriorreducedoverseasremainingcompetitorssubstantiallyrivalpartlyprivatelyWcharactersphysicalturnsbeautifulcomicplayingfunherselfrockstuffdanceevilGodpain.passionv = "company"I with EM0.0020.0010.0010.0010.0010.0010.0010.0010.0010.0010.0010.0010.0010.001no EMmanagement 0.092including 0.037top 0.028employees 0.027will 0.024plans 0.018unit 0.017couldn't 0.017hasn't 0.016subsidiary 0.014previously 0.014now 0.013since 0.011won't 0.011executives 0.011v = "Ford"wFord's 0.039Dearborn 0.020Chrysler's 0.014Chevrolet 0.013Lincoln 0.013truck 0.012Mazda 0.011vehicle 0.010Dodge 0.009incentive 0.009Buick 0.009dealer 0.008vans 0.008car's 0.008Honda 0.008with EMw a(wlv }Ford's 0.651auto 0.063Dearborn 0.056Chrysler 0.028Mercury 0.022Taurus 0.021Mustang 0.013Escort 0.011Lincoln 0.010Tempo 0.009parts 0.007car 0.006pattern 0.006Henry 0.006Jaguar 0.0060.0060.0050.0050.0050.0050.0050.0050.0050.0050.0050.0040.0040.0040.0040.004"love"w   olv)human 0.051lovers 0.044passion 0.039turns 0.031beautiful 0.030spirit 0.029marriage 0.020phil 0.019lounge 0.017dresses 0.017stereotype 0.016wonder 0.015songs 0.015beautifully 0.014muscular 0.014V ----deep 0.002changing 0.002starting 0.002simply 0.002tough 0.002dozens 0.002driving 0.002twice 0.002experts 0.002cheap 0.002winning 0.002minor 0.002critics 0.002nearby 0.002living 0.002"says"w a(wlv )adds 0.090low 0.053suggests 0.031concedes 0.024explains 0.019contends 0.017notes 0.016agrees 0.016thinks 0.015insists 0.015get 0.014hot 0.013early 0.013sees 0.012consultant 0.012Tillmann ~ Ney 123 Word Triggers and EM5 ConclusionsWe have presented a model and an algorithm fortraining a multi-word trigger model along with someexperimental evaluations.
The results can be sum-merized as follows:?
The trigger parameters for all word triggers arejointly trained using the EM algorithm.
Thisleads to a systematic (although small) improve-ment over the condition that each trigger pa-rameter is trained separately.?
The word-trigger model is used in combinationwith a full language model (m-gram/cache) .Thus the perplexity is reduced from 138.9 to127.2 for the 5-million training corpus and from92.2 to 87.4 for the 39-million corpus.ReferencesL.
E. Baum.
1972.
"An Inequality and AssociatedMaximization Technique in Statistical Estima-tion of a Markov Process", Inequalities, Vol.
3,No.
1, pp.
1-8.L.
R. Bahl, F. Jelinek, R. L. Mercer, A. Nadas.
1984.
"Next Word Statistical Predictor", IBM Tech.Disclosure Bulletin, Vol.
27, No.
7A, pp.
3941-42, December.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra,R.
L. Mercer.
1993.
"Mathematics of Sta-tistical Machine Translation: Parameter Es-timation", Computational Linguistics, Vol.
19,No.
2, pp.
263-311, June.A.
P. Dempster, N. M. Laird, D. B. Rubin.
1977.
"Maximum Likelihood from Incomplete Datavia the EM Algorithm", J. Royal Statist.
Soc.Ser.
B (methodological), Vol.
39, pp.
1-38.S.M.
Katz.
1993.
"Estimation of Probabilities fromSparse Data for the Language Model Compo-nent of a Speech Recognizer", in IEEE Trans.on Acoustics, Speech and Signal Processing,Vol.
35, pp.
400-401, March.R.
Lau, R. Rosenfeld, S. Roukos.
1993.
"Trigger-Based Language Models: A Maximum En-tropy Approach", in Proc.
IEEE Inter.
Conf.on Acoustics, Speech and Signal Processing,Minneapolis, MN, Vol.
II, pp.
45-48, April.H.
Ney, U. Essen, R. Kneser.
1994.
"On StructuringProbabilistic Dependencies in Language Mod-eling", Computer Speech and Language, Vol.
8,pp.
1-38.H.
Ney, M. Generet, F. Wessel.
1995.
"Extensionsof Absolute Discounting for Language Mod-eling", in Proc.
Fourth European Conferenceon Speech Communication and Technology,Madrid, pp.
1245-1248, September.D.B.
Paul and J.B. Baker.
1992.
"The Design forthe Wall Street Journal-based CSR Corpus",in Proc.
of the DARPA SLS Workshop, pp.357-361, February.S.
Della Pietra, V. Della Pietra, J. Gillett, J. Laf-ferty, H. Printz and L. Ures.
1994.
"Infer-ence and Estimation of a Long-Range Tri-gram Model", in Lecture Notes in ArtificialIntelligence, Grammatical Inference and Ap-plications, ICGI-94, Alicante, Spain, Springer-Verlag, pp.
78-92, September.R.
Rosenfeld.
1994.
"Adaptive Statistical Lan-guage Modeling: A Maximum Entropy Ap-proach", Ph.D. thesis, School of Computer Sci-ence, Carnegie Mellon University, Pittsburgh,PA, CMU-CS-94-138.C.
Tillmann and H. Ney.
1996.
"Selection Criteriafor Word Triggers in Language Modeling".
,in Lecture Notes in Artificial Intelligence, Int.Colloquium on Grammatical Inference, Mont-pellier, France, Springer-Verlag, pp.
95-106,September.Tillmann 8?
Ney 124 Word Triggers and EM
