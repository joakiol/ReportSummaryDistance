Proceedings of ACL-08: HLT, pages 434?442,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsWhich Are the Best Features for Automatic Verb ClassificationJianguo LiDepartment of LinguisticsThe Ohio State UniversityColumbus Ohio, USAjianguo@ling.ohio-state.eduChris BrewDepartment of LinguisticsThe Ohio State UniversityColumbus Ohio, USAcbrew@ling.ohio-state.eduAbstractIn this work, we develop and evaluate a widerange of feature spaces for deriving Levin-style verb classifications (Levin, 1993).
Weperform the classification experiments usingBayesian Multinomial Regression (an effi-cient log-linear modeling framework whichwe found to outperform SVMs for this task)with the proposed feature spaces.
Our exper-iments suggest that subcategorization framesare not the most effective features for auto-matic verb classification.
A mixture of syntac-tic information and lexical information worksbest for this task.1 IntroductionMuch research in lexical acquisition of verbs hasconcentrated on the relation between verbs and theirargument frames.
Many scholars hypothesize thatthe behavior of a verb, particularly with respect tothe expression of arguments and the assignment ofsemantic roles is to a large extent driven by deepsemantic regularities (Dowty, 1991; Green, 1974;Goldberg, 1995; Levin, 1993).
Thus measurementsof verb frame patterns can perhaps be used to probefor linguistically relevant aspects of verb meanings.The correspondence between meaning regularitiesand syntax has been extensively studied in Levin(1993) (hereafter Levin).
Levin?s verb classes arebased on the ability of a verb to occur or not occurin pairs of syntactic frames that are in some sensemeaning preserving (diathesis alternation).
The fo-cus is on verbs for which distribution of syntacticframes is a useful indicator of class membership,and, correspondingly, on classes which are relevantfor such verbs.
By using Levin?s classification, weobtain a window on some (but not all) of the poten-tially useful semantic properties of verbs.Levin?s verb classification, like others, helps re-duce redundancy in verb descriptions and enablesgeneralizations across semantically similar verbswith respect to their usage.
When the informationabout a verb type is not available or sufficient for usto draw firm conclusions about its usage, the infor-mation about the class to which the verb type be-longs can compensate for it, addressing the perva-sive problem of data sparsity in a wide range of NLPtasks, such as automatic extraction of subcategoriza-tion frames (Korhonen, 2002), semantic role label-ing (Swier and Stevenson, 2004; Gildea and Juraf-sky, 2002), natural language generation for machinetranslation (Habash et al, 2003), and deriving pre-dominant verb senses from unlabeled data (Lapataand Brew, 2004).Although there exist several manually-createdverb lexicons or ontologies, including Levin?s verbtaxonomy, VerbNet, and FrameNet, automatic verbclassification (AVC) is still necessary for extend-ing existing lexicons (Korhonen and Briscoe, 2004),building and tuning lexical information specific todifferent domains (Korhonen et al, 2006), and boot-strapping verb lexicons for new languages (Tsanget al, 2002).AVC helps avoid the expensive hand-coding ofsuch information, but appropriate features must beidentified and demonstrated to be effective.
In thiswork, our primary goal is not necessarily to obtainthe optimal classification, but rather to investigate434the linguistic conditions which are crucial for lex-ical semantic classification of verbs.
We developfeature sets that combine syntactic and lexical infor-mation, which are in principle useful for any Levin-style verb classification.
We test the general ap-plicability and scalability of each feature set to thedistinctions among 48 verb classes involving 1,300verbs, which is, to our knowledge, the largest in-vestigation on English verb classification by far.
Topreview our results, a feature set that combines bothsyntactic information and lexical information worksmuch better than either of them used alone.
In ad-dition, mixed feature sets also show potential forscaling well when dealing with larger number ofverbs and verb classes.
In contrast, subcategoriza-tion frames, at least on their own, are largely inef-fective for AVC, despite their evident effectivenessin supporting Levin?s initial intuitions.2 Related WorkEarlier work on verb classification has generallyadopted one of the two approaches for devising sta-tistical, corpus-based features.Subcategorization frame (SCF): Subcategoriza-tion frames are obviously relevant to alternationbehaviors.
It is therefore unsurprising that muchwork on verb classification has adopted them as fea-tures (Schulte im Walde, 2000; Brew and Schulte imWalde, 2002; Korhonen et al, 2003).
However, rely-ing solely on subcategorization frames also leads tothe loss of semantic distinctions.
Consider the frameNP-V-PPwith.
The semantic interpretation of thisframe depends to a large extent on the NP argumentselected by the preposition with.
In (1), the samesurface form NP-V-PPwith corresponds to three dif-ferent underlying meanings.
However, such seman-tic distinctions are totally lost if lexical informationis disregarded.
(1) a. I ate with a fork.
[INSTRUMENT]b. I left with a friend.
[ACCOMPANIMENT]c. I sang with confidence.
[MANNER]This deficiency of unlexicalized subcategoriza-tion frames leads researchers to make attempts toincorporate lexical information into the feature rep-resentation.
One possible improvement over subcat-egorization frames is to enrich them with lexical in-formation.
Lexicalized frames are usually obtainedby augmenting each syntactic slot with its head noun(2).
(2) a. NP(I)-V-PP(with:fork)b. NP(I)-V-PP(with:friend)c. NP(I)-V-PP(with:confidence)With the potentially improved discriminatorypower also comes increased exposure to sparse dataproblems.
Trying to overcome the problem of datasparsity, Schulte im Walde (2000) explores the ad-ditional use of selectional preference features byaugmenting each syntactic slot with the concept towhich its head noun belongs in an ontology (e.g.WordNet).
Although the problem of data sparsityis alleviated to certain extent (3), these featuresdo not generally improve classification performance(Schulte im Walde, 2000; Joanis, 2002).
(3) a. NP(PERSON)-V-PP(with:ARTIFACT)b. NP(PERSON)-V-PP(with:PERSON)c. NP(PERSON)-V-PP(with:FEELING)JOANIS07: Incorporating lexical information di-rectly into subcategorization frames has proved in-adequate for AVC.
Other methods for combiningsyntactic information with lexical information havealso been attempted (Merlo and Stevenson, 2001;Joanis et al, 2007).
These studies use a small col-lection of features that require some degree of expertlinguistic analysis to devise.
The deeper linguisticanalysis allows their feature set to cover a variety ofindicators of verb semantics, beyond that of frameinformation.
Joanis et al (2007) reports an experi-ment that involves 15 Levin verb classes.
They de-fine a general feature space that is supposed to beapplicable to all Levin classes.
The features theyuse fall into four different groups: syntactic slots,slot overlaps, tense, voice and aspect, and animacyof NPs.?
Syntactic slots: They encode the frequency ofthe syntactic positions (e.g.
SUBJECT, OB-JECT, PPat).
They are considered approxima-tion to subcategorization frames.?
Slot overlaps: They are supposed to capturethe properties of alternation by identifying ifa given noun can occur in different syntacticpositions relative to a particular verb.
For in-stance, in the alternation The ice melted and435The sun melted the ice, ice occurs in the sub-ject position in the first sentence but in the ob-ject position in the second sentence.
An over-lap feature records that there is a subject-objectalternation for melt.?
Tense, voice and aspect: Verb meaning and al-ternations also interact in interesting ways withtense, voice, and aspect.
For example, mid-dle construction is usually used in present tense(e.g.
The bread cuts easily).?
Animacy of NPs: The animacy of the seman-tic role corresponding to the head noun in eachsyntactic slot can also distinguish classes ofverbs.Joanis et al (2007) demonstrates that the gen-eral feature space they devise achieves a rate oferror reduction ranging from 48% to 88% over achance baseline accuracy, across classification tasksof varying difficulty.
However, they also show thattheir general feature space does not generally im-prove the classification accuracy over subcategoriza-tion frames (see table 1).Experimental Task All Features SCFAverage 2-way 83.2 80.4Average 3-way 69.6 69.4Average (?
6)-way 61.1 62.8Table 1: Results from Joanis et al (2007) (%)3 Integration of Syntactic and LexicalInformationIn this study, we explore a wider range of featuresfor AVC, focusing particularly on various ways tomix syntactic with lexical information.Dependency relation (DR): Our way to over-come data sparsity is to break lexicalized frames intolexicalized slots (a.k.a.
dependency relations).
De-pendency relations contain both syntactic and lexicalinformation (4).
(4) a. SUBJ(I), PP(with:fork)b. SUBJ(I), PP(with:friend)c. SUBJ(I), PP(with:confidence)However, augmenting PP with nouns selected bythe preposition (e.g.
PP(with:fork)) still gives riseto data sparsity.
We therefore decide to break itinto two individual dependency relations: PP(with),PP-fork.
Although dependency relations have beenwidely used in automatic acquisition of lexical infor-mation, such as detection of polysemy (Lin, 1998)and WSD (McCarthy et al, 2004), their utility inAVC still remains untested.Co-occurrence (CO): CO features mostly conveylexical information only and are generally consid-ered not particularly sensitive to argument structures(Rohde et al, 2004).
Nevertheless, it is worthwhiletesting whether the meaning components that arebrought out by syntactic alternations are also cor-related to the neighboring words.
In other words,Levin verbs may be distinguished on the dimensionof neighboring words, in addition to argument struc-tures.
A test on this claim can help answer the ques-tion of whether verbs in the same Levin class alsotend to share their neighboring words.Adapted co-occurrence (ACO): ConventionalCO features generally adopt a stop list to filter outfunction words.
However, some of the functionswords, prepositions in particular, are known to carrygreat amount of syntactic information that is relatedto lexical meanings of verbs (Schulte im Walde,2003; Brew and Schulte im Walde, 2002; Joaniset al, 2007).
In addition, whereas most verbs tend toput a strong selectional preference on their nominalarguments, they do not care much about the iden-tity of the verbs in their verbal arguments.
Based onthese observations, we propose to adapt the conven-tional CO features by (1) keeping all prepositions(2) replacing all verbs in the neighboring contexts ofeach target verb with their part-of-speech tags.
ACOfeatures integrate at least some degree of syntacticinformation into the feature space.SCF+CO: Another way to mix syntactic informa-tion with lexical information is to use subcategoriza-tion frames and co-occurrences together in hope thatthey are complementary to each other, and thereforeyield better results for AVC.4 Experiment Setup4.1 CorpusTo collect each type of features, we use the Giga-word Corpus, which consists of samples of recentnewswire text data collected from four distinct in-436ternational sources of English newswire.4.2 Feature ExtractionWe evaluate six different feature sets for their effec-tiveness in AVC: SCF, DR, CO, ACO, SCF+CO,and JOANIS07.
SCF contains mainly syntactic in-formation, whereas CO lexical information.
Theother four feature sets include both syntactic and lex-ical information.SCF and DR: These more linguistically informedfeatures are constructed based on the grammaticalrelations generated by the C&C CCG parser (Clarkand Curran, 2007).
Take He broke the door with ahammer as an example.
The grammatical relationsgenerated are given in table 2.he broke the door with a hammer.
(det door 3 the 2)(dobj broke 1 door 3)(det hammer 6 a 5)(dobj with 4 hammer 6)(iobj broke 1 with 4)(ncsubj broke 1 He 0 )Table 2: grammatical relations generated by the parserWe first build a lexicalized frame for the verbbreak: NP1(he)-V-NP2(door)-PP(with:hammer).This is done by matching each grammatical labelonto one of the traditional syntactic constituents.The set of syntactic constituents we use is summa-rized in table 3.constituent remarkNP1 subject of the verbNP2 object of the verbNP3 indirect object of the verbPPp prepositional phraseTO infinitival clauseGER gerundTHAT sentential complement headed by thatWH sentential complement headed by a wh-wordADJP adjective phraseADVP adverb phraseTable 3: Syntactic constituents used for building SCFsBased on the lexicalized frame, we constructan SCF NP1-NP2-PPwith for break.
The set ofDRs generated for break is [SUBJ(he), OBJ(door),PP(with), PP-hammer].CO: These features are collected using a flat 4-word window, meaning that the 4 words to theleft/right of each target verb are considered poten-tial CO features.
However, we eliminate any COfeatures that are in a stopword list, which con-sists of about 200 closed class words includingmainly prepositions, determiners, complementizersand punctuation.
We also lemmatize each word us-ing the English lemmatizer as described in Minnenet al (2000), and use lemmas as features instead ofwords.ACO: As mentioned before, we adapt the conven-tional CO features by (1) keeping all prepositions(2) replacing all verbs in the neighboring contexts ofeach target verb with their part-of-speech tags.
(3)keeping words in the left window only if they aretagged as a nominal.SCF+CO: We combine the SCF and CO features.JOANIS07: We use the feature set proposed inJoanis et al (2007), which consists of 224 features.We extract features on the basis of the output gener-ated by the C&C CCG parser.4.3 Verb ClassesOur experiments involve two separate sets of verbclasses:Joanis15: Joanis et al (2007) manually selectspairs, or triples of classes to represent a range ofdistinctions that exist among the 15 classes they in-vestigate.
For example, some of the pairs/triples aresyntactically dissimilar, while others show little syn-tactic distinction across the classes.Levin48: Earlier work has focused only on asmall set of verbs or a small number of verb classes.For example, Schulte im Walde (2000) uses 153verbs in 30 classes, and Joanis et al (2007) takeson 835 verbs and 15 verb classes.
Since one of ourprimary goals is to identify a general feature spacethat is not specific to any class distinctions, it is ofgreat importance to understand how the classifica-tion accuracy is affected when attempting to classifymore verbs into a larger number of classes.
In ourautomatic verb classification, we aim for a largerscale experiment.
We select our experimental verbclasses and verbs as follows: We start with all Levin197 verb classes.
We first remove all verbs that be-long to at least two Levin classes.
Next, we removeany verb that does not occur at least 100 times inthe English Gigaword Corpus.
All classes that areleft with at least 10 verbs are chosen for our experi-437ment.
This process yields 48 classes involving about1,300 verbs.
In our automatic verb classification ex-periment, we test the applicability of each featureset to distinctions among up to 48 classes 1.
To ourknowledge, this is, by far, the largest investigationon English verb classification.5 Machine Learning Method5.1 Preprocessing DataWe represent the semantic space for verbs as a ma-trix of frequencies, where each row corresponds toa Levin verb and each column represents a givenfeature.
We construct a semantic space with eachfeature set.
Except for JONAIS07 which only con-tains 224 features, all the other feature sets lead to avery high-dimensional space.
For instance, the se-mantic space with CO features contains over onemillion columns, which is too huge and cumber-some.
One way to avoid these high-dimensionalspaces is to assume that most of the features are irrel-evant, an assumption adopted by many of the previ-ous studies working with high-dimensional seman-tic spaces (Burgess and Lund, 1997; Pado and La-pata, 2007; Rohde et al, 2004).
Burgess and Lund(1997) suggests that the semantic space can be re-duced by keeping only the k columns (features) withthe highest variance.
However, Rohde et al (2004)have found it is simpler and more effective to dis-card columns on the basis of feature frequency, withlittle degradation in performance, and often someimprovement.
Columns representing low-frequencyfeatures tend to be noisier because they only involvefew examples.
We therefore apply a simple fre-quency cutoff for feature selection.
We only use fea-tures that occur with a frequency over some thresh-old in our data.In order to reduce undue influence of outlier fea-tures, we employ the four normalization strategies intable 4, which help reduce the range of extreme val-ues while having little effect on others (Rohde et al,2004).
The raw frequency (wv,f ) of a verb v oc-curring with a feature f is replaced with the normal-1In our experiment, we only use monosemous verbs fromthese 48 verb classes.
Due to the space limit, we do not list the48 verb classes.
The size of the most classes falls in the rangebetween 10 to 30, with a couple of classes having a size over100.ized value (w?v,f ), according to each normalizationmethod.
Our experiments show that using correla-tion for normalization generally renders the best re-sults.
The results reported below are obtained fromusing correlation for normalization.w?v,f =rowwv,fPj wv,jcolumnwv,fPi wi,flengthwv,fPj w2v,j1/2correlationTwv,f?Pj wv,jPi wi,f(Pj wv,j(T?Pj wv,j)Pi wi,f (T?Pi wi,f ))1/2T =PiPj wi,jTable 4: Normalization techniquesTo preprocess data, we first apply a frequency cut-off to our data set, and then normalize it using thecorrelation method.
To find the optimal thresholdfor frequency cut, we consider each value between 0and 10,000 at an interval of 500.
In our experiments,results on training data show that performance de-clines more noticeably when the threshold is lowerthan 500 or higher than 10,000.
For each task andfeature set, we select the frequency cut that offersthe best accuracy on the preprocessed training setaccording to k-fold stratified cross validation 2.5.2 ClassifierFor all of our experiments, we use the software thatimplements the Bayesian multinomial logistic re-gression (a.k.a BMR).
The software performs the so-called 1-of-k classification (Madigan et al, 2005).BMR is similar to Maximum Entropy.
It has beenshown to be very efficient with handling large num-bers of features and extremely sparsely populatedmatrices, which characterize the data we have forAVC 3.
To begin, let x = [x1, ..., xj , ..., xd]T be avector of feature values characterizing a verb to beclassified.
We encode the fact that a verb belongsto a class k ?
1, ...,K by a K-dimensional 0/1 val-ued vector y = (y1, ..., yK)T , where yk = 1 and allother coordinates are 0.
Multinomial logistic regres-210-fold for Joanis15 and 9-fold for Levin48.
We use a bal-anced training set, which contains 20 verbs from each class inJoanis15, but only 9 verbs from each class in Levin48.3We also tried Chang and Lin (2001)?s LIBSVM library forSupport Vector Machines (SVMs), however, BMR generallyoutperforms SVMs.438sion is a conditional probability model of the form,parameterized by the matrix ?
= [?1, ..., ?K ].
Eachcolumn of ?
is a parameter vector corresponding toone of the classes: ?k = [?k1, ..., ?kd]T .P (yk = 1|?k, x) = exp(?Tk x)/Xkiexp(?Tkix)6 Results and Discussion6.1 Evaluation MetricsFollowing Joanis et al (2007), we adopt a singleevaluation measure - macro-averaged recall - for allof our classification tasks.
As discussed below, sincewe always use balanced training sets for each indi-vidual task, it makes sense for our accuracy metric togive equal weight to each class.
Macro-averaged re-call treats each verb class equally, so that the size ofa class does not affect macro-averaged recall.
It usu-ally gives a better sense of the quality of classifica-tion across all classes.
To calculate macro-averagedrecall, the recall value for each individual verb classhas to be computed first.recall =no.
of test verbs in class c correctly labeledno.
of test verbs in class cWith a recall value computed for each verb class,the macro-averaged recall can be defined by:macro-averaged recall =1|C|Xc?Crecall for class cC : a set of verb classesc : an individual verb class|C| : the number of verb classes6.2 Joanis15With those manually-selected 15 classes, Joaniset al (2007) conducts 11 classification tasks includ-ing six 2-way classifications, two 3-way classifica-tions, one 6-way classification, one 8-way classifi-cation, and one 14-way classification.
In our exper-iments, we replicate these 11 classification tasks us-ing the proposed six different feature sets.
For eachclassification task in this task set, we randomly se-lect 20 verbs from each class as the training set.
Werepeat this process 10 times for each task.
The re-sults reported for each task is obtained by averagingthe results of the 10 trials.
Note that for each trial,each feature set is trained and tested on the sametraining/test split.The results for the 11 classification tasks are sum-marized in table 5.
We provide a chance baselineand the accuracy reported in Joanis et al (2007) 4 forcomparison of our results.
A few points are worthnoting:?
Although widely used for AVC, SCF, at leastwhen used alone, is not the most effective fea-ture set.
Our experiments show that the per-formance achieved by using SCF is generallyworse than using the feature sets that mix syn-tactic and lexical information.
As a matter offact, it even loses to the simplest feature setCOon 4 tasks, including the 14-way task.?
The two feature sets (DR, SCF+CO) we pro-pose that combine syntactic and lexical infor-mation generally perform better than those fea-ture sets (SCF, CO) that only include syntacticor lexical information.
Although there is not aclear winner, DR and SCF+CO generally out-perform other feature sets, indicating that theyare effective ways for combining syntactic andlexical information.
In particular, these twofeature sets perform comparatively well on thetasks that involve more classes (e.g.
14-way),exhibiting the tendency to scale well with largernumber of verb classes and verbs.
Another fea-ture set that combines syntactic and lexical in-formation, ACO, which keeps function wordsin the feature space to preserve syntactic infor-mation, outperforms the conventional CO onthe majority of tasks.
All these observationssuggest that how to mix syntactic and lexicalinformation is one of keys to an improved verbclassification.?
Although JOANIS07 also combines syntacticand lexical information, its performance is notcomparable to that of other feature sets that mixsyntactic and lexical information.
In fact, SCF4Joanis et al (2007) is different from our experiments in thatthey use a chunker for feature extraction and the Support VectorMachine for classification.439Experimental TaskRandom As Reported in Feature SetBaseline Joanis et al (2007) SCF DR CO ACO SCF+CO JOANIS071) Benefactive/Recipient 50 86.4 88.6 88.4 88.2 89.1 90.7 88.92) Admire/Amuse 50 93.9 96.7 97.5 92.1 90.5 96.4 96.63) Run/Sound 50 86.8 85.4 89.6 91.8 90.2 90.5 87.14) Light/Sound 50 75.0 74.8 90.8 86.9 89.7 88.8 82.15) Cheat/Steal 50 76.5 77.6 80.6 72.1 75.5 77.8 76.46) Wipe/Steal 50 80.4 84.8 80.6 79.0 79.4 84.4 83.97) Spray/Fill/Putting 33.3 65.6 73.0 72.8 59.6 66.6 73.8 69.68) Run/State Change/Object drop 33.3 74.2 74.8 77.2 76.9 77.6 80.5 75.59) Cheat/Steal/Wipe/Spray/Fill/Putting 16.7 64.3 64.9 65.1 54.8 59.1 65.0 64.310) 9)/Run/Sound 12.5 61.7 62.3 65.8 55.7 60.8 66.9 63.111) 14-way (all except Benefactive) 7.1 58.4 56.4 65.7 57.5 59.6 66.3 57.2Table 5: Experimental results for Joanis15 (%)and JOANIS07 yield similar accuracy in ourexperiments, which agrees with the findings inJoanis et al (2007) (compare table 1 and 5).6.3 Levin48Recall that one of our primary goals is to identifythe feature set that is generally applicable and scaleswell while we attempt to classify more verbs into alarger number of classes.
If we could exhaust all thepossible n-way (2 ?
n ?
48) classification taskswith the 48 Levin classes we will investigate, it willallow us to draw a firmer conclusion about the gen-eral applicability and scalability of a particular fea-ture set.
However, the number of classification tasksgrows really huge when n takes on certain value (e.g.n = 20).
For our experiments, we set n to be 2, 5,10, 20, 30, 40, or 48.
For the 2-way classification,we perform all the possible 1,028 tasks.
For the 48-way classification, there is only one possible task.We randomly select 100 n-way tasks each for n =5, 10, 20, 30, 40.
We believe that this series of taskswill give us a reasonably good idea of whether a par-ticular feature set is generally applicable and scaleswell.The smallest classes in Levin48 have only 10verbs.
We therefore reduce the number of trainingverbs to 9 for each class.
For each n = 2, 5, 10, 20,30, 40, 48, we will perform certain number of n-wayclassification tasks.
For each n-way task, we ran-domly select 9 verbs from each class as training data,and repeat this process 10 times.
The accuracy foreach n-way task is then computed by averaging theresults from these 10 trials.
The accuracy reportedfor the overall n-way classification for each selectedn, is obtained by averaging the results from each in-dividual n-way task for that particular n. Again, foreach trial, each feature set is trained and tested onthe same training/test split.The results for Levin48 are presented in table 6,which clearly reveals the general applicability andscalability of each feature set.?
Results from Levin48 reconfirm our findingthat SCF is not the most effective feature set forAVC.
Although it achieves the highest accuracyon the 2-way classification, its accuracy dropsdrastically as n gets bigger, indicating that SCFdoes not scale as well as other feature sets whendealing with larger number of verb classes.
Onthe other hand, the co-occurrence feature (CO),which is believed to convey only lexical infor-mation, outperforms SCF on every n-way clas-sification when n ?
10, suggesting that verbsin the same Levin classes tend to share theirneighboring words.?
The three feature sets we propose that com-bine syntactic and lexical information generallyscale well.
Again, DR and SCF+CO gener-ally outperform all other feature sets on all n-way classifications, except the 2-way classifica-tion.
In addition, ACO achieves a better perfor-mance on every n-way classification than CO.Although SCF and CO are not very effectivewhen used individually, they tend to yield thebest performance when combined together.?
Again, JOANIS07 does not match the perfor-mance of other feature sets that combine bothsyntactic and lexical information, but yieldssimilar accuracy as SCF.440Experimental Task No of Tasks Random BaselineFeature SetSCF DR CO ACO SCF+CO JOANIS072-way 1,028 50 84.0 83.4 77.8 80.9 82.9 82.45-way 100 20 71.9 76.4 70.4 73.0 77.3 72.210-way 100 10 65.8 73.7 68.8 71.2 72.8 65.920-way 100 5 51.4 65.1 58.8 60.1 65.8 50.730-way 100 3.3 46.7 56.9 48.6 51.8 57.8 47.140-way 100 2.5 43.6 54.8 47.3 49.9 55.1 44.248-way 1 2.2 39.1 51.6 42.4 46.8 52.8 38.9Table 6: Experimental results for Levin48 (%)6.4 Further DiscussionPrevious studies on AVC have focused on usingSCFs.
Our experiments reveal that SCFs, at leastwhen used alone, compare poorly to the feature setsthat mix syntactic and lexical information.
One ex-planation for the poor performance could be that weuse all the frames generated by the CCG parser inour experiment.
A better way of doing this wouldbe to use some expert-selected SCF set.
Levin clas-sifies English verbs on the basis of 78 SCFs, whichshould, at least in principle, be good at separatingverb classes.
To see if Levin-selected SCFs aremore effective for AVC, we match each SCF gen-erated by the C&C CCG parser (CCG-SCF) to oneof 78 Levin-defined SCFs, and refer to the resultingSCF set as unfiltered-Levin-SCF.
Following stud-ies on automatic SCF extraction (Brent, 1993), weapply a statistical test (Binomial Hypothesis Test) tothe unfiltered-Levin-SCF to filter out noisy SCFs,and denote the resulting SCF set as filtered-Levin-SCF.
We then perform the 48-way task (one ofLevin48) with these two different SCF sets.
Recallthat using CCG-SCF gives us a macro-averaged re-call of 39.1% on the 48-way task.
Our experimentsshow that using unfiltered-Levin-SCF and filtered-Levin-SCF raises the accuracy to 39.7% and 40.3%respectively.
Although a little performance gain hasbeen obtained by using expert-defined SCFs, the ac-curacy level is still far below that achieved by usinga feature set that combines syntactic and semanticinformation.
In fact, even the simple co-occurrencefeature (CO) yields a better performance (42.4%)than these Levin-selected SCF sets.7 Conclusion and Future WorkWe have performed a wide range of experimentsto identify which features are most informative inAVC.
Our conclusion is that both syntactic and lex-ical information are useful for verb classification.Although neither SCF nor CO performs well on itsown, a combination of them proves to be the most in-formative feature for this task.
Other ways of mixingsyntactic and lexical information, such as DR, andACO, work relatively well too.
What makes thesemixed feature sets even more appealing is that theytend to scale well in comparison to SCF and CO. Inaddition, these feature sets are devised on a generallevel without relying on any knowledge about spe-cific classes, thus potentially applicable to a widerrange of class distinctions.
Assuming that Levin?sanalysis is generally applicable across languages interms of the linking of semantic arguments to theirsyntactic expressions, these mixed feature sets arepotentially useful for building verb classificationsfor other languages.For our future work, we aim to test whether anautomatically created verb classification can be ben-eficial to other NLP tasks.
One potential applica-tion of our verb classification is parsing.
LexicalizedPCFGs (where head words annotate phrasal nodes)have proved a key tool for high performance PCFGparsing, however its performance is hampered bythe sparse lexical dependency exhibited in the PennTreebank.
Our experiments on verb classificationhave offered a class-based approach to alleviate datasparsity problem in parsing.
It is our goal to testwhether this class-based approach will lead to an im-proved parsing performance.8 AcknowledgmentsThis study was supported by NSF grant 0347799.We are grateful to Eric Fosler-Lussier, DetmarMeurers, Mike White and Kirk Baker for their valu-able comments.441ReferencesBrent, M. (1993).
From grammar to lexicon: Unsuper-vised learning of lexical syntax.
Computational Lin-guistics, 19(3):243?262.Brew, C. and Schulte im Walde, S. (2002).
Spectral clus-tering for German verbs.
In Proccedings of the 2002Conference on EMNLP, pages 117?124.Burgess, C. and Lund, K. (1997).
Modelling parsingconstraints with high-dimentional context space.
Lan-guage and Cognitive Processes, 12(3):177?210.Chang, C. and Lin, C. (2001).
LIBSVM:A library for support vector machines.http://www.csie.ntu.edu.tw.
cjlin/libsvm.Clark, S. and Curran, J.
(2007).
Formalism-independentparser evaluation with CCG and Depbank.
In Proceed-ings of the 45th Annual Meeting of ACL, pages 248?255.Dowty, D. (1991).
Thematic proto-roles and argumentselection.
Language, 67:547?619.Gildea, D. and Jurafsky, D. (2002).
Automatic labeling ofsemantic role.
Computational Linguistics, 28(3):245?288.Goldberg, A.
(1995).
Constructions.
University ofChicago Press, Chicago, 1st edition.Green, G. (1974).
Semantics and Syntactic Regularity.Indiana University Press, Bloomington.Habash, N., Dorr, B., and Traum, D. (2003).
Hybrid natu-ral language generation from lexical conceptual struc-tures.
Machine Translation, 18(2):81?128.Joanis, E. (2002).
Automatic verb classification using ageneral feature space.
Master?s thesis, University ofToronto.Joanis, E., Stevenson, S., and James, D. (2007).
A generalfeature space for automatic verb classification.
NaturalLanguage Engineering, 1:1?31.Korhonen, A.
(2002).
Subcategorization Acquisition.PhD thesis, Cambridge University.Korhonen, A. and Briscoe, T. (2004).
Extended lexical-semantic classification of english verbs.
In Proceed-ings of the 2004 HLT/NAACL Workshop on Computa-tional Lexical Semantics, pages 38?45, Boston, MA.Korhonen, A., Krymolowski, Y., and Collier, N. (2006).Automatic classification of verbs in biomedical texts.In Proceedings of the 21st International Conferenceon COLING and 44th Annual Meeting of ACL, pages345?352, Sydney, Australia.Korhonen, A., Krymolowski, Y., and Marx, Z.
(2003).Clustering polysemic subcategorization frame distri-butions semantically.
In Proceedings of the 41st An-nual Meeting of ACL, pages 48?55, Sapparo, Japan.Lapata, M. and Brew, C. (2004).
Verb class disambigua-tion using informative priors.
Computational Linguis-tics, 30(1):45?73.Levin, B.
(1993).
English Verb Classes and Alternations:A Preliminary Investigation.
University of ChicagoPress, Chicago, 1st edition.Lin, D. (1998).
Automatic retrieval and clustering of sim-ilar words.
In Proceedings of the 17th Internation Con-ference on COLING and 36th Annual Meeting of ACL.Madigan, D., Genkin, A., Lewis, D., and Fradkin, D.(2005).
Bayesian Multinomial Logistic Regression forAuthor Identification.
DIMACS Technical Report.McCarthy, D., Koeling, R., Weeds, J., and Carroll, J.(2004).
Finding predominant senses in untagged text.In Proceedings of the 42nd Annual Meeting of ACL,pages 280?287.Merlo, P. and Stevenson, S. (2001).
Automatic verb clas-sification based on statistical distribution of argumentstructure.
Computational Linguistics, 27(3):373?408.Minnen, G., Carroll, J., and Pearce, D. (2000).
Appliedmorphological processing of English.
Natural Lan-guage Engineering, 7(3):207?223.Pado, S. and Lapata, M. (2007).
Dependency-based con-struction of semantic space models.
Computional Lin-guistics, 33(2):161?199.Rohde, D., Gonnerman, L., and Plaut, D. (2004).
An im-proved method for deriving word meaning from lexicalco-occurrence.
http://dlt4.mit.edu/ dr/COALS.Schulte im Walde, S. (2000).
Clustering verbs seman-tically according to alternation behavior.
In Proceed-ings of the 18th International Conference on COLING,pages 747?753.Schulte im Walde, S. (2003).
Experiments on the choiceof features for learning verb classes.
In Proceedings ofthe 10th Conference of EACL, pages 315?322.Swier, R. and Stevenson, S. (2004).
Unsupervised se-mantic role labelling.
In Proceedings of the 2004 Con-ference on EMNLP, pages 95?102.Tsang, V., Stevenson, S., and Merlo, P. (2002).
Crosslin-guistic transfer in automatic verb classification.
InProceedings of the 19th International Conference onCOLING, pages 1023?1029, Taiwan, China.442
