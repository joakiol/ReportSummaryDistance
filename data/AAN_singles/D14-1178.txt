Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1713?1723,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsRandom Manhattan Integer Indexing:Incremental L1Normed Vector Space ConstructionBehrang Q.
Zadeh?
?Insight CentreNational University of Ireland, GalwayGalway, Irelandbehrang.qasemizadeh@insight-centre.orgSiegfried Handschuh???Dept.
of Computer Science and MathematicsUniversity of PassauBavaria, Germanysiegfried.handschuh@uni-passau.deAbstractVector space models (VSMs) are math-ematically well-defined frameworks thathave been widely used in the distributionalapproaches to semantics.
In VSMs, high-dimensional vectors represent linguisticentities.
In an application, the similar-ity of vectors?and thus the entities thatthey represent?is computed by a distanceformula.
The high dimensionality of vec-tors, however, is a barrier to the perfor-mance of methods that employ VSMs.Consequently, a dimensionality reductiontechnique is employed to alleviate thisproblem.
This paper introduces a noveltechnique called Random Manhattan In-dexing (RMI) for the construction of `1normed VSMs at reduced dimensionality.RMI combines the construction of a VSMand dimension reduction into an incre-mental and thus scalable two-step proce-dure.
In order to attain its goal, RMI em-ploys the sparse Cauchy random projec-tions.
We further introduce Random Man-hattan Integer Indexing (RMII): a compu-tationally enhanced version of RMI.
Asshown in the reported experiments, RMIand RMII can be used reliably to estimatethe `1distances between vectors in a vec-tor space of low dimensionality.1 IntroductionDistributional semantics embraces a set of meth-ods that decipher the meaning of linguistic en-tities using their usages in large corpora (Lenci,2008).
In these methods, the distributional proper-ties of linguistic entities in various contexts, whichare collected from their observations in corpora,are compared to quantify their meaning.
Vectorspaces are intuitive, mathematically well-definedframeworks to represent and process such infor-mation.1In a vector space model (VSM), linguis-tic entities are represented by vectors and a dis-tance formula is employed to measure their distri-butional similarities (Turney and Pantel, 2010).In a VSM, each element ~siof the standard basisof the vector space (informally, each dimension ofthe VSM) represents a context element.
Given ncontext elements, an entity whose meaning is be-ing analyzed is expressed by a vector ~v as a linearcombination of ~siand scalars ?i?
R such that~v = ?1~s1+ ?
?
?+?n~sn.
The value of ?iis derivedfrom the frequency of the occurrences of the entitythat ~v represents in/with the context element that~sirepresents.
As a result, the values assigned tothe coordinates of a vector (i.e.
?i) exhibit the cor-relation of entities and context elements in an n-dimensional real vector space Rn.
Each vector canbe written as a 1?n row matrix, e.g.
(?1, ?
?
?
, ?n).Therefore, a group of m vectors in a vector spaceis often represented by a matrix Mm?n.Latent semantic analysis (LSA) is a famil-iar technique that employs a word-by-documentVSM (Deerwester et al., 1990).2In this word-by-document model, the meaning of words (i.e.the linguistic entities) is described by their occur-rences in documents (i.e.
the context elements).Given m words and n distinct documents, eachword is represented by an n-dimensional vector~vi= (?i1, ?
?
?
, ?in), where ?ijis a numeric valuethat associates the word ~virepresents to the doc-ument dj, for 1 < j < n. For instance, thevalue of ?ijmay correspond to the frequency ofthe word in the document.
It is hypothesized thatthe relevance of words can be assessed by count-ing the documents in which they co-occur.
There-fore, words with similar vectors are assumed tohave the same meaning (Figure 1).1Amongst other representation frameworks.2See Martin and Berry (2007) for an overview of themathematical foundation of LSA.1713~s1?
d1~s2?
d2~s3?
d3~v1~v2?12?11?13?22?21?23Figure 1: Illustration of a word-by-documentmodel consisting of 2 words and 3 documents.The words are represented in a 3-dimensional vec-tor space, in which each ~si(each dimension) rep-resents each of the 3 documents in the model.~v1= (?11, ?12, ?13) and ~v2= (?21, ?22, ?23)represent the two words in the model.
The dashedline shows the Euclidean distance between the twovectors that represent words, while the sum ofdash-dotted lines is the Manhattan distance be-tween them.In order to assess the similarity between vectors,a vector space V is endowed with a norm struc-ture.
A norm ?.?
is a function that maps vectorsfrom V to the set of non-negative real numbers,i.e.
V 7?
[0,?).
The pair of (V, ?.?)
is then calleda normed space.
In a normed space, the similar-ity between vectors is assessed by their distances.The distance between vectors is defined by a func-tion that satisfies certain axioms and assigns a realvalue to each pair of vectors, i.e.dist : V ?
V 7?
R, d(~v,~t) = ?~v ?
~u?.
(1)The smaller the distance between two vectors, themore similar they are.Euclidean space is the most familiar exampleof a normed space.
It is a vector space that is en-dowed by the `2norm.
In Euclidean space, the `2norm?which is also called the Euclidean norm?of a vector ~v = (v1, ?
?
?
, vn) is defined as?~v?2=????n?i=1v2i.
(2)Using the definition of distance given in Equa-tion 1 and the `2norm, the Euclidean distance ismeasured asdist2(~v, ~u) = ?~v ?
~u?2=????n?i=1(vi?
ui)2.
(3)In Figure 1, the dashed line shows the Euclideandistance between the two vectors.
In `2normedvector spaces, various similarity metrics are de-fined using different normalization of the Eu-clidean distance between vectors, e.g.
the cosinesimilarity.The similarity between vectors, however, canalso be computed in `1normed spaces.3The `1norm for ~v is given by?~v?1=n?i=1|vi|, (4)where |.| signifies the modulus.
The distance in an`1normed vector space is often called the Man-hattan or the city block distance.
According to thedefinition given in Equation 1, the Manhattan dis-tance between two vectors ~v and ~u is given bydist1(~v, ~u) = ?~v ?
~u?1=n?k=1|vi?
uj|.
(5)In Figure 1, the collection of the dash-dotted linesis the `1distance between the two vectors.
Similarto the `2spaces, various normalizations of the `1distance4define a family of `1normed similaritymetrics.As the number of text units that are being mod-elled in a VSM increases, the number of contextelements that are required to be utilized to capturetheir meaning escalates.
This phenomenon is ex-plained using power-law distributions of text unitsin context elements (e.g.
the familiar Zipfian dis-tribution of words).
As a result, extremely high-dimensional vectors, which are also sparse?i.e.most of the elements of the vectors are zero?represent text units.
The high dimensionality ofthe vectors results in setbacks, which are colloqui-ally known as the curse of dimensionality.
For in-stance, in a word-by-document model that consistsof a large number of documents, a word appearsonly in a few documents, and the rest of the doc-uments are irrelevant to the meaning of the word.Few common documents between words results insparsity of the vectors; and the presence of irrele-vant documents introduces noise.Dimension reduction, which usually follows theconstruction of a VSM, alleviates the problems3The definition of the norm is generalized to `pspaceswith ?~v?p=(?i|vi|p)1/p, which is beyond the scope ofthis paper.4As long as the axioms in the distance definition hold.1714listed above by reducing the number of context el-ements that are employed for the construction ofthe VSM.
In its simple form, dimensionality re-duction can be performed using a selection pro-cess: choose a subset of contexts and eliminatethe rest using a heuristic.
Alternatively, transfor-mation methods can be employed.
A transforma-tion method maps a vector space Vnonto a Vmoflowered dimension, i.e.
?
: Vn7?
Vm,m  n.The vector space at reduced dimension, i.e.
Vm,is often the best approximation of the original Vnin a sense.
LSA employs a dimension reductiontechnique called truncated singular value decom-position (SVD).
In a standard truncated SVD, thetransformation guarantees the least distortion inthe `2distances.5Besides the problem of high computationalcomplexity of SVD computation,6which can beaddressed by incremental techniques (see e.g.Brand (2006)), matrix factorization methods suchas truncated SVD are data-sensitive: if the struc-ture of the data being analyzed changes, i.e.
wheneither the linguistic entities or context elementsare updated, e.g.
some are removed or new onesare added, the transformation should be recom-puted and reapplied to the whole VSM to reflectthe updates.
In addition, a VSM at the originalhigh dimension must be first constructed.
Follow-ing the construction of the VSM, the dimensionof the VSM is reduced in an independent process.Therefore, the VSM at reduced dimension is avail-able for processing only after the whole sequenceof these processes.
Construction of the VSM atits original dimension is computationally expen-sive and a delay in access to the VSM at reduceddimension is not desirable.
Hence, the applicationof truncated SVD is not suitable in several appli-cations, particularly when dealing with frequentlyupdated big text?data such as applications in theweb context.Random indexing (RI) is an alternative methodthat solves the problems stated above by combin-ing the construction of a vector space and the di-mensionality reduction process.
RI, which is in-troduced in Kanerva et al.
(2000), constructs aVSM directly at reduced dimension.
Unlike meth-ods that first construct a VSM at its original highdimension and conduct a dimensionality reduction5Please note that there are matrix factorization techniquesthat guarantee the least distortion in the `1distances, see e.g.Kwak (2008).6Matrix factorization techniques, in general.afterwards, the RI method avoids the constructionof the original high-dimensional VSM.
Instead, itmerges the vector space construction and the di-mensionality reduction process.
RI, thus, signifi-cantly enhances the computational complexity ofderiving a VSM from text.
However, the appli-cation of the RI technique (likewise the standardtruncated SVD in LSA) is limited to `2normedspaces, i.e.
when similarities are assessed using ameasure based on the `2distance.
It can be verifiedthat using RI causes large distortions in the `1dis-tances between vectors (Brinkman and Charikar,2005).
Hence, if the similarities are computed us-ing the `1distance, then the RI technique is notsuitable for the VSM construction.Depending on the distribution of vectors ina VSM, the performance of similarity measuresbased on the `1and the `2norms varies from onetask to another.
For instance, it is known thatthe `1distance is more robust to the presence ofoutliers and non-Gaussian noise than the `2dis-tance (e.g.
see the problem description in Ke andKanade (2003)).
Hence, the `1distance can bemore reliable than the `2distance in certain appli-cations.
For instance, Weeds et al.
(2005) suggestthat the `1distance outperforms other similaritymetrics in a term classification task.
In anotherexperiment, Lee (1999) observed that the `1dis-tance gives more desirable results than the Cosineand the `2measures.In this paper, we introduce a novel methodcalled Random Manhattan Indexing (RMI).
RMIconstructs a vector space model directly at re-duced dimension while it preserves the pairwise`1distances between vectors in the original high-dimensional VSM.
We then introduced a compu-tationally enhanced version of RMI called Ran-dom Manhattan Integer Indexing (RMII).
RMIand RMII, similar to RI, merge the constructionof a VSM and dimension reduction into an incre-mental and thus efficient and scalable process.In Section 2, we explain and evaluate the RMImethod.
In Section 3, the RMII method is ex-plained.
We compare the proposed method withRI in Section 4.
We conclude in Section 5.2 Random Manhattan IndexingWe propose the RMI method: a novel techniquethat adapts an incremental procedure for the con-struction of `1normed vector spaces at a reduceddimension.
The RMI method employs a two-step1715procedure: (a) the creation of index vectors and (b)the construction of context vectors .In the first step, each context element is as-signed exactly to one index vector ~ri.
Index vec-tors are high-dimensional and generated randomlysuch that entries rjof index vectors have the fol-lowing distribution:ri=?????
?1U1with probabilitys20 with probability 1?
s1U2with probabilitys2, (6)where U1and U2are independent uniform ran-dom variables in (0, 1).
In the second step, eachtarget linguistic entity that is being analyzed inthe model is assigned to a context vector ~vcinwhich all the elements are initially set to 0.
Foreach encountered occurrence of a linguistic entityand a context element?e.g.
through a sequentialscan of an input text collection?~vcthat representsthe linguistic entity is accumulated by the indexvector ~rithat represents the context element, i.e.~vc= ~vc+ ~ri.
This process results in a VSMof a reduced dimensionality that can be used toestimate the `1distances between linguistic enti-ties.
In the constructed VSM by RMI, the `1dis-tance between vectors is given by the sample me-dian (Indyk, 2000).
For given vectors ~v and ~u, theapproximate `1distance between vectors is esti-mated by?L1(~u,~v) = median{|vi?
ui|, i = 1, ?
?
?
,m}, (7)wherem is the dimension of the VSM constructedby RMI, and |.| denotes the modulus.RMI is based on the random projection (RP)technique for dimensionality reduction.
In RP, ahigh-dimensional vector space is mapped onto arandom subspace of lowered dimension expectingthat?with a high probability?relative distancesbetween vectors are approximately preserved.
Us-ing the matrix notation, this projection is given byM?p?m= Mp?nRn?m, m p, n,(8)where R is often called the random projection ma-trix, and M and M?denote p vectors in the orig-inal n-dimensional and reduced m-dimensionalvector spaces, respectively.In RMI, the stated mapping in Equation 8is given by Cauchy random projections.
Indyk(2000) suggests that vectors in a high-dimensionalspace Rncan be mapped onto a vector space oflowered dimension Rmwhile the relative pairwise`1distances between vectors are preserved with ahigh probability.
In Indyk (2000, Theorem 3) andIndyk (2006, Theorem 5), it is shown that for anm ?
m0= log(1/?
)O(1/), where ?
> 0 and ?
1/2, there exists a mapping from RnontoRmthat guarantees the `1distances between anypair of vectors ~u and ~v in Rnafter the mappingdoes not increase by a factor more than 1 +  withconstant probability ?, and it does not decrease bymore than 1?
 with probability 1?
?.In Indyk (2000), this projection is proved tobe obtained using a random projection matrix Rthat has Cauchy distribution?i.e.
for rijin R,rij?
C(0, 1).
Since R has a Cauchy distribu-tion, for every two vectors ~u and ~v in the high-dimensional space Rn, the projected differencesx =?~u ?
?~v also have Cauchy distribution, withthe scale parameter being the `1distances, i.e.x ?
C(0,?ni=1|ui?
vi|).
As a result, in Cauchyrandom projections, estimating the `1distancesboils down to the estimation of the Cauchy scaleparameter from independent and identically dis-tributed (i.i.d.)
samples x.
Because the expectationvalue of x is infinite,7the sample mean cannot beemployed to estimate the Cauchy scale parameter.Instead, using the 1-stability of Cauchy distribu-tion, Indyk (2000) proves that the median can beemployed to estimate the Cauchy scale parame-ter, and thus the `1distances at the projected spaceRm.Subsequent studies simplified the method pro-posed by Indyk (2000).
Li (2007) shows that Rwith Cauchy distribution can be substituted by asparse R that has a mixture of symmetric 1-Paretodistribution.
A 1-Pareto distribution can be sam-pled by 1/U , where U is an independent uniformrandom variable in (0, 1).
This results in a ran-dom matrix R that has the same distribution asdescribed by Equation 6.The RMI?s two-step procedure is explained us-ing the basic properties of matrix arithmetic andthe descriptions given above.
Given the projectionin Equation 8, the first step of RMI refers to theconstruction of R: index vectors are the row vec-tors of R. The second step of the process refersto the construction of M?
: context vectors are therow vectors of M?.
Using the distributive prop-erty of multiplication over addition in matrices,87That is E(x) =?, since x has a Cauchy distribution.8That is, (A+B)C = AC+BC.1716it can be verified that the explicit construction ofM and its multiplication to R can be substitutedby a number of summation operations.
M can berepresented by the sum of unit vectors in which aunit vector corresponds to the co-occurrence of alinguistic entity and a context element.
The resultof the multiplication of each unit vector and R isthe row vector that represents the context elementin R?i.e.
the index vector.
Therefore, M?can becomputed by the accumulation of the row vectorsof R that represent encountered context elements,as stated in the second step of the RMI procedure.2.1 Alternative Distance EstimatorsAs stated above, Indyk (2000) suggests using thesample median for the estimation of the `1dis-tances.
However, Li (2008) argues that sam-ple median estimator can be biased and inaccu-rate, specifically if m?i.e.
the targeted (reduced)dimensionality?is small.
Hence, Li (2008) sug-gests using the geometric mean estimator insteadof the median sample:9?L1(~u,~v) =(m?i=1|ui?
vi|)1m.
(9)We suggest computing the?L1(~u,~v) in Equation9 using arithmetic mean of logarithm-transformedvalues of |ui?
vi|.
Therefore, using the logarith-mic identities, the multiplications and the power inEquation 9 are, respectively, transformed to a sumand a multiplication:?L1(~u,~v) = exp(1mm?i=1ln(|ui?
vi|)).
(10)Equation 10 for computing?L1is more plausiblefor computational implementation than Equation9 (e.g.
the overflow is less likely to happen dur-ing the process).
Moreover, calculating the medianinvolves sorting an array of real numbers.
Thus,computation of the geometric mean in logarithmicscales can be faster than computation of the me-dian sample, especially when the value of m islarge.2.2 RMI?s ParametersIn order to employ the RMI method for the con-struction of a VSM at reduced dimension and theestimation of the `1distance between vectors, two9See also Li et al.
(2007, Lemma 5?9).model parameters should be decided: (a) the tar-geted (reduced) dimensionality of the VSM, whichis indicated by m in Equation 8 and (b) the num-ber of non-zero elements in index vectors, whichis determined by s in Equation 6.
In contrast to theclassic one-dimension-per-context-element meth-ods of VSM construction,10the value of m in RPsand thus in RMI is chosen independently of thenumber of context elements in the model (n inEquation 8).In RMI, m determines the probability and themaximum expected amount of distortions  in thepairwise distance between vectors.
Based on theproposed refinements of Indyk (2000, Theorem 3)by Li et al.
(2007), it is verified that the pairwise`1distance between any p vectors is approximatedwithin a factor 1 ?
, if m = O(log p/2), with aconstant probability.
Therefore, the value of  inRMI is subject to the number of vectors p in themodel.
For a fixed p, a larger m yields to lowerbounds on the distortion with a higher probabil-ity.
Because a small m is desirable from the com-putational complexity outlook, the choice of m isoften a trade-off between accuracy and efficiency.According to our experiment, m > 400 is suitablefor most applications.The number of non-zero elements in index vec-tors, however, is decided by the number of contextelements n and the sparseness of the VSM ?
atits original dimension.
Li (2007) suggests1O(?
?n)as the value of s in Equation 6.
VSMs employedin distributional semantics are highly sparse.
Thesparsity of a VSM in its original dimension ?
isoften considered to be around 0.0001?0.01.
Asthe original dimension of VSM n is very large?otherwise there would be no need for dimension-ality reduction?the index vectors are often verysparse.
Similar to m, larger s produces smaller er-rors; however, it imposes more processes duringthe construction of a VSM.2.3 Experimental Evaluation of RMIWe report the performance of the RMI methodwith respect to its ability to preserve the rela-tive `1distance between linguistic entities in aVSM.
Therefore, instead of a task-specific evalua-tion, we show that the relative `1distance betweena set of words in a high-dimensional word-by-document model remains intact when the model10That is, n context elements are modelled in an n-dimensional VSM.1717is constructed at reduced dimensionality using theRMI technique.
We further explore the effect ofthe RMI?s parameter setting in the observed re-sults.Depending on the structure of the data that isbeing analyzed and the objective of the task inhand, the performance of the `1distance for sim-ilarity measurement varies from one applicationto another.11The purpose of our reported evalu-ation, thus, is not to show the superiority of the `1distance (thus RMI) to another similarity measure(e.g.
the `2distance or the cosine similarity) andemployed techniques for dimensionality reduction(e.g.
RI or truncated SVD) in a specific task.
If, ina task, the `1distance shows higher performancethan the `2distance, then the RMI technique ispreferable to the RI technique or truncated SVD.Contrariwise, if the `2norm shows higher perfor-mance than the `1, then RI or truncated SVD aremore desirable than the RMI method.In our experiment, a word-by-document modelis first constructed from the UKWaC corpus at itsoriginal high dimension.
UKWaC is a freely avail-able corpus of 2,692,692 web documents, nearly2 billion tokens and 4 million types (Baroni et al.,2009).12Therefore, a word-by-document modelconstructed from this corpus using the classic one-dimension-per-context-element method has a di-mension of 2.69 million.
In order to keep the ex-periments computationally tractable, the reportedresults are limited to 31 words from this model,which are listed in Table 1.In the designed experiment, a word from the listis taken as the reference and its `1distance to theremaining 30 words is calculated using the vec-tor representations in the high-dimensional VSM.These 30 words are then sorted in ascending or-der by the calculated `1distance.
The procedureis repeated for all the 31 words in the list, one byone.
Therefore, the procedure results in 31 sortedlists, each containing 30 words.
Figure 2 shows anexample of the obtained sorted list, in which thereference is the word ?research?.13The procedure described above is replicated toobtain the lists of sorted words from VSMs thatare constructed by the RMI method at reduced11E.g.
see the experiments in Bullinaria and Levy (2007).12UkWaC can be obtained from http://goo.gl/3isfIE.13Please note that the number of possible arrangements of30 words without repetition in a list in which the order isimportant (i.e.
all permutations of 30 words) is 30!.PoS WordsNounwebsite email support softwarestudents skills project researchnhs link services organisationsAdjonline digital mobile sustainableglobal unique excellent disablednew current fantastic innovativeVerbuse visit improve providedhelp ensure developTable 1: Words employed in the experiments.nhsinnovativesustainablefantasticglobaldisabledmobiledigitalimprovedevelopuniqueorganisationsexcellentlinksoftwarecurrentskillsensureemailvisitprovidedonlineprojectwebsitestudentsservicessupporthelpusenewFigure 2: List of words sorted by their `1distanceto the word ?research?.
The distance increasesfrom left to right and top to bottom.dimensionality, when the method?s parameters?i.e.
the dimensionality of VSM and the number ofnon-zero elements in index vectors?are set dif-ferently.
We expect the obtained relative `1dis-tances between each reference word and the 30other words in an RMI-constructed VSM to be thesame as the obtained relative distances in the orig-inal high-dimensional VSM.
Therefore, for eachVSM that is constructed by the RMI technique,the resulting sorted lists of words are compared bythe sorted lists that are obtained from the originalhigh-dimensional VSM.We employ the Spearman?s rank correlation co-efficient (?)
to compare the sorted lists of wordsand thus the degree of distance preservation in theRMI-constructed VSMs at reduced dimensional-ity.
The Spearman?s rank correlation measures thestrength of association between two ranked vari-ables, i.e.
two lists of sorted words in our experi-ments.
Given a list of sorted words obtained fromthe original high-dimensional VSM (listo) and itscorresponding list obtained from a VSM of re-duced dimensionality (listRMI), the Spearman?srank correlation for the two lists is calculated by?
= 1?6?d2in(n2?
1), (11)where diis the difference in paired ranks of wordsin listoand listRMI, and n = 30 is the numberof words in each list.
We report the average of ?over the 31 lists of sorted words, denoted by ?
?, to171810020040080016003200481632640.50.70.91dimension|non-zeroelements|??0.40.60.8?
?Figure 3: The ??
axis shows the observed averageSpearman?
rank correlation between the order ofthe words in the lists that are sorted by the `1dis-tance obtained from the original high-dimensionalVSM and the VSMs that are constructed by RMIat reduced dimensionality using index vectors ofvarious numbers of non-zero elements.indicate the performance of RMI with respect toits ability for distance preservation.
The closer ?
?is to 1, the better the performance of RMI.Figure 3 shows the observed results at a glancewhen the distances are estimated using the median(Equation 7).
As shown in the figure, when the di-mension of the VSM is above 400 and the numberof non-zero elements is more than 12, the obtainedrelative distances from the VSMs constructed bythe RMI technique start to be analogous to the rel-ative distances that are obtained from the origi-nal high-dimensional VSM, i.e.
a high correlation(??
> 0.90).
For the baseline, we report the av-erage correlation of ?
?random= ?0.004 betweenthe sorted lists of words obtained from the high-dimensional VSM and 31 ?
1000 lists of sortedwords that are obtained by randomly assigned dis-tances.Figure 4 shows the same results as Figure 3,however, in minute detail and only for VSMs ofdimension m ?
{100, 400, 800, 3200}.
In theseplots, squares ( ) indicate the ?
?while the error barsshow the best and the worst observed ?
amongstall the sorted lists of words.
The minimum valueof ?-axis is set to 0.611, which is the worst ob-served correlation in the baseline (i.e.
randomlygenerated distances).
The dotted line (?
= .591)shows the best observed correlation in the baselineand the dashed-dotted line shows the average cor-relation in the baseline (?
= ?0.004).
As sug-gested in Section 2.2, it can be verified that anincrease in the dimension of VSMs (i.e.
m) in-creases the stability of the obtained results (i.e.the probability of preserving distances increases).Therefore, for large values of m (i.e.
m > 400),the difference between the best and the worst ob-served ?
decreases; average correlation ???
1 andthe observed relative distances in RMI-constructedVSMs tend to be identical to those in the originalhigh-dimensional VSM.Figure 5 represents the obtained results in thesame setting as above, however, when the dis-tances are approximated using the geometric mean(Equation 10).
The obtained average correlations??
from the geometric mean estimations are al-most identical to the median estimations.
How-ever, as expected, the geometric mean estimationsare more reliable for small values of m; particu-larly, the worst observed correlations when usingthe geometric mean are higher than those observedwhen using the median estimator.
?0.500.510.80.9?m = 100 m = 40020 40 60?0.500.512 12 700.80.9|non-zero elements|?m = 80020 40 602 12 70|non-zero elements|m = 3200Figure 4: Detailed observation of the ob-tained correlation between relative distances inRMI-constructed VSMs and the original high-dimensional VSM.
The `1distance is estimatedusing the median.
The squares denote ??
and the er-ror bars show the best and the worst observed cor-relations.
The dashed-dotted line shows the ran-dom baseline.1719?0.500.510.80.9?m = 100 m = 40020 40 60?0.500.512 12 700.80.9|non-zero elements|?m = 80020 40 602 12 70|non-zero elements|m = 3200Figure 5: The observed results when the `1dis-tance in RMI-constructed VSMs is estimated us-ing the geometric mean.3 Random Manhattan Integer IndexingThe application of the RMI method is hindered bytwo obstacles: float arithmetic operations requiredfor the construction and processing of the RMI-constructed VSMs and the calculation of the prod-uct of large numbers when `1distances are esti-mated using the geometric mean.The proposed method for the generation of in-dex vectors in RMI results in index vectors ofnon-zero elements that are real numbers.
Conse-quently, index vectors and thus context vectors arearrays of floating point numbers.
These vectorsmust be stored and accessed efficiently when usingthe RMI technique.
However, resources that arerequired for the storage and processing of floatingnumbers is high.
Even if the requirement for thestorage of index vectors is alleviated, e.g., usinga derandomization technique for their generation,context vectors that are derived from these indexvectors are still arrays of float numbers.
To tacklethis problem, we suggest substituting the value ofnon-zero elements of RMI?s index vectors (givenin Equation 6) from1Uto integer values of b1Uc,where b1Uc 6= 0.
We argue that the resulting ran-dom projection matrix still has a Cauchy distribu-tion.
Therefore, the proposed methodology to esti-mate the `1distance between vectors is also valid.The `1distance between context vectors mustbe estimated using either the median or the geo-metric mean.
The use of the median estimator?for the reasons stated in Section 2.1?is not plau-sible.
On the other hand, the computation of thegeometric mean can be laborious as the overflowis highly likely to happen during its computation.Using the value of b1Uc for non-zero elements ofindex vectors, we know that for any pair of contextvectors ~u = (u1, ?
?
?
, um) and ~v = (v1, ?
?
?
, vm),if ui6= vithen |ui?
vi| ?
1.
Therefore, for ui6=vi, ln |ui?vi| ?
0 and thus?mi=1ln(|ui?vi|) ?
0.In this case, the exponent in Equation 10 is a scalefactor that can be discarded without a change inthe relative distances between vectors.14Based onthe intuition that the distance between a vector anditself is zero and the explanation given above, in-spired by smoothing techniques and without beingable to provide mathematical proofs, we suggestestimating the relative distances between vectorsusing?L1(~u,~v) =m?i=1ui6=viln(|ui?
vi|).
(12)In order to distinguish the above changes in RMI,we name the resulting technique random Manhat-tan integer indexing (RMII).
The experiment de-scribed in Section 2.2 is repeated using the RMIImethod.
As shown in Figure 6, the obtained resultsare almost identical to the observed results whenusing the RMI technique.
While RMI performsslightly better than RMII in lower dimensions, e.g.m = 400, RMII shows more stable behaviour thanRMI at higher dimensions, e.g.
m = 800.4 Comparison of RMI and RIRMI and RI utilize a similar two-step procedureconsisting of the creation of index vectors and theconstruction of context vectors.
Both methods areincremental techniques that construct a VSM atreduced dimensionality directly, without requiringthe VSM to be constructed at its original high di-mension.
Despite these similarities, RMI and RIare motivated by different applications and math-14Please note that according to the axioms in the distancedefinition, the distance between two numbers is always a non-negative value.
When index vectors consist of non-zero ele-ments of real numbers, the value of |ui?
vi| can be between0 and 1, i.e.
0 < |ui?
vi| < 1.
Therefore, ln(|ui?
vi|) canbe a negative number and thus the exponent scale is requiredto make sure that the result is a non-negative number.1720?0.500.510.80.9?m = 100 m = 40020 40 60?0.500.512 12 700.80.9|non-zero elements|?m = 80020 40 602 12 70|non-zero elements|m = 3200Figure 6: The observed results when using theRMII method for the construction and estimationof the `1distances between vectors.
The method isevaluated in the same setup as the RMI technique.ematical theorems.
As described above, RMI ap-proximates the `1distance using a non-linear esti-mator, which has not yet been employed for theconstruction of VSMs and the calculation of `1distances in distributional approaches to seman-tics.
Moreover, RMI is justified using Cauchy ran-dom projections.In contrast, RI approximates the `2distance us-ing a linear estimator.
RI has initially been justi-fied using the mathematical model of the sparsedistributed memory (SDM)15.
Later, Sahlgren(2005) delineates the RI method using the lemmaproposed by Johnson and Lindenstrauss (1984)?which elucidates random projections in Euclideanspaces?and the reported refinement in Achlioptas(2001) for the projections employed in the lemma.Although both the RMI and RI methods canbe established as ?-stable random projections?respectively for ?
= 1 and ?
= 2?the meth-ods cannot be compared as they address differentgoals.
If, for a given task, the `1norm outperformsthe `2norm, then RMI is preferable to RI.
Con-trariwise, if the `2norm outperforms the `1norm,then RI is preferable to RMI.To support the earlier claim that RI-constructed15See Kanerva (1993) for an overview of the SDM model.
?0.500.510.80.9?m = 400, the `1distance m = 800, the `1distance20 40 60?0.500.512 12 700.80.9|non-zero elements|?m = 400, median estimator20 40 602 12 70|non-zero elements|m = 800, median estimatorFigure 7: Evaluation of RI for the `1distance esti-mation for m = 400 and m = 800 when the dis-tances are calculated using the standard definitionof distance in `1normed spaces and the median es-timator.
The obtained results using RI do not showcorrelation to the `1distances in the original high-dimensional VSM.VSMs cannot be used for the `1distance estima-tion, we evaluate the RI method in the experimen-tal setup that has been used for the evaluation ofRMI and RMII.
In these experiments, however,we use RI to construct vector spaces at reduceddimensionality and estimate the `1distance us-ing Equation 5 (the standard `1distance defini-tion) and Equation 7 (the median estimator) form ?
400, 800.
As shown in Figure 7, the experi-ments support the theoretical claims.5 ConclusionIn this paper, we introduce a novel technique,named Random Manhattan Indexing (RMI), forthe construction of `1normed VSMs directly atreduced dimensionality.
We further suggest theRandom Manhattan Integer Indexing (RMII) tech-nique, a computationally enhanced version of theRMI technique.
We demonstrated the `1distancepreservation ability of the proposed technique inan experimental setup using a word-by-documentmodel.
In these experiments, we showed how thevariable parameters of the methods, i.e.
the num-ber of non-zero elements in index vectors and the1721dimensionality of the VSM, influence the obtainedresults.
The proposed incremental (and thus effi-cient and scalable) methods significantly enhancethe computation of the `1distances in VSMs.AcknowledgementsThis publication has emanated from researchconducted with the financial support of Sci-ence Foundation Ireland under Grant NumberSFI/12/RC/2289.References[Achlioptas2001] Dimitris Achlioptas.
2001.Database-friendly random projections.
In Pro-ceedings of the Twentieth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of DatabaseSystems, PODS ?01, pages 274?281, New York, NY,USA.
ACM.
[Baroni et al.2009] Marco Baroni, Silvia Bernardini,Adriano Ferraresi, and Eros Zanchetta.
2009.
Thewacky wide web: a collection of very large linguis-tically processed web-crawled corpora.
LanguageResources and Evaluation, 43(3):209?226.
[Brand2006] Matthew Brand.
2006.
Fast low-rankmodifications of the thin singular value decom-position.
Linear Algebra and its Applications,415(1):20?30.
Special Issue on Large Scale Linearand Nonlinear Eigenvalue Problems.
[Brinkman and Charikar2005] Bo Brinkman and MosesCharikar.
2005.
On the impossibility of dimensionreduction in l1.
J. ACM, 52(5):766?788.
[Bullinaria and Levy2007] John A. Bullinaria andJoseph P. Levy.
2007.
Extracting semantic repre-sentations from word co-occurrence statistics: Acomputational study.
Behavior Research Methods,39:510?526.
[Deerwester et al.1990] Scott C. Deerwester, Susan T.Dumais, Thomas K. Landauer, George W. Furnas,and Richard A. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Societyof Information Science, 41(6):391?407.
[Indyk2000] Piotr Indyk.
2000.
Stable distribu-tions, pseudorandom generators, embeddings anddata stream computation.
In Foundations of Com-puter Science, 2000.
Proceedings.
41st Annual Sym-posium on, pages 189?197.
[Indyk2006] Piotr Indyk.
2006.
Stable distributions,pseudorandom generators, embeddings, and datastream computation.
J. ACM, 53(3):307?323, May.
[Johnson and Lindenstrauss1984] William Johnson andJoram Lindenstrauss.
1984.
Extensions of Lips-chitz mappings into a Hilbert space.
In Conferencein modern analysis and probability (New Haven,Conn., 1982), volume 26 of Contemporary Mathe-matics, pages 189?206.
American Mathematical So-ciety.
[Kanerva et al.2000] Pentti Kanerva, Jan Kristoferson,and Anders Holst.
2000.
Random indexing of textsamples for latent semantic analysis.
In Proceed-ings of the 22nd Annual Conference of the CognitiveScience Society, pages 103?6.
Erlbaum.
[Kanerva1993] Pentti Kanerva.
1993.
Sparse dis-tributed memory and related models.
In Mo-hamad H. Hassoun, editor, Associative neural mem-ories: theory and implementation, chapter 3, pages50?76.
Oxford University Press, Inc., New York,NY, USA.
[Ke and Kanade2003] Qifa Ke and Takeo Kanade.2003.
Robust subspace computation using `1norm.Technical Report CMU-CS-03-172, School of Com-puter Science, Carnegie Mellon University.
[Kwak2008] Nojun Kwak.
2008.
Principal componentanalysis based on l1-norm maximization.
PatternAnalysis and Machine Intelligence, IEEE Transac-tions on, 30(9):1672?1680, Sept.[Lee1999] Lillian Lee.
1999.
Measures of distribu-tional similarity.
In Proceedings of the 37th An-nual Meeting of the Association for ComputationalLinguistics on Computational Linguistics, ACL ?99,pages 25?32, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.
[Lenci2008] Alessandro Lenci.
2008.
Distributionalsemantics in linguistic and cognitive research.
Fromcontext to meaning: Distributional models of the lex-icon in linguistics and cognitive science, special is-sue of the Italian Journal of Linguistics, 20/1:1?31.
[Li et al.2007] Ping Li, Trevor J. Hastie, and Ken-neth W. Church.
2007.
Nonlinear estimators and tailbounds for dimension reduction in L1using cauchyrandom projections.
J. Mach.
Learn.
Res., 8:2497?2532.
[Li2007] Ping Li.
2007.
Very sparse stable randomprojections for dimension reduction in l?
(0 < ?
<2) norm.
In Proceedings of the 13th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, KDD ?07, pages 440?449, NewYork, NY, USA.
ACM.
[Li2008] Ping Li.
2008.
Estimators and tail bounds fordimension reduction in `?
(0 < ?
?
2) using sta-ble random projections.
In Proceedings of the Nine-teenth Annual ACM-SIAM Symposium on DiscreteAlgorithms, SODA ?08, pages 10?19, Philadelphia,PA, USA.
Society for Industrial and Applied Math-ematics.
[Martin and Berry2007] Dian I. Martin and Michael W.Berry, 2007.
Handbook of latent semantic analysis,chapter Mathematical foundations behind latent se-mantic analysis, pages 35?55.
Ro.1722[Sahlgren2005] Magnus Sahlgren.
2005.
An introduc-tion to random indexing.
In Methods and Applica-tions of Semantic Indexing Workshop at the 7th In-ternational Conference on Terminology and Knowl-edge Engineering, TKE 2005.
[Turney and Pantel2010] Peter D. Turney and PatrickPantel.
2010.
From frequency to meaning: vec-tor space models of semantics.
J. Artif.
Int.
Res.,37(1):141?188, January.
[Weeds et al.2005] Julie Weeds, James Dowdall,Gerold Schneider, Bill Keller, and David Weir.2005.
Using distributional similarity to organisebiomedical terminology.
Terminology, 11(1):3?4.1723
