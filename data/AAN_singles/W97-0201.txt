Getting Serious about Word Sense DisambiguationHwee Tou NgDefence Science Organisation20 Science Park DriveSingapore 118230Republic of Singaporenhweet ou@dso, gov.
sgAbstractRecent advances in large-scale, broad cov-erage part-of-speech tagging and syntacticparsing have been achieved in no small partdue to the availability of large amounts ofonline, human-annotated corpora.
In thispaper, I argue that a large, human sense-tagged corpus is also critical as well as ne-cessary to achieve broad coverage, high ac-curacy word sense disambiguation, wherethe sense distinction is at the level of agood desk-top dictionary such as WORD-NET.
Using the sense-tagged corpus of192,800 word occurrences reported in (Ngand Lee, 1996), I examine the effect of thenumber of training examples on the accur-acy of an exemplar-based classifier versusthe base-line, most-frequent-sense classi-tier.
I also estimate the amount of hu-man sense-tagged corpus and the manualannotation effort needed to build a large-scale, broad coverage word sense disambig-uation program which can significantly out-perform the most-frequent-sense classifier.Finally, I suggest hat intelligent exampleselection techniques may significantly re-duce the amount of sense-tagged corpusneeded and offer this research problem as afruitful area for word sense disambiguationresearch.1 IntroductionMuch recent research in the field of natural lan-guage processing (NLP) has focused on an empirical,corpus-based approach (Church and Mercer, 1993).The high accuracy achieved by a corpus-based ap-proach to part-of-speech tagging and noun phraseparsing, as demonstrated by (Church, 1988), has in-spired similar approaches to other problems in nat-ural language processing, including syntactic parsingand word sense disambiguation (WSD).The availability of large quantities of part-of-speech tagged and syntactically parsed sentenceslike the Penn Treebank corpus (Marcus, Santorini,and Marcinkiewicz, 1993) has contributed greatlyto the development of robust, broad coverage part-of-speech taggers and syntactic parsers.
The PennTreebank corpus contains asufficient number of part-of-speech tagged and syntactically parsed sentencesto serve as adequate training material for buildingbroad coverage part-of-speech taggers and parsers.Unfortunately, an analogous sense-tagged corpuslarge enough to achieve broad coverage, high accur-acy word sense disambiguation is not available atpresent.
In this paper, I argue that, given the cur-rent state-of-the-art capability of automated machinelearning algorithms, a supervised learning approachusing a large sense-tagged corpus is a viable wayto build a robust, wide coverage, and high accuracyWSD program.
In this view, a large sense-taggedcorpus is critical as well as necessary to achievebroad coverage, high accuracy WSD.The rest of this paper is organized as follows.
InSection 2, I briefly discuss the utility of WSD inpractical NLP tasks like information retrieval andmachine translation.
I also address ome objectionsto WSD research.
In Section 3, I examine the sizeof the training corpus on the accuracy of WSD, us-ing a corpus of 192,800 occurrences of 191 wordshand tagged with WORDNET senses (Ng and Lee,1996).
In Section 4, I estimate the amount of humansense-tagged corpus and the manual annotation ef-fort needed to build a broad coverage, high accuracyWSD program.
Finally, in Section 5, I suggest hatintelligent example selection techniques may signi-ficantly reduce the amount of sense-tagged corpusneeded and offer this research problem as a fruitfularea for WSD research.2 The Utility of Word SenseDisambiguationAlthough there is agreement in general about theutility of WSD within the NLP community, I willbriefly address ome objections to WSD in this sec-tion.
To justify the investment ofmanpower and timeto gather a large sense-tagged corpus, it is importantto examine the benefits brought about by WSD.Information retrieval (IR) is a practical NLP taskwhere WSD has brought about improvement in ac-curacy.
When tested on some standard IR test col-lection, the use of WSD improves precision by about4.3% (from 29.9% to 34.2%) (Schiitze and Peder-sen, 1995).
The work of (Dagan and Itai, 1994) hasalso successfully used WSD to improve the accur-acy of machine translation.
These examples clearlydemonstrate he utility of WSD in practical NLP ap-plications.In this paper, by word sense disambiguation, Imean identifying the correct sense of a word in con-text such that the sense distinction is at the level ofa good desk-top dictionary like WORDNET (Miller,1990).
I only focus on content word disambiguation(i.e., words in the part of speech noun t, verb, ad-jective and adverb).
This is also the task addressedby other WSD research such as (Bruce and Wiebe,1994; Miller et al, 1994).
When the task is to resolveword senses to the fine-grain distinction of WORD-NET senses, the accuracy figures achieved are gen-erally not very high (Miller et al, 1994; Ng and Lee,1996).
This indicates that WSD is a challenging taskand much improvement is still needed.However, if one were to resolve word sense to thelevel of homograph, or coarse sense distinction, thenquite high accuracy can be achieved (in excess of90%), as reported in (Wilks and Stevenson, 1996).Similarly, if the task is to distinguish between bin-ary, coarse sense distinction, then current WSD tech-niques can achieve very high accuracy (in excess of96% when tested on a dozen words in (Yarowsky,1995)).
This is to be expected, since homographcontexts are quite distinct and hence it is a muchsimpler task to disambiguate among a small num-ber of coarse sense classes.
This is in contrast odisambiguating word senses to the refined senses ofWoRDNET, where for instance, the average numberof senses per noun is 7.8 and the average number ofsenses per verb is 12.0 for the set of 191 most am-biguous words investigated in (Ng and Lee, 1996).We can readily collapse the refined senses ofWORDNET into a smaller set if only a coarse (ho-t I will only focus on common oun in this paper andignore proper noun.2mographic) sense distinction is needed, say for someNLP applications.
Indeed, the WORDNET softwarehas an option for grouping noun senses into a smallernumber of sense classes.
WSD techniques that workwell for refined sense distinction will apply equally tohomograph dlsambiguation.
That is, if we succeedin working on the harder WSD task of resolution intorefined senses, the same techniques will also work onthe simpler task of homograph disambiguation.A related objection to WSD research is that thesense distinction made by a good desk-top diction-ary like WOI~DNET is simply too refined, to the pointthat two humans cannot genuinely agree on the most.appropriate sense to assign to some word occurrence(Kilgarriff, 1996).
This objection has some merits.However, the remedy is not to throw out word sensescompletely, but rather to work on a level of sensedistinction that is somewhere in between homographdistinction and the refined WoRVNET sense distinc-tion.
The existing lumping of noun senses in WORD-NET into coarser sense groups is perhaps a goodcompromise.However, in the absence of well acceptedguidelines for making an appropriate l vel of sensedistinction, using the sense classification given inWOI~I)NET, an on-line, publicly available dictionary,seems a natural choice.
Hence, I believe that usingthe current WoRDNET sense distinction to build asense-tagged corpus is a reasonable approach to goforward.
In any case, if some aggregation of sensesinto coarser grouping is done in future, this can bereadily incorporated into my proposed sense-taggedcorpus which uses the refined sense distinction ofWOItDNET.In the rest of this paper, I will assume that broadcoverage, high accuracy WSD is indeed useful inpractical NLP tasks, and that resolving senses to therefined level of WORDNET is a worthwhile task topursue.3 The Effect of Training Corpus SizeA number of past research work on WSD, suchas (Leacock et al, 1993; Bruce and Wiebe, 1994;Mooney, 1996), were tested on a small numberof words like "line" and "interest".
Similarly,(Yarowsky, 1995) tested his WSD algorithm on adozen words.
The sense-tagged corpus SEMCOI~,prepared by (Miller et al, 1994), contains a sub-stantial subset of the Brown corpus tagged with therefined senses of WORDNET.
However, as reportedin (Miller et al, 1994), there are not enough train-ing examples per word in SP.MCOR to yield a broadcoverage, high accuracy WSD program, due to thefact that sense tagging is done on every word in arunning text in SEMCOR.To overcome this data sparseness problem ofWSD,  I initiated a mini-project in sense tagging andcollected a corpus in which 192,800 occurrences of191 words have been manually tagged with senses ofWORDNET (Ng and Lee, 1996).
These 192,800 wordoccurrences consist of only 121 nouns and 70 verbswhich are the most frequently occurring and mostambiguous words of English.
2To investigate the effect of the number of train-ing examples on WSD accuracy, I ran the exemplar-based WSD algorithm L~.XAS on varying number oftraining examples to obtain learning curves for the191 words (details of LEXAS are described in (Ng andLee, 1996)).
For each word, 10 random trials wereconducted and the accuracy figures were averagedover the I0 trials.
In each trial, I00 examples wererandomly selected to form the test set, while the re-maining examples (randomly shuffled) were used fortraining.
LEXAS was given training examples inmul-tiple s of i00, starting with I00,200,300, ... trainingexamples, up to the maximum number of trainingexamples (in a multiple of 100) available in the cor-pus.Note that each word w (of the 191 words) can havea different number of sense-tagged occurrences inourcorpus.
From the combination of Brown corpus (1million words) and Wall Street Journal corpus (2.5million words), up to 1,500 sentences each contain-ing an occurrence of the word w are extracted fromthe combined corpus, with each sentence containinga sense-tagged occurrence of w. When the combinedcorpus has less than 1,500 occurrences ofw, the max=imum number of available occurrences of w is used.For instance, while 137 words have at least 600 oc-currences in the combined corpus, only a subset of43 words has at least 1400 occurrences.
Figure 1and 2 show the learning curves averaged over these43 words and 137 words with at least 1300 and 500training examples, respectively.
Each figure showsthe accuracy of LEXAS versus the base-line, most-frequent-sense classifier.Both figures indicate that WSD accuracy contin-ues to climb as the number of training examples in-creases.
They confirm that all the training examplescollected in our corpus are effectively utilized byLZXAS to improve its WSD performance.
In fact,it appears that for this set of most ambiguous wordsof English, more training data may be beneficial tofurther improve WSD performance.I also report here the evaluation of LP.XAS on two2This corpus is scheduled for release by the lAn-guistic Data Consortium (LDC).
Contact the LDC atldc~unagi.cis.upenn.edu for details.Test setBC50WSJ6Sense 1 Most Frequent LEXAS~1d.5% 47.1% 58.7%44.8% 63.7% 75.2%Table 1: Evaluation of LEXASsubsets of test sentences of our sense-tagged corpus,as shown in Table 1.The two test sets, BC50 and WSJ6, are the sameas those reported in (Ng and Lee, 1996).
BC50 con-sists of 7,119 occurrences of the 191 words that occurin 50 text files of the Brown corpus.
The second testset, WSJ6, consists of 14,139 occurrences of these191 words that occur in 6 text files of the Wall StreetJournal corpus.The performance figures of LEXAS in Table 1 arehigher than those reported in (Ng and Lee, 1996).The classification accuracy of the nearest neighboralgorithm used by LEXAS (Cost and Salzberg, 1993)is quite sensitive to the number of nearest neighborsused to select he best matching example.
By using10-fold cross validation (Kohavi and John, 1995) toautomatically pick the best number of nearest neigh-bors to use, the performance of LSXAS has improved.4 Word Sense Disambiguation in theLargeIn (Gale et al, 1992), it was argued that any widecoverage WSD program must be able to perform sig-nificantly better than the most-frequent-sense classi-fier to be worthy of serious consideration.
The per-formance of LEXAS as indicated in Table 1 is signi-ficantly better than the most-frequent-sense classi-fier for the set of 191 words collected in our corpus.Figure 1 and 2 also confirm that all the training ex-amples collected in our corpus are effectively utilizedby LEXAS to improve its WSD performance.
Thisis encouraging as it demonstrates the feasibility ofbuilding a wide coverage WSD program using a su-pervised learning approach.Unfortunately, our corpus only contains taggedsenses for 191 words, and this set of words doesnot constitute a sufficiently large fraction of all oc-currences of content words in an arbitrarily chosenunrestricted text.
As such, our sense-tagged corpusis still not large enough to enable the building ofa wide coverage, high accuracy WSD program thatcan significantly outperform the most-frequent-senseclassifier over all content words encountered in an ar-bitrarily chosen unrestricted text.This brings us to the question: how much datado we need to achieve wide coverage, high accuracyWSD?365.064.063.062.061.0AccuracYan n(%) ....59.058.057.056.055.0I I I I I .0 " "  .
l~ . '
' 'w?
?
0  ?
"@@,*0.
@"?e .
?
?
0 .O?
- e "??
Lexas .*.-.."
Most ~eq.
.
.
.
-0?
Q*  ?
?
?
?
?
*  ?
.
?
.O***Q ?
**O* ?
*  *?
.0 .
**  * .10  * ?
?
1*0? "
"  I I I I0 200 400 600 800 1000 1200 1400Number of training examplesFigure 1: Effect of number of training examples on WSD accuracy averaged over 43 words with at least 1300training examples63.062.061.060.0 Accuracy(%)59.058.057.056.0100I I I I.e "??*??
,I I I***  ?
i, ?
0  ?
", .. ?
?
???.."
Lexas "*" -..'" Most f req .
.
, .
-DDO IOOI gI I I I I I I150 200 250 300 350 400 450 500Number of training examplesFigure 2: Effect of number of training examples on WSD accuracy averaged over 137 words with at least500 training examples4POS 80% 90% 95% 99%noun 975 1776 2638 4510verb 242 550 926 1806adj 374 769 1286 2384adv 36 76 128 269sum 1627 3171 4978 8969Table 2: Number of polysemous words in each partof speech making up the top 80%, ..., 99% of wordoccurrences in the Brown corpus.POS 80% 90% 95% 99%noun 472 946 1520 3130verb 203 429 707 1487adj 171 402 761 1748adv 35 69 104 206sum 881 1846 3092 6571Table 3: Number of polysemous words in each partof speech making up the top 80%, ..., 99% of wordoccurrences in the Wall Street Journal corpus.To shed light on this question, it is instructive toexamine the distribution of words and their occur-rence frequency in a large corpus.
Table 2 lists thenumber of polysemous words in each part of speechmaking up the top 80%, ..., top 99% of word occur-rences in the Brown corpus, where the polysemouswords are ordered in terms of their occurrence fre-quency from the most frequently occurring word tothe least frequently occurring word.
For example,Table 2 indicates that when the polysemous nounsare ordered from the most frequently occurring nounto the least frequently occurring noun, the top 975polysemous nouns constitute 80% of all noun occur-rences in the Brown corpus.
This 80% of all noun oc-currences include all nouns in the Brown corpus thatare monosemous (about 15.4%) and all rare nounsin the Brown corpus that do not appear in WORD-NP.T and hence have no valid sense definition (about3.3%) (i.e., the remaining 20% noun occurrences areall polysemous).
Table 3 lists the analogous statisticsfor the Wall Street Journal corpus.It is also the case that the last 5%-10% of poly-semous words in a corpus have only a small numberof distinct senses on average.
Table 4 lists the av-erage number of senses per polysemous word in theBrown corpus for the top 80%, ..., top 99%, and thebottom 20%, ..., bottom 1% of word occurrences,where the words are again ordered from the mostfrequently occurring word to the least frequently oc-curring word.
For example, the average number ofsenses per polysemous noun is 5.14 for the nounswhich account for the top 80% noun occurrences in5the Brown corpus.
Similarly, the average numberof senses per polysemous noun is 2.86 for the poly-semous nouns which account for the bottom 20% ofnoun occurrences in the Brown corpus.
Table 5 liststhe analogous statistics for the Wall Street Journalcorpus.Table 2 and 3 indicate that a sense-tagged corpuscollected for 3,200 words will cover at least 90% ofall (content) word occurrences in the Brown corpus,and at least 95% of all (content) word occurrences inthe Wall Street Journal corpus.
From Table 4, theaverage number of senses per polysemous word inthe Brown corpus for the remaining 10% word occur-rences is only 3.15 or less.
Similarly, from Table 5,the average number of senses per polysemous wordin the Wall Street Journal corpus for the remaining5% word occurrences is only 3.10 or less.
For theseremaining polysemous words which account for thelast 5%-10% word occurrences with an average ofabout 3 senses per word, we can always assign themost frequent sense as a first approximation in build-ing our wide coverage WSD program.Based on these figures, I estimate that a sense-tagged corpus of 3,200 words is sufficient o builda broad coverage, high accuracy WSD programcapable of significantly outperforming the most-frequent-sense classifier on average over all contentwords appearing in an arbitrary, unrestricted Eng-lish text.
Assuming an average of 1,000 sense-taggedoccurrences per word, this will mean a corpus of 3.2million sense-tagged word occurrences.
Assuminghuman sense tagging throughput at 200 words, or200,000 word occurrences, per man-year (which isthe approximate human tagging throughput of mycompleted sense-tagging mini-project), such a cor-pus will require about 16 man-years to construct.Given the benefits of a wide coverage, high accur-acy and domain-independent WSD program, I be-lieve it is justifiable to spend the 16 man-years ofhuman annotation effort needed to construct such asense-tagged corpus.5 Can We Do Better?My estimate of the amount of human annotation ef-fort needed can be considered as an upper bound onthe manual effort needed to construct the necessarysense-tagged corpus to achieve wide coverage WSD.It may turn out that we can achieve our goal withmuch less annotation effort.Recent work on intelligent example selection tech-niques suggest hat the quality of the examples usedfor supervised learning can have a large impacton the classification accuracy of the induced classi-tier.
For example, in (Engelson and Dagan, 1996),POS top 80%noun 5.14verb 8.75adj 5.87adv 4.22top 90%4.486.894.753.79top 95%4.075.774.083.48top 99%3.514.533.472.96bottom 20% bottom I0%2.86 2.713.43 3.152.86 2.722.55 2.46bottom 5% bottom 1%2.59 2.442.94 2.672.63 2.442.38 2.31Table 4: Average number of senses per polysemous word in the Brown corpus for the top 80%, ..., top 99%,and the bottom 20%, .. .
,  bottom 1% of word occurrences.POSnoun 5.44verb 8.72adj 6.13adv 4.00top 80% top 90% top 95% top 99% bottom 20%4.897.135.333.674.506.194.633.553.834.753.763.143.083.523.092.62bottom 10% bottom 5% bottom 1%2.95 2.83 2.603.30 3.10 2.872.95 2.81 2.602.56 2.48 2.37Table 5: Average number of senses per polysemous word in the Wall Street Journal corpus for the top 80%,.
.
.
,  top 99%, and the bottom 20%, .
.
.
,  bottom 1% of word occurrences.committee-based sample selection is applied to part-of-speech tagging to select for annotation only thoseexamples that are the most informative, and thisavoids redundantly annotating examples.
Similarly,in (Lewis and Catlett, 1994), uncertainty samplingof training examples achieved better accuracy thanrandom sampling of training examples for a text cat-egorization application.Intelligent example selection for supervised learn-ing is an important issue of machine learning in itsown right.
I believe it is of particular importanceto investigate this issue in the context of word sensedisambiguation, as the payoff is high, given that alarge sense tagged corpus is currently not availableand remains one of the most critical bottlenecks inachieving wide coverage, high accuracy WSD.ReferencesRebecca Bruce and Janyce Wiebe.
1994.
Word-sense disambiguation using decomposable mod-els.
In Proceedings of the 3~nd Annual Meetingof the Association for Computational Linguistics,Las Cruces, New Mexico.Kenneth Church.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.
InProceedings of the Second Conference on AppliedNatural Language Processing (ANLP), pages 136-143.Kenneth W. Church and Robert L. Mercer.
1993.Introduction to the special issue on computationallinguistics using large corpora.
ComputationalLinguistics, 19(1):1-24.Scott Cost and Steven Salzberg.
1993.
A weightednearest neighbor algorithm for learning with sym-bolic features.
Machine Learning, 10(1):57-78.Ido Dagan and Alon Itai.
1994.
Word sense dis-ambiguation using a second language monolingualcorpus.
Computational Linguistics, 20(4):563-596.Scan P. Engelson and Ido Dagan.
1996.
Minimiz-ing manual annotation cost in supervised train-ing from corpora.
In Proceedings of the 3~thAnnual Meeting of the Association for Computa-tional Linguistics (A CL), pages 319-326.William Gale, Kenneth Ward Church, and DavidYarowsky.
1992.
Estimating upper and lowerbounds on the performance of word-sense disam-biguation programs.
In Proceedings of the 30thAnnual Meeting of the Association for Computa-tional Linguistics, Newark, Delaware.Adam Kilgarriff.
1996.
"I don't believe in wordsenses", manuscript.Ron Kohavi and George H. John.
1995.
Automaticparameter selection by minimizing estimated er-ror.
In Machine Learning: Proceedings of theTwelfth International Conference.Claudia Leacock, Geoffrey Towell, and EllenVoorhees.
1993.
Corpus-based statistical senseresolution.
In Proceedings of the ARPA HumanLanguage Technology Workshop.David D. Lewis and Jason Catlett.
1994.
Heterogen-eous uncertainty sampling for supervised learning.In Machine Learning: Proceedings ofthe EleventhInternational Conference.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building a large6annotated corpus of English: The Penn Treebank.Computational Linguistics, 19(2):313-330.George A. Miller, Ed.
1990.
WordNet: An on-linelexical database.
International Journal of Lexico.graphy, 3(4):235-312.George A. Miller, Martin Chodorow, Shari Landes,Claudia Leacock, and Robert G. Thomas.
1994.Using a semantic oncordance for sense identific-ation.
In Proceedings of the ARPA Human Lan-guage Technology Workshop.Raymond J. Mooney.
1996.
Comparative xperi-ments on disambiguating word senses: An illus-tration of the role of bias in machine learning.In Proceedings of the First Conference on Em-pirical Methods in Natural Language Processing(EMNLP).Hwee Tou Ng and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsense: An exemplar-based approach.
In Proceed-ings of the 3,1th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 40-47.Hinrich Schiitze and Jan O. Pedersen.
1995.
In-formation retrieval based on word senses.
In Sym-posium on Document Analysis and InformationRetrieval.Yorick Wilks and Mark Stevenson.
1996.
The gram-mar of sense: Is word-sense tagging much morethan part-of-speech tagging?
In ComputationalLinguistics Eprint Archive, cmp-lg/9607028.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics, Cam-bridge, Massachusetts.7
