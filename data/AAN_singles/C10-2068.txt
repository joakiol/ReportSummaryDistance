Coling 2010: Poster Volume, pages 597?604,Beijing, August 2010Generating Simulated Relevance Feedback: A Prognostic SearchapproachNithin Kumar M and Vasudeva VarmaSearch and Information Extraction Lab,International Institute of Information Technology Hyderabad,nithin m@research.iiit.ac.in and vv@iiit.ac.inAbstractImplicit relevance feedback has provedto be a important resource in improv-ing search accuracy and personalization.However, researchers who rely on feed-back data for testing their algorithms orother personalization related problems areloomed with problems like unavailabil-ity of data, staling up of data and soon.
Given these problems, we are mo-tivated towards creating a synthetic userrelevance feedback data, based on insightsfrom query log analysis.
We call this sim-ulated feedback.
We believe that simu-lated feedback can be immensely benefi-cial to web search engine and personaliza-tion research communities by greatly re-ducing efforts involved in collecting userfeedback.
The benefits from ?Simulatedfeedback?
are - it is easy to obtain andalso the process of obtaining the feed-back data is repeatable, customizable anddoes not need the interactions of the user.In this paper, we describe a simple yeteffective approach for creating simulatedfeedback.
We have evaluated our systemusing the clickthrough data of the usersand achieved 77% accuracy in generatingclick-through data.1 IntroductionImplicit relevance feedback serves as a greatsource of information about user behaviour andsearch context.
A lot of research went throughin the recent past in making use of this great poolof information.
Relevance feedback is proven tosignificantly improve retrieval performance (Har-man, 1992; Salton and Buckley, 1990).
It has alsobeen successfully used to improve searching rank-ing, query expansion, personalization, user pro-filing et cetera (Steve Fox et al,, 2005; Rocchio,1999; Xuehua et al, 2005).Clickthrough data is the most prevalent form ofimplicit feedback used by researchers for person-alization purposes.
Click log data provides valu-able information about the interests, preferencesand semantic search intent of the user (Daniel andLevinson, 2004; Kelly and Belkin, 2001).
Unlikeexplicit feedback, clicks logs do not require anyspecial effort from the user (Rocchio, 1999).
It iscollected in the background while the user inter-acts with the search engine to quench his informa-tion need.
Hence, it is easy and feasible to collectlarge amounts of clickthrough data.However, using clickthrough data has its ownshare of problems.
Firstly, it is not available forpublic or even research communities at large forreasons like being a potential threat to privacy ofweb users.
Secondly, it only contains the URLsof the results that the user clicked and does notcontain the documents that the user has chosen.Given the dynamic nature of the web, content ofmany of the urls is prone to change and in somecases it might not exist.
In other cases, even ifthe old expected results remain good resources,search engines might not retrieve them in responseto queries.
It will return near-duplicate pages thathave equivalent content but different URLs.
Thusfeedback data may rapidly become stale with newpages replacing old ones as more approporiate re-sources.
And also, given the rapidly changingranking algorithms of web search engines, feed-597back data collected from the users becomes out-dated.
Hence researchers who rely on feedbackdata either for testing their algorithms or otherpersonalization related problems are faced withthe problems of non-availability of user feedbackdata.In this paper, we strive to address the aboveproblems by generating simulated relevance feed-back using prognostic search techniques.
Prog-nostic search is a process of simulating user?ssearch process and emulating their actions,through preferences captured in their profile.
Suchgenerated feedback can be used for research inpersonalization techniques and analyzing person-alization algorithms and search ranking func-tions(Harman, 1988).
The main advantage withthis system is that we can create data on the fly andhence not fear of it becoming stale.
Since it doesnot involve user?s actions, it is feasible to generatelarge amounts of data in this way.2 Contributions and OrganizationIn this paper, we propose a novel way of creat-ing simulated feedback.
The data thus producedcan be used for evaluating/training personaliza-tion systems.
Using our proposed method, givena user?s training data, we can produce syntheticimplicit feedback data - simulated feedback dataon the fly.
We also propose a novel user browsingmodel which extends the high performing cascademodel of (Craswell et al, 2008).
Our Patience pa-rameter can be used to build more complex userbrowsing models to bring the whole process ofgenerating implicit feedback data a step nearer tothe real world mechanisms.In section 3, we describe our approach to gener-ate simulated feedback data.
In sections 3.2.3 and3.2.4, we describe the process of browsing resultsand generating clicks which form the crux of ourapproach.
We evaluate our system and prove theusefulness of it in section 4.
Section 5 and 6 givean account of our experiments and the study ofworks related to ours already present in the litera-ture.
We conclude that our proposed approach canbe highly useful in personalization research andgive an account of our future directions in section7.Figure 1: System architecture3 Proposed ApproachSimulated feedback is a new type of feedbacksimilar to implicit and explicit relevance feedback.Simulated feedback is created by observing andanalyzing real world search log data.
We proposea two phase process to create simulated relevancefeedback as follows: In phase 1, we process realworld click-through data of a search engine andbuild user profiles using the data.
In phase 2, wesimulate a user?s search process and emulate theiractions based on their profile.
We call this processas ?Prognostic Search?.3.1 Creating ProfilesAfter closely examining and analyzing the seman-tics of the query log, we have chosen the followingparameters to characterize a user: an anonymoususer-id, perceived relevance threshold, patience,previous queries issued and search history of theuser.A user-id is used to distinguish and uniquelyidentify each and every user.
Perceived relevanceis the relevance estimate of the result accordingto the user on examining the title, snippet and theurl of the result.
And Perceived relevance thresh-old is the threshold limit of perceived relevanceof a result for the user to click it.
Patience of theuser is the trait which determines the number ofclicks and the depth to which the user examinesthe results.
We explain the process of comput-ing a user?s patience parameter in detail in section3.2.3.
We stored the previous queries and clicksof the user to capture the preferences of the user.To make use of the search history, we usedthe previous queries issued and previous resultsclicked by the user.
We store the titles and snip-pets of those results to capture the interests of the598user.
Here, our aim is to generate implicit rel-evance feedback which is very close to the realworld data.
To generate synthetic relevance feed-back, we instantiate these parameters with appro-priate values using real world data.3.2 Prognostic SearchPrognostic search is simulation of a user?s searchprocess and emulating their actions based ontheir interests and preferences captured in theiruser profile.
Simulating search process involvesfour steps viz., i)Query formulation, ii)Searching,iii)Browsing results and iv)Generating Clicks.Each of these processes are explained below.3.2.1 Query FormulationQuery formulation involves cognitive processof the user and requires background knowledgeabout the user like their interests, preferences andtheir knowledge base.
It is highly impossible tocapture the cognitive thought process of a user andemulate their method of generating a query.
Tosolve this problem, we randomly select a searchsession from a user?s history and send all thequeries in it sequentially to the search engine.This helps us to preserve the inter query rela-tions that naturally exist between the subsequentqueries in a session.3.2.2 SearchingThis step involves retrieving documents rele-vant to the query generated in the previous step.We used yahoo search engine which is very muchsimilar to the search engine from which the train-ing data is collected.3.2.3 Browsing resultsIn this step, we simulate the manner in which auser browses the results in the real world.
Basedon the observations in (Granka et al, 2004; Filipand Joachims, 2005), we assume that the userin the real world follows the browsing modelexplained in Algorithm 1.
In real world, a usermay follow more complex browsing models,but presently we have considered this browsingmodel to simplify things.Accordingly, to simulate the browsing processof the user explained in algorithm 1, we followedAlgorithm 1 User browsing model in real worldStep1: Start browsing with the top-most result.Step2: Examine title, snippet and URL of the re-sult.Step3: Click if the result looks promising.Step4: If(user has patience) go to step 5, else goto step 6.Step5: Select next result and go to step 2.Step6: Start examining the clicked results.Step7: If(information need satisfied) end the pro-cess, else go to step 8.Step8: Reformulate the query and go to step 1.the Algorithm 2.Algorithm 2 Simulated User browsing modelStep 1: Determine the number of results to bebrowsed based on patience parameter.Step 2: Browse the results in increasing order oftheir ranks and examine them.Step 3: Compute the perceived relevance score ofthe results.Step 4: In the same order, generate clicks basedon the perceived relevance scores of the results.Step 5: If(session has more queries) go to step 6,else end the process.Step 6: Select next query in the session and go tostep 1.Thus based on the patience parameter, wedetermine the number of results that the userbrowses.
In our analysis of query log parame-ters, we learned that the patience value of a usercan be characterized by the following parameters:number of clicks per session, maximum rank ofthe result clicked in a session, time spent in a ses-sion, the number of queries issued per second andthe average semantic relevance of the top ten re-sults of that session to the user.
We found outthat the patience of the user is directly propor-tional to the maximum rank of the result he hasclicked in a session.
We also found out that thenumber of clicks a user generates is inversely pro-portional to the number of queries he issues persecond and directly proportional to the amount oftime he spends per session.
Thus, a user with599more patience tends to examine more search re-sults and thus generate more clicks based on theirrelevance.
We explain these dependencies in de-tail in the experiments section.
So in order to learnthe Patience parameter of the user, we devised thefollowing formula:Patience = ??
(MR?
T ?
C ?
Sqi)Q (1)Here MR denotes the average of maximum rankof the results clicked by the user in a session, Tdenotes the average time spent in a session, C isthe average number of clicks in a session and Qdenotes the average number of queries issued persession and Sqi is the average semantic distanceof the top ten results of the query ?q?i.
Here, ????
isan equalization constant.3.2.4 Generating clicksThis is the most important step in our simula-tion process.
Typically, a user observes the visualinformation viz., title, snippet and the URL of aresult(Joachims et al, 2005).
Then based on theirinterests, they choose the results relevant to them.Similarly, we closely examine the results selectedin the previous step and then score them accordingto their relevance to the user.
We consider the title,snippet and the page-rank of the result and deter-mine its relevance to the user known as perceivedrelevance score.We first compute the semantic distance betweenthe title and snippet of the present result from thetitles and snippets of previously clicked results ofthe user.
The results already clicked by the userserve as a knowledge base of the interests andpreferences of the user.
Thus, the semantic dis-tance between the present result and the previousresult gives us an account of the relevance that thepresent result carries to the user.We used latent semantic analysis (LSA) to com-pute the semantic distance between the results.LSA does not take the dictionary meaning of thewords as input; it rather extracts the contextualmeaning of the word with respect to all otherwords in semantic space(Landauer et al, 2007).This property of LSA is very much useful in thepresent context.
A particular word may have alot of meanings but we are concerned about onlythose meanings of the word which the user inter-prets, which are captured in the sentences presentin the user?s click history.
Hence, we used LSAto compute the semantic distance between the re-sults.We also consider the page-rank of the result,which has proven to be an important factor inmaking the decision of a click.
In our study,we found that for about 89% of the queries withclicks, the top ranked document has been clickedand for 56% of the queries second ranked docu-ment has been clicked.
In Figure 3, we show theclick ratio for each of the top ten ranked docu-ments1.
Thereby, we derive that the rank of theresult is also a very important factor in decidingwhether a result has to be clicked or not.
We alsoconsider the distance of the present result fromthe previous click of the user.
In (Joachims etal., 2005), it is shown that the user is more bi-ased to click the result that immediately followsthe result he previously clicked.
In our simula-tion process, if this distance for any result exceeds10, then we terminate the browsing process andreformulate the query.
We believe that when thisdistance exceeds 10, it signifies that the quality ofthe results is low and hence can be ignored.We used the bayesian probabilistic techniquesto calculate the probability of the user clicking aresult based on the above discussed factors.
HenceClick being a Bernoulli variable, we haveP (c/R, q, u) = ?cR,q,u (1?
?R,q,u)1?c (2)Where ?R,q,u is the probability that user ?u?clicks the result ?R?
for a query ?q?.
We modelthe probability of a click, P (c/R, q, u) as a jointprobability of P(c,r,Rel,D) where ?r?
denotes therank of the result, ?Rel?
denotes the semantic rel-evance score of the result to the user ?
preciselyto his previous clicks ?
and ?D?
denotes the dis-tance of the previous click of the user.
We use thisprobability of the result as the Perceived relevancescore of the result.
Thus, we have:1In figure 3, we have normalized the clicks statistics withthe number of clicks for top ranked document.
So, the click-ratio for the top ranked document will be 1.600Figure 2: Graph showing Precision and Recall ofgenerating clicks for a particular userPerceived relevance = P (c/R, q, u) =P (c/r,Rel,D) ?
ln [P (r/c)] +ln [P (Rel/c)] + ln [P (D/c)] + ln [P (ci+1)] (3)Here, ?r?
denotes the rank of the result, ?Rel?denotes the perceived relevance of the result tothe user and ?D?
denotes the distance of the re-sult from the user?s previous clicked result.
Priorprobablities of each of these factors are calculatedfrom the data stored in the user profile.
We usedLaplace smoothing techniques to deal with zeroprobability entries.
P (ci+1) is the probability thatthe user may click a result after clicking ?i?
re-sults.
We also believe that the behaviour of theuser changes with each click he generates in a ses-sion.
Hence we used the factor P (ci+1) in de-termining the probability of the click2.
Then, wecompare this score with the Perceived relevancethreshold of the user and generate the clicks ac-cordingly.Computing Perceived Relevance Threshold: Us-ing the above formula, we generated clicks fordifferent values of Perceived Relevance Thresholdfor a user.
Figure 2 show the precision and recallvalues of generating clicks for different values ofPerceived Relevance Threshold of a user.
Thus,we plot the accuracy of our system for differentvalues of Patience Relevance Threshold and ac-cordingly set the threshold selecting the best val-ues for precision and recall of the system.4 ExperimentsClickthrough data is a valuable source of userinformation.
In our statistical analysis of click-2We used laplace smoothing technique to negate the ef-fect of zero probability instances.Figure 3: Ranks Vs Clicks-ratiothrough data, we have found that the page-rank ofa result can highly influence the user to make aclick which can be seen in figure 3.In our definition of Patience, we termed it asparameter to denote the depth to which the userexamines the results and the number of clicks hegenerates.
In equation 1, we show that the pa-tience value is inversely proportinal to the numberof queries the user issues in a session.
To provethis fact, we made a statistical analysis on the realworld querlogs3.
From the graphs shown in fig-ure 4, it can be clearly seen that the Patience of theuser is inversely proportional to the user?s numberof Queries/sec.
These graphs show the influenceof the factor Queries/sec on the number of clicksthe user generates for a query and the maximumrank clicked by the user in a session.
We drew thegraphs averaging the different queries/sec value ofa user in a session for each value of MR and num-ber of clicks respectively.
It is evident that boththe graphs are weakly decreasing functions.
Sincemaximum rank clicked and the number of clicksper session directly affect the Patience parameter,we can say that Queries/sec is inversely propor-tional to the Patience of the user.Both the graphs show occasional phases of in-creasing behaviour which can be attributed to avariety of reasons.
While plotting the graphs, fora given value of MR/number of clicks, we takeobservations from numerous sessions of the userand average the queries/sec value.
Thus, presenceof some outlier values may affect the overall out-3We performed these experiments on the query log dataof a popular commerical search engine.
The data consists of21 million web queries collected from 650,000 users.
Thequery log data consists of anonymous id given to the user,query, the time at which the query was posed, rank of theclicked URL (if any) and the URL of the document clickedby the user (if any).601Figure 4: Clicks Vs Queries/sec and MR VsQueries/secput of the graph.
It can also be attributed to thelow quality of results that the search engine mighthave returned due to various reasons.5 Related WorkAlthough simulation-based methods have beenused to test query modification techniques (Har-man, 1988) or to detect shifts in the interestsof computer users (Mostafa et al, 2003), to ourknowledge not much research went into creat-ing relevance feedback for web search based onsearch simulations.Searcher simulations were created by White etal (Mostafa et al, 2003; White et al, 2005), forevaluating implicit feedback models.
The simula-tions assume the role of a searcher, browsing theresults of a retrieval.
It is assumed that the ac-tual relevant and irrelevant documents for a queryare given.
The system creates simulations ofsearchers by simulating relevance paths i.e., howthe user would traverse results of retrieval.
Dif-ferent strategies were experimented like, the usersonly view relevant/non-relevant information, i.e.,follow relevant paths from only relevant or onlynon-relevant documents, or they view all rele-vant or all non-relevant information, i.e., followall relevance paths from top-ranked relevant doc-uments or top-ranked non-relevant documents etc.Their research tries to model only certain phasesof the search process like clicking the results andto some extent the process of looking and identi-fying the results to click.
It also does not considermodeling the nature of the searcher in context andalso does not calculate the relevance of a docu-ment for a user.
The search process is not com-plete without discussing or characterizing the userthat participates in the search and computing therelevance of a document for a user.In (Agichtein et al,, 2006), they show thatclickthrough data and other implicit data of auser can be used to build user models to effec-tively personalize the search results.
Craswell etal (Craswell et al, 2008) have also done somegood work in this area.
They try to model the re-sults browsing pattern of the user.
(Craswell etal., 2008) brings out the position bias in the user?sclick-decision making process.
It provides someinteresting browsing models which can be used inour prognostic search process.
We used the cas-cade model ?
best performing model ?
proposedby them to compare the effectiveness of our ap-proach.In our approach, we address some of these is-sues to improve the reliability of the simulatedfeedback and the scalability of the simulations.We first identify certain parameters that are nat-ural to the search process on the whole and aregeneric to hold well across search engines andusers.
Wherever applicable we try to characterizethese parameters as probabilistic distributions, us-ing large volumes of data from existing search en-gine clickthrough logs.
We then instantiate theseparameters by drawing values from these proba-bilistic distributions.
This ensures that the simu-lated feedback resembles as closely as possible tothe real world scenario and thus is of high qual-ity.
We can easily run the simulations on largesets of documents to create large amounts of sim-ulated feedback, as there are no interventions of ahuman to provide any kind of extra information orrelevance information on the document set.6 EvaluationIn this section, we present the evaluation proce-dure of our approach.
We first collected query602Table 1: System ConfigurationsSystem Patience ClicksSystem1 Random RandomSystem2 Random Proposed methodSystem3 Proposed method Proposed methodlog data of 60 users using a browser plug-in fortwo months.
Our query log data consists user-id,queries and the time at which they are entered, listof search results ?
rank, title, snippet and url ofthe result ?4 and the results clicked by the user.We used 70% of this query log data to build pro-files of the searchers and the rest of the data isused for evaluation purpose.
Using the rest of thequery log data, we initiated the prognostic searchprocess giving the queries sequentially in the or-der given by the user.
We compared the simu-lated clicks with the clicks already generated bythe user.
We found that the data generated by usis 77% accurate and its recall5 value is 68%.
Wemeasured the accuracy of our system as follows.Accuracy =No.
of simulated clicks clicked by the userTotal no.
of results clicked by the user (4)We also built two more systems which we con-sidered as the baseline systems.
The first systemgives a random value for the patience value ofthe user ?
random value is used to determine thenumber of documents to be browsed during theprognostic search process ?
and random value isgiven for the user?s Perceived relevance thresholdparameter.
The second system generates the pa-tience value of the user according to the processdescribed by us in section 3.2.3 and gives a ran-dom value for the Perceived relevance thresholdvalue of the user.
Systems built by us can be sum-marized as shown in table 1:Figure 5 shows a comparision of the accuraciesof the three systems.
Here, we can see that the4A typical search engine query log does not contain thesnippets of the results and the whole list of search results.
Itonly contains the link clicked by the user and the rank of thatresult.5Recall is the fraction of results clicked for this query andsimulated successfullyFigure 5: Results comparisionbaseline 1 which uses random values for patienceand generating clicks is only 10% accurate in gen-erating clickthrough data.
However, with the ad-dition of our generating clicks approach to thebaseline 1, the performance increased by 200%.And the system 3 which uses our proposed modelsfor both patience and generating clicks generates77% accurate data which is a 670% improvementover the baseline 1.We also performed manual evaluation of oursystem.
Since manual evaluation requires a lotof effort, we performed it using 25 judges.
Werandomly selected 25 users from our query logdata and used their data to build profiles.
Thenwe showed the clicks generated by our system tothese users.
Based on their judgements, we foundour system to be 79.5% accurate6.
Figure 6 showsthe accuracy levels of our system according to dif-ferent judges.
We also studied the reason behindthe increase in accuracy of our system during hu-man evaluation.
We re-examined the clicks gener-ated by the users and found that the users selectedthe results which they have not selected duringtheir regular search.
And the reasons behind theseextra clicks are: they have missed examining theseresults or they have already reached their desireddocument.
Thus it certifies that our system is ableto personalize the results and the perceived rele-vance technique can be used to re-rank the resultsto personalize them.As the cascade model is the best performingmodel in (Craswell et al, 2008), we evaluated oursystem on that model for comparision.
We foundour system to be 96% accurate.
We used the datacollected in our clickthrough logs for evaluating6We took the average of the accuracies of our system foreach of these judges/users.603Figure 6: Accuracy based on human judge evalu-ationour system using this model.7 Conclusion and Future workIn this paper, we proposed Simulated Feedbackbased on insights from clickthrough data and us-ing prognostic search methods to generate feed-back.
There is a lot of scope for interesting fu-ture directions to the current work.
It would bean interesting experiment to see the use of thesimulated feedback in evaluation of personalizedsearch algorithms.
Consider a personalized searchalgorithm, and use it to learn a user model fromexisting explicit/implicit feedback data.
Learn auser model using the same algorithm from simu-lated feedback and compare the results.
We planto pursue the same in future.As an extension to the current work, we aimto improve the web search process especially thequery formulation step with insights from a userstudy.
We are working towards incorporatingmuch richer and complex models for query for-mulation like HMMs etc.
Ability of the system toautomatically create query reformulations of theoriginal when no clicks are found is another in-teresting future work.
We also plan to dig moreinformation about the user by analysing the querylog data.
For example, the difference in the timebetween the clicks and the distance between theclicks can be used to analyze the browsing be-haviour of the user.
These observations can inturnbe used in generation of simulated feedback thusreducing its gap with real world implicit feedback.ReferencesMark Claypool, Phong Lee, Makoto Wased and David Brown.
2001.
Implicitinterest indicators.
In Intelligent User Interfaces.Granka L., Joachims J., and Gay G. 2004.
Eyetracking analysis of user be-havior in www search.
Conference on Research and Development in In-formation Retrieval, SIGIR.Harman D. 1988.
Towards interactive query expansion.
The 11th AnnualACM SIGIR Conference on Research and Development in InformationRetrieval, 321-331.Thorsten Joachims.
2002.
Optimizing search engines using clickthroughdata.
Proceedings of the eighth ACM SIGKDD international conferenceon Knowledge discovery and data mining, 133-142.Kelly D., and Belkin N.J. 2001.
Reading time, scrolling and interaction:Exploring implicit sources of user preferences for relevance feedback dur-ing interactive information retrieval.
In Proceedings of the 24th AnnualInternational Conference on Research and Development in InformationRetrieval, SIGIR, 408-409.Mostafa J., Mukhopadhyay S., and Palakal M. 2003.
Simulation studies ofdifferent dimensions of users?
interests and their impact on user modellingand information filtering.
Information Retrieval, 199-223.Filip Radlinski and Thorsten Joachims.
2005.
Evaluating the robustness oflearning from implicit feedback.
In ICML Workshop on Learning In WebSearch.Rocchio J.J. 1999.
The SMART Retrieval System Experiments in AutomaticDocument Processing.
Relevance Feedback in Information Retrieval.Sugiyama K., Hatano K., and Yoshikawa M. 2004.
Adaptive web searchbased on user profile constructed without any effort from users.
In Pro-ceedings of WWW, 675-684.Ryen W. White, Ian Ruthven, Joemon M. Jose and C.J van Rijsbergen.
2005.Evaluating implicit feedback models using searcher simulations.
ACMTransactions on Information Systems,ACM TOIS, 325-361.Xuehua Shen, Bin Tane and Bin Tan.
2005.
Implicit user modeling for per-sonalized search.
ACM Transactions on Information Systems.Feng Qiu and Junghoo Cho.
2006.
Automatic Identification of User interestfor personalized search.
In proceedings of WWW.Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan Dumais and ThomasWhite.
2005.
Evaluating implicit measures to improve web search.
ACMTransactions on Information Systems, 147-168.Eugene Agichtein, Eric Brill, Susan Dumais and Robert Ragno.
2006.
Learn-ing user interaction models for predicting web search result preferences.In proceedings of 29th conference on research and development in infor-mation retrieval, SIGIR, 3-10.Thorsten Joachims, Laura Granka and Bing Pan.
2005.
Accurately inter-preting clickthrough data as implicit feedback.
In proceedings of 28thconference on research and development in information retrieval, SIGIR.Thomas K. Landauer, Danielle S. Mc Namara and Simon Dennis.
2007.Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates.Craswell N., Zoeter O., Taylor M. and Ramsey B.
2008.
An experimentalcomparision of click position-bias models.
In First ACM InternationalConference on Web Search and Data Mining WSDM.Olivier Chapelle and Ya Zhang.
2009.
A Dynamic Bayesian Network ClickModel for Web Search Ranking.
In proceedings of International WorldWide Web Conference(WWW).Fan Guo, Chao Liu and Yi-Min Wang.
2009.
Efficient Multipl-Click Modelsin Web Search.
In Second ACM International Conference on Web Searchand Data Mining WSDM.Harman D. 1992.
Relevance feedback revisited.
In proceedings of 15th An-nual International ACM SIGIR Conference on Research and Developmentin Information Retrieval, 1-10.Salton G., and Buckley C. 1990.
Improving retrieval performance by rele-vance feedback.
Journal of the American Society for Information Science.Daniel E. Rose, and Danny Levinson.
2004.
Understanding user goals inWeb Search.
In proceedings of International World Wide Web Confer-ence(WWW).604
