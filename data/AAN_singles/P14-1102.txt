Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1084?1093,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsBootstrapping into Filler-Gap: An Acquisition StoryMarten van Schijndel Micha ElsnerThe Ohio State University{vanschm,melsner}@ling.ohio-state.eduAbstractAnalyses of filler-gap dependencies usu-ally involve complex syntactic rules orheuristics; however recent results suggestthat filler-gap comprehension begins ear-lier than seemingly simpler constructionssuch as ditransitives or passives.
Therefore,this work models filler-gap acquisition as abyproduct of learning word orderings (e.g.SVO vs OSV), which must be done at avery young age anyway in order to extractmeaning from language.
Specifically, thismodel, trained on part-of-speech tags, rep-resents the preferred locations of semanticroles relative to a verb as Gaussian mix-tures over real numbers.This approach learns role assignment infiller-gap constructions in a manner con-sistent with current developmental findingsand is extremely robust to initializationvariance.
Additionally, this model is shownto be able to account for a characteristic er-ror made by learners during this period (Aand B gorped interpreted as A gorped B).1 IntroductionThe phenomenon of filler-gap, where the argumentof a predicate appears outside its canonical posi-tion in the phrase structure (e.g.
[the apple]ithatthe boy ate tior [what]idid the boy eat ti), has longbeen an object of study for syntacticians (Ross,1967) due to its apparent processing complexity.Such complexity is due, in part, to the arbitrarylength of the dependency between a filler and itsgap (e.g.
[the apple]ithat Mary said the boy ate ti).Recent studies indicate that comprehension offiller-gap constructions begins around 15 months(Seidl et al, 2003; Gagliardi et al, 2014).
Thisfinding raises the question of how such a complexphenomenon could be acquired so early since chil-dren at that age do not yet have a very advancedgrasp of language (e.g.
ditransitives do not seemto be generalized until at least 31 months; Gold-berg et al 2004, Bello 2012).
This work showsthat filler-gap comprehension in English may beAgeWh-SWh-O1-113moNoNo15moYes(Yes)20moYesYesYes25moYesYesNoFigure 1: The developmental timeline of subject(Wh-S) and object (Wh-O) wh-clause extractioncomprehension suggested by experimental results(Seidl et al, 2003; Gagliardi et al, 2014).
Paren-theses indicate weak comprehension.
The final rowshows the timeline of 1-1 role bias errors (Naigles,1990; Gertner and Fisher, 2012).
Missing nodes de-note a lack of studies.acquired through learning word orderings ratherthan relying on hierarchical syntactic knowledge.This work describes a cognitive model of the de-velopmental timecourse of filler-gap comprehensionwith the goal of setting a lower bound on the mod-eling assumptions necessary for an ideal learnerto display filler-gap comprehension.
In particular,the model described in this paper takes chunkedchild-directed speech as input and learns orderingsover semantic roles.
These orderings then permitthe model to successfully resolve filler-gap depen-dencies.1Further, the model presented here is alsoshown to initially reflect an idiosyncratic role as-signment error observed in development (e.g.
Aand B kradded interpreted as A kradded B ; Gert-ner and Fisher, 2012), though after training, themodel is able to avoid the error.
As such, this workmay be said to model a learner from 15 months tobetween 25 and 30 months.1This model does not explicitly learn gap positions,but rather assigns thematic roles to arguments basedon where those arguments are expected to manifest.This approach to filler-gap comprehension is supportedby findings that show people do not actually link fillersto gap positions but instead link the filler to a verbwith missing arguments (Pickering and Barry, 1991)10842 BackgroundThe developmental timeline during which childrenacquire the ability to process filler-gap construc-tions is not well-understood.
Language comprehen-sion precedes production, and the developmentalliterature on the acquisition of filler-gap construc-tions is sparsely populated due to difficulties in de-signing experiments to test filler-gap comprehen-sion in preverbal infants.
Older studies typicallylooked at verbal children and the mistakes theymake to gain insight into the acquisition process(de Villiers and Roeper, 1995).Recent studies, however, indicate that filler-gap comprehension likely begins earlier than pro-duction (Seidl et al, 2003; Gagliardi and Lidz,2010; Gagliardi et al, 2014).
Therefore, studiesof verbal children are probably actually testingthe acquisition of production mechanisms (plan-ning, motor skills, greater facility with lexical ac-cess, etc) rather than the acquisition of filler-gap.
Note that these may be related since filler-gap could introduce greater processing load whichcould overwhelm the child?s fragile production ca-pacity (Phillips, 2010).Seidl et al (2003) showed that children are ableto process wh-extractions from subject position(e.g.
[who]itiate pie) as young as 15 monthswhile similar extractions from object position (e.g.
[what]idid the boy eat ti) remain unparseable untilaround 20 months of age.2This line of investiga-tion has been reopened and expanded by Gagliardiet al (2014) whose results suggest that the ex-perimental methodology employed by Seidl et al(2003) was flawed in that it presumed infants haveideal performance mechanisms.
By providing moretrials of each condition and controlling for the prag-matic felicity of test statements, Gagliardi et al(2014) provide evidence that 15-month old infantscan process wh-extractions from both subject andobject positions.
Object extractions are more diffi-cult to comprehend than subject extractions, how-ever, perhaps due to additional processing load inobject extractions (Gibson, 1998; Phillips, 2010).Similarly, Gagliardi and Lidz (2010) show that rel-ativized extractions with a wh-relativizer (e.g.
find[the boy]iwho tiate the apple) are easier to com-prehend than relativized extractions with that asthe relativizer (e.g.
find [the boy]ithat tiate theapple).Yuan et al (2012) demonstrate that 19-montholds use their knowledge of nouns to learn bothverbs and their associated argument structure.
In2Since the wh-phrase is in the same (or a very simi-lar) position as the original subject when the wh-phrasetakes subject position, it is not clear that these con-structions are true extractions (Culicover, 2013), how-ever, this paper will continue to refer to them as suchfor ease of exposition.their study, infants were shown video of a persontalking on a phone using a nonce verb with ei-ther one or two nouns (e.g.
Mary kradded Susan).Under the assumption that infants look longer atthings that correspond to their understanding ofa prompt, the infants were then shown two im-ages that potentially depicted the described action?
one picture where two actors acted independently(reflecting an intransitive proposition) and one pic-ture where one actor acted on the other (reflectinga transitive proposition).3Even though the infantshad no extralinguistic knowledge about the verb,they consistently treated the verb as transitive iftwo nouns were present and intransitive if only onenoun was present.Similarly, Gertner and Fisher (2012) show thatintransitive phrases with conjoined subjects (e.g.John and Mary gorped) are given a transitive in-terpretation (i.e.
John gorped Mary) at 21 months(henceforth termed ?1-1 role bias?
), though this ef-fect is no longer present at 25 months (Naigles,1990).
This finding suggests both that learnerswill ignore canonical structure in favor of usingall possible arguments and that children have abias to assign a unique semantic role to each argu-ment.
It is important to note, however, that cross-linguistically children do not seem to generalize be-yond two arguments until after at least 31 monthsof age (Goldberg et al, 2004; Bello, 2012), so apredicate occurring with three nouns would stilllikely be interpreted as merely transitive ratherthan ditransitive.Computational modeling provides a way to testthe computational level of processing (Marr, 1982).That is, given the input (child-directed speech,adult-directed speech, and environmental experi-ences), it is possible to probe the computationalprocesses that result in the observed output.
How-ever, previous computational models of grammarinduction (Klein and Manning, 2004), including in-fant grammar induction (Kwiatkowski et al, 2012),have not addressed filler-gap comprehension.4The closest work to that presented here is thework on BabySRL (Connor et al, 2008; Connor etal., 2009; Connor et al, 2010).
BabySRL is a com-putational model of semantic role acquistion usinga similar set of assumptions to the current work.BabySRL learns weights over ordering constraints(e.g.
preverbal, second noun, etc.)
to acquire se-mantic role labelling while still exhibiting 1-1 rolebias.
However, no analysis has evaluated the abil-3There were two actors in each image to avoid bias-ing the infants to look at the image with more actors.4As one reviewer notes, Joshi et al (1990) and sub-sequent work show that filler-gap phenomena can beformally captured by mildly context-sensitive grammarformalisms; these have the virtue of scaling up to adultgrammar, but due to their complexity, do not seem tohave been described as models of early acquisition.1085Susan said John gave girl book-3 -2 -1 0 1 2Table 1: An example of a chunked sentence (Su-san said John gave the girl a red book) with thesentence positions labelled.
Nominal heads of nounchunks are in bold.ity of BabySRL to acquire filler-gap constructions.Further comparison to BabySRL may be found inSection 6.3 AssumptionsThe present work restricts itself to acquiring filler-gap comprehension in English.
The model pre-sented here learns a single, non-recursive orderingfor the semantic roles in each sentence relative tothe verb since several studies have suggested thatearly child grammars may consist of simple lin-ear grammars that are dictated by semantic roles(Diessel and Tomasello, 2001; Jackendoff and Wit-tenberg, in press).
This work assumes learners canalready identify nouns and verbs, which is sup-ported by Shi et al (1999) who show that chil-dren at an extremely young age can distinguish be-tween content and function words and by Waxmanand Booth (2001) who show that children can dis-tinguish between different types of content words.Further, since Waxman and Booth (2001) demon-strate that, by 14 months, children are able to dis-tinguish nouns from modifiers, this work assumeslearners can already chunk nouns and access thenominal head.
To handle recursion, this work as-sumes that children treat the final verb in eachsentence as the main verb (implicitly assuming sen-tence segmentation), which ideally assigns roles toeach of the nouns in the sentence.Due to the findings of Yuan et al (2012),this work adopts a ?syntactic bootstrapping?
the-ory of acquisition (Gleitman, 1990), where struc-tural properties (e.g.
number of nouns) inform thelearner about semantic properties of a predicate(e.g.
how many semantic roles it confers).
Sinceinfants infer the number of semantic roles, thiswork further assumes they already have expecta-tions about where these roles tend to be realizedin sentences, if they appear.
These positions maycorrespond to different semantic roles for differentpredicates (e.g.
the subject of run and of melt);however, the role for predicates with a single argu-ment is usually assigned to the noun that precedesthe verb while a second argument is usually as-signed after the verb.
The semantic properties ofthese roles may be learned lexically for each pred-icate, but that is beyond the scope of this work.Therefore, this work uses syntactic and semanticroles interchangeably (e.g.
subject and agent).?
?
piGSC-1 0.5 .999GSN-1 3 .001GOC1 0.5 .999GON1 3 .001?
.00001Table 2: Initial values for the mean (?
), standarddeviation (?
), and prior (pi) of each Gaussian aswell as the skip penalty (?)
used in this paper.Finally, following the finding by Gertner andFisher (2012) that children interpret intransitiveswith conjoined subjects as transitives, this work as-sumes that semantic roles have a one-to-one corre-spondence with nouns in a sentence (similarly usedas a soft constraint in the semantic role labellingwork of Titov and Klementiev, 2012).4 ModelThe model represents the preferred locations ofsemantic roles relative to the verb as distribu-tions over real numbers.
This idea is adapted fromBoersma (1997) who uses it to learn constraintrankings in optimality theory.In this work, the final (main) verb is placed atposition 0; words (and chunks) before the verb aregiven progressively more negative positions, andwords after the verb are given progressively morepositive positions (see Table 1).
Learner expecta-tions of where an argument will appear relativeto the verb are modelled as two-component Gaus-sian mixtures: one mixture of Gaussians (GS?)
cor-responds to the subject argument, another (GO?
)corresponds to the object argument.
There is nomixture for a third argument since children do notgeneralize beyond two arguments until later in de-velopment (Goldberg et al, 2004; Bello, 2012).One component of each mixture learns to repre-sent the canonical position for the argument (G?C)while the other (G?N) represents some alternate,non-canonical position such as the filler positionin filler-gap constructions.
To reflect the fact thatlearners have had 15 months of exposure to theirlanguage before acquiring filler-gap, the mixture isinitialized so that there is a stronger probabilityassociated with the canonical Gaussian than withthe non-canonical Gaussian of each mixture.5Fi-nally, the one-to-one role bias is explicitly encodedsuch that the model cannot use a label that hasalready been used elsewhere in the sentence.5Akhtar (1999) finds that learners may not havestrong expectations of canonical argument positionsuntil four years of age, but the results of the currentstudy are extremely robust to changes in initialization,as discussed in Section 7 of this paper, so this assump-tion is mostly adopted for ease of exposition.1086ProbabilityPosition relative to verbProbabilityPosition relative to verbFigure 2: Visual representations of (Left) the initial model?s expectations of where arguments will appear,given the initial parameters in Table 2 and (Right) the converged model?s expectations of where argumentswill appear.Thus, the initial model conditions (see Figure 2)are most likely to realize an SVO ordering, al-though it is possible to obtain SOV (by samplinga negative number from the blue curve) or evenOSV (by also sampling the red curve very closeto 0).
The model is most likely to hypothesize apreverbal object when it has already assigned thesubject role to something and, in addition, there isno postverbal noun competing for the object label.In other words, the model infers that an object ex-traction may have occurred if there is a ?missing?postverbal argument.Finally, the probability of a given sequence is theproduct of the label probabilities for the compo-nent argument positions (e.g.
GSCgenerating anargument at position -2, etc).
Since many sentenceshave more than two nouns, the model is allowed toskip nouns by multiplying a penalty term (?)
intothe product for each skipped noun; the cost is setat 0.00001 for this study, though see Section 7 for adiscussion of the constraints on this parameter.
SeeTable 2 for initialization parameters and Figure 2for a visual representation of the initial expecta-tions of the model.This work uses a model with 2-component mix-tures for both subjects and objects (termed thesymmetric model).
This formulation achieves thebest fit to the training data according to theBayesian Information Criterion (BIC).6However,follow-up experiments find that the non-canonicalsubject Gaussian only improves the likelihood ofthe data by erroneously modeling postverbal nounsin imperative statements.
The lack of a canonicalsubject in English imperatives allows the model toimprove the likelihood of the data by using thenon-canonical subject Gaussian to capture ficti-6The BIC rewards improved log-likelihood but pe-nalizes increased model complexity.tious postverbal arguments.
When imperatives arefiltered out of the training corpus, the symmetricmodel obtains a worse BIC fit than a model thatlacks the non-canonical subject Gaussian.
There-fore, if one makes the assumption that impera-tives are prosodically-marked for learners (e.g.
thelearner is the implicit subject), the best model isone that lacks a non-canonical subject.7The re-mainder of this paper assumes a symmetric modelto demonstrate what happens if such an assump-tion is not made; for the evaluations described inthis paper, the results are similar in either case.This model differs from other non-recursivecomputational models of grammar induction (e.g.Goldwater and Griffiths, 2007) since it is not basedon Hidden Markov Models.
Instead, it determinesthe best ordering for the sentence as a whole.
Thisapproach bears some similarity to a GeneralizedMallows model (Chen et al, 2009), but the currentformulation was chosen due to being independentlyposited as cognitively plausible (Boersma, 1997).Figure 2 (Right) shows the converged, final stateof the model.
The model expects the first argu-ment (usually agent) to be assigned preverballyand expects the second (say, patient) to be assignedpostverbally; however, there is now a larger chancethat the second argument will appear preverbally.5 EvaluationThe model in this work is trained using transcribedchild-directed speech (CDS) from the BabySRLportions (Connor et al, 2008) of CHILDES(MacWhinney, 2000).
Chunking is performed us-7This finding suggests that a Dirichlet Process orother means of dynamically determining the numberof components in each mixture would converge to amodel that lacks non-canonical subjects if imperativefiltering were employed.1087Eve (n = 4820) Adam (n = 4461)P R F P R FInitial .54 .64 .59 .53 .60 .56Trained .52 .69 .59?.51 .65 .57?Initialc.56 .66 .60 .55 .62 .58Trainedc.54 .71 .61?.53 .67 .59?Table 3: Overall accuracy on the Eve and Adamsections of the BabySRL corpus.
Bottom rows re-flect accuracy when non-agent roles are collapsedinto a single role.
Note that improvements are nu-merically slight since filler-gap is relatively rare(Schuler, 2011).
?p << .01ing a basic noun-chunker from NLTK (Bird et al,2009).
Based on an initial analysis of chunker per-formance, yes is hand-corrected to not be a noun.Poor chunker perfomance is likely due to a mis-match in chunker training and testing domains(Wall Street Journal text vs transcribed speech),but chunking noise may be a good estimation oflearner uncertainty, so the remaining text is leftuncorrected.
All noun phrase chunks are then re-placed with their final noun (presumed the head)to approximate the ability of children to distin-guish nouns from modifiers (Waxman and Booth,2001).
Finally, for each sentence, the model assignssentence positions to each word with the final verbat zero.Viterbi Expectation-Maximization is performedover each sentence in the corpus to infer the pa-rameters of the model.
During the Expectationstep, the model uses the current Gaussian param-eters to label the nouns in each sentence with ar-gument roles.
Since the model is not lexicalized,these roles correspond to the semantic roles mostcommonly associated with subject and object.
Themodel then chooses the best label sequence for eachsentence.These newly labelled sentences are used duringthe Maximization step to determine the Gaussianparameters that maximize the likelihood of thatlabelling.
The mean of each Gaussian is updatedto the mean position of the words it labels.
Sim-ilarly, the standard deviation of each Gaussian isupdated with the standard deviation of the posi-tions it labels.
A learning rate of 0.3 is used toprevent large parameter jumps.
The prior proba-bility of each Gaussian is updated as the ratio ofthat Gaussian?s labellings to the total number oflabellings from that mixture in the corpus:pi?
?=| G?
?|| G?
?|(1)where ?
?
{S,O} and ?
?
{C,N}.Best results seem to be obtained when the skip-penalty is loosened by an order of magnitude dur-Subject Extraction filter: S x V .
.
.Object Extraction filter: O .
.
.
V .
.
.Eve (n = 1345) Adam (n = 1287)P R F P R FInitialc.53 .57 .55 .53 .52 .52Trainedc.55 .67 .61?.54 .63 .58?Table 4: (Above) Filters to extract filler-gap con-structions: A) the subject and verb are not ad-jacent, B) the object precedes the verb.
(Below)Filler-gap accuracy on the Eve and Adam sectionsof the BabySRL corpus when non-agent roles arecollapsed into a single role.
?p << .01ing testing.
Essentially, this forces the model totightly adhere to the perceived argument struc-ture during training to learn more rigid parame-ters, but the model is allowed more leeway to skiparguments it has less confidence in during testing.Convergence (see Figure 2) tends to occur afterfour iterations but can take up to ten iterationsdepending on the initial parameters.Since the model is unsupervised, it is trained ona given corpus (e.g.
Eve) before being tested onthe role annotations of that same corpus.
The Evecorpus was used for development purposes,8andthe Adam data was used only for testing.For testing, this study uses the semantic roleannotations in the BabySRL corpus.
These anno-tations were obtained by automatically semanticrole labelling portions of CHILDES with the sys-tem of Punyakanok et al (2008) before roughlyhand-correcting them (Connor et al, 2008).
TheBabySRL corpus is annotated with 5 differentroles, but the model described in this paper onlyuses 2 roles.
Therefore, overall accuracy results (seeTable 3) are presented both for the raw BabySRLcorpus and for a collapsed BabySRL corpus whereall non-agent roles are collapsed into a single role(denoted by a subscriptcin all tables).Since children do not generalize above two ar-guments during the modelled age range (Goldberget al, 2004; Bello, 2012), the collapsed numbersmore closely reflect the performance of a learnerat this age than the raw numbers.
The increase inaccuracy obtained from collapsing non-agent ar-guments indicates that children may initially gen-eralize incorrectly to some verbs and would needto learn lexically-specific role assignments (e.g.double-object constructions of give).
Since the cur-rent work is interested in general filler-gap com-prehension at this age, including over unknownverbs, the remaining analyses in this paper con-8This is included for transparency, though the ini-tial parameters have very little bearing on the final re-sults as stated in Section 7, so the danger of overfittingto development data is very slight.1088P R F P R FEve Subj (n = 691) Obj (n = 654)Initialc.66 .83 .74 .35 .31 .33Trainedc.64 .84 .72?.45 .52 .48?Adam Subj (n = 886) Obj (n = 1050)Initialc.69 .81 .74 .33 .27 .30Trainedc.66 .81 .73 .44 .48 .46?P R F P R FEve Wh- (n = 689) That (n = 125)Initialc.63 .45 .53 .43 .48 .45Trainedc.73 .75 .74?.44 .57 .50?Adam Wh- (n = 748) That (n = 189)Initialc.50 .37 .42 .50 .50 .50Trainedc.61 .65 .63?.47 .56 .51?Table 5: (Left) Subject-extraction accuracy and object-extraction accuracy and (Right) Wh-relative ac-curacy and that-relative accuracy; calculated over the Eve and Adam sections of the BabySRL corpuswith non-agent roles collapsed into a single role.
?p = .02?p << .01sider performance when non-agent arguments arecollapsed.9Next, a filler-gap version of the BabySRL cor-pus is created using a coarse filtering process: thenew corpus is comprised of all sentences where anassociated object precedes the final verb and allsentences where the relevant subject is not imme-diately followed by the final verb (see Table 4).
Forthese filler-gap evaluations, the model is trained onthe full version of the corpus in question (e.g.
Eve)before being tested on the filler-gap subset of thatcorpus.
The overall results of the filler-gap evalua-tion (see Table 4) indicate that the model improvessignificantly at parsing filler-gap constructions af-ter training.The performance of the model on role-assignment in filler-gap constructions may beanalyzed further in terms of how the modelperforms on subject-extractions compared withobject-extractions and in terms of how the modelperforms on that-relatives compared with wh-relatives (see Table 5).The model actually performs worse at subject-extractions after training than before training.This is unsurprising because, prior to training,subjects have little-to-no competition for prever-bal role assignments; after training, there is a pre-verbal extracted object category, which the modelcan erroneously use.
This slight, though signifi-cant in Eve, deficit is counter-balanced by a verysubstantial and significant improvement in object-extraction labelling accuracy.Similarly, training confers a large and significantimprovement for role assignment in wh-relativeconstructions, but it yields less of an improve-ment for that-relative constructions.
This differ-ence mimics a finding observed in the developmen-tal literature where children seem slower to ac-quire comprehension of that-relatives than of wh-relatives (Gagliardi and Lidz, 2010).9Though performance is slightly worse when argu-ments are not collapsed, all the same patterns emerge.6 Comparison to BabySRLThe acquisition of semantic role labelling (SRL) bythe BabySRL model (Connor et al, 2008; Connoret al, 2009; Connor et al, 2010) bears many sim-ilarities to the current work and is, to our knowl-edge, the only comparable line of inquiry to thecurrent one.
The primary function of BabySRL isto model the acquisition of semantic role labellingwhile making an idiosyncratic error which infantsalso make (Gertner and Fisher, 2012), the 1-1 rolebias error (John and Mary gorped interpreted asJohn gorped Mary).
Similar to the model presentedin this paper, BabySRL is based on simple orderingfeatures such as argument position relative to theverb and argument position relative to the otherarguments.This section will demonstrate that the model inthis paper initially reflects 1-1 role bias comparablyto BabySRL, though it progresses beyond this biasafter training.10Further, the model in this paper isable to reflect the concurrent acquisition of filler-gap whereas BabySRL does not seem well-suitedto such a task.
Finally, BabySRL performs unde-sirably in intransitive settings whereas the modelin this paper does not.Connor et al (2008) demonstrate that a super-vised perceptron classifier, based on positional fea-tures and trained on the silver role label annota-tions of the BabySRL corpus, manifests 1-1 rolebias errors.
Follow-up studies show that supervi-sion may be lessened (Connor et al, 2009) or re-moved (Connor et al, 2010) and BabySRL will stillreflect a substantial 1-1 role bias.Connor et al (2008) and Connor et al (2009)run direct analyses of how frequently their mod-els make 1-1 role bias errors.
A comparable eval-uation may be run on the current model bygenerating 1000 sentences with a structure ofNNV and reporting how many times the modelchooses a subject-first labelling (see Table 6).1110All evaluations in this section are preceded bytraining on the chunked Eve corpus.11While Table 6 analyzes erroneous labellings ofNNV structure, the ?Obj?
column of Table 5 (Left)1089Error rateInitial .36Trained .11Initial (given 2 args) .66Trained (given 2 args) .132008 arg-arg position .652008 arg-verb position 02009 arg-arg position .822009 arg-verb position .63Table 6: 1-1 role bias error in this model comparedto the models of Connor et al (2008) and Connoret al (2009).
That is, how frequently each modellabelled an NNV sentence SOV.
Since the Connoret al models are perceptron-based, they requireboth arguments be labelled.
The model presentedin this paper does not share this restriction, so theraw error rate for this model is presented in thefirst two lines; the error rate once this additionalrestriction is imposed is given in the second twolines.The results of Connor et al (2008) and Connoret al (2009) depend on whether BabySRL usesargument-argument relative position as a featureor argument-verb relative position as a feature(there is no combined model).
Further, the modelpresented here from Connor et al (2009) has aunique argument constraint, similar to the modelin this paper, in order to make comparison as di-rect as possible.The 1-1 role bias error rate (before training) ofthe model presented in this paper is comparableto that of Connor et al (2008) and Connor et al(2009), which shows that the current model pro-vides comparable developmental modeling benefitsto the BabySRL models.
Further, similar to realchildren (see Figure 1) the model presented in thispaper develops beyond this error by the end of itstraining,12whereas the BabySRL models still makethis error after training.Connor et al (2010) look at how frequentlytheir model correctly labels the agent in transitiveand intransitive sentences with unknown verbs (todemonstrate that it exhibits an agent-first bias).This evaluation can be replicated for the currentstudy by generating 1,000 sentences with the tran-sitive form of NVN and a further 1,000 sentenceswith the intransitive form of NV (see Table 7).Since Connor et al (2010) investigate the effectsshows model accuracy on NNV structures.12It is important to note that the unique argumentconstraint prevents the current model from actuallygetting the correct, conjoined-subject parse, but it nolonger exhibits agent-first bias, an important step foracquiring passives, which occurs between 3 and 4 years(Thatcher et al, 2008).NVN NVSents in Eve 1173 1513Sents in Adam 1029 1353Initial .67 1Trained .65 .96Weak (10) lexical .71 .59Strong (365) lexical .74 .41Gold Args .77 .58Table 7: Agent-prediction recall accuracy in tran-sitive (NVN) and intransitive (NV) settings of themodel presented in this paper (middle) and thecombined model of Connor et al (2010) (bottom),which has features for argument-argument relativeposition as well as argument-predicate relative po-sition and so is closest to the model presented inthis paper.of different initial lexicons, this evaluation com-pares against the resulting BabySRL from each ini-tializer: they initially seed their part-of-speech tag-ger with either the 10 or 365 most frequent nounsin the corpus or they dispense with the tagger anduse gold part-of-speech tags.As with subject extraction, the model in thispaper gets less accurate after training because ofthe newly minted extracted object category thatcan be mistakenly used in these canonical settings.While the model of Connor et al (2010) outper-forms the model presented here when in a tran-sitive setting, their model does much worse in anintransitive setting.
The difference in transitive set-tings stems from increased lexicalization, as is ap-parent from their results alone; the model pre-sented here initially performs close to their weaklylexicalized model, though training impedes agent-prediction accuracy due to an increased probabilityof non-canonical objects.For the intransitive case, however, whereas themodel presented in this paper is generally able tosuccessfully label the lone noun as the subject, themodel of Connor et al (2010) chooses to label lonenouns as objects about 40% of the time.
This likelystems from their model?s reliance on argument-argument relative position as a feature; when thereis no additional argument to use for reference, themodel?s accuracy decreases.
This is borne out bytheir model (not shown in Table 7) that omitsthe argument-argument relative position featureand solely relies on verb-argument position, whichachieves up to 70% accuracy in intransitive set-tings.
Even in that case, however, BabySRL stillchooses to label lone nouns as objects 30% of thetime.
The fact that intransitive sentences are morecommon than transitive sentences in both the Eveand Adam sections of the BabySRL corpus sug-gests that learners should be more likely to assign1090correct roles in an intransitive setting, which is notreflected in the BabySRL results.The overall reason for the different results be-tween the current work and BabySRL is thatBabySRL relies on positional features that mea-sure the relative position of two individual ele-ments (e.g.
where a given noun is relative to theverb).
Since the model in this paper operates overglobal orderings, it implicitly takes into accountthe positions of other nouns as it models argumentposition relative to the verb; object and subjectare in competition as labels for preverbal nouns,so a preverbal object is usually only assigned oncea subject has already been detected.Further, while BabySRL consistently reflects 1-1 role bias (corresponding to a pre 25-month oldlearner), it also learns to productively label fiveroles, which developmental studies have showndoes not take place until at least 31 months (Gold-berg et al, 2004; Bello, 2012).
Finally, it does notseem likely that BabySRL could be easily extendedto capture filler-gap acquisition.
The argument-verb position features impede acquisition of filler-gap by classifying preverbal arguments as agents,and the argument-argument position features in-hibit accurate labelling in intransitive settings andresult in an agent-first bias which would tend tolabel extracted objects as agents.
In fact, these ob-servations suggest that any linear classifier whichrelies on positioning features will have difficultiesmodeling filler-gap acquisition.In sum, the unlexicalized model presented in thispaper is able to achieve greater labelling accuracythan the lexicalized BabySRL models in intran-sitive settings, though this model does performslightly worse in the less common transitive set-ting.
Further, the unsupervised model in this pa-per initially reflects developmental 1-1 role bias aswell as the supervised BabySRL models, and itis able to progress beyond this bias.
Finally, un-like BabySRL, the model presented here provides acognitive model of the acquisition of filler-gap com-prehension, which BabySRL does not seem well-suited to model.7 DiscussionThis paper has presented a simple cognitive modelof filler-gap acquisition, which is able to captureseveral findings from developmental psychology.Training significantly improves role labelling inthe case of object-extractions, which improves theoverall accuracy of the model.
This boost is ac-companied by a slight decrease in labelling ac-curacy in subject-extraction settings.
The asym-metric ease of subject versus object comprehen-sion is well-documented in both children andadults (Gibson, 1998), and while training improvesthe model?s ability to process object-extractions,there is still a gap between object-extraction andsubject-extraction comprehension even after train-ing.Further, the model exhibits better comprehen-sion of wh-relatives than that-relatives similar tochildren (Gagliardi and Lidz, 2010).
This couldalso be an area where a lexicalized model coulddo better.
As Gagliardi and Lidz (2010) pointout, whereas wh-relatives such as who or whichalways signify a filler-gap construction, that canoccur for many different reasons (demonstrative,determiner, complementizer, etc) and so is a muchweaker filler-gap cue.
A lexical model could poten-tially pick up on clues which could indicate whenthat is a relativizer or simply improve on its com-prehension of wh-relatives even more.It is interesting to note that the cuurent modeldoes not make use of that as a cue at all andyet is still slower at acquiring that-relatives thanwh-relatives.
This fact suggests that the findingsof Gagliardi and Lidz (2010) may be partially ex-plained by a frequency effect: perhaps the input tochildren is simply biased such that wh-relatives aremuch more common than that-relatives (as shownin Table 5).This model also initially reflects the 1-1 role biasobserved in children (Gertner and Fisher, 2012) aswell as previous models (Connor et al, 2008; Con-nor et al, 2009; Connor et al, 2010) without sac-rificing accuracy in canonical intransitive settings.Finally, this model is extremely robust to differ-ent initializations.
The canonical Gaussian expec-tations can begin far from the verb (?3) or closeto the verb (?0.1), and the standard deviationsof the distributions and the skip-penalty can varywidely; the model always converges to give compa-rable results to those presented here.
The only con-straint on the initial parameters is that the proba-bility of the extracted object occurring preverballymust exceed the skip-penalty (i.e.
extraction mustbe possible).
In short, this paper describes a sim-ple, robust cognitive model of the development ofa learner between 15 months until somewhere be-tween 25- and 30-months old (since 1-1 role bias isno longer present but no more than two argumentsare being generalized).In future, it would be interesting to incorporatelexicalization into the model presented in this pa-per, as this feature seems likely to bridge the gapbetween this model and BabySRL in transitive set-tings.
Lexicalization should also help further dis-tinguish modifiers from arguments and improve theoverall accuracy of the model.It would also be interesting to investigate howwell this model generalizes to languages besidesEnglish.
Since the model is able to use the verbposition as a semi-permeable boundary betweencanonical subjects and objects, it may not work as1091well in verb-final languages, and thus makes theprediction that filler-gap comprehension may beacquired later in development in such languagesdue to a greater reliance on hierarchical syntax.Ordering is one of the definining characteris-tics of a language that must be acquired by learn-ers (e.g.
SVO vs SOV), and this work shows thatfiller-gap comprehension can be acquired as a by-product of learning orderings rather than having toresort to higher-order syntax.
Note that this modelcannot capture the constraints on filler-gap usagewhich require a hierarchical grammar (e.g.
subja-cency), but such knowledge is really only neededfor successful production of filler-gap construc-tions, which occurs much later (around 5 years;de Villiers and Roeper, 1995).
Further, the kind ofordering system proposed in this paper may forman initial basis for learning such grammars (Jack-endoff and Wittenberg, in press).8 AcknowledgementsThanks to Peter Culicover, William Schuler, LauraWagner, and the attendees of the OSU 2013 FallLinguistics Colloquium Fest for feedback on thiswork.
This work was partially funded by an OSUDept.
of Linguistics Targeted Investment for Ex-cellence (TIE) grant for collaborative interdisci-plinary projects conducted during the academicyear 2012-13.ReferencesNameera Akhtar.
1999.
Acquiring basic word or-der: evidence for data-driven learning of syn-tactic structure.
Journal of Child Language,26:339?356.Sophia Bello.
2012.
Identifying indirect objectsin French: An elicitation task.
In Proceedingsof the 2012 annual conference of the CanadianLinguistic Association.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python:Analyzing Text with the Natural LanguageToolkit.
O?Reilly, Beijing.Paul Boersma.
1997.
How we learn variation, op-tionality, and probability.
Proceedings of the In-stitute of Phonetic Sciences of the University ofAmsterdam, 21:43?58.Harr Chen, S.R.K.
Branavan, Regina Barzilay, andDavid R. Karger.
2009.
Content modeling usinglatent permutations.
Journal of Artificial Intel-ligence Research, 36:129?163.Michael Connor, Yael Gertner, Cynthia Fisher, andDan Roth.
2008.
Baby srl: Modeling early lan-guage acquisition.
In Proceedings of the TwelfthConference on Computational Natural LanguageLearning.Michael Connor, Yael Gertner, Cynthia Fisher, andDan Roth.
2009.
Minimally supervised model ofearly language acquisition.
In Proceedings of theThirteenth Conference on Computational Natu-ral Language Learning.Michael Connor, Yael Gertner, Cynthia Fisher, andDan Roth.
2010.
Starting from scratch in se-mantic role labelling.
In Proceedings of ACL2010.Peter Culicover.
2013.
Explaining syntax: repre-sentations, structures, and computation.
OxfordUniversity Press.Jill de Villiers and Thomas Roeper.
1995.
Bar-riers, binding, and acquisition of the dp-np dis-tinction.
Language Acquisition, 4(1):73?104.Holger Diessel and Michael Tomasello.
2001.
Theacquisition of finite complement clauses in en-glish: A corpus-based analysis.
Cognitive Lin-guistics, 12:1?45.Annie Gagliardi and Jeffrey Lidz.
2010.
Mor-phosyntactic cues impact filler-gap dependencyresolution in 20- and 30-month-olds.
In Postersession of BUCLD35.Annie Gagliardi, Tara M. Mease, and JeffreyLidz.
2014.
Discontinuous developmentin the acquisition of filler-gap dependen-cies: Evidence from 15- and 20-month-olds.
Harvard unpublished manuscript:http://www.people.fas.harvard.edu/?gagliardi.Yael Gertner and Cynthia Fisher.
2012.
Predictederrors in children?s early sentence comprehen-sion.
Cognition, 124:85?94.Edward Gibson.
1998.
Linguistic complexity:Locality of syntactic dependencies.
Cognition,68(1):1?76.Lila R. Gleitman.
1990.
The structural sources ofverb meanings.
Language Acquisition, 1:3?55.Adele E. Goldberg, Devin Casenhiser, and NityaSethuraman.
2004.
Learning argument struc-ture generalizations.
Cognitive Linguistics,14(3):289?316.Sharon Goldwater and Tom Griffiths.
2007.
Afully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the 45thAnnual Meeting of the Association for Compu-tational Linguistics.Ray Jackendoff and Eva Wittenberg.
in press.What you can say without syntax: A hierarchyof grammatical complexity.
In Fritz Newmeyerand Lauren Preston, editors, Measuring Linguis-tic Complexity.
Oxford University Press.Aravind K. Joshi, K. Vijay Shanker, and DavidWeir.
1990.
The convergence of mildly context-sensitive grammar formalisms.
Technical ReportMS-CIS-90-01, Department of Computer and In-formation Science, University of Pennsylvania.1092Dan Klein and Christopher D. Manning.
2004.Corpus-based induction of syntactic structure:Models of dependency and constituency.
In Pro-ceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics.Tom Kwiatkowski, Sharon Goldwater, Luke S.Zettlemoyer, and Mark Steedman.
2012.
Aprobabilistic model of syntactic and semanticacquisition from child-directed utterances andtheir meanings.
In Proceedings of EACL 2012.Brian MacWhinney.
2000.
The CHILDES project:Tools for analyzing talk.
Lawrence Elrbaum As-sociates, Mahwah, NJ, third edition.David Marr.
1982.
Vision.
A Computational In-vestigation into the Human Representation andProcessing of Visual Information.
W.H.
Free-man and Company.Letitia R. Naigles.
1990.
Children use syntax tolearn verb meanings.
The Journal Child Lan-guage, 17:357?374.Colin Phillips.
2010.
Some arguments and non-arguments for reductionist accounts of syntacticphenomena.
Language and Cognitive Processes,28:156?187.Martin Pickering and Guy Barry.
1991.
Sentenceprocessing without empty categories.
Languageand Cognitive Processes, 6(3):229?259.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.2008.
The importance of syntactic parsing andinference in semantic role labeling.
Computa-tional Linguistics, 34(2):257?287.John R. Ross.
1967.
Constraints on Variables inSyntax.
Ph.D. thesis, Massachusetts Institute ofTechnology.William Schuler.
2011.
Effects of filler-gap de-pendencies on working memory requirements forparsing.
In Proceedings of COGSCI, pages 501?506, Austin, TX.
Cognitive Science Society.Amanda Seidl, George Hollich, and Peter W.Jusczyk.
2003.
Early understanding of subjectand object wh-questions.
Infancy, 4(3):423?436.Rushen Shi, Janet F. Werker, and James L. Mor-gan.
1999.
Newborn infants?
sensitivity to per-ceptual cues to lexical and grammatical words.Cognition, 72(2):B11?B21.Katherine Thatcher, Holly Branigan, JanetMcLean, and Antonella Sorace.
2008.
Chil-dren?s early acquisition of the passive: Evidencefrom syntactic priming.
In Proceedings of theChild Language Seminar 2007, pages 195?205,University of Reading.Ivan Titov and Alexandre Klementiev.
2012.Crosslingual induction of semantic roles.
In Pro-ceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics (ACL-2011).Sandra R. Waxman and Amy E. Booth.
2001.
See-ing pink elephants: Fourteen-month-olds?
inter-pretations of novel nouns and adjectives.
Cogni-tive Psychology, 43:217?242.Sylvia Yuan, Cynthia Fisher, and Jesse Snedeker.2012.
Counting the nouns: Simple structuralcues to verb meaning.
Child Development,83(4):1382?1399.1093
