Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1202?1212,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDiscontinuous Incremental Shift-Reduce ParsingWolfgang MaierUniversit?at D?usseldorfInstitut f?ur Sprache und InformationUniversit?atsstr.
1, 40225 D?usseldorf, Germanymaierw@hhu.deAbstractWe present an extension to incrementalshift-reduce parsing that handles discon-tinuous constituents, using a linear clas-sifier and beam search.
We achieve veryhigh parsing speeds (up to 640 sent./sec.
)and accurate results (up to 79.52 F1onTiGer).1 IntroductionDiscontinuous constituents consist of more thanone continuous block of tokens.
They arisethrough phenomena which traditionally in linguis-tics would be analyzed as being the result of somekind of ?movement?, such as extraposition or top-icalization.
The occurrence of discontinuous con-stituents does not necessarily depend on the de-gree of freedom in word order that a language al-lows for.
They can be found, e.g., in almost equalproportions in English and German treebank data(Evang and Kallmeyer, 2011).Generally, discontinuous constituents are ac-counted for in treebank annotation.
One annota-tion method consists of using trace nodes that de-note the source of a movement and are co-indexedwith the moved constituent.
Another method isto annotate discontinuities directly by allowing forcrossing branches.
Fig.
1 shows an example forthe latter approach with which we are concernedin this paper, namely, the annotation of (1).
Thetree contains a discontinuous VP due to the factthat the fronted pronoun is directly attached.
(1) DasThatwollenwantwirweumkehrenreverse?This is what we want to reverse?Several methods have been proposed for pars-ing such structures.
Trace recovery can beenDasPDS wollenVMFIN wirPPER umkehrenVVINFVPSFigure 1: Example annotation with discontinuousconstituents from TiGerframed as a separate pre-, post- or in-processingtask to PCFG parsing (Johnson, 2002; Dienes andDubey, 2003; Jijkoun, 2003; Levy and Manning,2004; Schmid, 2006; Cai et al, 2011, amongothers); see particularly Schmid (2006) for moredetails.
Directly annotated discontinuous con-stituents can be parsed with a dependency parser,given a reversible transformation from discontin-uous constituency trees to non-projective depen-dency structures.
Transformations have been pro-posed by Hall and Nivre (2008), who use com-plex edge labels that encode paths between lexicalheads, and recently by Fern?andez-Gonz?alez andMartins (2015), who use edge labels to encode theattachment order of modifiers to heads.Direct parsing of discontinuous constituents canbe done with Linear Context-Free Rewriting Sys-tem (LCFRS), an extension of CFG which allowsits non-terminals to cover more than one contin-uous block (Vijay-Shanker et al, 1987).
LCFRSparsing is expensive: CYK chart parsing with abinarized grammar can be done in O(n3k) wherek is the block degree, the maximal number of con-tinuous blocks a non-terminal can cover (Seki etal., 1991).
For a typical treebank LCFRS (Maierand S?gaard, 2008), k ?
3, instead of k = 1 forPCFG.
In order to improve on otherwise imprac-tical parsing times, LCFRS chart parsers employdifferent strategies to speed up search: Kallmeyer1202and Maier (2013) use A?search; van Cranenburgh(2012) and van Cranenburgh and Bod (2013) use acoarse-to-fine strategy in combination with Data-Oriented Parsing; Angelov and Ljungl?of (2014)use a novel cost estimation to rank parser items.Maier et al (2012) apply a treebank transforma-tion which limits the block degree and therewithalso the parsing complexity.Recently Versley (2014) achieved a break-through with a EaFi, a classifier-based parser thatuses an ?easy-first?
approach in the style of Gold-berg and Elhadad (2010).
In order to obtain dis-continuous constituents, the parser uses a strat-egy known from non-projective dependency pars-ing (Nivre, 2009; Nivre et al, 2009): For everynon-projective dependency tree, there is a projec-tive dependency tree which can be obtained byreordering the input words.
Non-projective de-pendency parsing can therefore be viewed as pro-jective dependency parsing with an additional re-ordering of the input words.
The reordering canbe done online during parsing with a ?swap?
op-eration that allows to process input words out oforder.
This idea can be transferred, because alsofor every discontinuous constituency tree, one canfind a continuous tree by reordering the terminals.Versley (2014) uses an adaptive gradient methodto train his parser.
He reports a parsing speed of40-55 sent./sec.
and results that surpass those re-ported for the above mentioned chart parsers.In (continuous) constituency parsing, incremen-tal shift-reduce parsing using the structured per-ceptron is an established technique.
While thestructured perceptron for parsing has first beenused by Collins and Roark (2004), classifier-basedincremental shift-reduce parsing has been taken upby Sagae and Lavie (2005).
A general formula-tion for the application of the perceptron algorithmto various problems, including shift-reduce con-stituency parsing, has been introduced by Zhangand Clark (2011b).
Improvements have followed(Zhu et al, 2012; Zhu et al, 2013).
A similar strat-egy has been shown to work well for CCG parsing(Zhang and Clark, 2011a), too.In this paper, we contribute a perceptron-basedshift-reduce parsing architecture with beam search(following Zhu et al (2013) and Bauer (2014))and extend it such that it can create trees withcrossing branches (following Versley (2014)).
Wepresent strategies to improve performance on dis-continuous structures, such as a new feature set.Our parser is very fast (up to 640 sent./sec.
),and produces accurate results.
In our evaluation,where we pay particular attention to the parserperformance on discontinuous structures, we showamong other things that surprisingly, a grammar-based parser has an edge over a shift-reduce ap-proach concerning the reconstruction of discontin-uous constituents.The remainder of the paper is structured as fol-lows.
In subsection 2.1, we introduce the gen-eral parser architecture; the subsections 2.2 and2.3 introduce the features we use and our strat-egy for handling discontinuous structures.
Section3 presents and discusses the experimental results,section 4 concludes the article.2 Discontinuous Shift-Reduce ParsingOur parser architecture follows previous work,particularly Zhu et al (2013) and Bauer (2014).2.1 Shift-reduce parsing with perceptrontrainingAn item in our parser consists of a queue q oftoken/POS-pairs to be processed, and a stack s,which holds completed constituents.1The parseruses different transitions: SHIFT shifts a termi-nal from the queue on to the stack.
UNARY-Xreduces the first element on the stack to a newconstituent labeled X. BINARY-X-L and BINARY-X-R reduce the first two elements on the stack toa new X constituent, with the lexical head com-ing from the left or the right child, respectively.FINISH removes the last element from the stack.We additionally use an IDLE transition, which canbe applied any number of times after FINISH, toimprove the comparability of analyses of differentlengths (Zhu et al, 2013).The application of a transition is subject to re-strictions.
UNARY-X, e.g., can only be appliedwhen there is at least a single item on the stack.We implement all restrictions listed in the ap-pendix of Zhang and Clark (2009), and add addi-tional restrictions that block transitions involvingthe root label when not having arrived at the end ofa derivation.
We do not use an underlying gram-mar to filter out transitions which have not beenseen during training.For decoding, we use beam search (Zhang andClark, 2011b).
Decoding is started by putting the1As in other shift-reduce approaches, we assume that POStagging is done outside of the parser.1203DasPDS wollenVMFIN wirPPER umkehrenVVINFVPS@SFigure 2: Binarization examplestart item (empty stack, full queue) on the beam.Then, repeatedly, a candidate list is filled with allitems that result from applying legal transitions tothe items on the beam, followed by putting thehighest scoring n of them back on the beam (givena beam size of n).
Parsing is finished if the high-est scoring item on the beam is a final item (stackholds one item labeled with the root label, queueis empty), which can be popped.
Item scores arecomputed as in Zhang and Clark (2011b): Thescore of the i+1th item is computed as the sum ofthe score of the ith item and the dot product of aglobal feature weight vector and the local weightvector resulting from the changes induced by thecorresponding transition to the i + 1th item.
Thestart item has score 0.
We train the global weightvector with an averaged Perceptron with early up-date (Collins and Roark, 2004).Parsing relies on binary trees.
As in previ-ous work, we binarize the incoming trees head-outward with binary top and bottom productions.Given a constituent X which is to be binarized,all intermediate nodes which are introduced willbe labeled @X .
Lexical heads are marked withCollins-style head rules.
As an example, Fig.
2shows the binarized version of the tree of Fig.
1.Finally, since we are learning a sparse model,we also exploit the work of Goldberg and Elhadad(2011) who propose to include a feature in the cal-culation of a score only if it has been observed ?MINUPDATE times.2.2 FeaturesFeatures are generated by applying templates toparser items.
They reflect different configurationsof stack and queue.
As BASELINE features, weuse the feature set from Zhang and Clark (2009)without the bracketing features (as used in Zhu etal.
(2013)).
We furthermore experiment with fea-tures that reflect the presence of separating punctu-ation ?,?, ?
:?, ?;?
(SEPARATOR) (Zhang and Clark,2009), and with the EXTENDED features of Zhu etunigramss0tc, s0wc, s1tc, s1wc, s2tc, s2wc, s3tc, s3wc,q0wt, q1wt, q2wt, q3wt,s0lwc, s0rwc, s0uwc, s1lwc, s1rwc, s1uwcbigramss0ws1w, s0ws1c, s0cs1w, s0cs1c, s0wq0w, s0wq0t,s0cq0w, s0cq0t, s1wq0w, s1wq0t, s1cq0w, s1cq0t,q0wq1w, q0wq1t, q0tq1w, q0tq1ttrigramss0cs1cs2w, s0cs1cs2c, s0cs1cq0w, s0cs1cq0t,s0cs1wq0w, s0cs1wq0t, s0ws1cs2c, s0ws1cq0textendeds0llwc, s0lrwc, s0luwc, s0rlwc, s0rrwc,s0ruwc, s0ulwc, s0urwc, s0uuwc, s1llwc,s1lrwc, s1luwc, s1rlwc, s1rrwc, s1ruwcseparators0wp, s0wcp, s0wq, s0wcq, s0cs1cp, s0cs1cqs1wp, s1wcp, s1wq, s1wcqFigure 3: Feature templatesal.
(2013), which look deeper into the trees on thestack, i.e., up to the grand-children instead of onlyto children.Fig.
3 shows all the feature templates.
Note thatsiand qistands for the ith stack and queue item,w stands for the head word, t for the head tag andc for the constituent label (w, t and c are identi-cal on POS-level).
l and r (ll and rr) representthe left and right children (grand-children) of theelement on the stack; u handles the unary case.Concerning the separator features, p is a uniqueseparator punctuation between the head words ofs0and s1, q is the count of any separator punctua-tion between s0and s1.2.3 Handling DiscontinuitiesIn order to handle discontinuities, we use twovariants of a swap transition which are similarto swap-eager and swap-lazy from Nivre (2009)and Nivre et al (2009).
The first variant, SIN-GLESWAP, swaps the second item of the stackback on the queue.
The second variant COM-POUNDSWAPibundles a maximal number of ad-jacent swaps.
It swaps i items starting from thesecond item on the stack, with 1 ?
i < |s|.
Bothswap operations can only be applied if1.
the item has not yet been FINISHed and thelast transition has not been a transition withthe root category,2.
the queue is not empty,3.
all elements to be swapped are pre-terminals,and12044.
if the first item of the stack has a lower indexthan the second (this inhibits swap loops).SINGLESWAP can only been applied if thereare at least two items on the stack.
For COM-POUNDSWAPi, there must be at least i+ 1 items.Transition sequences are extracted from tree-bank trees with an algorithm that traverses the treebottom-up and collects the transitions.
For a giventree ?
, intuitively, the algorithm works as follows.We start out with a queue t containing the pre-terminals of ?
, a stack ?
that receives finished con-stituents, a counter s that keeps track of the num-ber of terminals to be swapped, and an empty se-quence r that holds the result.
First, the first ele-ment of t is pushed on ?
and removed from t.While |?| > 0 or |t| > 0, we repeat the follow-ing two steps.1.
Repeat while transitions can be added:(a) if the top two elements on ?, l and r,have the same parent p labeled X andl/r is the head of p, add BINARY-X-l/rto r, pop two elements from ?
and pushp;(b) if the top element on ?
is the only childof its parent p labeled X , add UNARY-X, pop an element of ?
and push p.2.
If |t| > 0, while the first element of t is notequal to the leftmost pre-terminal dominatedby the right child of the parent of the top el-ement on ?
(i.e., while there are terminalsthat must be swapped), add SHIFT to r, in-crement s, push the first element of t on ?and remove it from t. Finally, add anotherSHIFT to r, push first element of t to ?
andremove it from t (this will contribute to thenext reduction).
If s > 0, we must swap.
Ei-ther we add s many SWAP transitions or oneCOMPOUNDSWAPsto r. Then we move smany elements from ?
to the front of t, start-ing with the second element of ?.
Finally weset s = 0.As an example, consider the transition sequencewe would extract from the tree in Fig.
2.
UsingSINGLESWAP, we would obtain SHIFT, SHIFT,SHIFT, SHIFT, SINGLESWAP, SINGLESWAP,BINARY-VP-R, SHIFT, BINARY-@S-R, SHIFT,BINARY-S-L, FINISH.
Using COMPOUNDSWAPi,instead of two SINGLESWAPs, we would just ob-tain a single COMPOUNDSWAP2.unigramss0xwc, s1xwc, s2xwc, s3xwc,s0xtc, s1xwc, s2xtc, s3xwc,s0xy, s1xy, s2xy, s3xybigramss0xs1c, s0xs1w, s0xs1x, s0ws1x, s0cs1x,s0xs2c, s0xs2w, s0xs2x, s0ws2x, s0cs2x,s0ys1y, s0ys2y, s0xq0t, s0xq0wFigure 4: Features for discontinuous structuresWe explore two methods which improve theperformance on discontinuous structures.
Eventhough almost a third of all sentences in the Ger-man NeGra and TiGer treebanks contains at leastone discontinuous constituent, among all con-stituents, the discontinuous ones are rare, makingup only around 2%.
The first, simple method ad-dresses this sparseness by raising the importanceof the features that model the actual discontinu-ities by counting all feature occurrences at a goldswap transition twice (IMPORTANCE).Secondly, we use a new feature set (DISCO)with bigram and unigram features that conveys in-formation about discontinuities.
The features con-dition the possible occurrence of a gap on previ-ous gaps and their properties.2The feature tem-plates are shown in Fig.
4. x denotes the gaptype of a tree on the stack.
There are three possi-ble values, either ?none?
(tree is fully continuous),?pass?
(there is a gap at the root, i.e., this gap mustbe filled later further up in the tree), or ?gap?
(theroot of this tree fills a gap, i.e., its children havegaps, but the root does not).
Finally, y is the sumof all gap lengths.3 Experiments3.1 DataWe use the TiGer treebank release 2.2 (TIGER),and the NeGra treebank (NEGRA).
For TIGER,we use the first half of the last 10,000 sentencesfor development and the second half for testing.3We also recreate the split of Hall and Nivre (2008)(TIGERHN), for which we split TiGer in 10 parts,assigning sentence i to part imod10.
The first ofthose parts is used for testing, the concatenation ofthe rest for training.2See Maier and Lichte (2011) for a formal account ongaps in treebanks.3This split, which corresponds to the split used in theSPMRL 2013 shared task (Seddah et al, 2013), was proposedin Farkas and Schmid (2012).
We exclude sentences 46,234and 50,224, because of annotation errors.
Both contain nodeswith more than one parent node.1205From NeGra, we exclude all sentences longerthan 30 words (in order to make a comparisonwith rparse possible, see below), and split offthe last 10% of the treebank for testing, as wellas the previous 10% for development.
As a pre-processing step, in both treebanks we remove spu-rious discontinuities that are caused by materialwhich is attached to the virtual root node (mainlypunctuation).
All such elements are attached to theleast common ancestor node of their left and rightterminal neighbors (as proposed by Levy (2005),p. 163).
We furthermore create a continuous vari-ant NEGRACF of NEGRA with the method usu-ally used for PCFG parsing: For all maximal con-tinuous parts of a discontinuous constituent, a sep-arate node is introduced (Boyd, 2007).
Subse-quently, all nodes that do not cover the head childof the discontinuous constituent are removed.No further preprocessing or cleanup is applied.3.2 Experimental SetupOur parser is implemented in Java.
We run all ourexperiments with Java 8 on an Intel Core i5, al-locating 15 GB per experiment.
All experimentsare carried out with gold POS tags, as in previouswork on shift-reduce constituency parsing (Zhangand Clark, 2009).
Grammatical function labels arediscarded.For the evaluation, we use the correspondingmodule of discodop.4We report several metrics(as implemented in discodop):?
Extended labeled bracketing, in which abracket for a single node consists of its la-bel and a set of pairs of indices, delimitingthe continuous blocks it covers.
We do notinclude the root node in the evaluation andignore punctuation.
We report labeled preci-sion, recall and F1, as well as exact match (allbrackets correct).?
Leaf-ancestor (Sampson and Babarczy,2003), for which we consider all paths fromleaves to the root.?
Tree edit distance (Emms, 2008), which con-sists of the minimum edit distance betweengold tree and parser output.Aside from a full evaluation, we also evaluate onlythe constituents that are discontinuous.4http://github.com/andreasvc/discodop60626466687072742  4  6  8  10  12  14  16  18  20F1 on dev.
setIteration1248Figure 5: NEGRA dev results (F1) for differentbeam sizesWe perform 20 training iterations unless indi-cated otherwise.
When training stops, we averagethe model (as in Daum?e III (2006)).We run further experiments with rparse5(Kallmeyer and Maier, 2013) to facilitate a com-parison with a grammar-based parser.3.3 ResultsWe start with discontinuous parsing experimentson NEGRA and TIGER, followed by continu-ous parsing experiments, and a comparison togrammar-based parsing.3.3.1 Discontinuous ParsingNeGra The first goal is to determine the effectof different beam sizes with BASELINE featuresand the COMPOUNDSWAPioperation.
We run ex-periments with beam sizes 1, 2, 4 and 8; Fig.
5shows the results obtained on the dev set aftereach iteration.
Fig.
6 shows the average decod-ing speed during each iteration for each beam size(both smoothed).Tracking two items instead of one results in alarge improvement.
Raising the beam size from2 to 4 results in a smaller improvement.
The im-provement obtained by augmenting the beam sizefrom 4 to 8 is even smaller.
This behavior is mir-rored by the parsing speeds during training: Thedifferences in parsing speed roughly align with theresult differences.
Note that fast parsing duringtraining means that the parser does not performwell (yet) and that therefore, early update is donemore often.
Note finally that the average parsingspeeds on the test set after the last training iteration5http://github.com/wmaier/rparse1206All Discont.
onlyLR LP LF1E LR LP LF1EBASELINE combined withSWAP 74.74 75.60 75.17 43.54 15.70 15.82 15.76 12.31COMPOUNDSWAPi75.60 76.37 75.98 43.04 16.46 19.96 18.05 12.05BASELINE + COMPOUNDSWAPicombined withSEPARATOR 75.20 75.74 75.47 42.61 13.11 16.73 14.70 9.89EXTENDED 76.15 76.92 76.53 44.46 15.09 20.62 17.43 12.70DISCO 75.86 76.39 76.12 43.94 15.42 22.95 18.45 12.86IMPORTANCE 75.72 76.61 76.16 43.86 16.16 20.42 18.04 12.38BASELINE + COMPOUNDSWAPi+ DISCO combined withEXTENDED 76.68 77.19 76.93 44.10 15.27 26.88 19.47 13.61EXTENDED + SEPARATOR 76.21 76.45 76.33 43.29 15.57 26.56 19.63 13.52IMPORTANCE 76.22 76.86 76.54 43.75 16.01 29.41 20.73 13.89EXTENDED + IMPORTANCE 76.76 77.13 76.95 44.30 15.09 28.86 19.82 13.23Table 1: Results NEGRA, beam size 805001000150020002  4  6  8  10  12  14  16  18  20Parsingspeeds(sent./sec.
)Iteration1248Figure 6: NEGRA dev average parsing speeds persentence for different beam sizesrange from 640 sent./sec.
(greedy) to 80 sent./sec.
(beam size 8).For further experiments on NeGra, we choosea beam size of 8.
Tab.
1 shows the bracketingscores for various parser setups.
In Tab.
2, thecorresponding TED and Leaf-Ancestor scores areshown.In the first block of the tables, we com-pare SWAP with COMPOUNDSWAPi.
On allTED LABASELINE combined withSWAP 89.19 91.62COMPOUNDSWAPi89.60 91.93BASELINE + COMPOUNDSWAPicombined withSEPARATOR 89.41 91.77EXTENDED 89.68 91.99DISCO 89.42 91.83BASELINE + COMPOUNDSWAPi+ DISCO combined withIMPORTANCE 89.64 91.90EXTENDED 89.68 91.99EXTENDED + SEPARATOR 89.52 91.86EXTENDED + IMPORTANCE 89.80 91.98Table 2: Results NEGRA TED and Leaf-Ancestorconstituents, the latter beats the former by 0.8(F1).
On discontinuous constituents, using COM-POUNDSWAPigives an improvement of more thanfour points in precision and of about 0.8 pointsin recall.
A manual analysis confirms that asexpected, particularly discontinuous constituentswith large gaps profit from bundling swap transi-tions.In the second block, we run the BASELINEfeatures with COMPOUNDSWAPicombined withSEPARATOR, EXTENDED and DISCO.
The SEP-ARATOR features were not as successful as theywere for Zhang and Clark (2009).
All scores fordiscontinuous constituents drop (compared to thebaseline).
The EXTENDED features are more ef-fective and give an improvement of about half apoint F1on all constituents, as well as the highestexact match among all experiments.
On discontin-uous constituents, precision raises slightly but weloose about 1.4% in recall (compared to the base-line).
The latter seems to be due to the fact thatin comparison to the baseline, with EXTENDED,more sentences get erroneously analyzed as notcontaining any crossing branches.
This effect canbe explained with data sparseness and is less pro-nounced when more training data is available (seebelow).
Similarly to EXTENDED, the new DISCOfeatures lead to a slight gain over the baseline (onall constituents).
As with EXTENDED, on discon-tinuous constituents, we again gain precision (3%)but loose recall (0.5%), because more sentenceswrongly analyzed as not having discontinuitiesthan in the BASELINE.
A category-based evalua-tion of discontinuous constituents reveals that EX-TENDED has an advantage over DISCO when con-sidering all constituents.
However, we can also seethat the DISCO features yield better results than1207EXTENDED particularly on the frequent discontin-uous categories (NP, VP, AP, PP), which indicatesthat the information about gap type and gap lengthis useful for the recovery of discontinuities.
IM-PORTANCE (see Sec.
2.3) is not very successful,yielding results which lie in the vicinity of thoseof the BASELINE.In the third block of the tables, we test the per-formance of the DISCO features in combinationwith other techniques, i.e., we use the BASELINEand DISCO features with COMPOUNDSWAPiandcombine it with EXTENDED and SEPARATOR fea-tures as well as with the IMPORTANCE strategy.All experiments beat the BASELINE/DISCO com-bination in terms of F1.
EXTENDED and DISCOgive a cumulative advantage, resulting in an in-crease of precision of almost 4%, resp.
over 6% ondiscontinuous constituents, compared to the useof DISCO, resp.
EXTENDED alone.
Adding theSEPARATOR features to this combination does notbring an advantage.
The IMPORTANCE strategyis the most successful one in combination withDISCO, causing a boost of almost 10% on preci-sion of discontinuous constituents, leading to thehighest overall discontinuous F1of 29.41 (notablymore than 12 points higher than the baseline); alsoon all constituents we obtain the third-highest F1.Combining DISCO with IMPORTANCE and EX-TENDED leads to the highest overall F1on all con-stituents of 76.95, however, the results on discon-tinuous constituents are slightly lower than for IM-PORTANCE alone.
This confirms the previouslyobserved behavior: The EXTENDED features helpwhen considering all constituents, but they do notseem to be effective for the recovery of disconti-nuities in particular.In the TED and LA scores (Tab.
2), we see muchless variation than in the bracketing scores.
As re-ported in the literature (e.g., Rehbein and van Gen-abith (2007)), this is because of the fact that withbracketing evaluation, a single wrong attachmentcan ?break?
brackets which otherwise would becounted as correct.
Nevertheless, the trends frombracketing evaluation repeat.To sum up, the COMPOUNDSWAPioperationworks better than SWAP because the latter misseslong gaps.
The most useful feature sets were EX-TENDED and DISCO, both when used indepen-dently and when used together.
DISCO was partic-ularly useful for discontinuous constituents.
SEP-ARATOR yielded no usable improvements.
IM-PORTANCE has also proven to be effective, yield-ing the best results on discontinuous constituents(in combination with DISCO).
Over almost all ex-periments, a common error is that on root level,CS and S get confused, indicating that the presentfeatures do not provide sufficient information fordisambiguation of those categories.
We can alsoconfirm the tendency that discontinuous VPs inrelatively short sentences are recognized correctly,as reported by Versley (2014).TiGer We now repeat the most successful exper-iments on TIGER.
Tab.
3 shows the parsing resultsfor the test set.Some of the trends seen on the experiments withNEGRA are repeated.
EXTENDED and DISCOyields an improvement on all constituents.
How-ever, now not only DISCO, but also EXTENDEDlead to improved scores on discontinuous con-stituents.
As mentioned above, this can be ex-plained with the fact that for the EXTENDED fea-tures to be effective, the amount of training dataavailable in NEGRA was not enough.
Other thanin NEGRA, the DISCO features are now more ef-fective when used alone, leading to the highestoverall F1on discontinuous constituents of 19.45.They are, however, less effective in combinationwith EXTENDED.
This is partially remedied bygiving the swap transitions more IMPORTANCE,which leads to the highest overall F1on all con-stituents of 74.71.The models we learn are sparse, therefore, asmentioned above, we can exploit the work ofGoldberg and Elhadad (2011).
They propose toonly include the weight of a feature in the compu-tation of a score if it has been seen more than MIN-UPDATE times.
We repeat the BASELINE experi-ment with two different MINUPDATE settings (seeTab.
3).
As expected, the MINUPDATE models aremuch smaller.
The final model with the baselineexperiment uses 8.3m features (parsing speed ontest set 73 sent./sec.
), with MINUPDATE 5 3.3mfeatures (121 sent./sec.)
and with MINUPDATE10 1.8m features (124 sent./sec.).
With MINUP-DATE 10, the results do degrade.
However, withMINUPDATE 5 in addition to the faster parsing weconsistently improve over the baseline.Finally, in order to check the convergence, werun a further experiment in which we limit train-ing iterations to 40 instead of 20, together withbeam size 4.
We use the BASELINE features withCOMPOUNDSWAPicombined with DISCO, EX-1208All Discont.
onlyLR LP LF1E LR LP LF1EBASELINE + COMPOUNDSWAPi72.69 74.77 73.71 36.47 16.08 18.72 17.30 12.96+ EXTENDED 73.52 75.50 74.50 37.26 15.86 20.04 17.71 13.20+ DISCO 73.77 75.35 74.55 37.08 16.68 23.32 19.45 14.43+ DISCO + EXTENDED 73.97 75.29 74.62 37.54 15.56 22.21 18.30 13.64+ DISCO + EXTENDED + IMPORTANCE 74.01 75.41 74.71 37.20 15.61 23.53 18.77 13.84BASELINE + COMPOUNDSWAPiwithMINUPDATE 5 73.04 75.03 74.03 37.36 16.25 19.72 17.82 13.28MINUPDATE 10 72.71 74.55 73.62 36.85 15.78 18.56 17.06 13.07Table 3: Results TIGER, beam size 4LR LP LF1EBASELINE 81.89 82.49 82.19 49.05EXTENDED 82.20 82.70 82.45 49.54Table 4: Results NEGRACFTENDED, and IMPORTANCE.
The parsing speedon the test set drops to around 39 sentences persecond.
However, we achieve 75.10 F1, i.e., aslight improvement over the experiments in Tab.
3that confirms the tendencies visible in Fig.
5.3.3.2 Continuous ParsingWe investigate the impact of the swap transitionson both speed and parsing results by running anexperiment with NEGRACF using the BASELINEand EXTENDED features.
The corresponding re-sults are shown in Tab.
4.Particularly high frequency categories (NP, VP,S) are much easier to find in the continuous caseand show large improvements.
This explains whywithout the swap transition, F1with BASELINEfeatures is 6.9 points higher than the F1on discon-tinuous constituents (with COMPOUNDSWAPi).With the EXTENDED features, we obtain a smallimprovement.Note that with the shift-reduce approach, thedifference between the computational cost of pro-ducing discontinuous constituents vs. the cost ofproducing continuous constituents is much lowerthan for a grammar-based approach.
When pro-ducing continuous constituents, parsing is only20% faster than with the swap transition, namely97 instead of 81 sentences per second.In order to give a different perspective on therole of discontinuous constituents, we perform twofurther evaluations.
First, we remove the dis-continuities from the output of the discontinuousbaseline parser using the procedure described inSec.
3.1 and evaluate the result against the con-tinuous gold data.
We obtain an F1of 76.70,5.5 points lower than the continuous baseline.LR LP LF1EAll constituents 69.72 68.85 69.28 33.89Disc.
only 25.77 27.51 26.61 17.77Table 5: Results NEGRA rparseSecondly, we evaluate the output of the continu-ous baseline parser against the discontinuous golddata.
This leads to an F178.89, 2.9 point morethan the discontinuous baseline.
Both evaluationsconfirm the intuition that parsing is much easierwhen discontinuities (i.e., in our case the swaptransition) do not have to be considered.3.3.3 Comparison with other Parsersrparse In order to compare our parser with agrammar-based approach, we now parse NEGRAwith rparse, with the same training and test sets asbefore (i.e., we do not use the development set).We employ markovization with v = 1, h = 2 andhead driven binarization with binary top and bot-tom productions.The first thing to notice is that rparse is muchslower than our parser.
The average parsing speedis about 0.3 sent./sec.
; very long sentences requireover a minute to be parsed.
The parsing resultsare shown in Tab.
5.
They are about 5 pointsworse than those reported by Kallmeyer and Maier(2013).
This is due to the fact that they train on thefirst 90% of the treebank, and not on the first 80%as we do, which leads to an increased number ofunparsed sentences.
In comparison to the baselinesetting of the shift-reduce parser with beam size8, the results are around 10 points worse.
How-ever, rparse reaches an F1of 26.61 on discontinu-ous constituents, which is 5.9 points more than weachieved with the best setting with our parser.In order to investigate why the grammar-basedapproach outperforms our parser on discontinuousconstituents, we count the frequency of LCFRSproductions of a certain gap degree in the bina-rized grammar used in the rparse experiment.
The1209LF1EVersley (2014) 74.23 37.32this work 79.52 44.32H&N (2008) 79.93 37.78F&M (2015) 85.53 51.21Table 6: Results TIGERHN, sentence length?
40average occurrence count of rules with gap degree0 is 12.18.
Discontinuous rules have a much lowerfrequency, the average count of productions withone, two and three gaps being 3.09, 2.09, and 1.06,respectively.
In PCFG parsing, excluding low fre-quency productions does not have a large effect(Charniak, 1996); however, this does not hold forLCFRS parsing, where they have a major influ-ence (cf.
Maier (2013, p. 205)): This means thatremoving low frequency productions has a nega-tive impact on the parser performance particularlyconcerning discontinuous structures; however, italso means that low frequency discontinuous pro-ductions get triggered reliably.
This hypothesisis confirmed by the fact that the our parser per-forms much worse on discontinuous constituentswith a very low frequency (such as CS, makingup only 0.62% of all discontinuous constituents)than it performs on those with a high frequency(such as VP, making up 60.65% of all discontin-uous constituents), while rparse performs well onthe low frequency constituents.EaFi and Dependency Parsers We run an ex-periment with 40 iterations on TIGERHN, usingDISCO, EXTENDED and IMPORTANCE.
Tab.
6lists the results, together with the correspond-ing results of Versley (2014), Hall and Nivre(2008) (H&N) and Fern?andez-Gonz?alez and Mar-tins (2015) (F&M).Our results exceed those of EaFi6and the ex-act match score of H&N.
We are outperformed bythe F&M parser.
Note, that particularly the com-parison to EaFi must be handled with care, sinceVersley (2014) uses additional preprocessing: PP-internal NPs are annotated explicitly, and the par-enthetical sentences are changed to be embeddedby their enclosing sentence (instead of vice versa).We postpone a thorough comparison with bothEaFi and the dependency parsers to future work.6Note that Versley (2014) reports a parsing speed of 40-55 sent./sec.
; depending on the beam size and the training setsize, per second, our parser parses 39-640 sentences.3.4 DiscussionTo our knowledge, surprisingly, numerical scoresfor discontinuous constituents have not been re-ported anywhere in previous work.
The relativelylow overall performance with both grammar-basedand shift-reduce based parsing, along with the factthat the grammar-based approach outperforms theshift-reduce approach, is striking.
We have shownthat it is possible to push the precision on discon-tinuous constituents, but not the recall, to the levelof what can be achieved with a grammar-based ap-proach.Particularly the outcome of the experimentsinvolving the EXTENDED features and IMPOR-TANCE drives us to the conclusion that the majorproblem when parsing discontinuous constituentsis data sparseness.
More features cannot be theonly solution: A more reliable recognition of dis-continuous constituents requires a more robustlearning from larger amounts of data.4 ConclusionWe have presented a shift-reduce parser for dis-continuous constituents which combines previouswork in shift-reduce parsing for continuous con-stituents with recent work in easy-first parsing ofdiscontinuous constituents.
Our experiments con-firm that an incremental shift-reduce architecturewith a swap transition can indeed be used to parsediscontinuous constituents.
The swap transition isassociated with a low computational cost.
We haveobtained a speed-up of up to 2,000% in compar-ison to the grammar-based rparse, and we haveshown that we obtain better results than with thegrammar-based parser, even though the grammar-based strategy does better at the reconstruction ofdiscontinuous constituents.In future work, we will concentrate on methodsthat could remedy the data sparseness concerningdiscontinuous constituents, such as self-training.Furthermore, we will experiment with larger fea-ture sets that add lexical information.
An for-mal investigation of the expressivity of our parsingmodel is currently under way.AcknowledgmentsI wish to thank Miriam Kaeshammer for enlight-ening discussions and the three anonymous re-viewers for helpful comments and suggestions.This work was partially funded by DeutscheForschungsgemeinschaft (DFG).1210ReferencesKrasimir Angelov and Peter Ljungl?of.
2014.
Faststatistical parsing with parallel multiple context-freegrammars.
In Proceedings of the 14th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 368?376, Gothenburg,Sweden.John Bauer.
2014.
Stanford shift-reduce parser.http://nlp.stanford.edu/software/srparser.shtml.Adriane Boyd.
2007.
Discontinuity revisited: Animproved conversion to context-free representations.In Proceedings of The Linguistic Annotation Work-shop (LAW) at ACL 2007, pages 41?44, Prague,Czech Republic.Shu Cai, David Chiang, and Yoav Goldberg.
2011.Language-independent parsing with empty ele-ments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 212?216, Portland, OR.Eugene Charniak.
1996.
Tree-bank grammars.
Tech-nical Report CS-96-02, Department of ComputerScience, Brown University, Providence, RI.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 111?118, Barcelona, Spain.Hal Daum?e III.
2006.
Practical Structured LearningTechniques for Natural Language Processing.
Ph.D.thesis, University of Southern California, Los Ange-les, CA.P?eter Dienes and Amit Dubey.
2003.
Antecedent re-covery: Experiments with a trace tagger.
In Pro-ceedings of the 2003 Conference on Empirical Meth-ods in Natural Language Processing, pages 33?40,Sapporo, Japan.Martin Emms.
2008.
Tree distance and someother variants of Evalb.
In Proceedings of theSixth International Language Resources and Eval-uation (LREC?08), pages 1373?1379, Marrakech,Morocco.Kilian Evang and Laura Kallmeyer.
2011.
PLCFRSparsing of English discontinuous constituents.
InProceedings of the 12th International Conference onParsing Technologies (IWPT 2011), pages 104?116,Dublin, Ireland.Richard Farkas and Helmut Schmid.
2012.
Forestreranking through subtree ranking.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1038?1047, JejuIsland, Korea.Daniel Fern?andez-Gonz?alez and Andr?e F. T. Martins.2015.
Parsing as reduction.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and Teh 7th International JointConference on Natural Language Processing of theAsian Federation of Natural Language Processing,Beijing, China.
To appear.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 742?750, Los Angeles, CA.Yoav Goldberg and Michael Elhadad.
2011.
Learningsparser perceptron models.
Technical report, BenGurion University of the Negev.Johan Hall and Joakim Nivre.
2008.
Parsing dis-continuous phrase structure with grammatical func-tions.
In Bengt Nordstr?om and Aarne Ranta, editors,Advances in Natural Language Processing, volume5221 of Lecture Notes in Computer Science, pages169?180.
Springer, Gothenburg, Sweden.Valentin Jijkoun.
2003.
Finding non-local dependen-cies: Beyond pattern matching.
In The Compan-ion Volume to the Proceedings of 41st Annual Meet-ing of the Association for Computational Linguis-tics, pages 37?43, Sapporo, Japan.Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguis-tics, pages 136?143, Philadelphia, PA.Laura Kallmeyer and Wolfgang Maier.
2013.
Data-driven parsing using probabilistic linear context-free rewriting systems.
Computational Linguistics,39(1):87?119.Roger Levy and Christopher Manning.
2004.
Deepdependencies from context-free statistical parsers:Correcting the surface dependency approximation.In Proceedings of the 42nd Meeting of the Associa-tion for Computational Linguistics (ACL?04), MainVolume, pages 327?334, Barcelona, Spain.Roger Levy.
2005.
Probabilistic Models of Word Or-der and Syntactic Discontinuity.
Ph.D. thesis, Stan-ford University.Wolfgang Maier and Timm Lichte.
2011.
Characteriz-ing discontinuity in constituent treebanks.
In For-mal Grammar.
14th International Conference, FG2009.
Bordeaux, France, July 25-26, 2009.
RevisedSelected Papers, volume 5591 of LNCS/LNAI, pages167?182.
Springer-Verlag.Wolfgang Maier and Anders S?gaard.
2008.
Tree-banks and mild context-sensitivity.
In Philippede Groote, editor, Proceedings of the 13th Confer-ence on Formal Grammar (FG-2008), pages 61?76,Hamburg, Germany.
CSLI Publications.1211Wolfgang Maier, Miriam Kaeshammer, and LauraKallmeyer.
2012.
Data-driven PLCFRS parsing re-visited: Restricting the fan-out to two.
In Proceed-ings of the Eleventh International Conference onTree Adjoining Grammars and Related Formalisms(TAG+11), pages 126?134, Paris, France.Wolfgang Maier.
2013.
Parsing Discontinuous Struc-tures.
Dissertation, University of T?ubingen.Joakim Nivre, Marco Kuhlmann, and Johan Hall.2009.
An improved oracle for dependency parsingwith online reordering.
In Proceedings of the 11thInternational Conference on Parsing Technologies(IWPT?09), pages 73?76, Paris, France.Joakim Nivre.
2009.
Non-projective dependency pars-ing in expected linear time.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages351?359, Singapore.Ines Rehbein and Josef van Genabith.
2007.
Eval-uating evaluation measures.
In Proceedings of the16th Nordic Conference of Computational Linguis-tics NODALIDA-2007, pages 372?379, Tartu, Esto-nia.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the Ninth International Workshop on ParsingTechnology, pages 125?132, Vancouver, BC.Geoffrey Sampson and Anna Babarczy.
2003.
A test ofthe leaf-ancestor metric for parse accuracy.
Journalof Natural Language Engineering, 9:365?380.Helmut Schmid.
2006.
Trace prediction and recov-ery with unlexicalized PCFGs and slash features.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 177?184, Sydney, Australia.Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, MarieCandito, Jinho D. Choi, Rich?ard Farkas, JenniferFoster, Iakes Goenaga, Koldo Gojenola Gallete-beitia, Yoav Goldberg, Spence Green, Nizar Habash,Marco Kuhlmann, Wolfgang Maier, Yuval Mar-ton, Joakim Nivre, Adam Przepi?orkowski, RyanRoth, Wolfgang Seeker, Yannick Versley, VeronikaVincze, Marcin Woli?nski, and Alina Wr?oblewska.2013.
Overview of the SPMRL 2013 sharedtask: A cross-framework evaluation of parsingmorphologically rich languages.
In Proceedingsof the Fourth Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 146?182,Seattle, WA.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,and Tadao Kasami.
1991.
On Multiple Context-Free Grammars.
Theoretical Computer Science,88(2):191?229.Andreas van Cranenburgh and Rens Bod.
2013.
Dis-continuous parsing with an efficient and accurateDOP model.
In Proceedings of The 13th Interna-tional Conference on Parsing Technologies, Nara,Japan.Andreas van Cranenburgh.
2012.
Efficient parsingwith linear context-free rewriting systems.
In Pro-ceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 460?470, Avignon, France.Yannick Versley.
2014.
Experiments with easy-firstnonprojective constituent parsing.
In Proceedingsof the First Joint Workshop on Statistical Parsingof Morphologically Rich Languages and SyntacticAnalysis of Non-Canonical Languages, pages 39?53, Dublin, Ireland.K.
Vijay-Shanker, David Weir, and Aravind K. Joshi.1987.
Characterising structural descriptions used byvarious formalisms.
In Proceedings of the 25th An-nual Meeting of the Association for ComputationalLinguistics, pages 104?111, Stanford, CA.Yue Zhang and Stephen Clark.
2009.
Transition-based parsing of the Chinese treebank using a globaldiscriminative model.
In Proceedings of the 11thInternational Conference on Parsing Technologies(IWPT?09), pages 162?171, Paris, France.Yue Zhang and Stephen Clark.
2011a.
Shift-reduceCCG parsing.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages683?692, Portland, OR.Yue Zhang and Stephen Clark.
2011b.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.Muhua Zhu, Jingbo Zhu, and Huizhen Wang.
2012.Exploiting lexical dependencies from large-scaledata for better shift-reduce constituency parsing.
InProceedings of COLING 2012, pages 3171?3186,Mumbai, India.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,and Jingbo Zhu.
2013.
Fast and accurate shift-reduce constituent parsing.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages434?443, Sofia, Bulgaria.1212
