Coling 2010: Poster Volume, pages 1041?1049,Beijing, August 2010Web-based and combined language models:a case study on noun compound identificationCarlos Ramisch??
Aline Villavicencio?
Christian Boitet??
GETALP ?
Laboratory of Informatics of Grenoble, University of Grenoble?
Institute of Informatics, Federal University of Rio Grande do Sul{ceramisch,avillavicencio}@inf.ufrgs.br Christian.Boitet@imag.frAbstractThis paper looks at the web as a corpusand at the effects of using web countsto model language, particularly when weconsider them as a domain-specific versusa general-purpose resource.
We first com-pare three vocabularies that were rankedaccording to frequencies drawn fromgeneral-purpose, specialised and web cor-pora.
Then, we look at methods to com-bine heterogeneous corpora and evaluatethe individual and combined counts in theautomatic extraction of noun compoundsfrom English general-purpose and spe-cialised texts.
Better n-gram counts canhelp improve the performance of empiri-cal NLP systems that rely on n-gram lan-guage models.1 IntroductionCorpora have been extensively employed in sev-eral NLP tasks as the basis for automaticallylearning models for language analysis and gener-ation.
In theory, data-driven (empirical or statis-tical) approaches are well suited to take intrinsiccharacteristics of human language into account.
Inpractice, external factors also determine to whatextent they will be popular and/or effective for agiven task, so that they have shown different per-formances according to the availability of corpora,to the linguistic complexity of the task, etc.An essential component of most empirical sys-tems is the language model (LM) and, in partic-ular, n-gram language models.
It is the LM thattells the system how likely a word or n-gram is inthat language, based on the counts obtained fromcorpora.
However, corpora represent a sample ofa language and will be sparse, i.e.
certain words orexpressions will not occur.
One alternative to min-imise the negative effects of data sparseness andaccount for the probability of out-of-vocabularywords is to use discounting techniques, where aconstant probability mass is discounted from eachn-gram and assigned to unseen n-grams.
Anotherstrategy is to estimate the probability of an un-seen n-gram by backing off to the probability ofthe smaller n-grams that compose it.In recent years, there has also been some ef-fort in using the web to overcome data sparseness,given that the web is several orders of magnitudelarger than any available corpus.
However, it isnot straightforward to decide whether (a) it is bet-ter to use the web than a standard corpus for agiven task or not, and (b) whether corpus and webcounts should be combined and how this shouldbe done (e.g.
using interpolation or back-off tech-niques).
As a consequence there is a strong needfor better understanding of the impacts of web fre-quencies in NLP systems and tasks.More reliable ways of combining word countscould improve the quality of empirical NLP sys-tems.
Thus, in this paper we discuss web-basedword frequency distributions (?
2) and investigateto what extent ?web-as-a-corpus?
approaches canbe employed in NLP tasks compared to standardcorpora (?
3).
Then, we present the results oftwo experiments.
First, we compare word countsdrawn from general-purpose corpora, from spe-cialised corpora and from the web (?
4).
Second,we propose several methods to combine data fromheterogeneous corpora (?
5), and evaluate their ef-fectiveness in the context of a specific multiword1041expression task: automatic noun compound iden-tification.
We close this paper with some conclu-sions and future work (?
6).2 The web as a corpusConventional and, in particular, domain-specificcorpora, are valuable resources which provide aclosed-world environment where precise n-gramcounts can be obtained.
As they tend to be smallerthan general purpose corpora, data sparseness canconsiderably hinder the results of statistical meth-ods.
For instance, in the biomedical Genia cor-pus (Ohta et al, 2002), 45% of the words occuronly once (so-called hapax legomena), and this isa very poor basis for a statistical method to decidewhether this is a significant event or just randomnoise.One possible solution is to see the web as avery large corpus containing pages written in sev-eral languages and being representative of a largefraction of human knowledge.
However, there aresome differences between using regular corporaand the web as a corpus, as discussed by Kilgar-riff (2003).
One assumption, in particular, is thatpage counts can approximate word counts, so thatthe total number of pages is used as an estimatorof the n-gram count, regardless of how many oc-currences of the n-gram they contain.This simple underlying assumption has beenemployed for several tasks.
For example, Grefen-stette (1999), in the context of example-based ma-chine translation, uses web counts to decide whichof a set of possible translations is the most naturalone for a given sequence of words (e.g.
groupe detravail as work group vs labour collective).
Like-wise, Keller and Lapata (2003) use the web to esti-mate the frequencies of unseen nominal bigrams,while Nicholson and Baldwin (2006) look at theinterpretation of noun compounds based on theindividual counts of the nouns and on the globalcount of the compound estimated from the web asa large corpus.Villavicencio et al (2007) show that the weband the British National Corpus (BNC) could beused interchangeably to identify general-purposeand type-independent multiword expressions.
La-pata and Keller (2005) perform a careful andsystematic evaluation of the web as a corpus inother general-purpose tasks both for analysis andgeneration, comparing it with a standard corpus(the BNC) and using two different techniques tocombine them: linear interpolation and back-off.Their results show that, while web counts are notas effective for some tasks as standard counts, thecombined counts can generate results, for mosttasks, that are as good as the results produced bythe best individual corpus between the BNC andthe web.
Nakov (2007) further investigates thesetasks and finds that, for many of them, effectiveattribute selection can produce results that are atleast comparable to those from the BNC usingcounts obtained from the web.On the one hand, the web can minimise theproblem of sparse data, helping distinguish rarefrom invalid cases.
Moreover, a search engine al-lows access to ever increasing quantities of data,even for rare constructions and words, whichcounts are usually equated to the number of pagesin which they occur.
On the other hand, n-grams in the highest frequency ranges, such asthe words the, up and down, are often assignedthe estimated size of the web, uniformly.
Whilethis still gives an idea of their massive occur-rence, it does not provide a finer grained distinc-tion among them (e.g.
in the BNC, the, down andup occur 6,187,267, 84,446 and 195,426 times,respectively, while in Yahoo!
they all occur in2,147,483,647 pages).3 Standard vs web corporaWhen we compare n-gram counts estimated fromthe web with counts taken from a well-formedstandard corpus, we notice that web counts are?estimated?
or ?approximated?
as page counts,whereas standard corpus counts are the exactnumber of occurrences of the n-gram.
In this way,web counts are dependent on the particular searchengine?s algorithms and representations, and thesemay perform approximations to handle the largesize of their indexing structures and procedures,such as ignoring punctuation and using stopwordlists (Kilgarriff, 2007).
This assumption, as wellas the following discussion, are not valid for forcontrolled data sets derived from Web data, such1042as the Google 1 trillion n-grams1.
Thus, our re-sults cannot be compared to those using this kindof data (Bergsma et al, 2009).In data-driven techniques, some statistical mea-sures are based on contingency tables, and thecounts for each of the table cells can be straight-forwardly computed from a standard corpus.However, this is not the case for the web, wherethe occurrences of an n-gram are not preciselycalculated in relation to the occurrences of the(n?
1)-grams composing it.
For instance, then-gram the man may appear in 200,000 pages,while the words the and man appear in respec-tively 1,000,000 and 200,000 pages, implying thatthe word man occurs with no other word than the2.In addition, the distribution of words in a stan-dard corpus follows the well known Zipfian dis-tribution (Baayen, 2001) while, in the web, it isvery difficult to distinguish frequent words or n-grams as they are often estimated as the size of theweb.
For instance, the Yahoo!
frequencies plottedin figure 1(a) are flattened in the upper part, giv-ing the same page counts for more than 700 of themost frequent words.
Another issue is the size ofthe corpus, which is an important information, of-ten needed to compute frequencies from counts orto estimate probabilities in n-gram models.
Un-like the size of a standard corpus, which is easilyobtained, it is very difficult to estimate how manypages exist on the web, especially as this numberis always increasing.But perhaps the biggest advantage of the web isits availability, even for resource-poor languagesand domains.
It is a free, expanding and easily ac-cessible resource that is representative of languageuse, in the sense that it contains a great variabilityof writing styles, text genres, language levels andknowledge domains.4 Analysing n-gram frequenciesIn this section, we describe an experiment to com-pare the probability distribution of the vocabularyof two corpora, Europarl (Koehn, 2005) and Ge-nia (Ohta et al, 2002), that represent a sampleof general-purpose and specialised English.
In1This dataset is released through LDC and is not freelyavailable.
Therefore, we do not consider it in our evaluation.2In practice, this procedure can lead to negative counts.Vep Vgenia Vintertypes 104,144 20,876 6,798hapax 41,377 9,410 ?tokens 39,595,352 486,823 ?Table 1: Some characteristics of general vsdomain-specific corpora.addition to both corpora, we also considered thecounts from the web as a corpus, using Googleand Yahoo!
APIs, and these four corpora act as n-gram count sources.
To do that, we preprocessedthe data (?
4.1), extracted the vocabularies fromeach corpus and calculated their counts in ourfour n-gram count sources (?
4.2), analysing theirrank plots to compare how each of these sourcesmodels general-purpose and specialised language(?
4.3).
The experiments described in this sec-tion were implemented in the mwetoolkit andare available at http://sf.net/projects/mwetoolkit/.4.1 PreprocessingThe Europarl corpus v3.0 (ep) contains transcrip-tions of the speeches held at the European Par-liament, with more than 1.4M sentences and39,595,352 words.
The Genia corpus (genia) con-tains abstracts of scientific articles in biomedicine,with around 1.8K sentences and 486,823 words.These standard corpora were preprocessed in thefollowing way:1. conversion to XML, lemmatisation and POStagging3;2. case homogenisation, based on the followingcriteria:?
all-uppercase and mixed case wordswere normalised to their predominantform, if it accounts for at least 80% ofthe occurrences;?
uppercase words at the beginning ofsentences were lowercased;?
other words were not modified.3Genia contains manual POS tag annota-tion.
Europarl was tagged using the TreeTagger(www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger).1043This lowercasing algorithm helps to deal with themassive use of abbreviations, acronyms, namedentities, and formulae found in specialised cor-pora, such as those containing biomedical (andother specialised) scientific articles.For calculating arbitrary-sized n-grams in largetextual corpora efficiently, we implemented astructure based on suffix arrays (Yamamoto andChurch, 2001).
While suffix trees are often usedin LM tools, where n-grams have a fixed size, theyare not fit for arbitrary length n-gram searches andcan consume quite large amounts of memory tostore all the node pointers.
Suffix arrays, on theother hand, allow for arbitrary length n-grams tobe counted in a time that is proportional to log(N),where N is the number of words (which is equiva-lent to the number of suffixes) in the corpus.
Suf-fix arrays use a constant amount of memory pro-portional to N. In our implementation, where ev-ery word and every word position in the corpus areencoded as a 4-byte integer, it corresponds pre-cisely to 4?2?N plus the size of the vocabulary,which is generally very small if compared to N,given a typical token/type ratio.
The constructionof the suffix array takes O(N log2 N) operations,due to a sorting step at the end of the process.4.2 Vocabulary creationAfter preprocessing, we extracted all the unigramsurface forms (i.e.
all words) from ep and from ge-nia, generating two vocabularies, Vep and Vgenia,where the words are ranked in descending fre-quency order with respect to the corpus itself seenas a n-gram count source.
Formally, we can modela vocabulary as a set V of words vi ?V taken froma corpus.
A word count is the value c(vi) = n of afunction that goes from words to natural numbers,c : V ?
N. Therefore, there is always an implicitword order relation?r in a vocabulary, that can begenerated from V and c by using the order relation?
in N4.
Thus, a rank is defined as a partially-ordered set formed by a vocabulary?word orderpair relation: ?V,?r?.Table 1 summarises some measures of the ex-tracted vocabularies, where Vinter denotes the in-tersection of Vep and Vgenia.
Notice that Vinter4That is, ?v1,v2 ?V , suppose c(v1) = n1 and c(v2) = n2,then v1 ?r v2 if and only if n1 ?
n2.n-gram genia ep google yahoo642 1 4 8090K 220MAfrican 2 2028 15400K 916Mfatty 16 22 2550K 59700Kmedicine 4 643 21900K 934MMac 15 3 34500K 1910MSH2 27 1 113K 3270Kadvances 4 646 6200K 173Mthereby 29 2370 8210K 145MTable 2: Distribution of some words in Vinter.contains considerably less entries than the small-est vocabulary (Vgenia).
This shows to what ex-tent both types of text differ and how importantit is to use the correct techniques when work-ing with domain-specific data in empirical ap-proaches.
The table also shows the number of ha-pax legomena (i.e.
words that occur only once) ineach corpus, and in this aspect both corpora aresimilar5.
It also shows how sparseness affects lan-guage, since a vocabulary that is 400% bigger hasonly 5% less hapax legomena.For each entry in each vocabulary, we ob-tained a count estimated from four different n-gram count sources: ep, genia, Google as a cor-pus (google) and Yahoo!
as a corpus (yahoo).
Thelatter were configured to return only results forpages in English.
Table 2 shows an example ofentries extracted from Vinter.
Notice that there areno zeroes in columns genia and ep, since this vo-cabulary only contains words that occur at leastonce in these corpora.
Also, some words like Macand SH2, that are probably specialised terms, oc-cur more in genia than in ep even if the latter ismore than 80 times larger than the former.4.3 Rank analysesFor each vocabulary, we want to estimate howsimilar the ranks generated by each of the fourcount sources are.
Figure 1 shows the rank po-sition (x) against the frequency (y) of words inVgenia, Vep and Vinter, where each plotted point rep-resents a rank position according to corpus fre-5The percentual difference in the proportion of hapaxlegomena can be explained by the fact that genia is muchsmaller than ep.104410-510-410-310-210-11 10 100 1000 10000Normalized frequencyRankRankplot of vgeniageniaepgoogleyahoo(a) Rank plot of Vgenia.10-510-410-310-210-11 10 100 1000 10000Normalized frequencyRankRankplot of vepgeniaepgoogleyahoo(b) Rank plot of Vep.10-510-410-310-210-11 10 100 1000 10000Normalized frequencyRankRankplot of vintergeniaepgoogleyahoo(c) Rank plot of Vinter.Figure 1: Plot of normalised frequencies of vocabularies according to rank positions, log-log scale.quencies and may correspond to several differentwords.6 The four sources have similar shapedcurves for each of the three vocabularies: epand genia could be reasonably approximated bya linear regression curve (in the log-log domain).google and yahoo present Zipfian curves for lowfrequency ranges but have a flat line for higherfrequencies, and the phenomenon seems consis-tent in all vocabularies and more intense on yahoo.This is related to the problem discussed in sec-tion 3 which is that web-based frequencies are notaccurate to model common words because webcounts correspond to page counts and not to wordcounts, and that a common word will probably ap-pear dozens of times in a single page.
Nonethe-less, google seems more robust to this effect,and indeed yahoo returns exactly the same value(roughly 2 billion pages) for a large number ofcommon words, producing the perfectly straightline in the rank plots.
Moreover, the problemseems less serious in Vinter, but this could be dueto its much smaller size.
These results show thatgoogle is incapable of distinguishing among thetop-100 words while yahoo is incapable of distin-guishing among the top-1000 words, and this canbe a serious drawback for web-based counts bothin general-purpose and specialised NLP tasks.The curves agree in a large portion of the fre-quency range, and the only interval in which ge-nia and ep disagree is in lower frequencies (shownin the bottom right corner).
This happens be-6Given the Zipfian behaviour of word probability distri-butions, a log-log scale was used to plot the curves.cause general-purpose ep frequencies are muchless accurate to model the specialised genia vo-cabulary, specially in low frequency ranges whensparseness becomes more marked (figure 1(a)),and vice-versa (figure 1(b)).
This effect is min-imised in figure 1(c), corresponding to Vinter.Although both vocabularies present the sameword frequency distributions, it does not meanthat their ranks are similar for the four countsources.
Tables 3 and 4 show the correlationscores for the compared count sources and for thetwo vocabularies, using Kendall?s ?
.
The ?
corre-lation index estimates the probability that a wordpair in a given rank has the same respective po-sition in another rank, in spite of the distance be-tween the words7.In the two vocabularies, correlation is low,which indicates that the ranks tend to order wordsdifferently even if there are some similarities interms of the shape of the frequency distribution.When we compare genia with google and withyahoo, we observe that yahoo is slightly less cor-related with genia than google, probably becauseof its uniform count estimates for frequent words.However, both seem to be more similar to geniathan ep.A comparison of ep with google and with yahooshows that web frequencies are much more similarto a general-purpose count source like ep than toa specialised source like genia.
Additionally, bothyahoo and google seem equally correlated to ep.7For all correlation values, p < 0.001 for the alternativehypothesis that ?
is greater than 0.1045Vgenia Vgenia Vgenia Vgeniatop middle bottomgenia-ep 0.26 0.24 0.13 0.06genia-google 0.28 0.24 0.18 0.09genia-yahoo 0.27 0.22 0.17 0.09ep-google 0.57 0.68 0.53 0.49ep-yahoo 0.57 0.68 0.53 0.49google-yahoo 0.90 0.90 0.89 0.89Table 3: Kendall?s ?
for count sources in Vgenia.Vep Vep Vep Veptop middle bottomgenia-ep 0.26 0.36 0.07 0.04genia-google 0.27 0.39 0.15 0.12genia-yahoo 0.24 0.35 0.12 0.10ep-google 0.40 0.45 0.22 0.09ep-yahoo 0.38 0.44 0.20 0.08google-yahoo 0.86 0.89 0.84 0.83Table 4: Kendall?s ?
for count sources in Vep.Surprisingly, this correlation is higher for Vgeniathan for Vep, as web frequencies and ep frequen-cies are more similar for a specialised vocabularythan for a general-purpose vocabulary.
This couldmean that the three perform similarly (poorly) atestimating frequencies for the biomedical vocab-ulary (Vgenia) whereas they differ considerably atestimating general-purpose frequencies.The correlation of the rank (first column) is alsodecomposed into the correlation for top words(more than 10 occurrences), middle words (10 to3 occurrences) and bottom words (2 and 1 occur-rences).
Except for the pair google-yahoo, the cor-relation is much higher in the top portion of thevocabulary and is close to zero in the long tail.In spite of the logarithmic scale of the graphicsin figure 1, that show the largest difference in thetop part, the bottom part is actually the most ir-regular.
The only exception is ep compared withthe web count sources in Vgenia: these two pairs donot present the high variability of the other com-pared pairs, and this means that using ep counts(general-purpose) to estimate genia counts (spe-cialised) is similar to using web counts, indepen-dently of the position of the word in the rank.Counts from google and from yahoo are also verysimilar, specially if we also consider Spearman?s?
, that is very close to total correlation.
Web ranksare also more similar for a specialised vocabularythan for a general-purpose one, providing furtherevidence for the hypothesis that the higher corre-lation is a consequence of both sources being poorfrequency estimators.
That is, for a given vocabu-lary, when web count sources are good estimators,they will be more distinct (e.g.
having less zerofrequencies).5 Combining corpora frequenciesIn our second experiment, the goal is to proposeand to evaluate techniques for the combinationof n-gram counts from heterogeneous sources.Therefore, we will use the insights about the vo-cabulary differences presented in the previous sec-tion.
In this evaluation, we measure the impactof the suggested techniques in the identificationof noun?noun compounds in corpora.
Noun com-pounds are very frequent in general-purpose andspecialised texts (e.g.
bus stop, European Unionand gene activation).
We extract them automat-ically from ep and from genia using a standardmethod based on POS patterns and associationmeasures (Evert and Krenn, 2005; Pecina, 2008;Ramisch et al, 2010).5.1 Experimental setupThe evaluation task consists of, given a corpusof N words, extract all occurrences of adjacentpairs of nouns8 and then rank them using a stan-dard statistical measure that estimates the asso-ciation strength between the two nouns.
Analo-gously to the formalism adopted in section 4.2,we assume that, for each corpus, we generate aset NN containing n-grams v1...n ?
NN9 for whichwe obtain n-gram counts from four sources.
Theelements in NN are generated by comparing thePOS pattern noun?noun against all the bigrams inthe corpus and keeping only those pairs of adja-cent words that match the pattern.
The calculationof the association measure, considering a bigramv1v2, is based on a contingency table which cells8We ignore other types of compounds, e.g.
adjective?noun pairs.9We abbreviate a sequence v1 .
.
.vn as v1...n.1046contain all possible outcomes a1a2,ai ?
{vi,?vi}.For web-based counts, we corrected up to 2% ofthem by forcing the frequency of a unigram to beat least equal to the frequency of the bigram inwhich it occurs.
Such inconsistencies are incom-patible with statistical approaches based on con-tingency table, as discussed in section 2.The log-likelihood association measure (LL, al-ternatively called expected mutual information),estimates the difference between the observed ta-ble and the expected table under the assumption ofindependent events, where E(a1 .
.
.an) =n?i=1c(ai)Nn?1is calculated using maximum likelihood:LL(v1v2) = ?a1a2c(a1a2)?
log2c(a1a2)E(a1a2)The evaluation of the NN lists is performed au-tomatically with the help of existing noun com-pound dictionaries.
The general-purpose goldstandard, used to evaluate NNep, is composed ofbigram noun compounds extracted from severalresources: 6,212 entries from the Cambridge In-ternational Dictionary of English, 22,981 fromWordnet and 2,849 from the data sets of MWE200810.
Those were merged into a single general-purpose gold standard that contains 28,622 bi-gram noun compounds.
The specialised gold stan-dard, used to evaluate NNgenia, is composed of7,441 bigrams extracted from constituent annota-tion of the genia corpus with respect to conceptsin the Genia ontology (Kim et al, 2006).True positives (TPs) are the n-grams of NNthat are contained in the respective gold standard,while n-grams that do not appear in the gold stan-dard are considered false positives11.
While thisis a simplification that underestimates the perfor-mance of the method, it is appropriate for the pur-pose of this evaluation because we compare onlythe mean average precision (MAP) between twoNN ranks, in order to verify whether improve-ments obtained by the combined frequencies are10420 entries provided by Timothy Baldwin, 2,169 en-tries provided by Su Nam Kim and 250 entries provided byPreslav Nakov, freely available at http://multiword.sf.net/11In fact, nothing can be said about an n-gram that is notin a (limited-coverage) dictionary, further manual annotationwould be necessary to asses its relevance.significant.
Additionaly, MWEs are complex lin-guistic phenomena, and their annotation, speciallyin a domain corpus, is a difficult task that reacheslow agreement rates, sometimes even for expertnative speakers.
Therefore, not only for theo-retical reasons but also for practical reasons, weadopted an automatic evaluation procedure rath-ern than annotating the top candidates in the listsby hand.Since the log-likelihood measure is a functionthat assigns a real value to each n-gram, there isa rank relation ?r that will be used to calculateMAP as follows:MAP(NN,?r) =?v1...n?NNP(v1...n)?
p(v1...n)|TPs in NN| ,where p = 1 if v1...n is a TP, 0 else, and the preci-sion P(v1...n) of a given n-gram corresponds to thenumber of TPs before v1...n in ?NN,?r?
over thetotal number of n-grams before v1...n in ?NN,?r?.5.2 Combination heuristicsFrom the initial list of 176,552 lemmatised n-grams in NNep and 14,594 in NNgenia, we fil-tered out all hapax legomena in order to removenoise and avoid useless computations.
Then, wecounted the occurrences of v1, v2 and v1v2 in ourfour sources, and those were used to calculate thefour LL values of n-grams in both lists.
We alsopropose three heuristics to combine a set of mcount sources c1 through cm into a single countsource ccomb:ccomb(v1...n) =m?i=1wi(v1...n)?
ci(v1...n),where w(v1...n) is a function that assigns a weightbetween 0 and 1 for each count source accord-ing to the n-gram v1...n. Three different func-tions were used in our experiments: uniformlinear interpolation assumes a constant and uni-form weight w(v1...n) = 1/m for all n-grams; pro-portional linear interpolation assumes a constantweight wi(v1...n) = ((?mj=1 N j)?Ni)/?mj=1 N j thatis proportional to the inverse size of the corpus;and back-off uses the uniform interpolation ofweb frequencies whenever the n-gram count in theoriginal corpus falls below a threshold (empiri-cally defined as log2(N/100,000)).1047MAP of rank NNgenia NNepLLgenia 0.4400 0.0462LLep 0.4351 0.0371LLgoogle 0.4297 0.0532LLyahoo 0.4209 0.0508LLuni f orm 0.4254 0.0508LLproportional 0.4262 0.0520LLbacko f f 0.3719 0.0370Table 5: Performance of compound extraction.Table 5 shows that the performance of back-off is below all other techniques for both vocab-ularies, thus excluding it as a successful combina-tion heuristic.
The large difference between MAPscores for NNep and for NNgenia is explained bythe relative size of the gold standards: while thegeneral-purpose reference accounts for 16% of thesize of the NNep set, the specialised reference hasas many entries as 50% of NNgenia.
Moreover, theformer was created by joining heterogeneous re-sources while the latter was compiled by humanannotators from the Genia corpus itself.
The goalof our evaluation, however, is not to compare thedifficulty of each task, but to compare the com-bination heuristics presented in each row of thetable.The best MAP for NNgenia was obtained withgenia, that significantly outperforms all othersources except ep12.
On the other hand, the useof web-based or interpolated counts in extractingspecialised noun?noun compounds does not im-prove the performance of results based on sparsebut reliable counts drawn from well-formed cor-pora.
Nonetheless, the performance of ep in spe-cialised extraction is surprising and could only beexplained by some overlap between the corpora.Moreover, the interpolated counts are not signif-icantly different from google counts, even if thiscorpus should have the weakest weight in propor-tional interpolation.General-purpose compound extraction, how-ever, benefits from the counts drawn from largecorpora as google and yahoo.
Indeed, the former12Significance was assessed through a standard one-tailedt test for equal sample sizes and variances, ?
= 0.005.significantly outperforms all other count sources,closely followed by proportional counts.
Inboth vocabularies, proportional interpolation per-forms very similar to the best count source, but,strangely enough, it still does not outperformgoogle.
Further data inspection would be neededto explain these results for the interpolated combi-nation and to try to shed some light on the reasonwhy the backoff method performs so poorly.6 Future perspectivesIn this work, we presented a detailed evalua-tion of the use of web frequencies as estima-tors of corpus frequencies in general-purpose andspecialised tasks, discussing some important as-pects of corpus-based versus web-based n-gramfrequencies.
The results indicate that they arenot only very distinct but they are so in differentways.
The importance of domain-specific data formodelling a specialised vocabulary is discussed interms of using ep to get Vgenia counts.
Further-more, the web corpora were more similar to geniathan to ep, which can be explained by the fact that?similar?
is different from ?good?, i.e.
they mightbe equally bad in modelling genia while they aredistinctly better for ep.We also proposed heuristics to combine countsources inspired by standard interpolation andback-off techniques.
Results show that we can-not use web-based or combined counts to identifyspecialised noun compounds, since they do nothelp minimise data sparseness.
However, general-purpose extraction is improved with the use ofweb counts instead of counts drawn from standardcorpora.Future work includes extending this researchto other languages and domains in order to es-timate how much of these results depend on thecorpora sizes.
Moreover, as current interpolationtechniques usually combine two corpora, weightsare estimated in a more or less ad hoc proce-dure (Lapata and Keller, 2005).
Interpolating sev-eral corpora would need a more controlled learn-ing technique to obtain optimal weights for eachfrequency function.
Additionally, the evaluationshows that corpora perform differently accordingto the frequency range.
This insight could be usedto define weight functions for interpolation.1048AcknowledgementsThis research was partly supported by CNPq(Projects 479824/2009-6 and 309569/2009-5),FINEP and SEBRAE (COMUNICA projectFINEP/SEBRAE 1194/07).
Special thanks toFl?vio Brun for his thorough work as volunteerproofreader.ReferencesBaayen, R. Harald.
2001.
Word Frequency Distri-butions, volume 18 of Text, Speech and LanguageTechnology.
Springer.Bergsma, Shane, Dekang Lin, and Randy Goebel.2009.
Web-scale N-gram models for lexical disam-biguation.
In Boutilier, Craig, editor, Proceedingsof the 21st International Joint Conference on Arti-ficial Intelligence (IJCAI 2009), pages 1507?1512,Pasadena, CA, USA, July.Evert, Stefan and Brigitte Krenn.
2005.
Using smallrandom samples for the manual evaluation of sta-tistical association measures.
Computer Speech &Language Special issue on Multiword Expression,19(4):450?466.Grefenstette, Gregory.
1999.
The World Wide Webas a resource for example-based machine translationtasks.
In Proceedings of the Twenty-First Interna-tional Conference on Translating and the Computer,London, UK, November.
ASLIB.Keller, Frank and Mirella Lapata.
2003.
Usingthe web to obtain frequencies for unseen bigrams.Computational Linguistics Special Issue on the Webas Corpus, 29(3):459?484.Kilgarriff, Adam and Gregory Grefenstette.
2003.
In-troduction to the special issue on the web as corpus.Computational Linguistics Special Issue on the Webas Corpus, 29(3):333?347.Kilgarriff, Adam.
2007.
Googleology is bad science.Computational Linguistics, 33(1):147?151.Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, andJun?ichi Tsujii.
2006.
GENIA ontology.
Techni-cal report, Tsujii Laboratory, University of Tokyo.Koehn, Philipp.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings ofthe Tenth Machine Translation Summit(MT Summit2005), Phuket, Thailand, September.
Asian-PacificAssociation for Machine Translation.Lapata, Mirella and Frank Keller.
2005.
Web-based models for natural language processing.
ACMTransactions on Speech and Language Processing(TSLP), 2(1):1?31.Nakov, Preslav.
2007.
Using the Web as an ImplicitTraining Set: Application to Noun Compound Syn-tax and Semantics.
Ph.D. thesis, EECS Department,University of California, Berkeley, CA, USA.Nicholson, Jeremy and Timothy Baldwin.
2006.
Inter-pretation of compound nominalisations using cor-pus and web statistics.
In Moir?n, Bego?a Villada,Aline Villavicencio, Diana McCarthy, Stefan Ev-ert, and Suzanne Stevenson, editors, Proceedings ofthe COLING/ACL Workshop on Multiword Expres-sions: Identifying and Exploiting Underlying Prop-erties (MWE 2006), pages 54?61, Sidney, Australia,July.
Association for Computational Linguistics.Ohta, Tomoko, Yuka Tateishi, and Jin-Dong Kim.2002.
The GENIA corpus: an annotated researchabstract corpus in molecular biology domain.
InProceedings of the Second Human Language Tech-nology Conference (HLT 2002), pages 82?86, SanDiego, CA, USA, March.
Morgan Kaufmann Pub-lishers.Pecina, Pavel.
2008.
Reference data for czech collo-cation extraction.
In Gregoire, Nicole, Stefan Ev-ert, and Brigitte Krenn, editors, Proceedings of theLREC Workshop Towards a Shared Task for Multi-word Expressions (MWE 2008), pages 11?14, Mar-rakech, Morocco, June.Ramisch, Carlos, Aline Villavicencio, and ChristianBoitet.
2010. mwetoolkit: a framework for mul-tiword expression identification.
In Calzolari, Nico-letta, Khalid Choukri, Bente Maegaard, Joseph Mar-iani, Jan Odjik, Stelios Piperidis, Mike Rosner, andDaniel Tapias, editors, Proceedings of the SeventhInternational Conference on Language Resourcesand Evaluation (LREC 2010), Valetta, Malta, May.European Language Resources Association.Villavicencio, Aline, Valia Kordoni, Yi Zhang, MarcoIdiart, and Carlos Ramisch.
2007.
Validationand evaluation of automatically acquired multi-word expressions for grammar engineering.
InEisner, Jason, editor, Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL 2007), pages1034?1043, Prague, Czech Republic, June.
Associ-ation for Computational Linguistics.Yamamoto, Mikio and Kenneth W. Church.
2001.
Us-ing suffix arrays to compute term frequency anddocument frequency for all substrings in a corpus.Computational Linguistics, 27(1):1?30.1049
