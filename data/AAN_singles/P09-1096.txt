Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 852?860,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPSMS based Interface for FAQ RetrievalGovind KothariIBM India Research Labgokothar@in.ibm.comSumit NegiIBM India Research Labsumitneg@in.ibm.comTanveer A. FaruquieIBM India Research Labftanveer@in.ibm.comVenkatesan T. ChakaravarthyIBM India Research Labvechakra@in.ibm.comL.
Venkata SubramaniamIBM India Research Lablvsubram@in.ibm.comAbstractShort Messaging Service (SMS) is popu-larly used to provide information access topeople on the move.
This has resulted inthe growth of SMS based Question An-swering (QA) services.
However auto-matically handling SMS questions posessignificant challenges due to the inherentnoise in SMS questions.
In this work wepresent an automatic FAQ-based questionanswering system for SMS users.
We han-dle the noise in a SMS query by formu-lating the query similarity over FAQ ques-tions as a combinatorial search problem.The search space consists of combinationsof all possible dictionary variations of to-kens in the noisy query.
We present an ef-ficient search algorithm that does not re-quire any training data or SMS normaliza-tion and can handle semantic variations inquestion formulation.
We demonstrate theeffectiveness of our approach on two real-life datasets.1 IntroductionThe number of mobile users is growing at anamazing rate.
In India alone a few million sub-scribers are added each month with the total sub-scriber base now crossing 370 million.
The any-time anywhere access provided by mobile net-works and portability of handsets coupled with thestrong human urge to quickly find answers has fu-eled the growth of information based services onmobile devices.
These services can be simple ad-vertisements, polls, alerts or complex applicationssuch as browsing, search and e-commerce.
Thelatest mobile devices come equipped with highresolution screen space, inbuilt web browsers andfull message keypads, however a majority of theusers still use cheaper models that have limitedscreen space and basic keypad.
On such devices,SMS is the only mode of text communication.This has encouraged service providers to build in-formation based services around SMS technology.Today, a majority of SMS based information ser-vices require users to type specific codes to re-trieve information.
For example to get a duplicatebill for a specific month, say June, the user hasto type DUPBILLJUN.
This unnecessarily con-straints users who generally find it easy and intu-itive to type in a ?texting?
language.Some businesses have recently allowed users toformulate queries in natural language using SMS.For example, many contact centers now allow cus-tomers to ?text?
their complaints and requests forinformation over SMS.
This mode of communica-tion not only makes economic sense but also savesthe customer from the hassle of waiting in a callqueue.
Most of these contact center based servicesand other regular services like ?AQA 63336?1 byIssuebits Ltd, GTIP2 by AlienPant Ltd., ?Tex-perts?3 by Number UK Ltd. and ?ChaCha?4 usehuman agents to understand the SMS text and re-spond to these SMS queries.
The nature of tex-ting language, which often as a rule rather than ex-ception, has misspellings, non-standard abbrevia-tions, transliterations, phonetic substitutions andomissions, makes it difficult to build automatedquestion answering systems around SMS technol-ogy.
This is true even for questions whose answersare well documented like a FAQ database.
Un-like other automatic question answering systemsthat focus on generating or searching answers, ina FAQ database the question and answers are al-ready provided by an expert.
The task is thento identify the best matching question-answer pairfor a given query.In this paper we present a FAQ-based ques-tion answering system over a SMS interface.
Our1http://www.aqa.63336.com/2http://www.gtip.co.uk/3http://www.texperts.com/4http://www.chacha.com/852system allows the user to enter a question inthe SMS texting language.
Such questions arenoisy and contain spelling mistakes, abbrevia-tions, deletions, phonetic spellings, translitera-tions etc.
Since mobile handsets have limitedscreen space, it necessitates that the system havehigh accuracy.
We handle the noise in a SMSquery by formulating the query similarity overFAQ questions as a combinatorial search prob-lem.
The search space consists of combinationsof all possible dictionary variations of tokens inthe noisy query.
The quality of the solution, i.e.the retrieved questions is formalized using a scor-ing function.
Unlike other SMS processing sys-tems our model does not require training data orhuman intervention.
Our system handles not onlythe noisy variations of SMS query tokens but alsosemantic variations.
We demonstrate the effective-ness of our system on real-world data sets.The rest of the paper is organized as follows.Section 2 describes the relevant prior work in thisarea and talks about our specific contributions.In Section 3 we give the problem formulation.Section 4 describes the Pruning Algorithm whichfinds the best matching question for a given SMSquery.
Section 5 provides system implementationdetails.
Section 6 provides details about our exper-iments.
Finally we conclude in Section 7.2 Prior WorkThere has been growing interest in providing ac-cess to applications, traditionally available on In-ternet, on mobile devices using SMS.
Examplesinclude Search (Schusteritsch et al, 2005), accessto Yellow Page services (Kopparapu et al, 2007),Email 5, Blog 6 , FAQ retrieval 7 etc.
As high-lighted earlier, these SMS-based FAQ retrieval ser-vices use human experts to answer questions.There are other research and commercial sys-tems which have been developed for general ques-tion and answering.
These systems generallyadopt one of the following three approaches:Human intervention based, Information Retrievalbased, or Natural language processing based.
Hu-man intervention based systems exploit humancommunities to answer questions.
These sys-tems 8 are interesting because they suggest simi-lar questions resolved in the past.
Other systems5http://www.sms2email.com/6http://www.letmeparty.com/7http://www.chacha.com/8http://www.answers.yahoo.com/like Chacha and Askme9 use qualified human ex-perts to answer questions in a timely manner.
Theinformation retrieval based system treat questionanswering as an information retrieval problem.They search large corpus of text for specific text,phrases or paragraphs relevant to a given question(Voorhees, 1999).
In FAQ based question answer-ing, where FAQ provide a ready made database ofquestion-answer, the main task is to find the clos-est matching question to retrieve the relevant an-swer (Sneiders, 1999) (Song et al, 2007).
Thenatural language processing based system tries tofully parse a question to discover semantic struc-ture and then apply logic to formulate the answer(Molla et al, 2003).
In another approach the ques-tions are converted into a template representationwhich is then used to extract answers from somestructured representation (Sneiders, 2002) (Katz etal., 2002).
Except for human intervention basedQA systems most of the other QA systems workin restricted domains and employ techniques suchas named entity recognition, co-reference resolu-tion, logic form transformation etc which requirethe question to be represented in linguistically cor-rect format.
These methods do not work for SMSbased FAQ answering because of the high level ofnoise present in SMS text.There exists some work to remove noise fromSMS (Choudhury et al, 2007) (Byun et al, 2007)(Aw et al, 2006) (Kobus et al, 2008).
How-ever, all of these techniques require aligned cor-pus of SMS and conventional language for train-ing.
Building this aligned corpus is a difficult taskand requires considerable human effort.
(Acharyaet al, 2008) propose an unsupervised techniquethat maps non-standard words to their correspond-ing conventional frequent form.
Their method canidentify non-standard transliteration of a given to-ken only if the context surrounding that token isfrequent in the corpus.
This might not be true inall domains.2.1 Our ContributionTo the best of our knowledge we are the first tohandle issues relating to SMS based automaticquestion-answering.
We address the challengesin building a FAQ-based question answering sys-tem over a SMS interface.
Our method is unsu-pervised and does not require aligned corpus orexplicit SMS normalization to handle noise.
Wepropose an efficient algorithm that handles noisy9http://www.askmehelpdesk.com/853lexical and semantic variations.3 Problem FormulationWe view the input SMS S as a sequence of tokensS = s1, s2, .
.
.
, sn.
Let Q denote the set of ques-tions in the FAQ corpus.
Each question Q ?
Qis also viewed as a sequence of terms.
Our goalis to find the question Q?
from the corpus Q thatbest matches the SMS S. As mentioned in the in-troduction, the SMS string is bound to have mis-spellings and other distortions, which needs to betaken care of while performing the match.In the preprocessing stage, we develop a Do-main dictionary D consisting of all the terms thatappear in the corpusQ.
For each term t in the dic-tionary and each SMS token si, we define a simi-larity measure ?
(t, si) that measures how closelythe term t matches the SMS token si.
We say thatthe term t is a variant of si, if ?
(t, si) > 0; this isdenoted as t ?
si.
Combining the similarity mea-sure and the inverse document frequency (idf) of tin the corpus, we define a weight function ?
(t, si).The similarity measure and the weight function arediscussed in detail in Section 5.1.Based on the weight function, we define a scor-ing function for assigning a score to each questionin the corpus Q.
The score measures how closelythe question matches the SMS string S. Considera question Q ?
Q.
For each token si, the scor-ing function chooses the term from Q having themaximum weight; then the weight of the n chosenterms are summed up to get the score.Score(Q) =n?i=1[maxt:t?Q and t?si?
(t, si)](1)Our goal is to efficiently find the question Q?
hav-ing the maximum score.4 Pruning AlgorithmWe now describe algorithms for computing themaximum scoring question Q?.
For each tokensi, we create a list Li consisting of all terms fromthe dictionary that are variants of si.
Consider atoken si.
We collect all the variants of si from thedictionary and compute their weights.
The vari-ants are then sorted in the descending order oftheir weights.
At the end of the process we have nranked lists.
As an illustration, consider an SMSquery ?gud plc buy 10s strng on9?.
Here, n = 6and six lists of variants will be created as shownFigure 1: Ranked List of Variationsin Figure 1.
The process of creating the lists isspeeded up using suitable indices, as explained indetail in Section 5.Now, we assume that the lists L1, L2, .
.
.
, Lnare created and explain the algorithms for com-puting the maximum scoring question Q?.
We de-scribe two algorithms for accomplishing the abovetask.
The two algorithms have the same function-ality i.e.
they compute Q?, but the second algo-rithm called the Pruning algorithm has a betterrun time efficiency compared to the first algorithmcalled the naive algorithm.
Both the algorithms re-quire an index which takes as input a term t fromthe dictionary and returns Qt, the set of all ques-tions in the corpus that contain the term t. Wecall the above process as querying the index onthe term t. The details of the index creation is dis-cussed in Section 5.2.Naive Algorithm: In this algorithm, we scaneach list Li and query the index on each term ap-pearing in Li.
The returned questions are added toa collection C. That is,C =n?i=1???t?LiQt?
?The collection C is called the candidate set.
No-tice that any question not appearing in the candi-date set has a score 0 and thus can be ignored.
Itfollows that the candidate set contains the maxi-mum scoring question Q?.
So, we focus on thequestions in the collection C, compute their scoresand find the maximum scoring question Q?.
Thescores of the question appearing in C can be com-puted using Equation 1.The main disadvantage with the naive algorithmis that it queries each term appearing in each listand hence, suffers from high run time cost.
Wenext explain the Pruning algorithm that avoids thispitfall and queries only a substantially small subsetof terms appearing in the lists.Pruning Algorithm: The pruning algorithm854is inspired by the Threshold Algorithm (Fagin etal., 2001).
The Pruning algorithm has the prop-erty that it queries fewer terms and ends up witha smaller candidate set as compared to the naivealgorithm.
The algorithm maintains a candidateset C of questions that can potentially be the max-imum scoring question.
The algorithm works inan iterative manner.
In each iteration, it picksthe term that has maximum weight among all theterms appearing in the lists L1, L2, .
.
.
, Ln.
Asthe lists are sorted in the descending order of theweights, this amounts to picking the maximumweight term amongst the first terms of the n lists.The chosen term t is queried to find the setQt.
Theset Qt is added to the candidate set C. For eachquestion Q ?
Qt, we compute its score Score(Q)and keep it along with Q.
The score can be com-puted by Equation 1 (For each SMS token si, wechoose the term from Q which is a variant of siand has the maximum weight.
The sum of theweights of chosen terms yields Score(Q)).
Next,the chosen term t is removed from the list.
Eachiteration proceeds as above.
We shall now developa thresholding condition such that when it is sat-isfied, the candidate set C is guaranteed to containthe maximum scoring questionQ?.
Thus, once thecondition is met, we stop the above iterative pro-cess and focus only on the questions in C to findthe maximum scoring question.Consider end of some iteration in the above pro-cess.
Suppose Q is a question not included in C.We can upperbound the score achievable by Q, asfollows.
At best, Q may include the top-most to-ken from every list L1, L2, .
.
.
, Ln.
Thus, score ofQ is bounded byScore(Q) ?n?i=0?(Li[1]).
(Since the lists are sorted Li[1] is the term havingthe maximum weight in Li).
We refer to the RHSof the above inequality as UB.Let Q?
be the question in C having the maximumscore.
Notice that if Q?
?
UB, then it is guaranteedthat any question not included in the candidate setC cannot be the maximum scoring question.
Thus,the condition ?Q?
?
UB?
serves as the terminationcondition.
At the end of each iteration, we checkif the termination condition is satisfied and if so,we can stop the iterative process.
Then, we simplypick the question in C having the maximum scoreand return it.
The algorithm is shown in Figure 2.In this section, we presented the Pruning algo-Procedure Pruning AlgorithmInput: SMS S = s1, s2, .
.
.
, snOutput: Maximum scoring question Q?.BeginConstruct lists L1, L2, .
.
.
, Ln //(see Section 5.3).// Li lists variants of si in descending//order of weight.Candidate list C = ?.repeatj?
= argmaxi?(Li[1])t?
= Lj?
[1]// t?
is the term having maximum weight among// all terms appearing in the n lists.Delete t?
from the list Lj?
.Query the index and fetch Qt?// Qt?
: the set of all questions inQ//having the term t?For each Q ?
Qt?Compute Score(Q) andadd Q with its score into CUB =?ni=1 ?(Li[1])Q?
= argmaxQ?CScore(Q).if Score(Q?)
?
UB, then// Termination condition satisfiedOutput Q?
and exit.foreverEndFigure 2: Pruning Algorithmrithm that efficiently finds the best matching ques-tion for the given SMS query without the need togo through all the questions in the FAQ corpus.The next section describes the system implemen-tation details of the Pruning Algorithm.5 System ImplementationIn this section we describe the weight function,the preprocessing step and the creation of listsL1, L2, .
.
.
, Ln.5.1 Weight FunctionWe calculate the weight for a term t in the dic-tionary w.r.t.
a given SMS token si.
The weightfunction is a combination of similarity measurebetween t and si and Inverse Document Frequency(idf) of t. The next two subsections explain thecalculation of the similarity measure and the idf indetail.5.1.1 Similarity MeasureLet D be the dictionary of all the terms in the cor-pus Q.
For term t ?
D and token si of the SMS,the similarity measure ?
(t, si) between them is855?
(t, si) =????????
?LCSRatio(t,si)EditDistanceSMS(t,si)if t and si share samestarting character *0 otherwise(2)where LCSRatio(t, si) =length(LCS(t,si))length(t) and LCS(t, si) isthe Longest common subsequence between t and si.
* The rationale behind this heuristic is that while typing a SMS, peopletypically type the first few characters correctly.
Also, this heuristic helps limitthe variants possible for a given token.The Longest Common Subsequence Ratio(LCSR) (Melamed, 1999) of two strings is the ra-tio of the length of their LCS and the length of thelonger string.
Since in SMS text, the dictionaryterm will always be longer than the SMS token,the denominator of LCSR is taken as the length ofthe dictionary term.
We call this modified LCSRas the LCSRatio.Procedure EditDistanceSMSInput: term t, token siOutput: Consonant Skeleton Edit distanceBeginreturn LevenshteinDistance(CS(si), CS(t)) + 1// 1 is added to handle the case where// Levenshtein Distance is 0EndConsonant Skeleton Generation (CS)1. remove consecutive repeated characters// (call?
cal)2. remove all vowels//(waiting ?
wtng, great?
grt)Figure 3: EditDistanceSMSThe EditDistanceSMS shown in Figure 3compares the Consonant Skeletons (Prochasson etal., 2007) of the dictionary term and the SMS to-ken.
If the consonant keys are similar, i.e.
the Lev-enshtein distance between them is less, the simi-larity measure defined in Equation 2 will be high.We explain the rationale behind using theEditDistanceSMS in the similarity measure?
(t, si) through an example.
For the SMStoken ?gud?
the most likely correct form is?good?.
The two dictionary terms ?good?
and?guided?
have the same LCSRatio of 0.5 w.r.t?gud?, but the EditDistanceSMS of ?good?
is1 which is less than that of ?guided?, which hasEditDistanceSMS of 2 w.r.t ?gud?.
As a resultthe similarity measure between ?gud?
and ?good?will be higher than that of ?gud?
and ?guided?.5.1.2 Inverse Document FrequencyIf f number of documents in corpus Q contain aterm t and the total number of documents in Q isN, the Inverse Document Frequency (idf) of t isidf(t) = logNf(3)Combining the similarity measure and the idfof t in the corpus, we define the weight function?
(t, si) as?
(t, si) = ?
(t, si) ?
idf(t) (4)The objective behind the weight function is1.
We prefer terms that have high similaritymeasure i.e.
terms that are similar to theSMS token.
Higher the LCSRatio and lowerthe EditDistanceSMS , higher will be thesimilarity measure.
Thus for example, for agiven SMS token ?byk?, similarity measureof word ?bike?
is higher than that of ?break?.2.
We prefer words that are highly discrimi-native i.e.
words with a high idf score.The rationale for this stems from the factthat queries, in general, are composed of in-formative words.
Thus for example, for agiven SMS token ?byk?, idf of ?bike?
willbe more than that of commonly occurringword ?back?.
Thus, even though the similar-ity measure of ?bike?
and ?back?
are samew.r.t.
?byk?, ?bike?
will get a higher weightthan ?back?
due to its idf.We combine these two objectives into a singleweight function multiplicatively.5.2 PreprocessingPreprocessing involves indexing of the FAQ cor-pus, formation of Domain and Synonym dictionar-ies and calculation of the Inverse Document Fre-quency for each term in the Domain dictionary.As explained earlier the Pruning algorithm re-quires retrieval of all questions Qt that contains agiven term t. To do this efficiently we index theFAQ corpus using Lucene10.
Each question in theFAQ corpus is treated as a Document; it is tok-enized using whitespace as delimiter and indexed.10http://lucene.apache.org/java/docs/856The Domain dictionaryD is built from all termsthat appear in the corpus Q.The weight calculation for Pruning algorithmrequires the idf for a given term t. For each term tin the Domain dictionary, we query the Lucene in-dexer to get the number of Documents containingt.
Using Equation 3, the idf(t) is calculated.
Theidf for each term t is stored in a Hashtable, with tas the key and idf as its value.Another key step in the preprocessing stage isthe creation of the Synonym dictionary.
The Prun-ing algorithm uses this dictionary to retrieve se-mantically similar questions.
Details of this step isfurther elaborated in the List Creation sub-section.The Synonym Dictionary creation involves map-ping each word in the Domain dictionary to it?scorresponding Synset obtained from WordNet11.5.3 List CreationGiven a SMS S, it is tokenized using white-spacesto get a sequence of tokens s1, s2, .
.
.
, sn.
Digitsoccurring in SMS token (e.g ?10s?
, ?4get?)
are re-placed by string based on a manually crafted digit-to-string mapping (?10?
?
?ten?).
A list Li issetup for each token si using terms in the domaindictionary.
The list for a single character SMS to-ken is set to null as it is most likely to be a stopword .
A term t from domain dictionary is in-cluded in Li if its first character is same as that ofthe token si and it satisfies the threshold conditionlength(LCS(t, si)) > 1.Each term t that is added to the list is assigned aweight given by Equation 4.Terms in the list are ranked in descending or-der of their weights.
Henceforth, the term ?list?implies a ranked list.For example the SMS query ?gud plc 2 buy 10sstrng on9?
(corresponding question ?Where is agood place to buy tennis strings online??
), is to-kenized to get a set of tokens {?gud?, ?plc?, ?2?,?buy?, ?10s?, ?strng?, ?on9?}.
Single character to-kens such as ?2?
are neglected as they are mostlikely to be stop words.
From these tokens cor-responding lists are setup as shown in Figure 1.5.3.1 Synonym Dictionary LookupTo retrieve answers for SMS queries that aresemantically similar but lexically different fromquestions in the FAQ corpus we use the Synonymdictionary described in Section 5.2.
Figure 4 illus-trates some examples of such SMS queries.11http://wordnet.princeton.edu/Figure 4: Semantically similar SMS and questionsFigure 5: Synonym Dictionary LookUpFor a given SMS token si, the list of variationsLi is further augmented using this Synonym dic-tionary.
For each token si a fuzzy match is per-formed between si and the terms in the Synonymdictionary and the best matching term from theSynonym dictionary, ?
is identified.
As the map-pings between the Synonym and the Domain dic-tionary terms are maintained, we obtain the corre-sponding Domain dictionary term ?
for the Syn-onym term ?
and add that term to the list Li.
?
isassigned a weight given by?
(?, si) = ?
(?, si) ?
idf(?)
(5)It should be noted that weight for ?
is based onthe similarity measure between Synonym dictio-nary term ?
and SMS token si.For example, the SMS query ?hw2 countr quiksrv?
( corresponding question ?How to return avery fast serve??)
has two terms ?countr?
??counter?
and ?quik?
?
?quick?
belonging tothe Synonym dictionary.
Their associated map-pings in the Domain dictionary are ?return?
and?fast?
respectively as shown in Figure 5.
Duringthe list setup process the token ?countr?
is looked857up in the Domain dictionary.
Terms from the Do-main dictionary that begin with the same characteras that of the token ?countr?
and have a LCS > 1such as ?country?,?count?, etc.
are added to thelist and assigned a weight given by Equation 4.After that, the token ?countr?
is looked up in theSynonym dictionary using Fuzzy match.
In thisexample the term ?counter?
from the Synonymdictionary fuzzy matches the SMS token.
The Do-main dictionary term corresponding to the Syn-onym dictionary term ?counter?
is looked up andadded to the list.
In the current example the cor-responding Domain dictionary term is ?return?.This term is assigned a weight given by Equation5 and is added to the list as shown in Figure 5.5.4 FAQ retrievalOnce the lists are created, the Pruning Algorithmas shown in Figure 2 is used to find the FAQ ques-tionQ?
that best matches the SMS query.
The cor-responding answer to Q?
from the FAQ corpus isreturned to the user.The next section describes the experimentalsetup and results.6 ExperimentsWe validated the effectiveness and usability ofour system by carrying out experiments on twoFAQ data sets.
The first FAQ data set, referredto as the Telecom Data-Set, consists of 1500 fre-quently asked questions, collected from a Telecomservice provider?s website.
The questions in thisdata set are related to the Telecom providers prod-ucts or services.
For example queries about callrates/charges, bill drop locations, how to installcaller tunes, how to activate GPRS etc.
The sec-ond FAQ corpus, referred to as the Yahoo DataSet,consists of 7500 questions from three Yahoo!Answers12 categories namely Sports.Swimming,Sports.Tennis, Sports.Running.To measure the effectiveness of our system, auser evaluation study was performed.
Ten humanevaluators were asked to choose 10 questions ran-domly from the FAQ data set.
None of the eval-uators were authors of the paper.
They were pro-vided with a mobile keypad interface and asked to?text?
the selected 10 questions as SMS queries.Through that exercise 100 relevant SMS queriesper FAQ data set were collected.
Figure 6 showssample SMS queries.
In order to validate that thesystem was able to handle queries that were out of12http://answers.yahoo.com/Figure 6: Sample SMS queriesData Set Relevant Queries Irrelevant QueriesTelecom 100 50Yahoo 100 50Table 1: SMS Data Set.the FAQ domain, we collected 5 irrelevant SMSqueries from each of the 10 human-evaluators forboth the data sets.
Irrelevant queries were (a)Queries out of the FAQ domain e.g.
queries re-lated to Cricket, Billiards, activating GPS etc (b)Absurd queries e.g.
?ama ameyu tuem?
(sequenceof meaningless words) and (c) General Queriese.g.
?what is sports?.
Table 1 gives the numberof relevant and irrelevant queries used in our ex-periments.The average word length of the collected SMSmessages for Telecom and Yahoo datasets was 4and 7 respectively.
We manually cleaned the SMSquery data word by word to create a clean SMStest-set.
For example, the SMS query ?h2 mke apdl bke fstr?
was manually cleaned to get ?howto make pedal bike faster?.
In order to quantifythe level of noise in the collected SMS data, webuilt a character-level language model(LM)13 us-ing the questions in the FAQ data-set (vocabularysize is 44 characters) and computed the perplex-ity14 of the language model on the noisy and thecleaned SMS test-set.
The perplexity of the LM ona corpus gives an indication of the average num-ber of bits needed per n-gram to encode the cor-pus.
Noise will result in the introduction of manypreviously unseen n-grams in the corpus.
Highernumber of bits are needed to encode these improb-able n-grams which results in increased perplexity.From Table 2 we can see the difference in perplex-ity for noisy and clean SMS data for the Yahooand Telecom data-set.
The high level of perplexityin the SMS data set indicates the extent of noisepresent in the SMS corpus.To handle irrelevant queries the algorithm de-scribed in Section 4 is modified.
Only if theScore(Q?)
is above a certain threshold, it?s answeris returned, else we return ?null?.
The threshold13http://en.wikipedia.org/wiki/Language model14bits = log2(perplexity)858Cleaned SMS Noisy SMSYahoo bigram 14.92 74.58trigram 8.11 93.13Telecom bigram 17.62 59.26trigram 10.27 63.21Table 2: Perplexity for Cleaned and Noisy SMSFigure 7: Accuracy on Telecom FAQ Datasetwas determined experimentally.To retrieve the correct answer for the posedSMS query, the SMS query is matched againstquestions in the FAQ data set and the best match-ing question(Q?)
is identified using the Pruning al-gorithm.
The system then returns the answer tothis best matching question to the human evalua-tor.
The evaluator then scores the response on a bi-nary scale.
A score of 1 is given if the returned an-swer is the correct response to the SMS query, elseit is assigned 0.
The scoring procedure is reversedfor irrelevant queries i.e.
a score of 0 is assignedif the system returns an answer and 1 is assignedif it returns ?null?
for an ?irrelevant?
query.
Theresult of this evaluation on both data-sets is shownin Figure 7 and 8.Figure 8: Accuracy on Yahoo FAQ DatasetIn order to compare the performance of our sys-tem, we benchmark our results against Lucene?s15 Fuzzy match feature.
Lucene supports fuzzysearches based on the Levenshtein Distance, orEdit Distance algorithm.
To do a fuzzy search15http://lucene.apache.orgwe specify the ?
symbol at the end of each to-ken of the SMS query.
For example, the SMSquery ?romg actvt?
on the FAQ corpus is refor-mulated as ?romg?
0.3 actvt?
0.3?.
The param-eter after the ?
specifies the required similarity.The parameter value is between 0 and 1, with avalue closer to 1 only terms with higher similar-ity will be matched.
These queries are run on theindexed FAQs.
The results of this evaluation onboth data-sets is shown in Figure 7 and 8.
Theresults clearly demonstrate that our method per-forms 2 to 2.5 times better than Lucene?s Fuzzymatch.
It was observed that with higher valuesof similarity parameter (?
0.6, ?
0.8), the num-ber of correctly answered queries was even lower.In Figure 9 we show the runtime performance ofthe Naive vs Pruning algorithm on the Yahoo FAQDataset for 150 SMS queries.
It is evident fromFigure 9 that not only does the Pruning Algorithmoutperform the Naive one but also gives a near-constant runtime performance over all the queries.The substantially better performance of the Prun-ing algorithm is due to the fact that it queries muchless number of terms and ends up with a smallercandidate set compared to the Naive algorithm.Figure 9: Runtime of Pruning vs Naive Algorithmfor Yahoo FAQ Dataset7 ConclusionIn recent times there has been a rise in SMS basedQA services.
However, automating such serviceshas been a challenge due to the inherent noise inSMS language.
In this paper we gave an efficientalgorithm for answering FAQ questions over anSMS interface.
Results of applying this on twodifferent FAQ datasets shows that such a systemcan be very effective in automating SMS basedFAQ retrieval.859ReferencesRudy Schusteritsch, Shailendra Rao, Kerry Rodden.2005.
Mobile Search with Text Messages: Design-ing the User Experience for Google SMS.
CHI,Portland, Oregon.Sunil Kumar Kopparapu, Akhilesh Srivastava and ArunPande.
2007.
SMS based Natural Language Inter-face to Yellow Pages Directory, In Proceedings ofthe 4th International conference on mobile technol-ogy, applications, and systems and the 1st Interna-tional symposium on Computer human interactionin mobile technology, Singapore.Monojit Choudhury, Rahul Saraf, Sudeshna Sarkar, Vi-jit Jain, and Anupam Basu.
2007.
Investigation andModeling of the Structure of Texting Language, InProceedings of IJCAI-2007 Workshop on Analyticsfor Noisy Unstructured Text Data, Hyderabad.E.
Voorhees.
1999.
The TREC-8 question answeringtrack report.D.
Molla.
2003.
NLP for Answer Extraction in Tech-nical Domains, In Proceedings of EACL, USA.E.
Sneiders.
2002.
Automated question answeringusing question templates that cover the conceptualmodel of the database, In Proceedings of NLDB,pages 235?239.B.
Katz, S. Felshin, D. Yuret, A. Ibrahim, J. Lin, G.Marton, and B. Temelkuran.
2002.
Omnibase: Uni-form access to heterogeneous data for question an-swering, Natural Language Processing and Infor-mation Systems, pages 230?234.E.
Sneiders.
1999.
Automated FAQ Answering: Con-tinued Experience with Shallow Language Under-standing, Question Answering Systems.
Papers fromthe 1999 AAAI Fall Symposium.
Technical ReportFS-99?02, November 5?7, North Falmouth, Mas-sachusetts, USA, AAAI Press, pp.97?107W.
Song, M. Feng, N. Gu, and L. Wenyin.
2007.Question similarity calculation for FAQ answering,In Proceeding of SKG 07, pages 298?301.Aiti Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.A phrase-based statistical model for SMS text nor-malization, In Proceedings of COLING/ACL, pages33?40.Catherine Kobus, Franois Yvon and Graldine Damnati.2008.
Normalizing SMS: are two metaphors bet-ter than one?, In Proceedings of the 22nd Inter-national Conference on Computational Linguistics,pages 441?448 Manchester.Jeunghyun Byun, Seung-Wook Lee, Young-In Song,Hae-Chang Rim.
2008.
Two Phase Model for SMSText Messages Refinement, Association for the Ad-vancement of Artificial Intelligence.
AAAI Workshopon Enhanced MessagingRonald Fagin , Amnon Lotem , Moni Naor.
2001.Optimal aggregation algorithms for middleware, InProceedings of the 20th ACM SIGMOD-SIGACT-SIGART symposium on Principles of database sys-tems.I.
Dan Melamed.
1999.
Bitext maps and alignment viapattern recognition, Computational Linguistics.E.
Prochasson, Christian Viard-Gaudin, EmmanuelMorin.
2007.
Language Models for HandwrittenShort Message Services, In Proceedings of the 9thInternational Conference on Document Analysis andRecognition.Sreangsu Acharya, Sumit Negi, L. V. Subramaniam,Shourya Roy.
2008.
Unsupervised learning of mul-tilingual short message service (SMS) dialect fromnoisy examples, In Proceedings of the second work-shop on Analytics for noisy unstructured text data.860
