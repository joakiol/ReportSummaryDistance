Proceedings of NAACL HLT 2007, pages 452?459,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsMultilingual Structural Projection across Interlinear TextFei XiaDepartment of LinguisticsUniversity of WashingtonSeattle, WA 98195fxia@u.washington.eduWilliam D. LewisDepartment of LinguisticsUniversity of WashingtonSeattle, WA 98195wlewis2@u.washington.eduAbstractThis paper explores the potential for an-notating and enriching data for low-densitylanguages via the alignment and projec-tion of syntactic structure from parsed datafor resource-rich languages such as English.We seek to develop enriched resources for alarge number of the world?s languages, mostof which have no significant digital pres-ence.
We do this by tapping the body ofWeb-based linguistic data, most of whichexists in small, analyzed chunks embeddedin scholarly papers, journal articles, Webpages, and other online documents.
By har-vesting and enriching these data, we canprovide the means for knowledge discoveryacross the resulting corpus that can leadto building computational resources suchas grammars and transfer rules, which, inturn, can be used as bootstraps for build-ing additional tools and resources for thelanguages represented.11 IntroductionDeveloping natural language applications is generallydependent on the availability of annotated corpora.Building annotated resources, however, is a signif-icantly time consuming process involving consider-able human effort.
Although a number of projectshave been undertaken to develop annotated resourcesfor non-English languages, e.g., treebanks, the devel-opment of these resources has been no small feat, andto date have been limited to a very small number of1We would like to thank Dan Jinguji for creating theword alignment and source dependency structure goldstandards.
Our thanks also go to three anonymous re-viewers for their helpful comments and suggestions.the world?s languages (e.g., Chinese, German, Ara-bic, Korean, etc.).
Some notable efforts have beenundertaken to develop automated means for creatingannotated corpora through the projection of annota-tions (Yarowksy and Ngai, 2001; Xi and Hwa, 2005).The resulting methods, however, can only be appliedto a small number of language pairs due mostly tothe need for sizeable parallel corpora.
Unfortunately,most languages do not have parallel corpora of suffi-cient size, making these methods inapplicable for thevast majority of the world?s languages.We describe a method for bootstrapping resourcecreation by tapping the wealth of multilingual dataon the Web that has been created by linguists.
Ofparticular note is the linguistic presentation formatof ?interlinear text?, a common format used for pre-senting language data and analysis relevant to a par-ticular argument or investigation.
Since interlin-ear examples consist of orthographically or phoneti-cally encoded language data aligned with an Englishtranslation, the ?database?
of interlinear examplesfound on the Web, when taken together, constitute asignificant multilingual, parallel corpus covering hun-dreds to thousands of the world?s languages.We do not propose that a database of interlin-ear text alone is sufficient to create NLP resourcesand tools, but rather that it may act as a means formore rapidly developing such tools using less data.We contend that such a resource allows one to de-velop computational artifacts, such as grammars andtransfer rules, which can be used as ?seed?
knowledgefor building larger resources.
In particular, knowinga little about the structure of a language can helpin developing annotated corpora and tools, since alittle knowledge can go a long way in inducing accu-rate structure and annotations (Haghighi and Klein,2006).Of particular relevance to MT is the issue of struc-452tural divergence (Dorr, 1994).
Many MT models im-plicitly make the so-called direct correspondence as-sumption (DCA) as defined in (Hwa et al, 2002).However, to what extent that assumption holds istested only on a small number of language pairs us-ing hand aligned data (Fox, 2002; Hwa et al, 2002;Wellington et al, 2006).
A larger sample of typo-logically diverse language data can help test the as-sumption for hundreds of languages.We contend that the knowledge garnered fromstructural projections applied to interlinear text canbootstrap the development of resources and toolsacross parallel corpora, where such corpora could beof smaller size and the resulting tools more robust,opening the door to the development of tools and re-sources for a larger number of the world?s languages.Given the imminent death of half of the world?s 6,000languages (Krauss, 1992), the development of anylanguage specific tools for a larger percentage of theworld?s languages than is currently possible can aidin both their documentation and preservation.2 BackgroundThe practice of presenting language data in interlin-ear form has a long history in the field of linguistics,going back at least to the time of the structuralists(see (Swanton, 1912) for early examples).
The mod-ern form of interlinear data presentation started togel in the mid-1960s, resulting in the canonical threeline form shown in Ex (1), which we will refer toas Interlinear Glossed Text, or IGT.
The canonicalform consists of three lines: a line for the languagein question (often a sentence, which we will refer tohere as the source sentence), an English gloss line,and an English translation.2(1) Rhoddodd yr athro lyfr i?r bachgen ddoegave-3sg the teacher book to-the boy yesterday?The teacher gave a book to the boy yesterday?
(Bailyn, 2001)Although IGT is usually embedded in linguisticsdocuments as part of a larger analysis, in and ofitself it contains analysis and interesting informa-tion about the source language.
In particular, thegloss line, which is word and morpheme aligned withthe source, contains word and morpheme transla-tions for the source language data, and can even con-tain grammatically salient annotations (e.g., 3sg forThird Person Singular).
Further, the reader will note2As pointed out by a reviewer, there is a long tradi-tion in the classical languages for using interlinear trans-lations.
So, too, in other literature bases.
Our focus hereis strictly limited to IGT, the interlinear form used in thefield of linguistics.that many words are shared between the gloss andtranslation lines, allowing for the alignment betweenthese two lines as a intermediate step in the align-ment between the translation and the source.An effort is underway to collect these interlinearsnippets into an online searchable database, the pri-mary purpose of which is to help linguists find ana-lyzed data for languages they are interested in.
Weuse this resource, called ODIN, the Online Databaseof INterlinear text (Lewis, 2006)3, as our primarydata source.
At the time of this writing, ODIN con-tains 36,439 instances of interlinear data for 725 ofthe world?s languages.3 The Enrichment AlgorithmOur algorithm enriches the original IGT examples bybuilding syntactic structures over the English dataand then projects these onto the source languagedata via word alignment.
The term syntactic struc-ture in this paper refers to both phrase structure (PS)and dependency structure (DS).
The enrichment pro-cess has three steps:1.
Parse the English translation using an off-the-shelf parser.2.
Align the source sentence and English transla-tion with the help of the gloss line.3.
Project the English syntactic structures to ob-tain the source syntactic structures using wordalignment.3.1 Parsing English sentencesThere are many English parsers available to the pub-lic, and in this experiment we used Charniak?s parser(Charniak, 1997), which was trained on the EnglishPenn Treebank (Marcus et al, 1994).
Figure 1(a)shows a parse tree (in the Penn Treebank style) forthe English translation in Ex (1).
Given a parse tree,we use a head percolation table (Magerman, 1995)to create the corresponding dependency structure.Figure 2(a) shows the dependency structure derivedfrom the parse tree in Figure 1(a).3.2 Word alignmentBecause most of the 700+ languages in ODIN arelow-density languages with no on-line bilingual dic-tionaries or large parallel corpora, aligning the sourcesentence and its English translation directly wouldnot work well.
To take advantage of the unique lay-out of IGT examples, we propose using the gloss lineas a bridge between the other two lines; that is, wefirst align the source sentence and the gloss line, andthen align the gloss line and the English translation.The process is illustrated in Figure 3.3The url of ODIN is http://www.csufresno.edu/odin453SNP1 VPNNteacherVBDgaveNP2DTaNP4PPNNtheIN NP3yesterday(a)  English PSNNDTbookNNboyDTTheto(b)  Source PS after Step 2SNP1 VPNNathro(teacher)VBDrhoddodd(gave)NP2 NP4PPNNi?rIN NP3ddoe(yesterday)NNDTlyfr(book)NNbachogen(boy)DTyr(the)i?r(to-the)SNPNNVBDNP NPPPNNIN+DTNNNNDT(c)  Final source PSrhoddodd(gave) yr(the)athro(teacher)lyfr(book)i?r(to-the)bachogen(boy) ddoe(yesterday)Figure 1: English PS produced by Charniak?s parser, and source PS projected from the English PSteachergavea boythebook(a) English DStheyesterdayto athrobachgenlyfryrddoei?rRhoddoddi?r(b)  Source DS after Step 2athrobachgenlyfryrddoei?rRhoddodd(c)  Final source DSFigure 2: English DS derived from English PS, and source DS projected from the English DSThe      teacher  gave    a     book  to    the    boy   yesterdayRhoddodd     yr    athro       lyfr     i?r    bachgen     ddoeGloss:Transatlion:Source:gave-3sg    the    teacher   book   to-the  boy   yesterdayFigure 3: Aligning source sentence and Englishtranslation with help of the gloss lineThe alignment between the source sentence andthe gloss line is trivial and our preliminary exper-iments showed that simply using whitespace anddashes as delimiters, and assuming a one-to-onealignment produces almost perfect results.
In con-trast, the alignment between the gloss line and theEnglish translation is more complicated since align-ment links can cross and words on one side can linkto zero or more words on the other side.
We builttwo aligners for this stage, as described below.3.2.1 Statistical word alignerWe create a parallel corpus by using the gloss linesand the translation lines of all the IGT examples forall the languages in ODIN.
We then train IBM mod-els (Brown et al, 1993) using the GIZA++ package(Och and Ney, 2000).
In addition to the commonpractice of lowercasing words and combining wordalignments from both directions, we adopt the fol-lowing strategies to improve word alignment:Breaking words into morphemes: Since amulti-morpheme word in a gloss line often corre-sponds to multiple words in the translation line, wesplit each word on the gloss line into morphemes us-ing the standard IGT morpheme delimiters (e.g., ?-?).
For instance, the seven words in the gloss line ofEx (1) become nine morphemes.Adding (x,x) pairs: If a word x appears in thegloss and the translation lines of the same IGT ex-ample, it is highly likely that the two copies of thesame word should be aligned to each other.
To helpGIZA++ recognize this property, we first identifyand collect all such words and then add single wordpairs (x,x) to the training data.
For instance, fromEx (1), we would add a sentence pair for each mor-pheme (excepting -3sg which does not appear in thetranslation line).3.2.2 Heuristic word alignerOur second word aligner is based on the assump-tion that if two words (one on the gloss line, the otheron the translation line) have the same root form, theyare likely to be aligned to one other.We built a sim-ple English morphological analyzer and ran it on thetwo lines, and then linked the words with the same454root form.43.3 Tree projectionWe designed two projection algorithms: one whichprojects PS and the other which projects DS, bothfrom the English to the source language.53.3.1 Projecting dependency structureOur DS projection algorithm is similar to the pro-jection algorithms described in (Hwa et al, 2002) and(Quirk et al, 2005).
It has four steps: First, we copythe English DS, and remove all the unaligned Englishwords from the DS.6 Second, we replace each Englishword in the DS with the corresponding source words.If an English word x aligns to several source words,we will make several copies of the node for x, onecopy for each such source word.
The copies will allbe siblings in the DS.If a source word aligns to multiple English words,after Step 2 the source word will have several copiesin the resulting DS.
In the third step, we keep onlythe copy that is closest to the root and remove all theother copies.7 In Step 4, we attach unaligned sourcewords to the DS using the heuristics described in(Quirk et al, 2005).
Figure 2 shows the English DS,the source DS after Step 2, and the final DS.3.3.2 Projecting phrase structureOur PS projection algorithm also has four steps,the first two being the same as those for projectingDS.
In the third step, starting from the root of thecurrent source PS and for each node x with morethan one child, we reorder each pair of x?s childrenuntil they are in the same order as dictated by thesource sentence.
Let yi and yj be two children ofx, and their spans be Si = [ai, bi] and Sj = [aj , bj ].When we reorder yi and yj, there are four possiblescenarios:(1) Si and Sj don?t overlap: we put yi before yjif ai < aj or the opposity if ai > aj .4When a word is repeated in both the gloss and trans-lation, the individual occurrences are aligned individuallyin left-to-right order.5The DS projection algorithm as described does notguarantee that the yield of the resulting source DS hasthe same word order as the source sentence; however, ifneeded, the algorithm can be easily modified (by mak-ing its Step 3 similar to the Step 3 of the PS projectionalgorithm) to ensure the correct word order.6Every time we remove an internal node x from a DS,we make x?s children depend on x?s parent directly.7The heuristic is not as arbitrary as it sounds becausevery often when a source word aligns to multiple Englishwords, one of the English words dominates the rest inthe DS (e.g., the node for to in Figure 2(a) dominatesthe node for the).
We are using the dominant word torepresent the whole set.
(2) Si is a strict subset of Sj: we remove yjfrom the PS and promote its children: yj?schildren will become children of yj ?s parent.
(3) Sj is a strict subset of Si: we remove yi andpromote its children.
(4) Si and Sj overlap but neither is a strictsubset of the other: we remove both yi andyj and promote their children.
If both yi andyj are leaf nodes with the same span, we willmerge the two nodes.8The last step is to insert unaligned source wordsinto the source PS.
For each unaligned source wordx, we will find its closest left and right neighbors thatare aligned to some English words, and then attach xto the lowest common ancestor of the two neighbors.Figure 1 shows the English PS, the source PS afterStep 2, and the final source PS.
The three boxes in1(b) mark the nodes that are removed in Step 3.4 ExperimentsWe tested the feasibility of our approach on a smallset of IGT examples for seven languages: Ger-man (GER), Korean (KKN), Hausa (HUA), Mala-gasy (MEX), Welsh (WLS), Irish (GLI), and Yaqui(YAQ).
This set of languages was chosen because ofits typological diversity: GER and HUA are SVOlanguages, KKN and YAQ are SOV, GLI and WLSare VSO, and MEX is VOS.
In addition, while Ger-man and Korean are well-studied and have readilyaccessible resources that we could use to test the ef-fectiveness and accuracy of our methods, Yaqui, withabout 16,000 speakers, is a highly endangered lan-guage and serves as a demonstration of our methodsfor resource-poor and endangered languages.4.1 Creating the gold standard for the testsetThe number of IGT examples in ODIN varies greatlyacross the seven languages, ranging from less thanone hundred for Welsh to over seventeen hundred forGerman.
For each language, we randomly picked 50-150 IGT examples from the available examples whoseEnglish translations had at least five words.9 Theexamples were manually checked and corrupted ex-amples were thrown away.
The remaining examples8We will keep one copy and merge the POS tag ofthe words.
For instance, the tag IN+DT in Figure 1(c)was created when two copies of i?r in Figure 1(b) weremerged.9We skipped examples with very short English trans-lations because they are unlikely to contain much in theway of syntactic structures.455Table 1: The size and average sentence length of the test dataGER KKN HUA MEX WLS GLI YAQ Total# of IGT examples 104 103 77 87 53 46 68 538# of src words 739 526 441 498 313 252 404 3173Ave src sent leng 7.11 5.11 5.73 5.72 5.91 5.48 5.94 5.90# of Eng words 711 735 520 646 329 278 544 3823Ave Eng sent leng 7.41 7.14 6.75 7.43 6.21 6.04 8.01 7.11# of speakers 128M 78M 39M 9.4M 580K 260K 16K 255.3Mformed our test data.
Table 1 shows the size and av-erage sentence lengths of the test data by language.10The languages are sorted by number of speakers (asderived from the Ethnologue (Gordon, 2005)).We ran our algorithm on the test data, and thesystem produced the following: an English PS, En-glish DS, word alignment, projected source PS, andprojected source DS.
We asked human annotators tomanually check the output and correct the EnglishDS, word alignments and projected DS structureswhere necessary.11 12 In order to calculate inter-annotator agreement, the Yaqui data and half of theGerman data were each checked by two annotators,and the disagreement between the annotators wasadjudicated and a gold standard was created.
Theinter-annotator agreement (a.k.a.
the F-measure ofdependency or alignment links) on English DS, gloss-translation alignment, and projected source DS are96.34%, 96.35%, and 91.09%, respectively.
The restof the data were annotated by one annotator.4.2 Word alignment resultsWe tested our word aligners on 70% (374 examples)of the whole test set (538 examples), while reservingthe remaining 30% for future use.4.2.1 Statistical word alignerAs indicated earlier, the ODIN database contains36,439 IGT examples.
We removed duplicates13 and10There are three reasons why the sentences are soshort.
First, since IGT is used to present particular lin-guistically salient morphological or syntactic material,sentences in IGT are only as long as needed for thegiven expose?.
Second, space constraints often dictate us-ing shorter examples (i.e., they must fit on one line).Third, the IGT extraction algorithm currently used inODIN does not search for the less common multi-line(i.e., greater than three line) examples.11The English PS and source PS were not corrected;without a thorough linguistic study of the source lan-guages, it is impossible to devise appropriate gold stan-dards for their phrase structures.12The DS structures for the English and source lan-guage in the gold standard can be non-isomorphic.13Duplicates are common since it is standard practicein linguistics to copy and cite language examples fromother papers.Table 2: The training data for GIZA++# of sentences 28,902# of words in gloss lines 174,765# of morphemes in gloss lines 251,465# of words in translation lines 217,022Size of gloss word vocabulary 16360Size of gloss morpheme vocabulary 14050Size of translation word vocabulary 14029Table 3: The word alignment results when glosswords are not split into morphemesPrecision Recall F-measureGloss ?
trans 0.674 0.689 0.681Trans ?
gloss 0.721 0.823 0.769Intersection 0.948 0.620 0.750Union 0.590 0.892 0.711Refined 0.846 0.780 0.812examples with missing lines, and used the remain-ing 28,902 examples for GIZA++ training.14 Table2 shows the statistics of the training data with allwords lowercased.
Tables 3?5 show the performanceof the word aligner under three settings:(1): Not splitting words in the gloss lines into mor-phemes.
(2): Splitting words in gloss lines into morphemes.
(3): Doing (2) plus adding (x,x) sentence pairs intothe training data, where x is a word that appearsin both the gloss and translation lines of thesame IGT example.For each setting, we trained in both directions andcombined the two alignments by taking the intersec-tion, union, and refined as defined in (Och and Ney,2000).
The best F-score for each setting is in bold-face.
From the tables, it is clear that the third set-ting works the best, and combining the alignments14Interestingly, although the IGT examples in thetraining data come from hundreds of languages in ODIN,IBM Model 4 performs significantly better than Models1 and 2 (by at least two percent points for F-measure);therefore, all the GIZA++ results reported in the paperare based on Model 4.456Table 4: The word alignment results when glosswords are split into morphemesPrecision Recall F-measureGloss ?
trans 0.746 0.889 0.811Trans ?
gloss 0.797 0.863 0.829Intersection 0.958 0.811 0.878Union 0.659 0.941 0.775Refined 0.918 0.900 0.909Table 5: The word alignment results when (x,x) pairsare addedPrecision Recall F-measureGloss ?
trans 0.759 0.922 0.833Trans ?
gloss 0.801 0.924 0.858Intersection 0.956 0.885 0.919Union 0.666 0.961 0.787Refined 0.908 0.921 0.915from both directions works better than either direc-tion alone.154.2.2 Heuristic word alignerThe word aligner has two settings.
In the firstone, the aligner aligns two words if and only if theyhave the same orthographic form.
In the second, italigns two words if and only if they have the sameroot form.16 The results are shown in the first andsecond rows of Table 6.We experimented with various methods of com-bining the two aligners, and the best one is an aug-15For languages with hundreds of IGT examples, onemay wonder whether training GIZA++ with the data forthat language alone would outperform the system trainedwith IGT examples from all the languages in ODIN.
Toanswer this question, we ran three experiments on theGerman data (for which there are 1757 IGT examplesin ODIN after removing duplicates): (a) trained on the(gloss, translation) pairs for all IGT data, (b) trained onthe (gloss, translation) pairs of the German data alone,and (c) trained on the (source, translation) pairs of theGerman data.
The test was run against 58 IGT examples,a subset of the German test data in Table 1.
It turns outthat (a) performs much better than (b) and (c), whichjustifies the approach we proposed in Section 3.2.
Forinstance, the F-measures for the refined alignment for(a)-(c) are 92.5%, 90.2%, and 85.6%, respectively.16For the second setting, we wrote a 90-line Perl appli-cation that finds the root for each English word by usinga dozen regular expression patterns combined with a listof 163 irregular verbs with their inflected forms.Table 6: The performance of heuristic word alignerPrecision Recall F-measureNo morphing 0.983 0.742 0.846With morphing 0.983 0.854 0.914Augmented aligner 0.981 0.881 0.928mented heuristic word aligner which links two wordsif and only if they have the same root form or theyare good translations of each other according to thetranslation model built by GIZA++.17 The resultis shown in the last row of Table 6.
We used thisaligner for the structural projection experiment.4.3 Projection resultsWe evaluated the results of the major steps in our al-gorithm: the English DS derived from the parse treesproduced by the English parser, the word alignmentbetween the gloss and translation lines, and the pro-jected source DS.
We calculated the precision, recall,and F-score of the dependency links and word align-ment links.
The F-scores are shown in Table 7.18Both the English parser and the word aligner workreasonably well with most F-scores well above 90%.The F-scores for dependency links in the source DSare lower partly due to errors in early parts of theprocess (e.g., English DS and word alignment), whichpropagates to this step.
When we replace the auto-matically generated English DS and word alignmentwith the ones in gold standard, the F-measure ofsource DS increases significantly, as shown in Table8.To identify the causes of the remaining errors inthe oracle results, we manually checked and classifiedone third of the errors in the German data.
Amongthe 43 errors in the source DS, 26 (60.5%) are dueto language divergence (e.g., head switching), eight(18.6%) are errors made by the projection heuristics,and nine (20.9%) are due to non-exact translationssuch as the one shown in Ex (2).
Because languagedivergence can reveal interesting typological distinc-tions between languages, the first type of error may,in fact, identify examples that could be of great valueto linguists and computational linguists.
(2) der Antrag des oder der Dozententhe petition of-the.SG or of-the.PL docent.MSC?the petition of the docent.?
(Daniels, 2001)5 Discussion5.1 The IGT bias and knowledge discoveryfrom enriched dataFrom the enriched data, various kinds of informa-tion can be extracted, such as grammars and transferrules.
We extracted CFGs for the seven languages byreading off the context-free rules from the projected17We treat a word pair, (e,f), as a good translation ifand only if both P (e|f) and P (f |e) are high.18The Total word alignment F-measure is higher than0.928 as mentioned in Table 6 because the test set usedhere is the superset of the one used in that section.457Table 7: The system performance on the seven languagesGER KKN HUA MEX WLS GLI YAQ TotalEnglish DS 94.25 89.78 96.15 95.51 91.49 93.53 93.57 93.48Word alignment 94.91 94.20 94.71 94.26 95.65 88.11 93.64 94.03Source DS 78.14 82.16 84.71 84.22 84.39 78.17 79.36 81.45Table 8: The F-measure of source dependency links with perfect English DS and/or word alignmentGER KKN HUA MEX WLS GLI YAQ TotalWith gold Eng DS 82.21 87.67 88.46 85.23 91.72 80.16 83.81 85.42With gold alignment 85.77 86.15 86.07 88.44 84.98 82.40 86.27 86.00With both 91.21 91.67 89.82 89.65 94.25 85.77 90.68 90.64Table 9: Extracted CFGs and evidence of word orderHUA MEX GLI YAQWord order SVO VOS VSO SOV# of rule types 102 129 86 115# of rule tokens 384 466 202 295source PS.
The numbers of rule types and rule tokensfor four of the languages are listed in Table 9.It is important to note that IGT data is somewhatbiased: examples tend to be short and are selectedfor the purposes of a particular rhetorical context.They, therefore, deviate from the ?normal?
usagethat one might normally expect to find in a corpus oflanguage data.
As such, one might question whetherthe information extracted from IGT would also beskewed due to these biases.To test the usefulness of the data for answeringtypological questions, we wrote a tool that predictedthe canonical word order (e.g., SOV, SVO) of a lan-guage using simple heuristics.
It was able to pro-duce the correct answers for all seven languages inour sample.19 20 We suspect that the number ofIGT instances and their diversity (i.e., from multipledocuments) is crucial to overcoming the IGT bias,and feel that the same heuristics could be appliedto a much larger sample of languages.
These couldbe further adapted to additional typological param-eters beyond word order (e.g., orders of heads andmodifiers in PS).
We leave this to future work.Given syntactically enriched data, it is also possi-ble to search for patterns that are linguistically in-teresting.
For instance, we wrote a piece of codethat automatically identified examples with crossing19Our code simply went through all the rules in theextracted CFGs and checked the position of the verb withrespect to its subject and object.
The -SBJ and -OBJfunction tags were added to the English parse trees usingsimple heuristics and were carried over to the source PSvia the projection algorithm.20There is disagreement among linguists about Ger-man?s underlying word order, being either SVO or SOV.Our heuristics returned SOV.dependencies (i.e., the ones whose DS have crossinglinks).
One such example from the Yaqui data is inEx (3), where the coordinated noun phrase kow-tainto mis-ta ?the pig and the cat?
is separated by theverb bwuise-k ?grasp?.
Note that the crossing depen-dencies can only be discovered in the Yaqui data andnot in the English since none exist in the English.
(3) inepo kow-ta bwuise-k into mis-ta1SG pig-NNOM.SG grasp-PST and cat-NNOM.SG?I caught the pig and the cat.?
(Mart?
?nez Fabia?n,2006)So far, we have examined linguistically interestinginformation in the source.
In the future, we plan toexamine structures in both the source and English.For instance, we plan to extract transfer rules fromthe aligned source and English structures and alsocalculate head/modifier crossings between languagessimilar to those described in (Fox, 2002).5.2 Tools and resource buildingThe information that we discover about a languagecan help with the development of tools for the lan-guage.
The order of constituents, for instance, canbe used to inform prototype-driven learning strate-gies (Haghighi and Klein, 2006), which can thenbe applied to raw corpora.
It is also possible thatsmall samples of data showing the alignment inter-actions between source language structures and thoseof English can provide essential bootstrap informa-tion for informing machine translation systems (cf(Quirk and Corston-Oliver, 2006)).Proof of the utility of an enriched corpus built overODIN will depend crucially on its evaluation, and wefeel that an important part of our future work will bethe development of parsers that have been trained onprojected structures.
These parsers can be evaluatedagainst human built corpora such as treebanks (obvi-ously, only for those languages that have treebanks).Proof will also come from linguists who will be ableto use the corpus to search for constructions of in-terest (e.g., passives, relative clauses, etc.
), and willlikely be able to do so using standard tools such as458tgrep.21 Crucially, linguists would be able to conductsuch searches over a very large number of languages.6 ConclusionIn this paper we demonstrate a methodology for pro-jecting structure from annotated English data ontosource language data.
Because each IGT instanceprovides an English translation and an intermedi-ary gloss line, we are able to project full syntac-tic structures from the automatically parsed trans-lation.
The fact that our basic methodology andcode were applied to a typologically diverse sampleof seven languages without modification suggests thepotential for application to a much larger sample,perhaps numbering into the hundreds of languages.The resulting enriched structures could be of greatimportance to the fields of linguistics and compu-tational linguistics.
For the former, search facili-ties could be built over the data that would allowlinguists to find syntactically marked up data for alarge variety of languages, and could even accommo-date cross-linguistic comparisons and analyses.
Forthe latter, we could automatically discern grammarsand transfer rules from the aligned and marked updata, where these computational artifacts could actas bootstraps for the development of additional toolsand resources.ReferencesJohn Frederick Bailyn.
2001.
Inversion, dislocation andoptionality in russian.
In Gerhild Zybatow, editor,Current Issues in Formal Slavic Linguistics.Peter Brown, Vincent Pietra, Stephen Pietra, and RobertMercer.
1993.
The Mathematics of Statistical Ma-chine Translation: Parameter Estimation.
Computa-tional Linguistics, 19(2):263?311.Eugene Charniak.
1997.
Statistical Parsing with aContext-Free Grammar and Word Statistics.
In Proc.of AAAI-1997.Michael W. Daniels.
2001.
On a type-based analysisof feature neutrality and the coordination of unlikes.In Proceedings of the 8th International HPSG Confer-ence.
CSLI Publications.Bonnie J. Dorr.
1994.
Machine translation divergences:a formal description and proposed solution.
Computa-tional Linguistics, 20(4):597?635.Heidi Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proceedings of EMNLP 2002,Philadelphia, Pennsylvania.21This kind of search is reminiscent of Resnik?s Lin-guists Search Engine (http://lse.umiacs.umd.edu), whichallows structural search across text found on the Web.Raymond G. Gordon, editor.
2005.
Ethnologue: Lan-guages of the World.
SIL International, Dallas, TX,fifteenth edition.Aria Haghighi and Dan Klein.
2006.
Protoype-drivensequence models.
In Proceedings of HLT-NAACL, NewYork City, NY.Rebecca Hwa, Philip Resnik, Amy Weinberg, and OkanKolak.
2002.
Evaluating translational correspondenceusing annotation projection.
In Proceedings of the 40thAnnual Meeting of the ACL, Philadelphia, Pennsylva-nia.Michael Krauss.
1992.
The World?s Languages in Crisis.Language, 68(1):4?10.William D. Lewis.
2006.
ODIN: A Model for Adaptingand Enriching Legacy Infrastructure.
In Proceedingsof the e-Humanities Workshop, Amsterdam.
Held incooperation with e-Science 2006: 2nd IEEE Interna-tional Conference on e-Science and Grid Computing.David M. Magerman.
1995.
Statistical Decision-TreeModels for Parsing.
In Proc.
of the 33rd Annual Meet-ing of the Association for Computational Linguistics(ACL-1995), Cambridge, Massachusetts, USA.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,et al 1994.
The Penn Treebank: Annotating Pred-icate Argument Structure.
In Proc of ARPA Speechand Natural Language Workshop.Constantino Mart?
?nez Fabia?n.
2006.
Yaqui Coordina-tion.
Ph.D. thesis, University of Arizona.Franz-Josef Och and Hermann Ney.
2000.
Improved Sta-tistical Alignment Models.
In the 38th Annual Confer-ence of the Association for Computational Linguistics(ACL-2000), pages 440?447.Chris Quirk and Simon Corston-Oliver.
2006.
The im-pact of parse quality on syntactically-informed statis-tical machine translation.
In Proceedings of EMNLP2006.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency tree translation: Syntactically informedphrasal smt.
In Proceedings of ACL 2005.John R. Swanton.
1912.
Haida songs.
In Franz Boas,editor, Publications of the American Ethnological So-ciety, Volume III.
E. J. Brill.Benjamin Wellington, Sonjia Waxmonsky, and I. DanMelamed.
2006.
Empirical lower bounds on the com-plexity of translation equivalence.
In Proceedings ofACL 2006.Chenhai Xi and Rebecca Hwa.
2005.
A backoff modelfor bootstrapping resources for non-English languages.In Proceedings of HLT-EMNLP, pages 851?858, Van-couver, British Columbia, Canada.David Yarowksy and Grace Ngai.
2001.
Inducing Mul-tilingual POS taggers and NP Bracketers via robustprojection across aligned corpora.
In Proceedings ofNAACL-2001, pages 377?404.459
