Detecting Errors in Corpora Using Support Vector MachinesTetsuji Nakagawa?
and Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916?5 Takayama, Ikoma, Nara 630?0101, Japannakagawa378@oki.com, matsu@is.aist-nara.ac.jpAbstractWhile the corpus-based research relies on hu-man annotated corpora, it is often said that anon-negligible amount of errors remain even infrequently used corpora such as Penn Treebank.Detection of errors in annotated corpora is im-portant for corpus-based natural language pro-cessing.
In this paper, we propose a methodto detect errors in corpora using support vec-tor machines (SVMs).
This method is basedon the idea of extracting exceptional elementsthat violate consistency.
We propose a methodof using SVMs to assign a weight to each ele-ment and to find errors in a POS tagged corpus.We apply the method to English and JapanesePOS-tagged corpora and achieve high precisionin detecting errors.1 IntroductionCorpora are widely used in natural languageprocessing today.
For example, many statisticalpart-of-speech (POS) taggers have been devel-oped and they use corpora as the training datato obtain statistical information or rules (Brill,1995; Ratnaparkhi, 1996).
For natural languageprocessing systems based on a corpus, the quan-tity and quality of the corpus affect their per-formance.
In general, corpora are annotatedby hand, and therefore are error-prone.
Theseerrors are problematic for corpus-based sys-tems.
The errors become false training exam-ples and deteriorate the performance of the sys-tems.
Furthermore, incorrect instances may beused as testing examples and prevent the accu-rate measurement of performance.
Many stud-ies and improvements have been conducted for?
Presently with Service Media Laboratory, CorporateResearch and Development Center, Oki Electric IndustryCo.,Ltd.POS tagging, and major methods of POS tag-ging achieve an accuracy of 96?97% on the PennTreebank WSJ corpus, but obtaining higher ac-curacies is difficult (Ratnaparkhi, 1996).
It ismentioned that the limitation is largely causedby inconsistencies in the corpus (Ratnaparkhi,1996; Padro?
and Ma`rquez, 1998; van Halterenet al, 2001).
Therefore, correcting the errors ina corpus and improving its quality is important.However, to find and correct errors in corporaby hand is costly, since the size of corpora isusually very large.
Hence, automatic detectionof errors in corpora is necessary.One of the approaches for corpus error de-tection is use of machine learning techniques(Abney et al, 1999; Matsumoto and Yamashita,2000; Ma et al, 2001).
These methods regarddifficult elements for a learning model (boostingor neural networks) to learn as corpus errors.Abney et al (1999) studied corpus error detec-tion using boosting.
Boosting assigns weights totraining examples, and the weights are large forthe examples that are difficult to classify.
Misla-beled examples caused by annotators tend to bedifficult examples to classify and these authorsconducted error detection of POS tags and PPattachment information in a corpus by extract-ing examples with a large weight.Some probabilistic approaches for corpus er-ror detection have also been studied (Eskin,2000; Murata et al, 2000).
Eskin (2000) con-ducted corpus error detection using anomaly de-tection.
He supposed that all the elements in acorpus are generated by a mixture model con-sisting of two distributions, a majority distri-bution (typically a structured distribution) andan anomalous distribution (a uniform randomdistribution), and erroneous elements are gen-erated by the anomalous distribution.
For eachelement in a corpus, the likelihood of the mixedmodel is calculated in both cases when the el-ement is generated from the majority distribu-tion and from the anomalous one.
The elementis detected as an error if the likelihood in thelatter case is large enough.In this paper, we focus on detection of er-rors in corpora annotated with POS tags, andpropose a method for corpus error detection us-ing support vector machines (SVMs).
SVMsare one of machine learning models and appliedto many natural language processing tasks withsuccess recently.
In the next section, we explaina method to use SVMs for corpus error detec-tion.2 Corpus Error Detection UsingSupport Vector MachinesTraining data for corpus error detection is usu-ally not available, so we have to solve it as anunsupervised learning problem.
We consider inthe following way: in general, a corpus is builtaccording to a set of guidelines, thus it shouldbe consistent.
If there is an exceptional elementin the corpus that jeopardizes consistency, it islikely to be an error.
Therefore, corpus errordetection can be conducted by detecting excep-tional elements that causes inconsistency.While this is a simple and straightforwardapproach and any machine learning method isapplicable to this task, we will use SVMs asthe learning algorithm in the settings describedin Section 2.2.
The advantage of using SVMsin this setting is the following: In our setting,each position in the annotated corpus receivesa weight according to the SVM algorithm andthese weights can be used as the confidencelevel of erroneous examples.
By effectively us-ing those weights the inspection of the erroneousparts can be undertaken in the order of the con-fidence level, so that an efficient browsing ofcorpus becomes possible.
We believe this is aparticular advantage of our method comparedwith the methods that use other machine learn-ing methods.2.1 Support Vector MachinesSupport Vector Machines (SVMs) are a su-pervised machine learning algorithm for binaryclassification (Vapnik, 1998).
Given l trainingexamples of feature vector xi ?
RL with labelyi ?
{+1,?1}, SVMs map them into a high di-mensional space by a nonlinear function ?
(x)and linearly separate them.
The optimal hy-perplane to separate them is found by solvingthe following quadratic programming problem:minimize?1,...,?l12l?i,j=1?i?jyiyjK(xi,xj)?l?i=1?i,subject to 0 ?
?i ?
C (1 ?
i ?
l),l?i=1?iyi = 0,where the function K(xi,xj) is the inner prod-uct of the nonlinear function (K(xi,xj) =?
(xi) ??
(xj)) called a kernel function, and theconstant C controls the training errors and be-comes the upper bound of ?i.
Given a testexample x, its label y is decided by summingthe inner products of the test example and thetraining examples weighted by ?i:y = sgn( l?i=1?iyiK(xi,x) + b),where b is a threshold value.
Thus, SVMs as-sign a weight ?i to each training example.
Theweights are large for examples that are hard forSVMs to classify, that is, exceptional examplesin training data have a large weight.
We con-duct corpus error detection using the weights.To detect exceptional examples in a corpusannotated with POS tags, we first construct anSVM model for POS tagging using all the el-ements in a corpus as the training examples.Note that each example corresponds to a wordin the corpus.
Then SVMs assign weights tothe examples, and large weights are assigned todifficult examples.
Finally, we extract exampleswith a large weight greater than or equal to athreshold value ??.
In the next subsection, wedescribe how to construct an SVM model forPOS tagging.2.2 Revision Learning for POS taggingWe use a revision learning method (Nakagawaet al, 2002) for POS tagging with SVMs1.
Thismethod creates training examples of SVMs with1The well known one-versus-rest method (Allwein etal., 2000) can be also used for POS tagging with SVMs,but it has large computational cost and cannot han-dle segmentation of words directly that is necessary forJapanese morphological analysis.binary labels for each POS tag class using astochastic model (e.g.
n-gram) as follows: eachword in a corpus becomes a positive exampleof its POS tag class.
We then build a simplestochastic POS tagger based on n-gram (POSbigram or trigram) model, and words in the cor-pus that the stochastic model failed to tag witha correct part-of-speech are collected as nega-tive examples of the incorrect POS tag class.In such way, revision learning makes a model ofSVMs to revise outputs of the stochastic model.For example, assume that for a sentence:"11/CD million/CD yen/NNS are/VBP paid/VBN",a stochastic model tags incorrectly:"11/CD million/CD yen/NN are/VBP paid/VBN".In this case, the following training examplesare created for SVMs (each line corresponds toan example):<Class (Label)> <Feature Vector>CD (+1) (word:11, word-1:BOS, ...)CD (+1) (word:million, word-1:11, ...)NN (-1) (word:yen, word-1:million, ...)NNS (+1) (word:yen, word-1:million, ...)VBP (+1) (word:are, word-1:yen, ...)VBN (+1) (word:paid, word-1:are, ...)Thus, the positive and negative examples arecreated for each class (POS tag), and a modelof SVMs is trained for each class using thetraining examples.In English POS tagging, for each word w inthe tagged corpus, we use the following featuresfor SVMs:1. the POS tags and the lexical forms of thetwo words preceding w;2. the POS tags and the lexical forms of thetwo words succeeding w;3. the lexical form of w and the prefixes andsuffixes of up to four characters, the exis-tence of numerals, capital letters and hy-phens in w.Japanese morphological analysis can be con-ducted with revision learning almost in the sameway as English POS tagging, and we use the fol-lowing features for a morpheme ?:1.
the POS tags, the lexical forms and the in-flection forms of the two morphemes pre-ceding ?;2.
the POS tags and the lexical forms of thetwo morphemes succeeding ?;3.
the lexical form and the inflection form of?.2.3 Extraction of InconsistenciesSo far, we discussed how to detect exceptionalelements in a corpus.
However, it is insuffi-cient and inconvenient for corpus error detec-tion, because an exceptional element is not al-ways an error, that is, an exceptional elementmay be a correct or an incorrect exceptional el-ement.
Furthermore, it is often difficult to judgewhether it is a true error or not when only theexceptional element is shown.
To solve theseproblems, we extract not only an exceptionalexample but also another similar example thatis inconsistent with the exceptional example.
Ifthe exceptional example is correct, the secondexample is likely to be an error, and vice versa.We assume that an inconsistency occurs whentwo examples have similar features but have op-posite labels.
The similarity between two ex-amples xi and xj on SVMs is measured by thefollowing distance:d(xi,xj) =???(xi)??
(xj)?2,=?K(xi,xi) +K(xj ,xj)?
2K(xi,xj).We can extract inconsistencies from a corpusas follows: given an example x which was de-tected as an exceptional example (following theproposal in the previous subsection), we extractan example z with the smallest values of thedistance d(x, z) from the examples whose labelis different from x.
Intuitively, z is a closestopposite example to x in the SVMs?
higher di-mensional space and may be a cause for x to beattached a large weight.3 ExperimentsWe perform experiments of corpus error detec-tion using the Penn Treebank WSJ corpus (inEnglish), the RWCP corpus (in Japanese) andthe Kyoto University Corpus (in Japanese).
Inthe following experiments, we use SVMs withsecond order polynomial kernel, and the upperbound value C is set to 1.Table 1: Examples of Correctly Detected Errors and Incorrectly Detected Errors in the WSJ CorpusCorrectly Detected Errorspay about 11 million yen/NNS ( $ 77,000 budgeted about 11 million yen/NN ( $ 77,500, president and chief/JJ executive officer of named president and chief/NN executive officerfor its fiscal first quarter ended/VBN Sept. 30 its first quarter ended/VBD Sept. 30 wasIncorrectly Detected ErrorsEOS 3/LS .
EOS Send your child to Nov. 1-Dec .
EOS 3/CD .
EOS3.1 Experiments on the Penn TreebankWSJ Corpus (English)Experiments are performed on the Penn Tree-bank WSJ corpus, which consists of 53,113 sen-tences (1,284,792 tokens).We create models of SVMs for POS tag-ging using the corpus with revision learning.The distribution of the obtained weights ?i areshown in Figure 1.
The values of ?i concentratenear the lower bound zero and the upper boundC.
The examples with ?i near the upper boundseem to be exceptional.
Therefore, we regardedthe examples with ?i ?
0.5 as exceptional ex-amples (i.e.
??
= 0.5).
As a result, 1,740 ele-ments were detected as errors.
We implementeda browsing tool for corpus error detection withHTML (see Figure 2).
A detected inconsistencypair is displayed in the lower part of the screen.We examined by hand whether the detected er-rors are true errors or not for the first 200 el-ements in the corpus from the detected 1,740elements, and 199 were actual errors and 1 wasnot.
The precision (the ratio of correctly de-tected errors for all of the detected errors) was99.5%.
Examples of correctly detected errorsand incorrectly detected errors from the corpusare shown in Table 1.
The underlined wordswere detected as errors.
To judge whether theyare true errors or not is easy by comparing thepair of examples that contradict each other.To examine the recall (the ratio of correctlydetected errors for all of the existing actual er-rors in corpora), we conduct another experi-ments on an artificial data.
We made the arti-ficial data by randomly changing the POS tagsof randomly selected ambiguous tokens in theWSJ corpus.
The tags of 12,848 tokens (1% forthe whole corpus) are changed, and the results1101001000100001000001000000100000000 0.2 0.4 0.6 0.8 1NumberPositive ExamplesNegative Examples?Figure 1: Distribution of the Value ?
on theWSJ CorpusFigure 2: A Tool for Corpus Error Detectionare shown in Table 2 for various values of ?
?2.For the smaller threshold ?
?, the larger recallwere obtained, but the value is not high.2Precisions cannot be calculated automatically be-cause actual errors as well as the mixed errors are alsodetected.Table 2: Recall for the Artificial Data??
# of Correctly Detected Errors Recall1.0 607 4.7%0.5 1520 11.8%0.2 1555 12.1%0.1 1749 13.6%0.05 2381 18.5%11010010001000010000010000000 0.2 0.4 0.6 0.8 1Number?Positive ExamplesNegative ExamplesFigure 3: Distribution of the Value ?
on theRWCP Corpus3.2 Experiments on the RWCP Corpus(Japanese)We use the RWCP corpus, which consists of35,743 sentences (921,946 morphemes).The distribution of the weights ?i are shownin Figure 3.
The distribution of ?i shows thesame tendency as in the case of the WSJ corpus.We conducted corpus error detection for vari-ous values of ?
?, and examined by hand whetherthe detected errors are true errors or not.
Theresults are shown in Table 3, where the correctlydetected errors are distinguished into two types,one type is errors of word segmentation and theother is errors of POS tagging, since Japanesehas two kinds of ambiguities, word segmenta-tion and POS tagging.
Precision of more than80% are obtained, and the number of POS tagerrors is larger than that of segmentation errors.Examples of correctly detected errors and in-correctly detected errors from the corpus areshown in Table 4.
The underlined morphemeswere detected as errors.
In the examples ofcorrectly detected errors, both segmentation er-rors (upper) and POS tag errors (lower) are de-tected.
On the other hand, the examples of in-correctly detected errors show the limitationsof our method.
We use the two morphemes oneither side of the current morpheme as featuresfor SVMs.
In the examples, the two morphemeson either side are the same and only the POStag of the current morpheme is different, so thatSVMs cannot distinguish them and regard themas errors (inconsistency).3.3 Experiments on the KyotoUniversity Corpus (Japanese)Experiments are performed on a portion of theKyoto University corpus version 2.0, consistingof the articles of January 1, and from January 3to January 9 (total of 9,204 sentences, 229,816morphemes).
We set the value of ??
to 0.5.By repeating corpus error detection and cor-rection of the detected errors by hand, new er-rors that are not detected previously may bedetected.
To examine this, we repeated corpuserror detection and correction by hand.
Table 5shows the result.
All the detected errors in allrounds were true errors, that is, the precisionwas 100%.
Applying the corpus error detectionrepeatedly, the number of detected errors de-crease rapidly, and no errors are detected in thefourth round.
In short, even if we repeat corpuserror detection with feedback, few new errorswere detected in this experiment.4 DiscussionCompared to conventional probabilistic ap-proaches for corpus error detection, althoughprecise comparison is difficult, our approachachieved relatively high precision.
Using a prob-abilistic approach, Murata et al (2000) de-tected errors of morphemes in a corpus with aprecision of 70?80%, and Eskin (2000) detectederrors with a precision of 69%, but our approachachieved more than 80%.
The probabilisticmethods cannot handle infrequent events orcompare events with similar probabilities, sincethe probabilities cannot be calculated or com-pared with enough confidence, but our methodcan handle such infrequent events.SVMs are similar to boosting, and our ap-proach uses the weights attached by SVMsin a similar manner to what Abney et al(1999) studied.
However, we introduced a post-processing step to extract inconsistent similarTable 3: Number of Detected Errors on the RWCP Corpus??
Correct Detection (Segmentation Error/POS Tag Error) Incorrect Detection Precision1.0 110 ( 30 / 80 ) 8 93.2%0.5 165 ( 43 / 122 ) 11 93.8%0.2 171 ( 45 / 126 ) 12 93.4%0.1 188 ( 51 / 137 ) 31 85.8%0.05 300 ( 73 / 227 ) 73 80.4%Table 4: Examples of Correctly Detected Errors and Incorrectly Detected Errors in the RWCPCorpusTable 5: Number of Detected Errors on the Kyoto University Corpus for Repeated ExperimentRound 1 2 3 4Correct Detection 85 11 2 0(Segmentation Error) (21) (2) (0) (0)(POS Tag Error) (64) (9) (2) (0)Incorrect Detection 0 0 0 0Total 85 11 2 0examples, and this improved the precision of de-tection and usability.
Ma et al (2001) studiedcorpus error detection by finding conflicting ele-ments using min-max modular neural networks.Compared to their method, our method is use-ful in the point that the detected errors can besorted by the attached weights, because humancan check more likely elements first.In the experiment, our method had a highprecision but a low recall.
The value will becontrolled by tuning the features for SVMs aswell as the threshold value ?
?, and detectingmore errors in a corpus remains as future work.5 ConclusionIn this paper, we proposed a method for corpuserror detection using SVMs.
This method canextract inconsistencies in corpora.
We achievedprecision of 80?100% and showed that manyannotation errors exist in widely used corpora.The performance seems to be high enough forpractical use in corpus refinement.ReferencesSteven Abney, Robert E. Schapire, and YoramSinger.
1999.
Boosting Applied to Tag-ging and PP Attachment.
In Proceedings ofthe Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing andVery Large Corpora, pages 38?45.Erin L. Allwein, Robert E. Schapire, and YoramSinger.
2000.
Reducing Multiclass to Binary:A Unifying Approach for Margin Classifiers.In Proceedings of 17th International Confer-ence on Machine Learning, pages 9?16.Eric Brill.
1995.
Transformation-Based Error-Driven Learning and Natural Language Pro-cessing: A Case Study in Part-of-Speech Tag-ging.
Computational Linguistics, 21(4):543?565.Eleazar Eskin.
2000.
Detecting Errors withina Corpus using Anomaly Detection.
In Pro-ceedings of the 6th Applied Natural LanguageProcessing Conference and the 1st Meeting ofthe North American Chapter of the Associa-tion of Computational Linguistics, pages 148?153.Qing Ma, Bao-Liang Lu, Masaki Murata, Michi-nori Ichikawa, and Hitoshi Isahara.
2001.On-Line Error Detection of Annotated Cor-pus Using Modular Neural Networks.
In Pro-ceedings of International Conference on Arti-ficial Neural Networks (ICANN 2001), pages1185?1192.Yuji Matsumoto and Tatsuo Yamashita.
2000.Using Machine Learning Methods to ImproveQuality of Tagged Corpora and LearningModels.
In Proceedings of the Second Interna-tional Conference on Language Resource andEvaluation, pages 11?16.Masaki Murata, Masao Utiyama, KiyotakaUchimoto, Qing Ma, and Hitoshi Isahara.2000.
Corpus Error Detection and Correc-tion Using the Decision-List and Example-Based Methods.
In Information ProcessingSociety of Japan SIG Notes, Natural Lan-guage No.136, pages 49?56.
(in Japanese).Tetsuji Nakagawa, Taku Kudo, and Yuji Mat-sumoto.
2002.
Revision Learning and itsApplication to Part-of-Speech Tagging.
InProceedings of the 40th Annual Meeting ofthe Association for Computational Linguis-tics.
(to appear).Llu?
?s Padro?
and Llu?
?s Ma`rquez.
1998.
On theEvaluation and Comparison of Taggers: theEffect of Noise in Testing Corpora.
In Pro-ceedings of the joint 17th International Con-ference on Computational Linguistics and36th Annual Meeting of the Association forComputational Linguistics, pages 997?1002.Adwait Ratnaparkhi.
1996.
A Maximum En-tropy Model for Part-of-Speech Tagging.
InProceedings of the Conference on Empiri-cal Methods in Natural Language Processing,pages 133?142.Hans van Halteren, Jakub Zavrel, and Wal-ter Daelemans.
2001.
Improving Accuracyin Wordclass Tagging through Combinationof Machine Learning Systems.
ComputationalLinguistics, 27(2):199?230.Vladimir Vapnik.
1998.
Statistical LearningTheory.
Springer.
