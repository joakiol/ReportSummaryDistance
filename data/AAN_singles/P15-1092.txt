Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 950?960,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsPerceptually grounded selectional preferencesEkaterina ShutovaComputer LaboratoryUniversity of Cambridge, UKes407@cam.ac.ukNiket TandonMax Planck Institutefor Informatics, Germanyntandon@mpi-inf.mpg.deGerard de MeloIIISTsinghua University, Chinagdm@demelo.orgAbstractSelectional preferences (SPs) are widelyused in NLP as a rich source of semanticinformation.
While SPs have been tradi-tionally induced from textual data, humanlexical acquisition is known to rely on bothlinguistic and perceptual experience.
Wepresent the first SP learning method that si-multaneously draws knowledge from text,images and videos, using image and videodescriptions to obtain visual features.
Ourresults show that it outperforms linguisticand visual models in isolation, as well asthe existing SP induction approaches.1 IntroductionSelectional preferences (SPs) are the semantic con-straints that a predicate places onto its arguments.This means that certain classes of entities are morelikely to fill the predicate?s argument slot than oth-ers.
For instance, while the sentences ?The au-thors wrote a new paper.?
and ?The cat is eatingyour sausage!?
sound natural and describe plausi-ble real-life situations, the sentences ?The carrotate the keys.?
and ?The law sang a driveway.?
ap-pear implausible and difficult to interpret, as thearguments do not satisfy the verbs?
common pref-erences.
SPs provide generalisations about wordmeaning and use and find a wide range of appli-cations in natural language processing (NLP), in-cluding word sense disambiguation (Resnik, 1997;McCarthy and Carroll, 2003; Wagner et al, 2009),resolving ambiguous syntactic attachments (Hindleand Rooth, 1993), semantic role labelling (Gildeaand Jurafsky, 2002; Zapirain et al, 2010), naturallanguage inference (Zanzotto et al, 2006; Pantelet al, 2007), and figurative language processing(Fass, 1991; Mason, 2004; Shutova et al, 2013; Liet al, 2013).
Automatic acquisition of SPs fromlinguistic data has thus become an active area ofresearch.
The community has investigated a rangeof techniques to tackle data sparsity and to per-form generalisation from observed arguments totheir underlying types, including the use of Word-Net synsets as SP classes (Resnik, 1993; Li andAbe, 1998; Clark and Weir, 1999; Abney and Light,1999; Ciaramita and Johnson, 2000), word cluster-ing (Rooth et al, 1999; Bergsma et al, 2008; Sunand Korhonen, 2009), distributional similarity met-rics (Erk, 2007; Peirsman and Pad?o, 2010), latentvariable models (?O S?eaghdha, 2010; Ritter et al,2010), and neural networks (Van de Cruys, 2014).Little research, however, has been concernedwith the sources of knowledge that underlie thelearning of SPs.
There is ample evidence in cogni-tive and neurolinguistics that our concept learningand semantic representation are grounded in per-ception and action (Barsalou, 1999; Glenberg andKaschak, 2002; Barsalou, 2008; Aziz-Zadeh andDamasio, 2008).
This suggests that word mean-ing and relational knowledge are acquired not onlyfrom linguistic input but also from our experiencesin the physical world.
Multi-modal models of wordmeaning have thus enjoyed a growing interest in se-mantics (Bruni et al, 2014), outperforming purelytext-based models in tasks such as similarity es-timation (Bruni et al, 2014; Kiela et al, 2014),predicting compositionality (Roller and Schulteim Walde, 2013), and concept categorization (Sil-berer and Lapata, 2014).
However, to date theseapproaches relied on low-level image features suchas color histograms or SIFT keypoints to repre-sent the meaning of isolated words.
To the bestof our knowledge, there has not yet been a multi-modal semantic approach performing extraction of950predicate-argument relations from visual data.
Inthis paper, we propose the first SP model integrat-ing information about predicate-argument interac-tions from text, images, and videos.
We expectit to outperform purely text-based models of SPs,which suffer from two problems: topic bias andfigurative uses of words.
Such bias stems from thefact that we typically write about abstract topicsand events, resulting in high coverage of abstractsenses of words and comparatively lower coverageof the original physical senses (Shutova, 2011).
Forinstance, the verb cut is used predominantly in thedomains of economics and finance and its most fre-quent direct objects are cost and price, accordingto the British National Corpus (BNC) (Burnard,2007).
Predicate-argument distributions acquiredfrom text thus tend to be skewed in favour of ab-stract domains and figurative uses, inadequatelyreflecting our daily experiences with cutting, whichguide human acquisition of meaning.
Integratingpredicate-argument relations observed in the physi-cal world (in the form of image and video descrip-tions) with the more abstract text-based relationsis likely to yield a more realistic semantic model,with real prospects of improving the performanceof NLP applications that rely on SPs.We use the BNC as an approximation of linguis-tic knowledge and a large collection of tagged im-ages and videos from Flickr (www.flickr.com)as an approximation of perceptual knowledge.
Thehuman-annotated labels that accompany media onFlickr enable us to acquire predicate-argument co-occurrence information.
Our experiments focus onverb preferences for their subjects and direct ob-jects.
In summary, our method (1) performs wordsense disambiguation and part-of-speech (PoS) tag-ging of Flickr tag sequences to extract verb-nounco-occurrence; (2) clusters nouns to induce SPclasses using linguistic and visual features; (3)quantifies the strength of preference of a verb fora given class by interpolating linguistic and visualSP distributions.
We investigate the impact of per-ceptual information at different levels ?
from none(purely text-based model) to 100% (purely visualmodel).
We evaluate our model directly against adataset of human plausibility judgements of verb-noun pairs, as well as in the context of a semantictask: metaphor interpretation.
Our results showthat the interpolated model combining linguisticand visual relations outperforms the purely linguis-tic model in both evaluation settings.2 Related work2.1 Selectional preference inductionThe widespread interest in automatic acquisition ofSPs was triggered by the work of Resnik (1993),who treated SPs as probability distributions over allpotential arguments of a predicate, rather than a sin-gle argument class assigned to the predicate.
Theoriginal study used WordNet to define SP classesand to map the words in the corpus to those classes.Since then, the field has moved toward automaticinduction of SP classes from corpus data.
Rooth etal.
(1999) presented a probabilistic latent variablemodel of verb preferences.
In their approach, verb-argument pairs are generated from a latent variable,which represents a cluster of verb-argument inter-actions.
The latent variable distribution and theprobabilities that a latent variable generates theverb and the argument are learned from the datausing Expectation Maximization (EM).
The latentvariables enable the model to recognise previouslyunseen verb-argument pairs.
?O S?eaghdha (2010)and Ritter et al (2010) similarly model SPs within alatent variable framework, but use Latent DirichletAllocation (LDA) to learn the probability distri-butions, for single-argument and multi-argumentpreferences respectively.Pad?o et al (2007) and Erk (2007) used simi-larity metrics to approximate selectional prefer-ence classes.
Their underlying hypothesis is thata predicate-argument combination (p, a) is felici-tous if the predicate p is frequently observed in thedata with the arguments a?similar to a.
The sys-tems compute similarities between distributionalrepresentations of arguments in a vector space.Bergsma et al (2008) trained an SVM classifierto discriminate between felicitous and infelicitousverb-argument pairs.
Their training data consistedof observed verb-argument pairs (positive exam-ples) with unobserved, randomly-generated ones(negative examples).
They classified nominal ar-guments of verbs, using their verb co-occurrenceprobabilities and information about their semanticclasses as features.
Bergsma and Goebel (2011) ex-tended this method by incorporating image-drivennoun features.
They extract color and SIFT key-point features from images found for a particularnoun via Google image searches and add them tothe feature vectors to classify nouns as felicitousor infelicitous arguments of a given verb.
Thismethod is the closest in spirit to ours and the onlyone so far to investigate the relevance of visual fea-951tures to lexical preference learning.
However, ourwork casts the problem in a different framework:rather than relying on low-level visual properties ofnouns in isolation, we explicitly model interactionsof predicates and arguments within an image or avideo frame.Van de Cruys (2014) recently presented a deeplearning approach to SP acquisition.
He traineda neural network to discriminate between felic-itous and infelicitous arguments using the dataconstructed of positive (observed) and negative(randomly-generated) examples for training.
Thenetwork weights were optimized by requiring themodel to assign a higher score to an observed pairthan to the unobserved one by a given margin.2.2 Multi-modal methods in semanticsPrevious work has used multimodal data to de-termine distributional similarity or to learn multi-modal embeddings that project multiple modalitiesinto the same vector space.
Some studies rely onextensions of LDA to obtain correlations betweenwords and visual features (Feng and Lapata, 2010;Roller and Schulte im Walde, 2013).
Bruni et al(2012) integrated visual features into distributionalsimilarity models using simple vector concatena-tion.
Instead of generic visual features, Silberer etal.
(2013) relied on supervised learning to train 412higher-level visual attribute classifiers.Applications of multimodal embeddings includezero-shot object detection, i.e.
recognizing objectsin images without training data for the object class(Socher et al, 2013; Frome et al, 2013; Lazaridouet al, 2014), and automatic generation of imagecaptions (Kulkarni et al, 2013), video descriptions(Rohrbach et al, 2013), or tags (Srivastava et al,2014).
Other applications of multimodal data in-clude language modeling (Kiros et al, 2014) andknowledge mining from images (Chen et al, 2013;Divvala et al, 2014).
Young et al (2014) apply sim-plification rules to image captions, showing that theresulting hierarchy of mappings between naturallanguage expressions and images can be used forentailment tasks.3 Experimental dataTextual data.
We extract linguistic features forour model from the BNC.
In particular, we parsethe corpus using the RASP parser (Briscoe et al,2006) and extract subject?verb and verb?object re-lations from its dependency output.
These relationsare then used as features for clustering to obtain SPclasses, as well as to quantify the strength of asso-ciation between a particular verb and a particularargument class.Visual data.
For the visual features of our model,we mine the Yahoo!
Webscope Flickr-100M dataset(Shamma, 2014).
Flickr-100M contains 99.3 mil-lion images and 0.7 million videos with languagetags annotated by users, enabling us to generaliseSPs at a large scale.
The tags reflect how humansdescribe objects and actions from a visual perspec-tive.
We first stem the tags and remove words thatare absent in WordNet (typically named entitiesand misspellings), then identify their PoS basedon their visual context and extract verb?noun co-occurrences.4 Identifying visual verb-nounco-occurrenceIn the Flickr-100M dataset, tags are assigned to im-ages and videos in the form of sets of words, ratherthan grammatically coherent sentences.
However,the roles that individual words play are still dis-cernible from their visual context, as manifested bythe other words in a given set.
In order to identifyverbs and nouns co-occurring in the same images,we propose a list sense disambiguation method thatfirst maps each word to a set of possible WordNetsenses (accompanied by PoS information) and thenperforms a joint optimization on the space of candi-date word senses, such that their overall similarityis maximized.
This amounts to assigning thosesenses and PoS tags to the words in the set that bestfit together.For a given word i and one of its candidate Word-Net senses j, we consider an assignment variablexijand compute a sense frequency-based prior forit as Pij=11+R, where R is the WordNet rankof the sense.
We then compute a similarity scoreSij,i?j?between all pairs of sense choices for twowords i,i?and their respective candidate senses j,j?.For these, we rely on WordNet?s taxonomic path-based similarities (Pedersen et al, 2004) in the caseof noun-noun sense pairs, the Adapted Lesk sim-ilarity measure for adjective-adjective pairs, andfinally, WordNet verb-groups and VerbNet classmembership (Kipper-Schuler, 2005) for verb-verbpairs.
Note that even parts of speech that are dis-regarded later on can still be helpful at this stage,as we aim at a joint optimization over all words.After the similarities have been obtained for all rel-952evant sense pairs, we maximize the coherence ofthe senses of the words in the set as an Integer Lin-ear Program, using the Gurobi Optimizer (GurobiOptimization, 2014) and solvingmaximize?iPijxij+?ij?i?j?Sij,i?j?Bij,i?j?subject to?jxij?
1 ?i, xij?
{0, 1} ?i, j,Bij,i?j??
xij, Bij,i?j??
xi?j?,Bij,i?j??
{0, 1} ?i, j, i?j?.The binary variables Bij,i?j?are 1 iff xij= 1 andxi?j?= 1, indicating that both senses were simulta-neously chosen.
The optimizer disambiguates theinput words by selecting sense tuples x1j, x2j, .
.
.
,from which we can directly obtain the correspond-ing PoS information.
Verb-noun co-occurrenceinformation is then extracted from the PoS-taggedsets.5 Selectional preference model5.1 Acquisition of argument classesTo address the issue of data sparsity, we generaliseselectional preferences over argument classes, asopposed to individual arguments.
We obtain SPclasses by means of spectral clustering of nounswith lexico-syntactic features, which has beenshown effective in previous lexical classificationtasks (Brew and Schulte im Walde, 2002; Sun andKorhonen, 2009).Spectral clustering partitions the data, relying ona similarity matrix that records similarities betweenall pairs of data points.
We use Jensen-Shannondivergence to measure the similarity between fea-ture vectors for two nouns, wiand wj, defined asfollows:dJS(wi, wj) =12dKL(wi||m) +12dKL(wj||m),(1)where dKLis the Kullback-Leibler divergence, andm is the average of wiand wj.
We construct thesimilarity matrix S computing similarities SijasSij= exp(?dJS(wi, wj)).
The matrix S then en-codes a similarity graph G (over our nouns), whereSijare the adjacency weights.
The clustering prob-lem can then be defined as identifying the optimalpartition, or cut, of the graph into clusters, suchthat the intra-cluster weights are high and the inter-cluster weights are low.
We use the multiway nor-malized cut (MNCut) algorithm of Meila and Shi(2001) for this purpose.
The algorithm transformsS into a stochastic matrix P containing transitionprobabilities between the vertices in the graph asP = D?1S, (2)where the degree matrix D is a diagonal matrixwith Dii=?Nj=1Sij.
It then computes the Kleading eigenvectors of P , where K is the desirednumber of clusters.
The graph is partitioned byfinding approximately equal elements in the eigen-vectors using a simpler clustering algorithm, suchas k-means.
Meila and Shi (2001) have shown thatthe partition I derived in this way minimizes theMNCut criterion:MNCut(I) =K?k=1(1?
P (Ik?
Ik|Ik)), (3)which is the sum of transition probabilities acrossdifferent clusters.
Since k-means starts from a ran-dom cluster assignment, we run the algorithm mul-tiple times and select the partition that minimizesthe cluster distortion, i.e.
distances to cluster cen-troid.We cluster nouns using linguistic and visual fea-tures in two independent experiments.Clustering with linguistic features: We first clus-ter the 2,000 most frequent nouns in the BNC, us-ing their grammatical relations as features.
Thefeatures consist of verb lemmas appearing in thesubject, direct object and indirect object relationswith the given nouns in the RASP-parsed BNC,indexed by relation type.
The feature vectors arefirst constructed from the corpus counts, and sub-sequently normalized by the sum of the featurevalues.Clustering with visual features: We also clus-ter the 2,000 most frequent nouns in the Flickrdata.
Since our goal is to create argument classesfor verb preferences, we extract co-occurrence fea-tures that map to verb-noun relations from PoS-disambiguated image tags.
We use the verb lem-mas co-occurring with the noun in the same imagesand videos as features for clustering.
The featurevalues are again normalised by their sum.SP classes: Example clusters produced using lin-guistic and visual features are shown in Figures 1and 2.
Our cluster analysis reveals that the image-derived clusters tend to capture scene-like relations(e.g.
beach and ocean; guitar and concert), asopposed to types of entities, yielded by the lin-guistic features and better suited to generalise over953desire hostility anxiety passion doubt fear curiosity enthusi-asm impulse instinct emotion feeling suspicionofficial officer inspector journalist detective constable policepoliceman reporterbook statement account draft guide advertisement documentreport article letterFigure 1: Clusters obtained using linguistic fea-turespilot aircraft plane airline landing flight wing arrival departureairportconcert festival music guitar alternative band instrument audi-ence event performance rock benjamincost benefit crisis debt credit customer consumerFigure 2: Clusters obtained using visual featurespredicate-argument structure.
In addition, the im-age features tend to be sparse for abstract concepts,reducing both the quality and the coverage of ab-stract clusters.
We thus use the noun clusters de-rived with linguistic features as an approximationof SP classes.5.2 Quantifying selectional preferencesOnce the SP classes have been obtained, we needto quantify the strength of association of a givenverb with each of the classes.
We adopt an informa-tion theoretic measure proposed by Resnik (1993)for this purpose.
Resnik first measures selectionalpreference strength (SPS) of a verb in terms ofKullback-Leibler divergence between the distribu-tion of noun classes occurring as arguments of thisverb, p(c|v), and the prior distribution of the nounclasses, p(c).SPSR(v) =?cp(c|v) logp(c|v)p(c),(4)where R is the grammatical relation for which SPsare computed.
SPS measures how strongly thepredicate constrains its arguments.
Selectional as-sociation of the verb with a particular argumentclass is then defined as a relative contribution ofthat argument class to the overall SPS of the verb.AssR(v, c) =1SPSR(v)p(c|v) logp(c|v)p(c)(5)We use this measure to quantify verb SPs basedon linguistic and visual co-occurrence information.We first extract verb-subject and verb-direct objectrelations from the RASP-parsed BNC, map the ar-gument heads to SP classes and quantify selectionalassociation of a given verb with each SP class, thusacquiring its base preferences.
Since visual verb-noun co-occurrences do not contain informationabout grammatical relations, we rely on linguisticdata to provide a set of base arguments of the verbfor a given grammatical relation.
We then interpo-late the verb-argument probabilities from linguisticand visual models for the base arguments of theverb, thus preserving information about grammati-cal relations.5.3 Linguistic and visual model interpolationWe investigate two model interpolation techniques:simple linear interpolation and predicate-driven lin-ear interpolation.Linear interpolation combines information fromcomponent models by computing a weighted aver-age of their probabilities.
The interpolated probabil-ity of an event e is derived as pLI(e) =?i?ipi(e),where pi(e) is the probability of e in the model iand ?iis the interpolation weight defined such that?i?i= 1; and ?i?
[0, 1].
In our experiments, weinterpolate the probabilities p(c) and p(c|v) in thelinguistic (LM) and visual (VM) models, as follows:pLI(c) = ?LMpLM(c) + ?VMpVM(c) (6)pLI(c|v) = ?LMpLM(c|v) + ?VMpVM(c|v) (7)We experiment with a number of parameter settingsfor ?LMand ?VM.Predicate-driven linear interpolation derivespredicate-specific interpolation weights directlyfrom the data, as opposed to pre-setting them uni-versally for all verbs.
For each predicate v, we com-pute the interpolation weights based on its promi-nence in the respective corpus, as follows:?i(v) =reli(v)?krelk(v), (8)where rel is the relevance function of model i forverb v, computed as its relative frequency in therespective corpus: reli(v) =fi(v)?Vfi(v).
The interpo-lation weights for LM and VM are then computedas?LM(v) =relLM(v)relLM(v) + relVM(v)(9)?VM(v) =relVM(v)relLM(v) + relVM(v).
(10)The motivation for this approach comes from thefact that not all verbs are represented equally wellin linguistic and visual data.
For instance, whileconcrete verbs, such as run, push or throw, aremore likely to be prominent in visual data, abstractverbs, such as understand or speculate, are best954represented in text.
Relative linguistic and visualfrequencies of a verb provide a way to estimate therelevance of linguistic and visual features to its SPlearning.6 Direct evaluation and data analysisWe evaluate the predicate-argument scores as-signed by our models against a dataset of hu-man plausibility judgements of verb-direct objectpairs collected by Keller and Lapata (2003).
Theirdataset is balanced with respect to the frequencyof verb-argument relations, as well as their plausi-bility and implausibility, thus creating a realisticSP evaluation task.
Keller and Lapata selected 30predicates and matched each of them to three ar-guments from different co-occurrence frequencybands according to their BNC counts, e.g.
divertattention (high frequency), divert water (medium)and divert fruit (low).
This constituted their datasetof Seen verb-noun pairs, 90 in total.
Each of thepredicates was then also paired with three randomlyselected arguments with which it did not occur inthe BNC, creating the Unseen dataset.
The pairs inboth datasets were then rated for their plausibilityby 27 human subjects, and their judgements wereaggregated into a gold standard.
We compare theverb-argument scores generated by our linguistic(LSP), visual (VSP) and interpolated (ISP) SP mod-els against these two datasets in terms of Pearsoncorrelation coefficient, r, and Spearman rank cor-relation coefficient, ?.
The selectional associationscore of the cluster to which a given noun belongsis taken to represent the preference score of theverb for this noun.
If a noun is not present in ourargument clusters, we match it to its nearest clus-ter, as determined by its distributional similarityto the cluster centroid in terms of Jensen-Shannondivergence.We first compare LSP, VSP and ISP with staticand predicate-driven interpolation weights.
Theresults, presented in Table 1, demonstrate thatthe interpolated model outperforms both LSP andVSP used on their own.
The best performance isattained with the static interpolation weights of?LM= 0.8 (r = 0.540; ?
= 0.728) and ?LM= 0.9(r = 0.548; ?
= 0.699).
This suggests that whilelinguistic input plays a crucial role in SP induction(by providing both semantic and syntactic informa-tion), visual features further enhance the qualityof SPs, as we expected.
Figure 3 shows LSP- andVSP-acquired direct object preferences of the verbSeen Unseenr ?
r ?VSP 0.180 0.126 0.118 0.132ISP: ?LM= 0.1 0.279 0.532 0.220 0.371ISP: ?LM= 0.2 0.349 0.556 0.278 0.411ISP: ?LM= 0.3 0.385 0.558 0.305 0.423ISP: ?LM= 0.4 0.410 0.571 0.320 0.428ISP: ?LM= 0.5 0.448 0.579 0.329 0.430ISP: ?LM= 0.6 0.461 0.591 0.330 0.431ISP: ?LM= 0.7 0.523 0.713 0.335 0.431ISP: ?LM= 0.8 0.540 0.728 0.339 0.430ISP: ?LM= 0.9 0.548 0.699 0.342 0.429ISP: Predicate-driven 0.476 0.597 0.391 0.551LSP 0.512 0.688 0.412 0.559Table 1: Model comparison on the plausibility dataof Keller and Lapata (2003)LSP: (1) 0.309 expenditure cost risk expense emission budgetspending; (2) 0.201 dividend price rate premium rent rat-ing salary wages; (3) 0.088 employment investment growthsupplies sale import export production [..]ISP predicate-driven ?LM= 0.65(1) 0.346 expenditure cost risk expense emission budgetspending; (2) 0.211 dividend price rate premium rent rat-ing salary wages; (3) 0.126 tail collar strand skirt trousershair curtain sleeveVSP: (1) 0.224 tail collar strand skirt trousers hair curtainsleeve; (2) 0.098 expenditure cost risk expense emission bud-get spending; (3) 0.090 management delivery maintenancetransport service housing [..]Figure 3: Top three direct object classes for cutand their association scores, assigned by differentmodelscut, as well as the effects of merging the featuresin the interpolated model ?
the verbs?
experientialarguments (e.g.
hair or fabric) are emphasized bythe visual features.However, the model based on visual featuresalone performs poorly on the dataset of Keller andLapata (2003).
This is partly explained by the factthat a number of verbs in this dataset are abstractverbs, whose visual representations in the Flickrdata are sparse.
In addition, VSP (as other visualmodels used in isolation from text) is not syntax-aware and is unable to discriminate between differ-ent types of semantic relations.
VSP thus acquiressets of verb-argument relations that are closer innature to scene descriptions and semantic framesthan to lexico-syntactic paradigms.
Figure 4 showsthe differences between linguistic and visual ar-guments of the verb kill ranked by LSP and VSP.While LSP produces mainly semantic objects of kill,VSP output contains other types of arguments, suchas weapon (instrument) and death (consequence).Taking the argument classes produced by thelinguistic model as a basis and then re-ranking955LSP: (1) 0.523 girl other woman child person people; (2)0.164 fleet soldier knight force rebel guard troops crew armypilot; (3) 0.133 sister daughter parent relative lover cousinfriend wife mother husband brother father; (4) 0.048 beingspecies sheep animal creature horse baby human fish malelamb bird rabbit [..]; (5) 0.045 victim bull teenager prisonerhero gang enemy rider offender youth killer thief [..]VSP: (1) 0.180 defeat fall death tragedy loss collapse decline[..]; (2) 0.141 girl other woman child person people; (3) 0.128abuse suicide killing offence murder breach crime; (4) 0.113handle weapon horn knife blade stick sword [..]; (5) 0.095victim bull teenager prisoner hero gang enemy rider offenderyouth killer thief [..]Figure 4: Top five arguments of kill and their asso-ciation scores, assigned by LSP and VSP(1) 0.442 drink coffee champagne pint wine beer; (2) 0.182mixture dose substance drug milk cream alcohol chemical[..]; (3) 0.091 girl other woman child person people; (4) 0.053sister daughter parent relative lover cousin friend wife motherhusband brother father; (5) 0.050 drop tear sweat paint bloodwater juiceFigure 5: Error analysis: Mixed subjects and directobjects of drink, assigned by the predicate-drivenISPthem to incorporate visual statistics helps to avoidthe above problem for the interpolated models,whose output corresponds to grammatical relations.However, static interpolation weights (emphasiz-ing linguistic features over the visual ones for allverbs equally) outperformed the predicate-driveninterpolation technique, attaining correlations ofr = 0.548 and r = 0.476 respectively.
This ismainly due to the fact that some verbs are over-represented in the visual data (e.g.
the predicate-driven interpolation weight for the verb drink is?LM= 0.08).
As a result, candidate argumentclasses (selected based on syntactically-parsed lin-guistic input) are ranked predominantly based onvisual statistics.
This makes it possible to empha-size incorrectly parsed arguments (such as subjectrelations in the direct object SP distribution andvice versa).
The predicate-driven ISP output fordirect object SPs of drink, for instance, containsa mixture of subject and direct object classes, asshown in Figure 5.
Using a static model with ahigh ?LMweight helps to avoid such errors and,therefore, leads to a better performance.In order to investigate the composition of thevisual and linguistic datasets, we assess the averagelevel of concreteness of the verbs and nouns presentin the datasets.
We use the concreteness ratingsfrom the MRC Psycholinguistic Database (Wilson,1988) for this purpose.
In this database, nouns andFigure 6: WordNet top level class distributions forverbs in the visual and textual corporaSeen Unseenr ?
r ?Rooth et al (1999)* 0.455 0.487 0.479 0.520Pad?o et al (2007)* 0.484 0.490 0.398 0.430O?Seaghdha (2010) 0.520 0.548 0.564 0.605VSP 0.180 0.126 0.118 0.132ISP (best) 0.548 0.699 0.342 0.429LSP 0.512 0.688 0.412 0.559Table 2: Comparison to other SP induction meth-ods.
* Results reported in O?Seaghdha (2010).verbs are rated for concreteness on a scale from100 (highly abstract) to 700 (highly concrete).
Wemap the verbs and nouns in our textual and visualcorpora to their MRC concreteness scores.
We thencalculate a dataset-wide concreteness score as anaverage of the concreteness scores of individualverbs and nouns weighted by their frequency inthe respective corpus.
The average concretenessscores in the visual dataset were 506.4 (nouns) and498.1 (verbs).
As expected, they are higher than therespective scores in the textual data: 433.1 (nouns)and 363.4 (verbs).
In order to compare the typesof actions that are common in each of the datasets,we map the verbs to their corresponding top levelclasses in WordNet.
Figure 6 shows the comparisonof prominent verb classes in visual and textual data.One can see from the Figure that the visual datasetis well suited for representing motion, perceptionand contact, while abstract verbs related to e.g.communication, cognition, possession or changeare more common in textual data.We also compare the performance of our modelsto existing SP induction methods: the EM-basedclustering method of Rooth et al (1999), the vec-tor space similarity-based method of Pad?o et al(2007) and the LDA topic modelling approach of?O S?eaghdha (2010)1.
The best ISP configuration1Since Rooth et al?s (1999) and Pad?o et al?s (2007) modelswere not originally evaluated on the same dataset, we use the956(?LM= 0.9) outperforms all of these methods, aswell as our own LSP, on the Seen dataset, con-firming the positive contribution of visual features.However, it achieves less success on the Unseendata, where the methods of?O S?eaghdha (2010)and Rooth et al (1999) are leading.
This resultspeaks in favour of latent variable models for acqui-sition of SP estimates for rarely attested predicate-argument pairs.
In turn, this suggests that integrat-ing our ISP model (that currently outperforms oth-ers on more common pairs) with such techniquesis likely to improve SP prediction across frequencybands.7 Task-based evaluationIn order to investigate the applicability of perceptu-ally grounded SPs in wider NLP, we evaluate themin the context of an external semantic task ?
that ofmetaphor interpretation.
Since metaphor is basedon transferring imagery and knowledge across do-mains ?
typically from more familiar domains ofphysical experiences to the sphere of vague andelusive abstract thought ?
metaphor interpretationprovides an ideal framework for testing perceptu-ally grounded SPs.
Our experiments rely on themetaphor interpretation method of Shutova (2010),in which text-derived SPs are a central componentof the system.
We replace the SP component withour LSP and ISP (?LM= 0.8) models and com-pare their performance in the context of metaphorinterpretation.Shutova (2010) defined metaphor interpretationas a paraphrasing task, where literal paraphrasesfor metaphorical expressions are derived from cor-pus data using a set of statistical measures.
Forinstance, their system interprets the metaphor ?acarelessly leaked report?
as ?a carelessly disclosedreport?.
Focusing on metaphorical verbs in subjectand direct object constructions, Shutova first ap-plies a maximum likelihood model to extract andrank candidate paraphrases for the verb given thecontext, as follows:P (i, w1, ..., wN) =?Nn=1f(wn, i)(f(i))N?1?
?kf(ik), (11)where f(i) is the frequency of the paraphrase onits own and f(wn, i) the co-occurrence frequencyof the paraphrase with the context word wn.
Thisresults for their re-implementation reported by O?Seaghdha(2010), who conducted a comprehensive evaluation of SPmodels on the plausibility data of Keller and Lapata (2003).model favours paraphrases that match the givencontext best.
These candidates are then filteredbased on the presence of shared features with themetaphorical verb, as defined by their location anddistance in the WordNet hierarchy.
All the can-didates that have a common hypernym with themetaphorical verb within three levels of the Word-Net hierarchy are selected.
This results in a set ofparaphrases retaining the meaning of the metaphor-ical verb.
However, some of them are still figura-tively used.
Shutova further applies an SP modelto discriminate between figurative and literal para-phrases, treating a strong selectional preference fitas a likely indicator of literalness.
The candidatesare re-ranked by the SP model, emphasizing theverbs whose preferences the noun in the contextmatches best.
We use LSP and ISP scores to per-form this re-ranking step.We evaluate the performance of our models onthis task using the metaphor paraphrasing gold stan-dard of Shutova (2010).
The dataset consists of 52verb metaphors and their human-produced literalparaphrases.
Following Shutova, we evaluate theperformance in terms of mean average precision(MAP), which measures the ranking quality of GSparaphrases across the dataset.
MAP is defined asfollows:MAP =1MM?j=11NjNj?i=1Pji,where M is the number of metaphorical expres-sions, Njis the number of correct paraphrases forthe metaphorical expression j, Pjiis the precisionat each correct paraphrase (the number of correctparaphrases among the top i ranks).
As comparedto the gold standard, ISP attains a MAP score of0.65, outperforming both the LSP (MAP = 0.62)and the original system of Shutova (2010) (MAP= 0.62), demonstrating the positive contribution ofvisual features.8 ConclusionWe have presented the first SP induction methodthat simultaneously draws knowledge from text,images and videos.
Our experiments show that itoutperforms linguistic and visual models in iso-lation, as well as the previous approaches to SPlearning.
We believe that this model has a wideapplicability in NLP, where many systems alreadyrely on automatically induced SPs.
It can alsobenefit image caption generation systems, which957typically focus on objects rather than actions, byproviding information about predicate-argumentstructure.In the future, it would be interesting to derivethe information about predicate-argument relationsfrom low-level visual features directly.
However, toour knowledge, reliably mapping images to actions(i.e.
verbs) at a large-scale is still a challengingtask.
Human-annotated image and video descrip-tions allow us to investigate what types of verb?noun relations are in principle present in the visualdata and the ways in which they are different fromthe ones found in text.
Our results show that visualdata is better suited for capturing physical proper-ties of concepts as well as containing relations notexplicitly described in text.The presented interpolation techniques are alsoapplicable outside multi-modal semantics.
For in-stance, they can be generalised to acquire SPs fromunbalanced corpora of different sizes (e.g.
for lan-guages lacking balanced corpora) or to performdomain adaptation of SPs.
In the future, we wouldlike to apply SP interpolation to multilingual SPlearning, i.e.
integrating data from multiple lan-guages for more accurate SP induction and project-ing universal semantic relations to low-resourcelanguages.
It is also interesting to investigate SPlearning at the level of semantic predicates (e.g.automatically inducing FrameNet-style frames),where combining the visual and linguistic knowl-edge is likely to outperform text-based models ontheir own.AcknowledgementsEkaterina Shutova?s research is funded by theUniversity of Cambridge and the LeverhulmeTrust Early Career Fellowship.
Gerard de Melo?swork is funded by China 973 Program Grants2011CBA00300, 2011CBA00301, and NSFCGrants 61033001, 61361136003, 61450110088.We are grateful to the ACL reviewers for their in-sightful feedback.ReferencesSteven Abney and Marc Light.
1999.
Hiding a Seman-tic Hierarchy in a Markov Model.
In Proceedings ofthe Workshop on Unsupervised Learning in NaturalLanguage Processing, ACL, pages 1?8.Lisa Aziz-Zadeh and Antonio Damasio.
2008.
Embod-ied semantics for actions: Findings from functionalbrain imaging.
Journal of Physiology ?
Paris, 102(1-3).Lawrence W. Barsalou.
1999.
Perceptual symbol sys-tems.
Behavioral and Brain Sciences, 22(4):577?609.Lawrence W. Barsalou.
2008.
Grounded cognition.Annual Review of Psychology, 59(1):617?645.Shane Bergsma and Randy Goebel.
2011.
Using vi-sual information to predict lexical preference.
InProceedings of RANLP.Shane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative learning of selectional preferencefrom unlabeled text.
In Proceedings of EMNLP2008, EMNLP ?08, pages 59?68, Honolulu, Hawaii.Chris Brew and Sabine Schulte im Walde.
2002.
Spec-tral clustering for German verbs.
In Proceedings ofEMNLP, pages 117?124.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the COLING/ACL on Interactive presen-tation sessions, pages 77?80.Elia Bruni, Gemma Boleda, Marco Baroni, andNam Khanh Tran.
2012.
Distributional semanticsin Technicolor.
In Proceedings of ACL 2012, pages136?145, Jeju Island, Korea, July.
ACL.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Ar-tificial Intelligence Research, 49:1?47.Lou Burnard.
2007.
Reference Guide for the BritishNational Corpus (XML Edition).Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta.2013.
NEIL: Extracting Visual Knowledge fromWeb Data.
In Proceedings of ICCV 2013.Massimiliano Ciaramita and Mark Johnson.
2000.
Ex-plaining away ambiguity: Learning verb selectionalpreference with Bayesian networks.
In Proceedingsof COLING 2000, pages 187?193.Stephen Clark and David Weir.
1999.
An iterativeapproach to estimating frequencies over a seman-tic hierarchy.
In Proceedings of EMNLP/VLC 1999,pages 258?265.Santosh Divvala, Ali Farhadi, and Carlos Guestrin.2014.
Learning everything about anything: Webly-supervised visual concept learning.
In Proceedingsof CVPR 2014.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In Proceedings of ACL2007.Dan Fass.
1991. met*: A method for discriminatingmetonymy and metaphor by computer.
Computa-tional Linguistics, 17(1):49?90.958Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Proceedingsof NAACL 2010, pages 91?99.
ACL.Andrea Frome, Greg Corrado, Jon Shlens, SamyBengio, Jeffrey Dean, Marc?Aurelio Ranzato, andTomas Mikolov.
2013.
DeViSE: A deep visual-semantic embedding model.
In Proceedings of NIPS2013.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28.Arthur M. Glenberg and Michael P. Kaschak.
2002.Grounding language in action.
Psychonomic Bul-letin and Review, pages 558?565.Gurobi Optimization.
2014.
Gurobi optimizer refer-ence manual, version 5.6.
Houston, TX, USA.Donald Hindle and Mats Rooth.
1993.
Structural ambi-guity and lexical relations.
Computational Linguis-tics, 19:103?120.Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams.
Computa-tional Linguistics, 29(3):459?484.Douwe Kiela, Felix Hill, Anna Korhonen, and StephenClark.
2014.
Improving multi-modal representa-tions using image dispersion: Why less is some-times more.
In Proceedings of ACL 2014, Baltimore,Maryland.Karin Kipper-Schuler.
2005.
VerbNet: A broad-coverage, comprehensive verb lexicon.
Ph.D. thesis,University of Pennsylvania, PA.Ryan Kiros, Ruslan Salakhutdinov, and Richard S.Zemel.
2014.
Multimodal neural language models.In Proceedings of ICML 2014, pages 595?603.Girish Kulkarni, Visruth Premraj, Vicente Ordonez,Sagnik Dhar, Siming Li, Yejin Choi, Alexander C.Berg, and Tamara L. Berg.
2013.
Babytalk: Un-derstanding and generating simple image descrip-tions.
IEEE Trans.
Pattern Anal.
Mach.
Intell.,35(12):2891?2903.Angeliki Lazaridou, Elia Bruni, and Marco Baroni.2014.
Is this a wampimuk?
cross-modal map-ping between distributional semantics and the visualworld.
In Proceedings of ACL 2014, pages 1403?1414.
ACL.Hang Li and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the mdl principle.Computational Linguistics, 24(2):217?244.Hongsong Li, Kenny Q. Zhu, and Haixun Wang.
2013.Data-driven metaphor recognition and explanation.Transactions of the Association for ComputationalLinguistics, 1:379?390.Zachary Mason.
2004.
Cormet: a computational,corpus-based conventional metaphor extraction sys-tem.
Computational Linguistics, 30(1):23?44.Diana McCarthy and John Carroll.
2003.
Disam-biguating nouns, verbs, and adjectives using auto-matically acquired selectional preferences.
Compu-tational Linguistics, 29(4):639?654.Marina Meila and Jianbo Shi.
2001.
A random walksview of spectral segmentation.
In Proceedings of AI-STATS.Diarmuid?O S?eaghdha.
2010.
Latent variable mod-els of selectional preference.
In Proceedings of ACL2010.Sebastian Pad?o, Ulrike Pad?o, and Katrin Erk.
2007.Flexible, corpus-based modelling of human plau-sibility judgements.
In Proceedings of EMNLP-CoNLL.P.
Pantel, R. Bhagat, T. Chklovski, and E. Hovy.
2007.Isp: Learning inferential selectional preferences.
InProceedings of NAACL 2007.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet:: Similarity: measuring therelatedness of concepts.
In Demonstration Papersat HLT-NAACL 2004, pages 38?41.Y.
Peirsman and S. Pad?o.
2010.
Cross-lingual induc-tion of selectional preferences with bilingual vectorspaces.
In Proceedings of NAACL 2010, pages 921?929.Philip Resnik.
1993.
Selection and information: Aclass-based approach to lexical relationships.
Tech-nical report, University of Pennsylvania.Philip Resnik.
1997.
Selectional preference and sensedisambiguation.
In ACL SIGLEX Workshop on Tag-ging Text with Lexical Semantics, Washington, D.C.Alan Ritter, Mausam Etzioni, and Oren Etzioni.
2010.A latent dirichlet alocation method for selectionalpreferences.
In Proceedings ACL 2010, pages 424?434.Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater,Manfred Pinkal, and Bernt Schiele.
2013.
Translat-ing video content to natural language descriptions.In Proceedings of ICCV 2013.Stephen Roller and Sabine Schulte im Walde.
2013.A Multimodal LDA Model integrating Textual, Cog-nitive and Visual Modalities.
In Proceedings ofEMNLP 2013, pages 1146?1157, Seattle, WA.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a semanticallyannotated lexicon via EM-based clustering.
In Pro-ceedings of ACL 1999, pages 104?111.David Shamma.
2014.
One hundred million Cre-ative Commons Flickr images for research.
http://labs.yahoo.com/news/yfcc100m/.959Ekaterina Shutova, Simone Teufel, and Anna Korho-nen.
2013.
Statistical Metaphor Processing.
Com-putational Linguistics, 39(2).Ekaterina Shutova.
2010.
Automatic metaphor inter-pretation as a paraphrasing task.
In Proceedings ofNAACL 2010, pages 1029?1037, Los Angeles, USA.Ekaterina Shutova.
2011.
Computational Approachesto Figurative Language.
Ph.D. thesis, University ofCambridge, UK.Carina Silberer and Mirella Lapata.
2014.
Learn-ing grounded meaning representations with autoen-coders.
In Proceedings of ACL 2014, Baltimore,Maryland.Carina Silberer, Vittorio Ferrari, and Mirella Lapata.2013.
Models of semantic representation with vi-sual attributes.
In Proceedings of ACL 2013, pages572?582.Richard Socher, Milind Ganjoo, Christopher D. Man-ning, and Andrew Ng.
2013.
Zero-shot learningthrough cross-modal transfer.
In Proceedings ofNIPS 2013, pages 935?943.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
Journal of Machine Learning Re-search, 15:1929?1958.Lin Sun and Anna Korhonen.
2009.
Improving verbclustering with automatically acquired selectionalpreferences.
In Proceedings of EMNLP 2009.Tim Van de Cruys.
2014.
A neural network approachto selectional preference acquisition.
In Proceed-ings of EMNLP 2014.Wiebke Wagner, Helmut Schmid, and Sabine SchulteIm Walde.
2009.
Verb sense disambiguation us-ing a predicate-argument clustering model.
In Pro-ceedings of the CogSci Workshop on Semantic SpaceModels (DISCO).M.D.
Wilson.
1988.
The MRC PsycholinguisticDatabase: Machine Readable Dictionary, Version2.
Behavioural Research Methods, Instruments andComputers, 20:6?11.Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-enmaier.
2014.
From image descriptions to vi-sual denotations.
Transactions of the Associationof Computational Linguistics ?
Volume 2, Issue 1,pages 67?78.Fabio Massimo Zanzotto, Marco Pennacchiotti, andMaria Teresa Pazienza.
2006.
Discovering asym-metric entailment relations between verbs usingselectional preferences.
In Proceedings of COL-ING/ACL, pages 849?856.Be?nat Zapirain, Eneko Agirre, Llu?
?s M`arquez, and Mi-hai Surdeanu.
2010.
Improving semantic role clas-sification with selectional preferences.
In Proceed-ings of NAACL HLT 2010, pages 373?376.960
