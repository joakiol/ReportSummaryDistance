First Joint Conference on Lexical and Computational Semantics (*SEM), pages 199?208,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsEnsemble-based Semantic Lexicon Induction for Semantic TaggingAshequl QadirUniversity of UtahSchool of ComputingSalt Lake City, UT 84112, USAasheq@cs.utah.eduEllen RiloffUniversity of UtahSchool of ComputingSalt Lake City, UT 84112, USAriloff@cs.utah.eduAbstractWe present an ensemble-based frameworkfor semantic lexicon induction that incorpo-rates three diverse approaches for semanticclass identification.
Our architecture bringstogether previous bootstrapping methods forpattern-based semantic lexicon induction andcontextual semantic tagging, and incorpo-rates a novel approach for inducing semanticclasses from coreference chains.
The threemethods are embedded in a bootstrapping ar-chitecture where they produce independenthypotheses, consensus words are added to thelexicon, and the process repeats.
Our resultsshow that the ensemble outperforms individ-ual methods in terms of both lexicon qualityand instance-based semantic tagging.1 IntroductionOne of the most fundamental aspects of meaning isthe association between words and semantic cate-gories, which allows us to understand that a ?cow?is an animal and a ?house?
is a structure.
We willuse the term semantic lexicon to refer to a dictionarythat associates words with semantic classes.
Se-mantic dictionaries are useful for many NLP tasks,as evidenced by the widespread use of WordNet(Miller, 1990).
However, off-the-shelf resources arenot always sufficient for specialized domains, suchas medicine, chemistry, or microelectronics.
Fur-thermore, in virtually every domain, texts containlexical variations that are often missing from dic-tionaries, such as acronyms, abbreviations, spellingvariants, informal shorthand terms (e.g., ?abx?
for?antibiotics?
), and composite terms (e.g., ?may-december?
or ?virus/worm?).
To address this prob-lem, techniques have been developed to automatethe construction of semantic lexicons from text cor-pora using bootstrapping methods (Riloff and Shep-herd, 1997; Roark and Charniak, 1998; Phillips andRiloff, 2002; Thelen and Riloff, 2002; Ng, 2007;McIntosh and Curran, 2009; McIntosh, 2010), butaccuracy is still far from perfect.Our research explores the use of ensemble meth-ods to improve the accuracy of semantic lexicon in-duction.
Our observation is that semantic class as-sociations can be learned using several fundamen-tally different types of corpus analysis.
Bootstrap-ping methods for semantic lexicon induction (e.g.,(Riloff and Jones, 1999; Thelen and Riloff, 2002;McIntosh and Curran, 2009)) collect corpus-widestatistics for individual words based on shared con-textual patterns.
In contrast, classifiers for semantictagging (e.g., (Collins and Singer, 1999; Niu et al,2003; Huang and Riloff, 2010)) label word instancesand focus on the local context surrounding each in-stance.
The difference between these approaches isthat semantic taggers make decisions based on a sin-gle context and can assign different labels to differ-ent instances, whereas lexicon induction algorithmscompile corpus statistics from multiple instances ofa word and typically assign each word to a singlesemantic category.1 We also hypothesize that coref-erence resolution can be exploited to infer semantic1This approach would be untenable for broad-coverage se-mantic knowledge acquisition, but within a specialized domainmost words have a dominant word sense.
Our experimental re-sults support this assumption.199class labels.
Intuitively, if we know that two nounphrases are coreferent, then they probably belong tothe same high-level semantic category (e.g., ?dog?and ?terrier?
are both animals).In this paper, we present an ensemble-basedframework for semantic lexicon induction.
We in-corporate a pattern-based bootstrapping method forlexicon induction, a contextual semantic tagger, anda new coreference-based method for lexicon induc-tion.
Our results show that coalescing the decisionsproduced by diverse methods produces a better dic-tionary than any individual method alone.A second contribution of this paper is an analysisof the effectiveness of dictionaries for semantic tag-ging.
In principle, an NLP system should be able toassign different semantic labels to different sensesof a word.
But within a specialized domain, mostwords have a dominant sense and we argue that us-ing domain-specific dictionaries for tagging may beequally, if not more, effective.
We analyze the trade-offs between using an instance-based semantic tag-ger versus dictionary lookup on a collection of dis-ease outbreak articles.
Our results show that the in-duced dictionaries yield better performance than aninstance-based semantic tagger, achieving higher ac-curacy with comparable levels of recall.2 Related WorkSeveral techniques have been developed for seman-tic class induction (also called set expansion) usingbootstrapping methods that consider co-occurrencestatistics based on nouns (Riloff and Shepherd,1997), syntactic structures (Roark and Charniak,1998; Phillips and Riloff, 2002), and contextual pat-terns (Riloff and Jones, 1999; Thelen and Riloff,2002; McIntosh and Curran, 2008; McIntosh andCurran, 2009).
To improve the accuracy of in-duced lexicons, some research has incorporated neg-ative information from human judgements (Vyasand Pantel, 2009), automatically discovered neg-ative classes (McIntosh, 2010), and distributionalsimilarity metrics to recognize concept drift (McIn-tosh and Curran, 2009).
Phillips and Riloff (2002)used co-training (Blum and Mitchell, 1998) to ex-ploit three simple classifiers that each recognized adifferent type of syntactic structure.
The researchmost closely related to ours is an ensemble-basedmethod for automatic thesaurus construction (Cur-ran, 2002).
However, that goal was to acquire fine-grained semantic information that is more akin tosynonymy (e.g., words similar to ?house?
), whereaswe associate words with high-level semantic classes(e.g., a ?house?
is a transient structure).Semantic class tagging is closely related to namedentity recognition (NER) (e.g., (Bikel et al, 1997;Collins and Singer, 1999; Cucerzan and Yarowsky,1999; Fleischman and Hovy, 2002)).
Some boot-strapping methods have been used for NER (e.g.,(Collins and Singer, 1999; Niu et al, 2003) tolearn from unannotated texts.
However, most NERsystems will not label nominal noun phrases (e.g.,they will not identify ?the dentist?
as a person)or recognize semantic classes that are not associ-ated with proper named entities (e.g., symptoms).2ACE mention detection systems (e.g., (ACE, 2007;ACE, 2008)) can label noun phrases that are asso-ciated with 5-7 semantic classes and are typicallytrained with supervised learning.
Recently, (Huangand Riloff, 2010) developed a bootstrapping tech-nique that induces a semantic tagger from unanno-tated texts.
We use their system in our ensemble.There has also been work on extracting semanticclass members from the Web (e.g., (Pas?ca, 2004; Et-zioni et al, 2005; Kozareva et al, 2008; Carlson etal., 2009)).
This line of research is fundamentallydifferent from ours because these techniques benefitfrom the vast repository of information available onthe Web and are therefore designed to harvest a wideswath of general-purpose semantic information.
Ourresearch is aimed at acquiring domain-specific se-mantic dictionaries using a collection of documentsrepresenting a specialized domain.3 Ensemble-based Semantic LexiconInduction3.1 MotivationOur research combines three fundamentally differ-ent techniques into an ensemble-based bootstrap-ping framework for semantic lexicon induction:pattern-based dictionary induction, contextual se-mantic tagging, and coreference resolution.
Ourmotivation for using an ensemble of different tech-2Some NER systems will handle special constructions suchas dates and monetary amounts.200niques is driven by the observation that these meth-ods exploit different types of information to infer se-mantic class knowledge.
The coreference resolveruses features associated with coreference, such assyntactic constructions (e.g., appositives, predicatenominals), word overlap, semantic similarity, prox-imity, etc.
The pattern-based lexicon induction al-gorithm uses corpus-wide statistics gathered fromthe contexts of all instances of a word and comparesthem with the contexts of known category members.The contextual semantic tagger uses local contextwindows around words and classifies each word in-stance independently from the others.Since each technique draws its conclusions fromdifferent types of information, they represent inde-pendent sources of evidence to confirm whether aword belongs to a semantic class.
Our hypothe-sis is that, combining these different sources of ev-idence in an ensemble-based learning frameworkshould produce better accuracy than using any onemethod alone.
Based on this intuition, we createan ensemble-based bootstrapping framework that it-eratively collects the hypotheses produced by eachindividual learner and selects the words that werehypothesized by at least 2 of the 3 learners.
Thisapproach produces a bootstrapping process withimproved precision, both at the critical beginningstages of the bootstrapping process and during sub-sequent bootstrapping iterations.3.2 Component Systems in the EnsembleIn the following sections, we describe each of thecomponent systems used in our ensemble.3.2.1 Pattern-based Lexicon InductionThe first component of our ensemble is Basilisk(Thelen and Riloff, 2002), which identifies nounsbelonging to a semantic class based on collec-tive information over lexico-syntactic pattern con-texts.
The patterns are automatically generated us-ing AutoSlog-TS (Riloff, 1996).
Basilisk beginswith a small set of seed words for each seman-tic class and a collection of unannotated documentsfor the domain.
In an iterative bootstrapping pro-cess, Basilisk identifies candidate nouns, ranks thembased on its scoring criteria, selects the 5 most confi-dent words for inclusion in the lexicon, and this pro-cess repeats using the new words as additional seedsin subsequent iterations.3.2.2 Lexicon Induction with a ContextualSemantic TaggerThe second component in our ensemble is a con-textual semantic tagger (Huang and Riloff, 2010).Like Basilisk, the semantic tagger also begins withseed nouns, trains itself on a large collection ofunannotated documents using bootstrapping, and it-eratively labels new instances.
This tagger labelsnoun instances and does not produce a dictionary.To adapt it for our purposes, we ran the bootstrap-ping process over the training texts to induce a se-mantic classifier.
We then applied the classifier tothe same set of training documents and compiled alexicon by collecting the set of nouns that were as-signed to each semantic class.
We ignored wordsthat were assigned different labels in different con-texts to avoid conflicts in the lexicons.
We usedthe identical configuration described by (Huang andRiloff, 2010) that applies a 1.0 confidence thresholdfor semantic class assignment.3.2.3 Coreference-Based Lexicon ConstructionThe third component of our ensemble is a newmethod for semantic lexicon induction that exploitscoreference resolution.
Members of a coreferencechain represent the same entity, so all references tothe entity should belong to the same semantic class.For example, suppose ?Paris?
and ?the city?
are inthe same coreference chain.
If we know that city isa Fixed Location, then we can infer that Paris is alsoa Fixed Location.We induced lexicons from coreference chains us-ing a similar bootstrapping framework that beginswith seed nouns and unannotated texts.
Let S de-note a set of semantic classes and W denote a set ofunknown words.
For any s ?
S and w ?
W , letNs,w denote the number of instances of s in the cur-rent lexicon3 that are coreferent with w in the textcorpus.
Then we estimate the probability that wordw belongs to semantic class s as:P (s|w) = Ns,w?s?
?S Ns?,wWe hypothesize the semantic class of w,SemClass(w) by:SemClass(w) = argmaxs P (s|w)3In the first iteration, the lexicon is initialized with the seeds.201To ensure high precision for the induced lexicons,we use a threshold of 0.5.
All words with a prob-ability above this thresold are added to the lexicon,and the bootstrapping process repeats.
Although thecoreference chains remain the same throughout theprocess, the lexicon grows so more words in thechains have semantic class labels as bootstrappingprogresses.
Bootstrapping ends when fewer than 5words are learned for each of the semantic classes.Many noun phrases are singletons (i.e., they arenot coreferent with any other NPs), which limits theset of words that can be learned using coreferencechains.
Furthermore, coreference resolvers makemistakes, so the accuracy of the induced lexiconsdepends on the quality of the chains.
For our experi-ments, we used Reconcile (Stoyanov et al, 2010), afreely available supervised coreference resolver.3.3 Ensemble-based BootstrappingFrameworkFigure 1 shows the architecture of our ensemble-based bootstrapping framework.
Initially, each lexi-con only contains the seed nouns.
Each componenthypothesizes a set of candidate words for each se-mantic class, based on its own criteria.
The wordlists produced by the three systems are then com-pared, and we retain only the words that were hy-pothesized with the same class label by at least twoof the three systems.
The remaining words are dis-carded.
The consenus words are added to the lexi-con, and the bootstrapping process repeats.
As soonas fewer than 5 words are learned for each of thesemantic classes, bootstrapping stops.Figure 1: Ensemble-based bootstrapping frameworkWe ran each individual system with the same seedwords.
Since bootstrapping typically yields the bestprecision during the earliest stages, we used the se-mantic tagger?s trained model immediately after itsfirst bootstrapping iteration.
Basilisk generates 5words per cycle, so we report results for lexiconsgenerated after 20 bootstrapping cycles (100 words)and after 80 bootstrapping cycles (400 words).3.4 Co-Training FrameworkThe three components in our ensemble use differenttypes of features (views) to identify semantic classmembers, so we also experimented with co-training.Our co-training model uses an identical framework,but the hypotheses produced by the different meth-ods are all added to the lexicon, so each method canbenefit from the hypotheses produced by the others.To be conservative, each time we added only the 10most confident words hypothesized by each method.In contrast, the ensemble approach only addswords to the lexicon if they are hypothesized by twodifferent methods.
As we will see in Section 4.4,the ensemble performs much better than co-training.The reason is that the individual methods do not con-sistently achieve high precision on their own.
Con-sequently, many mistakes are added to the lexicon,which is used as training data for subsequent boot-strapping.
The benefit of the ensemble is that con-sensus is required across two methods, which servesas a form of cross-checking to boost precision andmaintain a high-quality lexicon.4 Evaluation4.1 Semantic Class DefinitionsWe evaluated our approach on nine semantic cate-gories associated with disease outbreaks.
The se-mantic classes are defined below.Animal: Mammals, birds, fish, insects and otheranimal groups.
(e.g., cow, crow, mosquito, herd)4http://www.nlm.nih.gov/research/umls/5http://www.maxmind.com/app/worldcities6http://www.listofcountriesoftheworld.com/7http://names.mongabay.com/most_common_surnames.htm8http://www.sec.gov/rules/other/4-460list.htm9http://www.utexas.edu/world/univ/state/10http://www.uta.fi/FAST/GC/usabacro.html/202Semantic External Word List SourcesClassAnimal WordNet: [animal], [mammal family], [animal group]Body Part WordNet: [body part], [body substance], [body covering], [body waste]DisSym WordNet: [symptom], [physical condition], [infectious agent]; Wikipedia: common and infectiousdiseases, symptoms, disease acronyms; UMLS Thesaurus4: diseases, abnormalities, microorganisms(Archaea, Bacteria, Fungus, Virus)Fixed Loc.
WordNet: [geographic area], [land], [district, territory], [region]; Wiki:US-states; Other:cities5, countries6Human WordNet: [person], [people], [personnel]; Wikipedia: people names, office holder titles, nationalities,occupations, medical personnels & acronyms, players; Other: common people names & surnames7Org WordNet: [organization], [assembly]; Wikipedia: acronyms in healthcare, medical organization acronyms,news agencies, pharmaceutical companies; Other: companies8, US-universities9, organizations10Plant & Food WordNet: [food], [plant, flora], [plant part]Temp.
Ref.
WordNet: [time], [time interval], [time unit],[time period]TimeBank: TimeBank1.2 (Pustejovsky et al, 2003) TIMEX3 expressionsTrans.
Struct.
WordNet: [structure, construction], [road, route], [facility, installation], [work place]Table 1: External Word List SourcesBody Part: A part of a human or animal body, in-cluding organs, bodily fluids, and microscopic parts.
(e.g., hand, heart, blood, DNA)Diseases and Symptoms (DisSym): Diseasesand symptoms.
We also include fungi and diseasecarriers because, in this domain, they almost alwaysrefer to the disease that they carry.
(e.g.
FMD, An-thrax, fever, virus)Fixed Location (Fixed Loc.
): Named locations,including countries, cities, states, etc.
We also in-clude directions and well-defined geographic areasor geo-political entities.
(e.g., Brazil, north, valley)Human: All references to people, includingnames, titles, professions, and groups.
(e.g., John,farmer, traders)Organization (Org.
): An entity that represents agroup of people acting as a single recognized body,including named organizations, departments, gov-ernments, and their acronyms.
(e.g., department,WHO, commission, council)Temporal Reference (Temp.
Ref.
): Any refer-ence to a time or duration, including months, days,seasons, etc.
(e.g., night, May, summer, week)Plants & Food11: plants, plant parts, or any typeof food.
(e.g., seed, mango, beef, milk)Transient Structures (Trans.
Struct.
): Transientphysical structures.
(e.g., hospital, building, home)Additionally, we defined a Miscellaneous classfor words that do not belong to any of the other cat-11We merged plants and food into a single category as it isdifficult to separate them because many food items are plants.egories.
(e.g., output, information, media, point).4.2 Data SetWe ran our experiments on ProMED-mail12 articles.ProMED-mail is an internet based reporting systemfor infectious disease outbreaks, which can involvepeople, animals, and plants grown for food.
OurProMED corpus contains 5004 documents.
We used4959 documents as (unannotated) training data forbootstrapping.
For the remaining 45 documents,we used 22 documents to train the coreference re-solver (Reconcile) and 23 documents as our test set.The coreference training set contains MUC-7 style(Hirschman, 1997) coreference annotations13.
Oncetrained, Reconcile was applied to the 4959 unanno-tated documents to produce coreference chains.4.3 Gold Standard Semantic Class AnnotationsTo obtain gold standard annotations for the test set,two annotators assigned one of the 9 semantic classlabels, or Miscellaneous, to each head noun based onits surrounding context.
A noun with multiple sensescould get assigned different semantic class labels indifferent contexts.
The annotators first annotated 13of the 23 documents, and discussed the cases wherethey disagreed.
Then they independelty annotated12http://www.promedmail.org/13We omit the details of the coreference annotations sinceit is not the focus of this research.
However, the annotatorsmeasured their agreement on 10 documents and achieved MUCscores of Precision = .82, Recall = .86, F-measure = .84.203the remaining 10 documents and measured inter-annotator agreement with Cohen?s Kappa (?)
(Car-letta, 1996).
The ?
score for these 10 documents was0.91, indicating a high level of agreement.
The an-notators then adjudicated their disagreements on all23 documents to create the gold standard.4.4 Dictionary EvaluationTo assess the quality of the lexicons, we estimatedtheir accuracy by compiling external word listsfrom freely available sources such as Wikipedia14and WordNet (Miller, 1990).
Table 1 shows thesources that we used, where the bracketed items re-fer to WordNet hypernym categories.
We searchedeach WordNet hypernym tree (also, instance-relationship) for all senses of the word.
Addition-ally, we collected the manually labeled words in ourtest set and included them in our gold standard lists.Since the induced lexicons contain individualnouns, we extracted only the head nouns of multi-word phrases in the external resources.
Thiscan produce incorrect entries for non-compositionalphrases, but we found this issue to be relatively rareand we manually removed obviously wrong entries.We adopted a conservative strategy and assumed thatany lexicon entries not present in our gold standardlists are incorrect.
But we observed many correct en-tries that were missing from the external resources,so our results should be interpreted as a lower boundon the true accuracy of the induced lexicons.We generated lexicons for each method sepa-rately, and also for the ensemble and co-trainingmodels.
We ran Basilisk for 100 iterations (500words).
We refer to a Basilisk lexicon of size Nusing the notation B[N ].
For example, B400 refersto a lexicon containing 400 words, which was gen-erated from 80 bootstrapping cycles.
We refer to thelexicon obtained from the semantic tagger as ST Lex.Figure 2 shows the dictionary evaluation results.We plotted Basilisk?s accuracy after every 5 boot-strapping cycles (25 words).
For ST Lex, we sortedthe words by their confidence scores and plotted theaccuracy of the top-ranked words in increments of50.
The plots for Coref, Co-Training, and EnsembleB[N] are based on the lexicons produced after eachbootstrapping cycle.14www.wikipedia.org/The ensemble-based framework yields consis-tently better accuracy than the individual methodsfor Animal, Body Part, Human and Temporal Refer-ence, and similar if not better for Disease & Symp-tom, Fixed Location, Organization, Plant & Food.However, relying on consensus from multiple mod-els produce smaller dictionaries.
Big dictionaries arenot always better than small dictionaries in practice,though.
We believe, it matters more whether a dic-tionary contains the most frequent words for a do-main, because they account for a disproportionatenumber of instances.
Basilisk, for example, oftenlearns infrequent words, so its dictionaries may havehigh accuracy but often fail to recognize commonwords.
We investigate this issue in the next section.4.5 Instance-based Tagging EvaluationWe also evaluated the effectiveness of the inducedlexicons with respect to instance-based semantictagging.
Our goal was to determine how useful thedictionaries are in two respects: (1) do the lexiconscontain words that appear frequently in the domain,and (2) is dictionary look-up sufficient for instance-based labeling?
Our bootstrapping processes en-force a constraint that a word can only belong to onesemantic class, so if polysemy is common, then dic-tionary look-up will be problematic.15The instance-based evaluation assigns a semanticlabel to each instance of a head noun.
When using alexicon, all instances of the same noun are assignedthe same semantic class via dictionary look-up.
Thesemantic tagger (SemTag), however, is applied di-rectly since it was designed to label instances.Table 2 presents the results.
As a baseline, theW.Net row shows the performance of WordNet forinstance tagging.
For words with multiple senses,we only used the first sense listed in WordNet.The Seeds row shows the results when perform-ing dictionary look-up using only the seed words.The remaining rows show the results for Basilisk(B100 and B400), coreference-based lexicon induc-tion (Coref), lexicon induction using the semantictagger (ST Lex), and the original instance-based tag-ger (SemTag).
The following rows show the resultsfor co-training (after 4 iterations and 20 iterations)15Only coarse polysemy across semantic classes is an issue(e.g., ?plant?
as a living thing vs. a factory).204Figure 2: Dictionary Evaluation Resultsand for the ensemble (using Basilisk size 100 andsize 400).
Table 3 shows the micro & macro averageresults across all semantic categories.Table 3 shows that the dictionaries produced bythe Ensemble w/B100 achieved better results thanthe individual methods and co-training with an Fscore of 80%.
Table 2 shows that the ensembleachieved better performance than the other methodsfor 4 of the 9 classes, and was usually competitiveon the remaining 5 classes.
WordNet (W.Net) con-sistently produced high precision, but with compar-atively lower recall, indicating that WordNet doesnot have sufficient coverage for this domain.4.6 AnalysisTable 4 shows the performance of our ensemblewhen using only 2 of the 3 component methods.Removing any one method decreases the averageF-measure by at least 3-5%.
Component pairsthat include induced lexicons from coreference (STLex+Coref and B100+Coref) yield high precisionbut low recall.
The component pair ST Lex+B100produces higher recall but with slightly lower accu-racy.
The ensemble framework boosted recall evenmore, while maintaining the same precision.We observe that some of the smallest lexiconsproduced the best results for instance-based seman-tic tagging (e.g., Organization).
Our hypothesis isthat consensus decisions across different methodshelps to promote the acquisition of high frequencydomain words, which are crucial to have in the dic-tionary.
The fact that dictionary look-up performedbetter than an instance-based semantic tagger alsosuggests that coarse polysemy (different senses that205Method Animal Body DisSym Fixed Human Org.
Plant & Temp.
Trans.Part Loc.
Food Ref.
Struct.P R F P R F P R F P R F P R F P R F P R F P R F P R FIndividual MethodsW.Net 92 88 90 93 59 72 99 77 87 86 58 69 83 55 66 86 44 59 65 79 71 93 85 89 85 64 73Seeds 100 54 70 92 55 69 100 59 74 95 10 18 100 22 36 100 41 58 100 61 76 100 52 69 100 09 17B100 99 77 86 94 73 82 100 66 80 96 23 37 96 31 47 91 58 71 82 64 72 68 83 75 67 22 33B400 94 90 92 51 86 64 100 69 81 97 35 51 91 51 65 79 77 78 46 82 59 49 94 64 83 78 80Coref 90 67 77 92 55 69 66 83 73 65 46 54 57 50 53 54 68 60 81 61 69 60 74 67 45 09 15ST Lex 94 89 91 68 77 72 80 91 85 91 74 82 79 43 55 84 62 71 51 68 58 73 91 81 82 49 61SemTag 91 90 90 52 68 59 77 90 83 91 78 84 81 48 60 80 63 70 43 82 56 77 93 84 83 53 64Co-Trainingpass4 64 76 70 67 73 70 91 79 85 91 39 54 98 44 61 83 69 76 43 68 53 73 94 82 49 36 42pass20 60 89 71 56 91 69 88 91 90 83 64 72 92 54 68 72 77 74 28 71 40 65 98 78 46 40 43Ensemblesw/B100 93 94 94 74 77 76 93 81 86 92 73 81 94 55 70 90 78 84 56 89 68 55 94 70 79 75 77w/B400 94 93 93 65 91 75 96 87 91 89 75 81 92 56 70 79 79 79 47 86 61 53 94 68 63 55 58Table 2: Instance-based Semantic Tagging Results (P = Precision, R = Recall, F = F-measure)Method Micro Average Macro AverageP R F P R FIndividual SystemsW.Net 88 66 75 87 68 76Seeds 99 35 52 99 40 57B100 89 50 64 88 55 68B400 77 66 71 77 74 75Coref 65 59 62 68 57 62ST Lex 82 72 77 78 72 75SemTag 80 74 77 75 74 74Co-Trainingpass4 77 61 68 73 64 68pass20 69 74 71 65 75 70Ensemblesw/B100 83 77 80 81 80 80w/B400 79 78 78 75 79 77Table 3: Micro & Macro Average for Semantic Taggingcut across semantic classes) is a relatively minor is-sue within a specialized domain.5 ConclusionsOur research combined three diverse methodsfor semantic lexicon induction in a bootstrappedensemble-based framework, including a novel ap-proach for lexicon induction based on coreferencechains.
Our ensemble-based approach performedbetter than the individual methods, in terms ofboth dictionary accuracy and instance-based seman-tic tagging.
In future work, we believe this ap-proach could be enhanced further by adding newtypes of techniques to the ensemble and by investi-Method Micro Average Macro AverageP R F P R FEnsemble with component pairsST Lex+Coref 92 59 72 92 57 70B100+Coref 92 40 56 94 44 60ST Lex+B100 82 69 75 81 75 77Ensemble with all componentsST Lex+B100+Coref 83 77 80 81 80 80Table 4: Ablation Study of the Ensemble Framework forSemantic Tagginggating better methods for estimating the confidencescores from the individual components.AcknowledgmentsWe are grateful to Lalindra de Silva for manuallyannotating data, Nathan Gilbert for help with Rec-oncile, and Ruihong Huang for help with the se-mantic tagger.
We gratefully acknowledge the sup-port of the National Science Foundation under grantIIS-1018314 and the Defense Advanced ResearchProjects Agency (DARPA) Machine Reading Pro-gram under Air Force Research Laboratory (AFRL)prime contract no.
FA8750-09-C-0172.
Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of the au-thors and do not necessarily reflect the view of theDARPA, AFRL, or the U.S. government.206ReferencesACE.
2007.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2007.ACE.
2008.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2008.Daniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: a high-performance learning name-finder.
In Proceedings ofANLP-97, pages 194?201.A.
Blum and T. Mitchell.
1998.
Combining Labeled andUnlabeled Data with Co-Training.
In Proceedings ofthe 11th Annual Conference on Computational Learn-ing Theory (COLT-98).Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: the kappa statistic.
Comput.
Linguist.,22:249?254, June.Andrew Carlson, Justin Betteridge, Estevam R. Hr-uschka Jr., and Tom M. Mitchell.
2009.
Couplingsemi-supervised learning of categories and relations.In HLT-NAACL 2009 Workshop on Semi-SupervisedLearning for NLP.M.
Collins and Y.
Singer.
1999.
Unsupervised Mod-els for Named Entity Classification.
In Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora (EMNLP/VLC-99).S.
Cucerzan and D. Yarowsky.
1999.
Language Inde-pendent Named Entity Recognition Combining Mor-phologi cal and Contextual Evidence.
In Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora (EMNLP/VLC-99).J.
Curran.
2002.
Ensemble Methods for Automatic The-saurus Extraction.
In Proceedings of the 2002 Confer-ence on Empirical Methods in Natural Language Pro-cessing.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb: an experimental study.
Artificial Intelligence,165(1):91?134, June.M.B.
Fleischman and E.H. Hovy.
2002.
Fine grainedclassification of named entities.
In Proceedings of theCOLING conference, August.L.
Hirschman.
1997.
MUC-7 Coreference Task Defini-tion.Ruihong Huang and Ellen Riloff.
2010.
Inducingdomain-specific semantic class taggers from (almost)nothing.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
SemanticClass Learning from the Web with Hyponym PatternLinkage Graphs.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies (ACL-08).T.
McIntosh and J. Curran.
2008.
Weighted mutualexclusion bootstrapping for domain independent lex-icon and template acquisition.
In Proceedings of theAustralasian Language Technology Association Work-shop.T.
McIntosh and J. Curran.
2009.
Reducing SemanticDrift with Bagging and Distributional Similarity.
InProceedings of the 47th Annual Meeting of the Associ-ation for Computational Linguistics.T.
McIntosh.
2010.
Unsupervised Discovery of NegativeCategories in Lexicon Bootstrapping.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing.G.
Miller.
1990.
Wordnet: An On-line Lexical Database.International Journal of Lexicography, 3(4).V.
Ng.
2007.
Semantic Class Induction and CoreferenceResolution.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics.Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Srihari.2003.
A bootstrapping approach to named entity clas-sification using successive learners.
In Proceedings ofthe 41st Annual Meeting on Association for Computa-tional Linguistics (ACL-03), pages 335?342.M.
Pas?ca.
2004.
Acquisition of categorized named en-tities for web search.
In Proc.
of the Thirteenth ACMInternational Conference on Information and Knowl-edge Management, pages 137?145.W.
Phillips and E. Riloff.
2002.
Exploiting Strong Syn-tactic Heuristics and Co-Training to Learn SemanticLexicons.
In Proceedings of the 2002 Conference onEmpirical Methods in Natural Language Processing,pages 125?132.J.
Pustejovsky, P. Hanks, R.
Saur?
?, A.
See, R. Gaizauskas,A.
Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,and M. Lazo.
2003.
The TIMEBANK Corpus.
InProceedings of Corpus Linguistics 2003, pages 647?656.E.
Riloff and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-Level Bootstrapping.In Proceedings of the Sixteenth National Conferenceon Artificial Intelligence.E.
Riloff and J. Shepherd.
1997.
A Corpus-Based Ap-proach for Building Semantic Lexicons.
In Proceed-ings of the Second Conference on Empirical Methodsin Natural Language Processing, pages 117?124.E.
Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proceedings of theThirteenth National Conference on Artificial Intelli-gence, pages 1044?1049.
The AAAI Press/MIT Press.B.
Roark and E. Charniak.
1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic Semantic207Lexicon Construction.
In Proceedings of the 36thAnnual Meeting of the Association for ComputationalLinguistics, pages 1110?1116.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.
Coref-erence resolution with Reconcile.
In Proceedings ofthe ACL 2010 Conference Short Papers, pages 156?161.M.
Thelen and E. Riloff.
2002.
A Bootstrapping Methodfor Learning Semantic Lexicons Using Extraction Pattern Contexts.
In Proceedings of the 2002 Conferenceon Empirical Methods in Natural Language Process-ing, pages 214?221.V.
Vyas and P. Pantel.
2009.
Semi-automatic entity setrefinement.
In Proceedings of North American Asso-ciation for Computational Linguistics / Human Lan-guage Technology (NAACL/HLT-09).208
