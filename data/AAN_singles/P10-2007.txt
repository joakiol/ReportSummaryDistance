Proceedings of the ACL 2010 Conference Short Papers, pages 33?37,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsThe Same-head Heuristic for CoreferenceMicha Elsner and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{melsner,ec}@cs.brown.eduAbstractWe investigate coreference relationshipsbetween NPs with the same head noun.It is relatively common in unsupervisedwork to assume that such pairs arecoreferent?
but this is not always true, es-pecially if realistic mention detection isused.
We describe the distribution of non-coreferent same-head pairs in news text,and present an unsupervised generativemodel which learns not to link some same-head NPs using syntactic features, improv-ing precision.1 IntroductionFull NP coreference, the task of discovering whichnon-pronominal NPs in a discourse refer to thesame entity, is widely known to be challenging.In practice, however, most work focuses on thesubtask of linking NPs with different head words.Decisions involving NPs with the same head wordhave not attracted nearly as much attention, andmany systems, especially unsupervised ones, op-erate under the assumption that all same-headpairs corefer.
This is by no means always the case?there are several systematic exceptions to the rule.In this paper, we show that these exceptions arefairly common, and describe an unsupervised sys-tem which learns to distinguish them from coref-erent same-head pairs.There are several reasons why relatively littleattention has been paid to same-head pairs.
Pri-marily, this is because they are a comparativelyeasy subtask in a notoriously difficult area; Stoy-anov et al (2009) shows that, among NPs headedby common nouns, those which have an exactmatch earlier in the document are the easiest toresolve (variant MUC score .82 on MUC-6) andwhile those with partial matches are quite a bitharder (.53), by far the worst performance is onthose without any match at all (.27).
This effectis magnified by most popular metrics for coref-erence, which reward finding links within largeclusters more than they punish proposing spu-rious links, making it hard to improve perfor-mance by linking conservatively.
Systems thatuse gold mention boundaries (the locations of NPsmarked by annotators)1 have even less need toworry about same-head relationships, since mostNPs which disobey the conventional assumptionare not marked as mentions.In this paper, we count how often same-headpairs fail to corefer in the MUC-6 corpus, show-ing that gold mention detection hides most suchpairs, but more realistic detection finds large num-bers.
We also present an unsupervised genera-tive model which learns to make certain same-head pairs non-coreferent.
The model is basedon the idea that pronoun referents are likely tobe salient noun phrases in the discourse, so wecan learn about NP antecedents using pronom-inal antecedents as a starting point.
Pronounanaphora, in turn, is learnable from raw data(Cherry and Bergsma, 2005; Charniak and Elsner,2009).
Since our model links fewer NPs than thebaseline, it improves precision but decreases re-call.
This tradeoff is favorable for CEAF, but notfor b3.2 Related workUnsupervised systems specify the assumption ofsame-head coreference in several ways: by as-1Gold mention detection means something slightly differ-ent in the ACE corpus, where the system input contains everyNP annotated with an entity type.33sumption (Haghighi and Klein, 2009), usinga head-prediction clause (Poon and Domingos,2008), and using a sparse Dirichlet prior on wordemissions (Haghighi and Klein, 2007).
(Thesethree systems, perhaps not coincidentally, use goldmentions.)
An exception is Ng (2008), who pointsout that head identity is not an entirely reliable cueand instead uses exact string match (minus deter-miners) for common NPs and an alias detectionsystem for proper NPs.
This work uses mentionsextracted with an NP chunker.
No specific resultsare reported for same-head NPs.
However, whileusing exact string match raises precision, manynon-matching phrases are still coreferent, so thisapproach cannot be considered a full solution tothe problem.Supervised systems do better on the task, butnot perfectly.
Recent work (Stoyanov et al, 2009)attempts to determine the contributions of variouscategories of NP to coreference scores, and shows(as stated above) that common NPs which partiallymatch an earlier mention are not well resolved bythe state-of-the-art RECONCILE system, whichuses pairwise classification.
They also show thatusing gold mention boundaries makes the corefer-ence task substantially easier, and argue that thisexperimental setting is ?rather unrealistic?.3 Descriptive study: MUC-6We begin by examining how often non-same-headpairs appear in the MUC-6 coreference dataset.To do so, we compare two artificial coreferencesystems: the link-all strategy links all, and only,full (non-pronominal) NP pairs with the same headwhich occur within 10 sentences of one another.The oracle strategy links NP pairs with the samehead which occur within 10 sentences, but only ifthey are actually coreferent (according to the goldannotation)2 The link-all system, in other words,does what most existing unsupervised systems doon the same-head subset of NPs, while the oraclesystem performs perfectly.We compare our results to the gold standard us-ing two metrics.
b3(Bagga and Baldwin, 1998)is a standard metric which calculates a precisionand recall for each mention.
The mention CEAF(Luo, 2005) constructs a maximum-weight bipar-2The choice of 10 sentences as the window size capturesmost, but not all, of the available recall.
Using nouns mentiondetection, it misses 117 possible same-head links, or about10%.
However, precision drops further as the window sizeincreases.tite matching between gold and proposed clusters,then gives the percentage of entities whose goldlabel and proposed label match.
b3 gives moreweight to errors involving larger clusters (sincethese lower scores for several mentions at once);for mention CEAF, all mentions are weightedequally.We annotate the data with the self-trained Char-niak parser (McClosky et al, 2006), then extractmentions using three different methods.
The goldmentions method takes only mentions marked byannotators.
The nps method takes all base nounphrases detected by the parser.
Finally, the nounsmethod takes all nouns, even those that do nothead NPs; this method maximizes recall, since itdoes not exclude prenominals in phrases like ?aBush spokesman?.
(High-precision models of theinternal structure of flat Penn Treebank-style NPswere investigated by Vadas and Curran (2007).
)For each experimental setting, we show the num-ber of mentions detected, and how many of themare linked to some antecedent by the system.The data is shown in Table 1. b3 shows a largedrop in precision when all same-head pairs arelinked; in fact, in the nps and nouns settings, onlyabout half the same-headed NPs are actually coref-erent (864 real links, 1592 pairs for nps).
Thisdemonstrates that non-coreferent same-head pairsnot only occur, but are actually rather common inthe dataset.
The drop in precision is much lessobvious in the gold mentions setting, however;most unlinked same-head pairs are not annotatedas mentions in the gold data, which is one reasonwhy systems run in this experimental setting canafford to ignore them.Improperly linking same-head pairs causes aloss in precision, but scores are dominated by re-call3.
Thus, reporting b3 helps to mask the impactof these pairs when examining the final f-score.We roughly characterize what sort of same-headed NPs are non-coreferent by hand-examining 100 randomly selected pairs.
39pairs denoted different entities (?recent employ-ees?
vs ?employees who have worked for longer?
)disambiguated by modifiers or sometimes bydiscourse position.
The next largest group (24)consists of time and measure phrases like ?tenmiles?.
12 pairs refer to parts or quantities3This bias is exaggerated for systems which only linksame-head pairs, but continues to apply to real systems; forinstance (Haghighi and Klein, 2009) has a b3 precision of 84and recall of 67.34Mentions Linked b3 pr rec F mention CEAFGold mentionsOracle 1929 1164 100 32.3 48.8 54.4Link all 1929 1182 80.6 31.7 45.5 53.8Alignment 1929 495 93.7 22.1 35.8 40.5NPsOracle 3993 864 100 30.6 46.9 73.4Link all 3993 1592 67.2 29.5 41.0 62.2Alignment 3993 518 87.2 24.7 38.5 67.0NounsOracle 5435 1127 100 41.5 58.6 83.5Link all 5435 2541 56.6 40.9 45.7 67.0Alignment 5435 935 83.0 32.8 47.1 74.4Table 1: Oracle, system and baseline scores on MUC-6 test data.
Gold mentions leave little roomfor improvement between baseline and oracle; detecting more mentions widens the gap betweenthem.
With realistic mention detection, precision and CEAF scores improve over baselines, while recalland f-scores drop.
(?members of...?
), and 12 contained a generic(?In a corporate campaign, a union tries...?).
9contained an annotator error.
The remaining 4were mistakes involving proper noun phrasesheaded by Inc. and other abbreviations; this caseis easy to handle, but apparently not the primarycause of errors.4 SystemOur system is a version of the popular IBM model2 for machine translation.
To define our generativemodel, we assume that the parse trees for the en-tire document D are given, except for the subtreeswith root nonterminal NP, denoted ni, which oursystem will generate.
These subtrees are relatedby a hidden set of alignments, ai, which link eachNP to another NP (which we call a generator) ap-pearing somewhere before it in the document, orto a null antecedent.
The set of potential genera-tors G (which plays the same role as the source-language text in MT) is taken to be all the NPsoccurring within 10 sentences of the target, plus aspecial null antecedent which plays the same roleas the null word in machine translation?
it servesas a dummy generator for NPs which are unrelatedto any real NP in G.The generative process fills in all the NP nodesin order, from left to right.
This process ensuresthat, when generating node ni, we have alreadyfilled in all the NPs in the set G (since these allprecede ni).
When deciding on a generator forNP ni, we can extract features characterizing itsrelationship to a potential generator gj .
These fea-tures, which we denote f(ni, gj , D), may dependon their relative position in the document D, andon any features of gj , since we have already gener-ated its tree.
However, we cannot extract featuresfrom the subtree under ni, since we have yet togenerate it!As usual for IBM models, we learn using EM,and we need to start our alignment function offwith a good initial set of parameters.
Since an-tecedents of NPs and pronouns (both salient NPs)often occur in similar syntactic environments, weuse an alignment function for pronoun corefer-ence as a starting point.
This alignment can belearned from raw data, making our approach un-supervised.We take the pronoun model of Charniak and El-sner (2009)4 as our starting point.
We re-expressit in the IBM framework, using a log-linear modelfor our alignment.
Then our alignment (parame-terized by feature weights w) is:p(ai = j|G,D) ?
exp(f(ni, gj , D) ?
w)The weights w are learned by gradient descenton the log-likelihood.
To use this model withinEM, we alternate an E-step where we calculatethe expected alignments E[ai = j], then an M-step where we run gradient descent.
(We have alsohad some success with stepwise EM as in (Liangand Klein, 2009), but this requires some tuning towork properly.
)4Downloaded from http://bllip.cs.brown.edu.35As features, we take the same features as Char-niak and Elsner (2009): sentence and word-countdistance between ni and gj , sentence position ofeach, syntactic role of each, and head type of gj(proper, common or pronoun).
We add binary fea-tures for the nonterminal directly over gj (NP, VP,PP, any S type, or other), the type of phrases mod-ifying gj (proper nouns, phrasals (except QP andPP), QP, PP-of, PP-other, other modifiers, or noth-ing), and the type of determiner of gj (possessive,definite, indefinite, deictic, other, or nothing).
Wedesigned this feature set to distinguish prominentNPs in the discourse, and also to be able to detectabstract or partitive phrases by examining modi-fiers and determiners.To produce full NPs and learn same-head coref-erence, we focus on learning a good alignmentusing the pronoun model as a starting point.
Fortranslation, we use a trivial model, p(ni|gai) = 1if the two have the same head, and 0 otherwise,except for the null antecedent, which draws headsfrom a multinomial distribution over words.While we could learn an alignment and thentreat all generators as antecedents, so that onlyNPs aligned to the null antecedent were not la-beled coreferent, in practice this model wouldalign nearly all the same-head pairs.
This istrue because many words are ?bursty?
; the prob-ability of a second occurrence given the first ishigher than the a priori probability of occurrence(Church, 2000).
Therefore, our model is actually amixture of two IBM models, pC and pN , where pCproduces NPs with antecedents and pN producespairs that share a head, but are not coreferent.
Tobreak the symmetry, we allow pC to use any pa-rameters w, while pN uses a uniform alignment,w ?
~0.
We interpolate between these two modelswith a constant ?, the single manually set parame-ter of our system, which we fixed at .9.The full model, therefore, is:p(ni|G,D) =?pT (ni|G,D)+ (1?
?
)pN (ni|G,D)pT (ni|G,D) =1Z?j?Gexp(f(ni, gj , D) ?
w)?
I{head(ni) = head(j)}pT (ni|G,D) =?j?G1|G|I{head(ni) = head(gj)}NPs for which the maximum-likelihood gener-ator (the largest term in either of the sums) is frompT and is not the null antecedent are marked ascoreferent to the generator.
Other NPs are markednot coreferent.5 ResultsOur results on the MUC-6 formal test set areshown in Table 1.
In all experimental settings,the model improves precision over the baselinewhile decreasing recall?
that is, it misses some le-gitimate coreferent pairs while correctly exclud-ing many of the spurious ones.
Because of theprecision-recall tradeoff at which the systems op-erate, this results in reduced b3 and link F. How-ever, for the nps and nouns settings, where theparser is responsible for finding mentions, thetradeoff is positive for the CEAF metrics.
For in-stance, in the nps setting, it improves over baselineby 57%.As expected, the model does poorly in the goldmentions setting, doing worse than baseline onboth metrics.
Although it is possible to get veryhigh precision in this setting, the model is far tooconservative, linking less than half of the availablementions to anything, when in fact about 60% ofthem are coreferent.
As we explain above, this ex-perimental setting makes it mostly unnecessary toworry about non-coreferent same-head pairs be-cause the MUC-6 annotators don?t often markthem.6 ConclusionsWhile same-head pairs are easier to resolve thansame-other pairs, they are still non-trivial and de-serve further attention in coreference research.
Toeffectively measure their effect on performance,researchers should report multiple metrics, sinceunder b3 the link-all heuristic is extremely diffi-cult to beat.
It is also important to report resultsusing a realistic mention detector as well as goldmentions.AcknowledgementsWe thank Jean Carletta for the SWITCHBOARDannotations, and Dan Jurafsky and eight anony-mous reviewers for their comments and sugges-tions.
This work was funded by a Google graduatefellowship.36ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In LREC Workshop onLinguistics Coreference, pages 563?566.Eugene Charniak and Micha Elsner.
2009.
EM worksfor pronoun anaphora resolution.
In Proceedings ofEACL, Athens, Greece.Colin Cherry and Shane Bergsma.
2005.
An Expecta-tion Maximization approach to pronoun resolution.In Proceedings of CoNLL, pages 88?95, Ann Arbor,Michigan.Kenneth W. Church.
2000.
Empirical estimates ofadaptation: the chance of two Noriegas is closer top/2 than p2.
In Proceedings of ACL, pages 180?186.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric Bayesianmodel.
In Proceedings of ACL, pages 848?855.Aria Haghighi and Dan Klein.
2009.
Simple corefer-ence resolution with rich syntactic and semantic fea-tures.
In Proceedings of EMNLP, pages 1152?1161.Percy Liang and Dan Klein.
2009.
Online EM for un-supervised models.
In HLT-NAACL.Xiaoqiang Luo.
2005.
On coreference resolution per-formance metrics.
In Proceedings of HLT-EMNLP,pages 25?32, Morristown, NJ, USA.
Association forComputational Linguistics.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of HLT-NAACL, pages 152?159.Vincent Ng.
2008.
Unsupervised models for corefer-ence resolution.
In Proceedings of EMNLP, pages640?649, Honolulu, Hawaii.
Association for Com-putational Linguistics.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov Logic.In Proceedings of EMNLP, pages 650?659, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP, pages656?664, Suntec, Singapore, August.
Associationfor Computational Linguistics.David Vadas and James Curran.
2007.
Adding nounphrase structure to the penn treebank.
In Proceed-ings of ACL, pages 240?247, Prague, Czech Repub-lic, June.
Association for Computational Linguis-tics.37
