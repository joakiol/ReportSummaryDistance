Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 206?215,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsWhat do We Know about Conversation Participants: Experiments onConversation EntailmentChen Zhang Joyce Y. ChaiDepartment of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48824, USA{zhangch6, jchai}@cse.msu.eduAbstractGiven the increasing amount of conversa-tion data, techniques to automatically ac-quire information about conversation par-ticipants have become more important.Towards this goal, we investigate the prob-lem of conversation entailment, a taskthat determines whether a given conversa-tion discourse entails a hypothesis aboutthe participants.
This paper describesthe challenges related to conversation en-tailment based on our collected data andpresents a probabilistic framework that in-corporates conversation context in entail-ment prediction.
Our preliminary exper-imental results have shown that conver-sation context, in particular dialogue act,plays an important role in conversation en-tailment.1 IntroductionConversation is a joint activity between its partic-ipants (Clark, 1996).
Their goals and their under-standing of mutual beliefs of each other shape thelinguistic discourse of conversation.
In turn, thislinguistic discourse provides tremendous informa-tion about conversation participants.
Given theincreasing amount of available conversation data(e.g., conversation scripts such as meeting scripts,court records, and online chatting), an importantquestion is what do we know about conversationparticipants?
The capability to automatically ac-quire such information can benefit many appli-cations, for example, development of social net-works and discovery of social dynamics.Related to this question, previous work has de-veloped techniques to extract profiling informa-tion about participants from conversation inter-views (Jing et al, 2007) and to automatically iden-tify dynamics between conversation participantssuch as agreement/disagreement from multipartymeeting scripts (Galley et al, 2004).
We approachthis question from a different angle as a conversa-tion entailment problem: given a conversation dis-course D and a hypothesis H concerning its par-ticipant, the goal is to identify whether D entailsH.
For instance, in the following example, the firsthypothesis can be entailed from the dialogue seg-ment while the second hypothesis cannot.Example 1:Dialogue Segment:A: And where about were you born?B: Up in Person Country.Hypothesis:(1) B was born in Person Country.
(2) B lives in Person Country.Inspired by textual entailment (Dagan et al,2005; Bar-Haim et al, 2006; Giampiccolo et al,2007), conversation entailment provides an inter-mediate step towards acquiring information aboutconversation participants.
What we should knowor would like to know about a participant can berather open.
The type of information needed aboutparticipants is also application-dependent and dif-ficult to generalize.
In conversation entailment, wewill not face this problem since hypotheses can beused to express any type of information about aparticipant one might be interested in.
Althoughhypotheses are currently given in our investiga-tion, they can potentially be automatically gener-ated based on information needs and/or theorieson cognitive status/mental models of conversationparticipants.
The capability to make correct entail-ment judgements based on these hypotheses willbenefit many applications such as information ex-traction, question answering, and summarization.As a first step in our investigation, we collecteda corpus of conversation entailment data fromnineteen human annotators.
Our data showed thatconversation entailment is more challenging than206the textual entailment task due to unique charac-teristics about conversation and conversational im-plicature.
To predict entailment, we developed aprobabilisitic framework that incorporates seman-tic representation of conversation context.
Ourpreliminary experimental results have shown thatconversation context, in particular dialogue acts,play an important role in conversation entailment.2 Related WorkRecent work has applied different approachesto acquire information about conversation par-ticipants based on human-human conversationscripts, for example, to extract profiling infor-mation from conversation interviews (Jing et al,2007) and to identify agreement/disagreementbetween participants from multiparty meetingscripts (Galley et al, 2004).
In human-machineconversation, inference about conversation partic-ipants has been studied as a part of user modeling.For example, earlier work has investigated infer-ence of user intention from utterances to controlclarification dialogue (Horvitz and Paek, 2001)and recognition of user emotion and attitude fromutterances for intelligent tutoring systems (Litmanand Forbes-Riley, 2006).
In contrast to previouswork, we propose a new angle to address informa-tion acquisition about conversation participants,namely, through conversation entailment.This work is inspired by a large body of recentwork on textual entailment initiated by the PAS-CAL RTE Challenge (Dagan et al, 2005; Bar-Haim et al, 2006; Giampiccolo et al, 2007).
Nev-ertheless, conversation discourse is very differentfrom written monologue discourse.
The conversa-tion discourse is shaped by the goals of its partici-pants and their mutual beliefs.
The key distinctivefeatures include turn-taking between participants,grounding between participants, and different lin-guistic phenomena of utterances (e.g., utterancesin conversation tend to be shorter, with disfluency,and sometimes incomplete or ungrammatical).
Itis the goal of this paper to explore how techniquesdeveloped for textual entailment can be extendedto address these unique behaviors in conversationentailment.3 Experimental DataThe first step in our investigation is to collect en-tailment data to help us better understand the prob-lem and facilitate algorithm development and eval-uation.3.1 Data Collection ProcedureWe selected 50 dialogues from the Switchboardcorpus (Godfrey and Holliman, 1997).
In each ofthese dialogues, two participants discuss a topicof interest (e.g., sports activities, corporate cul-ture, etc.).
To focus our work on the entailmentproblem, we use the transcribed scripts of the di-alogues in our experiments.
We also make use ofavailable annotations such as syntactic structures,disfluency markers, and dialogue acts.We had 15 volunteer annotators read the se-lected dialogues and create hypotheses about par-ticipants.
As a result, a total of 1096 entailmentexamples were created.
Each example consists ofa snippet from the dialogue (referred to as dia-logue segment in the rest of this paper), a hypothe-sis statement, and a truth value indicating whetherthe hypothesis can be inferred from the snippetgiven the whole history of that dialogue session.During annotation, we asked the annotators to pro-vide balanced examples for each dialogue.
That is,roughly half of the hypotheses are truly entailedand half are not.
Special attention was given tonegative entailment examples.
Since any arbitraryhypotheses that are completely irrelevant can benegative examples, a special criteria is enforcedthat any negative examples should have a major-ity word overlap with the snippet.
In addition, in-spired by previous work (Jing et al, 2007; Galleyet al, 2004), we particularly asked annotators toprovide hypotheses that address the profiling in-formation of the participants, their opinions anddesires, as well as the dynamic communicative re-lations between participants.A recent study shows that for many NLP an-notation tasks, the reliability of a small numberof non-expert annotations is on par with that ofan expert annotator (Snow et al, 2008).
It alsofound that for tasks such as affection recogni-tion, an average of four non-expert labels per itemare capable of emulating expert-level label qual-ity.
Based on this finding, in our study the en-tailment judgement for each example was furtherindependently annotated by four annotators (whowere not the original contributors of the hypothe-ses).
As a result, on average each entailment ex-ample (i.e., a pair of snippet and hypothesis) re-ceived five judgements.207Figure 1: Agreement histogram of entailmentjudgements3.2 Data and ExamplesFigure 1 shows a histogram of the agreements ofcollected judgements.
It indicates that conversa-tion entailment is in fact a quite difficult task evenfor humans.
Only 53% of all the examples (586out of 1096) are agreed upon by all human annota-tors.
The disagreement between users sometimesis caused by language ambiguity since conversa-tion scripts are often short and without clear sen-tence boundaries.
For example,Example 2:Dialogue Segment:A: Margaret Thatcher was prime minister, uh,uh, in India, so many, uh, women are headsof state.Hypothesis:A believes that Margaret Thatcher was primeminister of India.In the utterance of speaker A, the prepositionalphrase in India is ambiguous because it can eitherbe attached to the preceding sentence, which suffi-ciently entails the hypothesis; or it can be attachedto the succeeding sentence, which leaves it unclearwhich country A believes Margaret Thatcher wasprime minister of.Difference in recognition and handling of con-versational implicature is another issue that led todisagreement among annotators.
For example:Example 3:Dialogue Segment:A: Um, I had a friend who had fixed some, uh,chili, buffalo chili and, about a week beforewe went to see the movie.Hypothesis:A ate some buffalo chili.Example 4:Dialogue Segment:B: Um, I?ve visited the Wyoming area.
I?mnot sure exactly where Dances with Wolveswas filmed.Hypothesis:B thinks Dances with Wolves was filmed inWyoming.In the first example, a listener could assumethat A follows the maxim of relevance.
Therefore,a natural inference that makes ?fixing of buffalochili?
relevant is that A ate the buffalo chili.
Sim-ilarly, in the second example, the speaker A men-tions a visit to Wyoming, which can be consideredrelevant to the filming place of DANCES WITHWOLVES.
Some annotators recognized such rele-vance and some did not.Given the discrepencies between annotators, weselected 875 examples which have at least 75%agreement among the judgements in our currentinvestigation.
We further selected one-third of thisdata (291 examples) as our development data.
Theexperiments reported in Section 5 are based on thisdevelopment set.3.3 Types of HypothesesThe hypotheses collected from our study can becategorzied into the following four types:Fact.
Facts about the participants.
This includes:(1) profiling information about individual partici-pants (e.g., occupation, birth place, etc.
); (2) activ-ities associated with individual participants (e.g.,A bikes to work everyday); and (3) social rela-tions between participants (e.g., A and B are co-workers, A and B went to college together).Belief.
Participants?
beliefs and opinions about thephysical world.
Any statement about the physicalworld in fact is a belief of the speaker.
Technically,the state of the physical world that involves thespeaker him/herself is also a type of belief.
How-ever, here we assume a statement about oneself istrue and is considered as a fact.Desire.
Participants?
desire of certain actions oroutcomes (e.g., A wants to find a university job).These desires represent the states of the world theparticipant finds pleasant (although they could beconflicting to each other).Intent.
Participants?
deliberated intent, in partic-ular communicative intention which captures theintent from one participant on the other partici-pant such as whether A agrees/disagrees with B208on some issue, whether A intends to convince Bon something, etc.Most of these types are motivated by the Belief-Desire-Intention (BDI) model, which representskey mental states and reflects the thoughts ofa conversation participant.
Desire is differentfrom intention.
The former arises subconsciouslyand the latter arise from rational deliberation thattakes into consideration desires and beliefs (Allen,1995).
The fact type represents the facts abouta participant.
Both thoughts and facts are criti-cal to characterize a participant and thus impor-tant to serve many other downstream applications.The above four types account for 47.1%, 34.0%,10.7%, and 8.2% of our development set respec-tively.4 A Probabilistic FrameworkFollowing previous work (Haghighi et al, 2005;de Salvo Braz et al, 2005; MacCartney et al,2006), we approach conversation entailment usinga probabilistic framework.
To predict whether ahypothesis statement H can be inferred from a di-alogue segment D, we estimate the probabilityP (D  H|D,H)Suppose we have a representation of a dia-logue segment D in m clauses d1, .
.
.
, dm and arepresentation of the hypothesis H in n clausesh1, .
.
.
, hn.
Since a hypothesis is the conjunc-tion of the decomposed clauses, whether it can beinferred from a segment is equivalent to whetherall of its clauses can be inferred from the seg-ment.
We further simplify the problem by assum-ing that whether a clause is entailed from a dia-logue segment is conditionally independent fromother clauses.
Note that this conditional indepen-dence assumption is an over-simplification, but itgets things started.
Therefore:P (D  H|D,H)= P (d1 .
.
.
dm  h1 .
.
.
hn|d1, .
.
.
, dm, h1, .
.
.
, hn)= P (D  h1, .
.
.
, D  hn|D,h1, .
.
.
, hn)=n?j=1P (D  hj |D = d1 .
.
.
dm, hj)=n?j=1P (d1 .
.
.
dm  hj |d1, .
.
.
, dm, hj) (1)If this likelihood is above a certain threshold(e.g., 0.5 in our experiments), then H is consid-ered as a true entailment from D.Given this framework, two important questionsare: (1) how to represent and automatically createthe clauses from each pair of dialogue segment andhypothesis; and (2) how to estimate probabilitiesas shown in Equation 1?4.1 Clause RepresentationOur clause representation is inspired by previ-ous work on textual entailment (Dagan et al,2005; Bar-Haim et al, 2006; Giampiccolo et al,2007).
Clause representation has several advan-tages.
First, it can be acquired automatically froma parse tree (e.g., dependency parser).
Second,it can be used to facilitate both logic-based rea-soning as in (Tatu and Moldovan, 2005; Bos andMarkert, 2005; Raina et al, 2005) or probabilis-tic reasoning as in (Haghighi et al, 2005; deSalvo Braz et al, 2005; MacCartney et al, 2006).The key difference between our work and previ-ous work on textual entailment is the representa-tion of conversation discourse, which has not beenconsidered in previous work but is important forconversation entailment, as we will see later.More specifically, a clause is made up by twocomponents: Term and Predicate.Term: A term can be an entity or an event.
Anentity refers to a person, a place, an organization,or other real world entities.
This follows the con-cept of mention in the Automatic Content Extrac-tion (ACE) evaluation (Doddington et al, 2004).An event refers to an action or an activity.
Forexample, from the sentence ?John married Eva in1940?
we can identify an event of marriage.
Fol-lowing the neo-Davidsonian representation (Par-sons, 1990), all the events are reified as terms inour representation.Predicate: A predicate represents either a prop-erty (i.e., unary) for a term or a relation (i.e., bi-nary) between two terms.
For example, an entitycompany has a property of Russian as in the phrase?a Russian company?
(i.e., Russian(company)).An event visit has a property of recently (i.e.,recently(visit)) as in the phrase ?visit Brazil re-cently?.
From the phrase ?Prime Minister re-cently visited Brazil?, there are binary relations:PrimeMinister is the subject of the event visit (i.e.,subj(visit, Prime Minister)) and Brazil is theobject of the visit (i.e., obj(visit, Brazil)).This representation is a direct conversion fromthe dependency structure and can be used to rep-resent the semantics of utterances in the dialogue209segments and the semantics of hypotheses.
For ex-ample,Example 5:Dialogue Segment:B: Have you seen Sleeping with the Enemy?A: No.
I?ve heard that?s really great, though.B: You have to go see that one.Hypothesis:B suggests A to watch Sleeping with the Enemy.Appendix A shows the dependency structure ofthe dialogue utterances and the hypothesis fromExample 5.
Appendix B shows the correspond-ing clause representation of the dialogue segmentand the hypothesis.
Note that in this represen-tation, you and I are replaced with the respec-tive participants.
Since the clauses are generatedbased on parse trees, most relational predicates aresyntactic-driven.To facilitate conversation entailment, we fur-ther augment the representation of a dialogue seg-ment by incorporating conversation context.
Ap-pendix C shows the augmented representation forExample 5.
It represents the following additionalinformation:?
Utterance: A group of pseudo terms u1,u2, .
.
.
are used to represent individual utter-ances.?
Participant: A relational clausespeaker(?, ?)
is used to represent the speakerof this utterance, e.g., speaker(u1, B).?
Content: A relational clause content(?, ?)
isused to represent the content of an utterancewhere the second term is the head of the ut-terance as identified in the parsing structure.e.g., content(u3, heard)?
Dialogue act: A relational clause act(?, ?
)is used to represent the dialogue act of thespeaker for a particular utterance.
e.g.,act(u2, no answer).
A set of 42 dialogueacts from the Switchboard annotation areused here (Godfrey and Holliman, 1997).?
Utterance flow: A relational clausefollow(?, ?)
is used to connect each pair ofadjacent utterances.
e.g., follow(u2, u1).We currently do not consider overlap in utter-ances, but our representation can be modifiedto handle this situation by introducingadditional predicates.4.2 Entailment PredictionGiven the clause representation for a conversationsegment and a hypothesis, the next step is to makean entailment prediction (as in Equation 1) basedon two models: an Alignment Model and an Infer-ence Model.4.2.1 Alignment ModelThe alignment model is to find alignments (ormatches) between terms in the clause representa-tion for a hypothesis and those in the clause rep-resentation for a conversation segment.
We definean alignment as a mapping function g between aterm x in the dialogue segment and a term y in thehypothesis.
g(x, y) = 1 if x and y are aligned;otherwise g(x, y) = 0.
Note that a verb can bealigned to a noun as in g(sell, sale) = 1.
It is alsopossible that there are multiple terms from the seg-ment mapped to one term in the hypothesis, or viceversa.For any two terms x and y, the problem of pre-dicting the alignment function g(x, y) can be for-mulated as a binary classification problem.
Weused several features to train the classifier, whichinclude whether x and y are the same (or have thesame stem), whether one term is an acronym of theother, and their WordNet and distributional simi-larities (Lin, 1998).Given an augmented representation with con-versation context (as in Appendix C), we alsoalign event terms in the hypothesis (e.g., suggestin Example 5) to (pseudo) utterance terms in thedialogue segment.
We call it a pseudo alignment.This is currently done by a set of rules which asso-ciate event terms in the hypotheses with dialogueacts.
For example, the event term suggest may bealigned to an utterance with dialogue act of opin-ion.
Appendix D gives a correct alignment for Ex-ample 5, in which g(u4, x1) = 1 is a pseudo align-ment.4.2.2 Inference ModelAs shown in Equation 1, to predict the infer-ence of the entire hypothesis, we need to calculatethe probability that the dialogue segment entailseach clause from the hypothesis.
More specifi-cally, given a clause from the hypothesis hj , a setof clauses from the dialogue segment d1, .
.
.
, dm,and an alignment function g between them derivedby the method described in Section 4.2.1, we pre-dict whether d1, .
.
.
, dm entails hj under the align-ment g using two different classification models,210depending on whether hj is a property or a rela-tion (i.e.
whether it takes one argument (hj(?))
ortwo arguments (hj(?, ?
))):Given a property clause from the hypothe-sis, hj(x), we look for all the property clausesin the dialogue segment that describes thesame term as x, i.e.
a clause set D?
={di(x?)|di(x?)
?
D, g(x?, x) = 1}.
Then we pre-dict whether hj(x) can be inferred from theclauses in D?
by binary classification, using a setof features similar to those used in the alignmentmodel.Given a relational clause from the hypothe-sis, hj(x, y), we look for the relation betweenthe counterparts of x and y in the dialogue seg-ment.
That is, we find the set of terms X ?
={x?|x?
?
D, g(x?, x) = 1} and the set of termsY ?
= {y?|y?
?
D, g(y?, y) = 1} and look for theclosest relation between these two sets of terms inthe dependency structure.
If there is a path be-tween any x?
?
X ?
and any y?
?
Y ?
in the de-pendency structure with a length smaller than athreshold ?L, we predict that hj(x, y) can be in-ferred.
Note that our current handling of the re-lational clauses is rather simplified.
It only cap-tures whether two terms from an hypothesis areconnected by any relation in the dialogue segment.Appendix E shows the inference procedure ofthe four hypothesis clauses in Example 5.
Foreach relational clause hj(x, y), the shortest pathbetween the correspondingX ?
and Y ?
has a lengthof 3 or less, so each of these four clauses is en-tailed from the dialogue segment.
Based on Equa-tion 1 we can conclude that the overall hypothesisis entailed.We trained the alignment model and the in-ference model (e.g.,the threshold ?L) based onthe development data provided by the PASCAL 3challenges on textual entailment.5 Experimental ResultsTo understand unique behaviors of conversationentailment, we focused our current experimentson the development dataset (see Section 3.2).We are particularly interested in how the tech-niques for textual entailment can be improved forconversation entailment.
To do so, we appliedour entailment framework on the test data of thePASCAL-3 RTE Challenge (Giampiccolo et al,2007).
Among 800 testing examples, our ap-proach achieved an accuracy of 60.6%.
This re-sult is on par with the performance of the me-dian system of accuracy 61.8% (z-test, p=0.63) inthe PASCAL-3 RTE Challenge.
Our current ap-proach is very lean on the use of external knowl-edge.
Its competitive performance sets up a rea-sonable baseline for our investigation on conversa-tion entailment.
This same system, modified to tai-lor linguistic characteristics of conversation (e.g.,removal of disfluency), was used as the baseline inour experiments.5.1 Event AlignmentTo understand the effect of conversation contextin the event alignment, we compared two configu-rations of alignment model for events.
The firstconfiguration is based on the clause representa-tion of semantics of utterances (as shown in Ap-pendix B).
This is the same configuration as usedin textual entailment.
The second configurationis based on representation of both semantics fromutterances and conversation context (as shown inAppendix C).
We evaluate how well each config-uration aligns the event terms based on the pair-wise alignment decision: for any event term tH inthe hypothesis and any term tD in the dialogue,whether the model can correctly predict that thetwo terms should be aligned.Figure 2(a) shows the comparison of F-measurebetween the two models.
Depending on the thresh-old of alignment prediction, the precision and re-call of the prediction vary.
When the thresh-old is lower, the models tend to give more align-ments, resulting in lower precision and higher re-call.
When the threshold is higher, the models tendto give fewer alignments, thus resulting in higherprecision but lower recall.
When the thresholdis around 0.5, the alignment reaches its best F-measure.
Regardless of what threshold is cho-sen, the model based on both utterance and con-text consistently works better.
Figure 2(b) showsthe breakdown based on the types of hypothesis (atthreshold 0.5).
The model that incorporates con-versation context consistently performs better forall types.
Its improvement is particularly signifi-cant for the intent type of hypothesis.These results are not surprising.
Many eventterms in hypotheses (e.g., suggest, think, etc.)
donot have their counterparts directly expressed inutterances in the dialogue discourse.
Only throughthe modeling of dialog acts, these terms can bealigned to potential pseudo terms in the dialogue211segment.
For the fact type hypotheses, the eventterms in the hypotheses generally have their coun-terparts in the dialogue discourse.
That explainswhy the improvement for the fact type using con-versation context is minimal.
(a) Overall comparison on F-measure(b) Comparison for different types of hypothesisFigure 2: Experimental results on event alignment5.2 Entailment PredictionGiven correct alignments, we further evaluatedentailment prediction based on three configura-tions of the inference model: (1) the same infer-ence model learned from the textual entailmentdata and tested on the PASCAL-3 RTE Challenge(Text); (2) an improved model incorporating anumber of features relevant to dialogues (espe-cially syntactic structure of utterances) based onrepresentations without conversation context as inAppendix B (+Dialogue); (3) a further improvedmodel based on augmented representations of con-versation context and using dialogue acts duringthe prediction of entailment as in Appendix C(+Context).System Acc Prec Recall FText 53.6% 71.6% 29.3% 41.6%+Dialogue 58.4% 84.1% 32.3% 46.7%+Context 67.7% 91.7% 47.0% 62.1%Table 1: Experimental results on entailment pre-dictionFor each configuration we present two evalua-tion metrics: an accuracy of the overall predictionand a precision-recall measurement for the posi-tive entailment examples.
All the evaluations areperformed on our development data, which has56.4% of positive examples and 43.6% of negativeexamples.The evaluations results are shown in Table 1.The system learned from textual entailment per-forms lower than the prediction based on themajority class (56.4%).
Incorporating syntacticfeatures of dialogues did better but the differ-ence is not statistically significant.
Incorporat-ing conversation context, especially dialogue acts,achieves significantly better performance (z-test,p < 0.005).Table 2 shows the comparison of the three con-figurations based on different types of hypothesis.As expected, the basic system trained on textualentailment is not capable for any intent type ofhypotheses.
Modeling conversation context withdialogue acts improves inference for all types ofhypothesis, with most significant improvement forthe belief, desire, and intent types of hypothesis.6 ConclusionThis paper describes our initial investigation onconversation entailment to address information ac-quisition about conversation participants.
Sincethere are so many variables involved in the pre-diction, our experiments have been focused on aset of development data where most of the featuresare annotated.
This allowed us to study the effectof conversation context in both alignment and en-tailment.
Our future work will enhance the cur-rent approach by training the models based on ourdevelopment data and evaluate them on the test-ing data.
Conversation entailment is an importanttask.
Although the current exercise is targeted toprocess conversation scripts from human-humanconversation, it can potentially benefit human ma-chine conversation by enabling automated agentsto gain better understanding of their conversation212Fact Belief Desire IntentSystem Acc F Acc F Acc F Acc FText 58.4% 51.3% 52.5% 37.3% 51.6% 34.8% 33.3% 0+Dialogue 68.6% 62.6% 53.5% 36.1% 48.4% 33.3% 33.3% 0+Context 70.8% 64.9% 67.7% 62.8% 58.1% 47.8% 62.5% 60.9%Table 2: Experimental results on entailment prediction for different types of hypothesespartners.AcknowledgmentsThis work was partially supported by IIS-0347548and IIS-0840538 from the National Science Foun-dation.
We thank the anonymous reviewers fortheir valuable comments and suggestions.ReferencesJames Allen.
1995.
Natural language understanding.The Benjamin/Cummings Publishing Company, Inc.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini, and IdanSzpektor.
2006.
The second pascal recognisingtextual entailment challenge.
In Proceedings of theSecond PASCAL Challenges Workshop on Recognis-ing Textual Entailment, Venice, Italy.Johan Bos and Katja Markert.
2005.
Recognising tex-tual entailment with logical inference.
In Proceed-ings of HLT-EMNLP, pages 628?635.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailmentchallenge.
In PASCAL Challenges Workshop onRecognising Textual Entailment.Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-yakanok, Dan Roth, and Mark Sammons.
2005.
Aninference model for semantic entailment in naturallanguage.
In Proceedings of AAAI.G.
Doddington, A. Mitchell, M. Przybocki, andL.
Ramshaw.
2004.
The automatic content extrac-tion (ace) programctasks, data, and evaluation.
InProceedings of the 4th International Conference onLanguage Resources and Evaluation (LREC).Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech:Use of bayesian networks to model pragmatic de-pendencies.
In Proceedings of ACL, pages 669?676.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third pascal recogniz-ing textual entailment challenge.
In Proceedings ofthe ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing, pages 1?9.John J. Godfrey and Edward Holliman.
1997.Switchboard-1 Release 2.
Linguistic Data Consor-tium, Philadelphia.Aria Haghighi, Andrew Ng, and Christopher Manning.2005.
Robust textual inference via graph matching.In Proceedings of HLT-EMNLP, pages 387?394.Eric Horvitz and Tim Paek.
2001.
Harnessing mod-els of users?
goals to mediate clarification dialog inspoken language systems.
In Proceedings of the 8thInternational Conference on User Modeling, pages3?13.Hongyan Jing, Nanda Kambhatla, and Salim Roukos.2007.
Extracting social networks and biographicalfacts from conversational speech transcripts.
In Pro-ceedings of ACL, pages 1040?1047.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of InternationalConference on Machine Learning, pages 296?304.Diane Litman and Katherine Forbes-Riley.
2006.
Rec-ognizing student emotions and attitudes on the basisof utterances in spoken tutoring dialogues with bothhuman and computer tutors.
Speech Communica-tion, 48(5):559?590.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Man-ning.
2006.
Learning to recognize features of validtextual entailments.
In Proceedings of HLT-NAACL,pages 41?48.Terence Parsons.
1990.
Events in the Semantics of En-glish.
A Study in Subatomic Semantics.
MIT Press.Rajat Raina, Andrew Y. Ng, and Christopher D. Man-ning.
2005.
Robust textual inference via learningand abductive reasoning.
In Proceedings of AAAI,pages 1099?1105.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Ng.
2008.
Cheap and fast ?
but is it good?evaluating non-expert annotations for natural lan-guage tasks.
In Proceedings of EMNLP, pages 254?263.Marta Tatu and Dan Moldovan.
2005.
A semantic ap-proach to recognizing textual entailment.
In Pro-ceedings of HLT-EMNLP, pages 371?378.213APPENDIXA Dependency Structure of Dialogue Utterances and Hypothesis in Example 5Dialogue Segment:B:Have youseenSleeping withthe Enemy?A:No.I 've heardthat'sreally great, though.B:Youhavetogoseethat one.x 1Ax 2x 3Ax 4x 5x 6though(?
)Ax 7x 8x 9x 10BAx 1x 2x 3Hypothesis:BsuggestsAtowatchSleeping withthe Enemy.termspredicatesB Clause Representation of Dialogue Segment and Hypothesis for Example 5subj(x1,B),obj(x1,A),obj(x1,x2),obj(x2,x3)x 1=suggests,x 2=watch,x 3=Sleepingwiththe Enemy, A, BHypothesis:subj(x 7,A), obj(x 7,x 8), obj(x 8,x 9), obj(x 9,x 10)x 7=have, x8=go, x9=see,x 10=one, AB:subj(x 4,A), obj(x 4,x 6), subj(x6,x5),though(x 4)x 4=haveheard,x 5=that,x 6=isreallygreat,AA:subj(x 2,A), obj(x 2,x 3), aux(x 2,x 1)x 1=have, x2=seen,x 3=Sleepingwiththe Enemy, AB:ClausesTermsDialogue Segment:C Augmented Clause Representation of Dialogue Segment in Example 5speaker(u4,B),content(u 4,x 7),act(u4,opinion),subj(x 7,A), obj(x 7,x 8), obj(x 8,x 9), obj(x 9,x 10)u 4,x7=have, x8=go, x9=see,x 10=one, A, BB:follow(u2,u1),follow(u 3,u 2),follow(u4,u3)speaker(u2,A),content(u 2,-), act(u 2,no_answer),speaker(u3,A),content(u 3,x 4),act(u3,statement),subj(x 4,A), obj(x 4,x 6), subj(x6,x5),though(x 4)u 2,u3,x4=haveheard,x 5=that,x 6=isreallygreat,AA:speaker(u1,B),content(u 1,x 2),act(u1,wh_question),subj(x 2,A), obj(x 2,x 3), aux(x 2,x 1)u 1,x1=have, x2=seen,x 3=Sleepingwiththe Enemy, A, BB:ClausesTermsDialogue Segment (withcontext representation):214D The Alignment for Example 5x 2=seenx 3=Sleepingwiththe EnemyBA x 5=thatx 7=haveu 4: act(u 4,opinion)Dialogue Segmentx 1=suggestsx 3=Sleepingwiththe EnemyBA x 2=watchHypothesisx 4=haveheardx 8=gox 9=seex 10=onex 6=isreallygreatu 3: act(u 3,statement)u 2: act(u 2,no_answer)u 1: act(u 1,wh_question)x 1=haveE The Prediction of Inference for the Hypothesis Clauses in Example 5x 3,x 5,x 10x 3x 2,x 9x 2AABByesyesyesyesHypothesisClauseEntailed?1321PathLengthobj(x9,x10)content(u 4,x 7),obj(x7,x8),obj(x8,x9)content(u 4,x 7),subj(x 7,A)speaker(u4,B)Shortest Pathbetween theAlignedTerms in theDependencyStructureofDialogue Segmentx 2,x 9u 4u 4u 4AlignedTerms in theDialogue Segmentx 2x 1x 1x 1Terms in thisClauserelationrelationrelationrelationClauseTypeobj(x2,x3)obj(x1,x2)obj(x1,A)subj(x 1,B)HypothesisClause215
