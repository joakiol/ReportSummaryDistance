Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 617?624Manchester, August 2008Investigating Statistical Techniques for Sentence-Level EventClassificationMartina NaughtonSchool of Computer Science,University College Dublin,Belfield, Dublin 4, IrelandNicola StokesNICTA Victoria Laboratory,University of Melbourne,Victoria, AustraliaJoe CarthySchool of Computer ScienceUniversity College DublinBelfield, Dublin 4, IrelandAbstractThe ability to correctly classify sentencesthat describe events is an important task formany natural language applications suchas Question Answering (QA) and Sum-marisation.
In this paper, we treat eventdetection as a sentence level text classifi-cation problem.
We compare the perfor-mance of two approaches to this task: aSupport Vector Machine (SVM) classifierand a Language Modeling (LM) approach.We also investigate a rule based methodthat uses hand crafted lists of terms derivedfrom WordNet.
These terms are stronglyassociated with a given event type, and canbe used to identify sentences describing in-stances of that type.
We use two datasets inour experiments, and evaluate each tech-nique on six distinct event types.
Our re-sults indicate that the SVM consistentlyoutperform the LM technique for this task.More interestingly, we discover that themanual rule based classification system isa very powerful baseline that outperformsthe SVM on three of the six event types.1 IntroductionEvent detection is a core Natural Language Pro-cessing (NLP) task that focuses on the automaticidentification and classification of various eventtypes in text.
This task has applications in au-tomatic Text Summararisation and Question An-swering (QA).
For example, event recognition isa core task in QA since the majority of web userc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.questions have been found to relate to events andsituations in the world (Saur??
et al, 2005).
Forcomplex questions such as How many people werekilled in Baghdad in March?, QA systems oftenrely on event detection systems to identify all rel-evant events in a set of documents before formu-lating an answer.
More recently, much research insummarisation has focused on the use of phrasalconcepts such as events to represent sentences inextractive summarisation systems.
Specifically,(Filatova and Hatzivassiloglou, 2004) use event-based features to represent sentences and showsthat this approach improves the quality of the finalsummaries when compared with a baseline bag-of-words approach.In this paper, we investigate the use of statisticalmethods for identifying the sentences in a docu-ment that describe one or more instances of a spec-ified event type.
We treat this task as a text classi-fication problem where each sentence in a givendocument is either classified as containing an in-stance of the target event or not.
We view thistask as a filtering step in a larger pipeline NLP ar-chitecture (e.g.
a QA system) which helps speedup subsequent processing by removing irrelevant,non-event sentences.Two event detection approaches are explored inthis paper.
More specifically, we train a SupportVector Machine (SVM) using a variety of term,lexical and additional event based features to en-code each train/test instance.
We also adopt a prob-abilistic language modeling approach that captureshow text within sentences that describe event in-stances is likely to be generated.
We estimate aseries of models using three well-known smooth-ing approaches, including LaPlace, Jelinek-Mercerand Absolute Discounting Smoothing.
Their over-all behavior on classification performance is ex-617amined.
One advantage of language modeling fortext classification is that instead of explicitly pre-computing features and selecting a subset basedon arbitrary decisions (as is often the case withstandard classification learning approaches such asan SVM), the language modeling approach simplyconsiders all terms occurring in the text as candi-date features, and implicitly considers the contri-bution of every feature in the final model.
Thus,language modeling approaches avoids a potentiallyerror-prone feature selection process.Event classification at a sentence level is a chal-lenging task.
For example, if the target event is?Die?, we want our system to extract sentenceslike ?5 people were killed in the explosion.?
and?A young boy and his mother were found dead onWednesday evening.?.
However, it also needs todetect complex cases like: ?An ambulance rushedthe soldier to hospital, but efforts to save himfailed.?
and reject instances like ?Fragmentationmines have a killing range of 100 feet.?.
It seemsintuitive that a na?
?ve system that selects only sen-tences that contain terms with senses connectedwith death like ?kill?, ?die?
or ?execute?
as pos-itive instances would catch many positive cases.However, there are instances where this approachwould fail.
In this work we evaluate the effective-ness of such a shallow NLP approach, by devel-oping a manual rule based system that finds sen-tences connected to a target event type using ahand crafted list of terms created with senses foundin WordNet.We use two datasets in our experiments.
Thefirst is the ACE 2005 Multilingual Training Corpus(Walker et al, 2006) that was annotated for 33 dif-ferent event types.
However, within the ACE datathe number of instances referring to each eventtype is somewhat limited.
For this reason, we se-lect the six types with the highest frequency in thedata.
These include ?Die?, ?Attack?, ?Transport?,?Meet?, ?Injure?
and ?Charge-indict?
types.
Thesecond corpus is a collection of articles from theIraq Body Count (IBC) database1annotated forthe ?Die?
event.
This dataset arose from a largerhumanitarian project that focuses on the collec-tion of fatalities statistics from unstructured newsdata.
We use this additional corpus to augment theamount of data used for training and testing the?Die?
event type, and to investigate the use of extratraining data on overall classification performance.1http://www.iraqbodycount.org/Overall, our results demonstrate that the trainedSVM proves to be more effective than the LMbased approach for this task across all event types.We also show that our baseline system, a handcrafted rule-based system, performs surprisinglywell.
The remainder of this paper is organised asfollows.
Section 2 covers related work.
We con-tinue with details of the datasets used in the exper-iments in Section 3.
Section 4 describes our eventclassification approaches while Section 5 presentstheir results.
We conclude with a discussion of ex-perimental observations and opportunities for fu-ture work in Section 6.2 Background and Related WorkEvent detection, in the context of news stories, hasbeen an active area of research for the best part often years.
For example, the NIST sponsored TopicDetection and Tracking (TDT) project, which be-gan in 1998 investigated the development of tech-nologies that could detect novel events in seg-mented or unsegmented news streams, and trackthe progression of these event over time (Allan etal., 1998).
Although this project ended in 2004,event detection is still investigated by more re-cently established projects such as the AutomaticContent Extraction (ACE) program, and in do-mains outside of news text such as BiomedicalText Processing (Murff et al, 2003).The aim of the TDT First Story Detection (FSD)or New Event Detection (NED) task was to flagdocuments that discuss breaking news stories asthey arrive on a news stream.
Dragon Systemsadopted a LM approach to this task (Allan et al,1998; Yamron et al, 2002) building discriminatortopic models from the collection and representingdocuments using unigram term frequencies.
Thenthey used a single-pass clustering algorithm to de-termine the documents that describe new events.The overall goal of the TDT Event Tracking taskwas to track the development of specific eventsover time.
However, these TDT tasks were some-what restrictive in the sense that detection is car-ried out at document level.
Our work differs fromTDT research since event detection is performedat a sentence level where the amount of data tobuild discriminate models for recognising event in-stances is far more limited.The goal of the ACE Event Detection andRecognition task is to identify all event instances(as well as the attributes and participants of each618Table 1: ACE Corpus StatisticsDie Injure Attack Meet Transport Charge-IndictNumber of Documents 154 50 235 84 181 43Avg.
document length 29.19 29.74 29.62 31.41 32.78 14.81Avg.
event instances per document 2.31 1.64 3.55 1.55 2.55 1.72Avg.
event instances per sentence 1.13 1.11 1.12 1.02 1.08 1.03Table 2: IBC Corpus StatisticsIBC CorpusNumber of Documents 332Number of Sources 77Avg.
document length 25.98Avg.
events per document 4.6Avg.
events per sentence 1.14instance) of a pre-specified set of event types.
AnACE event is defined as a specific occurence in-volving zero or more ACE entities2, values andtime expressions.
Two spans of text are used toidentify each event: the event trigger and the eventmention.
An event trigger or anchor is the wordthat most clearly expresses its occurrence.
In manycases, this will be the main verb in the event men-tion.
It can also appear as a noun (?The meetinglasted 5 hours.?)
or an adjective (?the dead men.
.
.
?).
The event mention is the sentence that de-scribes the event.
Even though the task of iden-tifying event mentions is not directly evaluated inACE, systems still need to identify them so thatthe various attributes and participants within themention can be extracted.
The algorithms evalu-ated in this paper can also be applied to the detec-tion of event mentions that contain the ACE events.Overall five sites participated in this task in 2005.The most similar work to that describe in this pa-per is detailed in (Ahn, 2006), who treats the taskof finding all event triggers (used to identify eachevent) as a word classification task where the taskis to classify every term in a document with a la-bel defined by 34 classes.
Features used includedvarious lexical, WordNet, dependency and relatedentity features.3 CorporaThe ACE 2005 Multilingual Corpus was annotatedfor Entities, Relations and Events.
It consists ofarticles originating from six difference sources in-cluding Newswire (20%), Broadcast News (20%),Broadcast Conversation (15%), Weblog (15%),2An ACE Entity is an entity identified using guidelinesoutlined by ACE Entity Detection and Recognition task.Usenet Newsgroups (15%) and ConversationalTelephone Speech (15%).
Statistics on the docu-ments in this collection are presented in Table 1.We evaluate our methods on the following eventtypes which have a high number of instances in thecollection: ?Die?, ?Attack?, ?Transport?, ?Meet?,?Injure?
and ?Charge-indict?.The data we use from the IBC database con-sists of Newswire articles gathered from 77 differ-ent news sources.
Statistics describing this datasetare contained in Table 2.
To obtain a gold standardset of annotations for articles in the IBC corpus,we asked ten volunteers to mark up all the ?Die?event instances.
To maintain consistency acrossboth datasets, events in the IBC corpus were iden-tified in a manner that conforms to the ACE an-notation guidelines.
In order to approximate thelevel of inter-annotation agreement achieved forthe IBC corpus, two annotators were asked to an-notate a disjoint set of 250 documents.
Inter-rateragreements were calculated using the kappa statis-tic that was first proposed by (Cohen, 1960).
Usingthe annotated data, a kappa score of 0.67 was ob-tained, indicating that while the task is difficult forhumans the data is still useful for our training andtest purposes.
Discrepancies were adjudicated andresolved by an independent volunteer.4 Event Detection as ClassificationWe treat the task of determining whether a givensentence describes an instance of the target eventas a binary text classification task where it is as-signed one of the following classes:?
On-Event Sentence: a sentence that containsone or more instances of the target event type.?
Off-Event Sentence: a sentence that does notcontain any instances of the target event type.4.1 A Machine Learning ApproachIn an attempt to develop a gold standard ap-proach for this task we use Support Vector Ma-chines (SVM) to automatically classify each in-stance as either an ?on-event?
or ?off-event?
sen-tence.
SVMs have been shown to be robust in619classification tasks involving text where the dimen-sionality is high (Joachims, 1998).
Each sentenceforms a train/test instance for our classifier and isencoded using the following set of features.Terms: Stemmed terms with a frequency in thetraining data greater than 2 were used as a termfeature.
Stopwords were not used as term features.Noun Chunks: All noun chunks (e.g.
?ameri-can soldier?)
with a frequency greater than 2 in thetraining data were also used as a feature.Lexical Information: The presence or absence ofeach part of speech (POS) tag and chunk tag wasused as a feature.
We use the Maximum EntropyPOS tagger and Chunker that are available with theC&C Toolkit (Curran et al, 2007).
The POS Tag-ger uses the standard set of grammatical categoriesfrom the Penn Treebank and the chunker recog-nises the standard set of grammatical chunk tags:NP, VP, PP, ADJP, ADVP and so on.Additional Features: We added the followingadditional features to the feature vector: sen-tence length, sentence position, presence/absenceof negative terms (e.g.
no, not, didn?t, don?t, isn?t,hasn?t), presence/absence of a modal terms (e.g.may, might, shall, should, must, will), a looka-head feature that indicates whether the next sen-tence is an event sentence, a look-back feature in-dicating whether or not the previous sentence isan event sentence and the presence/absence of atime-stamp.
Time-stamps were identified using in-house software developed by the Language Tech-nology Group at the University of Melbourne3.In the past, feature selection methods have beenfound to have a positive effect on classification ac-curacy of text classification tasks.
To examine theeffects of such techniques on this task, we use In-formation Gain (IG) to reduce the number of fea-tures used by the classifier by a factor of 2.4.2 Language Modeling ApproachesThe Language modeling approach presented hereis based on Bayesian decision theory.
Consider thesituation where we wish to classify a sentence skinto a category c ?
C = {C1.
.
.
.
.
.
C|C|}.
Oneapproach is to choose the category that has thelargest posterior probability given the training text:c?= argmaxc?C{Pr(c|sk)} (1)Specifically, we construct a language modelLM(ci) for each class ci.
All models built3http://www.cs.mu.oz.au/research/lt/are unigram models that use a maximum like-lihood estimator to approximate term probabili-ties.
According to this model (built from sentences{s1.
.
.
sm} belonging to class ciin the trainingdata) we can calculate the probability that term wwas generated from class cias:P (w|LM(ci)) =tf(w, ci)|ci|(2)where tf(w, ci) is the term frequency of term w inci(that is, {s1.
.
.
sm}) and |ci| is the total numberof terms in class ci.
We make the usual assump-tions that word co-occurences are independent.
Asa result, the probability of a sentence is the productof the probabilities of its terms.
We calculate theprobability that a given test sentence skbelongs toclass cias follows:P (sk|LM(ci)) =?w?skP (w|LM(ci)) (3)However, this model will generally under-estimate the probability of any unseen word in thesentence, that is terms that do not appear in thetraining data used to build the language model.
Tocombat this, smoothing techniques are used to as-sign a non-zero probability to the unseen words,which improves the accuracy of the overall termprobability estimation.
Many smoothing meth-ods have been proposed over the years, and ingeneral, they work by discounting the probabili-ties of seen terms and assign this extra probabilitymass to unseen words.
In IR, it has been foundthat the choice of smoothing method significantlyaffects retrieval performance (Zhai and Lafferty,2001; Kraaij and Spitters, 2003).
For this reason,we experiment with the Laplace, Jelinek-Mercerand Absolute Discounting Smoothing methods,and compare their effects on classification perfor-mance in Section 5.For this classification task, we normalise allnumeric references, locations, person names andorganisations to ?DIGIT?, ?LOC?, ?PER?, and?ORG?
respectively.
This helps to reduce the di-mensionality of our models, and improve theirclassification accuracy, particular in cases whereunseen instances of these entities occur in the testdata.4.3 Baseline MeasuresWe compare the performance our ML and LM ap-proaches to the following plausible baseline sys-tems: Random assigns each instance (sentence)620randomly to one of the possible classes.
WhileMajority Class Baseline assigns each instance tothe class that is most frequent in the training data.In our case, this is the ?off-event?
class.According to the ACE annotation guidelines4event instances are identified in the text by find-ing event triggers that explicitly mark the occur-rence of the event.
As a result, each event instancetagged in our datasets have a corresponding triggerthat the annotators used to identify it.
For exam-ple, terms like ?killing?, ?death?
and ?murder?
arecommon triggers used to identify the ?Die?
eventtype.
Therefore, we expect that a system that se-lects sentences containing one or more candidatetrigger terms as positive ?on-event?
sentences fora given event type, would be a suitable baselinefor this task.
To investigate this further we add thefollowing baseline system:Manual Trigger-Based Classification: For eachevent type, we use WordNet to manually createa list of terms that are synonyms or hyponyms(is a type of) of the event type.
For example, inthe case of the ?Meet?
and ?Die?
events commontrigger terms include {?encounter?, ?visit?, ?re-unite?}
and {?die?, ?suicide?, ?assassination?}
re-spectively.
We classify each sentence for a givenevent type as follows: if a sentence contains oneor more terms in the trigger list for that event typethen it is assigned to the ?on-event?
class for thattype.
Otherwise it is assigned to the ?off-event?class.
Table 3 contains the number of trigger termsused for each event5.Table 3: Trigger term lists for the six event types used in theexperiments.Event Type Number of triggers termsDie 29Transport 14Meet 12Injure 10Charge-Indict 8Attack 85 Evaluation Methodology & ResultsA standard measure for classification performanceis classification accuracy.
However for corporawhere the class distribution is skewed (as is thecase in our datasets where approx.
90% of the in-4Available at http://projects.ldc.upenn.edu/ace/annotation/5The lists of the trigger terms used for each event type areavailable at http://inismor.ucd.ie/?martina/stances belong to the ?off-event?
class) this mea-sure can be misleading.
So instead we have usedprecision, recall and F1 to evaluate each technique.If a is the number of sentences correctly classi-fied by a system to class i, b is the total num-ber of sentences classified to class i by a system,and c is the total number of human-annotated sen-tences in class i.
Then the precision and recallfor class i can be defined as follows: Preci=ab,Recalli=ac.
Finally, F1 (the harmonic mean be-tween precision and recall) for class i is defined asF1i=2?Preci?RecalliPreci+Recalli.
In the results presented inthis section we present the precision recall and F1for each class as well as the overall accuracy score.Results: In our experiments we use a relativelyefficient implementation of an SVM called the Se-quential Minimal Optimisation (SMO) algorithm(Platt, 1999) which is provided by the Weka frame-work (Witten and Frank, 2000).
Results presentedin this section are divided into two parts.
In thefirst part, all results were obtained using the IBCdataset where the target event type is ?Die?.
Weprovide a more detailed comparison of the perfor-mance of each algorithm using this type as moredata was available for it.
In the second section,we examine the effectiveness of each approach forall six event types (listed in Section 3) using theACE data.
All reported scores were generated us-ing 50:50 randomly selected train/test splits aver-aged over 5 runs.As part of the first set of results Table 4shows the precision, recall and F1 achieved forthe ?on-event?
and ?off-event?
classes as wellas the overall classification accuracy obtained byeach approach.
Two variations of the SVM werebuilt.
The first version (denoted in the tableby SVM(All Features IG)) was built using allterms, nouns chunks, lexical and additional fea-tures to encode each train/test instance where thefeatures were reduced using IG.
In the secondversion, the same features were used but no fea-ture reduction was carried out (denoted in thetable by SVM(All Features)).
LangModel(JM),LangModel(DS) and LangModel(LP) representlanguage models smoothed using Jelinek-Mercer,Discount Smoothing and LaPlace techniques re-spectively.
Overall these results suggest that theSVM using IG for feature selection is the most ef-fective method for correctly classifying both ?on-event?
and ?off-event?
sentences.
Specifically, itachieves 90.23% and 96.70% F1 score for these621Table 4: % Precision, Recall and F1 for both classes as well as the classification accuracy achieved by all algorithms using a50:50 train/test split where the target event type is ?Die?.AlgorithmOn-Event Class Off-Event ClassAccuracyPrecision Recall F1 Precision Recall F1SVM(All Features IG) 90.61 89.87 90.23 96.15 97.26 96.70 94.60SVM(All Features) 89.63 88.52 89.06 96.08 96.49 96.28 94.45Trigger-Based Classification 83.10 93.34 87.92 97.25 93.24 95.20 93.09LangModel(DS) 63.11 82.4 71.46 93.13 83.16 87.86 82.98LangModel(JM) 59.46 86.01 70.31 94.22 79.53 86.25 81.22LangModel(LP) 59.22 79.56 67.89 91.89 80.88 86.03 80.54Majority Class (?off-event?)
0.0 0.0 0.0 74.50 100.00 85.38 74.17Random 26.57 51.73 35.10 75.58 50.0 60.18 50.34Table 5: % F1 for both classes achieved by the SVM usingdifferent combinations of features.Features F1(On-Event) F1(Off-Event)terms 89.52 96.43terms + nc 89.58 96.31terms + nc + lex 89.62 96.44All Features 90.23 96.70classes respectively.
When IG is not used we see amarginal decrease of approx.
1% in these scores.The fact that both versions of the SVM obtainapprox.
90% F1 scores for the ?on-event?
classis extremely encouraging when you consider thelarge skew in class distribution that is present here(i.e., the majority of training instances belong tothe?off-event?
class).To examine the effects of the various features onoverall performance, we evaluated the SVM usingdifferent feature combinations.
These results areshown in Table 5 where ?terms?, ?nc?, and ?lex?denote the terms, noun chunks and lexical featuresets respectively.
?All Features?
includes these fea-tures and the ?Additional Features?
described inSection 4.1.
One obvious conclusion from this ta-ble is that terms alone prove to be the most valu-able features for this task.
Only a little increase inperformance is achieved by adding the other fea-ture sets.The graphs in Figure 1 shows the % F1 of bothclasses achieved by all methods using varying lev-els of training data.
From these graphs we seethat the SVM obtains over 80% F1 for the ?on-event?
class and over 90% F1 for the ?off-event?class when only 10% of the training data is used.These results increase gradually when the amountof training data increases.
For levels of train-ing data greater than 30% the SVM consistencyachieves higher F1 scores for both classes than allother methods for this task.In general, the language modeling based tech-niques are not as effective as the SVM approachfor this classification task.
However, from Ta-ble 4 we see that all language models achieve ap-prox.
70% F1 for the ?on-event?
class and ap-prox.
86% F1 for the ?off-event?
class when only50% of the IBC data is used to build the mod-els.
This is encouraging since they require littleor no feature engineering and less time to train.Models smoothed with the Laplace method tend tohave the least impact out of the three model vari-ations.
This is due to the fact that this methodassigns the same probability to all unseen terms.Thus, a term like ?professor?
that may only occuronce in the dataset has the same likelihood of oc-curring in an ?on-event?
sentence as a term like?kill?
that has a very high frequency in the dataset.In contrast, the Jelinek-Mercer and Absolute Dis-counting smoothing methods estimate the proba-bility of unseen terms according to a backgroundmodel built using the entire collection.
Therefore,the probabilities assigned to unseen words is pro-portional to their global distribution in the entirecorpus.
Consequently, the probabilities assignedto unseen terms tend to be more reliable approxi-mations of true term probabilities.Overall, the trigger-based classification base-line approach performs very well achieving simi-lar scores to the SVM.
This suggests that select-ing sentences with terms associated with the targetevent is an effective way of solving this problem.That said, it still makes mistakes that the SVM andlanguage models have the ability to correct.
Forexample, many sentences that contain terms like?suicide?
and ?killing?
as part of a noun phase (e.g.
?suicide driver?
or ?killing range?)
do not report adeath.
The trigger classification baseline will clas-sify these as an ?on-event?
instances whereas theSVM correctly places them in the ?off-event?
cat-egory.
More interesting are the cases missed bythe trigger classification baseline and SVM that are622203040506070809010010 20 30 40 50 60 70 80 90F1(On-Event)Percentage TrainingSVMTrigger-Based ClassificatonLangModel(LP)LangModel(JM)LangModel(DS)Random203040506070809010010 20 30 40 50 60 70 80 90F1(Off-Event)Percentage TrainingSVMTrigger-Based ClassificatonLangModel(LP)LangModel(JM)LangModel(DS)RandomFigure 1: % F1 for the on-event (top) and off-event (bottom)classes for all methods using varying levels of training datawhere the target event is ?Die?.corrected by the language models.
These includesentences like ?Three bodies were found yesterdayin central Baghdad.?
and ?If the Americans havekilled them, then why dont they show the tape.?.
Infact, it turns out that over 50% of the errors pro-duced by the SVM and manual trigger-based ap-proach when the target event is ?Die?
are classifiedcorrectly by the language models.
Although this isencouraging, the overall error rate of the languagemodeling approach is too high to rely on it alone.However, this evidence suggests that it may proveuseful in the future to somehow combine the pre-dictions of all three approaches in a way that im-proves overall classification performance.We now move on to the second part of the exper-iments.
Here, we present the results for six eventtypes (as listed in Section 3) using only data fromACE corpus.
Figure 2 shows the % F1 of the ?on-event?
class achieved by all approaches for eachevent type.
We have omitted the % F1 scores forthe ?off-event?
class as they do not vary signifi-cantly across event types and are similar to thosereported in Table 4.
The SVM, language mod-els, trigger-based and random baselines achieve020406080100Die Charge-Indict Meet Attack Injure TransportF1(On-Event)ACE Event TypesSVMTrigger-Based ClassificatonLangModel(LP)LangModel(JM)LangModel(DS)RandomFigure 2: % F1 of the ?on-event?
class achieved byall methods for the six ACE event types.approx.
96%, 95%, 86% and 60% ?off-event?
F1scores respectively across all event types.On the other hand, Figure 2 demonstrates thatthe performance of each approach for the ?on-event?
class varies considerably across the eventtypes.
For instance, the trigger-based classificationbaseline out-performs all other approaches achiev-ing over 60% F1 score for the ?Meet?, ?Die?
and?Charge-Indict?
types.
However for events like?Attack?
and ?Transport?
this baselines F1 scoredrops to approx.
20% thus achieving scores thatare only marginally above the random baseline.
In-terestingly, we notice that although it performs bet-ter for events like ?Meet?
and ?Charge-Indict?, thenumber of trigger terms used to detect these typesis much smaller than the number used for the ?At-tack?
and ?Transport?
types (see Table 3).
This in-dicates that event types where this simple baselineperforms well are those where the vocabulary usedto describe them is small.
Event types where itachieves poor results are broader types like ?Trans-port?
and ?Attack?
that cover a larger spectrum ofevent instances from heterogeneous contexts andsituations.
However, we see from Figure 2 that theSVM performs well on such event types and as aresult out-performs the trigger-based selection pro-cess by approximately a factor of 4 for the ?Attack?event and a factor of 2 for the ?Transport?
event.When we compare both datasets we find that theACE data is made up of newswire articles, Broad-cast news, broadcast conversational texts, weblogs,usenet newsgroup texts and conversational tele-phone speech that has been transcribed, whereasthe IBC corpus consists mainly of newswire arti-cles reporting fatalities during the Iraqi War.
As623a result, event instances in the ACE data describ-ing the ?Die?
event type are likely to report fatali-ties not only from Iraq but also from more diversecontexts and situations.
To investigate how perfor-mance differs for the ?Die?
event type across thesedatasets, we compare the IBC results in Table 4with the ACE results in Figure 2.
We find that theF1 scores of the ?off-event?
class are not affectedmuch.
However, the F1 scores for the ?on-event?class for the SVM and trigger-based baseline arereduced by margins of approx.
12% and 5% re-spectively.
We also notice that the performance ofthe unigram language models are reduced signifi-cantly by a factor of 2 indicating that they struggleto approximate accurate term probabilities whenthe vocabulary is more diverse and the amount oftraining data is limited.6 DiscussionSentence level event classification is an importantfirst step for many NLP applications such as QAand summarisation systems.
For each event typeused in our experiments we treated this as a binaryclassification task and compared a variety of ap-proaches for identifying sentences that describedinstances of that type.
The results showed that thetrained SVM was more effective than the languagemodeling approaches across all event types.
An-other interesting contribution of this paper is thatthe trigger-based classification baseline performedbetter than expected.
Specifically, for three of thesix event types it out-performed the trained SVM.This suggests that although there are cases wheresuch terms appear in sentences that do not describeinstances of a given type (for instance, ?The boywas nearly killed.?
), these cases are in the minor-ity.
However, the success of this baseline is some-what dependent on the nature of the event in ques-tion.
For broader events like ?Transport?
and ?At-tack?
where the trigger terms can be harder to pre-dict, it performs quiet poorly.
Therefore, as partof future work, we hope to investigate ways of au-tomating the creation of these term lists for a spec-ified event type as this proved to be an effectiveapproach to this task.Acknowledgements.
This research was sup-ported by the Irish Research Council for Science,Engineering & Technology (IRCSET) and IBMunder grant RS/2004/IBM/1.
The authors alsowishes to thank the members of the LanguageTechnology Research Group at the University ofMelbourne and NICTA for their helpful discus-sions regarding this research.ReferencesAhn, David.
2006.
The stages of event extraction.
In Pro-ceedings of the ACL Workshop on Annotating and Reason-ing about Time and Events, pages 1?8, Sydney, Australia,July.Allan, James, Jaime Carbonell, George Doddington, JonathonYamron, and Yiming Yang.
1998.
Topic detection andtracking pilot study.
final report.Cohen, Jacob.
1960.
A coeficient of agreement for nomi-nal scales.
Educational and Psychological Measurement,20(1):37?46.Curran, James, Stephen Clark, and Johan Bos.
2007.
Lin-guistically motivated large-scale nlp with c & c and boxer.In Proceedings of the ACL 2007 Demonstrations Session(ACL-07 demo), pages 29?32.Filatova, Elena and Vasileios Hatzivassiloglou.
2004.
Event-based extractive summarization.
In In Proceedings of ACLWorkshop on Summarization, pages 104 ?
111.Joachims, Thorsten.
1998.
Text categorization with supportvector machines: learning with many relevant features.
InN?edellec, Claire and C?eline Rouveirol, editors, Proceed-ings of the 10th ECML, pages 137?142, Chemnitz, DE.Springer Verlag, Heidelberg, DE.Kraaij, Wessel and Martijn Spitters.
2003.
Language modelsfor topic tracking.
In Croft, Bruce and John Lafferty, edi-tors, Language Models for Information Retrieval.
KluwerAcademic Publishers.Murff, Harvey, Vimla Patel, George Hripcsak, and DavidBates.
2003.
Detecting adverse events for patient safetyresearch: a review of current methodologies.
Journal ofBiomedical Informatics, 36(1/2):131?143.Platt, John.
1999.
Fast training of support vector machinesusing sequential minimal optimization.
Advances in kernelmethods: support vector learning, pages 185?208.Saur?
?, Roser, Robert Knippen, Marc Verhagen, and JamesPustejovsky.
2005.
Evita: a robust event recognizer forqa systems.
In HLT, pages 700?707.Walker, Christopher., Stephanie.
Strassel, Julie Medero, andLinguistic Data Consortium.
2006.
ACE 2005 Multilin-gual Training Corpus.
Linguistic Data Consortium, Uni-versity of Pennsylvania.Witten, Ian and Eibe Frank.
2000.
Data mining: practicalmachine learning tools and techniques with Java imple-mentations.
Morgan Kaufmann Publishers Inc.Yamron, JP, L. Gillick, P. van Mulbregt, and S. Knecht.
2002.Statistical models of topical content.
The Kluwer Interna-tional Series on Information Retrieval, pages 115?134.Zhai, Chengxiang and John Lafferty.
2001.
A study ofsmoothing methods for language models applied to ad hocinformation retrieval.
In Research and Development in In-formation Retrieval, pages 334?342.624
