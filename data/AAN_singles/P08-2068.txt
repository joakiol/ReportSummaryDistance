Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 269?272,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsText Segmentation with LDA-Based Fisher KernelQi Sun, Runxin Li, Dingsheng Luo and Xihong WuSpeech and Hearing Research Center, andKey Laboratory of Machine Perception (Ministry of Education)Peking University100871, Beijing, China{sunq,lirx,dsluo,wxh}@cis.pku.edu.cnAbstractIn this paper we propose a domain-independent text segmentation method,which consists of three components.
LatentDirichlet alocation (LDA) is employed tocompute words semantic distribution, and wemeasure semantic similarity by the Fisherkernel.
Finally global best segmentation isachieved by dynamic programming.
Experi-ments on Chinese data sets with the techniqueshow it can be effective.
Introducing latentsemantic information, our algorithm is robuston irregular-sized segments.1 IntroductionThe aim of text segmentation is to partition a doc-ument into a set of segments, each of which is co-herent about a specific topic.
This task is inspiredby problems in information retrieval, summariza-tion, and language modeling, in which the abilityto provide access to smaller, coherent segments ina document is desired.A lot of research has been done on text seg-mentation.
Some of them utilize linguistic criteria(Beeferman et al, 1999; Mochizuki et al, 1998),while others use statistical similarity measures touncover lexical cohesion.
Lexical cohesion meth-ods believe a coherent topic segment contains partswith similar vocabularies.
For example, the Text-Tiling algorithm, introduced by (Hearst, 1994), as-sumes that the local minima of the word similaritycurve are the points of low lexical cohesion and thusthe natural boundary candidates.
(Reynar, 1998)has proposed a method called dotplotting dependingon the distribution of word repetitions to find tightregions of topic similarity graphically.
One of theproblems with those works is that they treat termsuncorrelated, assigning them orthogonal directionsin the feature space.
But in reality words are corre-lated, and sometimes even synonymous, so that textswith very few common terms can potentially be onclosely related topics.
So (Choi et al, 2001; Brantset al, 2002) utilize semantic similarity to identifycohesion.
Unsupervised models of texts that capturesemantic information would be useful, particularlyif they could be achieved with a ?semantic kernel?
(Cristianini et al, 2001) , which computes the simi-larity between texts by also considering relations be-tween different terms.
A Fisher kernel is a functionthat measures the similarity between two data itemsnot in isolation, but rather in the context providedby a probability distribution.
In this paper, we usethe Fisher kernel to describe semantic informationsimilarity.
In addition, (Fragkou et al, 2004; Ji andZha, 2004) has treated this task as an optimizationproblem with global cost function and used dynamicprogramming for segments selection.The remainder of the paper is organized as fol-lows.
In section 2, after a brief overview of ourmethod, some key aspects of the algorithm are de-scribed.
In section 3, some experiments are pre-sented.
Finally conclusion and future research di-rections are drawn in section 4.2 MethodologyThis paper considers the sentence to be the smallestunit, and a block b is the segment candidate whichconsists of one or more sentences.
We employ LDA269model (Blei et al, 2003) in order to find out latentsemantic topics in blocks, and LDA-based Fisherkernel is used to measure the similarity of adjacentblocks.
Each block is then given a final score basedon its length and semantic similarity with its previ-ous block.
Finally the segmentation points are de-cided by dynamic programming.2.1 LDA ModelWe adopt LDA framework, which regards the cor-pus as mixture of latent topics and uses document asthe unit of topic mixtures.
In our method, the blocksdefined in previous paragraph are regarded as ?doc-uments?
in LDA model.The LDA model defines two corpus-level parame-ters ?
and ?.
In its generative process, the marginaldistribution of a document p(d|?, ?)
is given by thefollowing formula:?p(?|?
)(N?n=1?kp(zk|?d)p(wn|zk, ?
))d?where d is a word sequence (w1, w2, ...wN ) oflength N .
?
parameterizes a Dirichlet distributionand derives the document-related random variable?d, then we choose a topic zk, k ?
{1...K} from themultinomial distribution of ?d.
Word probabilitiesare parameterized by a k?V matrix ?
with V beingthe size of vocabulary and ?vk = P (w = v|zk).
Weuse variational EM (Blei et al, 2003) to estimate theparameters.2.2 LDA-Based Fisher KernelIn general, a kernel function k(x, y) is a way of mea-suring the resemblance between two data items xand y.
The Fisher kernel?s key idea is to derive a ker-nel function from a generative probability model.
Inthis paper we follow (Hofmann, 2000) to considerthe average log-probability of a block, utilizing theLDA model.
The likelihood of b is given by:l(b) =N?i=1P?
(wi|b) logK?k=1?wik?
(k)bwhere the empirical distribution of words in theblock P?
(wi|b) can be obtained from the number ofword-block co-occurrence n(b, wi), normalized bythe length of the block.The Fisher kernel is defined asK(b1, b2) = 5T?
l(b1)I?1 5?
l(b2)which engenders a measure of similarity betweenany two blocks b1 and b2.
The derivation of thekernel is quite straightforward and following (Hof-mann, 2000) we finally have the result:K(b1, b2) = K1(b1, b2) +K2(b1, b2), withK1(b1, b2) =?k?
(k)b1 ?
(k)b2 /?
(k)corpusK2(b1, b2) =?i P?
(wi|b1)P?
(wi|b2)?kP (zk|b1,wi)P (zk|b2,wi)P (wi|zk)where K1(b1, b2) is a measure of how much b1 andb2 share the same latent topic, taking synonymyinto account.
And K2(b1, b2) is the traditional innerproduct of common term frequencies, but weightedby the degree to which these terms belong to thesame latent topic, taking polysemy into account.2.3 Cost Function and Dynamic ProgrammingThe local minima of LDA-based Fisher kernel sim-ilarities indicate low semantic cohesion and seg-mentation candidates, which is not enough to getreasonably-sized segments.
The lengths of segmen-tation candidates have to be considered, thus webuild a cost function including two parts of infor-mation.
Segmentation points can be given in termsof a vector ~t = (t0, ..., tm, ..., tM ), where tm is thesentence label with m indicating the mth block.
Wedefine a cost function as follows:J(~t;?)
=M?m=1?F (ltm+1,tm+1)+ K(btm?1+1,tm , btm+1,tm+1)where F (ltm+1,tm+1) is equal to(ltm+1,tm+1??
)22?2 andltm+1,tm+1 is equal to tm+1?tm indicating the num-ber of sentences in block m. The LDA-based ker-nel function measures similarity of block m?
1 andblock m, where block m?1 spans sentence tm?1+1to tm and block m spans sentence tm + 1 to tm+1The cost function is the sum of the costs of as-sumed unknown M segments, each of which ismade up of the length probability of block m and thesimilarity score of block m with its previous blockm ?
1.
The optimal segmentation ~t gives a globalminimum of J(~t;?
).2703 Experiments3.1 PreparationIn our experiments, we evaluate the performance ofour algorithms on Chinese corpus.
With news docu-ments from Chinese websites, collected from 10 dif-ferent categories, we design an artificial test corpusin the similar way of (Choi, 2000), in which wetake each n-sentence document as a coherent topicsegment, randomly choose ten such segments andconcatenate them as a sample.
Three data sets, Set3-5, Set 13-15 and Set 5-20, are prepared in our ex-periments, each of which contains 100 samples.
Thedata sets?
names are represented by a range numbern of sentences in a segment.Due to generality, we take three indices to eval-uate our algorithm: precision, recall and error ratemetric (Beeferman et al, 1999) .
And all exper-imental results are averaged scores generated fromthe individual results of different samples.
In orderto determine appropriate parameters, some hold-outdata are used.We compare the performance of our methods withthe algorithm in (Fragkou et al, 2004) on our testset.
In particular, the similarity representation is amain difference between those two methods.
Whilewe pay attention to latent topic information behindwords of adjacent blocks, (Fragkou et al, 2004) cal-culates word density as the similarity score function.3.2 ResultsIn order to demonstrate the improvement of LDA-based Fisher kernel technique in text similarity eval-uation, we omit the length probability part in the costfunction and compare the LDA-based Fisher kerneland the word-frequency cosine similarity by the er-ror rate Pk of segmenting texts.
Figure 1 showsthe error rates for different sets of data.
On av-erage, the error rates are reduced by as much asabout 30% over word-frequency cosine similaritywith our methods, which shows Fisher kernel sim-ilarity measure,with latent topic information addedby LDA, outperforms traditional word similaritymeasure.
The performance comparisons drawn fromSet 3-5 and Set 13-15 indicates that our similarity al-gorithm can uncover more descriptive statistics thantraditional one especially for segments with less sen-tences due to its prediction on latent topics.set 3-5 set  13-15 set 5-200.000.050.100.150.200.250.300.35PkLDA-based Fisher kernelWord-Frequency Cosine SimilarityFigure 1: Error Rate Pk on different data sets with differ-ent similarity metrics.In the cost function, there are three parameters ?, ?
and ?.
We determine appropriate ?
and ?
withhold-out data.
For the value of ?, we take it between0 and 1 because the length part is less important thanthe similarity part according to our preliminary ex-periments.
We design the experiment to study ?
?simpact on segmentation by varying it over a certainrange.
Experimental results in Figure 2 show thatthe reduce of error rate achieved by our algorithmis in a range from 14.71% to 53.93%.
Set 13-15achieves best segmentation performance, which in-dicates the importance of text structure: it is easierto segment the topic with regular length and moresentences.
The performance on Set 5-20 obtains thebest improvement with our methods, which illus-trates that LDA-based Fisher kernel can express textsimilarity more exactly than word density similarityon irregular-sized segments.Table 1: Evaluation against different algorithms on Set5-20.Algo.
Pk Recall PrecisionTextTiling 0.226 66.00% 60.72 %P.
Fragkou Algo.
0.344 69.00% 37.92 %Our Algo.
0.205 59.00% 62.27 %While most experiments of other authors weretaken on short regular-sized segments which wasfirstly presented by (Choi, 2000), we use compar-atively long range of segments, Set 5-20, to evaluatedifferent algorithms.
Table 1 shows that, in terms of2710 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.900.050.10.150.20.250.30.350.40.450.5lambdaPkSet 3?5Set 13?15Set 5?20Set 3?5Set 13?15Set 5?20Figure 2: Error Rate Pk when the ?
changes.
There aretwo groups of lines, the solid lines representing algorithmof (Fragkou et al, 2004) while the dash ones indicateperformance of our algorithm, and each line in a groupshows error rates in different data sets.Pk, our algorithm employing dynamic programmingas P. Fragkou Algo.
achieves the best performanceamong those three.
As for long irregular-sized textsegmentation, although local even-sized blocks sim-ilarity provides more exact information than the sim-ilarity between global irregular-sized texts, with theconsideration of latent topic information, the latterwill perform better in the task of text segmentation.Though the performance of the proposed method isnot superior to TextTiling method, it avoids thresh-olds selection, which makes it robust in applications.4 Conclusions and Future WorkWe present a new method for topic-based text seg-mentation that yields better results than previouslymethods.
The method introduces a LDA-basedFisher kernel to exploit text semantic similarities andemploys dynamic programming to obtain global op-timization.
Our algorithm is robust and insensitiveto the variation of segment length.
In the future,we plan to investigate more other similarity mea-sures based on semantic information and to dealwith more complicated segmentation tasks.
Also,we want to exam the factor importance of similar-ity and length in this text segmentation task.AcknowledgmentsThe authors would like to thank Jiazhong Nie for his helpand constructive suggestions.
The work was supportedin part by the National Natural Science Foundation ofChina (60435010; 60535030; 60605016), the NationalHigh Technology Research and Development Program ofChina (2006AA01Z196; 2006AA010103), the NationalKey Basic Research Program of China (2004CB318005),and the New-Century Training Program Foundation forthe Talents by the Ministry of Education of China.ReferencesDoug Beeferman, Adam Berger and John D. Lafferty.1999.
Statistical Models for Text Segmentation.
Ma-chine Learning, 34(1-3):177?210.David M. Blei and Andrew Y. Ng and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of machineLearning Research 3: 993?1022.Thorsten Brants, Francine Chen and Ioannis Tsochan-taridis.
2002.
Topic-Based Document Segmentationwith Probabilistic Latent Semantic Analysis.
CIKM?02211?218Freddy Choi, Peter Wiemer-Hastings and JohannaMoore.
2001.
Latent Semantic Analysis for Text Seg-mentation.
Proceedings of 6th EMNLP, 109?117.Freddy Y. Y. Choi.
2000.
Advances in Domain Inde-pendent Linear Text Segmentation.
Proceedings ofNAACL-00.Nello Cristianini, John Shawe-Taylor and Huma Lodhi.2001.
Latent Semantic Kernels.
Proceedings ofICML-01, 18th International Conference on MachineLearning 66?73.Pavlina Fragkou, Petridis Vassilios and Kehagias Athana-sios.
2004.
A Dynamic Programming Algorithm forLinear Text Segmentation.
J. Intell.
Inf.
Syst., 23(2):179?197.Marti Hearst.
1994.
Multi-Paragraph Segmentation ofExpository Text.
Proceedings of the 32nd.
AnnualMeeting of the ACL, 9?16.Thomas Hofmann.
2000.
Learning the Similarity ofDocuments: An Information-Geometric Approach toDocument Retrieval and Categorization.
Advances inNeural Information Processing Systems 12: 914?920.Xiang Ji and Hongyuan Zha.
2003.
Domain-Independent Text Segmentation Using AnisotropicDiffusion and Dynamic Programming.
Proceedingsof the 26th annual international ACM SIGIR Confer-ence on Research and Development in Informaion Re-trieval, 322?329.Hajime Mochizuki, Takeo Honda and Manabu Okumura.1998.
Text Segmentation with Multiple Surface Lin-guistic Cues.
Proceedings of the COLING-ACL?98,881-885.Jeffrey C. Reynar.
1998.
Topic Segmentation: Algo-rithms and Applications.
PhD thesis.
University ofPennsylvania.272
