Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1?6,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsExploring the Relative Role of Bottom-up and Top-down Information inPhoneme LearningAbdellah Fourtassi1, Thomas Schatz1,2, Balakrishnan Varadarajan3, Emmanuel Dupoux11Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris, France2SIERRA Project-Team, INRIA/ENS/CNRS, Paris, France3Center for Language and Speech Processing, JHU, Baltimore, USA{abdellah.fourtassi; emmanuel.dupoux; balaji.iitm1}@gmailthomas.schatz@laposte.netAbstractWe test both bottom-up and top-down ap-proaches in learning the phonemic statusof the sounds of English and Japanese.
Weused large corpora of spontaneous speechto provide the learner with an input thatmodels both the linguistic properties andstatistical regularities of each language.We found both approaches to help dis-criminate between allophonic and phone-mic contrasts with a high degree of accu-racy, although top-down cues proved to beeffective only on an interesting subset ofthe data.1 IntroductionDevelopmental studies have shown that, duringtheir first year, infants tune in on the phonemic cat-egories (consonants and vowels) of their language,i.e., they lose the ability to distinguish somewithin-category contrasts (Werker and Tees, 1984)and enhance their ability to distinguish between-category contrasts (Kuhl et al, 2006).
Currentwork in early language acquisition has proposedtwo competing hypotheses that purport to accountfor the acquisition of phonemes.
The bottom-uphypothesis holds that infants converge on the lin-guistic units of their language through a similarity-based distributional analysis of their input (Mayeet al, 2002; Vallabha et al, 2007).
In contrast,the top-down hypothesis emphasizes the role ofhigher level linguistic structures in order to learnthe lower level units (Feldman et al, 2013; Mar-tin et al, 2013).
The aim of the present work isto explore how much information can ideally bederived from both hypotheses.The paper is organized as follows.
First we de-scribe how we modeled phonetic variation fromaudio recordings, second we introduce a bottom-up cue based on acoustic similarity and top-down cues based of the properties of the lexicon.We test their performance in a task that consistsin discriminating within-category contrasts frombetween-category contrasts.
Finally we discussthe role and scope of each cue for the acquisitionof phonemes.2 Modeling phonetic variationIn this section, we describe how we modeled therepresentation of speech sounds putatively pro-cessed by infants, before they learn the relevantphonemic categories of their language.
FollowingPeperkamp et al (2006), we make the assumptionthat this input is quantized into context-dependentphone-sized unit we call allophones.
Consider theexample of the allophonic rule that applies to theFrench /r/:/r/?
{[X] / before a voiceless obstruent[K] elsewhereFigure 1: Allophonic variation of French /r/The phoneme /r/ surfaces as voiced ([K]) beforea voiced obstruent like in [kanaK Zon] (?canardjaune?, yellow duck) and as voiceless ([X]) beforea voiceless obstruent as in [kanaX puXpK] (?ca-nard pourpre?, purple duck).
Assuming speechsounds are coded as allophones, the challenge fac-ing the learner is to distinguish the allophonic vari-ation ([K], [X]) from the phonemic variation (re-lated to a difference in the meaning) like the con-trast ([K],[l]).Previous work has generated allophonic varia-tion using random contexts (Martin et al, 2013).This procedure does not take into account the factthat contexts belong to natural classes.
In addition,it does not enable to compute an acoustic distance.Here, we generate linguistically and acousticallycontrolled allophones using Hidden Markov Mod-els (HMMs) trained on audio recordings.12.1 CorporaWe use two speech corpora: the Buckeye Speechcorpus (Pitt et al, 2007), which consists of 40hours of spontaneous conversations with 40 speak-ers of American English, and the core of the Cor-pus of Spontaneous Japanese (Maekawa et al,2000) which also consists of about 40 hours ofrecorded spontaneous conversations and publicspeeches in different fields.
Both corpora are time-aligned with phonetic labels.
Following Boruta(2012), we relabeled the japanese corpus using 25phonemes.
For English, we used the phonemicversion which consists of 45 phonemes.2.2 Input generation2.2.1 HMM-based allophonesIn order to generate linguistically and acousticallyplausible allophones, we apply a standard HiddenMarkov Model (HMM) phoneme recognizer witha three-state per phone architecture to the signal,as follows.First, we convert the raw speech waveform ofthe corpora into successive vectors of Mel Fre-quency Cepstrum Coefficients (MFCC), computedover 25 ms windows, using a period of 10 ms(the windows overlap).
We use 12 MFCC coeffi-cients, plus the energy, plus the first and second or-der derivatives, yielding 39 dimensions per frame.Second, we start HMM training using one three-state model per phoneme.
Third, each phonememodel is cloned into context-dependent triphonemodels, for each context in which the phonemeactually occurs (for example, the phoneme /A/ oc-curs in the context [d?A?g] as in the word /dAg/(?dog?).
The triphone models are then retrained ononly the relevant subset of the data, correspondingto the given triphone context.
These detailed mod-els are clustered back into inventories of varioussizes (from 2 to 20 times the size of the phone-mic inventory) using a linguistic feature-based de-cision tree, and the HMM states of linguisticallysimilar triphones are tied together so as to max-imize the likelihood of the data.
Finally, the tri-phone models are trained again while the initialgaussian emission models are replaced by mix-ture of gaussians with a progressively increasingnumber of components, until each HMM state ismodeled by a mixture of 17 diagonal-covariancegaussians.
The HMM were built using the HMMToolkit (HTK: Young et al, 2006).2.2.2 Random allophonesAs a control, we also reproduce the random al-lophones of Martin et al (2013), in which allo-phonic contexts are determined randomly: for agiven phoneme /p/, the set of all possible con-texts is randomly partitioned into a fixed numbern of subsets.
In the transcription, the phoneme /p/is converted into one of its allophones (p1,p2,..,pn)depending on the subset to which the current con-text belongs.3 Bottom-up and top-down hypotheses3.1 Acoustic cueThe bottom-up cue is based on the hypothesis thatinstances of the same phoneme are likely to beacoustically more similar than instances of twodifferent phonemes (see Cristia and Seidl, in press)for a similar proposition).
In order to providea proxy for the perceptual distance between al-lophones, we measure the information theoreticdistance between the acoustic HMMs of these al-lophones.
The 3-state HMMs of the two allo-phones were aligned with Dynamic Time Warping(DTW), using as a distance between pairs of emit-ting states, a symmetrized version of the Kullback-Leibler (KL) divergence measure (each state wasapproximated by a single non-diagonal Gaussian):A(x, y) =?
(i,j)?DTW (x,y)KL(Nxi||Nyj) +KL(Nyj||Nxi)Where {(i, j) ?
DTW (x, y)} is the set of in-dex pairs over the HMM states that correspond tothe optimal DTW path in the comparison betweenphone model x and y, and Nxithe full covarianceGaussian distribution for state i of phone x. Forobvious reasons, the acoustic distance cue cannotbe computed for Random allophones.3.2 Lexical cuesThe top-down information we use in this study, isbased on the insight of Martin et al (2013).
It restson the idea that true lexical minimal pairs are notvery frequent in human languages, as compared tominimal pairs due to mere phonological processes.In fact, the latter creates variants (alternants) of thesame lexical item since adjacent sounds conditionthe realization of the first and final phoneme.
Forexample, as shown in figure 1, the phoneme /r/ sur-faces as [X] or [K] depending on whether or not the2next sound is a voiceless obstruent.
Therefore, thelexical item /kanar/ surfaces as [kanaX] or [kanaK].The lexical cue assumes that a pair of words dif-fering in the first or last segment (like [kanaX] and[kanaK]) is more likely to be the result of a phono-logical process triggered by adjacent sounds, thana true semantic minimal pair.However, this strategy clearly gives rise to falsealarms in the (albeit relatively rare) case of trueminimal pairs like [kanaX] (?duck?)
and [kanal](?canal?
), where ([X], [l]) will be mistakenly la-beled as allophonic.In order to mitigate the problem of false alarms,we also use Boruta (2011)?s continuous version,where each pair of phones is characterized by thenumber of lexical minimal pairs it forms.B(x, y) = |(Ax,Ay) ?
L2|+ |(xA, yA) ?
L2|where {Ax ?
L} is the set of words in the lex-icon L that end in the phone x, and {(Ax,Ay) ?L2} is the set of phonological minimal pairs inL?
L that vary on the final segment.In addition, we introduce another cue that couldbe seen as a normalization of Boruta?s cue:N (x, y) =|(Ax,Ay)?L2|+|(xA,yA)?L2||{Ax?L}|+|{Ay?L}|+|{xA?L}|+|{yA?L}|4 Experiment4.1 TaskFor each corpus we list all the possible pairs ofattested allophones.
Some of these pairs are allo-phones of the same phoneme (allophonic pair) andothers are allophones of different phonemes (non-allophonic pairs).
The task is a same-differentclassification, whereby each of these pairs is givena score from the cue that is being tested.
A goodcue gives higher scores to allophonic pairs.4.2 EvaluationWe use the same evaluation procedure as in Mar-tin et al (2013).
It is carried out by computingthe area under the curve of the Receiver Operat-ing Characteristic (ROC).
A value of 0.5 repre-sents chance and a value of 1 represents perfectperformance.In order to lessen the potential influence of thestructure of the corpus (mainly the order of the ut-terances) on the results, we use a statistical resam-pling scheme.
The corpus is divided into smallblocks (of 20 utterances each).
In each run, wedraw randomly with replacement from this set ofblocks a sample of the same size as the originalcorpus.
This sample is then used to retrain theacoustic models and generate a phonetic inven-tory that we use to re-transcribe the corpus andre-compute the cues.
We report scores averagedover 5 such runs.4.3 ResultsTable 1 shows the classification scores for the lex-ical cues when we vary the inventory size from2 allophones per phoneme in average, to 20 al-lophones per phoneme, using the Random allo-phones.
The top-down scores are very high, repli-cating Martin et al?s results, and even improvingthe performance using Boruta?s cue and our newNormalized cue.?
English JapaneseAllo./phon.
M B N M B N2 0.784 0.935 0.951 0.580 0.989 1.005 0.845 0.974 0.982 0.653 0.978 0.99110 0.886 0.974 0.981 0.733 0.944 0.97120 0.918 0.961 0.966 0.785 0.869 0.886Table 1 : Same-different scores for top-down cues onRandom allophones, as a function of the average number ofallophones per phoneme.
M=Martin et al, B=Boruta, N=NormalizedTable 2 shows the results for HMM-based allo-phones.
The acoustic score is very accurate forboth languages and is quite robust to variation.Top-down cues, on the other hand, perform, sur-prisingly, almost at chance level in distinguish-ing between allophonic and non-allophonic pairs.A similar discrepancy for the case of Japanesewas actually noted, but not explained, in Boruta(2012).?
English JapaneseAllo./phon.
A M B N A M B N2 0.916 0.592 0.632 0.643 0.885 0.422 0.524 0.5375 0.918 0.592 0.607 0.611 0.908 0.507 0.542 0.55110 0.893 0.569 0.571 0.571 0.827 0.533 0.546 0.54820 0.879 0.560 0.560 0.559 0.876 0.541 0.543 0.543Table 2 : Same-different scores for bottom-up and top-downcues on HMM-based allophones, as a function of theaverage number of allophones per phoneme.
A=Acoustic,M=Martin et al, B=Boruta, N= Normalized5 Analysis5.1 Why does the performance drop forrealistic allophones?When we list all possible pairs of allophones inthe inventory, some of them correspond to lexi-3cal alternants ([X], [K]) ?
([kanaX] and [kanaK]),others to true minimal pairs ([K], [l]) ?
([kanaK]and [kanal]), and yet others will simply not gen-erate lexical variation at all, we will call those:invisible pairs.
For instance, in English, /h/ and/N/ occur in different syllable positions and thuscannot appear in any minimal pair.
As definedabove, top-down cues are set to 0 in such pairs(which means that they are systematically classi-fied as non-allophonic).
This is a correct decisionfor /h/ vs. /N/, but not for invisible pairs that alsohappen to be allophonic, resulting in false nega-tives.
In tables 3, we show that, indeed, invisiblepairs is a major issue, and could explain to a largeextent the pattern of results found above.
In fact,the proportion of visible allophonic pairs (?allo?column) is way lower for HMM-based allophones.This means that the majority of allophonic pairs inthe HMM case are invisible, and therefore, will bemistakenly classified as non-allophonic.?
Random HMM?
English Japanese English JapaneseAllo./phon.
allo ?
allo allo ?
allo allo ?
allo allo ?
allo2 92.9 36.3 100 83.9 48.9 25.3 37.1 53.25 97.2 28.4 99.6 69.0 31.1 14.3 25.0 25.910 96.8 19.9 96.7 50.1 19.8 4.23 21.0 14.420 94.3 10.8 83.4 26.4 14.0 1.89 12.4 4.04Table 3 : Proportion (in %) of allophonic pairs (allo), andnon-allophonic pairs (?
allo) associated with at least onelexical minimal pair, in Random and HMM allophones.There are basically two reasons why an allo-phonic pair would be invisible ( will not generatelexical alternants).
The first one is the absence ofevidence, e.g., if the edges of the word with theunderlying phoneme do not appear in enough con-texts to generate the corresponding variants.
Thishappens when the corpus is so small that no wordending with, say, /r/ appears in both voiced andvoiceless contexts.
The second, is when the allo-phones are triggered on maximally different con-texts (on the right and the left) as illustrated below:/p/?
{[p1] / A B[p2] / C DWhen A doesn?t overlap with C and B does notoverlap with D, it becomes impossible for the pair([p1], [p2]) to generate a lexical minimal pair.
Thisis simply because a pair of allophones needs toshare at least one context to be able to form vari-ants of a word (the second or penultimate segmentof this word).When asked to split the set of contexts in twodistinct categories that trigger [p1] and [p2] (i.e.,A B and C D), the random procedure will of-ten make A overlap with B and C overlap with Dbecause it is completely oblivious to any acous-tic or linguistic similarity, thus making it alwayspossible for the pair of allophones to generate lex-ical alternants.
A more realistic categorization(like the HMM-based one), will naturally tend tominimize within-category distance, and maximizebetween-category distance.
Therefore, we willhave less overlap, making the chances of the pairto generate a lexical pair smaller.
The more al-lophones we have, the bigger is the chance to endup with non-overlapping categories (invisible allo-phonic pairs), and the more mistakes will be made,as shown in Table 3.5.2 Restricting the role of top-down cuesThe analysis above shows that top-down cues can-not be used to classify all contrasts.
The approxi-mation that consists in considering all pairs that donot generate lexical pairs as non-allophonic, doesnot scale up to realistic input.
A more intuitive,but less ambitious, assumption is to restrict thescope of top-down cues to contrasts that do gen-erate lexical variation (lexical alternants or trueminimal pairs).
Thus, they remain completely ag-nostic to the status of invisible pairs.
This restric-tion makes sense since top-down information boilsdown to knowing whether two word forms belongto the same lexical category (reducing variation toallophony), or to two different categories (varia-tion is then considered non-allophonic).
Phoneticvariation that does not cause lexical variation is, inthis particular sense, orthogonal to our knowledgeabout the lexicon.We test this hypothesis by applying the cuesonly to the subset of pairs that are associated withat least one lexical minimal pair.
We vary the num-ber of allophones per phoneme on the one hand(Table 4) and the size of the input on the otherhand (Table 5).
We refer to this subset by an aster-isk (*), by which we also mark the cues that applyto it.
Notice that, in this new framing, the M cue iscompletely uninformative since it assigns the samevalue to all pairs.As predicted, the cues perform very well on thissubset, especially the N cue.
The combination oftop-down and bottom-up cues shows that the for-mer is always useful, and that these two sources of4?
English Japanese?
?
Individual cues Combination ?
Individual cues CombinationAllo./phon.
* (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*2 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.9464 14.3 0.918 0.964 0.858 0.951 0.975 0.991 30.88 0.908 0.917 0.850 0.936 0.934 0.97610 4.24 0.893 0.937 0.813 0.939 0.960 0.968 16.06 0.827 0.839 0.899 0.957 0.904 0.93620 1.67 0.879 0.907 0.802 0.907 0.942 0.940 5.02 0.876 0.856 0.882 0.959 0.913 0.950Table 4 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of averagenumber of allophones per phonemes.?
English Japanese?
?
Individual cues Combination ?
Individual cues CombinationSize (hours) * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*1 9.87 0.885 0.907 0.741 0.915 0.927 0.969 34.78 0.890 0.883 0.835 0.915 0.889 0.9344 18.3 0.918 0.958 0.798 0.917 0.967 0.989 48.00 0.917 0.939 0.860 0.937 0.938 0.9738 21.3 0.916 0.964 0.837 0.942 0.971 0.992 51.71 0.915 0.940 0.889 0.937 0.954 0.97720 24.4 0.911 0.960 0.827 0.936 0.969 0.994 58.12 0.921 0.954 0.865 0.912 0.945 0.97140 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.946?
34.82 ?
?
?
?
?
?
72.16 ?
?
?
?
?
?Table 5 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of corpus size.
* (%) refers to the proportion of the subset of contrasts associated with at least one minimal pair.
The cues applied to thissubset are marked with an asterisk (*)information are not completely redundant.
How-ever, the scope of top-down cues (the proportion ofthe subset * ) shrinks as we increase the number ofallophones.
Table 5 shows that this problem can,in principle, be mitigated by increasing the amountof data available to the learner.
As we were limitedto only 40 hours of speech, we generated an artifi-cial corpus that uses the same lexicon but with allpossible word orders so as to maximize the num-ber of contexts in which words appear.
This artifi-cial corpus increases the proportion of the subset,but we are still not at 100 % coverage, which ac-cording the analysis above, is due (at least in part)to the irreducible set of non-overlapping pairs.6 ConclusionIn this study we explored the role of both bottom-up and top-down hypotheses in learning thephonemic status of the sounds of two typologicallydifferent languages.
We introduced a bottom-upcue based on acoustic similarity, and we used al-ready existing top-down cues to which we pro-vided a new extension.
We tested these hypothe-ses on English and Japanese, providing the learnerwith an input that mirrors closely the linguisticand acoustic properties of each language.
Weshowed, on the one hand, that the bottom-up cue isa very reliable source of information, across differ-ent levels of variation and even with small amountof data.
Top-down cues, on the other hand, werefound to be effective only on a subset of the data,which corresponds to the interesting contrasts thatcause lexical variation.
Their role becomes morerelevant as the learner gets more linguistic experi-ence, and their combination with bottom-up cuesshows that they can provide non-redundant infor-mation.
Note, finally, that even if this work isbased on a more realistic input compared to previ-ous studies, it still uses simplifying assumptions,like ideal word segmentation, and no low-levelacoustic variability.
Those assumptions are, how-ever, useful in quantifying the information that canideally be extracted from the input, which is a nec-essary preliminary step before modeling how thisinput is used in a cognitively plausible way.
Inter-ested readers may refer to (Fourtassi and Dupoux,2014; Fourtassi et al, 2014) for a more learning-oriented approach, where some of the assumptionsmade here about high level representations are re-laxed.AcknowledgmentsThis project is funded in part by the Euro-pean Research Council (ERC-2011-AdG-295810BOOTPHON), the Agence Nationale pour laRecherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02 PSL*), the Fondation de France,the Ecole de Neurosciences de Paris, and theR?egion Ile de France (DIM cerveau et pens?ee).
Wethank Luc Boruta, Sanjeev Khudanpur, IsabelleDautriche, Sharon Peperkamp and Benoit Crabb?efor highly useful discussions and contributions.5ReferencesLuc Boruta.
2011.
Combining Indicators of Al-lophony.
In Proceedings ACL-SRW, pages 88?93.Luc Boruta.
2012.
Indicateurs d?allophonie etde phon?emicit?e.
Doctoral dissertation, Universit?eParis-Diderot - Paris VII.A.
Cristia and A. Seidl.
In press.
The hyperarticula-tion hypothesis of infant-directed speech.
Journalof Child Language.Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-water, and James L. Morgan.
2013.
A role for thedeveloping lexicon in phonetic category acquisition.Psychological Review, 120(4):751?778.Abdellah Fourtassi and Emmanuel Dupoux.
2014.
Arudimentary lexicon and semantics help bootstrapphoneme acquisition.
In Proceedings of the 18thConference on Computational Natural LanguageLearning (CoNLL).Abdellah Fourtassi, Ewan Dunbar, and EmmanuelDupoux.
2014.
Self-consistency as an inductivebias in early language acquisition.
In Proceedingsof the 36th Annual Meeting of the Cognitive ScienceSociety.Patricia K. Kuhl, Erica Stevens, Akiko Hayashi,Toshisada Deguchi, Shigeru Kiritani, and Paul Iver-son.
2006.
Infants show a facilitation effect for na-tive language phonetic perception between 6 and 12months.
Developmental Science, 9(2):F13?F21.Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-toshi Isahara.
2000.
Spontaneous speech corpus ofjapanese.
In LREC, pages 947?952, Athens, Greece.Andrew Martin, Sharon Peperkamp, and EmmanuelDupoux.
2013.
Learning phonemes with a proto-lexicon.
Cognitive Science, 37(1):103?124.J.
Maye, J. F. Werker, and L. Gerken.
2002.
Infant sen-sitivity to distributional information can affect pho-netic discrimination.
Cognition, 82:B101?B111.Sharon Peperkamp, Rozenn Le Calvez, Jean-PierreNadal, and Emmanuel Dupoux.
2006.
The acqui-sition of allophonic rules: Statistical learning withlinguistic constraints.
Cognition, 101(3):B31?B41.M.
A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-mond, E. Hume, and Fosler-Lussier.
2007.
Buckeyecorpus of conversational speech.G.K.
Vallabha, J.L.
McClelland, F. Pons, J.F.
Werker,and S. Amano.
2007.
Unsupervised learningof vowel categories from infant-directed speech.Proceedings of the National Academy of Sciences,104(33):13273.Janet F. Werker and Richard C. Tees.
1984.
Cross-language speech perception: Evidence for percep-tual reorganization during the first year of life.
In-fant Behavior and Development, 7(1):49 ?
63.Steve J.
Young, D. Kershaw, J. Odell, D. Ollason,V.
Valtchev, and P. Woodland.
2006.
The HTK BookVersion 3.4.
Cambridge University Press.6
