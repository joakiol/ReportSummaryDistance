Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1544?1554,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsConnotationWordNet:Learning Connotation over the Word+Sense NetworkJun Seok Kang Song Feng Leman Akoglu Yejin ChoiDepartment of Computer ScienceStony Brook UniversityStony Brook, NY 11794-4400junkang, songfeng, leman, ychoi@cs.stonybrook.eduAbstractWe introduce ConnotationWordNet, a con-notation lexicon over the network of wordsin conjunction with senses.
We formulatethe lexicon induction problem as collec-tive inference over pairwise-Markov Ran-dom Fields, and present a loopy beliefpropagation algorithm for inference.
Thekey aspect of our method is that it isthe first unified approach that assigns thepolarity of both word- and sense-levelconnotations, exploiting the innate bipar-tite graph structure encoded in WordNet.We present comprehensive evaluation todemonstrate the quality and utility of theresulting lexicon in comparison to existingconnotation and sentiment lexicons.1 IntroductionWe introduce ConnotationWordNet, a connotationlexicon over the network of words in conjunctionwith senses, as defined in WordNet.
A connotationlexicon, as introduced first by Feng et al (2011),aims to encompass subtle shades of sentiment aword may conjure, even for seemingly objectivewords such as ?sculpture?, ?Ph.D.
?, ?rosettes?.Understanding the rich and complex layers of con-notation remains to be a challenging task.
As astarting point, we study a more feasible task oflearning the polarity of connotation.For non-polysemous words, which constitute asignificant portion of English vocabulary, learningthe general connotation at the word-level (ratherthan at the sense-level) would be a natural oper-ational choice.
However, for polysemous words,which correspond to most frequently used words,it would be an overly crude assumption that thesame connotative polarity should be assigned forall senses of a given word.
For example, consider?abound?, for which lexicographers of WordNetprescribe two different senses:?
(v) abound: (be abundant of plentiful; existin large quantities)?
(v) abound, burst, bristle: (be in a state ofmovement or action) ?The room aboundedwith screaming children?
; ?The garden bris-tled with toddlers?For the first sense, which is the most commonlyused sense for ?abound?, the general overtone ofthe connotation would seem positive.
That is, al-though one can use this sense in both positive andnegative contexts, this sense of ?abound?
seemsto collocate more often with items that are good tobe abundant (e.g., ?resources?
), than unfortunateitems being abundant (e.g., ?complaints?
).However, as for the second sense, for which?burst?
and ?bristle?
can be used interchangeablywith respect to this particular sense,1the generalovertone is slightly more negative with a touch ofunpleasantness, or at least not as positive as that ofthe first sense.
Especially if we look up the Word-Net entry for ?bristle?, there are noticeably morenegatively connotative words involved in its glossand examples.This word sense issue has been a universal chal-lenge for a range of Natural Language Processingapplications, including sentiment analysis.
Recentstudies have shown that it is fruitful to tease outsubjectivity and objectivity corresponding to dif-ferent senses of the same word, in order to improvecomputational approaches to sentiment analysis(e.g.
Pestian et al (2012), Mihalcea et al (2012)Balahur et al (2014)).
Encouraged by these recentsuccesses, in this study, we investigate if we canattain similar gains if we model the connotativepolarity of senses separately.There is one potential practical issue we wouldlike to point out in building a sense-level lexicalresource, however.
End-users of such a lexiconmay not wish to deal with Word Sense Disam-1Hence a sense in WordNet is defined by synset (= syn-onym set), which is the set of words sharing the same sense.1544biguation (WSD), which is known to be often toonoisy to be incorporated into the pipeline with re-spect to other NLP tasks.
As a result, researchersoften would need to aggregate labels across differ-ent senses to derive the word-level label.
Althoughsuch aggregation is not entirely unreasonable, itdoes not seem to be the most optimal and princi-pled way of integrating available resources.Therefore, in this work, we present the first uni-fied approach that learns both sense- and word-level connotations simultaneously.
This way, end-users will have access to more accurate sense-levelconnotation labels if needed, while also having ac-cess to more general word-level connotation la-bels.
We formulate the lexicon induction problemas collective inference over pairwise-Markov Ran-dom Fields (pairwise-MRF) and derive a loopy be-lief propagation algorithm for inference.The key aspect of our approach is that we ex-ploit the innate bipartite graph structure betweenwords and senses encoded in WordNet.
Althoughour approach seems conceptually natural, previousapproaches, to our best knowledge, have not di-rectly exploited these relations between words andsenses for the purpose of deriving lexical knowl-edge over words and senses collectively.
In ad-dition, previous studies (for both sentiment andconnotation lexicons) aimed to produce only ei-ther of the two aspects of the polarity: word-levelor sense-level, while we address both.Another contribution of our work is the intro-duction of loopy belief propagation (loopy-BP)as a lexicon induction algorithm.
Loopy-BP inour study achieves statistically significantly betterperformance over the constraint optimization ap-proaches previously explored.
In addition, it runsmuch faster and it is considerably easier to imple-ment.
Last but not least, by using probabilistic rep-resentation of pairwise-MRF in conjunction withLoopy-BP as inference, the resulting solution hasthe natural interpretation as the intensity of con-notation.
This contrasts to approaches that seekdiscrete solutions such as Integer Linear Program-ming(Papadimitriou and Steiglitz, 1998).ConnotationWordNet, the final outcome of ourstudy, is a new lexical resource that has conno-tation labels over both words and senses follow-ing the structure of WordNet.
The lexicon is pub-licly available at: http://www.cs.sunysb.edu/?junkang/connotation_wordnet.
)In what follows, we will first describe the net-memberships)antonyms)Pred1Arg)Arg1Arg)?)?
)prevent)suffer)enjoy)achieve)pain)losses)life)profit)success)win)investment)injure)accident)wound)ache)word)sense)gain)hurt)put)on)lose)injure,))wound)gain,))put)on)win,)gain,)acquire)win,)profits,)))winnings)ConnotaAve)Predicates) Arguments) Senses)Figure 1: GWORD+SENSEwith words and senses.work of words and senses (Section 2), then intro-duce the representation of the network structure aspairwise Markov Random Fields, and a loopy be-lief propagation algorithm as collective inference(Section 3).
We then present comprehensive eval-uation (Section 4 & 5 & 6), followed by relatedwork (Section 7) and conclusion (Section 8).2 Network of Words and SensesThe connotation graph, called GWORD+SENSE, is aheterogeneous graph with multiple types of nodesand edges.
As shown in Figure 1, it contains twotypes of nodes; (i) lemmas (i.e., words, 115K)and (ii) synsets (63K), and four types of edges;(t1) predicate-argument (179K), (t2) argument-argument (144K), (t3) argument-synset (126K),and (t4) synset-synset (3.4K) edges.The predicate-argument edges, first introducedby Feng et al (2011), depict the selectional prefer-ence of connotative predicates (i.e., the polarity ofa predicate indicates the polarity of its arguments)and encode their co-occurrence relations basedon the Google Web 1T corpus.
The argument-argument edges are based on the distributionalsimilarities among the arguments.
The argument-synset edges capture the synonymy between argu-ment nodes through the corresponding synsets.
Fi-nally, the synset-synset edges depict the antonymrelations between synset pairs.In general, our graph construction is similar tothat of Feng et al (2013), but there are a few im-portant differences.
Most notably, we model bothwords and synsets explicitly, and exploit the mem-bership relations between words and senses.
Weexpect that edges between words and senses willencourage senses that belong to the same word to1545receive the same connotation label.
Conversely,we expect that these edges will also encouragewords that belong to the same sense (i.e., synsetdefinition) to receive the same connotation label.Another benefit of our approach is that for var-ious WordNet relations (e.g., antonym relations),which are defined over synsets (not over words),we can add edges directly between correspondingsynsets, rather than projecting (i.e., approximat-ing) those relations over words.
Note that the lat-ter, which has been employed by several previousstudies (e.g., Kamps et al (2004), Takamura et al(2005), Andreevskaia and Bergler (2006), Su andMarkert (2009), Lu et al (2011), Kaji and Kit-suregawa (2007), Feng et al (2013)), could be asource of noise, as one needs to assume that thesemantic relation between a pair of synsets trans-fers over the pair of words corresponding to thatpair of synsets.
For polysemous words, this as-sumption may be overly strong.3 Pairwise Markov Random Fields andLoopy Belief PropagationWe formulate the task of learning sense- and word-level connotation lexicon as a graph-based clas-sification task (Sen et al, 2008).
More formally,we denote the connotation graph GWORD+SENSEbyG = (V,E), in which a total of n word and synsetnodes V = {v1, .
.
.
, vn} are connected withtyped edges e(vi, vj, t) ?
E, where edge typest ?
{pred-arg, arg-arg, syn-arg, syn-syn} de-pict the four edge types as described in Section2.
A neighborhood function N , where Nv={u| e(u, v) ?
E} ?
V , describes the underlyingnetwork structure.In our collective classification formulation, eachnode in V is represented as a random variable thattakes a value from an appropriate class label do-main; in our case, L = {+,?}
for positive andnegative connotation.
In this classification task,we denote by Y the nodes the labels of which needto be assigned, and let yirefer to Yi?s label.3.1 Pairwise Markov Random FieldsWe next define our objective function.
We pro-pose to use an objective formulation that utilizespairwise Markov Random Fields (MRFs) (Kinder-mann and Snell, 1980), which we adapt to ourproblem setting.
MRFs are a class of probabilisticgraphical models that are suited for solving infer-ence problems in networked data.
An MRF con-sists of an undirected graph where each node canbe in any of a finite number of states (i.e., classlabels).
The state of a node is assumed to be de-pendent on each of its neighbors and independentof other nodes in the graph.2In pairwise MRFs,the joint probability of the graph can be written asa product of pairwise factors, parameterized overthe edges.
These factors are referred to as cliquepotentials in general MRFs, which are essentiallyfunctions that collectively determine the graph?sjoint probability.Specifically, let G = (V,E) denote a networkof random variables, where V consists of the un-observed variables Y that need to be assigned val-ues from label set L. Let ?
denote a set of cliquepotentials that consists of two types of factors:?
For each Yi?
Y , ?i?
?
is a prior map-ping ?i: L ?
R?0, where R?0denotes non-negative real numbers.?
For each e(Yi, Yj, t) ?
E, ?tij?
?
is a com-patibility mapping ?tij: L ?
L ?
R?0.Objective formulation Given an assignment yto all the unobserved variables Y and x to ob-served ones X (variables with known labels, ifany), our objective function is associated with thefollowing joint probability distributionP (y|x) =1Z(x)?Yi?Y?i(yi)?e(Yi,Yj,t)?E?tij(yi, yj)(1)where Z(x) is the normalization function.
Ourgoal is then to infer the maximum likelihood as-signment of states (i.e., labels) to unobserved vari-ables (i.e., nodes) that will maximize Equation (1).Problem Definition Having introduced ourgraph-based classification task and objective for-mulation, we define our problem more formally.Given- a connotation graph G = (V,E) of wordsand synsets connected with typed edges,- prior knowledge (i.e., probabilities) of (someor all) nodes belonging to each class,- compatibility of two nodes with a given pairof labels being connected to each other;Classify the nodes Yi?
Y , into one of two classes;L = {+,?
}, such that the class assignments yimaximize our objective in Equation (1).We can further rank the network objects by theprobability of their connotation polarity.2This assumption yields a pairwise Markov Random Field(MRF); a special case of general MRFs (Yedidia et al, 2003).15463.2 Loopy Belief PropagationFinding the best assignments to unobserved vari-ables in our objective function is the inferenceproblem.
The brute force approach through enu-meration of all possible assignments is exponen-tial and thus intractable.
In general, exact in-ference is known to be NP-hard and there isno known algorithm which can be theoreticallyshown to solve the inference problem for gen-eral MRFs.
Therefore in this work, we em-ploy a computationally tractable (in fact linearlyscalable with network size) approximate infer-ence algorithm called Loopy Belief Propagation(LBP) (Yedidia et al, 2003), which we extend tohandle typed graphs like our connotation graph.Our inference algorithm is based on iterativemessage passing and the core of it can be conciselyexpressed as the following two equations:mi?j(yj) = ?
?yi?L(?tij(yi, yj) ?i(yi)?Yk?Ni?Y\Yjmk?i(yi)), ?yj?
L (2)bi(yi) = ?
?i(yi)?Yj?Ni?Ymj?i(yi),?yi?
L(3)A message mi?jis sent from node i to node jand captures the belief of i about j, which is theprobability distribution over the labels of j; i.e.what i ?thinks?
j?s label is, given the current la-bel of i and the type of the edge that connects iand j. Beliefs refer to marginal probability dis-tributions of nodes over labels; for example bi(yi)denotes the belief of node i having label yi.
?
and?
are the normalization constants, which respec-tively ensure that each message and each set ofmarginal probabilities sum to 1.
At every iteration,each node computes its belief based on messagesreceived from its neighbors, and uses the compat-ibility mapping to transform its belief into mes-sages for its neighbors.
The key idea is that afterenough iterations of message passes between thenodes, the ?conversations?
are likely to come to aconsensus, which determines the marginal proba-bilities of all the unknown variables.The pseudo-code of our method is given in Al-gorithm 1.
It first initializes all messages to 1and priors to unbiased (i.e., equal) probabilitiesfor all nodes except the seed nodes for which thesentiment is known (lines 3-9).
It then proceedsby making each Yi?
Y communicate messagesAlgorithm 1: CONNOTATION INFERENCE1 Input: Connotation graph G=(V,E), priorpotentials ?sfor seed words s ?
S, andcompatibility potentials ?tij2 Output: Connotation label probabilities foreach node i ?
V \P3 foreach e(Yi, Yj, t) ?
E do // initialize msg.s4 foreach yj?
L do5 mi?j(yj)?
16 foreach i ?
V do // initialize priors7 foreach yj?
L do8 if i ?
S then ?i(yj)?
?i(yj) else?i(yj)?
1/|L|9 repeat // iterative message passing10 foreach e(Yi, Yj, t) ?
E, Yj?
YV \Sdo11 foreach yj?
L do12 Use Equation (2)13 until all messages stop changing14 foreach Yi?
YV \Sdo // compute final beliefs15 foreach yi?
L do16 Use Equation (3)with their neighbors in an iterative fashion untilthe messages stabilize (lines 10-14), i.e.
conver-gence is reached.3At convergence, we calculatethe marginal probabilities, that is of assigning Yiwith label yi, by computing the final beliefs bi(yi)(lines 15-17).
We use these maximum likelihoodprobabilities for label assignment; for each node i,we assign the label Li?
maxyibi(yi).To completely define our algorithm, we need toinstantiate the potentials ?, in particular the priorsand the compatibilities, which we discuss next.Priors The prior beliefs ?iof nodes can be suit-ably initialized if there is any prior knowledge fortheir connotation sentiment (e.g., enjoy is posi-tive, suffer is negative).
As such, our methodis flexible to integrate available side information.In case there is no prior knowledge available, eachnode is initialized equally likely to have any of thepossible labels, i.e.,1|L|as in Algorithm 1 (line 9).Compatibilities The compatibility potentialscan be thought of as matrices, with entries3Although convergence is not theoretically guaranteed, inpractice LBP converges to beliefs within a small threshold ofchange (e.g., 10?6) fairly quickly with accurate results (Pan-dit et al, 2007; McGlohon et al, 2009; Akoglu et al, 2013).1547?tij(yi, yj) that give the likelihood of a node hav-ing label yi, given that it has a neighbor with labelyjto which it is connected through a type t edge.A key difference of our method from earlier mod-els is that we use clique potentials that differ foredge types, since the connotation graph is hetero-geneous.
This is exactly because the compatibil-ity of class labels of two adjacent nodes dependson the type of the edge connecting them: e.g.,+syn-arg??????
+ is highly compatible, whereas +syn-syn??????
+ is unlikely; as syn-arg edges capturesynonymy; i.e., words-sense memberships, whilesyn-syn edges depict antonym relations.A sample instantiation of the compatibilitiesis shown in Table 1.
Notice that the potentialsfor pred-arg, arg-arg, and syn-arg capture ho-mophily, i.e., nodes with the same label are likelyto connect to each other through these types ofedges.4On the other hand, syn-syn edges con-nect nodes that are antonyms of each other, andthus the compatibilities capture the reverse rela-tionship among their labels.Table 1: Instantiation of compatibility potentials.Entry ?tij(yi, yj) is the compatibility of a nodewith label yihaving a neighbor labeled yj, giventhe edge between i and j is type t, for small .t: t1AP + ?+ 1- ?
 1-t: t2AA + ?+ 1-2 2?
2 1-2(t1) pred-arg (t2) arg-argt: t3AS + ?+ 1- ?
 1-t: t4SS + ?+  1-?
1- (t3) syn-arg (t4) syn-syn(synonym relations) (antonym relations)Complexity analysis Most demanding compo-nent of Algorithm 1 is the iterative message pass-ing over the edges (lines 10-14), with time com-plexity O(ml2r), where m = |E| is the num-ber of edges in the connotation graph, l = |L|,the classes, and r, the iterations until convergence.Often, l is quite small (in our case, l = 2) andr  m. Thus running time grows linearly with thenumber of edges and is scalable to large datasets.4arg-arg edges are based on co-occurrence (see Section2), which does not carry as strong indication of the same con-notation as e.g., synonymy.
Thus, we enforce less homophilyfor nodes connected through edges of arg-arg type.4 Evaluation I: Agreement withSentiment LexiconsConnotationWordNet is expected to be the super-set of a sentiment lexicon, as it is highly likely forany word with positive/negative sentiment to carryconnotation of the same polarity.
Thus, we usetwo conventional sentiment lexicons, General In-quirer (GENINQ) (Stone et al, 1966) and MPQA(Wilson et al, 2005b), as surrogates to measurethe performance of our inference algorithm.4.1 Variants of Graph ConstructionThe construction of the connotation graph, de-noted by GWORD+SENSE, which includes words andsynsets, has been described in Section 2.
In ad-dition to this graph, we tried several other graphconstructions, the first three of which have previ-ously been used in (Feng et al, 2013).
We brieflydescribe these graphs below, and compare perfor-mance on all the graphs in the proceeding.GWORDW/ PRED-ARG: This is a (bipartite)subgraph of GWORD+SENSE, which only includesthe connotative predicates and their arguments.
Assuch, it contains only type t1edges.
The edgesbetween the predicates and the arguments can beweighted by their Point-wise Mutual Information(PMI)5based on the Google Web 1T corpus.GWORDW/ OVERLAY: The second graph is alsoa proper subgraph of GWORD+SENSE, which in-cludes the predicates and all the argument words.Predicate words are connected to their argumentsas before.
In addition, argument pairs (a1, a2) areconnected if they occurred together in the ?a1anda2?
or ?a2and a1?
coordination (Hatzivassiloglouand McKeown, 1997; Pickering and Branigan,1998).
This graph contains both type t1and t2edges.
The edges can also be weighted based onthe distributional similarities of the word pairs.GWORD: The third graph is a super-graph ofGWORDW/ OVERLAY, with additional edges,where argument pairs in synonym and antonymrelation are connected to each other.
Note that un-like the connotation graph GWORD+SENSE, it doesnot contain any synset nodes.
Rather, the wordsthat are synonyms or antonyms of each other aredirectly linked in the graph.
As such, this graphcontains all edge types t1through t4.5PMI scores are widely used in previous studies to mea-sure association between words (e.g., (Church and Hanks,1990), (Turney, 2001), (Newman et al, 2009)).1548GWORD+SENSEW/ SYNSIM: This is a super-graph of our original GWORD+SENSEgraph; thatis, it has all the predicate, arguments, and synsetnodes, as well as the four types of edges betweenthem.
In addition, we add edges of a fifth type t5between the synset nodes to capture their similar-ity.
To define similarity, we use the glossary def-initions of the synsets and derive three differentscores.
Each score utilizes the count(s1, s2) ofoverlapping nouns, verbs, and adjectives/adverbsamong the glosses of the two synsets s1and s2.GWORD+SENSEW/ SYNSIM1: We discard edgeswith count less than 3.
The weighted version hasthe counts normalized between 0 and 1.GWORD+SENSEW/ SYNSIM2: We normalizethe counts by the length of the gloss (theavg of two lengths), that is, p = count /avg(len gloss(s1), len gloss(s2))and discard edges with p < 0.5.
The weightedversion contains p values as edge weights.GWORD+SENSEW/ SYNSIM3: To further sparsifythe graph we discard edges with p < 0.6.
Toweigh the edges, we use the cosine similarity be-tween the gloss vectors of the synsets based on theTF-IDF values of the words the glosses contain.Note that the connotation inference algorithm,as given in Algorithm 1, remains exactly the samefor all the graphs described above.
The only dif-ference is the set of parameters used; while GWORDW/ PRED-ARG and GWORDW/ OVERLAY containone and two edge types, respectively and only usecompatibilities (t1) and (t2), GWORDuses all fouras given in Table 1.
The GWORD+SENSEW/ SYN-SIM graphs use an additional compatibility matrixfor the synset similarity edges of type t5, which isthe same as the one used for t1, i.e., similar synsetsare likely to have the same connotation label.
Thisflexibility is one of the key advantages of our al-gorithm as new types of nodes and edges can beadded to the graph seamlessly.4.2 Sentiment-Lexicon based PerformanceIn this section, we first compare the performanceof our connotation graph GWORD+SENSEto graphsthat do not include synset nodes but only words.Then we analyze the performance when the addi-tional synset similarity edges are added.
First, webriefly describe our performance measures.The sentiment lexicons we use as gold standardare small, compared to the size (i.e., number ofwords) our graphs contain.
Thus, we first findthe overlap between each graph and a senti-GENINQ MPQAP R F FVariations of GWORDW/ PRED-ARG 88.0 67.6 76.5 57.3W/ PRED-ARG-W 84.9 68.9 76.1 57.8W/ OVERLAY 87.8 70.4 78.1 58.4W/ OVERLAY-W 82.2 67.7 74.2 54.2GWORD88.5 83.1 85.7 69.7GWORD-W 75.5 71.5 73.4 53.2Variations of GWORD+SENSEGWORD+SENSE88.8 84.1 86.4 70.0GWORD+SENSE-W 76.8 73.0 74.9 54.6W/ SYNSIM1 87.2 83.3 85.2 67.9W/ SYNSIM2 83.9 80.8 82.3 65.1W/ SYNSIM3 86.5 83.2 84.8 67.8W/ SYNSIM1-W 88.0 84.3 86.1 69.2W/ SYNSIM2-W 86.4 83.7 85.0 68.5W/ SYNSIM3-W 86.7 83.4 85.0 68.2Table 2: Connotation inference performance onvarious graphs.
?-W?
indicates weighted versions(see ?4.1).
P: precision, R: recall, F: F1-score (%).ment lexicon.
Note that the overlap size may besmaller than the lexicon size, as some sen-timent words may be missing from our graphs.Then, we calculate the number of correct la-bel assignments.
As such, precision is defined as(correct / overlap), and recall as (correct/ lexicon size).
Finally, F1-score is their har-monic mean and reflects the overall accuracy.As shown in Table 2 (top), we first observe thatincluding the synonym and antonym relations inthe graph, as with GWORDand GWORD+SENSE, im-prove the performance significantly, almost by anorder of magnitude, over graphs GWORDW/ PRED-ARG and GWORDW/ OVERLAY that do not containthose relation types.
Furthermore, we notice thatthe performances on the GWORD+SENSEgraph arebetter than those on the word-only graphs.
Thisshows that including the synset nodes explicitly inthe graph structure is beneficial.
What is more,it gives us a means to obtain connotation labelsfor the synsets themselves, which we use in theevaluations in the next sections.
Finally, we notethat using the unweighted versions of the graphsprovide relatively more robust performance, po-tentially due to noise in the relative edge weights.Next we analyze the performance when the newedges between synsets are introduced, as given inTable 2 (bottom).
We observe that connecting thesynset nodes by their gloss-similarity (at least inthe ways we tried) does not yield better perfor-mance than on our original GWORD+SENSEgraph.Different from earlier, the weighted versions ofthe similarity based graphs provide better perfor-1549mance than their unweighted counterparts.
Thissuggests that glossary similarity would be a morerobust means to correlate nodes; we leave it as fu-ture work to explore this direction for predicate-argument and argument-argument relations.4.3 Parameter SensitivityOur belief propagation based connotation senti-ment inference algorithm has one user-specifiedparameter  (see Table 1).
To study the sensitivityof its performance to the choice of , we reran ourexperiments for  = {0.02, 0.04, .
.
.
, 0.24}6andreport the accuracy results on our GWORD+SENSEinFigure 2 for the two lexicons.
The results indicatethat the performances remain quite stable across awide range of the parameter choice.precisionrecallF-scorePerformance020406080100?0.02 0.06 0.10 0.14 0.18 0.22precisionrecallF-scorePerformance020406080100?0.02 0.06 0.10 0.14 0.18 0.22(a) GENINQ EVAL (b) MPQA EVALFigure 2: Performance is stable across various .5 Evaluation II: Human Evaluation onConnotationWordNetIn this section, we present the result of humanevaluation we executed using Amazon Mechani-cal Turk (AMT).
We collect two separate sets oflabels: a set of labels at the word-level, and an-other set at the sense-level.
We first describe thelabeling process of sense-level connotation: Weselected 350 polysemous words and one of theirsenses, and each Turker was asked to rate the con-notative polarity of a given word (or of a givensense), from -5 to 5, 0 being the neutral.7For eachword, we asked 5 Turkers to rate and we took theaverage of the 5 ratings as the connotative inten-sity score of the word.
We labeled a word as nega-tive if its intensity score is less than 0 and positiveotherwise.
For word-level labels we apply similarprocedure as above.6Note that for  > 0.25, compatibilities of ?t2in Table 1are reversed, hence the maximum of 0.24.7Because senses in WordNet can be tricky to understand,care should be taken in designing the task so that the Turkerswill focus only on the corresponding sense of a word.
There-fore, we provided the part of speech tag, the WordNet glossof the selected sense, and a few examples as given in Word-Net.
As an incentive, each Turker was rewarded $0.07 per hitwhich consists of 10 words to label.Lexicon Word-level Sense-levelSentiWordNet 27.22 14.29OpinionFinder 31.95 -Feng2013 62.72 -GWORD+SENSE(95%) 84.91 83.43GWORD+SENSE(99%) 84.91 83.71E-GWORD+SENSE(95%) 86.98 86.29E-GWORD+SENSE(99%) 86.69 85.71Table 3: Word-/Sense-level evaluation results5.1 Word-Level EvaluationWe first evaluate the word-level assignment ofconnotation, as shown in Table 3.
The agreementbetween the new lexicon and human judges variesbetween 84% and 86.98%.
Sentiment lexiconssuch as SentiWordNet (Baccianella et al (2010))and OpinionFinder (Wilson et al (2005a)) showlow agreement rate with human, which is some-what as expected: human judges in this study arelabeling for subtle connotation, not for more ex-plicit sentiment.
OpinionFinder?s low agreementrate was mainly due to the low hit rate of the words(successful look-up rate, 33.43%).
Feng2013 isthe lexicon presented in (Feng et al, 2013) and itshowed a relatively higher 72.13% hit rate.Note that belief propagation was run until 95%and 99% of the nodes were converged in theirbeliefs.
In addition, the seed words with knownconnotation labels originally consist of 20 positiveand 20 negative predicates.
We also extended theseed set with the sentiment lexicon words and de-note these runs with E- for ?Extended?.5.2 Sense-Level EvaluationWe also examined the agreement rates on thesense-level.
Since OpinionFinder and Feng2013do not provide the polarity scores at the sense-level, we excluded them from this evaluation.
Be-cause sense-level polarity assignment is a harder(more subtle) task, the performance of all lexiconsdecreased to some degree in comparison to that ofword-level evaluations.5.3 Pair-wise Intensity RankingA notable goodness of our induction algorithm isthat the outcome of the algorithm can be inter-preted as an intensity of the corresponding conno-tation.
But are these values meaningful?
We an-swer this question in this section.
We formulate apair-wise ranking task as a binary decision task asfollows: given a pair of words, we ask which oneis more positive (or more negative) than the other.Since we collect human labels based on scales, we1550Lexicon Correct UndecidedSentiWordNet 33.77 23.34GWORD+SENSE(95%) 74.83 0.58GWORD+SENSE(99%) 73.01 0.58E-GWORD+SENSE(95%) 73.84 1.16E-GWORD+SENSE(99%) 74.01 1.16Table 4: Results of pair-wise intensity evaluation,for intensity difference threshold = 2.0already have this information at hand.
Becausedifferent human judges have different notion ofscales however, subtle differences are more likelyto be noisy.
Therefore, we experiment with vary-ing degrees of differences in their scales, as shownin Figure 3.
Threshold values (ranging from 0.5 to3.0) indicate the minimum differences in scales forany pair of words, for the pair to be included in thetest set.
As expected, we observe that the perfor-mance improves as we increase the threshold (aspairs get better separated).
Within range [0.5, 1.5](249 pairs examined), the accuracies are as high as68.27%, which shows that even the subtle differ-ences of the connotative intensities are relativelywell reflected in the new lexicons.SentiWordNetGWord+Sense(95%)GWord+Sense(99%)e-GWord+Sense(95%)e-GWord+Sense(99%)Accuracy (%)406080Threshold0.5 1.0 2.0 3.0Figure 3: Trend of accuracy for pair-wise intensityevaluation over thresholdThe results for pair-wise intensity evaluation(threshold=2.0, 1,208 pairs) are given in Table 4.Despite that intensity is generally a harder prop-erty to measure (than the coarser binary catego-rization of polarities), our connotation lexiconsperform surprisingly well, reaching up to 74.83%accuracy.
Further study on the incorrect cases re-veals that SentiWordNet has many pair of wordswith the same polarity score (23.34%).
Such casesseems to be due to the limited score patterns ofSentiWordNet.
The ratio of such cases are ac-counted as Undecided in Table 4.6 Evaluation III: Sentiment Analysisusing ConnotationWordNetFinally, to show the utility of the resulting lexi-con in the context of a concrete sentiment analysistask, we perform lexicon-based sentiment analy-sis.
We experiment with SemEval dataset (Strap-parava and Mihalcea, 2007) that includes the hu-man labeled dataset for predicting whether a newsheadline is a good news or a bad news, which weexpect to have a correlation with the use of con-notative words that we focus on in this paper.
Thegood/bad news are annotated with scores (rangingfrom -100 to 87).
We construct several data sets byapplying different thresholds on scores.
For exam-ple, with the threshold set to 60, we discard the in-stances whose scores lie between -60 and 60.
Forcomparison, we also test the connotation lexiconfrom (Feng et al, 2013) and the combined senti-ment lexicon GENINQ+MPQA.Note that there is a difference in how humansjudge the orientation and the degree of connota-tion for a given word out of context, and how theuse of such words in context can be perceived asgood/bad news.
In particular, we conjecture thathumans may have a bias toward the use of posi-tive words, which in turn requires calibration fromthe readers?
minds (Pennebaker and Stone, 2003).That is, we might need to tone down the level ofpositiveness in order to correctly measure the ac-tual intended positiveness of the message.With this in mind, we tune the appropriate cali-bration from a small training data, by using 1 foldfrom N fold cross validation, and using the re-maining N ?
1 folds as testing.
We simply learnthe mixture coefficient ?
to scale the contributionof positive and negative connotation values.
Wetune this parameter ?8for other lexicons we com-pare against as well.
Note that due to this param-eter learning, we are able to report better perfor-mance for the connotation lexicon of (Feng et al,2013) than what the authors have reported in theirpaper (labeled with *) in Table 5.Table 5 shows the results for N=15, where thenew lexicon consistently outperforms other com-petitive lexicons.
In addition, Figure 4 shows thatthe performance does not change much based onthe size of training data used for parameter tuning(N={5, 10, 15, 20}).7 Related WorkSeveral previous approaches explored the use ofgraph propagation for sentiment lexicon induction(Velikovich et al, 2010) and connotation lexicon8What is reported is based on ?
?
{20, 40, 60, 80}.
Moredetailed parameter search does not change the results much.1551LexiconSemEval Threshold20 40 60 80Instance Size 955 649 341 86Feng2013 71.5 77.1 81.6 90.5GENINQ+MPQA 72.8 77.2 80.4 86.7GWORD+SENSE(95%) 74.5 79.4 86.5 91.9GWORD+SENSE(99%) 74.6 79.4 86.8 91.9E-GWORD+SENSE(95%) 72.5 76.8 82.3 87.2E-GWORD+SENSE(99%) 72.6 76.9 82.5 87.2Feng2013* 70.8 74.6 80.8 93.5GENINQ+MPQA* 64.5 69.0 74.0 80.5Table 5: SemEval evaluation results, for N=15Feng2013MPQA+GenInqGWord+Sense(95%)GWord+Sense(99%)e-GWord+Sense(95%)e-GWord+Sense(99%)Accuracy(%)50607080N5 10 15 20Figure 4: Trend of SemEval performance over N ,the number of CV foldsinduction (Feng et al, 2013).
Our work intro-duces the use of loopy belief propagation overpairwise-MRF as an alternative solution to thesetasks.
At a high-level, both approaches share thegeneral idea of propagating confidence or beliefover the graph connectivity.
The key difference,however, is that in our MRF representation, wecan explicitly model various types of word-word,sense-sense and word-sense relations as edge po-tentials.
In particular, we can naturally encode re-lations that encourage the same assignment (e.g.,synonym) as well as the opposite assignment (e.g.,antonym) of the polarity labels.
Note that integra-tion of the latter is not straightforward in the graphpropagation framework.There have been a number of previous studiesthat aim to construct a word-level sentiment lex-icon (Wiebe et al, 2005; Qiu et al, 2009) anda sense-level sentiment lexicon (Esuli and Sebas-tiani, 2006).
But none of these approaches con-sidered to induce the polarity labels at both theword-level and sense-level.
Although we focus onlearning connotative polarity of words and sensesin this paper, the same approach would be applica-ble to constructing a sentiment lexicon as well.There have been recent studies that addressword sense disambiguation issues for sentimentanalysis.
SentiWordNet (Esuli and Sebastiani,2006) was the very first lexicon developed forsense-level labels of sentiment polarity.
In recentyears, Akkaya et al (2009) report a successful em-pirical result where WSD helps improving senti-ment analysis, while Wiebe and Mihalcea (2006)study the distinction between objectivity and sub-jectivity in each different sense of a word, andtheir empirical effects in the context of sentimentanalysis.
Our work shares the high-level spirit ofaccessing the sense-level polarity, while also de-riving the word-level polarity.In recent years, there has been a growing re-search interest in investigating more fine-grainedaspects of lexical sentiment beyond positive andnegative sentiment.
For example, Mohammad andTurney (2010) study the affects words can evokein people?s minds, while Bollen et al (2011) studyvarious moods, e.g., ?tension?, ?depression?, be-yond simple dichotomy of positive and negativesentiment.
Our work, and some recent work byFeng et al (2011) and Feng et al (2013) share thisspirit by targeting more subtle, nuanced sentimenteven from those words that would be consideredas objective in early studies of sentiment analysis.8 ConclusionWe have introduced a novel formulation of lexiconinduction operating over both words and senses,by exploiting the innate structure between thewords and senses as encoded in WordNet.
In addi-tion, we introduce the use of loopy belief propaga-tion over pairwise-Markov Random Fields as aneffective lexicon induction algorithm.
A notablestrength of our approach is its expressiveness: var-ious types of prior knowledge and lexical relationscan be encoded as node potentials and edge po-tentials.
In addition, it leads to a lexicon of bet-ter quality while also offering faster run-time andeasiness of implementation.
The resulting lexi-con, called ConnotationWordNet, is the first lex-icon that has polarity labels over both words andsenses.
ConnotationWordNet is publicly availablefor research and practical use.AcknowledgmentsThis research was supported by the Army Re-search Office under Contract No.
W911NF-14-1-0029, Stony Brook University Office of Vice Pres-ident for Research, and gifts from Northrop Grum-man Aerospace Systems and Google.
We thankreviewers for many insightful comments and sug-gestions.1552ReferencesCem Akkaya, Janyce Wiebe, and Rada Mihalcea.2009.
Subjectivity word sense disambiguation.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume1-Volume 1, pages 190?199.
Association for Com-putational Linguistics.Leman Akoglu, Rishi Chandy, and Christos Faloutsos.2013.
Opinion fraud detection in online reviews bynetwork effects.Alina Andreevskaia and Sabine Bergler.
2006.
Min-ing wordnet for a fuzzy sentiment: Sentiment tagextraction from wordnet glosses.
In EACL, pages209?216.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In LREC, volume 10, pages 2200?2204.Alexandra Balahur, Rada Mihalcea, and Andr?es Mon-toyo.
2014.
Computational approaches to subjec-tivity and sentiment analysis: Present and envisagedmethods and applications.
Computer Speech & Lan-guage, 28(1):1?6.Johan Bollen, Huina Mao, and Alberto Pepe.
2011.Modeling public mood and emotion: Twitter senti-ment and socio-economic phenomena.
In ICWSM.K.
W. Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational Linguistics, 1(16):22?29.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sen-tiwordnet: A publicly available lexical resourcefor opinion mining.
In In Proceedings of the 5thConference on Language Resources and Evaluation(LREC06, pages 417?422.Song Feng, Ritwik Bose, and Yejin Choi.
2011.
Learn-ing general connotation of words using graph-basedalgorithms.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 1092?1103.
Association for Computa-tional Linguistics.Song Feng, Jun Seok Kang, Polina Kuznetsova, andYejin Choi.
2013.
Connotation lexicon: A dashof sentiment beneath the surface meaning.
In TheAssociation for Computer Linguistics, pages 1774?1784.Vasileios Hatzivassiloglou and Kathleen McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of the Joint ACL/EACL Con-ference, pages 174?181.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Build-ing lexicon for sentiment analysis from massive col-lection of html documents.
In EMNLP-CoNLL,pages 1075?1083.Jaap Kamps, MJ Marx, Robert J Mokken, and MaartenDe Rijke.
2004.
Using wordnet to measure seman-tic orientations of adjectives.Ross Kindermann and J. L. Snell.
1980.
Markov Ran-dom Fields and Their Applications.Yue Lu, Malu Castellanos, Umeshwar Dayal, andChengXiang Zhai.
2011.
Automatic constructionof a context-aware sentiment lexicon: an optimiza-tion approach.
In Proceedings of the 20th interna-tional conference on World wide web, pages 347?356.
ACM.Mary McGlohon, Stephen Bay, Markus G. Anderle,David M. Steier, and Christos Faloutsos.
2009.Snare: a link analytic system for graph labelingand risk detection.
In John F. Elder IV, FranoiseFogelman-Souli, Peter A. Flach, and MohammedZaki, editors, KDD, pages 1265?1274.
ACM.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.2012.
Multilingual subjectivity and sentiment anal-ysis.
In Tutorial Abstracts of ACL 2012, pages 4?4.Association for Computational Linguistics.Saif Mohammad and Peter Turney.
2010.
Emotionsevoked by common words and phrases: Using me-chanical turk to create an emotion lexicon.
In Pro-ceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 26?34, Los Angeles,CA, June.
Association for Computational Linguis-tics.David Newman, Sarvnaz Karimi, and Lawrence Cave-don.
2009.
External evaluation of topic models.In Australasian Document Computing Symposium,pages 11?18, Sydney, December.Shashank Pandit, Duen Horng Chau, Samuel Wang,and Christos Faloutsos.
2007.
Netprobe: a fast andscalable system for fraud detection in online auctionnetworks.
In WWW, pages 201?210.Christos H Papadimitriou and Kenneth Steiglitz.
1998.Combinatorial optimization: algorithms and com-plexity.
Courier Dover Publications.James W Pennebaker and Lori D Stone.
2003.
Wordsof wisdom: language use over the life span.
Journalof personality and social psychology, 85(2):291.John P Pestian, Pawel Matykiewicz, Michelle Linn-Gust, Brett South, Ozlem Uzuner, Jan Wiebe, K Bre-tonnel Cohen, John Hurdle, Christopher Brew, et al2012.
Sentiment analysis of suicide notes: A sharedtask.
Biomedical Informatics Insights, 5(Suppl.1):3.Martin J. Pickering and Holly P. Branigan.
1998.
Therepresentation of verbs: Evidence from syntacticpriming in language production.
Journal of Mem-ory and Language, 39:633?651.1553Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2009.
Expanding domain sentiment lexicon throughdouble propagation.
In IJCAI, volume 9, pages1199?1204.Prithviraj Sen, Galileo Namata, Mustafa Bilgic, LiseGetoor, Brian Gallagher, and Tina Eliassi-Rad.2008.
Collective classification in network data.
AIMagazine, 29(3):93?106.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General In-quirer: A Computer Approach to Content Analysis.MIT Press, Cambridge, MA.Carlo Strapparava and Rada Mihalcea.
2007.
Semeval-2007 task 14: Affective text.
In Proceedings ofthe 4th International Workshop on Semantic Evalu-ations, pages 70?74.
Association for ComputationalLinguistics.Fangzhong Su and Katja Markert.
2009.
Subjectiv-ity recognition on word senses via semi-supervisedmincuts.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 1?9.
Association for Com-putational Linguistics.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of words us-ing spin model.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, pages 133?140.
Association for ComputationalLinguistics.Peter D. Turney.
2001.
Mining the Web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedingsof the Twelfth European Conference on MachineLearning (ECML-01), pages 491?502, Freiburg,Germany.Leonid Velikovich, Sasha Blair-Goldensohn, KerryHannan, and Ryan McDonald.
2010.
The via-bility of web-derived polarity lexicons.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics.
Association forComputational Linguistics.Janyce Wiebe and Rada Mihalcea.
2006.
Word senseand subjectivity.
In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, pages 1065?1072.
Asso-ciation for Computational Linguistics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Eval-uation (formerly Computers and the Humanities),39(2/3):164?210.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patward-han.
2005a.
Opinionfinder: A system for subjec-tivity analysis.
In Proceedings of HLT/EMNLP onInteractive Demonstrations, pages 34?35.
Associa-tion for Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HumanLanguage Technologies Conference/Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP 2005), Vancouver, CA.Jonathan S. Yedidia, William T. Freeman, and YairWeiss.
2003.
Understanding belief propagation andits generalizations.
In Exploring AI in the new mil-lennium, pages 239?269.1554
