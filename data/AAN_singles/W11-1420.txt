Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 161?169,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsNon-English Response Detection Method for Automated Proficiency ScoringSystemSu-Youn Yoon and Derrick HigginsEducational Testing Service660 Rosedale Road, Princeton, NJ, USA{syoon,dhiggins}@ets.orgAbstractThis paper presents a method for identifyingnon-English speech, with the aim of support-ing an automated speech proficiency scoringsystem for non-native speakers.The method uses a popular technique from thelanguage identification domain, a single phonerecognizer followed by multiple language-dependent language models.
This methoddetermines the language of a speech samplebased on the phonotactic differences amonglanguages.The method is intended for use with non-native English speakers.
Therefore, themethod must be able to distinguish non-English responses from non-native speakers?English responses.
This makes the task morechallenging, as the frequent pronunciation er-rors of non-native speakers may weaken thephonetic and phonotactic distinction betweenEnglish responses and non-English responses.In order to address this issue, the speakingrate measure was used to complement thelanguage identification based features in themodel.The accuracy of the method was 98%, andthere was 45% relative error reduction overa system based on the conventional languageidentification technique.
The model usingboth feature sets furthermore demonstrated animprovement in accuracy for speakers at allEnglish proficiency levels.1 IntroductionWe developed a non-English response identifica-tion method as a supplementary module for the au-tomated speech proficiency scoring of non-nativespeakers.
The method can identify speech samplesof test takers who try to game the system by speak-ing in their native languages.
For the items thatelicited spontaneous speech, fluency features suchas speaking rate have been one of the most impor-tant features in the automated scoring.
By speak-ing in their native languages, speakers can generatefluent speech, and the automated proficiency scor-ing system may assign a high score.
This problemhas been rarely recognized, and none of research hasfocused on it as to the authors?
knowledge.
In or-der to address this issue, the automated proficiencyscoring system in this study first filters out the re-sponses in non-English languages, and for the re-maining responses, it predicts the proficiency scoreusing a scoring model.Non-English detection is strongly related to lan-guage identification(Lamel and Gauvain, 1993;Zissman, 1996; Li et al, 2007); language identifi-cation is the process of determining which languagea spoken response is in, while non-English detec-tion makes a binary decision whether the spoken re-sponse is in English or not.
Due to the strong simi-larity between the two tasks, the language identifica-tion method was used here for non-English responsedetection.In contrast to previous research, the method de-scribed here was intended for use with non-nativespeakers, and the English responses for model train-ing and evaluation were accordingly collected fromnon-native speakers.
Among other differences,non-native speakers?
speech tends to display non-standard pronunciation characteristics which can161make the task of language identification more chal-lenging.
For instance, when native Korean speak-ers speak English, they may replace some Englishphonemes not in their language with their nativephones, and epenthesize vowels within consonantclusters.
Such processes tend to reduce the pho-netic and phonotactic distinction between Englishand other languages.
The frequency of these pro-nunciation errors is influenced by speakers?
na-tive language and proficiency level, with lower-proficiency speakers likely to exhibit the greatestdegree of divergence from standard pronunciation.Language identification method may not effectivelydistinguish non-fluent speakers?
English responsesfrom non-English responses.
In order to addressthese non-native speech characteristics, the modeldescribed here includes the speaking rate feature,which has been found to be an indicator of speak-ing proficiency in previous research(Strik and Cuc-chiarini, 1999; Zechner et al, 2009).
Non-fluentspeakers?
English responses can be distinguishedfrom non-English responses by slow speaking rate.This paper will proceed as follows: we first re-view previous studies in section 2, then describe thedata in section 3, and present the experiment in sec-tion 4.
The results and discussion are presented insection 5, and the conclusions are presented in sec-tion 6.2 Previous WorkMany previous studies in language identificationfocused on phonetic and phonotactic differencesamong languages.
The frequencies of phones andphone sequences differ according to languages andsome phone sequences occur only in certain lan-guages.
The literature in language identificationcaptured this characteristic using the likelihoodscore of speech recognizers, which signals the de-gree of a match between the test sentences andspeech recognizer models.
Both the language model(hereafter, LM) and acoustic model (hereafter, AM)of a phone recognizer are optimized for the acousticcharacteristics and the phoneme distribution of thetraining data.
If a spoken response is recognized us-ing a recognizer trained on a different language, itmay result in a low likelihood score due to a mis-match between the test sentences and the models.Lamel and Gauvain (1993) trained multiplelanguage-dependent-phone-recognizers and se-lected the language with the highest matching scoreas the input language (hereafter, parallel PRLM).For instance, if the test data contained English andHindi speech data, the English-phone-recognizerand the Hindi-phone-recognizer were trained in-dependently.
In the test, the given speech sampleswere recognized using two phone recognizers,and the language that had a higher matchingscore was selected.
However, training multiplephone recognizers was time-consuming and laborintensive; therefore, Zissman (1996) proposed asystem using single-language phone recognitionfollowed by multiple language-dependent languagemodeling (hereafter, PRLM).
PRLM was able toachieve comparable performance to parallel PRLMfor long speech (longer than 30 seconds), and in atwo-language situation, the error rate was between5 and 7%.Instead of language-dependent LM, Li et al(2007) used vector space modeling (VSM).
Theyapplied metrics frequently used in information re-trieval.
As with the PRLM method, the speech wasconverted into phone sequences using the phone rec-ognizer, and cooccurrence statistics such as term fre-quency (TF) and inverse document frequency (IDF)were calculated.
The method outperformed thePRLM approach for long speech.These methods can be challenging and time-consuming to implement, as they require implemen-tation of methods beyond those typically availablein a standard word-based recognition system.
Inparticular, the application of the phone recognizerincreases the processing time substantially.
Be-cause of this problem, Lim et al (2004) presented amethod based on the features that were readily avail-able for speech recognizers: a confidence score andthe cross-entropy of the LM.
The confidence scoringmethod measured the acoustic match between theword hypotheses and the real sound, while the cross-entropy measured how well a sentence matched agiven language model.
If the test sentence was rec-ognized by the speech recognizer in a different lan-guage, the phonetic and lexical mismatches betweentwo languages resulted in a low confidence score anda high cross-entropy.
Using this methodology, Limet al (2004) achieved 99.8% accuracy in their three-162way classification task.The current study can be distinguished from theprevious studies in the following points.
First of all,special features were implemented to model non-native speech since the method was developed fornon-native speech.
In our study, the data containednon-native speakers?
English speech, characterizedby inaccurate pronunciation.
It resulted in a mis-match between the speech-recognizer models andtest sentences, even for utterances in English.
In par-ticular, the mismatch was more salient in non-fluentspeakers?
speech, which comprised a high propor-tion of our data.
In order to address this issue, speak-ing rate, which has achieved good performance inthe estimation of non-native speakers?
speaking pro-ficiency (Strik and Cucchiarini, 1999; Zechner etal., 2009), was implemented as an additional feature.Secondly, in contrast to previous studies that deter-mined which language the speech was in, we madea binary decision whether the speech was in Englishor not.
Finally, the method was developed as part ofa language assessment system.3 DataThe OGI Multi-language corpus (Muthusamy et al,1992), a standard language identification develop-ment data set, was used in the training and evalua-tion of the system.
It contains a total of 1,957 callsfrom speakers of 10 different languages (English,Farsi, French, German, Japanese, Korean, MandarinChinese, Spanish, Tamil, and Vietnamese).
The cor-pus was composed of short speech and long speech;the short files contained approximately 10 secondsspeech, while the long files contained speech rangedfrom 30 seconds to 50 seconds.The method described here was implemented todistinguish non-English responses from non-nativespeakers?
English responses.
Therefore, the Englishdata used to train and evaluate the model for non-English response detection was collected from non-native speakers.
In particular, responses to the En-glish Practice Test (EPT) were used.
The EPT isan online practice test which allows students to gainfamiliarity with the format of a high-stakes test ofEnglish proficiency and receive immediate feedbackon their test responses based on automated scor-ing methods.
The speaking section of the EPT as-sessment consists of 6 items in which speakers areprompted to provide open-ended responses of 45-60seconds in length.
The scoring scale of each item isdiscrete from 1 to 4, where 4 indicates high speakingproficiency and 1 low proficiency.The non-English detection task is composed oftwo major components: training of PRLM, andtraining of the classifier which makes a binary de-cision about whether a speech sample is in the En-glish language, given PRLM-based features and thespeaking rate.The OGI corpus was used in training of bothPRLM and the classifier; a total of 9,033 short filesfrom the OGI corpus were used in PRLM training,and 158 long files were used in classifier training.
(The small number of long files in the OGI corpuslimited the number of samples comparable in lengthto our English-language data described below, sothat only these 158 OGI samples could be used inclassifier training and evaluation.)
For English, onlyshort samples were selected for use in this experi-ment.In addition, a total of 3,021 EPT responses wereused in classifier training.
As the English profi-ciency levels of speakers may have an influenceon the accuracy of non-English response detection,the EPT responses were selected to include simi-lar numbers of responses for each score level.
Re-sponses were classified into four groups accordingto their proficiency scores and 1000 responses wererandomly selected from each group.
For score 1and 4, where the number of available responses wassmaller than 1000, all available responses were se-lected.
Ultimately, 156 responses for score 1, 1000responses for score 2 and score 3, and 865 responsesfor score 4 were selected.The resultant training and evaluation data sets aresummarized in Table 1.Due to the lack of non-Engilsh responses in EPTdata, 158 non-English utterances in OGI data wereused in both training and evaluation of non-Englishdetection.
EPT responses were collected from manydifferent countries, and speakers with 75 differentnative languages were participated in the data collec-tion.
Due to the large variations, many of their nativelanguages were not covered by OGI data.
However,all 9 languages in OGI data were in top 15 L1 lan-guages and covered approximately 60% of speakers?163Partition name Purpose Number ofEnglish filesNumber of non-English filesPRLM-train Training of Language-dependent LM1,716 (OGI) 7,317 (OGI)EN-detection Training and evaluation of non-English detection classifier3,021 (EPT) 158 (OGI)Table 1: Data partitionnative languages.4 Experiment4.1 OverviewDue to the efficiency in processing time and im-plementation, a PRLM was implemented instead ofa parallel PRLM.
However, the difference betweenPRLM and parallel PRLM in this study may not besignificant since PRLM has been shown to be com-parable to parallel PRLM for test samples longerthan 30 seconds, and the duration of test instances inthis study was longer than 30 seconds.
In additionto PRLM, speaking rate was calculated as a feature.4.2 PRLM based featuresThe PRLM based method in this study is composedof three parts: training of a phone recognizer, train-ing of language-dependent LMs, and generation ofPRLM-based features.
In contrast to the conven-tional language identification approach that only fo-cused on identifying the language with the highestmatching score, 6 additional features were imple-mented to capture the difference between Englishmodel and other models.Phone recognizer: An English triphone acousticmodel was trained on 30 hours of non-native Englishspeech (EPT data) using the HTK toolkit (Young etal., 2002).
The model contained 43 monophonesand 4,887 triphones.
Due to the difference in thesampling rate of EPT (11,025 Hz) and the OGI cor-pus (8,000 Hz), the EPT data was down-sampled to8,000 Hz and the acoustic model was trained usingthe down-sampled data.
In order to avoid the in-fluence of English in phone hypothesis generation,a triphone bigram language model with a uniformprobability distribution was used as the LM.
(Allpossible combinations of two triphones were gener-ated and a uniform probability was assigned to eachcombination.)
The phone recognition accuracy ratewas 42.7% on the 94 held-out EPT test samples.This phone recognizer was used in phone hypoth-esis generation for all data; the same recognizer wasused for all languages.Language-dependent LMs: For responses in thePRLM-train partition, phone hypothesis was gener-ated using the English recognizer.
Instead of themanual transcription, a language-dependent phonebigram LM was trained using the phone hypothe-sis.
In order to avoid a data sparseness problem, tri-phones were converted into monophones by remov-ing left and right context phones, and a bigram LMwith closed vocabulary was trained.
10 language-dependent bigram LMs, including one for English,were trained.PRLM based feature generation: For each re-sponse in the EN-detection partition, phone hypoth-esis was generated, and triphones were convertedinto monophones.
Given monophone hypothesis, anLM score was calculated for each language using alanguage-dependent LM.
A total of 10 LM scoreswere calculated.Since the LM score increases as the number ofphones increases, the LM score was normalized bythe number of phones in each response, in orderto avoid the influence of hypothesis length.
7 fea-tures were generated based on these normalized LMscores:?
MaxLanguage: The language with the maxi-mum LM score?
SecondLanguage: The language with thesecond-largest LM score.?
MaxScore: Normalized LM score of theMaxLanguage.164?
MaxDifference: Difference between normal-ized English LM score and MaxScore?
MaxRatio: Ratio between normalized EnglishLM score and MaxScore?
AverageDifference: Difference between nor-malized English LM score and the average ofnormalized LM scores for languages other thanEnglish?
AverageRatio: Ratio between normalized En-glish LM score and the average of normalizedLM scores for languages other than EnglishAmong above 6 features, 4 features (MaxDiffer-ence, MaxRatio, AverageDifference, and AverageR-atio) were designed to measure the difference be-tween matching of a test responses with Englishmodel and it with the other models.
These featuresmay be particularly effective when MaxLanguage ofthe English response is not English; these values willbe close to 0 when the divergence due to non-nativecharacteristics result in only slightly better matchwith other language than that with English.4.3 Speaking rate calculationThe speaking rate was calculated as a feature rele-vant to establishing speakers?
proficiency level, asestablished in previous research.
Speaking rate wascalculated from the phone hypothesis as the numberof phones divided by the duration of responses (cf.Strik and Cucchiarini (1999)).4.4 Model buildingFor each response, both PRLM-based features andspeaking rate were calculated, and a decision treemodel was trained to predict binary values (0 for En-glish and 1 for non-English) using the J48 algorithm(WEKA implementation of C4.5) of the WEKA ma-chine learning toolkit (Hall et al, 2009).Due to the limited number of non-English re-sponses in the EN-detection partition, three-foldcross validation was performed during classifiertraining and evaluation.
The 3,179 responses werepartitioned into three sets to include approximatelysame numbers of non-English responses and Englishresponses for each proficiency score group.
Eachpartition contained 52 ?
53 non-English responsesand 1007 English responses.
In each fold, the de-cision tree was trained using two of these partitionsand tested on the remaining one.5 EvaluationFirst, the accuracy of the PRLM method was eval-uated based on multiple forced-choice experimentswith two alternatives using OGI data; in additionto non-English responses in EN-detection partition,English responses from the OGI data were used inthis experiment.
For each response (in English andone other language), phone hypothesis was gener-ated and two normalized LM scores were calculatedusing the English LM and the LM for the other lan-guage.
The MaxLanguage was hypothesized as thesource language of the speech.
The same experimentwas performed for 9 combinations of English andother languages.
Each experiment was comprisedof 17 English utterances and 17 non-English utter-ances1.
The majority class baseline was thus 0.5.The mean accuracy of the 9 experiments in this studywas 0.943, which is comparable to (1996)?s perfor-mance: in his study, the best performing PRLMexhibited an average accuracy of 0.950.
This ini-tial evaluation used the same data and feature asZissman (1996).
(Of the seven PRLM-based fea-tures listed above, only MaxLanguage was used in(1996)?s study.
)Table 2 summarizes the evaluation results of thenon-English response detection experiments usingthree-fold cross-validation within the EN-detectionpartition.
In order to investigate the impact of dif-ferent types of features, the features were classi-fied into four sets?MaxLanguage only, PRLM(encompassing all PRLM features), SpeakingRate,and all?and models were trained using each set.The baseline using majority voting demonstrated anaccuracy of 0.95 by classifying all responses as En-glish responses.All models achieved improvements over baseline.In particular, the model using all features achieveda 66% relative error reduction over the baseline of0.95.
Furthermore, the all-features model outper-formed the model based only on PRLM or speaking1Due to the languages where the available responses wereonly 17, the same 17 English responses were used in the allexperiment although 18 responses were available165Features Acc.
Pre.
Rec.
F-scoreBase-line0.950 0.000 0.000 0.000Max-Language0.969 0.943 0.411 0.572PRLM 0.966 0.675 0.633 0.649Speaking-Rate0.962 0.886 0.278 0.415All 0.983 0.909 0.746 0.816Table 2: Performance of non-English response detectionrate; the accuracy of the all-features model was ap-proximately 1-2% higher than other models in abso-lute value and represented approximately a 45-50%relative error reduction over these models.The PRLM-based model had higher overall accu-racy than the speaking rate-based model, and the dif-ference was even more salient by the F-score mea-sure: the PRLM-based model achieved an F-scoreapproximately 24% higher than the speaking rate-based model.The model based on all PRLM features did notachieve a higher accuracy than the model based ononly MaxLanguage.
However, there was a clear im-provement in F-score by using the additional fea-tures.
The PRLM-based model achieved an F-scoreapproximately 8% higher than the model based onlyon MaxLanguage.In order to investigate the influence of speakers?proficiency on the accuracy of non-English detec-tion, the responses in EN-detection were dividedinto 4 groups according to proficiency score, and theperformance was calculated for each score group;the performance of each score group was calcu-lated using subset comprised of all non-English re-sponses and English responses with the correspond-ing scores.A majority class baseline (classifying all re-sponses as English) was again used.
Table 3 sum-marizes the results observed, by score level, for thebaseline model and for four different models used inTable 2.
Note that the baseline is lower in Table 3than in Table 2, because the ratio of English to non-English responses is lower for each of the subsets ofthe EN-detection partitions used for the evaluationsFigure 1: Relationship between proficiency score andMaxDifferenceat a given score level.For all score groups, the model using all featuresachieved high accuracy.
The model?s accuracy on alldata sets except for score group 1 was approximately0.96 and the F-score approximately 0.85.
The accu-racy on score group 1 was 0.87, relatively lower thanother score groups.
This is largely due to the smallernumber of English responses available at score level1, and the consequent lower baseline on this dataset.
However, the relative error reduction was muchlarger; it was 74% for score group 1.For all score groups, the PRLM-based mod-els outperformed MaxLanguage based models andspeaking rate based models.
Additional PRLMfeatures improved the performance over the mod-els only based on MaxLanguage (conventional lan-guage identification method).
In addition, the com-bination of both types of features resulted in furtherimprovement.The consistent improvement of the model usingboth PRLM and speaking rate features suggests acompensatory relationship between these features.In order to investigate this relationship in further de-tail, two representative features, MaxDifference andAverageDifference were selected, and boxplots werecreated.
Figure 1 and Figure 2 show the relationshipbetween proficiency score and PRLM features.
Inthese figures, the label ?NE?
is used to indicate thenon-English group, while the labels 1, 2, 3, and 4correspond to each score group.Figure 1 shows that MaxDifference decreases as166Score Features Acc.
Pre.
Rec.
F-score1 Baseline 0.497 0.000 0.000 0.000MaxLanguage 0.696 0.970 0.411 0.577PRLM 0.792 0.936 0.633 0.752SpeakingRate 0.636 1.000 0.278 0.432All 0.869 0.992 0.746 0.8512 Baseline 0.865 0.000 0.000 0.000MaxLanguage 0.919 0.983 0.411 0.579PRLM 0.930 0.811 0.633 0.709SpeakingRate 0.901 1.000 0.278 0.432All 0.962 0.971 0.746 0.8433 Baseline 0.865 0.000 0.000 0.000MaxLanguage 0.920 1.000 0.411 0.582PRLM 0.939 0.903 0.633 0.738SpeakingRate 0.901 0.983 0.278 0.430All 0.963 0.976 0.746 0.8454 Baseline 0.846 0.000 0.000 0.000MaxLanguage 0.908 0.987 0.411 0.579PRLM 0.936 0.934 0.633 0.752SpeakingRate 0.882 0.896 0.278 0.417All 0.955 0.956 0.746 0.837Table 3: Performance of non-English detection according to speakers?
proficiency levelFigure 2: Relationship between proficiency score and Av-erageDifferencethe speaker?s proficiency decreases, although thefeature displays a large variance.
The feature meanfor non-English responses is lower than for scoregroups 2, 3, and 4, but the distinction betweennon-English and English becomes smaller as theproficiency score decreases.
The feature mean forscore group 1 is even lower than for non-English re-sponses.
This obscures the distinction between En-glish responses and non-English responses at lowerscore levels.As Figure 2 shows, AverageDifference is rela-tively stable across score levels, compared to MaxD-ifference.
Although the mean feature value de-creases as the proficiency score decreases, the de-crease is smaller than for MaxDifference.
In addi-tion, the mean feature values of the English groupsare consistently higher than those for non-Englishresponses.Figure 3 shows the relationship between profi-ciency score and speaking rate.For the speaking rate feature, the distinction be-tween non-English and English responses increasesas speakers?
proficiency level decreases, as shown167Figure 3: Relationship between proficiency score andSpeakingRatein Figure 3.
The speaking rate of non-English re-sponses is the highest among all groups compared,and the speaking rate decreases for English re-sponses as the speaker?s proficiency score decreases.Thus, the PRLM features tend to display better dis-crimination between English and non-English re-sponses at the higher end of the proficiency scale,while the SpeakingRate feature provides better dis-crimination at the lower end of the scale.
By com-bining both feature classes, we are able to producea model which outperforms both a PRLM-basedmodel and a model using speaking rate alone.6 ConclusionIn this study, we presented a non-English responsedetection method for non-native speakers?
speech.
Adecision tree model was trained using PRLM-basedfeatures and speaking rate.The method was intended for use as a supple-mentary module of an automated speech proficiencyscoring system.
The characteristics of non-nativeEnglish speech (frequent pronunciation errors) re-duced the phonetic distinction between English re-sponses and non-English responses, and correspond-ingly, the differences between the feature values fornon-English and English speech decreased as well.In order to address this issue, a speaking rate fea-ture was added to the model.
This feature was spe-cialized for second language (L2) learners?
speech,as speaking rate has previously proved useful in es-timating non-native speakers?
speaking proficiency.In contrast to PRLM-based features, the speakingrate feature showed increasing discrimination be-tween non-English and English speech samples asspeakers?
proficiency level decreased.
The com-plementary relationship between PRLM-based fea-tures and speaking rate led to an improvement inthe model when these features were combined.
Im-provements resulting from the combined feature setextended across speakers at all proficiency levelsstudied in the context of this paper.The speaking rate becomes less effective if testtakers speak slowly in their native languages.
How-ever, the test takers are unlikely to use this strategy,since it will result in a low score although they cangame the system.Due to lack of non-English responses in EPT data,non-English utterances were extracted from OGIdata.
Since the features in this study were not di-rectly related to acoustic scores, the acoustic dif-ferences between EPT and OGI data may not givesignificant impact on the results.
However, in orderto avoid any influence by differences between cor-pora, the non-English responses will be collected us-ing EPT setup and the evaluation will be performedusing new data in future.ReferencesMark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Update.In SIGKDD Explorations, volume 11.Lori F. Lamel and Jean-Luc Gauvain.
1993.
Cross-lingual experiments with phone recognition.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech, and Signal Processing, volume 2,pages 507?510.Haizhou Li, Bin Ma, and Chin-Hui Lee.
2007.
A vec-tor space modeling approach to spoken language iden-tification.
Audio, Speech and Language Processing,15:271 ?
284.Boon Pang Lim, Haizhou Li, and Yu Chen.
2004.
Lan-guage identification through large vocabulary continu-ous speech recognition.
In Proceedings of the 2004 In-ternational Symposium on Chinese Spoken LanguageProcessing, pages 49 ?
52.Yeshwant K. Muthusamy, Ronald A. Cole, and Beat-rice T. Oshika.
1992.
The OGI multi-language tele-phone speech corpus.
In Proceedings of the Inter-168national Conference on Spoken Language Processing,pages 895?898.Helmer Strik and Catia Cucchiarini.
1999.
Automatic as-sessment of second language learners?
fluency.
In Pro-ceedings of the 14th International Congress of Pho-netic Sciences, pages 759?762, San Francisco, USA.Steve Young, Gunnar Evermann, Dan Kershaw, GarethMoore, Julian Odell, Dave Ollason, Dan Povey,Valtcho Valtchev, and Phil Woodland.
2002.
The HTKBook (for HTK Version3.2).
Microsoft Corporationand Cambridge University Engineering Department.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spoken en-glish.
Speech Communication, 51(10):883 ?
895.Marc A. Zissman.
1996.
Comparison of four ap-proaches to automatic language identification of tele-phone speech.
Speech and Audio Processing, 4:31 ?44.169
