Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101?109,Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational LinguisticsIdentification and Treatment of Multiword Expressions applied toInformation RetrievalOtavio Costa Acosta, Aline Villavicencio, Viviane P. MoreiraInstitute of InformaticsFederal University of Rio Grande do Sul (Brazil){ocacosta,avillavicencio,viviane}@inf.ufrgs.brAbstractThe extensive use of Multiword Expressions(MWE) in natural language texts promptsmore detailed studies that aim for a more ade-quate treatment of these expressions.
A MWEtypically expresses concepts and ideas thatusually cannot be expressed by a single word.Intuitively, with the appropriate treatment ofMWEs, the results of an Information Retrieval(IR) system could be improved.
The aim ofthis paper is to apply techniques for the au-tomatic extraction of MWEs from corpora toindex them as a single unit.
Experimental re-sults show improvements on the retrieval ofrelevant documents when identifying MWEsand treating them as a single indexing unit.1 IntroductionOne of the motivations of this work is to investi-gate if the identification and appropriate treatmentof Multiword Expressions (MWEs) in an applica-tion contributes to improve results and ultimatelylead to more precise man-machine interaction.
Theterm ?multiword expression?
has been used to de-scribe a large set of distinct constructions, for in-stance support verbs, noun compounds, institution-alized phrases and so on.
Calzolari et al (2002) de-fines MWEs as a sequence of words that acts as asingle unit at some level of linguistic analysis.The nature of MWEs can be quite heterogeneousand each of the different classes has specific char-acteristics, posing a challenge to the implementa-tion of mechanisms that provide unified treatmentfor them.
For instance, even if a standard system ca-pable of identifying boundaries between words, i.e.a tokenizer, may nevertheless be incapable of recog-nizing a sequence of words as an MWEs and treat-ing them as a single unit if necessary (e.g.
to kick thebucket meaning to die).
For an NLP application tobe effective, it requires mechanisms that are able toidentify MWEs, handle them and make use of themin a meaningful way (Sag et al, 2002; Baldwin etal., 2003).
It is estimated that the number of MWEsin the lexicon of a native speaker of a language hasthe same order of magnitude as the number of sin-gle words (Jackendoff, 1997).
However, these ra-tios are probably underestimated when consideringdomain-specific language, in which the specializedvocabulary and terminology are composed mostlyby MWEs.In this paper, we perform an application-orientedevaluation of the inclusion of MWE treatment intoan Information Retrieval (IR) system.
IR systemsaim to provide users with quick access to datathey are interested (Baeza-Yates and Ribeiro-Neto,1999).
Although language processing is not vi-tal to modern IR systems, it may be convenient(Sparck Jones, 1997) and in this scenario, NLP tech-niques may contribute in the selection of MWEs forindexing as single units in the IR system.
The se-lection of appropriate indexing terms is a key factorfor the quality of IR systems.
In an ideal system,the index terms should correspond to the conceptsfound in the documents.
If indexing is performedonly with the atomic terms, there may be a loss ofsemantic content of the documents.
For example, ifthe query was pop star meaning celebrity, and theterms were indexed individually, the relevant docu-ments may not be retrieved and the system would101return instead irrelevant documents about celestialbodies or carbonated drinks.
In order to investigatethe effects of indexing of MWEs for IR, the resultsof queries are analyzed using IR quality metrics.This paper is structured as follows: in section2 we discuss briefly MWEs and some of the chal-lenges they represent.
This is followed in section 3by a discussion of the materials and methods em-ployed in this paper, and in section 4 of the evalu-ation performed.
We finish with some conclusionsand future work.2 Multiword ExpressionsThe concept of Multiword Expression has beenwidely viewed as a sequence of words that acts as asingle unit at some level of linguistic analysis (Cal-zolari et al, 2002), or as Idiosyncratic interpreta-tions that cross word boundaries (or spaces) (Saget al, 2002).One of the great challenges of NLP is the identifi-cation of such expressions, ?hidden?
in texts of var-ious genres.
The difficulties encountered for identi-fying Multiword Expressions arise for reasons like:?
the difficulty to find the boundaries of a multi-word, because the number of component wordsmay vary, or they may not always occur in acanonical sequence (e.g.
rock the boat, rock theseemingly intransigent boat and the bourgeoisboat was rocked);?
even some of the core components of an MWEmay present some variation (e.g.
throw NP tothe lions/wolves/dogs/?birds/?butterflies);?
in a multilingual perspective, MWEs of asource language are often not equivalent totheir word-by-word translation in the target lan-guage (e.g.
guarda-chuva in Portuguese as um-brella in English and not as ?store rain).The automatic discovery of specific types ofMWEs has attracted the attention of many re-searchers in NLP over the past years.
With the recentincrease in efficiency and accuracy of techniques forpreprocessing texts, such as tagging and parsing,these can become an aid in improving the perfor-mance of MWE detection techniques.
In terms ofpractical MWE identification systems, a well knownapproach is that of Smadja (1993), who uses a setof techniques based on statistical methods, calcu-lated from word frequencies, to identify MWEs incorpora.
This approach is implemented in a lexico-graphic tool called Xtract.
More recently there hasbeen the release of the mwetoolkit (Ramisch et al,2010) for the automatic extraction of MWEs frommonolingual corpora, that both generates and vali-dates MWE candidates.
As generation is based onsurface forms, for the validation, a series of crite-ria for removing noise are provided, including some(language independent) association measures suchas mutual information, dice coefficient and maxi-mum likelihood.
Several other researchers have pro-posed a number of computational techniques thatdeal with the discovery of MWEs: Baldwin andVillavicencio (2002) for verb-particle constructions,Pearce (2002) and Evert and Krenn (2005) for col-locations, Nicholson and Baldwin (2006) for com-pound nouns and many others.For our experiments, we used some standard sta-tistical measures such as mutual information, point-wise mutual information, chi-square, permutationentropy (Zhang et al, 2006), dice coefficient, andt-test to extract MWEs from a collection of docu-ments (i.e.
we consider the collection of documentsindexed by the IR system as our corpus).3 Materials and MethodsBased on the hypothesis that the MWEs can improvethe results of IR systems, we carried out an evalua-tion experiment.
The goal of our evaluation is todetect differences between the quality of the stan-dard IR system, without any treatment for MWEs,and the same system improved with the identifica-tion of MWEs in the queries and in the documents.In this section we describe the different resourcesand methods used in the experiments.3.1 Resources and ToolsFor this evaluation we used two large newspaper cor-pora, containing a high diversity of terms:?
Los Angeles Times (Los Angeles, USA - 1994)?
The Herald (Glasgow, Scotland - 1995)Together, both corpora cover a large set of sub-jects present in the news published by these newspa-102pers in the years listed.
The language used is Amer-ican English, in the case of the Los Angeles Timesand British English, in the case of The Herald.
Here-after, the corpus of the Los Angeles Times will be re-ferred as LA94 and The Herald as GH95.
Together,they contain over 160,000 news articles (Table 1)and each news article is considered as a document.Corpus DocumentsLA94 110.245GH95 56.472Total 166.717Table 1: Total documentsThe collection of documents, as well as the querytopics and the list of relevance judgments (whichwill be discussed afterwards), were prepared in thecontext of the CLEF 2008 (Cross Language Eval-uation Forum), for the task entitled Robust-WSD(Acosta et al, 2008).
This task aimed to explorethe contribution of the disambiguation of words tobilingual or monolingual IR.
The task was to as-sess the validity of word-sense disambiguation forIR.
Thus, the documents in the corpus have been an-notated by a disambiguation system.
The structureof a document contains information about the identi-fier of a term in a document (TERM ID), the lemmaof a term (LEMA) and also its morphosyntactic tag(POS).
In addition, it contains the form in which theterm appeared in the text (WF) and information of theterm in the WordNet (Miller, 1995; Fellbaum, 1998)as SYNSET SCORE and CODE, both not used forthe experiment.
An example of the representation ofa term in the document is shown in Figure 1.<TERMID="GH950102-000000-126"LEMA="underworld"POS="NN"><WF>underworld</WF><SYNSET SCORE="0.5"CODE="06120171-n"/><SYNSET SCORE="0.5"CODE="06327598-n"/></TERM>Figure 1: Structure of a term in the original documentsIn this paper, we extracted the terms located inthe LEMA attribute, in other words, in their canonicalform (e.g.
letter bomb for letter bombs).
The use oflemmas and not the words (e.g.
write for wrote, writ-ten, etc.)
to the formation of the corpus, avoids lin-guistic variations that can affect the results of the ex-periments.
As a results, our documents were formedonly by lemmas and the next step is the indexingof documents using an IR system.
For this task weused a tool called Zettair (Zettair, 2008), which is acompact textual search engine that can be used bothfor the indexing and for querying text collections.Porter?s Stemmer (Porter, 1997) as implemented inZettair was also used.
Stemming can provide furtherconflation of related terms.
For example, bomb andbombing were not merged in the lemmatized textsbut after stemming they are conflated to a single rep-resentation.After indexing, the next step is the preparation ofthe query topics.
Just as the corpus, only the lemmasof the query topics were extracted and used.
The testcollection has a total of 310 query topics.
The judg-ment of whether a document is relevant to a querywas assigned according to a list of relevant docu-ments, manually prepared and supplied with the ma-terial provided by CLEF.
We used Zettair to generatethe ranked list of documents retrieved in responseto each query.
For each query topic, the 1,000 topscoring documents were selected.
We used the co-sine metric to calculate the scores and rank the doc-uments.Finally, to calculate the retrieval evaluation met-rics (detailed in Section 3.5) we used the tool treceval.
This tool compares the list of retrieved docu-ments (obtained from Zettair) against the list of rel-evant documents (provided by CLEF).3.2 Multiword Expression as Single TermsIn this work, we focused on MWEs composed ofexactly two words (i.e.
bigrams).
In order to incor-porate MWEs as units for the IR system to index,we adopted a very simple heuristics that concate-nated together all terms composing an MWE using?
?
(e.g.
letter bomb as letter bomb).
Figure 2 ex-emplifies this concatenation.
Each bigram present ina predefined dictionary and occurring in a documentis treated as a single term, for indexing and retrievalpurposes.
The rationale was that documents contain-ing specific MWEs can be indexed more adequatelythan those containing the words of the expressionseparately.
As a result, retrieval quality should in-crease.103<TERMID="GH950102-000000-126"LEMA="underworld"POS="NN"><WF>underworld</WF><SYNSET SCORE="0.5"CODE="06120171-n"/><SYNSET SCORE="0.5"CODE="06327598-n"/></TERM>OriginalTopic:-WhatwastheroleoftheHubbletelescopeinprovingtheexistenceofblackholes?ModifiedTopic:-whatbetheroleofthehubbletelescopeinprovetheexistenceofblackhole?black_hole<num>141</num><title>letterbombforkiesbauerfindinformationontheexplosionofaletterbombinthestudioofthetvchannelpro7presenterarabellakiesbauer.letter_bombletter_bombtv_channel</title>Figure 2: Modified query.3.3 Multiword Expressions DictionariesIn order to determine the impact of the quality ofthe dictionary used in the performance of the IR sys-tem, we examined several different sources of MWEof varying quality.
The dictionaries containing theMWEs to be inserted into the corpus as a singleterm, are created by a number of techniques involv-ing automatic and manual extraction.
Below we de-scribe how these MWE dictionaries were created.?
Compound Nouns (CN) - for the creation ofthis dictionary, we extracted all bigrams con-tained in the corpus.
Since the number of avail-able bigrams was very large (99,744,811 bi-grams) we filtered them using the informationin the original documents, the morphosyntactictags.
Along with the LEMA field, extracted inthe previous procedure, we also extracted thevalue of the field POS (part-of-speech).
In or-der to make the experiment feasible, we usedonly bigrams formed by compound nouns, inother words, when the POS of both words wasNN (Noun).
Thus, with bigrams consistingof sequences of NN as a preprocessing stepto eliminate noise that could affect the exper-iment, the number of bigrams with MWE can-didates was reduced to 308,871.
The next stepwas the selection of bigrams that had the high-est frequency in the text, so we chose candi-dates occurring at least ten times in the wholecorpus.
As a result, the first list of MWEs wascomposed by 15,001 bigrams, called D1.?
Best Compound Nouns - after D1, we re-fined the list with the use of statistical methods.The methods used were the mutual informationand chi-square.
It was necessary to obtain fre-quency values from Web using the search toolYahoo!, because despite the number of termsin the corpus, it was possible that the newspa-per genre of our corpus would bias the counts.For this work we used the number of pagesin which a term occurs as a measure of fre-quency.
With the association measures basedon web frequencies, we generated a ranking indecreasing order of score for each entry.
Wemerged the rankings by calculating the averagerank between the positions of each MWE; thefirst 7,500 entries composed the second dictio-nary, called D2.?
Worst Compound Nouns - this dictionary wascreated from bigrams that have between fiveand nine occurrences and are more likely to co-occur by chance.
It was created in order toevaluate whether the choice of the potentiallymore noisy MWEs entailed a negative effect inthe results of IR, compared to the previous dic-tionaries.
The third dictionary, with 17,328 bi-grams, is called D3.?
Gold Standard - this was created from a sub-list of the Cambridge International Dictionaryof English (Procter, 1995), containing MWEs.Since this list contains all types of MWEs,it was necessary to further filter these to ob-tain compound nouns only, using morphosyn-tactic information obtained by the TreeTagger(Schmid, 1994), which for English is reportedto have an accuracy of 96.36%?
(Schmid,1994).
Formed by 568 MWEs, the fourth dic-tionary will be called D4.?
Decision Tree - created from the use of theJ48 algorithm (Witten and Frank, 2000) fromWeka (Hall et al, 2009), a data mining tool.With this algorithm it is possible to make aMWE classifier in terms of a decision tree.
Thisrequires providing training data with true andfalse examples of MWE.
The training set con-tained 1,136 instances, half true (D4) and halffalse MWEs (taken from D3).
After combiningseveral statistical methods, the best result forclassification was obtained with the use of mu-tual information, chi-square, pointwise mutualinformation, and Dice.
The model obtainedfrom Weka was applied to test data containing15,001 MWE candidates (D1).
The 12,782 bi-grams classified as true compose the fifth dic-104tionary, called D5.?
Manual - for comparative purposes, we alsocreated two dictionaries by manually evaluat-ing the text of the 310 query topics.
The firstdictionary contained all bigrams which wouldachieve a different meaning if the words wereconcatenated (e.g.
space shuttle).
This dictio-nary, was called D6 and contains 254 expres-sions.
The other one was created by a spe-cialist (linguist) who classified as true or falsea list of MWE candidates from the query top-ics.
The linguist selection of MWEs formed D7with 178 bigrams.3.4 Creating IndicesFor the experiments, we needed to manipulate thecorpus in different ways, using previously built dic-tionaries.
The MWEs from dictionaries have beeninserted in the corpus as single terms, as describedbefore.
For each dictionary, an index was created inthe IR system.
These indices are described below:1.
Baseline (BL) - corpus without MWE.2.
Compound Nouns (CN) - with 15 MWEs ofD1.3.
Best CN (BCN) - with 7,500 MWEs of D2.4.
Worst CN (WCN) - with 17,328 MWEs of D3.5.
Gold Standard (GS) - with 568 MWEs of D4.6.
Decision Tree (DT) - with 12,782 MWEs ofD5.7.
Manual 1 (M1) - with 254 MWEs of D6.8.
Manual 2 (M2) - with 178 MWEs of D7.3.5 Evaluation MetricsTo evaluate the results of the IR system, we needto use metrics that estimate how well a user?s querywas satisfied by the system.
IR evaluation is basedon recall and precision.
Precision (Eq.
1) is the por-tion of the retrieved documents which is actually rel-evant to the query.
Recall (Eq.
2) is the fractionof the relevant documents which is retrieved by theIRS.Precision(P ) =#Relevant?#Retrieved#Retrieved(1)Recall(R) =#Relevant?#Retrieved#Relevant(2)Precision and Recall are set-based measures,therefore, they do not take into consideration the or-dering in which the relevant items were retrieved.In order to evaluate ranked retrieval results the mostwidely used measurement is the average precision(AvP ).
AvP emphasizes returning more relevantdocuments earlier in the ranking.
For a set ofqueries, we calculate the Mean Average Precision(MAP) according to Equation 3 (Manning et al,2008).MAP (Q) =1|Q||Q|?j=11mjmj?k=1P (Rjk) (3)where |Q| is the number of queries, Rjk is the setof ranked retrieval results from the top result untildocument dk, and mj is the number of relevant doc-uments for query j.4 Experiment and EvaluationsThe experiments performed evaluate the insertion ofMWEs in results obtained in the IR system.
Theanalysis is divided into two evaluations: (A) totalset of query topics, where an overview is given ofthe MWE insertion effects and (B) topics modifiedby MWEs, where we evaluate only the query topicsthat contain MWEs.4.1 Evaluation AThis evaluation investigates the effects of insertingMWEs in documents and queries.
After each typeof index was generated, MWEs were also includedin the query topics, in accordance to the dictionar-ies used for each index (for Baseline BL, the querytopics had no modifications).With eight corpus variations, we obtained indi-vidual results for each one of them.
The resultspresented in Table 2 were summarized by the ab-solute number of relevant documents retrieved and105the MAP for the entire set of query topics.
In total,6,379 relevant documents are returned for the 310query topics.Index Rel.
Retrieved MAPBL 3,967 0.1170CN 4,007 0.1179BCN 3,972 0.1156WCN 3,982 0.1150GS 3,980 0.1193DT 4,002 0.1178M1 4,064 0.1217M2 4,044 0.1205Table 2: Results ?
Evaluation A.It is possible to see a small improvement in theresults for the indices M1 and M2 in relation to thebaseline (BL).
This happens because the choice ofcandidate MWEs was made from the contents of thedocument topics and not, as with other indices, fromthe whole corpus.
Considering the indices built withMWEs extracted from the corpus, the best result isindex GS.In second place, comes the CN index, witha subtle improvement over the Baseline.
BL surpris-ingly got a better result than the Best and Worst CN.The loss in retrieval quality as a result from MWEidentification for BCN was not expected.When comparing the gain or loss in MAP of indi-vidual query topics, we can see how the index BCNcompares to the Baseline: BCN had better MAP in149 and worse MAP in 108 cases.
However, the av-erage loss is higher than the average gain, this ex-plains why BL obtains a better result overall.
In or-der do decide if one run is indeed superior to an-other, instead of using the absolute MAP value, wechose to calculate a margin of 5%.
The intuitionbehind this is that in IR, a difference of less than5% between the results being compared is not con-sidered significant (Buckley and Voorhees, 2000).To be considered as gain the difference between thevalues resulting from two different indices for thesame query topic should be greater than 5%.
Differ-ences of less than 5% are considered ties.
This way,MAP values of 0.1111 and 0.1122 are consideredties.
Given this margin, we can see in Tables 3 and4 that the indices BCN and WCN are better com-pared to the baseline.
In the case of BCN, the gainis almost 20% of cases and the WCN, the differencebetween gain and loss is less than 2%.Gain 60 19.35%Loss 35 11.29%Ties 215 69.35%Total 310 100.00%Difference between Gain and Loss 8,06%Table 3: BCN x BaselineGain 26 8.39%Loss 21 6.77%Ties 263 84.84%Total 310 100.00%Difference between Gain and Loss 1.61%Table 4: WCN x BaselineFinally, this first experiment guided us towarda deeper evaluation of the query topics that haveMWEs, because there is a possibility that the MWEinsertions in documents can decrease the accuracyof the system on topics that have no MWE.4.2 Evaluation BThis evaluation studies in detail the effects on thedocument retrieval in response to topics in whichthere were MWEs.
For this purpose, we used thesame indices used before and we performed an in-dividual evaluation of the topics, to obtain a betterunderstanding on where the identification of MWEsimproves or degrades the results.As each dictionary was created using a differentmethodology, the number of expressions containedin each dictionary is also different.
Thus, for eachmethod, the number of query topics considered ashaving MWEs varies according to the dictionaryused.
Table 5 shows the number of query topicscontaining MWEs for each dictionary used, and as aconsequence, the percentage of modified query top-ics over the complete set of 310 topics.First, it is interesting to observe the values ofMAP for all topics that have been altered by theidentification of MWEs.
These values are shown inTable 6.As shown in Table 6 we verified that the GS in-dex obtained the best result compared to others.
This106Index Topics with MWEs % ModifiedBL 0 0.00%CN 75 24.19%BCN 41 13.23%WCN 28 9.03%GS 9 2.90%DT 51 16.45%M1 195 62.90%M2 152 49.03%Table 5: Topics with MWEsIndex MAPCN 0.1011BCN 0.0939WCN 0.1224GS 0.2393DT 0.1193M1 0.1262M2 0.1236Table 6: Results - Evaluation Bwas somewhat expected since the MWEs in that dic-tionary are considered ?real?
MWEs.
After GS, bestresults were obtained from the manual indices M1and M2.
The index that we consider as containingthe lowest confident MWEs (WCN), obtained betterresults than Decision Trees, Nominal Compoundsand Best Nominal Compounds, in this order.
Onepossible reason for this to happen is that the numberof MWEs inserted is higher than in the other indices.Compared with the BL, all indices with MWE inser-tion have improved more than degraded the results,in quantitative terms.
Our largest gain was withthe index GS, where 55.56% of the topics have im-proved, but the same index showed the highest per-centage of loss, 22.22%.
Analyzing the WCN, wecan identify that this index has the lowest gain com-pared to all other indices: 32.14%, although havingalso the lowest loss.
But, 60.71 % of the topics mod-ified had no significant differences compared to theBaseline.
Thus, we can conclude that the WCN in-dex is the one that modifies the least the result of aquery.
The indices CN and BCN had a similar result,and knowing that a dictionary used to create BCN isa subset of the dictionary CN, we can conclude thatthe gain values, choosing the best MWE candidates,does not affect the accuracy, which only improvessubtly.
But the computational cost for the insertionof these MWEs in the corpus was reduced by half.
Interms of gain percentage, indices M1 and M2 weresuperior only to WCN, but they are close to otherresults, including the DT index, which obtained anintermediate result between manual dictionaries andCN.
Analyzing some topics in depth, like topic 141(Figure 3), the best the result among all the indiceswas obtained by the CN.<TERMID="GH950102-000000-126"LEMA="underworld"POS="NN"><WF>underworld</WF><SYNSET SCORE="0.5"CODE="06120171-n"/><SYNSET SCORE="0.5"CODE="06327598-n"/></TERM>OriginalTopic:-WhatwastheroleoftheHubbletelescopeinprovingtheexistenceofblackholes?ModifiedTopic:-whatbetheroleofthehubbletelescopeinprovetheexistenceofblackhole?black_hole<num>141</num><title>letterbombforkiesbauerfindinformationontheexplosionofaletterbombinthestudioofthetvchannelpro7presenterarabellakiesbauer.letter_bombletter_bombtv_channel</title>Figure 3: Topic #141Table 7 shows the top ten scoring documents re-trieved for query topic 141 in the baseline.
The rele-vant document (in bold) is the fourth position in theBaseline.
After inserting the expression letter bombtwice (because it occurs twice in the original topic),and tv channel that were in dictionary D1 used by theCN index, the relevant document is scored higherand as a consequence is returned in the first posi-tion of the ranking(Table 8) .
The MAP of this topichas increased 75 percentage points, from 0.2500 inBaseline to 1.000 in the CN index.
We see also thatthe document that was in first position in the Base-line ranking, has its score decreased and was rankedin fourth position in the ranking given by the CN.This document contained information on a ?smallbomb located outside the of the Russian embassy?and has is not relevant to topic 141, being properlyrelegated to a lower position.An interesting fact about this topic is that only theMWE letter bomb influences the result.
This wasverified as in the index BCN, whose dictionary doesnot have this MWE, the topic was changed only be-cause of the MWE tv channel and there was no gainor loss for the result.The second highest gain was of M1 index, in topic173.
The gain was of 28 percentage points.
On theother hand, we found a downside in M1 and M2indices, although they improved results on average,they have reached very high values of loss in sometopics.107Position Document ScoreP1 LA043094-0230 0.470900P2 GH950823-000105 0.459994P3 GH951120-000182 0.439536P4 GH950610-000164 0.430784P5 GH950614-000122 0.428766P6 LA091894-0425 0.428429P7 GH950829-000082 0.422941P8 GH950220-000162 0.411968P9 GH950318-000131 0.406006P10 GH950829-000037 0.402806Table 7: Ranking for Topic #141 - BaselinePosition Document ScoreP1 GH950610-000164 0.457950P2 GH950614-000122 0.436753P3 GH950823-000105 0.423938P4 LA043094-0230 0.421757P5 GH951120-000182 0.400123P6 GH950829-000082 0.393195P7 LA091894-0425 0.386613P8 GH950705-000100 0.384116P9 GH950220-000162 0.382157P10 GH950318-000131 0.380471Table 8: Ranking for Topic #141 - CNIn sum, the MWEs insertion seems to improve re-trieval bringing more relevant documents, due to amore precise indexing of specific terms.
However,the use of these expressions also brought a negativeimpact for some cases, because some topics requirea semantic analysis to return relevant documents (asfor example topic 130, which requires relevant doc-uments to mention the causes of the death of KurtCobain ?
documents which mention his death with-out mentioning the causes were not considered rele-vant).5 Conclusions and Future WorkThis work consists in investigating the impact ofMultiword Expressions on applications, focusing oncompound nouns in Information Retrieval systems,and whether a more adequate treatment for these ex-pressions can bring possible improvements in the in-dexing these expressions.
MWEs are found in allgenres of texts and their appropriate use is being tar-geted for study, both in linguistics and computing,due to the different characteristic variations of thistype of expression, which ends up causing problemsfor the success of computational methods that aimtheir processing.In this work we aimed at achieving a better under-standing of several important points associated withthe use of Multiword Expressions in IR systems.
Ingeneral, the MWEs insertion improves the results ofretrieval for relevant documents, because the index-ing of specific terms makes it easier to retrieve spe-cific documents related to these terms.
Nevertheless,the use of these expressions made the results worsein some c]ases, because some topics require a se-mantic analysis to return relevant documents.
Someof these documents are related to the query, but donot satisfy all criteria in the query topic.
We con-clude also that the quality of MWEs used directlyinfluenced the results.For future work, we would like to use other MWEtypes and not just compound nouns as used in thiswork.
Other methods of extraction and a furtherstudy in Named Entities are good themes to comple-ment this subject.
A variation of corpora, differentfrom newspaper articles, because each domain has aspecific terminology, can also be an interesting sub-ject for further evaluation.ReferencesOtavio Acosta, Andre Geraldo, Viviane Moreira Orengo,and Aline Villavicencio.
2008.
Ufrgs@clef2008:Indexing multiword expressions for information re-trieval.
Aarhus, Denmark.
Working Notes of theWorkshop of the Cross-Language Evaluation Forum -CLEF.Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
1999.Modern Information Retrieval.
ACM Press / Addison-Wesley.Timothy Baldwin and Aline Villavicencio.
2002.
Ex-tracting the unextractable: A case study on verb-particles.
Sixth Conference on Computational NaturalLanguage Learning - CoNLL 2002.Timothy Baldwin, C. Bannard, T. Tanaka, and D. Wid-dows.
2003.
An empirical model of multiword ex-pression decomposability.
ACL 2003 Workshop onMultiword Expressions: Analysis, Acquisition andTreatment.108Chris Buckley and Ellen M. Voorhees.
2000.
Evaluatingevaluation measure stability.
In SIGIR ?00: Proceed-ings of the 23rd annual international ACM SIGIR con-ference on Research and development in informationretrieval, pages 33?40, New York, NY, USA.
ACM.Nicoletta Calzolari, Charles Fillmore, Ralph Grishman,Nancy Ide, Alessandro Lenci, Catherine MacLeod,and Antonio Zampolli.
2002.
Towards best prac-tice for multiword expressions in computational lex-icons.
Third International Conference on LanguageResources and Evaluation - LREC.Stefan Evert and Brigitte Krenn.
2005.
Using small ran-dom samples for the manual evaluation of statisticalassociation measures.
Computer Speech & Language- Special Issue on Multiword Expression - Volume 19,Issue 4, p. 450-466.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.Ray Jackendoff.
1997.
The architecture of the languagefaculty.
MIT Press.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schtze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.
1394399.George A. Miller.
1995.
Wordnet: a lexical database forenglish.
Commun.
ACM, 38:39?41, November.Jeremy Nicholson and Timothy Baldwin.
2006.
Interpre-tation of compound nominalisations using corpus andweb statistic.
Workshop on Multiword Expressions:Identifying and Exploiting Underlying Properties.Darren Pearce.
2002.
A comparative evaluation of collo-cation extraction techniques.
Third International Con-ference on Language Resources and Evaluation.Martin F. Porter.
1997.
An algorithm for suffix strip-ping.
pages 313?316, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.Paul Procter.
1995.
Cambridge international dictionaryof English.
Cambridge University Press, Cambridge,New York.Carlos Ramisch, Aline Villavicencio, and ChristianBoitet.
2010.
Multiword expressions in the wild?
themwetoolkit comes in handy.
In Coling 2010: Demon-strations, pages 57?60, Beijing, China, August.
Coling2010 Organizing Committee.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickiger.
2002.
Multiword expres-sions.
a pain in the neck for nlp.
Third InternationalConference on Computational Linguistics and intelli-gent Text Processing.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing.Frank Smadja.
1993.
Retrieving collocations from text:Xtract.
Computational Linguistics.Karen Sparck Jones.
1997.
What is the role of nlp in textretrieval?
University of Cambridge.Ian H. Witten and Eibe Frank.
2000.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann, San Fran-cisco.Zettair.
2008.
The zettair search engine.
(dispon?
?vel viaWWW em http://www.seg.rmit.edu.au/zettair).Yi Zhang, Valia Kordoni, Aline.
Villavicencio, andMarco Idiart.
2006.
Automated multiword expressionprediction for grammar engineering.
COLING/ACL2006 Workshop on Multiword Expressions: Identify-ing and Exploiting Underlying Properties.109
