First Joint Conference on Lexical and Computational Semantics (*SEM), pages 679?683,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsPenn: Using Word Similarities to better Estimate Sentence SimilaritySneha Jha and H. Andrew Schwartz and Lyle H. UngarUniversity of PennsylvaniaPhiladelphia, PA, USA{jhasneha, hansens, ungar}@seas.upenn.eduAbstractWe present the Penn system for SemEval-2012 Task 6, computing the degree of seman-tic equivalence between two sentences.
Weexplore the contributions of different vectormodels for computing sentence and word sim-ilarity: Collobert and Weston embeddings aswell as two novel approaches, namely eigen-words and selectors.
These embeddings pro-vide different measures of distributional simi-larity between words, and their contexts.
Weused regression to combine the different simi-larity measures, and found that each providespartially independent predictive signal abovebaseline models.1 IntroductionWe compute the semantic similarity between pairsof sentences by combining a set of similarity met-rics at various levels of depth, from surface wordsimilarity to similarities derived from vector mod-els of word or sentence meaning.
Regression is thenused to determine optimal weightings of the differ-ent similarity measures.
We use this setting to as-sess the contributions from several different wordembeddings.Our system is based on similarities computed us-ing multiple sets of features: (a) naive lexical fea-tures, (b) similarity between vector representationsof sentences, and (c) similarity between constituentwords computed using WordNet, using the eigen-word vector representations of words , and using se-lectors, which generalize words to a set of words thatappear in the same context.2 System DescriptionThis section briefly describes the feature sets used toarrive at a similarity measure between sentences.
Wecompare the use of word similarities based on threedifferent embeddings for words neural embeddingsusing recursive autoencoders, eigenwords and selec-tors.2.1 Neural Models of Word RepresentationAn increasingly popular approach is to learn repre-sentational embeddings for words from a large col-lection of unlabeled data (typically using a genera-tive model), and to use these embeddings to augmentthe feature set of a supervised learner.
These modelsare based on the distributional hypothesis in linguis-tics that words that occur in similar contexts tendto have similar meanings.
The similarities betweenthese vectors indicate similarity in the meanings ofcorresponding words.The state of the art model in paraphrase detectionuses an unsupervised recursive autoencoder (RAE)model based on an unfolding objective that learnfeature vectors for phrases in syntactic parse trees(Socher et al, 2011).
The idea of neural languagemodels is to jointly learn an embedding of wordsinto an n-dimensional vector space that capture dis-tributional syntactic and semantic information viathe words co-occurrence statistics.
Further detailsand evaluations of these embeddings are discussedin Turian et al (2010).Once the distributional syntactic and semanticmatrix is learned on an unlabeled corpus, one canuse it for subsequent tasks by using each words vec-tor to represent that word.
For initial word embed-dings, we used the 100-dimensional vectors com-679puted via the unsupervised method of Collobert andWeston (2008).
These word embeddings are matri-ces of size |V | ?
n where |V | is the size of the vo-cabulary and n is the dimensionality of the semanticspace.
This matrix usually captures co-occurrencestatistics and its values are learned.
We used theembeddings provided by Socher et al (2011).
Al-though the original paper employed a dynamic pool-ing layer in addition to the RAE that captures theglobal structure of the similarity matrix, we foundthe resulting sentence-level RAE itself was useful.In turn, we use these vector representations at thesentence level where the cosine similarity betweenthe sentence vectors serves as a measure of sentencesimilarity.
All parameters for the RAE layer are keptsame as described by Socher et al (2011).2.2 Eigenword SimilarityRecent spectral methods use large amounts of un-labeled data to learn word representations, whichcan then be used as features in supervised learnersfor linguistic tasks.
Eigenwords, a spectral methodfor computing word embeddings based on contextwords that characterize the meanings of words, canbe efficiently computed by a set of methods based onsingular value decomposition (Dhillon et al, 2011).Such representations are dense, low dimensionaland real-valued like the vector representations in theprevious section except that they are induced us-ing eigen-decomposition of the word co-occurrencematrix instead of neural networks.
This methoduses Canonical Correlation Analysis (CCA) be-tween words and their immediate contexts to es-timate word representations from unlabeled data.CCA is the analog to Principal Component Analysis(PCA) for pairs of matrices.
It computes the direc-tions of maximal correlation between a pair of matri-ces.
CCAs invariance to linear data transformationsenables proofs showing that keeping the dominantsingular vectors faithfully captures any state infor-mation.
(For this work, we used the Google n-gramcollection of web three-grams as the unlabeled data.
)Each dimension of these representations captures la-tent information about a combination of syntacticand semantic word properties.
In the original paper,the word embeddings are context-specific.
For thistask, we only use context-oblivious embeddings i.e.one embedding per word type for this task, basedon their model.
Word similarity can then be cal-culated as cosine similarity between the eigenwordrepresentation vectors for any two words.To move from word-level similarity to sentence-level a few more steps are necessary.
We adaptedthe method of matrix similarity given by Stevensonand Greenwood (2005).
One calculates similaritybetween all pairs of words, and each sentence is rep-resented as a binary vector (with elements equal to 1if a word is present and 0 otherwise).
The similaritybetween these sentences vectors ~a and~b is given by:s(~a,~b) =~aW~b|~a||~b|(1)where W is a semantic similarity matrix contain-ing information about the similarity of word pairs.Each element in matrix W represents the similarityof words according to some lexical or spectral simi-larity measure.2.3 Selector SimilarityAnother novel method to account for the similaritybetween words is via comparison of Web selectors(Schwartz and Gomez, 2008).
Selectors are wordsthat take the place of an instance of a target wordwithin its local context.
For example, in ?he ad-dressed the strikers at the rally?, selectors for ?strik-ers?
might be ?crowd?, ?audience?, ?workers?, or ?stu-dents?
words which can realize the same constituentposition as the target word.
Since selectors are de-termined based on the context, a set of selectors is anabstraction for the context of a word instance.
Thus,comparing selector sets produces a measure of wordinstance similarity.
A key difference between selec-tors and the eigenwords used in this paper are thatselectors are instance specific.
This has the benefitthat selectors can distinguish word senses, but thedrawback that each word instance requires its ownset of selectors to be acquired.Although selectors have previously only beenused for worse sense disambiguation, one can alsouse them to compute similarity between two wordinstances by taking the cosine similarity of vectorscontaining selectors for each instance.
In our case,we compute the cosine similarity for each pair ofnoun instances and populate the semantic similaritymatrix in formula (1) to generate a sentence-level680similarity estimate.
Combining web selector- basedword similarity features with the word embeddingsfrom the neural model gave us the best overall per-formance on the aggregated view of the data sets.2.4 Other Similarity MetricsKnowledge-Based.
We use WordNet to calculatesemantic distances between all open-class words inthe sentence pairs.
There are three classificationsof similarity metrics over WordNet: path-based,information- content based, and gloss-based (Ped-erson et al, 2004).
We chose to incorporate thosemeasures performing best in the Schwartz & Gomez(2011) application-oriented evaluation: (a) the path-based measure of Schwartz & Gomez (2008); (b)the information-content measure of Jiang & Conrath(1997) utilizing the difference in information con-tent between concepts and their point of intersection;(c) the gloss-based measure of Patwardhan & Peder-sen (2006).
By including metrics utilizing differentsources of information, we suspect they will eachhave something novel to contribute.Because WordNet provides similarity betweenconcepts (word senses), we take the maximum simi-larity between all senses of each word to be the sim-ilarity between the two words.
Such similarity canthen be computed between multiple pairs of wordsto populate the semantic similarity matrix W in for-mula (1) and generate sentence-level similarity esti-mates as described above.
The information-contentand path-based measures are restricted to compar-ing nouns and verbs and only across the same partof speech.
On the other hand, the gloss-based mea-sure, which relies on connections through conceptdefinitions, is more general and can compare wordsacross parts of speech.Surface Metrics.
We added the following set oflexical features to incorporate some surface infor-mation lost in the vector-based representations.?
difference in the lengths of the two sentences?
average length of the sentences?
number of common words based on exactstring match?
number of content words in common?
number of common words in base form?
number of similar numerals in the sentences3 Evaluation and ResultsWe combine the similarity metrics discussed previ-ously via regression (Pedregosa et al, 2011).
Weincluded the following sets of features:?
System-baseline: surface metrics, knowledge-based metrics.
(discussed in section 2.4).?
Neu: Neural Model similarity (section 2.1)?
Ew: Eigenword similarity (section 2.2)?
Sel: Selector similarity (section 2.3)To capture possible non-linear relations, we addeda squared and square-rooted column correspondingto each feature in the feature matrix.
We also triedto combine all the features to form composite mea-sures by defining multiple interaction terms.
Boththese sets of additional features improved the per-formance of our regression model.
We used all fea-tures to train both a linear regression model and aregularized model based on ridge regression.
Theregularization parameter for ridge regression was setvia cross-validation over the training set.
All pre-dictions of similarity values were capped within therange [0,1].
Our systems were trained on the follow-ing data sets:?
MSR-Paraphrase, Microsoft Research Para-phrase Corpus-750 pairs of sentences.?
MSR-Video, Microsoft Research Video De-scription Corpus-750 pairs of sentences.?
SMT-Europarl, WMT2008 development dataset (Europarl section)-734 pairs of sentences.Our performance in the official submission for theSemEval task can be seen in Table 1.
LReg indi-cates the run with linear regression, ELReg addsthe eigenwords feature and ERReg also uses eigen-words but with ridge regression.
At the time of sub-mission, we were not ready to test with the selectorfeatures yet.
Ridge regression consistently outper-formed linear regression for every run of our sys-tem, but overall Pearson score for our system usinglinear regression scored the highest.
Table 2 presentsa more thorough examination of results.681MSRpar MSRvid SMT-eur On-WN SMT-news ALLnrm Mean ALLtask-baseline .4334 .2996 .4542 .5864 .3908 .6732 (85) .4356 (70) .3110 (87)LReg .5460 .7818 .3547 .5969 .4137 .8043 (36) .5699 (41) .6497 (33)ELReg .5480 .7844 .3513 .6040 .3607 .8048 (34) .5654 (44) .6622 (27)ERReg .5610 .7857 .3568 .6214 .3732 .8083 (28) .5755 (37) .6573 (28)Table 1: Pearson?s r scores for the official submission.
ALLnrm: Pearson correlation after the system outputs for eachdataset are fitted to the gold standard using least squares, and corresponding rank.
Mean: Weighted mean across the5 datasets, where the weight depends on the number of pairs in the dataset.
ALL: Pearson correlation with the goldstandard for the five datasets, and corresponding rank.
Parentheses indicate official rank out of 87 systems.MSRpar MSRvid SMT-eur On-WN SMT-news Mean ALLsystem-baseline .5143 .7736 .3574 .5017 .3867 .5343 .6542+Neu .5243 .7811 .3772 .4860 .3410 .5318 .6643+Ew .5267 .7787 .3853 .5237 .4495 .5560 .6724+Sel .4973 .7684 .3129 .4812 .4016 .5306 .6492+Neu, +Ew .5481 .7831 .2751 .5576 .3424 .5404 .6647+Neu, +Sel .5230 .7775 .3724 .5327 .3787 .5684 .6818+Ew, +Sel .5239 .7728 .2842 .5191 .4038 .5320 .6554+Neu, +Ew, +Sel .5441 .7835 .2644 .5877 .3578 .5472 .6645Table 2: Pearson?s r scores for runs based on various combinations of features.
Mean: Weighted mean across the 5datasets, where the weight depends on the number of pairs in the dataset.
ALL: Pearson correlation with the goldstandard for the five datasets, and corresponding rank.Discussion.
In the aggregate, we see that each ofthe similarity metrics has the ability to improve re-sults when used with the right combination of otherfeatures.
For example, while selector similarity byitself does not seem to help overall, using this met-ric in conjunction with the neural model of similar-ity gives us our best results.
Interestingly, the oppo-site is true of eigenword similarity, where the bestresults are seen when they are independent of selec-tors or the neural models.
The decreased correla-tions can be accounted for by the new features intro-ducing over fitting, and one should note that no suchreductions in performance are significant comparedto the baseline, where as our best performance is asignificant (p < 0.05) improvement.There are a few potential directions for future im-provements.
We did not tune our system differentlyfor different data sets although there is evidence ofspecific features favoring certain data sets.
In thecase of the neural model of similarity we expectthat deriving phrase level representations from thesentences and utilizing the dynamic pooling layershould give us a more thorough measure of simi-larity beyond the sentence-level vectors we used inthis work.
For eigenwords, we would like to experi-ment with context-aware vectors as was described in(Dhillon et.
al, 2011).
Lastly, we were only able toacquire selectors for nouns, but we believe introduc-ing selectors for other parts of speech will increasethe power of the selector similarity metric.4 ConclusionIn this paper, we described two novel word-levelsimilarity metrics, namely eigenword similarity andselector similarity, that leverage Web-scale corporain order to build word-level vector representations.Additionally, we explored the use of a vector-modelat the sentence-level by unfolding a neural model ofsemantics.
We utilized these metrics in addition toknowledge-based similarity, and surface-level simi-larity metrics in a regression system to estimate sim-ilarity at the sentence level.
The performance of thefeatures varies significantly across corpora but at theaggregate, eigenword similarity, selector similarity,and the neural model of similarity all are shown to becapable of improving performance beyond standardsurface-level and WordNet similarity metrics alone.682ReferencesEneko Agirre, Daniel Cer, Mona Diab and AitorGonzalez.
2012.
The SemEval-2012 Task-6 : APilot on Semantic Textual Similarity.
In Proceed-ings of the 6th International Workshop on SemanticEvaluation (SemEval 2012).Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing : deepneural networks with multitask learning.
In Inter-national Conference on Machine Learning.
Pages160-167.Paramveer Dhillon, Dean Foster and Lyle Ungar.2011.
Multiview learning of word embeddings viaCCA.
In Proceedings of Neural Information Pro-cessing Systems.Jay Jiang and David Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical taxon-omy.
In Proceedings on International Conferenceon Research in Computational Linguistics, pages1933.Dekang Lin.
1997.
Using syntactic dependency aslocal context to resolve word sense ambiguity.
InProceedings of the 35th annual meeting of Associa-tion for Computational Linguistics, pages 64-71.Ted Pedersen, Siddharth Patwardhan and JasonMichelizzi.
2004.
WordNet::Similarity-measuringthe relatedness of concepts.
In Proceedings ofthe North American Chapter of the Association forComputational Linguistics.F.
Pedregosa, G. Varoquaux, A. Gramfort, V.Michel, B. Thirion, G. Grisel, M. Blondel, P. Pret-tenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A.Passos, D. Cournapeau, M. Brucher, M. Perrot, E.Duchesnay.
2011.
Scikit-learn: Machine Learningin Python.
Journal of Machine Learning Research.Vol 12.2825-2830Hansen A. Schwartz and Fernando Gomez.
2008.Acquiring knowledge from the web to be used as se-lectors for noun sense disambiguation.
In Proceed-ings of the Twelfth Conference on ComputationalNatural Language Learning.Hansen A. Schwartz and Fernando Gomez.
2011.Evaluating semantic metrics on tasks of conceptsimilarity.
In Proceedings of the twenty-fourthFlorida Artificial Intelligence Research Society.Palm Beach, Florida: AAAI Press.Richard Socher, Eric H. Huang, Jeffrey Penning-ton, Andrew Y. Ng and Christopher Manning.
2011.Dynamic Pooling and Unfolding Recursive Autoen-coders for Paraphrase Detection.
In Advances inNeural Information Processing Systems.Mark Stevenson and Mark A. Greenwood.
2005.
ASemantic Approach to IE Pattern Induction.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 379386.Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word representations: a simple and generalmethod for semi-supervised learning.
In Proceed-ings of the annual meeting of Association for Com-putational Linguistics.683
