Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 193?200, Vancouver, October 2005. c?2005 Association for Computational LinguisticsPredicting Sentences using N-Gram Language ModelsSteffen Bickel, Peter Haider, and Tobias SchefferHumboldt-Universita?t zu BerlinDepartment of Computer ScienceUnter den Linden 6, 10099 Berlin, Germany{bickel, haider, scheffer}@informatik.hu-berlin.deAbstractWe explore the benefit that users in sev-eral application areas can experience froma ?tab-complete?
editing assistance func-tion.
We develop an evaluation metricand adapt N -gram language models tothe problem of predicting the subsequentwords, given an initial text fragment.
Us-ing an instance-based method as base-line, we empirically study the predictabil-ity of call-center emails, personal emails,weather reports, and cooking recipes.1 IntroductionPrediction of user behavior is a basis for the con-struction of assistance systems; it has therefore beeninvestigated in diverse application areas.
Previousstudies have shed light on the predictability of thenext unix command that a user will enter (Motodaand Yoshida, 1997; Davison and Hirsch, 1998), thenext keystrokes on a small input device such as aPDA (Darragh and Witten, 1992), and of the trans-lation that a human translator will choose for a givenforeign sentence (Nepveu et al, 2004).We address the problem of predicting the subse-quent words, given an initial fragment of text.
Thisproblem is motivated by the perspective of assis-tance systems for repetitive tasks such as answer-ing emails in call centers or letters in an adminis-trative environment.
Both instance-based learningand N -gram models can conjecture completions ofsentences.
The use of N -gram models requires theapplication of the Viterbi principle to this particulardecoding problem.Quantifying the benefit of editing assistance to auser is challenging because it depends not only onan observed distribution over documents, but alsoon the reading and writing speed, personal prefer-ence, and training status of the user.
We developan evaluation metric and protocol that is practical,intuitive, and independent of the user-specific trade-off between keystroke savings and time lost due todistractions.
We experiment on corpora of service-center emails, personal emails of an Enron execu-tive, weather reports, and cooking recipes.The rest of this paper is organized as follows.We review related work in Section 2.
In Section 3,we discuss the problem setting and derive appropri-ate performance metrics.
We develop the N -gram-based completion method in Section 4.
In Section 5,we discuss empirical results.
Section 6 concludes.2 Related WorkShannon (1951) analyzed the predictability of se-quences of letters.
He found that written Englishhas a high degree of redundancy.
Based on this find-ing, it is natural to ask whether users can be sup-ported in the process of writing text by systems thatpredict the intended next keystrokes, words, or sen-tences.
Darragh and Witten (1992) have developedan interactive keyboard that uses the sequence ofpast keystrokes to predict the most likely succeed-ing keystrokes.
Clearly, in an unconstrained applica-tion context, keystrokes can only be predicted withlimited accuracy.
In the specific context of enteringURLs, completion predictions are commonly pro-193vided by web browsers (Debevc et al, 1997).Motoda and Yoshida (1997) and Davison andHirsch (1998) developed a Unix shell which pre-dicts the command stubs that a user is most likelyto enter, given the current history of entered com-mands.
Korvemaker and Greiner (2000) have de-veloped this idea into a system which predicts en-tire command lines.
The Unix command predic-tion problem has also been addressed by Jacobs andBlockeel (2001) who infer macros from frequentcommand sequences and predict the next commandusing variable memory Markov models (Jacobs andBlockeel, 2003).In the context of natural language, several typ-ing assistance tools for apraxic (Garay-Vitoria andAbascal, 2004; Zagler and Beck, 2002) and dyslexic(Magnuson and Hunnicutt, 2002) persons have beendeveloped.
These tools provide the user with a list ofpossible word completions to select from.
For theseusers, scanning and selecting from lists of proposedwords is usually more efficient than typing.
By con-trast, scanning and selecting from many displayedoptions can slow down skilled writers (Langlais etal., 2002; Magnuson and Hunnicutt, 2002).Assistance tools have furthermore been developedfor translators.
Computer aided translation systemscombine a translation and a language model in orderto provide a (human) translator with a list of sug-gestions (Langlais et al, 2000; Langlais et al, 2004;Nepveu et al, 2004).
Foster et al (2002) introducea model that adapts to a user?s typing speed in or-der to achieve a better trade-off between distractionsand keystroke savings.
Grabski and Scheffer (2004)have previously developed an indexing method thatefficiently retrieves the sentence from a collectionthat is most similar to a given initial fragment.3 Problem Setting and EvaluationGiven an initial text fragment, a predictor that solvesthe sentence completion problem has to conjectureas much of the sentence that the user currently in-tends to write, as is possible with high confidence?preferably, but not necessarily, the entire remainder.The perceived benefit of an assistance system ishighly subjective, because it depends on the expen-diture of time for scanning and deciding on sug-gestions, and on the time saved due to helpful as-sistance.
The user-specific benefit is influenced byquantitative factors that we can measure.
We con-struct a system of two conflicting performance indi-cators: our definition of precision quantifies the in-verse risk of unnecessary distractions, our definitionof recall quantifies the rate of keystroke savings.For a given sentence fragment, a completionmethod may ?
but need not ?
cast a completion con-jecture.
Whether the method suggests a completion,and how many words are suggested, will typicallybe controlled by a confidence threshold.
We con-sider the entire conjecture to be falsely positive if atleast one word is wrong.
This harsh view reflectsprevious results which indicate that selecting, andthen editing, a suggested sentence often takes longerthan writing that sentence from scratch (Langlais etal., 2000).
In a conjecture that is entirely acceptedby the user, the entire string is a true positive.
Aconjecture may contain only a part of the remainingsentence and therefore the recall, which refers to thelength of the missing part of the current sentence,may be smaller than 1.For a given test collection, precision and recallare defined in Equations 1 and 2.
Recall equalsthe fraction of saved keystrokes (disregarding theinterface-dependent single keystroke that is mostlikely required to accept a suggestion); precision isthe ratio of characters that the users have to scanfor each character they accept.
Varying the confi-dence threshold of a sentence completion method re-sults in a precision recall curve that characterizes thesystem-specific trade-off between keystroke savingsand unnecessary distractions.Precision =?accepted completions string length?suggested completions string length(1)Recall =?accepted completions string length?all queries length of missing part(2)4 Algorithms for Sentence CompletionIn this section, we derive our solution to the sen-tence completion problem based on linear interpola-tion of N -gram models.
We derive a k best Viterbidecoding algorithm with a confidence-based stop-ping criterion which conjectures the words that mostlikely succeed an initial fragment.
Additionally, we194briefly discuss an instance-based method that pro-vides an alternative approach and baseline for ourexperiments.In order to solve the sentence completion problemwith an N -gram model, we need to find the mostlikely word sequence wt+1, .
.
.
, wt+T given a wordN -gram model and an initial sequence w1, .
.
.
, wt(Equation 3).
Equation 4 factorizes the joint proba-bility of the missing words; the N -th order Markovassumption that underlies the N -gram model simpli-fies this expression in Equation 5.argmaxwt+1,...,wt+TP (wt+1, .
.
.
, wt+T |w1, .
.
.
, wt) (3)= argmaxwt+1,...,wt+TT?j=1P (wt+j |w1, .
.
.
, wt+j?1) (4)= argmaxT?j=1P (wt+j |wt+j?N+1, .
.
.
, wt+j?1) (5)The individual factors of Equation 5 are provided bythe model.
The Markov order N has to balance suffi-cient context information and sparsity of the trainingdata.
A standard solution is to use a weighted linearmixture of N -gram models, 1 ?
n ?
N , (Brown etal., 1992).
We use an EM algorithm to select mixingweights that maximize the generation probability ofa tuning set of sentences that have not been used fortraining.We are left with the following questions: (a)how can we decode the most likely completion effi-ciently; and (b) how many words should we predict?4.1 Efficient PredictionWe have to address the problem of finding themost likely completion, argmaxwt+1,...,wt+TP (wt+1, .
.
.
, wt+T |w1, .
.
.
, wt) efficiently, eventhough the size of the search space grows exponen-tially in the number of predicted words.We will now identify the recursive structure inEquation 3; this will lead us to a Viterbi al-gorithm that retrieves the most likely word se-quence.
We first define an auxiliary variable?t,s(w?1, .
.
.
, w?N |wt?N+2, .
.
.
, wt) in Equation 6; itquantifies the greatest possible probability over allarbitrary word sequences wt+1, .
.
.
, wt+s, followedby the word sequence wt+s+1 = w?1, .
.
.
, wt+s+N =w?N , conditioned on the initial word sequencewt?N+2, .
.
.
, wt.In Equation 7, we factorize the last transition andutilize the N -th order Markov assumption.
In Equa-tion 8, we split the maximization and introduce anew random variable w?0 for wt+s.
We can now referto the definition of ?
and see the recursion in Equa-tion 9: ?t,s depends only on ?t,s?1 and the N -grammodel probability P (w?N |w?1, .
.
.
, w?N?1).
?t,s(w?1, .
.
.
, w?N |wt?N+2, .
.
.
, wt) (6)= maxwt+1,...,wt+sP (wt+1, .
.
.
, wt+s, wt+s+1 = w?1,.
.
.
, wt+s+N = w?N |wt?N+2, .
.
.
, wt)= maxwt+1,...,wt+sP (w?N |w?1, .
.
.
, w?N?1) (7)P (wt+1, .
.
.
, wt+s, wt+s+1 = w?1,.
.
.
, wt+s+N?1 = w?N?1|wt?N+2, .
.
.
, wt)= maxw?0maxwt+1,...,wt+s?1P (w?N |w?1, .
.
.
, w?N?1) (8)P (wt+1, .
.
.
, wt+s?1, wt+s = w?0,.
.
.
, wt+s+N?1 = w?N?1|wt?N+2, .
.
.
, wt)= maxw?0P (w?N |w?1, .
.
.
, w?N?1)?t,s?1(w?0, .
.
.
, w?N?1|wt+N?2, .
.
.
, wt)(9)Exploiting the N -th order Markov assumption,we can now express our target probability (Equation3) in terms of ?
in Equation 10.maxwt+1,...,wt+TP (wt+1, .
.
.
, wt+T |wt?N+2, .
.
.
, wt) (10)= maxw?1,...,w?N?t,T?N (w?1, .
.
.
, w?N |wt?N+2, .
.
.
, wt)The last N words in the most likely sequenceare simply the argmaxw?1,...,w?N ?t,T?N (w?1, .
.
.
, w?N |wt?N+2, .
.
.
, wt).
In order to collect the precedingmost likely words, we define an auxiliary variable ?in Equation 11 that can be determined in Equation12.
We have now found a Viterbi algorithm that islinear in T , the completion length.
?t,s(w?1, .
.
.
, w?N |wt?N+2, .
.
.
, wt) (11)= argmaxwt+smaxwt+1,...,wt+s?1P (wt+1, ..., wt+s, wt+s+1 = w?1, ...,wt+s+N = w?N |wt?N+2, ..., wt)= argmaxw?0?t,s?1(w?0, .
.
.
, w?N?1|wt?N+2, .
.
.
, wt)P (w?N |w?1, .
.
.
, w?N?1) (12)The Viterbi algorithm starts with the most recentlyentered word wt and moves iteratively into the fu-ture.
When the N -th token in the highest scored ?
isa period, then we can stop as our goal is only to pre-dict (parts of) the current sentence.
However, since195there is no guarantee that a period will eventuallybecome the most likely token, we use an absoluteconfidence threshold as additional criterion: whenthe highest ?
score is below a threshold ?, we stopthe Viterbi search and fix T .In each step, Viterbi stores and updates|vocabulary size|N many ?
values?unfeasiblymany except for very small N .
Therefore, in Table1 we develop a Viterbi beam search algorithmwhich is linear in T and in the beam width.
Beamsearch cannot be guaranteed to always find themost likely word sequence: When the globallymost likely sequence w?t+1, .
.
.
, w?t+T has an initialsubsequence w?t+1, .
.
.
, w?t+s which is not amongthe k most likely sequences of length s, then thatoptimal sequence is not found.Table 1: Sentence completion with Viterbi beamsearch algorithm.Input: N -gram language model, initial sentence fragmentw1, .
.
.
, wt, beam width k, confidence threshold ?.1.
Viterbi initialization:Let ?t,?N (wt?N+1, .
.
.
, wt|wt?N+1, .
.
.
, wt) = 1;let s = ?N + 1;beam(s ?
1) = {?t,?N (wt?N+1, .
.
.
, wt|wt?N+1,.
.
.
, wt)}.2.
Do Viterbi recursion until break:(a) For all ?t,s?1(w?0, .
.
.
, w?N?1| .
.
.)
inbeam(s ?
1), for all wN in vocabulary, store?t,s(w?1, .
.
.
, w?N | .
.
.)
(Equation 9) in beam(s)and calculate ?t,s(w?1, .
.
.
, w?N | .
.
.)
(Equation12).
(b) If argmaxwN maxw?1,...,w?N?1?t,s(w?1, .
.
.
, w?N | .
.
.)
= period then break.
(c) If max ?t,s(w?1, .
.
.
, w?N |wt?N+1, .
.
.
, wt) < ?then decrement s; break.
(d) Prune all but the best k elements in beam(s).
(e) Increment s.3.
Let T = s+N .
Collect words by path backtracking:(w?t+T?N+1, .
.
.
, w?t+T )= argmax ?t,T?N (w?1, .
.
.
, w?N |...).For s = T ?N .
.
.
1:w?t+s = ?t,s(w?t+s+1, .
.
.
, w?t+s+N |wt?N+1, .
.
.
, wt).Return w?t+1, .
.
.
, w?t+T .4.2 Instance-based Sentence CompletionAn alternative approach to sentence completionbased on N-gram models is to retrieve, from thetraining collection, the sentence that starts most sim-ilarly, and use its remainder as a completion hypoth-esis.
The cosine similarity of the TFIDF representa-tion of the initial fragment to be completed, and anequally long fragment of each sentence in the train-ing collection gives both a selection criterion for thenearest neighbor and a confidence measure that canbe compared against a threshold in order to achievea desired precision recall balance.A straightforward implementation of this near-est neighbor approach becomes infeasible when thetraining collection is large because too many train-ing sentences have to be processed.
Grabski andScheffer (2004) have developed an indexing struc-ture that retrieves the most similar (using cosine sim-ilarity) sentence fragment in sub-linear time.
We usetheir implementation of the instance-based methodin our experimentation.5 Empirical Studieswe investigate the following questions.
(a) Howdoes sentence completion with N -gram modelscompare to the instance-based method, both in termsof precision/recall and computing time?
(b) Howwell can N -gram models complete sentences fromcollections with diverse properties?Table 2 gives an overview of the four documentcollections that we use for experimentation.
Thefirst collection has been provided by a large onlinestore and contains emails sent by the service centerin reply to customer requests (Grabski and Scheffer,2004).
The second collection is an excerpt of therecently disclosed email correspondence of Enron?smanagement staff (Klimt and Yang, 2004).
We use3189 personal emails sent by Enron executive JeffDasovich; he is the individual who sent the largestnumber of messages within the recording period.The third collection contains textual daily weatherreports for five years from a weather report provideron the Internet.
Each report comprises about 20sentences.
The last collection contains about 4000cooking recipes; this corpus serves as an example ofa set of thematically related documents that might befound on a personal computer.We reserve 1000 sentences of each data set fortesting.
As described in Section 4, we split theremaining sentences in training (75%) and tuning196Table 2: Evaluation data collections.Name Language #Sentences Entropyservice center German 7094 1.41Enron emails English 16363 7.17weather reports German 30053 4.67cooking recipes German 76377 4.14(25%) sets.
We mix N -gram models up to an orderof five and estimate the interpolation weights (Sec-tion 4).
The resulting weights are displayed in Fig-ure 1.
In Table 2, we also display the entropy of thecollections based on the interpolated 5-gram model.This corresponds to the average number of bits thatare needed to code each word given the precedingfour words.
This is a measure of the intrinsic redun-dancy of the collection and thus of the predictability.1111222233334455540% 20% 40% 60% 80% 100%cooking recipesweather reportsEnron emailsservice centerFigure 1: N -gram interpolation weights.Our evaluation protocol is as follows.
The beamwidth parameter k is set to 20.
We randomly draw1000 sentences and, within each sentence, a posi-tion at which we split it into initial fragment andremainder to be predicted.
A human evaluator ispresented both, the actual sentence from the collec-tion and the initial fragment plus current comple-tion conjecture.
For each initial fragment, we firstcast the most likely single word prediction and askthe human evaluator to judge whether they wouldaccept this prediction (without any changes), giventhat they intend to write the actual sentence.
We in-crease the length of the prediction string by one ad-ditional word and recur, until we reach a period orexceed the prediction length of 20 words.For each judged prediction length, we record theconfidence measure that would lead to that predic-tion.
With this information we can determine theresults for all possible threshold values of ?.
To saveevaluation time, we consider all predictions that areidentical to the actual sentence as correct and skipthose predictions in the manual evaluation.We will now study how the N -gram method com-pares to the instance-based method.
Figure 2 com-pares the precision recall curves of the two meth-ods.
Note that the maximum possible recall is typi-cally much smaller than 1: recall is a measure of thekeystroke savings, a value of 1 indicates that the usersaves all keystrokes.
Even for a confidence thresh-old of 0, a recall of 1 is usually not achievable.Some of the precision recall curves have a con-cave shape.
Decreasing the threshold value in-creases the number of predicted words, but it alsoincreases the risk of at least one word being wrong.In this case, the entire sentence counts as an incor-rect prediction, causing a decrease in both, precisionand recall.
Therefore ?
unlike in the standard in-formation retrieval setting ?
recall does not increasemonotonically when the threshold is reduced.For three out of four data collections, the instance-based learning method achieves the highest max-imum recall (whenever this method casts a con-jecture, the entire remainder of the sentence ispredicted?at a low precision), but for nearly allrecall levels the N -gram model achieves a muchhigher precision.
For practical applications, a highprecision is needed in order to avoid distracting,wrong predictions.
Varying the threshold, the N -gram model can be tuned to a wide range of differentprecision recall trade-offs (in three cases, precisioncan even reach 1), whereas the confidence thresholdof the instance-based method has little influence onprecision and recall.We determine the standard error of the precisionfor the point of maximum F1-measure.
For all datacollections and both methods the standard error isbelow 0.016.
Correct and incorrect prediction ex-amples are provided in Table 3 for the service centerdata set, translated from German into English.
Theconfidence threshold is adjusted to the value of max-imum F1-measure.
In two of these cases, the predic-tion nicely stops at fairly specific terms.How do precision and recall depend on the stringlength of the initial fragment and the string lengthof the completion cast by the systems?
Figure 3shows the relationship between the length of the ini-tial fragment and precision and recall.
The perfor-mance of the instance-based method depends cru-cially on a long initial fragment.
By contrast, when1970.50.60.70.80.910  0.2  0.4  0.6PrecisionRecallservice centerN-graminstance-based00.20.40.60.80  0.01  0.02  0.03  0.04  0.05PrecisionRecallEnron emailsN-graminstance-based00.20.40.60.810  0.02  0.04  0.06PrecisionRecallweather reportsN-graminstance-based00.20.40.60.810  0.05  0.1  0.15PrecisionRecallcooking recipesN-graminstance-basedFigure 2: Precision recall curves for N -gram and instance-based methods of sentence completion.Table 3: Prediction examples for service center data.Initial fragment (bold face) and intended, missing part PredictionPlease complete your address.
your address.Kindly excuse the incomplete shipment.
excuse theOur supplier notified us that the pants are undeliverable.
notified us that theThe mentioned order is not in our system.
not in our system.We recommend that you write down your login name and password.
that you write down your login name and password.The value will be accounted for in your invoice.
be accounted for in your invoice.Please excuse the delay.
delay.Please excuse our mistake.
the delay.If this is not the case give us a short notice.
us your address and customer id.the fragment length exceeds four with the N-grammodel, then this length and the accuracy are nearlyindependent; the model considers no more than thelast four words in the fragment.Figure 4 details the relation between string lengthof the prediction and precision/recall.
We see thatwe can reach a constantly high precision over the en-tire range of prediction lengths for the service centerdata with the N-gram model.
For the other collec-tions, the maximum prediction length is 3 or 5 wordsin comparison to much longer predictions cast by thenearest neighbor method.
But in these cases, longerpredictions result in lower precision.How do instance-based learning and N -gramcompletion compare in terms of computation time?The Viterbi beam search decoder is linear in the pre-diction length.
The index-based retrieval algorithmis constant in the prediction length (except for the fi-nal step of displaying the string which is linear butcan be neglected).
This is reflected in Figure 5 (left)which also shows that the absolute decoding timeof both methods is on the order of few millisecondson a PC.
Figure 5 (right) shows how prediction timegrows with the training set size.We experiment on four text collections with di-verse properties.
The N -gram model performs re-markably on the service center email collection.Users can save 60% of their keystrokes with 85%of all suggestions being accepted by the users, orsave 40% keystrokes at a precision of over 95%.
Forcooking recipes, users can save 8% keystrokes at60% precision or 5% at 80% precision.
For weatherreports, keystroke savings are 2% at 70% correctsuggestions or 0.8% at 80%.
Finally, Jeff Dasovichof Enron can enjoy only a marginal benefit: below1% of keystrokes are saved at 60% entirely accept-able suggestions, or 0.2% at 80% precision.How do these performance results correlate withproperties of the model and text collections?
In Fig-ure 1, we see that the mixture weights of the higherorder N -gram models are greatest for the servicecenter mails, smaller for the recipes, even smallerfor the weather reports and smallest for Enron.
With50% of the mixture weights allocated to the 1-grammodel, for the Enron collection the N -gram comple-tion method can often only guess words with highprior probability.
From Table 2, we can further-more see that the entropy of the text collection isinversely proportional to the model?s ability to solvethe sentence completion problem.
With an entropy1980.40.60.810  2  4  6  8  10  12  14  16  18  20PrecisionQuery lengthservice centerN-graminstance-based  00.20.40.60.812  4  6  8  10PrecisionQuery lengthEnron emailsN-graminstance-based00.20.40.60  2  4  6  8  10  12  14  16  18  20PrecisionQuery lengthweather reportN-graminstance-based  00.20.40.60.810  2  4  6  8  10  12  14  16  18  20PrecisionQuery lengthcooking recipesN-graminstance-based0.30.40.50.60.70.80  2  4  6  8  10  12  14  16  18  20RecallQuery lengthservice centerN-graminstance-based  00.050.12  4  6  8  10RecallQuery lengthEnron emailsN-graminstance-based00.050.10.150  2  4  6  8  10  12  14  16  18  20RecallQuery lengthweather reportN-graminstance-based0.050.10.150.20.250  2  4  6  8  10  12  14  16  18  20RecallQuery lengthcooking recipesN-graminstance-basedFigure 3: Precision and recall dependent on string length of initial fragment (words).0.40.50.60.70.80.90  2  4  6  8  10  12  14  16  18  20PrecisionPrediction lengthservice centerN-graminstance-based  00.20.40.60.82  4  6  8  10PrecisionPrediction lengthEnron emailsN-graminstance-based00.10.20.30.40  2  4  6  8  10  12  14  16  18  20PrecisionPrediction lengthweather reportN-graminstance-based00.20.40.60.810  2  4  6  8  10  12  14  16  18  20PrecisionPrediction lengthcooking recipesN-graminstance-based0.50.60.70.80.90  2  4  6  8  10  12  14  16  18  20RecallPrediction lengthservice centerN-graminstance-based00.20.40.62  4  6  8  10RecallPrediction lengthEnron emailsN-graminstance-based0.050.10.150  2  4  6  8  10  12  14  16  18  20RecallPrediction lengthweather reportN-graminstance-based00.20.40.60.810  2  4  6  8  10  12  14  16  18  20RecallPrediction lengthcooking recipesN-graminstance-basedFigure 4: Precision and recall dependent on prediction string length (words).of only 1.41, service center emails are excellentlypredictable; by contrast, Jeff Dasovich?s personalemails have an entropy of 7.17 and are almost asunpredictable as Enron?s share price.6 ConclusionWe discussed the problem of predicting how a userwill complete a sentence.
We find precision (thenumber of suggested characters that the user has toread for every character that is accepted) and recall(the rate of keystroke savings) to be appropriate per-formance metrics.
We developed a sentence com-pletion method based on N -gram language models.We derived a k best Viterbi beam search decoder.Our experiments lead to the following conclusions:(a) The N -gram based completion method has abetter precision recall profile than index-based re-trieval of the most similar sentence.
It can be tunedto a wide range of trade-offs, a high precision canbe obtained.
The execution time of the Viterbi beamsearch decoder is in the order of few milliseconds.
(b) Whether sentence completion is helpfulstrongly depends on the diversity of the documentcollection as, for instance, measured by the entropy.For service center emails, a keystroke saving of 60%can be achieved at 85% acceptable suggestions; bycontrast, only a marginal keystroke saving of 0.2%can be achieved for Jeff Dasovich?s personal emailsat 80% acceptable suggestions.
A modest but signif-icant benefit can be observed for thematically relateddocuments: weather reports and cooking recipes.19902468101  2  3  4  5  6  7  8  9  10  11Predictiontime -msPrediction lengthservice centern-graminstance-based10203040501  2  3Predictiontime -msPrediction lengthweather reportsn-graminstance-based  00.511.510  20  30  40  50  60  70  80  90  100Predictiontime -msTraining set size in %service centern-graminstance-based01020304010  20  30  40  50  60  70  80  90  100Predictiontime -msTraining set size in %weather reportn-graminstance-basedFigure 5: Prediction time dependent on prediction length in words (left) and prediction time dependent ontraining set size (right) for service center and weather report collections.AcknowledgmentThis work has been supported by the German Sci-ence Foundation DFG under grant SCHE540/10.ReferencesP.
Brown, S. Della Pietra, V. Della Pietra, J. Lai, andR.
Mercer.
1992.
An estimate of an upper boundfor the entropy of english.
Computational Linguistics,18(2):31?40.J.
Darragh and I. Witten.
1992.
The Reactive Keyboard.Cambridge University Press.B.
Davison and H. Hirsch.
1998.
Predicting sequences ofuser actions.
In Proceedings of the AAAI/ICML Work-shop on Predicting the Future: AI Approaches to TimeSeries Analysis.M.
Debevc, B. Meyer, and R. Svecko.
1997.
An adap-tive short list for documents on the world wide web.
InProceedings of the International Conference on Intel-ligent User Interfaces.G.
Foster, P. Langlais, and G. Lapalme.
2002.
User-friendly text prediction for translators.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.N.
Garay-Vitoria and J. Abascal.
2004.
A comparison ofprediction techniques to enhance the communicationof people with disabilities.
In Proceedings of the 8thERCIM Workshop User Interfaces For All.K.
Grabski and T. Scheffer.
2004.
Sentence completion.In Proceedings of the ACM SIGIR Conference on In-formation Retrieval.N.
Jacobs and H. Blockeel.
2001.
The learning shell:automated macro induction.
In Proceedings of the In-ternational Conference on User Modelling.N.
Jacobs and H. Blockeel.
2003.
Sequence predic-tion with mixed order Markov chains.
In Proceedingsof the Belgian/Dutch Conference on Artificial Intelli-gence.B.
Klimt and Y. Yang.
2004.
The Enron corpus: A newdataset for email classification research.
In Proceed-ings of the European Conference on Machine Learn-ing.B.
Korvemaker and R. Greiner.
2000.
Predicting Unixcommand lines: adjusting to user patterns.
In Pro-ceedings of the National Conference on Artificial In-telligence.P.
Langlais, G. Foster, and G. Lapalme.
2000.
Unit com-pletion for a computer-aided translation typing system.Machine Translation, 15:267?294.P.
Langlais, M. Loranger, and G. Lapalme.
2002.
Trans-lators at work with transtype: Resource and evalua-tion.
In Proceedings of the International Conferenceon Language Resources and Evaluation.P.
Langlais, G. Lapalme, and M. Loranger.
2004.Transtype: Development-evaluation cycles to boosttranslator?s productivity.
Machine Translation (Spe-cial Issue on Embedded Machine Translation Systems,17(17):77?98.T.
Magnuson and S. Hunnicutt.
2002.
Measuring the ef-fectiveness of word prediction: The advantage of long-term use.
Technical Report TMH-QPSR Volume 43,Speech, Music and Hearing, KTH, Stockholm, Swe-den.H.
Motoda and K. Yoshida.
1997.
Machine learningtechniques to make computers easier to use.
In Pro-ceedings of the Fifteenth International Joint Confer-ence on Artificial Intelligence.L.
Nepveu, G. Lapalme, P. Langlais, and G. Foster.
2004.Adaptive language and translation models for interac-tive machine translation.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.C.
Shannon.
1951.
Prediction and entropy of printedenglish.
In Bell Systems Technical Journal, 30, 50-64.W.
Zagler and C. Beck.
2002.
FASTY - faster typingfor disabled persons.
In Proceedings of the EuropeanConference on Medical and Biological Engineering.200
