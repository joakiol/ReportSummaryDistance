Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 57?64,Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational LinguisticsThe Web is not a PERSON, Berners-Lee is not an ORGANIZATION, andAfrican-Americans are not LOCATIONS:An Analysis of the Performance of Named-Entity RecognitionRobert KrovetzLexical ResearchHillsborough, NJ 08844rkrovetz@lexicalresearch.comPaul Deane Nitin MadnaniEducational Testing ServicePrinceton, NJ 08541{pdeane,nmadnani}@ets.orgAbstractMost work on evaluation of named-entityrecognition has been done in the context ofcompetitions, as a part of Information Extrac-tion.
There has been little work on any form ofextrinsic evaluation, and how one tagger com-pares with another on the major classes: PER-SON, ORGANIZATION, and LOCATION.We report on a comparison of three state-of-the-art named entity taggers: Stanford, LBJ,and IdentiFinder.
The taggers were comparedwith respect to: 1) Agreement rate on the clas-sification of entities by class, and 2) Percent-age of ambiguous entities (belonging to morethan one class) co-occurring in a document.We found that the agreement between the tag-gers ranged from 34% to 58%, depending onthe class and that more than 40% of the glob-ally ambiguous entities co-occur within thesame document.
We also propose a unit testbased on the problems we encountered.1 IntroductionNamed-Entity Recognition (NER) has been an im-portant task in Computational Linguistics for morethan 15 years.
The aim is to recognize and clas-sify different types of entities in text.
These mightbe people?s names, or organizations, or locations, aswell as dates, times, and currencies.
Performanceassessment is usually made in the context of In-formation Extraction, of which NER is generally acomponent.
Competitions have been held from theearliest days of MUC (Message Understanding Con-ference), to the more recent shared tasks in CoNLL.Recent research has focused on non-English lan-guages such as Spanish, Dutch, and German (Meul-der et al, 2002; Carreras et al, 2003; Rossler, 2004),and on improving the performance of unsupervisedlearning methods (Nadeau et al, 2006; Elsner et al,2009).There are no well-established standards for eval-uation of NER.
Since criteria for membership in theclasses can change from one competition to another,it is often not possible to compare performance di-rectly.
Moreover, since some of the systems in thecompetition may use proprietary software, the re-sults in a competition might not be replicable byothers in the community; however, this applies tothe state of the art for most NLP applications ratherthan just NER.Our work is motivated by a vocabulary as-sessment project in which we needed to identifymulti-word expressions and determine their asso-ciation with other words and phrases.
However,we found that state-of-the-art software for named-entity recognition was not reliable; false positivesand tagging inconsistencies significantly hinderedour work.
These results led us to examine the state-of-the-art in more detail.The field of Information Extraction (IE) has beenheavily influenced by the Information Retrieval (IR)community when it comes to evaluation of systemperformance.
The use of Recall and Precision met-rics for evaluating IE comes from the IR commu-nity.
However, while the IR community regularlyconducts a set of competitions and shared tasks us-ing standardized test collections, the IE communitydoes not.
Furthermore, NER is just one component57of an IE pipeline and any proposed improvementsto this component must be evaluated by determiningwhether the performance of the overall IE pipelinehas improved.
However, most, if not all, NER eval-uations and shared tasks only focus on intrinsic NERperformance and ignore any form of extrinsic eval-uation.
One of the contributions of this paper isa freely available unit test based on the systematicproblems we found with existing taggers.2 Evaluation MethodologyWe compared three state-of-the-art NER taggers:one from Stanford University (henceforth, Stanfordtagger), one from the University of Illinois (hence-forth, the LBJ tagger) and BBN IdentiFinder (hence-forth, IdentiFinder).The Stanford Tagger is based on Conditional Ran-dom Fields (Finkel et al, 2005).
It was trained on100 million words from the English Gigawords cor-pus.
The LBJ Tagger is based on a regularized av-erage perceptron (Ratinov and Roth, 2009).
It wastrained on a subset of the Reuters 1996 news cor-pus, a subset of the North American News Corpus,and a set of 20 web pages.
The features for boththese taggers are based on local context for a targetword, orthographic features, label sequences, anddistributional similarity.
Both taggers include non-local features to ensure consistency in the tagging ofidentical tokens that are in close proximity.
Identi-Finder is a state-of-the-art commercial NER taggerthat uses Hidden Markov Models (HMMs) (Bikel etal., 1999).Since we did not have gold standard annotationsfor any of the real-world data we evaluated on, weinstead compared the three taggers along two dimen-sions:?
Agreement on classification.
How well dothe taggers work on the three most diffi-cult classes: PERSON, ORGANIZATION, andLOCATION and, more importantly, to whatextent does one tagger agree with another?What types of mistakes do they make system-atically?11Although one could draw a distinction between named en-tity identification and classification, we focus on the final outputof the taggers, i.e., classified named entities.?
Ambiguity in discourse.
Although entitiescan potentially have more than one entity clas-sification, such as Clinton (PERSON or LO-CATION), it would be surprising if they co-occurred in a single discourse unit such as adocument.
How frequently does each taggerproduce multiple classifications for the sameentity in a single document?We first compared the two freely available, aca-demic taggers (Stanford and LBJ) on a corpus of425 million words that is used internally at the Ed-ucational Testing Service.
Note that we could notcompare these two taggers to IdentiFinder on thiscorpus since IdentiFinder is not available for publicuse without a license.Next, we compared all three taggers on the Amer-ican National Corpus.
The American National Cor-pus (ANC) has recently released a copy which istagged by IdentiFinder.2 Since the ANC is a pub-licly available corpus, we tagged it using both theStanford and LBJ taggers and could then compareall three taggers along the two intended dimensions.We found that the public corpus had many of thesame problems as the ones we found with our in-ternally used corpus.
Some of these problems havebeen discussed before (Marrero et al, 2009) but notin sufficient detail.The following section describes our evaluation ofthe Stanford and LBJ taggers on the internal ETScorpus.
Section 4 describes a comparison of all threetaggers on the American National Corpus.
Section 5describes the unit test we propose.
In Section 6, wepropose and discuss the viability of the ?one named-entity tag per discourse?
hypothesis.
In Section 7,we highlight the problems we find during our com-parisons and propose a methodology for improvedintrinsic evaluation for NER.
Finally, we concludein Section 8.3 Comparing Stanford and LBJIn this section, we compare the two academic tag-gers in terms of classification agreement by classand discourse ambiguity on the ETS SourceFindercorpus, a heterogeneous corpus containing approx-imately 425 million words, and more than 270, 0002http://www.anc.org/annotations.html58Person Organization LocationStanford LBJ Stanford LBJ Stanford LBJShiloh A.sub.1 RNA Santa Barbara Hebrew The New RepublicYale What Arnold FIGURE ASCII DNAMotown Jurassic Park NaCl Number: Tina MomLe Monde Auschwitz AARGH OMITTED Jr. Ph.DDrosophila T. Rex Drosophila Middle Ages Drosophila DrosophilaTable 1: A sampling of false positives for each class as tagged by the Stanford and LBJ taggersCommon Entities PercentagePerson 548,864 58%Organization 249,888 34%Location 102,332 37%Table 2: Agreement rate by class between the Stanford and LBJ taggersarticles.
The articles were extracted from a set of60 different journals, newspapers and magazines fo-cused on both literary and scientific topics.Although Named Entity Recognition is reportedin the literature to have an accuracy rate of 85-95%(Finkel et al, 2005; Ratinov and Roth, 2009), it wasclear by inspection that both the Stanford and theLBJ tagger made a number of mistakes.
The ETScorpus begins with an article about Tim Berners-Lee, the man who created the World Wide Web.At the beginning of the article, ?Tim?
as well as?Berners-Lee?
are correctly tagged by the Stanfordtagger as belonging to the PERSON class.
Butlater in the same article, ?Berners-Lee?
is incorrectlytagged as ORGANIZATION.
The LBJ tagger makesmany mistakes as well, but they are not necessarilythe same mistakes as the mistakes made by the Stan-ford tagger.
For example, the LBJ tagger sometimesclassifies ?The Web?
as a PERSON, and the Stan-ford tagger classifies ?Italian?
as a LOCATION.3Table 1 provides an anecdotal list of the ?entities?that were misclassified by the two taggers.4Both taggers produced about the same numberof entities overall: 1.95 million for Stanford, and3?Italian?
is classified primarily as MISC by the LBJ tagger.These terms are sometimes called Gentilics or Demonyms.4Both taggers can use a fourth class MISC in addition tothe standard entity classes PERSON, ORGANIZATION, andLOCATION.
We ran Stanford without the MISC class and LBJwith MISC.
However, the problems highlighted in this paperremain equally prevalent even without this discrepancy.1.8 million for LBJ.
The agreement rate betweenthe taggers is shown in Table 2.
We find that thehighest rate of agreement is for PERSONS, withan agreement rate of 58%.
The agreement rate onLOCATIONS is 37%, and the agreement rate onORGANIZATIONS is 34%.
Even on cases wherethe taggers agree, the classification can be incorrect.Both taggers classify ?African Americans?
as LO-CATIONS.5 Both treat ?Jr.?
as being part of a per-son?s name, as well as being a LOCATION (in fact,the tagging of ?Jr.?
as a LOCATION is more fre-quent in both).For our second evaluation criterion, i.e., within-discourse ambiguity, we determined the percent-age of globally ambiguous entities (entities that hadmore than one classification across the entire corpus)that occurred with multiple taggings within a singledocument.
This analysis showed that the problemsdescribed above are not anecdotal.
Table 3 showsthat at least 40% of the entities that have more thanone classification co-occur within a document.
Thisis true for both taggers and all of the named entityclasses.65The LBJ tagger classifies the majority of instances of?African American?
as MISC.6The LBJ tagger also includes the class MISC.
We looked atthe co-occurrence rate between the different classes and MISC,and we found that the majority of each group co-occurred withina document there as well.59Stanford LBJOverlap Co-occurrence Overlap Co-occurrencePerson-Organization 98,776 40% 58,574 68%Person-Location 72,296 62% 55,376 69%Organization-Location 80,337 45% 64,399 63%Table 3: Co-occurrence rates between entities with more than one tag for Stanford and LBJ taggersStanford-BBN LBJ-BBNCommon Entities Percentage Common Entities PercentagePerson 8034 28% 27,687 53%Organization 12533 50% 21,777 51%Location(GPE) 3289 28% 5475 47%Table 4: Agreement rate by class between the Stanford (and LBJ) and BBN IdentiFinder taggers on the ANC Corpus4 Comparing All 3 TaggersA copy of the American National Corpus was re-cently released with a tagging by IdentiFinder.
Wetagged the corpus with the Stanford and LBJ taggerto see how the results compared.We found many of the same problems with theAmerican National Corpus as we found with theSourceFinder corpus used in the previous section.The taggers performed very well for entities thatwere common in each class, but we found misclas-sifications even for terms at the head of the Zipfiancurve.
Terms such as ?Drosophila?
and ?RNA?
wereclassified as a LOCATION.
?Affymetrix?
was clas-sified as a PERSON, LOCATION, and ORGANI-ZATION.Table 4 shows the agreement rate between theStanford and IdentiFinder taggers as well as that be-tween the LBJ and IdentiFinder taggers.
A sampleof terms that were classified as belonging to morethan one class, across all 3 taggers, is given in Table5.All taggers differ in how the entities are tok-enized.
The Stanford tagger tags each componentword of the multi-word expressions separately.
Forexample, ?John Smith?
is tagged as John/PERSONand Smith/PERSON.
But it would be tagged as[PER John Smith] by the LBJ tagger, and similarlyby IdentiFinder.
This results in a higher overlap be-tween classes in general, and there is a greater agree-ment rate between LBJ and IdentiFinder than be-tween Stanford and either one.The taggers also differ in the number of entitiesthat are recognized overall, and the percentage thatare classified in each category.
IdentiFinder recog-nizes significantly more ORGANIZATION entitiesthan Stanford and LBJ.
IdentiFinder also uses a GPE(Geo-Political Entity) category that is not found inthe other two.
This splits the LOCATION class.
Wefound that many of the entities that were classified asLOCATION by the other two taggers were classifiedas GPE by IdentiFinder.Although the taggers differ in tokenization as wellas categories, the results on ambiguity in a discoursesupport our findings on the larger corpus.
The re-sults are shown in Table 6.
For both the Stanford andLBJ tagger, between 42% and 58% of the entitieswith more than one classification co-occur within adocument.
For IdentiFinder, the co-occurrence ratewas high for two of the groupings, but significantlyless for PERSON and GPE.5 Unit Test for NERWe created a unit test based on our experiences incomparing the different taggers.
We were particularabout choosing examples that test the following:1.
Capitalized, upper case, and lower case ver-sions of entities that are true positives for PER-SON, ORGANIZATION, and LOCATION (fora variety of frequency ranges).2.
Terms that are entirely in upper case that are notnamed entities (such as RNA and AAARGH).60Person/Organization Person/Location Organization/LocationBacillus Bacillus AffymetrixMichelob Aristotle Arp2/3Phenylsepharose ArrayOligoSelector ANOVASynagogue Auschwitz GodzillaTransactionalism Btk:ER MacbethTable 5: A sampling of terms that were tagged as belonging to more than one class in the American National CorpusStanford LBJ IdentiFinderOverlap Co-occurrence Overlap Co-occurrence Overlap Co-occurrencePerson-Org 5738 53% 2311 58% 8379 57%Person-Loc(GPE) 4126 58% 3283 43% 2412 22%Org-Loc(GPE) 5109 57% 4592 50% 4093 60%Table 6: Co-occurrence rates between entities with more than one tag for the American National Corpus3.
Terms that contain punctuation marks such ashyphens, and expressions (such as ?A.sub.1?
)that are clearly not named entities.4.
Terms that contain an initial, such as ?T.
Rex?,?M.I.T?, and ?L.B.J.?5.
Acronym forms such as ETS and MIT, somewith an expanded form and some without.6.
Last names that appear in close proximity to thefull name (first and last).
This is to check on theimpact of discourse and consistency of tagging.7.
Terms that contain a preposition, such as ?Mas-sachusetts Institute of Technology?.
This is in-tended to test for correct extent in identifyingthe entity.8.
Terms that are a part of a location as well as anorganization.
For example, ?Amherst, MA?
vs.?Amherst College?.An excerpt from this unit test is shown in Table 7.We provide more information about the full unit testat the end of the paper.6 One Named-Entity Tag per DiscoursePrevious papers have noted that it would be unusualfor multiple occurrences of a token in a document tobe classified as a different type of entity (Mikheevet al, 1999; Curran and Clark, 2003).
The Stan-ford and LBJ taggers have features for non-local de-pendencies for this reason.
The observation is sim-ilar to a hypothesis proposed by Gale, Church, andYarowsky with respect to word-sense disambigua-tion and discourse (Gale et al, 1992).
They hypoth-esized that when an ambiguous word appears in adocument, all subsequent instances of that word inthe document will have the same sense.
This hy-pothesis is incorrect for word senses that we find ina dictionary (Krovetz, 1998) but is likely to be cor-rect for the subset of the senses that are homony-mous (unrelated in meaning).
Ambiguity betweennamed entities is similar to homonymy, and for mostentities it is unlikely that they would co-occur in adocument.7 However, there are cases that are excep-tions.
For example, Finkel et al (2005) note that inthe CoNLL dataset, the same term can be used for alocation and for the name of a sports team.
Ratinovand Roth (2009) note that ?Australia?
(LOCATION)can occur in the same document as ?Bank of Aus-tralia?
(ORGANIZATION).Existing taggers treat the non-local dependenciesas a way of dealing with the sparse data problem,and as a way to resolve tagging differences by look-ing at how often one token is classified as one type7Krovetz (1998) provides some examples where differentnamed entities co-occur in a discourse, such as ?New York?
(city) and ?New York?
(state).
However, these are both in thesame class (LOCATION) and are related to each other.61This is not a Unit Test(a tribute to Rene Magritte and RMS)Although we created this test with humor, we intend it as a serioustest of the phenomena we encountered.
These problems includeambiguity between entities (such as Bill Clinton and Clinton,Michigan), uneven treatment of variant forms (MIT, M.I.T., andMassachusetts Institute of Technology - these should all belabeled the same in this text - are they?
), and frequent falsepositives such as RNA and T. Rex....Table 7: Excerpt from a Unit test for Named-Entity Recognitionversus another.
We propose that these dependenciescan be used in two other aspects: (a) as a sourceof error in evaluation and, (b) as a way to identifysemantically related entities that are systematic ex-ceptions.
There is a grammar to named entity types.
?Bank of Australia?
is a special case of Bank of[LOCATION].
The same thing is true for ?ChinaDaily?
as a name for a newspaper.
We propose thatco-occurrences of different labels for particular in-stances can be used to create such a grammar; at thevery least, particular types of co-occurrences shouldbe treated as an exception to what is otherwise anindication of a tagging mistake.7 DiscussionThe Message Understanding Conference (MUC) hasguidelines for named-entity recognition.
But theguidelines are just that.
We believe that there shouldbe standards.
Without such standards it is difficultto determine which tagger is correct, and how theaccuracy varies between the classes.We propose that the community focus on fourclasses: PERSON, ORGANIZATION, LOCA-TION, and MISC.
This does not mean that the otherclasses are not important.
Rather it is recognition ofthe following facts:?
These classes are more difficult than dates,times, and currencies.?
There is widespread disagreement between tag-gers on these classes, and evidence that they aremisclassifying unique entities a significant per-centage of the time.?
We need at least one class for handling termsthat do not fit into the first three classes.?
The first three classes have important value inother areas of NLP.Although we recognize that an extrinsic evalu-ation of named entity recognition would be ideal,we also realize that intrinsic evaluations are valu-able in their own right.
We propose that the exist-ing methodology for intrinsically evaluating namedentity taggers can be improved in the following man-ner:1.
Create test sets that are organized across a va-riety of domains.
It is not enough to work withnewswire and biomedical text.2.
Use standardized sets that are designed to testdifferent types of linguistic phenomena, andmake it a de facto norm to use more than oneset as part of an evaluation.3.
Report accuracy rates separately for the threemajor classes.
Accuracy rates should be furtherbroken down according to the items in the unittest that are designed to assess mistakes: or-thography, acronym processing, frequent falsepositives, and knowledge-based classification.4.
Establish a way for a tagging system to expressuncertainty about a classification.62The approach taken by the American NationalCorpus is a good step in the right direction.
Likethe original Brown Corpus and the British NationalCorpus, it breaks text down according to informa-tional/literary text types, and spoken versus writtentext.
The corpus also includes text that is drawn fromthe literature of science and medicine.
However, therelatively small number of files in the corpus makesit difficult to assess accuracy rates on the basis of re-peated occurrences within a document, but with dif-ferent tags.
Because there are hundreds of thousandsof files in the internal ETS corpus, there are manyopportunities for observations.
The tagged versionof the American National Corpus has about 8800files.
This is one of the biggest differences betweenthe evaluation on the corpus we used internally atETS and the American National Corpus.The use of a MISC class is needed for reasonsthat are independent of certainty.
This is why wepropose a goal of allowing systems to express thisaspect of the classification.
We suggest a meta-tag ofa question-mark.
The meta-tag can be applied to anyclass.
Entities for which the system is uncertain canthen be routed for active learning.
This also allows abasic separation of entities into those for which thesystem is confident of its classification, and those forwhich it is not.8 ConclusionAlthough Named Entity Recognition has a reportedaccuracy rate of more than 90%, the results showthey make a significant number of mistakes.
Thehigh accuracy rates are based on inadequate meth-ods for testing performance.
By considering onlythe entities where both taggers agree on the classifi-cation, it is likely that we can obtain improved accu-racy.
But even so, there are cases where both taggersagree yet the agreement is on an incorrect tagging.The unit test for assessing NER performance isfreely available to download.8As with Information Retrieval test collections, wehope that this becomes one of many, and that they beadopted as a standard for evaluating performance.8http://bit.ly/nertestAcknowledgmentsThis work has been supported by the Institutefor Education Sciences under grant IES PR/AwardNumber R305A080647.
We are grateful to MichaelFlor, Jill Burstein, and anonymous reviewers fortheir comments.ReferencesDaniel M. Bikel, Richard M. Schwartz, and Ralph M.Weischedel.
1999.
An Algorithm that Learns What?sin a Name.
Machine Learning, 34:211?231.Xavier Carreras, Llus Mrquez, and Llus Padr.
2003.Named entity recognition for Catalan using Spanishresources.
In Proceedings of EACL.James R. Curran and Stephen Clark.
2003.
LanguageIndependent NER using a Maximum Entropy Tagger.In Proceeding of the 7th Conference on ComputationalNatural Language Learning (CoNLL), pages 164?167.Micha Elsner, Eugene Charniak, and Mark Johnson.2009.
Structured Generative Models for UnsupervisedNamed-Entity Clustering.
In Proceedings of NAACL,pages 164?172.Jenny Rose Finkel, Trond Grenager, and Christopher D.Manning.
2005.
Incorporating Non-local Informationinto Information Extraction Systems by Gibbs Sam-pling.
In Proceedings of ACL, pages 363?370.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One Sense per Discourse.
In Pro-ceedings of the ARPA Workshop on Speech and Natu-ral Language Processing, pages 233?237.Robert Krovetz.
1998.
More than One Sense Per Dis-course.
In Proceedings of the ACL-SIGLEX Work-shop: SENSEVAL-1.Monica Marrero, Sonia Sanchez-Cuadrado, Jorge MoratoLara, and George Andreadakis.
2009.
Evaluation ofNamed Entity Extraction Systems.
Advances in Com-putational Linguistics, Research in Computing Sci-ence, 41:47?58.Fien De Meulder, V Eronique Hoste, and Walter Daele-mans.
2002.
A Named Entity Recognition System forDutch.
In Computational Linguistics in the Nether-lands, pages 77?88.Andrei Mikheev, Marc Moens, and Claire Grover.
1999.Named Entity Recognition Without Gazetteers.
InProceedings of EACL, pages 1?8.David Nadeau, Peter D. Turney, and Stan Matwin.
2006.Unsupervised Named-Entity Recognition: GeneratingGazetteers and Resolving Ambiguity.
In Proceedingsof the Canadian Conference on Artificial Intelligence,pages 266?277.63L.
Ratinov and D. Roth.
2009.
Design Challengesand Misconceptions in Named Entity Recognition.
InProceedings of the Thirteenth Conference on Compu-tational Natural Language Learning (CoNLL), pages147?155.Marc Rossler.
2004.
Adapting an NER-System for Ger-man to the Biomedical Domain.
In Proceedings ofthe International Joint Workshop on Natural LanguageProcessing in Biomedicine and its Applications, pages92?95.64
