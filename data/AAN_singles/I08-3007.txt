Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 27?34,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingJoint Grammar Development by Linguists and ComputerScientistsMichael MaxwellCenter for Advanced Study of Language/University of MarylandCollege Park, Maryland, USAmmaxwell@casl.umd.eduAnne DavidCenter for Advanced Study of Language/University of MarylandCollege Park, Maryland, USAadavid@casl.umd.eduAbstractFor languages with inflectional morpho-logy, development of a morphologicalparser can be a bottleneck to furtherdevelopment.
We focus on two difficulties:first, finding people with expertise in bothcomputer programming and the linguisticsof a particular language, and second, theshort lifetime of software such as parsers.We describe a methodology to split parserbuilding into two tasks: descriptivegrammar development, and formalgrammar development.
The two grammarsare combined into a single document usingLiterate Programming.
The formalgrammar is designed to be independent of aparticular parsing engine?s programminglanguage, so that it can be readily ported toa new parsing engine, thus helping solvethe software lifetime problem.1 Problems for Grammar DevelopmentAfter several decades of widespread effort incomputational linguistics, the vast majority of theworld?s languages lack significant computationalresources.
For many languages, this is attributableto the lack of even more basic resources, such asstandardized writing systems or dictionaries.
Buteven for many languages that have been written forcenturies, computational resources are scarce.One resource that is needed for languages withsignificant inflectional morphology is a morpho-logical parser.1 To the degree that a language hascomplex morphology, parsers are difficult to build.1 In fact, it is more common to create a morphologicaltransducer, that is, a program which functions to both parseand generate inflected words.
However, because it is morefamiliar, in this paper we will frequently use the term ?parser.
?While there has been considerable research intoautomatically deriving a morphological parserfrom a corpus (see for example Creutz and Lagus,2007; Goldsmith, 2001; Goldsmith and Hu, 2004;and the papers in Maxwell, 2002), the results arestill far from producing reliable, wide-coverageparsers.
Hence most morphological parsers are stillbuilt by hand.
This paper focuses on practicalaspects of how such parsers can best be built, andpresents a model for collaborative development.Hand-built parsers suffer from at least twodrawbacks, which we will call the ?ExpertiseProblem?
and the ?Half-Life Problem.?
TheExpertise Problem concerns a difficulty forbuilding a parser in the first place: it is hard to findone person with the necessary knowledge of boththe linguistics of the target language and thecomputational technology for building parsers.The Half-Life Problem concerns the fact thatonce a parser has been built, its life is limited bythe life of the software it has been implemented in,and this lifetime is often short.The following subsections further describe thesetwo problems, while the remainder of the paperfocuses largely on the Expertise Problem.
Wefocus specifically on the development ofmorphological grammars.
The techniquesdescribed here may be usable with syntacticgrammars as well, but we have not investigatedthat problem.
We also focus in this paper on thedevelopment issue; testing and debugginggrammars is not discussed in this paper.1.1 The Expertise ProblemWriting software requires two kinds of expertise:knowledge of the problem to be solved, andknowledge of how to program software.
Forparsers, the problem-specific knowledge requiresunderstanding the grammar of the target language.Since everyone speaks at least one language, it27might seem that finding someone who understandsthe grammar of any particular language should beeasy.
Unfortunately, as generations of fieldlinguists have discovered, this is not true.
A nativespeaker?s knowledge of a language is notoriouslyimplicit; converting that knowledge into explicitrules is no simple task.
Furthermore, finding aspeaker of the language who combines explicitunderstanding of the grammar with softwareengineering skills is even more difficult.
Thedifficulty is compounded when the number ofspeakers of the language is small.
We thereforebelieve that for many languages of the world, forthe near future, the way to develop computationaltools in general, and morphological parsers inparticular, lies in teamwork.An example of the team approach was theBOAS project (Oflazer et al, 2001).
A BOASteam consisted of two people?a ?languageinformant?
and a programmer?plus a computerprogram which interviewed the informant andcreated the grammar rules.
The computer programis described as a ?linguist in a box?
(Oflazer et al,61).
The method we describe uses computationaltools, but purely human teamwork.A potential problem with the team approach liesin facilitating communication between teammembers.
While electronic communication makesdistributed teams possible, there is still a questionof how best to enable people with disparate skillsto actually understand each other.
We return to thisbelow, when we discuss our collaborative method.1.2 The Half-Life ProblemAnother problem with computational tools is theirlack of longevity.
While it would be difficult toformally investigate, we estimate the averagelifetime for computational linguistic tools to befive or ten years.
In part, this is due to the (lack of)longevity of the underlying software.2 Of course,some vendors provide backwards compatibility,and not all software becomes extinct thatquickly?but that is the meaning of ?half-life.
?Software obsolescence can be postponed by thejudicious choice of programming languages,2 One of us (Maxwell) was involved in a project in which twoof the programming languages became defunct before theprogram was complete.
In both cases, the cost of porting toalternative dialects of the programming language was deemedprohibitive.avoiding platform- or OS-specific commands, theuse of open source methods, etc.
However, this canonly prolong the life of a program, not extend itindefinitely.3 There are few if any programs thatwere written in 1980 that still run on any butcomputers outside of a museum?and 1980 wasonly twenty-seven years ago.In contrast, natural languages change slowly,apart from the infusion of new vocabulary.
Thegrammar of a language spoken today is unlikely tobe significantly different from the grammar of thatsame language fifty or a hundred years ago; andbarring catastrophe, any changes which do happenare likely to be incremental.One might argue that the short half-life ofsoftware is unimportant, since twenty years fromnow it may be possible to generate amorphological parser automatically from a corpusand a dictionary.
Perhaps, but this remains to beseen.
In the meanwhile, the time and effort that gointo writing such tools mandates that the tools beusable for long after the project is completed.Another motivation for wanting to build parsingtools with a longer half-life is that they constitute adescription of (part of) the grammar of a language,in two senses: first, the grammar that the parseruses is in effect a formal description of thelanguage?s morphology (or syntax).
This formaldescription has the advantage over traditionalgrammar descriptions of being unambiguous.A second way in which a parser constitutesdocumentation of a language is that it can be usedto analyze language texts, and?if it supports ageneration mode?to produce paradigms.
That is, aparser is an active description, not a static one.However, linguists have drawn attention to theissue of longevity for computer-based languagedocumentation and description.
In their seminalpaper, Bird and Simons (2003) point out that theuse of digital technologies brings the potential thatarchive language data can become unusable muchmore quickly than printed grammaticaldescriptions.
Indeed, scholars of today canunderstand grammars of South Asian languagespenned thousands of years ago.3 Old software can of course be kept on ?life support?
byrunning it on old machines running old operating systems.
Butthat is a solution for museums, not for software that isintended to be actively used.28Since a parser embodies a description of thegrammar of a language, it should be written toprovide an explicit, computationallyimplementable description of the language,portable to future parsing engines even after thelanguage is extinct.
As we show below, this is notan impossible goal.2 A method for Grammar DevelopmentWe have embarked on a project to buildmorphological parsers of languages in a way thatovercomes the Expertise and Half-Life problemsdescribed in the previous section.
The first parserwas for the Bengali, or Bangla, language.
Ourchoice of Bangla was driven by a number ofconsiderations, many of which are not relevanthere.
Most any language with a significant amountof inflectional morphology would have worked.However, in retrospect the choice was a good one,as it forced us to deal with a number of bothcomputational and linguistic issues that a morehighly resourced language such as Spanish wouldnot have presented.
At the same time, Bangla issufficiently documented by traditional grammarsthat the task was achievable, although not as easyas we had anticipated.We are writing two kinds of grammarssimultaneously: the first is a traditional descriptiveor reference grammar, written in English prose bya linguist (Anne David), intended to be read bylinguists.
The other is a formal grammar, written ina formal specification language, by acomputational linguist (Mike Maxwell) andintended for conversion into the programminglanguage of a parsing engine.
(Neither of us is aspeaker of the Bangla language.)
The twogrammars are intertwined, as described below, sothat each supports the other in such a way that wecan combine our differing expertise while alsoavoiding the lack of longevity that plaguestraditional parser development.The following subsections describe themethodology we are using, and its advantages.2.1 Descriptive GrammarThe descriptive grammar we have written is not, ofitself, ground-breaking.
Like most referencegrammars of the morphology of a language, it hasa chapter on the phonology and writing system ofBangla, and chapters for the various parts ofspeech.
The latter chapters describe the inflectional(and some derivational) affixes each part of speechtakes, and how the resulting inflected forms definethe paradigms.
The usage of these forms is alsodescribed, with examples sufficient to illustrate theusage; it is not, however, a pedagogical grammar.We were surprised to discover that no thoroughand reliable English-language descriptive grammarof modern colloquial Bangla exists, despite itshaving well over 200 million native speakers.Instead, we had to glean our description of Banglamorphology from half a dozen or so grammars ofvarying quality (some of them pedagogical4),several journal articles, and a couple ofdissertations.
Doing so meant comparing andreconciling sometimes widely differingdescriptions and analyses; three major problemswe encountered were contradictory accounts, lackof clarity, and gaps in coverage.
Writing a formalgrammar forced us to both resolve these issues andclarify our descriptive grammar.For example, we knew from our sources that thelocative/ instrumental case in Bangla has severalallomorphs; however, the descriptions of theirdistribution differed, and one of our chief sourceswas, in fact, quite vague on the conditioningenvironments.
Moreover, one particular vowelalternation that takes place in certain verb formsgoes unmentioned in nearly all of our sources andis inaccurately described in one of the two that domention it.
In this instance, a native speakerconfirmed the correct forms for us.
Opinionsamong the written sources on how to classifyBangla verbs differed widely as well, withanywhere from two to seven classes proposed.
Weended up choosing the system that defined sevenstem classes, since it is the only one that enablesthe generation of any verb form, given a stem.Resolving such problems was made easier bythe help of a consultant in the Bangla language.Professor Clint Seely, Emeritus of the Universityof Chicago.
He corrected our many mistakes andhelped clear up ambiguities in our sources.The difficulties we encountered inunderstanding grammatical descriptions, recon-ciling different grammatical accounts, and fillingin gaps in coverage underline the fact that wecould not have simply picked up a grammar and4 In fact, the clearest and most reliable sources of informationwere pedagogical grammars.29written a formal grammar from it.
For languageswhich have any degree of inflectionalcomplexity?and Bengali does, although there arelanguages with still more complicated mor-phologies?the problems are too great for such asimple approach.
One might ask why it is sodifficult to convert a published grammar into amorphological parser.
One answer is thatlanguages are inherently complex.
It is commonfor published descriptions to overlook complexity,either in the interest of presenting a simple andgeneral description, or perhaps because the authoris unaware of some of the issues.Also, as any reader or writer of technical papersknows, it is all too easy to talk about complextopics unclearly.
In our case, writing the formalgrammar at the same time as the descriptivegrammar forced a clarity and breadth of coveragein our descriptive grammar which we would nototherwise have attained.
Moreover, byincorporating a formal grammar into thedescriptive grammar, we have gone beyondprevious work on Bangla, or most other languages.The following section describes this.2.2 Formal GrammarFor the formal grammar of Bangla morphology, weneed a description which is unambiguous andcapable of being used to build a morphologicalparser.
As discussed above, ambiguity is a factabout natural language, and one which has longplagued software specification efforts (Berry andKamsties, 2003).
Building a parser from adescriptive grammar is analogous to buildingtraditional software from a software specification.Since our descriptive grammar is a naturallanguage specification, it is not what animplementer would want to rely on.
We thereforeneeded a formal language for grammar writing.One approach would be to use the programminglanguage of an existing parsing tool.
Amith andMaxwell (2005a) propose using the xfst language(the language of one of the Xerox finite state tools,see Beesley and Karttunen, 2003).
While thiswould meet the need for an unambiguousrepresentation, it would fail to meet our goal oflongevity: the Xerox tools will likely not be usedin ten years, and there is no reason to think thatwhatever morphological parsing engines areavailable then will use the same programminglanguage?nor that grammar engineers willunderstand the xfst programming language.Our formal grammar needs to be unambiguous,iconic, and self-documenting.
We have thereforechosen to represent our formal grammar in XML,and have developed an XML schema for encodinglinguistic structures, based on a UML modeldeveloped by SIL researchers.5 The design goalsof our XML schema are described in more detail inMaxwell and David (forthcoming).2.3 Combining Descriptive and FormalGrammarsHowever, as we have argued elsewhere (Amith andMaxwell, 2005a; 2005b), neither a descriptive nora formal grammar is adequate to our purposes byitself.
Descriptive grammars are inherentlyambiguous and sometimes vague, while formalgrammars are hard to understand.
If a formalgrammar could be combined with the descriptivegrammar, we would have an antidote to theseproblems: the combination could be neitherambiguous nor vague.The question is then whether there is a way tocombine the two sorts of grammars.
Such a methodwould need to support the following:(1) Developing the grammars in parallel.
(2) Combining the grammars so that thedescription of each aspect of the grammar ispresented to the human reader along with thecorresponding aspect of the formal grammar.
(3) Extracting the formal grammar for use by theparsing engine.In fact, there already is a method that accomplishes(2) and (3): Literate Programming, developed byDonald Knuth (1984, 1992) as a way of document-ing computer programs.
We use an XML/DocBook implementation of Literate Programming(Walsh and Muellner, 1999; Walsh, 2002), sinceXML provides numerous advantages for long-termarchiving (cf.
Bird and Simons, 2002).There remains the need for a methodology fordeveloping the descriptive and formal grammars inparallel, point (1) in the above list.
We turn to thisquestion in the next section.5 The SIL model can be downloaded fromhttp://fieldworks.sil.org/.302.4 Collaborative Grammar DevelopmentWe are writing our descriptive grammar of Banglain a commercial program, XMLmind (http://www.xmlmind.com/xmleditor/).
The formalgrammar is being written in a programmer?s editor,although with suitable style sheets, it could bewritten in XMLmind.
The formal grammarconsists of a number of ?fragments,?
each pairedwith a section in the descriptive grammar, so thatthe descriptive and formal grammaticaldescriptions are mutually supportive (see theappendix for a short excerpt).Our working arrangement is one of iterativedevelopment, with descriptive grammar writingleading formal grammar writing.
Crucially, thisiterative development allows frequent exchangesfor clarification.
A typical interchange (one whichactually took place) is the following.
The languageexpert writes a section of the descriptive grammaron Bengali noun qualifiers.
The computationalgrammar writer reads the description and tries toimplement it, but a question arises: is thediminutive qualifier used in all the environmentsthat the three allomorphs of the non-diminutivequalifier are used, or only one of thoseenvironments?
The language expert finds examplesshowing the diminutive in all environments,enabling the computational grammar writer toproceed.
Crucially, the descriptive grammar wasthen modified to clarify this issue, and to includethe new examples.Although we are writing our grammars a shorthallway apart, this interchange was accomplishedlargely by email; we could as well have been acontinent apart.In summary, our division of labor, together withthe fact that we are simultaneously developing thetwo kinds of grammar using our computationaltools and incorporating immediate feedback, hasmade possible a much better result than if one of uswrote the descriptive grammar, and the other laterwrote the formal grammar.2.5 Conversion to publishable grammarAs evident from the small portion of our grammarin the appendix, the formal grammar isunderstandable in its XML form, but it is not?pretty?
; nor does it bear any obvious resemblanceto modern linguistic formalisms.6 At the sametime, the use of XML means that a variety of toolsare available for editing the grammar, checking itsvalidity against the schema, and converting it intothe programming language of a parsing engine.Fortunately, the flexibility of XML makes itpossible to display (and eventually publish) theformal grammar using linguistic formalisms, suchas the following:__V#   /ktpktphhh????????????????????
?The ability to create such display forms of theunderlying XML data?referred to by Knuth as?weaving?
?is important as we look to publishingthe combined descriptive and formal grammar.
Thecreation of the style sheets necessary for this isplanned for next year.2.6 Conversion to parserTo build a parser from our grammar, we firstextract the formal grammar as an XML documentfrom the combined descriptive and formalgrammar.
This is a standard process in LiterateProgramming, called ?tangling?
; we use a simpleXSLT (Extensible Stylesheet LanguageTransformation), developed by Norman Walsh(http://docbook.sourceforge.net/release/litprog/current/fo/ldocbook.xsl).Second, the extracted XML formal grammar isread by a small Python program, then convertedinto the programming language of the targetmorphological parsing engine.A computer-readable lexicon must also beconverted into the programming language of theparsing engine, a comparatively simple task.Finally, the converted grammar and lexicon areread by the parsing engine to produce the parser.Currently, the target parsing engine is the StuttgartFinite State Transducer Tools (http://www.ims.uni-6 We have resisted the temptation to make our linguistics toomodern, since linguistic theories also have a short half-life.We model an eclectic but largely 1950s era version oflinguistics.
For example, phonological natural classes aredefined by listing the phonemes of which they are composed,rather than using distinctive features; we use orderedphonological rules, rather than Optimality Theory-styleconstraints rankings.
While these may be outmoded, they arequite understandable.31stuttgart.de/projekte/gramotron/SOFTWARE/SFST.html).
We fully expect that any choice ofparsing engine we make today will be supersededin the future by better and more capable parsingengines.
Targeting a different parsing engine willrequire rewriting only that part of the conversionprogram that re-writes the program-internalrepresentation into the target programminglanguage (plus a converter for the lexicon).Verifying that the conversion process workscorrectly with a new parsing engine will requirestandard test data.
Much of this test data can beautomatically extracted from the paradigm tablesand example sentences of the descriptive grammar.3 Previous workCollaborative work on natural language processingprograms is not of itself a new idea.
It is quitecommon to split up the task of developing agrammar among people with skills in linguistics,lexicography, and software development.
In thatsense, our work is very traditional.Ours is not even the first effort at developing aframework for collaborative development ofcomputational linguistic tools.
Butt et al (1999)describe the development of grammars in severallanguages, including English, French and German(with other languages added later).
However, theirfocus was on enabling collaboration amonggrammar writers working in different languages;each author was assumed to be more or less skilledin one target language and in computationallinguistics.
Their focus thus differs from ours in itsscope and in the nature of the collaboration.Copestake and Flickinger (2000) devote asection to ?Collaborative grammar coding,?
butconclude that in order to work on a (syntactic)parser, a developer needs to combine skills in thelinguistic theory being implemented, grammardebugging, and the grammar of the targetlanguage.
In our work, we are attempting to makeit possible to split this expertise between differentpeople, and to provide them with a collaborativetool.Significant effort has been directed at enablingcollaborative annotation of corpora, e.g.Cunningham et al 2002, and Ma et al 2002.
Thisis similar to our approach in allowing collaborationbetween annotators and experts (annotationsupervisors); but unlike our project, collaborativeannotation does not address grammar development.Finally, there are linguistic developmentenvironments such as SIL?s FLEx(www.sil.org/computing/fieldworks/flex/), and theplanned Montage project (Bender et al, 2004),which are intended to help linguists writecomputational grammars, incorporating orgenerating descriptive grammars.
While these areuseful tools?we are in fact looking into usingFLEx to produce interlinear sentences for ourgrammars?they are not intended for the samekind of collaborative effort that we describe here.4 ConclusionWhat is new about the project we describe istherefore the development of a computationalframework within which computationallyimplemented grammar development can be splitinto distinct tasks: one task for a person (or a team)with knowledge of a particular language, andanother task for a person (or team) with skills incomputer science.
(Lexicography may constitute athird task, depending on whether suitable machine-readable dictionaries are already available.
)If this division of labor we describe here wereapplicable only to the working relationshipbetween the authors, it would be of little generalinterest.
However, we believe a similar division ofskills between language expert and computationalexpert to be quite commonplace, making the samedivision of labor workable in a variety ofscenarios.
This has implications for the develop-ment of linguistic software in low densitylanguages: finding someone who is expert in both alanguage and its grammar, and in computationaltechniques, is likely to be particularly difficult inthe case of languages which have not been well-documented, or minority languages, or languagesspoken in countries where there is not a history ofwork in natural language processing.It is easy to imagine other scenarios where thisdivision of labor would work.
For example, thelinguistic team might be part of the language orlinguistics department of a university, while thecomputational team might be part of a computerscience department.
Grammar development couldeasily be an open source project, with thedevelopers never meeting face-to-face.32A question which occurred to us many timesduring this project is, who can best build agrammar or parser for a language: people like us,who are linguists but do not know the language, ornative speakers of the language?
The answer is notat all obvious.
We suggest that the answer isneither one?alone.
None of the language speakersor researchers we talked with in the course of thisproject had the expertise to build and test formalgrammars or morphological parsers.
At the sametime, when the grammars we consulted were notclear, or contradicted each other, we needed toconsult with native speakers or researchers todetermine the correct answers.Hence, we feel strongly that parsers andgrammars should be built by teams includingpeople with a variety of skills.
Given moderntechnology, it seems clear that the division of laborwhich our method allows means that there is noreason the people involved in the project need evenbe in the same country, or all speak the targetlanguage.In sum, we are developing a methodology tobuild certain kinds of NLP resources in lowerdensity languages, and we have demonstrated thistechnology for morphological parsing.ReferencesAmith, Jonathan D., and Maxwell, Michael.
2005a.Language Documentation: The Nahuatl Grammar.
InAlexander Gelbuck (ed.)
Computational Linguisticsand Intelligent Text Processing.
Lecture Notes inComputer Science.
474-485.
Berlin: Springer.Amith, Jonathan D., and Maxwell, Michael.
2005b.
?Language Documentation: Archiving Grammars.
?Chicago Linguistic Society 41.Beesley, Kenneth R., and Karttunen, Lauri.
2003.
FiniteState Morphology: CSLI Studies in ComputationalLinguistics.
Chicago: University of Chicago Press.Bender, Emily M.; Dan Flickinger; Jeff Good; and IvanA.
Sag.
2004.
?Montage: Leveraging Advances inGrammar Engineering, Linguistic Ontologies, andMark-up for the Documentation of UnderdescribedLanguages.?
Proceedings of the Workshop on FirstSteps for Language Documentation of MinorityLanguages: Computational Linguistic Tools forMorphology, Lexicon and Corpus Compilation,LREC 2004.Berry, Daniel M., and Kamsties, Erik.
2003.
Ambiguityin Requirements Specification.
In Julio CesarSampaio do Prado Leite and Jorge Horacio Doorn(eds.)
Perspectives on Software Requirements.
TheSpringer International Series in Engineering andComputer Science.
Vol.
753.
Berlin: Springer.Bird, Steven, and Simons, Gary.
2002.
SevenDimensions of Portability for LanguageDocumentation and Description.
In Proceedings ofthe Workshop on Portability Issues in HumanLanguage Technologies, Third InternationalConference on Language Resources and Evaluation.Paris: European Language Resources Association.Bird, Steven, and Simons, Gary.
2003.
Sevendimensions of portability for language documentationand description.
Language 79:557-582.Butt, Myriam, King, Tracy Holloway, Ni?o, Mar?a-Eugenia, and Segond, Fr?d?rique.
1999.
A GrammarWriter's Cookbook: CSLI Lecture Notes, 95.Stanford, CA: CSLI Publications.Copestake, Ann, and Flickinger, Dan.
2000.
An opensource grammar development environment andbroad-coverage English grammar using HPSG.
InProceedings of the Second conference on LanguageResources and Evaluation (LREC-2000).
Athens,Greece.Creutz, Mathias, and Lagus, Krista.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Transactions on Speech andLanguage Processing 4.Cunningham, H., Tablan, V., Bontcheva, K., andDimitrov, M. 2002.
Language engineering tools forcollaborative corpus annotation.http://citeseer.ist.psu.edu/734322.html.Goldsmith, John.
2001.
Unsupervised Learning of theMorphology of a Natural Language.
ComputationalLinguistics 27:153-198.Goldsmith, John , and Hu, Yu.
2004.
From Signatures toFinite State Automata.
Midwest ComputationalLinguistics Colloquium, Bloomington IN.Knuth, Donald E. 1984.
Literate programming.
TheComputer Journal 27:97-111.Knuth, Donald E. 1992.
Literate Programming: CSLILecture Notes.
Stanford: Center for the Study ofLanguage and Information.Ma, Xiaoyi, Lee, Haejoong, Bird, Steven, and Maeda,Kazuaki.
2002.
Models and Tools for CollaborativeAnnotation.
In Proceedings of the ThirdInternational Conference on Language Resourcesand Evaluation.
Paris: European LanguageResources Association.33Maxwell, Michael B.
2002.
Proceedings of theWorkshop on Morphological and PhonologicalLearning.
New Brunswick, NJ: ACL.Maxwell, Michael B., and Anne David.
Forthcoming.
?Interoperable Grammars.?
Paper to be presented atThe First International Conference on GlobalInteroperability for Language Resources (ICGL2008), Hong Kong.Nirenburg, Sergei, Biatov, Konstantin, Farwell, David,Helmreich, Stephen, McShane, Marjorie, Ponsford,Dan, Raskin, Victor, and Sheremetyeva, Svetlana.1999.
Toward Descriptive ComputationalLinguistics.http://crl.nmsu.edu/expedition/publications/boas-acl99.pdf.Oflazer, Kemal, Nirenburg, Sergei, and McShane,Marjorie.
2001.
Bootstrapping MorphologicalAnalyzers by Combining Human Elicitation andMachine Learning.
Computational Linguistics 27:59-85.Walsh, Norman, and Muellner, Leonard.
1999.DocBook: The Definitive Guide.
Sebastopol,California: O'Reilly & Associates, Inc.Walsh, Norman.
2002.
Literate Programming in XML.XML 2002, Baltimore, MD.Appendix: Sample Grammar Excerpt3.2.
Future TenseThe future tense is used to express:?
a future state or action?
propriety or ability [etc.
]?Person Suffix (C)VC- (C)aC- (C)V- (C)a- (C)V(i)- Causative 3-/on-a/to hear/thak-a/to stay/h-oya/to become/kha-oya/to eat/ca-oya/to want/ekha-no/to teach/kama-no/to bite1st -/-bo// n bo//thak bo//h-bo//kha-bo//cai-bo//ekha-bo//kama bo/Table 6.2: FutureTense Verb Forms[Additional rows omitted to save space]The formal grammar's listing of future tense suffixes appears below.<Mo:InflectionalAffix gloss="-1Fut" id="af1Fut"><!--The two "allomorphs" are really allographs--><Mo:Allomorph form=""><!--Spelled 'bo'; usually (not always) after a C-stem --></Mo:Allomorph><Mo:Allomorph form=""><!--Spelled 'b'; usually (not always) after a vowel stem --></Mo:Allomorph><Mo:inflectionFeatures><Fs:f name="Tense"><Fs:symbol value="Future"/></Fs:f><Fs:f name="Mood"><Fs:symbol value="Indicative"/></Fs:f><Fs:f name="Person"><Fs:symbol value="1"/></Fs:f></Mo:inflectionFeatures>/Mo:InflectionalAffix><!-- Etc.
for the remaining future tense suffixes -->34
