Modality and Negation:An Introduction to theSpecial IssueRoser Morante?University of AntwerpCaroline Sporleder?
?Saarland UniversityTraditionally, most research in NLP has focused on propositional aspects of meaning.
To trulyunderstand language, however, extra-propositional aspects are equally important.
Modalityand negation typically contribute significantly to these extra-propositional meaning aspects.Although modality and negation have often been neglected by mainstream computational lin-guistics, interest has grown in recent years, as evidenced by several annotation projects dedicatedto these phenomena.
Researchers have started to work on modeling factuality, belief and certainty,detecting speculative sentences and hedging, identifying contradictions, and determining thescope of expressions of modality and negation.
In this article, we will provide an overview of howmodality and negation have been modeled in computational linguistics.1.
IntroductionModality and negation are two grammatical phenomena that have been studied for along time.
Aristotle was the initial main contributor to the analysis of negation froma philosophical perspective.
Since then, thousands of studies have been performed, asillustrated by the Basic Bibliography of Negation in Natural Language (Seifert and Welte1987).
One of the first categorizations of modality is proposed by Otto Jespersen (1924)in the chapter about Mood, where the grammarian distinguishes between ?categoriescontaining an element of will?
and categories ?containing no element of will.?
Hisgrammar devotes also a chapter to negation.In contrast to the substantial number of theoretical studies, the computational treat-ment of modality and negation is a newly emerging area of research.
The emergenceof this area is a natural consequence of the consolidation of areas that focus on thecomputational treatment of propositional aspects of meaning, like semantic role label-ing, and a response to the need for processing extra-propositional aspects of meaningas a further step towards text understanding.
That there is more to meaning thanjust propositional content is a long-held view.
Prabhakaran, Rambow, and Diab (2010)?
CLiPS, University of Antwerp, Prinsstraat 13, B-2000 Antwerpen, Belgium.E-mail: roser.morante@ua.ac.be.??
Computational Linguistics, Saarland University, Postfach 15 11 50, D-66041 Saarbru?cken, Germany.E-mail: csporled@coli.uni-sb.de.Submission received: 5 April 2011; revised submission received: 18 January 2012; accepted for publication:24 January 2012.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 2illustrate this statement with the following examples, where the event LAY OFF(GM,workers) is presented with different extra-propositional meanings:(1) a. GM will lay off workers.b.
A spokesman for GM said GM will lay off workers.c.
GM may lay off workers.d.
The politician claimed that GM will lay off workers.e.
Some wish GM would lay of workers.f.
Will GM lay off workers?g.
Many wonder whether GM will lay off workers.Generally speaking, modality is a grammatical category that allows the expressionof aspects related to the attitude of the speaker towards her statements in terms ofdegree of certainty, reliability, subjectivity, sources of information, and perspective.
Weunderstand modality in a broad sense, which involves related concepts like ?subjec-tivity?, ?hedging?, ?evidentiality?, ?uncertainty?, ?committed belief,?
and ?factuality?.Negation is a grammatical category that allows the changing of the truth value ofa proposition.
A more detailed definition of these concepts with examples will bepresented in Sections 2 and 3.Modality and negation are challenging phenomena not only from a theoreticalperspective, but also from a computational point of view.
So far two main tasks havebeen addressed in the computational linguistics community: (i) the detection of variousforms of negation and modality and (ii) the resolution of the scope of modality andnegation cues.
Whereas modality and negation tend to be lexically marked, the classof markers is heterogeneous, especially in the case of modality.
Determining whethera sentence is speculative or whether it contains negated concepts cannot be achievedby simple lexical look-up of words potentially indicating modality or negation.
Modalverbs like might are prototypical modality markers, but they can be used in multiplesenses.
Multiword expressions can also express modality (e.g., this brings us to the largestof all mysteries or little was known).
Modality and negation interact with mood and tensemarkers, and also with each other.
Finally, discourse factors also add to the complexityof these phenomena.Incorporating information about modality and negation has been shown to beuseful for a number of applications such as recognizing textual entailment (de Marneffeet al 2006; Snow, Vanderwende, and Menezes 2006; Hickl and Bensley 2007), machinetranslation (Baker et al 2010), trustworthiness detection (Su, Huang, and Chen 2010),classification of citations (Di Marco, Kroon, and Mercer 2006), clinical and biomedicaltext processing (Friedman et al 1994; Szarvas 2008), and identification of text struc-ture (Grabar and Hamon 2009).This overview is organized as follows: Sections 2 and 3 define modality and nega-tion, respectively.
Section 4 gives details of linguistic resources annotated with variousaspects of negation and modality.
We also discuss properties of the different annotationschemes that have been proposed.
Having discussed the linguistic basis as well as theavailable resources, the remainder of the article then provides an overview of automatedmethods for dealing with modality and negation.
Most of the work in this area hasbeen carried out at the sentence or predicate level.
Section 5 discusses various methodsfor detecting speculative sentences.
This is only a first step, however.
For a more fine-224Morante and Sporleder Modality and Negationgrained analysis, it is necessary to deal with modality and negation on a sub-sentential(i.e., predicate) level.This is addressed in Section 6, which also discusses various methods for the impor-tant task of scope detection.
Section 7 then moves on to work on detecting negation andmodality at a discourse level, that is, in the context of recognizing contrasts and con-tradictions.
Section 8 takes a closer look at dealing with positive and negative opinionsand summarizes studies in the field of sentiment analysis that have explicitly modeledmodality and negation.
Section 9 provides an overview of the articles in this specialissue.
Finally, Section 10 concludes this article by outlining some of the remainingchallenges.Some notational conventions should be clarified.
In the literature, the affixes, wordsor multiword expressions that express modality and negation have been referred to astriggers, signals, markers, and cues.
Here, we will refer to them as cues and we willmark them in bold in the examples.
The boundaries of their scope will be marked withsquare brackets.2.
ModalityFrom a theoretical perspective, modality can be defined as a philosophical concept,as a subject of the study of logic, or as a grammatical category.
There are many def-initions and classifications of modal phenomena.
Even if we compiled an exhaustiveand precise set of existing definitions, we would still be providing a limited viewon what modality is, because, as Salkie, Busuttil, and van der Auwera (2009, page 7)put it:.
.
.modality is a big intrigue.
Questions erstwhile considered solved become openquestions again.
New observations and hypotheses come to light, not least becausethe subject matter is changing.Defining modality from a computational linguistics perspective for this specialissue becomes even more difficult because several concepts are used to refer to phe-nomena that are related to modality, depending on the task at hand and the specificphenomena that the authors address.
To mention some examples, research focuses oncategorizing modality, on committed belief tagging, on resolving the scope of hedgecues, on detecting speculative language, and on computing factuality.
These conceptsare related to the attitude of the speaker towards her statements in terms of degree ofcertainty, reliability, subjectivity, sources of information, and perspective.
Because thisspecial issue focuses on the computational treatment of modality, we will provide ageneral theoretical description of modality and the related concepts mentioned in thecomputational linguistics literature at the cost of offering a simplified view of theseconcepts.Jespersen (1924, page 329) attempts to place all moods in a logically consistentsystem, distinguishing between ?categories containing an element of will?
and ?cat-egories containing no element of will?
?later named as propositional modality andevent modality by Palmer (1986).
Lyons (1977, page 793) describes epistemic modalityas concerned with matters of knowledge and belief, ?the speaker?s opinion or attitudetowards the proposition that the sentence expresses or the situation that the propositiondescribes.?
Palmer (1986, page 8) distinguishes propositional modality, which is ?con-cernedwith the speaker?s attitude to the truth-value or factual status of the proposition?225Computational Linguistics Volume 38, Number 2as in Example (2a), and event modality, which ?refers to events that are not actualized,events that have not taken place but are merely potential?
as in Example (2b):(2) a. Kate must be at home now.b.
Kate must come in now.Within propositional modality, Palmer defines two types: epistemic, used by speakers?to express their judgement about the factual status of the proposition,?
and evidential,used ?to indicate the evidence that they have for its factual status?
(Palmer 1986, 8?9).
He also defines two types of event modality: deontic, which relates to obligationor permission and to conditional factors ?that are external to the relevant individual,?and dynamic, where the factors are internal to the individual (Palmer 1986, pages 9?13).
Additionally, Palmer indicates other categories that may be marked as irrealis andmay be found in the mood system: future, negative, interrogative, imperative-jussive,presupposed, conditional, purposive, resultative, wishes, and fears.
Palmer explainshow modality relates to tense and aspect: The three categories are concerned with theevent reported by the utterance, whereas tense is concerned with the time of the eventand aspect is ?concerned with the nature of the event in terms of its internal temporalconstituency?
(Palmer 1986, pages 13?16).From a philosophical standpoint, von Fintel (2006) defines modality as ?a categoryof linguistic meaning having to do with the expression of possibility and necessity.
?In this sense ?a modalized sentence locates an underlying or prejacent propositionin the space of possibilities.?
Von Fintel describes several types of modal meaning(alethic, epistemic, deontic, bouletic, circumstantial, and teleological), some of whichare introduced by von Wright (1951), and shows that modal meaning can be expressedby means of several types of expressions, such as modal auxiliaries, semimodal verbs,adverbs, nouns, adjectives, and conditionals.Within the modal logic framework several authors provide a more technical ap-proach to modality.
Modal logic (von Wright 1951; Kripke 1963) attempts to representformally the reasoning involved in expressions of the type it is necessary that ... andit is possible that ... starting from a weak logic called K (Garson 2009).
Taken in abroader sense, modal logic also aims at providing an analysis for expressions of deontic,temporal, and doxastic logic.
Within the modal logic framework, modality is analyzedin terms of possible worlds semantics (Kratzer 1981).
The initial idea is that modalexpressions are considered to express quantification over possible worlds.Kratzer (1981, 1991), however, argues that modal expressions are more complexthan quantifiers and that their meaning is context-dependent.
Recent work on modalityin the framework of modal logic is presented by Portner (2009, pages 2?8), who groupsmodal forms into three categories: sentential modality (?the expression of modal mean-ing at the level of the whole sentence?
); sub-sentential modality (?the expression ofmodal meaningwithin constituents smaller than a full clause?
); and discoursemodality(?any contribution to meaning in discourse which cannot be accounted for in terms of atraditional semantic framework?
).From a typological perspective, the study of modality seeks to describe how thelanguages of the world express different types of modality (Palmer 1986; van derAuwera and Plungian 1998).
Knowing how modality is expressed across languages isrelevant for the computational linguistics community, not only because it is essentialfor developing automated systems for languages other than English, but also because226Morante and Sporleder Modality and Negationit throws some light on the underlying phenomena that might be beneficial for thedevelopment of novel methods for dealing with modality.Concepts related to modality that have been studied in computational linguisticsare: hedging, evidentiality, uncertainty, factuality, and subjectivity.
The term hedging isoriginally due to Lakoff (1972, page 195), who describes hedges as ?words whose job isto make things more or less fuzzy.?
Lakoff starts from the observation that ?naturallanguage concepts have vague boundaries and fuzzy edges and that, consequently,natural language sentences will very often be neither true, nor false, nor nonsensical,but rather true to a certain extent and false to a certain extent, true in certain aspectsand false in certain aspects?
(Lakoff 1972, page 183).
In order to deal with this aspect oflanguage, he extends the classical propositional and predicate logic to fuzzy logic andfocuses on the study of hedges.
Hyland (1998) studies hedging in scientific texts.
Heproposes a pragmatic classification of hedge expressions based on an exhaustive analy-sis of a corpus.
The catalogue of hedging cues includes modal auxiliaries, epistemiclexical verbs, epistemic adjectives, adverbs, nouns, and a variety of non-lexical cues.Evidentiality is related to the expression of the information source of a statement.As Aikhenvald (2004, page 1) puts it:In about a quarter of the world?s languages, every statement must specify the type ofsource on which it is based [...].
This grammatical category, whose primary meaning isinformation source, is called ?evidentiality?.This grammatical category was already introduced by Boas (1938), and has been studiedafterwards, although less than modality.
There is no agreement on whether it shouldbe a subcategory of modality (Palmer 1986; de Haan 1995) or a category by itself (deHaan 1999; Aikhenvald 2004).
A broader definition relates evidentiality to the expres-sion of the speaker?s attitude towards the information being presented (Chafe 1986).Ifantidou (2001, page 5) considers that the function of evidentials is to indicate thesource of knowledge (observation, hearsay, inference, memory) on which a statementis based and the speaker?s degree of certainty about the proposition expressed.Certainty is a type of subjective information that can be conceived of as a varietyof epistemic modality (Rubin, Liddy, and Kando 2005).
Here we take their definition(page 65):.
.
.
certainty is viewed as a type of subjective information available in texts and aform of epistemic modality expressed through explicitly-coded linguistic means.Such devices [...] explicitly signal presence of certainty information that covers afull continuum of writer?s confidence, ranging from uncertain possibility andwithholding full commitment to statements.Factuality involves polarity, epistemic modality, evidentiality, and mood.
It isdefined by Saur??
(2008, page 1) as:.
.
.
the level of information expressing the commitment of relevant sources towardsthe factual nature of eventualities in text.
That is, it is in charge of conveying whethereventualities are characterized as corresponding to a fact, to a possibility, or to asituation that does not hold in the world.Factuality can be expressed by several linguistic means: negative polarity particles,modality particles, event-selecting predicates which project factuality information onthe events denoted by their arguments (claim, suggest, promise, etc.
), and syntactic227Computational Linguistics Volume 38, Number 2constructions involving subordination.
The factuality of a specific event can changeduring the unfolding of the text.
As described in Saur??
and Pustejovsky (2009), depend-ing on the polarity, events are depicted as either facts or counterfacts.
Depending onthe level of uncertainty combined with polarity, events will be presented as possiblyfactual (3a) or possibly counterfactual (3b).
(3) a.
The United States may extend its naval quarantine to Jordan?s Red Seaport of Aqaba.b.
They may not have enthused him for their particular brand of politicalidealism.The term subjectivity is introduced by Banfield (1982).
Work on subjectivity incomputational linguistics is initially due to Wiebe, Wilson, and collaborators (Wiebe1994; Wiebe et al 2004; Wiebe, Wilson, and Cardie 2005; Wilson 2008; Wilson et al 2005;Wilson, Wiebe, and Hwa 2006) and focuses on learning subjectivity from corpora.
AsWiebe et al (2004, page 279) put it:Subjective language is language used to express private states in the context of a text orconversation.
Private state is a general covering term for opinions, evaluations,emotions, and speculations.Subjectivity is expressed by means of linguistic expressions of various types fromwords to syntactic devices that are called subjective elements.
Subjective statementsare presented from the point of view of someone, who is called the source.
As Wiebeet al (2004) highlight, subjective does not mean not true.
For example, in Example (4a),criticized expresses subjectivity, but the events CRITICIZE and SMOKE are presented asbeing true.
Not all events contained in subjective statements need to be true, however.Modal expressions can be used to express subjective language, as in Example (4b),where the modal cue perhaps combined with the future tense is used to present the eventFORGIVE as non-factual.
(4) a. John criticized Mary for smoking.b.
Perhaps you?ll forgive me for reposting his response.Modality and evidentiality are grammatical categories, whereas certainty, hedging,and subjectivity are pragmatic positions, and event factuality is a level of information.
Inthis special issue we will use the term modality in a broad sense, similar to the extendedmodality of Matsuyoshi et al (2010), which they use to refer to ?modality, polarity, andother associated information of an event mention.?
Subjectivity in the general senseand opinion are beyond the scope of this special issue, however, because research inthese areas focuses on different topics and already has a well defined framework ofreference.Modality-related phenomena are not rare.
According to Light, Qiu, and Srinivasan(2004), 11% of sentences in MEDLINE contain speculative language.
Vincze et al (2008)report that around 18% of sentences occurring in biomedical abstracts are specula-tive.
Nawaz, Thompson, and Ananiadou (2010) find that around 20% of the eventsin a biomedical corpus belong to speculative sentences and that 7% of the events areexpressed with some degree of speculation.
Szarvas (2008) notes that a significantproportion of the gene names mentioned in a corpus of biomedical articles appear in228Morante and Sporleder Modality and Negationspeculative sentence (638 occurences out of a total of 1,968).
This means that approx-imately 1 in every 3 genes should be excluded from the interaction detection process.Rubin (2006) reports that 59% of the sentences in a corpus of 80 articles from The NewYork Times were identified as epistemically modalized.3.
NegationNegation is a complex phenomenon that has been studied from many perspectives, in-cluding cognition, philosophy, and linguistics.
As described by Lawler (2010, page 554),cognitively, negation ?involves some comparison between a ?real?
situation lackingsome particular element and an ?imaginal?
situation that does not lack it.?
In the logicformalisms, ?negation is the only significant monadic functor,?
whose behavior is de-scribed by the Law of Contradiction that asserts that no proposition can be both trueand not true.
In natural language, negation functions as an operator, like quantifiers andmodals.
A main characteristic of operators is that they have a scope, which means thattheir meaning affects other elements in the text.
The affected elements can be located inthe same clause (5a) or in a previous clause (5b).
(5) a.
We didn?t find the book.b.
We thought we would find the book.
This was not the case.The study of negation in philosophy started with Aristotle, but nowadays is still atopic that generates a considerable number of publications in the field of philosophy,logic, psycholinguistics, and linguistics.
Horn (1989) provides an extensive descriptionof negation from a historic perspective and an analysis of negation in relation to seman-tic and pragmatic phenomena.
Tottie (1991) studies negation as a grammatical categoryfrom a descriptive and quantitative point of view, based on the analysis of empiricalmaterial.
She defines two main types of negation in natural language: rejections ofsuggestions and denials of assertions.
Denials can be explicit and implicit.Languages have devices for negating entire propositions (clausal negation) or con-stituents of clauses (constituent negation).
Most languages have several grammaticaldevices to express clausal negation, which are used with different purposes like negat-ing existence, negating facts, or negating different aspects, modes, or speech acts (Payne1997).
As described by Payne (page 282):.
.
.
a negative clause is one that asserts that some event, situation, or state of affairsdoes not hold.
Negative clauses usually occur in the context of some presupposition,functioning to negate or counter-assert that presupposition.van der Wouden (1997) defines what a negative context is, showing that negationcan be expressed by a variety of grammatical categories.
We reproduce some of hisexamples in Example (6).
(6) a. Verbs: We want to avoid doing any look-up, if possible.b.
Nouns: The positive degree is expressed by the absence of anyphonic sequence.c.
Adjectives: It is pointless to teach any of the vernacular languagesas a subject in schools.229Computational Linguistics Volume 38, Number 2d.
Adverbs: I?ve never come across anyone quite as brainwashed asyour student.e.
Prepositions: You can exchange without any problem.f.
Determiners: This fact has no direct implications for any of the twomethods of font representation.g.
Pronouns: Nobody walks anywhere in Tucson.h.
Complementizers: Leave the door ajar, lest any latecomers shouldfind themselves shut out.i.
Conjunctions: But neither this article nor any other similar review Ihave seen then had the methodological discipline to take the oppositepoint of view.Negation can also be expressed by affixes, as in motionless or unhappy, and by chang-ing the intonation or facial expression, and it can occur in a variety of syntacticconstructions.Typical negation problems that persist in the study of negation are determiningthe scope when negation occurs with quantifiers (7a), neg-raising (7b), the use ofpolarity items (7c) (any, the faintest idea), double or multiple negation (7d), and affixalnegation (Tottie 1991).
(7) a.
All the boys didn?t leave.b.
I don?t think he is coming.c.
I didn?t see anything.d.
I don?t know nothing no more.Like modality, negation is a frequent phenomenon in texts.
Tottie reports that nega-tion is twice as frequent in spoken text (27.6 per 1,000 words) as in written text (12.8 per1,000 words).
Elkin et al (2005) find that 1,823 out of 14,792 concepts in 41 HealthRecords from JohnsHopkins University are identified as negated by annotators.
Nawaz,Thompson, and Ananiadou (2010) report that more than 3% of the biomedical eventsin 70 abstracts of the GENIA corpus are negated.
Councill, McDonald, and Velikovich(2010) annotate a corpus of product reviews with negation information and they findthat 19% of the sentences contain negations (216 out of 1,135).3.1 Negation versus Negative PolarityNegation and negative polarity are interrelated concepts, but it is important to noticethat they are different.
Negation has been defined as a grammatical phenomenon usedto state that some event, situation, or state of affairs does not hold, whereas polarityis a relation between semantic opposites.
As Israel (2004, page 701) puts it, ?as suchpolarity encompasses not just the logical relation between negative and affirmativepropositions, but also the conceptual relations defining contrary pairs like hot?cold,long?short, and good?bad.?
Israel defines three types of polar oppositions: contradiction,a relation in which one term must be true and the other false; contrariety, a relationin which only one term may be true, although both can be false; and reversal, which230Morante and Sporleder Modality and Negationinvolves an opposition between scales (?necessary, likely, possible?
?impossible, un-likely, uncertain?.).
The relation between negation and polarity lies in the fact thatnegation can reverse the polarity of an expression.In this context, negative polarity items (NPIs) ?are expressions with a limited distri-bution, part of which includes negative sentences?
(Hoeksema 2000, page 115), like anyin Example (8a) or ever in Example (8b).
Lawler (2010, page 554) defines NPI as ?a termapplied to lexical items, fixed phrases, or syntactic construction types that demonstrateunusual behavior around negation.?
NPIs felicitously occur only in the scope of somenegative element, such as didn?t in Example (8b).
If this element is removed, the sentencebecomes agrammatical, as shown in Example (8c).
The presence of an NPI in a contextdoes not guarantee that something is being negated, however, because NPIs can alsooccur in certain grammatical circumstances, like interrogatives as in Example (8d).
(8) a. I didn?t read any book.b.
He didn?t ever read the book.c.
* He ever read the book.d.
Do you think I could ever read this book?Polarity is a discrete category that can take two values: positive and negative.
De-termining the polarity of words, and phrases is a central task in sentiment analysis,in particular, disambiguating the contextual polarity of words (Wilson, Wiebe, andHoffman 2009).
Thus, in the context of sentiment analysis positive and negative polarityrefers to positive and negative opinions, emotions, and evaluations.Negation is a topic of study in sentiment analysis because it is what Wilson,Wiebe, and Hoffman (2009, page 402) call a polarity influencer, an element that canchange the polarity of an expression.
As they put it, however, ?many things besidesnegation can influence contextual polarity, and even negation is not always straight-forward.?
We discuss different ways of modeling negation in sentiment analysis inSection 8.
The study of negative polarity is beyond the scope of this special issue,however.4.
Categorizing and Annotating Modality and NegationOver the last few years, several corpora of texts from various domains have beenannotated at different levels (expression, event, relation, sentence) with informationrelated to modality and negation.
Compared to other phenomena like semantic ar-gument structure, dialogue acts, or discourse relations, however, no comprehensiveannotation standard has been defined for modality and negation.
In this section, wedescribe the categorization schemes that have been proposed and the corpora that havebeen annotated.In the framework of the OntoSem project (Nirenburg and Raskin 2004) a corpushas been annotated with modality categories and an analyzer has been developedthat takes as input unrestricted raw text and carries out several levels of linguisticanalysis, including modality at the semantic level (Nirenburg and McShane 2008).The output of the semantic analysis is represented as formal text-meaning represen-tations.
Modality information is encoded as part of the semantic module in the lexicalentries of the modality cues.
Four modality attributes are encoded: MODALITY TYPE,231Computational Linguistics Volume 38, Number 2VALUE, SCOPE, and ATTRIBUTED-TO.
The MODALITY TYPES are: polarity, whether aproposition is positive or negated; volition, the extent to which someone wants ordoes not want the event/state to occur; obligation, the extent to which someoneconsiders the event/state to be necessary; belief, the extent to which someone be-lieves the content of the proposition; potential, the extent to which someone believesthat the event/state is possible; permission, the extent to which someone believes thatthe event/state is permitted; and evaluative, the extent to which someone believesthe event/state is a good thing.
The SCALAR VALUE ranges from zero to one.
TheSCOPE attribute is the predicate that is affected by the modality and the ATTRIBUTED-TO attribute indicates to whom the modality is assigned, the default value beingthe speaker.
In Example (9), should is identified as a modality cue and character-ized with the type obligative, value 0.8, scope camouflage, and is attributed to thespeaker.
(9) Entrance to the tower should be totally camouflagedThe publicly available MPQA Opinion Corpus1 (Wiebe, Wilson, and Cardie 2005)contains 10,657 sentences in 535 documents of English newswire annotated with in-formation about private states at the word and phrase level.
For every expression ofprivate state a private state frame is defined indicating the SOURCE of the private state,whose private state is being expressed; the TARGET, what the private state is about;and properties like INTENSITY, SIGNIFICANCE, and TYPE OF ATTITUDE.
Three types ofprivate state expressions are considered for the annotation: explicit mentions like fearsin Example (10a), speech events like said in Example (10b), and expressive subjectiveelements, like full of absurdities in Example (10b).
Apart from representing private statesin private state frames, Wiebe, Wilson, and Cardie also define objective speech eventframes that represent ?material that is attributed to some source, but is presented asan objective fact?
(page 171).
Having two types of frames allows a distinction betweenopinion-oriented material (10a, 10b) and factual material (10c).
(10) a.
?The U.S. fears a spill-over,?
said Xirao-Nima.b.
?The report is full of absurdities,?
Xirao-Nima said.c.
Sergeant O?Leary said the incident took place at 2:00 pm.Rubin, Liddy, and Kando (2005) define a model for categorizing certainty.
Themodel distinguishes four dimensions: LEVEL, which encodes the degree of certainty;PERSPECTIVE, which encodes whose certainty is involved; FOCUS, the object of certainty;and TIME, which encodes at what time the certainty is expressed.
Each dimension isfurther subdivided into categories, resulting in 72 possible dimension?category combi-nations.
The four certainty LEVELS are absolute (Example (11a)), high (Example (11b)),moderate (Example (11c)), and low (Example (11d)).
PERSPECTIVE separates the writer?spoint of view and the reported point of view.
FOCUS is divided into abstract and fac-tual information.
TIME can be past, present, or future.
The model is used to annotatecertainty markers in 32 articles from The New York Times along these dimensions.
Rubin1 The MPQA corpus is available from http://www.cs.pitt.edu/mpqa/mpqa corpus.html.
Last accessedon 8 December 2011.232Morante and Sporleder Modality and Negationet al find that editorials have a higher frequency of modality markers per sentence thannews stories.
(11) a.
An enduring lesson of the Reagan years, of course, is that it really doestake smoke and mirrors to produce tax cuts, spending initiatives and abalanced budget at the same time.b.
... but clearly an opportunity is at hand for the rest of the world topressure both sides to devise a lasting peace based on democratic valuesand respect for human rights.c.
That fear now seems exaggerated, but it was not entirely fanciful.d.
So far the presidential candidates are more interested in talking aboutwhat a surplusmight buy than in the painful choices that lie ahead.The model is adapted in Rubin (2006, 2007) by adding a category uncertainty forcertainty LEVEL, changing the FOCUS categories into facts and events and opin-ions, emotions, or judgements, and adding the irrelevant category for TIME.
Inter-annotator agreement measures are reported for 20 articles of the 80 annotated articlesrandomly selected from The New York Times (Rubin 2006).
For the task of decidingwhether a statement wasmodalized by an explicit certaintymarker or not, an agreementof 0.33 ?cohen is reached.
The agreement measures per dimension were 0.15 for level,0.13 for focus, 0.44 for perspective, and 0.41 for time.The Automatic Content Extraction 2008 corpus (Linguistic Data Consortium 2008)for relation detection and recognition collects English and Arabic texts from a varietyof resources including radio and TV broadcast news, talk shows, newswire articles,Internet news groups, Web logs, and conversational telephone speech.
Relations areordered pairs of entities and are annotated with modality and tense attributes.
The twomodality attributes are asserted and other.
Asserted relations pertain to situations inthe real world, whereas other relations pertain to situations in ?some other world de-fined by counterfactual constraints elsewhere in the context.?
If the entities constitutingthe arguments of a relation are hypothetical, then the relation can still be understoodas asserted.
In Example (12), the ORG-Aff.Membership relation between terrorists andAl-Qaeda is annotated as asserted and the Physical.Located relation between terroristsand Baghdad is annotated as other.
The attributes for TENSE are past, future, present,and unspecified.
(12) We are afraid Al-Qaeda terrorists will be in Baghdad.The Penn Discourse TreeBank (Prasad et al 2008) is a corpus annotated with in-formation related to discourse structure.
Discourse connectives are considered to bethe anchors of discourse relations and to act as predicates taking two abstract objects.Abstract objects can be assertions, beliefs, facts, or eventualities.
Discourse connectivesand their arguments are assigned attribution-related features (Prasad et al 2006) such asSOURCE (writer, other, arbitrary), TYPE (reflecting the nature of the relation betweenthe agent and the abstract object), SCOPAL POLARITY of attribution, and DETERMINACY(indicating the presence of contexts canceling the entailment of attribution).
The textspans signaling the attribution are also marked.
Prasad et al (2006) report that 34% ofthe discourse relations have some non-writer agent.
SCOPAL POLARITY is annotatedto identify cases when verbs of attribution (say, think, ...) are negated syntactically233Computational Linguistics Volume 38, Number 2(didn?t say) or lexically (denied).
An argument of a connective is marked Neg for SCOPALPOLARITY when the interpretation of the connective requires the surface negation totake semantic scope over the lower argument.
As stated by Prasad et al, in Example (13),the but clause entails an interpretation such as ?I think it?s not a main consideration,?for which the negation must take narrow scope over the embedded clause rather thanthe higher clause.
(13) ?Having the dividend increases is a supportive element in the marketoutlook, but I don?t think it?s a main consideration,?
he says.TimeML (Pustejovsky et al 2005) is a specification language for events and temporalexpressions in natural language that has been applied to the annotation of corpora likeTimeBank (Pustejovsky et al 2006).
As described in Saur?
?, Verhagen, and Pustejovsky(2006), TimeML encodes different types of modality at the lexical and syntactic levelwith different tags.
At the lexical level, Situation Selecting Predicates (SSPs) are en-coded by means of the attribute CLASS within the EVENT tag, which allows to encodethe difference between SSPs that are actions (Example (14a)2) and SSPs that are states(Example (14b)).
SSPs of perception (Example (14c)) and reporting (Example (14d)) areencoded with more specific values due to their role in providing evidentiality.
Informa-tion about modal auxiliaries and negative polarity, which are also lexically expressed,is encoded in the attributes MODALITY and POLARITY.
Modality at the syntactic levelis encoded as an attribute of the tag SLINK (Subordination Link), which can haveseveral values: factive, counterfactive, evidential, negative evidential, modal,and conditional.
(14) a.
Companies such as Microsoft or a combined worldcom MCI are tryingto monopolize Internet access.b.
Analysts also suspect suppliers have fallen victim to their own success.c.
Some neighbors told Birmingham police they saw a man running.d.
No injuries were reported over the weekend.FactBank (Saur??
and Pustejovsky 2009) is a corpus of events annotated with factu-ality information, which adds to the TimeBank corpus an additional level of semanticinformation.
Events are annotated with a discrete set of factuality values using a bat-tery of criteria that allow annotators to differentiate among these values.
It consistsof 208 documents that contain 9,488 annotated events.
The categorization model isbased on Horn?s (1989) analysis of epistemic modality in terms of scalar predication.For epistemic modality Horn proposes the scale ?certain, {probable/likely}, possible?.For the negative counterpart he proposes the scale ?uncertain, {unlikely/improbable},impossible?.
Saur??
and Pustejovsky map this system into the traditional Square ofOpposition (Parsons 2008), which originated with Aristotle.
The resulting degreesof factuality defined in FactBank are the following: fact, counterfact, probable,not probable, possible, not certain, certain but unknown output, and unknownor uncommitted.
An example of the certain but unknown output value is shown inExample (15) for the event COME, and examples of the unknown or uncommitted value2 The event affected by the SSP is underlined.234Morante and Sporleder Modality and Negationfor the same event are found in Example (16).
Discriminatory co-predication tests areprovided for the annotators to determine the factuality of events.
The interannotatoragreement reported for assigning factuality values is ?cohen 0.81.
(15) John knows whether Mary came.
(16) a. John does not know whether Mary came.b.
John does not know that Mary came.c.
John knows that Paul said that Mary came.A corpus of 50,108 event mentions in blogs and Web posts in Japanese has beenannotated with information about extended modality (Matsuyoshi et al 2010).
The an-notation scheme of extended modality is based on four desiderata: information shouldbe assigned to the event mention; the modality system has to be language independent;polarity should be divided into two classes: POLARITY ON THE ACTUALITY of theevent and SUBJECTIVE POLARITY from the perspective of the source?s evaluation; andthe annotation labels should not be too fine-grained.
In Example (17) the polarity onactuality is negative for the events STUDY and PASS because they did not occur, but thesubjective polarity for the PASS event is positive.
Extended modality is characterizedalong seven components: SOURCE, indicating who expresses an attitude towards theevent; TIME, future or non future; CONDITIONAL, whether a target event mention is aproposition with a condition; PRIMARY MODALITY TYPE, determining the fundamentalmeaning of the event mention (assertion, volition, wish, imperative, permission, in-terrogative); ACTUALITY, degree of certainty; EVALUATION, subjective polarity, whichcan be positive, negative, or neutral; and FOCUS, what aspect of the event is the focusof negation, inference, or interrogation.
Reported inter-annotator agreement for twoannotators on 300 event mentions ranges from 0.69 to 0.76 ?cohen depending on thecategory.
(17) If I had studied mathematics harder, I could have passed the examination.A publicly available modality lexicon3 has been developed by Baker et al (2010)in order to automatically annotate a corpus with modality information.
This lexiconcontains modal cues related to factivity.
The lexicon entries consist of five components:the cue sequence of words, part-of-speech (PoS) for each word, a modality type, a headword, and one or more subcategorization codes.
Three components are identified insentences that contain a modality cue: the TRIGGER is the word or sequence of wordsthat expresses modality; the TARGET is the event, state, or relation that the modalityscopes over; and the HOLDER is the experiencer or cognizer of themodality.
This schemedistinguishes eight modalities: requirement (does H require P?
), permissive (does Hallow P?
), success (does H succeed in P?
), effort (does H try to do P?
), intention (doesH intend P?
), ability (can H do P?
), want (does H want P?
), and belief (with whatstrength does H believe P?).
The annotation guidelines to annotate the modalities aredefined in Baker et al (2009).3 Web site of the modality lexicon: http://www.umiacs.umd.edu/?bonnie/ModalityLexicon.txt.Last accessed on 8 December 2011.235Computational Linguistics Volume 38, Number 2The scope of negation has been annotated on a corpus of Conan Doyle stories(Morante, Schrauwen, and Daelemans 2011)4 (The Hound of the Baskervilles and TheAdventure of Wisteria Lodge), which have also been annotated with coreference andsemantic roles for the SemEval Task Linking Events and Their Participants in Discourse(Ruppenhofer et al 2010).
As for negation, the corpus is annotated with negation cuesand their scope in a way similar to the BioScope corpus (Vincze et al 2008) describedsubsequently, and in addition negated events are also marked, if they occur in fac-tual statements.
Blanco and Moldovan (2011) take a different approach by annotatingthe focus, ?that part of the scope that is most prominently or explicitly negated,?
inthe 3,993 verbal negations signaled with MNEG in the PropBank corpus.
According to theauthors, the annotation of the focus allows the derivation of the implicit positive mean-ing of negated statements.
For example, in Example (18) the focus of the negation is onuntil 2008, and the implicit positive meaning is ?They released the UFO files in 2008.?
(18) They didn?t release the UFO files until 2008.The corpora and categorization schemes described here reflect research focusing ongeneral-domain texts.
With the growth of research on biomedical text mining, annota-tion of modality phenomena in biomedical texts has become central.
Scientific languagemakes use of speculation and hedging to express lack of definite belief.
Light, Qiu, andSrinivasan (2004) are pioneers in analyzing the use of speculative language in scientifictexts.
They study the expression of levels of belief in MEDLINE abstracts by meansof hypotheses, tentative conclusions, hedges, and speculations, and annotate a corpusof abstracts in order to check whether the distinctions between high speculative, lowspeculative, and definite sentences could be made reliably.
Their findings suggest thatthe speculative versus definite distinction is reliable while the distinction between lowand high speculative is not.The annotation work by Wilbur, Rzhetsky, and Shatkay (2006) is motivated by theneed to identify and characterize parts of scientific documents where reliable infor-mation can be found.
They define five dimensions to characterize scientific sentences:FOCUS (scientific versus general), POLARITY (positive versus negative statement),LEVEL OF CERTAINTY in the range 0?3, STRENGTH of evidence, and DIRECTION/TREND(increase or decrease in certain measurement).A corpus5 of six articles from the functional genomics literature has been anno-tated at the sentence level for speculation (Medlock and Briscoe 2007).
Sentences areannotated as being speculative or not.
Of the 1,157 sentences, 380 were found to bespeculative.
An inter-annotator agreement of 0.93 ?cohen is reported.BioInfer (Pyysalo et al 2007) is a corpus of 1,100 sentences from abstracts ofbiomedical research articles annotated with protein, gene, and RNA relationships.
Theannotation scheme captures information about the absence of a relation.
Statementsexpressing absence of a relation such as not affected by or independent of are annotatedusing a predicate NOT, as in this example: not:NOT(affect:AFFECT(deletion of SIR3,silencing)).The Genia Event corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences wherebiological events are annotated with negation and uncertainty.
In the case of negation,4 Web site of the Conan Doyle corpus: http://www.clips.ua.ac.be/BiographTA/corpora.html.Last accessed on 8 December 2011.5 The Medlock and Briscoe corpus is available from http://www.benmedlock.co.uk/hedgeclassif.html.Last accessed on 8 December 2011.236Morante and Sporleder Modality and Negationevents are marked with the label exists or non-exists.
In the case of uncertainty,events are labeled into three categories: certain, which is chosen by default; probable,if the event existence cannot be stated with certainty; and doubtful, if the event is underinvestigation or forms part of a hypothesis.
Linguistic cues are not annotated.The BioScope corpus (Vincze et al 2008) is a freely available resource6 that gathersmedical and biological texts.
It consists of three parts: clinical free-texts (radiologyreports), full-text biological articles, and biological article abstracts from the GENIAcorpus (Collier et al 1999).
In total it contains 20,000 sentences.
Instances of negativeand speculative language are annotated with information about the linguistic cues thatexpress them and their scope.
Negation is understood as the implication of the non-existence of something as in Example (19a).
Speculative statements express the possibleexistence of something as in Example (19b).
The scope of a keyword is determined bysyntax and it is extended to the largest syntactic unit to the right of the cue, including allthe complements and adjuncts of verbs and auxiliaries.
The inter-annotator agreementrate for scopes is defined as the F-measure of one annotation, treating the secondone as the gold standard.
It ranges from 62.50 for speculation in full articles to 92.46for negation in abstracts.
All agreement measures are lower for speculation than fornegation.
The BioScope corpus has been provided as a training corpus for the biologicaltrack of the 2010 edition of the CoNLL Shared Task on Learning to Detect Hedges and theirScope in Natural Language Text (Farkas et al 2010b).
The additional test files provided inthe Shared Task are annotated in the same way.
(19) a. Mildly hyperinflated lungs [without focal opacity].b.
This result [suggests that the valency of Bi in the material is smallerthan +3].Because the Genia Event and BioScope corpus share 958 abstracts, it is possible tocompare their annotations, as it is done by Vincze et al (2010).
Their study shows thatthe scopes of BioScope are not directly useful to detect the certainty status of the eventsin Genia, and that the BioScope annotation is more easily adaptable to non-biomedicalapplications.
A description of negation cues and their scope in biomedical texts, basedon the cues that occur in the BioScope corpus, can be found in Morante (2010), whereinformation is provided relative to the ambiguity of the negation cue and to the type ofscope, as well as examples.
The description shows that the scope depends mostly on thePoS of the cue and on the syntactic features of the clause.The NaCTeM team has annotated events in biomedical texts with meta-knowledgethat includes polarity andmodality (Thompson et al 2008).
Themodality categorizationscheme covers epistemic modality and speculation and contains information about thefollowing dimensions: KNOWLEDGE TYPE, LEVEL OF CERTAINTY, and POINT OF VIEW.Four types of knowledge are defined, three of which are based on Palmer?s (1986) clas-sification of epistemic modality: speculative, deductive, sensory, and experimentalresults or findings.
The levels of certainty are four: absolute, high, moderate, andlow.
The possible values for POINT OF VIEW are writer and other.
An updated ver-sion of the meta-knowledge annotation scheme is presented by Nawaz, Thompson,and Ananiadou (2010).
The scheme consists of six dimensions: KNOWLEDGE TYPE,certainty level, source, lexical polarity, manner, and LOGICAL TYPE.
Three6 The BioScope corpus is available from http://www.inf.u-szeged.hu/rgai/bioscope.
Last accessed on8 December 2011.237Computational Linguistics Volume 38, Number 2levels of certainty are defined: low confidence or considerable speculation, highconfidence or slight speculation, and no expression of uncertainty or speculation.Information about negation is encoded in the LEXICAL POLARITY dimension, whichidentifies negated events.
Negation is defined here as ?the absence or non-existence ofan entity or a process.
?For languages other than English there are much fewer resources.
A corpus of 6,740sentences from the Stockholm Electronic Patient Record Corpus (Dalianis and Velupillai2010) has been annotated with certain and uncertain expressions as well as speculativeand negation cues, with the purpose of creating a resource for the development of au-tomatic detection of speculative language in Swedish clinical text.
The categories usedare: certain, uncertain, and undefined at sentence level, and negation, speculativewords, and undefined speculative words at token level.
Inter-annotator agreementfor certain sentences and negation are high, but for the rest of the classes results arelower.5.
Detection of Speculative SentencesInitial work on processing speculation focuses on classifying sentences as specula-tive or definite (non-speculative), depending on whether they contain speculationcues.Light, Qiu, and Srinivasan (2004) explore the ability of a Support Vector Machine(SVM) classifier to perform this task on a corpus of biomedical abstracts using a stem-ming representation.
The results of the system are compared to a majority decisionbaseline and to a substring matching baseline produced by classifying as speculativesentences which contain the following strings: suggest, potential, likely, may, at least,in part, possible, potential, further investigation, unlikely, putative, insights, point toward,promise, and propose.
The precision results are higher for the SVM classifier (84% com-pared with 55% for the substring matching method), but the recall results are higher forthe substring matching method (79% compared with 39% for the SVM classifier).Medlock and Briscoe (2007) model hedge classification as a weakly supervisedmachine learning task performed on articles from the functional genomics literature.They develop a probabilistic learner to acquire training data, which returns a labeleddata set from which a probabilistic classifier is trained.
The training corpus consists of300,000 randomly selected sentences; the manually annotated test corpus consists ofsix full articles.7 Their classifier obtains 0.76 BEP (Break Even Point), outperformingbaseline results obtained with a substring matching technique.
Error analysis showsthat the system has problems distinguishing between a speculative assertion and onerelating to a pattern of observed non-universal behavior, like Example (20), which iswrongly classified as speculative.
(20) Each component consists of a set of subcomponents that can be localizedwithin a larger distributed neural system.Medlock (2008) presents an extension of this work by experimenting with more fea-tures (PoS, stems, and bigrams).
Experiments show that although the PoS representation7 The Drosophila melanogaster corpus is available at http://www.benmedlock.co.uk/hedgeclassif.html.Last accessed on 8 December 2011.238Morante and Sporleder Modality and Negationdoes not yield significant improvement over the results in Medlock and Briscoe (2007),the system achieves a weakly significant improvement with a stemming representation.The best results are obtained with a combination of stems and adjacent stem bigramsrepresentation (0.82 BEP).Following Medlock and Briscoe (2007), Szarvas (2008) develops a Maximum En-tropy classifier that incorporates bigrams and trigrams in the feature representationand performs a reranking based feature selection procedure that allows a reductionof the number of keyword candidates from 2,407 to 253.
The system is trained on thedata set of Medlock and Briscoe and evaluated on four newly annotated biomedicalfull articles8 and on radiology reports.
The best results of the system are achievedby performing automatic and manual feature selection consecutively and by addingexternal dictionaries.
The final results on biomedical articles are 85.29 BEP and 85.08F1 score.
The results for the external corpus of radiology reports are lower, at 82.07F1 score.A different type of system is presented by Kilicoglu and Bergler (2008), who applya linguistically motivated approach to the same classification task by using knowledgefrom existing lexical resources and incorporating syntactic patterns, including un-hedgers, lexical cues, and patterns that strongly suggest non-speculation.
Additionally,hedge cues are weighted by automatically assigning an information gain measure tothem and by assigning weights semi-automatically based on their types and centralityto hedging.
The hypothesis behind this approach is that ?a more linguistically orientedapproach can enhance recognition of speculative language.?
The results are evaluatedon the Drosophila data set from Medlock and Briscoe (2007) and the four annotatedBMC Bioinformatics articles from Szarvas (2008).
The best results on the Drosophila dataset are obtained with the semi-automatic weighting scheme, which achieves a compet-itive BEP of 0.85.
The best results on the BMC Bioinformatics articles are obtained alsowith semi-automatic weighting yielding a BEP of 0.82 improving over previous results.According to Kilicoglu and Bergler, the best results of the semi-automatic weightingscheme are due to the fact that the scheme relies on the particular semantic propertiesof the hedging indicators.
The relatively stable results of the semi-automatic weightingscheme across data sets could indicate that this scheme is more generalizable than onebased on machine learning techniques.
The false negatives are due to missing syntacticpatterns and to certain derivational forms of epistemic words (suggest?suggestive)that are not identified.
False positives are due to word sense ambiguity of hedgingcues like could and appear, and to weak hedging cues like epistemic deductive verbs(conclude, estimate), some adverbs (essentially, usually), and nominalizations (implication,assumption).A different task is introduced by Shatkay et al (2008).
The task consists of classifyingsentence fragments from biomedical texts along five dimensions, two of which areCERTAINTY (four levels) and POLARITY (negated or not).
Fragments are individualstatements in the sentences as exemplified in Example (21).
For certainty level, thefeature vector represents single words, bigrams, and trigrams; for polarity detection,it represents single words and syntactic phrases.
They perform a binary classifica-tion per class using SVMs.
Results on polarity classification are 1.0 F-measure for thepositive class and 0.95 for the negative class, and results on level of certainty vary8 The four annotated BMC Bioinformatics articles are available at http://www.inf.u-szeged.hu/?szarvas/homepage/hedge.html.
Last accessed on 8 December 2011.239Computational Linguistics Volume 38, Number 2from 0.99 F-measure for level 3, which is the majority class, to 0.46 F-measure forlevel 2.
(21) ?fragment 1We demonstrate that ICG-001 binds specifically to CBP?
?fragment 2 but not the related transcriptional coactivator p3000?Ganter and Strube (2009) introduce a new domain of analysis.
They develop asystem for automatic detection of Wikipedia sentences that contain weasel words, asin Example (22).
Weasel words are ?words and phrases aimed at creating an impressionthat something specific and meaningful has been said, when in fact only a vague orambiguous claim has been communicated.
?9 As Ganter and Strube indicate, weaselwords are closely related to hedges and private states.
Wikipedia editors are advisedto avoid weasel words because they ?help to obscure the meaning of biased expressionsand are therefore dishonest.
?10(22) a.
Others argue {{weasel-inline}} that the news media are simply cateringto public demand.b.
... therefore America is viewed by some {{weasel-inline}} technologyplanners as falling further behind Europe.Ganter and Strube experiment with two classifiers, one based on words precedingthe weasel and another one based on syntactic patterns.
The similar results (around0.70 BEP) of the two classifiers show that word frequency and distance to the weasel tagprovide sufficient information.
The classifier that uses syntactic patterns outperformsthe classifier based on words on data manually re-annotated by the authors, however,suggesting that the syntactic patterns detect weasel words that have not yet beentagged.Classification of uncertain sentences was consolidated as a task with the 2010edition of the CoNLL Shared Task on Learning to Detect Hedges and their Scope in Nat-ural Language Text (Farkas et al 2010b), where Task 1 consisted in detecting uncertainsentences.
Systems were required to perform a binary classification task on two typesof data: biological abstracts and full articles, and paragraphs fromWikipedia.
As Farkaset al describe, the approaches to solving the task follow two major directions: Somesystems handle the task as a classical sentence disambiguation problem and apply abag-of-words approach, and other systems focus on identifying speculation cues, sothat sentences containing cues would be classified as uncertain.
In this second groupsome systems apply a token-based classification approach and others use sequentiallabeling.
The typical feature set for Task 1 includes the wordform, lemma or stem, PoSand chunk codes; and some systems incorporate features from the dependency and/orconstituent parse tree of the sentences.
The evaluation of Task 1 is performed at thesentence level using the F1 score of the uncertain class.
The scores for precision arehigher than for recall, and systems are ranked in different positions for each of the datasets, which suggests that the systems are optimized for one of the data types.
The top-ranked systems for biological data follow a sequence labeling approach, whereas the9 Definition of weasel words in Wikipedia: http://en.wikipedia.org/wiki/Weasel word.
Last accessedon 8 December 2011.10 Wikipedia instructions about weasel words are available at http://simple.wikipedia.org/wiki/Wikipedia:Avoid weasel words.
Last accessed on 8 December 2011.240Morante and Sporleder Modality and Negationtop-ranked systems for Wikipedia data follow a bag-of-words approach.
None of thetop-ranked systems uses features derived from syntactic parsing.
The best system forWikipedia data (Georgescul 2010) implements an SVM and obtains an F1 score of 60.2,whereas the best system for biological data (Tang et al 2010) incorporates conditionalrandom fields (CRF) and obtains an F1 score of 86.4.As a follow-up of the CoNLL Shared Task, Velldal (2011) proposes to handle thehedge detection task as a simple disambiguation problem, restricted to the words thathave previously been observed as hedge cues.
This reduces the number of examplesthat need to be considered and the relevant feature space.
Velldal develops a large-margin SVM classifier based on simple sequence-oriented n-gram features collected forPoS-tags, lemmas, and surface forms.
This system produces better results (86.64 F1) thanthe best system of the CoNLL Shared Task (Tang et al 2010).From the research presented in this section it seems that classifying sentences asto whether they are speculative or not can be performed by using knowledge-poormachine learning approaches as well as by linguistically motivated methods.
It wouldbe interesting to determine whether a combination of both approaches would yieldbetter results.
In the machine learning approaches the features used to solve this taskare mainly shallow features such as words, bigrams, and trigrams.
Syntax features donot seem to add new information, although a linguistically informed method based onsyntactic patterns can produce similar results to machine-learning approaches basedon shallow features.
Hedge cues are ambiguous and domain dependent, reducing theportability of hedge classifiers.
It has also been shown that it is feasible to build a hedgeclassifier in an unsupervised manner.6.
Event-Level Detection of Modality and NegationAlthoughmodality and negation detection at the sentence level can be useful for certainpurposes, it is often the case that not all the information contained in a sentence isaffected by the presence of modality and negation cues.
Modality and negation cues areoperators that have a scope and only the part of the sentence within the scope will beaffected by them.
For example, the sentence in Example (23a)11 would be classified asspeculative in a sentence-level classification task, despite the fact that the cue unlikelyscopes only over the clause headed by the event PRODUCE.
In Example (23b) thenegation cue scopes over the subject of led, assigning negative polarity to the eventCOPE WITH, but not to the rest of the events.
(23) a.
He is now an enthusiastic proponent of austerity and reform but thishas lost him voters and [was unlikely to produce sufficient growth, orjobs, to win him new ones by next spring].b.
Its [inability to cope with file-sharing] led to the collapse ofrecorded-music sales and the growing dependence on live music.Research focusing on determining the scope of cues has revolved around two typesof tasks: finding the events and concepts that are negated or speculated, and resolvingthe full scope of cues.
Sections 6.1 and 6.2 describe them in detail.11 The two examples are sentences from articles in The Economist.241Computational Linguistics Volume 38, Number 26.1 Finding Speculated and Negated Events and EntitiesResearch on finding negated concepts originated in the medical domain motivated bythe need to index, extract, and encode clinical information that can be useful for patientcare, education, and biomedical studies.
In order to automatically process informationcontained in clinical reports it is of great importance to determine whether symptoms,signs, treatments, outcomes, or any other clinical relevant factors are present or not.
AsElkin et al (2005) state, ?erroneous assignment of negation can lead to missing allergiesand other important health data that can negatively impact patient safety.?
Chapmanet al (2001a) point out that accurate indexing of reports requires differentiating perti-nent negatives from positive conditions.
Pertinent negatives are ?findings and diseasesexplicitly or implicitly described as absent in a patient.
?The first systems developed to find negated concepts in clinical reports are rule-based and use lexical information.
NegExpander (Aronow, Fangfang, and Croft 1999)is a module of a health record classification system.
It adds a negation prefix to thenegated tokens in order to differentiate between a concept and its negated variant.Negfinder (Mutalik, Deshpande, and Nadkarni 2001) finds negated patterns in dictatedmedical documents.
It is a pipeline system that works in three steps: concept findingto identify UMLS concepts; input transformation to replace every instance of a conceptwith a coded representation; and a lexing/parsing step to identify negations, negationpatterns, and negation terminators.
In this system negation is defined as ?words imply-ing the total absence of a concept or thing in the current situation.?
Some phenomena areidentified as difficulties for the system: The fact that negation cues can be single wordsor complex verb phrases like could not be currently identified; verbs that when precededby not negate their subject, as in X is not seen; and the fact that a single negation cue canscope over several concepts (A, B, and C are absent) or over some but not all of them (thereis no A, B and C, but D seemed normal).
Elkin et al (2005) describe a rule-based system thatassigns a level of certainty to concepts in electronic health records.
Negation assignmentis performed by the automated negation assignment grammar as part of the rule basedsystem that decides whether a concept has been positively, negatively, or uncertainlyasserted.Chapman et al (2001a, 2001b) developed NegEx,12 a regular expression based algo-rithm for determining whether a finding or disease mentioned within narrative medicalreports is present or absent.
The system uses information about negation phrases thatare divided in two groups: pseudo-negation phrases that seem to indicate negation, butinstead identify double negatives (not ruled out), and phrases that are used as negationswhen they occur before or after Unified Medical Language System terms.
The precisionof the system is 84% and the recall 78%.
Among the system?s weaknesses, the authorsreport detecting the scope of not and no.
In the three examples in (24a?c) the systemwould find that infection is negated.
In Example (24d) edemawould be found as negated,and in Example (24e) cva also.
NegEx has been also adapted to process Swedish clinicaltext (Skeppstedt 2010).
(24) a.
This is not the source of the infection.b.
We did not treat the infection.c.
We did not detect an infection.12 Web site of NegEx: http://code.google.com/p/negex/.
Last accessed on 8 December 2011.242Morante and Sporleder Modality and Negationd.
No cyanosis and positive edema.e.No history of previous cva.ConText (Harkema et al 2009) is an extension of NegEx.
This system uses alsoregular expressions and contextual information in order to determine whether clin-ical conditions mentioned in clinical reports are negated, hypothetical, historical, orexperienced by someone other than the patient.
As for negation, a term is negated ifit falls within the scope of a negation cue.
In this approach, the scope of a cue extendsto the right of the cue and ends in a termination term or at the end of the sentence.
Thesystem is evaluated on six different types of reports obtaining an average precision of94% and average recall of 92%.
Harkema et al find that negation cues have the sameinterpretation across report types.The systems described here cannot determine correctly the scope of negation cueswhen the concept is separated by multiple words from the cue.
This motivated Huangand Lowe (2007) to build a system based on syntax information.
Negated phrases are lo-cated within a parse tree by combining regular expression matching and a grammaticalapproach.
To construct the negation grammar, the authors manually identify sentenceswith negations in 30 radiology reports andmark up negation cues, negated phrases, andnegation patterns.
The system achieves a precision of 98.6% and a recall of 92.6%.
Thelimitations of this system are related to the comprehensiveness of a manually derivedgrammar and to the performance of the parser.Apart from rule-based systems, machine learning techniques have also been ap-plied to find negated and speculated concepts.
Goldin and Chapman (2003) experimentwith Na?
?ve Bayes and decision trees to determine whether a medical observation isnegated by theword not in a corpus of hospital reports.
The F-measure of both classifiersis similar, 89% and 90%, but Na?
?ve Bayes gets a higher precision and the decision tree ahigher recall.
Averbuch et al (2004) develop an Information Gain algorithm for learningnegative context patterns in discharge summaries and measure the effect of contextidentification on the performance of medical information retrieval.
4,129 documentsare annotated with appearances of certain terms, which are annotated as positive ornegative, as in Example (25).
(25) a.
The patient presented with episodes of nausea and vomiting associatedwith epigastric pain for the past 2 weeks.
POSITIVEb.
The patient was able to tolerate food without nausea or vomiting.NEGATIVETheir algorithm scores 97.47 F1.
It selects certain items as indicators of negative context(any, changes in, changes, denies, had no, negative for, of systems, was no, without), but itdoes not select no and not.
As Averbuch et al (2004, page 284) put it, ?Apparently, themere presence of the word ?no?
or ?not?
is not sufficient to indicate negation.?
Theauthors point out five sources of errors: coordinate clauses with but, as in Example (26a)where weight loss is predicted as negative; future reference, as in Example (26b), wherethe symptoms were predicted as positive; negation indicating existence, as in Exam-ple (26c), where nausea is predicted as negative; positive adjectives, as in Example (26d),where appetite andweight loss are predicted as negative; andwrong sentence boundaries.
(26) a.
There were no acute changes, but she did have a 50 pound weight loss.243Computational Linguistics Volume 38, Number 2b.
The patient was given clear instructions to call for any worsening pain,fever, chills, bleeding.c.
The patient could not tolerate the nausea and vomiting associated withCarboplatininal Pain.d.
There were no fevers, headache, or dizziness at home and no diffuseabdominal pain, fair appetite with significant weight loss.Rokach, Romano, and Maimon (2008) present a pattern-based algorithm for iden-tifying context in free-text medical narratives.
The algorithm automatically learnspatterns similar to the manually written patterns for negation detection using twoalgorithms: longest common sequence and Teiresias (Rigoutsos and Floratos 1998),an algorithm designed to discover motifs in biological sequences.
A non-ranker filterfeature selection algorithm is applied to select the informative patterns (35 out of 2,225).In the classification phase three classifiers are combined sequentially, each learningdifferent types of patterns.
Experimental results show that the sequential combinationof decision tree classifiers obtains 95.9 F-measure, outperforming the results of singlehiddenMankovmodels and CRF classifiers based on several versions of a bag-of-wordsrepresentation.Goryachev et al (2006) compare the performance of four different methods of nega-tion detection, two regular expression basedmethods that are adaptations of NegEx andNegExpander, and two classification-based methods, Na?
?ve Bayes and SVM, trainedon 1,745 discharge reports.
They find that the regular expression-based methods showbetter agreement with humans and better accuracy than the classification methods.Goryachev et al indicate that the reason why the classifiers do not perform as wellas NegEx and NegExpander may be related to the fact that the classifiers are trained ondischarge summaries and tested on outpatient notes.Another comparison of approaches to assertion classification is made by Uzuner,Zhang, and Sibanda (2009), who develop a statistical assertion classifier, StAC, toclassify medical problems in patient records into four categories: positive, negative,uncertain, and alter-association13 assertions.
The StAC approach makes use oflexical and syntactic context in conjunction with SVM.
It is evaluated on dischargesummaries and on radiology reports.
The comparison with an extended version of theNegEx algorithm (ENegEx), adapted to capture alter-association in addition to positive,negative, and uncertain assertions, shows a better performance of the statistical classi-fier for all categories, even when it is trained and tested on different corpora.
Resultsalso show that the StAC classifier can solve the task by using the words that occur in afour-word window around the target problem and that it performs well across corpora.This work focuses mostly on negation in clinical documents, but processing nega-tion and speculation also plays a role in extracting relations and events from the abun-dant literature on molecular biology.
Finding negative cases is useful for filtering outfalse positives in relation extraction, as support for automatic database curation, or forrefining pathways.Sanchez-Graillet and Poesio (2007) develop a heuristics-based system that extractsnegated protein?protein interactions using a full dependency parser from articles aboutchemistry.
The system uses cue words and information from the syntax tree to findpotential constructions that express negation.
If a negation construction is found, the13 Alter-association assertions state that the problem is not associated with the patient.244Morante and Sporleder Modality and Negationsystem extracts the arguments of the predicate that is negated based on the dependencytree.
The maximum F1 score that the system achieves is 62.96%, whereas the upper-bound of the system with gold-standard protein recognition is 76.68% F1 score.The BioNLP?09 Shared Task on Event Extraction (Kim et al 2009) addressed bio-molecular event extraction.
It consisted of three subtasks each aiming at different levelsof specificity, one of which was dedicated to finding whether the recognized biologicalevents are negated or speculated.
Six teams submitted systems with results varyingfrom 2.64 to 23.13 F-measure for negation and 8.95 to 25.27 for speculation.
To partic-ipate in this subtask the systems had to first perform Task 1 in order to detect events,which explains the low results.
The best scores were obtained by a system that appliessyntax-based heuristics (Kilicoglu and Bergler 2009).
Once events are identified, thesystem analyzes the dependency path between the event trigger and speculation ornegation cues in order to determine whether the event is within the scope of the cues.Sarafraz and Nenadic (2010a) further explore the potential of machine learningtechniques to detect negated events in the BioNLP?09 Shared Task data.
They train anSVM with a model that represents lexical, semantic, and syntax features.
The systemworks with gold-standard event detection and results are obtained by performing10-fold cross-validation experiments.
Evaluation is performed only on gene regula-tion events, which means that the results are not comparable with the Shared Taskresults.
The best results are obtained when all features are combined, achieving a 53.85F1 score.
Error analysis shows that contrastive patterns like that in Example (27) withthe cue unlike are recurrent as a source of errors.
Sarafraz and Nenadic (2010b) havealso compared a machine learning approach with a rule-based approach based oncommand relations, finding that the machine learning approach produces better re-sults.
Optimal results are obtained when individual classifiers are trained for eachevent class.
(27) Unlike TNFR1, LMP1 can interact directly with receptor-interactingprotein (RIP) and stably associates with RIP in EBV-transformedlymphoblastoid cell lines.Modality and negation processing at the event level has also been performed ontexts from a domain outside the biomedical domain.
Here we describe systems thatprocess the factuality of events and a modality tagger.EvITA and SlinkET (Saur?
?, Verhagen, and Pustejovsky 2006a, 2006b) are two sys-tems for automatically identifying and tagging events in text and assigning to themcontextual modality features.
EvITA assigns modality and polarity values to eventsusing pattern-matching techniques over chunks.
SlinkET is a rule-based system thatidentifies contexts of subordination that involve some types of modality, referred to asSLINKs in TimeML (Pustejovsky et al 2005), and assigns one of the following typesto them: factive, counterfactive, evidential, negative evidential or modal.
Thereported performance for SlinkET is 92% precision and 56% recall (Saur?
?, Verhagen,and Pustejovsky 2006a).
DeFacto (Saur??
2008) is a factuality profiler.
As Saur??
puts it,the algorithm assumes a conceptual model where factuality is a property that speakers(sources) attribute to events.
Two relevant aspects of the algorithm are that it processesthe interaction of different factuality markers scoping over the same event and that itidentifies the relevant sources of the event.
The system is described in detail in the articleby Saur??
and Pustejovsky included in this special issue.Baker et al (2010) take a different approach.
Instead of focusing on an event inorder to find its factuality, they focus on modality cues in order to find the predicate245Computational Linguistics Volume 38, Number 2that is within their scope (target).
They describe two modality taggers that identifymodality cues and modality targets, a string-based tagger and a structure-based tagger,and compare their performances.
The string-based tagger takes as input text taggedwith PoS and marks as modality cues words or phrases that match exactly cues from amodality lexicon.
More information about the modality taggers and their application inmachine translation can be found in the article by Baker et al included in this specialissue.Finally, Diab et al (2009) model belief categorization as a sequence labeling task,which allows them to treat cue detection and scope recognition in a unified fashion.Diab et al distinguish three belief categories.
For committed belief the writer indicatesclearly that he or she believes a proposition.
In the case of non-committed belief thewriter identifies the proposition as something in which he or she could believe butabout which the belief is not strong.
This category is further subdivided into weakbelief, which is often indicated by modals, such as may, and reported speech.
Thefinal category, not applicable, refers to cases which typically do not have a belief valueassociated with them, for example because the proposition does not have a truth value.This category covers questions and wishes.
Diab et al manually annotated a data setconsisting of 10,000 words with these categories and then used it to train and test anautomatic system for belief identification.
The system makes use of a variety of lexical,contextual, and syntactic features.
Diab et al found that relatively simple features suchas the tokens in a window around the target word and the PoS tags lead to the bestperformance, possibly due to the fact that some of the higher level features, such as theverb type, are noisy.6.2 Full Scope ResolutionThe scope resolution task consists of determining at a sentence level which tokensare affected by modality and negation cues.
Thanks to the existence of the BioScopecorpus several full scope resolvers have been developed.
The task was first modeledas a classification problem with the purpose of finding the scope of negation cues inbiomedical texts (Morante, Liekens, and Daelemans 2008).
It was further developedfor modality and negation cues by recent work on the same corpus (Morante andDaelemans 2009a, 2009b; O?zgu?r and Radev 2009), and it was consolidated with theedition of the 2010 CoNLL Shared Task on Learning to Detect Hedges and their Scope inNatural Language Text (Farkas et al 2010a).Morante, Liekens, and Daelemans (2008) approach the scope resolution task as aclassification task.
Their conception of the task is inspired by Ramshaw and Marcus?s(1995) representation of text chunking as a tagging problem and by the standard CoNLLrepresentation format (Buchholz andMarsi 2006).
By setting up the task in this way theyshow that the task can be modeled as a sequence labeling problem, and by conformingto the existing CoNLL standards they show that scope resolution could be integratedin a joint learning setting with dependency parsing and semantic role labeling.
Theirsystem is a memory-based scope finder that tackles the task in two phases: cue identifi-cation and scope resolution, which are modeled as consecutive token level classificationtasks.
Morante and Daelemans (2009b) present another scope resolution system thatuses a different architecture, can deal with multiword negation cues, and is tested onthe three subcorpora of the BioScope corpus.
For resolving the scope, three classifiers(kNN, SVM, CRF++) predict whether a token is the first token in the scope sequence,the last, or neither.
A fourth classifier is a metalearner that uses the predictions of the246Morante and Sporleder Modality and Negationthree classifiers to predict the scope classes.
The system is evaluated on three corporausing as measure the percentage of fully correct scopes (PCS), which is 66.07 for thecorpus of abstracts on which the classifiers are trained, 41.00 for the full articles and70.75 for the clinical reports.
They show that the system is portable to different corpora,although performance fluctuates.Full scope resolution of negation cues has been performed as a support task todetermine the polarity of sentiments.
In this context, negation is conceived as a con-textual valence shifter (Kennedy and Inkpen 2006).
If a sentiment is found withinthe scope of a negation cue, its polarity should be reversed.
Several proposals de-fine the scope of a negation cue in terms of a certain number of words to the rightof the cue (Pang, Lee, and Vaithyanathan 2002; Hu and Liu 2004), but this solution isnot accurate enough.
This is why research has been performed on integrating scoperesolver into sentiment analysis systems (Jia, Yu, and Meng 2009; Councill, McDonald,and Velikovich 2010).Jia, Yu, and Meng (2009) describe a rule-based system that uses information froma parse tree.
The algorithm first detects a candidate scope and then prunes the wordswithin the candidate scope that do not belong to the scope.
The candidate scope of anegation term t is formed by the descendant leaf nodes of the least common-ancestor ofthe node representing t and the node representing the word t?
immediately to the rightof t, that are found to the right of t?.
Heuristic rules are applied in order to determinethe boundaries of the candidate scope.
The rules involve the use of delimiters (elementsthat mark the end of the scope), and conditional delimiters (elements that mark the endof the scope under certain conditions).
Additionally, situations are defined in which anegation cue does not have a scope: phrases like not only, not just, not to mention, nowonder, negative rhetorical questions, and restricted comparative sentences.
Jia, Yu, andMeng report that incorporating their scope resolution algorithm into two systems thatdetermine the polarity of sentiment words in reviews and in the TREC blogospherecollection produces better accuracy results than incorporating other algorithms that aredescribed in the literature.Councill, McDonald, and Velikovich (2010) present a system in some aspects similarto the system described by Morante and Daelemans (2009b).
The main differences withMorante et al?s system are that in the first phase, the cues are detected by means of adictionary of 35 cues instead of being machine learned; in the second phase only a CRFclassifier is used, and this classifier incorporates features from dependency syntax.
Thesystem is trained and evaluated on the abstracts and clinical reports of the BioScopecorpus and on a corpus of product reviews.
The PCS reported for the BioScope corpusis 53.7 and 39.8 for the Product Reviews corpus.
Cross training results are also reportedshowing that the system obtains better results for the Product Reviews corpus whentrained on BioScope, which, according to the authors, would indicate that the scopeboundaries are more difficult to predict in the Product Reviews corpus.
Councill et alalso report that the scores of their sentiment analysis systemwith negation incorporatedimprove by 29.5% and 11.4% for positive and negative sentiment, respectively.
Fornegative sentiment precision improves 46.8% and recall 6.6%.It is worth mentioning that the systems trained on the BioScope corpus cannot dealwith intersentential, implicit, and affixal negation.
Further research could focus on theseaspects of negation.
Apart from scope resolvers for negation, several full scope resolvershave been developed for modality.Morante and Daelemans (2009a) test whether the scope resolver for negation(Morante and Daelemans 2009b) is portable to resolve the scope of hedge cues, showingthat the same scope resolution approach can be applied to both negation and hedging.247Computational Linguistics Volume 38, Number 2In the scope resolution phase, the system achieves 65.55% PCS in the abstracts corpus,which is very similar to the result obtained by the negation resolver (66.07% PCS).The system is also evaluated on the three types of text of the BioScope corpus.
Thedifference in performance for abstracts and full articles follows the same trends as inthe negation system, whereas the drop in performance for the clinical subcorpus ishigher, which indicates that there is more variation of modality cues across corporathan there is of negation cues.The modality scope resolver described by O?zgur and Radev (2009) solves the taskin two phases also, but differently from Morante and Daelemans (2009a); in the secondphase the scope boundaries are found with a rule-based module that uses informationfrom the syntax tree.
This system is evaluated on the abstracts and full articles of theBioScope corpus.
The scope resolution is evaluated in terms of accuracy, achieving79.89% in abstracts and 61.13% in full articles.Task 2 of the 2010 edition of the CoNLL Shared Task (Farkas et al 2010b) consistedof resolving the scope of hedge cues on biomedical texts.
A scope-level F1 measure wasused as the main evaluation metric where true positives were scopes which exactlymatched the gold-standard cues and gold-standard scope boundaries assigned to thecue word.
The best system (Morante, van Asch, and Daelemans 2010) achieved aF1 score of 57.3.
As Farkas et al (2010b) describe, each Task 2 system was built upona Task 1 system, attempting to recognize the scopes for the predicted cue phrases.
Mostsystems regarded multiple cues in a sentence to be independent from each other andformed different classification instances from them.
The scope resolution for a certaincue was typically carried out by a token based classification.
Systems differ in thenumber of class labels used as a target and in the machine learning approaches ap-plied.
Most systems, following Morante and Daelemans (2009a), used three class labels:first, last, and none, and two systems used four classes by adding inside, whereasthree systems followed a binary classification approach.
Most systems included a post-processing mechanism to produce continuous scopes, according to the BioScope anno-tation.
Sequence labeling and token-based classification machine learning approacheswere applied, and information from the dependency path between the cue and thetoken in question was generally encoded in the feature space.The system that scored the best results for Task 2 (Morante, van Asch, andDaelemans 2010) follows the same approach as Morante and Daelemans (2009a), al-though it introduces substantial differences: This system uses only one classifier tosolve Task 2, whereas the system described in Morante and Daelemans (2009a) usedthree classifiers and a metalearner; this system uses features from both shallow anddependency syntax, instead of only shallow syntax features; and it incorporates in thefeature representation information from a lexicon of hedge cues generated from thetraining data.As a follow-up of the CoNLL Shared Task, ?vrelid, Velldal, and Oepen (2010)investigate the contribution of syntax to scope resolution.
They apply a hybrid, two-stage approach to the scope resolution task.
In the first stage, a Maximum Entropyclassifier, combining surface-oriented and syntax features, identifies cue words, andmultiword cues are identified in a postprocessing step.
In the second stage a small setof hand-crafted rules operating over dependency representations are applied to resolvethe scope.
This system is evaluated following exactly the same settings as the CoNLLShared Task.
The results do not improve over the best shared task results but show thathandcrafted syntax-based rules achieve a very competitive performance.
?vrelid et alreport that the errors of their system are mostly of two classes: (i) failing to recognizephrase and clause boundaries, as in Example (28a), and (ii) not dealing successfully with248Morante and Sporleder Modality and Negationrelatively superficial properties of the text as in Example (28b).
The scope boundariesproduced by the system are marked with ???.
(28) a.
... [the reverse complement ?mR of m will be considered to be ...?].b.
This ?
[might affect the results] if there is a systematic bias on thecomposition of a protein interaction set?.Finally, Zhu et al (2010) approach the scope learning problem via simplified shallowsemantic parsing.
The cue is regarded as the predicate and its scope is mapped intoseveral constituents as the arguments of the cue.
The system resolves the scope of nega-tion and modality cues in the standard two phase approach.
For cue identification theyapply an SVM that uses features from the surrounding words and from the structure ofthe syntax tree.
The scope resolution task is different than in previous systems.
The taskis addressed in three consecutive phases: (1) argument pruning, consisting of collectingas argument candidates any constituent in the parse tree whose parent covers the givencue except the cue node itself and its ancestral constituents; (2) argument identificationwhere a binary classifier is applied to determine the argument candidates as eithervalid arguments or non-arguments; and (3) postprocessing to guarantee that the scopeis a continuous sequence of arguments.
The system is trained on the abstracts part ofthe BioScope corpus and tested on the three parts of the BioScope corpus.
Evaluatingthe system following the CoNLL Shared Task setting would shed more light on theadvantages of the semantic parsing approach as compared to other approaches.From the systems and results described in this section, we can conclude that al-though there has been substantial research on the scope resolution task, there is stillroom for improvement.
The performance of scope resolvers is still far from havingreached the level of well established tasks like semantic role labeling or parsing.
Prob-ably, better results can be obtained by a combination of more experimental work withalgorithms and a deeper analysis of the task from a linguistic perspective so that therepresentation models can be improved.
The article by Velldal et al in this special issueprovides new insights into the task.7.
Processing Contradiction and ContrastThe concept of negation is closely related to the discourse-level concepts of ?contradic-tion?
and ?contrast,?
which typically require an explicit or implicit negation.Contradiction is a relation that holds between two documents with contradictorycontent.
Detecting contradiction is important for tasks which extract information frommulti-document collections, such as question-answering and multi-document summa-rization.
Since 2007 contradiction detection has also been included as a subtask in theTextual Entailment Challenge (Giampiccolo et al 2007), spurring an increased interestin the development of systems which can automatically detect contradictions.
The twocontradictory sentence pairs in Examples (29) and (30) (both from Harabagiu et al[2006]) illustrate the relation between contradiction and negation.
In Example (29) thecontradiction is signaled by the explicit negation marker never, whereas in Example (30)the negation is implicit and signaled by the use of call off in the second sentence (whichis an antonym of begin in the first sentence).
(29) a. Joachim Johansson held off a dramatic fightback from defendingchampion Andy Roddick, to reach the semi-finals of the US Openon Thursday night.249Computational Linguistics Volume 38, Number 2b.
Defending champion Andy Roddick never took on Joachim Johansson.
(30) a.
In California, one hundred twenty Central Americans, due to bedeported, began a hunger strike when their deportation was delayed.b.
A hunger strike was called off.Although contradiction typically occurs across documents, contrast is a discourserelation within documents.
At least some types of contrast involve negation, notablythose that involve a denial of expectation.
The negation can be explicit as in Exam-ple (31a), implicit (31b), or entailed (31c) (see Umbach [2004]).
(31) a. John cleaned his room, but he didn?t wash the dishes.b.
John cleaned his room, but he skipped the washing up.c.
John cleaned up the room, but Bill did the dishes.Given this interrelation between negation and contradiction on the one hand andnegation and contrast on the other, it it not surprising that negation detection has beenstudied in the context of discourse relation classification and contradiction detection.Most studies in this area use fairly standard?i.e., sentence-based?methods for nega-tion detection.
Once the negation has been detected it is then used as a feature for thehigher-level tasks of contradiction or contrast detection.For instance, Harabagiu, Hickl, and Lacatusu (2006) discuss a system which firstdetects negated expressions and then finds contradictions on the basis of the de-tected negations.
To detect explicit negation Harabagiu et al use a lexicon of explicitcues.
To determine the scope they use a set of heuristics, which varies depending onwhether the negated object is an event, an entity, or a state.
For events, the negation isassumed to scope over the whole predicate?argument structure.
For entities and forstates realized by nominalizations the negation is assumed to scope over the wholeNP.
Implicit negations are detected by searching for antonymy chains in WordNet.
deMarneffe, Rafferty, and Manning (2008) also make use of negation detection to discovercontradictions.
They do so rather implicitly, however, by using a number of featureswhich check for explicit negation, polarity, and antonymy.
Ritter et al (2008) present acontradiction detection system that uses the TEXTRUNNER system (Banko et al 2007)to extract relations of the form R(x,y) (e.g., was born in(Mozart,Salzburg)).
They theninspect potential contradictions (i.e., relations which overlap in one variable but not inthe other) and filter out non-contradictions by looking, for example, for synonyms andmeronyms.In the context of contrast detection in discourse processing, negation detection israrely used as an explicit step.
An exception is Kim et al (2006), who are concernedwith discovering contrastive information about protein interaction in biomedical texts.They only deal with explicitly marked negation which occurs in the context of a contrastrelation marked by a contrast signaling connective such as but.
Unlike Kim et al, Pitler,Louis, and Nenkova (2009) are concerned with detecting implicit discourse relations?namely, relations which are not explicitly signalled by a connective such as but.
To detectsuch relations, they define a number of features, including polarity features.
Hence theymake implicit use of negation information but do not aim to detect it as a separatesubtask.250Morante and Sporleder Modality and Negation8.
Positive and Negative OpinionsMuch work in the NLP community has been carried out in the area of identifyingpositive and negative opinions, also known as opinion mining, sentiment analysis, orsubjectivity analysis.14 Sentiment analysis touches on the topic of this special issue asboth negation and modality cues can help determine the opinion of an opinion holderon a subject.
Negation in particular has received attention in the sentiment analysiscommunity as negation can affect the polarity of an expression.
Negation and polarityare two different concepts, however (see Section 3.1).
The relation between negation andpolarity is also not always entirely straightforward.
For example, whereas negation canchange the polarity of an expression from positive to negative (e.g., good vs. not good inExamples (32a) vs. (32b)) it can also shift negative polarity to neutral or even positivepolarity (32c).
(32) a.
This is a good camera.b.
This is not a good camera.c.
This is by no means a bad camera.In this section, we discuss some approaches that make explicit use of negation inthe context of sentiment analysis.
For a recent general overview of work on sentimentanalysis, we refer the reader to Pang and Lee (2008).Wiegand et al (2010) present a survey of the role of negation in sentiment analysis.They indicate that it is necessary to perform fine-grained linguistic analysis in orderto extract features for machine learning or rule-based opinion analysis systems.
Thefeatures allow the incorporation of information about linguistic phenomena such asnegation (Wiegand et al 2010, page 60).
Early approaches made use of negation ina bag-of-words model by prefixing a word x with a negation marker if a negationword was detected immediately preceding x (Pang, Lee, and Vaithyanathan 2002).Thus x and NOT x were treated as two completely separate features.
Although thismodel is relatively simple to compute and leads to an improvement over a bag-of-words model without negation, Pang, Lee, and Vaithyanathan (2002) found that theeffect of adding negation was relatively small, possibly because the introductionof additional features corresponding to negated words increases the feature spaceand thereby also data sparseness.
Later work introduced more sophisticated use ofnegation, for example, by explicitly modeling negation expressions as polarity shifters,which change the polarity of an expression (Kennedy and Inkpen 2006; Polanyiand Zaenen 2006), or by introducing specific negation features (Wilson, Wiebe, andHoffman 2005; Wilson, Wiebe, and Hwa 2006; Wilson 2008).
It was found that thesemore sophisticated models typically lead to a significant improvement over a simplebag-of-words model with negation prefixes.
This improvement can to a large extentbe directly attributed to the better modeling of negation (Wilson, Wiebe, and Hoffman2009).
Whereas modeling negation in opinion mining frequently involves determiningthe polarity of opinions (Hu and Liu 2004; Kim and Hovy 2004; Wilson, Wiebe, andHoffman 2005; Wilson 2008), some researchers have also used negation models to14 The three terms are used sometimes interchangeably and sometimes reserved for somewhat differentcontexts.
We follow here the definitions of Pang and Lee (2008) who use ?opinion mining?
and?sentiment analysis?
as largely synonymous terms and ?subjectivity analysis?
as a cover term for both.251Computational Linguistics Volume 38, Number 2determine the strength of opinions (Popescu and Etzioni 2005; Wilson, Wiebe, and Hwa2006).
Choi and Cardie (2010) found that performing both tasks jointly can lead to asignificant improvement over a pipeline model in which the two tasks are performedseparately.
Councill, McDonald, and Velikovich (2010) also show that explicit modelingof negation has a positive effect on polarity detection.9.
Overview of the Articles in this Special IssueFor this special issue we invited articles on all aspects of the computational modelingand processing of modality and negation.
Given that this area is both theoreticallycomplex?with several competing linguistic theories having been put forward forvarious aspects of negation and modality?and computationally challenging, weparticularly encouraged submissions with a substantial analysis component, eitherin the form of a data or task analysis or in the form of a detailed error analysis.
Wereceived 25 submissions overall, reflecting a significant interest in these phenomenain the computational linguistics community.
After a rigorous review process, weselected five articles, covering various aspects of the topic.
Three of the articles (Saur?
?and Pustejovsky; de Marneffe et al; and Szarvas et al) deal with one specific aspectof modality, namely, certainty (in the widest sense) from both a theoretical and acomputational perspective.
The remaining two articles (Velldal et al and Baker et al)deal with both negation and modality detection in a more application-focused setting.The following paragraphs provide a detailed overview of the articles.In the first article, Saur??
and Pustejovsky introduce their model of factuality.
Theydistinguish the dimensions of polarity and certainty and use a four-point scale for thelatter.
They also explicitly model different sources and embedding of factuality acrossseveral levels.
They then present a linguistically motivated, symbolic system, DeFacto,for computing factuality and attributing it to the correct sources.
The model operateson dependency parses and exploits a number of lexical cues together with hard-codedrules to process factuality within a sentence in a top?down fashion.Whereas Saur??
and Pustejowsky focus on lexical and intra-sentential aspects offactuality, the article by de Marneffe et al looks specifically at the pragmatic componentof factuality (called veridicality in their article).
They argue that although individuallexemes might be associated with discrete veridicality categories out of context, spe-cific usages are better viewed as evoking probability distributions over veridicalitycategories, where world knowledge and discourse context can shift the probabilitiesin one or the other direction.
To support this hypothesis, de Marneffe et al carriedout an annotation study with linguistically naive subjects, which provides evidence forconsiderable variation between subjects, especially with respect to neighboring veridi-cality categories.
In a second step, de Marneffe et al show how this type of pragmaticveridicality can be modeled in a supervised machine learning setting.In the following article, Szarvas et al provide a cross-domain and cross-genreview of (un-)certainty.
They propose a novel categorization scheme for uncertainty thatunifies existing schemes, which, they argue, are to some extent domain- and genre-dependent.
They provide a detailed analysis of different linguistic manifestations ofuncertainty in several types of text and then propose a method for adapting uncertaintydetection systems to novel domains.
They show that instead of simply boosting theavailable training data from the target domain with randomly selected data from thesource domain, it is often more beneficial to select those instances from the sourcedomain that contain uncertainty cues that are also observed in the target domain.
In252Morante and Sporleder Modality and Negationthis scenario, the additional data from the source domain is exploited to fine-tune thedisambiguation of target domain cues rather than to learn novel cues.Moving from certainty to negation and speculation in a more general sense, Velldalet al show how deep and shallow approaches can be combined for cue detection andscope resolution.
They assume a closed class of speculation and negation cues and castcue detection as a disambiguation rather than a classification task, using supervisedmachine learning based on n-gram features.In a second step, they tackle scope resolution, for which they propose two models.The first implements a number of syntax-driven rules over dependency structures, andthe second model is data-driven and ranks candidate scopes on the basis of constituenttrees.The final article, by Baker et al, also addresses modality and negation processing,but within a particular application scenario, namely, machine translation.
The authorspropose a novel annotation scheme for modality and negation and two rule-basedtaggers for identifying cues and scopes.
The first tagger employs string matching incombination with a semi-automatically developed cue lexicon; the second goes beyondthe surface string and utilizes heuristics based on syntax.
In the machine translationprocess, syntax trees in the source language are then automatically enriched withmodality and negation information before being translated.10.
Final RemarksIn this article, we have given an overview of the treatment of negation and modalityin computational linguistics.
Although much work has been done in recent years andmany models for dealing with various aspects of these two phenomena have beenproposed, it is clear that much still remains to be done.The first challenge is a theoretical one and pertains to the categorization and anno-tation of negation and, especially, modality.
Currently, many annotation schemes existin parallel (see Section 4).
As a consequence, the existing annotated corpora are all rel-atively small.
Significant progress in this area depends on the availability of annotatedresources, however, both for training and testing automated systems and for (corpus)linguistic studies that can support the development of linguistically informed systems.Ideally, any larger scale resource creation project should be preceded by a discussion inthe computational linguistics community about which aspects of negation andmodalityshould be annotated and how this should be done (see, e.g., Nirenburg and McShane[2008]).
To some extent this is already happening and the public release of annotatedresources such as the MPQA (Wiebe, Wilson, and Cardie 2005) or the BioScope (Vinczeet al 2008) corpus, as well as the organization of shared tasks (Farkas et al 2010a), aresteps in the right direction.
Related to this challenge is the question of which aspectsof extra-propositional meaning need to be modeled for which applications.
Outsidesentiment analysis, relatively little research has been carried out in this area so far.A second challenge involves the adequate modeling of modality and negation.For example, although we can detect extra-propositional content, few researchers sofar have investigated how interactions between extra-propositional meaning aspectscan be adequately modeled.
Also, most approaches have addressed the detection ofnegation at a sentence or predicate level.
Discourse-level interdependencies betweendifferent aspects of extra-propositional content have been largely ignored.
To addressthis challenge, we believe that more research into linguistically motivated approachesis necessary.253Computational Linguistics Volume 38, Number 2Finally, most research so far has been carried out on English and on selected do-mains and genres (biomedical, reviews, newswire).
It would be interesting to also lookat different languages and devisemethods for cross-lingual bootstrapping.
It would alsobe good to broaden the set of domains and genres (including fiction, scientific texts,weblogs, etc.)
since extra-propositional meaning is particularly susceptible to domainand genre effects.AcknowledgmentsRoser Morante?s research is funded bythe GOA project BioGraph: Text Miningon Heterogeneous Databases: An Applicationto Optimized Discovery of Disease RelevantGenetic Variants of the University ofAntwerp, Belgium.
Caroline Sporlederis supported by the German ResearchFoundation DFG (Cluster of ExcellenceMultimodal Computing and Interaction[MMCI]).ReferencesAikhenvald, Alexandra Y.
2004.
Evidentiality.Oxford University Press, New York.Aronow, David B., Feng Fangfang, and W.Bruce Croft.
1999.
Ad hoc classification ofradiology reports.
JAMIA, 6(5):393?411.Averbuch, Mordechai, Tom H. Karson,Benjamin Ben-Ami, Oded Maimon, andLior Rokach.
2004.
Context-sensitivemedical information retrieval.
InProceedings of the 11th World Congress onMedical Informatics (MEDINFO-2004),pages 1?8, San Francisco, CA.Baker, Kathrin, Michael Bloodgood, MonaDiab, Bonnie Dorr, Ed Hovy, Lori Levin,Marjorie McShane, Teruko Mitamura,Sergei Nirenburg, Christine Piatko, OwenRambow, and Gramm Richardson.
2009.SIMT SCALE 2009 modality annotationguidelines.
Technical report 4, HumanLanguage Technology Center ofExcellence, Baltimore, MD.Baker, Kathrin, Michael Bloodgood, BonnieDorr, Nathaniel W. Filardo, Lori Levin,and Christine Piatko.
2010.
A modalitylexicon and its use in automatic tagging.In Proceedings of the Seventh Conference onInternational Language Resources andEvaluation (LREC?10), pages 1402?1407,Valetta.Banfield, Ann.
1982.
Unspeakable Sentences.Routledge and Kegan Paul, Boston, MA.Banko, Michele, Michael Cafarella, StephenSoderland, Matt Broadhead, and OrenEtzioni.
2007.
Open information extractionfrom the Web.
In Proceedings of InternationalJoint Conferences on Artificial Intelligence2007, pages 2670?2676, Hyderabad.Blanco, Eduardo and Dan Moldovan.
2011.Semantic representation of negation usingfocus detection.
In Proceedings of 49thAnnual Meeting of the Association forComputational Linguistics, pages 19?24,Portland, OR.Boas, Franz, 1938.
Language.
In Franz Boas,General Anthropology.
D.C. Heath andCompany, Boston, MA, pages 124?145.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings ofthe X CoNLL Shared Task, pages 149?164,New York.Chafe, Wallace.
1986.
Evidentiality inEnglish conversation and academicwriting.
In W. Chafe and J. Nichols,editors.
Evidentiality: The Linguistic Codingof Epistemology.
Ablex, Norwood, NJ.Chapman, Wendy W., Will Bridewell, PaulHanbury, Gregory F. Cooper, and Bruce G.Buchanan.
2001a.
A simple algorithm foridentifying negated findings and diseasesin discharge summaries.
J Biomed Inform,34:301?310.Chapman, Wendy W., Paul Hanbury,Gregory F. Cooper, and Bruce G.Buchanan.
2001b.
Evaluation of negationphrases in narrative clinical reports.In Proceedings of the American MedicalInformatics Association Symposium.
2001,pages 105?109, Washington, DC.Choi, Yejin and Claire Cardie.
2010.Hierarchical sequential learning forextracting opinions and their attributes.In Proceedings of the ACL 2010 ConferenceShort Papers, pages 269?274, Uppsala.Collier, Nigel, Hyun S. Park, NorihiroOgata, Yuka Tateisi, Chikashi Nobata,Tomoko Ohta, Tateshi Sekimizu, HisaoImai, Katsutoshi Ibushi, and Jun?ichi Tsujii.1999.
The GENIA project: Corpus-basedknowledge acquisition and informationextraction from genome research papers.In Proceedings of EACL-99, pages 271?272,Bergen.Councill, Isaac, Ryan McDonald, and LeonidVelikovich.
2010.
What?s great and what?snot: Learning to classify the scope ofnegation for improved sentiment analysis.In Proceedings of the Workshop on Negation254Morante and Sporleder Modality and Negationand Speculation in Natural LanguageProcessing, pages 51?59, Uppsala.Dalianis, Hercules and Sumithra Velupillai.2010.
How certain are clinical assessments?Annotating Swedish clinical text for(un)certainties, speculations andnegations.
In Proceedings of the SeventhConference on International LanguageResources and Evaluation (LREC?10),Valletta.de Haan, Frederik.
1995.
The Interaction ofModality and Negation: A Typological Study.Garland Publishing, Inc., New York.de Haan, Frederik.
1999.
Evidentialityand epistemic modality: Setting theboundaries.
Journal of Linguistics,18:83?102.de Marneffe, Marie-Catherine, BillMaccartney, Trond Grenager, Daniel Cer,Anna Rafferty, and Christopher D.Manning.
2006.
Learning to distinguishvalid textual entailments.
In Proceedings ofthe Second PASCAL Challenges Workshopon Recognising Textual Entailment,pages 74?79, Venice.de Marneffe, Marie-Catherine, Anna N.Rafferty, and Christopher D. Manning.2008.
Finding contradictions in text.
InProceedings of ACL 2008, pages 1039?1047,Columbus, OH.Di Marco, Chrysanne, Frederick Kroon, andRobert Mercer.
2006.
Using hedges toclassify citations in scientific articles.
InW.
Bruce Croft, James Shanahan, Yan Qu,and Janyce Wiebe, editors, ComputingAttitude and Affect in Text: Theory andApplications, volume 20 of The InformationRetrieval Series.
Springer, Amsterdam,pages 247?263.Diab, Mona T., Lori Levin, Teruko Mitamura,Owen Rambow, VinodkumarPrabhakaran, and Weiwei Guo.
2009.Committed belief annotation and tagging.In ACL-IJNLP 09: Proceedings of theThird Linguistic Annotation Workshop,pages 68?73, Singapore.Elkin, Peter L., Steven H. Brown, Brent A.Bauer, Casey S. Husser, William Carruth,Larry R. Bergstrom, and Dietlind L.Wahner-Roedler.
2005.
A controlledtrial of automated classification ofnegation from clinical notes.
BMC MedicalInformatics and Decision Making, 5(13).doi: 10.1186/1472-6947-5-13.Farkas, Richa?rd, Veronika Vincze, Gyo?rgyMo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.2010a.
The CoNLL 2010 shared task:Learning to detect hedges and their scopein natural language text.
In Proceedings ofthe CoNLL2010 Shared Task, pages 1?12,Uppsala.Farkas, Richa?rd, Veronika Vincze, Gyo?rgySzarvas, Gyo?rgy Mo?ra, and Ja?nos Csirik,editors.
2010b.
Proceedings of the FourteenthConference on Computational NaturalLanguage Learning, Uppsala.Friedman, Carol, Philip Alderson, John H. M.Austin, James J. Cimino, and Stephen B.Johnson.
1994.
A general natural?languagetext processor for clinical radiology.JAMIA, 1(2):161?174.Ganter, Viola and Michael Strube.
2009.Finding hedges by chasing weasels:Hedge detection using wikipedia tags andshallow linguistic features.
In Proceedingsof the ACL-IJCNLP 2009 Conference ShortPapers, pages 173?176, Suntec.Garson, James.
2009.
Modal logic.In Edward N. Zalta, editor, TheStanford Encyclopedia of Philosophy.Standord University, CA.
Availableat http://plato.stanford.edu/archives/win2009/entries/logic-modal/.Georgescul, Maria.
2010.
A hedgehop over amax-margin framework using hedge cues.In Proceedings of the Fourteenth Conferenceon Computational Natural LanguageLearning, pages 26?31, Uppsala.Giampiccolo, Danilo, Bernardo Magnini,Ido Dagan, and Bill Dolan.
2007.The third pascal recognizing textualentailment challenge.
In Proceedings ofthe ACL-PASCAL Workshop on TextualEntailment and Paraphrasing, pages 1?9,Prague.Goldin, Ilya M. and Wendy W. Chapman.2003.
Learning to detect negation with?Not?
in medical texts.
In Proceedings ofACM-SIGIR 2003, Toronto.Goryachev, Sergey, Margarita Sordo,Qing T. Zeng, and Long Ngo.
2006.Implementation and evaluation of fourdifferent methods of negation detection.Technical report DSG.
Harvard MedicalSchool, Boston, MA.Grabar, Natalia and Thierry Hamon.2009.
Exploitation of speculationmarkers to identify the structure ofbiomedical scientifc writing.
InAMIA 2009 Symposium Proceedings,pages 203?207, San Francisco, CA.Harabagiu, Sanda, Andrew Hickl, andFinley Lacatusu.
2006.
Negation, contrastand contradiction in text processing.In Proceedings of the 21st InternationalConference on Artificial Intelligence,pages 755?762, Las Vegas, NV.255Computational Linguistics Volume 38, Number 2Harkema, Henk, John N. Dowling, TylerThornblade, and Wendy W. Chapman.2009.
ConText: An algorithm fordetermining negation, experiencer,and temporal status from clinicalreports.
Journal of Biomedical Informatics,42:839?851.Hickl, Andrew and Jeremy Bensley.
2007.A discourse commitment-basedframework for recognizing textualentailment.
In Proceedings of theACL-PASCAL Workshop on TextualEntailment and Paraphrasing,pages 171?176, Stroudsburg, PA.Hoeksema, Jack.
2000.
Negative polarityitems: triggering, scope, and c-command.In L. Horn and Y. Kato, editors, Negationand Polarity.
Oxford University Press,Oxford, pages 115?146.Horn, Laurence R. 1989.
A Natural Historyof Negation.
Chicago University Press,Chicago, IL.Hu, Minqing and Bing Liu.
2004.
Mining andsummarizing customer reviews.
In KDD?04: Proceedings of the Tenth ACM SIGKDDInternational Conference on KnowledgeDiscovery and Data Mining, pages 168?177,New York.Huang, Yang and Henry J. Lowe.
2007.
Anovel hybrid approach to automatednegation detection in clinical radiologyreports.
Journal of the American MedicalInformatics Association, 14(3):304?311.Hyland, Ken.
1998.
Hedging in ScientificResearch Articles.
John Benjamins B.V.,Amsterdam.Ifantidou, Elly.
2001.
Evidentials andRelevance.
John Benjamins, Amsterdam.Israel, Michael.
2004.
The pragmatics ofpolarity.
In L. Horn and G. Ward, editors,The Handbook of Pragmatics.
Blackwell,Oxford, pages 701?723.Jespersen, Otto.
1924.
The Philosophy ofGrammar.
Allen and Unwin, London.Jia, Lifeng, Clement Yu, and Weiyi Meng.2009.
The effect of negation on sentimentanalysis and retrieval effectivenesss.
InProceedings of the 18th ACM Conference onInformation and Knowledge Management,pages 1827?1830, New York City.Kennedy, Alistair and Diana Inkpen.2006.
Sentiment classification of moviereviews using contextual valence shifters.Computational Intelligence, 22(2):110?125.Kilicoglu, Halil and Sabine Bergler.
2008.Recognizing speculative languagein biomedical research articles: alinguistically motivated perspective.BMC Bioinformatics, 9(Suppl 11):S10.Kilicoglu, Halil and Sabine Bergler.
2009.Syntactic dependency based heuristics forbiological event extraction.
In Proceedingsof the Workshop on Current Trends inBiomedical Natural Language Processing:Shared Task, BioNLP ?09, pages 119?127,Stroudsburg, PA.Kim, Jin-Dong, Tomoko Ohta, SampoPyysalo, Yoshinobu Kano, and Jun?ichiTsujii.
2009.
Overview of BioNLP sharedtask on event extraction.
In Proceedings ofthe BioNLP 2009 Workshop CompanionVolume for Shared Task 2009, pages 1?9,Boulder, CO.Kim, Jin-Dong, Tomoko Ohta, and Jun?ichiTsujii.
2008.
Corpus annotation for miningbiomedical events from literature.
BMCBioinformatics, 9(10).
doi:10.1186/1471-2105-9-10.Kim, Jung-Jae, Zhuo Zhang, Jong C. Park,and See-Kiong Ng.
2006.
BioContrasts:Extracting and exploiting protein?proteincontrastive relations from biomedicalliterature.
Bioinformatics, 22:597?605.Kim, Soo-Min and Ed Hovy.
2004.Determining the sentiment of opinions.In COLING ?04: Proceedings of the 20thInternational Conference on ComputationalLinguistics, page 1367, Morristown, NJ.Kratzer, Angelika.
1981.
The notionalcategory of modality.
In H. J. Eikmeyerand H. Rieser, editors,Words, Worlds,and Contexts.
New Approaches in WordSemantics.
De Gruyter, Berlin, pages 38?74.Kratzer, Angelika.
1991.
Modality.
InA.
von Stechow and D. Wunderlich,editors, Semantics: An InternationalHandbook of Contemporary Research.De Gruyter, Berlin, pages 639?650.Kripke, Saul.
1963.
Semantic considerationson modal logic.
Acta Philosophica Fennica,16:83?94.Lakoff, George.
1972.
Hedges: a study inmeaning criteria and the logic of fuzzyconcepts.
Chicago Linguistics Society Papers,8:183?228.Lawler, John.
2010.
Negation and negativepolarity.
In P. C. Hogan, editor, CambridgeEncyclopedia of the Language Sciences.Cambridge University Press, Cambridge,UK, pages 554?555.Light, Mark, Xin Y. Qiu, and PadminiSrinivasan.
2004.
The language ofbioscience: Facts, speculations, andstatements in between.
In Proceedings ofBioLINK 2004, pages 17?24, Boston, MA.Linguistic Data Consortium.
2008.
ACE(Automatic Content Extraction) Englishannotation guidelines for relations.256Morante and Sporleder Modality and NegationTechnical Report Version 6.2 2008.04.28.LDC, Philadelphia, PA.Lyons, John.
1977.
Semantics.
CambridgeUniversity Press, Cambridge, UK.Matsuyoshi, Suguru, Megumi Eguchi,Chitose Sao, Koji Murakami, Kentaro Inui,and Yuji Matsumoto.
2010.
Annotatingevent mentions in text with modality,focus, and source information.
InProceedings of the Seventh Conference onInternational Language Resources andEvaluation (LREC?10), pages 1456?1463,Valletta.Medlock, Ben.
2008.
Exploring hedgeidentification in biomedical literature.JBI, 41:636?654.Medlock, Ben and Ted Briscoe.
2007.
Weaklysupervised learning for hedge classificationin scientific literature.
In Proceedings ofACL 2007, pages 992?999, Prague.Morante, Roser.
2010.
Descriptive analysisof negation cues in biomedical texts.In Proceedings of the Seventh Conferenceon International Language Resources andEvaluation (LREC?10), pages 1429?1436,Valletta.Morante, Roser and Walter Daelemans.2009a.
Learning the scope of hedge cues inbiomedical texts.
In Proceedings of BioNLP2009, pages 28?36, Boulder, CO.Morante, Roser and Walter Daelemans.2009b.
A metalearning approach toprocessing the scope of negation.In Proceedings of CoNLL 2009,pages 28?36, Boulder, CO.Morante, Roser, Anthony Liekens, andWalter Daelemans.
2008.
Learning thescope of negation in biomedical texts.In Proceedings of EMNLP 2008,pages 715?724, Honolulu, HI.Morante, Roser, Sarah Schrauwen, andWalter Daelemans.
2011.
Annotation ofnegation cues and their scope.
Guidelinesv1.0.
Technical Report 3, CLiPS, Antwerp,Belgium.Morante, Roser, Vincent van Asch, andWalter Daelemans.
2010.
Memory-basedresolution of in-sentence scopes ofhedge cues.
In Proceedings of CoNLL,pages 40?47, Uppsala.Mutalik, Pradeep G., Aniruddha Deshpande,and Prakash M. Nadkarni.
2001.
Use ofgeneral-purpose negation detection toaugment concept indexing of medicaldocuments.
A quantitative study usingthe UMLS.
Journal of the American MedicalInformatics Association, 8(6):598?609.Nawaz, Raheel, Paul Thompson, andSophia Ananiadou.
2010.
Evaluating ameta-knowledge annotation scheme forbio-events.
In Proceedings of the Workshopon Negation and Speculation in NaturalLanguage Processing, pages 69?77, Uppsala.Nirenburg, Sergei and Marjorie McShane.2008.
Annotating modality.
OntoSem finalproject report.
University of Maryland,Baltimore County.Nirenburg, Sergei and Victor Raskin.2004.
Ontological Semantics.
MIT Press,Cambridge, MA.
?vrelid, Lilja, Erik Velldal, and StephanOepen.
2010.
Syntactic scope resolutionin uncertainty analysis.
In Proceedingsof the 23rd International Conference onComputational Linguistics, COLING ?10,pages 1379?1387, Stroudsburg, PA.O?zgu?r, Arzucan and Dragomir R. Radev.2009.
Detecting speculations and theirscopes in scientific text.
In Proceedings ofEMNLP 2009, pages 1398?1407, Singapore.Palmer, Frank R. 1986.Mood andModality.
Cambridge University Press,Cambridge, UK.Pang, Bo and Lillian Lee.
2008.
Opinionmining and sentiment analysis.Foundations and Trends in InformationRetrieval, 2(1?2):1?135.Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?Sentiment classification using machinelearning techniques.
In Proceedings of the2002 Conference on Empirical Methods inNatural Language Processing, pages 79?86.Philadelphia, PA.Parsons, Terence.
2008.
The traditionalsquare of opposition.
In E. N. Zalta,editor, The Stanford Encyclopedia ofPhilosophy.
Stanford University.
Availableat http://plato.stanford.edu/archives/fall2008/entries/square/.Payne, Thomas E. 1997.
DescribingMorphosyntax.
Cambridge UniversityPress, Cambridge, UK.Pitler, Emily, Annie Louis, and Ani Nenkova.2009.
Automatic sense prediction forimplicit discourse relations in text.
InProceedings of the Joint Conference of the47th Annual Meeting of the ACL and the4th International Joint Conference onNatural Language Processing of the AFNLP,pages 683?691, Suntec.Polanyi, Livia and Annie Zaenen.
2006.Contextual valence shifters.
In W. BruceCroft, James Shanahan, Yan Qu, andJanyce Wiebe, editors, Computing Attitudeand Affect in Text: Theory and Applications,volume 20 of The Information RetrievalSeries.
Springer, Netherlands, pages 1?10.257Computational Linguistics Volume 38, Number 2Popescu, Ana-Maria and Oren Etzioni.
2005.Extracting product features and opinionsfrom reviews.
In HLT ?05: Proceedings of theconference on Human Language Technologyand Empirical Methods in Natural LanguageProcessing, pages 339?346, Morristown, NJ.Portner, Paul.
2009.Modality.
OxfordUniversity Press, Oxford, UK.Prabhakaran, Vinodkumar, Owen Rambow,and Mona Diab.
2010.
Automaticcommitted belief tagging.
In Proceedings ofCOLING 2010, pages 1014?1022, Beijing.Prasad, Rashmi, Nikhil Dinesh, Alan Lee,Aravind Joshi, and Bonnie Webber.
2006.Annotating attribution in the PennDiscourse TreeBank.
In SST ?06: Proceedingsof the Workshop on Sentiment and Subjectivityin Text, pages 31?38, Morristown, NJ.Prasad, Rashmi, Nikhil Dinesh, Alan Lee,Eleni Miltsakaki, Livio Robaldo, AravindJoshi, and Bonnie Webber.
2008.
The PennDiscourse TreeBank 2.0.
In Proceedings ofthe Sixth International Language Resourcesand Evaluation (LREC?08), pages 2961?2968,Marrakech.Pustejovsky, James, Robert Knippen, JessicaLittman, and Roser Saur??.
2005.
Temporaland event information in natural languagetext.
Language Resources and Evaluation,39(2?3):123?164.Pustejovsky, James, Mark Verhagen, RoserSaur?
?, Jessica Littman, Robert Gaizauskas,Graham Katz, Inderjeet Mani, RobertKnippen, and Andrea Setzer.
2006.Timebank 1.2.
Linguistic DataConsortium, Philadelphia, PA.Pyysalo, Sampo, Filip Ginter, JuhoHeimonen, Jari Bjo?rne, Jorma Boberg,Jouni Ja?rvinen, and Tapio Salakoski.2007.
BioInfer: A corpus for informationextraction in the biomedical domain.BMC Bioinformatics, 8(50).
doi:10.1186/1471-2105-8-50.Ramshaw, Lance and Mitch Marcus.
1995.Text chunking using transformation-basedlearning.
In Proceedings of ACL ThirdWorkshop on Very Large Corpora,pages 82?94, Cambridge, MA.Rigoutsos, Isidore and Aros Floratos.
1998.Combinatorial pattern discovery inbiological sequences: The TEIRESIASalgorithm.
Bioinformatics, 14(1):55?67.Ritter, Alan, Stephen Soderland, DougDowney, and Oren Etzioni.
2008.
It?s acontradiction?no, it?s not: A case studyusing functional relations.
In Proceedings ofEMNLP 2008, pages 11?20, Honolulu, HI.Rokach, Lior, Roni Romano, and OdedMaimon.
2008.
Negation recognition inmedical narrative reports.
InformationRetrieval Online, 11(6):499?538.Rubin, Victoria L. 2006.
Identifying Certaintyin Texts.
Ph.D. thesis, Syracuse University,Syracuse, NY.Rubin, Victoria L. 2007.
Stating withcertainty or stating with doubt:intercoder reliability results for manualannotation of epistemically modalizedstatements.
In NAACL ?07: HumanLanguage Technologies 2007: The Conferenceof the North American Chapter of theAssociation for Computational Linguistics;Companion Volume, Short Papers,pages 141?144, Morristown, NJ.Rubin, Victoria L., Elizabeth Liddy,and Noriko Kando.
2005.
Certaintyidentification in texts: Categorizationmodel and manual tagging results.
InComputing Attitude and Affect in Text:Theory and Applications, volume 20 ofInformation Retrieval Series.
Springer-Verlag, New York, pages 61?76.Ruppenhofer, Joseph, Caroline Sporleder,Roser Morante, Colin Baker, andMartha Palmer.
2010.
Semeval-2010 task10: Linking events and their participantsin discourse.
In Proceedings of the 5thInternational Workshop on SemanticEvaluation, pages 45?50, Uppsala.Salkie, Raphael, Pierre Busuttil, andJohan van der Auwera.
2009.
Introduction.Modality in English.
Theory and Description,Mouton de Gruyter, Berlin, pages 1?8.Sanchez-Graillet, Olivia and MassimoPoesio.
2007.
Negation of protein?proteininteractions: analysis and extraction.Bioinformatics, 23(13):424?432.Sarafraz, Farzaneh and Goran Nenadic.2010a.
Identification of negatedregulation events in the literature:Exploring the feature space.
In Proceedingsof the Symposium for Semantic Mining inBiomedicine (SMBM 2010), pages 137?141,Hinxton.Sarafraz, Farzaneh and Goran Nenadic.2010b.
Using SVMs with the commandrelation features to identify negated eventsin biomedical literature.
In Proceedings ofthe Workshop on Negation and Speculation inNatural Language Processing, pages 78?85,Uppsala.Saur?
?, Roser.
2008.
A Factuality Profiler forEventualities in Text.
Ph.D. thesis, BrandeisUniversity, Waltham, MA.Saur?
?, Roser and James Pustejovsky.
2009.FactBank: A corpus annotated withevent factuality.
Language Resources andEvaluation, 43(3):227?268.258Morante and Sporleder Modality and NegationSaur?
?, Roser, Mark Verhagen, and JamesPustejovsky.
2006a.
Annotating andrecognizing event modality in text.In Proceedings of FLAIRS 2006,pages 333?339, Melbourne.Saur?
?, Roser, Mark Verhagen, and JamesPustejovsky.
2006b.
SlinkET: A partialmodality parser for events.
In Proceedingsof LREC 2006, pages 1332?1337, Genoa.Seifert, Stephan and Werner Welte.
1987.A Basic Bibliography of Negation in NaturalLanguage.
Gu?nter Narr, Tu?bingen.Shatkay, Hagit, Fengxia Pan, AndreyRzhetsky, and W. John Wilbur.
2008.Multi-dimensional classification ofbiomedical texts: Toward automatedpractical provision of high-utilitytext to diverse users.
Bioinformatics,24(18):2086?2093.Skeppstedt, Maria.
2010.
Negation detectionin Swedish clinical text.
In Proceedingsof the NAACL HLT 2010 Second LouhiWorkshop on Text and Data Mining of HealthDocuments, pages 15?21, Los Angeles, CA.Snow, Rion, Lucy Vanderwende, and ArulMenezes.
2006.
Effectively using syntax forrecognizing false entailment.
In Proceedingsof the Main Conference on Human LanguageTechnology Conference of the North AmericanChapter of the Association of ComputationalLinguistics, pages 33?40, Morristown, NJ.Su, Qi, Chu-Ren Huang, and Helen Kai-yunChen.
2010.
Evidentiality for texttrustworthiness detection.
In Proceedings ofthe 2010 Workshop on NLP and Linguistics:Finding the Common Ground, pages 10?17,Uppsala.Szarvas, Gyo?rgy.
2008.
Hedge classificationin biomedical texts with a weaklysupervised selection of keywords.
InProceedings of ACL 2008, pages 281?289,Columbus, OH.Tang, Buzhou, Xiaolong Wang, Xuan Wang,Bo Yuan, and Shixi Fan.
2010.
A cascademethod for detecting hedges and theirscope in natural language text.
InProceedings of the Fourteenth Conferenceon Computational Natural LanguageLearning, pages 13?17, Uppsala.Thompson, Paul, Gilua Venturi, JohnMcNaught, Simonetta Montemagni, andSophia Ananiadou.
2008.
Categorisingmodality in biomedical texts.
In Proceedingsof the LREC 2008 Workshop on Building andEvaluating Resources for Biomedical TextMining 2008, pages 27?34, Marrakech.Tottie, Gunnel.
1991.
Negation in EnglishSpeech and Writing: A Study in Variation.Academic Press, New York.Umbach, Carla.
2004.
On the notion ofcontrast in information structure anddiscourse structure.
Journal of Semantics,21:155?175.Uzuner, O?zlem, Xiaoran Zhang, andTawanda Sibanda.
2009.
Machine learningand rule-based approaches to assertionclassification.
JAMIA, 16(1):109?115.van der Auwera, Johan and Vladimir A.Plungian.
1998.
Modality?s semantic map.Linguistic Typology, 2:79?124.van der Wouden, Ton.
1997.
NegativeContexts: Collocation, Polarity, and MultipleNegation.
Routledge, London.Velldal, Erik.
2011.
Predicting speculation:A simple disambiguation approachto hedge detection in biomedicalliterature.
Journal of Biomedical Semantics,2(Suppl 5):S7.Vincze, Veronika, Gyo?rgy Szarvas, RichardFarkas, Gyo?rgy Mo?ra, and Ja?nos Csirik.2008.
The BioScope corpus: biomedicaltexts annotated for uncertainty, negationand their scopes.
BMC Bioinformatics,9(Suppl 11):S9.Vincze, Veronika, Gyo?rgy Szarvas, Gyo?rgyMo?ra, Tomoko Ohta, and Richard Farkas.2010.
Linguistic scope-based andbiological event-based speculation andnegation annotations in the genia eventand BioScope corpora.
In Proceedingsof the Symposium for Semantic Mining inBiomedicine (SMBM 2010), pages 84?92,Hinxton.von Fintel, Kai.
2006.
Modality andlanguage.
In D. M. Borchert, editor,Encyclopedia of Philosophy.
MacMillanReference, Detroit, MI, second edition.Available at http://mit.edu/fintel/fintel-2006-modality.pdf.von Wright, Georg H. 1951.
An Essay inModal Logic.
North Holland, Amsterdam.Wiebe, Janyce.
1994.
Tracking point of viewin narrative.
Computational Linguistics,20(2):233?287.Wiebe, Janyce, Theresa Wilson, RebeccaBruce, Matthew Bell, and Melanie Martin.2004.
Learning subjective language.Computational Linguistics, 30(3):277?308.Wiebe, Janyce, Theresa Wilson, and ClaireCardie.
2005.
Annotating expressions ofopinions and emotions in language.Language Resources and Evaluation,38:165?210.Wiegand, Michael, Alexandra Balahur,Benjamin Roth, Dietrich Klakow, andAndre?s Montoyo.
2010.
A survey on therole of negation in sentiment analysis.
InProceedings of the Workshop on Negation and259Computational Linguistics Volume 38, Number 2Speculation in Natural Language Processing,pages 60?68, Uppsala.Wilbur, W. John, Andrey Rzhetsky, andHagit Shatkay.
2006.
New directions inbiomedical text annotations: Definitions,guidelines and corpus construction.BMC Bioinformatics, 7:356.Wilson, Theresa.
2008.
Fine-grainedSubjectivity and Sentiment Analysis:Recognizing the Intensity, Polarity, andAttitudes of Private States.
Ph.D. thesis,University of Pittsburgh, Pittsburgh, PA.Wilson, Theresa, Paul Hoffmann, SwapnaSomasundaran, Jason Kessler, JanyceWiebe, Yejin Choi, Claire Cardie, EllenRiloff, and Siddharth Patwardhan.
2005.OpinionFinder: A system for subjectivityanalysis.
In Proceedings of HLT/EMNLP onInteractive Demonstrations, pages 34?35,Morristown, NJ.Wilson, Theresa, Janyce Wiebe, and PaulHoffman.
2005.
Recognizing contextualpolarity in phrase-level sentimentanalysis.
In Proceedings of HLT-EMNLP,pages 347?354, Vancouver.Wilson, Theresa, Janyce Wiebe, and PaulHoffman.
2009.
Recognizing contextualpolarity: An exploration of features forphrase-level analysis.
ComputationalLinguistics, 35(3):399?433.Wilson, Theresa, Janyce Wiebe, and RebeccaHwa.
2006.
Recognizing strong and weakopinion clauses.
Computational Intelligence,22(2):73?99.Zhu, Qiaoming, Junhui Li, Hongling Wang,and Guodong Zhou.
2010.
A unifiedframework for scope learning viasimplified shallow semantic parsing.In Proceedings of EMNLP 2010,pages 714?724, Cambridge, MA.260
