Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1125?1134, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsExact Sampling and Decoding in High-Order Hidden Markov ModelsSimon Carter?ISLA, University of AmsterdamScience Park 904, 1098 XH Amsterdam,The Netherlandss.c.carter@uva.nlMarc Dymetman Guillaume BouchardXerox Research Centre Europe6, chemin de Maupertuis38240 Meylan, France{first.last}@xrce.xerox.comAbstractWe present a method for exact optimizationand sampling from high order Hidden MarkovModels (HMMs), which are generally han-dled by approximation techniques.
Motivatedby adaptive rejection sampling and heuris-tic search, we propose a strategy based onsequentially refining a lower-order languagemodel that is an upper bound on the truemodel we wish to decode and sample from.This allows us to build tractable variable-orderHMMs.
The ARPA format for language mod-els is extended to enable an efficient use of themax-backoff quantities required to computethe upper bound.
We evaluate our approachon two problems: a SMS-retrieval task and aPOS tagging experiment using 5-gram mod-els.
Results show that the same approach canbe used for exact optimization and sampling,while explicitly constructing only a fraction ofthe total implicit state-space.1 IntroductionIn NLP, sampling is important for many real tasks,such as: (i) diversity in language generation ormachine translation (proposing multiple alternativeswhich are not clustered around a single maximum);(ii) Bayes error minimization, for instance in Statis-tical Machine Translation (Kumar and Byrne, 2004);(iii) learning of parametric and non-parametricBayesian models (Teh, 2006).However, most practical sampling algorithms arebased on MCMC, i.e.
they are based on local moves?This work was conducted during an internship at XRCE.starting from an initial valid configuration.
Often,these algorithms are stuck in local minima, i.e.
ina basin of attraction close to the initialization, andthe method does not really sample the whole statespace.
This is a problem when there are ambiguitiesin the distribution we want to sample from: by hav-ing a local approach such as MCMC, we might onlyexplore states that are close to a given configuration.The necessity of exact sampling can be ques-tioned in practice.
Approximate sampling tech-niques have been developed over the last centuryand seem sufficient for most purposes.
However,the cases where one actually knows the quality ofa sampling algorithm are very rare, and it is com-mon practice to forget about the approximation andsimply treat the result of a sampler as a set of i.i.d.data.
Exact sampling provides a de-facto guaranteethat the samples are truly independent.
This is par-ticularly relevant when one uses a cascade of algo-rithms in complex NLP processing chains, as shownby (Finkel et al 2006) in their work on linguisticannotation pipelines.In this paper, we present an approach for exactdecoding and sampling with an HMM whose hid-den layer is a high-order language model (LM),which innovates on existing techniques in the fol-lowing ways.
First, it is a joint approach to sam-pling and optimization (i.e.
decoding), which isbased on introducing a simplified, ?optimistic?, ver-sion q(x) of the underlying language model p(x),for which it is tractable to use standard dynamic pro-gramming techniques both for sampling and opti-mization.
We then formulate the problem of sam-pling/optimization with the original model p(x) in1125terms of a novel algorithm which can be viewedas a form of adaptive rejection sampling (Gilksand Wild, 1992; Gorur and Teh, 2008), in whicha low acceptance rate (in sampling) or a low ratiop(x?)/q(x?)
(in optimization, with x?
the argmaxof q) leads to a refinement of q, i.e., a slightly morecomplex and less optimistic q but with a higher ac-ceptance rate or ratio.Second, it is the first technique that we are awareof which is able to perform exact samplingwith suchmodels.
Known techniques for sampling in suchsituations have to rely on approximation techniquessuch as Gibbs or Beam sampling (see e.g.
(Teh etal., 2006; Van Gael et al 2008)).
By contrast, ourtechnique produces exact samples from the start, al-though in principle, the first sample may be obtainedonly after a long series of rejections (and thereforerefinements).
In practice, our experiments indicatethat a good acceptance rate is obtained after a rel-atively small number of refinements.
It should benoted that, in the case of exact optimization, a sim-ilar technique to ours has been proposed in an im-age processing context (Kam and Kopec, 1996), butwithout any connection to sampling.
That paper,written in the context of image processing, appearsto be little known in the NLP community.Overall, our method is of particular interest be-cause it allows for exact decoding and samplingfrom HMMs where the applications of existing dy-namic programming algorithms such as Viterbi de-coding (Rabiner, 1989) or Forward-Backward sam-pling (Scott, 2002) are not feasible, due to a largestate space.In Section 2, we present our approach anddescribe our joint algorithm for HMM sam-pling/optimization, giving details about our exten-sion of the ARPA format and refinement proce-dure.
In Section 3 we define our two experimentaltasks, SMS-retrieval and POS tagging, for which wepresent the results of our joint algorithm.
We finallydiscuss perspectives and conclude in Section 4.2 Adaptive rejection sampling andheuristic search for high-order HMMsNotation Let x = {x1, x2, ...x?}
be a given hid-den state sequence (e.g.
each xi is an English word)which takes values in X = {1, ?
?
?
, N}?
where ?is the length of the sequence and N is the numberof latent symbols.
Subsequences (xa, xa+1, ?
?
?
, xb)are denoted by xba, where 1 ?
a ?
b ?
?.
Leto = {o1, o2, ...o?}
be the set of observations asso-ciated to these words (e.g.
oi is an acoustic realiza-tion of xi).
The notations p, q and q?
refer to un-normalized densities, i.e.
non-negative measures onX .
Since only discrete spaces are considered, weuse for short p(x) = p({x}).
When the contextis not ambiguous, sampling according to p meanssampling according to the distribution with densityp?
(x) = p(x)p(X ) , where p(X ) =?X p(x)dx is the totalmass of the unnormalized distribution p.Sampling The objective is to sample a se-quence with density p?
(x) proportional to p(x) =plm(x)pobs(o|x) where plm is the probability of thesequence x under a n-gram model and pobs(o|x)is the probability of observing the noisy sequenceo given that the correct/latent sequence is x. As-suming the observations depend only on the currentstate, this probability becomesp(x) =?
?i=1plm(xi|xi?1i?n+1)pobs(oi|xi) .
(1)To find the most likely sequence given an ob-servation, or to sample sequences from Equa-tion 1, standard dynamic programming techniquesare used (Rabiner, 1989; Scott, 2002) by expand-ing the state space at each position.
However, asthe transition order n increases, or the number of la-tent tokens N that can emit to each observation olincreases, the dynamic programming approach be-comes intractable, as the number of operations in-creases exponentially in the order of O(?Nn).If one can find a proposal distribution q which isan upper bound of p ?
i.e such that q(x) ?
p(x) forall sequences x ?
X ?
and which it is easy to sam-ple from, the standard rejection sampling algorithmcan be used:1.
Sample x ?
q/q(X ), with q(X ) =?X q(x)dx;2.
Accept x with probability p(x)/q(x), other-wise reject x;To obtain multiple samples, the algorithm is re-peated several times.
However, for simple bounds,1126!
""#$%&'(% )(*%!"
#!$ % !"
%$& %&+,% -#./,)%!"
#'( % !"
)*+,(% %$% 0% 1% 2% 3%)(*4%!"
%$&- %(a)!
""#$%$% &%'()%*)+%*)+%!"
#!$ % !% &$'(#!$ %!"
&$' % #,-./*%'0/%*)+1% !"
&$') %*)+1%!"
&$') %2% 3%4%5%!"
#*+ % !"
,-./+& %(b)Figure 1: An example of an initial q-automaton (a), and the refined q-automaton (b) Each state correspondsto a context (only state 6 has a non-empty context) and each edge represents the emission of a symbol.Thick edges are representing the path for the sampling/decoding of two dog(s) barked, thin edgescorresponding to alternative symbols.
By construction, w1(dog) ?
w2(dog|two) so that the total weightof (b) is smaller than the total weight of (a).the average acceptance rate ?
which is equal top(X )/q(X ) ?
can be so large that rejection sam-pling is not practical.
In adaptive rejection sampling(ARS), the initial bound q is incrementally improvedbased on the values of the rejected elements.
Whileoften based on log-concave distributions which areeasy to bound, ARS is valid for any type of bound,and in particular can be applied to the upper boundson n-gram models introduced by (Kam and Kopec,1996) in the context of optimization.
When a sam-ple is rejected, our algorithm assumes that a smallset of refined proposals is available, say q?1, ?
?
?
, q?m,where m is a small integer value.
These refinementsare improved versions of the current proposal q inthe sense that they still upper-bound the target dis-tribution p, but their mass is strictly smaller than themass of q, i.e.
q?
(X ) < q(X ).
Thus, each such re-finement q?, while still being optimistic relative tothe target distribution p, has higher average accep-tance rate than the previous upper bound q.
A boundon the n-gram LM will be presented in Section 2.1.Optimization In the case of optimization, the ob-jective is to find the sequence maximizing p(x).Viterbi on high-order HMMs is intractable but wehave access to an upper bound q, for which Viterbiis tractable.
Sampling from q is then replaced byfinding the maximum point x of q, looking at the ra-tio r(x) = p(x)/q(x), and accepting x if this ratio isequal to 1, otherwise refining q into q?
exactly as inthe sampling case.
This technique is able to find theexact maximum of p, similarly to standard heuristicsearch algorithms based on optimistic bounds.
Westop the process when q and p agree at the valuemaximizing q which implies that we have found theglobal maximum.2.1 Upper bounds for n-gram modelsTo apply ARS on the target density given byEquation 1 we need to define a random se-quence of proposal distributions {q(t)}?t=1 such thatq(t)(x) ?
p(x), ?x ?
X , ?t ?
{0, 1, ?
?
?
}.Each n-gram xi?n+1, ..., xi in the hidden layer con-tributes an n-th order factor wn(xi|xi?1i?n+1) ?plm(xi|xi?1i?n+1)pobs(oi|xi).
The key idea is thatthese n-th order factors can be upper bounded byfactors of order n?
k by maximizing over the head(i.e.
prefix) of the context, as if part of the con-text was ?forgotten?.
Formally, we define the max-backoff weights as:wn?k(xi|xi?1i?n+1+k) ?
maxxi?n+ki?n+1wn(xi|xi?1i?n+1),(2)By construction, the max-backoff weights wn?k arefactors of order n?
k and can be used as surrogatesto the original n-th order factors of Equation (1),leading to a nested sequence of upper bounds untilreaching binary or unary factors:p(x) = ?
?i=1wn(xi|xi?1i?n+1) (3)?
?
?i=1wn?1(xi|xi?1i?n+2) (4)?
?
??
?
?i=1w2(xi|xi?1) (5)?
?
?i=1w1(xi) := q(0)(x) .
(6)Now, one can see that the loosest bound (6) basedon unigrams corresponds to a completely factorizeddistribution which is straightforward to sample andoptimize.
The bigram bound (5) corresponds to astandard HMM probability that can be efficiently de-coded (using Viterbi algorithm) and sampled (usingbackward filtering-forward sampling).
1 In the con-text of ARS, our initial proposal q(0)(x) is set to1Backward filtering-forward sampling (Scott, 2002) refersto the process of running the Forward algorithm (Rabiner,1127the unigram bound (6).
The bound is then incre-mentally improved by adaptively refining the max-backoff weights based on the values of the rejectedsamples.
Here, a refinement refers to the increaseof the order of some of the max-backoff weights inthe current proposal (thus most refinements consistof n-grams with heterogeneous max-backoff orders,not only those shown in equations (3)-(6)).
Thisoperation tends to tighten the bound and thereforeincrease the acceptance probability of the rejectionsampler, at the price of a higher sampling complex-ity.
The are several possible ways of choosing theweights to refine; in Section 2.2 different refinementstrategies will be discussed, but the main technicaldifficulty remains in the efficient exact optimizationand sampling of a HMM with n-grams of variableorders.
The construction of the refinement sequence{q(t)}t?0 can be easily explained and implementedthrough aWeighted Finite State Automaton (WFSA)referred as a q-automaton, as illustrated in the fol-lowing example.Example We give now a high-level description ofthe refinement process to give a better intuition ofour method.
In Fig.
1(a), we show a WFSA rep-resenting the initial proposal q(0) corresponding toan example with an acoustic realization of the se-quence of words (the, two, dogs, barked).
Theweights on edges of this q-automaton correspond tothe unigram max-backoffs, so that the total weightcorresponds to Equation (6).
Considering sampling,we suppose that the first sample from q(0) producesx1 = (the, two, dog, barked), markedwith bold edges in the drawing.
Now, computing theratio p(x1)/q(0)(x1) gives a result much below 1,because from the viewpoint of the full model p, thetrigram (the two dog) is very unlikely; in otherwords the ratiow3(dog|the two)/w1(dog) (and,in fact, already the ratio w2(dog|two)/w1(dog))is very low.
Thus, with high probability, x1 is re-jected.
When this is the case, we produce a re-fined proposal q(1), represented by the WFSA inFig.
1(b), which takes into account the more real-1989), which creates a lattice of forward probabilities that con-tains the probability of ending in a latent state at a specific timet, given the subsequence of previous observations ot1, for all theprevious latent sub-sequences xt?11 , and then recursively mov-ing backwards, sampling a latent state based on these probabil-ities.Algorithm 1 ARS for HMM algorithm.1: while not Stop(h) do2: if Optimisation then3: Viterbi x ?
q4: else5: Sample x ?
q6: r ?
p(x)/q(x)7: Accept-or-Reject(x, r)8: Update(h, x)9: if Rejected(x) then10: for all i ?
{2, ?
?
?
, ?}
do11: q ?
UpdateHMM (q, x, i)12: return q along with accepted x?s in hAlgorithm 2 UpdateHMMInput: A triplet (q, x, i) where q is a WFSA, x is a se-quence determining a unique path in the WFSA andi is a position at which a refinement must be done.1: n :=ORDERi(xi1) + 1 #implies xi?1i?n+2 ?
Si?12: if xi?1i?n+1 /?
Si?1 then3: CREATE-STATE(xi?1i?n+1, i?
1)4: #move incoming edges, keeping WFSA determin-istic5: for all s ?
SUFi?2(xi?2i?n+1) do6: e := EDGE(s, xi?1)7: MOVE-EDGE-END(e,xi?1i?n+1)8: #create outgoing edges9: for all (s, l,?)
?
Ti(xi?1i?n+2) do10: CREATE-EDGE(xi?1i?n+1,s,l,?
)11: #update weights12: for all s ?
SUFi?1(xi?1i?n+1) do13: weight of EDGE(s, xi) := wn(xi|xi?1i?n+1)14: returnistic weight w2(dog|two) by adding a node (node6) for the context two.
We then perform a samplingtrial with q(1), which this time tends to avoid produc-ing dog in the context of two; if the new sampleis rejected, the refinement process continues untilwe start observing that the acceptance rate reachesa fixed threshold value.
The case of optimization issimilar.
Suppose that with q(0) the maximum is x1,then we observe that p(x1) is lower than q(0)(x1),reject suboptimal x1 and refine q(0) into q(1).2.2 AlgorithmWe describe in detail the algorithm and procedurefor updating a q-automaton with a max-backoff oflonger context.Algorithm 1 gives the pseudo-code of the sam-1128pling/optimization strategy.
On line 1, h representsthe history of all trials so far, where the stopping cri-terion for decoding is whether the last trial in thehistory has been accepted, and for sampling whetherthe ratio of accepted trials relative to all trials ex-ceeds a certain threshold.
The WFSA is initial-ized so that all transitions only take into accountthe w1(xi) max-backoffs, i.e.
the initial optimistic-bound ignores all contexts.
Then depending onwhether we are sampling or decoding, in lines 2-5,we draw an event from our automaton using eitherthe Viterbi algorithm or Forward-Backward sam-pling.
If the sequence is rejected at line 7, then theq-automaton is updated in lines 10 and 11.
This isdone by expanding all the factors involved in thesampling/decoding of the rejected sequence x to ahigher order.
That is, while sampling or decodingthe automaton using the current proposal q(t), thecontexts used in the path of the rejected sequenceare replaced with higher order contexts in the newrefined proposal qt+1(x).The update process of the q-automaton repre-sented as a WFSA is described in Algorithm 2.
Thisprocedure guarantees that a lower, more realisticweight is used in all paths containing the n-gramxii?n+1 while decoding/sampling the q-automaton,where n is the order at which xii?n+1 has been ex-panded so far.
The algorithm takes as input a max-backoff function, and refines the WFSA such thatany paths that include this n-gram have a smallerweight thanks to the fact that higher-order max-backoff have automatically smaller weights.The algorithm requires the following functions:?
ORDERi(x) returns the order at which the n-gram has been expanded so far at position i.?
Si returns the states at a position i.?
Ti(s) returns end states, labels and weights ofall edges that originate from this state.?
SUFi(x) returns the states at iwhich have a suf-fix matching the given context x.
For emptycontexts, all states at i are returned.?
EDGE(s, l) returns the edge which originatesfrom s and has label l. Deterministic WFSA,such as those used here, can only have a singletransition with a label l leaving from a state s.?
CREATE-STATE(s, i) creates a statewith name s at position i, CREATE-EDGE(s1, s2, l,?)
creates an edge (s1, s2)between s1 and s2 with weight ?
and labell, and MOVE-EDGE-END(e, s) sets the endof edge e to be the state s, keeping the samestarting state, weight and label.At line 1, the expansion of the current n-gram isincreased by one so that we only need to expand con-texts of size n ?
2.
Line 2 checks whether the con-text state exists.
If it doesn?t it is created at lines 3-10.
Finally, the weight of the edges that could be in-volved in the decoding of this n-gram are updated toa smaller value given by a higher-order max-backoffweight.The creation of a new state in lines 3-10 isstraightforward: At lines 5-7, incoming edges aremoved from states at position i ?
2 with a match-ing context to the newly created edge.
At lines 9-10 edges heading out of the context state are cre-ated.
They are simply copied over from all edgesthat originate from the suffix of the context state, aswe know these will be legitimate transitions (i.e wewill always transition to a state of the same order orlower).Note that we can derive many other variants ofAlgorithm 2 which also guarantee a smaller totalweight for the q-automaton.
We chose to present thisversion because it is relatively simple to implement,and numerical experiments comparing different re-finement approaches (including replacing the max-backoffs with the highest-possible context, or pick-ing a single ?culprit?
to refine) showed that this ap-proach gives a good trade-off between model com-plexity and running time.2.3 Computing Max-Backoff FactorsAn interesting property of the max-backoff weightsis that they can be computed recursively; taking a3-gram LM as an example, we have:w1(xi) = maxxi?1w2(xi|xi?1)w2(xi|xi?1) = maxxi?2w3(xi|xi?1i?2)w3(xi|xi?1i?2) = p(xi|xi?1i?2) p(oi|xi).The final w3(xi|xi?1i?2) upper bound function is sim-ply equal to the true probability (multiplied by the1129conditional probability of the observation), as anyextra context is discarded by the 3-gram languagemodel.
It?s easy to see that as we refine q(t) byreplacing existing max-backoff weights with morespecific contexts, the q(t) tends to p at t tends to in-finity.In the HMM formulation, we need to be ableto efficiently compute at run-time the max-backoffsw1(the), w2(dog|the), ?
?
?
, taking into accountsmoothing.
To do so, we present a novel method forconverting language models in the standard ARPAformat used by common toolkits such as (Stolcke,2002) into a format that we can use.
The ARPA fileformat is a table T composed of three columns: (1)an n-gram which has been observed in the trainingcorpus, (2) the log of the conditional probability ofthe last word in the n-gram given the previous words(log f(.
)), and (3) a backoff weight (bow(.))
usedwhen unseen n?grams ?backoff?
to this n-gram.
2The probability of any n-gram xii?n (in the pre-vious sense, i.e.
writing p(xii?n) for p(xi|xi?1i?n)) isthen computed recursively as:p(xii?n) =?f(xii?n) if xii?n ?
Tbow(xi?1i?n) p(xii?n+1) otherwise.
(7)Here, it is understood that if xi?1i?n is in T , then itsbow(.)
is read from the table, otherwise it is taken tobe 1.Different smoothing techniques will lead to dif-ferent calculations of f(xii?n) and bow(xi?1i?n), how-ever both backoff and linear-interpolation methodscan be formulated using the above equation.Starting from the ARPA format, we pre-computea new table MAX-ARPA, which has the same linesas ARPA, each corresponding to an n-gram xii?n ob-served in the corpus, and the same f and bow, butwith two additional columns: (4) a max log prob-ability (log mf(xii?n)), which is equal to the maxi-mum log probability over all the n-grams extendingthe context of xii?n, i.e.
which have xii?n as a suffix;(5) a ?max backoff?
weight (mbow(xii?n)), which isa number used for computing the max log probabil-ity of an n-gram not listed in the table.
From theMAX-ARPA table, the max probability w of any n-2See www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html, last accessed at1/3/2012, for further details.gram xii?n, i.e the maximum of p(xii?n?k) over alln-grams extending the context of xii?n, can then becomputed recursively (again very quickly) as:w(xii?n) =?mf(xii?n) if xii?n ?
Tmbow(xi?1i?n) p(xii?n) otherwise.
(8)Here, if xi?1i?n is in T , then its mbow(.)
is readfrom the table, otherwise it is taken to be 1.
Alsonote that the procedure calls p, which is computedas described in Equation 7.
33 ExperimentsIn this section we empirically evaluate our joint, ex-act decoder and sampler on two tasks; SMS-retrieval(Section 3.1), and supervised POS tagging (Sec-tion 3.2).3.1 SMS-RetrievalWe evaluate our approach on an SMS-message re-trieval task.
A latent variable x ?
{1, ?
?
?
, N}?represents a sentence represented as a sequence ofwords: N is the number of possible words in thevocabulary and ?
is the number of words in thesentence.
Each word is converted into a sequenceof numbers based on a mobile phone numeric key-pad.
The standard character-to-numeric functionnum : {a,b, ?
?
?
,z, ., ?
?
?
, ?}?
{1, 2, ?
?
?
, 9, 0} isused.
For example, the words dog and fogare represented by the sequence (3, 6, 4) becausenum(d)=num(f)=3, num(o)=6 and num(g)=4.Hence, observed sequences are sequences of nu-meric strings separated by white spaces.
To takeinto account typing errors, we assume we observea noisy version of the correct numeric sequence(num(xi1), ?
?
?
, num(xi|xi|) that encodes the wordxi at the i-th position of the sentence x.
The noisemodel is:p(oi|xi) ?|xi|?t=11k ?
d(oit, num(xit)) + 1, (9)where d(a, b) is the physical distance between thenumeric keys a and b and k is a user provided con-3In this discussion of theMAX-ARPA table we have ignoredthe contribution of the observation p(oi|xi), which is a constantfactor over the different max-backoffs for the same xi and doesnot impact the computation of the table.11300 10 20 30 4050 60 70 80 901001  2  3  4  5  6  7  8  9  10avg#iterationsinput length3 4 5  0 50100 150 200 250 300350 400 450 5001  2  3  4  5  6  7  8  9 10avg#statesinput length3 4 5Figure 2: On the left we report the average #of iterations taken to decode given different LMsover input sentences of different lengths, and on theright we show the average # of states in the final q-automaton once decoding is completed.stant that controls the ambiguity in the distribution;we use 64 to obtain moderately noisy sequences.We used the English side of the Europarl cor-pus (Koehn, 2005).
The language model was trainedusing SRILM (Stolcke, 2002) on 90% of the sen-tences.
On the remaining 10%, we randomly se-lected 100 sequences for lengths 1 to 10 to obtain1000 sequences from which we removed the onescontaining numbers, obtaining a test set of size 926.Decoding Algorithm 1 was run in the optimizationmode.
In the left plot of Fig.
2, we show the numberof iterations (running Viterbi then updating q) thatthe different n-gram models of size 3, 4 and 5 taketo do exact decoding of the test-set.
For a fixed sen-tence length, we can see that decoding with largern-gram models leads to a sub-linear increase w.r.t.n in the number of iterations taken.
In the right plotof Fig.
2, we show the average number of states inour variable-order HMMs.To demonstrate the reduced nature of our q-automaton, we show in Tab.
1 the distribution ofn-grams in our final model for a specific input sen-tence of length 10.
The number of n-grams in thefull model is?3.0?1015.
Exact decoding here is nottractable using existing techniques.
Our HMM hasonly 9008 n-grams in total, including 118 5-grams.n: 1 2 3 4 5q: 7868 615 231 176 118Table 1: # of n-grams in our variable-order HMM.Finally, we show in Tab.
2 an example run ofour algorithm in the optimization setting for a giveninput.
Note that the weight according to our q-automaton for the first path returned by the Viterbialgorithm is high in comparison to the true log prob-ability according to p.Sampling For the sampling experiments, we limitthe number of latent tokens to 100.
We refine our q-automaton until we reach a certain fixed cumulativeacceptance rate (AR).
We also compute a rate basedonly on the last 100 trials (AR-100), as this tends tobetter reflect the current acceptance rate.In Fig.
3a, we plot a running average of the ratioat each iteration over the last 10 trials, for a singlesampling run using a 5-gram model for an exampleinput.
The ratios start off at 10?20, but gradually in-crease as we refine our HMM.
After ?
500 trials,we start accepting samples from p. In Fig.
3b, weshow the respective ARs (bottom and top curves re-spectively), and the cumulative # of accepts (middlecurve), for the same input.
Because the cumulativeaccept ratio takes into account all trials, the final ARof 17.7% is an underestimate of the true accept ra-tio at the final iteration; this final accept ratio can bebetter estimated on the basis of the last 100 trials, forwhich we read AR-100 to be at around 60%.We note that there is a trade-off between the timeneeded to construct the forward probability latticeneeded for sampling, and the time it takes to adaptthe variable-order HMM.
To resolve this, we pro-pose to use batch-updates: making B trials from thesame q-automaton, and then updating our model inone step.
By doing this, we noted significant speed-ups in sampling times.
In Tab.
3, we show variousinput: 3637 843 66639 39478 *oracle: does the money exist ?best: does the money exist .Viterbi paths log q(x) log p(x)q1 does the money exist ) -0.11 -17.42q50 does the owned exist .
-11.71 -23.54q100 ends the money exist .
-12.76 -17.09q150 does vis money exist .
-13.45 -23.74q170 does the money exist .
-13.70 -13.70Table 2: Viterbi paths given different qt.
Here, forthe given input, it took 170 iterations to find the bestsequence according to p, so we only show every 50thpath.11311e-20 1e-18 1e-16 1e-14 1e-121e-10 1e-08 1e-06 0.0001 0.0110  500 1000 1500 2000ratioiterations(a)0 10 2030 40 50600  500  1000 1500 2000  0 100200 300 400500 600acceptratio%#acceptsiterations#ACCARAR 100(b)100 200 300400 500 600700 8001  2  3  4  5  6  7  8  9  10avg#iterationsinput length543(c)0 200 400 600800 1000 1200 14001600 18001  2  3  4  5  6  7  8  9 10avg#statesinput length543(d)Figure 3: In 3a, we plot the running average over the last 10 trials of the ratio.
In 3b, we plot the cumulative# of accepts (middle curve), the accept rate (bottom curve), and the accept rate based on the last 100samples (top curve).
In 3c, we plot the average number of iterations needed to sample up to an AR of 20%for sentences of different lengths in our test set, and in 3d, we show the average number of states in ourHMMs for the same experiment.B: 1 10 20 30 40 50 100time: 97.5 19.9 15.0 13.9 12.8 12.5 11.4iter: 453 456 480 516 536 568 700Table 3: In this table we show the average amount oftime in seconds and the average number of iterations(iter) taken to sample sentences of length 10 givendifferent values of B.statistics for sampling up to AR-100 = 20 given dif-ferent values for B.
We ran this experiment usingthe set of sentences of length 10.
A value of 1 meansthat we refine our automaton after each rejected trial,a value of 10 means we wait until rejecting 10 trialsbefore updating our automaton in one step.
We cansee that while higher values of B lead to more iter-ations, as we do not need to re-compute the forwardtrellis needed for sampling, the time needed to reachthe specific AR threshold actually decreases, from97.5 seconds to 11.4 seconds, an 8.5% speedup.
Un-less explicitly stated otherwise, further experimentsuse a B = 100.We now present the full sampling results on ourtest-set in Fig.
3c and 3d, where we show the aver-age number of iterations and states in the final mod-els once refinements are finished (AR-100=20%) fordifferent orders n over different lengths.
We notea sub-linear increase in the average number of tri-als and states when moving to higher n; thus, forlength=10, and for n = 3, 4, 5, # trials: 3-658.16,4-683.3, 5-700.9, and # states: 3-1139.5, 4-1494.0,5-1718.3.Finally, we show in Tab.
4, the ranked samplesdrawn from an input sentence, according to a 5-gramLM.
After refining our model up to AR-100 = 20%,input: 3637 843 66639 39478 *oracle: does the money exist ?best: does the money exist .samples # log q(x) log p(x)does the money exist .
429 -13.70 -13.70does the money exist ?
211 -14.51 -14.51does the money exist !
72 -15.49 -15.49does the moody exist .
45 -15.70 -15.70does the money exist : 25 -16.73 -16.73Table 4: Top-5 ranked samples for an example in-put.
We highlight in bold the words which are differ-ent to the Viterbi best of the model.
The oracle andbest are not the same for this input.we continued drawing samples until we had 1000exact samples from p (out of ?
4.7k trials).
Weshow the count of each sequence in the 1000 sam-ples, and the log probability according to p for thatevent.
We only present the top-five samples, thoughin total there were 90 unique sequences sampled, 50of which were only sampled once.3.2 POS-taggingOur HMM is the same as that used in (Brants, 2001);the emission probability of a word given a POStag xi is calculated using maximum likelihood tech-niques.
That is, p(oi|xi) =c(oi,xi)c(xi).
Unseen wordsare handled by interpolating longer suffixes withshorter, more general suffixes.
To train our languagemodel, we use the SRILM toolkit (Stolcke, 2002)We build LMs of up to size 9.
We present resultson the WSJ Penn Treebank corpus (Marcus et al1993).
We use sections 0-18 to train our emissionand transitions probabilities, and report results on113295.6 95.65 95.795.75 95.8 95.8595.9 95.953  4  5  6  7  8  9accuracy%n-gram order(a)0 2000 4000 60008000 10000 12000 1400016000 180003  4  5  6  7  8  9timen-gram orderARSF(b)50 60 70 8090 100 110 1201303  4  5  6  7  8  9avg#iterationsn-gram order(c)100 200 300 400500 600 700 8009003  4  5  6  7  8  9avg#statesn-gram order(d)Figure 4: In 4a, we report the accuracy results given different n-gram models on the WSJ test-set.
In 4b, weshow the time taken (seconds) to decode the WSJ test-set given our method (ARS), and compare this to thefull model (F).
In 4c, the average number of iterations needed to sample the test-set given different n-gramlanguage models is given, and 4d shows the average number of states in the variable-order HMMs.sections 22-24.We first present results for our decoding experi-ments.
In Fig.
4a we show the accuracy results ofour different models on the WSJ test-set.
We seethat the best result is achieved with the 5-gram LMgiving an accuracy of 95.94%.
After that, resultsstart to drop, most likely due to over-fitting of theLM during training and an inability for the smooth-ing technique to correctly handle this.In Fig.
4b, we compare the time it takes in secondsto decode the test-set with the full model at each n-gram size; that is a WFSA with all context statesand weights representing the true language modellog probabilities.
We can see that while increas-ing the n-gram model size, our method (ARS) ex-hibits a linear increase in decoding time, in contrastto the exponential factor exhibited when running theViterbi algorithm over the full WFSA (F).
Note forn-gram models of order 7 and higher, we could notdecode the entire test set as creating the full WFSAwas taking too long.Finally in both Figs 4c and 4d, we show the aver-age number of iterations taken to sample from theentire test-test, and the average number of statesin our variable-order HMMs, with AR-100=60%.Again we note a linear increase in both Fig., in con-trast to the exponential nature of standard techniquesapplied to the full HMM.4 Conclusion and PerspectivesWe have presented a dual-purpose algorithm that canbe used for performing exact decoding and samplingon high-order HMMs.
We demonstrated the valid-ity of our method on SMS-retrieval and POS exam-ples, showing that the ?proposals?
that we obtain re-quire only a fraction of the space that would resultfrom explicitly representing the HMM.
We believethat this ability to support exact inference (both ap-proximation and sampling) at a reasonable cost hasimportant applications, in particular when movingfrom inference to learning tasks, which we see as anatural extension of this work.By proposing a common framework for samplingand optimization our approach has the advantagethat we do not need separate skills or expertise tosolve the two problems.
In several situations, wemight be interested not only in the most probable se-quence, but also in the distribution of the sequences,especially when diversity is important or in the pres-ence of underlying ambiguities.The interplay between optimization and samplingis a fruitful area of research that can lead to state-of-the art performances on inference and decod-ing tasks in the special case of high-order HMMdecoding, but the method is generic enough tobe generalized to many others models of interestfor NLP applications.
One family of models isprovided by agreement-based models, for exampleHMM+PCFG, where distribution p takes the formof a product: p(x) = pHMM(x)pPCFG(x).
Evenif the factors pHMM(x) and pPCFG(x) can be de-coded and sampled efficiently, the product of themis intractable.
Dual decomposition is a genericmethod that has been proposed for handling decod-ing (i.e.
optimization) with such models, by decou-pling the problem into two alternating steps that caneach be handled by dynamic programming or otherpolynomial-time algorithms (Rush et al 2010), anapproach that has been applied to Statistical Ma-chine Translation (phrase-based (Chang and Collins,11332011) and hierarchical (Rush and Collins, 2011))among others.
However, sampling such distributionsremains a difficult problem.
We are currently ex-tending the approach described in this paper to han-dle such applications.
Again, using ARS on a se-quence of upper bounds to the target distribution,our idea is to express one of the two models as a con-text free grammar and incrementally compute theintersection with the second model, maintaining agood trade-off between computational tractability ofthe refinement and a reasonable acceptance rate.ReferencesThorsten Brants.
2001.
Tnt - a statistical part-of-speechtagger.
In Proceedings of the Sixth conference ofApplied Natural Language Processing (ANLP 2001),pages 224?231.Yin-Wen Chang and Michael Collins.
2011.
Exact de-coding of phrase-based translation models through la-grangian relaxation.
In Proceedings of the Conferenceon Empirical Methods for Natural Language Process-ing (EMNLP 2011).Jenny Rose Finkel, Christopher D. Manning, and An-drew Y. Ng.
2006.
Solving the problem of cascadingerrors: approximate bayesian inference for linguisticannotation pipelines.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP 2006), pages 618?626.W.
R. Gilks and P. Wild.
1992.
Adaptive rejec-tion sampling for gibbs sampling.
Applied Statistics,42(2):337?348.Dilan Gorur and Yee Whye Teh.
2008.
Concave convexadaptive rejection sampling.
Technical report, GatsbyComputational Neuroscience Unit.Anthony C. Kam and Gary E. Kopec.
1996.
Documentimage decoding by heuristic search.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,18:945?950.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of Ma-chine Translation Summit (MT-Summit 2005), pages79?86.Shankar Kumar and William Byrne.
2004.
Minimumbayes risk decoding for statistical machine translation.In Joint Conference of Human Language Technologiesand the North American chapter of the Association forComputational Linguistics (HLT-NAACL 2004).Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: the penn treebank.
Computional Lin-guistics, 19:313?330.Lawrence R. Rabiner.
1989.
A tutorial on hidden markovmodels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?286, Febru-ary.Alexander M. Rush and Michael Collins.
2011.
Exactdecoding of syntactic translation models through la-grangian relaxation.
In Proceedings of the Conferenceon Empirical Methods for Natural Language Process-ing (EMNLP 2011), pages 26?37.Alexander M. Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Proceedings of the Conference onEmpirical Methods for Natural Language Processing(EMNLP 2010).Steven L. Scott.
2002.
Bayesian methods for hiddenmarkov models: Recursive computing in the 21st cen-tury.
Journal of the American Statistical Association,97:337?351.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference of Spoken Language Processing (INTER-SPEECH 2002), pages 257?286.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Yee Whye Teh.
2006.
A hierarchical bayesian languagemodel based on pitman-yor processes.
In Proceedingsof the 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the As-sociation for Computational Linguistics (ACL 2006),pages 985?992.Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, andZoubin Ghahramani.
2008.
Beam sampling for the in-finite hidden Markov model.
In Proceedings of the In-ternational Conference on Machine Learning (ICML2008), volume 25.1134
