Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1017?1026, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsReading The Web with Learned Syntactic-Semantic Inference RulesNi Lao1?, Amarnag Subramanya2, Fernando Pereira2, William W. Cohen11Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA2Google Research, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USAnlao@cs.cmu.edu, {asubram, pereira}@google.com, wcohen@cs.cmu.eduAbstractWe study how to extend a large knowledgebase (Freebase) by reading relational informa-tion from a large Web text corpus.
Previousstudies on extracting relational knowledgefrom text show the potential of syntacticpatterns for extraction, but they do not exploitbackground knowledge of other relationsin the knowledge base.
We describe adistributed, Web-scale implementation of apath-constrained random walk model thatlearns syntactic-semantic inference rules forbinary relations from a graph representationof the parsed text and the knowledge base.Experiments show significant accuracy im-provements in binary relation prediction overmethods that consider only text, or only theexisting knowledge base.1 IntroductionManually-created knowledge bases (KBs) often lackbasic information about some entities and theirrelationships, either because the information wasmissing in the initial sources used to create theKB, or because human curators were not confidentabout the status of some putative fact, and so theyexcluded it from the KB.
For instance, as we willsee in more detail later, many person entries inFreebase (Bollacker et al 2008) lack nationalityinformation.
To fill those KB gaps, we might usegeneral rules, ideally automatically learned, such as?if person was born in town and town is in country?This research was carried out during an internship atGoogle Researchthen the person is a national of the country.?
Ofcourse, rules like this may be defeasible, in this casefor example because of naturalization or politicalchanges.
Nevertheless, many such imperfect rulescan be learned and combined to yield useful KBcompletions, as demonstrated in particular with thePath-Ranking Algorithm (PRA) (Lao and Cohen,2010; Lao et al 2011), which learns such rules onheterogenous graphs for link prediction tasks.Alternatively, we may attempt to fill KB gaps byapplying relation extraction rules to free text.
Forinstance, Snow et al(2005) and Suchanek et al(2006) showed the value of syntactic patterns inextracting specific relations.
In those approaches,KB tuples of the relation to be extracted serve aspositive training examples to the extraction ruleinduction algorithm.
However, the KB containsmuch more knowledge about other relations thatcould potentially be helpful in improving relationextraction accuracy and coverage, but that is notused in such purely text-based approaches.In this work, we use PRA to learn weightedrules (represented as graph path patterns) thatcombine both semantic (KB) and syntactic infor-mation encoded respectively as edges in a graph-structured KB, and as syntactic dependency edgesin dependency-parsed Web text.
Our approach caneasily incorporate existing knowledge in extractiontasks, and its distributed implementation scales tothe whole of the Freebase KB and 60 million parseddocuments.
To the best of our knowledge, this is thefirst successful attempt to apply relational learningmethods to heterogeneous data with this scale.10171.1 Terminology and NotationIn this study, we use a simplified KB consisting of aset C of concepts and a set R of labels.
Each label rdenotes some binary relation partially represented inthe KB.
The concrete KB is a directed, edge-labeledgraph G = (C, T ) where T ?
C ?
R ?
C is theset of labeled edges (also known as triples) (c, r, c?
).Each triple represents an instance r(c, c?)
of therelation r ?
R. The KB may be incomplete, thatis, r(c, c?)
holds in the real world but (c, r, c?)
6?
T .Our method will attempt to learn rules to infer suchmissing relation instances by combining the KBwith parsed text.We denote by r?1 the inverse relation of r:r(c, c?)
?
r?1(c?, c).
For instance Parent?1 isequivalent to Children.
It is convenient to take Gas containing triple (c?, r?1, c) whenever it containstriple (c, r, c?
).A path type in G is a sequence pi = ?r1, .
.
.
, rm?.An instance of the path type is a sequence of nodesc0, .
.
.
, cm such that ri(ci?1, ci).
For instance, ?thepersons who were born in the same town as thequery person?, and ?the nationalities of persons whowere born in the same town as the query person?
canbe reached respectively through paths matching thefollowing typespi1 :?BornIn,BornIn?1?pi2 :?BornIn,BornIn?1,Nationality?1.2 Learning Syntactic-Semantic Rules withPath-Constrained Random WalksGiven a query concept s ?
C and a relationr ?
R, PRA begins by enumerating a large set ofbounded-length path types.
These path types aretreated as ranking ?experts,?
each generating somerandom instance of the path type starting from s, andranking end nodes t by their weights in the resultingdistribution.
Finally, PRA combines the weightscontributed by different ?experts?
by using logisticregression to predict the probability that the relationr(s, t) holds.In this study, we test the hypothesis that PRA canbe used to find useful ?syntactic-semantic patterns??
that is, patterns that exploit both semanticand syntactic relationships, thereby using semanticknowledge as background in interpreting syntacticwroteSheMentiondobjCharlottewasnsubjnsubjJane EyreCharlotteBronteMentionJane EyreMentionCoreference ResolutionEntityResolutionFreebaseNews CorpusDependency TreesWritePatrick Bront?HasFather?ProfessionWriterFigure 1: Knowledge base and parsed text as a labeledgraph.
For clarity, some word nodes are omitted.relationships.
As shown in Figure 1, we extend theKB graph G with nodes and edges from text thathas been syntactically analyzed with a dependencyparser1 and where pronouns and other anaphoricreferring expressions have been clustered with theirantecedents.
The text nodes are word/phraseinstances, and the edges are syntactic dependencieslabeled by the corresponding dependency type.Mentions of entities in the text are linked to KBconcepts by mention edges created by an entityresolution process.Given for instance the queryProfession(CharlotteBronte, ?
), PRA producesa ranked list of answers that may have the relationProfession with the query node CharlotteBronte.The features used to score answers are therandom walk probabilities of reaching a certainprofession node from the query node by pathswith particular path types.
PRA can learn pathtypes that combine background knowledge inthe database with syntactic patterns in the textcorpus.
We now exemplify some path typesinvolving relations described in Table 3.
Type?M, conj,M?1,Profession?is active (matchespaths) for professions of persons who are mentionedin conjunction with the query person as in?collaboration between McDougall and Simon1Stanford dependencies (de Marneffe and Manning, 2008).1018Philips?.
For a somewhat subtler example, type?M,TW,CW?1,Profession?1,Profession?is activefor persons who are mentioned by their titles as in?President Barack Obama?.
The type subsequence?Profession?1,Profession?ensures that onlyprofession concepts are activated.
The featuresgenerated from these path types combine syntacticdependency relations (conj) and textual informationrelations (TW and CW) with semantic relations inthe KB (Profession).Experiments on three Freebase relations (profes-sion, nationality and parents) show that exploitingexisting background knowledge as path featurescan significantly improve the quality of extractioncompared with using either Freebase or the textcorpus alone.1.3 Related WorkInformation extraction from varied unstructured andstructured sources involves both complex relationalstructure and uncertainty at all levels of the extrac-tion process.
Statistical Relational Learning (SRL)seeks to combine statistical and relational learningmethods to address such tasks.
However, most SRLapproaches (Friedman et al 1999; Richardson andDomingos, 2006) suffer the complexity of inferenceand learning when applied to large scale problems.Recently, Lao and Cohen (2010) introduced PathRanking algorithm, which is applicable to largerscale problems such as literature recommendation(Lao and Cohen, 2010) and inference on a largeknowledge base (Lao et al 2011).Much of the previous work on automatic relationextraction was based on certain lexico-syntacticpatterns.
Hearst (1992) first noticed that patternssuch as ?NP and other NP?
and ?NP such as NP?often imply hyponym relations (NP here refers toa noun phrase).
However, such approaches torelation extraction are limited by the availability ofdomain knowledge.
Later systems for extractingarbitrary relations from text mostly use shallowsurface text patterns (Etzioni et al 2004; Agichteinand Gravano, 2000; Ravichandran and Hovy, 2002).The idea of using sequences of dependency edgesas features for relation extraction was explored bySnow et al(2005) and Suchanek et al(2006).
Theydefine features to be shortest paths on dependencytrees which connect pairs of NP candidates.This study is most closely related to work ofMintz et al(2009), who also study the problem ofextending Freebase with extraction from parsed text.As in our work, they use a logistic regression modelwith path features.
However, their approach does notexploit existing knowledge in the KB.
Furthermore,their path patterns are used as binary-values features.We show experimentally that fractional-valuedfeatures generated by random walks provide muchhigher accuracy than binary-valued ones.Culotta et al(2006)?s work is similar to ourapproach in the sense of relation extraction bydiscovering relational patterns.
However whilethey focus on identifying relation mentions in text(microreading),this work attempts to infer newtuples by gathering path evidence over the wholecorpus (macroreading).
In addition, their workinvolves a few thousand examples, while we aim forWeb-scale extraction.Do and Roth (2010) use a KB (YAGO) toaid the generation of features from free text.However their method is designed specifically forextracting hierarchical taxonomic structures, whileour algorithm can be used to discover relations forgeneral general graph-based KBs.In this paper we extend the PRA algorithm alongtwo dimensions: combining syntactic and semanticcues in text with existing knowledge in the KB;and a distributed implementation of the learning andinference algorithms that works at Web scale.2 Path Ranking AlgorithmWe briefly review the Path Ranking algorithm(PRA), described in more detail by Lao and Cohen(2010).
Each path type pi = ?r1, r2, ..., r`?
specifiesa real-valued feature.
For a given query-answer nodepair (s, t), the value of the feature pi is P (s?
t;pi),the probability of reaching t from s by a randomwalk that instantiates the type.
More specifically,suppose that the random walk has just reached vi bytraversing edges labeled r1, .
.
.
, ri with s=v0.
Thenvi+1 is drawn at random from all nodes reachablefrom vi by edges labeled ri+1.
A path type pi isactive for pair (s, t) if P (s?
t;pi) > 0.Let B = {?, pi1, ..., pin} be the set of all pathtypes of length no greater than ` that occur inthe graph together with the dummy type ?, which1019represents the bias feature.
For convenience, we setP (s ?
t;?)
= 1 for any nodes s, t. The score forwhether query node s is related to another node t byrelation r is given byscore(s, t) =?pi?BP (s?
t;pi)?pi ,where ?pi is the weight of feature pi.
The modelparameters to be learned are the vector ?
=?
?pi?pi?B .
The procedures used to discover B andestimate ?
are described in the following.
Finally,note that we train a separate PRA model for eachrelation r.Path Discovery: Given a graph and a targetrelation r, the total number of path types is anexponential function of the maximum path length` and considering all possible paths would becomputationally very expensive.
As a result, B isconstructed using only path types that satisfy thefollowing two constraints:1. the path type is active for more than K trainingquery nodes, and2.
the probability of reaching any correct targetnode t is larger than a threshold ?
on averagefor the training query nodes s.We will discuss how K, ?
and the training queriesare chosen in Section 5.
In addition to making thetraining more efficient, these constraints are alsohelpful in removing low quality path types.Training Examples: For each relation r of inter-est, we start with a set of node pairs Sr = {(si, ti)}.From Sr, we create the training setDr = {(xi, yi)},where xi = ?P (si ?
ti;pi)?pi?B is the vectorof path feature values for the pair (si, ti), and yiindicates whether r(si, ti) holds.Following previous work (Lao and Cohen, 2010;Mintz et al 2009), node pairs that are in r inthe KB are legitimate positive training examples2.One can generate negative training examples byconsidering all possible pairs of concepts whosetype is compatible with r (as given by the schema)and are not present in the KB.
However this2In our experiments we subsample the positive examples.See section 3.2 for more details.procedure leads to a very large number of negativeexamples (e.g., for the parents relation, any pair ofperson concepts which are related by this relationwould be valid negative examples) which not onlymakes training very expensive but also introducesan incorrect bias in the training set.
FollowingLao and Cohen (2010) we use a simple biasedsampling procedure to generate negative examples:first, the path types discovered in the previous (pathdiscovery) step are used to construct an initial PRAmodel (all feature weights are set to 1.0); then, foreach query node si, this model is used to retrievecandidate answer nodes, which are then sorted indescending order by their scores; finally, nodes atthe k(k + 1)/2-th positions are selected as negativesamples, where k = 0, 1, 2, ....Logistic Regression Training: Given a trainingset D, we estimate parameters ?
by maximizing thefollowing objectiveF(?)
=1|D|?
(x,y)?Df(x, y;?)?
?1??
?1 ?
?2??
?22where ?1 and ?2 control the strength of the L1-regularization which helps with structure selectionand L22-regularization which prevents overfitting.The log-likelihood f(x, y;?)
of example (x, y) isgiven byf(x, y,?)
= y ln p(x,?)
+ (1?
y) ln(1?
p(x,?))p(x,?)
=exp(?Tx)1 + exp(?Tx).Inference: After a model is trained for a relationr in the knowledge base, it can be used to producenew instances of r. We first generate unlabeledqueries s which belong to the domain of r. Querieswhich appear in the training set are excluded.
Foreach unlabeled query node s, we apply the trainedPRA model to generate a list of candidate t nodestogether with their scores.
We then sort all thepredictions (s, t) by their scores in descending order,and evaluate the top ones.3 Extending PRAAs described in the previous section, the PRA modelis trained on positive and negative queries generatedfrom the KB.
As Freebase contains millions of1020concepts and edges, training on all the generatedqueries is computationally challenging.
Further,we extend the Freebase graph with parse paths ofmentions of concepts in Freebase in millions of Webpages.
Yet another issue is that the training queriesgenerated using Freebase are inherently biasedtowards the distribution of concepts in Freebaseand may not reflect the distribution of mentions ofthese concepts in text data.
As one of the goals ofour approach is to learn relation instances that aremissing in Freebase, training on such a set biasedtowards the distribution of concepts in Freebase maynot lead to good performance.
In this section weexplain how we modified the PRA algorithm toaddress those issues.3.1 Scaling UpMost relations in Freebase have a large set ofexisting triples.
For example, for the professionrelation, there are around 2 million persons inFreebase, and about 0.3 million of them have knownprofessions.
This results in more than 0.3 milliontraining queries (persons), each with one or morepositive answers (professions), and many negativeanswers, which make training computationallychallenging.
Generating all the paths for millionsof queries over a graph with millions of conceptsand edges further complicates the computationalissues.
Incorporating the parse path features fromthe text only exacerbates the matter.
Finally once wehave trained a PRA model for a given relation, sayprofession, we would like to infer the professions forall the 1.7 million persons whose professions are notknown to Freebase (and possibly predict changes tothe profession information of the 0.3 million peoplewhose professions were given).We use distributed computing to deal with thelarge number of training and prediction queriesover a large graph.
A key observation is that thedifferent stages of the PRA algorithm are basedon independent computations involving individualqueries.
Therefore, we can use the MapReduceframework to distribute the computation (Dean andGhemawat, 2008).
For path discovery, we modifyLao et als path finding (2011) approach to decouplethe queries: instead of using one depth-first searchthat involves all the queries, we first find all pathsup to certain length for each query node in themap stage, and then collect statistics for each pathfrom all the query nodes in the reduce stage.
Weused a 500-machine, 8GB/machine cluster for thesecomputations.Another challenge associated with applying PRAto a graph constructed using a large amounts oftext is that we cannot load the entire graph on asingle machine.
To circumvent this problem, we firstindex all parsed sentences by the concepts that theymention.
Therefore, to perform a random walk for aquery concept s, we only load the sentences whichmention s.3.2 Sampling Training DataUsing the r-edges in the KB as positive examplesdistorts the training set.
For example, for theprofession relation, there are 0.3 million personsfor whom Freebase has profession information, andamongst these 0.24 million are either politiciansor actors.
This may not reflect the distributionof professions of persons mentioned in Web data.Using all of these as training queries will mostcertainly bias the trained model towards theseprofessions as PRA is trained discriminatively.
Inother words, training directly with this data wouldlead to a model that is more likely to predictprofessions that are popular in Freebase.
To avoidthis distortion, we use stratified sampling.
For eachrelation r and concept t ?
C, we count the numberof r edges pointing to tNr,t = |{(s, r, t) ?
T}| .Given a training query (s, r, t) we sample itaccording toPr,t = min(1,?m+Nr,tNr,t)We fix m = 100 in our experiments.
If we take theprofession relation as an example, the above impliesthat for popular professions, we only sample about?Nr,t out of the Nr,t possible queries that end in t,whereas for the less popular professions we wouldaccept all the training queries.3.3 Text Graph ConstructionAs we are processing Web text data (see followingsection for more detail), the number of mentions1021of a concept follows a somewhat heavy-taileddistribution: there are a small number of verypopular concepts (head) and a large number of notso popular concepts (tail).
For instance the conceptBarackObama is mentioned about 8.9 million timesin our text corpus.
To prevent the text graph frombeing dominated by the head concepts, for eachsentence that mentions concept c ?
C, we acceptit as part of the text graph with probability:Pc = min(1,?k + ScSc)where Sc is the number of sentences in which c ismentioned in the whole corpus.
In our experimentswe use k = 105.
This means that if Sc  k, then weonly sample about?Sc of the sentences that containa mention of the concept, while if Sc  k, then allmentions of that concept will likely be included.4 DatasetsWe use Freebase as our knowledge base.
Freebasedata is harvested from many sources, includingWikipedia, AMG, and IMDB.3 As of this writing,it contains more than 21 million concepts and 70million labeled edges.
For a large majority of con-cepts that appear both in Freebase and Wikipedia,Freebase maintains a link to the Wikipedia page ofthat concept.We also collect a large Web corpus and identify60 million pages that mention concepts relevantto this study.
The free text on those pagesare POS-tagged and dependency parsed with anaccuracy comparable to that of the current Stanforddependency parser (Klein and Manning, 2003).
Theparser produces a dependency tree for each sentencewith each edge labeled with a standard dependencytag (see Figure 1).In each of the parsed documents, we use POS tagsand dependency edges to identify potential referringnoun phrases (NPs).
We then use a within-documentcoreference resolver comparable to that of Haghighiand Klein (2009) to group referring NPs intoco-referring clusters.
For each cluster that contains aproper-name mention, we find the Freebase conceptor concepts, if any, with a name or alias that matches3www.wikipedia.org, www.allmusic.com, www.imdb.com.Table 1: Size of training and test sets for each relation.Task Training Set Test SetProfession 22,829 15,219Nationality 14,431 9,620Parents 21,232 14,155the mention.
If a cluster has multiple possiblematching Freebase concepts, we choose a singlesense based on the following simple model.
Foreach Freebase concept c ?
C, we computeN(c,m),the number of times the concept c is referred bymention m by using both the alias informationin Freebase and the anchors of the correspondingWikipedia page for that concept.
Based on N(c,m)we can calculate the empirical probability p(c|m) =N(c,m)/?c?
N(c?,m).
If u is a cluster withmention set M(u) in the document, and C(m) theset of concepts in KB with name or alias m, weassign u to concept c?
= argmaxc?C(m),m?M(u)p(c|m),provided that there exists at least one c ?
C(m) andm ?
M(u) such that p(c|m) > 0.
Note that M(c)only contains the proper-name mentions in cluster c.5 ResultsWe use three relations profession, nationality andparents for our experiments.
For each relation, weselect its current set of triples in Freebase, and applythe stratified sampling (Section 3.2) to each of thethree triple sets.
The resulting triple sets are thenrandomly split into training (60% of the triples) andtest (the remaining triples).
However, the parentsrelation yields 350k triples after stratified sampling,so to reduce experimental effort we further randomlysub-sample 10% of that as input to the train-testsplit.
Table 1 shows the sizes of the training andtest sets for each relation.To encourage PRA to find paths involving thetext corpus, we do not count relation M (whichconnects concepts to their mentions) or M?1 whencalculating path lengths.
We use L1/L22-regularizedlogistic regression to learn feature weights.
ThePRA hyperparameters (?
and K as defined inSection 2) and regularizer hyperparameters aretuned by threefold cross validation (CV) on thetraining set.
We average the models across allthe folds and choose the model that gives the best1022Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption.
Here KB, Text andKB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB andtext.
KB+Text[b] is the binarized PRA approach trained on both KB and text.
The best performing system (resultsshown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportionssignificance test.Task KB Text KB+Text KB+Text[b]Profession 0.532 0.516 0.583 0.453Nationality 0.734 0.729 0.812 0.693Parents 0.329 0.332 0.392 0.319performance on the training set for each relation.We report results of two evaluations.
First, weevaluate the performance of the PRA algorithmwhen trained on a subset of existing Freebase factsand tested on the rest.
Second, we had humanannotators verify facts proposed by PRA that are notin Freebase.5.1 Evaluation with Existing KnowledgePrevious work in relation extraction from parsedtext (Mintz et al 2009) has mostly used binaryfeatures to indicate whether a pattern is present inthe sentences where two concepts are mentioned.To investigate the benefit of having fractional valuedfeatures generated by random walks (as in PRA), wealso evaluate a binarized PRA approach, for whichwe use the same syntactic-semantic pattern featuresas PRA does, but binarize the feature values fromPRA: if the original fractional feature value waszero, the feature value is set to zero (equivalent tonot having the feature in that example), otherwise itis set to 1.Table 2 shows a comparison of the resultsobtained using the PRA algorithm trained usingonly Freebase (KB), using only the text corpusgraph (Text), trained with both Freebase and thetext corpus (KB+Text) and the binarized PRAalgorithm using both Freebase and the text corpus(KB+Text[b]).
We report Mean Reciprocal Rank(MRR) where, given a set of queries Q,MRR =1|Q|?q?Q1rank of q?s first correct answer.Comparing the results of first three columns wesee that combining Freebase and text achievessignificantly better results than using either Freebaseor text alone.
Further comparing the results of lasttwo columns we also observe a significant drop inMRR for the binarized version of PRA.
This clearlyshows the importance of using the random walkprobabilities.
It can also be seen that the MRR forthe parents relation is lower than those for otherrelations.
This is mainly because there are largernumber of potential answers for each query node ofParent relation than for each query node of the othertwo relations ?
all persons in Freebase versus allprofessions or nationalities.
Finally, it is importantto point out that our evaluations are actually lowerbounds of actual performance, because, for instance,a person might have a profession besides the ones inFreebase and in such cases, this evaluation does notgive any credit for predicting those professions ?they are treated as errors.
We try to address this issuewith the manual evaluations in the next section.Table 2 only reports results for the maximum pathlength ` = 4 case.
We found that shorter maximumpath lengths give worse results: for instance, with` = 3 for the profession relation, MRR drops to0.542, from 0.583 for ` = 4 when using bothFreebase and text.
This difference is significantat the 0.0001 level according to a difference ofproportions test.
Further we find that using longerpath length takes much longer time to train and test,but does not lead to significant improvements overthe ` = 4 case.
For example, for profession, ` = 5gives a MRR of 0.589.Table 3 shows the top weighted features thatinvolve text edges for PRA models trained on bothFreebase and the text corpus.
To make themeasier to understand, we group them based on theirfunctionality.
For the profession and nationalitytasks, the conjunction dependency relation (in group1,4) plays an important role: these features first findpersons mentioned in conjunction with the query1023Table 3: Top weighted path types involving text edges for each task grouped according to functionality.
M relationsconnect each concept in knowledge base to its mentions in the corpus.
TW relations connect each token in a sentence tothe words in the text representation of this token.
CW relations connect each concept in knowledge base to the wordsin the text representation of this concept.
We use lower case names to denote dependency edges, word capitalizednames to denote KB edges, and ?
?1 ?
to denote the inverse of a relation.Profession Top Weighted Features Comments1?M, conj,M?1,Profession?Professions of persons mentioned in conjunctionwith the query person: ?McDougall and SimonPhillips collaborated ...?
?M, conj?1,M?1,Profession?2?M,TW,CW?1,Profession?1,Profession?Active if a person is mentioned by his profession:?The president said ...?3?M,TW,TW?1,M?1,Children,Profession?First find persons with similar names ormentioned in similar ways, then aggregate theprofessions of their children/parents/advisors:starting from the concept BarackObama, wordssuch as ?Obama?, ?leader?, ?president?, and?he?
are reachable through path ?M,TW??M,TW,TW?1,M?1,Parents,Profession?
?M,TW,TW?1,M?1,Advisors,Profession?Nationality Top Weighted Features Comments4?M, conj,TW,CW?1,Nationality?The nationalities of persons mentioned inconjunction with the query person: ?McDougalland Simon Phillips collaborated ...?
?M, conj?1,TW,CW?1,Nationality?5?M, nc?1,TW,CW?1,Nationality?The nationalities of persons mentioned close tothe query person through other dependencyrelations.
?M, tmod?1,TW,CW?1,Nationality?
?M, nn,TW,CW?1,Nationality?6?M, poss, poss?1,M?1,PlaceOfBirth,ContainedBy?The birth/death places of the query person withrestrictions to different syntactic constructions.
?M, title, title?1,M?1,PlaceOfDeath,ContainedBy?Parents Top Weighted Features Comments7?M,TW,CW?1,Parents?The parents of persons with similar names ormentioned in similar ways: starting from theconcept CharlotteBronte words such as?Bronte?, ?Charlotte?, ?Patrick?
?, and ?she?
arereachable through path ?M,TW?.8?M, nsubj, nsubj?1,TW,CW?1?Persons with similar names or mentioned insimilar ways to the query person with variousrestrictions or expansions.
?nsubj, nsubj?1?and?nc?1, nc?require the query to be subject andnoun compound respectively.
?TW?1,TW?expands further by word similarities.
?M, nsubj, nsubj?1,M?1,CW,CW?1?
?M, nc?1, nc,TW,CW?1??M,TW,CW?1?
?M,TW,TW?1,TW,CW?1?1024person, and then find their professions or nation-alities.
The features in group 2 capture the factthat sometimes people are mentioned by their pro-fessions.
The subpath?Profession?1,Profession?ensures that only profession related concepts areactivated.
Features in group 3 first find personswith similar names or mentioned in similar waysto the query person, and then aggregate theprofessions of their children, parents, or advisors.Features in group 6 can be seen as specialversions of feature ?PlaceOfBirth,ContainedBy?and ?PlaceOfDeath,ContainedBy?.
The subpaths?M, poss, poss?1,M?1?and?M, title, title?1,M?1?return the random walks back to the query node onlyif the mentions of the query node have poss (standsfor possessive modifier, e.g.
?Bill?s clothes?)
or title(stands for person?s title, e.g.
?President Obama?
)edges in text; otherwise these features are inactive.Therefore, these features are active only for specificsubsets of queries.
Features in group 8 generally findpersons with similar names or mentioned in similarways to the query person.
However, they furtherexpand or restrict this person set in various ways.Typically, each trained model includes hundredsof paths with non-zero weights, so the bulk ofclassifications are not based on a few high-precision-recall patterns, but rather on the combination ofa large number of lower-precision high-recall orhigh-precision lower-recall rules.5.2 Manual EvaluationWe performed two sets of manual evaluations.
Ineach case, an annotator is presented with the triplespredicted by PRA, and asked if they are correct.
Theannotator has access to the Freebase and Wikipediapages for the concepts (and is able to issue searchqueries about the concepts).In the first evaluation, we compared the perfor-mance of two PRA models, one trained using thestratified sampled queries and another trained usinga randomly sampled set of queries for the professionrelation.
For each model, we randomly sample 100predictions from the top 1000 predictions (sorted bythe scores returned by the model).
We found that thePRA model trained with stratified sampled querieshas 0.92 precision, while the other model has only0.84 precision (significant at the 0.02 level).
Thisshows that stratified sampling leads to improvedTable 4: Human judgement for predicted new beliefs.Task p@100 p@1k p@10kProfession 0.97 0.92 0.84Nationality 0.98 0.97 0.90Parents 0.86 0.81 0.79performance.We also evaluated the new beliefs proposed bythe models trained for all the three relations usingstratified sampled queries.
We estimated precisionfor the top 100 predictions and randomly sampled100 predictions each from the top 1,000 and 10,000predictions.
Here we use the PRA model trainedusing both KB and text.
The results of thisevaluation are shown in Table 4.
It can be seenthat the PRA model is able to produce very highprecision predications even when one considers thetop 10,000 predictions.Finally, note that our model is inductive.
Forinstance, for the profession relation, we are able topredict professions for the around 2 million personsin Freebase.
The top 1000 profession facts extractedby our system involve 970 distinct people, the top10,000 facts involve 8,726 distinct people, and thetop 100,000 facts involve 79,885 people.6 ConclusionWe have shown that path constrained random walkmodels can effectively infer new beliefs from alarge scale parsed text corpus with backgroundknowledge.
Evaluation by human annotators showsthat by combining syntactic patterns in parsedtext with semantic patterns in the backgroundknowledge, our model can propose new beliefswith high accuracy.
Thus, the proposed randomwalk model can be an effective way to automateknowledge acquisition from the web.There are several interesting directions to con-tinue this line of work.
First, bidirectional searchfrom both query and target nodes can be an efficientway to discover long paths.
This would especiallyuseful for parsed text.
Second, relation paths thatcontain constant nodes (lexicalized features) andconjunction of random walk features are potentiallyvery useful for extraction tasks.1025AcknowledgmentsWe thank Rahul Gupta, Michael Ringgaard, JohnBlitzer and the anonymous reviewers for helpfulcomments.
The first author was supported by aGoogle Research grant.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:extracting relations from large plain-text collections.In Proceedings of the fifth ACM conference on Digitallibraries, DL ?00, pages 85?94, New York, NY, USA.ACM.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: acollaboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Management ofdata, SIGMOD ?08, pages 1247?1250, New York, NY,USA.
ACM.Aron Culotta, Andrew McCallum, and Jonathan Betz.2006.
Integrating probabilistic extraction models anddata mining to discover relations and patterns in text.In Proceedings of the Human Language TechnologyConference of the NAACL, Main Conference, pages296?303, New York City, USA, June.
Association forComputational Linguistics.Marie-Catherine de Marneffe and Chris Manning.2008.
Stanford dependencies.
http://www.tex.ac.uk/cgi-bin/texfaq2html?label=citeURL.Jeffrey Dean and Sanjay Ghemawat.
2008.
Mapreduce:simplified data processing on large clusters.
Commun.ACM, 51(1):107?113, January.Quang Do and Dan Roth.
2010.
Constraints basedtaxonomic relation classification.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1099?1109, Cambridge,MA, October.
Association for Computational Linguis-tics.Oren Etzioni, Michael Cafarella, Doug Downey, StanleyKok, Ana-Maria Popescu, Tal Shaked, StephenSoderland, Daniel S. Weld, and Alexander Yates.2004.
Web-scale information extraction in knowitall:(preliminary results).
In Proceedings of the 13thinternational conference on World Wide Web, WWW?04, pages 100?110, New York, NY, USA.
ACM.Nir Friedman, Lise Getoor, Daphne Koller, and AviPfeffer.
1999.
Learning Probabilistic RelationalModels.
In IJCAI, volume 16, pages 1300?1309.Aria Haghighi and Dan Klein.
2009.
Simple coref-erence resolution with rich syntactic and semanticfeatures.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 1152?1161, Singapore, August.
Association forComputational Linguistics.Marti A. Hearst.
1992.
Automatic acquisition ofhyponyms from large text corpora.
In Proceedingsof COLING-92, pages 539?545.
Association forComputational Linguistics, August.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Erhard Hinrichs and DanRoth, editors, Proceedings of the 41st Annual Meetingon Association for Computational Linguistics, pages423?430.
Association for Computational Linguistics,July.Ni Lao and William Cohen.
2010.
Relational retrievalusing a combination of path-constrained randomwalks.
Machine Learning, 81:53?67.Ni Lao, Tom Mitchell, and William W. Cohen.
2011.Random walk inference and learning in a largescale knowledge base.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 529?539, Edinburgh,Scotland, UK., July.
Association for ComputationalLinguistics.Mike Mintz, Steven Bills, Rion Snow, and DanielJurafsky.
2009.
Distant supervision for relationextraction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages1003?1011, Suntec, Singapore, August.
Associationfor Computational Linguistics.Deepak Ravichandran and Eduard Hovy.
2002.Learning surface text patterns for a question answeringsystem.
In Proceedings of 40th Annual Meetingof the Association for Computational Linguistics,pages 41?47, Philadelphia, Pennsylvania, USA, July.Association for Computational Linguistics.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning, 62:107?136.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In Lawrence K. Saul, Yair Weiss, andLe?on Bottou, editors, Advances in Neural InformationProcessing Systems 17, pages 1297?1304, Cambridge,MA.
NIPS Foundation, MIT Press.Fabian M. Suchanek, Georgiana Ifrim, and GerhardWeikum.
2006.
Combining linguistic and statisticalanalysis to extract relations from web documents.
InProceedings of the 12th ACM SIGKDD internationalconference on Knowledge discovery and data mining,KDD ?06, pages 712?717, New York, NY, USA.ACM.1026
