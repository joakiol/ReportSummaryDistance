Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 585?592,Sydney, July 2006. c?2006 Association for Computational LinguisticsAn Iterative Implicit Feedback Approach to Personalized SearchYuanhua Lv 1, Le Sun 2, Junlin Zhang 2, Jian-Yun Nie 3, Wan Chen 4, and Wei Zhang 21, 2 Institute of Software, Chinese Academy of Sciences, Beijing, 100080, China3 University of Montreal, Canada1 lvyuanhua@gmail.com2 {sunle, junlin01, zhangwei04}@iscas.cn3 nie@iro.umontreal.ca   4 chenwan@nus.edu.sgAbstractGeneral information retrieval systems aredesigned to serve all users without con-sidering individual needs.
In this paper,we propose a novel approach to person-alized search.
It can, in a unified way,exploit and utilize implicit feedback in-formation, such as query logs and imme-diately viewed documents.
Moreover, ourapproach can implement result re-rankingand query expansion simultaneously andcollaboratively.
Based on this approach,we develop a client-side personalized websearch agent PAIR (Personalized Assis-tant for Information Retrieval), whichsupports both English and Chinese.
Ourexperiments on TREC and HTRDP col-lections clearly show that the new ap-proach is both effective and efficient.1 IntroductionAnalysis suggests that, while current informationretrieval systems, e.g., web search engines, do agood job of retrieving results to satisfy the rangeof intents people have, they are not so well indiscerning individuals?
search goals (J. Teevan etal., 2005).
Search engines encounter problemssuch as query ambiguity and results ordered bypopularity rather than relevance to the user?s in-dividual needs.To overcome the above problems, there havebeen many attempts to improve retrieval accuracybased on personalized information.
RelevanceFeedback (G. Salton and C. Buckley, 1990) is themain post-query method for automatically im-proving a system?s accuracy of a user?s individualneed.
The technique relies on explicit relevanceassessments (i.e.
indications of which documentscontain relevant information).
Relevance feed-back has been proved to be quite effective forimproving retrieval accuracy (G. Salton and C.Buckley, 1990; J. J. Rocchio, 1971).
However,searchers may be unwilling to provide relevanceinformation through explicitly marking relevantdocuments (M. Beaulieu and S. Jones, 1998).Implicit Feedback, in which an IR system un-obtrusively monitors search behavior, removesthe need for the searcher to explicitly indicatewhich documents are relevant (M. Morita and Y.Shinoda, 1994).
The technique uses implicitrelevance indications, although not being as ac-curate as explicit feedback, is proved can be aneffective substitute for explicit feedback in in-teractive information seeking environments (R.White et al, 2002).
In this paper, we utilize theimmediately viewed documents, which are theclicked results in the same query, as one type ofimplicit feedback information.
Research showsthat relative preferences derived from immedi-ately viewed documents are reasonably accurateon average (T. Joachims et al, 2005).Another type of implicit feedback informationthat we exploit is users?
query logs.
Anyone whouses search engines has accumulated lots of clickthrough data, from which we can know whatqueries were, when queries occurred, and whichsearch results were selected to view.
These querylogs provide valuable information to capture us-ers?
interests and preferences.Both types of implicit feedback informationabove can be utilized to do result re-ranking andquery expansion, (J. Teevan et al, 2005; XuehuaShen.
et al, 2005) which are the two general ap-proaches to personalized search.
(J. Pitkow et al,2002) However, to the best of our knowledge,how to exploit these two types of implicit feed-back in a unified way, which not only brings col-laboration between query expansion and resultre-ranking but also makes the whole system moreconcise, has so far not been well studied in theprevious work.
In this paper, we adopt HITS al-gorithm (J. Kleinberg, 1998), and propose a585HITS-like iterative approach addressing such aproblem.Our work differs from existing work in severalaspects: (1) We propose a HITS-like iterativeapproach to personalized search, based on which,implicit feedback information, including imme-diately viewed documents and query logs, can beutilized in a unified way.
(2) We implement re-sult re-ranking and query expansion simultane-ously and collaboratively triggered by everyclick.
(3) We develop and evaluate a client-sidepersonalized web search agent PAIR, whichsupports both English and Chinese.The remaining of this paper is organized asfollows.
Section 2 describes our novel approachfor personalized search.
Section 3 provides thearchitecture of PAIR system and some specifictechniques.
Section 4 presents the details of theexperiment.
Section 5 discusses the previouswork related to our approach.
Section 6 drawssome conclusions of our work.2 Iterative Implicit Feedback ApproachWe propose a HITS-like iterative approach forpersonalized search.
HITS (Hyperlink-InducedTopic Search) algorithm, first described by (J.Kleinberg, 1998), was originally used for thedetection of high-score hub and authority webpages.
The Authority pages are the central webpages in the context of particular query topics.The strongest authority pages consciously do notlink one another1 ?
they can only be linked bysome relatively anonymous hub pages.
The mu-tual reinforcement principle of HITS states that aweb page is a good authority page if it is linked bymany good hub pages, and that a web page is agood hub page if it links many good authoritypages.
A directed graph is constructed, of whichthe nodes represent web pages and the directededges represent hyperlinks.
After iterativelycomputing based on the reinforcement principle,each node gets an authority score and a hub score.In our approach, we exploit the relationshipsbetween documents and terms in a similar way toHITS.
Unseen search results, those results whichare retrieved from search engine yet not beenpresented to the user, are considered as ?authoritypages?.
Representative terms are considered as?hub pages?.
Here the representative terms are theterms extracted from and best representing theimplicit feedback information.
Representativeterms confer a relevance score to the unseen1 For instance, There is hardly any other company?s Webpage linked from ?http://www.microsoft.com/?search results ?
specifically, the unseen searchresults, which contain more good representativeterms, have a higher possibility of being relevant;the representative terms should be more repre-sentative, if they occur in the unseen search re-sults that are more likely to be relevant.
Thus,also there is mutual reinforcement principle ex-isting between representative terms and unseensearch results.
By the same token, we constructeda directed graph, of which the nodes indicate un-seen search results and representative terms, andthe directed edges represent the occurrence of therepresentative terms in the unseen search results.The following Table 1 shows how our approachcorresponds to HITS algorithm.The Directed GraphApproachesNodes EdgesHITS Authority Pages Hub Pages HyperlinksOurApproachUnseen SearchResultsRepresentativeTerms Occurrence2Table 1.
Our approach versus HITS.Because we have already known that the rep-resentative terms are ?hub pages?, and that theunseen search results are ?authority pages?, withrespect to the former, only hub scores need to becomputed; with respect to the latter, only author-ity scores need to be computed.Finally, after iteratively computing based onthe mutual reinforcement principle we canre-rank the unseen search results according totheir authority scores, as well as select the repre-sentative terms with highest hub scores to ex-pand the query.
Below we present how to con-struct a directed graph to begin with.2.1 Constructing a Directed GraphWe can view the unseen search results and therepresentative terms as a directed graph G = (V, E).A sample directed graph is shown in Figure 1:Figure 1.
A sample directed graph.The nodes V correspond to the unseen searchresults (the rectangles in Figure 1) and the repre-2 The occurrence of the representative terms in the unseensearch results.586sentative terms (the circles in Figure 1); a di-rected edge ?p?q?E?
is weighed by the fre-quency of the occurrence of a representative termp in an unseen search result q (e.g., the numberput on the edge ?t1?r2?
indicates that t1 occurstwice in r2).
We say that each representative termonly has an out-degree which is the number of theunseen search results it occurs in, as well as thateach unseen search result only has an in-degreewhich is the count of the representative terms itcontains.
Based on this, we assume that the un-seen search results and the representative termsrespectively correspond to the authority pagesand the hub pages ?
this assumption is usedthroughout the proposed algorithm.2.2 A HITS-like Iterative AlgorithmIn this section, we present how to initialize thedirected graph and how to iteratively compute theauthority scores and the hub scores.
And thenaccording to these scores, we show how to re-rankthe unseen search results and expand the initialquery.Initially, each unseen search result of the queryare considered equally authoritative, that is,0 0 01 2 | |1 | |YYy y y= ?= =                  (1)Where vector Y indicates authority scores of theoverall unseen search results, and |Y| is the size ofsuch a vector.
Meanwhile, each representativeterm, with the term frequency tfj in the historyquery logs that have been judged related to thecurrent query, obtains its hub score according tothe follow formulation:0| |1Xj ij itf tfx == ?
(2)Where vector X indicates hub scores of the overallrepresentative terms, and |X| is the size of thevector X.
The nodes of the directed graph areinitialized in this way.
Next, we associate eachedge with a weight:,( )ji i jw tft r?
=                     (3)Where tfi,j indicates the term frequency of therepresentative term ti occurring in the unseensearch result rj; ?w(ti?
rj)?
is the weight of edgethat link from ti to rj.
For instance, in Figure 1,w(t1?
r2) = 2.After initialization, the iteratively computing ofhub scores and authority scores starts.The hub score of each representative term isre-computed based on three factors: the authorityscores of each unseen search result where thisterm occurs; the occurring frequency of this termin each unseen search result; the total occurrenceof every representative term in each unseen searchresult.
The formulation for re-computing hubscores is as follows:( 1)::( )( )'k jiijnjijnkjjnwwt rt rt ryx t r+?
??
?
?= ??
?
(4)Where x`i(k+1) is the hub score of a representativeterm ti after (k+1)th iteration; yjk is the authorityscore of an unseen search result rj after kth itera-tion; ?
?j: ti?rj?
indicates the set of all unseensearch results those ti occurs in; ?
?n: tn?rj?
in-dicates the set of all representative terms those rjcontains.The authority score of each unseen search re-sult is also re-computed relying on three factors:the hub scores of each representative term thatthis search result contains; the occurring fre-quency of each representative term in this searchresult; the total occurrence of each representativeterm in every unseen search results.
The formu-lation for re-computing authority scores is asfollows:( 1)::( )( )'k kijmijimijiimwwt rt rt ry x t r+?
??
?
?= ??
?
(5)Where y`j(k+1) is the authority score of an unseensearch result rj after (k+1)th iteration; xik  is thehub score of a representative term ti after kth it-eration; ?
?i: ti?rj?
indicates the set of all repre-sentative terms those rj contains; ?
?m: ti?rm?indicates the set of all unseen search results thoseti occurs in.After re-computation, the hub scores and theauthority scores are normalized to 1.
The formu-lation for normalization is as follows:| | | |1 1and' '' 'j iiY Xjkkk ky xy xy x= ==   =?
?
(6)The iteration, including re-computation andnormalization, is repeated until the changes of thehub scores and the authority scores are smallerthan some predefined threshold ?
(e.g.
10-6).Specifically, after each repetition, the changes inauthority scores and hub scores are computedusing the following formulation:2 2( 1) ( 1)| | | |1 1( ) ( )k k k ki ij jY xj ic y y x x+ += == ?
+ ??
?
(7)The iteration stops if c<?.
Moreover, the itera-tion will also stop if repetition has reached a587predefined times k (e.g.
30).
The procedure of theiteration is shown in Figure 2.As soon as the iteration stops, the top n unseensearch results with highest authority scores areselected and recommended to the user; the top mrepresentative terms with highest hub scores areselected to expand the original query.
Here n is apredefined number (in PAIR system we set n=3,n is given a small number because using implicitfeedback information is sometimes risky.)
m isdetermined according to the position of the big-gest gap, that is, if ti ?
ti+1 is bigger than the gapof any other two neighboring ones of the top halfrepresentative terms, then m is given a value i.Furthermore, some of these representative terms(e.g.
top 50% high score terms) will be again usedin the next time of implementing the iterativealgorithm together with some newly incomingterms extracted from the just now click.Figure 2.
The HITS-like iterative algorithm.3 Implementation3.1 System DesignIn this section, we present our experimental sys-tem PAIR, which is an IE Browser Helper Object(BHO) based on the popular Web search engineGoogle.
PAIR has three main modules: ResultRetrieval module, User Interactions module, andIterative Algorithm module.
The architecture isshown in Figure 3.The Result Retrieval module runs in back-grounds and retrieves results from search engine.When the query has been expanded, this modulewill use the new keywords to continue retrieving.The User Interactions module can handle threetypes of basic user actions: (1) submitting a query;(2) clicking to view a search result; (3) clickingthe ?Next Page?
link.
For each of these actions,the system responds with: (a) exploiting and ex-tracting representative terms from implicit feed-back information; (b) fetching the unseen searchresults via Results Retrieval module; (c) sendingthe representative terms and the unseen searchresults to Iterative Algorithm module.Figure 3.
The architecture of PAIR.The Iterative Algorithm module implementsthe HITS-like algorithm described in section 2.When this module receives data from User In-teractions module, it responds with: (a) iterativelycomputing the hub scores and authority scores; (b)re-ranking the unseen search results and expand-ing the original query.Some specific techniques for capturing andexploiting implicit feedback information are de-scribed in the following sections.3.2 Extract Representative Terms fromQuery LogsWe judge whether a query log is related to thecurrent query based on the similarity between thequery log and the current query text.
Here thequery log is associated with all documents thatthe user has selected to view.
The form of eachquery log is as follows<query text><query time> [clicked documents]*The ?clicked documents?
consist of URL, titleand snippet of every clicked document.
The rea-son why we utilize the query text of the currentquery but not the search results (including title,snippet, etc.)
to compute the similarity, is out ofconsideration for efficiency.
If we had used thesearch results to determine the similarity, thecomputation could only start once the search en-gine has returned the search results.
In our method,instead, we can exploit query logs while searchengine is doing retrieving.
Notice that althoughour system only utilizes the query logs in the last24 hours; in practice, we can exploit much morebecause of its low computation cost with respectto the retrieval process performed in parallel.Iterate (T, R, k, ?
)T: a collection of m termsR: a collection of n search resultsk: a natural number?
: a predefined thresholdApply (1) to initialize Y.Apply (2) to initialize X.Apply (3) to initialize W.For i = 1, 2?, kApply (4) to (Xi-1, Yi-1) and obtain X`i.Apply (5) to (Xi-1, Yi-1) and obtain Y`i.Apply (6) to Normalize X`i and Y`i, and respectivelyobtain Xi and Yi.Apply (7) and obtain c.If c<?, then break.EndReturn (X, Y).588Table 2.
Sample results of re-ranking.
The search results in boldface are the ones that our system rec-ommends to the user.
?-3?
and ?-2?
in the right side of some results indicate the how their ranks descend.We use the standard vector space retrievalmodel (G. Salton and M. J. McGill, 1983) tocompute the similarity.
If the similarity betweenany query log and the current query exceeds apredefined threshold, the query log will be con-sidered to be related to current query.
Our systemwill attempt to extract some (e.g.
30%) represen-tative terms from such related query logs ac-cording to the weights computed by applying thefollowing formulation:( )i i iw f idftt =                      (8)Where tfi and idfi respectively are the term fre-quency and inverse document frequency of ti inthe clicked documents of a related query log.This formulation means that a term is more rep-resentative if it has a higher frequency as well asa broader distribution in the related query log.3.3 Extract Representative Terms fromImmediately Viewed DocumentsThe representative terms extracted from immedi-ately viewed documents are determined based onthree factors: term frequency in the immediatelyviewed document, inverse document frequency inthe entire seen search results, and a discriminantvalue.
The formulation is as follows:( ) ( )Ni ii ir ddw dx xtf idfx x= ?
?
(9)Where tfxidr is the term frequency of term xi in theviewed results set dr; tfxidr is the inverse documentfrequency of xi in the entire seen results set dN.And the discriminant value d(xi) of xi is computedusing the weighting schemes F2 (S. E. Robertsonand K. Sparck Jones, 1976) as follows:( ) ln( ) ( )ir Rdn r N Rx = ?
?
(10)Where r is the number of the immediately vieweddocuments containing term xi; n is the number ofthe seen results containing term xi; R is the num-ber of the immediately viewed documents in thequery; N is the number of the entire seen results.3.4 Sample ResultsUnlike other systems which do result re-rankingand query expansion respectively in differentways, our system implements these two functionssimultaneously and collaboratively ?
Queryexpansion provides diversified search resultswhich must rely on the use of re-ranking to bemoved forward and recommended to the user.Figure 4.
A screen shot for query expansion.After iteratively computing using our approach,the system selects some search results with tophighest authority scores and recommends them tothe user.
In Table 2, we show that PAIR suc-cessfully re-ranks the unseen search results of?jaguar?
respectively using the immediatelyGoogle result PAIR resultquery = ?jaguar?
query = ?jaguar?
After the 4th result being clickedquery = ?jaguar??car?
?
query logs1 Jaguar www.jaguar.com/Jaguarwww.jaguar.com/Jaguar UK - Jaguar Carswww.jaguar.co.uk/2 Jaguar CA - Jaguar Cars www.jaguar.com/ca/en/Jaguar CA - Jaguar Carswww.jaguar.com/ca/en/Jaguar UK - R is for?www.jaguar-racing.com/3 Jaguar Cars www.jaguarcars.com/Jaguar Carswww.jaguarcars.com/Jaguarwww.jaguar.com/4 Apple - Mac OS X www.apple.com/macosx/Apple - Mac OS Xwww.apple.com/macosx/Jaguar CA - Jaguar Carswww.jaguar.com/ca/en/                      -25 Apple - Support ?
www.apple.com/support/...Amazon.com: Mac OS X 10.2?www.amazon.com/exec/obidos/...Jaguar Carswww.jaguarcars.com/                        -26 Jaguar UK - Jaguar Cars www.jaguar.co.uk/Mac OS X 10.2 Jaguar?arstechnica.com/reviews/os?Apple - Mac OS Xwww.apple.com/macosx/                     -27 Jaguar UK - R is for?
www.jaguar-racing.com/Macworld: News: Macworld?maccentral.macworld.com/news/?Apple - Support ?www.apple.com/support/...                    -28 Jaguar dspace.dial.pipex.com/?Apple - Support?www.apple.com/support/...                -3Jaguardspace.dial.pipex.com/?9 Schr?dinger -> Home www.schrodinger.com/Jaguar UK - Jaguar Carswww.jaguar.co.uk/                       -3Schr?dinger -> Homewww.schrodinger.com/10 Schr?dinger -> Site Map www.schrodinger.com/...Jaguar UK - R is for?www.jaguar-racing.com/                  -3Schr?dinger -> Site Mapwww.schrodinger.com/...589viewed documents and the query logs.
Simulta-neously, some representative terms are selectedto expand the original query.
In the query of?jaguar?
(without query logs), we click someresults about ?Mac OS?, and then we see that aterm ?Mac?
has been selected to expand theoriginal query, and some results of the new query?jaguar Mac?
are recommended to the user underthe help of re-ranking, as shown in Figure 4.4 Experiment4.1 Experimental MethodologyIt is a challenge to quantitatively evaluate thepotential performance improvement of the pro-posed approach over Google in an unbiased way(D. Hawking et al, 1999; Xuehua Shen et al,2005).
Here, we adopt a similar quantitativeevaluation as what Xuehua Shen et al (2005) doto evaluate our system PAIR and recruit 9 stu-dents who have different backgrounds to partici-pate in our experiment.
We use query topics fromTREC 2005 and 2004 Hard Track, TREC 2004Terabyte track for English information retrieval,3and use query topics from HTRDP 2005 Evalua-tion for Chinese information retrieval.4 The rea-son why we utilize multiple TREC tasks ratherthan using a single one is that more queries aremore likely to cover the most interesting topicsfor each participant.Initially, each participant would freely choosesome topics (typically 5 TREC topics and 5HTRDP topics).
Each query of TREC topics willbe submitted to three systems: UCAIR 5 (Xue-hua Shen et al, 2005), ?PAIR No QE?
(PAIRsystem of which the query expansion function isblocked) and PAIR.
Each query of HTRDP topicsneeds only to be submitted to ?PAIR No QE?
andPAIR.
We do not evaluate UCAIR using HTRDPtopics, since it does not support Chinese.
For eachquery topic, the participants use the title of thetopic as the initial keyword to begin with.
Alsothey can form some other keywords by them-selves if the title alone fails to describe some de-tails of the topic.
There is no limit on how manyqueries they must submit.
During each queryprocess, the participant may click to view someresults, just as in normal web search.Then, at the end of each query, search resultsfrom these different systems are randomly andanonymously mixed together so that every par-3 Text REtrieval Conference.
http://trec.nist.gov/4 2005 HTRDP Evaluation.
http://www.863data.org.cn/5 The latest version released on November 11, 2005.http://sifaka.cs.uiuc.edu/ir/ucair/ticipant would not know where a result comesfrom.
The participants would judge which ofthese results are relevant.At last, we respectively measure precision attop 5, top 10, top 20 and top 30 documents ofthese system.4.2 Results and AnalysisAltogether, 45 TREC topics (62 queries in all) arechosen for English information retrieval.
712documents are judged as relevant from Googlesearch results.
The corresponding number ofrelevant documents from UCAIR, ?PAIR No QE?and PAIR respectively is: 921, 891 and 1040.Figure 5 shows the average precision of these foursystems at top n documents among such 45 TRECtopics.Figure 5.
Average precision for TREC topics.45 HTRDP topics (66 queries in all) are chosenfor Chinese information retrieval.
809 documentsare judged as relevant from Google search results.The corresponding number of relevant documentsfrom ?PAIR No QE?
and PAIR respectively is:1198 and 1416.
Figure 6 shows the average pre-cision of these three systems at top n documentsamong such 45 HTRDP topics.Figure 6.
Average precision for HTRDP topics.PAIR and ?PAIR No QE?
versus GoogleWe can see clearly from Figure 5 and Figure 6that the precision of PAIR is improved a lotcomparing with that of Google in all measure-590ments.
Moreover, the improvement scale in-creases from precision at top 10 to that of top 30.One explanation for this is that the more implicitfeedback information generated, the more repre-sentative terms can be obtained, and thus, theiterative algorithm can perform better, leading tomore precise search results.
?PAIR No QE?
alsosignificantly outperforms Google in these meas-urements, however, with query expansion, PAIRcan perform even better.
Thus, we say that resultre-ranking and query expansion both play animportant role in PAIR.Comparing Figure 5 with Figure 6, one can seethat the improvement of PAIR versus Google inChinese IR is even larger than that of English IR.One explanation for this is that: before imple-menting the iterative algorithm, each Chinesesearch result, including title and snippet, is seg-mented into words (or phrases).
And only thenoun, verb and adjective of these words (orphrases) are used in next stages, whereas, we onlyremove the stop words for English search result.Another explanation is that there are some Chi-nese web pages with the same content.
If one ofsuch pages is clicked, then, occasionally somerepetition pages are recommended to the user.However, since PAIR is based on the search re-sults of Google and the information concerningthe result pages that PAIR can obtained is limited,which leads to it difficult to avoid the replica-tions.PAIR and ?PAIR No QE?
versus UCAIRIn Figure 5, we can see that the precision of?PAIR No QE?
is better than that of UCAIRamong top 5 and top 10 documents, and is almostthe same as that of UCAIR among top 20 and top30 documents.
However, PAIR is much betterthan UCAIR in all measurements.
This indicatesthat result re-ranking fails to do its best withoutquery expansion, since the relevant documents inoriginal query are limited, and only the re-rankingmethod alone cannot solve the ?relevant docu-ments sparseness?
problem.
Thus, the query ex-pansion method, which can provide fresh andrelevant documents, can help the re-rankingmethod to reach an even better performance.Efficiency of PAIRThe iteration statistic in evaluation indicates thatthe average iteration times of our approach is 22before convergence on condition that we set thethreshold ?
= 10-6.
The experiment shows that thecomputation time of the proposed approach isimperceptible for users (less than 1ms.
)5 Related WorkThere have been many prior attempts to person-alized search.
In this paper, we focus on the re-lated work doing personalized search based onimplicit feedback information.Some of the existing studies capture users?
in-formation need by exploiting query logs.
Forexample, M. Speretta and S. Gauch (2005) builduser profiles based on activity at the search siteand study the use of these profiles to providepersonalized search results.
F. Liu et al (2002)learn user's favorite categories from his queryhistory.
Their system maps the input query to a setof interesting categories based on the user profileand confines the search domain to these catego-ries.
Some studies improve retrieval performanceby exploiting users?
browsing history (F. Tanud-jaja and L. Mu, 2002; M. Morita and Y. Shinoda,1994) or Web communities (A. Kritikopoulosand M. Sideri, 2003; K. Sugiyama et al, 2004)Some studies utilize client side interactions, forexample, K. Bharat (2000) automatically discov-ers related material on behalf of the user byserving as an intermediary between the user andinformation retrieval systems.
His system ob-serves users interacting with everyday applica-tions and then anticipates their information needsusing a model of the task at hand.
Some lateststudies combine several types of implicit feed-back information.
J. Teevan et al (2005) explorerich models of user interests, which are builtfrom both search-related information, such aspreviously issued queries and previously visitedWeb pages, and other information about the usersuch as documents and email the user has readand created.
This information is used to re-rankWeb search results within a relevance feedbackframework.Our work is partly inspired by the study ofXuehua Shen et al (2005), which is closely re-lated to ours in that they also exploit immediatelyviewed documents and short-term history queries,implement query expansion and re-ranking, anddevelop a client-side web search agents that per-form eager implicit feedback.
However, theirwork differs from ours in three ways: First, theyuse the cosine similarity to implement query ex-pansion, and use Rocchio formulation (J. J.Rocchio, 1971) to re-rank the search results.Thus, their query expansion and re-ranking arecomputed separately and are not so concise andcollaborative.
Secondly, their query expansion isbased only on the past queries and is imple-mented before the query, which leads to that591their query expansion does not benefit fromuser?s click through data.
Thirdly, they do notcompute the relevance of search results and therelativity of expanded terms in an iterative fash-ion.
Thus, their approach does not utilize the re-lation among search results, among expandedterms, and between search results and expandedterms.6 ConclusionsIn this paper, we studied how to exploit implicitfeedback information to improve retrieval accu-racy.
Unlike most previous work, we propose anovel HITS-like iterative algorithm that canmake use of query logs and immediately vieweddocuments in a unified way, which not onlybrings collaboration between query expansionand result re-ranking but also makes the wholesystem more concise.
We further propose somespecific techniques to capture and exploit thesetwo types of implicit feedback information.
Us-ing these techniques, we develop a client-sideweb search agent PAIR.
Experiments in Englishand Chinese collections show that our approachis both effective and efficient.However, there is still room to improve theperformance of the proposed approach, such asexploiting other types of personalized informa-tion, choosing some more effective strategies toextract representative terms, studying the effectsof the parameters used in the approach, etc.AcknowledgementWe would like to thank the anonymous review-ers for their helpful feedback and corrections,and to the nine participants of our evaluation ex-periments.
Additionally, this work is supportedby the National Science Fund of China undercontact 60203007.ReferencesA.
Kritikopoulos and M. Sideri, 2003.
The CompassFilter: Search engine result personalization usingWeb communities.
In Proceedings of ITWP, pages229-240.D.
Hawking, N. Craswell, P.B.
Thistlewaite, and D.Harman, 1999.
Results and challenges in websearch evaluation.
Computer Networks,31(11-16):1321?1330.F.
Liu, C. Yu, and W. Meng, 2002.
Personalized websearch by mapping user queries to categories.
InProceedings of CIKM, pages 558-565.F.
Tanudjaja and L. Mu, 2002.
Persona: a contextual-ized and personalized web search.
HICSS.G.
Salton and M. J. McGill, 1983.
Introduction toModern Information Retrieval.
McGraw-Hill.G.
Salton and C. Buckley, 1990.
Improving retrievalperformance by relevance feedback.
Journal of theAmerican Society for Information Science,41(4):288-297.J.
J. Rocchio, 1971.
Relevance feedback in informa-tion retrieval.
In The SMART Retrieval System :Experiments in Automatic Document Processing,pages 313?323.
Prentice-Hall Inc.J.
Kleinberg, 1998.
Authoritative sources in a hyper-linked environment.
ACM, 46(5):604?632.J.
Pitkow, H. Schutze, T. Cass, R. Cooley, D.Turnbull, A. Edmonds, E. Adar, and T. Breuel,2002.
Personalized search.
Communications of theACM, 45(9):50-55.J.
Teevan, S. T. Dumais, and E. Horvitz, 2005.
Per-sonalizing search via automated analysis of interestsand activities.
In Proceedings of SIGIR, pages449-456.K.
Bharat, 2000.
SearchPad: Explicit capture ofsearch context to support Web search.
ComputerNetworks, 33(1-6): 493-501.K.
Sugiyama, K. Hatano, and M. Yoshikawa, 2004.Adaptive Web search based on user profile con-structed without any effort from user.
In Proceed-ings of WWW, pages 675-684.M.
Beaulieu and S. Jones, 1998.
Interactive searchingand interface issues in the okapi best match retrievalsystem.
Interacting with Computers, 10(3):237-248.M.
Morita and Y. Shinoda, 1994.
Information filteringbased on user behavior analysis and best match textretrieval.
In Proceedings of SIGIR, pages 272?281.M.
Speretta and S. Gauch, 2005.
Personalizing searchbased on user search history.
Web Intelligence,pages 622-628.R.
White, I. Ruthven, and J. M. Jose, 2002.
The use ofimplicit evidence for relevance feedback in webretrieval.
In Proceedings of ECIR, pages 93?109.S.
E. Robertson and K. Sparck Jones, 1976.
Relevanceweighting of search terms.
Journal of theAmerican Society for Information Science,27(3):129-146.T.
Joachims, L. Granka, B. Pang, H. Hembrooke, andG.
Gay, 2005.
Accurately Interpreting ClickthroughData as Implicit Feedback, In Proceedings ofSIGIR, pages 154-161.Xuehua Shen, Bin Tan, and Chengxiang Zhai, 2005.Implicit User Modeling for Personalized Search.
InProceedings of CIKM, pages 824-831.592
