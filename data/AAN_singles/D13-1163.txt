Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1563?1573,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsLexical Chain Based Cohesion Models forDocument-Level Statistical Machine TranslationDeyi Xiong1, Yang Ding2, Min Zhang1?
and Chew Lim Tan21School of Computer Science and Technology, Soochow University, Suzhou, China 215006{dyxiong, minzhang}@suda.edu.cn2School of Computing, National University of Singapore, Singapore 117417{a0082379, tancl}@comp.nus.edu.sgAbstractLexical chains provide a representation of thelexical cohesion structure of a text.
In this pa-per, we propose two lexical chain based co-hesion models to incorporate lexical cohesioninto document-level statistical machine trans-lation: 1) a count cohesion model that rewardsa hypothesis whenever a chain word occurs inthe hypothesis, 2) and a probability cohesionmodel that further takes chain word transla-tion probabilities into account.
We computelexical chains for each source document to betranslated and generate target lexical chainsbased on the computed source chains via max-imum entropy classifiers.
We then use thegenerated target chains to provide constraintsfor word selection in document-level machinetranslation through the two proposed lexicalchain based cohesion models.
We verify theeffectiveness of the two models using a hier-archical phrase-based translation system.
Ex-periments on large-scale training data showthat they can substantially improve translationquality in terms of BLEU and that the prob-ability cohesion model outperforms previousmodels based on lexical cohesion devices.1 IntroductionGiven a source document, traditionally most statisti-cal machine translation (SMT) systems translate thedocument sentence by sentence.
In such a transla-tion scheme, sentences are translated independentof any other sentences.
However, a text is normallywritten cohesively, in which sentences are connected?Corresponding authorto each other via syntactic and lexical devices.
Thislinguistic phenomenon is called as textual cohesion(Halliday and Hasan, 1976).Cohesion is a surface-level property of well-formed texts.
It deals with five categories of rela-tionships between text units, namely co-reference,ellipsis, substitution, conjunction and lexical cohe-sion that is realized via semantically related words.The former four cohesion relations can be groupedas grammatical cohesion.
Generally speaking,grammatical cohesion is less common and harderto identify than lexical cohesion (Barzilay and El-hadad, 1997).As most SMT systems translate a text in asentence-by-sentence fashion, they tend to build lesslexical cohesion than human translators (Wong andKit, 2012).
We therefore study lexical cohesion fordocument-level translation.
We use lexical chains(Morris and Hirst, 1991) to capture lexical cohe-sion in a text.
Lexical chains are connected graphsthat represent the lexical cohesion structure of a text.They have been successfully used for informationretrieval (Stairmand, 1996), document summariza-tion (Barzilay and Elhadad, 1997) and so on.
In thispaper, we investigate how lexical chains can be usedto incorporate lexical cohesion into document-leveltranslation.Our basic assumption is that the lexical chains ofa target document are direct correspondences of thelexical chains of its counterpart source document.This assumption is reasonable as the target docu-ment translation should be faithful to the source doc-ument in terms of both text meaning and structure.Based on this assumption, we propose a framework1563to incorporate lexical cohesion into target documenttranslation via lexical chains, which works as fol-lows.?
Compute lexical chains for each source docu-ment that is to be translated;?
Project the computed source lexical chains ontothe corresponding target document by translat-ing source chain words into target chain wordsusing maximum entropy classifiers;?
Incorporate lexical cohesion into the target doc-ument translation via cohesion models built onthe projected target lexical chains .We build two lexical chain based cohesion mod-els.
The first model is a count model that rewards ahypothesis whenever a word in the projected targetlexical chains occur in the hypothesis.
As a sourcechain word may be translated into many differenttarget words, we further extend the count model toa second cohesion model: a probability model thattakes chain word translation probabilities into ac-count.We test the two lexical chain based cohesion mod-els on a hierarchical phrase-based SMT system thatis trained with large-scale Chinese-English bilin-gual data.
Experiment results show that our lexi-cal chain based cohesion models can achieve sub-stantial improvements over the baseline.
Further-more, the probability cohesion model is better thanthe count model and it also outperforms previouscohesion models based on lexical cohesion devices(Xiong et al 2013).To the best of our knowledge, this is the first at-tempt to explore lexical chains for statistical ma-chine translation.
The remainder of this paper is or-ganized as follows.
Section 2 discusses related workand highlights the differences between our methodand previous work.
Section 3 briefly introduceslexical chains and algorithms that compute lexicalchains.
Section 4 elaborates the proposed lexicalchain based framework, including details on sourcelexical chain computation, target lexical chain gen-eration and the two lexical chain based cohesionmodels.
Section 5 presents our large-scale experi-ments and results.
Finally, we conclude with futuredirections in Section 6.2 Related WorkRecent years have witnessed growing research in-terests in document-level statistical machine trans-lation.
Such research efforts can be roughly di-vided into two groups: 1) general document-levelmachine translation that does not explore or ex-plores very little linguistic discourse information;2) linguistically-motivated document-level machinetranslation that incorporates discourse informationsuch as cohesion and coherence into SMT.
Recentstudies (Guillou, 2013; Beigman Klebanov andFlor, 2013) show that this discourse information isvery important for document-level machine transla-tion.General Document-Level Machine TranslationTiedemann (2010) propose cache-based languageand translation models for document-level machinetranslation.
These models are built on recently trans-lated sentences.
Following this cache-based ap-proach, Gong et al(2011) further introduce twoadditional caches.
They use a static cache to storebilingual phrases extracted from documents in train-ing data that are similar to the document being trans-lated.
They also adopt a topic cache with targetlanguage topic words.
Xiao et al(2011) studythe translation consistency issue in document-levelmachine translation.
They use a hard constraint toconsistently translate ambiguous source words intothe most frequent translation options.
Ture et al(2012) soften this consistency constraint by integrat-ing three counting features into decoder.Using Lexical Cohesion Devices in Document-Level SMT Lexical cohesion devices are seman-tically related words, including word repetition,synonyms/near-synonyms, hyponyms and so on.They are also the cohesion-building elements in lex-ical chains.Wong and Kit (2012) use lexical cohesion devicebased metrics to improve machine translation evalu-ation at the document level.
These metrics measurethe proportion of content words that are used as lex-ical cohesion devices in machine-generated transla-tions.
Hardmeier et al(2012) propose a document-wide phrase-based decoder and integrate a semanticlanguage model into the decoder.
They argue thattheir semantic language model can capture lexicalcohesion by exploring n-grams that cross sentence1564boundaries.Most recently Xiong et al(2013) integratethree categories of lexical cohesion devices intodocument-level machine translation.
They definethree cohesion models based on lexical cohesion de-vices: a direct reward model, a conditional probabil-ity model and a mutual information trigger model.The latter two models measure the strength of lexicalcohesion relation between two lexical items.
Theyare incorporated into SMT to calculate how appro-priately lexical cohesion devices are used in doc-ument translation.
As lexical chains capture lexi-cal cohesion relations among sequences of relatedwords rather than those only between two words, ex-periments in Section 5 show that our lexical chainbased probability cohesion model is better than thelexical cohesion device based trigger model, whichis the best among the three cohesion models pro-posed by Xiong et al(2013).Modeling Coherence in Document-Level SMTIn discourse analysis, cohesion is often studied to-gether with coherence which is another dimensionof the linguistic structure of a text (Barzilay andElhadad, 1997).
Cohesion is related to the sur-face structure of a text while coherence is concernedwith the underlying meaning connectedness in a text(Vasconcellos, 1989).
Compared with cohesion, co-herence is not easy to be detected.
Even so, variousmodels have been proposed to explore coherence fordocument summarization and generation (Barzilayand Lapata, 2008; Louis and Nenkova, 2012).
Fol-lowing this line, Xiong and Zhang (2013) integratea topic-based coherence model into document-levelmachine translation, where coherence is defined as acontinuous sentence topic transition.Our lexical chain based cohesion models are alsorelated to previous work on using word and phrasesense disambiguation for lexical choice in SMT(Carpuat and Wu, 2007b; Carpuat and Wu, 2007a;Chan et al 2007).
The difference is that we usedocument-wide lexical chains to build our cohesionmodels rather than sentence-level context features.In our framework, lexical choice is performed tomake the selected words consistent with the lexicalcohesion structure of a document.Carpuat (2009) explores the principle of one senseper discourse (Gale et al 1992) in the context ofSMT and imposes the constraint of one translationper discourse on document translation.
We alsouse the one sense per discourse principle to performword sense disambiguation on the source side in ourlexical chaining algorithm (See Section 4.1).3 Background: Lexical Chain and ChainComputationLexical chains are sequences of semantically relatedwords (Morris and Hirst, 1991).
They represent thelexical cohesion structure of a text.
Figure 2 displayssix lexical chains computed from the Chinese newsarticle shown in Figure 1.
Words in these lexicalchains have lexical cohesion relations such as rep-etition, synonym, which may range over the entiretext.
For example, in the lexical chain LC1 of Fig-ure 2, the same word ?dWgu_?
(Germany) repeats9 times.
In the lexical chain LC3, the two words?z`ngcSi?
(president) and ?zhdx[?
(chairman) aresynonym words.
Generally, a text can have manydifferent lexical chains, each of which represents athread of cohesion through the text.Several lexical chaining algorithms have beenproposed to compute lexical chains from texts.
Nor-mally they need an ontology to obtain semantic re-lations between words.
Word sense disambiguation(WSD) is also used to determine the sense of eachword in a text.
Generally a lexical chain compu-tation algorithm completes the following three sub-tasks:?
Building a representation of a text with a setof candidate words and assigning semantic re-lations between the candidate words accordingto the ontology;?
Choosing the right sense for each candidateword via WSD;?
Building chains over the semantically relatedand disambiguated candidate words.These three sub-tasks can be done separately or si-multaneously.Morris and Hirst (Morris and Hirst, 1991) de-fine the first lexical chain computation algorithmthat adopts a greedy strategy to immediately disam-biguate a word at its first occurrence.
This algo-rithm runs in linear time but suffers from inaccu-rate disambiguation.
Barzilay and Elhadad (Barzi-lay and Elhadad, 1997) significantly improve WSD1565dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma c[zh[dWgu_ diUnx]n g^ngsZ xuRnbe , qiSn jiRnsh]hu] zhdx[ qZsh[Yr su] de xZlYXr jiRng dRnrYn gRig^ngsZ de l[nsh[ z`ngcSi , wWiqZ lie gY yuY , zh[dUo su`ma de j]rYn rWnxuTn jiVrYn wWizh\"( fTxZnshY b^ Sng diUn ) dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma jZntiRn c[qe tR de zh[we , tRshu^ , y_uyc tR xiTnrSn bc zUi shaudUo dWgu_ diUnx]n g^ngsZ jiRnsh]hu] de ch^ngfYn x]nrYn ,c[zh[ sh] tR wWiyZ de xuTnzW"t_uzZrWn huRny[ng zhYxiUng xuRnbe , dWgu_ diUnx]n g^ngsZ de gdpiUo yZnc\ zUi fTlSnkYfcgdpiUo jiRoy] sh]chTng shUng zhTng bTifYnzhZsh[yZ y\shUng"su`ma zUi dWgu_ diUnx]n g^ngsZ b^ Sng z`ngbe zhUokRi jiRnsh]hu] tYbiW hu]y] zh^ng fRbiToyZ xiUng shVngm[ng , tR shu^ : 7 w` y\ yUoqic jiRnsh]hu] jiXchc w` de zh[we"8y_uyc liTng gY yuY hau jiRng jdx[ng dUxuTn , dUn liSnhWzhYngfd zUi m[n diUo zh^ng shVngwUngluahau , dWgu_ z`ngl\ shZ ruadW s]hb xZwUng zUi gdjiU xiUcua zh] xZn dZ sh[ , dWgu_ diUnx]ng^ngsZ shebTiwUn m[ng xiTo gdd^ng de zZjZn b]ng wYi xiRoshZ , Wr zhZch[ tR"dWgu_ diUnx]n gdjiU haulSi hu[ wXn , y\ sh[yZdiTnyZbR ^uyuSn zua sh^u , shUngzhTngbTifYnzhZbRdiTnwds]"dWgu_ cSizhYngbe huRny[ng su`ma c[zh[ de juWd]ng"Figure 1: An example of a Chinese news article (written in pinyin).LC1: {dWgu_, dWgu_, b^, dWgu_, dWgu_, dWgu_,dWgu_, b^, dWgu_, dWgu_, dWgu_}LC2:{jiRnsh]hu], fTxZnshY, jiRnsh]hu], z`ngbe,jiRnsh]hu], jiRnsh]hu]}LC3: {z`ngcSi, zhdx[, z`ngcSi, z`ngl\}LC4: {c[zh[, c[qe, c[zh[, c[zh[}LC5: {zhTng, xiUcua, shUngzhTng}LC6: {xuRnbe, xuRnbe, fRbiTo}Figure 2: Six lexical chains from the example in Figure1.accuracy by processing all possible combinations ofword senses in a text to disambiguate words.
Un-fortunately, their algorithm runs slowly in quadratictime.
Galley and Mckeown (2003) present an algo-rithm that are better than the former two algorithmsboth in terms of running efficiency and WSD accu-racy.
They separate the WSD sub-task from the taskof lexical chain building and impose a ?one senseper discourse?
constraint in the WSD step.4 Translating Documents Using LexicalChainsIn this section, we describe how we incorporate lex-ical cohesion into document-level machine transla-tion using lexical chains.
We divide the lexical chainbased document-level machine translation processinto three steps: (1) computing lexical chains forsource documents with a source language ontology,(2) generating target lexical chains from the com-puted source lexical chains, and finally (3) incorpo-rating lexical cohesion encoded in the generated tar-get lexical chains into document-level translation vialexical chain based cohesion models.
The remainderof this section will elaborate these three steps.4.1 Source Lexical Chains ComputationWe follow the chain computation algorithm intro-duced by Galley and McKeown (2003) to build lex-ical chains on source (Chinese) documents.
In thealgorithm, the chaining process includes three steps:choosing candidate words to build a disambiguationgraph (Galley and McKeown, 2003) for each doc-ument, disambiguating the candidate words and fi-nally building lexical chains over the disambiguatedcandidate words.The disambiguation graph can be considered asa representation of all possible interpretations of itscorresponding text.
In the graph, nodes are candi-date words with different senses and edges betweenword senses are weighted according to their seman-tic relations, such as synonym, hypernym and so on.We use an extended version of a Chinese thesaurusTongyici Cilin (Cilin for short) to define word sensesand semantic relations between senses.
The ex-1566level 1level 2level 3level 4level 5Figure 3: The architecture of the extended Cilin.
For sim-plicity, we only draw a binary tree to represent the hier-archical structure of Cilin.
This doesn?t mean that eachsemantic class at level i has only two sub-classes at leveli+ 1.
Actually, they have multiple sub-classes.tended Cilin contains 77,343 Chinese words, whichare organized in a hierarchical structure containing5 levels as shown in Figure 3.
In the 5th level, eachnode represents an atomic concept which consists ofa set of synonyms.
These atomic concepts are justlike synsets in WordNet.
We use them to representsenses of words in the disambiguation graph.We select nouns, verbs, abbreviations and idiomsas candidate words for the disambiguation graph.These words are identified by a Chinese part-to-speech tagger LTP (Che et al 2010) in a preprocess-ing step.
In order to build the disambiguation graph,we first build an array indexed by the atomic con-cepts of Cilin, then insert a copy of each candidateword into its all concept (sense) entries in the array.After that, we create all semantic links among sensesof different candidate words in the disambiguationgraph following Galley and McKeown (2003).In the second step, we use the principle of onesense per discourse to perform WSD for each can-didate word in the disambiguation graph.
We sumthe weights of all semantic links under the differentsenses of the candidate word in question.
The sensewith the highest sum of weights is considered as themost probable sense for this word.
We then assignthis sense to all occurrences of the word in the doc-ument by adopting the constraint of one sense perdiscourse.Once all candidate words are disambiguated, wecan build lexical chains over these words by remov-ing all semantic links that connect those unselectedword senses.
The six lexical chains shown in Fig-ure 2 are computed from the Chinese document inFigure 1 exactly following the algorithm of Galleyand McKeown (2003).
The only difference is thatwe use Cilin rather than WordNet as the ontology.4.2 Target Lexical Chains GenerationSince a faithful target document translation shouldfollow the same cohesion structure as that in its cor-responding source document, we generate target lex-ical chains from the computed source lexical chains.Given a source lexical chain LCs = {sji} where theith chain word sji is from the jth sentence of thesource document Ds, we generate a target lexicalchain LCt = {tji} using maximum entropy (Max-Ent) classifiers.
Particularly, we translate a word sjiin the source lexical chain into a target word tji inthe target lexical chain using a corresponding Max-Ent classifier as follows1.P (tji |C(sji )) =exp(?k ?kfk(tji , C(sji )))?t exp(?k ?kfk(t, C(sji )))(1)where fk are binary features, ?k are weights of thesefeatures, and C(sji ) is the surrounding context ofchain word sji .We train one MaxEnt classifier per unique sourcechain word.
For each classifier, we define twogroups of binary features: 1) the preceding andsucceeding two words of sji in the jth sentence({w?2, w?1, sji , w+1, w+2}); 2) the preceding andsucceeding one word of sji in the lexical chain LCs({spi?1, sji , sqi+1}).
All features are in the followingbinary form.f(tji , C(sji )) ={1, if tji = ?
and C(sji ).?
= ?0, else(2)where the symbol ?
is a placeholder for a possibletarget word, the symbol?
indicates a contextual ele-ment for the chain word sji (e.g., the preceding wordin the jth sentence or the succeeding word in thelexical chain LCs), and the symbol ?
represents thevalue of ?.Given a source document Ds and its N lexicalchains {LCks }Nk=1 computed from the document as1We collect training instances from word-aligned bilingualdata to train the MaxEnt classifier.1567described in Section 4.1, we can generate the Ntarget lexical chains {LCkt }Nk=1 using our MaxEntclassifiers.
Each target word tji in the target lexi-cal chain LCkt is the translation of its correspondingsource word sji in the source lexical chain LCks withthe highest probability P (tji |C(sji )) according to Eq.
(1).As we know, the MaxEnt classifier can gen-erate multiple translations for each source word.In order to incorporate these multiple chain wordtranslations, we can generate a super target lexi-cal chain LCt from a source lexical chain LCs,where  is a pre-defined threshold used to se-lect multiple translations.
For example, given asource lexical chain LCs = {a, b, c}, we canhave the corresponding super target lexical chainLCt = {{a1t , a2t ...}, {b1t , b2t ...}, {c1t , c2t ...}}, wherexit is the translation of x with a translation probabil-ity P (xit|C(x)) ?
 according to Eq.
(1).
Integrat-ing multiple translations for each source chain word,we can reduce the error propagation of the MaxEntclassifier to some extent.
Our experiments also con-firm that the super target lexical chains with multi-ple translation options for each chain word are betterthan the target lexical chains with only one transla-tion per chain word.
Therefore we build our cohe-sion models based on the super target lexical chains,which will be described in the next section.4.3 Lexical Chain Based Cohesion ModelsOnce we generate the super target lexical chains{LCkt }Nk=1 for the target document Dt, we can usethem to provide constraints for the target documenttranslation.
Our key interest is to make the targetdocument translation TDt as cohesive as possible.We therefore propose lexical chain based cohesionmodels to measure the cohesion of the target docu-ment translation.
The basic idea is to reward a trans-lation hypothesis if a word from the super target lexi-cal chains occurs in the hypothesis.
According to thedifference in the reward strategy, we have two cohe-sion models: a count cohesion model and a proba-bility cohesion model.Count Cohesion Model Mc(TDt , {LCkt }Nk=1):This model rewards a translation hypothesis of thejth sentence in the document whenever a lexicalchain word tji occurs in the hypothesis.
The modelmaintains a counter and accumulates the counterwhen necessary.
It is factorized into the sentencecohesion metric Mc(Tj , {LCkt }Nk=1), where Tj isthe translation of the jth sentence in the target docu-ment.
Mc(Tj , {LCkt }Nk=1) is formulated as follows.Mc(Tj , {LCkt }Nk=1) =?w?Tj?tji?Ce?
(w,tji ) (3)where C represents {LCkt }Nk=1, and the ?
functionis defined as follows.?
(w, tji ) ={1, if tji = w0, otherwise(4)Probability Cohesion ModelMp(TDt , {LCkt }Nk=1): This model rewards atranslation hypothesis according to the translationprobability of a chain word that occurs in thehypothesis.
The translation probability is computedby Eq.
(1).
The model is also factorized into thesentence cohesion metric Mp(Tj , {LCkt }Nk=1)which is formulated as follows.Mp(Tj ,{LCkt }Mk=1) =?w?Tj?tji?Ce?
(w,tji ) ?
P (tji |C(sji )) (5)where P (tji |C(sji ) is the translation probability com-puted according to Eq.
(1).4.4 DecodingThe proposed lexical chain based cohesion modelsare integrated into the log-linear translation frame-work of SMT as a cohesion feature.
Before translat-ing a source document, we compute lexical chainsfor the source document as described in Section 4.1.We then generate the super target lexical chains.
Inorder to efficiently calculate our lexical chain basedcohesion models, we reorganize words in the supertarget lexical chains into vectors.
We associate eachsource sentence Sj a vector to store target lexicalchain words that are to occur in the correspondingtarget sentence Tj .Although we still translate a source documentsentence by sentence, we capture the global cohe-sion structure of the document via lexical chains anduse the lexical chain based cohesion models to con-strain word selection in document translation.
Fig-ure 4 shows the architecture of an SMT system withthe lexical chain based cohesion model.1568Figure 4: Architecture of an SMT system with the lexicalchain based cohesion model.5 ExperimentsIn this section, we conducted a series of experimentsto validate the effectiveness of the proposed lexicalchain based cohesion models for Chinese-to-Englishdocument-level machine translation.
We used a hier-archical phrased-based SMT system (Chiang, 2007)trained on large-scale data.
In particular, we aim at:?
Measuring the impact of the threshold  on theprobability cohesion model and selecting thebest threshold on a development test set.?
Investigating the effect of the two lexical-chainbased cohesion models.?
Comparing our lexical chain based cohesionmodels against the previous lexical cohesiondevice based models (Xiong et al 2013).5.1 SetupWe collected our bilingual training data fromLDC, which includes the corpus LDC2002E18,LDC2003E07, LDC2003E14, LDC2004E12,LDC2004T07, LDC2004T08 (Only Hong KongNews), LDC2005T06 and LDC2005T10.
Thecollected bilingual training data contains 3.8Msentence pairs with 96.9M Chinese words and109.5M English words.
We trained a 4-gramlanguage model on the Xinhua portion of theEnglish Gigaword corpus (306 million words) viathe SRILM toolkit (Stolcke, 2002) with Kneser-Neysmoothing.Training MT05 MT06 MT08#Doc 103,236 100 79 109#Sent 2.80M 1,082 1,664 1,357#Chain 3.52M 1700 2172 1693#AvgC 35.72 17 27.49 15.53#AvgW 14.81 5.89 6.89 5.63Table 1: Statistics of the training, development and testsets, which show the number of documents (#Doc) andsentences (#Sent), the number of lexical chains extractedfrom the source documents (#Chain), the average numberof lexical chains per document (#AvgC) and the averagenumber of words per lexical chain (#AvgW).In order to build the lexical chain based cohesionmodels, we selected corpora with document bound-aries explicitly provided from the bilingual trainingdata together with the whole Hong Kong paralleltext corpus as the cohesion model training data2.
Weshow the statistics of these selected corpora in Table1.
They contain 103,236 documents and 2.80M sen-tences.
Averagely, each document consists of 28.4sentences.
From the source documents of the se-lected corpora, we extract 3.52M lexical chains.
Onaverage, there are 35.72 lexical chains per documentand 14.81 words per lexical chain.We used the off-the-shelf MaxEnt toolkit3 to trainone MaxEnt classifier per unique source lexicalchain word (61,121 different source chain words intotal).
We performed 100 iterations of the L-BFGSalgorithm implemented in the training toolkit foreach chain word with both Gaussian prior and eventcutoff set to 1 to avoid overfitting.
After event cutoff,we have an average of 17.75 different classes (targettranslations) per source chain word.We used the NIST MT05 as the tuning set for theminimum error rate training (MERT) [Och, 2003],the NIST MT06 as the development test set and theMT08 as the final test set.
The numbers of doc-uments/sentences in the NIST MT05, MT06 andMT08 are 100/1082, 79/1664 and 109/1357 respec-tively.
They contain 17, 27.49 and 15.53 lexicalchains per document respectively.We used the case-insensitive BLEU-4 (Papineni2The training data includes LDC2003E14, LDC2004T07,LDC2005T06, LDC2005T10 and LDC2004T08 (Hong KongHansards/Laws/News).3Available at: http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html1569 MT060.05 30.530.1 31.640.2 31.450.3 30.730.4 31.01Table 2: BLEU scores of the probability cohesionmodel Mp(TDt , {LCkt }Nk=1) with different values forthe threshold .et al 2002) as our evaluation metric.
As MERT isnormally instable, we ran the tuning process threetimes for all our experiments and presented the av-erage BLEU scores on the three MERT runs as sug-gested by Clark et al2011).5.2 Setting the Threshold As the two lexical chain based cohesion models arebuilt on the super target lexical chains that are asso-ciated with a parameter , we need to tune the thresh-old parameter  on the development test set NISTMT06.
We conducted a group of experiments usingthe probability cohesion model defined in Eq.
(5)to find the best threshold.
Experiment results areshown in Table 2.If we set the threshold too small (e.g., 0.05), thesuper target lexical chains may contain too manynoisy words that are not the translations of sourcelexical chain words, which may jeopardise the qual-ity of the super target lexical chains.
The cohesionmodel built on these noisy super target lexical chainsmay select incorrect words rather than the properlexical chain words.
On the other hand, if we set thethreshold too large (e.g., 0.3 or 0.4), we may takethe risk of not selecting the appropriate chain wordtranslations into the super target lexical chains.
Itseems that the best threshold is 0.1 as we obtainedthe highest BLEU score 31.64 on the NIST MT06with this threshold.
Therefore we set the threshold to 0.1 in all experiments thereafter.5.3 Effect of the Count and ProbabilityCohesion ModelAfter we found the best threshold, we carried out ex-periments to test the effect of the two lexical chainbased cohesion models: the count and probabilitycohesion model.
We compared them against theSystem MT06 MT08 AvgBaseline 30.43 23.32 26.88LexChainCount(top 1) 30.46 23.52 26.99LexChainCount 30.79 23.34 27.07LexChainProb 31.64 24.54 28.09Table 3: Effects of the lexical chain based count andprobability cohesion models.
LexChainCount: the countmodel defined in Eq.
(3).
LexChainProb: the probabilitymodel defined in Eq.
(5).baseline system that does not integrate any lexicalchain information.
We also compared the count co-hesion model (LexChainCount(top1)) built on thetarget lexical chains where each target chain word isthe best translation of its corresponding source lex-ical chain word according to Eq.
(1).
Experimentresults are shown in Table 3.From Table 3, we can observe that?
Our lexical chain based cohesion models areable to substantially improve the translationquality in terms of BLEU score.
We achievean average improvement of up to 1.21 BLEUpoints over the baseline on the two test setsMT06 and MT08.?
The count cohesion model built on the supertarget lexical chains is better than that basedon the target lexical chains only with top onetranslations (27.07 vs. 26.99).
This showsthe advantage of the super target lexical chains{LCkt }Nk=1 over the standard target lexical chi-ans {LCkt }Nk=1.?
Finally, the probability cohesion model is muchbetter than the count cohesion model (28.09vs.
27.07).
This suggests that we should takeinto account chain word translation probabili-ties when we reward hypotheses where targetlexical chain words occur.5.4 Lexical Chains vs. Lexical CohesionDevicesAs we have mentioned in Section 2, lexical cohe-sion devices can be also used to build lexical cohe-sion models to capture lexical cohesion relations in atext.
We therefore want to compare our lexical chainbased cohesion models with the lexical cohesion de-vice based cohesion models.1570System MT06 MT08 AvgBaseline 30.43 23.32 26.88LexDeviceTrigger 31.35 24.11 27.73LexChainProb 31.64 24.54 28.09Table 4: The lexical chain based probability cohesionmodel (LexChainProb) vs. the lexical cohesion devicebased trigger model (LexDeviceTrigger).We re-implemented the mutual information trig-ger model that is the best lexical cohesion modelbased on lexical cohesion devices among the threemodels proposed by Xiong et al(2013).
The mu-tual information trigger model measures the associ-ation strength of two lexical cohesion items x and yin a lexical cohesion relation xRy.
In the model, itis required that x occurs in a sentence preceding thesentence where y occurs and that the two items havea lexical cohesion relation such as word repetition,synonym.
The model treats x as the trigger and y asthe triggered item.
The mutual information betweenthe trigger x and the triggered item y estimates howpossible y will occur given x is mentioned in a text.The comparison results are reported in Table 4.Our lexical chain based probability cohesion modeloutperforms the lexical cohesion device based trig-ger model by 0.36 BLEU points.
The reason for thissuperiority of our cohesion model over the triggermodel may be that the former model captures lex-ical cohesion relations among sequences of wordsthrough lexical chains while the latter model cap-tures lexical cohesion relations only between two re-lated words.6 ConclusionsWe have presented two lexical chain based cohesionmodels that incorporate the lexical cohesion struc-ture of a text into document-level machine transla-tion.
We project the lexical chains of a source docu-ment to the corresponding target document by trans-lating each word in each source lexical chain intotheir counterparts via MaxEnt classifiers.
The pro-jected target lexical chains provide a representationof the lexical cohesion structure of the target doc-ument that is to be generated.
We build two co-hesion models based on the projected target lexi-cal chains: a count model that rewards a hypothesisaccording to the time of occurrence of target lexi-cal chain words in the hypothesis and a probabilitymodel that further takes translation probabilities intoaccount when rewarding hypotheses.
These two co-hesion models are used to constrain word selectionfor document translation so that the generated doc-ument is consistent with the projected lexical cohe-sion structure.We have integrated the two proposed cohesionmodels into a hierarchical phrase-based SMT sys-tem.
Experiment results on large-scale data validatethat?
The lexical chain based cohesion models areable to substantially improve translation qual-ity in terms of BLEU.?
The probability cohesion model is better thanthe count cohesion model.?
The lexical chain based probability cohesionmodel is better than the previous mutual infor-mation trigger model that adopts lexical cohe-sion devices to capture lexical cohesion rela-tions between two related words.As we mentioned in Section 2, cohesion is closelyconnected to coherence.
It provides a surface indi-cator for coherence identification (Barzilay and El-hadad, 1997).
In the future, we would like to uselexical chains to identify coherence and incorporateboth cohesion and coherence into document-levelmachine translation.ReferencesRegina Barzilay and Michael Elhadad.
1997.
Using lex-ical chains for text summarization.
In In Proceedingsof the ACL Workshop on Intelligent Scalable Text Sum-marization, pages 10?17.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Beata Beigman Klebanov and Michael Flor.
2013.
As-sociative texture is lost in translation.
In Proceedingsof the Workshop on Discourse in Machine Translation,pages 27?32, Sofia, Bulgaria, August.
Association forComputational Linguistics.Marine Carpuat and Dekai Wu.
2007a.
How phrasesense disambiguation outperforms word sense disam-biguation for statistical machine translation.
In Pro-ceedings of the 11th Conference on Theoretical and1571Methodological Issues in Machine Translation, pages43?52.Marine Carpuat and Dekai Wu.
2007b.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 61?72, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Marine Carpuat.
2009.
One translation per discourse.In Proceedings of the Workshop on Semantic Evalu-ations: Recent Achievements and Future Directions(SEW-2009), pages 19?27, Boulder, Colorado, June.Association for Computational Linguistics.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics, pages 33?40, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Wanxiang Che, Zhenghua Li, and Ting Liu.
2010.
Ltp:a chinese language technology platform.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics: Demonstrations, COLING ?10,pages 13?16, Stroudsburg, PA, USA.
Association forComputational Linguistics.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisti-cal machine translation: Controlling for optimizer in-stability.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 176?181, Port-land, Oregon, USA, June.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.
In Pro-ceedings of the workshop on Speech and Natural Lan-guage, Harriman, NY, February.Michel Galley and Kathleen McKeown.
2003.
Improv-ing word sense disambiguation in lexical chaining.
InProceedings of the 18th international joint conferenceon Artificial intelligence, IJCAI?03, pages 1486?1488,San Francisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011.Cache-based document-level statistical machine trans-lation.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 909?919, Edinburgh, Scotland, UK., July.Liane Guillou.
2013.
Analysing lexical consistency intranslation.
In Proceedings of the Workshop on Dis-course in Machine Translation, pages 10?18, Sofia,Bulgaria, August.
Association for Computational Lin-guistics.M.A.K Halliday and Ruqayia Hasan.
1976.
Cohesion inEnglish.
London: Longman.Christian Hardmeier, Joakim Nivre, and Jo?rg Tiedemann.2012.
Document-wide decoding for phrase-based sta-tistical machine translation.
In Proceedings of the2012 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning, pages 1179?1190, Jeju Island,Korea, July.Annie Louis and Ani Nenkova.
2012.
A coherencemodel based on syntactic patterns.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1157?1168, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indicator ofthe structure of text.
Comput.
Linguist., 17(1):21?48,March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.M.A.
Stairmand.
1996.
A computational analysis oflexical cohesion with applications in information re-trieval.
UMIST.Andreas Stolcke.
2002.
Srilm?an extensible languagemodeling toolkit.
In Proceedings of the 7th Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, Colorado, USA, September.Jo?rg Tiedemann.
2010.
Context adaptation in statisticalmachine translation using models with exponentiallydecaying cache.
In Proceedings of the 2010 Workshopon Domain Adaptation for Natural Language Process-ing, pages 8?15, Uppsala, Sweden, July.Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012.Encouraging consistent translation choices.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 417?426, Montre?al, Canada, June.Muriel Vasconcellos.
1989.
Cohesion and coherence inthe presentation of machine translation products.
InJames E.Alatis, editor, Geogetown University RoundTable on Languages and Linguistics 1989, pages 89?105, Washington, D.C. Georgetown University Press.Billy T. M. Wong and Chunyu Kit.
2012.
Extend-ing machine translation evaluation metrics with lexi-cal cohesion to document level.
In Proceedings of the15722012 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning, pages 1060?1068, Jeju Island,Korea, July.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.2011.
Document-level consistency verification in ma-chine translation.
In Proceedings of the 2011 MT sum-mit XIII, pages 131?138, Xiamen, China, September.Deyi Xiong and Min Zhang.
2013.
A topic-based co-herence model for statistical machine translation.
InProceedings of the Twenty-Seventh AAAI Conferenceon Artificial Intelligence (AAAI-13), Bellevue, Wash-ington, USA, July.Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,and Qun Liu.
2013.
Modeling lexical cohesion fordocument-level machine translation.
In Proceedingsof the Twenty-Third International Joint Conference onArtificial Intelligence (IJCAI-13), Beijing, China, Au-gust.1573
