Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 88?98,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPThe GREC Named Entity Generation Challenge 2009:Overview and Evaluation ResultsAnja Belz Eric KowNLT GroupUniversity of BrightonBrighton BN2 4GJ, UK{asb,eykk10}@bton.ac.ukJette ViethenCentre for LTMacquarie UniversitySydney NSW 2109jviethen@ics.mq.edu.auAbstractThe GREC-NEG Task at Generation Chal-lenges 2009 required participating sys-tems to select coreference chains for allpeople entities mentioned in short en-cyclopaedic texts about people collectedfrom Wikipedia.
Three teams submittedsix systems in total, and we additionallycreated four baseline systems.
Systemswere tested automatically using a range ofexisting intrinsic metrics.
We also eval-uated systems extrinsically by applyingcoreference resolution tools to the outputsand measuring the success of the tools.In addition, systems were tested in an in-trinsic evaluation involving human judges.This report describes the GREC-NEG Taskand the evaluation methods applied, givesbrief descriptions of the participating sys-tems, and presents the evaluation results.1 IntroductionThe GREC-NEG task is about how to generate ap-propriate references to people entities in the con-text of a piece of discourse longer than a sentence.Rather than requiring participants to generate re-ferring expressions (REs) from scratch, the GREC-NEG data provides sets of possible REs for selec-tion.
This was the first time we ran a shared taskusing this data.
GREC-NEG is a step further fromthe related GREC-MSR Task in that it requires sys-tems to generate plural as well as singular refer-ences, for all people entities mentioned in a text(GREC-MSR in contrast only had singular refer-ences to a single entity).
Moreover in GREC-NEG,possible REs for each entity are provided as one setfor each entity (rather than one set for each con-text), so the task of selecting an appropriate REfor a given context is harder than in GREC-MSR.The main aim for participating systems in GREC-NEG?09 was to select an appropriate type of RE(name, common noun, pronoun, or empty refer-ence).The immediate motivating application contextfor the GREC Tasks is the improvement of referen-tial clarity and coherence in extractive summariesand multiply edited texts (such as Wikipedia arti-cles) by regenerating REs contained in them.The motivating theoretical interest for the GRECTasks is to discover what kind of information isuseful in the input when making decisions aboutdifferent properties of referring expressions whensuch expressions are being generated in context(this is in contrast to most traditional referring ex-pression generation work in NLG which views theREG task as context-independent).The GREC-NEG data is derived from thenewly created GREC-People corpus which con-sists of 1,000 annotated introduction sections fromWikipedia articles in the category People.Nine teams from seven countries registered forthe GREC-NEG?09 Task, of which three teams ul-timately submitted six systems in total (see Ta-ble 1).
We also used the corpus texts themselvesas ?system?
outputs, and created four baseline sys-tems.
We evaluated the resulting 11 systems usinga range of intrinsic and extrinsic evaluation meth-ods.
This report presents the results of all evalu-ations (Section 6), along with descriptions of theGREC-NEG data (Sections 2) and task (Section 3),the test sets and evaluation methods (Section 4),and the participating systems (Section 5).Team System name(s)Univ.
Delaware UDel-NEG-1, UDel-NEG-2, UDel-NEG-3ICSI, Berkeley ICSI-CRFUniv.
Wolverhampton WLV-STAND, WLV-BIASTable 1: GREC-NEG?09 teams and systems.2 GREC-NEG DataThe GREC-NEG data is derived from the newlycreated GREC-People corpus which consists88of 1,000 annotated introduction sections fromWikipedia articles in the category People.
An in-troduction section was defined as the textual con-tent of a Wikipedia article from the title up to(and excluding) the first section heading, the ta-ble of contents or the end of the text, whichevercomes sooner.
Each text belongs to one of threesubcategories: inventors, chefs and early musiccomposers.
For the purposes of the GREC-NEG?09competition, the GREC-People corpus was dividedinto training, development and test data.
The num-ber of texts in the 3 data sets and 3 subdomains areas follows:All Inventors Chefs ComposersTotal 1,000 307 306 387Training 809 249 248 312Development 91 28 28 35Test 100 31 30 39In these texts we have annotated mentions of peo-ple by marking up the word strings that function asreferential expressions (REs) and annotating themwith coreference information as well as syntacticand semantic features.
The subject of each text is aperson, so there is at least one coreference chain ineach text.
The numbers of coreference chains (en-tities) in the 900 texts in the training/developmentsets are as shown in Table 2.
The texts vary greatlyin length, from 13 words to 935, with an averageof 128.98 words.2.1 Annotation of REs in GREC-PeopleThis section describes the different types of re-ferring expression (RE) that we annotated in theGREC-People corpus.
These manual annotationswere then automatically checked and converted tothe XML format described in Section 2.2 (whichencodes slightly less information, as explained be-low).
In terminology and the treatment of syntaxused in the annotation scheme and discussion of itin this report we rely heavily on The CambridgeGrammar of the English Language by Huddlestonand Pullum which we will refer to as CGEL forshort below (Huddleston and Pullum, 2002).In the example sentences below, (unbroken) un-derlines are used for referential expressions (REs)that are an example of the specific type of RE theyare intended to illustrate, whereas dashed under-lines are used for other annotated REs.
Corefer-ence between REs is indicated by subscripts i, j, ...immediately to the right of an underline (theirscope is one example sentence, i.e.
an i in one ex-ample sentence does not represent the same en-tity as an i in another example sentence).
Squarebrackets indicate supplements.
The syntactic com-ponent relativised by a relative pronoun is indi-cated by vertical bars.
Supplements and their an-chors (in the case of appositive supplements), andrelative clauses and the component they relativise(in the case of relative-clause supplements) are co-indexed by superscript x, y, ....
Dependents inte-grated in an RE are indicated by curly brackets.Supplements and dependents are highlighted inbold where they specifically are being discussed.In the XML format of the annotations, the be-ginning and end of a reference is indicated by<REF><REFEX>... </REFEX></REF> tags, andother properties discussed in the following sec-tions (e.g.
syntactic category) are encoded as at-tributes on these tags (for details see Section 2.2).For GREC-NEG?09 we decided not to transfer theannotations of integrated dependents and relativeclauses to the XML format.
Such dependentsare included within <REFEX>...</REFEX> annota-tions where appropriate, but without being markedup as separate constituents.2.1.1 Syntactic Category and FunctionThis section describes the types of REs we annotedin the GREC-People Corpus.I Subject NPs: referring subject NPs, includingpronouns and special cases of VP coordination:1.
Hei was born in Ramsay township, near Almonte, On-tario, Canada, the eldest son of |Scottish immigrants,{John Naismith and Margaret Young} |xj,k [whoj,k hadarrived in the area in 1851 and j,k worked in themining industry]x.2.
The Banu Musa brothersi,j,k were three 9th centuryPersian scholars, of Baghdad, active in the House ofWisdom.Ia Subjects of gerund-participials:1.
Hisi research on hearing and speech eventually culmi-nated in Belli being awarded the first U.S. patent forthe invention of the telephone in 1876.2.
Fessendeni used the alternator-transmitter to send outa short program from Brant Rock, which included hisiplaying the song O Holy Night on the violin and ireading a passage from the Bible.II Object NPs: referring NPs including pro-nouns that function as direct or indirect objects ofVPs and prepositional phrases; e.g.:1.
Many of the alpinists arrested with Vitaly Abalakoviwere executed.2.
Hei entrusted themj,k,l to Ishaq bin Ibrahimal-Mus?abixm , [a former governor of Baghdad]xm .89Entities 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23Texts 437 192 80 63 38 31 16 18 4 7 9 1 1 0 0 0 0 0 0 1 1 0 1Table 2: Numbers of person entities (hence coreference chains) in texts in the training/development data,e.g.
there are 38 texts which mention exactly 5 person entities.IIa Reflexive pronouns:1.
Smithi called himselfi the ?Komikal Konjurer?.III Subject-determiner genitives: genitive NPs(including genitive forms of pronouns) that func-tion as subject-determiners, i.e.
syntactic compo-nents that ?combine the function of determiner,marking the NP as definite, with that of comple-ment (more specifically subject).?
(CGEL, p. 56):1.
Theyi,j,k shared the 1956 Nobel Prize in Physics fortheiri,j,k invention.2.
On the eve of hisi death in 1605, the Mughal em-pire spanned almost 500 million acres (doubling dur-ing Akbar?si reign).Note that this category excludes lexicalised cases,e.g.
the so-called ?Newton?s method?.IIIa REs in composite nominals: this is theonly type of RE we have annotated that is not anNP, but a nominal.
This type functions as inte-grated attributive complement, e.g.:1.
The Eichengru?ni version was ignored by historians ...2.
The new act was a great success, largely despite thevarious things Blacktoni and Smithj were doing be-tween the Edisonk films.Note that this category too excludes lexicalisedcases, e.g.
the Nobel Prizes; the Gatling gun.2.1.2 Annotation of supplementsWe have annotated two kinds of supplements inthe GREC-People corpus, supplementary relativeclauses (CGEL, p. 1058), and appositive supple-ments.
The former is not transferred to the XMLannotation, for more information see (Belz, 2009).The following examples illustrate annotation ofappositive supplements (which are in bold):1.
John W. Campbell, Jr.xi[the editor of Astounding magazinei ]x.2.
was the eldest of the six children of Thomas Aspdinxi ,[a bricklayer living in the Hunslet district of Leedsi ]xIn the XML version, anchor and supple-ment are simply annotated as two (or occasion-ally three) independent, usually adjacent REs(REFEXs); the syntactic function of the second(and third) RE is marked as appositive supplement(SYNFUNC="app-supp").2.1.3 Further aspects of the annotationAs can be seen from some of the examples above,we annotated all embedded references.
Themaximum depth of embedding that occurs in theGREC-People corpus is 3.We annotated all plural REs that refer to groupsof people where the number of group members isknown.
For an explanation of our treatment ofREs that are coordinations of NPs, see the GREC-NEG?09 documentation (Belz, 2009).We have annotated all mentions of individualperson entities even if they are not actually namedanywhere in the text, and including cases of bothdefinite and indefinite references, e.g.:1.
The resolution?s sponsori described it as ...2.
... with the help of Robert Cailliauj anda {young} student staff {at CERN}k .2.2 XML AnnotationFigure 1 shows one of the XML-annotated textsfrom the GREC-NEG data.
Each such text con-sists of two initial lines of XML declarations fol-lowed by a GREC-ITEM.
A GREC-ITEM consists of aTEXT element followed by an ALT-REFEX element.A TEXT has one attribute (an ID unique within thecorpus), and is composed of one TITLE followedby any number of PARAGRAPHs.
A TITLE is just astring of characters.
A PARAGRAPH is any combi-nation of character strings and REF elements.The REF element indicates a reference, in thesense of ?an instance of referring?
(which could,in principle, be realised by gesture or graphically,as well as by a string of words, or a combination ofthese).
A REF is composed of one REFEX element(the ?selected?
referential expression for the givenreference; in the corpus texts it is the referentialexpression found in the corpus).The attributes of the REF element are ENTITY(entity identifier), MENTION (mention identifier),SEMCAT (semantic category), SYNCAT (syntacticcategory), and SYNFUNC (syntactic function).
Forfull details and ranges of values see (Belz, 2009).ENTITY and MENTION together constitute a uniqueidentifier for a reference within a text; together90<?xml version="1.0" encoding="utf-8"?><!DOCTYPE GREC-ITEM SYSTEM "genchal09-grec.dtd"><GREC-ITEM><TEXT ID="15"><TITLE>Alexander Fleming</TITLE><PARAGRAPH> <REF ENTITY="0" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="subj"><REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX></REF> (6 August 1881 - 11 March 1955) was a Scottish biologist and pharmacologist.<REF ENTITY="0" MENTION="2" SEMCAT="person" SYNCAT="np" SYNFUNC="subj"><REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX></REF> published many articles on bacteriology, immunology, and chemotherapy.<REF ENTITY="0" MENTION="3" SEMCAT="person" SYNCAT="np" SYNFUNC="subj-det"><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX></REF> best-known achievements are the discovery of the enzyme lysozyme in 1922 and the discoveryof the antibiotic substance penicillin from the fungus Penicillium notatum in 1928, for which<REF ENTITY="0" MENTION="4" SEMCAT="person" SYNCAT="np" SYNFUNC="subj"><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX></REF> shared the Nobel Prize in Physiology or Medicine in 1945 with<REF ENTITY="1" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj"><REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX></REF> and<REF ENTITY="2" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj"><REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX></REF>.</PARAGRAPH></TEXT><ALT-REFEX><REFEX ENTITY="0" REG08-TYPE="empty" CASE="no_case">_</REFEX><REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Fleming?s</REFEX><REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Sir Alexander Fleming?s</REFEX><REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX><REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="accusative">him</REFEX><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">who</REFEX><REFEX ENTITY="1" REG08-TYPE="empty" CASE="no_case">_</REFEX><REFEX ENTITY="1" REG08-TYPE="name" CASE="genitive">Florey?s</REFEX><REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX><REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="accusative">him</REFEX><REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="genitive">his</REFEX><REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">he</REFEX><REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">who</REFEX><REFEX ENTITY="2" REG08-TYPE="empty" CASE="no_case">_</REFEX><REFEX ENTITY="2" REG08-TYPE="name" CASE="genitive">Chain?s</REFEX><REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX><REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="accusative">him</REFEX><REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="genitive">his</REFEX><REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">he</REFEX><REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">who</REFEX></ALT-REFEX></GREC-ITEM>Figure 1: Example XML-annotated text from the GREC-NEG?09 data.with the TEXT ID, they constitute a unique iden-tifier for a reference within the entire corpus.A REFEX element indicates a referential expres-sion (a word string that can be used to refer to anentity).
The attributes of the REFEX element areREG08-TYPE (name, common, pronoun, empty), andCASE (nominative, accusative, etc.
).We allow arbitrary-depth embedding of refer-ences.
This means that a REFEX element may haveREF element(s) embedded in it.
See also next butone paragraph for embedding in REFEX elementsthat are contained in ALT-REFEX lists.The second (and last) component of aGREC-ITEM is an ALT-REFEX element whichis a list of REFEX elements.
For the GREC-NEG?09Task, these were obtained by collecting the set ofall REFEXs that are in the text, and adding severaldefaults including pronouns and other cases (e.g.genitive) of REs already in the list.REF elements that are embedded in REFEX ele-ments contained in an ALT-REFEX list have an un-specified MENTION id (the ???
value).
Furthermore,such REF elements have had their enclosed REFEXremoved.
For example:<ALT-REFEX>...<REFEX ENTITY="2" REG08-TYPE="common" CASE="plain">a friend of <REF ENTITY="1" MENTION="?"
SEMCAT="person" SYNCAT="np" SYNFUNC="obj"></REF></REFEX>...</ALT-REFEX>3 The GREC-NEG TaskThe test data inputs were identical to the train-ing/development data (Figure 1), except that REFelements in the test data do not contain a REFEXelement, i.e.
they are ?empty?.
The task for par-ticipating systems is to select one REFEX from theALT-REFEX list for each REF in each TEXT in thetest sets.
If the selected REFEX contains an em-91bedded REF then participating systems also needto select a REFEX for this embedded REF and to setthe value of its MENTION attribute.
The same ap-plies to all further embedded REFEXs, at any depthof embedding.4 Evaluation ProceduresThe GREC-NEG data set was divided into training,development and test data.
We performed eval-uations on the test data, using a range of differentevaluation methods, including intrinsic and extrin-sic, automatically assessed and human-evaluated,as described in the following sections.Participants computed evaluation scores on thedevelopment set, using the geval-2.0.pl codeprovided by us which computes Word String Ac-curacy, REG?08-Type Recall and Precision, string-edit distance and BLEU.4.1 Test setsWe created two versions of the test data for theGREC-NEG Task:1.
GREC-NEG Test Set 1a: randomly selected 10% subset(100 texts) of the GREC-People corpus (with the sameproportion of texts in the 3 subdomains as in the train-ing/development data).2.
GREC-NEG Test Set 1b: the same subset of texts as in(1a); for this set we did not use the REs in the corpus,but replaced each of them with human-selected alterna-tives obtained in an online experiment as described in(Belz and Varges, 2007); this test set therefore containsthree versions of each text where all the REFEXs in agiven version were selected by one ?author?.Test Set 1a has a single version of each text, andthe scoring metrics below that are based on count-ing matches (Word String Accuracy counts match-ing word strings, REG08-Type Recall/Precisioncount matching REG08-Type attribute values)simply count the number of matches a systemachieves against that single text.Test Set 1b, however, has three versions of eachtext, so the match-based metrics first calculate thenumber of matches for each of the three versionsand then use (just) the highest number of matches.4.2 Automatic intrinsic evaluationsThe chief humanlikeness measures we computedwere REG08-Type Recall and Precision.
REG08-Type Precision is defined as the proportion ofREFEXs selected by a participating system whichmatch the reference REFEXs (where match countsare obtained as explained in the preceding sec-tion).
REG08-Type Recall is defined as the propor-tion of reference REFEXs for which a participatingsystem has produced a match.The reason why we use REG08-Type Recall andPrecision for GREC-NEG rather than REG08-TypeAccuracy as in GREC-MSR is that in GREC-NEG(unlike in GREC-MSR) there may be a differentnumber of REFEXs in system outputs and the ref-erence texts in the test set (because there are em-bedded references in GREC-People, and systemsmay select REFEXs with or without embedded ref-erences for any given REF).We also computed String Accuracy, defined asthe proportion of word strings selected by a par-ticipating system that match those in the referencetexts.
This was computed on complete, ?flattened?word strings contained in the outermost REFEX i.e.embedded REFEX word strings were not consideredseparately.We also computed BLEU-3, NIST, string-editdistance and length-normalised string-edit dis-tance, all on word strings defined as for String Ac-curacy.
BLEU and NIST are designed for multipleoutput versions, and for the string-edit metrics wecomputed the mean of means over the three text-level scores (computed against the three versionsof a text).
For details, see GREC-MSR report inthis volume.4.3 Human-assessed intrinsic evaluationsGiven that the motivating application context forthe GREC-NEG Task is improving referential clar-ity and coherence in multiply edited texts, wedesigned the human-assessed intrinsic evaluationas a preference-judgment test where subjects ex-pressed their preference, in terms of two criteria,for either the original Wikipedia text or the versionof it with system-generated referring expressionsin it.
The intrinsic human evaluation involved out-puts for 30 randomly selected items from the testset from 5 of the 6 participating systems,1 the fourbaselines and the original corpus texts (10 systemsin total).
We used a Repeated Latin Squares de-sign which ensures that each subject sees the samenumber of outputs from each system and for eachtest set item.
There were three 10x10 squares, anda total of 600 individual judgments in this evalu-ation (60 per system: 2 criteria x 3 articles x 101We left out UDel-NEG-1 given our limited resources andthe fact that this is a kind of baseline system.92Figure 2: Example of text pair presented in human intrinsic evaluation of GREC-NEG systems.evaluators).
We recruited 10 native speakers ofEnglish from among students currently complet-ing a linguistics-related degree at Kings CollegeLondon and University College London.Following detailed instructions, subjects didtwo practice examples, followed by the 30 textsto be evaluated, in random order.
Subjects car-ried out the evaluation over the internet, at a timeand place of their choosing.
They were allowed tointerrupt and resume the experiment (though dis-couraged from doing so).Figure 2 shows what subjects saw during theevaluation of an individual text pair.
The place(left/right) of the original Wikipedia article wasrandomly determined for each individual evalua-tion of a text pair.
People references are high-lighted in yellow/orange, those that are identicalin both texts are yellow, those that are differentare orange.
The evaluator?s task is to express theirpreference in terms of each quality criterion bymoving the slider pointers.
Moving the slider tothe left means expressing a preference for the texton the left, moving it to the right means preferringthe text on the right; the further to the left/right theslider is moved, the stronger the preference.
Thetwo criteria were explained in the introduction asfollows (the wording of the first is from DUC):1.
Referential Clarity: It should be easy to identify whothe referring expressions are referring to.
If a personis mentioned, it should be clear what their role in thestory is.
So, a reference would be unclear if a personis referenced, but their identity or relation to the storyremains unclear.2.
Fluency: A referring expression should ?read well?,i.e.
it should be written in good, clear English, and theuse of titles and names should seem natural.
Note thatthe Fluency criterion is independent of the ReferentialClarity criterion: a reference can be perfectly clear, yetnot be fluent.It was not evident to the evaluators that slid-ers were associated with numerical values.
Sliderpointers started out in the middle of the scale (nopreference).
The values associated with the pointson the slider ranged from -10.0 to +10.0.4.4 Extrinsic automatic evaluationAn evaluation we piloted in REG?08 was an auto-matic approach to extrinsic evaluation (for a moredetailed description, see the GREC-MSR results re-port elsewhere in this volume).
The basic premiseis that poorly chosen reference chains seem likelyto affect the reader?s ability to resolve REs.
In ourautomatic extrinsic method, the role of the readeris played by an automatic coreference resolutiontool and the expectation is that the tool performsworse (is less able to identify coreference chains)with more poorly chosen referential expressions.93To counteract the possibility of results being afunction of a specific coreference resolution algo-rithm or tool, we used two different resolvers?those included in LingPipe2 and OpenNLP (Mor-ton, 2005)?and averaged results.
For the samereason we used three different performance mea-sures: MUC-6 (Vilain et al, 1995), CEAF (Luo,2005), and B-CUBED (Bagga and Baldwin, 1998).5 SystemsBase-rand, Base-freq, Base-1st, Base-name:We created four baseline systems each with adifferent way of selecting a REFEX from thoseREFEXs in the ALT-REFEX list that have match-ing entity IDs.
Base-rand selects a REFEX at ran-dom.
Base-1st selects the first REFEX.
Base-freqselects the first REFEX with a REG08-TYPE thatis the overall most frequent (as determined fromthe training/development data) given the SYNCAT,SYNFUNC and SEMCAT of the reference.
Base-name selects the shortest REFEX with attributeREG08-TYPE=name.UDel: The UDel-NEG-1 system is identical tothe UDel system that was submitted to the GREC-MSR Task (for a description of that system seeGREC-MSR?09 results report in this volume), ex-cept that it was adapted to the different data for-mat of GREC-NEG.
UDel-NEG-2 is identical toUDel-NEG-1 except that it was retrained on GREC-NEG data and the feature set was extended by en-tity and mention IDs.
UDel-NEG-3 additionallyutilised improved identification of other entities.ICSI-CRF: The ICSI-CRF system construes theGREC-MSR task as a sequence labelling task anddetermines the most likely current class labelgiven preceding labels using a Conditional Ran-dom Field model trained using the follow featuresfor the current reference, the most recent preced-ing reference, and the most recent reference to thesame entity: preceding and following word uni-gram and bigram; suffix of preceding and follow-ing word; preceding and following punctuation;reference ID; and whether this is the beginning ofa paragraph.
If more than one class label remains,the last in the list of possible REs in the GREC-MSRdata is selected.WLV: The WLV systems start with sentencesplitting and POS tagging.
WLV-STAND then em-2http://alias-i.com/lingpipe/ploys a J48 decision tree classifier to obtain a prob-ability for each REF/REFEX pair that it is a goodpair in the current context.
The context is repre-sented by the following set of features.
Featuresof the REFEX word string: is it the longest of thepossible REFEXs; number of words; all REFEX fea-tures supplied in GREC-NEG data.
Features of theREF: is it part of the first chain in the text; is it thefirst mention of the entity; is it at the beginning ofthe sentence; all REF features supplied in GREC-NEG data.
Other features: do the preceding wordsmatch ?, but?, ?and then?
and similar phrases; dis-tance in sentences to last mention; REG08-Typeselected for the two preceding REFs; POS tags of4 words before and 3 words after; correlation be-tween SYNFUNC and CASE values; size of the chain.WLV-BIAS is the same except that it is retrainedon reweighted training instances.
The reweightingscheme assigns a cost of 3 to false negatives and 1to false positives.6 ResultsThis section presents the results of all the evalua-tion methods described in Section 4.
We start withREG08-Type Precision and Recall, the intrinsic au-tomatic metrics which participating teams weretold was going to be the chief evaluation method,followed by Word String Accuracy and other in-trinsic automatic metrics (Section 6.2), the intrin-sic human evaluation (Section 6.3) and the extrin-sic automatic evaluation (Section 6.4).System REG08-Type WS Acc.
Norm.
SERecall PrecisionICSI-CRF 83.05 83.05 0.786 0.197WLV-BIAS 77.61 80.26 0.735 0.239UDelNEG-3 75.27 75.27 0.333 0.636UDelNEG-2 74.95 74.95 0.323 0.646UDelNEG-1 68.87 68.87 0.315 0.658WLV-STAND 66.20 68.46 0.626 0.351Table 5: Self-reported evaluation scores for devel-opment set.6.1 REG08-Type Precision and RecallParticipants computed scores for the developmentset (91 texts) themselves, using the geval evalua-tion tool provided by us.
These scores are shownin Table 5, and are also included in the partici-pants?
reports elsewhere in this volume.3REG08-Type Recall and Precision results forTest Set 1a are shown in column 2 of Table 3.As would be expected, results on the test data are3 ICSI-CRF scores obtained directly from ISCI team.94SystemREG08-Type Precision and Recall Scores against Corpus (Test Set 1a)All Chefs Composers InventorsPrecision Recall R P R P R PICSI-CRF 79.12 A 76.92 A 70.01 73.54 78.11 80.18 80.05 81.86WLV-BIAS 73.77 B 72.70 A 69.82 71.52 73.53 74.38 73.65 74.56WLV-STAND 64.49 C 63.55 B 58.28 59.70 65.38 66.14 64.78 65.59Base-freq 61.52 C 59.6 B 49.41 51.86 63.95 65.74 60.59 62.12UDel-NEG-2 53.21 D 51.14 C 44.38 47.17 50.50 52.22 57.88 59.80UDel-NEG-3 52.49 D 50.45 C 43.49 46.23 49.79 51.48 57.39 59.29UDel-NEG-1 50.47 D 48.51 C 42.90 45.60 47.78 49.41 54.43 56.23Base-rand 43.32 E 42.00 D 38.76 40.43 41.77 43.00 45.07 46.21Base-name 40.60 E 39.09 D 44 97 47.80 39.06 40.32 34.24 35.28Base-1st 10.99 F 10.81 E 12.43 12.73 9.30 9.43 12.07 12.22Table 3: REG08-Type Precision and Recall scores against corpus version of Test Set for complete set andfor subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.SystemREG08-Type Precision and Recall Scores against human topline (Test Set 1b)All Chefs Composers InventorsPrecision Recall R P R P R PCorpus 82.67 A 84.01 A 84.24 82.25 84.47 83.26 83.04 82.02ICSI-CRF 79.33 A B 78.38 B 76.36 77.54 78.81 79.74 79.30 80.10WLV-BIAS 77.78 B 77.78 B 77.58 77.58 77.86 77.86 77.81 77.81WLV-STAND 67.51 C 67.51 C 65.76 65.76 68.60 68.60 67.08 67.08Base-freq 65.38 C 64.37 C 58.48 59.94 68.07 68.97 62.84 63.64UDel-NEG-2 57.39 D 56.06 D 55.15 57.23 54.86 55.92 58.85 60.05UDel-NEG-3 57.25 D 55.92 D 55.76 57.86 54.57 55.62 58.35 59.54Base-name 55.22 D 54.01 D 54.24 56.29 57.04 58.05 48.63 49.49UDel-NEG-1 53.57 D 52.32 D E 51.21 53.14 50.80 51.78 55.86 57.00Base-rand 48.46 E 47.75 E 47.88 48.77 46.44 47.13 49.88 50.51Base-1st 12.54 F 12.54 F 13.94 13.94 10.45 10.45 14.96 14.96Table 4: REG08-Type Recall and Precision scores against human topline version of Test Set for completeset and for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.somewhat worse (than on the development data).Also included in this table are results for the 4baseline systems, and it is clear that selecting themost frequent RE type given SEMCAT, SYNFUNC andSYNCAT (as done by the Base-freq system) pro-vides a strong baseline for RE type selection.The last 6 columns in Table 3 contain Recall (R)and Precision (P) results for the three subdomains.For most of the systems results are slightly betterfor Inventors than for Composers, and better forComposers than for Chefs.
A contributing factorto this may be the fact that texts in Chefs tend tobe much more colloquial.
Base-1st has by far theworst results; this is because it selects the emptyreference in almost all cases (because ALT-REFEXlists are sorted and if a list contains an empty ref-erence it will end up at the beginning).We carried out univariate ANOVAs with Sys-tem as the fixed factor, and ?Number of REFEXsin a text?
as a random factor, and REG08-Type Re-call as the dependent variable in one ANOVA, andREG08-Type Precision in the other.
The result forRecall was F(10,704) = 81.547, p < 0.001.4 Theresult for Precision was F(10,722) = 79.359, p <0.001.
The columns containing capital letters inTable 3 show the homogeneous subsets of systems4We included the corpus texts themselves in the analysis,hence 10 degrees of freedom (11 systems).as determined by a post-hoc Tukey HSD analysis.Systems whose scores are not significantly differ-ent (at the .05 level) share a letter.Table 4 shows analogous results computedagainst Test Set 1b (which has three versions ofeach text).
These should be considered as thechief results of the GREC-NEG?09 Task evalua-tions, as stated in the participants?
guidelines.
Ta-ble 4 includes results for the corpus texts, com-puted (as are results for the system outputs in Ta-ble 4) against the three versions of each text in TestSet 1b.
We performed univariate ANOVAs withSystem as the fixed factor, Number of REFEXs asa random factor, and Recall as the dependent vari-able in one, and Precision in the other.
The resultfor Recall was F(10,724) = 72.528, p < .001),and for Precision F(10,722) = 75.476, p < .001.For both cases, we compared the mean scores withTukey?s HSD.
As can be seen from the resultinghomogeneous subsets (letter columns in Table 4),system ranks are the same for Precision and forRecall.
In terms of Precision, the difference be-tween the corpus texts and the ICSI-CRF systemwas not significant.6.2 Other automatic intrinsic metricsIn addition to the chief evaluation measure re-ported on in the preceding section, we computed95SystemString similarity against Corpus (Test Set 1a)Word String AccuracyBLEU-3 NIST SE norm.
SEAll Chefs Composers InventorsICSI-CRF 74.84 A 68.24 76.63 77.10 0.75 5.78 0.70 0.23WLV-BIAS 68.57 B 66.35 69.08 69.47 0.76 5.62 0.82 0.29WLV-STAND 59.55 C 54.72 61.24 60.56 0.73 5.34 1.01 0.39Base-name 28.48 D 35.53 27.51 24.43 0.5 4.09 1.80 0.67UDel-NEG-1 16.58 E 20.13 15.09 16.28 0.43 2.47 2.1 0.82UDel-NEG-2 16.44 E 19.81 14.79 16.54 0.45 2.37 2.08 0.83UDel-NEG-3 16.37 E 19.18 15.09 16.28 0.45 2.41 2.08 0.83Base-rand 8.22 F 8.49 7.10 9.92 0.17 0.9 2.43 0.89Base-1st 7.28 F 7.23 6.36 8.91 0.16 0.98 2.54 0.90Base-freq 2.52 G 4.40 2.37 1.27 0.31 1.91 2.34 0.90Table 6: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set 1a (systemsin order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracyonly.SystemString similarity against human topline (Test Set 1b)Word String AccuracyBLEU-3 NIST SE norm.
SEAll Chefs Composers InventorsCorpus 81.90 A 83.33 82.25 80.15 0.95 7.15 0.71 0.25ICSI-CRF 74.55 B 71.70 75.15 75.83 0.86 6.35 0.92 0.31WLV-BIAS 69.07 C 69.50 68.49 69.72 0.88 6.17 1.03 0.36WLV-STAND 59.70 D 58.18 60.36 59.80 0.84 5.81 1.21 0.45Base-name 37.27 E 42.14 36.83 34.10 0.65 5.57 1.73 0.63UDel-NEG-1 19.25 F 22.96 17.60 19.08 0.51 2.62 2.17 0.82UDel-NEG-2 18.96 F 22.96 17.31 18.58 0.53 2.42 2.15 0.83UDel-NEG-3 18.89 F 22.64 17.75 17.81 0.53 2.49 2.15 0.82Base-rand 10.45 G 10.06 9.91 11.70 0.25 1.11 2.49 0.89Base-1st 8.65 G 8.49 7.54 10.69 0.24 1.29 2.64 0.92Base-freq 3.24 H 4.40 3.55 1.78 0.39 2.1 2.40 0.90Table 7: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set 1b (systemsin order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy.Word String Accuracy and the other string simi-larity metrics described in Section 4.2.
The result-ing scores for Test Set 1a (the corpus texts) areshown in Table 6.
Ranks for peer systems rela-tive to each other are very similar to the resultsreported in the last section.
However, the ranks ofthe baseline systems have changed substantially,both in relation to each other and to the peer sys-tems.
In particular, Base-freq has moved all theway down to the bottom of the table.
The rea-son is that this method is geared towards select-ing the correct type of RE, but pays no attentionto whether it selects a syntactically appropriate REfor the given context, instead simply selecting thefirst RE from the ALT-REFEX list that has the se-lected type; in the GREC-NEG?09 Task (unlike theGRE-MSR task) this just happens to be an RE inthe genitive case most of the time which is over-all rarer than nominative/plain.
It is likely that theWord String scores for the UDel-NEG systems arelow for a similar reason.We performed a univariate ANOVA with Systemas the fixed factor and Number of REFEXs as arandom factor and Word String Accuracy as thedependent variable.
The result for System wasF(10,726) = 103.339; the homogeneous subsets re-sulting from the Tukey HSD post-hoc analysis areshown in columns 3?9 of Table 6.Table 7 shows analogous results for humantopline Test Set 1b (which has three versions ofeach text).
We carried out the same kind of ANOVAas for Test Set 1a; the result for System on WordString Accuracy was F(10,726) = 106.755, p <0.001.
System rankings and homogeneous sub-sets are the same as for Test Set 1a; scores acrossthe board are somewhat higher, because of the wayscores are computed for Test Set 1b: it is the high-est score a system achieves (at text-level) againstany of the three versions of a test set text that istaken into account.Results for BLEU-3, NIST and the two string-edit distance metrics are shown in the rightmost 4columns of Tables 6 and 7.
Systems whose WordString Accuracy scores differ significantly are as-signed the same ranks by NIST and the two string-edit distance metrics as by Word String Accuracy(except for Base-1st and Base-freq which swapranks in some.
BLEU-3 does the same and alsoflips ICSI-CRF and WLV-BIAS.6.3 Human-assessed intrinsic measuresIn the human intrinsic evaluation, evaluators ratedsystem outputs in terms of whether they preferredthem over the original Wikipedia texts.
As a re-96Clarity FluencySystem Mean + 0 ?
System Mean + 0 ?Corpus 0 A 0 30 0 Corpus 0 A 0 30 0ICSI-CRF -1.447 A B 3 17 10 ICSI-CRF -0.353 A 9 14 7WLV-BIAS -2.437 A B C 3 14 13 WLV-BIAS -2.257 A B 2 14 14Base-name -2.583 B C 7 7 16 WLV-STAND -5.823 B C 1 3 26WLV-STAND -4.477 C D 1 9 20 Base-name -4.257 C D 2 5 23UDelNEG-3 -6.427 D E 1 4 26 UDelNEG-3 -6.263 C D E 1 3 26UDelNEG-2 -6.667 D E 1 3 26 UDelNEG-2 -7.13 D E 0 3 27Base-rand -8.183 E F 0 1 29 Base-rand -7.513 D E 0 0 30Base-freq -8.26 E F 0 0 30 Base-freq -7.57 D E 0 0 30Base-1st -9.357 F 0 0 30 Base-1st -8.477 E 0 0 30Table 8: Results for Clarity and Fluency preference judgement experiment.
Mean = mean of individualscores (where scores ranged from -10.0 to + 10.0); + = number of times system was preferred; ?
=number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was preferred.sult of the experiment we had for each system andeach evaluation criterion a set of scores rangingfrom -10.0 to +10.0, where 0 meant no prefer-ence, negative scores meant a preference for theWikipedia text, and positive scores a preferencefor the system-produced text.The second column of the left half of Table 8summarises the Clarity scores for each system interms of their mean; if the mean is negative theevaluators overall preferred the Wikipedia texts,if it is positive evaluators overall preferred thesystem.
The more negative the score, the morestrongly evaluators preferred the Wikipedia texts.Columns 9-11 show corresponding counts of howmany times each system was preferred (+), dis-preferred (?
), and neither (0), when compared toWikipedia.The other half of Table 8 shows correspondingresults for Fluency.We ran a factorial multivariate ANOVA with Flu-ency and Clarity as the dependent variables.
In thefirst version of the ANOVA, the fixed factors wereSystem, Evaluator and Wikipedia Side (indicatingwhether the Wikipedia text was shown on the leftor right during evaluation).
This showed no signif-icant effect of Wikipedia Side on either Fluency orClarity, and no significant interaction between anyof the factors.
There was however a mild effect ofEvaluator on both Fluency and Clarity.
We ran theANOVA again, this time with just System and Eval-uator as fixed factors.
The result for System onFluency was F(9,200) = 37.925, p < .001, and forSystem on Clarity it was F(9,200) = 35.439, p <.001.
Post-hoc Tukey?s HSD tests revealed the sig-nificant pairwise differences indicated by the lettercolumns in Table 8.Correlation between individual Clarity and Flu-ency ratings as estimated with Pearson?s coeffi-cient was r = 696, p < .01, indicating that thetwo criteria covary to some extent.Apart from Base-name and WLV-STANDswitching places, system ranks are the same forFluency and Clarity.
Moreover, system ranksare very similar to those produced by the string-similarity scores above.
Perhaps the most strikingresult is that the ICSI-CRF system does succeedin improving Fluency compared to the originalWikipedia texts: it is preferred 9 times whereasthe Wikipedia texts are preferred only 7 times.System (MUC+CEAF+B3)/3 M C B3WLV-BIAS 62.64 A 57 62 69ICSI-CRF 61.28 A B 53 61 69Base-name 61.11 A B 55 61 68Corpus 59.56 A B C 53 59 67UDel-NEG-3 56.13 B C D 48 56 65UDel-NEG-2 55.9 B C D 47 55 65Base-freq 55.85 B C D 47 56 65UDel-NEG-1 54.79 C D 46 54 64WLV-STAND 51.69 D 41 53 61Base-rand 34.86 E 15 38 51Base-1st 26.36 F 2 31 46Table 9: MUC, CEAF and B-CUBED F-Scores forall systems; homogeneous subsets (Tukey HSD),alpha = .05, for mean of F-Scores.6.4 Automatic extrinsic measuresWe fed the outputs of all 11 systems through thetwo coreference resolvers, and computed meanMUC, CEAF and B-CUBED F-Scores as describedin Section 4.4.
The second column in Table 9shows the mean of means of these three F-Scores,to give a single overall result for each of for thisevaluation method.
A univariate ANOVA with(text-level) mean F-Score as the dependent vari-able and System as the single fixed factor revealeda significant main effect of System on mean F-Score (F(10,1089) = 91.634, p < .001).
A post-hoc comparison of the means (Tukey HSD, alpha= .05) found the significant differences indicatedby the homogeneous subsets in columns 3?8 (Ta-ble 9).
The numbers in the last three columns arethe separate MUC, CEAF and B-CUBED F-Scores97for each system, averaged over the two resolvertools (and rounded for reasons of space.7 Concluding RemarksThis was the first time the GREC-NEG Task wasrun.
It is a new task not only for an NLG shared-task challenge, but also as a research task in gen-eral (post-processing extractive summaries in or-der to improve their quality seems to be just takingoff as a research subfield).
There was substantialinterest in the GREC-NEG Task (as indicated by thenine teams that originally registered).
However,only 3 teams were ultimately able to submit a sys-tem.In particular because of the inclusion of pluralreferences, multiple entities per text and embed-ded references, the GREC-NEG Task has a higherentrance level than the GREC-MSR Task.
We areplanning to run it again at Generation Challenges2010 next year, and are considering the possibilityof providing participants with a baseline systemwhich would help e.g.
with processing embeddedreferences.We are also planning to add a named entityrecognition preprocessing task, so that this newtask in combination with GREC-NEG can be usedto perform end-to-end post-processing of extrac-tive summaries (and other types of multiply editedtexts) to improve the clarity and fluency of the re-ferring expressions in them.AcknowledgmentsMany thanks to the members of the Corpora andSIGGEN mailing lists, and Brighton Universitycolleagues who helped with the online MSRE se-lection experiments for GREC-NEG test set 1b.Thanks are also due to the Kings College Lon-don and University College London students whohelped with the intrinsic evaluation experiment.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scor-ing coreference chains.
In Proceedings of the Lin-guistic Coreference Workshop at LREC?98, pages563?566.A.
Belz and S. Varges.
2007.
The GREC corpus:Main subject reference in context.
Technical ReportNLTG-07-01, University of Brighton.A.
Belz, 2009.
GREC Named Entity Generation Chal-lenge 2009: Participants?
Pack.R.
Huddleston and G. Pullum.
2002.
The CambridgeGrammar of the English Language.
Cambridge Uni-versity Press.X.
Luo.
2005.
On coreference resolution performancemetrics.
Proc.
of HLT-EMNLP, pages 25?32.T.
Morton.
2005.
Using Semantic Relations to ImproveInformation Retrieval.
Ph.D. thesis, University ofPensylvania.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic corefer-ence scoring scheme.
Proceedings of MUC-6, pages45?52.98
