Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 97?105,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsEffects of Meaning-Preserving Corrections on Language LearningDana Angluin ?Department of Computer ScienceYale University, USAdana.angluin@yale.eduLeonor Becerra-BonacheLaboratoire Hubert CurienUniversite?
de Saint-Etienne, Franceleonor.becerra@univ-st-etienne.frAbstractWe present a computational model of lan-guage learning via a sequence of interactionsbetween a teacher and a learner.
Experimentslearning limited sublanguages of 10 naturallanguages show that the learner achieves ahigh level of performance after a reasonablenumber of interactions, the teacher can pro-duce meaning-preserving corrections of thelearner?s utterances, and the learner can de-tect them.
The learner does not treat correc-tions specially; nonetheless in several cases,significantly fewer interactions are needed bya learner interacting with a correcting teacherthan with a non-correcting teacher.1 IntroductionA child learning his or her native language typicallydoes so while interacting with other people who areusing the language to communicate in shared situ-ations.
The correspondence between situations andutterances seems likely to be a very important sourceof information for the language learner.
Once achild begins to produce his or her own utterances,other people?s responses to them (or lack thereof)are another source of information about the lan-guage.
When the child?s utterances fall short ofadult-level competence, sometimes the other personin the conversation will repeat the child?s utterancein a more correct form.
A number of studies havefocused on the phenomenon of such corrections andquestions of their frequency in child-directed speech?Research supported by the National Science Foundation,Grant CCF-0916389.and whether children can and do make use of them;some of these studies are discussed in the next sec-tion.In this paper we construct a computational modelwith a learner and a teacher who interact in a se-quence of shared situations.
In each situation theteacher and learner interact as follows.
First thelearner uses what it has learned about the languageto (attempt to) generate an utterance appropriate tothe situation.
The teacher then analyzes the correct-ness of the learner?s utterance and either generatesan utterance intended as a correction of the learner?sutterance, or generates another utterance of its ownappropriate to the situation.
Finally, the learner usesinformation given by its own utterance, the teacher?sutterance and the situation to update its knowledgeof the language.
At the conclusion of this interac-tion, a new interaction is begun with the next situa-tion in the sequence.Both the learner and the teacher engage in com-prehension and production of utterances which areintended to be appropriate to their shared situation.This setting allows us to study several questions:whether the teacher can offer meaningful correc-tions to the learner, whether the learner can detectintended corrections by the teacher, and whether thepresence of corrections by the teacher has an ef-fect on language acquisition by the learner.
For ourmodel, the answer to each of these questions is yes,and while the model is in many respects artificial andsimplified, we believe it sheds new light on these is-sues.
Additional details are available (Angluin andBecerra-Bonache, 2010).972 Meaning-preserving correctionsFormal models of language acquisition have mainlyfocused on learning from positive data, that is, utter-ances that are grammatically correct.
But a questionthat remains open is: Do children receive negativedata and can they make use of it?Chomsky?s poverty of stimulus argument hasbeen used to support the idea of human innate lin-guistic capacity.
It is claimed that there are princi-ples of grammar that cannot be learned from positivedata only, and negative evidence is not available tochildren.
Hence, since children do not have enoughevidence to induce the grammar of their native lan-guage, the additional knowledge language learnersneed is provided by some form of innate linguisticcapacity.E.
M. Gold?s negative results in the frameworkof formal language learning have also been used tosupport the innateness of language.
Gold provedthat superfinite classes of languages are not learn-able from positive data only, which implies thannone of the language classes defined by Chomskyto model natural language is learnable from positivedata only (Gold, 1967).Brown and Hanlon (Brown and Hanlon, 1970)studied negative evidence understood as explicitapprovals or disapprovals of a child?s utterance(e.g.,?That?s right?
or ?That?s wrong.?)
Theyshowed that there is no dependence between thesekinds of answers and the grammaticality of chil-dren?s utterances.
These results were taken as show-ing that children do not receive negative data.
Butdo these results really show this?
It seems evidentthat parents rarely address their children in that way.During the first stages of language acquisition chil-dren make a lot of errors, and parents are not con-stantly telling them that their sentences are wrong;rather the important thing is that they can communi-cate with each other.
However, it is worth studyingwhether other sources of negative evidence are pro-vided to children.
Is this the only form of negativedata?
Do adults correct children in a different way?Some researchers have studied other kinds ofnegative data based on reply-types (e.g., Hirsh-Pasek et al (Hirsh-Pasek et al, 1984), Demetraset al (Demetras et al, 1986) and Morgan andTravis (Morgan and Travis, 1989).)
These studiesargue that parents provide negative evidence to theirchildren by using different types of reply to gram-matical versus ungrammatical sentences.
Marcusanalyzed such studies and concluded that there isno evidence that this kind of feedback (he called itnoisy feedback) is required for language learning,or even that it exists (Marcus, 1993).
He arguedfor the weakness, inconsistency and inherently ar-tificial nature of this kind of feedback.
Moreover, hesuggested that even if such feedback exists, a childwould learn which forms are erroneous only aftercomplex statistical comparisons.
Therefore, he con-cluded that internal mechanisms are necessary to ex-plain how children recover from errors in languageacquisition.Since the publication of the work of Marcus, theconsensus seemed to be that children do not have ac-cess to negative data.
However, a study carried outby Chouinard and Clark shows that this conclusionmay be wrong (Chouinard and Clark, 2003).
First,they point out that the reply-type approach does notconsider whether the reply itself also contains cor-rective information, and consequently, replies thatare corrective are erroneously grouped with thosethat are not.
Moreover, if we consider only reply-types, they may not help to identify the error made.Hence, Chouinard and Clark propose another viewof negative evidence that builds on Clark?s principleof contrast (Clark, 1987; Clark, 1993).
Parents oftencheck up on a child?s erroneous utterances, to makesure they have understood them.
They do this byreformulating what they think the child intended toexpress.
Hence, the child?s utterance and the adult?sreformulation have the same meaning, but differentforms.
Because children attend to contrasts in form,any change in form that does not mark a differentmeaning will signal to children that they may haveproduced an utterance that is not acceptable in thetarget language.
In this way, reformulations iden-tify the locus of any error, and hence the existenceof an error.
Chouinard and Clark analyze longitu-dinal data from five children between two and fouryears old, and show that adults reformulate erro-neous child utterances often enough to help learn-ing.
Moreover, these results show that children notonly detect differences between their own utteranceand the adult reformulation, but that they make useof that information.98In this paper we explore this new view of nega-tive data proposed by Chouinard and Clark.
Cor-rections (in form of reformulations) have a semanticcomponent that has not been taken into account inprevious studies.
Hence, we propose a new com-putational model of language learning that gives anaccount of meaning-preserving corrections, and inwhich we can address questions such as: What arethe effects of corrections on learning syntax?
Cancorrections facilitate the language learning process?3 The ModelWe describe the components of our model, and giveexamples drawn from the primary domain we haveused to guide the development of the model.3.1 Situation, meanings and utterances.A situation is composed of some objects and someof their properties and relations, which pick outsome aspects of the world of joint interest to theteacher and learner.
A situation is represented as aset of ground atoms over some constants (denotingobjects) and predicates (giving properties of the ob-jects and relations between them.)
For example, asituation s1 consisting of a big purple circle to theleft of a big red star is represented by the follow-ing set of ground atoms: s1 = {bi1 (t1), pu1 (t1),ci1 (t1), le2 (t1, t2), bi1 (t2), re1 (t2), st1 (t2)}.Formally, we have a finite set P of predicatesymbols, each of a specific arity.
We also have aset of constant symbols t1, t2, .
.
., which are usedto represent distinct objects.
A ground atom is anexpression formed by applying a predicate symbolto the correct number of constant symbols as argu-ments.We also have a set of of variables x1, x2, .
.
.. Avariable atom is an expression formed by applyinga predicate symbol to the correct number of vari-ables as arguments.
A meaning is a finite sequenceof variable atoms.
Note that the atoms do not con-tain constants, and the order in which they appear issignificant.
A meaning is supported in a situation ifthere exists a support witness, that is, a mapping ofits variables to distinct objects in the situation suchthat the image under the mapping of each atom inthe meaning appears in the situation.
If a meaning issupported in a situation by a unique support witnessthen it is denoting in the situation.
We assume thatboth the teacher and learner can determine whethera meaning is denoting in a situation.We also have a finite alphabet W of words.
Anutterance is a finite sequence of words.
The tar-get language is the set of utterances the teacher mayproduce in some situation; in our examples, this in-cludes utterances like the star or the star to the rightof the purple circle but not star of circle small thegreen.
We assume each utterance in the target lan-guage is assigned a unique meaning.
An utteranceis denoting in a situation if the meaning assignedto utterance is denoting in the situation.
Intuitively,an utterance is denoting if it uniquely picks out theobjects it refers to in a situation.In our model the goal of the learner is to be ableto produce every denoting utterance in any given sit-uation.
Our model is probabilistic, and what we re-quire is that the probability of learner errors be re-duced to very low levels.3.2 The target language and meaningtransducers.We represent the linguistic competence of theteacher by a finite state transducer that both recog-nizes the utterances in the target language and trans-lates each correct utterance to its meaning.
Let Adenote the set of all variable atoms over P .
We de-fine a meaning transducer M with input symbolsW and output symbols A as follows.
M has a fi-nite set Q of states, an initial state q0 ?
Q, a finiteset F ?
Q of final states, a deterministic transitionfunction ?
mappingQ?W toQ, and an output func-tion ?
mapping Q?W to A?
{?
}, where ?
denotesthe empty sequence.The transition function ?
is extended in the usualway to ?
(q, u).
The language of M , denoted L(M)is the set of all utterances u ?
W ?
such that?
(q0, u) ?
F .
For each utterance u, we defineM(u)to be the meaning of u, that is, the finite sequence ofnon-empty outputs produced by M in processing u.Fig.
1 shows a meaning transducer M1 for a limitedsublanguage of Spanish.
M1 assigns the utterance eltriangulo rojo the meaning (tr1 (x1), re1 (x1)).3.3 The learning task.Initially the teacher and learner know the predicatesP and are able to determine whether a meaning is99a / ?0 1 263 4 el / ?
circulo / ci1(x1) cuadrado / sq1(x1) triangulo / tr1(x1) encima / ab2(x1,x2)a / ?
la / ?
5 izquierda / le2(x1,x2) derecha / le2(x2,x1)rojo / re1(x1) verde / gr1(x1) azul / bl1(x1) encima / ab2(x1,x2)   7 del / ?
8 9 circulo / ci1(x2) cuadrado / sq1(x2) triangulo / tr1(x2) rojo / re1(x2) verde / gr1(x2) azul / bl1(x2)Figure 1: Meaning transducer M1.denoting in a situation.
The learner and teacher bothalso know a shared set of categories that classify asubset of the predicates into similarity groups.
Thecategories facilitate generalization by the learnerand analysis of incorrect learner utterances by theteacher.
In our geometric shape domain the cate-gories are shape, size, and color; there is no categoryfor the positional relations.
Initially the teacher alsohas the meaning transducer for the target language,but the learner has no language-specific knowledge.4 The Interaction of Learner and TeacherIn one interaction of the learner and teacher, a newsituation is generated and presented to both of them.The learner attempts to produce a denoting utter-ance for the situation, and the teacher analyzes thelearner?s utterance and decides whether to producea correction of the learner?s utterance or a new de-noting utterance of its own.
Finally, the learner usesthe situation and the teacher?s utterance to update itscurrent grammar for the language.In this section we describe the algorithms used bythe learner and teacher to carry out the steps of thisprocess.4.1 Comprehension and the co-occurrencegraph.To process the teacher?s utterance, the learnerrecords the words in the utterance and the predi-cates in the situation in an undirected co-occurrencegraph.
Each node is a word or predicate symbol andthere is an edge for each pair of nodes.
Each node uhas an occurrence count, c(u), recording the numberof utterances or situations it has occurred in.
Eachedge (u, v) also has an occurrence count, c(u, v),recording the number of utterance/situation pairs inwhich the endpoints of the edge have occurred to-gether.
From the co-occurrence graph the learner de-rives a directed graph with the same nodes, the im-plication graph, parameterized by a noise threshold?
(set at 0.95 in the experiments.)
For each orderedpair of nodes u and v, the directed edge (u, v) is in-cluded in the implication graph if c(u, v)/c(u) ?
?.The learner then deletes edges from predicates towords and computes the transitively reduced im-plication graph.The learner uses the transitively reduced implica-tion graph to try to find the meaning of the teacher?sutterance by translating the words of the utteranceinto a set of sequences of predicates, and determin-ing if there is a unique denoting meaning corre-sponding to one of the predicate sequences.
If so, theunique meaning is generalized into a general formby replacing each predicate by its category gener-alization.
For example, if the learner detects theunique meaning (tr1 (x1), re1 (x1)), it is general-ized to the general form (shape1 (x1), color1 (x1)).The learner?s set of general forms is the basis for itsproduction.4.2 Production by the learner.Each general form denotes the set of possible mean-ings obtained by substituting appropriate symbolsfrom P for the category symbols.
To produce a de-noting utterance for a situation, the learner finds allthe meanings generated by its general forms usingpredicates from the situation and tests each meaningto see if it is denoting, producing a set of possible de-noting meanings.
If the set is empty, the learner pro-duces no utterance.
Otherwise, it attempts to trans-late each denoting meaning into an utterance.The learner selects one of these utterances witha probability depending on a number stored withthe corresponding general form recording the lasttime a teacher utterance matched it.
This ensuresthat repeatedly matched general forms are selectedwith asymptotically uniform probability, while gen-eral forms that are only matched a few times are se-lected with probability tending to zero.1004.3 From meaning to utterance.The process the learner uses to produce an utter-ance from a denoting meaning is as follows.
Fora meaning that is a sequence of k atoms, there aretwo related sequences of positions: the atom posi-tions 1, 2, .
.
.
, k and the gap positions 0, 1, .
.
.
, k.The atom positions refer to the corresponding atoms,and gap position i refers to the position to the rightof atom i, (where gap position 0 is to the left of atoma1.)
The learner generates a sequence of zero ormore words for each position in left to right order:gap position 0, atom position 1, gap position 1, atomposition 2, and so on, until gap position k. The re-sulting sequences of words are concatenated to formthe final utterance.The choice of what sequence of words to pro-duce for each position is represented by a decisiontree.
For each variable atom the learner has en-countered, there is a decision tree that determineswhat sequence of words to produce for that atomin the context of the whole meaning.
For exam-ple, in a sublanguage of Spanish in which there areboth masculine and feminine nouns for shapes, theatom re1 (x1) has a decision tree that branches onthe value of the shape predicate applied to x1 to se-lect either rojo or roja as appropriate.
For the gappositions, there are decision trees indexed by thegeneralizations of all the variable atoms that haveoccurred; the variable atom at position i is general-ized, and the corresponding decision tree is used togenerate a sequence of words for gap position i. Gapposition 0 does not follow any atom position and hasa separate decision tree.If there is no decision tree associated with a givenatom or gap position in a meaning, the learner fallsback on a ?telegraphic speech?
strategy.
For a gapposition with no decision tree, no words are pro-duced.
For an atom position whose atom has no as-sociated decision tree, the learner searches the tran-sitively reduced implication graph for words thatapproximately imply the predicate of the atom andchooses one of maximum observed frequency.4.4 The teacher?s response.If the learner produces an utterance, the teacher an-alyzes it and then chooses its own utterance for thesituation.
The teacher may find the learner?s utter-ance correct, incorrect but correctable, or incorrectand uncorrectable.
If the learner?s utterance is in-correct but correctable, the teacher chooses a pos-sible correction for it.
The teacher randomly de-cides whether or not to use the correction as its ut-terance according to the correction probability.
Ifthe teacher does not use the correction, then its ownutterance is chosen uniformly at random from thedenoting utterances for the situation.If the learner?s utterance is one of the correct de-noting utterances for the situation, the teacher clas-sifies it as correct.
If the learner?s utterance isnot correct, the teacher ?translates?
the learner?s ut-terance into a sequence of predicates by using themeaning transducer for the language.
If the result-ing sequence of predicates corresponds to a denot-ing meaning, the learner?s utterance is classified ashaving an error in form.
The correction is cho-sen by considering the denoting utterances with thesame sequence of predicates as the learner?s utter-ance, and choosing one that is ?most similar?
to thelearner?s utterance.
For example, if the learner?s ut-terance was el elipse pequeno and (el1 , sm1 ) cor-responds to a denoting utterance for the situation,the teacher chooses la elipse pequena as the cor-rection.
If the learner?s utterance is neither correctnor an error in form, the teacher uses a measure ofsimilarity between the learner?s sequence of predi-cates and those of denoting utterances to determinewhether there is a ?close enough?
match.
If so, theteacher classifies the learner?s utterance as havingan error in meaning and chooses as the possiblecorrection a denoting utterance whose predicate se-quence is ?most similar?
to the learner?s predicatesequence.
If the learner produces an utterance andnone of these cases apply, then the teacher classifiesthe learner?s utterance as uninterpretable and doesnot offer a correction.When the teacher has produced an utterance, thelearner analyzes it and updates its grammar of thelanguage as reflected in the co-occurrence graph,the general forms, and the decision trees for wordchoice.
The decision trees are updated by comput-ing an alignment between the teacher?s utterance andthe learner?s understanding of the teacher?s meaning,which assigns a subsequence of words from the ut-terance to each atom or gap position in the meaning.Each subsequence of words is then added to the data101for the decision tree corresponding to the position ofthat subsequence.If the learner has produced an utterance and findsthat the teacher?s utterance has the same meaning,but is expressed differently, then the learner classi-fies the teacher?s utterance as a correction.
In thecurrent model, the learner reports this classification,but does not use it in any way.5 Empirical ResultsWe have implemented and tested our learning andteaching procedures in order to explore questionsabout the roles of corrections in language learning.We have used a simplified version of the MiniatureLanguage Acquisition task proposed by Feldman etal.
(Feldman et al, 1990).
Although this task is notas complex as those faced by children, it involvesenough complexity to be compared to many real-word tasks.The questions that we address in this section arethe following.
(1) Can the learner accomplish thelearning task to a high level of correctness and cov-erage from a ?reasonable?
number of interactions(that is, well short of the number needed to memo-rize every legal situation/sentence pair)?
(2) Whatare the effects of correction or non-correction bythe teacher on the learner?s accomplishment of thelearning tasks?5.1 The specific learning tasks.Each situation has two objects, each with three at-tributes (shape, color and size), and one binary rela-tion between the two objects (above or to the left of.
)The attribute of shape has six possible values (cir-cle, square, triangle, star, ellipse, and hexagon), thatof color has six possible values (red, orange, yellow,green, blue, and purple), and that of size three possi-ble values (big, medium, and small.)
There are 108distinct objects and 23,328 distinct situations.
Situ-ations are generated uniformly at random.For several natural languages we construct a lim-ited sublanguage of utterances related to these situ-ations.
A typical utterance in English is the mediumpurple star below the small hexagon.
There are 168meanings referring to a single object and 112,896meanings referring to two objects, for a total of113,064 possible meanings.
The 113,064 possiblemeanings are instances of 68 general forms: 4 refer-ring to a single object and 64 referring to two ob-jects.
These languages are the 68-form languages.We consulted at least one speaker of each lan-guage to help us construct a meaning transducer totranslate appropriate phrases in the language to all113,064 possible meanings.
Each transducer wasconstructed to have exactly one accepted phrase foreach possible meaning.
We also constructed trans-ducers for reduced sublanguages, consisting of thesubset of utterances that refer to a single object (168utterances) and those that refer to two objects, but in-clude all three attributes of both (46,656 utterances.
)Each meaning in the reduced sublanguage is an in-stance of one of 8 general forms, while most of thelexical and syntactic complexity of the 68-form lan-guage is preserved.
We refer to these reduced sub-languages as the 8-form languages.5.2 How many interactions are needed tolearn?The level of performance of a learner is measuredusing two quantities: the correctness and complete-ness of the learner?s utterances in a given situation.The learning procedure has a test mode in which thelearner receives a situation and responds with theset of U utterances it could produce in that situa-tion, with their corresponding production probabili-ties.
The correctness of the learner is the sum of theproduction probabilities of the elements of U thatare in the correct denoting set.
The completenessof the learner is the fraction of all correct denotingutterances that are in U .
The averages of correct-ness and completeness of the learner for 200 ran-domly generated situations are used to estimate theoverall correctness and completeness of the learner.A learner reaches a level p of performance if bothcorrectness and completeness are at least p.In the first set of trials the target level of per-formance is 0.99 and the learner and teacher en-gage in a sequence of interactions until the learnerfirst reaches this level of performance.
The perfor-mance of the learner is tested at intervals of 100 in-teractions.
Fig.
2 shows the number of interactionsneeded to reach the 0.99 level of performance foreach 68-form language with correction probabilitiesof 0.0 (i.e., the teacher never corrects the learner)and 1.0 (i.e., the teacher offers a correction to the102learner every time it classifies the learner?s utteranceas an error in form or an error in meaning.)
Forcorrection probability 1.0, it also shows the numberof incorrect utterances by the learner, the number ofcorrections offered by the teacher, and the percent-age of teacher utterances that were corrections.
Eachentry is the median value of 10 trials except those inthe last column.
It is worth noting that the learnerdoes not treat corrections specially.0.0 1.0 incorrect corrections c/u%English 700 750 25.0 11.5 1.5%German 800 750 71.5 52.5 7.0%Greek 3400 2600 344.0 319.0 12.3%Hebrew 900 900 89.5 62.5 6.9%Hungarian 750 800 76.5 58.5 7.3%Mandarin 700 800 50.0 31.5 3.9%Russian 3700 2900 380.0 357.0 12.3%Spanish 1000 850 86.0 68.0 8.0%Swedish 1000 900 54.0 43.5 4.8%Turkish 800 900 59.0 37.0 4.1%Figure 2: Interactions, incorrect learner utterances andcorrections by the teacher to reach the 0.99 level of per-formance for 68-form languages.In the column for correction probability 0.0 thereare two clear groups: Greek and Russian, each withat least 3400 interactions and the rest of the lan-guages, each with at most 1000 interactions.
Thefirst observation is that the learner achieves correct-ness and completeness of 0.99 for each of these lan-guages after being exposed to a small fraction of allpossible situations and utterances.
Even 3700 inter-actions involve at most 16.5% of all possible situa-tions and at most 3.5% of all possible utterances bythe teacher, while 1000 interactions involve fewerthan 4.3% of all situations and fewer than 1% of allpossible utterances.5.3 How do corrections affect learning?In the column for correction probability 1.0 we seethe same two groups of languages.
For Greek, thenumber of interactions falls from 3400 to 2600, adecrease of about 24%.
For Russian, the number ofinteractions falls from 3700 to 2900, a decrease ofabout 21%.
Corrections have a clear positive effectin these trials for Greek and Russian, but not for therest of the languages.Comparing the numbers of incorrect learner utter-ances and the number of corrections offered by theteacher, we see that the teacher finds corrections fora substantial fraction of incorrect learner utterances.The last column of Fig.
2 shows the percentage ofthe total number of teacher utterances that were cor-rections, from a low of 1.5% to a high of 12.3%.There are several processes at work in the im-provement of the learner?s performance.
Compre-hension improves as more information accumulatesabout words and predicates.
New correct generalforms are acquired, and unmatched incorrect gen-eral forms decrease in probability.
More data im-proves the decision tree rules for choosing phrases.Attainment of the 0.99 level of performance may belimited by the need to acquire all the correct generalforms or by the need to improve the correctness ofthe phrase choices.In the case of Greek and Russian, most of the tri-als had acquired their last general form by the timethe 0.90 level of performance was reached, but forthe other languages correct general forms were stillbeing acquired between the 0.95 and the 0.99 lev-els of performance.
Thus the acquisition of gen-eral forms was not a bottleneck for Greek and Rus-sian, but was for the other languages.
Because theteacher?s corrections generally do not help with theacquisition of new general forms (the general formin a correction is often the same one the learnerjust used), but do tend to improve the correctnessof phrase choice, we do not expect correction to re-duce the number of interactions to attain the 0.99level of performance when the bottleneck is the ac-quisition of general forms.
This observation led usto construct reduced sublanguages with just 8 gen-eral forms to see if correction would have more ofan effect when the bottleneck of acquiring generalforms was removed.The reduced sublanguages have just 8 generalforms, which are acquired relatively early.
Fig.
3gives the numbers of interactions to reach the 0.99level of performance (except for Turkish, where thelevel is 0.95) for the 8-form sublanguages with cor-rection probability 0.0 and 1.0.
These numbers arethe means of 100 trials (except for Greek and Rus-sian, which each had 20 trials); the performance ofthe learner was tested every 50 interactions.Comparing the results for 8-form sublanguageswith corresponding 68-form languages, we see thatsome require notably fewer interactions for 8-form1030.0 1.0 % reductionEnglish 247.0 202.0 18.2 %German 920.0 683.5 25.7 %Greek 6630.0 4102.5 38.1 %Hebrew 1052.0 771.5 26.7 %Hungarian 1632.5 1060.5 35.0 %Mandarin 340.5 297.5 12.6 %Russian 6962.5 4640.0 33.4 %Spanish 908.0 630.5 30.6 %Swedish 214.0 189.0 11.7 %Turkish 1112.0* 772.0* 30.6 %Figure 3: Interactions to reach the 0.99 level of perfor-mance for 8-form languages.
(For Turkish: the 0.95level.
)sublanguages (English, Mandarin, and Swedish)while others require notably more (Greek, Hungar-ian and Russian.)
In the case of Turkish, the learnercannot attain the 0.99 level of performance for the8-form sublanguage at all, though it does so for the68-form language; this is caused by limitations inlearner comprehension as well as the differing fre-quencies of forms.
Thus, the 8-form languages areneither uniformly easier nor uniformly harder thantheir 68-form counterparts.
Arguably, the restric-tions that produce the 8-form languages make them?more artificial?
than the 68-form languages; how-ever, the artificiality helps us understand more aboutthe possible roles of correction in language learning.Even though in the case of the 8-form languagesthere are only 8 correct general forms to acquire, thedistribution on utterances with one object versus ut-terances with two objects is quite different from thecase of the 68-form languages.
For a situation withtwo objects of different shapes, there are 40 denot-ing utterances in the case of 68-form languages, ofwhich 8 refer to one object and 32 refer to two ob-jects.
In the case of the 8-form languagues, there are10 denoting utterances, of which 8 refer to one ob-ject and 2 refer to two objects.
Thus, in situationsof this kind (which are 5/6 of the total), utterancesreferring to two objects are 4 times more likely inthe case of 68-form languages than in the case of 8-form languages.
This means that if the learner needsto see utterances involving two objects in order tomaster certain aspects of syntax (for example, casesof articles, adjectives and nouns), the waiting time isnoticeably longer in the case of 8-form languages.This longer waiting time emphasizes the effectsof correction, because the initial phase of learningis a smaller fraction of the whole.
In the third col-umn of Fig.
3 we show the percentage reduction inthe number of interactions to reach the 0.99 levelof performance (except: 0.95 for Turkish) from cor-rection probability 0.0 to correction probability 1.0for the 8-form languages.
For each language, cor-rections produce a reduction, ranging from a low of11.7% for Swedish to a high of 38.1% for Greek.This confirms our hypothesis that corrections cansubstantially help the learner when the problem ofacquiring all the general forms is not the bottleneck.6 Discussion and Future WorkWe show that a simple model of a teacher can offermeaning-preserving corrections to the learner andsuch corrections can significantly reduce the num-ber of interactions for the learner to reach a highlevel of performance.
This improvement does notdepend on the learner?s ability to detect corrections:the effect depends on the change in the distributionof teacher utterances in the correcting versus non-correcting conditions.
This suggests re-visiting dis-cussions in linguistics that assume that the learnermust identify teacher corrections in order for themto have an influence on the learning process.Our model of language is very simplified, andwould have to be modified to deal with issues suchas multi-word phrases bearing meaning, morpho-logical relations between words, phonological rulesfor word choice, words with more than one mean-ing and meanings that can be expressed in morethan one way, languages with freer word-orders andmeaning components expressed by non-contiguoussequences of words.
Other desirable directionsto explore include more sophisticated use of co-occurrence information, more powerful methods oflearning the grammars of meanings, feedback to al-low the learning of production to improve compre-hension, better methods of alignment between utter-ances and meanings, methods to allow the learner?ssemantic categories to evolve in response to lan-guage learning, and methods allowing the learner tomake use of its ability to detect corrections.104ReferencesD.
Angluin and L. Becerra-Bonache.
2010.
A Modelof Semantics and Corrections in Language Learning.Technical Report, Yale University Department ofComputer Science, YALE/DCS/TR-1425.R.
Brown and C. Hanlon.
1970.
Derivational complexityand the order of acquisition in child speech.
In J.R.Hayes (ed.
): Cognition and the Development of Lan-guage.
Wiley, New York, NY.M.M.
Chouinard and E.V.
Clark.
2003.
Adult Reformu-lations of Child Errors as Negative Evidence.
Journalof Child Language, 30:637?669.E.V.
Clark 1987.
The principle of contrast: a constrainton language acquisition.
In B. MacWhinney (ed.
):Mechanisms of language acquisition.
Erlbaum, Hills-dale, NJ.E.V.
Clark 1993.
The Lexicon in Acquisition.
CambridgeUniversity Press, Cambridge, UK.M.
J. Demetras, K. N. Post and C.E.
Snow.
1986.
Brownand Hanlon revisited: mothers?
sensitivity to ungram-matical forms.
Journal of Child Language, 2:81?88.J.A.
Feldman, G. Lakoff, A. Stolcke and S. Weber 1990.Miniature Language Acquisition: A Touchstone forCognitive Science.
Annual Conference of the Cogni-tive Science Society, 686?693.E.M.
Gold.
1967.
Language identification in the limit.Information and Control, 10:447?474.K.
Hirsh-Pasek, R.A. Treiman M. and Schneiderman.1984.
Brown and Hanlon revisited: mothers?
sensi-tivity to ungrammatical forms.
Journal of Child Lan-guage, 2:81?88.G.F.
Marcus 1993.
Negative evidence in language acqui-sition.
Cognition, 46:53?95.J.L.
Morgan and L.L.
Travis.
1989.
Limits on negativeinformation in language input.
Journal of Child Lan-guage, 16:531?552.105
