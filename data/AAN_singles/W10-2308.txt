Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 48?54,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsCross-lingual comparison between distributionally determinedword similarity networksOlof Go?rnerupSwedish Institute of Computer Science(SICS)164 29 Kista, Swedenolofg@sics.seJussi KarlgrenSwedish Institute of Computer Science(SICS)164 29 Kista, Swedenjussi@sics.seAbstractAs an initial effort to identify universaland language-specific factors that influ-ence the behavior of distributional models,we have formulated a distributionally de-termined word similarity network model,implemented it for eleven different lan-guages, and compared the resulting net-works.
In the model, vertices constitutewords and two words are linked if they oc-cur in similar contexts.
The model is foundto capture clear isomorphisms across lan-guages in terms of syntactic and semanticclasses, as well as functional categories ofabstract discourse markers.
Language spe-cific morphology is found to be a dominat-ing factor for the accuracy of the model.1 IntroductionThis work takes as its point of departure the factthat most studies of the distributional character ofterms in language are language specific.
A modelor technique?either geometric (Deerwester et al,1990; Finch and Chater, 1992; Lund and Burgess,1996; Letsche and Berry, 1997; Kanerva et al,2000) or graph based (i Cancho and Sole?, 2001;Widdows and Dorow, 2002; Biemann, 2006)?that works quite well for one language may not besuitable for other languages.
A general questionof interest is then: What strengths and weaknessesof distributional models are universal and what arelanguage specific?In this paper we approach this question by for-mulating a distributionally based network model,apply the model on eleven different languages, andthen compare the resulting networks.
We com-pare the networks both in terms of global statisti-cal properties and local structures of word-to-wordrelations of linguistic relevance.
More specif-ically, the generated networks constitute words(vertices) that are connected with edges if theyare observed to occur in similar contexts.
Thenetworks are derived from the Europarl corpus(Koehn, 2005)?the annotated proceedings of theEuropean parliament during 1996-2006.
This isa parallel corpus that covers Danish, Dutch, En-glish, Finnish, French, German, Greek, Italian,Portuguese, Spanish and Swedish.The objective of this paper is not to provide aextensive comparison of how distributional net-work models perform in specific applications forspecific languages, for instance in terms of bench-mark performance, but rather to, firstly, demon-strate the expressive strength of distributionallybased network models and, secondly, to highlightfundamental similarities and differences betweenlanguages that these models are capable of captur-ing (and fail in capturing).2 MethodsWe consider a base case where a context is definedas the preceding and subsequent words of a focusword.
Word order matters and so a context formsa word pair.
Consider for instance the followingsentence1:Ladies and gentlemen, once again, wesee it is essential for Members to bringtheir voting cards along on a Monday.Here the focus word essential occurs in the con-text is ?
for, the word bring in the context to ?their etcetera (the asterisk ?
denotes an interme-diate focus word).
Since a context occurs with aword with a certain probability, each word wi isassociated with a probability distribution of con-texts:Pi = {Pr[wpwiws|wi]}wp,ws?W , (1)1Quoting Nicole Fontaine, president of the European Par-liament 1999-2001, from the first session of year 2000.48where W denotes the set of all words andPr[wpwiws|wi] is the conditional probability thatcontext wp ?ws occurrs, given that the focus wordis wi.
In practice, we estimate Pi by countingthe occurring contexts of wi and then normalizingthe counts.
Context counts, in turn, were derivedfrom trigram counts.
No pre-processing, such asstemming, was performed prior to collecting thetrigrams.2.1 Similarity measureIf two words have similar context distributions,they are assumed to have a similar function inthe language.
For instance, it is reasonable to as-sume that the word ?salt?
to a higher degree occursin similar contexts as ?pepper?
compared to, say,?friendly?.
One could imagine that a narrow 1+1neighborhood only captures fundamental syntacticagreement between words, which has also been ar-gued in the literature (Sahlgren, 2006).
However,as we will see below, the intermediate two-wordcontext also captures richer word relationships.We measure the degree of similarity by com-paring the respective context distributions.
Thiscan be done in a number of ways.
For example,as the Euclidian distance (also known as L2 diver-gence), the Harmonic mean, Spearman?s rank cor-relation coefficient and the Jensen-Shannon diver-gence (information radius).
Here we quantify thedifference between two words wi and wj , denoteddij , by the variational distance (or L1 divergence)between their corresponding context distributionsPi and Pj :dij =?c?C|Pi(X = c)?
Pj(X = c)|, (2)where X is a stochastic variable drawn from C,which is the set of contexts that either wi or wjoccur in.
0 ?
dij ?
2, where dij = 0 ifthe two distributions are identical and dij = 2if the words do not share any contexts at all.
Itis not obvious that the variational distance is thebest choice of measure.
However, we chose toemploy it since it is a well-established and well-understood statistical measure; since it is straight-forward and fast to calculate; and since it appearsto be robust.
To compare, we have also testedto employ the Jensen-Shannon divergence (a sym-metrized and smoothed version of Kullback infor-mation) and acquire very similar results as thosepresented here.
In fact, this is expected since thetwo measures are found to be approximately lin-early related in this context.
However, for the twofirst reasons listed above, the variational distanceis our divergence measure of choice in this study.2.2 Network representationA set of words and their similarity relations arenaturally interpreted as a weighted and undirectednetwork.
The vertices then constitute words andtwo vertices are linked by an edge if their corre-sponding words wi and wj have overlapping con-text sets.
The strength of the links vary depend-ing on the respective degrees of word similarities.Here the edge between two words wi and wj?sis weighted with wij = 2 ?
dij (note again thatmaxij dij = 2) since a large word difference im-plies a weak link and vice versa.In our experiment we consider the 3000 mostcommon words, excluding the 19 first ones, ineach language.
To keep the data more manage-able during analysis we employ various thresh-olds.
Firstly, we only consider context words thatoccur five times or more.
As formed by the re-maining context words, we then only consider tri-grams that occur three times or more.
This allowsus to cut away a large chunk of the data.
We havetested to vary these thresholds and the resultingnetworks are found to have very similar statisti-cal properties, even though the networks differ bya large number of very weak edges.3 Results3.1 Degree distributionsThe degree gi of a vertex i is defined as the sumof weights of the edges of the vertex: gi =?wij .The degree distribution of a network may providevaluable statistical information about the networksstructure.
For the word networks, Figure 1, the de-gree distributions are all found to be highly right-skewed and have longer tails than expected fromrandom graphs (Erdo?s and Re?nyi, 1959).
Thischaracteristics is often observed in complex net-works, which typically also are scale-free (New-man, 2003).
Interestingly, the word similarity net-works are not scale-free as their degree distribu-tions do no obey power-laws: Pr(g) ?
g??
forsome exponent ?.
Instead, the degree distributionsof each word network appears to lay somewherebetween a power-law distribution and an exponen-tial distribution (Pr(g) ?
e?g/?).
However, dueto quite noisy statistics it is difficult to reliably49measure and characterize the tails in the word net-works.
Note that there appears to be a bump inthe distributions for some languages at around de-gree 60, but again, this may be due to noise andmore data is required before we can draw any con-clusions.
Note also that the degree distribution ofFinnish stands out: Finnish words typically haveless or weaker links than words in the other lan-guages.
This is reasonably in view of the specialmorphological character of Finnish compared toIndo-European languages (see below).3.2 Community structuresThe acquired networks display interesting globalstructures that emerge from the local and pair-wise word to word relations.
Each network forma single strongly connected component.
In otherwords, any vertex can be reached by any other ver-tex and so there is always a path of ?associations?between any two words.
Furthermore, all wordnetworks have significant community structures;vertices are organized into groups, where there arehigher densities of edges within groups than be-tween them.
The strength of community structurecan be quantified as follows (Newman and Gir-van, 2004): Let {vi}ni=1 be a partition of the setof vertices into n groups, ri the fraction of edgeweights that are internal to vi (i.e.
the sum of in-ternal weights over the sum of all weights in thenetwork), and si the fraction of edge weights ofthe edges starting in vi.
The modularity strength isthen defined asQ =n?i=1(ri ?
s2i ).
(3)Q constitutes the fraction of edge weights givenby edges in the network that link vertices withinthe same communities, minus the expected valueof the same quantity in a random network with thesame community assignments (i.e.
the same ver-tex set partition).
There are several algorithms thataim to find the community structure of a networkby maximizing Q.
Here we use an agglomerativeclustering method by Clauset (2005), which worksas follows: Initialize by assigning each vertex toits own cluster.
Then successively merge clusterssuch that the positive change of Q is maximized.The procedure is repeated as long as Q increases.Typically Q is close to 0 for random partitionsand indicates strong community structure whenapproaching its maximum 1.
In practice Q is typi-cally within the range 0.3 to 0.7, also for highlymodular networks (Newman and Girvan, 2004).As can be seen in Table 1, all networks are highlymodular, although the degree of modularity variesbetween languages.
Greek in particular stands out.However, the reason for this remains an open ques-tion that requires further investigations.Dutch 0.43 Swedish 0.58German 0.43 French 0.63Spanish 0.48 Finnish 0.68Portuguese 0.51 Italian 0.68English 0.53 Greek 0.78Danish 0.55Table 1: Community modularity.Communities become more apparent whenedges are pruned by a threshold as they crystal-ize into isolated subgraphs.
This is exemplifiedfor English in Figure 2.4 DiscussionWe examine the resulting graphs and show in thissection through some example subgraphs how fea-tures of human language emerge as charactersiticsof the model.4.1 Morphology mattersMorphology is a determining and observable char-acteristic of several languages.
For the purposesof distributional study of linguistic items, mor-phological variation is problematic, since it splitsone lexical item into several surface realisations,requiring more data to perform reliable and ro-bust statistical analysis.
Of the languages stud-ied in this experiment, Finnish stands out atypi-cal through its morphological characteristics.
Intheory, Finnish nouns can take more than 2 000surface forms, through more than 12 cases in sin-gular and plural as well as possessive suffixesand clitic particles (Linden and Pirinen, 2009),and while in practice something between six andtwelve forms suffice to cover about 80 per centof the variation (Kettunen, 2007) this is still anorder of magnitude more variation than in typi-cal Indo-European languages such as the othersin this sample.
This variation is evident in Fig-ure 1?Finnish behaves differently than the Indo-European languages in the sample: as each wordis split in several other surface forms, its links toother forms will be weaker.
Morphological anal-ysis, transforming surface forms to base forms500200400600800100012001400160018000  50  100  150  200  250  300  350degree countdegreeDanishGermanGreekEnglishSpanishFinnishFrenchItalianDutchPortugueseSwedishFigure 1: Degree histograms of word similarity networks.# %#%l' activit?l' adh?sionl' adoptionl' am?liorationl' applicationl' augmentationl' ?galit?l' ?laborationl' ?tablissementl' ?valuationl' examenl' ex?cutionl' existence l' harmonisationl' histoirel' interdictionl' interventionl' introductionl' octroi l' ouverturel' utilisationl adh?sion l adoption l application l ?galit?l ?largissementl ensemblel utilisationFigure 3: French definite nouns clustered.would strengthen those links.In practice, the data sparsity caused by mor-phological variation causes semantically homoge-nous classes to be split.
Even for languages suchas English and French, with very little data varia-tion we find examples where morphological varia-tion causes divergence as seen in Figure 3, whereFrench nouns in definite form are clustered.
It isnot surprising that certain nouns in definite formassume similar roles in text, but the neatness ofthe graph is a striking exposition of this fact.These problems could have been avoided withbetter preprocessing?simple such processing inthe case of English and French, and considerablymore complex but feasible in the case of Finnish?but are retained in the present example as proxiesfor the difficulties typical of processing unknownlanguages.
Our methodology is robust even inface of shoddy preprocessing and no knowledgeof the morphological basis of the target language.In general, as a typological fact, it is reasonable toassume that morphological variation is offset forthe language user in a greater freedom in choice ofword order.
This would seem to cause a great dealof problems for an approach such as the presentone, since it relies on the sequential organisationof symbols in the signal.
However, it is observ-able that languages with free word order have pre-ferred unmarked arrangements for their sentencestructure, and thus we find stable relationships inthe data even for Finnish, although weaker than forthe other languages examined.4.2 Syntactic classesPrevious studies have shown that a narrow con-text window of one neighour to the left and oneneighbour to the right such as the one used inthe present experiments retrieves syntactic rela-tionships (Sahlgren, 2006).
We find several suchexamples in the graphs.
In Figure 2 we can seesubgraphs with past participles, auxiliary verbs,progressive verbs, person names.4.3 Semantic classesSome of the subgraphs we find are models of clearsemantic family resemblance as shown in Fig-ure 4.
This provides us with a good argument forblurring the artificial distinction between syntaxand semantics.
Word classes are defined by theirmeaning and usage alike; the a priori distinctionbetween classification by function such as auxil-iary verbs given above and classification by mean-ing such months and places given here is not fruit-ful.
We expect to be able to provide much more in-51adaptadditionaddressafghanistanalbaniaalgeriaamendamsterdamasiaassessauditorsaustria barcelonabelarusbelgiumberlinbosniabritainbrusselsbulgariaburmachechnyachinaclarifyclaritycoherencecompetitivenesscondemncoordinatecopenhagencorruptioncroatiacubaculturecyprusdefinedenmarkdiscussdublinefficiencyeliminateemployersemuenhanceeuropolexamineexpressionfinlandflexibilityfrancefraudfulfilgenevagermanyglobalisationgmosgreeceharmonisationharmonizationhelsinkiimplementimproveincorporateindiaindonesiainnovationinquiryiraniraqirelandisraelissueitalyjapankindkosovoliberalisationlondonluxembourgmacedoniamaintainmattermenministersmonitorafternoonevening morningmoroccomovementnatoniceolafopennessopposeorderoriginovercomepalestinearticlesparagraphparagraphsparliament'people'polandpoliticsportugalpovertypreservepresidentsabolitionabsenceadoptionaimappointmentbackgroundchairmancompletionconceptcontinuationcreationdefinitionestablishmenteventfacegroundsinclusionintroductionleaderlightmiddleoccasionpossibility promotionprospectpurposepurposespursueracismratifyregulatereinforcerejectrelationreligionremovereplaceresolveretainromaniarussiasafeguardscienceconsideringdebatingdiscussingexamining aimsfailsrefersseeksaprildecemberfebruaryjanuaryjulyjunemarchnovemberoctoberseptemberserbiaslovakiacoversincludesinvolvesreflects barrosoprodisantersolanasolvesortspainsportstrasbourgstrengthenaddressedappliedcompletedcondemnedconductedcreateddebateddiscussedencouragedestablishedexaminedfinancedfundedguaranteedimplementedimprovedintroducedjustifiedmaintainedmonitoredpromotedprotectedpursuedregulated resolvedrespectedsolvedstrengthenedconfirmedemphasisedhighlighted notedstressedsubjectsubsidiarityafraiddelightedgladhappypleasedsorrysureswedenaustrian belgiandanishdutchfinnishfrenchgermangreekirishitalianportuguesespanish swedishtackleachievingaddressing applyingchangingdefendingdefiningguaranteeingimplementingmaintainingprotectingsafeguarding supportingtacklingtampereterrorismbelievefeelhopethinkforemosthencesecondlythirdlytorturetourismtransparencytreatturkeyfivefoursevententhreetwotypeukraineacknowledgeemphasiseemphasizehighlightraiserealiserecogniserecognizereiteraterememberstressunderlineunemploymentapparentevidentobviousregrettabletrueunfortunateunion'appearbelongfailrefuse seemask inviteremind urgeamericansbalkansbureaucapcitizencourtsecbeurofactsgreensgroundigcinternetnetherlandsombudsmanpalestiniansrailways roadsstreatiesukusaviennacrucialessentialunacceptablevitalable entitledgoingprepared supposedwillingintendwant wishcontinues intendswantswisheswithdrawchildrenconsumers employeesfarmersfishermenforestsimmigrantsindividualsjournalistsminoritiesngosproducersrefugeessmesstudentswomenworkersmustshouldwillwoulddaysmonthsweeksyearszimbabweFigure 2: English.
Network involving edges with weights w ?
0.85.
For sake of clarity, only subgraphswith three or more words are shown.
Note that the threshold 0.85 is used only for the visualization.
Thefull network consists of the 3000 most common words in English, excluding the 19 most common ones.52avrild?cembref?vrierjanvierjuilletjuinmaimarsnovembreoctobreseptembreaprildecemberfebruarijanuarijuli junimajmarsnovemberoktoberseptemberenhanceimprovepreservereinforcesafeguard strengthencorruption fraudpovertyracismterrorismtortureFigure 4: Examples of semantically homogenousclasses in English, French and Swedish.formed classification schemes than the traditional?parts of speech?
if we define classes by their dis-tributional qualities rather than by the ?content?they ?represent?, schemes which will cut acrossthe function-topic distinction.4.4 Abstract discourse markers are afunctional categoryFurther, several subgraphs have clear collectionsof discourse markers of various types where theterms are markers of informational organisation inthe text, as exemplified in Figure 5.5 ConclusionsThis preliminary experiment supports future stud-ies to build knowledge structures across lan-guages, using distributional isomorphism betweenlinguistic material in translated or even compara-ble corpora, on several levels of abstraction, fromfunction words, to semantic classes, to discoursemarkers.
The isomorphism across the languagesis clear and incontrovertible; this will allow us tocontinue experiments using collections of multi-lingual materials, even for languages with rela-tively little technological support.
Previous stud-ies show that knowledge structures of this typethat are created in one language show consider-able isomorphism to knowledge structures createdin another language if the corpora are comparable(Holmlund et al, 2005).
Holmlund et alshow howtranslation equivalences can be established usingep?ilem?tt?kuitenkinluonnollisestiselv?stikinsiistietenkintietystitodellakintokivarmastif?rmodligenf?rvissogivetviss?kerligensannerligensj?lvfalletuppenbarligenvisserligenFigure 5: Examples of discourse functional classesin Swedish and Finnish.
The terms in the two sub-graphs are discourse markers and correspond toEnglish ?certainly?, ?possibly?, ?evidently?, ?nat-urally?, ?absolutely?, ?hence?
and similar terms.two semantic networks automatically created intwo languages by providing a relatively limited setof equivalence relations in a translation lexicon.This study supports those findings.The results presented here display the potentialof distributionally derived network representationsof word similarities.
Although geometric (vectorbased) and probabilistic models have proven vi-able in various applications, they are limited bythe fact that word or term relations are constrainedby the geometric (often Euclidian) space in whichthey live.
Network representations are richer inthe sense that they are not bound by the same con-straints.
For instance, a polyseme word (?may?
forexample) can have strong links to two other words(?might?
and ?September?
for example), wherethe two other words are completely unrelated.
Inan Euclidean space this relation is not possible dueto the triangle inequality.
It is possible to em-bed a network in a geometric space, but this re-quires a very high dimensionality which makes therepresentation both cumbersome and inefficient interms of computation and memory.
This has beenaddressed by coarse graining or dimension reduc-tion, for example by means of singular value de-53composition (Deerwester et al, 1990; Letsche andBerry, 1997; Kanerva et al, 2000), which resultsin information loss.
This can be problematic, inparticular since distributional models often facedata sparsity due to the curse of dimensionality.In a network representation, such dimension re-duction is not necessary and so potentially impor-tant information about word or term relations isretained.The experiments presented here also show thepotential of moving from a purely probabilisticmodel of term occurrence, or a bare distributionalmodel such as those typically presented using ageometric metaphor, in that it affords the possibil-ity of abstract categories inferred from the primarydistributional data.
This will give the possibilityof further utilising the results in studies, e.g.
forlearning syntactic or functional categories in morecomplex constructional models of linguistic form.Automatically establishing lexically and function-ally coherent classes in this manner will have bear-ing on future project goals of automatically learn-ing syntactic and semantic roles of words in lan-guage.
This target is today typically pursued rely-ing on traditional lexical categories which are notnecessarily the most salient ones in view of actualdistributional characteristics of words.Acknowledgments: OG was supported by Johanand Jacob So?derberg?s Foundation.
JK was sup-ported by the Swedish Research Council.ReferencesChris.
Biemann.
2006.
Chinese whispers - an efficientgraph clustering algorithm and its application to nat-ural language processing problems.
In Proceedingsof the HLT-NAACL-06 Workshop on Textgraphs-06,New York, USA.Aaron Clauset.
2005.
Finding local community struc-ture in networks.
Physical Review E, 72:026132.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41:391?407.Pal Erdo?s and Alfre?d Re?nyi.
1959.
On random graphs.Publications Mathematicae, 6:290.Steven Finch and Nick Chater.
1992.
Bootstrap-ping syntactic categories.
In Proceedings of theFourteenth Annual Conference of the Cognitive Sci-ence Society, pages 820?825, Bloomington, IN.Lawrence Erlbaum.Jon Holmlund, Magnus Sahlgren, and Jussi Karlgren.2005.
Creating bilingual lexica using referencewordlists for alignment of monolingual semanticvector spaces.
In Proceedings of 15th Nordic Con-ference of Computational Linguistics.Ramon Ferrer i Cancho and Ricard V. Sole?.
2001.
Thesmall world of human language.
Proceedings of theRoyal Society of London.
Series B, Biological Sci-ences, 268:2261?2266.Pentti Kanerva, Jan Kristoferson, and Anders Holst.2000.
Random indexing of text samples for latentsemantic analysis.
In Proceedings of the 22nd An-nual Conference of the Cognitive Science Society,pages 103?6.Kimmo Kettunen.
2007.
Management of keywordvariation with frequency based generation of wordforms in ir.
In Proceedings of SIGIR 2007.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Todd Letsche and Michael Berry.
1997.
Large-scaleinformation retrieval with latent semantic indexing.Information Sciences, 100(1-4):105?137.Krister Linden and Tommi Pirinen.
2009.
Weightingfinite-state morphological analyzers using hfst tools.In Proceedings of the Finite-State Methods and Nat-ural Language Processing.
Pretoria, South Africa.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, and Computers, 28(2):203?208.Mark Newman and Michelle Girvan.
2004.
Find-ing and evaluating community structure in networks.Physical Review E, 69(2).Mark E. J. Newman.
2003.
The structure and functionof complex networks.
SIAM Review, 45(2):167?256.Magnus Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
PhD Dissertation, De-partment of Linguistics, Stockholm University.Dominic Widdows and Beate Dorow.
2002.
A graphmodel for unsupervised lexical acquisition.
In In19th International Conference on ComputationalLinguistics, pages 1093?1099.54
