Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 10?19,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsAnalysis of Stopping Active Learning based on Stabilizing PredictionsMichael BloodgoodCenter for Advanced Study of LanguageUniversity of MarylandCollege Park, MD 20740meb@umd.eduJohn GrothendieckRaytheon BBN Technologies9861 Broken Land Parkway, Suite 400Columbia, MD 21046jgrothen@bbn.comAbstractWithin the natural language processing(NLP) community, active learning hasbeen widely investigated and applied in or-der to alleviate the annotation bottleneckfaced by developers of new NLP systemsand technologies.
This paper presents thefirst theoretical analysis of stopping activelearning based on stabilizing predictions(SP).
The analysis has revealed three ele-ments that are central to the success of theSP method: (1) bounds on Cohen?s Kappaagreement between successively trainedmodels impose bounds on differences inF-measure performance of the models; (2)since the stop set does not have to be la-beled, it can be made large in practice,helping to guarantee that the results trans-fer to previously unseen streams of ex-amples at test/application time; and (3)good (low variance) sample estimates ofKappa between successive models can beobtained.
Proofs of relationships betweenthe level of Kappa agreement and the dif-ference in performance between consecu-tive models are presented.
Specifically, ifthe Kappa agreement between two mod-els exceeds a threshold T (where T > 0),then the difference in F-measure perfor-mance between those models is boundedabove by 4(1?T )T in all cases.
If precisionof the positive conjunction of the modelsis assumed to be p, then the bound can betightened to 4(1?T )(p+1)T .1 IntroductionActive learning (AL), also called query learningand selective sampling, is an approach to reducethe costs of creating training data that has receivedconsiderable interest (e.g., (Argamon-Engelsonand Dagan, 1999; Baldridge and Osborne, 2008;Bloodgood and Vijay-Shanker, 2009b; Bloodgoodand Callison-Burch, 2010; Hachey et al 2005;Haertel et al 2008; Haffari and Sarkar, 2009;Hwa, 2000; Lewis and Gale, 1994; Sassano,2002; Settles and Craven, 2008; Shen et al 2004;Thompson et al 1999; Tomanek et al 2007; Zhuand Hovy, 2007)).Within the NLP community, active learning hasbeen widely investigated and applied in order toalleviate the annotation bottleneck faced by devel-opers of new NLP systems and technologies.
Themain idea is that by judiciously selecting whichexamples to have labeled, annotation effort will befocused on the most helpful examples and less an-notation effort will be required to achieve givenlevels of performance than if a passive learningpolicy had been used.Historically, the problem of developing meth-ods for detecting when to stop AL was tabled forfuture work and the research literature was fo-cused on how to select which examples to have la-beled and analyzing the selection methods (Cohnet al 1996; Seung et al 1992; Freund et al 1997;Roy and McCallum, 2001).
However, to realizethe savings in annotation effort that AL enables,we must have a method for knowing when to stopthe annotation process.
The challenge is that if westop too early while useful generalizations are stillbeing made, then we can wind up with a modelthat performs poorly, but if we stop too late afterall the useful generalizations are made, then hu-man annotation effort is wasted and the benefits ofusing active learning are lost.Recently research has begun to develop meth-ods for stopping AL (Schohn and Cohn, 2000;Ertekin et al 2007b; Ertekin et al 2007a; Zhuand Hovy, 2007; Laws and Schu?tze, 2008; Zhuet al 2008a; Zhu et al 2008b; Vlachos, 2008;Bloodgood, 2009; Bloodgood and Vijay-Shanker,2009a; Ghayoomi, 2010).
The methods are all10heuristics based on estimates of model confidence,error, or stability.
Although these heuristic meth-ods have appealing intuitions and have had ex-perimental success on a small handful of tasksand datasets, the methods are not widely usable inpractice yet because our community?s understand-ing of the stopping methods remains too coarseand inexact.
Pushing forward on understandingthe mechanics of stopping at a more exact levelis therefore crucial for achieving the design ofwidely usable effective stopping criteria.Bloodgood and Vijay-Shanker (2009a) intro-duce the terminology aggressive and conserva-tive to describe the behavior of stopping meth-ods1 and conduct an empirical evaluation of thedifferent published stopping methods on severaldatasets.
While most stopping methods tend tobehave conservatively, stopping based on stabiliz-ing predictions computed via inter-model Kappaagreement has been shown to be consistently ag-gressive without losing performance (in terms ofF-Measure2) in several published empirical tests.This method stops when the Kappa agreement be-tween consecutively learned models during ALexceeds a threshold for three consecutive itera-tions of AL.
Although this is an intuitive heuristicthat has performed well in published experimentalresults, there has not been any theoretical analysisof the method.The current paper presents the first theoreticalanalysis of stopping based on stabilizing predic-tions.
The analysis helps to explain at a deeperand more exact level why the method works as itdoes.
The results of the analysis help to character-ize classes of problems where the method can beexpected to work well and where (unmodified) itwill not be expected to work as well.
The theoryis suggestive of modifications to improve the ro-bustness of the stopping method for certain classesof problems.
And perhaps most important, theapproach that we use in our analysis provides anenabling framework for more precise analysis ofstopping criteria and possibly other parts of the ac-tive learning decision space.In addition, the information presented in this pa-1Aggressive methods stop sooner, aggressively trying toreduce unnecessary annotations while conservative methodsare careful not to risk losing model performance, even if itmeans annotating many more examples than were necessary.2For the rest of this paper, we will use F-measure to de-note F1-measure, that is, the balanced harmonic mean of pre-cision and recall, which is a standard metric used to evaluateNLP systems.per is useful for works that consider switching be-tween different active learning strategies and oper-ating regions such as (Baram et al 2004; Do?nmezet al 2007; Roth and Small, 2008).
Knowingwhen to switch strategies, for example, is sim-ilar to the stopping problem and is another set-ting where detailed understanding of the varianceof stabilization estimates and their link to perfor-mance ramifications is useful.
More exact un-derstanding of the mechanics of stopping is alsouseful for applications of co-training (Blum andMitchell, 1998), and agreement-based co-training(Clark et al 2003) in particular.
Finally, theproofs of the Theorems regarding the relationshipsbetween Cohen?s Kappa statistic and F-measuremay be of broader use in works that consider inter-annotator agreement and its ramifications for per-formance appraisals, a topic that has been of long-standing interest in computational linguistics (Car-letta, 1996; Artstein and Poesio, 2008).In the next section we summarize the stabiliz-ing predictions (SP) stopping method.
Section 3analyzes SP and Section 4 concludes.2 Stopping Active Learning based onStabilizing PredictionsThe intuition behind the SP method is that themodels learned during AL can be applied to a largerepresentative set of unlabeled data called a stopset and when consecutively learned models havehigh agreement on their predictions for classify-ing the examples in the stop set, this indicates thatit is time to stop (Bloodgood and Vijay-Shanker,2009a; Bloodgood, 2009).
The active learningstopping strategy explicitly examined in (Blood-good and Vijay-Shanker, 2009a) (after the generalform is discussed) is to calculate Cohen?s Kappaagreement statistic between consecutive rounds ofactive learning and stop once it is above 0.99 forthree consecutive calculations.Since the Kappa statistic is an important as-pect of this method, we now discuss some back-ground regarding measuring agreement in general,and Cohen?s Kappa in particular.
Measurementof agreement between human annotators has re-ceived significant attention and in that context,the drawbacks of using percentage agreement havebeen recognized (Artstein and Poesio, 2008).
Al-ternative metrics have been proposed that takechance agreement into account.
Artstein and Poe-sio (2008) survey several agreement metrics.
Most11of the agreement metrics they discuss are of theform:agreement = Ao ?Ae1?Ae, (1)whereAo = observed agreement, andAe = agree-ment expected by chance.
The different metricsdiffer in how they compute Ae.
All the instancesof usage of an agreement metric in this article willhave two categories and two coders.
The two cat-egories are ?+1?
and ?-1?
and the two coders arethe two consecutive models for which agreementis being measured.Cohen?s Kappa statistic3 (Cohen, 1960) mea-sures agreement expected by chance by modelingeach coder (in our case model) with a separate dis-tribution governing their likelihood of assigning aparticular category.
Formally, Kappa is defined byEquation 1 with Ae computed as follows:Ae =?k?
{+1,?1}P (k|c1) ?
P (k|c2), (2)where each ci is one of the coders (in our case,models), and P (k|ci) is the probability that coder(model) ci labels an instance as being in categoryk.
Kappa estimates the P (k|ci) in Equation 2based on the proportion of observed instances thatcoder (model) ci labeled as being in category k.3 AnalysisThis section analyzes the SP stopping method.Section 3.1 analyzes the variance of the estima-tor of Kappa that SP uses and in particular the re-lationship of this variance to specific aspects ofthe operationalization of SP, such as the stop setsize.
Section 3.2 analyzes relationships betweenthe Kappa agreement between two models and thedifference in F-measure between those two mod-els.3.1 Variance of Kappa EstimatorSP bases its decision to stop on the informationcontained in the contingency tables between theclassifications of models learned at consecutiveiterations during AL.
In determining whether tostop at iteration t, the classifications of the currentmodel Mt are compared with the classifications ofthe previous model Mt?1.
Table 1 shows the pop-ulation parameters for these two models, where:3We note that there are other agreement measures (beyondCohen?s Kappa) which could also be applicable to stoppingbased on stabilizing predictions, but an analysis of these isoutside the scope of the current paper.MtMt?1 + - Total+ pi++ pi+?
pi+.- pi?+ pi??
pi?.Total pi.+ pi.?
1Table 1: Contingency table population probabili-ties forMt (model learned at iteration t) andMt?1(model learned at iteration t-1).population probability piij for i, j ?
{+,?}
is theprobability of an example being placed in categoryi by model Mt?1 and category j by model Mt;population probability pi.j for j ?
{+,?}
is theprobability of an example being placed in categoryj by model Mt; and population probability pii.
fori ?
{+,?}
is the probability of an example beingplaced in category i by model Mt?1.
The actualprobability of agreement is pio = pi++ + pi??.
Asindicated in Equation 2, Kappa models the prob-ability of agreement expected due to chance byassuming that classifications are made indepen-dently.
Hence, the probability of agreement ex-pected by chance in terms of the population prob-abilities is pie = pi+.pi.++pi?.pi.?.
From the defini-tion of Kappa (see Equation 1), we then have thatthe Kappa parameter K in terms of the populationprobabilities is given byK = pio ?
pie1?
pie.
(3)For practical applications we will not know thetrue population probabilities and we will have toresort to using sample estimates.
The SP methoduses a stop set of size n for deriving its estimates.Table 2 shows the contingency table counts forthe classifications of models Mt and Mt?1 on asample of size n. The population probabilities piijcan be estimated by the relative frequencies pij fori, j ?
{+,?, .
}, where: p++ = a/n; p+?
= b/n;p?+ = c/n; p??
= d/n; p+.
= (a+ b)/n; p?.
=(c+d)/n; p.+ = (a+ c)/n; and p.?
= (c+d)/n.Let po = p++ + p?
?, the observed proportion ofagreement and let pe = p+.p.+ + p?.p.
?, the pro-portion of agreement expected by chance if we as-sume that Mt and Mt?1 make their classificationsindependently.
Then the Kappa measure of agree-ment K between Mt and Mt?1 (see Equation 3) isestimated byK?
= po ?
pe1?
pe.
(4)12MtMt?1 + - Total+ a b a+ b- c d c+ dTotal a+ c b+ d nTable 2: Contingency table counts for Mt (modellearned at iteration t) and Mt?1 (model learned atiteration t-1).Using the delta method, as described in (Bishopet al 1975), Fleiss et al(1969) derived an estima-tor of the large-sample variance of K?.
Accordingto Hale and Fleiss (1993), the estimator simplifiestoV ar(K?)
= 1n(1?
pe)2?
{ ?i?{+,?}pii[1?
4p?i(1?
K?)]?
(K?
?
pe(1?
K?
))2 + (1?
K?)2??i,j?{+,?
}pij [2(p?i + p?j)?
(pi.
+ p.j)]2},(5)where p?i = (pi.
+ p.i)/2.
From Equation 5, wecan see that the variance of our estimate of Kappais inversely proportional to the size of the stop setwe use.Bloodgood and Vijay-Shanker (2009a) used astop set of size 2000 for each of their datasets.Although this worked well in the results they re-ported, we do not believe that 2000 is a fixed sizethat will work well for all tasks and datasets wherethe SP method could be used.
Table 3 showsthe variances of K?
computed using Equation 5at the points at which SP stopped AL for each ofthe datasets4 from (Bloodgood and Vijay-Shanker,2009a).These variances indicate that the size of 2000was typically sufficient to get tight estimates ofKappa, helping to illuminate the empirical successof the SP method on these datasets.
More gener-ally, the SP method can be augmented with a vari-ance check: if the variance of estimated Kappa ata potential stopping point exceeds some desired4We note that each of the datasets was set up as a binaryclassification task (or multiple binary classification tasks).Further details and descriptions of each of the datasets canbe found in (Bloodgood and Vijay-Shanker, 2009a).threshold, then the stop set size can be increasedas needed to reduce the variance.Looking at Equation 5 again, one can note thatwhen pe is relatively close to 1, the variance of K?can be expected to get quite large.
In these situ-ations, users of SP should expect to have to uselarger stop set sizes and in extreme conditions, SPmay not be an advisable method to use.3.2 Relationship between Kappa agreementand change in performance betweenmodelsHeretofore, the published literature contained onlyinformal explanations of why stabilizing predic-tions is expected to work well as a stoppingmethod (along with empirical tests demonstrat-ing successful operation on a handful of tasks anddatasets).
In the remainder of this section wedescribe the mathematical foundations for stop-ping methods based on stabilizing predictions.
Inparticular, we will prove that even in the worstpossible case, if the Kappa agreement betweentwo subsequently learned models is greater thana threshold T , then it must be the case that thechange in performance between these two modelsis bounded above by 4(1?T )T .
We then go on toprove additional Theorems that tighten this boundwhen assumptions are made about model preci-sion.Lemma 3.1 Suppose F-measure F and Kappa Kare computed from the same contingency table ofcounts, such as the one given in Table 2.
Supposead?
bc ?
0.
Then F ?
K.Proof By definition, in terms of the contingencytable counts,K = 2ad?
2bc(a+ b)(b+ d) + (a+ c)(c+ d) (6)andF = 2a2a+ b+ c .
(7)Rewriting F so that it will have the same numera-tor as K, we have:F = F(d?
bcad?
bca)(8)=( 2a2a+ b+ c)(d?
bcad?
bca)(9)= 2ad?
2bc2ad+ bd+ cd?
2bc?
b2c+bc2a.
(10)13Task-Dataset Variance of K?NER-DNA (10-fold CV) 0.0000223NER-cellType (10-fold CV) 0.0000211NER-protein (10-fold CV) 0.0000074Reuters (10 Categories) 0.000029820 Newsgroups (20 Categories) 0.0000739WebKB Student (10-fold CV) 0.0000137WebKB Project (10-fold CV) 0.0000190WebKB Faculty (10-fold CV) 0.0000115WebKB Course (10-fold CV) 0.0000179TC-spamassassin (10-fold CV) 0.0000042TC-TREC-SPAM (10-fold CV) 0.0000043Average (macro-avg) 0.0000209Table 3: Estimates of the variance of K?.
For each dataset, the estimate of the variance of K?
is computed(using Equation 5) from the contingency table at the point at which SP stopped AL and the average ofall the variances (across all folds of CV) is displayed.
The last row contains the macro-average of theaverage variances for all the datasets.We can see that the expression for F in Equa-tion 10 has the same numerator as K in Equa-tion 6 but the denominator ofK in Equation 6 is?the denominator of F in Equation 10.
Therefore,F ?
K.Theorem 3.2 LetMt be the model learned at iter-ation t of active learning and Mt?1 be the modellearned at iteration t ?
1.
Let Kt be the estimateof Kappa agreement between the classifications ofMt and Mt?1 on the examples in the stop set.
LetF?t be the F-measure between the classifications ofMt and truth on the stop set.
Let F?t?1 be the F-measure between the classifications of Mt?1 andtruth on the stop set.
Let ?Ft be F?t ?
F?t?1.
Sup-pose T > 0.
Then Kt > T ?
|?Ft| ?
4(1?T )T .Proof Suppose Mt, Mt?1, Kt, F?t, F?t?1, ?Ft,and T are defined as stated in the statement ofTheorem 3.2.
Let Ft be the F-measure betweenthe classifications of Mt and Mt?1 on the exam-ples in the stop set.
Let Table 2 show the con-tingency table counts for Mt versus Mt?1 on theexamples in the stop set.
Then, from their defi-nitions, we have Kt = 2(ad?bc)(a+b)(b+d)+(a+c)(c+d) andFt = 2a2a+b+c .
There exist true labels for the ex-amples in the stop set, which we don?t know sincethe stop set is unlabeled, but nonetheless must ex-ist.
We use the truth on the stop set to split Table 2into two subtables of counts, one table for all theexamples that are truly positive and one table forall the examples that are truly negative.
Table 4MtMt?1 + - Total+ a1 b1 a1 + b1- c1 d1 c1 + d1Total a1 + c1 b1 + d1 n1Table 4: Contingency table counts for Mt (modellearned at iteration t) versus Mt?1 (model learnedat iteration t-1) for only the examples in the stopset that have truth = +1.MtMt?1 + - Total+ a?1 b?1 a?1 + b?1- c?1 d?1 c?1 + d?1Total a?1 + c?1 b?1 + d?1 n?1Table 5: Contingency table counts for Mt (modellearned at iteration t) versus Mt?1 (model learnedat iteration t-1) for only the examples in the stopset that have truth = -1.shows the contingency table for Mt versus Mt?1for all of the examples in the stop set that have truelabels of +1 and Table 5 shows the contingency ta-ble for Mt versus Mt?1 for all of the examples inthe stop set that have true labels of -1.From Tables 2, 4, and 5 one can see that a isthe number of examples in the stop set that bothMt and Mt?1 classified as positive.
Furthermore,out of these a examples, a1 of them truly are pos-14MtTruth + - Total+ a1 + c1 b1 + d1 n1- a?1 + c?1 b?1 + d?1 n?1Total a+ c b+ d nTable 6: Contingency table counts for Mt (modellearned at iteration t) versus truth.
(Derived fromTables 4 and 5Mt?1Truth + - Total+ a1 + b1 c1 + d1 n1- a?1 + b?1 c?1 + d?1 n?1Total a+ b c+ d nTable 7: Contingency table counts for Mt?1(model learned at iteration t-1) versus truth.
(De-rived from Tables 4 and 5itive and a?1 of them truly are negative.
Similarexplanations hold for the other counts.
Also, fromTables 2, 4, and 5, one can see that the equalitiesa = a1 + a?1, b = b1 + b?1, c = c1 + c?1, andd = d1 + d?1 all hold.
The contingency tablesfor Mt versus truth and Mt?1 versus truth can bederived from Tables 4 and 5.
For convenience, Ta-ble 6 shows the contingency table for Mt versustruth and Table 7 shows the contingency table forMt?1 versus truth.
Suppose that Kt > T .
Thisimplies, by Lemma 3.15, that Ft > T .
This im-plies that2a2a+b+c > T (11)?
2a > (2a+ b+ c)T (12)?
2a(1?
T ) > (b+ c)T (13)?
b+ c < 2a(1?T )T .
(14)Note that Equations 12 and 14 are justified since2a+ b+ c > 0 and T > 0, respectively.From Table 6 we can see thatF?t = 2(a1+c1)2(a1+c1)+b1+d1+a?1+c?1 ; from Table 7we can see that F?t?1 = 2(a1+b1)2(a1+b1)+c1+d1+a?1+b?1 .For notational convenience, let: g =2(a1 + c1) + b1 + d1 + a?1 + c?1; andh = 2(a1 + b1) + c1 + d1 + a?1 + b?1.5Note that the condition ad ?
bc ?
0 of Lemma 3.1 ismet since Kt > T and T > 0 imply Kt > 0, which in turnimplies ad?
bc > 0.It follows that?Ft =2(a1 + c1)g ?2(a1 + b1)h (15)= (2a1 + 2c1)h?
(2a1 + 2b1)ggh (16)For notational convenience, let: x = 2(a1c1 +a1b?1 + c21 + c1d1 + c1a?1 + c1b?1); and y =2(a1b1 + a1c?1 + b21 + b1d1 + b1a?1 + b1c?1).Then picking up from Equation 16, it follows that?Ft =x?
ygh (17)= 2[u1 + c1u2 ?
b1u3]gh , (18)where u1 = a1c1 ?
a1b1 + a1b?1 ?
a1c?1, u2 =c1+d1+a?1+b?1, and u3 = b1+d1+a?1+c?1.For notational convenience, let: dA = c1 ?
b1and dB = c?1 ?
b?1.
Then it follows that?Ft =2u4gh , (19)where: u4 = a1(dA ?
dB) + dA(d1 + a?1 + b1 +c1) + c1b?1 ?
b1c?1.Noting that g = h+ dA + dB , we have?Ft =2u4h(h+ dA + dB).
(20)Noting that 2u4 = 2[dA(a1 + b1 + c1 + d1 +a?1 + b?1)?
dB(a1 + b1)] and letting u5 = a1 +b1 + c1 + d1 + a?1 + b?1, we have?Ft =2[dAu5 ?
dB(a1 + b1)]h(h+ dA + dB).
(21)Therefore,|?Ft| ?
2(???
?dAu5h(h+ dA + dB)????+???
?dB(a1 + b1)h(h+ dA + dB)????)
(22)Recall that b+ c = b1 + b?1 + c1 + c?1.
Thenobserve that the following three inequalities hold:b+ c ?
dA; b+ c ?
dB; and h(h+dA +dB) > 0.Therefore,|?Ft| ?
2(b+c)[2a1+2b1+c1+d1+a?1+b?1]h(h+dA+dB) (23)= 2(b+c)hh(h+dA+dB) (24)= 2(b+c)h+dA+dB (25)?
2(2a)(1?T )T (h+dA+dB) (26)=(4(1?T )T)( ah+dA+dB).
(27)15Observe that h+dA+dB = 2a1+b1+2c1+d1+a?1 + c?1.
Therefore, ah+dA+dB ?
1.
Therefore,we have|?Ft| ?4(1?
T )T .
(28)Note that in deriving Inequality 26, we usedthe previously derived Inequality 14.
Also, theproof of Theorem 3.2 assumes a worst possiblecase in the sense that all examples where the clas-sifications of Mt and Mt?1 differ are assumedto have truth values that all serve to maximizeone model?s F-measure and minimize the othermodel?s F-measure so as to maximize |?Ft| asmuch as possible.
A resulting limitation is that thebound is loose in many cases.
It may be possibleto derive tighter bounds, perhaps by easing off toan expected case instead of a worst case and/or bymaking additional assumptions.6Taking this possibility up, we now prove tighterbounds when assumptions about the precision ofthe models Mt and Mt?1 are made.
Consider thatin the proof of Theorem 3.2 when transitioningfrom Equality 27 to Inequality 28, we used thefact that ah+dA+dB ?
1.
Note that ah+dA+dB =a2a1+b1+2c1+d1+a?1+c?1 , from which one sees thatah+dA+dB = 1 only if all of a1, b1, c1, d1 and c?1are all zero.
This is a pathological case.
In manypractically important classes of cases to consider,ah+dA+dB will be strictly less than 1, and often sub-stantially less than 1.
The following two Theoremsprove tighter bounds on |?Ft| than Theorem 3.2by utilizing this insight.Theorem 3.3 Suppose Mt, Mt?1, Kt, F?t, F?t?1,?Ft, and T are defined as stated in the statementof Theorem 3.2.
Let the contingency tables be de-fined as they were in the proof of Theorem 3.3.
LetMPositiveConjunction be a model that only clas-sifies an example as positive if both models Mtand Mt?1 classify the example as positive.
Sup-pose that MPositiveConjunction has perfect preci-sion on the stop set, or in other words that everysingle example from the stop set that both Mt andMt?1 classify as positive is truthfully positive (i.e.,a?1 = 0).
Then Kt > T ?
|?Ft| ?
2(1?T )T .Proof The proof of Theorem 3.2 holds exactlyas it is up until Equality 27.
Now, using theadditional assumption that a?1 = 0, we have6If one is planning to undertake this challenge, we wouldsuggest further consideration of Inequalities 22, 23, 26, and28 as a possible starting point.ah+dA+dB ?12 .
Therefore, we have|?Ft| ?2(1?
T )T .
(29)Theorem 3.3 is a special case (in the limit) ofa more general Theorem.
Before stating and prov-ing the more general Theorem, we prove a Lemmathat will be helpful in making the proof of the gen-eral Theorem clearer.Lemma 3.4 Let f , dA, dB and contingency ta-ble counts be defined as they were in the proofof Theorem 3.2.
Suppose a1 = xa?1.
Thenah+dA+dB ?x+12x+1 .Proof a1 = xa?1 by hypothesis.
a = a1 + a?1by definition of contingency table counts.
Hence,a = (x+ 1)a?1.
Therefore,ah+ dA + dB?
(x+1)a?12xa?1+a?1 (30)= (x+1)a?1(2x+1)a?1= x+12x+1 .The following Theorem generalizes Theo-rem 3.3 to cases when MPositiveConjunction hasprecision p in (0, 1).7Theorem 3.5 Suppose Mt, Mt?1, Kt, F?t, F?t?1,?Ft, and T are defined as stated in the statementof Theorem 3.2.
Let the contingency tables be de-fined as they were in the proof of Theorem 3.2.
LetMPositiveConjunction be a model that only classi-fies an example as positive if both models Mt andMt?1 classify the example as positive.
Supposethat MPositiveConjunction has precision p on thestop set.
Then Kt > T ?
|?Ft| ?
4(1?T )(p+1)T .Proof The proof of Theorem 3.2 holds exactly asit is up until Equality 27.
MPositiveConjunction hasprecision p on the stop set?
p = a1a1+a?1 .
Solv-ing for a1 in terms of a?1 we have a1 = p1?pa?1.Therefore, applying Lemma 3.4 with x = p1?p , wehave ah+dA+dB ?p1?p+12p1?p+1.
Therefore we have|?Ft| ?
4(p1?p+12p1?p+1)(1?T )T (31)= 4(1?T )(p+1)T .
(32)7The case when p = 0 is handled by Theorem 3.2 and thecase when p = 1 is handled by Theorem 3.3.16Precision 1p+1 (to 3 decimal places)50% 0.66780% 0.55690% 0.52695% 0.51398% 0.50599% 0.50399.9% 0.500Table 8: Values of the scaling factor from Theo-rem 3.5 for different precision values.The scaling factor 1p+1 in Theorem 3.5 showshow the precision of the conjunctive model affectsthe bound.
Theorem 3.2 had the scaling factor im-plicitly set to 1 in order to handle the pathologi-cal case where the positive conjunctive model hasprecision = 0.
In Theorem 3.3, where the positiveconjunctive model has precision = 1 on the exam-ples in the stop set, the scaling factor is set to 1/2.Theorem 3.5 generalizes the scaling factor so thatit is a function of the precision of the positive con-junctive model.
For convenience, Table 8 showsthe scaling factor values for a few different preci-sion values.The bounds in Theorems 3.2, 3.3, and 3.5 allbound the difference in performance on the stopset of two consecutively learned models Mt andMt?1.
An issue to consider is how connected thedifference in performance on the stop set is to thedifference in performance on a stream of applica-tion examples generated according to the popula-tion probabilities.
Taking up this issue, considerthat the proof of Theorems 3.2, 3.3, and 3.5 wouldhold as it is if we had used sample proportions in-stead of sample counts (this can be seen by simplydividing every count by n, the size of the stop set).Since the stop set is unbiased (selected at randomfrom the population), as n approaches infinity, thesample proportions will approach the populationprobabilities and the difference between the dif-ference in performance between Mt and Mt?1 onthe stop set and on a stream of application exam-ples generated according to the population proba-bilities will approach zero.4 ConclusionsTo date, the work on stopping criteria has beendominated by heuristics based on intuitions andexperimental success on a small handful of tasksand datasets.
But the methods are not widelyusable in practice yet because our community?sunderstanding of the stopping methods remainstoo inexact.
Pushing forward on understandingthe mechanics of stopping at a more exact levelis therefore crucial for achieving the design ofwidely usable effective stopping criteria.This paper presented the first theoretical anal-ysis of stopping based on stabilizing predictions.The analysis revealed three elements that are cen-tral to the SP method?s success: (1) the sample es-timates of Kappa have low variance; (2) Kappa hastight connections with differences in F-measure;and (3) since the stop set doesn?t have to be la-beled, it can be arbitrarily large, helping to guar-antee that the results transfer to previously unseenstreams of examples at test/application time.We presented proofs of relationships betweenthe level of Kappa agreement and the difference inperformance between consecutive models.
Specif-ically, if the Kappa agreement between two mod-els is at least T, then the difference in F-measureperformance between those models is boundedabove by 4(1?T )T .
If precision of the positive con-junction of the models is assumed to be p, then thebound can be tightened to 4(1?T )(p+1)T .The setup and methodology of the proofs canserve as a launching pad for many further inves-tigations, including: analyses of stopping; worksthat consider switching between different activelearning strategies and operating regions; andworks that consider stopping co-training, and es-pecially agreement-based co-training.
Finally, therelationships that have been exposed between theKappa statistic and F-measure may be of broaderuse in works that consider inter-annotator agree-ment and its interplay with system evaluation, atopic that has been of long-standing interest.ReferencesShlomo Argamon-Engelson and Ido Dagan.
1999.Committee-based sample selection for probabilis-tic classifiers.
Journal of Artificial Intelligence Re-search (JAIR), 11:335?360.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Jason Baldridge and Miles Osborne.
2008.
Ac-tive learning and logarithmic opinion pools for hpsgparse selection.
Nat.
Lang.
Eng., 14(2):191?222.17Yoram Baram, Ran El-Yaniv, and Kobi Luz.
2004.
On-line choice of active learning algorithms.
Journal ofMachine Learning Research, 5:255?291, March.Yvonne M. Bishop, Stephen E. Fienberg, and Paul W.Holland.
1975.
Discrete Multivariate Analysis:Theory and Practice.
MIT Press, Cambridge, MA.Michael Bloodgood and Chris Callison-Burch.
2010.Bucking the trend: Large-scale cost-focused activelearning for statistical machine translation.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, pages 854?864,Uppsala, Sweden, July.
Association for Computa-tional Linguistics.Michael Bloodgood and K Vijay-Shanker.
2009a.
Amethod for stopping active learning based on stabi-lizing predictions and the need for user-adjustablestopping.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL-2009), pages 39?47, Boulder, Colorado,June.
Association for Computational Linguistics.Michael Bloodgood and K Vijay-Shanker.
2009b.
Tak-ing into account the differences between activelyand passively acquired data: The case of activelearning with support vector machines for imbal-anced datasets.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associa-tion for Computational Linguistics, pages 137?140,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Michael Bloodgood.
2009.
Active learning with sup-port vector machines for imbalanced datasets and amethod for stopping active learning based on sta-bilizing predictions.
Ph.D. thesis, University ofDelaware, Newark, DE, USA.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In COLT?98: Proceedings of the eleventh annual conferenceon Computational learning theory, pages 92?100,New York, NY, USA.
ACM.J.
Carletta.
1996.
Assessing agreement on classifica-tion tasks: The kappa statistic.
Computational lin-guistics, 22(2):249?254.Stephen Clark, James Curran, and Miles Osborne.2003.
Bootstrapping pos-taggers using unlabelleddata.
In Walter Daelemans and Miles Osborne,editors, Proceedings of the Seventh Conference onNatural Language Learning at HLT-NAACL 2003,pages 49?55.J.
Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Mea-surement, 20:37?46.David A. Cohn, Zoubin Ghahramani, and Michael I.Jordan.
1996.
Active learning with statistical mod-els.
Journal of Artificial Intelligence Research,4:129?145.Meryem Pinar Do?nmez, Jaime G. Carbonell, andPaul N. Bennett.
2007.
Dual strategy activelearning.
In Joost N. Kok, Jacek Koronacki,Ramon Lo?pez de Ma?ntaras, Stan Matwin, DunjaMladenic, and Andrzej Skowron, editors, MachineLearning: ECML 2007, 18th European Conferenceon Machine Learning, Warsaw, Poland, September17-21, 2007, Proceedings, volume 4701 of Lec-ture Notes in Computer Science, pages 116?127.Springer.Seyda Ertekin, Jian Huang, Le?on Bottou, and C. LeeGiles.
2007a.
Learning on the border: active learn-ing in imbalanced data classification.
In Ma?rio J.Silva, Alberto H. F. Laender, Ricardo A. Baeza-Yates, Deborah L. McGuinness, Bj?rn Olstad, ?ys-tein Haug Olsen, and Andre?
O. Falca?o, editors, Pro-ceedings of the Sixteenth ACM Conference on Infor-mation and Knowledge Management, CIKM 2007,Lisbon, Portugal, November 6-10, 2007, pages 127?136.
ACM.Seyda Ertekin, Jian Huang, and C. Lee Giles.
2007b.Active learning for class imbalance problem.
InWessel Kraaij, Arjen P. de Vries, Charles L. A.Clarke, Norbert Fuhr, and Noriko Kando, editors,SIGIR 2007: Proceedings of the 30th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, Amsterdam,The Netherlands, July 23-27, 2007, pages 823?824.ACM.Joseph L. Fleiss, Jacob Cohen, and B. S. Everitt.
1969.Large sample standard errors of kappa and weightedkappa.
Psychological Bulletin, 72(5):323 ?
327.Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-tali Tishby.
1997.
Selective sampling using thequery by committee algorithm.
Machine Learning,28:133?168.Masood Ghayoomi.
2010.
Using variance as a stop-ping criterion for active learning of frame assign-ment.
In Proceedings of the NAACL HLT 2010Workshop on Active Learning for Natural LanguageProcessing, pages 1?9, Los Angeles, California,June.
Association for Computational Linguistics.Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the effects of selective sampling on theannotation task.
In Proceedings of the Ninth Confer-ence on Computational Natural Language Learning(CoNLL-2005), pages 144?151, Ann Arbor, Michi-gan, June.
Association for Computational Linguis-tics.Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-roll, and Peter McClanahan.
2008.
Assessing thecosts of sampling methods in active learning for an-notation.
In Proceedings of ACL-08: HLT, Short Pa-pers, pages 65?68, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.Gholamreza Haffari and Anoop Sarkar.
2009.
Activelearning for multilingual statistical machine trans-lation.
In Proceedings of the Joint Conference of18the 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 181?189, Suntec,Singapore, August.
Association for ComputationalLinguistics.Cecilia A. Hale and Joseph L. Fleiss.
1993.
Interval es-timation under two study designs for kappa with bi-nary classifications.
Biometrics, 49(2):pp.
523?534.Rebecca Hwa.
2000.
Sample selection for statisticalgrammar induction.
In Hinrich Schu?tze and Keh-Yih Su, editors, Proceedings of the 2000 Joint SIG-DAT Conference on Empirical Methods in NaturalLanguage Processing, pages 45?53.
Association forComputational Linguistics, Somerset, New Jersey.Florian Laws and Hinrich Schu?tze.
2008.
Stopping cri-teria for active learning of named entity recognition.In Proceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages465?472, Manchester, UK, August.
Coling 2008 Or-ganizing Committee.David D. Lewis and William A. Gale.
1994.
A se-quential algorithm for training text classifiers.
In SI-GIR ?94: Proceedings of the 17th annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 3?12, NewYork, NY, USA.
Springer-Verlag New York, Inc.D.
Roth and K. Small.
2008.
Active learning forpipeline models.
In Proceedings of the NationalConference on Artificial Intelligence (AAAI), pages683?688.Nicholas Roy and Andrew McCallum.
2001.
Towardoptimal active learning through sampling estimationof error reduction.
In In Proceedings of the 18th In-ternational Conference on Machine Learning, pages441?448.
Morgan Kaufmann.Manabu Sassano.
2002.
An empirical study of activelearning with support vector machines for japaneseword segmentation.
In ACL ?02: Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 505?512, Morristown, NJ,USA.
Association for Computational Linguistics.Greg Schohn and David Cohn.
2000.
Less is more:Active learning with support vector machines.
InProc.
17th International Conf.
on Machine Learn-ing, pages 839?846.
Morgan Kaufmann, San Fran-cisco, CA.Burr Settles and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 1070?1079, Honolulu, Hawaii, October.Association for Computational Linguistics.H.
S. Seung, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In COLT ?92: Proceedings ofthe fifth annual workshop on Computational learn-ing theory, pages 287?294, New York, NY, USA.ACM.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, andChew-Lim Tan.
2004.
Multi-criteria-based ac-tive learning for named entity recognition.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 589?596, Barcelona, Spain, July.Cynthia A. Thompson, Mary Elaine Califf, and Ray-mond J. Mooney.
1999.
Active learning for naturallanguage parsing and information extraction.
In InProceedings of the 16th International Conference onMachine Learning, pages 406?414.
Morgan Kauf-mann, San Francisco, CA.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
An approach to text corpus constructionwhich cuts annotation costs and maintains reusabil-ity of annotated data.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 486?495.Andreas Vlachos.
2008.
A stopping criterion foractive learning.
Computer Speech and Language,22(3):295?312.Jingbo Zhu and Eduard Hovy.
2007.
Active learn-ing for word sense disambiguation with methods foraddressing the class imbalance problem.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 783?790.Jingbo Zhu, Huizhen Wang, and Eduard Hovy.
2008a.Learning a stopping criterion for active learning forword sense disambiguation and text classification.In IJCNLP.Jingbo Zhu, Huizhen Wang, and Eduard Hovy.
2008b.Multi-criteria-based strategy to stop active learningfor data annotation.
In Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics (Coling 2008), pages 1129?1136, Manchester,UK, August.
Coling 2008 Organizing Committee.19
