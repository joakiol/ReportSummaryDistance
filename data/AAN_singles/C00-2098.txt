A Context-Sensitive Model for Probabilistie LR Parsing of SpokenLanguage with Transformation-Based PostproeessingTobias Ruland, Siemens AG, ZT IK 5, 1)-81730 MtinchenTel.
: +49-173-369 30 67, Fax: +49-89-929 54 54, Tobias.Ruland@web.deAbstractThis paper describes a hybrid approach tospontaneous speech parsing.
The implelnentedparser uses an extended probabilistic LR parsingmodel with rich context and its output is post-processed by a symbolic tree transformation routinethat tries to eliminate systematic errors of theparser.
The parser has been trained for threedifferent languages and was successflflly integratedin tile Verbmobil speech-to-speech translationsystem.
The parser achieves more than 90%/90%labeled precision/recall on pmsed Verbmobilutterances while 3% of German and 5% of allEnglish input caunot be parsed.1 IntroductionVerbmobil (Wahlster, 1993) is a spontaneousspeech-to-speech translation system and translatesspoken German to English/Japanese and vice versa.Tile main domains are "appointment scheduling"and "travel planning".
There are several parallelanalysis and translation modules in Verbmobil asdescribed in (Ruhmd et al, 1998) and one of thoseanalysis modules is the probabilistic parserde,;cribed ill this paper.
A schematic diagraln of theVerbmobil system architecture is shown in figure 1.The input for the Vcrbmobil speaker independentspeech recognizers is spontaneously spokenGerman (vocabuhlry 10,254 word forms), English(7,534 word forms) and Japanese (2,848 wordforms).
The output of the speech recognizers andtile prosody module is a prosodically annotatedword graph.
This word graph is sent to theIntegrated Processing module which controls thethree parsers (HPSG parser (Kiefer et al, 1999),chunk parser (Abney, 1991) and our probabilisticparser) of tile "deep" (semantics based) translationbranch of Verbmobil.
Our probabilistic parser is ashift-reduce parser and uses an A*-search to findthe best scored path in the lattice that can be parsedby its context fi'ee grammar.
Tile output of tileparser is the best scored context free analysis forthis path.
This syntax tree is passed to atransformation unit that corrects known systematicerrors of tile probabilistic parser to correct trees.The result of this process is passed to a semanticsconstruction module and processed by the othermodules of the deep translation branch as shown infigure 1.2 Spontaneous Speech ParsingTile Integrated Processing unit uses tile acousticscores of the word hypotheses in tile word graphand a statistical trigram model to guide allconnected parsers through the lattice using an A*-search algorithm.
This is similar to the workpresented by (Schmkl, 1994) and (Kompe et al,1997).
This A*-search algorithm is used by theprobabilistic shift-reduce parser (see section 3) tofind the best scored path through the word graphaccording to acoustic and hmguage modelinfornmtion.
If the parser uns into a syntactic "deadend" in the word graph (that is a path that cannot beanalyzed by tile context-fl'ee gralllmar of the shift-reduce pmser), the parser searches the best SCOledalternative path ill tile word graph, that call beparsed using tile context-fiee grammar.We extracted context fiee grammars for German,English and Japanese flom the Verbmobil treebank(German: 25,881 trees; English: 23,140 trees;Japanese: 4,534 trees) to be able to parsespontaneous utterances.
The treebanks consist ol'annotated transliterations of face-to-face dialogs inthe Verbmobil domains and contain utterances like?
and then well you you you have hotelin./bnnationno 1 am not how about what aboulTuesday the sixteenthactually it yeah so seven hour fi ightThe gramnmr of the parser covers onlyspontaneous speech phenomenas that are containedin the treebanks.During the developlnent o1' the parser weencountered severe problems with the size of thecontext-free grammar extracted from the treebanks.The German grammar extracted from a treebankcontaining 20,000 trees resulted in a LALR parsingtable with lnore than 3,000,000 entries, whichcannot be trained on only 20,000 utterances.
Thereason was that there are many rules in thetreebank, which occur only once or twice but inl'latethe context-flee grammar and thus tile size of the677example-based translation"Guten TagHerr Som~taMF.lg"Figure 1size of the parsing table.
For this reason weeliminate trees from our training materialcontaining rules that occur unfrequently in thetreebank and use only rules achieving a lninimalrule count.
This threshold is determinedexperimentally in our training process.3 A new context sensitive approach toprobabilistic shift-reduce parsingThe work of Siemens in Verbmobil phase 1 showedthat a combination of shift-reduce and unification-based parsing of word graphs works well onspontaneous speech but is not very robust on low-word-accuracy input (the word error rate of theVerbmobil speech recognizers i about 25% today).One way to gain a higher degree of robustness i touse a context-free grammar instead of anunification-based grammar, hence we decided toimplement and test a context-fi'ee probabilisticLALR parser in Verbmobil phase 2.3.1.
Previous approachesThere am several approaches (see for example(Wright & Wrigley, 1991), (Briscoe & Carroll,1993/1996), (Lavie, 1996) or (Inui et al, 1997)) toprobabilistic shift-reduce parsing but only Lavie'sparser, whose probabilistic model is very similar to(Briscoe & Carroll, 1993), has been tested onspontaneously spoken utterances.While the model presented by (Wright &Wrigley, 1991) was equivalent to the standardPCFG (probabilistic context-free grammar, see(Charniak, 1993)) model, which is not context-sensitive and thus has certain limitations in theprecision that it can achieve, later work tried toimplement slight context-sensitivity (as e.g.
theprobability of a shift/reduce-action in Briscoe andCarroll's model depends oll the current andsucceeding LR parser state and the look-aheadsymbol).3.2.
Bringing context to probabilistie shift-reduce parsingLike other work oi1 probabilistic parsing our modelis based on the equationP(T IW) - -V(T ) 'P (WIT)  ' (2)where T is the analysis of a word sequence W and awidely used approximation for P(~T)  is given byP(WIT)~ lq P(w, ll) , (3)w,GWwhere /i is the part-of speech tag for word wi inanalysis T.Finding a realistic approximation for P(7) is verydifficult but important to achieve high parsingaccuracy.
Supposed we approximate P(WIT) byequation (3).
Then P(WIT) is nothing more thanP(~L), where L is the part-of-speech tag sequencefor a given utterance W. If our goal is to select thebest analysis T for a given tag sequence L we do notnecessarily depend on a good approximation ofP(T), but simply select he best analysis for a givenL by finding a T that maximizes P(TIL ) (and notP(7)).
Hence, in our model we use P(7\]L) instead ofP(T) so that, (4 )kwhere Tk is the set of possible analyses for L. Let Dbe the set of all complete shift-reduce parser actionsequences for L, i.e.
dk is the sequence of shift- andreduce-actions that generates analysis Tk.
Then we678can define P(dIL) (=I'(7\]L)) asHVdcD:  V(d lL )=HV(a , , I k , , )  , (5)j - .
Iwhere \[d\] is the number of parser actions in d, adj isthejth parser action in d and &,: is the context of tileparser while executing ad,i.3.3.
Choosing a context"C, ontext" ill equation (5) might be everything.
Itcan be tile classical (CurrentParserState;LookAheadSymbol)-tuple, it may also containiuformation about the following (look-ahead)word(s), elements on the parser stack or tile mostprobable dialogue act of tile utterance, evensemantical iuformation about roles of thesyntactical head of the phrase on the top of theparser stack.The training procedure of our probabilistic parseris straightforward:I. Construct complete parser action sequencesfor each tree in the training set.
Save allinformation (on every action) about he whole"context" we have chosen to use.2.
Count the occurences of all actions indifferent subcontexts.
A subcontext may bethe whole context or a (even empty) selectionof features o1' the whole context.
Compute theprobability of a parser action regarding to thesubcontext as the relative frequency of theaction within lifts subcontext.The reason why we build subcontexts i  thatthere is a relevant sparse-data-problem inVerbmobil.
A treebank containing between 20,000and 30,000 trees is too small to give reliable wtluesfor larger contexts in a parsing table containing500,000 entries or more.
Hence we use thesmoothing technique that is known as backing-offin statistical language modelling (Chamiak, 1993)and approximate he probability of an action a withcontext k using its subcontexts ci:1"(alk)=C, (6)<,1"(.I..,)with ~x~.
smnming up to 1.
Tile values for ~x: aredetermined experimentally.
We have chosen threecontexts for evaluation (KI and K2 also exist in ourmodel but are irrelevant for this evaluation):?
K3: LR parser state and look-aheadsymbol,?
K4 :K3  plus phrase head of the topelement of the LR parsing stack,?
K5:K4 plus look-ahead word.Please see section 5.1. for tile detailed results of thisevaluation.4 Transformation-based error correctionParsing spontaneous speech - even in a limiteddomain - is a quite ambitious task for a context fi'eegranunar parser.
We have a large set of non-terminals ill our grammar that also encodefunctional information like Head or Modifier,gralnmatical information like accusative-complelnent or vexb-prefix besides phrase structureinformation.
Our current grammars contain 240non-terminals for German, 178 for English and 200for Japanese and the lexicon is derivedautomatically fiom the tree bank and externalresources (there were only minor efforts inimproving the lexicon manually).During the development of the parser weobserved a constantly declining Exacl Match rate oftile parser fiom over 80% in the early stages (withjust a few hundred trees of training data) to under50% today.
The reason was that the first trainingsamples were simple utterances on "appointmentscheduling" only, while the treebank nowadayscontains pontaneous tterances from two domainsand that there was a growing number ofinconsistencies ill the treebank due to annotationerrors and a growing number of annotators.
Hencewe had lo develop a technique to improve the exactlnatch rate particularly with regard to the followingsemantics construction process that depends oncorrect syntactic analyses to produce a correctsemantic representation f the utterance.
(Brill, 1993) applied transformation-basedlearning lnethods to natural language processing,especially to part-of-speech tagging.
He showedthat it can be effective to let a system make a firstguess that may be improved or corrected byfollowing transformation-based teps.
We observedmany systematical errors in tile output of theprobabilistic parser, hence we adopted this idea andtook tile probabilistic shift-reduce parser as theguesser and tried to learn tree transformations fromour training data to improve this first guess.
Weintegrated the learned transformations intoVerbmobil as shown in figure 2.The transforlnations map a tree to another tree,changing parts that had be identified as incorrect inthe learning process.
The output of the learningprocess are simple Prolog clauses of tile form679offline"utterance" ~Probabilist i c "~ans  format ion~ ,.~-~.~ parser .Ji X rul es  ~ '  ~treeba lexicon<~probabilistic h i,j tPeebank tparser ~/' utterances ~ ~  I utterances 1~transformation ~~-~-~'~i learningsemanticsconstructionFigure 2~ _,~\[" Verbmobil ~ "translated ...Jransla n ~ utterance"t rans  (+ InputTree ,  -OutputTree)  : -  !
.
,that are sorted by the number of matches on thetraining corpus.4.1 The Prob lemThe task of learning transformations that aresuitable to post-process the output of a probabilisticparser can be implemented asshown in figure 2:1. train the probabilistic parser on a training setO (containing utterances and their human-annotated analyses).2. parse all utterances of O and save theCO~Tesponding parser outputs P.3.
find the set of as-general-as-possibletransformations T that map all incorrect reesof P into corresponding correct rees in O andselect the "optimal" transformation from thisset.The first point has been described in section 3.3.and the second point is trivial.
The as-general-as-possible tran,sfonnation is the mapping of a tree ofP into a tree for the same utterance in O thatachieves a high degree of generalization a d fulfilscertain conditions, which are explained in section4.2.1.
find the set (\] of all common subtrees of r\[)and 0.2. find the set ;~ of all potential transformations.A transformation t is formed by substitution(0i) of one or more elements of ~) by logicalvariables in @ und 0 (i.e.
t: 0~(@) ~ 0~(0))3. choose the "optilnal" transformation from ~.Syntactical trees are represented asProlog terms inour learning process.
Since the transformationshould be able to map large correct structures in </)to their (correct) counterparts in O the first point ofthe algorithm is done by setting (} equal to the setof all (Prolog) subtenns that are common in @ and0 (i.e.
G=subterms (?\[)) (\]subterms (0))JIt is crucial here to attach a unique identifier toeach word (like "l-hi","2-Mr.","3-Smith") becauseone word (like the article "the") could occur severaltimes in one sentence and it is important to keepthose occurences eparated for the second step ofthe learning algorithm.The second step computes all potential treetransformations by substituting one or moreelements of O in q) and 0 by identical (Prolog)variables.
In this regard "substitution" is anoperation, that is inverse to the substitution known4.2.
The Learning AlgorithmThe learning algorithm to derive the most generaltree transformations for incorrect trees in O isstraightforward.
To find the most generaltransformation for a source tree @EP to be mappedinto a destination tree ()cO do:subt rees  (+Tree ,  -SubTrees)  could simplybe defined (in Prolog) assubt rees(+T, -S )  :- f inda l l (X ,  subt ree(X ,T )  ,S) .subt ree  (S, S) .subt ree(S ,_ :L )  :- member (M,L )  , subt ree(S ,N)  .Trees are represented as terms like a:\[b,c\], forexalnple.680flom predicate logic.Choosing tile "optinml" transformation from thespace of all transl'ormations in the third step is amulti-dimensional problem.
The dilnensions are:?
fault tolerance?
coverage of the training corpus, degree of generalizationlrault tolerance is a parameter that indicates howmany correction errors on the training corpus thehuman supervisor is willing to tolerate, i.e.
howmany of tile correct parser trees may be transformedinto incorrect ones.
Accepting transfom~ation errorsmay improve the grade of generalization of thetransformation but for Verbmobil we decided not tobe fault tolerant.
A correct analysis should be keptcorrect in our point of view.Coverage o/" the training corpus means lhat ifstep 2 of the learning algorithm has found severalpossible transformations l'or a J)-O-pair thetransformation tG'77 that covers the most examplesin P/O shonkl be preferred because thistransformation is likely to occur more often in thertnlning system or test situation.13esides the heuristical generalization criterion ofcoverage of thc training corpus we also introduceda formal one.
If there are several transfornmtionsthat do not generate rrors on the training corpusand have exactly the same lnaximuln coverage, weselect the transformation which has the smallestmean distance of its logical variables to the root ofthe tree, because we expect the most generaltransformation to have its variable parts "near theroot" of the trees.
I)istance is measnred in levelsfrom the root.
For example, tile transformation ifigure 3 has a mean root distance of the variablesof ( (1 +2) + (I +3) ) / 4 = 1.75.jptt  ..... u t t - .IA\] eX o?
@ BI _:px_aufFigure 3Using this learning algorithm we generate a setof optimal transformations for many errors theparser produced on the set of training utterances.There are still some utterances for which no validtransforlnation can be found because all potentialtransforlnations would generate errors on thetraining corpus, what we are not willing to accept.5 Evaluation resultsAt the time tiffs paper is written we have doneseveral experiments on different aspects of outwork, some of which ate published here.5.1.
Experiments on context sensitivityThe question of this experiment was: "We havedeveloped a probabilistic parsing model using morecontext information.
Does it generate any benefit?
"To answer this question we trained the parser on19,750 german trees and tested on 1,000 (unseen)utterances with contexts of different sizes (thecontexts K3, K4 and K5 am explained in section3.3).
As shown in figure 4 (the x-axis is a weightthat controls tile influence of the context in thebacldng-off process) labeled precision of the K5-parser performs always better than the parsers usingless context.
Labeled recall of the K5-parser issuperior as long as the large context is notoverweighted.
Higher weigh|s increase some kindof "memory effect" so that the trained model doesnot generalize well on (unseen) test data.
TheOl)timal K5 weight is around 0.1 and 0.2 as you cansee in figure 4.5.2.
Evaluation of the probabilistie parserWe ewduated the parser on German, English andJapanese Verbmobil data.
The results of thisewtluation are given in the following table:7)'aining set/trees\]Test set \[utterances\]GelTilall19.7501.000English17.7931.000Eract Match 46,3% 55,4%Incorrect parses 50,3% 39,3%Not pmwed 3,4% 5,3%contextj'ree rules 988 2.205Labeled Precision 90,2% 90,6%Labeled Recall (all 83,5% 78,5%utterances)Labeled Recall(parsed utterances) 91,0% 90,9%Japan.3.21830067,7%21,3%!
1,0%93284,9%63,1%86,3%It is quite interesting that despite of tile low exactmatch rate out parser achieves high precision/recallvalues on parsed utterances.
The reason is that wehave - for the semantics construction process - alarge number of nomtenninal symbols in out"context-fiee grammars and the parser often chooses681Ix\]929088~86oe84o?.a-82807876iLabe led  Prec is ion  K5 - -Labe led  Prec is ion  K4 .
.
.
.
._ _ _ - - ~.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
LabeI~O'PF~T~i6P~-K5 = ........ =.
.
.
.
- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Labe led  Reca l l  K5  .
.
.
.Labe led  Reca l l  K4  .
.
.
.
.Labe led  Reca l l  K~ .
.
.
.
.. .
.
.
.
.
.
.
.
i  .
.
.
.  "
' "  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- ' ' ' ' ' ' "  .
.
.
.
.
.  "
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.. .
.
.
.
".
.
, .
.
.
'.
.
.
.
.
".
.
.
- .
".
.
.
.
.
.
i.i.'.
: .
.
.
.
.
.
.
.
.. .
, .
.
- ', .
.
.
.. .
.
.
.
.
.
.
.
:  .L- .
-  _ .=:  _._ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
~ ~'-~ =--- "-- -2 -_-_-2222-- --2- =- ~=-~z- _.oI ,I I I I0+1 0.2 0.3 0+4 0+5Figure 40.6only one or two slightly incorrect symbols perparse.
The mean parsing time per utterance wasabout 400ms for German and English and about30ms for Japanese on a 166-Mhz Sun Ultra-Iworkstation.5.3.
Influence of transformation-based errorcorrectionIt is important to have a very high exact match ratefor the semantics construction process.
As showu inthe table of section 5.2. the exact match rates arequite low thus we have learned transformationsfrom the training data to improve the output of theGerman and English parser (there was not enoughtraining data to do so for Japanese) and evaluatedthe results shown in the following table (TT is anabbreviation for Tree Translfbrmations).As shown in this table the tree transformationsimprove the exact match rate relatively by 16% forGerman and 10% for English.German EnglishExact Match (w/o TT) 46,3% 55,4%hlcorrect parses 50,3% 39,3%Not parsed 3,4% 5,3%Exact Match (after 77) 53,8% 61,2%Incorrect parses (after TT) 42,8% 33,5%Labeled Precision (w/o 7T) 90,2% 90,6%German EnglishLabeled Precision (after TT) 90,8% 91,4%Labeled Recall (all 83,5% 78,5%utterances, w/o TT)Labeled Recall (all 84,0% 79,2%utterances, after TT)Labeled Recall (parsed 91,0% 90,9%utterances, w/o TT)Labeled Recall (parsed 91,6% 91,7%utterances, after TT)6 ConclusionIn this article we have extended probabilistic shift-reduce parsing to be more context-sensitive thanprevious works and have demonstrated that a biggercontext improves the performance of a probabilisticshift-reduce parser.
It was shown that our model issuitable to parse utterances of the Verbmobildomain in three different languages.
It was alsoshown that the exact match rate of a probabilisticparser can be improved significantly using asymbolic transformation-based post-processingstep.Our method of learning tree transforlnations hasgenerated first promising results but it is based onthe mapping of whole trees to whole trees.
It couldbe a direction of further research to extend thisprocess of learning transformations on smaller682(sub-)structures like single phrases.
That shouldimprove generalization and hel t ) improving theexact match rate on the difficult dolnain of parsingspontaneously spoken utterances.AcknowledgementsThis research was supported by the German FederalMinistry for Education, Science, Research andTechnology under grant no.
01IV701A3.
I woukllike to thank all Verbmobil colleagues, especiallythe colleagues of IMS Stuttgart and University ofTiibingen, who supported this work by theircooperation, i would also like to thank theanonymous reviewers for their valuable comments.ReferencesAbney, S. P. Palwing by Chunks.
In: Berwick, R. C.,Abney, S. P., Tenny, C.
(eds.)
l'rincO~le-BasedPal;s'ing: Computation and Psycholillguistics.
KluwerAcademic Publishers, 1991.Brill, E. A Coqms-Based Applvach To LmtguageLearning.
PhD Thesis, l)ep~}rlment of Computer andInformation Science, University of Pennsylwmia,1993.B,iscoe, T., Carroll, J. Generaliz.ed f'robabilistie LRPatwiug of Natural Language (Corpora) withIh~/ication-Based GrammaJw.
In: CompulationalLinguistics, Vol.
19, No.
l, 1993.Briscoe, T., Carroll, J. Apportiotting Develolmteltt EJfortitt a Plvbabilistic LR-Palwiltg 5),stent hroughEvaluation.
In: l'roceedings of the ACL SIGDATCmtference on Enqfirical Methods in NaturalLanguage Piveessing, Philadelphia, PA. 92-I00, May1996.Charniak, E. Statistical Language Leartting.
MIT Press,Cambridge, Mass., 1993.lnui, K., Sornlertlamvanich, V., Tanaka, H., Tokunaga,T.
A New Fotwlaligation of Probabilistic GLRPalwing.
In: Proceedings of the InternationalWorkshop on Patwing Technologies, 1997.Kiefer, B., Krieger, H.-U., Carroll, J., Malouf, R. A Bagof Useful Techniques ./br Efficient attd Robustl~cuwing.
In: Ptvceedings of the 37th Ammal Meetingc!f the Association for Comptttational Linguistics,ACL-99, pp.
473-480, 1999.Kompe, R., Batliner, A., Block, H.-U., Kiel31ing, A.,Niemann, H., N6th, E., Ruland, T., Schachtl, S.Inq~roviug Patwing of Spontaneous Speech with thettelp of Ptvsodic Boundaries.
In: Ptwceediugs of theICASSP, pp.
75-78, Mfinchen, 1997.Lavie, A. GLR*: A Robust Grammar-Focused Parserfor Spontaneously Spoken Lallguage.
PhD Thesis,Carnegie Mellon University, Pittsburgh, 1996.Ruland, T., Rupp, C.J., Spilker, J., Weber, H., Worm, K.Makiug the Most of Multiplicity: A Multi-ParserMulti-Strategy Architecture for the Robust Processittgof Spoken Language.
In: Proceedings of the ICSLP,Sidney, 1998.Schmid, L. Patwing Word Graphs Using a LinguisticGrammar and a Statistical lxmguage Model.
In:Pivceedings of the IEEE htternational CotCerence ouAcoustics, Speech attd Signal Processing (ICASSP'94), Adelaide, 1994.Wahlster, W. Translation of face-to-face dialogs.
In:ProceediHgs of MT Summit IV, Kobe, Japan, pp.
127-135, July 1993.Wright, J. H., Wrigley, E. N. GLR Patwing withPivbabilio,.
In: Tomita, M.
(ed.)
Generalised LRPalwing.
Kluwer Academic Publishers, Boston, 199 I.683
