Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 73?81,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsAnnotating UnderquantificationAurelie HerbelotUniversity of CambridgeCambridge, United Kingdomah433@cam.ac.ukAnn CopestakeUniversity of CambridgeCambridge, United Kingdomaac10@cam.ac.ukAbstractMany noun phrases in text are ambigu-ously quantified: syntax doesn?t explicitlytell us whether they refer to a single en-tity or to several, and what portion of theset denoted by the Nbar actually takes partin the event expressed by the verb.
Wedescribe this ambiguity phenomenon interms of underspecification, or rather un-derquantification.
We attempt to validatethe underquantification hypothesis by pro-ducing and testing an annotation schemefor quantification resolution, the aim ofwhich is to associate a single quantifierwith each noun phrase in our corpus.1 Quantification resolutionWe are concerned with ambiguously quantifiednoun phrases (NPs) and their interpretation, as il-lustrated by the following examples:1.
Cats are mammals = All cats...2.
Cats have four legs = Most cats...3.
Cats were sleeping by the fire = Some cats...4.
The beans spilt out of the bag = Most/All ofthe beans...5.
Water was dripping through the ceiling =Some water...We are interested in quantification resolution,that is, the process of giving an ambiguously quan-tified NP a formalisation which expresses a uniqueset relation appropriate to the semantics of the ut-terance.
For instance, we wish to arrive at:6.
All cats are mammals.|??
?| = |?|where ?
is the set of all cats and?
the set of all mammals.Resolving the quantification value of NPs is im-portant for many NLP tasks.
Let us imagine an in-formation extraction system having retrieved thetriples ?cat ?
is ?
mammal?
and ?cat ?
chase ?mouse?
for inclusion in a factual database aboutfelines.
The problem with those representation-poor triples is that they do not contain the nec-essary information about quantification to answersuch questions as ?Are all cats mammals??
or ?Doall cats chase mice??
Or if they attempt to answerthose queries, they give the same answer to both.Ideally, we would like to annotate such triples withquantifiers which have a direct mapping to proba-bility adverbs:7.
All cats are mammals AND Tom is a cat ?Tom is definitely a mammal.8.
Some cats chase mice AND Tom is a cat ?Tom possibly chases mice.Adequate quantification is also necessary for in-ference based on word-level entailment: an exis-tentially quantified NP can be replaced by a suit-able hypernym but this is not possible in non-existential cases: (Some) cats are in my gardenentails (Some) animals are in my garden but (All)cats are mammals doesn?t imply that (All) animalsare mammals.In Herbelot (to appear), we provide a formalsemantics for ambiguously quantified NPs, whichrelies on the idea that those NPs exhibit an under-specified quantifier, i.e.
that for each NP in a cor-pus, a set relation can be agreed upon.
Our formal-isation includes a placeholder for the quantifier?sset relation.
In line with inference requirements,we assume a three-fold partitioning of the quan-tificational space, corresponding to the natural lan-guage quantifiers some, most and all (in additionto one, for the description of singular, unique enti-ties).
The corresponding set relations are:9.
some(?, ?)
is true iff 0 < |?
?
?| < |??
?|10.
most(?, ?)
is true iff |??
?| ?
|??
?| < |?|11.
all(?, ?)
is true iff |?
?
?| = |?|This paper is an attempt to show that our for-malisation lends itself to evaluation by human an-notation.
The labels produced will also serve astraining and test sets for an automatic quantifica-tion resolution system.732 Under(specified) quantificationBefore we present our annotation scheme, we willspell out the essential idea behind what we call un-derquantification.The phenomenon of ambiguous quantificationoverlaps with genericity (see Krifka et al 1995,for an introduction to genericity).
Generic NPsare frequently expressed syntactically as bare plu-rals, although they occur in definite and indefinitesingulars too, as well as bare singulars.
Thereare many views on the semantics of generics(e.g.
Carlson, 1995; Pelletier and Asher, 1997;Heyer, 1990; Leslie, 2008) but one of them is thatthey quantify (Cohen, 1996), although, puzzlinglyenough, not always with the same quantifier:12.
Frenchmen eat horsemeat = Some/Relatively-many Frenchmen... (For the relatively manyreading, see Cohen, 2001.)13.
Cars have four wheels = Most cars...14.
Typhoons arise in this part of the Pacific =Some typhoons... OR Most/All typhoons...This behaviour has so far prevented linguistsfrom agreeing on a single formalisation for allgenerics.
The only accepted assumption is that anoperator GEN exists, which acts as a silent quan-tifier over the restrictor (subject) and matrix (ver-bal predicate) of the generic statement.
The formalproperties of GEN are however subject to debate:in particular, it is not clear which natural languagequantifier it would map onto (some view it as most,but this approach requires some complex domainrestriction to deal with sentences such as 12).In this paper, we take a different approachwhich sidesteps some of the intractable prob-lems associated with the literature on generics andwhich also extends to definite plurals.
Instead oftalking of ambiguous quantification, we will talkof underspecified quantification, or underquan-tification.
By this, we mean that the bare plural,rather than exhibiting a silent, GEN quantifier,simply features a placeholder in the logical formwhich must be filled with the appropriate quan-tifier (e.g., uq(x, cat?
(x), sleep?
(x)), where uq isthe placeholder quantifier).
This account catersfor the facts that so-called generics can so easilybe quantified via traditional quantifiers, thatGENis silent in all known languages, and it explainsalso why it is the bare form which has the high-est productivity, and can refer to a range of quan-tified sets, from existentials to universals.
Usingthe underquantification hypothesis, we can para-phrase any generic of the form ?X does Y?
as ?thereis a set of things X, a certain number of whichdo Y?
(note the partitive construction).
Such aparaphrase allows us to also resolve ambiguouslyquantified definite plurals, which have tradition-ally been associated with universals, outside of thegenericity phenomenon (e.g.
Lyons, 1999).Because of space constraints, we will not giveour formalisation for underquantification in thispaper (see Herbelot,to appear, for details).
It in-volves a representation of the partitive constructexemplified above and requires knowledge of thedistributive or collective status of the verbal pred-icate.
We also argue that if generics can always bequantified, their semantics may involve more thanquantification.
So we claim that in certain cases, adouble formalisation of the NP as a quantified en-tity and a kind is desirable.
We understand kindsin the way proposed by Chierchia (1998), that isas the plurality of all instances denoted by a givenword in the world under consideration.
Under thekind reading, we can interpret 12 as meaning Col-lectively, the group of all Frenchmen has the prop-erty of eating horsemeat.3 Motivation3.1 Linguistic motivationIt is usual to talk of ?annotation?
generically, tocover any process that involves humans using a setof guidelines to mark some specific linguistic phe-nomenon in some given text.
However, we wouldargue that, when considering the aims of an anno-tation task and its relation to the existing linguisticliterature, it becomes possible to distinguish be-tween various types of annotation.
Further, wewill show that our own effort situates itself in alittle studied relation to formal semantics.The most basic type of annotation is theone where computational linguists mark largeamounts of textual data with well-known and well-understood labels.
The production of tree bankslike the Penn Treebank (Marcus et al 1993) makesuse of undisputed linguistic categories such asparts of speech.
The aim is to make the computerlearn and use irrefutable bits of linguistics.
(Notethat, despite agreement, the representation of thosecategories may differ: see for example the rangeof available parts of speech tag sets.)
This typeof task mostly involves basic syntactic knowledge,but can be taken to areas of syntax and seman-74tics where the studied phenomena have a (some-what) clear, agreed upon definition (Kingsbury etal, 2002).
We must clarify that in those cases, thechoice of a formalism may already imply a certaintheoretical position ?
leading to potential incom-patibilities between formalisms.
However, the cat-egories for such annotation are themselves fixed:there is a generally agreed broad understanding ofconcepts such as noun phrases and coordination.Another type of annotation concerns taskswhere the linguistic categories at play are notfixed.
One example is discourse annotation ac-cording to rhetorical function (Teufel et al 2006)where humans are asked to differentiate betweenseveral discursive categories such as ?contrast?
or?weakness?.
In such a task, the computational lin-guist develops a theory where different states orvalues are associated with various phenomena.
Inorder to show that the world functions according tothe model presented, experimentation is required.This usually takes the form of an annotation taskwhere several human subjects are required to markpieces of text following guidelines inferred fromthe model.
The intuition behind the annotation ef-fort is that agreement between humans support theclaims of the theory (Teufel, in press).
In particu-lar, it may confirm that the phenomena in questionindeed exist and that the values attributed to themare clearly defined and distinguishable.
The workis mostly of a descriptive nature ?
it creates phe-nomenological definitions that encompass bits ofobservable language.Our own work is similar to the latter type ofannotation in that it is trying to capture a phe-nomenon that is still under investigation in the lin-guistic literature.
However, it is also different be-cause the categories we use are fixed by language:the quantifiers some, most and all exist and we as-sume that their definition is agreed upon by speak-ers of English.
What we are trying to investigateis whether those quantifiers should be used at allin the context of ambiguous quantification.The type of annotation carried out in this pa-per can be said to have more formal aims than thetasks usually attempted in computational linguis-tics.
In particular, it concerns itself with some ofthe broad claims made by formal semantics: itsmodel-theoretical view and the use of generalisedquantifiers to formalise noun phrases.In Section 1, we assumed that quantifiers de-note relations between sets and presented the taskof quantification resolution as choosing the ?cor-rect?
set relation for a particular noun phrase in aparticular sentence ?
implying some sort of truthvalue at work throughout the process: the correctset relation produces the sentence with truth value1 while the other set relations produce a truth valueof 0.
What we declined to discuss, though, is theway that those reference sets were selected in nat-ural language, i.e.
we didn?t make claims aboutwhat model, or models, are used by humans whenthey compute the truth value of a given quantifiedstatement.
The annotation task may not answerthis question but it should help us ascertain to whatextent humans share a model of the world.In Section 2, we also argued that all subjectgeneric noun phrases could be analysed in termsof quantification.
That is, an (underspecified) gen-eralised quantifier is at work in sentences that con-tain such generic NPs.
It is expected that if theannotation is feasible and shows good agreementbetween annotators, the quantification hypothesiswould be confirmed.
Thus, annotation may allowus to make semantic claims such as ?genericitydoes quantify?.
Note that the categories we assumeare intuitive and do not depend on a particular rep-resentation: it is possible to reuse our annotationwith a different formalism as long as the theoreti-cal assumption of quantification is agreed upon.We are not aware of any annotation work incomputational linguistics that contributes to vali-dating (or invalidating) a particular formal theory.In that respect, the experiments presented in thispaper are of a slightly different nature than thestandard research on annotation (despite the factthat, as we will show in the next section, they alsoaim at producing data for a language analysis sys-tem).3.2 Previous work on genericity annotationThe aim of our work being the production of an au-tomatic quantification resolution system, we needan annotated corpus to train and test our machinelearning algorithm.
There is no corpus that weknow of which would give us the required data.The closest contestants are the ACE corpus (2008)and the GNOME corpus (Poesio, 2000) whichboth focus on the phenomenon of genericity, as de-scribed in the linguistic literature.
Unfortunately,neither of those corpora are suitable for use in ageneral quantification task.The ACE corpus only distinguishes between75?generic?
and ?specific?
entities.
The classificationproposed by the authors of the corpus is there-fore a lot broader than the one we are attempt-ing here and there is no direct correspondencebetween their labels and natural language quanti-fiers: we have shown in Section 2 that genericitydidn?t map to a particular division of the quantifi-cational space.
Furthermore, the ACE guidelinescontradict to some extent the literature on generic-ity.
They require for instance that a generic men-tion be quantifiable with all, most or any.
Thisimplies that statements such as Mosquitoes carrymalaria either refer to a kind only (i.e.
they arenot quantified) or are not generic at all.
Further,despite the above reference to quantification, theauthors seem to separate genericity and universalquantification as two antithetical phenomena, asshown by the following quote: ?Even if the au-thor may intend to use a GEN reading, if he/sherefers to all members of the set rather than the setitself, use the SPC tag?.The GNOME annotation scheme is closer inessence to the literature on genericity and muchmore detailed than the ACE guidelines.
However,the scheme distinguishes only between genericand non-generic entities, as in the ACE corpuscase, and the corpus itself is limited to three gen-res: museum labels, pharmaceutical leaflets, andtutorial dialogues.
The guidelines are thereforetailored to the domains under consideration; forinstance, bare noun phrases are said to be typicallygeneric.
This restricted solution has the advantageof providing good agreement between annotators(Poesio, 2004 reports a Kappa value of 0.82 forthis annotation).4 Annotation corpusWe use as corpus a snapshot of the English ver-sion of the online encyclopaedia Wikipedia.1 Thechoice is motivated by the fact that Wikipedia canbe taken as a fairly balanced corpus: although it ispresented as an encyclopaedia, it contains a widevariety of text ranging from typical encyclopaedicdescriptions to various types of narrative texts(historical reconstructions, film ?spoilers?, fictionsummaries) to instructional material like rules ofgames.
Further, each article in Wikipedia is writ-ten and edited by many contributors, meaning thatspeaker heterogeneity is high.
We would also ex-pect an encyclopaedia to contain relatively many1http://www.wikipedia.orggenerics, allowing us to assess how our quantifi-cational reading fares in a real annotation task.
Fi-nally, the use of an open resource means that thecorpus can be freely distributed.2In order to create our annotation corpus, we firstisolated the first 100,000 pages in our snapshotand parsed them into a Robust Minimal Recur-sion Semantics (RMRS) representation (Copes-take, 2004) using first the RASP parser (Briscoeet al 2006) and the RASP to RMRS converter(Ritchie, 2004).
We then extracted all construc-tions of the type Subject-Verb-Object from the ob-tained corpus and randomly selected 300 of those?triples?
to be annotated.
Another 50 randomtriples were selected for the purpose of annotationtraining (see Section 7.1).We show in Figure 1 an example of an anno-tation instance produced by the parser pipeline.The data provided by the system consists of thetriple itself, followed by the argument structureof that triple, including the direct dependents ofits constituents, the number and tense informationfor each constituent, the file from which the triplewas extracted and the original sentence in whichit appeared.
The information provided to annota-tors is directly extracted from that representation.
(Note that the examples were not hand-checked,and some parsing errors may have remained.
)5 Evaluating the annotationIn an annotation task, two aspects of agreement areimportant when trying to prove or refute a partic-ular linguistic model: stability and reproducibility(Krippendorf, 1980).
Reproducibility refers to theconsistency with which humans apply the schemeguidelines, i.e.
to the so-called inter-annotatoragreement.
Stability relates to whether the sameannotator will consistently produce the same an-notations at different points in time.
The measurefor stability is called intra-annotator agreement.Both measures concern the repeatability of an an-notation experiment.In this work, agreement is calculated for eachpair of annotators according to the Kappa mea-sure.
There are different versions of Kappa de-pending on how multiple annotators are treatedand how the probabilities of classes are calculatedto establish the expected agreement between anno-tators, Pr(e): we use Fleiss?
Kappa (Fleiss, 1971),which allows us to compute agreement between2For access, contact the first author.76digraph G211 {"TRIPLE: weed include pigra" [shape=box];include -> weed [label="ARG1 n"];include -> pigra [label="ARG2 n"];invasive -> weed [label="ARG1 n"];compound_rel -> pigra [label="ARG1 n"];compound_rel -> mimosa [label="ARG2 n"];"DNT INFO: lemma::include() tense::present lpos::v (arg::ARG1 var::weed() num::pl pos::)(arg::ARG2 var::pigra() num::sg pos::)" [shape=box];"FILE: /anfs/bigtmp/newr1-50/page101655" [shape=box];"ORIGINAL: Invasive weeds include Mimosa pigra, which covers 80,000 hectaresof the Top End, including vast areas of Kakadu. "
[shape=box]; }Figure 1: Example of annotation instancemultiple annotators.6 An annotation scheme forquantification resolution6.1 Scheme structureOur complete annotation scheme can be found inHerbelot (to appear).
The scheme consists of fiveparts.
The first two present the annotation materialand the task itself.
Some key definitions are given.The following part describes the various quantifi-cation classes to be used in the course of the an-notation.
Participants are then given detailed in-structions for the labelling of various grammaticalconstructs.
Finally, in order to keep the demandon the annotators?
cognitive load to a minimum,the last part reiterates the annotation guidelines inthe form of diagrammatic decision trees.In the next sections, we give a walk-through ofthe guidelines and definitions provided.6.2 MaterialOur annotators are first made familiar with the ma-terial provided to them.
This material consistsof 300 entries comprising a single sentence anda triple Subject-Verb-Object which helps the an-notator identify which subject noun phrase in thesentence they are requested to label (the ?ORIG-INAL?
and ?TRIPLE?
lines in the parser output ?see Figure 1).
No other context is provided.
Thisis partly to make the task shorter (letting us anno-tate more instances) and partly to allow for somelimited comparison between human and machineperformance (by restricting the amount of infor-mation given to our annotators, we force them ?
tosome extent ?
to use the limited information thatwould be available to an automatic quantificationresolution system, e.g.
syntax).6.3 DefinitionsIn our scheme, we introduce the annotators to theconcepts of quantification and kind.3Quantification is described in simple terms, asthe process of ?paraphrasing the noun phrase ina particular sentence using an unambiguous termexpressing some quantity?.
An example is given.15.
Europeans discovered the Tuggerah Lakes in1796 = Some Europeans discovered the Tug-gerah Lakes in 1796.We only allow the three quantifiers some, mostand all.
In order to keep the number of classesto a manageable size, we introduce the additionalconstraint that the process of quantification mustyield a single quantifier.
We force the annotatorto choose between the three proposed options andintroduce priorities in cases of doubt: most has pri-ority over all, some has priority over the other twoquantifiers.
This ensures we keep a conservativeattitude with regard to inference (see Section 1).Kinds are presented as denoting ?the group in-cluding all entities described by the noun phraseunder consideration?, that is, as a supremum.
(Asmentioned in Section 2, the verbal predicate ap-plies collectively to that supremum in the corre-sponding formalisation.
)Quantification classes are introduced in a sep-arate part of the scheme.
We define the five la-bels SOME, MOST, ALL, ONE and QUANT (for al-ready quantified noun phrases) and give examplesfor each one of them.We try, as much as possible, to keep annotatorsaway from performing complex reference resolu-tion.
Their first task is therefore to simply attempt3Distributivity and collectivity are also introduced in thescheme because they are a necessary part of our proposed for-malisation.
However, as this paper focuses on the annotationof quantification itself, we will not discuss this side of theannotation task.77to paraphrase the existing sentence by appendinga relevant quantifier to the noun phrase to be anno-tated.
In some cases, however, this is impossibleand no quantifier yields a correct English sentence(this often happens in collective statements).
Tohelp our annotators make decisions in those cases,we ask them to distinguish what the noun phrasemight refer to when they first hear it and what itrefers to at the end of the sentence, i.e., when theverbal predicate has imposed further constraintson the quantification of the NP.6.4 GuidelinesGuidelines are provided for five basic phrasetypes: quantified noun phrases, proper nouns, plu-rals, non-bare singulars and bare singulars.6.4.1 Quantified noun phrasesThis is the simplest case: a noun phrase that isalready quantified such as some people, 6 millioninhabitants or most of the workers.
The annotatorsimply marks the noun phrase with a QUANT label.6.4.2 Proper nounsProper nouns are another simple case.
But be-cause what annotators understand as a proper nounvaries, we provide a definition.
We note first thatproper nouns are often capitalised.
It should how-ever be clear that, while capitalised entities suchas Mary, Easter Island or Warner Bros refer tosingular, unique objects, others refer to groups orinstances of those groups: The Chicago Bulls, aRoman.
The latter can be quantified:16.
The Chicago Bulls won last week.
(ALL ?collective)17.
A Roman shows courage in battle.
(MOST ?distributive)We define proper nouns as noun phrases that?contain capitalised words and refer to a conceptwhich doesn?t have instances?.
All proper nounsare annotated as ONE.6.4.3 PluralsPlurals must be appropriately quantified and theannotators must also specify whether they arekinds or not.
This last decision can simply bemade by attempting to paraphrase the sentencewith either a definite singular or an indefinite sin-gular ?
potentially leading to a typical genericstatement.6.4.4 (Non-bare) singularsLike plurals, singulars must be tested for a kindreading.
This is done by attempting to pluralise thenoun phrase.
If pluralisation is possible, then thekind interpretation is confirmed and quantificationis performed.
If not (certain non-mass terms haveno identifiable parts), the singular refers to a singleentity and is annotated as ONE.6.4.5 Bare singularsWe regard bare singulars as essentially plural, un-der the linguistic assumption of non-overlappingatomic parts ?
for instance, water is considered acollection of H2O molecules, rice is regarded asa collection of grains of rice, etc (see Chierchia,1998).
In order to make this relation clear, weask annotators to try and paraphrase bare singularswith an (atomic part) plural equivalent and follow,as normal, the decision tree for plurals:18.
Free software allows users to co-operate inenhancing and refining the programs they use?
Open source programs allow users...When the paraphrase is impossible (as in certainnon-mass terms which have no identifiable parts),the noun phrase is deemed a unique entity and la-belled ONE.7 Implementation and results7.1 Task implementationThree annotators were used in our experiment.One annotator was one of the authors; theother two annotators were graduate students (non-linguists), both fluent in English.
The two grad-uate students were provided with individual train-ing sessions where they first read the annotationguidelines, had the opportunity to ask for clarifi-cations, and subsequently annotated, with the helpof the author, the 50 noun phrases in the train-ing set.
The actual annotation task was performedwithout communication with the scheme author orthe other annotators.7.2 Kappa evaluationWe made an independence assumption betweenquantification value and kind value, and evaluatedagreement separately for each type of annotation.Intra-annotator agreement was calculated overthe set of annotations produced by one of the au-thors.
The original annotation experiment was re-produced at three months?
interval and Kappa was78Class Kind QuantificationKappa 0.85 0.84Table 1: Intra-annotator agreements for both tasksClass Kind QuantificationKappa 0.67 0.72Table 2: Inter-annotator agreements for both taskscomputed between the original set and the new set.Table 1 shows results over 0.8 for both tasks, cor-responding to ?perfect agreement?
according to theLandis and Koch classification (1977).
This indi-cates that the stability of the scheme is high.Table 2 shows inter-annotator agreements ofover 0.6 for both tasks, which correspond to ?sub-stantial agreement?.
This result must be taken withcaution, though.
Although it shows good agree-ment overall, it is important to ascertain in whatmeasure it holds for separate classes.
In an ef-fort to report such per class agreement, we cal-culate Kappa values for each label by evaluatingeach class against all others collapsed together (assuggested by Krippendorf, 1980).Table 3 indicates that substantial agreement ismaintained for separate classes in the kind annota-tion task.
Table 4, however, suggests that, if agree-ment is perfect for the ONE and QUANT classes,it is very much lower for the SOME, MOST andALL classes.
While it is clear that the latter threeare the most complex to analyse, we can showthat the lower results attached to them are partlydue to issues related to Kappa as a measure ofagreement.
Feinstein and Cicchetti (1990), fol-lowed by Di Eugenio and Glass (2004) provedthat Kappa is subject to the effect of prevalenceand that different marginal distributions can leadto very different Kappa values for the same ob-served agreement.
It can be shown, in particu-lar, that an unbalanced, symmetrical distributionof the data produces much lower figures than bal-anced or unbalanced, asymmetrical distributionsbecause the expected agreement gets inflated.
Ourconfusion matrices indicate that our data falls intothe category of unbalanced, symmetrical distribu-tion: the classes are not evenly distributed but an-notators agree on the relative prevalence of eachclass.
Moreover, in the quantification task itself,the ONE class covers roughly 50% of the data.This means that, when calculating per class agree-Class KIND NOT-KIND QUANTKappa 0.63 0.71 0.88Table 3: Per class inter-annotator agreement forthe kind annotationClass ONE SOME MOST ALL QUANTKappa 0.81 0.45 0.44 0.51 0.88Table 4: Per class inter-annotator agreement forthe quantification annotationment, we get an approximately balanced distri-bution for the ONE label and an unbalanced, butstill symmetrical, distribution for the other labels.This leads to the expected agreement being ratherlow for the ONE class and very high for the otherclasses.
Table 5 reproduces the per class agree-ment figures obtained for the quantification taskbut shows, in addition, the observed and expectedagreements for each label.
Although the observedagreement is consistently close to, or over, 0.9, theKappa values differ widely in conjunction with ex-pected agreement.
This results in relatively low re-sults for SOME, MOST and ALL (the QUANT labelhas nearly perfect agreement and therefore doesn?tsuffer from prevalence).Class Kappa Pr(a) Pr(e)ONE 0.814 0.911 0.521SOME 0.445 0.893 0.808MOST 0.438 0.931 0.877ALL 0.509 0.867 0.728QUANT 0.884 0.987 0.885Table 5: The effect of prevalence on per classagreement, quantification task.
Pr(a) is the ob-served agreement between annotators, Pr(e) theexpected agreement.With regard to the purpose of creating a goldstandard for a quantification resolution system, wealso note that out of 300 quantification annota-tions, there are only 14 cases in which a majoritydecision cannot be found, i.e., at least two anno-tators agreed in 95% of cases.
Thus, despite somelow Kappa results, the data can adequately be usedfor the production of training material.44As far as such data ever can be: Reidsma and Carletta,2008, show that systematic disagreements between annota-tors will produce bad machine learning, regardless of theKappa obtained on the data.79In Section 8, we introduce difficulties encoun-tered by our subjects, as related in post-annotationdiscussions.
We focus on quantification.8 Annotation issues8.1 ReferenceAlthough we tried to make the task as simple aspossible for the annotators by asking them to para-phrase the sentences that they were reading, theywere not free from having to work out the refer-ent of the NP (consciously or unconsciously) andwe have evidence that they did not always pickthe same referent, leading to disagreements at thequantification stage.
Consider the following:19.
Subsequent annexations by Florence in thearea have further diminished the likelihood ofincorporation.In the course of post-annotation discussions, itbecame clear that not all annotators had chosen thesame referent when quantifying the subject NP inthe first clause.
One annotator had chosen as refer-ent subsequent annexations, leading to the readingSome subsequent annexations, conducted by Flo-rence in the area, have further diminished the like-lihood of incorporation.
The other two annotatorshad kept the whole NP as referent, leading to thereading All the subsequent annexations conductedby Florence in the area have further diminishedthe likelihood of incorporation.8.2 World knowledgeBeing given only one sentence as context for theNP to quantify, annotators sometimes lacked theworld knowledge necessary to make an informeddecision.
This is illustrated by the following:20.
The undergraduate schools maintain a non-restrictive Early Action admissions pro-gramme.Discussion revealed that all three annotators hada different interpretation of what the mentionedEarly Action programme might refer to, and ofthe duties of the undergraduate schools with re-gard to it.
This led to three different quantifica-tions: SOME, MOST and ALL.8.3 Interaction with timeThe existence of interactions between NP quantifi-cation and what we will call temporal quantifica-tion is not surprising: we refer to the literature ongenericity and in particular to Krifka et al(1995)who talk of characteristic predication, or habitual-ity, as a phenomenon encompassed by genericity.We do not intend to argue for a unified theory ofquantification, as temporal quantification involvescomplexities which are beyond the scope of thiswork.
However, the interactions observed betweentemporality and NP quantification might explainfurther disagreements in the annotation task.
Thefollowing is a sentence that contains a temporaladverb (sometimes) and that produced some dis-agreement amongst annotators:21.
Scottish fiddlers emulating 18th-centuryplaying styles sometimes use a replica of thetype of bow used in that period.Two annotators labelled the subject of that sen-tence as MOST, while the third one preferredSOME.
In order to understand the issue, considerthe following, related, statement:22.
Mosquitoes sometimes carry malaria.This sentence has the possible readings: Somemosquitoes carry malaria or Mosquitoes, fromtime to time in their lives, carry malaria.
The firstreading is clearly the preferred one.The structure of (21) is identical to that of (22)and it should therefore be taken as similarly am-biguous: it either means that some of the Scottishfiddlers emulating 18th-century playing styles usea replica of the bow used in that period, or that aScottish fiddler who emulates 18th-century play-ing styles, from time to time, uses a replica of sucha bow.
The two readings may explain the labelsgiven to that sentence by the annotators.9 ConclusionTaking prevalence effects into account, we believethat our agreement results can be taken as evidencethat underquantification is analysable in a consis-tent way by humans.
We also consider them asstrong support for our claim that ?genericity quan-tifies?.
Our scheme could however be refined fur-ther.
In a future version, we would add guidelinesregarding the selection of the referent of the nounphrase, encourage the use of external resources toobtain the context of a given sentence (or simplyprovide the actual context of the sentence), andgive some pointers as to how to resolve issues orambiguities caused by temporal quantification.80ReferencesACE.
2008.
ACE (Automatic Content Extraction) En-glish Annotation Guidelines for Entities, Version 6.62008.06.13.
Linguistic Data Consortium.Edward Briscoe, John Carroll and Rebecca Watson.2006.
?The Second Release of the RASP System?.In Proceedings of the COLING/ACL 2006 Interac-tive Presentation Sessions, Sydney, Australia, 2006.Gregory Carlson.
1995.
?Truth-conditions of GenericsSentences: Two Contrasting Views?.
In Gregory N.Carlson and Francis Jeffrey Pelletier, Editors, TheGeneric Book, pages 224 ?
237.
Chicago UniversityPress.Gennaro Chierchia.
1998.
?Reference to kinds acrosslanguages?.
Natural Language Semantics, 6:339?405.Ariel Cohen.
1996.
Think Generic: The Meaningand Use of Generic Sentences.
Ph.D. Dissertation.Carnegie Mellon University at Pittsburgh.
Publishedby CSLI, Stanford, 1999.Ann Copestake.
2004.
?Robust Minimal RecursionSemantics?.
www.cl.cam.ac.uk/?aac10/papers/rmrsdraft.pdf.Barbara Di Eugenio and Michael Glass.
2004.
?Thekappa statistic: a second look?.
Computational Lin-guistics, 30(1):95?101.Alvan R. Feinstein and Domenic V. Cicchetti.
1990.?High agreement but low kappa: I.
The problems oftwo paradoxes?.
Journal of Clinical Epidemiology,43(6):543?549.Joseph Fleiss.
1971.
?Measuring nominal scale agree-ment among many raters?.
Psychological Bulletin,76(5):378-382.Aurelie Herbelot.
To appear.
Underspecified quantifi-cation.
Ph.D. Dissertation.
Computer Laboratory,University of Cambridge, United Kingdom.Gerhard Heyer.
1990.
?Semantics and KnowledgeRepresentation in the Analysis of Generic Descrip-tions?.
Journal of Semantics, 7(1):93?110.Paul Kingsbury, Martha Palmer and Mitch Marcus.2002.
?Adding Semantic Annotation to the PennTreeBank?.
In Proceedings of the Human LanguageTechnology Conference (HLT 2002), San Diego,California, pages 252?256.Manfred Krifka, Francis Jeffry Pelletier, Gregory N.Carlson, Alice ter Meulen, Godehard Link and Gen-naro Chierchia.
1995.
?Genericity: An Introduc-tion?.
In Gregory N. Carlson and Francis JeffryPelletier, Editors.
The Generic Book, pages 1?125.Chicago: Chicago University Press.Klaus Krippendorff.
1980.
Content Analysis: An In-troduction to Its Methodology.
Newbury Park, CA:Sage.J.
Richard Landis and Gary G. Koch.
1977.
?TheMeasurement of Observer Agreement for Categor-ical Data?.
Biometrics, 33:159?174.Sara-Jane Leslie.
2008.
?Generics: Cognition and Ac-quisition.?
Philosophical Review, 117(1):1?47.Christopher Lyons.
1999.
Definiteness.
CambridgeUniversity Press, Cambridge, UK.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
?Building a large annotatedcorpus of english: The penn treebank?.
Computa-tional Linguistics, 19(2):313?330.Francis Jeffry Pelletier and Nicolas Asher.
1997.?Generics and defaults?.
In: Johan van Benthem andAlice ter Meulen, Editors, Handbook of Logic andLanguage, pages 1125?1177.
Amsterdam: Elsevier.Massimo Poesio.
2000.
?The GNOME annota-tion scheme manual?, Fourth Version.
http://cswww.essex.ac.uk/Research/nle/corpora/GNOME/anno_manual_4.htmMassimo Poesio.
2004.
?Discourse Annotation and Se-mantic Annotation in the GNOME Corpus?.
In: Pro-ceedings of the ACL Workshop on Discourse Anno-tation, Barcelona, Spain.Dennis Reidsma and Jean Carletta.
2008.
?Reliabilitymeasurement without limits?.
Computational Lin-guistics, 34(3), pages 319?326.Anna Ritchie.
2004.
?Compatible RMRS Repre-sentations from RASP and the ERG?.
http://www.cl.cam.ac.uk/TechReports/UCAM-CL-TR-661.Simone Teufel, Advaith Siddharthan, Dan Tidhar.2006.
?An annotation scheme for citation function?.In: Proceedings of Sigdial-06, Sydney, Australia,pages 80?87.Simone Teufel.
2010.
The Structure of Scientific Arti-cles: Applications to Summarisation and CitationIndexing.
CSLI Publications.
In press.81
