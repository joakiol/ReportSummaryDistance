Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 289?292,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsEvaluation Metrics for the Lexical Substitution TaskSanaz Jabbari Mark Hepple Louise GuthrieDepartment of Computer Science, University of Sheffield211 Portobello Street, Sheffield, S1 4DP, UK{S.Jabbari,M.Hepple,L.Guthrie}@dcs.shef.ac.ukAbstractWe identify some problems of the evaluationmetrics used for the English Lexical Substitu-tion Task of SemEval-2007, and propose al-ternative metrics that avoid these problems,which we hope will better guide the future de-velopment of lexical substitution systems.1 IntroductionThe English Lexical Substitution task at SemEval-2007 (here called ELS07) requires systems to findsubstitutes for target words in a given sentence (Mc-Carthy & Navigli, 2007: M&N).
For example, wemight replace the target word match with game inthe sentence they lost the match.
System outputs areevaluated against a set of candidate substitutes pro-posed by human subjects for test items.
Targets aretypically sense ambiguous (e.g.
match in the aboveexample), and so task performance requires a com-bination of word sense disambiguation (by exploit-ing the given sentential context) and (near) synonymgeneration.
In this paper, we discuss some problemsof the evaluation metrics used in ELS07, and thenpropose some alternative measures that avoid theseproblems, and which we believe will better serve toguide the development of lexical substitution sys-tems in future work.1 The subtasks within ELS07divide into two groups, in terms of whether they fo-cus on a system?s ?best?
answer for a test item, or ad-dress the broader set of answer candidates a system1We consider here only the case of substituting for singleword targets.
Subtasks of ELS07 involving multi-word substi-tutions are not addressed.can produce.
In what follows, we address these twocases in separate sections, and then present some re-sults for applying our new metrics for the secondcase.
We begin by briefly introducing the test ma-terials that were created for the ELS07 evaluation.2 Evaluation MaterialsBriefly stated, the ELS07 dataset comprises around2000 sentences, providing 10 test sentences eachfor some 201 preselected target words, which wererequired to be sense ambiguous and have at leastone synonym, and which include nouns, verbs, ad-jectives and adverbs.
Five human annotators wereasked to suggest up to three substitutes for the tar-get word of each test sentence, and their collectedsuggestions serve as the gold standard against whichsystem outputs are compared.
Around 300 sentenceswere distributed as development data, and the re-mainder retained for the final evaluation.To assist defining our metrics, we formally de-scribe this data as follows.2 For each sentence tiin the test data (1 ?
i ?
N , N the number of testitems), let Hidenote the set of human proposed sub-stitutes.
A key aspect of the data is the count of hu-man annotators that proposed each candidate (sincea term appears a stronger candidate if proposed byannotators).
For each ti, there is a function freqiwhich returns this count for each term within Hi(and 0 for any other term), and a value maxfreqicorresponding to the maximal count for any term inHi.
The pairing of Hiand freqiin effect provides amultiset representation of the human answer set.
We2For consistency, we also restate the original ELS07 metricsin these terms, whilst preserving their essential content.289use |S|i in what follows to denote the multiset car-dinality of S according to freqi, i.e.
?a?Sfreqi(a).Some of the ELS07 metrics use a notion of modeanswer mi, which exists only for test items thathave a single most-frequent human response, i.e.a unique a ?
Hisuch that freqi(a) = maxfreqi.To adapt an example from M&N, an item with tar-get word happy (adj) might have human answers{glad ,merry , sunny , jovial , cheerful } with counts(3,3,2,1,1) respectively.
We will abbreviate this an-swer set as Hi= {G:3,M:3,S:2,J:1,Ch:1} where itis used later in the paper.3 Best Answer MeasuresTwo of the ELS07 tasks address how well systemsare able to find a ?best?
substitute for a test item, forwhich individual test items are scored as follows:best(i) =?a?Aifreqi(a)|Hi|i?
|Ai|mode(i) ={1 if bgi= mi0 otherwiseFor the first task, a system can return a set of an-swers Ai(the answer set for item i), but since thescore achieved is divided by |Ai|, returning multipleanswers only serves to allow a system to ?hedge itsbets?
if it is uncertain which candidate is really thebest.
The optimal score on a test item is achieved byreturning a single answer whose count is maxfreqi,with proportionately lesser credit being received forany answer in Hiwith a lesser count.
For the sec-ond task, which uses the mode metric, only a singlesystem answer ?
its ?best guess?
bgi?
is allowed,and the score is simply 0 or 1 depending on whetherthe best guess is the mode.
Overall performance iscomputed by averaging across a broader set of testitems (which for the second task includes only itemshaving a mode value).
M&N distinguish two over-all performance measures: Recall, which averagesover all relevant items, and Precision, which aver-ages only over those items for which the system gavea non-empty response.We next discuss these measures and make an al-ternative proposal.
The task for the first measureseems a reasonable one, i.e.
assessing the ability ofsystems to provide a ?best?
answer for a test item,but allowing them to offer multiple candidates (to?hedge their bets?).
However, the metric is unsatis-factory in that a system that performs optimally interms of this task (i.e.
which, for every test item, re-turns a single correct ?most frequent?
response) willget a score that is well below 1, because the score isalso divided by |Hi|i, the multiset cardinality of Hi,whose size varies between test items (being a con-sequence of the number of alternatives suggested bythe human annotators), but which is typically largerthan the numerator value maxfreqiof an optimal an-swer (unless Hiis singleton).
This problem is fixedin the following modified metric definition, by di-viding instead by maxfreqi, as then a response con-taining a single optimal answer will score 1.best(i) =?a?Aifreqi(a)maxfreqi?
|Ai|best1(i) =freqi(bgi)maxfreqiWith Hi= {G:3,M:3,S:2,J:1,Ch:1}, for example,an optimal response Ai= {M} receives score 1,where the original metric gives score 0.3.
Singletonresponses containing a correct but non-optimal an-swer receive proportionately lower credit, e.g.
forAi= {S} we score 0.66 (vs. 0.2 for the origi-nal metric).
For a non-singleton answer set includ-ing, say, a correct answer and an incorrect one, thecredit for the correct answer will be halved, e.g.
forAi= {S,X} we score 0.33.Regarding the second task, we think it reasonableto have a task where systems may offer only a single?best guess?
response, but argue that the mode met-ric used has two key failings: it is too brittle in beingapplicable only to items that have a mode answer,and it loses information valuable to system rank-ing, in assigning no credit to a response that mightbe good but not optimal.
We propose instead thebest1metric above, which assigns score 1 to a bestguess answer with count maxfreqi, but applies to alltest items irrespective of whether or not they havea unique mode.
For answers having lesser counts,proportionately less credit is assigned.
This metricis equivalent to the new best metric shown beside itfor the case where |Ai| = 1.For assessing overall performance, we suggestjust taking the average of scores across all test items,c.f.
M&N?s Recall measure.
Their Precision met-ric is presumably intended to favour a system thatcan tell whether it does or does not have any goodanswers to return.
However, the ability to draw a290boundary between good vs. poor candidates will bereflected widely in a system?s performance and cap-tured elsewhere (not least by the coverage metricsdiscussed later) and so, we argue, does not need tobe separately assessed in this way.
Furthermore, thefact that a system does not return any answers mayhave other causes, e.g.
that its lexical resources havefailed to yield any substitution candidates for a term.4 Measures of CoverageA third task of ELS07 assesses the ability of systemsto field a wider set of good substitution candidatesfor a target, rather than just a ?best?
candidate.
This?out of ten?
(oot) task allows systems to offer a setAiof upto 10 guesses per item i, and is scored as:oot(i) =?a?Aifreqi(a)|Hi|iSince the score is not divided by the answer setsize |Ai|, no benefit derives from offering less than10 candidates.3 When systems are asked to field abroader set of candidates, we suggest that evalua-tion should assess if the response set is good in con-taining as many correct answers as possible, whilstcontaining as few incorrect answers as possible.
Ingeneral, systems will tackle this problem by com-bining a means of ranking candidates (drawn fromlexical resources) with a means of drawing a bound-ary between good and bad candidates, e.g.
thresh-old setting.4 Since the oot metric does not penaliseincorrect answers, it does not encourage systems todevelop such boundary methods, even though this isimportant to their ultimate practical utility.The view of a ?good?
answer set described abovesuggests a comparison of Aito Hiusing versionsof ?recall?
and ?precision?
metrics, that incorporatethe ?weighting?
of human answers via freqi.
Let usbegin by noting the obvious definitions for recall and3We do not consider here a related task which assesseswhether the mode answer miis found within an answer set ofup to 10 guesses.
We do not favour the use of this metric forreasons parallel to those discussed for the mode metric of theprevious section, i.e.
brittleness and information loss.4In Jabbari et al (2010), we define a metric that directlyaddresses the ability of systems to achieve good ranking of sub-stitution candidates.
This is not itself a measure of lexical sub-stitution task performance, but addresses a component abilitythat is key to the achievement of lexical substitution tasks.precision metrics without count-weighting:R(i) =|Hi?Ai||Hi|P (i) =|Hi?Ai||Ai|Our definitions of these metrics, given below, doinclude count-weighting, and require some explana-tion.
The numerator of our recall definition is |Ai|inot |Hi?
Ai|i as |Ai|i= |Hi?
Ai|i (as freqias-signs 0 to any term not in Hi), an observation whichalso affects the numerator of our P definition.
Re-garding the latter?s denominator, merely dividing by|Ai|i would not penalise incorrect terms (as, again,freqi(a) = 0 for any a /?
Hi), so this is done di-rectly by adding k|Ai?Hi|, where |Ai?Hi| is thenumber of incorrect answers, and k some penaltyfactor, which might be k = 1 in the simplest case.
(Note that our weighted R metric is in fact equiv-alent to the oot definition above.)
As usual, an F-score can be computed as the harmonic mean ofthese values (i.e.
F = 2PR/(P + R)).
For as-sessing overall performance, we might average P ,R and F scores across all test items.R(i) =|Ai|i|Hi|iP (i) =|Ai|i|Ai|i+ k|Ai?Hi|With Hi= {G:3,M:3,S:2,J:1,Ch:1}, for example,the perfect response set Ai= {G,M,S, J,Ch}gives P and R scores of 1.
The responseAi= {G,M,S, J,Ch,X, Y, Z, V,W}, containingall correct answers plus 5 incorrect ones, gets R =1, but only P = 0.66 (assuming k = 1, giving10/(10 + 5)).
The response Ai= {G,S, J,X, Y },with 3 out of 5 correct answers, plus 2 incorrectones, gets R = 0.6 (6/10) and P = 0.75 (6/6 + 2))5 Applying the Coverage measureAlthough the ?best guess?
task is a valuable indicatorof the likely utility of a lexical substitution systemwithin various broader applications, we would arguethat the core task for lexical substitution is coverage,i.e.
the ability to field a broad set of correct substi-tution candidates.
This task requires systems both tofield and rank promising candidates, and to have ameans of drawing a boundary between the good andbad candidates, i.e.
a boundary strategy.In this section, we apply the coverage metrics tothe outputs of some lexical substitution systems, and291Model 1 2 3 4 5 6 7 8 9 10bow .067 .114 .151 .173 .191 .201 .212 .219 .222 .225lm .119 .192 .228 .246 .256 .267 .271 .272 .271 .271cmlc .139 .205 .251 .271 .284 .288 .291 .290 .289 .286KU .173 .244 .287 .307 .318 .321 .320 .318 .314 .311Table 3: Coverage F-scores (macro-avgd), for simple boundary strategies (with penalty factor k = 1).All By part-of-speechModel words nouns adj verb advbow .326 .343 .334 .205 .461lm .393 .372 .442 .252 .562cmlc .414 .404 .447 .311 .534KU .462 .408 .511 .398 .567Table 1: Out-of-ten recall scores for all the systems (witha subdivision by pos of target item).All By part-of-speechModel words nouns adj verb advbow .298 .315 .302 .189 .422lm .371 .35 .408 .24 .539cmlc .395 .383 .419 .31 .506KU .435 .379 .477 .385 .536Table 2: Optimal F-scores (macro-avgd) for coverage,computed over the (oot) ranked outputs of the systems(with penalty factor k = 1).compare the indication it provides of relative sys-tem performance to that of the oot metric.
We con-sider three systems described in Jabbari (2010), de-veloped as part of an investigation into the meansand benefits of combining models of lexical context:(i) bow: a system using a bag-of-words model torank candidates, (ii) lm: using a (simple) n-gram lan-guage model, and (iii) cmlc: using a model that com-bines bow and lm models into one.
We also considerthe system KU, which uses a very large languagemodel and an advanced treatment of smoothing, andwhich performed well at ELS07 (Yuret, 2007).5 Ta-ble 1 shows the oot scores for these systems, includ-ing a breakdown by part-of-speech, which indicate aperformance ranking: bow < lm < cmlc < KUOur first problem is that these systems are devel-oped for the oot task, not coverage, so after rank-5We thank Deniz Yuret for allowing us to use his system?soutputs in this analysis.ing their candidates, they do not attempt to drawa boundary between the candidates worth returningand those not.
Instead, we here use the oot out-puts to compute an optimal performance for eachsystem, i.e.
we find, for the ranked candidates ofeach question, the cut-off position giving the high-est F-score, and then average these scores acrossquestions, which tells us the F-score the systemcould achieve if it had an optimal boundary strategy.These scores, shown in Table 2, indicate a ranking ofsystems in line with that in Table 1, which is not sur-prising as both will ultimately reflect the quality ofcandidate ranking achieved by the systems.Table 3 shows the coverage results achieved byapplying a naive boundary strategy to the systemoutputs.
The strategy is just to always return thetop n candidates for each question, for a fixed valuen.
Again, performance correlates straightforwardlywith the underlying quality of ranking.
Comparingtables, we see, for example, that by always returning6 candidates, the system KU could achieve a cover-age of .32 as compared to the .435 optimal score.ReferencesD.
McCarthy and R. Navigli.
2007.
SemEval-2007 Task 10: English Lexical Substitution Task.Proc.
of the 4th Int.
Workshop on Semantic Eval-uations (SemEval-2007), Prague.S.
Jabbari.
2010.
A Statistical Model of Lexical Con-text, PhD Thesis, University of Sheffield.S.
Jabbari, M. Hepple and L.Guthrie.
2010.
Evaluat-ing Lexical Substitution: Analysis and NewMea-sures.
Proc.
of the 7th Int.
Conf.
on LanguageResources and Evaluation (LREC-2010).
Malta.D.
Yuret.
2007.
KU: Word Sense Disambiguation bySubstitution.
In Proc.
of the 4th Int.
Workshop onSemantic Evaluations (SemEval-2007), Prague.292
