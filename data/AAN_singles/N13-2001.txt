Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 1?7,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsCritical Reflections on Evaluation Practices in Coreference ResolutionGordana Ilic?
HolenDepartment of InformaticsUniversity of OsloNorwaygordanil@ifi.uio.noAbstractIn this paper we revisit the task of quantitativeevaluation of coreference resolution systems.We review the most commonly used metrics(MUC, B3, CEAF and BLANC) on the basis oftheir evaluation of coreference resolution infive texts from the OntoNotes corpus.
We ex-amine both the correlation between the met-rics and the degree to which our human judge-ment of coreference resolution agrees with themetrics.
In conclusion we claim that loss ofinformation value is an essential factor, insuf-ficiently adressed in current metrics, in humanperception of the degree of success or failureof coreference resolution.
We thus conjec-ture that including a layer of mention infor-mation weight could improve both the coref-erence resolution and its evaluation.1 Introduction and motivationCoreference resolution (CR) is the task of link-ing together multiple expressions of a given entity(Yang et al 2003).
The field has experienced asurge of interest with several shared tasks in re-cent years: SemEval 2010 (Recasens et al 2010),CoNLL 2011 (Pradhan et al 2011) and CoNLL2012 (Pradhan et al 2012).
However the field hasfrom the very start been riddled with problems re-lated to the scoring and comparison of CR systems.Currently there are five metrics in wider use: MUC(Vilain et al 1995), B3 (Bagga and Baldwin, 1998),the two CEAF metrics (Luo, 2005) and BLANC (Re-casens and Hovy, 2011).
As there is no global agree-ment on which metrics are the most appropriate, theabove-mentioned shared tasks have used a combi-nation of several metrics to evaluate the contenders.Although coreference resolution is a subproblem ofnatural language understanding, coreference resolu-tion evaluation metrics have predominately been dis-cussed in terms of abstract entities and hypotheticalsystem errors.
In our view, it is of utmost importanceto observe actual texts and actual system errors.2 Background: The metricsIn this section, we will present the five metrics in theusual terms of precision, recall and F-score.
We fol-low the predominant practice and use the term men-tion for individual referring expressions, and entityfor sets of mentions that refer to the same object(Luo et al 2004).
We use the term key entity (K)for gold entities, and response entity (R) for entitieswhich were produced by the CR system.2.1 Link-based: MUC and BLANCThe MUC metric (Vilain et al 1995) is based oncomparing the number of links in the key entity(|K| ?
1) to the number of links missing from theresponse entity, routinely calculated as the numberof partitions of the key entity |p(K)| minus one, soRecall = (|K|?1)?
(|p(K)|?1)|K|?1 =|K|?|p(K)||K|?1 .
Forthe whole document, recalls for entities are simplyadded: Recall =?
|Ki|?|p(Ki)|?
(|Ki|?1) In calculating pre-cision, the case is inverted: The base entity is nowthe response, and the question posed is how manymissing links have to be added to the key partitionsto form the response entity.BLANC (Recasens and Hovy, 2011) is a variant ofthe Rand index (Rand, 1971) adapted for the task1of coreference resolution.
The BLANC metric makesuse of both coreferent and non-coreferent links, cor-rect and incorrect.
The final precision, recall andF-score are the average of the P, R and F-score ofcorresponding coreferential and non-referential val-ues.
However, since this is an analysis of isolatedentities, there are no non-coreferential links.
Forthat reason, in this paper we only present corefer-ential precision, recall and F-score for this metric:Pc = rcrc+wc , Rc = rcrc+wn and Fc = 2PcRcPc+Rc , whererc is the number of correct coreferential links, wcthe number of incorrect coreferential links, and wnis the number of non-coreferential links incorrectlymarked as coreferent by the system.2.2 Entity and mention-based: B3 and CEAFB3 (Bagga and Baldwin, 1998) calculates precisionand recall for every mention in the document, andthen combines them to an overall precision and re-call.
Precision of a single mention mi is the numberof correct mentions in the response entity Ri thatcontainsmi divided by the total number of mentionsin Ri.
Recall of mi is again the number of correctmentions in Ri, this time divided by the number ofmentions in the key entity Ki that contains mentionmi.
The precision and recall for the entire docu-ment can be calculated as weighted sums of preci-sion and recall of the individual mentions.
The de-fault weight, also used in this experiment is 1n , wheren is the number of mentions in the document.CEAF (Luo, 2005) is based on the best alignmentof subsets of key and response entities.
For anymapping g ?
Gm the total similarity ?
(g) is thesum of all similarities.
The best alignment g?
isfound by maximizing the sum of similarities ?
(g)between the key and response entities, while themaximum total similarity is the sum of the best sim-ilarities.
Precision and recall are defined in termsof the similarity measure ?(g?
): P = ?(g?
)?i ?
(Ri,Ri)R = ?(g?
)?i ?
(Ki,Ki).There are two versions of CEAF with differentsimilarity measures, ?m(K,R) = |K ?
R| and?e(K,R) = 2|K?R||K|+|R| .
?e is the basis for CEAFewhich shows a measure of correct entities whileCEAFm, based on ?m, shows the percentage of cor-rectly resolved mentions.MUC B3 CEAFe CEAFm BLANC MELAMUC ?
0.46 0.22 0.47 0.35 0.63B3 0.59 ?
0.47 0.56 0.42 0.61CEAFe 0.46 0.59 ?
0.51 0.26 0.38CEAFm 0.57 0.70 0.62 ?
0.46 0.60BLANC 0.57 0.70 0.57 0.68 ?
0.35MELA 0.59 0.73 0.59 0.70 0.70 ?Table 1: Kendall ?
rank correlation coefficient for teamsparticipating in CoNLL shared tasks, with CoNLL 2011in the upper right, CoNLL 2012 in the lower left corner.3 Correlating CoNLL shared tasks resultsTo illustrate the complexity of the present evaluationbest practices, we have applied the Kendall ?
rankcorrelation coefficient to the ratings the metrics gavecoreference resolution systems that competed in thetwo recent CoNLL shared tasks.
The official metricsof the CoNLL shared tasks was MELA (Denis andBaldridge, 2009), a weighted average of MUC, B3and CEAFe.The results for CoNLL 2011 (Table 1) showa rather weak correlation among the metrics go-ing down to as low as 0.22 between CEAFe andMUC.
Somewhat surprisingly, the two link-basedmetrics, MUC and BLANC, also show a low degree ofagreement (0.35), while the mention-based metrics,CEAFm and B3, show the highest agreement of allnon-composite metrics.
However, this agreement isnot particularly high either as the two metrics agreeon just above the half of all the cases (0.56).The results for CoNLL 2012 show much highercorrelation among the metrics ranging from 0.46 to0.70.
Again CEAFm and B3 show the highest corre-lation, but unlike in 2011 BLANC ?joins?
this clus-ter.
CEAFe and MUC are again least correlated, whileCEAFe and BLANC, in 2011 almost independent,show average correlation (0.57) in 2012.In our view, comparatively low correlations aswell as surprising variation from year to year sug-gests a certain degree of ?fuzziness?
in quantitativecoreference resolution evaluation.
We leave the in-vestigation of variation between the two years forfuture work.4 Error analysisTo better understand the functioning of the met-rics we have conducted an error analysis on thekey/response entity pairs from five short texts from2the development corpus of the CoNLL 2011 SharedTask (Pradhan et al 2011), one text from each ofthe five represented genres: Broadcast Conversa-tions (BC), Broadcast News (BN), Magazine (MZ),News Wire (NW) and Web Blogs and News Groups(WB).
The texts were chosen as randomly as pos-sible, the only constraint being length1.
The goldstandard texts are originally from OntoNotes 4.0,and contain 64 mentions distributed among 21 keyentities.
The response texts are the output of Stan-ford?s Multi-Pass Sieve Coreference Resolution Sys-tem (Lee et al 2011).4.1 CategorizationInstead of classifying entities according to theirscore by some of the metrics, or a combination ofseveral of them, as done by the CoNLL shared tasks,we have based the classification on a notion of lin-guistic common sense ?
our subjective idea of howhumans evaluate the success or failure of CR.
Wedivide key/response entity pairs into four categories:?
Category 1: Perfect match?
Category 2: Partial match?
Category 3: Merged entities?
Category 4: Failed coreference resolutionWe will concentrate on the amount of informationalvalue from the key entity that has been preserved inthe response entity.
In the course of these experi-ments, our aim is to see if that rather informal ideacan be operationalized in a way amenable to futureuse in automated CR and/or quantitative evaluation.4.1.1 Category 1: Perfect matchThis class consists of four key/response entitypairs with complete string match.
The key andresponse entities being identical, all metrics showunanimously precision and recall of 100%.
Theinformational value is, of course, completely pre-served.
Unfortunately, those examples are few andsimple: They constitute only 19% of the entities and14% of mentions in this sample, and all seem to beachieved by the simplest form of string matching.Key Response MUC B3 CEAFe CEAFm BLANCentities entitiesBC45?
The KMT P 100.00 100.00 90.90 100.00 100.00vice chairman R 80.00 83.33 90.90 83.33 66.67?
Wang ?
Wang F 88.89 90.91 90.90 90.91 80.00Jin-pyng Jin-pyng?
his ?
his?
his ?
his?
He ?
He?
he ?
heBC22?
KMT ?
KMT P 100.00 100.00 80.00 100.00 100.00Chairman Chairman R 50.00 66.67 80.00 66.67 33.33Lien Chan Lien Chan F 66.67 80.00 80.00 80.00 50.00?
ChairmanLien Chan?
Lien Chan ?
Lien ChanBN1?
Bill ?
Bill P 100.00 100.00 76.92 100.00 68.42Clinton Clinton R 85.71 62.50 76.92 62.50 46.43?
The President F 92.31 76.92 76.92 76.92 55.32?
he?
his?
Mr.Clinton ?
Mr.Clinton?
his ?
his?
He ?
He?
he ?
heNW2?
New ?
New P 100.00 100.00 88.89 100.00 100.00Zealand Zealand R 75.00 80.00 88.89 80.00 60.00?
New ?
New F 85.71 88.89 88.89 88.89 75.00Zealand Zealand?
New ?
NewZealand Zealand?
New ?
NewZealand?s Zealand?s?
New?
ZealandTable 2: Category 2a: Partial match (partial entities)4.1.2 Category 2: Partial matchThe partial response entities can be divided in twosubcategories: 2a) The cases where the response en-tities are partial, i.e.they form a proper subset of thekey entity mentions (Table 2) and 2b) The caseswhere the response mentions are partial, i.e.
sub-strings of the corresponding key mentions (Table 3).The scoring of the examples has followed CoNLLshared tasks?
strict mention detection requirements2with the consequence that Category 2b entities havereceived considerably lower scores than the Cate-gory 2a entities even in cases where the loss ofinformational value has been comparable.
For in-stance, the response entity NW1 (Table 3) has re-ceived an average F-score of 56.67%, but its lossof informational value is comparable to that in enti-ties BC45 and BN1 (Table 2).
The BC45?s responseentity has lost the information that Jiyun Tianis a vice-chief, while entities BC45 and BN1have lost the information that the person referred to1The texts longer than five sentences were discarded, tomake the analysis tractable.2Only response mentions with boundaries identical to thegold mentions are recognized as correct (Pradhan et al 2011)3is The KMT vice chairman (BC45) and ThePresident (BN1).
However, the latter mentionshave received a considerably higher average F-scoreof 88.32% and 75.68% respectively.
This indicatesthat stricter mention detection requirements do notnecessarily improve the quality of CR evaluation.Key Response MUC B3 CEAFe CEAFm BLANCentities entitiesMZ22?
a school in ?
a school P 0.00 50.00 50.00 50.00 0.00Shenzhen for in Shenzhen R 0.00 50.00 50.00 50.00 0.00the children of F 0.00 50.00 50.00 50.00 0.00Hong Kongexpats?
the school ?
the schoolin Shenzhen in ShenzhenNW0?
China?s ?
People?s P 0.00 0.00 0.00 0.00 0.00People?s Congress R 0.00 0.00 0.00 0.00 0.00Congress F 0.00 0.00 0.00 0.00 0.00?
China?s ?
People?sPeople?s CongressCongressNW1?
vice-chief ?
committee P 50.00 66.67 66.67 66.67 33.33committee member R 50.00 66.67 66.67 66.67 33.33member Jiyun Tian F 50.00 66.67 66.67 66.67 33.33Jiyun Tian?
Jiyun Tian ?
Jiyun Tian?
He ?
HeNW5?
China?s ?
China?s P 0.00 50.00 50.00 50.00 0.00People?s People?s R 0.00 50.00 50.00 50.00 0.00Congress Congress F 0.00 50.00 50.00 50.00 0.00delegation delegationled byvice-chiefcommitteemember JiyunTian?
the ?
thedelegation delegationfrom China?s from China?sPeople?s People?sCongress CongressTable 3: Category 2b: Partial match (partial mentions).4.1.3 Category 3: Merged entitiesThis category consists of response entities thatcontain mentions from two or more key entities (Ta-ble 4).
Our sample contains only four examples inthis category, but it is still possible to discern twosubcategories:1.
The new information is incorrectIn the key entity MZ40, the sex of the gender-neutralher ten-year-old child has been given bythe mention him.
Replacing it with the mentionshe in the response entity gives the wrong informa-tion about the child?s sex.
Entities BN2 and MZ17also belong to this subcategory, but here the men-tions in the response entity are morphologically in-consistent, thus making the mistake easier to detect.2.
The new information is correct or neutralIn entity pair MZ19 the key mention the lattergroup was replaced with response mention them,Key Response MUC B3 CEAFe CEAFm blancentities entitiesBN2?
The P 66.67 25.00 33.33 25.00 0.00President R 0.00 50.00 33.33 50.00 0.00?
he and his ?
he and his F 0.00 33.33 33.33 33.33 0.00wife, now a wife, now aNew York New Yorksenator senator?
their ?
he?
hisMZ19?
the more ?
the more P 50.00 66.67 66.67 66.67 33.33affluent affluent R 50.00 66.67 66.67 66.67 33.33Taiwanese Taiwanese F 50.00 66.67 66.67 66.67 33.33?
their ?
their?
the latter ?
themgroupMZ17?
her elder ?
her elder P 0.00 33.33 40.00 33.33 0.00son and son and R 0.00 50.00 40.00 50.00 0.00daughter daughter F 0.00 40.00 40.00 40.00 0.00?
them ?
him?
himMZ40?
Her ?
Her P 50.00 66.67 57.14 66.67 33.33ten-year-old ten-year-old R 33.33 50.00 57.14 50.00 20.00child child F 40.00 57.14 57.14 57.14 25.00?
him ?
she?
The child ?
The child?
himTable 4: Category 3: Merged entitiesthe omitted and replacement mentions having verysimilar informational content.As expected, the scores in Category 3 are lowerthen those in Category 2 (as a whole), but they arestill consistently better than the scores of the Cate-gory 2b.4.1.4 Category 4: Unsuccessful coreferenceresolutionThe entities in this category (Table 5) are dividedinto two subcategories:No response entity has been given Two of thekey entities (MZ38 and NW4) were not aligned withany response entities, and not surprisingly all met-rics agree that the CR precision, recall and F-scoreequal zero.The response entities do not contain a single?heavy?
mention that is correct Although the re-sponse entities in the remaining entity pairs are non-empty, an intuitive CR evaluation says there is notmuch sense in aligning near-vacuous mentions ifthe entity is otherwise wrong or empty.
Alreadyin the two rather simple cases of WB0 and WB1the metrics show large discrepancies: While link-based MUC and BLANC correctly give an F-score of0.00 as there are no correct links in the entity, themention-based B3 and CEAF measures award them4Key Response MUC B3 CEAFe CEAFm BLANCentities entitiesWB0?
the beauty ?
the one P 0.00 50.00 50.00 50.00 0.00industry hand R 0.00 50.00 50.00 50.00 0.00?
it ?
it F 0.00 50.00 50.00 50.00 0.00WB1?
the consumer ?
clinical P 0.00 50.00 50.00 50.00 0.00dermatologists R 0.00 50.00 50.00 50.00 0.00?
they ?
they F 0.00 50.00 50.00 50.00 0.00MZ33?
Chang, P 100.00 100.00 75.00 100.00 100.00Mei-liang, R 50.00 60.00 75.00 60.00 30.00chairperson F 66.67 75.00 75.00 75.00 46.15of the TBADWomen?sDivision,?
her ?
her?
she ?
she?
Her ?
Her?
sheTable 5: Category 4: Unsuccessful coreference resolutionwith a rather high F-score of 50.00.Entity MZ33 has been awarded high F-scores byall metrics, averaging 67.56%.
However, almost allinformation from the key entity in MZ33 has beenlost in the response entity: The key entity containsinformation on a person, a female, a Taiwanese na-tional, her name (Chang Mei-lian) and the ad-ditional information that she is a chairpersonof the TBAD, Women?s Division.
Theresponse entity contains the information that itsmentions refer to a female, which is most probablya person, but might be a ship, or a well loved pet.None of the metrics indicate that such a substantialloss of information renders the coreference resolu-tion of MZ33 practically useless for a human user.5 Entity rankingAs some of the metrics yield consistently lowerF-score levels, it is more appropriate to comparerankings of entities than the actual F-scores (Table6).
We have also ?
to infuse an iota of old-schoolarmchair linguistics ?
added a sixth rating column,showing intuitive rankings, based on informationalvalue retained.
The lowest rankings for any metricare marked in bold.The entities showing broad agreement among themetrics are only the best (Category 1) and the worstones (MZ38 and NW4, Category 4).The metrics disagreement surfaces with entitiesWB0 and WB1 of Category 4.
The link-based met-rics, MUC and BLANC, rank them last (13th), whilethey are ranked much higher (13th out of 19) bythe mention-based and entity-based metrics (B3 andEntity MUC B3 CEAFe CEAFm BLANC HumanBC45 6 5 5 5 5 7BC22 8 7 7 7 8 8BC51 1 1 1 1 1 1BN0 1 1 1 1 1 1BN1 5 8 8 8 7 13BN2 13 18 18 18 13 15MZ19 10 10 10 10 10 14MZ33 8 9 9 9 9 17MZ17 13 17 17 17 13 15MZ40 12 12 12 12 12 17MZ22 13 13 13 13 13 10MZ24 1 1 1 1 1 1MZ38 ?
19 19 19 13 19NW0 13 19 19 19 13 10NW1 10 10 10 10 10 8NW2 7 6 6 6 6 5NW3 1 1 1 1 1 1NW4 ?
19 19 19 13 19NW5 13 13 13 13 13 10WB0 13 13 13 13 13 19WB1 13 13 13 13 13 19Table 6: Ranking of our example entities.CEAF).
In this case the human evaluator agrees withthe link-based metrics: If there is not a single cor-rect link within an entity, our intuition says that nouseful CR has taken place.However, the presence of a single correct coref-erent link is not sufficient for our intuition of suc-cessful resolution.
Consider entities MZ22 andNW5 (Table 3): They also consist of two en-tities where only one is correct, and have re-ceived the same ratings as WB0 and WB1, butin this case we judge CR as much more success-ful.
There are two main differences between thisand the previous case.
Firstly, the correct men-tion is in the previous case a meaning-lean pro-noun (it and they) while the correct mentionin this case is a ?full-bodied?
NP (the schoolin Shenzhen and the delegation fromChina?s People?s Congress).
In addition,in both of the Category 2 entity pairs, the incorrectmention holds an informational value very close oridentical to that of the correct mention.
This exam-ple illustrates the importance of informational valuecontent of the mentions for the human evaluation ofthe resolution.6 Formalizing the intuitionWe have earlier (?4.1) introduced a classificaionbased on an informal notion of (human) intuitivecoreference resolution evaluation.
In this section wewill try to formalize the classification.Category 1 The key entities and response entitiesare identical:5?x(x ?
K ?
x ?
R) (1)Category 2 The response entity is a proper subsetof the key entity:?x(x ?
R?
x ?
K)?
?y(y ?
K ?
y /?
R) (2a)This is the only condition for Category 2a.
Cat-egory 2b shares the condition (2a), but to formalizeit, we have to add overlap(x,y) relation.
We can de-fine it as a common substring for x and y of a certainlength, possibly including at least one major syntac-tic category, or even the lexical ?head?
if some wayof operationalizing that notion is available.
?x(x ?
K ?
x ?
R)?
?y?z(y ?
K ?
z ?
R ?
overlap(y, z)) (2b)We need at least two correct mentions in the re-sponse entity, and at least one that overlaps, as re-sponse entities containing only one correct mentiondo not have any correct links.Category 3 Response entity contains a subset ofthe key entity mentions as well as additional men-tion(s) belonging to some other entity (E):?x(x ?
K ?
x ?
R)?
?y(y /?
K ?
y ?
R ?
y ?
E) (3)Category 4 The entities belonging to this categoryhave a twofold definition: The response entity is ei-ther empty or if it contains one correct mention, itcannot contain an overlapping mention.
?x(x ?
K ?
x /?
R)?
?x(x ?
K ?
x ?
R)?
?y?z((y ?
K ?
z ?
R)?
?overlap(y, z))(4)The classification that has been introduced as a in-formal one in ?4.1 is thus computable given an op-erational definition of overlap.
In future work wewill investigate the distribution of the four error cat-egories on a larger sample.7 Conclusion and outlookIn this paper we have compared metrics on the ba-sis of their evaluation of coreference resolution per-formed on real-life texts, and contrasted their eval-uation to an intuitive human evaluation of corefer-ence resolution.
We conjecture that humans requireboth correct coreferent links and correct (whole orpartial) mentions of a certain information weight toconsider a resolution successful.This approach has some shortcomings.
Firstly,the manual nature of the analysis has imposed alimit on the number of the examples, so our datamay not be representative.
Secondly, there is un-certainity connected to how well the coreferenceresolution evaluation metrics are suited to be usedin this way.
The latter drawback is the more seri-ous one: the metrics were not designed to evaluatesingle key/response pairs, but whole texts.
How-ever, we would argue that if we want to discovernew insights into the evaluation process, some levelof approximation is necessary.
There are at leasttwo arguments in favor of this particular approxi-mation: Firstly, all metrics are based on evaluatingkey/response pairs.
Analyzing their performance atthis level can be a reasonable indicator of their per-formance on the text level.
Secondly, even if metricsare treated ?unfairly?, they are all treated equally.We thus believe that this work can be seen as anillustration of remaining evaluation challenges in thefield of coreference resolution.A natural extension of this work would be in-cluding more humans in evaluating coreference res-olution systems, to provide a more representativehuman judgement.
This evaluation should then beextended from evaluating coreference resolution ofsingle key/response entity pairs, to assessing thequality of coreference resolution on a text as awhole.And, finally: Every mention carries an in-formation value, and this weight varies fromquite heavy (as in vice-chief committeemember Jiyun Tian), to somewhat lighter(Jiyun Tian) to virtually weightless (He).
In-formation weights are not distributed randomly, butconform to discourse structure.
It would be inter-esting to map the pattern of their distribution, andsee if incorporating this information could improveboth coreference resolution and its quantitative eval-uation.8 AcknowledgementsWe would like to thank the anonymous reviewers fortheir useful comments.6ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In The First Interna-tional Conference on Language Resources and Evalu-ation Workshop on Linguistics Coreference, pages 563?
566, Granada, Spain.Pascal Denis and Jason Baldridge.
2009.
Global jointmodels for coreference resolution and named entityclassication.
Procesamiento del Lenguaje Natural,42:87?96.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the CoNLL-2011 Shared Task.
In Proceedingsof the 15th Conference on Computational Natural Lan-guage Learning: Shared Task, pages 28?34, Portland,Oregon.Xiaoqiang Luo, Abe Ittycheriah Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the Bell tree.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguis-tics, pages 136?143, Barcelona, Spain.Xiaoqiang Luo.
2005.
On coreference resolution per-formance metrics.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP), pages 25?32, Vancouver, Canada.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofthe 15th Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?27, Portland,Oregon.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 Shared Task: Modeling multilingual unrestrictedcoreference in OntoNotes.
In Proceedings of the JointConference on EMNLP and CoNLL: Shared Task,pages 1?40, Jeju Island, Korea.W.
M. Rand.
1971.
Objective criteria for evaluation ofclustering methods.
Journal of American StatisticalAssociation, 66(336):846?850.Marta Recasens and Eduard Hovy.
2011.
BLANC: Im-plementing the Rand index for coreference evaluation.Natural Language Engineering, 17(04):485?510.Marta Recasens, Lluis Ma`rquez, Emili Sapena,M.
Anto`nia Marti, Mariona Taule?, Ve?roniqueHoste, Massimo Poesio, and Yannick Versley.
2010.Semeval-2010 task 1: Coreference resolution in multi-ple languages.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, ACL 2010, pages1?8, Uppsala, Sweden.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the Sixth Message understanding Conference(MUC-6), pages 45?52, San Francisco, CA.Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew LimTan.
2003.
Coreference resolution using competitionlearning approach.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 176?183, Sapporo, Japan.7
