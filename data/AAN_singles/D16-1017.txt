Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 171?182,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsEvent participant modelling with neural networksOttokar TilkInstitute of CyberneticsTallinn University of Technology12618 Tallinn, Estoniaottokar.tilk@phon.ioc.eeVera Demberg and Asad Sayeedand Dietrich Klakow and Stefan ThaterSaarland University66123 Saarbru?cken, Germany{vera,asayeed,stth}@coli.uni-sb.de;dietrich.klakow@lsv.uni-sb.deAbstractA common problem in cognitive modellingis lack of access to accurate broad-coveragemodels of event-level surprisal.
As shown in,e.g., Bicknell et al (2010), event-level knowl-edge does affect human expectations for ver-bal arguments.
For example, the model shouldbe able to predict that mechanics are likely tocheck tires, while journalists are more likelyto check typos.
Similarly, we would liketo predict what locations are likely for play-ing football or playing flute in order to esti-mate the surprisal of actually-encountered lo-cations.
Furthermore, such a model can beused to provide a probability distribution overfillers for a thematic role which is not men-tioned in the text at all.To this end, we train two neural network mod-els (an incremental one and a non-incrementalone) on large amounts of automatically role-labelled text.
Our models are probabilistic andcan handle several roles at once, which alsoenables them to learn interactions between dif-ferent role fillers.
Evaluation shows a drasticimprovement over current state-of-the-art sys-tems on modelling human thematic fit judge-ments, and we demonstrate via a sentence sim-ilarity task that the system learns highly usefulembeddings.1 IntroductionOur goals in this paper are to learn a representa-tion of events and their thematic roles based on largequantities of automatically role-labelled text and tobe able to calculate probability distributions over thepossible role fillers of specific missing roles.
In thissense, the task is closely related to work on selec-tional preference acquisition (Van de Cruys, 2014).We focus here on the roles agent, patient, loca-tion, time, manner and the predicate itself.
Themodel we develop is trained to represent the event-relevant context and hence systematically captureslong-range dependencies.
This has been previouslyshown to be beneficial also for more general lan-guage modelling tasks (e.g., Chelba and Jelinek,1998; Tan et al, 2012).This type of modelling is potentially relevant to awide range of tasks, for instance for performing the-matic fit judgment tasks, detecting anomalous events(Dasigi and Hovy, 2014), or predicting event struc-ture that is not explicitly present in the text.
Thelatter could be useful for inferring missing informa-tion in entailment tasks or improving identificationof thematic roles outside the sentence containing thepredicate.
Potential applications also include predi-cate prediction based on arguments and roles, whichhas been noted to be relevant for simultaneous ma-chine translation for a verb-final to a verb-medialsource language (Grissom II et al, 2014).
Withincognitive modelling, our model could help to moreaccurately estimate semantic surprisal for broad-coverage texts, when used in combination with anincremental role labeller (e.g., Konstas and Keller,2015), or to provide surprisal estimates for contentwords as a control variable for psycholinguistic ex-perimental materials.In this work, we focus on the predictability ofverbs and nouns, and we suggest that the predictabil-ity of these words depends to a large extent onthe relationship of these words to other nouns and171verbs, especially those connected via the same event.We choose a neural network (NN) model becausewe found that results from existing related models,e.g.
Baroni and Lenci?s Distributional Memory, de-pend heavily on how exactly the distributional spaceis defined, while having no principled way of opti-mizing the space.
A crucial advantage of a neuralnetwork-based approach is thus that the model canbe trained to optimize the distributional representa-tion for the task.Our model is trained specifically to predict miss-ing semantic role-fillers based on the predicate andother available role-fillers of that predicate.
Themodel can also predict the predicate based on the se-mantic roles and their fillers.
In our model, there isno difference in how the semantic roles or the pred-icate are treated.
Thus, when we refer here to roles,we usually mean both semantic roles and the predi-cate, unless otherwise explicitly stated.Our model is compositional in that it has access toseveral role-fillers (including the verb) at the sametime, and can thus represent interdependencies be-tween participants of an event and predict from acombined representation.
Consider, for example, thepredicate serve, whose likely patients include e.g.,drinks.
If we had the agent robber, we would liketo be able to predict a patient like sentence, in thesense of ?the robber will serve his sentence.
.
.
?
Thistask is related to modelling thematic fit.
In this pa-per, we evaluate our model on a variety of thematicfit rating datasets as well as on a sentence similaritydataset that tests for successful compositionality inour model?s representations.This paper makes the following contributions:?
We compare two novel NN models for gener-ating a probability distribution over selectionalpreferences given one or more roles and fillers.?
We show that our technique outperforms stateof the art thematic fit models on many datasets.?
We show that the embeddings thus obtained areeffective in measuring sentence similarity.1.1 Neural networksNeural networks have proven themselves to be verywell suited for language modeling.
By learningdistributed representations of words (Bengio et al,2003), they are able to generalize to new contextsthat were not observed word-by-word in the trainingcorpus.
They can also use a relatively large numberof context words in order to make predictions aboutthe upcoming word.
In fact, the recurrent neural net-work (RNN) LM (Mikolov et al, 2010) does not ex-plicitly fix the context size at all but is potentiallyable to compress the relevant information about theentire context in its recurrent layer.
These are theproperties that we would like to see in our role-fillerprediction model as well.Neural networks have also been used for selec-tional preference acquisition, as in Van de Cruys(2014).
His selectional preference model differsfrom our model in several aspects.
First, unlike ourmodel it is limited to a fixed number of inputs.
An-other difference is that his model uses separate em-beddings for all input words, while ours enables par-tial parameter sharing.
Finally and crucially for role-filler prediction, selectional preference models scorethe inputs, while our model gives a probability dis-tribution over all words for the queried target role.We discuss the components necessary for ourmodel in more detail in section 3.2 Data sourceOur source of training data is the ukWaC corpus,which is part of the WaCky project, as well as theBritish National Corpus.
The corpus consists of webpages crawled from the .uk web domain, contain-ing approximately 138 million sentences.These sentences were run through a semantic rolelabeller and head words were extracted as describedin Sayeed et al (2015).
The semantic role labellerused, SENNA (Collobert and Weston, 2007), gener-ates PropBank-style role labels.
While PropBank ar-gument positions (ARG0, ARG1, etc.)
are primarilydesigned to be verb-specific, rather than directly rep-resenting ?classical?
thematic roles (agent, patient,etc.
), in the majority of cases, ARG0 lines up withagent roles and ARG1 lines up with patient roles.PropBank-style roles have been used in other recentefforts in thematic fit modelling (e.g., Baroni et al,2014; Vandekerckhove et al, 2009),For processing purposes, the corpus was dividedinto 3500 segments.
Fourteen segments (approx 500thousand sentences) each were used for develop-ment and testing, and the rest were used for training.172In order to construct our incremental model andcompare it to n-gram language models, we needed aprecise mapping between the lemmatized argumentwords and their positions in the original sentence.This required aligning the SENNA tokenization andthe original ukWaC tokenization used for Malt-Parser.
Because of the heterogeneous nature of webdata, this alignment was not always achievable?weskipped a small number of sentences in this case.
Inthe development and testing portions of the data set,we filtered sentences containing predicates wherethere were multiple role-assignees with the samerole for the same predicate.3 Model design and implementationOur model is a neural network with a single non-linear hidden layer and a Softmax output layer.
Allinputs are one-hot encoded?i.e., represented as abinary vector with size equal to the number of pos-sible input values, where all entries are zero exceptthe entry at the index corresponding to the currentinput value.3.1 Two-part view of the modelThe parameters of a neural network classifier with asingle hidden layer and one-hot encoded inputs canbe viewed as serving two distinct purposes: movingfrom inputs towards outputs, the first weight matrixthat we encounter is responsible for learning dis-tributed representations (or embeddings) of the in-puts; the second weight matrix represents the param-eters of a maximum entropy classifier that uses thelearned embeddings as inputs.Considering the task of role-filler prediction, wewould want these two sets of parameters to have thefollowing properties:?
The classifier layer should be different for eachtarget role, because the suitable filler given thecontext can clearly be very different dependingon the role (e.g., verb vs.
agent).?
The embedding layer should also be differentdepending on the role of context word.
Other-wise, the network would not have any informa-tion about the role of the context word.
For ex-ample, the suitable verb filler for context worddog in an agent role is probably very differentfrom what it would be, were it in a patient role(e.g.
bark vs. feed).We now briefly describe some incrementally im-proved intermediate approaches that we also consid-ered as they help to understand the steps that led toour final solution for achieving the desired proper-ties of the embedding and classifier layer.A naive way to accomplish the aspired propertieswould be to have a separate model for each inputrole and target role pair.
This approach has severaldrawbacks.
For a start, there is no obvious way tomodel interactions of different input roles and fillersin order to make predictions based on multiple in-put role-word pairs simultaneously.
Another prob-lem is that the parameters are trained only on a frac-tion of available training data?e.g., verb embed-ding weights are trained independently for each tar-get role classifier.
Finally, given that we have chosento distinguish between n different roles, it would re-quire us to train and tune hyper-parameters for n2models.One of these problems (data under-utilization) canbe alleviated by sharing role-specific embedding andclassifier weights across different models.
For ex-ample, the verb embedding matrix would be sharedacross all models that predict different role fillersbased on input verbs.
Other problems remain, andtraining the large number of models becomes evenmore difficult because of parameter synchronization,but this is a step towards the next improvement.Shared role-specific embedding and classifierweights enable us to combine all input-target rolepair models into a single model.
This can be done bystacking role-specific embedding matrices to form a3-way embedding tensor and building a classifier pa-rameter tensor analogously.
Having a single modelsaves us from tuning multiple models and makesmodelling interactions between inputs possible.Despite these advantages, having two tensors inour model has a drawback of rapidly growing thenumber of parameters as vocabulary size, number ofroles, and hidden layer size increase.
This may leadto over-fitting and increases training time.A more subtle weakness is the fact that thiskind of model lacks parameter sharing across role-specific embedding weight matrices.
It is clear thatsome characteristics of words (e.g., semantics) usu-173ally remain the same across different roles.
Thus itis practical to share some information across role-specific weights so that the embeddings can benefitfrom more data and learn better semantic represen-tations while leaving room for role-specific traits.For these reasons we replace the tensors with theirfactored form in our models.3.2 Factored parameter tensorsFactoring classifier and embedding tensors helps toalleviate both the efficiency and parameter sharingproblems brought out in Section 3.1.Given vocabulary size |V |, number of roles |R|and hidden layer size H , each tensor T would re-quire |V | ?
|R| ?
H parameters.
The number ofparameters can be reduced by expressing the tensoras a sum of F rank-one tensors (Hitchcock, 1927).This technique enables us to replace the tensor Twith three factor matrices A, B and C. Each tensorelement T [i, j, k] can then be written as:T [i, j, k] =F?f=1A[i, f ]B[j, f ]C[f, k] (1)Assuming lateral slices of T represent role-specificweight matrices (index j denotes roles), we writeeach role specific weight matrix W as:W = A diag(rB)C (2)where r is a one-hot encoded role vector and diagis a function that returns a square matrix with the ar-gument vector on the main diagonal and zeros else-where.
For example, with a vocabulary of 50000words, 7 roles and number of factors and hid-den units equal to 512, the factorization reducesthe number of parameters from 179M to 26M andgreatly improves training speed.
Factorization alsoenables parameter sharing, since factor matrices Aand C are shared across all roles.Factored tensors have been used in different neu-ral network models before.
Starting with restrictedBoltzmann machines, Memisevic and Hinton (2010)used a factored 3-way interaction tensor in their im-age transformation model.
Sutskever et al (2011)created a character level RNN LM that was effi-ciently able to use input character specific recurrentweights by using a factored tensor.
Aluma?e (2013)used a factored tensor in a multi-domain LM to beable to use a domain-specific hidden layer weightmatrix that would take into account the differenceswhile exploiting similarities between domains.
Amulti-modal LM by Kiros et al (2014) uses a fac-tored tensor to change the effective output layerweights based on image features.It has been noticed before, that training modelswith factored tensors as parameters using gradientdescent is difficult (Sutskever et al, 2011; Kiroset al, 2014).
As explained by Sutskever et al(2011), this is caused by the fact that each tensorelement is represented as a product of three param-eters, which may cause disproportionate updates ifthese three factors have magnitudes that are too dif-ferent.
Another problem is that if the factor matrixB happens to have too small or too large values, thenthis might also cause instabilities in the lower layersas the back-propagated gradients are scaled by role-specific row of B in our model.
This situation ismagnified in our models, since we have not one, buttwo factored layers.To solve this problem, Sutskever et al (2011) sug-gest using 2nd order methods instead of gradient de-scent.
Aluma?e (2013) has alleviated the problemof shrinking back-propagated gradients by addinga bias (initialized with ones) to the domain-specificfactor vector.
We found that using AdaGrad (Duchiet al, 2011) to update the parameters is very effec-tive.
The method provides parameter-specific learn-ing rates that depend on the historic magnitudes ofthe gradients of these parameters.
This seems toneutralize the effect of vanishing or exploding gra-dients by reducing the step size for parameters thattend to have large gradients and allow a bigger learn-ing rate for parameters with smaller gradients.3.3 General structure of the modelOur general approach, common to both role-fillermodels, is shown in Figure 1.
First, role-specificword embedding vector e is computed by implicitlytaking a fiber (word indexed row of a role indexedslice) from the factored embedding tensor:e = wAe diag(rBe)Ce (3)h = PReLU(e+ bh) (4)where w and r are one-hot encoded word and rolevectors respectively, bh is hidden layer bias, and Ae,174Figure 1: General structure of role-filler models.Be and Ce represent the factor matrices that the em-bedding tensor is factored into.
Next, we apply aparametric rectifier (PReLU; He et al, 2015) non-linearity to the role-specific word embedding to ob-tain the hidden activation vector h.The hidden layer activation vector h is fed to theSoftmax output layer through a target role specificclassifier weight matrix (a target role-indexed sliceof the classifier parameter tensor):c = hAc diag(tBc)Cc (5)y = Softmax(c+ by) (6)where t is a one-hot encoded target role vector,by is output layer bias, and y is the output of themodel representing the probability distribution overthe output vocabulary.3.4 Modeling input interactionsThe general approach described in Section 3.3 alsoallows us to model interactions between different in-put role-word pairs.
If we know the order in whichthe inputs were introduced, then we can add a recur-rent connection to the hidden layer to implement anincremental role filler predictor.
When word order isunknown, then input role-word pair representationscan be added together to compose the representa-tion of the entire predicate context1.
We chose addi-1In applications like natural language generation, for exam-ple, where role-fillers need to be predicted, it is not necessarilyalways the case that the order will be known in advance or thatthe thematic fit model will be used to generate the full sentencein correct word order.tion over concatenation (often preferred in languagemodels) because the non-incremental model doesnot need to preserve information about word order,and addition also enables using a variable number ofinputs.The incremental model adds information aboutthe previous hidden state ht?1 to the current inputword role-specific embedding et through recurrentweights Wr.
So, Equation 4 is replaced with:ht = PReLU(et + ht?1Wr + ptWp + bh) (7)where pt is a binary predicate boundary indicatorthat informs the model about the start of a new pred-icate and equals 1 when the target word belongsto a new predicate and 0 otherwise.
The predi-cate boundary input pt is connected to the networkthrough parameter vector Wp.
The hidden state h0is initialized to zeros.The non-incremental model adds role-specificembedding vectors of all input words together toform the representation of the entire predicate con-text and replaces Equation 4 with:h = PReLU(N?i=1ei + bh) (8)where N is the number of input role-word pairs.3.5 Training detailsFirst, we give details that are common to both theRNN and NN models.
The models are trained withmini-batches of 128 samples.
The hidden layer con-sists of 256 PReLU units; embedding and classifiertensor factorization layer sizes are 256 and 512 re-spectively.
The input and output vocabularies are thesame, consisting of 50,000 most frequent lemma-tized words in the training corpus.
The role vocab-ulary consists of 5 argument roles (ARG0, ARG1,ARGM-LOC, ARGM-TMP and ARGM-MNR), theverb is treated as the sixth role, and all the other rolesare mapped to a shared OTHER label.
Parametersare updated using AdaGrad (Duchi et al, 2011) witha learning rate of 0.1.
All models are implementedusing Theano (Bastien et al, 2012; Bergstra et al,2010) and trained on GPUs for 8 days.RNN model gradients are computed using back-propagation through time (Rumelhart et al, 1986)175Model Name Dev Test3-gram LM 450.1 ?
2.6 438.9 ?
2.63-gram CWM 859.6 ?
4.6 834.9 ?
4.5RNN CWM 485.8 ?
2.7 473.2 ?
2.6RNN RF 244.6 ?
1.4 237.8 ?
1.4NN RF 248.2 ?
1.4 241.9 ?
1.4Table 1: Perplexities on dev/test dataset.over 3 time steps.
The NN model is trained on mini-batches of 128 samples that are randomly drawnwith replacement from the training set.3.6 Model comparisonPerplexity allows us to compare all our models insimilar terms, and evaluate the extent to which ac-cess to thematic roles helps the model to predictmissing role fillers.
For comparability, the perplexi-ties of all models are computed only on content wordprobabilities (i.e., predicates and their arguments).We also report the 95% confidence interval for per-plexity, which is computed according to Klakowand Peters (2002).
All models are trained on exactlythe same sentences of lemmatized words.
Probabil-ity mass is distributed across the vocabulary of the50,000 most frequent content words in the trainingcorpus.3.6.1 ModelsFirst, we compare our model to a conventional 3-gram language model 3-gram LM, conditioning onthe previous context containing the immediately pre-ceding context of content and function words.
Alln-grams are discounted with Kneser-Ney smooth-ing, and n-gram probability estimates are interpo-lated with lower order estimates.
Sentence onset inall models is padded with a special sentence onsettag.
The vocabulary of context words for this modelconsists of all words from the training corpus.As a second model, we train a 3-gram contentword model 3-gram CWM, which is an N -gramLM that is trained only on content words.Next, we have RNN CWM?an RNNLM (Mikolov et al, 2010) trained on contentwords only.
The context size of this model is notexplicitly defined and the model can potentiallyutilize more context words than 3-gram CWM (evenfrom outside the sentence boundary).Our incremental role-filler RNN RF is similar toRNN CWM, except for using role-specific embed-ding and classifier weights (slices of factored ten-sor).
It thus has additional information about thecontent word roles2.Finally, the non-incremental role-filler NN RFloses the information about word order and the abil-ity to use information outside predicate boundariesand trades it for the ability to see the future (i.e., thecontext includes both the preceding and the follow-ing content words and their roles).3.6.2 ResultsThe results of content word perplexity evalua-tion are summarized in Table 1.
The thematic-role informed models outperform all other mod-els by a very large margin, cutting perplexity al-most in half.
The incremental model achieves aslightly lower perplexity than the non-incrementalone (237.8 vs. 241.9), hinting that the content wordorder and out-of-predicate role-word pairs can beeven more informative than a preview of upcomingrole-word pairs.The difference between normal LM and the CWMcan be explained by the loss of information fromfunction words, combined with additional sparsityin the model because content word sequences aremuch sparser than sequences of content and func-tion words.This also explains why using a neural network-based RNN CWM model improves the performanceso much (perplexity drops from 834.9 to 473.2),as neural network based language models are wellknown for their ability to generalize well to unseencontexts by learning distributed representations ofwords (Bengio et al, 2003).4 Evaluation on thematic fit ratingsIn order to see whether our model accurately repre-sents events and their typical thematic role fillers, weevaluate our model on a range of existing datasetscontaining human thematic fit ratings.
This evalua-tion also allows us to compare our model to existingmodels that have been used on this task.2A reviewer kindly points out, as a matter of historical inter-est, that the high-level architecture of the RNN RF model bearssome resemblance to the parallel distributed processing modelin McClelland et al (1989) and St. John and McClelland (1990).176Data source # ratings Roles NN RF BL2010 GSD2015 BDK2014Pado (agent, patient) 414 ARG0, ARG1, ARG2 0.52 (8) 0.53 (0) 0.53 (0) 0.41McRae (agent, patient) 1444 ARG0, ARG1 0.38 (20) 0.32 (70) 0.36 (70) 0.28Ferretti (location) 274 ARGM-LOC 0.44 (3) 0.23 (3) 0.29 (3) -Ferretti (instrument) 248 ARGM-MNR 0.45 (6) 0.36 (17) 0.42 (17) -Greenberg (patient) 720 ARG1 0.61 (8) 0.46 (18) 0.48 (18) -Pado+McRae+Ferretti 2380 0.41 (37) 0.35 (90) 0.38 (90) -Table 2: Thematic fit evaluation scores, consisting of Spearman?s ?
correlations between average human judgements and modeloutput, with numbers of missing values (due to missing vocabulary entries) in brackets.
The baseline scores come from the TypeDM(Baroni and Lenci, 2010) model, further developed and evaluated in Greenberg et al (2015a,b) and the neural network predict modeldescribed in Baroni et al (2014).
NN RF is the non-incremental model presented in this article.
Our model maps ARG2 in Padoto OTHER role.
Significances were calculated using paired two-tailed significance tests for correlations (Steiger, 1980).
NN RFwas significantly better than both of the other models on the Greenberg and Ferretti location datasets and significantly better thanBL2010 but not GSD2015 on McRae and Pado+McRae+Ferretti; differences were not statistically significant for Pado and Ferrettiinstruments.4.1 Related workState-of-the-art computational models of thematic fitquantify the similarity between a role filler of a verband the proto-typical filler for that role for the verbbased on distributional vector space models.
Forexample, the thematic fit of grass as a patient forthe verb eat would be determined by the cosine ofa distributional vector representation of grass anda prototypical patient of eat.
The proto-typical pa-tient is in turn obtained from averaging representa-tions of words that typically occur as a patient ofeat (e.g., Erk, 2007; Baroni and Lenci, 2010; Say-eed and Demberg, 2014; Greenberg et al, 2015b).For more than one role, information from both theagent and the predicate can be used to jointly to pre-dict a patient (e.g., Lenci, 2011).4.2 DataPrevious studies obtained thematic fit ratings fromhumans by asking experimental participants to ratehow common, plausible, typical, or appropriatesome test role-fillers are for given verbs on ascale from 1 (least plausible) to 7 (most plausible)(McRae et al, 1998; Ferretti et al, 2001; Binderet al, 2001; Pado?, 2007; Pado?
et al, 2009; Vandek-erckhove et al, 2009; Greenberg et al, 2015a).
Thedatasets include agent, patient, location and instru-ment roles.
For example, in the Pado?
et al (2009)dataset, the noun sound has a very low rating of 1.1as the subject of hear and a very high rating of 6.8as the object of hear.
Each of the verb-role-nountriples was rated by several humans, and our evalua-tions are done against the average human score.
Thedatasets differ from one another in size (as shownin Table 2), choice of verb-noun pairs, and in howexactly the question was asked of human raters.4.3 MethodsA major difference between what the state-of-the-art models do and what our model does is that ourmodel distributes a probability mass of one acrossthe vocabulary, while the thematic fit models haveno such overall constraint; they will assign a highnumber to all words that are similar to the proto-typical vector, without having to distribute probabil-ity mass.
Specifically, this implies that two synony-mous fillers, one of which is a frequent word likefire, and the other of which is an infrequent word,e.g., blaze, will get similar ratings by the distribu-tional similarity models, but quite different ratingsby the neural network model, as the more frequentword will have higher probability.
Greenberg et al(2015a) showed that human ratings are insensitiveto noun frequency.
Hence, we report results that ad-just for frequency effects by setting the output layerbias of the neural network model to zero.
Since theoutput unit biases of the neural network model areindependent from the inputs, they correlate strongly(rs = 0.74, p = 0.0) with training corpus word fre-quencies after being trained.
Therefore, setting thelearned output layer bias vector to a zero-vector is asimple way to reduce the effect of word frequencieson the model?s output probability distribution.177Role # ratings ?
(# NaN)ARG0 924 0.38 (14)ARG1 1615 0.51 (22)ARG2 39 0.59 (0)ARGM-MNR 248 0.45 (6)ARGM-LOC 274 0.44 (3)ALL 3100 0.45 (45)Table 3: Per role thematic-fit evaluation scores in terms ofSpearmans ?
correlations between average human judgementsand model output.4.4 ResultsWe can see that the neural network model outper-forms the baselines on all the datasets except thePado dataset.
An error analysis on the role fillerprobabilities generated by the neural net points tothe effect of level of constraint of the verb on the es-timates.
For a relatively non-constraining verb, theneural net model will have to distribute the probabil-ity mass across many different suitable fillers, whilethe semantic similarity models do not suffer fromthis.
This implies that filler fit is not directly compa-rable across verbs in the NN model (only filler pre-dictability is comparable).Per role results are shown in Table 3.
Surpris-ingly, the model output has the highest correlationwith the averaged human judgements for the targetrole ARG2, despite the fact that ARG2 is mapped toOTHER along with several other roles.
The modelstruggles the most when it comes to predicting fillersfor ARG0.
There is no noticeable correlation be-tween the role-specific performance and the role oc-currence frequency in the samples of our trainingset.
This implies that parameter sharing betweenroles does indeed help when it comes to balancingthe performance between rare and ubiquitous rolesas discussed in section 3.1.4.5 CompositionalityThe above thematic role fit data sets only assess thefit between two words.
Our model can howeveralso model the interaction between different roles;see Figure 2 for an example of model predictions.We are only aware of one small dataset that can beused to systematically test the effectiveness of thecompositionality for this task.
The Bicknell et al(2010) dataset contains triples like journalist checkModel NN RF Lenci 2011Accuracy 1 0.687 0.671Accuracy 2 0.828 0.844Table 4: Accuracies on the Bicknell evaluation task.spelling vs. mechanic check spelling and journalistcheck tires vs. mechanic check tires together withhuman congruity judgments.The goal in this task is for the model to repro-duce the human judgments on the 64 sentence pairs.Lenci (2011), which we compare against in Table4, proposed a first compositional model based onTypeDM to evaluate on this task.We use two accuracy scores for the evaluation,which we call ?Accuracy 1?
and ?Accuracy 2?.
?Ac-curacy 1?
counts a hit iff the model assigns thecomposed subject-verb combination a higher scorewhen we test a human-rated better-fitting object incontrast with when we test a worse-fitting one; inother words, a hit is achieved when journalist checkspelling should be better than journalist check tires,if we give the model journalist check as the predi-cate to test against different objects.
(The result fromLenci for this task was transmitted by private com-munication.
)?Accuracy 2?
counts a hit iff, given an object, thecomposed subject-verb combination gives a higherscore when the subject is better fitting.
That is, ahit is achieved when journalist check spelling has ahigher score than mechanic check spelling, settingthe query to the model as journalist check and me-chanic check and finding a score for spelling in thatcontext.
This accuracy metric is proposed and eval-uated in Lenci (2011).Evaluation shows that our model performs simi-larly to that of Lenci, although only limited conclu-sions can be drawn due to the small data set size.5 Evaluation of event representations:sentence similarityTo show that our model learns to represent inputwords and their roles in a useful way that reflects themeaning and interactions between inputs, we evalu-ate our non-incremental model on a sentence simi-larity task from Grefenstette and Sadrzadeh (2015).We assign similarity scores to sentence pairs bycomputing representations for each sentence by tak-178Figure 2: Examples of model predictions for the verb servewith different agents and target roles patient and location.ing the hidden layer state (Equation 8) of the non-incremental model given the words in the sentenceand their corresponding roles.
Sentence similarityis then rated with the cosine similarity between therepresentations of the two sentences.Spearman?s rank correlation between the cosinesimilarities produced by our model and human rat-ings are shown in Table 5.
Our model achievesmuch higher correlation with human ratings than thebest result reported by Grefenstette and Sadrzadeh(2015), showing our model?s ability to composemeaningful representations of multiple input wordsand their roles.We also compare our model with another NNword representation model baseline that does notembed role information; by this comparison, wecan determine the size of the improvement broughtby our role-specific embeddings.
The baseline sen-tence representations are constructed by element-wise addition of pre-trained word2vec (Mikolovet al, 2013) word embeddings3.
Scores are againcomputed by using cosine similarity.
The large gapbetween our model?s and word2vec baseline?s per-formance illustrates the importance of embeddingrole information in word representations.6 ConclusionsIn this paper we proposed two neural network archi-tectures for learning proto-typical event representa-3https://code.google.com/p/word2vec/# ratings NN RF Kronecker W2V Humans199 0.34 0.26 0.13 0.62Table 5: Sentence similarity evaluation scores on GS2013dataset (Grefenstette and Sadrzadeh, 2015), consisting of Spear-man?s ?
correlations between human judgements and modeloutput.
Kronecker is the best performing model from Grefen-stette and Sadrzadeh (2015).
NN RF is the non-incrementalmodel presented in this article, and W2V is the word2vec base-line.
Human performance (inter-annotator agreement) showsthe upper bound.tions.
These models were trained to generate prob-ability distributions over role fillers for a given se-mantic role.
In our perplexity evaluation, we demon-strated that giving the model access to thematic roleinformation substantially improved prediction per-formance.
We also compared the performance ofour model to the performance of current state-of-the-art models in predicting human thematic fit ratingsand showed that our model outperforms the existingmodels by a large margin.
Finally, we also showedthat the event representations from the hidden layerof our model are highly effective in a sentence sim-ilarity task.
In future work, we intend to test thepotential contribution of this model when applied tolarger tasks such as entailment and inference tasksas well as semantic surprisal-based prediction tasks.7 AcknowledgementsThis research was funded by the German ResearchFoundation (DFG) as part of SFB 1102: ?Informa-tion Density and Linguistic Encoding?
as well asthe Cluster of Excellence ?Multimodal Computingand Interaction?
(MMCI).
Also, the authors wishto thank the anonymous reviewers whose valuableideas contributed to this paper.ReferencesAluma?e, T. (2013).
Multi-domain neural networklanguage model.
In INTERSPEECH, pages 2182?2186.
Citeseer.Baroni, M., Dinu, G., and Kruszewski, G. (2014).Don?t count, predict!
a systematic comparison ofcontext-counting vs. context-predicting semanticvectors.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguis-tics, volume 1, pages 238?247.179Baroni, M. and Lenci, A.
(2010).
Distributionalmemory: A general framework for corpus-basedsemantics.
Comput.
Linguist., 36(4):673?721.Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J.,Goodfellow, I. J., Bergeron, A., Bouchard, N.,and Bengio, Y.
(2012).
Theano: new features andspeed improvements.
Deep Learning and Unsu-pervised Feature Learning NIPS 2012 Workshop.Bengio, Y., Ducharme, R., Vincent, P., and Jan-vin, C. (2003).
A neural probabilistic languagemodel.
The Journal of Machine Learning Re-search, 3:1137?1155.Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P.,Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y.
(2010).
Theano: a CPUand GPU math expression compiler.
In Proceed-ings of the Python for Scientific Computing Con-ference (SciPy).
Oral Presentation.Bicknell, K., Elman, J. L., Hare, M., McRae, K., andKutas, M. (2010).
Effects of event knowledge inprocessing verbal arguments.
Journal of Memoryand Language, 63(4):489?505.Binder, K. S., Duffy, S. A., and Rayner, K. (2001).The effects of thematic fit and discourse contexton syntactic ambiguity resolution.
Journal ofMemory and Language, 44(2):297?324.Chelba, C. and Jelinek, F. (1998).
Exploitingsyntactic structure for language modeling.
InProceedings of the 36th Annual Meeting of theAssociation for Computational Linguistics and17th International Conference on ComputationalLinguistics-Volume 1, pages 225?231.
Associa-tion for Computational Linguistics.Collobert, R. and Weston, J.
(2007).
Fast semanticextraction using a novel neural network architec-ture.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics,pages 560?567, Prague, Czech Republic.
Associ-ation for Computational Linguistics.Dasigi, P. and Hovy, E. H. (2014).
Model-ing newswire events using neural networks foranomaly detection.
In COLING, pages 1414?1422.Duchi, J., Hazan, E., and Singer, Y.
(2011).
Adap-tive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Erk, K. (2007).
A simple, similarity-based modelfor selectional preferences.
In Proceedings of the45th Annual Meeting of the Association of Com-putational Linguistics, pages 216?223, Prague,Czech Republic.
Association for ComputationalLinguistics.Ferretti, T. R., McRae, K., and Hatherell, A.
(2001).Integrating verbs, situation schemas, and thematicrole concepts.
Journal of Memory and Language,44(4):516?547.Greenberg, C., Demberg, V., and Sayeed, A.(2015a).
Verb polysemy and frequency effectsin thematic fit modeling.
In Proceedings of the6th Workshop on Cognitive Modeling and Com-putational Linguistics, pages 48?57, Denver, Col-orado.
Association for Computational Linguis-tics.Greenberg, C., Sayeed, A., and Demberg, V.(2015b).
Improving unsupervised vector-spacethematic fit evaluation via role-filler prototypeclustering.
In Proceedings of the 2015 Conferenceof the North American Chapter of the Associationfor Computational Linguistics Human LanguageTechnologies (NAACL HLT).Grefenstette, E. and Sadrzadeh, M. (2015).
Concretemodels and empirical evaluations for the categor-ical compositional distributional model of mean-ing.
Computational Linguistics.Grissom II, A. C., Boyd-Graber, J., He, H., Morgan,J., and Daume?
III, H. (2014).
Don?t until the finalverb wait: Reinforcement learning for simultane-ous machine translation.
In Proceedings of the2014 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 1342?1352.He, K., Zhang, X., Ren, S., and Sun, J.
(2015).
Delv-ing deep into rectifiers: Surpassing human-levelperformance on imagenet classification.
arXivpreprint arXiv:1502.01852.Hitchcock, F. L. (1927).
The expression of a tensoror a polyadic as a sum of products.
Journal ofMathematics and Physics, (6):164?189.Kiros, R., Salakhutdinov, R., and Zemel, R. (2014).180Multimodal neural language models.
In Proceed-ings of the 31st International Conference on Ma-chine Learning (ICML-14), pages 595?603.Klakow, D. and Peters, J.
(2002).
Testing the cor-relation of word error rate and perplexity.
SpeechCommunication, 38(1):19?28.Konstas, I. and Keller, F. (2015).
Semantic rolelabeling improves incremental parsing.
In Pro-ceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the7th International Joint Conference on NaturalLanguage Processing (Volume 1: Long Papers),pages 1191?1201, Beijing, China.
Association forComputational Linguistics.Lenci, A.
(2011).
Composing and updating verb ar-gument expectations: A distributional semanticmodel.
In Proceedings of the 2Nd Workshop onCognitive Modeling and Computational Linguis-tics, CMCL ?11, pages 58?66, Stroudsburg, PA,USA.
Association for Computational Linguistics.McClelland, J. L., St. John, M., and Taraban, R.(1989).
Sentence comprehension: A parallel dis-tributed processing approach.
Language and cog-nitive processes, 4(3-4):SI287?SI335.McRae, K., Spivey-Knowlton, M. J., and Tanen-haus, M. K. (1998).
Modeling the influence ofthematic fit (and other constraints) in on-line sen-tence comprehension.
Journal of Memory andLanguage, 38(3):283?312.Memisevic, R. and Hinton, G. E. (2010).
Learningto represent spatial transformations with factoredhigher-order Boltzmann machines.
Neural Com-putation, 22(6):1473?1492.Mikolov, T., Karafia?t, M., Burget, L., Cernocky`,J., and Khudanpur, S. (2010).
Recurrent neu-ral network based language model.
In INTER-SPEECH 2010, 11th Annual Conference of the In-ternational Speech Communication Association,Makuhari, Chiba, Japan, September 26-30, 2010,pages 1045?1048.Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S.,and Dean, J.
(2013).
Distributed representationsof words and phrases and their compositionality.In Advances in neural information processing sys-tems, pages 3111?3119.Pado?, U.
(2007).
The integration of syntax and se-mantic plausibility in a wide-coverage model ofhuman sentence processing.
PhD thesis, SaarlandUniversity.Pado?, U., Crocker, M. W., and Keller, F. (2009).A probabilistic model of semantic plausibil-ity in sentence processing.
Cognitive Science,33(5):794?838.Rumelhart, D. E., Hinton, G. E., and Williams,R.
J.
(1986).
Learning representations by back-propagating errors.
NATURE, 323:9.Sayeed, A. and Demberg, V. (2014).
Combiningunsupervised syntactic and semantic models ofthematic fit.
In Proceedings of the first ItalianConference on Computational Linguistics (CLiC-it 2014).Sayeed, A., Demberg, V., and Shkadzko, P. (2015).An exploration of semantic features in an unsu-pervised thematic fit evaluation framework.
In IJ-CoL vol.
1, n. 1 december 2015: Emerging Topicsat the First Italian Conference on ComputationalLinguistics, pages 25?40.
Accademia UniversityPress.St.
John, M. F. and McClelland, J. L. (1990).Learning and applying contextual constraints insentence comprehension.
Artificial Intelligence,46(1-2):217?257.Steiger, J. H. (1980).
Tests for comparing elementsof a correlation matrix.
Psychological Bulletin,87(2):245.Sutskever, I., Martens, J., and Hinton, G. E. (2011).Generating text with recurrent neural networks.
InProceedings of the 28th International Conferenceon Machine Learning (ICML-11), pages 1017?1024.Tan, M., Zhou, W., Zheng, L., and Wang, S. (2012).A scalable distributed syntactic, semantic, andlexical language model.
Computational Linguis-tics, 38(3):631?671.Van de Cruys, T. (2014).
A neural network approachto selectional preference acquisition.
In Proceed-ings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 26?35.181Vandekerckhove, B., Sandra, D., and Daelemans, W.(2009).
A robust and extensible exemplar-basedmodel of thematic fit.
In EACL 2009, 12th Con-ference of the European Chapter of the Associa-tion for Computational Linguistics, Proceedingsof the Conference, Athens, Greece, March 30 -April 3, 2009, pages 826?834.182
