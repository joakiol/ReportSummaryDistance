Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 64?69,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImprovements to Monolingual English Word Sense Disambiguation?Weiwei GuoComputer Science DepartmentColumbia UniversityNew York, NY, 10115, USAwg2162@cs.columbia.eduMona T. DiabCenter for Computational Learning SystemsColumbia UniversityNew York, NY 10115, USAmdiab@ccls.columbia.eduAbstractWord Sense Disambiguation remains one ofthe most complex problems facing compu-tational linguists to date.
In this paper wepresent modification to the graph based stateof the art algorithm In-Degree.
Our modifi-cations entail augmenting the basic Lesk sim-ilarity measure with more relations based onthe structure of WordNet, adding SemCor ex-amples to the basic WordNet lexical resourceand finally instead of using the LCH similaritymeasure for computing verb verb similarity inthe In-Degree algorithm, we use JCN.
We re-port results on three standard data sets usingthree different versions of WordNet.
We re-port the highest performing monolingual un-supervised results to date on the Senseval 2 allwords data set.
Our system yields a perfor-mance of 62.7% using WordNet 1.7.1.1 IntroductionDespite the advances in natural language process-ing (NLP), Word Sense Disambiguation (WSD) isstill considered one of the most challenging prob-lems in the field.
Ever since the field?s inception,WSD has been perceived as one of the central prob-lems in NLP as an enabling technology that couldpotentially have far reaching impact on NLP appli-cations in general.
We are starting to see the be-ginnings of a positive effect of WSD in NLP appli-cations such as Machine Translation (Carpuat andWu, 2007; Chan et al, 2007).
Advances in re-search on WSD in the current millennium can beattributed to several key factors: the availability oflarge scale computational lexical resources such as?The second author has been partially funded by DARPAGALE project.
We would also like to thank the useful com-ments rendered by three anonymous reviewers.WordNets (Fellbaum, 1998; Miller, 1990), the avail-ability of large scale corpora, the existence and dis-semination of standardized data sets over the past 10years through the different test beds of SENSEVALand SEMEVAL competitions,1 devising more robustcomputing algorithms to handle large scale data sets,and simply advancement in hardware machinery.In this paper, we address the problem of WSD ofall the content words in a sentence.
In this frame-work, the task is to associate all tokens with theircontextually relevant meaning definitions from somecomputational lexical resource.
We present an en-hancement on an existing graph based algorithm, In-Degree, as described in (Sinha and Mihalcea, 2007).Like the previous work, our algorithm is unsuper-vised.
We show significant improvements over pre-vious state of the art performance on several exist-ing data sets, SENSEVAL2, SENSEVAL3 and SE-MEVAL.2 Word Sense DisambiguationThe definition of WSD has taken on several differentmeanings in recent years.
In the latest SEMEVAL(2007) workshop, there were 18 tasks defined, sev-eral of which were on different languages, howeverwe notably recognize the widening of the defini-tion of the task of WSD.
In addition to the tradi-tional all words and lexical sample tasks, we notenew tasks on word sense discrimination (no senseinventory is needed, the different senses are merelydistinguished), lexical substitution using synonymsof words as substitutes, as well as meaning defini-tions obtained from different languages namely us-ing words in translation.Our paper is about the classical all words task ofWSD.
In this task, all the content bearing words in arunning text are disambiguated from a static lexical1http://www.semeval.org64resource.
For example a sentence such as I walkedby the bank and saw many beautiful plants there.will have the verbs walked, saw, the nouns bank,plants, the adjectives many, beautiful, and the ad-verb there, be disambiguated from a standard lexi-cal resource.
Hence using WordNet,2 walked will beassigned the meaning to use one?s feet to advance;advance by steps, saw will be assigned the meaningto perceive by sight or have the power to perceiveby sight, the noun bank will be assigned the mean-ing sloping land especially the slope beside a bodyof water and so on.3 Related WorksMany systems over the years have been used for thetask.
A thorough review of the current state of theart is in (Navigli, 2009).
Several techniques havebeen used to tackle the problem ranging from rulebased/knowledge based approaches to unsupervisedand supervised machine learning approaches.
Todate, the best approaches that solve the all wordsWSD task are supervised as illustrated in the dif-ferent SenseEval and SEMEVAL All Words tasks(M. Palmer and Dang, 2001; Snyder and Palmer,2004; Pradhan et al, 2007).In this paper, we present an unsupervised ap-proach to the all words WSD problem relying onWordNet similarity measures.
We will review onlythree of the most relevant related research due tospace limitations.
We acknowledge the existence ofmany research papers that tackled the problem usingunsupervised approaches.Firstly, in work by (Pedersen and Patwardhan,2005), the authors investigate different word simi-larity measures as a means of disambiguating wordsin context.
They compare among different similar-ity measures.
They show that using an extension onthe Lesk similarity measure (Lesk, 1986) betweenthe target words and their contexts and the contextsof those of the WordNet synset entries (Gloss Over-lap), outperforms all the other similarity measures.Their approach is unsupervised.
They exploit thedifferent relations in WordNet.
They also go beyondthe single word overlap, they calculate the overlapin n-grams.
They report results on the English Lex-ical sample task from Senseval 2 which comprised2http://wordnet.princeton.edunouns, verbs and adjectives.
The majority of thewords in this set is polysemous.
They achieve anF-measure of 41.2% on nouns, 21.2% on verbs, and25.1% on adjectives.The second related work to ours is the work by(Mihalcea, 2005).
Mihalcea (2005) introduced agraph based unsupervised technique for all wordsense disambiguation.
Similar to the previous study,the author relied on the similarity of the WordNetentry glosses using the Lesk similarity measure.
Thestudy introduces a graph based sequence model ofthe problem.
All the open class words in a sentenceare linked via an undirected graph where all the pos-sible senses are listed.
Then dependency links aredrawn between all the sense pairs.
Weights on thearcs are determined based on the semantic similar-ity using the Lesk measure.
The algorithm is basi-cally to walk the graph and find the links with thehighest possible weights deciding on the appropri-ate sense for the target words in question.
This algo-rithm yields an overall F-score of 54.2% on the Sen-seval 2 all words data set and an F-score of 64.2%on nouns alone.Finally, the closest study relevant to the currentpaper yields state of the art performance is an un-supervised approach described in (Sinha and Mihal-cea, 2007).
In this work, the authors combine dif-ferent semantic similarity measures with differentgraph based algorithms as an extension to work in(Mihalcea, 2005).
The authors proposed a graph-based WSD algorithm.
Given a sequence of wordsW = {w1, w2...wn}, each word wi with severalsenses {si1, si2...sim}.
A graph G = (V,E) is definedsuch that there exists a vertex v for each sense.
Twosenses of two different words may be connected byan edge e, depending on their distance.
That twosenses are connected suggests they should have in-fluence on each other, so normally a maximum al-lowable distance is set.
They explore 4 differentgraph based algorithms.
The highest yielding algo-rithm in their work is the In-Degree algorithmcombining different WordNet similarity measuresdepending on POS.
They used the Jiang and Conrath(JCN) (Jiang and Conrath., 1997) similarity mea-sure within nouns, the Leacock & Chodorow (LCH)(Leacock and Chodorow, 1998) similarity measurewithin verbs, and the Lesk (Lesk, 1986) similaritymeasure within adjectives and within adverbs and65across different POS tags.
They evaluate their workagainst the Senseval 2 all words task.
They tune theparameters of their algorithm ?
specifically the nor-malization ratio for some of these measures ?
basedon the Senseval 3 data set.
They report a state of theart unsupervised system that yields an overall per-formance of 57.2%.4 Our ApproachIn this paper, we extend the (Sinha and Mihalcea,2007) work (hence forth SM07) in some interestingways.
We focus on the In-Degree graph basedalgorithm as it was the best performer in the SM07work.
The In-Degree algorithm presents the prob-lem as a weighted graph with senses as nodes andsimilarity between senses as weights on edges.
TheIn-Degree of a vertex refers to the number of edgesincident on that vertex.
In the weighted graph, theIn-Degree for each vertex is calculated by summingthe weights on the edges that are incident on it.After all the In-Degree values for each sense iscomputed, the sense with maximum value is cho-sen as the final sense for that word.
SM07 com-bine different similarity measures.
They show thatbest combination is JCN for noun pairs and LCHfor verb pairs, and Lesk for within adjectives andwithin adverbs and also across different POS, for ex-ample comparing senses of verbs and nouns.
Sincedifferent similarity measures use different similarityscales, SM07 did not directly use the value returnedfrom the similarity metrics.
Instead, the values werenormalized.
Lesk value is observed in a range from0 to an arbitrary value, so values larger than 240were set to 1, and the rest is mapped to an interval[0,1].
Similarily JCN and LCH were normalized tothe interval from [0,1].3In this paper, we use the basic In-Degree algo-rithm while applying some modifications to the ba-sic similarity measures exploited and the WordNetlexical resource.
Similar to the original In-Degreealgorithm, we produce a probabilistic ranked list ofsenses.
Our modifications are described as follows:JCN for Verb-Verb Similarity In our implemen-tation of the In-Degree algorithm, we use the JCNsimilarity measure for both Noun-Noun similarity3These values were decided on based on calibrations on theSENSEVAL 3 data set.calculation similar to SM07.
In addition, instead ofusing LCH for Verb-Verb similarity, we use JCN forVerb Verb similarity based on our empirical obser-vation on SENSEVAL 3 data, JCN yields better per-formance than when employing LCH among verbs.Expand Lesk Following the intuition in (Peder-sen and Patwardhan, 2005) ?
henceforth (PEA05)?
we expand the basic Lesk similarity measure totake into account the glosses for all the relationsfor the synsets on the contextual words and com-pare them with the glosses of the target word senses,hence going beyond the is-a relation.
The idea isbased on the observation that WordNet senses aretoo fine-grained, therefore the neighbors share a lotof semantic meanings.
To find similar senses, weuse the relations: hypernym, hyponym, similar at-tributes, similar verb group, pertinym, holonym, andmeronyms.4 The algorithm assumes that the wordsin the input are POS tagged.
It is worth noting thedifferences between our algorithm and the PEA05algorithm, though we take our cue from it.
InPEA05, the authors retrieve all the relevant neigh-bors to form a large bag of words for both the targetsense and the surrounding sense and they specifi-cally focus on the Lesk similarity measure.
In ourcurrent work, we employ the neighbors in a dis-ambiguation strategy using different similarity mea-sures one pair at a time.This algorithm takes as input a target sense and asense pertaining to a word in the surrounding con-text, and returns a sense similarity score.
It is worthnoting that we do not apply the WN relations ex-pansion to the target sense.
It is only applied to thecontextual word.
We experimented with expandingboth the contextual sense and the target sense andwe found that the unreliability of some of the rela-tions is detrimental to the algorithm?s performance.Hence we decided empirically to expand only thecontextual word.We employ the same normalization values used inSM07 for the different similarity measures.
Namelyfor the Lesk and Expand-Lesk we use the same cutoff value of 240, accordingly, if the Lesk or Expand-Lesk similarity value returns 0 <= 240 it is con-4We have run experiments varying the number of relations toemploy and they all yielded relatively similar results.
Hence inthis paper, we report results using all the relations listed above.66verted to a real number in the interval [0,1], any sim-ilarity over 240 is by default mapped to a 1.
ForJCN, similar to SM07, the values are from 0.04 to0.2, we mapped them to the interval [0,1].
It is worthnoting that we did not run any calibration studies be-yond the what was reported in SM07.SemCor Expansion of WordNet A basic part ofour approach relies on using the Lesk algorithm.
Ac-cordingly, the availability of glosses associated withthe WordNet entries is extremely beneficial.
There-fore, we expand the number of glosses availablein WordNet by using the SemCor data set, therebyadding more examples to compare.
The SemCorcorpus is a corpus that is manually sense tagged(Miller, 1990).
In this expansion, depending on theversion of WordNet, we use the sense-index file inthe WordNet Database to convert the SemCor data tothe appropriate version sense annotations.
We aug-ment the sense entries for the different POS Word-Net databases with example usages from SemCor.The augmentation is done as a look up table externalto WordNet proper since we did not want to dabblewith the WordNet offsets.
We set a cap of 30 addi-tional examples per synset.
Many of the synsets hadno additional examples.
A total of 26875 synsets inWordNet 1.7.1 and a total of 25940 synsets are aug-mented with SemCor examples.55 Experiments and Results5.1 DataWe experiment with all the standard data sets,namely, Senseval 2 (SV2) (M. Palmer and Dang,2001), Senseval 3 (SV3) (Snyder and Palmer, 2004),and SEMEVAL (SM) (Pradhan et al, 2007) EnglishAll Words data sets.
We used the true POS tag setsin the test data as rendered in the Penn Tree Bank.We exclude the data points that have a tag of ?U?in the gold standard since our system does not al-low for an unknown option (i.e.
it has to produce asense tag).
We present our results on 3 versions ofWordNet (WN), 1.7.1 for ease of comparison withprevious systems, 2.1 for SEMEVAL data, and 3.0in order to see whether the trends in performancehold across WN versions.5It is worth noting that some example sentences are repeatedacross different synsets and POS since the SemCor data is an-notated as an All-Words tagged data set.5.2 Evaluation MetricsWe use the scorer2 software to report fine-grained (P)recision and (R)ecall and (F)-measure onthe different data sets.5.3 BaselinesWe consider here the two different baselines.
1.
Arandom baseline (RAND) is the most appropriatebaseline for an unsupervised approach.
We considerthe first sense baseline to be a supervised baselinesince it depends crucially on SemCor in ranking thesenses within WordNet.6 It is worth pointing out thatour algorithm is still an unsupervised algorithm eventhough we use SemCor to augment WordNet sincewe do not use any annotated data in our algorithmproper.
2.
The SM07 baseline which we considerour true baseline.5.4 Experimental ConditionsWe explore 4 different experimental conditions:JCN-V which uses JCN instead of LCH for verb-verb similarity comparison, we consider this ourbase condition; +ExpandL is adding the Lesk Ex-pansion to the base condition; +SemCor adds theSemCor expansion to the base condition; and finally+ExpandL SemCor, adds the latter both conditionssimultaneously.5.5 ResultsTable 1 illustrates the obtained results on the threedata sets reporting only overall F-measure.
The cov-erage for SV2 is 98.36% losing some of the verb andadverb target words.
The coverage for SV3 is 99.7%and that of SM is 100%.
These results are on theentire data set as described in Table ??.
Moreover,Table 2 presents the detailed results for the Senseval2 data set using WN 1.7.1 since it is the most stud-ied data set and for ease of comparison with previ-ous studies.
We break the results down by POS tag(N)oun, (V)erb, (A)djective, and Adve(R)b.6From an application standpoint, we do not find the firstsense baseline to be of interest since it introduces a strong levelof uniformity ?
removing semantic variability ?
that is not de-sirable.
Even if the frist sense achieves higher results in thesedata sets, it is an artifact of the size of the data and the verylimited number of documents under investigation.67Condition SV2-WN171 SV2-WN30 SV3-WN171 SV3-WN30 SM-WN2.1 SM-WN30RAND 39.9 41.8 32.9 33.4 25.4SM07 59.7 59.8 54 53.8 40.4 40.8JCN-V 60.2 60.2 55.9 55.5 44.1 45.5+ExpandL 60.9 60.6 55.7 55.5 43.7 45.1+SemCor 62.04 62.2 59.7 60.3 46.8 46.8+ExpandL SemCor 62.7 62.9 59.5 59.6 45.9 45.7Table 1: F-measure % for all experimental conditions on all data setsCondition N V A RRAND 43.7 21 41.2 57.4SM07 68.7 33.01 65.2 63.1JCN-V 68.7 35.46 65.2 63.1+ExpandL 70 35.86 65.6 62.8+SemCor 68.3 37.86 68.6 68.75+ExpandL SemCor 69.5 38.66 68.2 69.15Table 2: F-measure results per POS tag per condition for SV2 using WN 1.7.1.6 DiscussionOur overall results on all the data sets clearly out-perform the baseline as well as state of the art per-formance using an unsupervised system (SM07) inoverall accuracy across all the data sets.
Our im-plementation of SM07 is slightly higher than thosereported in (Sinha and Mihalcea, 2007), 57.12% isprobably due to the fact that we do not consider theitems tagged as ?U?
and also we resolve some ofthe POS tag mismatches between the gold set andthe test data.
We note that for the SV2 data set ourcoverage is not 100% due to some POS tag mis-matches that could not have been resolved automat-ically.
These POS tag problems have to do mainlywith multiword expressions and the like.In observing the performance of the overall sys-tem, we note that using JCN for verbs clearly outper-forms using the LCH similarity measure across theboard on all data sets as illustrated in Table 1.
Us-ing SemCor to augment WordNet examples seems tohave the biggest impact on SV3 and SM comparedto ExpandL.
This may be attributed to the fact thatthe percentage of polysemous words in the latter twosets is much higher than it is for SV2.
CombiningSemCor with ExpandL yields the best results for theSV2 data sets.
There seems to be no huge notabledifference between the three versions of WN, thoughWN3.0 seems to yield slightly higher results maybedue to higher consistency in the overall structurewhen comparing WN1.7.1, WN2.1, and WN3.0.
Wedo recognize that we can?t directly compare the var-ious WordNets except to draw conclusions on struc-tural differences remotely.
It is also worth notingthat less words in WN3.0 used SemCor expansions.Observing the results yielded per POS in Table2, ExpandL seems to have the biggest impact onthe Nouns only.
This is understandable since thenouns hierachy has the most dense relations and themost consistent ones.
SemCor augmentation of WNseemed to benefit all POS significantly except fornouns.
In fact the performance on the nouns deteri-orated from the base condition JCN-V from 68.7 to68.3%.
This maybe due to inconsistencies in the an-notations of nouns in SemCor or the very fine granu-larity of the nouns in WN.
We know that 72% of thenouns, 74% of the verbs, 68.9% of the adjectives,and 81.9% of the adverbs directly exploited the useof SemCor augmented examples.
Combining Sem-Cor and ExpandL seems to have a positive impacton the verbs and adverbs, but not on the nouns andadjectives.
These trends are not held consistentlyacross data sets.
For example, we see that SemCoraugmentation helps both all POS tag sets over usingExpandL alone or when combined with SemCor.
Inorder to analyze this further, we explore the perfor-mance on the polysemous POS only in all the datasets.
We note that the same trend persists, SemCoraugmentation has a negative impact on the SV2 dataset in both WN 1.7.1. and WN 3.0. yet it benefitsall POS in the other data sets, namely SV3 WN1.7.1and SV3 WN3.0, SM WN2.1 and SM WN3.0.We did some basic data analysis on the items weare incapable of capturing.
Several of them are cases68of metonymy in examples such as ?the English areknown...?, the sense of English here is clearly in ref-erence to the people of England, however, our WSDsystem preferred the language sense of the word.
Ifit had access to syntactic/semantic role we would as-sume it could capture that this sense of the wordentails volition for example.
Other types of errorsresulted from the lack of a method to help identifymultiwords.7 Conclusions and Future DirectionsIn this paper, we presented improvements on stateof the art monolingual all words WSD using a wellestablished graph based algorithm coupled with en-hancements on basic similarity measures.
We alsoexplored the impact of augmenting WordNet withmore gloss examples from a hand annotated re-source as a means of improving WSD performance.We present the best results to date for an unsuper-vised approach on standard data sets: Senseval 2(62.7%) using WN1.7.1, and Senseval 3 (59.7%) us-ing WN1.7.1.
In the future, we would like to explorethe incorporation of multiword chunks, documentlevel lexical chains, and syntactic features in themodeling of the Lesk overlap measure.
We wouldlike to further explore why ExpandL conditions didnot yield the expected high performance across thedifferent POS tags.
Moreover, we are still curiousas to why SemCor expansion did not help the nounsperformance in SV2 conditions specifically.ReferencesMarine Carpuat and Dekai Wu.
2007.
Improving statisti-cal machine translation using word sense disambigua-tion.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 61?72, Prague, Czech Re-public, June.
Association for Computational Linguis-tics.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics, pages 33?40, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Christiane Fellbaum.
1998.
?wordnet: An electronic lex-ical database?.
MIT Press.J.
Jiang and D. Conrath.
1997.
Semantic similarity basedon corpus statistics and lexical taxonomy.
In Proceed-ings of the International Conference on Research inComputational Linguistics, Taiwan.C.
Leacock and M. Chodorow.
1998.
Combining lo-cal context and wordnet sense similarity for wordsense identification.
In WordNet, An Electronic Lex-ical Database.
The MIT Press.M.
Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: How to tell a pine conefrom an ice cream cone.
In In Proceedings of the SIG-DOC Conference, Toronto, June.S.
Cotton L. Delfs M. Palmer, C. Fellbaum and H. Dang.2001.
English tasks: all-words and verb lexical sam-ple.
In In Proceedings of ACL/SIGLEX Senseval-2,Toulouse, France, June.Rada Mihalcea.
2005.
Unsupervised large-vocabularyword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 411?418, Vancouver, British Columbia,Canada, October.
Association for Computational Lin-guistics.George A. Miller.
1990.
Wordnet: a lexical database forenglish.
In Communications of the ACM, pages 39?41.Roberto Navigli.
2009.
Word sense disambiguation:a survey.
In ACM Computing Surveys, pages 1?69.ACM Press.Banerjee Pedersen and Patwardhan.
2005.
Maximiz-ing semantic relatedness to perform word sense disam-biguation.
In University of Minnesota SupercomputingInstitute Research Report UMSI 2005/25, Minnesotta,March.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
Semeval-2007 task-17: En-glish lexical sample, srl and all words.
In Proceed-ings of the Fourth International Workshop on Seman-tic Evaluations (SemEval-2007), pages 87?92, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervisedgraph-based word sense disambiguation using mea-sures of word semantic similarity.
In Proceedings ofthe IEEE International Conference on Semantic Com-puting (ICSC 2007), Irvine, CA.Benjamin Snyder and Martha Palmer.
2004.
The englishall-words task.
In Rada Mihalcea and Phil Edmonds,editors, Senseval-3: Third International Workshop onthe Evaluation of Systems for the Semantic Analysis ofText, pages 41?43, Barcelona, Spain, July.
Associationfor Computational Linguistics.69
