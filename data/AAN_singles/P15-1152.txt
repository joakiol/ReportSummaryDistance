Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1577?1586,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsNeural Responding Machine for Short-Text ConversationLifeng Shang, Zhengdong Lu, Hang LiNoah?s Ark LabHuawei Technologies Co. Ltd.Sha Tin, Hong Kong{shang.lifeng,lu.zhengdong,hangli.hl}@huawei.comAbstractWe propose Neural Responding Ma-chine (NRM), a neural network-based re-sponse generator for Short-Text Conver-sation.
NRM takes the general encoder-decoder framework: it formalizes the gen-eration of response as a decoding processbased on the latent representation of the in-put text, while both encoding and decod-ing are realized with recurrent neural net-works (RNN).
The NRM is trained witha large amount of one-round conversationdata collected from a microblogging ser-vice.
Empirical study shows that NRMcan generate grammatically correct andcontent-wise appropriate responses to over75% of the input text, outperforming state-of-the-arts in the same setting, includingretrieval-based and SMT-based models.1 IntroductionNatural language conversation is one of themost challenging artificial intelligence problems,which involves language understanding, reason-ing, and the utilization of common sense knowl-edge.
Previous works in this direction mainly fo-cus on either rule-based or learning-based meth-ods (Williams and Young, 2007; Schatzmann etal., 2006; Misu et al, 2012; Litman et al, 2000).These types of methods often rely on manual effortin designing rules or automatic training of modelwith a particular learning algorithm and a smallamount of data, which makes it difficult to developan extensible open domain conversation system.Recently due to the explosive growth of mi-croblogging services such as Twitter1and Weibo2,the amount of conversation data available on theweb has tremendously increased.
This makes a1https://twitter.com/.2http://www.weibo.com/.data-driven approach to attack the conversationproblem (Ji et al, 2014; Ritter et al, 2011) pos-sible.
Instead of multiple rounds of conversation,the task at hand, referred to as Short-Text Conver-sation (STC), only considers one round of conver-sation, in which each round is formed by two shorttexts, with the former being an input (referred to aspost) from a user and the latter a response given bythe computer.
The research on STCmay shed lighton understanding the complicated mechanism ofnatural language conversation.Previous methods for STC fall into two cat-egories, 1) the retrieval-based method (Ji et al,2014), and 2) the statistical machine translation(SMT) based method (Sordoni et al, 2015; Rit-ter et al, 2011).
The basic idea of retrieval-based method is to pick a suitable response byranking the candidate responses with a linear ornon-linear combination of various matching fea-tures (e.g.
number of shared words).
The maindrawbacks of the retrieval-based method are thefollowing?
the responses are pre-existing and hard to cus-tomize for the particular text or requirementfrom the task, e.g., style or attitude.?
the use of matching features alone is usu-ally not sufficient for distinguishing posi-tive responses from negative ones, even aftertime consuming feature engineering.
(e.g., apenalty due to mismatched named entities isdifficult to incorporate into the model)The SMT-based method, on the other hand, isgenerative.
Basically it treats the response genera-tion as a translation problem, in which the model istrained on a parallel corpus of post-response pairs.Despite its generative nature, the method is intrin-sically unsuitable for response generation, becausethe responses are not semantically equivalent tothe posts as in translation.
Actually one post canreceive responses with completely different con-tent, as manifested through the example in the fol-1577lowing figure:Post Having my fish sandwich right nowUserA For god?s sake, it is 11 in the morningUserB Enhhhh... sounds yummyUserC which restaurant exactly?Empirical studies also showed that SMT-basedmethods often yield responses with grammaticalerrors and in rigid forms, due to the unnecessaryalignment between the ?source?
post and the ?tar-get?
response (Ritter et al, 2011).
This rigid-ity is still a serious problem in the recent workof (Sordoni et al, 2015), despite its use of neu-ral network-based generative model as features indecoding.1.1 OverviewIn this paper, we take a probabilistic model to ad-dress the response generation problem, and pro-pose employing a neural encoder-decoder for thistask, named Neural Responding Machine (NRM).The neural encoder-decoder model, as illustratedin Figure 1, first summarizes the post as a vectorrepresentation, then feeds this representation to adecoder to generate responses.
We further gener-alize this scheme to allow the post representationto dynamically change during the generation pro-cess, following the idea in (Bahdanau et al, 2014)originally proposed for neural-network-based ma-chine translation with automatic alignment.EncoderHaving my fish sandwich right nowFor god's sake, it is 11 in the morningDecoderEnhhhh... sounds yummy which restaurant exactly?vectorFigure 1: The diagram of encoder-decoder frame-work for automatic response generation.NRM essentially estimates the likelihood of aresponse given a post.
Clearly the estimated prob-ability should be complex enough to represent allthe suitable responses.
Similar framework hasbeen used for machine translation with a remark-able success (Kalchbrenner and Blunsom, 2013;Auli et al, 2013; Sutskever et al, 2014; Bah-danau et al, 2014).
Note that in machine trans-lation, the task is to estimate the probability of atarget language sentence conditioned on the sourcelanguage sentence with the same meaning, whichis much easier than the task of STC which weare considering here.
In this paper, we demon-strate that NRM, when equipped with a reasonableamount of data, can yield a satisfying estimator ofresponses (hence response generator) for STC, de-spite the difficulty of the task.Our main contributions are two-folds: 1) wepropose to use an encoder-decoder-based neu-ral network to generate a response in STC; 2)we have empirically verified that the proposedmethod, when trained with a reasonable amount ofdata, can yield performance better than traditionalretrieval-based and translation-based methods.1.2 RoadMapIn the remainder of this paper, we start with in-troducing the dataset for STC in Section 2.
Thenwe elaborate on the model of NRM in Section 3,followed by the details on training in Section 4.After that, we report the experimental results inSection 5.
In Section 6 we conclude the paper.2 The Dataset for STCOur models are trained on a corpus of roughly 4.4million pairs of conversations from Weibo3.2.1 Conversations on Sina WeiboWeibo is a popular Twitter-like microblogging ser-vice in China, on which a user can post short mes-sages (referred to as post in the reminder of thispaper) visible to the public or a group of users fol-lowing her/him.
Other users make comment on apublished post, which will be referred to as a re-sponse.
Just like Twitter, Weibo also has the lengthlimit of 140 Chinese characters on both posts andresponses, making the post-response pair an idealsurrogate for short-text conversation.2.2 Dataset DescriptionTo construct this million scale dataset, we firstcrawl hundreds of millions of post-response pairs,and then clean the raw data in a similar way assuggested in (Wang et al, 2013), including 1) re-moving trivial responses like ?wow?, 2) filteringout potential advertisements, and 3) removing theresponses after first 30 ones for topic consistency.Table 1 shows some statistics of the dataset used3http://www.noahlab.com.hk/topics/ShortTextConversation1578Training#posts 219,905#responses 4,308,211#pairs 4,435,959Test Data #test posts 110Labeled Dataset(retrieval-based)#posts 225#responses 6,017#labeled pairs 6,017Fine Tuning(SMT-based)#posts 2,925#responses 3,000#pairs 3,000Table 1: Some statistics of the dataset.
LabeledDataset and Fine Tuning are used by retrieval-based method for learning to rank and SMT-basedmethod for fine tuning, respectively.in this work.
It can be seen that each post have 20different responses on average.
In addition to thesemantic gap between post and its responses, thisis another key difference to a general parallel dataset used for traditional translation.3 Neural Responding Machines for STCThe basic idea of NRM is to build a hidden rep-resentation of a post, and then generate the re-sponse based on it, as shown in Figure 2.
Inthe particular illustration, the encoder convertsthe input sequence x = (x1, ?
?
?
, xT) into a setof high-dimensional hidden representations h =(h1, ?
?
?
, hT), which, along with the attention sig-nal at time t (denoted as ?t), are fed to the context-generator to build the context input to decoder attime t (denoted as ct).
Then ctis linearly trans-formed by a matrix L (as part of the decoder) intoa stimulus of generating RNN to produce the t-thword of response (denoted as yt).In neural translation system, L converts the rep-resentation in source language to that of target lan-guage.
In NRM, L plays a more difficult role: itneeds to transform the representation of post (orsome part of it) to the rich representation of manyplausible responses.
It is a bit surprising that thiscan be achieved to a reasonable level with a lineartransformation in the ?space of representation?, asvalidated in Section 5.3, where we show that onepost can actually invoke many different responsesfrom NRM.The role of attention signal is to determinewhich part of the hidden representation h shouldbe emphasized during the generation process.
Itshould be noted that ?tcould be fixed over time orEncoderContext GeneratorDecoderAttention SignalFigure 2: The general framework and dataflow ofthe encoder-decoder-based NRM.changes dynamically during the generation of re-sponse sequence y.
In the dynamic settings, ?tcan be function of historically generated subse-quence (y1, ?
?
?
, yt?1), input sequence x or theirlatent representations, more details will be shownlater in Section 3.2.We use Recurrent Neural Network (RNN) forboth encoder and decoder, for its natural abilityto summarize and generate word sequence of ar-bitrary lengths (Mikolov et al, 2010; Sutskever etal., 2014; Cho et al, 2014).?????
?Figure 3: The graphical model of RNN decoder.The dashed lines denote the variables related to thefunction g(?
), and the solid lines denote the vari-ables related to the function f(?
).3.1 The Computation in DecoderFigure 3 gives the graphical model of the de-coder, which is essentially a standard RNN lan-guage model except conditioned on the context in-put c. The generation probability of the t-th wordis calculated byp(yt|yt?1, ?
?
?
, y1,x) = g(yt?1, st, ct),1579where ytis a one-hot word representation, g(?)
isa softmax activation function, and stis the hiddenstate of decoder at time t calculated byst= f(yt?1, st?1, ct),and f(?)
is a non-linear activation function andthe transformation L is often assigned as pa-rameters of f(?).
Here f(?)
can be a logisticfunction, the sophisticated long short-term mem-ory (LSTM) unit (Hochreiter and Schmidhuber,1997), or the recently proposed gated recurrentunit (GRU) (Chung et al, 2014; Cho et al, 2014).Compared to ?ungated?
logistic function, LSTMand GRU are specially designed for its long termmemory: it can store information over extendedtime steps without too much decay.
We use GRUin this work, since it performs comparably toLSTM on squence modeling (Chung et al, 2014;Greff et al, 2015), but has less parameters and eas-ier to train.We adopt the notation of GRU from (Bahdanauet al, 2014), the hidden state stat time t is a linearcombination of its previous hidden state st?1anda new candidate state s?t:st= (1 ?
zt) ?
st?1+ zt?
s?t,where ?
is point-wise multiplication, ztis the up-date gate calculated byzt= ?
(Wze(yt?1) + Uzst?1+ Lzct) , (1)and s?tis calculated bys?t=tanh (We(yt?1) + U(rt?
st?1) + Lct) , (2)where the reset gate rtis calculated byrt= ?
(Wre(yt?1) + Urst?1+ Lrct) .
(3)In Equation (1)-(2), and (3), e(yt?1) is word em-bedding of the word yt?1, L = {L,Lz, Lr} spec-ifies the transformations to convert a hidden rep-resentation from encoder to that of decoder.
Inthe STC task, L should have the ability to trans-form one post (or its segments) to multiple differ-ent words of appropriate responses.3.2 The Computation in EncoderWe consider three types of encoding schemes,namely 1) the global scheme, 2) the local scheme,and the hybrid scheme which combines 1) and 2).Global Scheme: Figure 4 shows the graphicalmodel of the RNN-encoder and related contextgenerator for a global encoding scheme.
Thehidden state at time t is calculated by ht=f(xt, ht?1) (i.e.
still GRU unit), and with a trivialcontext generation operation, we essentially usethe final hidden state hTas the global represen-tation of the sentence.
The same strategy has beentaken in (Cho et al, 2014) and (Sutskever et al,2014) for building the intermediate representationfor machine translation.
This scheme however hasits drawbacks: a vectorial summarization of theentire post is often hard to obtain and may lose im-portant details for response generation, especiallywhen the dimension of the hidden state is not bigenough4.
In the reminder of this paper, a NRMwith this global encoding scheme is referred to asNRM-glo.?
?Context GeneratorFigure 4: The graphical model of the encoder inNRM-glo, where the last hidden state is used asthe context vector ct= hT.Local Scheme: Recently, Bahdanau et al(2014) and Graves (2013) introduced an attentionmechanism that allows the decoder to dynamicallyselect and linearly combine different parts of theinput sequence ct=?Tj=1?tjhj, where weight-ing factors ?tjdetermine which part should be se-lected to generate the new word yt, which in turnis a function of hidden states ?tj= q(hj, st?1),as pictorially shown in Figure 5.
Basically, the at-tention mechanism ?tjmodels the alignment be-tween the inputs around position j and the outputat position t, so it can be viewed as a local match-ing model.
This local scheme is devised in (Bah-danau et al, 2014) for automatic alignment be-4Sutskever et al (2014) has to use 4, 000 dimension forsatisfying performance on machine translation, while (Cho etal., 2014) with a smaller dimension perform poorly on trans-lating an entire sentence.1580tween the source sentence and the partial targetsentence in machine translation.
This scheme en-joys the advantage of adaptively focusing on someimportant words of the input text according to thegenerated words of response.
A NRM with thislocal encoding scheme is referred to as NRM-loc.?
?Attention SignalContext GeneratorFigure 5: The graphical model of the encoder inNRM-loc, where the weighted sum of hidden satesis used as the context vector ct=?Tj=1?tjhj.3.3 Extensions: Local and Global ModelIn the task of STC, NRM-glo has the summariza-tion of the entire post, while NRM-loc can adap-tively select the important words in post for vari-ous suitable responses.
Since post-response pairsin STC are not strictly parallel and a word in differ-ent context can have different meanings, we con-jecture that the global representation in NRM-glomay provide useful context for extracting the localcontext, therefore complementary to the schemein NRM-loc.
It is therefore a natural extensionto combine the two models by concatenating theirencoded hidden states to form an extended hid-den representation for each time stamp, as illus-trated in Figure 6.
We can see the summarizationhgTis incorporated into ctand ?tjto provide aglobal context for local matching.
With this hy-brid method, we hope both the local and global in-formation can be introduced into the generation ofresponse.
The model with this context generationmechanism is denoted as NRM-hyb.It should be noticed that the context generatorin NRM-hyb will evoke different encoding mecha-nisms in the global encoder and the local encoder,although they will be combined later in forminga unified representation.
More specifically, thelast hidden state of NRM-glo plays a role differ-ent from that of the last state of NRM-loc, sinceit has the responsibility to encode the entire inputsentence.
This role of NRM-glo, however, tendsto be not adequately emphasized in training thehybrid encoder when the parameters of the twoencoding RNNs are learned jointly from scratch.For this we use the following trick: we first ini-tialize NRM-hyb with the parameters of NRM-locand NRM-glo trained separately, then fine tune theparameters in encoder along with training the pa-rameters of decoder.global  encoderlocal  encoder?
?Attention SignalContext Generator ?
?Figure 6: The graphical model for the encoderin NRM-hyb, while context generator function isct=?Tj=1?tj[hlj;hgT], here [hlj;hgT] denotes theconcatenation of vectors hljand hgTTo learn the parameters of the model, we max-imize the likelihood of observing the original re-sponse conditioned on the post in the training set.For a new post, NRMs generate their responses byusing a left-to-right beam search with beam size =10.4 ExperimentsWe evaluate three different settings of NRM de-scribed in Section 3, namely NRM-glo, NRM-loc, and NRM-hyb, and compare them to retrieval-based and SMT-based methods.4.1 Implementation DetailsWe use Stanford Chinese word segmenter5to splitthe posts and responses into sequences of words.Although both posts and responses are written inthe same language, the distributions on words forthe two are different: the number of unique wordsin post text is 125,237, and that of response text is679,958.
We therefore construct two separate vo-cabularies for posts and responses by using 40,000most frequent words on each side, covering 97.8%5http://nlp.stanford.edu/software/segmenter.shtml1581usage of words for post and 96.2% for responserespectively.
All the remaining words are replacedby a special token ?UNK?.
The dimensions of thehidden states of encoder and decoder are both1,000.
Model parameters are initialized by ran-domly sampling from a uniform distribution be-tween -0.1 and 0.1.
All our models were trained ona NVIDIA Tesla K40 GPU using stochastic gra-dient descent (SGD) algorithm with mini-batch.The training stage of each model took about twoweeks.4.2 Competitor ModelsRetrieval-based: with retrieval-based models,for any given post p?, the response r?is retrievedfrom a big post-response pairs (p, r) repository.Such models rely on three key components: a bigrepository, sets of feature functions ?i(p?, (p, r)),and a machine learning model to combine thesefeatures.
In this work, the whole 4.4 millionWeibo pairs are used as the repository, 14 fea-tures, ranging from simple cosine similarity tosome deep matching models (Ji et al, 2014) areused to determine the suitability of a post to agiven post p?through the following linear modelscore(p?, (p, r)) =?i?i?i(p?, (p, r)).
(4)Following the ranking strategy in (Ji et al, 2014),we pick 225 posts and about 30 retrieved re-sponses for each of them given by a baseline re-triever6from the 4.4M repository, and manuallylabel them to obtain labeled 6,017 post-responsepairs.
We use ranking SVM model (Joachims,2006) for the parameters ?ibased on the labeleddataset.
In comparison to NRM, only the top oneresponse is considered in the evaluation process.SMT-based: In SMT-based models, the post-response pairs are directly used as parallel datafor training a translation model.
We use the mostwidely used open-source phrase-based translationmodel-Moses (Koehn et al, 2007).
Another par-allel data consisting of 3000 post-response pairs isused to tune the system.
In (Ritter et al, 2011),the authors used a modified SMT model to obtainthe ?Response?
of Twitter ?Stimulus?.
The mainmodification is in replacing the standard GIZA++word alignment model (Och and Ney, 2003) with anew phrase-pair selection method, in which all the6we use the default similarity function of Lucene7possible phrase-pairs in the training data are con-sidered and their associated probabilities are es-timated by the Fisher?s Exact Test, which yieldsperformance slightly better than default setting8.Compared to retrieval-based methods, the gener-ated responses by SMT-based methods often havefluency or even grammatical problems.
In thiswork, we choose the Moses with default settingsas our SMT model.5 Results and AnalysisAutomatic evaluation of response generation isstill an open problem.
The widely accepted evalu-ation methods in translation (e.g.
BLEU score (Pa-pineni et al, 2002)) do not apply, since the rangeof the suitable responses is so large that it is prac-tically impossible to give reference with adequatecoverage.
It is also not reasonable to evaluate withPerplexity, a generally used measurement in statis-tical language modeling, because the naturalnessof response and the relatedness to the post can notbe well evaluated.
We therefore resort to humanjudgement, similar to that taken in (Ritter et al,2011) but with an important difference.5.1 Evaluation MethodsWe adopt human annotation to compare the per-formance of different models.
Five labelers withat least three-year experience of SinaWeibo are in-vited to do human evaluation.
Responses obtainedfrom the five evaluated models are pooled and ran-domly permuted for each labeler.
The labelers areinstructed to imagine that they were the authorsof the original posts and judge whether a response(generated or retrieved) is appropriate and naturalto a input post.
Three levels are assigned to a re-sponse with scores from 0 to 2:?
Suitable (+2): the response is evidently an ap-propriate and natural response to the post;?
Neutral (+1): the response can be a suitableresponse in a specific scenario;?
Unsuitable (0): it is hard or impossible to finda scenario where response is suitable.To make the annotation task operable, the suit-ability of generated responses is judged from thefollowing five criteria:(a) Grammar and Fluency: Responses shouldbe natural language and free of any fluency orgrammatical errors;8Reported results showed that the new model outper-formed the baseline SMT model 57.7% of the time.1582Post ???????????...????
There are always 8 players at the Italian restricted area.
Unbelievable!
Related Criterion LabelsResponse1 ?????????????????
I am a big fan of the Italy team, waiting for the football game to start Logic Consistency UnsuitableResponse2 ??????????
Italian food is absolutely delicious.
Semantic Relevance UnsuitableResponse3 ?????!
Unbelievable!
Generality NeutralResponse4 ?????
?0?0 ????????
Ha!Ha!Ha!
it is still 0:0, no goal so far.
Scenario Dependence NeutralResponse5 ????????????
This is exactly the Italian defending style football game.
??
SuitableFigure 7: An example post and its five candidate responses with human annotation.
The content of thepost implies that the football match is already started, while the author of Response1 is still waiting forthe match to start.
Response2 talks about the food of Italy.
Response3 is a widely used response, but itis suitable to this post.
Response4 states that the current score is still 0:0, it is a suitable response only inthis specific scenario.Models Mean Score Suitable (+2) Neutral (+1) Unsuitable (0) AgreementNRM-glo 0.969 34.0% 28.9% 37.1% 0.397NRM-loc 1.065 36.0% 34.5% 29.5% 0.247NRM-hyb 1.156 39.3% 37.1% 23.6% 0.309Rtr.-based 0.984 29.8% 38.7% 31.5% 0.346SMT-based 0.313 5.6 % 20.0% 74.4% 0.448Table 2: The results of evaluated methods.
Mean score is the average value of annotated scores over allannotations.
(Rtr.-based means the retrieval-based method)ID Post NRM-glo NRM-loc NRM-hyb Rtr.-based1????????
?High fever attacks me HYHU\1HZ<HDU?V'D\?????????
?Rest more to boostyour immune system.?????????
?Go to see a doctor HYHU\1HZ<HDU?V'D\??????????
?Get well soon and stayhealthy!???+LJKIHYHU?2???????????????
?I gain one more year.Grateful to my group, sohappy??????????
?Happy birthday!
Willstand by you forever!???????????
?Happy birthday!
Wishyou always stay youngand pretty!???????????
?Getting old now.
Timehas no mercy.??????
?I just gain only onemore year.3?????????????
?First day of being avegetarian.
Hold on,CuiDuoLa????????
?I come to offer mysupport.
Keep it up!????????
?Hold on, keep it up.????????
?I have been being avegetarian everyday???
?Hold on!4????????????????
?We should go out withsome cute guys to enjoya great outing in such anice weather.????????
?Where is it?
It is sobeautiful!?????????
?Such a nice sunny day!I am in a great mood.?????????
?It is indeed a very niceweather.????????????
?WenShanOHW?VJRRXWto get someinspiration.
Ha!
Ha!Figure 8: Some responses generated by different models (originally in Chinese with their literal Englishtranslation), where the words in boldfaces are entity names.1583(b) Logic Consistency: Responses should be log-ically consistent with the test post;(c) Semantic Relevance: Responses should besemantically relevant to the test post;(d) Scenario Dependence: Responses can de-pend on a specific scenario but should not con-tradict the first three criteria;(e) Generality: Responses can be general butshould not contradict the first three criteria;If any of the first three criteria (a), (b), and (c)is contradicted, the generated response should belabeled as ?Unsuitable?.
The responses that aregeneral or suitable to post in a specific scenarioshould be labeled as ?Neutral?.
Figure 7 showsan example of the labeling results of a post and itsresponses.
The first two responses are labeled as?Unsuitable?
because of the logic consistency andsemantic relevance errors.
Response4 depends onthe scenario (i.e., the current score is 0:0), and istherefore annotated as ?Neutral?.Model A Model BAveragerankingsp valueNRM-loc NRM-glo (1.463, 1.537) 2.01%NRM-hyb NRM-glo (1.434, 1.566) 0.01%NRM-hyb NRM-loc (1.465, 1.535) 3.09%Rtr.-based NRM-glo (1.512, 1.488) 48.1%Rtr.-based NRM-loc (1.533, 1.467) 6.20%Rtr.-based NRM-hyb (1.552, 1.448) 0.32%SMT NRM-hyb (1.785, 1.215) 0.00 %SMT Rtr.-based (1.738, 1.262) 0.00 %Table 3: p-values and average rankings of Fried-man test for pairwise model comparison.
(Rtr.-based means the retrieval-based method)5.2 ResultsOur test set consists of 110 posts that do not ap-pear in the training set, with length between 6 to22 Chinese words and 12.5 words on average.
Theexperimental results based on human annotationare summarized in Table 2, consisting of the ra-tio of three categories and the agreement amongthe five labelers for each model.
The agreement isevaluated by Fleiss?
kappa (Fleiss, 1971), as a sta-tistical measure of inter-rater consistency.
Exceptthe SMT-based model, the value of agreement isin a range from 0.2 to 0.4 for all the other mod-els, which should be interpreted as ?Fair agree-ment?.
The SMT-based model has a relativelyhigher kappa value 0.448, which is larger than 0.4and considered as ?Moderate agreement?, sincethe responses generated by the SMT often have thefluency and grammatical errors, making it easy toreach an agreement on such unsuitable cases.From Table 2, we can see the SMT method per-forms significantly worse than the retrieval-basedand NRM models and 74.4% of the generated re-sponses were labeled as unsuitable mainly due tofluency and relevance errors.
This observationconfirms with our intuition that the STC dataset,with one post potentially corresponding to manyresponses, can not be simply taken as parallel cor-pus in a SMT model.
Surprisingly, more than 60%of responses generated by all the three NRM arelabeled as ?Suitable?
or ?Neutral?, which meansthat most generated responses are fluent and se-mantically relevant to post.
Among all the NRMvariants?
NRM-loc outperforms NRM-glo, suggestingthat a dynamically generated context mightbe more effective than a ?static?
fixed-lengthvector for the entire post, which is consistentwith the observation made in (Bahdanau et al,2014) for machine translation;?
NRM-hyp outperforms NRM-loc and NRM-glo, suggesting that a global representation ofpost is complementary to dynamically gener-ated local context.The retrieval-based model has the similar meanscore as NRM-glo, and its ratio on neutral casesoutperforms all the other methods.
This is be-cause 1) the responses retrieved by retrieval-basedmethod are actually written by human, so theydo not suffer from grammatical and fluency prob-lems, and 2) the combination of various featurefunctions potentially makes sure the picked re-sponses are semantically relevant to test posts.However the picked responses are not customizedfor new test posts, so the ratio of suitable cases islower than the three neural generation models.To test statistical significance, we use theFriedman test (Howell, 2010), which is a non-parametric test on the differences of several re-lated samples, based on ranking.
Table 3 showsthe average rankings over all annotations and thecorresponding p-values for comparisons betweendifferent pairs of methods.
The comparison be-tween retrieval-based and NRM-glo is not signif-icant and their difference in ranking is tiny.
Thisindicates that the retrieval-based method is com-1584parable to the NRM-glo method.
The NRM-hyboutperforms all the other methods, and the differ-ence is statistically significant (p < 0.05).
Thedifference between NRM-loc and retrieval-basedmethod is marginal (p = 0.062).
SMT is signif-icantly worse than retrieval-based and NRM-hybmethods.5.3 Case StudyFigure 8 shows some example responses gener-ated by our NRMs (only the one with biggestlikelihood is given) and the comparable retrieval-based model.
It is intriguing to notice that threeNRM variants give suitable but quite distinct re-sponses, with different perspectives and choicesof words.
This, as we conjecture, is caused byboth the architecture variations among models aswell as the variations from random effects likethe initialization of parameters.
Another interest-ing observation is on the fourth example, wherethe retrieval-based method returns a response withthe mismatched entity name ?WenShan?, whichis actually a quite common problem for retrieval-based model, where the inconsistency details (e.g.,dates, named entities), which often render the re-sponse unsuitable, cannot be adequately consid-ered in the matching function employed in retriev-ing the responses.
In contrast, we observe thatNRMs tend to make general response and barelygenerate those details.Post ??????????????
First day of being a vegetarian.
Hold on, CuiDuoLaR1 ?????????
Hold on, Keep up your vegetarian diet.R2?????????????????????
?Would you like to be healthy?
Would you like to livelong?
If so, follow me on Weibo.R3 ???????????
This is the so-called vegetarian diet ?R4????????
?Eat up so that you will have enough energy to go fora diet!R5?????????????
?Health is the essence for winning your future battles.No rush!Figure 9: Multiple responses generated by theNRM-hyb.We also use the NRM-hyb as an example to in-vestigate the ability of NRM to generate multi-ple responses.
Figure 9 lists 5 responses to thesame post, which are gotten with beam search withbeam size = 500, among which we keep only thebest one (biggest likelihood) for each first word.It can be seen that the responses are fluent, rele-vant to the post, and still vastly different from eachother, validating our initial conjecture that NRM,when fueled with large and rich training corpus,could work as a generator that can cover a lot ofmodes in its density estimation.It is worth mentioning that automatic evaluationmetrics, such as BLEU (Papineni et al, 2002) asadopted by machine translation and recently SMT-based responding models (Sordoni et al, 2015), donot work very well on this task, especially whenthe reference responses are few.
Our results showthat the average BLEU values are less than 2 forall models discussed in this paper, including SMT-based ones, on instances with single reference.Probably more importantly, the ranking given bythe BLEU value diverges greatly from the humanjudgment of response quality.6 Conclusions and Future WorkIn this paper, we explored using encoder-decoder-based neural network system, with coined nameNeural Responding Machine, to generate re-sponses to a post.
Empirical studies confirm thatthe newly proposed NRMs, especially the hybridencoding scheme, can outperform state-of-the-artretrieval-based and SMT-based methods.
Our pre-liminary study also shows that NRM can generatemultiple responses with great variety to a givenpost.
In future work, we would consider addingthe intention (or sentiment) of users as an externalsignal of decoder to generate responses with spe-cific goals.AcknowledgmentsThe authors would like to thank Tao Cai for tech-nical support.
This work is supported in part byChina National 973 project 2014CB340301.ReferencesMichael Auli, Michel Galley, Chris Quirk, and Ge-offrey Zweig.
2013.
Joint language and transla-tion modeling with recurrent neural networks.
InEMNLP, pages 1044?1054.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.1585Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014.
Learning phrase representationsusing rnn encoder-decoder for statistical machinetranslation.
arXiv preprint arXiv:1406.1078.Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,and Yoshua Bengio.
2014.
Empirical evaluation ofgated recurrent neural networks on sequence model-ing.
arXiv preprint arXiv:1412.3555.Joseph L Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological bul-letin, 76(5):378.Alex Graves.
2013.
Generating sequences with recur-rent neural networks.
preprint arXiv:1308.0850.Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn?
?k,Bas R. Steunebrink, and J?urgen Schmidhuber.2015.
LSTM: A search space odyssey.
CoRR,abs/1503.04069.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.David C. Howell.
2010.
Fundamental Statistics for theBehavioral Sciences.
PSY 200 (300) QuantitativeMethods in Psychology Series.
Wadsworth CengageLearning.Zongcheng Ji, Zhengdong Lu, and Hang Li.
2014.
Aninformation retrieval approach to short text conver-sation.
arXiv preprint arXiv:1408.6988.Thorsten Joachims.
2006.
Training linear svms in lin-ear time.
In SIGKDD, pages 217?226.
ACM.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In EMNLP, pages1700?1709.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th annual meeting of the ACL oninteractive poster and demonstration sessions, pages177?180.
ACL.Diane Litman, Satinder Singh, Michael Kearns, andMarilyn Walker.
2000.
Njfun: a reinforcementlearning spoken dialogue system.
In Proceedingsof the 2000 ANLP/NAACL Workshop on Conversa-tional systems, pages 17?20.
ACL.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH 2010, pages 1045?1048.Teruhisa Misu, Kallirroi Georgila, Anton Leuski, andDavid Traum.
2012.
Reinforcement learning ofquestion-answering dialogue policies for virtual mu-seum guides.
In SIGDIAL, pages 84?93.
ACL.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Alan Ritter, Colin Cherry, and William B Dolan.
2011.Data-driven response generation in social media.
InEMNLP, pages 583?593.
Association for Computa-tional Linguistics.Jost Schatzmann, Karl Weilhammer, Matt Stuttle, andSteve Young.
2006.
A survey of statistical user sim-ulation techniques for reinforcement-learning of di-alogue management strategies.
The knowledge en-gineering review, 21(02):97?126.Alessandro Sordoni, Michel Galley, Michael Auli,Chris Brockett, Yangfeng Ji, MegMitchell, Jian-YunNie, Jianfeng Gao, and Bill Dolan.
2015.
A neuralnetwork approach to context-sensitive generation ofconversational responses.
Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics Human Language Technologies(NAACL-HLT 2015), June.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In NIPS, pages 3104?3112.Hao Wang, Zhengdong Lu, Hang Li, and EnhongChen.
2013.
A dataset for research on short-textconversations.
In EMNLP, pages 935?945.Jason D Williams and Steve Young.
2007.
Partiallyobservable markov decision processes for spokendialog systems.
Computer Speech & Language,21(2):393?422.1586
