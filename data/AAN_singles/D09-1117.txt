Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1124?1132,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPBidirectional Phrase-based Statistical Machine TranslationAndrew FinchNICT, Keihanna Science City,Kyoto, 619-0288, Japanandrew.finch@nict.go.jpEiichiro SumitaNICT, Keihanna Science City,Kyoto, 619-0288, Japaneiichiro.sumita@nict.go.jpAbstractThis paper investigates the effect of di-rection in phrase-based statistial machinetranslation decoding.
We compare a typ-ical phrase-based machine translation de-coder using a left-to-right decoding strat-egy to a right-to-left decoder.
We alsoinvestigate the effectiveness of a bidirec-tional decoding strategy that integratesboth mono-directional approaches, withthe aim of reducing the effects due to lan-guage specificity.
Our experimental eval-uation was extensive, based on 272 differ-ent language pairs, and gave the surprisingresult that for most of the language pairs,it was better decode from right-to-left thanfrom left-to-right.
As expected the rela-tive performance of left-to-right and right-to-left strategies proved to be highly lan-guage dependent.
The bidirectional ap-proach outperformed the both the left-to-right strategy and the right-to-left strategy,showing consistent improvements that ap-peared to be unrelated to the specific lan-guages used for translation.
Bidirectionaldecoding gave rise to an improvement inperformance over a left-to-right decodingstrategy in terms of the BLEU score in99% of our experiments.1 IntroductionHuman language production by its very nature isan ordered process.
That is to say, words are writ-ten/uttered in a sequence.
The current genera-tion of phrase-based statistical machine translation(SMT) systems also generate their target word se-quences according to an order.
Since the gener-ation process is symmetrical, there are two pos-sible strategies that could be used to generate thetarget: from beginning to end; or from end to be-ginning.
Generating the target in the ?wrong?
di-rection (the opposite direction to the way in whichhumans do) is counter intuitive, and possibly as aresult of this, SMT systems typically generate thetarget word sequence in the same order as humanlanguage production.
However it is not necessar-ily the case that this is most effective strategy forall language pairs.
In this paper we investigate theeffect of direction in phrase-based SMT decoding.For the purposes of this paper, we will referto target word sequence generation that followsthe same order as human language production asforward generation, and generation in the oppo-site direction to human language production as re-verse generation.
These are often referred ?left-to-right?
and ?right-to-left?
respectively in the litera-ture, but we avoid this notation as many languagesare naturally written from right-to-left.In earlier work (Watanabe and Sumita, 2002),it was hypothesized that the optimal direction fordecoding was dependent on the characteristics ofthe target language.
Their results show that forJapanese to English translation a reverse decod-ing strategy was the most effective, whereas forEnglish to Japanese translation, a forward decod-ing strategy proved superior.
In addition they im-plemented a bidirectional decoder, but their re-sults were mixed.
For English to Japanese transla-tion, decoding bidirectionally gives higher perfor-mance, but for Japanese to English translation theywere unable to improve performance by decod-ing bidirectionally.
Their experiments were per-formed using a decoder based on IBM Model 4using the translation techniques developed at IBM(Brown et al, 1993).This work is closely related to the techniquesproposed in (Watanabe and Sumita, 2002), but inour case we decode within the framework of aphrase-based SMT system, rather than the IBMmodel.
Our intention was to explore the effect ofdirection in decoding within the context of a more1124contemporary machine translation paradigm, andto experiment with a broader range of languages.The underlying motivation for our studies howeverremains the same.
Languages have considerablydifferent structure, and certain grammatical con-structs tend to occupy particular positions withinsentences of the same language, but different po-sitions across languages.
These differences maymake it easier to tackle the automatic translationof a sentence in a given language from a partic-ular direction.
Our approach differs in that thedecoding process of a phrased-based decoder isquite different from that used by (Watanabe andSumita, 2002) since decoding is done using largerunits making the re-ordering process much sim-pler.
In (Watanabe and Sumita, 2002) only onelanguage pair is considered, for our experimentswe extended this to include translation among 17different languages including the Japanese and En-glish pair used in (Watanabe and Sumita, 2002).We felt that it was important to consider as manylanguages as possible in this study, as intuitionand evidence from the original study suggests thatthe effect of direction in decoding is likely to bestrongly language dependent.The next section briefly describes the mecha-nisms underlying phrase-based decoding.
Thenwe explain the principles behind the forward, re-verse and bidirectional decoding strategies used inour experiments.
Section 3 presents the experi-ments we performed.
Section 4 gives the resultsand some analysis.
Finally in Section 5, we con-clude and offer possible directions for future re-search.2 Phrase-based TranslationFor our experiments we use the phrase-based ma-chine translation techniques described in (Koehn,2004) and (Koehn et al, 2007), integrating ourmodels within a log-linear framework (Och andNey, 2002).One of the advantages of a log-linear model isthat it is possible to integrate a diverse set of fea-tures into the model.
For the decoders used in theexperiments in this paper, we included the follow-ing feature functions:?
An n-gram language model over the targetword sequence- Ensures the target word sequence is alikely sequence of words in the targetlanguage?
A phrase translation model- Effects the segmentation of the sourceword sequence, and is also responsiblefor the transformation of source phrasesinto target phrases.?
A target word sequence length model- Controls the length of the target wordsequence.
This is usually a constantterm added for each word in the trans-lation hypothesis.?
A lexicalized distortion model- Influences the reordering of the trans-lated source phrases in the target wordsequence using lexical context on theboundaries of the phrases being re-ordered.2.1 DecodingIn a phrase-based SMT decoder, the word se-quence of the target language is typically gener-ated in order in a forward manner.
The wordsat the start of the translation are generated first,then the subsequent words, in order until the fi-nal word of the target word sequence is gener-ated.
As the process is phrase-based, the trans-lation is generated in a phrase-by-phrase manner,rather word-by-word.
The basic idea is to seg-ment the source word sequence into subsequences(phrases), then translate each phrase individually,and finally compose the target word sequence byreordering the translations of the source phrases.This composition must occur in a particular order,such that target words are generated sequentiallyfrom the start (or end in the case of reverse de-coding) of the sentence.
The reason that the targetneeds to be generated sequentially is to allow ann-gram language model to be applied to the partialtarget word sequence at each step of the decodingprocess.This process is illustrated in Figure 1.
In thedecoding for both forward and reverse decodersthe source sentence is segmented into 2 phrases:?where is?
and ?the station?
(although in this ex-ample the segmentation is the same for both de-coding strategies, it is not necessarily the casesince the search processes are different).
In theforward decoding process, first the English phrase?the station?
is translated into the Japanese phrase?eki wa?.
Initially the target sequence consists1125Left to right Right to leftwhere is the station<s><s> eki wa<s> eki wa doko </s></s>doko </s><s> eki wa doko </s>P(eki | <s> )P(wa | eki, <s>)P(doko | wa, eki, <s>)P(</s> | doko, wa, eki, <s>)PLM=GenerationP(doko | </s> )P(wa | doko, </s>)P(eki | wa, doko, </s>)P(<s> | eki, wa, doko, </s>)PLM=where is the stationFigure 1: The phrase-based decoding process for an English to Japanese translation, in both forwardand reverse directions.
The n-gram language model probability calculation for the completed translationhypotheses are also shown on the bottom of the figure.
See Section 2.1 for a description of the decodingprocess.of only the start of sentence marker ??s??.
Thismarker only serves as context to indicate the startof the sequence for the benefit of the languagemodel.
The first target phrase is separated into itscomponent words and each word is added in orderto the target word sequence.
Each addition causesan application of the language model, hence inFigure 1 the first term of PLMis P (eki|?s?
), thesecond is P (wa|?s?)
and so on.
For reverse de-coding, the target sentence is generated startingfrom the end of sentence marker ?/s?
with the lan-guage model context being to the right of the cur-rent word.
For the case of bidirectional decoding,the model probability for the hypothesis is a linearinterpolation of the scores for both forward and re-verse hypotheses.2.2 Direction in DecodingDirection in decoding influences both the modelsused by the decoder and the search process itself.The direction of decoding determines the orderin which target words are generated, the sourcephrases being translated in any order, therefore itis likely to be features of the target language ratherthan those of the the source language that deter-mine the effect that the decoding direction has ondecoder performance.2.2.1 The Language ModelThe fundamental difference between the languagemodels of a forward decoder and that of a reversedecoder is the direction in which the model looksfor its context.
The forward model looks backto the start of the sentence, whereas the reversemodel looks forward to the end of the sentence.2.2.2 The SearchAssuming a full search, a unigram language modeland no limitations on reordering, the forward andreverse decoding processes are equivalent.
Whenthese constraints are lifted, as is the case in theexperiments in this paper, the two search processesdiverge and can give rise to hypotheses that aredifferent in character.The partial hypotheses from early in the searchprocess for forward decoding represent hypothe-ses for the first few words of the target word se-quence, whereas the early partial hypotheses ofa reverse decoder hold the last few words.
Thishas two consequences for the search.
The first isthat (assuming a beam search as used in our ex-periments), certain candidate word sequences inthe early stages of the search might be outside thebeam and be pruned.
The consequence of thisis that sentences that start with (or end with inthe case of reverse decoding) the pruned word se-quence will not be considered during the remain-der of the search.
The second is that word se-1126quences in the partial hypotheses are used in thecontext of the models used in the subsequent de-coding.
Thus, correctly decoding the start (or endfor reverse decoding) of the sentence will benefitthe subsequent decoding process.3 Experiments3.1 Experimental DataThe experiments were conducted on all possi-ble pairings among 17 languages.
A key to theacronyms used for languages together with in-formation about their respective characteristics isgiven in Table 1.We used all of the first ATR Basic Travel Ex-pression Corpus (BTEC1) (Kikui et al, 2003) forthese experiments.
This corpus contains the kindof expressions that one might expect to find in aphrase-book for travelers.
The corpus is similar incharacter to the IWSLT06 Evaluation Campaignon Spoken Language Translation (Paul, 2006) J-Eopen track.
The sentences are relatively short (seeTable 1) with a simple structure and a fairly narrowrange of vocabulary due to the limited domain.The experiments were conducted on data thatcontained no case information, and also no punc-tuation (this was an arbitrary decision that we be-lieve had no impact on the results).We used a 1000 sentence development corpusfor all experiments, and the corpus used for eval-uation consisted of 5000 sentences with a singlereference for each sentence.3.2 TrainingEach instance of the decoder is a standard phrase-based machine translation decoder that operatesaccording to the same principles as the publiclyavailable PHARAOH (Koehn, 2004) and MOSES(Koehn et al, 2007) SMT decoders.
In theseexperiments 5-gram language models built withWitten-Bell smoothing were used along with a lex-icalized distortion model.
The system was trainedin a standard manner, using a minimum error-ratetraining (MERT) procedure (Och, 2003) with re-spect to the BLEU score (Papineni et al, 2001)on held-out development data to optimize the log-linear model weights.
For simplicity, the MERTprocedure was performed on independently on theforward and reverse decoders for the bidirectionalsystem, rather them attempting to tune the param-eters for the full system.3.3 Translation Engines3.3.1 ForwardThe forward decoding translation systems used inthese experiments represent the baseline of our ex-periments.
They consist of phrase-based, multi-stack, beam search decoders commonly used inthe field.3.3.2 ReverseThe reverse decoding translation systems used inthese experiments were exactly the same as theforward decoding systems.
The difference beingthe that word sequences in the training, develop-ment, and source side of the test corpora were re-versed prior to training the systems.
The final out-put of the reverse decoders was reordered in a postprocessing step before evaluation.3.3.3 BidirectionalThe decoder used for the bidirectional decodingexperiments was modified in order to be able todecode both forward and reverse in separate in-stances of the decoder.
Models for decoding inforward and reverse directions are loaded, and twodecoding instances created.
Scores for hypothesesthat share the same target word sequence from thetwo decoders were combined at the end of the de-coding process linearly using equal interpolationweights.
Hypotheses that were generated by onlyone of the component decoders were not pruned.The scores from these hypotheses only had a con-tribution from the decoder that was able to gener-ate them, the contribution from the other decoderbeing zero.3.4 Decoding ConstraintsThe experiments reported in this paper were con-ducted with loose constraints on the decoding asoverconstraining the decoding process could leadto differences between unidirectional and bidirec-tional strategies.
More specificially, the decod-ing was done with a beam width of 100, no beamthresholding and no constraints on the reorderingprocess.
Figure 2 shows the effect of varying thebeam width (stack size) in the search for forwarddecoder of the English to Japanese translation ex-periment.
At the beam width of 100 used in ourexperiments, the gains from doubling the beamwith are small (0.07 BLEU percentage points).It is also important to note that a future costidentical to that used in the MOSES decoder1127Abbreviation Language #Words Avg.
sent length Vocabulary Orderar Arabic 806853 5.16 47093 SVOda Danish 806853 5.16 47093 SVOde German 907354 5.80 23443 SVOen English 970252 6.21 12900 SVOes Spanish 881709 5.64 18128 SVOfr French 983402 6.29 17311 SVOid Indonesian (Malay) 865572 5.54 15527 SVOit Italian 865572 5.54 15527 SVOja Japanese 1149065 7.35 15405 SOVko Korean 1091874 6.98 17015 SOVms Malaysian (Malay) 873959 5.59 16182 SVOnl Dutch 927861 5.94 19775 SVOpt Portuguese 881428 5.64 18217 SVOru Russian 781848 5.00 32199 SVOth Thai 1211690 7.75 6921 SVOvi Vietnamese 1223341 7.83 8055 SVOzh Chinese 873375 5.59 14854 SVOTable 1: Key to the languages, corpus statistics and word order.
SVO denotes a language that predomi-nantly has subject-verb-object order, and SOV denotes a language that predominantly has subject-object-verb orderStack size BLEU Score1 0.39542 0.40324 0.40758 0.411516 0.414932 0.416164 0.4181128 0.4188256 0.4197512 0.41971024 0.41970.390.3980.4060.4140.4220 256 512 768 1024BLEUScoreStack sizeFigure 2: The performance of a forward decoder(En-Ja) with increasing stack size.
(Koehn et al, 2007) was also included in thescores for partial hypothesis during the decoding.3.5 Computational OverheadIn the current implementation, bidirectional de-coding takes twice as long as a mono-directionalsystem.
However, in a multi-threaded environ-ment, each instance of the decoder is able to runon its own thread in parallel, and so this slowdowncan be mitigated in some circumstances.
Futuregenerations of the bidirectional decoder will moretightly couple the two decoders, and we believethis will lead to faster and more effective search.3.6 EvaluationThe results presented in this paper are given interms of the BLEU score (Papineni et al, 2001).This metric measures the geometric mean of n-gram precision of n-grams drawn from the outputtranslation and a set of reference translations forthat translation.There are large number of proposed methodsfor carrying out machine translation evaluation.Methods differ in their focus of characteristics ofthe translation (for example fluency or adequacy),and moreover anomolous results can occur if asingle metric is relied on.
Therefore, we alsocarried out evaluations using the NIST (Dodding-ton, 2002), METEOR (Banerjee and Lavie, 2005),WER (Hunt, 1989), PER (Tillmann et al, 1997)and TER (Snover et al, 2005) machine translationevaluation techniques.4 ResultsThe results of the experiments in terms of theBLEU score are given in Tables ?
?, 5, 3 and3.
These results show the performance of the re-verse and bidirectional decoding strategies relativeto the usual forward decoding strategy.
The cellsin the tables that represent experiments in which1128ar da de en es fr id it ja ko ms nl pt ru th vi zhar - 47.8 48.8 51.7 48.8 47.3 46.5 49.2 29.8 27.8 46.9 49.0 49.0 47.8 39.7 43.0 27.8da 58.3 - 58.7 63.0 58.6 55.7 53.5 58.5 37.5 35.1 54.4 59.6 59.0 55.4 48.1 51.7 35.2de 53.8 55.5 - 59.4 55.9 51.9 50.3 55.3 34.2 32.0 50.8 57.0 55.9 51.2 45.7 48.9 32.7en 63.6 65.8 64.8 - 67.0 61.0 58.4 65.8 41.1 38.7 59.1 67.6 66.7 58.7 52.8 57.7 38.6es 57.6 58.2 58.0 65.6 - 56.6 54.2 61.1 38.3 36.4 54.3 59.6 62.6 55.1 47.6 51.3 36.0fr 57.8 58.3 58.0 62.3 58.9 - 52.7 57.4 39.1 37.7 53.8 58.3 57.9 54.8 47.7 50.4 37.6id 54.7 52.8 52.8 56.6 53.7 51.0 - 53.1 37.2 35.6 86.4 53.8 53.0 51.3 46.4 48.4 34.9it 54.1 53.4 54.4 59.4 56.4 51.8 49.2 - 34.4 32.8 49.9 55.1 56.2 50.5 44.0 47.0 33.6ja 38.2 39.2 38.6 41.9 39.9 40.2 40.7 39.5 - 69.4 40.4 39.5 39.7 37.8 37.3 37.2 52.1ko 34.4 35.3 34.6 38.2 36.3 36.2 36.8 35.6 66.4 - 36.6 35.6 36.3 34.5 34.2 34.1 46.4ms 54.5 52.7 52.6 56.2 53.4 50.6 82.5 53.2 36.8 34.9 - 53.6 53.4 51.3 46.7 49.2 34.8nl 55.1 57.3 58.8 63.2 58.5 54.5 52.4 57.1 36.7 34.1 53.4 - 58.3 53.5 48.7 50.7 35.2pt 56.8 57.7 57.6 63.8 62.0 55.5 52.7 59.7 37.8 36.4 53.4 58.7 - 54.2 47.1 50.6 35.8ru 51.4 49.1 50.2 53.3 52.0 48.7 48.6 51.6 31.9 29.5 49.1 50.9 50.5 - 41.8 43.7 30.0th 53.8 55.0 54.8 58.2 55.8 53.3 55.0 54.8 41.4 39.2 55.4 55.9 55.5 53.0 - 56.0 40.4vi 53.6 53.6 54.2 57.4 54.2 51.4 52.3 53.3 37.6 35.8 53.3 54.6 54.4 51.7 50.3 - 36.2zh 32.0 33.0 32.6 34.6 33.2 33.7 34.2 33.2 47.8 43.5 33.9 33.4 32.6 32.2 31.1 29.7 -Table 2: Baseline BLEU scores for all systems.
The figures represent the scores in BLEU percentagepoints of the baseline left-to-right decoding systems.
Source languages are indicated by the columnheaders, the row headers denoting the target languages.the forward strategy outperformed the contrastingstrategy are shaded in gray.
The numbers in thecells represent the difference in BLEU percentagepoints for the systems being compared in that cell.It is clear from Table 3 that for most of the lan-guage pairs (67% of them for BLEU, and a simi-lar percentage for all the other metrics except ME-TEOR), better evaluation scores were achieved byusing a reverse decoding strategy than a forwardstrategy.
This is a surprising result because lan-guage is produced naturally in a forward manner(by definition), and therefore one might expect thisto also be the optimal direction for word sequencegeneration in decoding.4.1 Word Order TypographyFollowing (Watanabe and Sumita, 2002), to ex-plain the effects we observe in our results we lookto the word order typography of the target lan-guage (Comrie and Vogel, 2000).
The word or-der of a language is defined in terms of the orderin which you would expect to encounter the finiteverb (V) and its arguments, subject (S) and ob-ject (O).
In most languages S precedes O and V.Whether or not O precedes or follows V definesthe two most prevalent word order types SOV andSVO (Comrie and Vogel, 2000).Two of the target languages in this study(Japanese and Korean) have the SOV word type,the remainder having the SVO word order type.In Table 3 looking at the rows for ja and ko wecan see that for both of these languages reversedecoding outperformed forward decoding in only4 out of 12 experiments.
Furthermore these twolanguages were the two languages that benefitedthe most (in terms of the number of experimentalcases) from forward decoding.
The two languagesalso agree on the best decoding direction for 12 ofthe 16 language pairs.
This apparent correlationmay reflect similarities between the two languages(word order type, or other common features of thelanguages).Given this evidence, it seems plausible thatword order does account in part for the differencesin performance when decoding in differing direc-tions, but this can only be part of the explanationsince there are 4 source languages for which re-verse decoding yielded higher performance.It should be noted that our results differ fromthose of (Watanabe and Sumita, 2002) for En-glish to Japanese translation, who observed gainswhen decoding in the reverse direction for this lan-guage pair.
It is hard to compare our results di-rectly with theirs however, due to the differencesin the decoders used in the experiments (ours be-ing phrase-based, and theirs based on the IBM ap-1129ar da de en es fr id it ja ko ms nl pt ru th vi zhar - 0.87 0.34 1.30 0.93 1.63 0.66 0.58 0.12 0.36 0.85 0.33 0.88 0.22 1.33 1.04 0.88da 0.25 - 0.41 0.71 0.56 0.70 1.10 0.31 0.46 0.07 0.96 0.13 0.62 0.17 1.28 0.71 0.29de 0.41 0.04 - 0.38 0.52 0.15 0.80 0.01 0.47 0.72 0.60 0.25 0.21 0.05 0.47 0.68 0.20en 0.04 0.05 0.21 - 0.05 0.13 0.58 0.02 0.73 0.35 0.39 0.07 0.52 0.05 0.67 0.63 0.29es 0.14 0.19 0.05 0.35 - 0.68 0.01 0.08 0.25 0.31 0.25 0.25 0.17 0.07 0.43 0.44 0.78fr 0.37 0.57 0.38 0.66 0.21 - 0.36 0.28 0.15 0.45 0.22 0.46 0.64 0.10 0.25 0.58 0.31id 0.16 0.02 0.31 1.45 0.58 0.50 - 0.34 0.03 0.27 0.00 0.42 0.57 0.36 0.53 1.04 0.59it 0.28 0.72 0.36 0.27 0.08 0.30 0.11 - 0.07 0.12 0.37 0.23 0.05 0.37 0.04 0.63 0.37ja 0.36 0.22 0.03 0.03 0.22 0.13 0.64 0.36 - 0.21 0.57 0.46 0.08 0.33 0.08 0.83 0.70ko 0.35 0.01 0.31 0.03 0.12 0.07 0.13 0.21 0.42 - 0.29 0.07 0.42 0.40 0.44 0.62 0.05ms 0.06 0.49 0.53 1.38 0.99 0.71 0.47 0.34 0.11 0.32 - 0.62 0.27 0.10 0.83 0.99 0.11nl 0.26 0.03 0.26 0.30 0.20 0.19 0.47 0.23 0.13 0.06 0.06 - 0.08 0.09 0.06 1.00 0.15pt 0.03 0.34 0.06 0.51 0.07 0.17 0.06 0.18 0.13 0.65 0.08 0.10 - 0.06 0.09 0.85 0.35ru 0.25 0.58 0.67 0.74 0.01 0.48 0.50 0.27 0.41 0.38 0.13 0.38 0.46 - 0.88 0.56 0.49th 0.19 0.28 0.21 0.41 0.05 0.23 0.30 0.00 0.34 0.04 0.25 0.07 0.21 0.08 - 0.46 0.25vi 0.21 0.34 0.24 0.65 0.72 0.34 0.06 0.59 0.24 0.22 0.19 0.12 0.11 0.18 0.63 - 0.15zh 0.43 0.26 0.42 0.05 0.15 0.31 0.16 0.28 0.00 0.31 0.40 0.14 0.67 0.18 0.39 0.21 -Table 3: Gains in BLEU score from reverse decoding over a forward decoding strategy The numbersin the cells are the differences in BLEU percentage points between the systems.
Shaded cells indicatethe cases where forward decoding give a higher score.
Source languages are indicated by the columnheaders, the row headers denoting the target languages.Metric Bi>For Bi>Rev Rev>ForBLEU 98.90 84.93 67.65NIST 98.53 78.31 75.00METEOR 99.63 95.96 50.74WER 99.26 92.85 66.18PER 98.53 84.97 70.59TER 99.63 91.18 68.75Table 4: Summary of the results using several au-tomatic metrics for evaluation.
Numbers in the ta-ble correspond to the percentage of experimentsin which the condition at the head of the columnwas true (for example figure in the first row andfirst column means that for 98.9 percent of the lan-guage pairs the BLEU score for the bidirectionaldecoder was better than that of the forward de-coder)proach (Brown et al, 1993)).The results were the similar in character whenother MT evaluation methods were used.
Theseresults are summarized in Table 3.4.2 Bidirectional DecodingTable 5 shows the performance of the bidirectionaldecoder relative to a forward decoder.
As can beseen from the table, in 269 out of the 272 experi-ments the bidirectional decoder outperformed theunidirectional decoder.
The gains ranged from amaximum of 1.81 BLEU (translating from Thaito Arabic) points, to a minimum of -0.04 BLEUpoints (translating from Indonesian to Japanese)with the average gain over all experiments being0.56 BLEU points.
It is clear from our experi-ments that there is much to be gained from decod-ing bidirectionally.
Our results were almost unani-mously positive, and in all three negative cases thedrop in performance was small.5 ConclusionIn this paper we have investigated the effects onphrase-based machine translation performance ofthree different decoding strategies: forward, re-verse and bidirectional.
The experiments wereconducted on a large set of source and target lan-guages consisting of 272 experiments representingall possible pairings from a set of 17 languages.These languages were very diverse in characterand included a broad selection of European andAsian languages.
The experimental results re-vealed that for SVO word order languages it isusually better to decode in a reverse manner, and incontrast, for SOV word order languages it is usu-1130ar da de en es fr id it ja ko ms nl pt ru th vi zhar - 0.66 0.51 1.03 0.65 0.75 0.59 0.47 0.46 0.85 0.59 0.69 0.39 0.30 1.81 1.30 0.85da 0.27 - 0.61 0.63 0.38 0.60 0.59 0.29 1.04 0.79 0.69 0.45 0.89 0.27 1.28 0.87 0.47de 0.52 0.51 - 0.54 0.44 0.42 0.70 0.40 0.74 0.45 0.83 0.37 0.28 0.34 0.77 0.90 0.84en 0.53 0.01 0.32 - 0.23 0.25 0.56 0.19 1.11 0.59 0.28 0.27 0.45 0.60 0.89 0.61 0.58es 0.28 0.48 0.45 0.56 - 0.43 0.12 0.26 0.57 0.64 0.56 0.06 0.04 0.24 1.16 1.23 0.68fr 0.70 0.33 0.54 0.66 0.46 - 0.49 0.57 0.24 0.13 0.11 0.43 0.33 0.55 0.91 1.09 0.57id 0.24 0.32 0.36 0.93 0.70 0.65 - 0.35 0.75 0.77 0.11 0.46 0.69 0.57 0.99 0.85 0.47it 0.13 0.55 0.32 0.43 0.47 0.51 0.64 - 0.65 0.42 0.77 0.51 0.51 0.69 0.85 0.98 0.58ja 0.38 0.62 0.60 0.61 0.38 0.73 0.04 0.43 - 0.35 0.05 0.70 0.30 0.38 0.53 0.17 0.02ko 0.49 0.62 0.90 0.40 0.34 0.57 0.47 0.47 0.02 - 0.23 0.52 0.20 0.83 0.70 0.44 0.83ms 0.37 0.57 0.63 0.92 0.81 0.75 0.36 0.54 0.70 1.31 - 0.76 0.35 0.51 1.14 0.70 0.35nl 0.35 0.14 0.54 0.33 0.30 0.46 0.68 0.69 0.77 0.63 0.44 - 0.42 0.67 0.71 1.13 0.55pt 0.46 0.21 0.37 0.21 0.17 0.49 0.47 0.24 0.88 0.45 0.54 0.39 - 0.41 0.94 1.15 0.90ru 0.69 0.63 0.69 0.77 0.26 0.50 0.79 0.52 0.69 0.90 0.66 0.69 0.40 - 1.19 1.23 0.47th 0.90 0.49 0.53 0.77 0.64 0.38 0.21 0.60 0.37 0.96 0.38 0.63 0.68 0.72 - 0.33 0.45vi 0.64 0.61 0.42 1.09 0.84 0.63 0.34 0.70 0.59 0.39 0.16 0.56 0.36 0.50 0.77 - 0.53zh 0.23 0.48 0.96 0.33 0.49 0.32 0.27 0.43 0.43 0.69 0.31 0.97 0.85 0.23 0.40 0.50 -Table 5: Gains in BLEU score from decoding bidirectionally over a forward decoding strategy.
Thenumbers in the cells are the differences in BLEU percentage points between the systems.
Shaded cellsindicate the cases where forward decoding gave a higher score.
Source languages are indicated by thecolumn headers, the row headers denoting the target languages.ally better to decode in a forward direction.
Ourmain contribution has been to show that a bidirec-tional decoding strategy is superior to both mono-directional decoding strategies.
It might be arguedthat the gains arise simply from system combina-tion.
However, our systems are combined in a sim-ple linear fashion, and gains will only arise whenthe second system contributes novel and useful in-formation to into the combination.
Furthermore,our systems are trained on two copies of the samedata, no additional data is required.
The gainsfrom decoding bidirectionally were obtained veryconsistently, with only loose constraints on the de-coding.
This can be seen clearly in Table 5 wherethe results are almost unanimously positive.
More-over, these gains appear to be independent of thelinguistic characteristics of the source and targetlanguages.In the future we would like to explore the pos-sibilities created by more tightly coupling the for-ward and reverse components of the bidirectionaldecoder.
Scores from partial hypotheses of bothprocesses could be combined and used at eachstep of the decoding, making the search more in-formed.
Furthermore, forward partial hypothesesand reverse hypotheses would ?meet?
during de-coding (when one decoding direction has coveredwords in the source that the other has yet to cover),and provide paths for each other to a final state inthe search.AcknowledgmentThis work is partly supported by the Grant-in-Aid for Scientific Research (C) Number 19500137and ?Construction of speech translation founda-tion aiming to overcome the barrier between Asianlanguages?, the Special Coordination Funds forPromoting Science and Technology of the Min-istry of Education, Culture, Sports, Science andTechnology, Japan.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
Me-teor: an automatic metric for mt evaluation with im-proved correlation with human judgments.
In ACL-2005: Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Sum-marization, pages 65?72.P.
Brown, S. Della Pietra, V. Della Pietra, and R.J. Mer-cer.
1993.
The Mathematics of Statistical MachineTranslation: Parameter Estimation.
ComputationalLinguistics, 19(2):263?311.Bernard Comrie and Petra M Vogel, editors.
2000.
Ap-proaches to the Typography of Word Classes.
Mou-ton de Gruyter, Berlin.1131G.
Doddington.
2002.
Automatic Evaluation ofMachine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the HLTConference, San Diego, California.Melvyn J.
Hunt.
1989.
Figures of merit for assess-ing connected-word recognisers.
In In Proceed-ings of the ESCA Tutorial and Research Workshopon Speech Input/Output Assessment and SpeechDatabases, pages 127?131.G.
Kikui, E. Sumita, T. Takezawa, and S. Yamamoto.2003.
Creating corpora for speech-to-speech trans-lation.
In Proceedings of EUROSPEECH-03, pages381?384.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowa, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:open source toolkit for statistical machine transla-tion.
In ACL 2007: proceedings of demo and postersessions, pages 177?180, Prague, Czeck Republic,June.Philipp Koehn.
2004.
Pharaoh: a beam search de-coder for phrase-based statistical machine transla-tion models.
In Machine translation: from realusers to research: 6th conference of AMTA, pages115?124, Washington, DC.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL 2002), pages 295?302.Franz J. Och.
2003.
Minimum error rate training forstatistical machine trainslation.
In Proceedings ofthe ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic eval-uation of machine translation.
In ACL ?02: Proceed-ings of the 40th Annual Meeting on Association forComputational Linguistics, pages 311?318, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Michael Paul.
2006.
Overview of the iwslt 2006 eval-uation campaign.
In Proceedings of the IWLST.Mathew Snover, Bonnie Dorr, Richard Schwartz, JohnMakhoul, Linnea Micciula, and Ralph Weischedel.2005.
A study of translation error rate with tar-geted human annotation.
Technical report, Univer-sity of Maryland, College Park and BBN Technolo-gies, July.C.
Tillmann, S. Vogel, H. Ney, A. Zubiaga, andH.
Sawaf.
1997.
Accelerated dp based searchfor statistical translation.
In In European Conf.on Speech Communication and Technology, pages2667?2670.Taro Watanabe and Eiichiro Sumita.
2002.
Bidirec-tional decoding for statistical machine translation.In Proceedings of the 19th international conferenceon Computational linguistics, pages 1?7, Morris-town, NJ, USA.
Association for Computational Lin-guistics.1132
