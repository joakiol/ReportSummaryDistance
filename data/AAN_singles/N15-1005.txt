Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 42?52,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsRandomized Greedy Inference for Joint Segmentation, POS Tagging andDependency ParsingYuan Zhang, Chengtao Li, Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{yuanzh, ctli, regina}@csail.mit.eduKareem DarwishALT Research GroupQatar Computing Research Institutekdarwish@qf.org.qaAbstractIn this paper, we introduce a new approachfor joint segmentation, POS tagging and de-pendency parsing.
While joint modeling ofthese tasks addresses the issue of error prop-agation inherent in traditional pipeline archi-tectures, it also complicates the inference task.Past research has addressed this challenge byplacing constraints on the scoring function.In contrast, we propose an approach that canhandle arbitrarily complex scoring functions.Specifically, we employ a randomized greedyalgorithm that jointly predicts segmentations,POS tags and dependency trees.
Moreover,this architecture readily handles different seg-mentation tasks, such as morphological seg-mentation for Arabic and word segmentationfor Chinese.
The joint model outperforms thestate-of-the-art systems on three datasets, ob-taining 2.1% TedEval absolute gain againstthe best published results in the 2013 SPMRLshared task.11 IntroductionParsing accuracy is greatly impacted by the qual-ity of preprocessing steps such as tagging and wordsegmentation.
Li et al (2011) report that the dif-ference between using the gold POS tags and us-ing the automatic counterparts reaches about 6% independency accuracy.
Prior research has demon-strated that joint prediction alleviates error propaga-tion inherent in pipeline architectures, where mis-takes cascade from one task to the next (Bohnet et1The source code is available at https://github.com/yuanzh/SegParser.al., 2013; Tratz, 2013; Hatori et al, 2012; Zhanget al, 2014a).
However, jointly modeling all theprocessing tasks inevitably increases inference com-plexity.
Prior work addressed this challenge by in-troducing constraints on scoring functions to keepinference tractable (Qian and Liu, 2012).In this paper, we propose a method for joint pre-diction that imposes no constraints on the scoringfunction.
The method is able to handle high-orderand global features for each individual task (e.g.,parsing), as well as features that capture interactionsbetween tasks.
The algorithm achieves this flexibil-ity by operating over full assignments that specifysegmentation, POS tags and dependency tree, mov-ing from one complete configuration to another.Our approach is based on the randomized greedyalgorithm from our earlier dependency parsing sys-tem (Zhang et al, 2014b).
We extend this algorithmto jointly predict the segmentation and the POS tagsin addition to the dependency parse.
The searchspace for the algorithm is a combination of parsetrees and lattices that encode alternative morpho-logical and POS analyses.
The inference algorithmgreedily searches over this space, iteratively mak-ing local modifications to POS tags and dependencytrees.
To overcome local optima, we employ multi-ple restarts.This simple, yet powerful approach can be eas-ily applied to a range of joint prediction tasks.
Inprior work, joint models have been designed for aspecific language.
For instance, joint models forChinese are designed with word segmentation inmind (Hatori et al, 2012), while algorithms for pro-cessing Semitic languages are tailored for morpho-42logical analysis (Tratz, 2013; Goldberg and Elhadad,2011).
In contrast, we show that our algorithmcan be effortlessly applied to all these distinct lan-guages.
Language-specific characteristics drive thelattice construction and the feature selection, whilethe learning and inference methods are language-agnostic.We evaluate our model on three datasets: SPMRL(Modern Standard Arabic), classical Arabic andCTB5 (Chinese).
Our model consistently outper-forms state-of-the-art systems designed for theselanguages.
We obtain a 2.1% TedEval gain againstthe best published results in the 2013 SPMRL sharedtask (Seddah et al, 2013).
The joint model resultsin significant gains against its pipeline counterpart,yielding 2.4% absolute F-score increase in depen-dency parsing on the same dataset.
Our analysis re-veals that most of this gain comes from the improvedprediction on OOV words.2 Related WorkJoint Segmentation, POS tagging and SyntacticParsing It has been widely recognized that jointprediction is an appealing alternative for pipeline ar-chitectures (Goldberg and Tsarfaty, 2008; Hatori etal., 2012; Habash and Rambow, 2005; Gahbiche-Braham et al, 2012; Zhang and Clark, 2008; Bohnetand Nivre, 2012).
These approaches have been par-ticularly prominent for languages with difficult pre-processing, such as morphologically rich languages(e.g., Arabic and Hebrew) and languages that re-quire word segmentation (e.g., Chinese).
For the for-mer, joint prediction models typically rely on a lat-tice structure to represent alternative morphologicalanalyses (Goldberg and Tsarfaty, 2008; Tratz, 2013;Cohen and Smith, 2007).
For instance, transition-based models intertwine operations on the latticewith operations on a dependency tree.
Other jointarchitectures are more decoupled: in Goldberg andTsarfaty (2008), a lattice is used to derive the bestmorphological analysis for each part-of-speech al-ternative, which is in turn provided to the pars-ing algorithm.
In both cases, tractable inference isachieved by limiting the representation power of thescoring function.
Our model also uses a lattice toencode alternative analyses.
However, we employthis structure in a different way.
The model samplesthe full path from the lattice, which corresponds toa valid segmentation and POS tagging assignment.Then the model improves the path and the corre-sponding tree via a hill-climbing strategy.
This ar-chitecture allows us to incorporate arbitrary featuresfor segmentation, POS tagging and parsing.In joint prediction models for Chinese, latticestructures are not typically used.
Commonly thesemodels are formulated in a transition-based frame-work at the character level (Zhang and Clark, 2008;Zhang et al, 2014a; Wang and Xue, 2014).
Whilethis formulation can handle a large space of possibleword segmentations, it can only capture features thatare instantiated based on the stack and queue status.Our approach offers two advantages over prior work:(1) we can incorporate arbitrary features for wordsegmentation and parsing; (2) we demonstrate thata lattice-based approach commonly used for otherlanguages can be effectively utilized for Chinese.Randomized Greedy Inference Our prior workhas demonstrated that a simple randomized greedyapproach delivers near optimal dependency pars-ing (Zhang et al, 2014b).
Our analysis explainsthis performance with the particular properties of thesearch space in dependency parsing.
We show howto apply this strategy to a more challenging infer-ence task and demonstrate that a randomized greedyalgorithm achieves excellent performance in a sig-nificantly larger search space.3 Randomized Greedy System for JointPredictionIn this section, we introduce our model for joint mor-phological segmentation, tagging and parsing.
Ourdescription will first assume that word boundariesare provided (e.g., the case of Arabic).
Later, wewill describe how this model can be applied to ajoint prediction task that involves word segmenta-tion (e.g., Chinese).3.1 NotationLet x = {xi}|x|i=1be a sentence of length |x| thatconsists of tokens xi.
We use s = {si}|x|i=1to de-note a segmentation of all the tokens in sentence x,and si= {si,j}|si|j=1to denote a segmentation of thetoken xi, where si,jis the jth morpheme of the to-ken xi.
Similarly, we use t, tiand ti,jfor the POS43w/PRTw/CkAn/Vw/C k/PAn/NAn/Cti,12 Ti,1= {C,PRT}ti,22 Ti,2= {V }SiTi= Ti,1?
Ti,21si = w+ kAnxi = wkAnsi,1 = wti,1ti,1 2 Ti,1 = {C,PRT}ti,22 Ti,2= {V }SiTi= Ti,1?
Ti,21ti,12 Ti,1= {C,PRT}ti,22 Ti,2= {V }SiTi= Ti,1?
Ti,21Figure 1: Example lattice structures for the Arabictoken ?wkAn?.
It has two candidate segmentations:w+kAn or w+k+An.
The first segmentation consistsof two morphemes.
The first morpheme w has twocandidate POS.tags for each sentence, token and morpheme.
Weuse y to denote a dependency tree over morphemes,and yi,jto denote the head of morpheme si,j.
Dur-ing training, the algorithm is provided with tuplesthat specify ground truth values for all the variablesD = {(x, s?,?t, y?
)}.We also assume access to a morphological ana-lyzer and a POS tagger that provide candidate anal-yses.
Specifically, for each token xi, the algorithm isprovided with candidate segmentations Si, and can-didate POS tags Tiand Ti,j.
These alternative anal-yses are captured in the lattice structure (see Fig-ure 1 for an example).
Finally, we use Y to denotethe set of all valid dependency trees defined overmorphemes.3.2 DecodingWe parameterize the scoring function asscore(x, s, t, y) = ?
?
f(x, s, t, y) (1)where ?
is the parameter vector and f(x, s, t, y) isthe feature vector associated with the sentence andall variables.The goal of decoding is to find a set of valid val-ues for (s, t, y) ?
S ?
T ?
Y that maximizes thescore defined in Eq.
1.
Our randomized greedy al-gorithm finds a high scoring assignment for (s, t, y)via a hill-climbing process with multiple randomrestarts.
(Section 3.3 describes how the parameters?
are learned.
)Figure 2 shows the framework of our random-ized greedy algorithm.
First, we draw a full pathfrom the lattice structure in two steps: (1) samplinga morphological segmentation s from S; (2) sam-pling POS tags t for each morpheme.
Next, wesample a dependency tree y from the parse space.Based on this random starting point, we iterativelyhill-climb t and y in a bottom-up order.2In ourearlier work (Zhang et al, 2014b), we showed thisstrategy guarantees that we can climb to any targettree in a finite number of steps.
We repeat the sam-pling and the hill-climbing processes above until wedo not find a better solution for K iterations.
Weintroduce the details of this process below.SampleSeg and SamplePOS: Given a sentencex, we first draw segmentations s and POS tags t(0)from the first-order distribution using the currentlearned parameter values.
For segmentation, first-order features only depend on each token xiand itsmorphemes si,j.
Similarly, for POS, first-order fea-tures are defined based on si,jand ti,j.
The sam-pling process is straightforward due to the fact thatthe candidate sets |Si| and |Ti,j| are both small.
Wecan enumerate and compute the probabilities propor-tional to the exponential of the first-order scores asfollows.3p(si) ?
exp{?
?
f(x, si)}p(ti,j) ?
exp{?
?
f(x, si, ti,j)}(2)SampleTree: Given a random sample of the seg-mentations s and the POS tags t(0), we draw a ran-dom tree y(0)from the first-order distribution usingWilson?s algorithm (Wilson, 1996).4HillClimbPOS: After sampling the initial valuess, t(0)and y(0), the hill-climbing algorithm improvesthe solution via locally greedy changes.
The hill-climbing algorithm iterates between improving thePOS tags and the dependency tree.
For POS tagging,it updates each ti,jin a bottom-up order as followsti,j?
argmaxti,j?Ti,jscore(x, s, ti,j, t?
(i,j), y) (3)where t?
(i,j)are the rest of the POS tags when weupdate ti,j.2We do not hill-climb segmentation, or else we have tojointly find the optimal t and y, and the resulting computationalcost is too high.3We notice that the distribution becomes significantlysharper after training for several epochs.
Therefore, we alsosmooth the distribution by multiplying the score with a scalingfactor.4We also smooth the distribution in the same way as in seg-mentation and POS tagging.44Input: parameter ?, sentence xOutput: segmentations s, POS tags t and depen-dency tree y1: s?
SampleSeg(x)2: t(0)?
SamplePos(x, s)3: y(0)?
SampleTree(x, s, t(0))4: k = 05: repeat6: t(k+1)?
HillClimbPOS(x, s, t(k), y(k))7: y(k+1)?
HillClimbTree(x, s, t(k+1), y(k))8: k ?
k + 19: until no change in this iteration10: return (s, t(k), y(k))Figure 2: The hill-climbing algorithm with randominitializations.
Details of the sampling and hill-climbing functions in Line 1-3 and 6-7 are providedin Section 3.2.HillClimbTree: We improve the dependency treey via a similar hill-climbing process.
Specifically,we greedily update the head yi,jof each morphemein a bottom-up order as followsyi,j?
argmaxyi,j?Yi,jscore(x, s, t, yi,j, y?
(i,j)) (4)where Yi,jis the set of candidate heads such thatchanging yi,jto any candidate does not violate thetree constraint.3.3 TrainingWe learn the parameters ?
in a max-margin frame-work, using on-line updates.
For each update, weneed to compute the segmentations, POS tags andthe tree that maximize the cost-augmented score:(s?,?t, y?)
= argmaxs?S,t?T ,y?Y{?
?f(x, s, t, y)+Err(s, t, y)}(5)whereErr(s, t, y) is the number of errors of (s, t, y)against the ground truth (s?,?t, y?).
The parameters arethen updated to guide the selection against the vio-lation.
This is done via standard passive-aggressiveupdates (Crammer et al, 2006).3.4 Adapting to Chinese Joint PredictionIn this section we describe how the proposed modelcan be adapted to languages that do not delineate!Xinhua News Agency!Xinhua !society	!February  13th!February 	!13th!Beijing !reportFigure 3: Example lattice structures for the Chi-nese sentence ?????????????
(Xin-hua Press at Beijing reports on February 13th).
Thetoken ???
has two candidate segmentations: ???
or??
+ ?.words with spaces, and thus require word segmen-tation.
The main difference lies in the constructionof the lattice structure.
We employ a state-of-the-artword segmenter to produce candidate word bound-aries.
We consider boundaries common across allthe top-k candidates as true word boundaries.
Theremaining tokens (i.e., strings between these bound-aries) are treated as words to be further segmentedand labeled with POS tags.
Figure 3 shows an ex-ample of the Chinese word lattice structure we con-struct.
Once the lattice is constructed, the joint pre-diction model is applied as described above.4 FeaturesSegmentation Features For both Arabic and Chi-nese, each segmentation is represented by its scorefrom the preprocessing system, and by the corre-sponding morphemes (or words in Chinese).
Fol-lowing previous work (Zhang and Clark, 2010), wealso add character-based features for Chinese wordsegmentation, including the first and the last charac-ters in the word, and the length of the word.POS Tag Features Table 1 summarizes the POStag features employed by the model.
First, weuse the feature templates proposed in our previ-ous work on Arabic joint parsing and POS correc-tion (Zhang et al, 2014c).
In addition, we incor-porate character-based features specifically designedfor Chinese.
These features are mainly inspired byprevious transition-based models on Chinese jointPOS tagging and word segmentation (Zhang andClark, 2010).Dependency Parsing Features The feature tem-plates for dependency parsing are mainly drawnfrom our previous work (Zhang et al, 2014b).
Fig-451-gram?t0, w?2?, ?t0, w?1?, ?t0, w0?, ?t0, w1?, ?t0, w2?,?t0, w?1, w0?, ?t0, w0, w1?, ?s(t0)?, ?t0, s(t0)?2-gram ?t?1, t0?, ?t?2, t0?, ?t?1, t0, w?1?, ?t?1, t0, w0?3-gram?t?1, t0, t1?, ?t?2, t0, t1, ?, ?t?1, t0, t2?,?t?2, t0, t2?4-gram?t?2, t?1, t0, t+1?, ?t?2, t?1, t0, t2?,?t?2, t0, t1, t2?5-gram ?t?2, t?1, t0, t1, t2?Character?t0, pre1(w0)?, ?t0, pre2(w0)?, ?t0, suf1(w0)?,?t0, suf2(w0)?, ?t0, cn(w0)?, ?t0, len(w0)?Table 1: POS tag feature templates.
t0and w0de-notes the POS tag and the word at the current posi-tion.
t?xand txdenote left and right context tags,and similarly for words.
s(?)
denotes the score ofthe POS tag produced by the preprocessing tagger.The last row shows the ?Character?-based featuresfor Chinese.
pre1(?)
and pre2(?)
denote the wordprefixes with one and two characters respectively.suf1(?)
and suf2(?)
denote the word suffixes simi-larly.
cn(?)
denotes the n-th character in the word.len(?)
denotes the length of the word, capped at 5 iflonger.arc!h m consecutive sibling!h m s grandparent!g h mgrand-sibling!g h m stri-siblings!h m s tFigure 4: First- to third-order dependency parsingfeatures.ure 4 shows the first- to third-order feature templatesthat we use in our model.
We also use global fea-tures to capture the adjacent conjuncts agreement ina coordination structure, and the valency patterns foreach POS category.
Note that most dependency fea-tures are implicitly cross-task in that they includePOS tag and segmentation information.
For exam-ple, the standard feature involves the POS tags of thewords on both ends of the arc.5 Experimental Setup5.1 DatasetsWe evaluate our model on two Arabic datasets andone Chinese dataset.
For the first Arabic dataset,we use the dataset used in the Statistical Parsing ofDataset SPMRL Classical CTB5Language Arabic Arabic ChineseTrain#sent 14.4k 15.4k 17.5k#token 451k 573k 442kDev.#sent 1.8k ?
348#token 56.9k ?
6.6kTest.#sent 1.8k 163 348#token 55.6k 7.9k 8.0kTable 2: Statistics of datasets.Morphologically Rich Languages (SPMRL) SharedTask 2013 (Seddah et al, 2013).5We follow theofficial split for training, development and testingset.
We use the core set of 12 POS categories pro-vided by Marton et al (2013).
In the second Ara-bic dataset, the training set is a dependency con-version of the Arabic Treebank, which primarily in-cludes Modern Standard Arabic (MSA) text.
How-ever, we test on a new corpus, which consists ofclassical Arabic text obtained from the Comprehen-sive Islamic Library (CIS).6A native Arabic speakerwith background in computational linguistics anno-tated the morphological segmentation and POS tags.This corpus is an excellent testbed for a joint modelbecause classical Arabic may use rather different vo-cabulary from MSA, while their syntactic grammarsare very similar to each other.
Therefore incorporat-ing syntactic information should be particularly ben-eficial to morphological segmentation and POS tag-ging.
For Chinese, we use the Chinese Penn Tree-bank 5.0 (CTB5) and follow the split in previouswork (Zhang and Clark, 2010).Table 2 summarizes the statistics of the datasets.For the SPMRL test set, we follow the commonpractice which limits the sentence lengths up to70 (Seddah et al, 2013).
For classical Arabic andChinese, we evaluate on all the test sentences.5.2 Generating Lattice StructuresIn this section we introduce the methodology forconstructing candidate sets for segmentation and5This dataset is originally provided by the LDC (Maamouriet al, 2004), specifically its SPMRL 2013 dependency in-stance, derived from the Columbia Catib Treebank (Habash andRoth, 2009; Habash et al, 2009) and extended according to theSPMRL 2013 extension scheme (Seddah et al, 2013).6This classical Arabic dataset is publicly available at http://farasa.qcri.org/46MADA analysisWord EmlypEmly/NOUN+p/NSUFF, gen:f/num:s/per:naEmly/ADJ+p/NSUFF, gen:f/num:s/per:naEml/NOUN+y/NSUFF+p/PRON, gen:m/num:d/per:naLattice structureEmly/NOUNEmly/ADJp/NSUFFgen:f/num:s/per:naEml/NOUN y/NSUFF p/PRONgen:m/num:d/per:naFigure 5: Example MADA analysis for the wordEmlyp and the corresponding lattice structure.POS tagging.
Table 3 provides statistics on the gen-erated candidate sets.SPMRL 2013 Following Marton et al (2013), weuse the MADA system to generate candidate mor-phological analyses and POS tags.
For each tokenin the sentence, MADA provides a list of possiblemorphological analyses and POS tags, each associ-ated with a score.
The score of each segmentation orPOS tag equals the highest score of the MADA anal-ysis in which it appears.
In addition, we associateeach segmentation with MADA analyses on gender,number and person.
Figure 5 shows an example ofMADA output for the token Emlyp and the corre-sponding lattice structure.Classical Arabic We construct the lattice for thiscorpus in a similar fashion to the SPMRL datasetwith two main departures.
First, we use the Ara-bic morphological analyzer developed by Darwishet al (2014) because MADA is primarily trained forMSA and performs poorly on classical Arabic.
Sec-ond, we implement a CRF-based morpheme-levelPOS tagger and generate the POS tag candidates foreach morpheme based on their marginal probabili-ties, truncated by a probability threshold.CTB5 We first re-train the Stanford Chinese wordsegmenter on CTB5 and generate a top-10 list foreach sentence.7We treat the word boundaries sharedacross all the 10 candidates as the confident ones,7We use 10-fold cross validation to avoid overfitting on thetraining set.DatasetSeg POSF1 Oracle Avg.
|Si| F1 Avg.
|Ti,j|SPMRL 99.4 99.8 1.23 96.9 1.71Classical 92.4 97.0 1.16 82.4 3.01CTB5 95.3 99.0 1.22 91.4 2.02Table 3: Quality of the lattice structures on eachdataset.
For SPMRL and CTB5, we show the statis-tics on the development sets.
For classical Arabic,we directly show the statistics on the testing set be-cause the development set is not available.and construct the lattice as described in Section 3.4.Our model then focuses on disambiguating the restof the word boundaries in the candidates.
To gen-erate POS candidates, we apply a CRF-based tag-ger with Chinese-specific features used in previouswork (Hatori et al, 2011).5.3 Evaluation MeasuresFollowing standard practice in previous work (Ha-tori et al, 2012; Zhang et al, 2014a), we use F-score as the evaluation metric for segmentation, POStagging and dependency parsing.
We report themorpheme-level F-score for Arabic and the word-level F-score for Chinese.
In addition, we use TedE-val (Tsarfaty et al, 2012) to evaluate the joint pre-diction on the SPMRL dataset, because TedEvalscore is the only evaluation metric used in the of-ficial report.
We directly use the evaluation toolsprovided on the SPMRL official website.85.4 BaselinesState-of-the-Art Systems For the SPMRLdataset, we directly compare with Bj?orkelund et al(2013).
This system achieves the best TedEvalscore in the track of dependency parsing withpredicted information and we directly republishthe official result.
We also compute the F-score ofthis system on each task using our own evaluationscript.9For the CTB5 dataset, we directly compareto the arc-eager system by Zhang et al (2014a),which slightly outperforms the arc-standard systemby Hatori et al (2012).8http://www.spmrl.org/spmrl2013-sharedtask.html9F-score evaluation for Arabic is not straightforward due tothe stem changes in the morphological analysis.
Therefore, thecomparison of F-scores is only approximate.47ModelSPMRL Classical Arabic CTB5Seg POS Dep TedEval Seg POS Seg POS DepPipeline 99.18 95.76 84.79 92.86 92.37 82.40 97.45 93.42 79.46Joint 99.52 97.43 87.23 93.87 94.35 84.44 98.04 94.47 82.01Best Published 96.42 91.66 82.41 91.74 ?
?
97.76 94.36 81.70Table 4: Segmentation, POS tagging and unlabeled attachment dependency F-scores (%) and TedEval score(%) on different datasets.
The first line denotes the performance by the pipeline variation of our model.The second row shows the results by our joint model.
?Best Published?
includes the best reported re-sults: Bj?orkelund et al (2013) for the SPMRL 2013 shared task and Zhang et al (2014a) for the CTB5dataset.
Note that the POS F-scores are not directly comparable because Bj?orkelund et al (2013) use adifferent POS tagset from us.Seg POS Dep00.511.522.533.54SeenOOV(a) SPMRLSeg POS0246810SeenOOV(b) Classical ArabicSeg POS Dep012345678SeenOOV(c) CTB5Figure 6: Absolute F-score (%) improvement of the joint model over the pipeline counterpart on seen andout-of-vocabulary (OOV) words.System Variants We also compare against apipeline variation of our model.
In our pipelinemodel, we predict segmentations and POS tags bythe same system that we use to generate candidates.The subsequent standard parsing model then oper-ates on the predicted segmentations and POS tags.5.5 Experimental DetailsFollowing our earlier work (Zhang et al, 2014b), wetrain a first-order classifier to prune the dependencytree space.10Following common practice, we aver-age parameters over all iterations after training withpassive-aggressive online learning algorithm (Cram-mer et al, 2006; Collins, 2002).
We use the sameadaptive random restart strategy as in our earlierwork (Zhang et al, 2014b) and set K = 300.
In ad-dition, we also apply an aggressive early-stop strat-egy during training for efficiency.
If we have founda violation against the ground truth during the first50 iterations, we immediately stop and update the10We set the probability threshold to 0.05 and limit the num-ber of candidate heads up to 20, which gives a 99.5% pruningrecall on both the SPMRL and the CTB5 development sets.parameters based on the current violation.
The rea-soning behind this early-stop strategy is that weakerviolations for some training sentences are alreadysufficient for separable training sets (Huang et al,2012).6 ResultsComparison to State-of-the-art Systems Table 4summarizes the performance of our model and thebest published results for the SPMRL and the CTB5datasets.11On both datasets, our system outper-forms the baselines.
On the SPMRL 2013 sharedtask, our approach yields a 2.1% TedEval score gainover the top performing system (Bj?orkelund et al,2013).
We also improve the segmentation and de-pendency F-scores by 3.1% and 4.8% respectively.Note that the POS F-scores are not directly com-parable because Bj?orkelund et al (2013) use a dif-ferent POS tagset from us.
On the CTB5 dataset,we outperform the state-of-the-art with respect to all11We are not aware of any published results on the ClassicalArabic Dataset.480 5 10 15 20859095100# MADA AnalysisScore (%)SegPosDepTedEvalFigure 7: Performance with different sizes of thecandidate sets on the SPMRL dataset.
The graphshows the TedEval and F-scores when consideringthe best k analyses by MADA, and the variation isachieved by changing k.# Restarts0 200 400 600 800 1000Score0.920.940.960.981Figure 8: The normalized score of the output tree asthe function of the number of restarts.
We normalizescores of each sentence by the highest score among3,000 restarts for this sentence.
We show the curveup to 1,000 restarts because it reaches convergenceafter 500 restarts.tasks: segmentation (0.3%), tagging (0.1%), and de-pendency parsing (0.3%).12Impact of the Joint Prediction As Table 4 shows,our joint prediction model consistently outperformsthe corresponding pipeline model in all three tasks.This observation is consistent with findings in pre-vious work (Hatori et al, 2012; Tratz, 2013).
Wealso observe that gains are higher (2%) on the clas-sical Arabic dataset, which demonstrates that jointprediction is particularly helpful in bridging the gapbetween MSA and classical Arabic.12Zhang et al (2014a) improve the dependency F-score to82.14% by adding manually annotated intra-word dependencyinformation.
Even without such gold word structure annota-tions, our model still achieves a comparable result.DatasetSeg POS DepSeen OOV Seen OOV Seen OOVSPMRL 48.4 27.8 44.7 15.0 15.9 17.5Classical 13.8 34.8 4.2 17.2 ?
?CTB5 20.3 25.7 14.2 19.9 13.0 15.6Table 5: F-score error reductions (%) of the jointmodel over the pipeline counterpart on seen andOOV words.Figure 6 shows the break of the improvementbased on seen and out-of-vocabulary (OOV) words.As expected, across all languages OOV words bene-fit more from the joint prediction, as they constitutea common source of error propagation in a pipelinemodel.
The extent of improvement depends on theunderlying accuracy of the preprocessing for seg-mentation and POS tagging on OOV words.
Forinstance, we observe a higher gain (7%) on ChineseOOV words which have a 61.5% accuracy when pro-cessed by the original stand-along POS tagger.
Onthe SPMRL dataset, the gain on OOV words is lower(3%), while preprocessing accuracy is higher (82%).Their error reductions on OOV words are neverthe-less close to each other.
Table 5 summarizes the re-sults on F-score error reduction.We also observe that the error reductions of OOVwords/morphemes on the Chinese and the Classi-cal Arabic dataset are larger than that of the in-vocabulary counterparts (e.g.
26% vs. 20% on Chi-nese word segmentation).
However, we have the op-posite observation on the segmentation and POS tag-ging on the SPMRL dataset (28% vs. 48%).
Thiscan be explained by analyzing the oracle perfor-mance in which we select the best solution from pos-sible candidates.
The oracle error reduction of OOVmorphemes in the SPMRL dataset is relatively low(44%), compared to the 61% oracle error reductionof OOV morphemes in the Classical Arabic dataset.Impact of the Number of Alternative AnalysesIn Figure 7, we plot the performance on the SPMRLdataset as a function of the number k of MADAanalyses that we use to construct the candidate sets.For low k, increasing the number of analyses im-proves performance across all evaluation metrics.However, the performance converges at around k =15.49Scoremax-Scorelocal0 10 20 30 40 50%LocalOptima020406080100Figure 9: Cumulative distribution function (CDF)for the number of local optima versus the score ofthese local optima obtained from each restart, on theSPMRL dataset.
The score captures the differencebetween a local optimum and the best one among3,000 restarts.Convergence Properties To assess the qualityof the approximation obtained by the randomizedgreedy inference, we would like to compare itagainst the optimal solution.
Following our earlierwork (Zhang et al, 2014b), we use the highest scoreamong 3,000 restarts for each sentence as a proxy forthe optimal solution.
Figure 8 shows the normalizedscore of the retrieved solution as a function of thenumber of restarts.
We observe that most sentencesconverge quickly.13Specifically, more than 97%of the sentences converge within first 300 restarts.Since for the vast majority of cases our system con-verges fast, we achieve a comparable speed to thatof other state-of-the-art joint systems.
For example,our model achieves high performance on Chinese atabout 0.5 sentences per second.
The speed is aboutthe same as that of the transition-based system (Ha-tori et al, 2012) with beam size 64, the setting thatachieved best accuracy in their work.Quality of Local Optima Figure 9 shows the cu-mulative distribution function (CDF) for the num-ber of local optima versus the score of these localoptima obtained from each restart.
More specifi-cally, the score captures the difference between a lo-cal optimum and the maximal score among 3,000restarts.
We can see that most of the local op-tima reached by hill-climbing have scores close to13As expected, we also observe that convergence is slowerwhen comparing to standard dependency parsing with a simi-lar randomized greedy algorithm (Zhang et al, 2014b), becausejoint prediction results in a harder inference problem.the maximum.
For instance, about 30% of the lo-cal optima are identical to the best solution, namelyscoremax?
scorelocal= 0.7 ConclusionsIn this paper, we propose a general randomizedgreedy algorithm for joint segmentation, POS tag-ging and dependency parsing.
On both Arabic andChinese, our model achieves improvement on thethree tasks over state-of-the-art systems and pipelinevariants of our system.
In particular, we demonstratethat OOV words benefits more from the power ofjoint prediction.
Finally, our experimental resultsshow that increasing candidate sizes improves per-formance across all evaluation metrics.AcknowledgmentsThis research is developed in a collaborationof MIT with the Arabic Language Technologies(ALT) group at Qatar Computing Research Institute(QCRI) within the Interactive sYstems for AnswerSearch (IYAS) project.
The authors acknowledgethe support of the U.S. Army Research Office un-der grant number W911NF-10-1-0533, and of theDARPA BOLT program.
We thank Meishan Zhangand Anders Bj?orkelund for answering questions andsharing the outputs of their systems.
We also thankthe MIT NLP group and the ACL reviewers for theircomments.
Any opinions, findings, conclusions, orrecommendations expressed in this paper are thoseof the authors, and do not necessarily reflect theviews of the funding organizations.ReferencesAnders Bj?orkelund, Ozlem Cetinoglu, Rich?ard Farkas,Thomas Mueller, and Wolfgang Seeker.
2013.
(re)ranking meets morphosyntax: State-of-the-art re-sults from the SPMRL 2013 shared task.
In Pro-ceedings of the Fourth Workshop on Statistical Pars-ing of Morphologically-Rich Languages, pages 135?145, Seattle, Washington, USA, October.
Associationfor Computational Linguistics.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Proceed-ings of the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-50tional Natural Language Learning, pages 1455?1465.Association for Computational Linguistics.Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Rich?ardFarkas, Filip Ginter, and Jan Hajic.
2013.
Joint mor-phological and syntactic analysis for richly inflectedlanguages.
TACL, 1:415?428.Shay B Cohen and Noah A Smith.
2007.
Joint morpho-logical and syntactic disambiguation.
In Proceedingsof EMNLP.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing - Volume 10, EMNLP ?02.
Associa-tion for Computational Linguistics.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
The Journal of Machine Learn-ing Research.Kareem Darwish, Ahmed Abdelali, and HamdyMubarak.
2014.
Using stem-templates to improvearabic pos and gender/number tagging.
In Inter-national Conference on Language Resources andEvaluation (LREC-2014).Souhir Gahbiche-Braham, H?elene Bonneau-Maynard,Thomas Lavergne, and Franc?ois Yvon.
2012.
Jointsegmentation and pos tagging for arabic using a crf-based classifier.
In LREC, pages 2107?2113.Yoav Goldberg and Michael Elhadad.
2011.
Joint he-brew segmentation and parsing using a pcfg-la latticeparser.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies: short papers-Volume 2,pages 704?709.
Association for Computational Lin-guistics.Yoav Goldberg and Reut Tsarfaty.
2008.
A single gener-ative model for joint morphological segmentation andsyntactic parsing.
In ACL, pages 371?379.
Citeseer.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics, pages 573?580.
Association forComputational Linguistics.Nizar Habash and Ryan Roth.
2009.
Catib: Thecolumbia arabic treebank.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221?224, Suntec, Singapore, August.
Association for Com-putational Linguistics.Nizar Habash, Reem Faraj, and Ryan Roth.
2009.
Syn-tactic Annotation in the Columbia Arabic Treebank.
InProceedings of MEDAR International Conference onArabic Language Resources and Tools, Cairo, Egypt.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2011.
Incremental joint pos taggingand dependency parsing in chinese.
In IJCNLP, pages1216?1224.
Citeseer.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2012.
Incremental joint approach toword segmentation, pos tagging, and dependency pars-ing in chinese.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 1045?1053.
Asso-ciation for Computational Linguistics.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 142?151.Association for Computational Linguistics.Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-liang Chen, and Haizhou Li.
2011.
Joint models forchinese pos tagging and dependency parsing.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 1180?1191.
Association for Computational Linguistics, July.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank:Building a Large-Scale Annotated Arabic Corpus.
InNEMLAR Conference on Arabic Language Resourcesand Tools.Yuval Marton, Nizar Habash, Owen Rambow, and SarahAlkhulani.
2013.
Spmrl?13 shared task system:The cadim arabic dependency parser.
In Proceed-ings of the Fourth Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 76?80.Xian Qian and Yang Liu.
2012.
Joint chinese word seg-mentation, pos tagging and parsing.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 501?511.
Associa-tion for Computational Linguistics.Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie Can-dito, Jinho D. Choi, Rich?ard Farkas, Jennifer Fos-ter, Iakes Goenaga, Koldo Gojenola Galletebeitia,Yoav Goldberg, Spence Green, Nizar Habash, MarcoKuhlmann, Wolfgang Maier, Joakim Nivre, AdamPrzepi?orkowski, Ryan Roth, Wolfgang Seeker, Yan-nick Versley, Veronika Vincze, Marcin Woli?nski, AlinaWr?oblewska, and Eric Villemonte de la Clergerie.2013.
Overview of the SPMRL 2013 shared task:A cross-framework evaluation of parsing morpholog-ically rich languages.
In Proceedings of the FourthWorkshop on Statistical Parsing of Morphologically-Rich Languages, pages 146?182, Seattle, Washington,USA, October.
Association for Computational Lin-guistics.51Stephen Tratz.
2013.
A cross-task flexible transitionmodel for arabic tokenization, affix detection, affixlabeling, pos tagging, and dependency parsing.
InFourth Workshop on Statistical Parsing of Morpholog-ically Rich Languages, page 34.
Citeseer.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2012.
Joint evaluation of morphological segmenta-tion and syntactic parsing.
In Proceedings of the 50thAnnual Meeting of the Association for ComputationalLinguistics: Short Papers-Volume 2, pages 6?10.
As-sociation for Computational Linguistics.Zhiguo Wang and Nianwen Xue.
2014.
Joint pos tag-ging and transition-based constituent parsing in chi-nese with non-local features.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages733?742, Baltimore, Maryland, June.
Association forComputational Linguistics.David Wilson.
1996.
Generating random spanning treesmore quickly than the cover time.
In Proceedings ofthe twenty-eighth annual ACM symposium on Theoryof computing, pages 296?303.
ACM.Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and pos tagging using a single perceptron.In ACL, pages 888?896.Yue Zhang and Stephen Clark.
2010.
A fast decoderfor joint word segmentation and pos-tagging using asingle discriminative model.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 843?852.
Association forComputational Linguistics.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2014a.
Character-level chinese dependency pars-ing.
In ACL.Yuan Zhang, Tao Lei, Regina Barzilay, and TommiJaakkola.
2014b.
Greed is good if randomized: Newinference for dependency parsing.
In EMNLP.Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola,and Amir Globerson.
2014c.
Steps to excellence:Simple inference with refined scoring of dependencytrees.
In ACL.52
