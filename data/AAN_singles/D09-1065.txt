Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 619?627,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPEEG responds to conceptual stimuli and corpus semanticsBrian MurphyCIMeC, University of TrentoRovereto 38068, Italybrian.murphy@unitn.itMarco BaroniCIMeC, University of TrentoRovereto 38068, Italymarco.baroni@unitn.itMassimo PoesioCIMeC, University of TrentoRovereto 38068, Italymassimo.poesio@unitn.itAbstractMitchell et al (2008) demonstrated thatcorpus-extracted models of semanticknowledge can predict neural activationpatterns recorded using fMRI.
Thiscould be a very powerful technique forevaluating conceptual models extractedfrom corpora; however, fMRI is expensiveand imposes strong constraints on datacollection.
Following on experimentsthat demonstrated that EEG activationpatterns encode enough information todiscriminate broad conceptual categories,we show that corpus-based semantic rep-resentations can predict EEG activationpatterns with significant accuracy, andwe evaluate the relative performance ofdifferent corpus-models on this task.1 IntroductionModels of semantic relatedness induced from cor-pus data have proven effective in a number of em-pirical tasks (Sahlgren, 2006) and there is increas-ing interest in whether distributional informationextracted from corpora correlates with aspectsof speakers?
semantic knowledge: see Lund andBurgess (1996), Landauer and Dumais (1997), Al-muhareb (2006), Pad?o and Lapata (2007), Schulteim Walde (2008), among many others.
For thispurpose, corpus models have been tested on data-sets that are based on semantic judgements (met-alinguistic or meta-cognitive intuitions about syn-onymy, semantic distance, category-membership)or behavioural experiments (semantic priming,property generation, free association).
While allthese data are valuable, they are indirect reflec-tions of semantic knowledge, and when the pre-dictions they make diverge from those of corpora,interpretation is problematic: is the corpus modelmissing essential aspects of semantics, or are non-semantic factors biasing the data elicited from in-formants?Reading semantic processes and representationsdirectly from the brain would be an ideal way toget around these limitations.
Until recently, anal-ysis of linguistic quantities using neural data col-lected with EEG (measurement at the scalp of volt-ages induced by neuronal firing) or fMRI (mea-surement of changes of oxygen concentrations inthe brain tied to cognitive processes) had neitherthe advantages of corpora (scale) nor of infor-mants (finer grained judgements).However, some clear patterns of differential ac-tivity have been found for broad semantic classes.Viewing images of natural (typically animals andplants) and non-natural (typically artefacts liketools or vehicles) objects elicits different loci ofactivity in fMRI (Martin and Chao, 2001) andEEG (Kiefer, 2001), that persist across partici-pants.
Differences have also been found in re-sponse to auditorily or visually presented words ofdifferent lexical classes, such as abstract/concrete,and verb/noun (Pulverm?uller, 2002).
But interpre-tation of such group results remains somewhat dif-ficult, as they may be consistent with more thanone distinction: the natural/artefactual divisionjust mentioned, may rather be between living/non-living entities, dynamic/static entities, or be basedon embodied experience (e.g.
manipulable or not).More recently, however, machine learning andother numerical techniques have been successfullyapplied to extract semantic information from neu-ral data in a more discriminative fashion, downto the level of individual concepts.
The workpresented here builds on two strands of previ-ous work: Murphy et al (2008) use EEG datato perform semantic categorisation on single stim-uli; and Mitchell et al (2008) introduce an fMRI-based method that detects word level distinctionsby learning associations between features of neu-ral activity and semantic features derived from a619corpus.
We combine these innovations by intro-ducing a method that extracts featural represen-tations from the EEG signal, and uses corpus-based models to predict word level distinctions inpatterns of EEG activity.
The proposed methodachieves a performance level significantly abovechance (also when distinguishing between con-cepts from the same semantic category, e.g., dogand cat), and approaching that achieved withfMRI.The paper proceeds as follows.
The next sectiondescribes a simple behavioural experiment whereItalian-speaking participants had to name photo-graphic images of mammals and tools while theirEEG activity was being recorded, and continuesto detail how the rich and multidimensional sig-nals collected were reduced to a small set of op-timally informative features using a new method.Section 3 describes a series of corpus-based se-mantic models derived from both a raw-text webcorpus, and from various parsings of a conven-tional corpus.
In Section 4 we describe the train-ing of a series of linear models, that each learnthe associations between a set of corpus semanticfeatures and an individual EEG activity feature.By combining these models it is possible to pre-dict the EEG activity pattern for a single unseenword, and compare this to the observed patternfor the corresponding concept.
Results (Section5) show that these predictions succeed at a levelsignificantly above chance, both for coarser dis-tinctions between words in different superordinatecategories (e.g., differentiating between drill andgorilla), and, at least for the model based on thelarger web corpus, for those within the same cate-gory (e.g., drill vs spanner, koala vs gorilla).2 Neural Activation Data2.1 Data collectionEEG data was gathered from native speakers ofItalian during a simple behavioural experiment atthe CIMeC/DiSCoF laboratories at Trento Univer-sity.
Seven participants (five male and two fe-male; age range 25-33; all with college educa-tion) performed a silent naming task.
Each of themwas presented1on screen with a series of contrast-normalised greyscale photographs of tools (gar-den and work tools) and land mammals (exclud-ing emotionally valent domesticated animals and1Using the E-Prime software package: http://www.pstnet.com/e-prime/.~1.5s0.5s 0.5s 2sFigure 1: Presentation of image stimulipredators), for which they had to think of the mostappropriate name (see figure 1).
They were not ex-plicitly asked to group the entities into superordi-nate categories, or to concentrate on their seman-tic properties, but completing the task involved re-solving each picture to its corresponding concept.Images remained on screen until a keyboard re-sponse was received from the participant to indi-cate a suitable label had been found, and presenta-tions were interleaved with three second rest peri-ods.
Thirty stimuli in each of the two classes wereeach presented six times, in random order, to givea total of 360 image presentations in the session.Response rates were over 95%, and a post-sessionquestionnaire determined that participants agreedon image labels in approximately 90% of cases.English terms for the concepts used are listed be-low.Mammals anteater, armadillo, badger, beaver, bi-son, boar, camel, chamois, chimpanzee, deer,elephant, fox, giraffe, gorilla, hare, hedge-hog, hippopotamus, ibex, kangaroo, koala,llama, mole, monkey, mouse, otter, panda,rhinoceros, skunk, squirrel, zebraTools Allen key, axe, chainsaw, craft knife, crow-bar, file, garden fork, garden trowel, hack-saw, hammer, mallet, nail, paint brush, paintroller, pen knife, pick axe, plaster trowel,pliers, plunger, pneumatic drill, power drill,rake, saw, scissors, scraper, screw, screw-driver, sickle, spanner, tape measureThe EEG signals were recorded at 500Hz from64 scalp locations based on the 10-20 standard620montage.2The EEG recording computer and stim-ulus presentation computer were synchronised bymeans of parallel port transmitted triggers.
Af-ter the experiment, pre-processing of the recordedsignals was carried out using the EEGLAB pack-age (Delorme and Makeig, 2003): signals wereband-pass filtered at 1-50Hz to remove slow driftsand high-frequency noise, and then down-sampledto 120Hz.
An ICA decomposition was subse-quently applied (Makeig et al, 1996), and signalcomponents due to eye-movements were manuallyidentified and removed.As a preliminary test to verify that the recordedsignals included category specific patterns, weapplied a discriminative classification techniquebased on source-separation, similar to that de-scribed in Murphy et al (2008).
This found thatthe categories of mammals and tools could be dis-tinguished with an accuracy ranging from 57% to80% (mean of 72% over the seven participants).2.2 Feature extractionThe features extracted are metrics of signal powerat a particular scalp location, in a particular fre-quency band, and at a particular time latency rel-ative to the presentation of each image stimulus.Termed Event Related Synchronisation (ERS) orEvent Related Spectral Perturbation (ERSP), suchfrequency-specific changes in signal amplitude areknown to correlate with a wide range of cogni-tive functions (Pfurtscheller and Lopes da Silva,1999), and have specifically been shown to be sen-sitive to category distinctions during the process-ing of linguistic and visual stimuli (Murphy et al,2008; Gilbert et al, 2009).Feature extraction and selection is performedindividually on a per-participant basis.
As a firststep all signal channels are z-score normalisedto control for varying conductivity at each elec-trode site, and a Laplacian sharpening is appliedto counteract the spatial blurring of signals causedby the skull, and so minimise redundancy of infor-mation between channels.For each stimulus presentation, 14,400 signalpower features are extracted: 64 electrode chan-nels by 15 frequency bands (of width 3.3Hz, be-tween 1 and 50Hz) by 15 time intervals (of length67ms, in the first second after image presentation).A z-score normalisation is carried out across all2Using a Brain Vision BrainAmp system: http://www.brainproducts.com/.Figure 2: Mean rank of selected features in thetime/frequency space (left panel) and on the scalp(right panel) for participant Estimulus presentations to equalise variance acrossfrequencies and times: to control both for the low-pass filtering action of the skull, and for the re-duced synchronicity of activity at increasing laten-cies.
For each stimulus a mean is then taken overeach of six presentations to arrive at a more reli-able power estimate for each feature.3The feature ranking method used in Mitchell etal.
(2008) evaluates the extent to which the rela-tionship among stimuli is stable across across pre-sentations, using a correlational measure,4but pre-liminary analyses with this selection method onEEG features proved disappointing.
Here, two ad-ditional ranking criteria are used: each feature isevaluated for its noisiness (the amount of powervariation seen across presentations of the samestimulus), and for its distinctiveness (the amountof variation in power estimates across differentstimuli).
A combination of these three strategiesis used to rank the features by their informative-ness, and the top 50 features are then selected foreach participant.5A qualitative evaluation of the feature selec-tion strategy can be carried out by examiningthe distribution of features selected.
Figure 2shows the distribution of selected features over thetime/frequency spectrum (left panel), and over thescalp (right panel - viewed from above, with thenose pointing upwards).
The distribution seen is3Stimulus power features are isolated by band-pass filter-ing for the required frequencies, cropping following the rel-evant time interval relative to each image presentation, andthen taking the variance of the resulting signal, which is pro-portional to power.4See the associated supplementary materials of Mitchellet al (2008) for details: http://www.sciencemag.org/cgi/content/full/320/5880/1191/DC1.5Several combinations of these parameters (selectionthresholds of 5, 20, 50, 100, 200 features; ranking criteria inisolation and in combination) were investigated - the one cho-sen gave highest overall performance with the web-derivedcorpus model: 50 features, combined ranking criteria.621Figure 3: First two components of principal com-ponents analysis of selected features for partici-pant E (crosses: mammals; circles: tools)plausible in reference to previous work: lower fre-quencies (Pfurtscheller and Lopes da Silva, 1999),latencies principally in the first few hundred mil-liseconds (Kiefer, 2001), and activity in the visualcentres at the rear of the head, as well as parietalareas (Pulverm?uller, 2005).
A principal compo-nents analysis can also be performed on the se-lected features to see if they reflect any plausi-ble semantic space.
As figure 3 shows, the fea-ture selection stage has captured quite faithfullythe mammal/tool distinction in a totally unsuper-vised fashion.3 Corpus-based semantic modelsData from linguistics (Pustejovsky, 1995; Fill-more, 1982) and neuroscience (Barsalou, 1999;Barsalou, 2003; Pulverm?uller, 2005) underlinehow certain verbs, by emphasising typical ways inwhich we interact with entities and how they be-have, are pivotal in the representation of concretenominal concepts.
Following these traditions,Mitchell et al (2008) use 25 manually pickedverbs as their corpus-based features.Here that approach is replicated by translatingthese verbs into Italian.
Mitchell et al (2008) se-lected verbs that denote our interaction with ob-jects and living things, such as smell and ride.While the translations are not completely faithful(because frequent verbs of this sort tend to spandifferent sets of senses in the two languages), theaim was to respect the same principle when build-ing the Italian list.
The full list, with our backtranslations into English, is presented in Table 1.We refer to this set as the ?Mitchell?
verbs.alzare ?raise?
annusare ?smell/sniff?aprire ?open?
ascoltare ?listen?assaggiare ?taste?
avvicinare ?near?cavalcare ?ride?
correre ?run/flow?dire ?say/tell?
entrare ?enter?guidare ?drive?
indossare ?wear?maneggiare ?handle?
mangiare ?eat?muovere ?move?
pulire ?clean?puzzare ?stink?
riempire ?fill?rompere ?break?
sentire ?feel/hear?sfregare ?rub?
spingere ?push?temere ?fear?
toccare ?touch?vedere ?see?Table 1: The ?Mitchell?
verbs, with English trans-lationsAs in Mitchell et al (2008), in order to finda corpus large enough to provide reliable co-occurrence statistics for our target concepts andthe 25 verbs, we resorted to the Web, queried us-ing the Yahoo!
API.6In particular, we representeach concept by a vector that records how manytimes it co-occurred with each target verb withina span of 5 words left and right, according to Ya-hoo!
counts.
We refer to this corpus-based modelas the yahoo-mitchell model below.While manual verb picking has proved effec-tive for Mitchell and colleagues (and for us, as wewill see in a moment), ultimately what we are in-terested in is discovering the most distinctive fea-tures of each conceptual category.
We are there-fore interested in more systematic approaches toinducing corpus-based concept descriptions, andin which of these approaches works best for thistask.
The alternative models we consider werenot extracted from the Web, but from an existingcorpus, so that we could rely on pre-existing lin-guistic annotation (POS tagging, lemmatization,dependency paths), and perform more flexible,annotation-aware queries to collect co-occurrencestatistics.More specifically, we used the la Repub-blica/SSLMIT corpus7, that contains about 400million tokens of newspaper text.
From this, weextracted four models where nominal concepts arerepresented in terms of patterns of co-occurrencewith verbs (we collected statistics for the top20,000 most common nouns in the corpus, includ-ing the concepts used as stimuli in the silent nam-6http://developer.yahoo.com/search/7http:://sslmit.unibo.it/repubblica/622ing experiment, and the top 5,000 verbs).
We firstre-implemented a ?classic?
window-based wordspace model (Sahlgren, 2006), referred to belowas repubblica-window, where each noun lemma isrepresented by its co-occurrence with verb lem-mas within the maximum span of a sentence, withno more than one other intervening noun.
Therepubblica-position model is similar, but it alsorecords the position of the verb with respect tothe noun (so that X-usare ?X-use?
and usare-X?use-X?
count as different features), analogouslyto the seminal HAL model (Lund and Burgess,1996).
It has been shown that models that takethe syntactic relation between a target word anda collocate feature into account can outperform?flat?
models in some tasks (Pad?o and Lapata,2007).
The next two models are based on the de-pendency parse of the la Repubblica corpus docu-mented by Lenci (2009).
We only counted as col-locates those verbs that were linked to nouns bya direct path (such as subject and object) or viapreposition-mediated paths (e.g., tagliare con for-bici ?to cut with scissors?
), and where the pathswere among the top 30 most frequent in the cor-pus.
In the repubblica-depfilter model, we recordco-occurrence with verbs that are linked to thenouns by one of the top 30 paths, but we donot preserve the paths themselves in the features.This is analogous to the model proposed by Pad?oand Lapata (2007).
In the repubblica-deppathmodel, we preserve the paths as part of the fea-tures (so that subj-uccidere ?subj-kill?
and obj-uccidere count as different features), analogouslyto Lin (1998), Curran and Moens (2002) and oth-ers.
For all models, following standard practice incomputational linguistics (Evert, 2005), we trans-form raw co-occurrence counts into log-likelihoodratios.Following the evaluation paradigm of Mitchellet al (2008), linear models trained on corpus-based features are used to predict the pattern ofEEG activity for unseen concepts.
This onlyworks if we have a very limited number of fea-tures (or else we would have more parameters toestimate than data-points to estimate them).
TheRepubblica-based models have thousands of fea-tures (one per verb collocate, or verb+path collo-cate).
We adopt two strategies to select a reducednumber of features.
In the topfeat versions, wefirst pick the 50 features that have the highest asso-ciation with each of the target concepts.
We thencount in how many of these concept-specific toplists a feature occurs, and we pick the 25 featuresthat occur in the largest number of them.
The intu-ition is that this should give us a good trade-off be-tween how characteristic the features are (we onlyuse features that are highly associated with someof our concepts), and their generalization capabili-ties (we pick features that are associated with mul-tiple concepts).
Randomly selected examples ofthe features extracted in this way for the variousRepubblica models are reported in Table 2.repubblica-window repubblica-positionabbattere ?demolish?
X-ferire ?X-wound?afferrare ?seize?
X-usare ?X-use?impugnare ?grasp?
dipingere-X ?paint-X?tagliare ?cut?
munire-X ?supply-X?trovare ?find?
tagliare-X ?cut-X?repubblica-depfilter repubblica-deppathabbattere ?demolish?
con+tagliare ?with+cut?correre ?run?
obj+abbattere ?obj+demolish?parlare ?speak?
obj+uccidere ?obj+kill?saltare ?jump?
intr-subj+vivere ?intr-subj+live?tagliare ?cut?
tr-subj+aprire ?tr-subj+open?Table 2: Examples of top features from the la Re-pubblica modelsAlternatively, instead of feature selection weperform feature reduction by means of a SingularValue Decomposition (SVD) of the noun-by-verbmatrix.
We apply the SVD to matrices that includethe top 20,000 most frequent nouns in the cor-pus (including our target concepts) since the qual-ity of the resulting reduced model should improveif we can exploit richer patterns of correlationsamong the columns ?
verbs ?
across rows ?
nouns(Landauer and Dumais, 1997; Sch?utze, 1997).
Inthe svd versions of our models, we pick as fea-tures the top 25 left singular vectors, weightedby the corresponding singular values.
These fea-tures do not have a straightforward interpretation,but they tend to group verb meanings that belongto broad semantic domains.
For example, amongthe original verbs that are most strongly correlatedwith one of the top singular vectors of repubblica-window we find giocare ?play?, vincere ?win?
andperdere ?lose?.
Another singular vector is asso-ciated with ammontare ?amount?, costare ?cost?,pagare ?pay?, etc.
One of the top singular vec-tors of repubblica-deppath is strongly correlatedwith in+scendere ?descend into?, in+mettere ?putinto?, in+entrare ?enter into?, though not all sin-gular vectors are so clearly characterized by theverbs they correlate with.623None of the la Repubblica models had full cov-erage of our concept stimulus set (see the secondcolumn of Table 3 below), because our extractionmethod missed some multi-word units, and fea-ture selection led to losing some more items dueto data sparseness (e.g., some target words had nocollocates connected by the dependency paths weselected).
The experiments reported in the nextsection used all the target concepts available ineach model, but a replication using the 50 conceptsthat were common to all models obtained resultsthat are comparable.
For a direct comparison be-tween Yahoo!
and la Repubblica derived features,we tried collecting statistics for the Mitchell verbsfrom Repubblica as well, but the resulting modelwas extremely sparse, and we do not report its per-formance here.Finally, it is important to note that any repre-sentation yielded by a corpus semantic model doesnot characterise a concept directly, but is rather anaggregate of the various senses and usages of thenoun chosen to represent it.
This obvious limita-tion will persist until comprehensive, robust andcomputationally efficient word-sense disambigua-tion techniques become available.
However thesemodels are designed to extract semantic (as op-posed to syntactic or phonological) properties ofwords, and as noted in the introduction, have beendemonstrated to correlate with behavioural effectsof conceptual processing.4 Predicting EEG patterns usingcorpus-based modelsIn Section 2.2 above we showed how we extractedfeatures summarizing the spatial, temporal andfrequency distribution of the EEG signal collectedwhile participants were processing each of the tar-get concepts.
In Section 3, we described variousways to obtain a compact representation of thesame concepts in terms of corpus-derived features.We will now discuss the method we employed toverify whether the corpus-derived features can beused to predict the EEG patterns ?
that is whethersemantics can be used to predict neural activity.Our hope is that a good corpus-based model willprovide a decomposition of concepts into mean-ingful properties, corresponding to coherent sub-patterns of activation in the brain, and thus capturegeneralizations across concepts.
For example, ifa concept is particularly visually evocative (e.g.,zebra), we might expect it to be strongly associ-ated with the verb see, while also causing partic-ular activation of the vision centres of the brain.Similarly, concepts with strong associations witha particular sound (e.g., cuckoo) might be seman-tically associated with hear while also dispropor-tionately activating auditory areas of the brain.
Itshould thus be possible to learn a model of corpus-to-EEG-pattern correspondences on training data,and use it to predict the EEG activation patterns ofunseen concepts.We follow the paradigm proposed by Mitchell etal.
(2008) for fMRI data.
For each participant andselected EEG feature, we train a model where thelevel of activation of the latter in response to dif-ferent concepts is approximated by a linear com-bination of the corpus features:~f = C~?
+ ~where~f is the vector of activations of a specificEEG feature for different concepts, the matrix Ccontains the values of the corpus features for thesame concepts (row-normalised to z-scores),~?
isthe weight we must learn for each corpus feature,and~ is a vector of error terms.
We use the methodof least squared errors to learn the weights thatmaximize the fit of the model.
We can then predictthe activation of an EEG feature in response to anew concept that was not in the training data by a~?-weighted sum of the values of each corpus fea-ture for the new concept.
In some cases collinear-ity in the corpus data (regular linear relationshipsamong the corpus-feature columns) prevented theestimation procedure from finding a solution.
Insuch cases (due to the small number of data, rel-ative to the number of unknowns), the least in-formative corpus-features (those that correlated onaverage most highly with other features) were iter-atively removed until a solution was reached.
Allmodels were trained with between 23 and 25 cor-pus features.Again following Mitchell and colleagues, weadopt a leave-2-out paradigm in which a linearmodel for each EEG feature is trained in turn onall concepts minus 2.
For each of the 2 left outconcepts, we predict the EEG activation patternusing the trained linear model and their corpusfeatures, as just described.
We then try to cor-rectly match the predicted and observed activa-tions, by measuring the Euclidean distance be-tween the model-generated EEG activity (a vec-tor of estimated power levels for the n EEG fea-624tures selected) and the corresponding EEG activ-ity recorded in the experiment (other distance met-rics gave similar results to the ones reported here).Given the 2 left-out concepts a and b, we com-pute 2 matched distances (i.e., distance betweenpredicted and observed pattern for a, and the samefor b) and 2 mismatched distances (predicted a andobserved b; predicted b and observed a).
If the av-erage of the matched distances is lower than theaverage of the mismatched distances, we considerthe prediction successful ?
otherwise we count isas a failed prediction.
At chance levels, expectedmatching accuracy is 50%.5 ResultsTable 3 shows the comparative results for all thecorpus models introduced in Section 3.
The thirdcolumn (all) shows the overall accuracy in cor-rectly matching predicted to observed EEG ac-tivity patterns, and so successfully distinguishingword meanings.
The significance of the figures isindicated with the conventional annotation (calcu-lated using a one-way two-sided t-test across theindividual participant accuracy figures against anexpected population mean of 50%).8The secondcolumn shows the coverage of each model of the60 mammal and tool concepts used, which rangedfrom full (for the yahoo-mitchell model) to 51 con-cepts (for the depfilter-topfeat model).
The corre-sponding number of matching comparisons overwhich accuracy was calculated ranged from 1770down to 1225.As suggested by previous work (Murphy et al,2008), and illustrated by figure 3, coarse distinc-tions between words in different superordinate cat-egories (e.g., hammer vs armadillo; giraffe vsnail) may be easier to detect than those amongconcepts within the same category (e.g., ham-mer vs nail; giraffe vs armadillo).
The fourthand fifth columns give these accuracies, and whilebetween-category discriminations do prove morereliable, they indicate that, for the top rated modelat least, finer within-category distinctions are alsobeing captured.
Figures from the top two perform-ing models are given for individual participants intables 4 and 5.8On average, the difference seen between matched andmismatched pairs was small, at about 3% of the distancebetween observed and predicted representations, and wasmarginally bigger for correct than for incorrect predictions(p < 0.01).part.
overall within betweenA 54 53 55B 54 47 60C 62 56 67D 61 56 67E 68 58 78F 52 54 51G 57 51 63Table 4: Accuracy levels for individual participantsessions, yahoo-mitchell web corpuspart.
overall within betweenA 49 52 46B 59 57 60C 60 60 59D 50 45 55E 56 53 58F 64 64 65G 52 49 55Table 5: Accuracy levels for individual participantsessions, repubblica-window-svd6 DiscussionOur results show that corpus-extracted conceptualmodels can be used to distinguish between theEEG activation levels associated with conceptualcategories to a degree that is significantly abovechance.
Though category specific patterns are de-tectable in the EEG signal alone (as illustrated bythe PCA analysis in figure 3), on that basis we can-not be sure that semantics is being detected.
Someother property of the stimuli that co-varies with thesemantic classes of interest could be responsible,such as visual complexity, conceptual familiarity,lexical frequency, or phonological form.
Only bycross-training with individual corpus features andshowing that these hold a predictive relationship toneural activity have we been able to establish thatEEG patterns encode semantics.Present evidence indicates that fMRI may pro-vide richer data for training such models than EEG(Mitchell and colleagues obtain an average accu-racy of 77%, and 65% for the within category set-ting).
However, fMRI has several clear disadvan-tages as a tool for language researchers.
First ofall, the fine spatial resolution it provides (downto 2-3mm), while of great interest to neuroscien-tists, is not in itself linguistically informative.
Itscoarse temporal resolution (of the order of severalseconds), makes it ill-suited to analysing on-linelinguistic processes.
EEG on the other hand, de-spite its low spatial resolution (several centime-tres), gives millisecond-level temporal resolution,625model coverage all within cat between catyahoo-mitchell 100 58.3** (5.7) 53.6* (3.7) 63.0** (8.9)repubblica-window-svd 96.7 55.7* (5.6) 54.3 (6.5) 56.9* (5.9)repubblica-window-topfeat 93.3 52.1 (4.3) 48.7 (3.6) 55.4 (7.0)repubblica-deppath-svd 93.3 51.4 (8.7) 49.0 (8.0) 54.0 (10.0)repubblica-depfilter-topfeat 85.0 51.1 (9.6) 49.3 (9.6) 53.1 (10.0)repubblica-position-topfeat 93.3 50.0 (5.2) 46.0 (4.7) 53.6 (8.0)repubblica-deppath-topfeat 86.7 49.9 (9.0) 47.0 (9.3) 52.4 (9.6)repubblica-position-svd 96.7 49.4 (10.2) 46.6 (9.8) 52.3 (11.3)repubblica-depfilter-svd 93.3 48.9 (11.1) 47.1 (8.9) 50.6 (12.9)Table 3: Comparison across corpus models, with percentage concept coverage, mean cross-subject per-centage prediction accuracy and standard deviation; ?p < 0.05, ?
?
p < 0.01enabling the separate analysis of sequential cogni-tive processes and states (e.g., auditory process-ing, word comprehension, semantic representa-tion).
fMRI is also prohibitively expensive formost researchers (ca.
300 euros per hour at costprice), compared to EEG (ca.
30 euros per hour).Finally, there is no prospect of fMRI being minia-turised, while wearable EEG systems are alreadybecoming commercially available, making exper-imentation in more ecological settings a possibil-ity (e.g., playing with a child, meeting at a desk,walking around).
In short, while EEG can be usedto carry out systematic investigations of categori-cal distinctions, doing so with fMRI would be pro-hibitively expensive.Present results indicate that distinctions be-tween categories are easier than distinctions be-tween category elements; and that selecting theconceptual features by hand gives better resultsthan discovering them automatically.
Both ofthese results however may be due to limitationsof the current method.
One limitation is that wehave been using the same set of features for allconcepts, which is likely to blur the distinctionsbetween members of a category more than thosebetween categories.
A second limitation of ourpresent methodology is that it is constrained to usevery small numbers of semantic features, whichlimits its applicability.
For example it is hard toconceive of a small set of verbs, or other parts-of-speech, whose co-occurrence patterns could suc-cessfully characterise the full range of meaningfound in the human lexicon.
Even the more eco-nomical corpus-extracted conceptual models tendto run in the hundreds of features (Almuhareb,2006).
We are currently working on variations inthe method that will address these shortcomings.The web-based model with manually pickedfeatures outperformed all la Repubblica-basedmodels.
However, the results attained withrepubblica-window-svd are encouraging, espe-cially considering that we are reporting results foran EEG feature configuration optimised for theweb data (see footnote 5), and that la Repubblicais several orders of magnitude smaller than theweb.
That data sparseness might be the main is-sue with la Repubblica models is suggested bythe fact that repubblica-window-svd is the leastsparse of them, since it does not filter data by posi-tion or dependency path, and compresses informa-tion from many verbs via SVD.
In future research,we plan to extract richer models from larger cor-pora.
And as the discriminative accuracy of cross-training techniques improves, further insights intothe relative validity of corpus representations willbe attainable.
One research aim is to see if individ-ual corpus semantic properties are encoded neu-rally, so providing strong evidence for a particularmodel.
These techniques may also prove more ob-jective and reliable in evaluating representations ofabstract concepts, for which it is more difficult tocollect reliable judgements from informants.ReferencesA.
Almuhareb.
2006.
Attributes in lexical acquisition.Dissertation, University of Essex.L.
Barsalou.
1999.
Perceptual symbol systems.
Be-havioural and Brain Sciences, 22:577?660.L.
Barsalou.
2003.
Situated simulation in the humanconceptual system.
Language and Cognitive Pro-cesses, 18:513?562.J.R.
Curran and M. Moens.
2002.
Improvements inautomatic thesaurus extraction.
In Proceedings ofSIGLEX, pages 59?66.A.
Delorme and S. Makeig.
2003.
Eeglab: an opensource toolbox for analysis of single-trial dynamicsincludingindependent component analysis.
Journalof Neuroscience Methods, 134:9?21.626S.
Evert.
2005.
The statistics of word cooccurrences.Dissertation, Stuttgart University.Ch.
J. Fillmore.
1982.
Frame semantics.
In Linguis-tic Society of Korea, editor, Linguistics in the Morn-ing Calm, pages 111?138.
Hanshin, Seoul.J.
Gilbert, L. Shapiro, and G. Barnes.
2009.
Processingof living and nonliving objects diverges in the visualprocessing system: evidence from meg.
In Proceed-ings of the Cognitive Neuroscience Society AnnualMeeting.M.
Kiefer.
2001.
Perceptual and seman-tic sources of category-specific effects in objectcategorization:event-related potentials during pic-ture and word categorization.
Memory and Cogni-tion, 29(1):100?116.T.
Landauer and S. Dumais.
1997.
A solution to Platosproblem: the latent semantic analysis theory of ac-quisition, induction, and representation of knowl-edge.
Psychological Review, 104(2):211?240.A.
Lenci.
2009.
Argument alternations in italian verbs:a computational study.
In Atti del XLII CongressoInternazionale di Studi della Societ`a di LinguisticaItaliana.D.
Lin.
1998.
Automatic retrieval and clusteringof similar words.
In COLING-ACL98, Montreal,Canada.K.
Lund and C. Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, and Computers, 28:203?208.S.
Makeig, A.J.
Bell, T. Jung, and T.J. Sejnowski.1996.
Independent component analysis of elec-troencephalographic data.
In in Advances in Neu-ral Information Processing Systems, pages 145?151.MIT Press.A.
Martin and L. Chao.
2001.
Semantic memory andthe brain: structure and processes.
Current Opinionsin Neurobiology, 11:194?201.T.
Mitchell, S. Shinkareva, A. Carlson, K. Chang,V.
Malave, R. Mason, and M. Just.
2008.
Predictinghuman brain activity associated with the meaningsof nouns.
Science, 320:1191?1195.B.
Murphy, M. Dalponte, M. Poesio, and L. Bruz-zone.
2008.
Distinguishing concept categories fromsingle-trial electrophysiological activity.
In Pro-ceedings of the Annual Meeting of the Cognitive Sci-ence Society.S.
Pad?o and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.G.
Pfurtscheller and F. Lopes da Silva.
1999.
Event-related EEG/MEG synchronization and desynchro-nization: Basic principles.
Clinical Neurophysiol-ogy, 110:1842?1857.F.
Pulverm?uller.
2002.
The neuroscience of language:on brain circuits of words and serial order.
Cam-bridge University Press, Cambridge.F.
Pulverm?uller.
2005.
Brain mechanisms linking lan-guage and action.
Nature Reviews Neuroscience,6:576?582.J.
Pustejovsky.
1995.
The Generative Lexicon.
MITPress, Cambridge.M.
Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Dissertation, StockholmUniversity.S.
Schulte im Walde.
2008.
Theoretical adequacy, hu-man data and classification approaches in modellingword properties, word relatedness and word classes.Habilitation, Saarland University.H.
Sch?utze.
1997.
Ambiguity Resolution in NaturalLanguage Learning.
CSLI, Stanford.627
