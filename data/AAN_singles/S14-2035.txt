Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 218?222,Dublin, Ireland, August 23-24, 2014.DAEDALUS at SemEval-2014 Task 9:Comparing Approaches for Sentiment Analysis in TwitterJulio Villena-Rom?nDaedalus, S.A.jvillena@daedalus.esJanine Garc?a-MoreraDaedalus, S.A.jgarcia@daedalus.esJos?
Carlos Gonz?lez-Crist?balUniversidad Polit?cnica de Madridjosecarlos.gonzalez@upm.esAbstractThis paper describes our participation at SemEval-2014 sentiment analysis task, in both contextual andmessage polarity classification.
Our idea was to com-pare two different techniques for sentiment analysis.First, a machine learning classifier specifically builtfor the task using the provided training corpus.
On theother hand, a lexicon-based approach using naturallanguage processing techniques, developed for a ge-neric sentiment analysis task with no adaptation to theprovided training corpus.
Results, though far from thebest runs, prove that the generic model is more robustas it achieves a more balanced evaluation for messagepolarity along the different test sets.1 IntroductionSemEval1 is an international competitive evalua-tion workshop on semantic related tasks.
Amongthe ten different tasks that have been proposed in2014, Task 9 at SemEval-20142 focuses on sen-timent analysis in Twitter.Sentiment analysis could be described as theapplication of natural language processing andtext analytics to identify and extract subjectiveinformation from texts.
Given a message in Eng-lish, the objective is to determine if the text ex-presses a positive, negative or neutral sentimentin that context.It is a major technological challenge and thetask is so hard that even humans often disagreeon the sentiment of a given text, as issues thatone individual may find acceptable or relevantmay not be the same to others, along with multi-lingual aspects and different cultural factors.This work is licensed under a Creative Commons Attribu-tion 4.0 International Licence.
Page numbers and proceed-ings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1 http://alt.qcri.org/semeval2014/2 http://alt.qcri.org/semeval2014/task9/The task defines two subtasks, where the dif-ference is that whereas the output in subtask Bmust be the message polarity classification, i.e.,the global polarity of the whole message, subtaskA is focused on contextual polarity disambigua-tion, i.e., the message contains a marked instanceof a word or phrase and the expected output mustbe the polarity of that specific instance within thewhole message.Daedalus (2014) is a leading provider of lan-guage-based solutions in Spain, and long-timeparticipants in different research conferences andevaluation workshops such as CLEF (2014) andNTCIR (2014), in many different tasks includingsentiment analysis (Villena-Rom?n et al., 2008;Villena-Rom?n et al., 2012).This paper describes our participation in bothcontextual (subtask A) and message (subtask B)polarity classification.
The main idea behind ourparticipation is to compare two different tech-niques for sentiment analysis: a machine learningapproach using the provided corpus to train amodel specifically adapted to that scenarioagainst a lexicon-based approach using advancednatural language processing techniques for cap-turing the meaning of the text, developed prior tothe task and obviously without using the provid-ed corpus.Our point of view is that although machinelearning classifiers generally achieve better re-sults in competitive evaluations that provide atraining corpus, when these same models are ap-plied to a different scenario, the precision andrecall metrics are drastically reduced, thus affect-ing to the perception and confidence of stake-holders in sentiment analysis technologies.Our different approaches, experiments and re-sults achieved are presented and discussed in thefollowing sections.2182 Constrained Runs: Machine LearningClassifierThe first approach is a simple quite naive ma-chine learning classifier trained exclusively withthe provided training corpus.
This is the ap-proach adopted for constrained runs in both sub-task A and B.First, based on the Vector Space Model (Sal-ton et al., 1975), the text of each tweet is con-verted into a term vector where terms are as-sumed to represent the semantic content of themessage.
Textalytics parsing API (Textalytics,2014a)  offered through a REST-based web ser-vice is used to get the lemma of each word andfilter part-of-speech categories: currently nouns,verbs, adjectives and adverbs are selected asterms.
A weighted term vector based on the clas-sical TF-IDF is used.
Both the training and thetest set are preprocessed in this same way.After this preprocessing, a classifier trained onthe training corpus is used to classify the testcorpus.
Many different supervised learning algo-rithms where evaluated with 10-fold cross vali-dation, using Weka (Hall et al., 2009).
We finallyselected Multinomial Naive Bayes algorithm,training three different binary classifiers: posi-tive/not_positive, negative/not_negative and neu-tral/not_neutral.
To select the final global mes-sage polarity, a simple rule-based decision ismade:if positive and not_negative andnot_neutral then positiveelse if negative and not_positive andnot_neutral then negativeelse neutralThis is directly the output for subtask B. Forsubtask A, this same global polarity is assignedto each text fragment, i.e., subtask A and B aretreated in the same way.3 Unconstrained Runs: Lexicon-BasedModelOur second approach, used in the unconstrainedruns in both subtasks, is based on 1) the infor-mation provided by a semantic model that in-cludes rules and resources (polarity units, modi-fiers, stopwords) annotated for sentiment analy-sis, 2) a detailed morphosyntactic analysis of thetweet to lemmatize and split the text into seg-ments, useful to control the scope of semanticunits and perform a fine-grained detection of ne-gation in clauses, and 3) the use of an aggrega-tion algorithm to calculate the global polarityvalue of the text based on the local polarity val-ues of the different segments, including an outli-er detection.We consider this approach to be unconstrainedbecause the lexicon in the semantic model(which would be valid itself for a constrainedrun) has been generated, tested and validated us-ing additional training data.All this functionality is encapsulated and pro-vided by our Textalytics API for multilingualsentiment analysis (Textalytics, 2014b) in severallanguages, including English.
Apart from the textitself, a required input parameter is the semanticmodel to use in the sentiment evaluation.
Thissemantic model defines the domain of the text(the analysis scenario) and is mainly based on anextensive set of dictionaries and rules that incor-porate both the well-known ?domain-independent?
polarity values (for instance, ingeneral, in all contexts, good is positive and aw-ful is negative) and also the specificities of eachanalysis scenario (for instance, an increase in theinterest rate is probably positive for financialcompanies but negative for the rest).First the local polarity of the different clausesin the text (segments) is identified based on thesentence syntactic tree and then the relationamong them is evaluated in order to obtain aglobal polarity value for the whole given text.The detailed process may be shortly described asfollows:1.
Segment detection.
The text is parsedand split into segments, based on thepresence of punctuation marks and capi-talization of words.2.
Linguistic processing: each segment istokenized (considering multiword units)and then each token is analyzed to ex-tract its lemma(s).
In addition, a morpho-syntactic analysis divides the segmentinto proposition or clauses and builds thesentence syntactic tree.
This division isuseful, as described later, for detectingthe negation and analyzing the effect ofmodifiers on the polarity values.3.
Detection of negation.
The next step isto iterate over every token of each seg-ment to tag whether the token is affectedby negation or not.
If a given token is af-fected by negation, the eventual polaritylevel is reversed (turns from positive tonegative and the other round).
For thispurpose, the semantic model includes a219list of negation units, such as the obviousnegation particles (adverbs) such as not(and contracted forms), neither and alsoexpressions such as against, far from, noroom for, etc.4.
Detection of modifiers.
Some specialunits do not assign a specific polarityvalue but operate as modifiers of thisvalue, incrementing or decrementing it.These units included in the semanticmodel can be assigned a + (positive), ++(strong positive), - (negative) or --(strong negative) value.
For instance, ifgood is positive (P), very good is bestrong positive (P+), thus very would bea positive modifier (+).
Other examplesare additional, a lot, completely (posi-tive) or descend, almost (negative).5.
Polarity tagging.
The next step is to de-tect polarity units in the segments.
Thesemantic model assigns one of the fol-lowing values, ranging from the mostpositive to the most negative: P++, P+,P, P-, P--, N--, N-, N, N+ and N++.Moreover, these units can include a con-text filter, i.e., one or several words orexpressions that must appear or not inthe segment so that the unit is consideredin the sentiment analysis.
The final valuefor each token is calculated from the po-larity value of the unit in the semanticmodel, adding or subtracting the polarityvalue of the modifier (if thresholds arefulfilled) and considering the negation(again, if thresholds are fulfilled).6.
Segment scoring.
To calculate the over-all polarity of each segment, an aggrega-tion algorithm is applied to the set of po-larity values detected in the segment.The average of polarity values is calcu-lated and assigned as the score of thesegment, ranging from -1 (strong nega-tive) to +1 (strong positive).
In additionto this numeric score, discrete nominalvalues are also assigned (N+, N, NEU, P,P+).
When there are no polarity units,the segment is assigned with a polarityvalue of NONE.
The aggregation algo-rithm performs an outlier filtering to tryto reduce the effect of wrong detections,based on a threshold over the standarddeviation from the average.7.
Global text scoring.
The same aggrega-tion algorithm is applied to the local po-larity values of each segment to calculatethe global polarity value of the text, rep-resented by an average value (both nu-meric and nominal values).Although unconstrained runs were allowed touse the training corpus for improving the model,we were interested on not doing so, as we point-ed out in the introduction, to compare the robust-ness of both models.For the purpose of both subtasks, the provid-ed output was adapted so that P+ and P weregrouped into positive and similarly N+ and Ninto negative.
In subtask B, the global polaritywas directly used as the output, whereas in sub-task A, the polarity assigned to each text frag-ment was the polarity value of the segment inwhich this text fragment is located.
As comparedto the constrained task, this allows a more fine-grained assignment of polarity and, expectedly,achieve a better evaluation.Although we had different models available,some developed for specific domains such as thefinancial, telecommunications and tourism do-mains, for this task, a general-purpose model forEnglish was used.
This model was initially basedon the linguistic resources provided by GeneralInquirer3 in English.
Some information about themodel is shown in Table 1.Unit Type CountNegation (NEG) 31Modifiers (MOD) 117-- 3- 16+ 75++ 23Polarity (POL) 4 606N++ 81N+ 297N 2 222N- 221N-- 13P-- 6P- 82P 1 340P+ 316P++ 28Stopwords (SW) 59Macros 19TOTAL UNITS 4 832Table 1.
English semantic model.3 http://www.wjh.harvard.edu/~inquirer2204 ResultsWe submitted two runs, constrained  and uncon-strained, for each subtask, so four runs in all.
Asdefined by the organization, the evaluation met-ric was the average F-measure (averaged F-positive and F-negative, ignoring F-neutral).Separate rankings for several test dataset werealso produced for comparing different scenarios.Results achieved for runs in subtask Aare shown in Table 2.Run A B C D E AvgDAEDALUS-A-constrained61.0 63.9 67.4 61.0 45.3 59.7DAEDALUS-A-unconstrained58.7 56.0 62.0 58.1 49.2 56.7Average 77.1 77.4 80.0 76.8 68.3 75.9NRC-Canada-A-constrained(best run)85.5 88.0 90.1 86.6 77.1 85.5A=LiveJournal 2014, B=SMS 2013, C=Twitter 2013D=Twitter 2014, E=Twitter 2014 SarcasmTable 2.
Results for subtask A.We did not specifically the contextual polarityclassification in subtask A, so results are notgood.
The machine learning classifier achieved aslightly better result on average for all test corpusthan the lexicon-based model, as expected, abouta 5% improvement.
As compared to other partic-ipants, we rank the second-to-last group (19 outof 20) and our best experiment is 27% below theaverage, and 43% below the best run.The best test set for our experiments is theTwitter 2013 corpus, as it is the most similar tothe training corpus.
If Twitter 2014 Sarcasm cor-pus is removed from the evaluation, whichclearly is the most difficult set for all runs, theconstrained run is only 22% below the averageand 38% below the best run, so a relative im-provement against the others.Run A B C D E AvgDAEDALUS-B-constrained40.8 40.9 36.6 33.0 29.0 36.1DAEDALUS-B-unconstrained61.0 55.0 59.0 57.6 35.2 53.6Average 63.5 55.6 59.8 60.4 45.4 57.0TeamX-B-constrained(best run)69.4 57.4 72.1 71.0 56.5 65.3A=LiveJournal 2014, B=SMS 2013, C=Twitter 2013D=Twitter 2014, E=Twitter 2014 SarcasmTable 3.
Results for subtask B.On the other hand, results achieved for runs insubtask B are shown in Table 3.
The subtask wasa bit more difficult than the first one, and resultsare in general worse than in the first subtask, asmore difficult aspects arise in the global polarityassignment, such as the appearance of coordi-nated or subordinated clauses or a higher impactof negation.We think that the specific consideration ofthese issues is the main reason why in this caseour best run is the lexicon-based model, with animprovement of 48 % over the constrained run.Also results are more robust as they are moreconsistent for the different test sets.
The best re-sults are achieved for the LiveJournal 2014 cor-pus, which presumably contains longer texts withmore formal writing corpus, so benefiting withthe use of the advanced linguistic preprocessing.Comparing to other participants, we rank 29out of 42 groups, and our best experiment is just6% below the average, and 22% below the bestrun.
If, again, the worst set, the Twitter 2014Sarcasm corpus, is removed from the evaluation,our unconstrained run is around the average (2%below), and, a bit surprisingly, the best groupchanges to the one that submitted the best run insubtask A, and our experiment is just 23% below(comparing to 38% below in subtask A).5 Conclusions and Future WorkOur main conclusion after our first participationin SemEval is that, although results are not goodcompared to the best ranked participants, ourlexicon-based model, externally developed for ageneric sentiment analysis task, without any ad-aptation to the provided training corpus, and cur-rently in production, is robust and achieves abalanced evaluation result for message polarityalong the different test corpus analyzed.
Despiteof the difficulty of the task, results are valuableand validate the fact that this technology is readyto be included into an automated workflow pro-cess for social media mining.Due to lack of time, no error analysis has beencarried out yet by studying the confusion matrixfor the different categories, which is left as short-term future work.
We expect to get a better un-derstanding of the miss classifications of our sys-tem and find a way to solve the issues that mayarise.
Probably there is still much to do in boththe enlargement of the semantic resources andalso the improvement of the linguistic processing(specially building the sentence syntactic tree) ina general domain for a non-formal writing style.221AcknowledgementsThis work has been supported by several SpanishR&D projects: Ciudad2020: Hacia un nuevomodelo de ciudad inteligente sostenible(INNPRONTA IPT-20111006), MA2VICMR:Improving the Access, Analysis and Visibility ofMultilingual and Multimedia Information in Web(S2009/TIC-1542) and MULTIMEDICA: Multi-lingual Information Extraction in Health Domainand Application to Scientific and InformativeDocuments (TIN2010-20644-C03-01).ReferencesCLEF.
2014.
CLEF Initiative (Conference and Labsof the Evaluation Forum).
http://www.clef-initiative.eu/NTCIR.
2014.
NII Testbeds and Community for In-formation Access Research.http://research.nii.ac.jp/ntcir/Julio Villena-Rom?n, Sara Lana-Serrano and Jos?
C.Gonz?lez-Crist?bal.
2008.
MIRACLE at NTCIR-7MOAT: First Experiments on Multilingual OpinionAnalysis.
7th NTCIR Workshop Meeting.
Evalua-tion of Information Access Technologies: Infor-mation Retrieval, Question Answering and Cross-Lingual Information Access.
Tokyo, Japan, De-cember 2008.Julio Villena-Rom?n, Sara Lana-Serrano, CristinaMoreno-Garc?a, Janine Garc?a-Morera, Jos?
CarlosGonz?lez-Crist?bal.
2012.
DAEDALUS at RepLab2012: Polarity Classification and Filtering on Twit-ter Data.
CLEF 2012 Labs and Workshop Note-book Papers.
Rome, Italy, September 2012.Daedalus.
2014. http://www.daedalus.es/G.
Salton, A. Wong, and C. S. Yang.
1975.
A vectorspace model for automatic indexing, Communica-tions of the ACM, v.18 n.11, p.613-620, Nov.1975.Textalytics.
2014.
Meaning as a service.http://textalytics.com/home.Textalytics Parser API.
2014.
Lemmatization, PoSand Parsing v1.2.http://textalytics.com/core/parser-infoTextalytics Sentiment API.
2014.
Sentiment Analysisv1.1.
http://textalytics.com/core/sentiment-infoM.
Hall, E. Frank, G. Holmes, B. Pfahringer, P.Reutemann, and I.H.
Witten.
2009.
The WEKAData Mining Software: An Update.
SIGKDD Ex-plorations, Volume 11, Issue 1.222
