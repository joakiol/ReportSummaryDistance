Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?5,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsHigher-Order Constituent Parsing and Parser Combination?Xiao Chen and Chunyu KitDepartment of Chinese, Translation and LinguisticsCity University of Hong KongTat Chee Avenue, Kowloon, Hong Kong SAR, China{cxiao2,ctckit}@cityu.edu.hkAbstractThis paper presents a higher-order model forconstituent parsing aimed at utilizing more lo-cal structural context to decide the score ofa grammar rule instance in a parse tree.
Ex-periments on English and Chinese treebanksconfirm its advantage over its first-order ver-sion.
It achieves its best F1 scores of 91.86%and 85.58% on the two languages, respec-tively, and further pushes them to 92.80%and 85.60% via combination with other high-performance parsers.1 IntroductionFactorization is crucial to discriminative parsing.Previous discriminative parsing models usually fac-tor a parse tree into a set of parts.
Each part is scoredseparately to ensure tractability.
In dependencyparsing (DP), the number of dependencies in a partis called the order of a DP model (Koo and Collins,2010).
Accordingly, existing graph-based DP mod-els can be categorized into tree groups, namely, thefirst-order (Eisner, 1996; McDonald et al, 2005a;McDonald et al, 2005b), second-order (McDonaldand Pereira, 2006; Carreras, 2007) and third-order(Koo and Collins, 2010) models.Similarly, we can define the order of constituentparsing in terms of the number of grammar rulesin a part.
Then, the previous discriminative con-stituent parsing models (Johnson, 2001; Henderson,2004; Taskar et al, 2004; Petrov and Klein, 2008a;?The research reported in this paper was partially supportedby the Research Grants Council of HKSAR, China, through theGRF Grant 9041597 (CityU 144410).Petrov and Klein, 2008b; Finkel et al, 2008) are thefirst-order ones, because there is only one grammarrule in a part.
The discriminative re-scoring models(Collins, 2000; Collins and Duffy, 2002; Charniakand Johnson, 2005; Huang, 2008) can be viewed asprevious attempts to higher-order constituent pars-ing, using some parts containing more than onegrammar rule as non-local features.In this paper, we present a higher-order con-stituent parsing model1 based on these previousworks.
It allows multiple adjacent grammar rulesin each part of a parse tree, so as to utilize morelocal structural context to decide the plausibility ofa grammar rule instance.
Evaluated on the PTBWSJ and Chinese Treebank, it achieves its best F1scores of 91.86% and 85.58%, respectively.
Com-bined with other high-performance parsers underthe framework of constituent recombination (Sagaeand Lavie, 2006; Fossum and Knight, 2009), thismodel further enhances the F1 scores to 92.80% and85.60%, the highest ones achieved so far on thesetwo data sets.2 Higher-order Constituent ParsingDiscriminative parsing is aimed to learn a functionf : S ?
T from a set of sentences S to a set of validparses T according to a given CFG, which maps aninput sentence s ?
S to a set of candidate parsesT (s).
The function takes the following discrimina-tive form:f(s) = arg maxt?T (s)g(t, s) (1)1http://code.google.com/p/gazaparser/1thea portion ofDTwill32$ million realized from the sales be ...VPNP NPQP PPVBNIN PPbegin(b) split(m) end(e)...Figure 1: A part of a parse tree centered at NP?
NP VPwhere g(t, s) is a scoring function to evaluate theevent that t is the parse of s. Following Collins(2002), this scoring function is formulated in the lin-ear formg(t, s) = ?
??
(t, s), (2)where ?
(t, s) is a vector of features and ?
the vectorof their associated weights.
To ensure tractability,this model is factorized asg(t, s) =?r?tg(Q(r), s) =?r?t?
?
?
(Q(r), s), (3)where g(Q(r), s) scores Q(r), a part centered atgrammar rule instance r in t, and ?
(Q(r), s) is thevector of features for Q(r).
Each Q(r) makes itsown contribution to g(t, s).
A part in a parse treeis illustrated in Figure 1.
It consists of the centergrammar rule instance NP?
NP VP and a set of im-mediate neighbors, i.e., its parent PP ?
IN NP, itschildren NP ?
DT QP and VP ?
VBN PP, and itssibling IN ?
of.
This set of neighboring rule in-stances forms a local structural context to provideuseful information to determine the plausibility ofthe center rule instance.2.1 FeatureThe feature vector ?
(Q(r), s) consists of a seriesof features {?i(Q(r), s))|i ?
0}.
The first feature?0(Q(r), s) is calculated with a PCFG-based gen-erative parsing model (Petrov and Klein, 2007), asdefined in (4) below, where r is the grammar rule in-stance A ?
B C that covers the span from the b-thto the e-th word, splitting at the m-th word, x, y andz are latent variables in the PCFG-based model, andI(?)
and O(?)
are the inside and outside probabili-ties, respectively.All other features ?i(Q(r), s) are binary func-tions that indicate whether a configuration exists inQ(r) and s. These features are by their own na-ture in two categories, namely, lexical and structural.All features extracted from the part in Figure 1 aredemonstrated in Table 1.
Some back-off structuralfeatures are used for smoothing, which cannot bepresented due to limited space.
With only lexicalfeatures in a part, this parsing model backs off to afirst-order one similar to those in the previous works.Adding structural features, each involving a least aneighboring rule instance, makes it a higher-orderparsing model.2.2 DecodingThe factorization of the parsing model allows us todevelop an exact decoding algorithm for it.
Follow-ing Huang (2008), this algorithm traverses a parseforest in a bottom-up manner.
However, it deter-mines and keeps the best derivation for every gram-mar rule instance instead of for each node.
Be-cause all structures above the current rule instanceis not determined yet, the computation of its non-local structural features, e.g., parent and sibling fea-tures, has to be delayed until it joins an upper levelstructure.
For example, when computing the scoreof a derivation under the center rule NP ?
NP VPin Figure 1, the algorithm will extract child featuresfrom its children NP ?
DT QP and VP ?
VBN PP.The parent and sibling features of the two child rulescan also be extracted from the current derivation andused to calculate the score of this derivation.
Butparent and sibling features for the center rule willnot be computed until the decoding process reachesthe rule above, i.e., PP?
IN NP.This algorithm is more complex than the approx-imate decoding algorithm of Huang (2008).
How-ever, its efficiency heavily depends on the size of theparse forest it has to handle.
Forest pruning (Char-?0(Q(r), s) =?x?y?zO(Ax, b, e)P(Ax ?
By Cz)I(By, b,m)I(Cz,m, e)I(S, 0, n)(4)2Template Description CommentsLexicalfeatureN-gram on inner/outer edgewb/e+l(l=0,1,2,3,4) & b/e & l & NPSimilar to the distributionalsimilarity cluster bigramsfeatures in Finkel et al (2008)wb/e?l(l=1,2,3,4,5) & b/e & l & NPwb/e+lwb/e+l+1(l=0,1,2,3) & b/e & l & NPwb/e?l?1wb/e?l(l=1,2,3,4) & b/e & l & NPwb/e+lwb/e+l+1wb/e+l+2(l=0,1,2) & b/e & l & NPwb/e?l?2wb/e?l?1wb/e?l(l=1,2,3) & b/e & l & NPBigram on edges wb/e?1wb/e & NP Similar to the lexical spanfeatures in Taskar et al (2004)and Petrov and Klein (2008b)Split pair wm?1wm & NP?
NP VPInner/Outer pairwbwe?1 & NP?
NP VPwb?1we & NP?
NP VPRule bigramLeft & NP & NP Similar to the bigrams featuresin Collins (2000)Right & NP & NPStructuralfeatureParent PP?
IN NP & NP?
NP VPSimilar to the grandparentrules features in Collins (2000)ChildNP?
DT QP & VP?
VBN PP & NP?
NP VPNP?
DT QP & NP?
NP VPVP?
VBN PP & NP?
NP VPSibling Left & IN?
of & NP?
NP VPTable 1: Examples of lexical and structural featureniak and Johnson, 2005; Petrov and Klein, 2007)is therefore adopted in our implementation for ef-ficiency enhancement.
A parallel decoding strategyis also developed to further improve the efficiencywithout loss of optimality.
Interested readers can re-fer to Chen (2012) for more technical details of thisalgorithm.3 Constituent RecombinationFollowing Fossum and Knight (2009), our con-stituent weighting scheme for parser combinationuses multiple outputs of independent parsers.
Sup-pose each parser generates a k-best parse list for aninput sentence, the weight of a candidate constituentc is defined as?
(c) =?i?k?i?
(c, ti,k)f(ti,k), (5)where i is the index of an individual parser, ?ithe weight indicating the confidence of a parser,?
(c, ti,k) a binary function indicating whether c iscontained in ti,k, the k-th parse output from the i-th parser, and f(ti,k) the score of the k-th parse as-signed by the i-th parser, as defined in Fossum andKnight (2009).The weight of a recombined parse is defined as thesum of weights of all constituents in the parse.
How-ever, this definition has a systematic bias towards se-lecting a parse with as many constituents as possibleEnglish ChineseTrain.
Section 2-21 Art.
1-270,400-1151Dev.
Section 22/24 Art.
301-325Test.
Section 23 Art.
271-300Table 2: Experiment Setupfor the highest weight.
A pruning threshold ?, simi-lar to the one in Sagae and Lavie (2006), is thereforeneeded to restrain the number of constituents in a re-combined parse.
The parameters ?i and ?
are tunedby the Powell?s method (Powell, 1964) on a develop-ment set, using the F1 score of PARSEVAL (Blacket al, 1991) as objective.4 ExperimentOur parsing models are evaluated on both Englishand Chinese treebanks, i.e., the WSJ section of PennTreebank 3.0 (LDC99T42) and the Chinese Tree-bank 5.1 (LDC2005T01U01).
In order to comparewith previous works, we opt for the same split asin Petrov and Klein (2007), as listed in Table 2.
Forparser combination, we follow the setting of Fossumand Knight (2009), using Section 24 instead of Sec-tion 22 of WSJ treebank as development set.In this work, the lexical model of Chen and Kit(2011) is combined with our syntactic model underthe framework of product-of-experts (Hinton, 2002).A factor ?
is introduced to balance the two models.It is tuned on a development set using the gold sec-3English ChineseR(%) P(%) F1(%) R(%) P(%) F1(%)Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22First-order 91.33 91.79 91.56 84.14 86.23 85.17Higher-order 91.62 92.11 91.86 84.24 86.54 85.37Higher-order+?
91.60 92.13 91.86 84.45 86.74 85.58Stanford parser - - - 77.40 79.57 78.47C&J parser 91.04 91.76 91.40 - - -Conbination 92.02 93.60 92.80 82.44 89.01 85.60Table 3: The performance of our parsing models on the English and Chinese test sets.System F1(%) EX(%)SingleCharniak (2000) 89.70Berkeley parser 89.87 36.7Bod (2003) 90.70Carreras et al (2008) 91.1Re-scoringCollins (2000) 89.70Charniak and Johnson (2005) 91.02The parser of Charniak and Johnson 91.40 43.54Huang (2008) 91.69 43.5CombinationFossum and Knight (2009) 92.4Zhang et al (2009) 92.3Petrov (2010) 91.85 41.9Self-trainingZhang et al (2009) (s.t.+combo) 92.62Huang et al (2010) (single) 91.59 40.3Huang et al (2010) (combo) 92.39 43.1Our single 91.86 40.89Our combo 92.80 41.60Table 4: Performance comparison on the English test settion search algorithm (Kiefer, 1953).
The parame-ters ?
of each parsing model are estimated from atraining set using an averaged perceptron algorithm,following Collins (2002) and Huang (2008).The performance of our first- and higher-orderparsing models on all sentences of the two test setsis presented in Table 3, where ?
indicates a tunedbalance factor.
This parser is also combined withthe parser of Charniak and Johnson (2005)2 and theStanford.
parser3 The best combination results inTable 3 are achieved with k=70 for English andk=100 for Chinese for selecting the k-best parses.Our results are compared with the best previous oneson the same test sets in Tables 4 and 5.
All scores2ftp://ftp.cs.brown.edu/pub/nlparser/3http://nlp.stanford.edu/software/lex-parser.shtmlSystem F1(%) EX(%)SingleCharniak (2000) 80.85Stanford parser 78.47 26.44Berkeley parser 83.22 31.32Burkett and Klein (2008) 84.24CombinationZhang et al (2009) (combo) 85.45Our single 85.56 31.61Our combo 85.60 29.02Table 5: Performance comparison on the Chinese test setlisted in these tables are calculated with evalb,4and EX is the complete match rate.5 ConclusionThis paper has presented a higher-order model forconstituent parsing that factorizes a parse tree intolarger parts than before, in hopes of increasing itspower of discriminating the true parse from the oth-ers without losing tractability.
A performance gainof 0.3%-0.4% demonstrates its advantage over itsfirst-order version.
Including a PCFG-based modelas its basic feature, this model achieves a betterperformance than previous single and re-scoringparsers, and its combination with other parsers per-forms even better (by about 1%).
More importantly,it extends the existing works into a more generalframework of constituent parsing to utilize morelexical and structural context and incorporate morestrength of various parsing techniques.
However,higher-order constituent parsing inevitably leads toa high computational complexity.
We intend to dealwith the efficiency problem of our model with someadvanced parallel computing technologies in our fu-ture works.4http://nlp.cs.nyu.edu/evalb/4ReferencesE.
Black, S. Abney, D. Flickenger, R. Grishman, P. Har-rison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans,M.
Liberman, M. Marcus, S. Roukos, B. Santorini,and T. Strzalkowski.
1991.
A procedure for quanti-tatively comparing the syntactic coverage of Englishgrammars.
In Proceedings of DARPA Speech and Nat-ural Language Workshop, pages 306?311.Rens Bod.
2003.
An efficient implementation of a newDOP model.
In EACL 2003, pages 19?26.David Burkett and Dan Klein.
2008.
Two languagesare better than one (for syntactic parsing).
In EMNLP2008, pages 877?886.Xavier Carreras, Michael Collins, and Terry Koo.
2008.TAG, dynamic programming, and the perceptron forefficient, feature-rich parsing.
In CoNLL 2008, pages9?16.Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In EMNLP-CoNLL2007, pages 957?961.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In ACL 2005, pages 173?180.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In NAACL 2000, pages 132?139.Xiao Chen and Chunyu Kit.
2011.
Improving part-of-speech tagging for context-free parsing.
In IJCNLP2011, pages 1260?1268.Xiao Chen.
2012.
Discriminative Constituent Parsingwith Localized Features.
Ph.D. thesis, City Universityof Hong Kong.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In ACL2002, pages 263?270.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In ICML 2000, pages 175?182.Michael Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In EMNLP 2002, pages1?8.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In COLING1996, pages 340?345.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In ACL-HLT 2008, pages 959?967.Victoria Fossum and Kevin Knight.
2009.
Combiningconstituent parsers.
In NAACL-HLT 2009, pages 253?256.James Henderson.
2004.
Discriminative training of aneural network statistical parser.
In ACL 2004, pages95?102.Geoffrey E. Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural Com-putation, 14(8):1771?1800.Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010.Self-training with products of latent variable gram-mars.
In EMNLP 2010, pages 12?22.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In ACL-HLT 2008,pages 586?594.Mark Johnson.
2001.
Joint and conditional estimationof tagging and parsing models.
In ACL 2001, pages322?329.J.
Kiefer.
1953.
Sequential minimax search for a maxi-mum.
Proceedings of the American Mathematical So-ciety, 4:502?506.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In ACL 2010, pages 1?11.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In EACL 2006, pages 81?88.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In ACL 2005, pages 91?98.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In EMNLP-HLT2005, pages 523?530.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In NAACL-HLT 2007, pages404?411.Slav Petrov and Dan Klein.
2008a.
Discriminative log-linear grammars with latent variables.
In NIPS 20,pages 1?8.Slav Petrov and Dan Klein.
2008b.
Sparse multi-scalegrammars for discriminative latent variable parsing.
InEMNLP 2008, pages 867?876.Slav Petrov.
2010.
Products of random latent variablegrammars.
In NAACL-HLT 2010, pages 19?27.M.
J. D. Powell.
1964.
An efficient method for findingthe minimum of a function of several variables withoutcalculating derivatives.
Computer Journal, 7(2):155?162.Kenji Sagae and Alon Lavie.
2006.
Parser combinationby reparsing.
In NAACL-HLT 2006, pages 129?132.Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, andChristopher Manning.
2004.
Max-margin parsing.
InEMNLP 2004, pages 1?8.Hui Zhang, Min Zhang, Chew Lim Tan, and HaizhouLi.
2009.
K-best combination of syntactic parsers.In EMNLP 2009, pages 1552?1560.5
