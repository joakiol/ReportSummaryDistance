Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1329?1333,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsEveryone Likes Shopping!Multi-class Product Categorization for e-CommerceZornitsa KozarevaYahoo!
Labs701 First AvenueSunnyvale, CA 94089zornitsa@kozareva.comAbstractOnline shopping caters the needs of millionsof users on a daily basis.
To build an accuratesystem that can retrieve relevant products fora query like ?MB252 with travel bags?
onerequires product and query categorizationmechanisms, which classify the text asHome&Garden>Kitchen&Dining>KitchenAppliances>Blenders.
One of the biggestchallenges in e-Commerce is that providerslike Amazon, e-Bay, Google, Yahoo!
andWalmart organize products into differentproduct taxonomies making it hard andtime-consuming for sellers to categorizegoods for each shopping platform.To address this challenge, we propose anautomatic product categorization mechanism,which for a given product title assigns the cor-rect product category from a taxonomy.
Weconducted an empirical evaluation on 445, 408product titles and used a rich product taxon-omy of 319 categories organized into 6 lev-els.
We compared performance against mul-tiple algorithms and found that the best per-forming system reaches .88 f-score.1 Introduction and Related WorkOver the past decade, e-Commerce has rapidlygrown enabling customers to purchase any productwith a click of a button.
A key component for thesuccess of such online shopping platforms is theirability to quickly and accurately retrieve the desiredproducts for the customers.
To be able to do so,shopping platforms use taxonomies (Kanagal et al,2012), which hierarchically organize products fromgeneral to more specific classes.
Taxonomies sup-port keyword search and guarantee consistency ofthe categorization of similar products, which fur-ther enables product recommendation (Ziegler et al,2004; Weng et al, 2008) and duplicate removal.Shopping platforms like Amazon, e-Bay, Google,Yahoo!, Walmart among others use different tax-onomies to organize products making it hard andlabor-intensive for sellers to categorize the products.Sometimes sellers are encouraged to find similarproducts to those they sell and adopt this categoryto their products.
However, this mechanism leads totwo main problems: (1) it takes a lot of time for amerchant to categorize items and (2) such taggingscan be inconsistent since different sellers might cat-egorize the same product differently.
To solve theseproblems, ideally one would like to have an auto-mated procedure, which can classify any product ti-tle into a product taxonomy.
Such process will bothalleviate human labor and further improve productcategorization consistency in e-Commerce websites.Recently, a lot of interest has been developedaround the induction of taxonomies using hierarchalLDA models (Zhang et al, 2014) and the categoriza-tion of products using product descriptions (Chenand Warren, 2013).
Despite these efforts, yet nostudy focuses on classifying products using only ti-tles.
The question we address in this paper is: Givena product title and a product taxonomy, can we ac-curately identify the corresponding category (root-to-leaf path in the taxonomy) that the title belongsto?The main contributions of the paper are:?
We built multi-class classification algorithmthat classifies product titles into 319 distinctclasses organized in 6 levels.?
We conducted an empirical evaluation with445, 408 product titles and reach .88 f-score.1329?
During the error analysis we found out thatour algorithm predicted more specific and fine-grained categories compared to those providedby humans.2 Product Categorization Task DefinitionWe define our task as:Task Definition: Given a set of titles describing prod-ucts and a product taxonomy of 319 nodes organizedinto 6 levels, the goal is to build a multi-class classi-fier, which can accurately predict the product categoryof a new unlabeled product title.The algorithm takes as input a product title ?MB22B22 piece with bonus travel/storage bag?
and re-turns as output the whole product category hierarchy?Home and Garden >Kitchen&Dining>KitchenAppliances>Blenders?
as illustrated in Figure 1.!!!!!!!!!!!!!!!!!!
!MB22B!22!Piece!With!Bonus!Travel!/Storage!Bag!Home%&%Garden%>%%%Kitchen%&%Dining%>%%% %Kitchen%Appliances%>%% % % %Blenders%product((categoriza.on(Figure 1: Example of Product Title Categorization.3 Classification MethodsWe model the product categorization task as classi-fication problem, where for a given collection of la-beled training examples P , the objective is to learna classification function f : pi?
ci.
Here, piis aproduct title and ci?
{1, ...,K} is its correspondingcategory (one of 319 product taxonomy classes).We learn a linear classifier model f (parametrizedby a weight vector w) that minimizes the mis-classification error on the training corpus P :minw?pi?P?
(ci6= f(w, pi)) + ?||w||22where, ?(.)
is an indicator function which is 1 iff theprediction matches the true class and ?
is a regular-ization parameter.For our experiments, we used two multi-classification algorithms from the large scale ma-chine learning toolkit Vowpal Wabbit (Beygelzimeret al, 2009): one-against-all (OAA) and error cor-rection tournament (ECT).
OAA reduces the K-way multi-classification problem into multiple bi-nary classification tasks by iteratively classifyingeach product title for category K and comparingit against all other categories.
ECT also reducesthe problem to binary classification but employs asingle-elimination tournament strategy to comparea set of K players and repeats this process forO(logK) rounds to determine the multi-class label.4 Feature ModelingNext we describe the set of features we used to trainour model.4.1 Lexical InformationN-grams are commonly used features in text classi-fication.
As a baseline system, we use unigram andbigram features.4.2 Mutual Information DictionaryLexical features require very large amount of train-ing data to produce accurate predictions.
To gen-eralize the categorization models, we use seman-tic dictionaries, which capture the presence of aterm with a product category.
Ideally, we wouldlike to use existing dictionaries for each productcategory, however such information is not avail-able.
For instance, WordNet provides at most syn-onyms/hyponyms/hypernyms for a given categoryname, but it does not provide products, brand namesand the meaning of abbreviations.We decided to generate our own dictionaries, bytaking all product titles and estimating the mutual in-formation MI(w,Ci) = logf(w,Ci)(f(w,?
).f(?,Ci)of everyword w and product category Ci.
For the dictionary,we keep all word-category pairs with MI above 5.During feature generation, for each title we estimatethe percentage of words found with each categoryCiaccording to our automatically generated dictionary.The dimensions of the feature vector is equal to thetotal number of categories.
The size of the generatedlexicon is 34, 337 word-category pairs.4.3 LDA TopicsWe also incorporate latent information associatedwith product titles using topic modeling techniques.1330We learn latent topics corresponding to terms oc-curring in the titles using Latent Dirichlet Alloca-tion (David Blei and Jordan, 2003).
We capture themeaning of a title using the learned topic distribu-tion.
For our experimental setting, we use the MAL-LET (McCallum, 2002) implementation of LDA andbuild it in the following manner.Method: Given a set of titles and descriptions Dof products from different categories, find K la-tent topics.
The generative story is modeled as fol-lows:for each product category skwhere k ?
{1, ...,K} doGenerate ?skaccording to Dir(?
)end forfor each title i in the corpus D doChoose ?i?
Dir(?
)for each word wi,jwhere j ?
{1, ..., Ni} doChoose a topic zi,j?Multinomial(?i)Choose a word wi,j?Multinomial(?zi,j)end forend forInference: We perform inference on this model us-ing collapsed Gibbs sampling, where each of thehidden sense variables zi,jare sampled conditionedon an assignment for all other variables, while inte-grating over all possible parameter settings (Griffithsand Steyvers, 2002).
We set the hyperparameter ?
tothe default value of 0.01 and ?=50.
During featuregeneration, we take all words in the title and estimatethe percentage of words associated with each topicsk.
The topic-word mapping is constructed from theword distribution learnt for a given topic.
The num-ber of features is equal to the number of topics.Figure 2 shows an example of the different topicsassociated with the word bag for different producttitles.T"Sac&Disposable&Paper&Filter&Tea&Bags,&Size&2,&100"Count&SKB&Mixer&Bag&for&Powered&Mackie&mixers&&NauEca&Baby"Girls&Infant&Printed&Paper&Bag&Waist&Dress&&t22't13't59'Figure 2: Learnt Topic Assignments for bag.4.4 Neural Network EmbeddingsWhile LDA allows us to capture the latent topicsof the product titles, recent advances in unsuper-vised algorithms have demonstrated that deep neu-ral network architectures can be effective in learningsemantic representation of words and phrases fromlarge unlabeled corpora.To model the semantic representations of producttitles, we learn embeddings over the corpus P usingthe technique of (Mikolov et al, 2013a; Mikolov etal., 2013b).
We use a feedforward neural networkarchitecture in which the training objective is to findword (vector) representations that are useful for pre-dicting the current word in a product title based onthe context.
Formally, given a sequence of trainingwords w1, w2, ..., wTthe objective is to maximizethe average log probability1TT?t=1?
?n?j?n,j 6=0log p(wt|wt+j)where n is the size of the training context andp(wt|wt+j) predicts the current position wtusingthe surrounding context words wt+jand learnedwith hierarchical softmax algorithm.Since word2vec provides embeddings only forwords or at most two word phrases, to represent aproduct title p containing a sequence of M word to-kens (w1, ..., wM), we retrieve the embeddings of allwords and take the average score.p = [e1, ..., ed]where, ei=1MM?j=1eiwjHere, d is the embedding vector size, eiand eiwjare the vector values at position i for the product pand word wjin p, respectively.To build the embeddings, we use a vector size of200 and context of 5 consecutive words in our exper-iments.
We then use the new vector representation[e1, ..., ed] (d features per title) to train and test themachine learning model.5 Data DescriptionTo conduct our experimental studies, we have usedand manually annotated product titles from Yahoo?sshopping platform.
For each title, we asked two an-notators to provide the whole product category fromthe root to the leaf and used these annotations as agold standard.We split the data into a training set of 353, 809examples and a test set of 91, 599 examples.
Our1331product taxonomy consists of 6 hierarchical levels.Figure 3 shows the total number of categories perlevel.
The highest density is at levels 3 and 4.!!!8!
!
!
!1!!!31!
!
!2!!!93!
!
!3!!137!
!
!4!!!49!
!
!5!!!!1!!
!
!6!!!!8!
!
!
!1!!!31!
!
!2!!!93!
!
!3!!137!
!
!4!!!49!
!
!5!!!!1!!
!
!6!levels!
#categories!Figure 3: Product Taxonomy.6 Experiments and ResultsIn this section, we describe the evaluation metric andthe sets of experiments we have conducted.6.1 Evaluation MetricTo evaluate the performance of the product catego-rization algorithms, we calculate f-score on the testset.
The results are on exact match from top-to-leafpath of the gold and predicted categories.6.2 ResultsTable 1 shows the obtained results.
For each fea-ture we report the performance of the two machinelearning algorithms one-against-all (OAA) and errorcorrecting tournament (ECT).features OAA ECTunigram .72 .63unigram+bigram .67 .58MI Dictionary .85 .77LDA Dictionary .79 .67NN-Embeddings .88 .80Table 1: Results on Product Categorization.The highest performance is achieved with the neu-ral network embedding representation.
Between thetwo classifiers one-against-all consistently achievedthe highest scores for all different feature sets.
Wealso studied various feature combinations, howeverembeddings reached the highest performance.6.3 Error AnalysisWe analyzed the produced outputs and noticed thatsometimes the predicted category could be differentfrom the gold one, but often the predicted categorywas semantically similar or more descriptive than??
flat$slat$sleigh$crib$espresso$8022n$furniture(>(baby(&(toddler(furniture(>(cribs(&(toddler(beds(baby(&(toddler(GOLD(PREDICTED(??
angel$line$flat$slat$sleigh$changer$w/drawer$$natural$8583$furniture(>(baby(&(toddler(furniture(>(cribs(&(toddler(beds(GOLD(??
cabela's()pped(berber(camo(comfy(cup(??
carolina(pet(company(large(faux(suede(&()pped(berber(round(comfy(cup(green(animals'&'pet'supplies'>'pet'supplies'>'small'animal'supplies'>'small'animal'bedding'GOLD'animals'&'pet'supplies'>'pet'supplies'>'dog'supplies'>'dog'beds'GOLD'PREDICTED'animals'&'pet'supplies'>'pet'supplies'>'small'animal'supplies'>'small'animal'bedding'??
aprica&side&carrier&bou-que&pink&??
julie&brown&girl's&jersey&tunic/pink&9&pink&apparel& &accessories&>&clothing&>&shirts& &tops&GOLD&baby& &toddler&>&baby&transport&>&baby&carriers&GOLD&apparel& &accessories&>&clothing&>&shirts& &tops&PREDICTED&Figure 4: Examples of Categorized Products.those provided by humans.
Figure 4 shows someexamples of the errors we discovered.For instance, the title cabela?s tipped beer comocomfy cup was classified as Small Animal Bedding,while the gold standard category was Dog Beds.
Inour case we penalized such predictions, but still thetwo top level categories of Animals and Pet Sup-plies are similar.
The major difference between theprediction and gold label is that the humans anno-tated bed as belonging to Dog Beds, while our al-gorithm predicted it as Small Animal Bedding.
Dur-ing manual inspection, we also noticed that often ourclassifier produces more descriptive categories com-pared to humans.
For example, flat slat sleigh cribespresso 8022n had gold category Baby & Toddler,while our algorithm correctly identified the more de-scriptive category Cribs and Toddler Beds.7 ConclusionsIn this paper we have presented the first product cat-egorization algorithm which operates on product ti-tle level.
We classified products into a taxonomyof 319 categories organized into a 6 level taxon-omy.
We collected data for our experiments andconducted multiple empirical evaluations to studythe effect of various features.
Our experimentsshowed that neural network embeddings lead to thebest performance reaching .88 f-score.
We man-ually inspected the produced classification outputsand found that often the predicted categories aremore specific and fine-grained compared to thoseprovided by humans.1332AcknowledgmentsWe would like to thank the anonymous reviewers fortheir useful feedback and suggestions.ReferencesAlina Beygelzimer, John Langford, and Pradeep Raviku-mar.
2009.
Error-correcting tournaments.
In Proceed-ings of the 20th International Conference on Algorith-mic Learning Theory, ALT?09, pages 247?262.Jianfu Chen and David Warren.
2013.
Cost-sensitivelearning for large-scale hierarchical classification.
InProceedings of the 22Nd ACM International Confer-ence on Conference on Information &#38; KnowledgeManagement, CIKM ?13, pages 1351?1360.Andrew Ng David Blei and Michael Jordan.
2003.
La-tent dirichlet alocation.
Journal of Machine Leaning,3:993?1022.Thomas L Griffiths and Mark Steyvers.
2002.
A prob-abilistic approach to semantic representation.
In Pro-ceedings of the Twenty-Fourth Annual Conference ofCognitive Science Society.Bhargav Kanagal, Amr Ahmed, Sandeep Pandey,Vanja Josifovski, Jeff Yuan, and Lluis Garcia-Pueyo.2012.
Supercharging recommender systems using tax-onomies for learning user purchase behavior.
Proc.VLDB Endow., 5(10):956?967, June.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Tomas Mikolov, Kai Chen, Greg Corrado, and Jeff Dean.2013a.
Efficient estimation of word representations invector space.
CoRR.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013b.
Distributed represen-tations of words and phrases and their compositional-ity.
CoRR, abs/1310.4546.Li-Tung Weng, Yue Xu, Yuefeng Li, and Richi Nayak.2008.
Exploiting item taxonomy for solving cold-startproblem in recommendation making.
In ICTAI (2),pages 113?120.
IEEE Computer Society.Yuchen Zhang, Amr Ahmed, Vanja Josifovski, andAlexander Smola.
2014.
Taxonomy discovery forpersonalized recommendation.
In Proceedings of the7th ACM International Conference on Web Search andData Mining, WSDM ?14, pages 243?252.Cai-Nicolas Ziegler, Georg Lausen, and Lars Schmidt-Thieme.
2004.
Taxonomy-driven computation ofproduct recommendations.
In Proceedings of the Thir-teenth ACM International Conference on Informationand Knowledge Management, CIKM ?04, pages 406?415.1333
