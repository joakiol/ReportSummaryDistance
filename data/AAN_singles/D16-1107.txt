Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1025?1029,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsFluency detection on communication networksTom Lippincott and Benjamin Van DurmeHuman Language Technology Center of ExcellenceJohns Hopkins Universitytom@cs.jhu.edu, vandurme@cs.jhu.eduAbstractWhen considering a social media corpus, weoften have access to structural informationabout how messages are flowing between peo-ple or organizations.
This information is par-ticularly useful when the linguistic evidenceis sparse, incomplete, or of dubious quality.In this paper we construct a simple model toleverage the structure of Twitter data to helpdetermine the set of languages each user is flu-ent in.
Our results demonstrate that imposingseveral intuitive constraints leads to improve-ments in performance and stability.
We re-lease the first annotated data set for exploringthis task, and discuss how our approach maybe extended to other applications.1 IntroductionLanguage identification (LID) is an important firststep in many NLP pipelines since most downstreamtasks need to employ language-specific resources.In many situations, LID is a trivial task that can beaddressed e.g.
by a simple Naive Bayes classifiertrained on word and character n-gram data (Lui andBaldwin, 2012): a document of significant lengthwill be quickly disambiguated based on its vocab-ulary (King et al, 2014).
However, social mediaplatforms like Twitter produce data sets in whichindividual documents are extremely short, and lan-guage use is idiosyncratic: LID performance onsuch data is dramatically lower than on traditionalcorpora (Bergsma et al, 2012; Carter et al, 2013).The widespread adoption of social media throughoutthe world amplifies the problem as less-studied lan-guages lack the annotated resources needed to trainthe most effective NLP models (e.g.
treebanks forstatistical parsing, tagged corpora for part-of-speechtagging, etc).
All of this motivates the research com-munity?s continued interest in LID (Zampieri et al,2014).Tweet #1 ???
?????????
?
??
??
?????
?Tweet #2 omg favourite day of the week!Table 1: Multilingual social media users often communicate indifferent languages depending on their intended audience, suchas with these Russian and English tweets posted by the sameTwitter accountIn this paper, we consider the closely-related taskof determining an actor?s fluencies, the set of lan-guages they are capable of speaking and understand-ing.
The observed language data will be the same asfor LID, but is now considered to indicate a latentproperty of the actor.
This information has a num-ber of downstream uses, such as providing a strongprior on the language of the actor?s future communi-cations, constructing monolingual data sets, and rec-ommending appropriate content for display or fur-ther processing.This paper also focuses on the situation wherea very small amount of content has been observedfrom the particular user.
While this may seemstrange considering the volume of data generated bysocial media, this is dominated by particularly activeusers: for example, 30% of Twitter users post onlyonce per month (Leetaru et al, 2013).
This content-starved situation is exacerbated by certain use-cases,such as responding to emergency events where sud-den focus is directed at a particular location, or fo-cusing on new users with shallow histories.10252 Previous WorkTwitter and other social media platforms are a ma-jor area of ongoing NLP research, including dedi-cated workshops(NAA, 2015; ACL, 2014).
Previ-ous work has considered macroscopic properties ofthe entire Twitter network (Gabielkov et al, 2014),and pondered whether it is an ?information?
or ?so-cial?
network (Myers et al, 2014).
Studies have fo-cused on determining user attributes such as gender(Li et al, 2015), political allegiance (Volkova et al,2014), brand affinity (Pennacchiotti and Popescu,2011a), sentiment analysis (West et al, 2014), andmore abstract roles (Beller et al, 2014).
Such demo-graphic information is known to help downstreamtasks (Hovy, 2015).
Research involving social mediacommunication networks has typically focused onhomophily, the tendency of users to connect to oth-ers with similar properties (Barbera?, 2014).
A num-ber of papers have employed features drawn fromboth the content and structure of network entities inpursuit of latent user attributes (Pennacchiotti andPopescu, 2011b; Campbell et al, 2014; Suwan etal., 2015).3 DefinitionsWe refer to the entities that produce and consumecommunications as Actors, and the communications(packets of language data) as Messages.
Each mes-sage occurs in a particular Language, and each actorhas a set of Fluencies, representing the ability to pro-duce and consume a message in a given language.We refer to a connected graph of such entities asa Communication Network.
For Twitter data, mes-sages are simply associated with a single actor, whois in turn associated with other actors via the ?fol-lowing?
relationship, the actor?s ?friends?
in Twit-ter?s terminology.1 We assume each message (tweet)is written in a single language, and actors are eitherfluent or not in each possible language.1Note that, confusingly, Twitter?s ?friend?
relationship is notsymmetric: Mary?s friends are users she has decided to follow,and not necessarily vise-versa.4 Twitter Data SetTo build a suitable data set2 for fluency detection,we first identified 1000 Twitter users who, accord-ing to the Twitter LID system, have tweeted in Rus-sian and at least one additional language.
For eachof these ?seed?
users, we gather a local context(a ?snowflake?)
as follows: we choose 20 of theirfriends at random.
For each of these friends, wechoose 15 of their friends (again, at random).
Fi-nally, we randomly pull 200 tweets for each identi-fied user.
The data set consists of 989 seed users,165,042 friends, and 55,019,811 tweets.
We pre-serve all Twitter meta-data for the users and tweets,such as location, follower count, hashtags, etc,though for the purposes of this paper we are onlyinterested in the friendship structure and messagetext.
We then had an annotator determine the setof languages each of the 1000 seed users is fluentin.
For each seed user, the annotator was presentedwith their 200 tweets, grouped by Twitter languageID, and was asked to 1) flag users that appear to bebots and 2) list the languages they believe the useris fluent in.
These steps are reflected in Figure 4.Over 50% (507) of the users were flagged as pos-sible bots and not used in this study.
The remaining482 were observed employing 7 different languages:Russian, Ukrainian, German, Polish, Bulgarian, Lat-vian, and English.
At most, a single user was foundto be fluent in three languages.Figure 1: Structure of one snowflake in the Twitter Fluencydata set.5 Structure-Aware Fluency ModelOur goal was to explicitly model each actor?s flu-ency in different languages, using a model with sim-2The full data set is available at www.university.edu/link1026ple, interpretable parameters that can be used to en-code well-motivated assumptions about the data.
Inparticular, we want to bias the model towards thebelief that actors typically speak a small number oflanguages, and encode the belief that all actors par-ticipating in a message are highly likely to be fluentin its language.
Our basic hypothesis is that, in ad-dition to scores from traditional LID modules, sucha model will benefit from considering the behaviorof an actor?s interlocutors.
To test this, we designeda model that employs scores from an existing LIDsystem, and compare performance with and withoutawareness of the communication network structure.To demonstrate the effectiveness of the model in sit-uations with sparse or unreliable linguistic content,we perform experiments where the number of mes-sages associated with each actor has been randomlydown-sampled.Linear Programming Linear Programming (LP)is a method for specifying constraints and cost func-tions in terms of linear relationships between vari-ables, and then finding the optimal solution that re-spects the constraints.
The restriction to linear equa-tions ensures that the objective function is itself lin-ear, and can be efficiently solved.
If some or allvariables are restricted to take discrete values, re-ferred to as (Mixed) Integer Linear Programming(ILP), finding a solution becomes NP-hard, thoughcommon special cases remain efficiently solvable.We specify our model as an ILP with the hopethat it provides sufficient expressiveness for thetask, while remaining intuitive and tractable.
Infer-ence is performed using the Gurobi modeling toolkit(Gurobi Optimization, 2015).Model definition Given a communication net-work with no LID information, ideally we wouldlike to determine the language of each message, andthe set of languages each actor is fluent in.
Ini-tially, we assume access to a probablistic LID sys-tem that maps unicode text to a distribution overpossible languages.
We use the following notation:A1:T and M1:U are the actors and messages, respec-tively.
F (ai) is a binary vector indicating which lan-guages we believe actor ai is fluent in.
L(mi) is aone-on binary vector indicating which language webelieve message mi is written in.
P (mi) is the set ofactors participating in message mi: for Twitter data,where messages are (usually) not directed at specificusers, we treat a user and the users?
friends as par-ticipants.
LID(mi) is a real vector representing theprobability of message mi being in each language,according to the LID system.To build our ILP model, we iterate over actorsand messages, defining constraints and the objectivefunction as we go.
There are two types of structuralconstraints: first, we restrict each message to have asingle language assignment:?L(mi) = 1 (1)Second, we ensure that all actors participating ina given message are fluent in its language:?a ?
P (mi), L(mi)?
F (a) = 1 (2)The objective function also has two components:first, the language fit encourages the model to assigneach message a language that has high probabilityaccording the the LID system:LF =?m?ML(m)?
LID(m) (3)Second, the structure fit minimizes the cardinal-ity of the actors?
fluency sets (subject to the struc-tural constraints), and thus avoids the trivial solutionwhere each actor is fluent in all languages:SF = ?
?a?A?F (a) (4)Finally, the two components of the objective func-tion are combined with an empirically-determinedlanguage weight to get the complete objective func-tion:LW?
LF + (1.0?
LW)?
SF (5)Note that these are not all linear relationships:in particular, the multiplication operator cannot beused in ILP when the operands are both variables,as in equation 2.
There are however techniques thatcan represent these situations in a linear program byintroducing helper variables and constraints (Biss-chop, 2015).1027Language Identification Scores and FluencyBaseline To get LID scores, we ran the VaLID sys-tem (Bergsma et al, 2012) on each message, andnormalize the output into distributions over 261 pos-sible languages.
VaLID is trained on Wikipedia data(i.e.
out-of-domain relative to Twitter), althoughit does employ hand-specified rules for sanitizingtweet text, such as normalizing whitespace and re-moving URLs and user tags.
VaLID uses a data-compression approach that is competitive with Twit-ter?s in-house LID, despite no consideration of geo-graphic or user priors.
These language scores areused in the structure-aware model to compute thelanguage fit.Because VaLID makes no use of the communi-cation network structure, we also use its scores tocreate a baseline structure-unaware fluency model.To get structure-unaware baseline scores for the flu-ency identification task, we average the LID distri-butions for each actor?s messages and consider themfluent in a language if its probability is above anempirically-determined threshold.Tuning parameters We empirically determine thethresholds for the baseline model and the languageweights for the structure-aware model via a simplegrid search, repeated 100 times.
We randomly splitthe data into 20%/80% tune/test sets, and evaluatefilter thresholds and language weights from 0 to 1in .01 increments, with messages per actor rangingbetween 1 and 10.
We expected the baseline modelto have a consistent optimal threshold (though withhigher performance variance with fewer messages),and this was borne out with optimal performance ata threshold of 0.06, independent of the number ofmessages per actor.
For the structure-aware model,the optimal language weight was 0.9, although theentire range from 0.1?0.9 showed similar perfor-mance.
This result was surprising, as we expect thestructure-aware model to rely heavily on the struc-tural fit when the number of messages is small, andon the language fit when the number is large.
Thistrend doesn?t emerge because the structural fit actu-ally relies on the language fit to make assignmentsfor the seed actor?s friends and their messages.Figure 2: Performance of baseline and structure-aware modelsas a function of the number of messages per actor used as ev-idence.
Each bar represents the average over 100 random tun-ing/testing splits, with whiskers showing the standard deviation.6 Results and discussionFigure 2 compares the performance3 of thestructure-aware ILP model with the baseline modelas a function of the number of messages per ac-tor, using the empirically-determined threshold andlanguage weight.
At the left extreme, the modelsonly have a single, randomly-selected message fromeach actor.
As this number increases, the baselinemodel improves as it becomes more likely to haveseen enough messages to reflect the actor?s full spec-trum of language use.
The structure-aware model isable to make immediate use of the actor?s friends,immediately reaching high performance even whenthe language data is very sparse.
Its most frequenttype of error is over-hypothesizing fluency in bothUkrainian and Russian, when the user is in factmonolingual, followed by incorrectly hypothesizingfluency in English.
This is understandable given thesimilarity of the languages in the former case, andthe popularity of English expressions, titles, and thelike in the latter.7 ConclusionWe have presented promising results from lever-aging structural information from a communica-tion network to improve performance on fluencydetection in situations where direct linguistic datais sparse.
In addition to defining the task itself,3F-score calculated based on correct and hypothesizedfluency-assignments for each actor.1028we release an annotated data set for training andevaluating future models.
Planned future work in-cludes a more flexible decoupling of the languageand structure fits (in light of Section 5), and mov-ing from pre-existing LID systems to joint modelswhere LID scores are directly informed by structuralinformation.References2014.
ACL Joint Workshop on Social Dynamics and Per-sonal Attributes in Social Media.Pablo Barbera?.
2014.
Birds of the same feather tweettogether: Bayesian ideal point estimation using twitterdata.
Political Analysis, 23:76?91.Charley Beller, Rebecca Knowles, Craig Harman, ShaneBergsma, Margaret Mitchell, and Benjamin VanDurme.
2014.
I?m a belieber: Social roles via self-identification and conceptual attributes.
In Proceed-ings of the 52rd Annual Meeting of the Associationfor Computational Linguistics, pages 181?186, Balti-more, Maryland, USA.Shane Bergsma, Paul McNamee, Mossaab Bagdouri,Clayton Fink, and Theresa Wilson.
2012.
Languageidentification for creating language-specific Twittercollections.
In Proc.
Second Workshop on Languagein Social Media, pages 65?74.Johannes Bisschop.
2015.
Aimms optimization model-ing.W.M.
Campbell, E. Baseman, and K. Greenfield.
2014.langid.py: An off-the-shelf language identificationtool.
In Proceedings of the Second Workshop on Natu-ral Language Processing for Social Media, pages 59?65, Dublin, Ireland.Simon Carter, Wouter Weerkamp, and Manos Tsagkias.2013.
Microblog language identification: Overcomingthe limitations of short, unedited and idiomatic text.Lang.
Resour.
Eval., 47(1):195?215, March.Maksym Gabielkov, Ashwin Rao, and Arnaud Legout.2014.
Studying social networks at scale: Macroscopicanatomy of the twitter social graph.
In SIGMETRICS?14, Austin, Texas, USA.Inc.
Gurobi Optimization.
2015.
Gurobi optimizer refer-ence manual.Dirk Hovy.
2015.
Demographic factors improve clas-sification performance.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics, pages 752?762, Beijing, China.Ben King, Dragomir Radev, and Steven Abney.
2014.Experiments in sentence language identification withgroups of similar languages.
In Proceedings of theFirst Workshop on Applying NLP Tools to SimilarLanguages, Varieties and Dialects, pages 146?154,Dublin, Ireland.Kalev Leetaru, Shaowen Wang, Guofeng Cao, AnandPadmanabhan, and Eric Shook.
2013.
Mapping theglobal twitter heartbeat: The geography of twitter.First Monday, 18(5).Shoushan Li, Jingjing Wang, Guodong Zhou, and Hanx-iao Shi.
2015.
Interactive gender inference with in-teger linear programming.
In Proceedings of the 24thInternational Conference on Artificial Intelligence, IJ-CAI?15, pages 2341?2347.
AAAI Press.Marco Lui and Timothy Baldwin.
2012. langid.py: Anoff-the-shelf language identification tool.
In Proceed-ings of the 50th Annual Meeting of the Association forComputational Linguistics, pages 25?30, Jeju, Repub-lic of Korea.Seth A. Myers, Aneesh Sharma, Pankaj Gupta, andJimmy Lin.
2014.
Information network or social net-work?
the structure of the twitter follow graph.
InWWW ?14 Companion, Seoul, Korea.2015.
NAACL International Workshop on Natural Lan-guage Processing for Social Media.Marco Pennacchiotti and Ana-Maria Popescu.
2011a.Democrats, republicans and starbucks afficionados:User classification in twitter.
In KDD ?11, San Diego,California, USA.Marco Pennacchiotti and Ana-Maria Popescu.
2011b.
Amachine learning approach to twitter user classifica-tion.
In Proceedings of the Fifth International AAAIConference on Weblogs and Social Media, pages 281?288.Shakira Suwan, Dominic Lee, and Carey Priebe.
2015.Bayesian vertex nomination using content and context.WIREs Comput Stat, 7:400?416.Svitlana Volkova, Glen Coppersmith, and Benjamin VanDurme.
2014.
Inferring user political preferencesfrom streaming communications.
In Proceedings ofthe 52rd Annual Meeting of the Association for Com-putational Linguistics, pages 186?196, Baltimore,Maryland, USA.Robert West, Hristo Paskov, Jure Leskovec, and Christo-pher Potts.
2014.
Exploiting social network struc-ture for person-to-person sentiment analysis.
Transac-tions of the Association for Computational Linguistics,2:297?310.Marcos Zampieri, Liling Tang, Nikola Ljubes?ic?, and Jo?rgTiedemann.
2014.
Discriminating similar languagesshared task at coling 2014.1029
