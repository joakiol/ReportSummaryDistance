Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 413?417,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsIssues Concerning Decoding with Synchronous Context-free GrammarTagyoung Chung, Licheng Fang and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractWe discuss some of the practical issues thatarise from decoding with general synchronouscontext-free grammars.
We examine problemscaused by unary rules and we also examinehow virtual nonterminals resulting from bina-rization can best be handled.
We also inves-tigate adding more flexibility to synchronouscontext-free grammars by adding glue rulesand phrases.1 IntroductionSynchronous context-free grammar (SCFG) iswidely used for machine translation.
There are manydifferent ways to extract SCFGs from data.
Hiero(Chiang, 2005) represents a more restricted form ofSCFG, while GHKM (Galley et al, 2004) uses a gen-eral form of SCFG.In this paper, we discuss some of the practical is-sues that arise from decoding general SCFGs thatare seldom discussed in the literature.
We focus onparsing grammars extracted using the method putforth by Galley et al (2004), but the solutions tothese issues are applicable to other general forms ofSCFG with many nonterminals.The GHKM grammar extraction method producesa large number of unary rules.
Unary rules are therules that have exactly one nonterminal and no ter-minals on the source side.
They may be problematicfor decoders since they may create cycles, which areunary production chains that contain duplicated dy-namic programming states.
In later sections, we dis-cuss why unary rules are problematic and investigatetwo possible solutions.GHKM grammars often have rules with manyright-hand-side nonterminals and require binariza-tion to ensure O(n3) time parsing.
However, bina-rization creates a large number of virtual nontermi-nals.
We discuss the challenges of, and possible so-lutions to, issues arising from having a large num-ber of virtual nonterminals.
We also compare bina-rizing the grammar with filtering rules according toscope, a concept introduced by Hopkins and Lang-mead (2010).
By explicitly considering the effectof anchoring terminals on input sentences, scope-3 rules encompass a much larger set of rules thanChomsky normal form but they can still be parsed inO(n3) time.Unlike phrase-based machine translation, GHKMgrammars are less flexible in how they can seg-ment sentence pairs into phrases because they arerestricted not only by alignments between words insentence pairs, but also by target-side parse trees.
Ingeneral, GHKM grammars suffer more from datasparsity than phrasal rules.
To alleviate this issue,we discuss adding glue rules and phrases extractedusing methods commonly used in phrase-based ma-chine translation.2 Handling unary rulesUnary rules are common in GHKM grammars.
Weobserved that as many as 10% of the rules extractedfrom a Chinese-English parallel corpus are unary.Some unary rules are the result of alignment er-rors, but other ones might be useful.
For example,Chinese lacks determiners, and English determinersusually remain unaligned to any Chinese words.
Ex-tracted grammars include rules that reflect this fact:NP ?
NP, the NPNP ?
NP, a NP413However, unary rules can be problematic:?
Unary production cycles corrupt the translationhypergraph generated by the decoder.
A hyper-graph containing a unary cycle cannot be topo-logically sorted.
Many algorithms for parame-ter tuning and coarse-to-fine decoding, such asthe inside-outside algorithm and cube-pruning,cannot be run in the presence of unary cycles.?
The existence of many unary rules of the form?NP ?
NP, the NP?
quickly fills a pruning binwith guesses of English words to insert withoutany source-side lexical evidence.The most obvious way of eliminating problem-atic unary rules would be converting grammars intoChomsky normal form.
However, this may resultin bloated grammars.
In this section, we presenttwo different ways to handle unary rules.
The firstinvolves modifying the grammar extraction method,and the second involves modifying the decoder.2.1 Modifying grammar extractionWe can modify the grammar extraction method suchthat it does not extract any unary rules.
Galley et al(2004) extracts rules by segmenting the target-sideparse parse tree based on frontier nodes.
We modifythe definition of a frontier node in the following way.We label frontier nodes in the English parse tree, andexamine the Chinese span each frontier node cov-ers.
If a frontier node covers the same span as thefrontier node that immediately dominates it, then thedominated node is no longer considered a frontier.This modification prevents unary rules from beingextracted.Figure 1 shows an example of an English-Chinesesentence pair with the English side automaticallyparsed.
Frontier nodes in the tree in the originalGHKM rule extraction method are marked with abox.
With the modification, only the top bold-faced NP would be considered a frontier node.
TheGHKM rule extraction results in the following rules:NPB ???
?, the snowy egretNP ?
NPB, NPBPP ?
NP, with NPNP ?
PP, romance PPWith the change, only the following rule is extracted:NPNPBNNPromancePPINwithNPNPBDTtheJJsnowyNNegret??
?
?
?Figure 1: A sentence fragment pair with erroneous align-ment and tokenizationNP ???
?, romance with the snowy egretWe examine the effect of this modification has ontranslation performance in Section 5.2.2 Modifying the decoderModifying how grammars are extracted has an ob-vious down side, i.e., the loss of generality.
In theprevious example, the modification results in a badrule, which is the result of bad alignments.
Beforethe modification, the rule set includes a good rule:NPB ???
?, the snowy egretwhich can be applied at test time.
Because of this,one may still want to decode with all available unaryrules.
We handle unary rules inside the decoder inthe following ways:?
Unary cycle detectionThe na?ve way to detect unary cycles is back-tracking on a unary chain to see if a newly gen-erated item has been generated before.
The run-ning time of this is constrained only by the num-ber of possible items in a chart span.
In prac-tice, however, this is often not a problem: if allunary derivations have positive costs and a pri-ority queue is used to expand unary derivations,414only the best K unary items will be generated,where K is the pruning constant.?
Ban negative cost unary rulesWhen tuning feature weights, an optimizer maytry feature weights that may give negative coststo unary productions.
This causes unary deriva-tions to go on forever.
The solution is to seta maximum length for unary chains, or to bannegative unary productions outright.3 Issues with binarization3.1 Filtering and binarizationSynchronous binarization (Zhang et al, 2006) isan effective method to reduce SCFG parsing com-plexity and allow early language model integration.However, it creates virtual nonterminals which re-quire special attention at parsing time.
Alternatively,we can filter rules that have more than scope-3 toparse in O(n3) time with unbinarized rules.
Thisrequires Earley (Earley, 1970) style parsing, whichdoes implicit binarization at decoding time.
Scope-filtering may filter out unnecessarily long rules thatmay never be applied, but it may also throw outrules with useful contextual information.
In addi-tion, scope-filtering does not accommodate early lan-guage model state integration.
We compare the twowith an experiment.
For the rest of the section, wediscuss issues created by virtual nonterminals.3.2 Handling virtual nonterminalsOne aspect of grammar binarization that is rarelymentioned is how to assign probabilities to binarizedgrammar rules.
The na?ve solution is to assign prob-ability one to any rule whose left-hand side is a vir-tual nonterminal.
This maintains the original model.However, it is generally not fair to put chart items ofvirtual nonterminals and those of regular nontermi-nals in the same bin, because virtual items have arti-ficially low costs.
One possible solution is adding aheuristic to push up the cost of virtual items for faircomparison.For our experiments, we use an outside estimateas a heuristic for a virtual item.
Consider the follow-ing rule binarization (only the source side shown):A ?
BCD : ?
log(p) ?
V ?
BC : 0A ?
VD : ?
log(p)A ?
BCD is the orginal rule and ?
log(p) is the costof the rule.
In decoding time, when a chart item isgenerated from the binarized rule V ?
BC, we add?
log(p) to its total cost as an optimistic estimate ofthe cost to build the original unbinarized rule.
Theheuristic is used only for pruning purposes, and itdoes not change the real cost.
The idea is similarto A* parsing (Klein and Manning, 2003).
One com-plication is that a binarized rule can arise from multi-ple different unbinarized rules.
In this case, we pickthe lowest cost among the unbinarized rules as theheuristic.Another approach for handling virtual nontermi-nals would be giving virtual items separate bins andavoiding pruning them at all.
This is usually notpractical for GHKM grammars, because of the largenumber of nonterminals.4 Adding flexibility4.1 Glue rulesBecause of data sparsity, an SCFG extracted fromdata may fail to parse sentences at test time.
Forexample, consider the following rules:NP ?
JJ NN, JJ NNJJ ?
c1, e1JJ ?
c2, e2NN ?
c3, e3This set of rules is able to parse the word sequencec1 c3 and c2 c3 but not c1 c2 c3, if we have not seen?NP ?
JJ JJ NN?
at training time.
Because SCFGsneither model adjunction, nor are they markovized,with a small amount of data, such problems can oc-cur.
Therefore, we may opt to add glue rules as usedin Hiero (Chiang, 2005):S ?
C, CS ?
S C, S Cwhere S is the goal state and C is the glue nonter-minal that can produce any nonterminals.
We re-fer to these glue rules as the monotonic glue rules.We rely on GHKM rules for reordering when we usethe monotonic glue rules.
However, we can also al-low glue rules to reorder constituents.
Wu (1997)presents a better-constrained grammar designed toonly produce tail-recursive parses.
See Table 1 forthe complete set of rules.
We refer to these rules asABC glue rules.
These rules always generate left-415S ?
A A ?
[A B] B ?
?
B A ?S ?
B A ?
[B B] B ?
?
A A ?S ?
C A ?
[C B] B ?
?
C A ?A ?
[A C] B ?
?
B C ?A ?
[B C] B ?
?
A C ?A ?
[C C] B ?
?
C C ?Table 1: The ABC Grammar.
We follow the conventionof Wu (1997) that square brackets stand for straight rulesand angle brackets stand for inverted rules.heavy derivations, weeding out ambiguity and mak-ing search more efficient.
We learn probabilities ofABC glue rules by using expectation maximization(Dempster et al, 1977) to train a word-level Inver-sion Transduction Grammar from data.In our experiments, depending on the configura-tion, the decoder failed to parse about 5% of sen-tences without glue rules, which illustrates their ne-cessity.
Although it is reasonable to believe that re-ordering should always have evidence in data, aswith GHKM rules, we may wish to reorder basedon evidence from the language model.
In our ex-periments, we compare the ABC glue rules with themonotonic glue rules.4.2 Adding phrasesGHKM grammars are more restricted than thephrase extraction methods used in phrase-basedmodels, since, in GHKM grammar extraction,phrase segmentation is constrained by parse trees.This may be a good thing, but it suffers from lossof flexibility, and it also cannot use non-constituentphrases.
We use the method of Koehn et al (2003)to extract phrases, and, for each phrase, we add arule with the glue nonterminal as the left-hand sideand the phrase pair as the right-hand side.
We exper-iment to see whether adding phrases is beneficial.There have been other efforts to extend GHKMgrammar to allow more flexible rule extraction.
Gal-ley et al (2006) introduce composed rules whereminimal GHKM rules are fused to form larger rules.Zollmann and Venugopal (2006) introduce a modelthat allows more generalized rules to be extracted.BLEUBaseline + monotonic glue rules 20.99No-unary + monotonic glue rules 23.83No-unary + ABC glue rules 23.94No-unary (scope-filtered) + monotonic 23.99No-unary (scope-filtered) + ABC glue rules 24.09No-unary + ABC glue rules + phrases 23.43Table 2: BLEU score results for Chinese-English withdifferent settings5 Experiments5.1 SetupWe extracted a GHKM grammar from a Chinese-English parallel corpus with the English side parsed.The corpus consists of 250K sentence pairs, whichis 6.3M words on the English side.
Terminal-awaresynchronous binarization (Fang et al, 2011) was ap-plied to all GHKM grammars that are not scope-filtered.
MERT (Och, 2003) was used to tune pa-rameters.
We used a 392-sentence development setwith four references for parameter tuning, and a 428-sentence test set with four references for testing.
Ourin-house decoder was used for experiments with atrigram language model.
The decoder is capableof both CNF parsing and Earley-style parsing withcube-pruning (Chiang, 2007).For the experiment that incorporated phrases, thephrase pairs were extracted from the same corpuswith the same set of alignments.
We have limitedthe maximum size of phrases to be four.5.2 ResultsOur result is summarized in Table 2.
The baselineGHKM grammar with monotonic glue rules yieldeda worse result than the no-unary grammar with thesame glue rules.
The difference is statistically signif-icant at p < 0.05 based on 1000 iterations of pairedbootstrap resampling (Koehn, 2004).Compared to using monotonic glue rules, usingABC glue rules brought slight improvements forboth the no-unary setting and the scope-filtered set-ting, but the differences are not statistically signifi-cant.
In terms of decoding speed and memory usage,using ABC glues and monotonic glue rules were vir-tually identical.
The fact that glue rules are seldomused at decoding time may account for why there is416little difference in using monotonic glue rules and us-ing ABC glue rules.
Out of all the rules that were ap-plied to decoding our test set, less than one percentwere glue rules, and among the glue rules, straightglue rules outnumbered inverted ones by three toone.Compared with binarized no-unary rules, scope-3 filtered no-unary rules retained 87% of the rulesbut still managed to have slightly better BLEU score.However, the score difference is not statistically sig-nificant.
Because the size of the grammar is smaller,compared to using no-unary grammar, it used lessmemory at decoding time.
However, decoding speedwas somewhat slower.
This is because the decoderemploys Early-style dotted rules to handle unbina-rized rules, and in order to decode with scope-3rules, the decoder needs to build dotted items, whichare not pruned until a rule is completely matched,thus leading to slower decoding.Adding phrases made the translation resultslightly worse.
The difference is not statisticallysignificant.
There are two possible explanations forthis.
Since there were more features to tune, MERTmay have not done a good job.
We believe themore important reason is that once a phrase is used,only glue rules can be used to continue the deriva-tion, thereby losing the richer information offeredby GHKM grammar.6 ConclusionIn this paper, we discussed several issues concerningdecoding with synchronous context-free grammars,focusing on grammars resulting from the GHKMextraction method.
We discussed different ways tohandle cycles.
We presented a modified grammarextraction scheme that eliminates unary rules.
Wealso presented a way to decode with unary rules inthe grammar, and examined several different issuesresulting from binarizing SCFGs.
We finally dis-cussed adding flexibility to SCFGs by adding gluerules and phrases.Acknowledgments We would like to thank theanonymous reviewers for their helpful comments.This work was supported by NSF grants IIS-0546554 and IIS-0910611.ReferencesDavid Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL-05, pages 263?270, Ann Arbor, MI.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society,39(1):1?21.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 6(8):451?455.Licheng Fang, Tagyoung Chung, and Daniel Gildea.2011.
Terminal-aware synchronous binarization.
InProceedings of the ACL 2011 Conference Short Pa-pers, Portland, Oregon, June.
Association for Compu-tational Linguistics.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of NAACL-04, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.2006.
Scalable inference and training of context-rich syntactic translation models.
In Proceedings ofCOLING/ACL-06, pages 961?968, July.Mark Hopkins and Greg Langmead.
2010.
SCFG decod-ing without binarization.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 646?655, Cambridge, MA,October.
Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2003.
A* pars-ing: Fast exact Viterbi parse selection.
In Proceedingsof NAACL-03.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of NAACL-03, Edmonton, Alberta.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, pages 388?395, Barcelona, Spain, July.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of ACL-03.Dekai Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Compu-tational Linguistics, 23(3):377?403.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of NAACL-06, pages 256?263, New York, NY.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proc.
Workshop on Statistical Machine Translation,pages 138?141.417
