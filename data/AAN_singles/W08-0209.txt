Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 71?79,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCombining Open-Source with Research to Re-engineera Hands-on Introductory NLP CourseNitin Madnani Bonnie J. DorrLaboratory for Computational Linguistics and Information ProcessingInstitute for Advanced Computer StudiesDepartment of Computer ScienceUniversity of Maryland, College Park{nmadnani,bonnie}@umiacs.umd.eduAbstractWe describe our first attempts to re-engineerthe curriculum of our introductory NLP courseby using two important building blocks: (1)Access to an easy-to-learn programming lan-guage and framework to build hands-on pro-gramming assignments with real-world dataand corpora and, (2) Incorporation of interest-ing ideas from recent NLP research publica-tions into assignment and examination prob-lems.
We believe that these are extremely im-portant components of a curriculum aimed at adiverse audience consisting primarily of first-year graduate students from both linguisticsand computer science.
Based on overwhelm-ingly positive student feedback, we find thatour attempts were hugely successful.1 IntroductionDesigning an introductory level natural languageprocessing course for a class of first year computerscience and linguistics graduate students is a chal-lenging task.
It is important to strive for balance be-tween breadth and depth?it is important not onlyto introduce the students to a variety of languageprocessing techniques and applications but also toprovide sufficient detail about each.
However, weclaim that there is another important requirement fora successful implementation of such a course.
Likeany other graduate-level course offered to first yearstudents, it should encourage them to approach so-lutions to problems as researchers.
In order to meetsuch a requirement, the course should have two im-portant dimensions:1.
Access to a programming framework that pro-vides the tools and data used in the real worldso as to allow the students to explore each topichands-on and easily attempt creative solutionsto problems.
The framework should be simpleenough to use so that students are not boggeddown in its intricacies and can focus on thecourse concepts.2.
Exposure to novel and innovative research ineach topic.
One of the most valuable contribu-tions of a large community, such as the NLPand CL community, is the publicly accessiblerepository of research publications for a rangeof topics.
While the commonly used textbooksdescribe established and mainstream researchmethods for each topic in detail, more recentresearch papers are usually omitted.
By usingsuch papers as the bases for programmingassignments?instantiated in the frameworkdescribed earlier?and exam questions, stu-dents can gain important insights into how newsolutions to existing problems are formulated;insights that can only come from a hands-onapproach to problem solving.In this paper, we describe our attempts to engineersuch a course.
In section 2, we describe the specificgoals we had in mind for such a course and how itdiffers from the previous version of the introductorycourse we taught at our institution.
Section 3 dis-cusses how we fully integrated an open-source pro-gramming framework into our curriculum and usedit for programming assignments as well as in-class71sessions.
In a similar vein, section 4 describes ourpreliminary efforts to combine interesting researchideas for various topics with the framework above.We also have definite plans to expand the course cur-riculum to take more novel ideas from recent NLPliterature for each topic and adapt them to instructivehands-on assignments.
Furthermore, we are devel-oping extensions and add-ons for the programmingframework that we plan to contribute to the project.We outline these plans in section 6 and conclude insection 7.2 GoalsWe wanted our course curriculum to fulfill somespecific goals that we discuss below, provide moti-vation wherever appropriate.?
A Uniform Programming Framework.
Theprevious version of our introductory coursetook a more fragmented approach and used dif-ferent programming languages and tools fordifferent assignments.
For example, we usedan in-house HMM library written in C for anyHMM-based assignments and Perl for someother assignments.
As expected, such an ap-proach requires students to familiarize them-selves with a different programming interfacefor each assignment and discourages studentsto explore on their own.
To address this con-cern, we chose the Python (Python, 2007) pro-gramming language and the Natural LanguageToolkit (Loper and Bird, 2002), written entirelyin Python, for all our assignments and program-ming tasks.
We discuss our use of NLTK inmore detail in the next section.?
Real-world Data & Corpora.
In our previ-ous course, students did not have access to anyof the corpora that are used in actual NLP re-search.
We found this to be a serious short-coming and wanted to ensure that our new cur-riculum allowed students to use real corpora forevaluating their programming assignments.?
Exposure to Research.
While we had cer-tainly made it a point to introduce recent re-search work in our lectures for all topics inthe previous course, we believed that a muchricher integration was required in order to al-low a more realistic peek into NLP research.?
Satisfying a Diverse Audience.
We wanted thecurriculum to appeal to both computer scienceand linguistics students since they the coursewas cross-listed in both departments.?
Continuing Interest.
A large number of thestudents enrolled in the course were undecidedabout what research area to pursue.
We wantedto present a fair picture of what NLP researchactually entails and encourage any interestedstudents to take the more advanced part of thecourse being offered later in the year.3 Incorporating Open SourceWe use the Python programming language andNLTK as our programming framework for thecurriculum.
Python is currently one of the mostpopular programming languages?it is fully objectoriented and multi-platform, natively supportshigh-level dynamic data types such as lists andhashes (termed dictionaries in Python), has veryreadable syntax and, most importantly, ships withan extensive standard library for almost every con-ceivable task.
Although Python already has mostof the functionality needed to perform very simpleNLP tasks, its still not powerful enough for moststandard ones.
This is where the Natural LanguageToolkit (NLTK) comes in.
NLTK1, written entirelyin Python, is a collection of modules and corpora,released under an open-source license, that allowsstudents to learn and conduct research in NLP (Birdet al, 2008).
The most important advantage ofusing NLTK is that it is entirely self-contained.Not only does it provide convenient functionsand wrappers that can be used as building blocksfor common NLP tasks, it also provides raw andpre-processed versions of standard corpora usedfrequently in NLP literature.
Together, Python andNLTK constitute one of the most potent tools forinstruction of NLP (Madnani, 2007) and allow usto develop hands-on assignments that can appealto a broad audience including both linguistics andcomputer science students.1http://nltk.org72Figure 1: An Excerpt from the output of a Python script used for an in-class exercise demonstrating the simplicity ofthe Python-NLTK combination.In order to illustrate the simplicity and utility ofthis tool to the students, we went through an in-classexercise at the beginning of the class.
The exerciseasked the students to solve the following simplelanguage processing problem:Find the frequency of occurrences of the followingwords in the state-of-the-union addresses of the last6 American Presidents: war, peace, economy & aid.Also draw histograms for each word.We then went through a step-by-step process of howone would go about solving such a problem.
Thesolution hinged on two important points:(a) NLTK ships with a corpus of the last 50 yearsof state-of-the-union addresses and provides anative conditional frequency distribution objectto easily keep track of conditional counts.
(b) Drawing a histogram in Python is as simple asthe statement print ?#?
*n where n is thecount for each query word.Given these two properties, the Python solution forthe problem was only 20 lines long.
Figure 1 showsan excerpt from the output of this script.
This ex-ercise allowed us to impress upon the students thatthe programming framework for the course is sim-ple and fun so that they may start exploring it ontheir own.
We describe more concrete instances ofNLTK usage in our curriculum below.3.1 HMMs & Part-of-speech TaggingHidden Markov Models (Rabiner, 1989) haveproven to be a very useful formalism in NLPand have been used in a wide range of problems,e.g., parsing, machine translation and part-of-speech(POS) tagging.
In our previous curriculum, wehad employed an in-house C++ implementation ofHMMs for our assignments.
As part of our newcurriculum, we introduced Markov models (andHMMs) in the context of POS tagging and in a muchmore hands-on fashion.
To do this, we created anassignment where students were required to imple-ment Viterbi decoding for an HMM and output thebest POS tag sequence for any given sentence.
Therewere several ways in which NLTK made this ex-tremely simple:?
Since we had the entire source code of theHMM module from NLTK available, we fac-tored out the part of the code that handled theHMM training, parameterized it and providedthat to students as a separate module they theycould use to train the HMMs.
Such refactor-ing not only allows for cleaner code boundariesbut it also allows the students to use a varietyof training parameters (such as different formsof smoothed distributions for the transition andemission probabilities) and measure their ef-fects with little effort.
Listing 1 shows howthe refactoring was accomplished: the train-ing code was put into a separate module calledhmmtrainer and automatically called in the73Listing 1: A skeleton of the refactored NLTK HMM codeused to build a hands-on HMM assignmentimport hmmtrainerimport nltk.LidStoneProbDist as lidstoneclass hmm:def __init__(self):params = hmmtrainer.train(smooth=lidstone)self.params = paramsdef decode(self, word_sequence)def tag(self, word_sequence)main hmm class when instantiating it.
The stu-dents had to write the code for the decode andtag methods of this class.
The HMM train-ing was setup to be able to use a variety ofsmoothed distributions, e.g.
Lidstone, Laplaceetc., all available from NLTK.?
NLTK ships with the tokenized and POS taggedversion of the Brown corpus?one of the mostcommon corpora employed for corpus linguis-tics and, in particular, for evaluating POS tag-gers.
We used Section A of the corpus for train-ing the HMMs and asked the students to evalu-ate their taggers on Section B.Another advantage of this assignment was that the ifstudents were interested in how the supervised train-ing process actually worked, they could simply ex-amine the hmmtrainermodule that was also writ-ten entirely in Python.
An assignment with suchcharacteristics in our previous course would haverequired knowledge of C++, willingness to wadethrough much more complicated code and wouldcertainly not have been as instructive.3.2 Finite State AutomataAnother topic where we were able to leverage thestrengths of both NLTK and Python was whenintroducing the students to finite state automata.Previously, we only discussed the fundamentals offinite state automata in class and then asked thestudents to apply this knowledge to morphologicalparsing by using PC-Kimmo (Koskenniemi, 1983).However, working with PC-Kimmo required thestudents to directly fill entries in transition tablesListing 2: An illustration of the simple finite state trans-ducer interface in NLTKfrom nltk_contrib.fst import fstf = fst.FST(?test?)
#instantiatef.add_state(?1?)
# add statesf.add_state(?2?)f.add_state(?3?
)f.initial_state = 1 # set initialf.set_final(?2?)
# set finalsf.set_final(?3?
)f.add_arc(?1?,?2?,?a?, ?A?)
# a ?> Af.add_arc(?1?,?3?,?b?, ?B?)
# b ?> Bprint f.transduce([?a?, ?a?, ?b?, ?b?
])using a very rigid syntax.In the new curriculum, we could easily rely onthe finite state module that ships with NLTK to usesuch automata in a very natural way as shown inListing 2.
With such an easy to use interface, wecould concentrate instead on the more importantconcepts underlying the building and cascading oftransducers to accomplish a language processingtask.As our example task, we asked the studentsto implement the Soundex Algorithm, a phoneticalgorithm commonly used by libraries and theCensus Bureau to represent people?s names as theyare pronounced in English.
We found that not onlydid the students easily implement such a complextransducer, they also took the time to perform someanalysis on their own and determine the short-comings of the Soundex algorithm.
This was onlypossible because of the simple interface and shortdevelopment cycle provided by the Python-NLTKcombination.
In addition, NLTK also provides asingle method2 that can render the transducer as apostscript or image file that can prove extremelyuseful for debugging.In our new version of the course, we consciouslychose to use primarily open-source technologies inthe curriculum.
We feel that it is important to say afew words about this choice: an open-source project2This method interfaces with an existing installation ofGraphviz, a popular open-source graph drawing software (Ell-son et al, 2004).74not only allows instructors to examine the sourcecode and re-purpose it for their own use (as wedid in section 3.1) but it also encourages studentsto delve deep into the programming frameworkif they are curious about how something works.In fact, a few of our students actually discoveredsubtle idiosyncrasies and bugs in the NLTK sourcewhile exploring on their own, filed bug reportswhere necessary and shared the findings with theentire class.
This experience allowed all studentsto understand the challenges of language processing.More importantly, we believe an open-sourceproject fosters collaboration in the community thatit serves.
For example, a lot of the functionality ofNLTK hinges on important technical contributions,such as our SRILM interface described in section 6,from the large academic NLP community that can beused by any member of the community for researchand for teaching.4 Incorporating ResearchBesides employing a uniform programming frame-work that the students could pick up easily and learnto explore on their own, the other important goalof the new curriculum was to incorporate ideas andtechniques from interesting NLP research publica-tions into assignments and exams.
The motivation,of course, was to get our students to think aboutand possibly even implement these ideas.
Since wecannot discuss all instances in the curriculum wherewe leveraged research publications (due to spaceconsiderations), we only discuss two such instancesin detail below.The first topic for which we constructed a moreopen-ended research-oriented assignment was lex-ical semantics.
We focused, in particular, on theWordNet (Fellbaum, 1998) database.
WordNet isa very popular lexical database and has been usedextensively in NLP literature over the years.
In theprevious course, our assignment on lexical seman-tics asked the students to use the online interface toWordNet to learn the basic concept of a synset andthe various relations that are defined over synsetssuch as hyponymy, hypernymy etc.
A very sim-ple change would have been to ask the students touse the WordNet interface included with NLTK toperform the same analysis.
However, we thoughtthat a more interesting assignment would be to ex-plore the structure of the four WordNet taxonomies(Noun, Verb, Adjective and Adverb).
This taxon-omy can be simplified and thought of as a directedacyclic graph G = (V,E) where each synset u ?
Vis a node and each edge (u, v) ?
E represents thatv is a hypernym of u.
Given such a graph, somevery interesting statistics can be computed about thetopology ofWordNet itself (Devitt and Vogel, 2004).In our assignment, we asked the students to use theNLTK WordNet interface to compute some of thesestatistics automatically and answer some interestingquestions:(a) What percentage of the nodes in the Noun tax-onomy are leaf nodes?
(b) Which are the nine most general root nodes inthe Noun taxonomy and what is the node dis-tribution across these roots?
(c) Compute the branching factor (number of de-scendants) for each node in the Noun taxonomyboth including and excluding leaf nodes.
Whatpercentage of nodes have a branching factorless than 5?
Less than 20?
Does this tell some-thing you about the shallowness/depth of thetaxonomy?
(d) If we plot a graph with the number of sensesof each verb in the Verb taxonomy against itspolysemy rank, what kind of graph do we get?What conclusion can be drawn from this graph?
(e) Compare the four taxonomies on average pol-ysemy, both including and excluding monose-mous words.
What conclusions can you drawfrom this?Of course, the assignment also contained the usualquestions pertaining to the content of the WordNetdatabase rather than just its structure.
We believethat this assignment was much more instructivebecause not only did it afford the students a closeexamination into the usage as well as structure of avaluable NLP resource, but also required them toapply their knowledge of graph theory.75The second instance where we used a research pa-per was when writing the HMM question for the fi-nal exam.
We thought it would be illuminating toask the students to apply what they had learned inclass about HMMs to an instance of HMM used inan actual NLP scenario.
For this purpose, we chosethe HMM described in (Miller et al, 1999) and asshown in Figure 2.
As part of the question, we ex-qSqDqGEqE?s= 1.01.01.0a0a1bqD(ui)bqGE(ui)1.0Figure 2: An HMM used and described in a popular re-search publication formed the basis of a question in thefinal exam.plained the information retrieval task: generate aranked list of documents relevant to a user queryU = ?ui?, where the rank of the document D isbased on the probability P (D is relevant|U).
Wefurther explained that by applying Bayes?
theoremto this quantity and assuming a uniform prior overdocument selection, the only important quantity wasthe probability of the query U being generated by arelevant document D, or P (U |D is relevant).
Therest of the question demonstrated how this genera-tive process could be modeled by the HMM in Fig-ure 2:?
Start at the initial state qS .?
Transition with the probability a0 to state qDwhich represents choosing a word directly fromdocument D OR transition with probability a1to state qGE which represents choosing a wordfrom ?General English?, i.e., a word unrelatedto the document but that occurs commonly inother queries.?
If in state qD, emit the current, say ith, queryword either directly from document D withemission probability bqD(ui).
Otherwise, if instate qGE , emit the current query word from?General English?
with emission probabilitybqGE (ui).?
Transition to the end state qE .?
If we have generated all the words in the query,then stop here.
If not, transition to qS andrepeat.Given this generative process, we then asked the stu-dents to answer the following questions:(a) Derive a simplified closed-form expression forthe posterior probability P (U |D is relevant)in terms of the transition probabilities{a0, a1} and the emissions probabilities{bqD(ui), bqGE (ui)}.
You may assume thatU = ?ui?ni=1.
(b) What HMM algorithm will you use to com-pute P (U |D is relevant) when implementingthis model?
(c) How will you compute the maximum like-lihood estimate for the emission probabilitybqD(ui) ?
(d) What about bqGE (ui) ?
Is it practical to com-pute the actual value of this estimate?
Whatreasonable approximation might be used inplace of the actual value?This question not only required the students to applythe concepts of probability theory and HMMs thatthey learned in class but also to contemplate moreopen-ended research questions where there may beno one right answer.For both these and other instances where we usedideas from research publications to build assign-ments and exam questions, we encouraged the stu-dents to read the corresponding publications afterthey had submitted their solutions.
In addition, wediscussed possible answers with them in an onlineforum set up especially for the course.765 Indicators of SuccessSince this was our first major revision of the curricu-lum for an introductory NLP course, we were inter-ested in getting student feedback on the changes thatwe made.
To elicit such feedback, we designed asurvey that asked all the students in the class (a totalof 30) to rate the new curriculum on a scale of oneto five on various criteria, particularly for the expe-rience of using NLTK for all programming assign-ments and on the quality of the assignments them-selves.0 20 40 60 80 100 Excellent Good Satisfactory Fair Poorpercentage of studentsFigure 3: Histogram of student feedback on the experi-ence of using the Python-NLTK combination.0 20 40 60 80 100 Excellent Good Satisfactory Fair Poorpercentage of studentsFigure 4: Histogram of student feedback on the qualityof course assignments.Figures 3 and 4 show the histograms of the stu-dents?
survey responses for these two criteria.
Theoverwhelmingly positive ratings clearly indicatethat we were extremely successful in achieving thedesired goals for our revised curriculum.
As part ofthe survey, we had also asked the students to provideany comments they had about the curriculum.
Wereceived a large number of positive comments someof which we quote below:?Using Python and NLTK for assignments removedany programming barriers and enabled me to focuson the course concepts.?
?The assignments were absolutely fantastic andsupplemented the material presented in class.?
?A great experience for the students.
?The first comment?echoed by several linguisticsas well as computer science students?validatesour particular choice of programming languageand framework.
In the past, we had observedthat linguistics students with little programmingbackground spent most of their time figuring outhow to wield the programming language or tool toaccomplish simple tasks.
However, the combinationof Python and NLTK provided a way for them towork on computational solutions without takingtoo much time away from learning the core NLPconcepts.While it is clearly apparent to us that the studentsreally liked the new version of the curriculum, itwould also have been worthwhile to carry out acomparison of students?
reviews of the old and newcurricula.
The most frequent comments that we sawin older versions of the course were similar to thefollowing:?Although I feel you did a decent job repeating andpointing out the interesting facts of the book, I don?tthink you really found many compelling examples ofusing these techniques in practice.
?The feedback we received for the revamped curricu-lum, such as the second comment above, clearly in-dicated that we had addressed this shortcoming ofthe older curriculum.
However, due to significantformat changes in the review forms between variousofferings of this course, it is not possible to conducta direct, retroactive comparison.
It is our intent tooffer such comparisons in the future.776 Future PlansGiven the success that we had in our first attemptto re-engineer the introductory NLP course, we planto continue: (1) our hands-on approach to program-ming assignments in the NLTK framework and, (2)our practice of adapting ideas from research publi-cations as the bases for assignment and examinationproblems.
Below we describe two concrete ideas forthe next iteration of the course.1.
Hands-on Statistical Language Modeling.For this topic, we have so far restricted our-selves to the textbook (Jurafsky and Mar-tin, 2000); the in-class discussion and pro-gramming assignments have been missing ahands-on component.
We have written aPython interface to the SRI Language Model-ing toolkit (Stolcke, 2002) for use in our re-search work.
This interface uses the Simpli-fied Wrapper & Interface Generator (SWIG) togenerate a Python wrapper around our C codethat does all the heavy lifting via the SRILMlibraries.
We are currently working on integrat-ing this module into NLTK which would allowall NLTK users, including our students in thenext version of the course, to build and querystatistical language models directly inside theirPython code.
This module, combined with thelarge real-world corpora, would provide a greatopportunity to perform hands-on experimentswith language models and to understand thevarious smoothing methods.
In addition, thiswould also allow a language model to be usedin an assignment for any other topic should weneed it.2.
Teaching Distributional Similarity.
Theidea that a language possesses distributionalstructure?first discussed at length by Har-ris (1954)?says that one can describe a lan-guage in terms of relationships between the oc-currences of its elements (words, morphemes,phonemes).
The name for the phenomenonis derived from an element?s distribution?setsof other elements in particular positions thatoccur with the element in utterances or sen-tences.
This work led to the concept of distribu-tional similarity?words or phrases that sharethe same distribution, i.e., the same set of wordsor in the same context in a corpus, tend to havesimilar meanings.
This is an extremely popularconcept in corpus linguistics and forms the ba-sis of a large body of work.
We believe that thisis an important topic that should be included inthe curriculum.
We plan to do so in the contextof lexical paraphrase acquisition or synonymsautomatically from corpora, a task that reliesheavily on this notion of distributional similar-ity.
There has been a lot of work in this area inthe past years (Pereira et al, 1993; Gasperin etal., 2001; Glickman and Dagan, 2003; Shimo-hata and Sumita, 2005), much of which can beeasily replicated using the Python-NLTK com-bination.
This would allow for a very hands-ontreatment and would allow the students to gaininsight into this important, but often omitted,idea from computational linguistics.7 ConclusionOur primacy goal was to design an introductory levelnatural language processing course for a class of firstyear computer science and linguistics graduate stu-dents.
We wanted the curriculum to encourage thestudents to approach solutions to problems with themind-set of a researcher.
To accomplish this, we re-lied on two basic ideas.
First, we used a program-ming framework which provides the tools and dataused in the real world so as to allow hands-on ex-ploration of each topic.
Second, we adapted ideasfrom recent research papers into programming as-signments and exam questions to provide studentswith insight into the process of formulating a solu-tion to common NLP problems.
At the end of thecourse, we asked all students to provide feedbackand the verdict from both linguistics and computerscience students was overwhelmingly in favor of thenew more hands-on curriculum.ReferencesSteven Bird, Ewan Klein, Edward Loper, and JasonBaldridge.
2008.
Multidisciplinary Instruction withthe Natural Language Toolkit.
In Proceedings of theThird ACL Workshop on Issues in Teaching Computa-tional Linguistics.Ann Devitt and Carl Vogel.
2004.
The Topology of78WordNet: Some metrics.
In Proceedings of the Sec-ond International WordNet Conference (GWC2004).J.
Ellson, E.R.
Gansner, E. Koutsofios, S.C. North, andG.
Woodhull.
2004.
Graphviz and Dynagraph ?
Staticand Dynamic Graph Drawing Tools.
In Graph Draw-ing Software, pages 127?148.
Springer-Verlag.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Caroline Gasperin, P. Gamallo, A. Agustini, G. Lopes,and Vera de Lima.
2001.
Using syntactic contexts formeasuring word similarity.
In Workshop on Knowl-edge Acquisition Categorization, ESSLLI.Oren Glickman and Ido Dagan.
2003.
Identifying lex-ical paraphrases from a single corpus: A case studyfor verbs.
In Recent Advantages in Natural LanguageProcessing (RANLP?03).Zellig Harris.
1954.
Distributional Structure.
Word,10(2):3.146?162.Daniel Jurafsky and James H. Martin.
2000.
Speechand Language Processing: An Introduction to Natu-ral Language Processing, Computational Linguistics,and Speech Recognition.
Prentice Hall.Kimmo Koskenniemi.
1983.
Two-level morphology: ageneral computational model for word-form recogni-tion and production.
Publication No.
11, University ofHelsinki: Department of General Linguistics.Edward Loper and Steven Bird.
2002.
NLTK: The Nat-ural Language Toolkit.
In Proceedings of ACL Work-shop on Effective Tools and Methodologies for Teach-ing NLP and CL, pages 62?69.Nitin Madnani.
2007.
Getting Started on Natural Lan-guage Processing with Python.
ACM Crossroads,13(4).D.
R. Miller, T. Leek, and R. M. Schwartz.
1999.
Ahidden Markov model information retrieval system.
InProceedings of SIGIR, pages 214?221.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of english words.
In Proceed-ings of ACL, pages 183?190.Python.
2007.
The Python Programming Language.http://www.python.org.Lawrence R. Rabiner.
1989.
A tutorial on hidden markovmodels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?286.Mitsuo Shimohata and Eiichiro Sumita.
2005.
Acquir-ing synonyms from monolingual comparable texts.
InProceedings of IJCNLP, pages 233?244.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of InternationalConference on Spoken Language Processing (ICSLP).79
