Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 519?527,Beijing, August 2010Reranking Models in Fine-grained Opinion AnalysisRichard Johansson and Alessandro MoschittiUniversity of Trento{johansson, moschitti}@disi.unitn.itAbstractWe describe the implementation ofreranking models for fine-grained opinionanalysis ?
marking up opinion expres-sions and extracting opinion holders.
Thereranking approach makes it possibleto model complex relations betweenmultiple opinions in a sentence, allowingus to represent how opinions interactthrough the syntactic and semanticstructure.
We carried out evaluations onthe MPQA corpus, and the experimentsshowed significant improvements over aconventional system that only uses localinformation: for both tasks, our systemsaw recall boosts of over 10 points.1 IntroductionRecent years have seen a surge of interest in theautomatic processing of subjective language.
Thetechnologies emerging from this research have ob-vious practical uses, either as stand-alone appli-cations or supporting other NLP tools such asinformation retrieval or question answering sys-tems.
While early efforts in subjectivity analysisfocused on coarse-grained tasks such as retriev-ing the subjective documents from a collection,most recent work on this topic has focused on fine-grained tasks such as determining the attitude of aparticular person on a particular topic.
The devel-opment and evaluation of such systems has beenmade possible by the release of manually anno-tated resources using fairly fine-grained represen-tations to describe the structure of subjectivity inlanguage, for instance the MPQA corpus (Wiebeet al, 2005).A central task in the automatic analysis of sub-jective language is the indentification of subjectiveexpressions: the text pieces that allow us to drawthe conclusion that someone has a particular feel-ing about something.
This is necessary for fur-ther analysis, such as the determination of opin-ion holder and the polarity of the opinion.
TheMPQA corpus defines two types of subjective ex-pressions: direct subjective expressions (DSEs),which are explicit mentions of attitude, and ex-pressive subjective elements (ESEs), which signalthe attitude of the speaker by the choice of words.The prototypical example of a DSE would be averb of statement or categorization such as praiseor disgust, and the opinion holder would typi-cally be a direct semantic argument of this verb.ESEs, on the other hand, are less easy to cate-gorize syntactically; prototypical examples wouldinclude value-expressing adjectives such as beau-tiful and strongly charged words like appease-ment, while the relation between the expressionand the opinion holder is typically less clear-cutthan for DSEs.
In addition to DSEs and ESEs, theMPQA corpus also contains annotation for non-subjective statements, which are referred to as ob-jective speech events (OSEs).Examples (1) and (2) show two sentences fromthe MPQA corpus where DSEs and ESEs havebeen manually annotated.
(1) He [made such charges]DSE [despite thefact]ESE that women?s political, social and cul-tural participation is [not less than that]ESE ofmen.
(2) [However]ESE , it is becoming [ratherfashionable]ESE to [exchange harsh words]DSEwith each other [like kids]ESE .The task of marking up these expressions hasusually been approached using straightforwardsequence labeling techniques using using simplefeatures in a small contextual window (Choi etal., 2006; Breck et al, 2007).
However, due to519the simplicity of the feature sets, this approachfails to take into account the fact that the semanticand pragmatic interpretation of sentences is notonly determined by words but also by syntacticand shallow-semantic relations.
Crucially, takinggrammatical relations into account allows us tomodel how expressions interact in various waysthat influence their interpretation as subjectiveor not.
Consider, for instance, the word said inexamples (3) and (4) below, where the interpre-tation as a DSE or an OSE is influenced by thesubjective content of the enclosed statement.
(3) ?We will identify the [culprits]ESE of theseclashes and [punish]ESE them,?
he [said]DSE .
(4) On Monday, 80 Libyan soldiers disembarkedfrom an Antonov transport plane carrying militaryequipment, an African diplomat [said]OSE .In addition, the various opinions expressed ina sentence are very interdependent when it comesto the resolution of their holders, i.e.
determiningthe entity that harbors the sentiment manifestedtextually in the opinion expression.
Clearly, thestructure of the sentence is influential also for thistask: an ESE will be quite likely to be linked tothe same opinion holder as a DSE directly aboveit in the syntactic tree.In this paper, we demonstrate how syntacticand semantic structural information can be usedto improve the detection of opinion expressionsand the extraction of opinion holders.
While thisfeature model makes it impossible to use the stan-dard sequence labeling method, we show that witha simple strategy based on reranking, incorporat-ing structural features results in a significant im-provement.
In an evaluation on the MPQA corpus,the best system we evaluated, a reranker using thePassive?Aggressive learning algorithm, achieveda 10-point absolute improvement in soft recall,and a 5-point improvement in F-measure, over thebaseline sequence labeler.
Similarly, the recall isboosted by almost 11 points for the holder extrac-tion (3 points in F-measure) by modeling the inter-action of opinion expressions with respect to hold-ers.2 Related WorkSince the most significant body of work in sub-jectivity analysis has been dedicated to coarse-grained tasks such as document polarity classi-fication, most approaches to analysing the senti-ment of natural-language text have relied funda-mentally on purely lexical information (see (Panget al, 2002; Yu and Hatzivassiloglou, 2003), in-ter alia) or low-level grammatical informationsuch as part-of-speech tags and functional words(Wiebe et al, 1999).
This is not unexpected sincethese problems have typically been formulated astext categorization problems, and it has long beenagreed in the information retrieval community thatvery little can be gained by complex linguisticprocessing for tasks such as text categorizationand search (Moschitti and Basili, 2004).As the field moves towards increasingly sophis-ticated tasks requiring a detailed analysis of thetext, the benefit of syntactic and semantic analy-sis becomes more clear.
For the task of subjec-tive expression detection, Choi et al (2006) andBreck et al (2007) used syntactic features in a se-quence model.
In addition, syntactic and shallow-semantic relations have repeatedly proven usefulfor subtasks of subjectivity analysis that are in-herently relational, above all for determining theholder or topic of a given opinion.
Choi et al(2006) is notable for the use of a global modelbased on hand-crafted constraints and an integerlinear programming optimization step to ensure aglobally consistent set of opinions and holders.Works using syntactic features to extract top-ics and holders of opinions are numerous (Bethardet al, 2005; Kobayashi et al, 2007; Joshi andPenstein-Rose?, 2009; Wu et al, 2009).
Seman-tic role analysis has also proven useful: Kimand Hovy (2006) used a FrameNet-based seman-tic role labeler to determine holder and topic ofopinions.
Similarly, Choi et al (2006) success-fully used a PropBank-based semantic role labelerfor opinion holder extraction.
Ruppenhofer et al(2008) argued that semantic role techniques areuseful but not completely sufficient for holder andtopic identification, and that other linguistic phe-nomena must be studied as well.
One such lin-guistic pheonomenon is the discourse structure,520which has recently attracted some attention in thesubjectivity analysis community (Somasundaranet al, 2009).3 Modeling Interaction over Syntacticand Semantic StructurePrevious systems for opinion expression markuphave typically used simple feature sets which haveallowed the use of efficient off-the-shelf sequencelabeling methods based on Viterbi search (Choi etal., 2006; Breck et al, 2007).
This is not pos-sible in our case since we would like to extractstructural, relational features that involve pairs ofopinion expressions and may apply over an arbi-trarily long distance in the sentence.While it is possible that search algorithms forexact or approximate inference can be construc-tured for the argmax problem in this model, wesidestepped this issue by using a reranking de-composition of the problem:?
Apply a standard Viterbi-based sequence la-beler based on local context features but nostructural interaction features.
Generate asmall candidate set of size k.?
Generate opinion holders for every proposedopinion expression.?
Apply a complex model using interactionfeatures to pick the top candidate from thecandidate set.The advantages of a reranking approach com-pared to more complex approaches requiring ad-vanced search techniques are mainly simplicityand efficiency: this approach is conceptually sim-ple and fairly easy to implement provided that k-best output can be generated efficiently, and fea-tures can be arbitrarily complex ?
we don?t have tothink about how the features affect the algorithmiccomplexity of the inference step.
A common ob-jection to reranking is that the candidate set maynot be diverse enough to allow for much improve-ment unless it is very large; the candidates maybe trivial variations that are all very similar to thetop-scoring candidate.3.1 Syntactic and Semantic StructuresWe used the syntactic?semantic parser by Johans-son and Nugues (2008) to annnotate the sen-tences with dependency syntax (Mel?c?uk, 1988)and shallow semantic structures in the PropBank(Palmer et al, 2005) and NomBank (Meyers et al,2004) frameworks.
Figure 1 shows an exampleof the annotation: The sentence they called him aliar, where called is a DSE and liar is an ESE, hasbeen annotated with dependency syntax (abovethe text) and PropBank-based semantic role struc-ture (below the text).
The predicate called, whichis an instance of the PropBank frame call.01,has three semantic arguments: the Agent (A0), theTheme (A1), and the Predicate (A2), which are re-alized on the surface-syntactic level as a subject,a direct object, and an object predicative comple-ment, respectively.
]ESEThey calledcall.01SBJOPRDliarhim[ [aA1A0 A2]DSENMODOBJFigure 1: Syntactic and shallow semantic struc-ture.3.2 Base Sequence Labeling ModelTo solve the first subtask, we implemented a stan-dard sequence labeler for subjective expressionmarkup, similar to the approach by Breck et al(2007).
We encoded the opinionated expressionbrackets using the IOB2 encoding scheme (TjongKim Sang and Veenstra, 1999) and trained themodel using the metod by Collins (2002).The sequence labeler used word, POS tag, andlemma features in a window of size 3.
In addi-tion, we used prior polarity and intensity featuresderived from the lexicon created by Wilson et al(2005).
It is important to note that prior subjec-tivity does not always imply subjectivity in a par-ticular context; this is why contextual features areessential for this task.This sequence labeler was used to generate thecandidate set for the reranker.
To generate rerank-ing training data, we carried out a 5-fold hold-outprocedure: We split the training set into 5 pieces,521trained a sequence labeler on pieces 1 to 4, appliedit to piece 5 and so on.3.3 Base Opinion Holder ExtractorFor every opinion expression, we extracted opin-ion holders, i.e.
mentions of the entity holdingthe opinion denoted by the opinion expression.Since the problem of holder extraction is in manyways similar to semantic argument detection ?when the opinion expression is a verb, finding theholder typically entails finding a SPEAKER argu-ment ?
we approached this problem using meth-ods inspired by semantic role labeling.
We thustrained support vector machines using the LIB-LINEAR software (Fan et al, 2008), and appliedthem to the noun phrases in the same sentenceas the holder.
Separate classifiers were trained toextract holders for DSEs, ESEs, and OSEs.
Theclassifiers used the following feature set:SYNTACTIC PATH.
Similarly to the path fea-ture widely used in SRL, we extract a featurerepresenting the path in the dependency treebetween the expression and the holder (Jo-hansson and Nugues, 2008).
For instance,the path from the DSE called to the holderThey is SBJ?.SHALLOW-SEMANTIC RELATION.
If there is adirect shallow-semantic relation between theexpression and the holder, use a feature rep-resenting its semantic role, such as A0 forThey with respect to called.EXPRESSION HEAD WORD AND POS.HOLDER HEAD WORD AND POS.DOMINATING EXPRESSION TYPE.CONTEXT WORDS AND POS FOR HOLDER.EXPRESSION VERB VOICE.However, there are also differences comparedto typical argument extraction in SRL.
First, it isimportant to note that the MPQA corpus does notannotate direct links from opinions to a holders,but from opinions to holder coreference chains.To handle this issue, we created positive traininginstances for allmembers of the coreference chainin the same sentence as the opinion, and negativeinstances for the other noun phrases.Secondly, an opinion may be linked not to anovert noun phrase in a sentence, but to an im-plicit holder; a special case of implicit holder isthe writer of the text.
We trained separate clas-sifiers to detect these situations.
These classifiersdid not use the features requiring a holder phrase.Finally, there is a restriction that every expres-sion may have at most one holder, so at test timewe select only the highest-scoring opinion holdercandidate.3.4 Opinion Expression Reranker FeaturesThe rerankers use two types of structural fea-tures: syntactic features extracted from the depen-dency tree, and semantic features extracted fromthe predicate?argument (semantic role) graph.The syntactic features are based on pathsthrough the dependency tree.
This creates a smallcomplication for multiword opinion expressions;we select the shortest possible path in such cases.For instance, in example (1) above, the path willbe computed betweenmade and despite, and in (2)between fashionable and exchange.We used the following syntactic interaction fea-tures:SYNTACTIC PATH.
Given a pair opinion ex-pressions, we use a feature representingthe labels of the two expressions and thepath between them through the syntactictree.
For instance, for the DSE calledand the ESE liar in Figure 1, we representthe syntactic configuration using the featureDSE:OPRD?
:ESE, meaning that the pathfrom the DSE to the ESE follows an OPRDlink downward.LEXICALIZED PATH.
Same as above,but with lexical information attached:DSE/called:OPRD?:ESE/liar.DOMINANCE.
In addition to the features basedon syntactic paths, we created a more genericfeature template describing dominance re-lations between expressions.
For instance,from the graph in Figure 1, we extract thefeature DSE/called?ESE/liar, mean-ing that a DSE with the word called domi-nates an ESE with the word liar.The semantic features were the following:522PREDICATE SENSE LABEL.
For every pred-icate found inside an opinion expression,we add a feature consisting of the expres-sion label and the predicate sense identi-fier.
For instance, the verb call which isalso a DSE is represented with the featureDSE/call.01.PREDICATE AND ARGUMENT LABEL.
For ev-ery argument of a predicate inside an opin-ion expression, we also create a featurerepresenting the predicate?argument pair:DSE/call.01:A0.CONNECTING ARGUMENT LABEL.
When apredicate inside some opinion expression isconnected to some argument inside anotheropinion expression, we use a feature con-sisting of the two expression labels and theargument label.
For instance, the ESE liaris connected to the DSE call via an A2 la-bel, and we represent this using a featureDSE:A2:ESE.Apart from the syntactic and semantic features,we also used the score output from the base se-quence labeler as a feature.
We normalized thescores over the k candidates so that their expo-nentials summed to 1.3.5 Opinion Holder Reranker FeaturesIn addition, we modeled the interaction betweendifferent opinions with respect to their holders.We used the following two features to representthis interaction:SHARED HOLDERS.
A feature representingwhether or not two opinion expressions havethe same holder.
For instance, if a DSEdominates an ESE and they have the sameholder as in Figure 1 where the holderis They, we represent this by the featureDSE:ESE:true.HOLDER TYPES + PATH.
A feature repre-senting the types of the holders, combinedwith the syntactic path between the expres-sions.
The types take the following pos-sible values: explicit, implicit, writer.
InFigure 1, we would thus extract the featureDSE/Expl:OPRD?
:ESE/Expl.Similar to base model feature for the expressiondetection, we also used a feature for the outputscore from the holder extraction classifier.3.6 Training the RerankerWe trained the reranker using the method em-ployed by many rerankers following Collins(2002), which learns a scoring function that istrained to maximize performance on the rerank-ing task.
While there are batch learning algo-rithms that work in this setting (Tsochantaridiset al, 2005), online learning methods have beenmore popular for performance reasons.
We inves-tigated two online learning algorithms: the popu-lar structured perceptron (Collins, 2002) and thePassive?Aggressive (PA) algorithm (Crammer etal., 2006).
To increase robustness, we used anaveraged implementation (Freund and Schapire,1999) of both algorithms.The difference between the two algorithms isthe way the weight vector is incremented in eachstep.
In the perceptron, for a given input x, weupdate based on the difference between the correctoutput y and the predicted output y?, where?
is thefeature representation function:y?
?
argmaxh w ?
?
(x, h)w ?
w + ?
(x, y)?
?
(x, y?
)In the PA algorithm, which is based on the the-ory of large-margin learning, we instead find they?
that violates the margin constraints maximally.The update step length ?
is computed based on themargin; this update is bounded by a regularizationconstant C:y?
?
argmaxh w ?
?
(x, h) +??
(y, h)?
?
min(C, w(?(x,y?)??(x,y))+??(y,y?)??(x,y?)??
(x,y)?2)w ?
w + ?(?
(x, y)?
?
(x, y?
))The algorithm uses a cost function ?.
We usedthe function ?
(y, y?)
= 1 ?
F (y, y?
), where F isthe soft F-measure described in Section 4.1.
Withthis approach, the learning algorithm thus directlyoptimizes the measure we are interested in, i.e.
theF-measure.4 ExperimentsWe carried out the experiments on version 2 ofthe MPQA corpus (Wiebe et al, 2005), which we523split into a test set (150 documents, 3,743 sen-tences) and a training set (541 documents, 12,010sentences).4.1 Evaluation MetricsSince expression boundaries are hard to define ex-actly in annotation guidelines (Wiebe et al, 2005),we used soft precision and recall measures toscore the quality of the system output.
To de-rive the soft precision and recall, we first definethe span coverage c of a span s with respect toanother span s?, which measures h ow well s?
iscovered by s:c(s, s?)
= |s ?
s?||s?|In this formula, the operator | ?
| counts tokens, andthe intersection ?
gives the set of tokens tha t twospans have in common.
Since our evaluation takesspan labels (DSE, ESE, OSE) into account, we setc(s, s?)
to zero if the labels associated with s ands?
are different.Using the span coverage, we define the span setcoverage C of a set of spans S with respect to aset S?:C(S,S?)
=?sj?S?s?k?S?c(sj , s?k)We now define the soft precision P and recallR of a proposed set of spans S?
with respect to agold standard set S as follows:P (S, S?)
= C(S,S?
)|S?| R(S, S?)
=C(S?,S)|S|Note that the operator | ?
| counts spans in this for-mula.Conventionally, when measuring the quality ofa system for an information extraction task, a pre-dicted entity is counted as correct if it exactlymatches the boundaries of a corresponding en-tity in the gold standard; there is thus no rewardfor close matches.
However, since the boundariesof the spans annotated in the MPQA corpus arenot strictly defined in the annotation guidelines(Wiebe et al, 2005), measuring precision and re-call using exact boundary scoring will result infigures that are too low to be indicative of theusefulness of the system.
Therefore, most workusing this corpus instead use overlap-based preci-sion and recall measures, where a span is countedas correctly detected if it overlaps with a span inthe gold standard (Choi et al, 2006; Breck et al,2007).
As pointed out by Breck et al (2007), thisis problematic since it will tend to reward longspans ?
for instance, a span covering the wholesentence will always be counted as correct if thegold standard contains any span for that sentence.The precision and recall measures proposedhere correct the problem with overlap-based mea-sures: If the system proposes a span covering thewhole sentence, the span coverage will be lowand result in a low soft precision.
Note that ourmeasures are bounded below by the exact mea-sures and above by the overlap-based measures:replacing c(s, s?)
with ?c(s, s?)?
gives the exactmeasures and replacing c(s, s?)
with ?c(s, s?)?
theoverlap-based measures.To score the extraction of opinion holders, westarted from the same basic approach.
However,the evaluation of this task is more complex be-cause a) we only want to give credit for holdersfor correctly extracted opinion expressions; b) thegold standard links opinion expressions to coref-erence chains rather than individual mentions ofholders; c) the holder may be the writer or im-plicit (see 3.3).
We therefore used the followingmethod: Given a holder h linked to an expres-sion e, we first located the expression e?
in thegold standard that most closely corresponds to e,that is e?
= argmaxx c(x, e), regardless of thelabels of e and e?.
We then located the gold stan-dard holder h?
by finding the closest correspond-ing holder in the coreference chain H linked to e?:h?
= argmaxx?H c(x, h).
If h is proposed as thewriter, we score it as perfectly detected (coverage1) if the coreference chain H contains the writer,and a full error (coverage 0) otherwise, and simi-lar if h is implicit.4.2 Machine Learning MethodsWe compared the machine learning methods de-scribed in Section 3.
In these experiments, weused a candidate set size k of 8.
Table 1 showsthe results of the evaluations using the precisionand recall measures described above.
The base-line is the result of taking the top-scoring labeling524from the base model.System P R FBaseline 63.36 46.77 53.82Perceptron 62.84 48.13 54.51PA 63.50 51.79 57.04Table 1: Evaluation of reranking learning meth-ods.We note that the best performance was obtainedusing the PA algorithm.
While these results aresatisfactory, it is possible that they could be im-proved further if we would use a batch learningmethod such as SVMstruct (Tsochantaridis et al,2005) instead of the online learning methods usedhere.4.3 Candidate Set SizeIn any method based on reranking, it is importantto study the influence of the candidate set size onthe quality of the reranked output.
In addition, aninteresting question is what the upper bound onreranker performance is ?
the oracle performance.Table 2 shows the result of an experiment that in-vestigates these questions.
We used the rerankerbased on the Passive?Aggressive method in thisexperiment since this reranker gave the best re-sults in the previous experiment.Reranked Oraclek P R F P R F1 63.36 46.77 53.82 63.36 46.77 53.822 63.70 48.17 54.86 72.66 55.18 62.724 63.57 49.78 55.84 79.12 62.24 69.688 63.50 51.79 57.04 83.72 68.14 75.1316 63.00 52.94 57.54 86.92 72.79 79.2332 62.15 54.50 58.07 89.18 76.76 82.5164 61.02 55.67 58.22 91.08 80.19 85.28128 60.22 56.45 58.27 92.63 83.00 87.55256 59.87 57.22 58.51 94.01 85.27 89.43Table 2: Oracle and reranker performance as afunction of candidate set size.As is common in reranking tasks, the rerankercan exploit only a fraction of the potential im-provement ?
the reduction of the F-measure erroris between 10 and 15 percent of the oracle errorreduction for all candidate set sizes.The most visible effect of the reranker is thatthe recall is greatly improved.
However, this doesnot seem to have an adverse effect on the precisionuntil the candidate set size goes above 16 ?
in fact,the precision actually improves over the baselinefor small candidate set sizes.
After the size goesabove 16, the recall (and the F-measure) still rises,but at the cost of decreased precision.4.4 Syntactic and Semantic FeaturesWe studied the impact of syntactic and seman-tic structural features on the performance of thereranker.
Table 3 shows the result of the investi-gation for syntactic features.
Using all the syntac-tic features (and no semantic features) gives an F-measure roughly 4 points above the baseline, us-ing the PA reranker with a k of 64.
We then mea-sured the F-measure obtained when each one ofthe three syntactic features has been removed.
Itis clear that the unlexicalized syntactic path is themost important syntactic feature; the effect of thetwo lexicalized features seems to be negligible.System P R FBaseline 63.36 46.77 53.82All syntactic 62.45 53.19 57.45No SYN PATH 64.40 48.69 55.46No LEX PATH 62.62 53.19 57.52No DOMINANCE 62.32 52.92 57.24Table 3: Effect of syntactic features.A similar result was obtained when studying thesemantic features (Table 4).
Removing the con-necting labels feature, which is unlexicalized, hasa greater effect than removing the other two se-mantic features, which are lexicalized.System P R FBaseline 63.36 46.77 53.82All semantic 61.26 53.85 57.31No PREDICATE SL 61.28 53.81 57.30No PRED+ARGLBL 60.96 53.61 57.05No CONN ARGLBL 60.73 50.47 55.12Table 4: Effect of semantic features.4.5 Opinion Holder ExtractionTable 5 shows the performance of the opinionholder extractor.
The baseline applies the holder525classifier (3.3) to the opinions extracted by thebase sequence labeler (3.2), without modeling anyinteractions between opinions.
A large perfor-mance boost is then achieved simply by applyingthe opinion expression reranker (k = 64); this issimply the consequence of improved expressiondetection, since a correct expression is required toget credit for a holder).However, we can improve on this by addingthe holder interaction features: both the SHAREDHOLDERS and HOLDER TYPES + PATH featurescontribute to improving the recall even further.System P R FBaseline 57.66 45.14 50.64Reranked expressions 52.35 52.54 52.45SHARED HOLDERS 52.43 55.21 53.78HTYPES + PATH 52.22 54.41 53.30Both 52.28 55.99 54.07Table 5: Opinion holder extraction experiments.5 ConclusionWe have shown that features derived from gram-matical and semantic role structure can be usedto improve two fundamental tasks in fine-grainedopinion analysis: the detection of opinionated ex-pressions in subjectivity analysis, and the extrac-tion of opinion holders.
Our feature sets are basedon interaction between opinions, which makes ex-act inference intractable.
To overcome this issue,we used an implementation based on reranking:we first generated opinion expression sequencecandidates using a simple sequence labeler sim-ilar to the approach by Breck et al (2007).
Wethen applied SRL-inspired opinion holder extrac-tion classifiers, and finally a global model apply-ing to all opinions and holders.Our experiments show that the interaction-based models result in drastic improvements.
Sig-nificantly, we see significant boosts in recall (10points for both tasks) while the precision de-creases only slightly, resulting in clear F-measureimprovements.
This result compares favorablywith previously published results, which havebeen precision-oriented and scored quite low onrecall.We analyzed the impact of the syntactic and se-mantic features and saw that the best model is theone that makes use of both types of features.
Themost effective features we have found are purelystructural, i.e.
based on tree fragments in a syn-tactic or semantic tree.
Features involving wordsdid not seem to have the same impact.There are multiple opportunities for futurework in this area.
An important issue that we haveleft open is the coreference problem for holder ex-traction, which has been studied by Stoyanov andCardie (2006).
Similarly, recent work has tried toincorporate complex, high-level linguistic struc-ture such as discourse representations (Somasun-daran et al, 2009); it is clear that these structuresare very relevant for explaining the way humansorganize their expressions of opinions rhetori-cally.
However, theoretical depth does not nec-essarily guarantee practical applicability, and thechallenge is as usual to find a middle ground thatbalances our goals: explanatory power in theory,significant performance gains in practice, compu-tational tractability, and robustness in difficult cir-cumstances.6 AcknowledgementsThe research described in this paper has receivedfunding from the European Community?s SeventhFramework Programme (FP7/2007-2013) undergrant 231126: LivingKnowledge ?
Facts, Opin-ions and Bias in Time, and under grant 247758:Trustworthy Eternal Systems via Evolving Soft-ware, Data and Knowledge (EternalS).ReferencesBethard, Steven, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2005.
Extract-ing opinion propositions and opinion holders usingsyntactic and lexical cues.
In Shanahan, James G.,Yan Qu, and Janyce Wiebe, editors, Computing At-titude and Affect in Text: Theory and Applications.Breck, Eric, Yejin Choi, and Claire Cardie.
2007.Identifying expressions of opinion in context.
InProceedings of IJCAI-2007, Hyderabad, India.Choi, Yejin, Eric Breck, and Claire Cardie.
2006.Joint extraction of entities and relations for opinionrecognition.
In Proceedings of EMNLP 2006.526Collins, Michael.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and ex-periments with perceptron algorithms.
In Proceed-ings of the 2002 Conference on Empirical Methodsin Natural Language Processing (EMNLP 2002),pages 1?8.Crammer, Koby, Ofer Dekel, Joseph Keshet, ShaiShalev-Schwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 2006(7):551?585.Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Freund, Yoav and Robert E. Schapire.
1999.
Largemargin classification using the perceptron algo-rithm.
Machine Learning, 37(3):277?296.Johansson, Richard and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysiswith PropBank and NomBank.
In CoNLL 2008:Proceedings of the Twelfth Conference on NaturalLanguage Learning, pages 183?187, Manchester,United Kingdom.Joshi, Mahesh and Carolyn Penstein-Rose?.
2009.Generalizing dependency features for opinion min-ing.
In Proceedings of ACL/IJCNLP 2009, ShortPapers Track.Kim, Soo-Min and Eduard Hovy.
2006.
Extract-ing opinions, opinion holders, and topics expressedin online news media text.
In Proceedings ofACL/COLING Workshop on Sentiment and Subjec-tivity in Text.Kobayashi, Nozomi, Kentaro Inui, and Yuji Mat-sumoto.
2007.
Extracting aspect-evaluation andaspect-of relations in opinion mining.
In Proceed-ings of Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-CoNLL-2007).Mel?c?uk, Igor A.
1988.
Dependency Syntax: Theoryand Practice.
State University Press of New York,Albany.Meyers, Adam, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The NomBank project:An interim report.
In HLT-NAACL 2004 Work-shop: Frontiers in Corpus Annotation, pages 24?31,Boston, United States.Moschitti, Alessandro and Roberto Basili.
2004.Complex linguistic features for text classification:A comprehensive study.
In Proceedings of ECIR.Palmer, Martha, Dan Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?105.Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification us-ing machine learning techniques.
In Proceedings ofEMNLP.Ruppenhofer, Josef, Swapna Somasundaran, andJanyce Wiebe.
2008.
Finding the sources and tar-gets of subjective expressions.
In Proceedings ofLREC.Somasundaran, Swapna, Galileo Namata, JanyceWiebe, and Lise Getoor.
2009.
Supervised andunsupervised methods in employing discourse rela-tions for improving opinion polarity classification.In Proceedings of EMNLP.Stoyanov, Veselin and Claire Cardie.
2006.
Partiallysupervised coreference resolution for opinion sum-marization through structured rule learning.
In Pro-ceedings of EMNLP 2006.Tjong Kim Sang, Erik F. and Jorn Veenstra.
1999.Representing text chunks.
In Proceedings ofEACL99, pages 173?179, Bergen, Norway.Tsochantaridis, Iannis, Thorsten Joachims, ThomasHofmann, and Yasemin Altun.
2005.
Large marginmethods for structured and interdependent outputvariables.
Journal of Machine Learning Research,6(Sep):1453?1484.Wiebe, Janyce, Rebecca Bruce, and Thomas O?Hara.1999.
Development and use of a gold standard dataset for subjectivity classifications.
In Proceedingsof the 37th Annual Meeting of the Association forComputational Linguistics.Wiebe, Janyce, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2-3):165?210.Wilson, Theresa, Janyce Wiebe, and Paul Hoff-mann.
2005.
Recognizing contextual polarity inphrase-level sentiment analysis.
In Proceedings ofHLT/EMNLP 2005.Wu, Yuanbin, Qi Zhang, Xuanjing Huang, and LideWu.
2009.
Phrase dependency parsing for opinionmining.
In Proceedings of EMNLP.Yu, Hong and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separatingfacts from opinions and identifying the polarity ofopinion sentences.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP-2003), pages 129?136, Sap-poro, Japan.527
