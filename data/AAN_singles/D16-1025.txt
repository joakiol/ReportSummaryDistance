Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 257?267,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNeural versus Phrase-Based Machine Translation Quality: a Case StudyLuisa BentivogliFBK, TrentoItalyArianna BisazzaUniversity of AmsterdamThe NetherlandsMauro CettoloFBK, TrentoItalyMarcello FedericoFBK, TrentoItalyAbstractWithin the field of Statistical Machine Trans-lation (SMT), the neural approach (NMT) hasrecently emerged as the first technology ableto challenge the long-standing dominance ofphrase-based approaches (PBMT).
In particu-lar, at the IWSLT 2015 evaluation campaign,NMT outperformed well established state-of-the-art PBMT systems on English-German, alanguage pair known to be particularly hardbecause of morphology and syntactic differ-ences.
To understand in what respects NMTprovides better translation quality than PBMT,we perform a detailed analysis of neural vs.phrase-based SMT outputs, leveraging highquality post-edits performed by professionaltranslators on the IWSLT data.
For the firsttime, our analysis provides useful insights onwhat linguistic phenomena are best modeledby neural models ?
such as the reordering ofverbs ?
while pointing out other aspects thatremain to be improved.1 IntroductionThe wave of neural models has eventually reachedthe field of Statistical Machine Translation (SMT).After a period in which Neural MT (NMT) wastoo computationally costly and resource demandingto compete with state-of-the-art Phrase-Based MT(PBMT)1, the situation changed in 2015.
For thefirst time, in the latest edition of IWSLT2 (Cettolo et1We use the generic term phrase-based MT to cover standardphrase-based, hierarchical and syntax-based SMT approaches.2International Workshop on Spoken Language Translation(http://workshop2015.iwslt.org/)al., 2015), the system described in (Luong and Man-ning, 2015) overtook a variety of PBMT approacheswith a large margin (+5.3 BLEU points) on a diffi-cult language pair like English-German ?
anticipat-ing what, most likely, will be the new NMT era.This impressive improvement follows the dis-tance reduction previously observed in the WMT2015 shared translation task (Bojar et al, 2015).Just few months earlier, the NMT systems de-scribed in (Jean et al, 2015b) ranked on par withthe best phrase-based models on a couple of lan-guage pairs.
Such rapid progress stems from the im-provement of the recurrent neural network encoder-decoder model, originally proposed in (Sutskever etal., 2014; Cho et al, 2014b), with the use of the at-tention mechanism (Bahdanau et al, 2015).
Thisevolution has several implications.
On one side,NMT represents a simplification with respect to pre-vious paradigms.
From a management point of view,similar to PBMT, it allows for a more efficient useof human and data resources with respect to rule-based MT.
From the architectural point of view, alarge recurrent network trained for end-to-end trans-lation is considerably simpler than traditional MTsystems that integrate multiple components and pro-cessing steps.
On the other side, the NMT pro-cess is less transparent than previous paradigms.
In-deed, it represents a further step in the evolutionfrom rule-based approaches that explicitly manipu-late knowledge, to the statistical/data-driven frame-work, still comprehensible in its inner workings, toa sub-symbolic framework in which the translationprocess is totally opaque to the analysis.What do we know about the strengths of NMT257and the weaknesses of PBMT?
What are the linguis-tic phenomena that deep learning translation modelscan handle with such greater effectiveness?
To an-swer these questions and go beyond poorly informa-tive BLEU scores, we perform the very first compar-ative analysis of the two paradigms in order to shedlight on the factors that differentiate them and deter-mine their large quality differences.We build on evaluation data available for theIWSLT 2015 MT English-German task, and com-pare the results of the first four top-ranked partic-ipants.
We choose to focus on one language pairand one task because of the following advantages:(i) three state-of-the art PBMT systems comparedagainst the NMT system on the same data and inthe very same period (that of the evaluation cam-paign); (ii) a challenging language pair in terms ofmorphology and word order differences; (iii) avail-ability of MT outputs?
post-editing done by pro-fessional translators, which is very costly and thusrarely available.
In general, post-edits have the ad-vantage of allowing for informative and detailedanalyses since they directly point to translation er-rors.
In this specific framework, the high qualitydata created by professional translators guaranteesreliable evaluations.
For all these reasons we presentour study as a solid contribution to the better under-standing of this new paradigm shift in MT.After reviewing previous work (Section 2), we in-troduce the analyzed data and the systems that pro-duced them (Section 3).
We then present three in-creasingly fine levels of MT quality analysis.
Wefirst investigate how MT systems?
quality varies withspecific characteristics of the input, i.e.
sentencelength and type of content of each talk (Section 4).Then, we focus on differences among MT systemswith respect to morphology, lexical, and word or-der errors (Section 5).
Finally, based on the findingthat word reordering is the strongest aspect of NMTcompared to the other systems, we carry out a fine-grained analysis of word order errors (Section 6).2 Previous WorkTo date, NMT systems have only been evaluated byBLEU in single-reference setups (Bahdanau et al,2015; Sutskever et al, 2014; Luong et al, 2015;Jean et al, 2015a; Gu?lc?ehre et al, 2015).
Ad-ditionally, the Montreal NMT system submitted toWMT 2015 (Jean et al, 2015b) was part of a man-ual evaluation experiment where a large number ofnon-professional annotators were asked to rank theoutputs of multiple MT systems (Bojar et al, 2015).Results for the Montreal system were very positive?
ranked first in English-German, third in German-English, English-Czech and Czech-English ?
whichconfirmed and strengthened the BLEU results pub-lished so far.
Unfortunately neither BLEU nor man-ual ranking judgements tell us which translation as-pects are better modeled by different MT frame-works.
To this end, a detailed and systematic erroranalysis of NMT vs. PBMT output is required.Translation error analysis, as a way to identifysystems?
weaknesses and define priorities for theirimprovement, has received a fair amount of atten-tion in the MT community.
In this work we opt forthe automatic detection and classification of transla-tion errors based on manual post-edits of the MToutput.
We believe this choice provides an opti-mal trade-off between fully manual error analysis(Farru?s Cabeceran et al, 2010; Popovic?
et al, 2013;Daems et al, 2014; Federico et al, 2014; Neubiget al, 2015), which is very costly and complex,and fully automatic error analysis (Popovic?
and Ney,2011; Irvine et al, 2013), which is noisy and biasedtowards one or few arbitrary reference translations.Existing tools for translation error detectionare either based on Word Error Rate (WER)and Position-independent word Error Rate (PER)(Popovic?, 2011) or on output-reference alignment(Zeman et al, 2011).
Regarding error classifi-cation, Hjerson (Popovic?, 2011) detects five maintypes of word-level errors as defined in (Vilar et al,2006): morphological, reordering, missing words,extra words, and lexical choice errors.
We followa similar but simpler error classification (morpho-logical, lexical, and word order errors), but detectthe errors differently using TER as this is the mostnatural choice in our evaluation framework based onpost-edits (see also Section 3.4).
Irvine et al (2013)propose another word-level error analysis techniquespecifically focused on lexical choice and aimed atunderstanding the effects of domain differences onMT.
Their error classification is strictly related tomodel coverage and insensitive to word order dif-ferences.
The technique requires access to the sys-258tem?s phrase table and is thus not applicable to NMT,which does not rely on a fixed inventory of transla-tion units extracted from the parallel data.Previous error analyses based on manually post-edited translations were presented in (Bojar, 2011;Koponen, 2012; Popovic?
et al, 2013).
We are thefirst to conduct this kind of study on the output of aneural MT system.3 Experimental SettingWe perform a number of analyses on data and re-sults of the IWSLT 2015 MT En-De task, whichconsists in translating manual transcripts of EnglishTED talks into German.
Evaluation data are pub-licly available through the WIT3 repository (Cettoloet al, 2012).33.1 Task DataTED Talks4 are a collection of rather short speeches(max 18 minutes each, roughly equivalent to 2,500words) covering a wide variety of topics.
All talkshave captions, which are translated into many lan-guages by volunteers worldwide.
Besides represent-ing a popular benchmark for spoken language tech-nology, TED Talks embed interesting research chal-lenges.
Translating TED Talks implies dealing withspoken rather than written language, which is henceexpected to be structurally less complex, formal andfluent (Ruiz and Federico, 2014).
Moreover, as hu-man translations of the talks are required to followthe structure and rhythm of the English captions, alower amount of rephrasing and reordering is ex-pected than in the translation of written documents.As regards the English-German language pair, thetwo languages are interesting since, while belongingto the same language family, they have marked dif-ferences in levels of inflection, morphological varia-tion, and word order, especially long-range reorder-ing of verbs.3.2 Evaluation DataFive systems participated in the MT En-De task andwere manually evaluated on a representative subsetof the official 2015 test set.
The Human Evaluation(HE) set includes the first half of each of the 12 test3wit3.fbk.eu4http://www.ted.com/System Approach DataPBSY Combination: Phrase+Syntax-based 175M/(Huck and GHKM string-to-tree; hierarchical + 3.1BBirch, 2015) sparse lexicalized reordering modelsHPB Hierarchical Phrase-based 166M/(Jehl et al, source pre-ordering (dependency tree 854M2015) -based); re-scoring with neural LMSPB Standard Phrase-based 117M/(Ha et al, source pre-ordering (POS- and tree- 2.4B2015) based); re-scoring with neural LMsNMT Recurrent neural network (LSTM) 120M/(Luong & Man- attention-based; source reversing; ?ning, 2015) rare words handlingTable 1: MT systems?
overview.
Data column: size of paral-lel/monolingual training data for each system in terms of En-glish and German tokens.talks, for a total of 600 sentences and around 10Kwords.
Five professional translators were asked topost-edit the MT output by applying the minimal ed-its required to transform it into a fluent sentence withthe same meaning as the source sentence.
Data wereprepared so that all translators equally post-editedthe five MT outputs, i.e.
120 sentences for each eval-uated system.The resulting evaluation data consist of five newreference translations for each of the sentences inthe HE set.
Each one of these references representsthe targeted translation of the system output fromwhich it was derived, but the other four additionaltranslations can also be used to evaluate each MTsystem.
We will see in the next sections how we ex-ploited the available post-edits in the more suitableway depending on the kind of analysis carried out.3.3 MT SystemsOur analysis focuses on the first four top-rankingsystems, which include NMT (Luong and Manning,2015) and three different phrase-based approaches:standard phrase-based (Ha et al, 2015), hierarchi-cal (Jehl et al, 2015) and a combination of phrase-based and syntax-based (Huck and Birch, 2015).
Ta-ble 1 presents an overview of each system, as wellas figures about the training data used.5The phrase+syntax-based (PBSY) system com-bines the outputs of a string-to-tree decoder, trainedwith the GHKM algorithm, with those of two stan-5Detailed information about training data was kindly madeavailable by participating teams.259dard phrase-based systems featuring, among others,adapted phrase tables and language models enrichedwith morphological information, hierarchical lexi-calized reordering models and different variations ofthe operational sequence model.The hierarchical phrase-based MT (HPB) systemleverages thousands of lexicalised features, data-driven source pre-ordering (dependency tree-based),word-based and class-based language models, andn-best re-scoring models based on syntactic and neu-ral language models.The standard phrase-based MT (SPB) system fea-tures an adapted phrase-table combining in-domainand out-domain data, discriminative word lexiconmodels, multiple language models (word-, POS- andclass-based), data-driven source pre-ordering (POS-and constituency syntax-based), n-best re-scoringmodels based on neural lexicons and neural lan-guage models.Finally, the neural MT (NMT) system is an en-semble of 8 long short-term memory (LSTM) net-works of 4 layers featuring 1,000-dimension wordembeddings, attention mechanism, source revers-ing, 50K source and target vocabularies, and out-of-vocabulary word handling.
Training with TED datawas performed on top of models trained with largeout-domain parallel data.With respect to the use of training data, it is worthnoticing that NMT is the only system not employ-ing monolingual data in addition to parallel data.Moreover, NMT and SPB were trained with smalleramounts of parallel data with respect to PBSY andHPB (see Table 1).3.4 Translation Edit Rate MeasuresThe Translation Edit Rate (TER) (Snover et al,2006) naturally fits our evaluation framework,where it traces the edits done by post-editors.
Also,TER shift operations are reliable indicators of re-ordering errors, in which we are particularly inter-ested.
We exploit the available post-edits in two dif-ferent ways: (i) for Human-targeted TER (HTER)we compute TER between the machine translationand its manually post-edited version (targeted ref-erence), (ii) for Multi-reference TER (mTER), wecompute TER against the closest translation amongall available post-edits (i.e.
targeted and additionalreferences) for each sentence.system BLEU HTER mTERPBSY 25.3 28.0 21.8HPB 24.6 29.9 23.4SPB 25.8 29.0 22.7NMT 31.1?
21.1?
16.2?Table 2: Overall results on the HE Set: BLEU, computedagainst the original reference translation, and TER, computedwith respect to the targeted post-edit (HTER) and multiple post-edits (mTER).Throughout sections 4 and 5, we mark a scoreachieved by NMT with the symbol * if this is bet-ter than the score of its best competitor at statisticalsignificance level 0.01.
Significance tests for HTERand mTER are computed by bootstrap re-sampling,while differences among proportions are assessedvia one-tailed z-score tests.4 Overall Translation QualityTable 2 presents overall system results accordingto HTER and mTER, as well as BLEU computedagainst the original TED Talks reference translation.We can see that NMT clearly outperforms all otherapproaches both in terms of BLEU and TER scores.Focusing on mTER results, the gain obtained byNMT over the second best system (PBSY) amountsto 26%.
It is also worth noticing that mTER is con-siderably lower than HTER for each system.
This re-duction shows that exploiting all the available post-edits as references for TER is a viable way to controland overcome post-editors variability, thus ensuringa more reliable and informative evaluation about thereal overall performance of MT systems.
For thisreason, the two following analyses rely on mTER.In particular, we investigate how specific character-istics of input documents affect the system?s overalltranslation quality, focusing on (i) sentence lengthand (ii) the different talks composing the dataset.4.1 Translation quality by sentence lengthLong sentences are known to be difficult to trans-late by the NMT approach.
Following previous work(Cho et al, 2014a; Pouget-Abadie et al, 2014; Bah-danau et al, 2015; Luong et al, 2015), we investi-gate how sentence length affects overall translationquality.
Figure 1 plots mTER scores against sourcesentence length.
NMT clearly outperforms everyPBMT system in any length bin, with statistically260Figure 1: mTER scores on bins of sentences of different length.Points represent the average mTER of the MT outputs for thesentences in each given bin.significant differences.
As a general tendency, theperformance of all approaches worsens as sentencelength increases.
However, for sentences longer than35 words we see that NMT quality degrades moremarkedly than in PBMT systems.
Considering thepercentage decrease with respect to the precedinglength bin (26-35), we see that the %?
for NMT(-15.4) is much larger than the average %?
for thethree PBMT systems (-7.9).
Hence, this still seemsan issue to be addressed for further improving NMT.4.2 Translation quality by talkAs we saw in Section 3.1, the TED dataset is veryheterogeneous since it consists of talks covering dif-ferent topics and given by speakers with differentstyles.
It is therefore interesting to evaluate trans-lation quality also at the talk level.Figure 2 plots the mTER scores for each of thetwelve talks included in the HE set, sorted in ascend-ing order of NMT scores.
In all talks, the NMT sys-tem outperforms the PBMT systems in a statisticallysignificant way.We analysed different factors which could impacttranslation quality in order to understand if they cor-relate with such performance differences.
We stud-ied three features which are typically considered asindicators of complexity (see (Franc?ois and Fairon,2012) for an overview), namely (i) the length of thetalk, (ii) its average sentence length, and (iii) theFigure 2: mTER scores per talk, sorted in ascending order ofNMT scores.type-token ratio6 (TTR) which ?
measuring lexicaldiversity ?
reflects the size of a speaker?s vocabularyand the variety of subject matter in a text.For the first two features we did not find any cor-relation; on the contrary, we found a moderate Pear-son correlation (R=0.7332) between TTR and themTER gains of NMT over its closest competitor ineach talk.
This result suggests that NMT is able tocope with lexical diversity better than any other con-sidered approach.5 Analysis of Translation ErrorsWe now turn to analyze which types of linguistic er-rors characterize NMT vs. PBMT.
In the literature,various error taxonomies covering different levels ofgranularity have been developed (Flanagan, 1994;Vilar et al, 2006; Farru?s Cabeceran et al, 2010;Stymne and Ahrenberg, 2012; Lommel et al, 2014).We focus on three error categories, namely (i) mor-phology errors, (ii) lexical errors, and (iii) word or-der errors.
As for lexical errors, a number of existingtaxonomies further distinguish among translation er-rors due to missing words, extra words, or incor-rect lexical choice.
However, given the proven dif-ficulty of disambiguating between these three sub-classes (Popovic?
and Ney, 2011; Fishel et al, 2012),we prefer to rely on a more coarse-grained linguisticerror classification where lexical errors include all ofthem (Farru?s Cabeceran et al, 2010).6The type-token-ratio of a text is calculated dividing thenumber of word types (vocabulary) by the total number of wordtokens (occurrences).261For error analysis we rely on HTER results underthe assumption that, since the targeted translation isgenerated by post-editing the given MT output, thismethod is particularly informative to spot MT er-rors.
We are aware that translator subjectivity is stillan issue (see Section 4), however in this more fine-grained analysis we prefer to focus on what a hu-man implicitly annotated as a translation error.
Thisparticularly holds in our specific evaluation frame-work, where the goal is not to measure the absolutenumber of errors made by each system, but to com-pare systems with each other.
Moreover, the post-edits collected for each MT output within IWSLTallow for a fair and reliable comparison since sys-tems were equally post-edited by all translators (seeSection 3.2), making all analyses uniformly affectedby such variability.5.1 Morphology errorsA morphology error occurs when a generated wordform is wrong but its corresponding base form(lemma) is correct.
Thus, we assess the ability ofsystems to deal with morphology by comparing theHTER score computed on the surface forms (i.e.morphologically inflected words) with the HTERscore obtained on the corresponding lemmas.
Theadditional matches counted on lemmas with respectto word forms indicate morphology errors.
Thus, thecloser the two HTER scores, the more accurate thesystem in handling morphology.To carry out this analysis, the lemmatized (andPOS tagged) version of both MT outputs and cor-responding post-edits was produced with the Ger-man parser ParZu (Sennrich et al, 2013).
Then, theHTER-based evaluation was slightly adapted in or-der to be better suited to an accurate detection ofmorphology errors.
First, punctuation was removedsince ?
not being subject to morphological inflection?
it could smooth the results.
Second, shift errorswere not considered.
A word form or a lemma thatmatches a corresponding word or lemma in the post-edit, but is in the wrong position with respect to it,is counted as a shift error in TER.
Instead ?
whenfocusing on morphology ?
exact matches are not er-rors, regardless their position in the text.77Note that the TER score calculated by setting to 0 thecost of shifts approximates the Position-independent ErrorRate (Tillmann et al, 1997).system HTERnoShftword lemma %?PBSY 27.1 22.5 -16.9HPB 28.7 23.5 -18.4SPB 28.3 23.2 -18.0NMT 21.7?
18.7?
-13.7Table 3: HTER ignoring shift operations computed on wordsand corresponding lemmas, and their % difference.Table 3 presents HTER scores on word forms andlemmas, as well as their percentage difference whichgives an indication of morphology errors.
We cansee that NMT generates translations which are mor-phologically more correct than the other systems.
Inparticular, the %?
for NMT (-13.7) is lower thanthat of the second best system (PBSY, -16.9) by3.2% absolute points, leading to a percentage gainof around 19%.
We can thus say that NMT makesat least 19% less morphology errors than any otherPBMT system.5.2 Lexical errorsAnother important feature of MT systems is theirability to choose lexically appropriate words.
In or-der to compare systems under this aspect, we con-sider HTER results at the lemma level as a wayto abstract from morphology errors and focus onlyon actual lexical choice problems.
The evaluationon the lemmatised version of the data performed toidentify morphology errors fits this purpose, sinceits driving assumptions (i.e.
punctuation can be ex-cluded and lemmas in the wrong order are not errors)hold for lexical errors too.The lemma column of Table 3 shows that NMToutperforms the other systems.
More precisely, theNMT score (18.7) is better than the second best(PBSY, 22.5) by 3.8% absolute points.
This corre-sponds to a relative gain of about 17%, meaning thatNMT makes at least 17% less lexical errors than anyPBMT system.
Similarly to what observed for mor-phology errors, this can be considered a remarkableimprovement over the state of the art.5.3 Word order errorsTo analyse reordering errors, we start by focusing onshift operations identified by the HTER metrics.
Thefirst three columns of Table 4 show, respectively:(i) the number of words generated by each system262system #words #shifts %shifts KRSPBSY 11,517 354 3.1 84.6HPB 11,417 415 3.6 84.3SPB 11,420 398 3.5 84.5NMT 11,284 173 1.5?
88.3?Table 4: Word reordering evaluation in terms of shift opera-tions in HTER calculation and of KRS.
For each system, thenumber of generated words, the number of shift errors and theircorresponding percentages are reported.
(ii) the number of shifts required to align each sys-tem output to the corresponding post-edit; and (iii)the corresponding percentage of shift errors.
Noticethat the shift error percentages are incorporated inthe HTER scores reported in Table 2.
We can seein Table 4 that shift errors in NMT translations aredefinitely less than in the other systems.
The errorreduction of NMT with respect to the second bestsystem (PBSY) is about 50% (173 vs. 354).It should be recalled that these numbers only re-fer to shifts detected by HTER, that is (groups of)words of the MT output and corresponding post-editthat are identical but occurring in different positions.Words that had to be moved and modified at thesame time (for instance replaced by a synonym ora morphological variant) are not counted in HTERshift figures, but are detected as substitution, inser-tion or deletion operations.
To ensure that our re-ordering evaluation is not biased towards the align-ment between the MT output and the post-edit per-formed by HTER, we run an additional assessmentusing KRS ?
Kendall Reordering Score (Birch etal., 2010) ?
which measures the similarity betweenthe source-reference reorderings and the source-MToutput reorderings.8 Being based on bilingual wordalignment via the source sentence, KRS detects re-ordering errors also when post-edit and MT wordsare not identical.
Also unlike TER, KRS is sensitiveto the distance between the position of a word in theMT output and that in the reference.Looking at the last column of Table 4, we can saythat our observations on HTER are confirmed by theKRS results: the reorderings performed by NMT aremuch more accurate than those performed by anyPBMT system.9 Moreover, according to the approx-8To compute the word alignments required by KRS, we usedthe FastAlign tool (Dyer et al, 2013).9To put our results into perspective, note that Birch (2011)imate randomization test, KRS differences are statis-tically significant between NMT and all other sys-tems, but not among the three PBMT systems.Given the concordant results of our two quanti-tative analyses, we conclude that one of the ma-jor strengths of the NMT approach is its ability toplace German words in the right position even whenthis requires considerable reordering.
This outcomecalls for a deeper investigation, which is carried outin the following section.6 Fine-grained Word Order ErrorAnalysisWe have observed that word reordering is a verystrong aspect of NMT compared to PBMT, accord-ing to both HTER and KRS.
To better understandthis finding, we investigate whether reordering er-rors concentrate on specific linguistic constructionsacross our systems.
Using the POS tagging anddependency parsing of the post-edits produced byParZu, we classify the shift operations detected byHTER and count how often a word with a given POSlabel was misplaced by each of the systems (alone oras part of a shifted block).
For each word class, wealso compute the percentage order error reductionof NMT with respect to the PBMT system that hashighest reordering accuracy overall, that is PBSY.Results are presented in Table 5, ranked by NMT-vs-PBSY gain.
Punctuation is omitted as well asword classes that were shifted less than 10 times byall systems.
Examples of salient word order errortypes are presented in Table 6.The upper part of Table 5 shows that verbs areby far the most often misplaced word category in allPBMT systems ?
an issue already known to affectstandard phrase-based SMT between German andEnglish (Bisazza and Federico, 2013).
Reordering isparticularly difficult when translating into German,since the position of verbs in this language variesaccording to the clause type (e.g.
main vs. subor-dinate).
Our results show that even syntax-informedPBMT does not solve this issue.
Using syntax atdecoding time, as done by one of the systems com-bined within PBSY, appears to be a better strategyreports a difference of 5 KRS points between the translations ofa PBMT system and those produced by four human translatorstested against each other, in a Chinese-English experiment.263Class NMT- NMT PBSY HPB SPBvs-PBSYV -70% 35 116 133 155PRO -57% 22 51 53 62PTKZU -54% 6 13 4 11ADV -50% 14 28 44 36N -47% 37 70 99 56KON -33% 6 9 8 12PREP -18% 18 22 27 28PTKNEG -17% 10 12 10 7ART -4% 26 27 38 35aux:V -87% 3 23 17 18neb:V -83% 2 12 7 19objc:V -79% 3 14 21 24subj:PRO -70% 12 40 34 46root:V -68% 6 19 28 27adv:ADV -67% 8 24 33 28obja:N -65% 6 17 28 12cj:V -59% 7 17 21 22part:PTKZU -54% 6 13 4 11obja:PRO -38% 5 8 14 7mroot:V -36% 7 11 26 20pn:N -36% 16 25 33 19subj:N -33% 6 9 10 7pp:PREP -30% 14 20 19 23adv:PTKNEG -17% 10 12 10 7det:ART -4% 26 27 38 34all -48% 222 429 493 488Table 5: Main POS tags and dependency labels of words oc-curring in shifted blocks detected by HTER.
NMT-vs-PBSYdenotes the reduction of reordering errors in NMT vs. PBSYsystem.
Only word classes that were shifted 10 or more timesin at least one system output are shown.than using it for source pre-ordering, as done by theHPB and SPB systems.
However this only resultsin a moderate reduction of verb reordering errors (-12% and -25% vs. HPB and SPB respectively).
Onthe contrary, NMT reduces verb order errors by animpressive -70% with respect to PBSY (-74% and-77% vs. HPB and SPB respectively) despite beingtrained on raw parallel data without any syntacticannotation, nor explicit modeling of word reorder-ing.
This result shows that the recurrent neural lan-guage model at the core of the NMT architecture isvery successful at generating well-formed sentenceseven in languages with less predictable word order,like German (see examples in Table 6(a,b)).
NMT,though, gains notably less on nouns (-47%), whichis the second most often misplaced word categoryin PBSY.
More insight on this is provided by thelower part of the table, where reordering errors aredivided by their dependency label as well as POStag.
Here we see that order errors on nouns arenotably reduced by NMT when they act as syntac-tic objects (-65% obja:N) but less when they act aspreposition complements (-36% pn:N) or subjects (-33% subj:N).The smallest NMT-vs-PBSY gains are observedon prepositions (-18% PREP), negation particles(-17% PTKNEG) and articles (-4% ART).
Manualinspection of a data sample reveals that misplacedprepositions are often part of misplaced preposi-tional phrases acting, for instance, as temporal orinstrumental adjuncts (e.g.
?in my life?, ?with thisvideo?).
In these cases, the original MT output isoverall understandable and grammatical, but doesnot conform to the order of German semantic argu-ments that is consistently preferred by post-editors(see example in Table 6(c)).
Articles, due to theircommonness, are often misaligned by HTER andmarked as shift errors instead of being marked astwo unrelated substitutions.
Finally, negation parti-cles account for less than 1% of the target tokens butplay a key role in determining the sentence meaning.Looking closely at some error examples, we foundthat the correct placement of the German particlenicht was determined by the focus of negation in thesource sentence, which is difficult to detect in En-glish.
For instance in Table 6(d) two interpretationsare possible (?that did not work?
or ?that worked, butnot for systematic reasons?
), each resulting in a dif-ferent, but equally grammatical, location of nicht.
Infact, negation-focus detection calls for a deep un-derstanding of the sentence semantics, often requir-ing extra-sentential context (Blanco and Moldovan,2011).
When faced with this kind of translation de-cisions, NMT performs as poorly as its competitors.In summary, our fine-grained analysis confirmsthat NMT concentrates its word order improvementson important linguistic constituents and, specificallyin English-German, is very close to solving the infa-mous problem of long-range verb reordering whichso many PBMT approaches have only poorly man-aged to handle.
On the other hand, NMT still strug-gles with more subtle translation decisions depend-ing, for instance, on the semantic ordering of adjunctprepositional phrases or on the focus of negation.264Auxiliary-main verb construction [aux:V]:SRC in this experiment , individuals were shown hundreds of hours of YouTube videosHPB in diesem Experiment , Individuen gezeigt wurden Hunderte von Stunden YouTube-Videos%(a) PE in diesem Experiment wurden Individuen Hunderte von Stunden Youtube-Videos gezeigtNMT in diesem Experiment wurden Individuen hunderte Stunden YouTube Videos gezeigt!PE in diesem Experiment wurden Individuen hunderte Stunden YouTube Videos gezeigtVerb in subordinate (adjunct) clause [neb:V]:SRC ... when coaches and managers and owners look at this information streaming ...PBSY ... wenn Trainer und Manager und Eigentu?mer betrachten diese Information Streaming ...%(b) PE ... wenn Trainer und Manager und Eigentu?mer dieses Informations-Streaming betrachten ...NMT ... wenn Trainer und Manager und Besitzer sich diese Informationen anschauen ...!PE ... wenn Trainer und Manager und Besitzer sich diese Informationen anschauen ...Prepositional phrase [pp:PREP det:ART pn:N] acting as temporal adjunct:SRC so like many of us , I ?ve lived in a few closets in my lifeSPB so wie viele von uns , ich habe in ein paar Schra?nke in meinem Leben gelebt%(c) PE so habe ich wie viele von uns wa?hrend meines Lebens in einigen Verstecken gelebtNMT wie viele von uns habe ich in ein paar Schra?nke in meinem Leben gelebt%PE wie viele von uns habe ich in meinem Leben in ein paar Schra?nken gelebtNegation particle [adv:PTKNEG]:SRC but I eventually came to the conclusion that that just did not work for systematic reasonsHPB aber ich kam schlielich zu dem Schluss , dass nur aus systematischen Gru?nden nicht funktionieren!
(d) PE aber ich kam schlielich zu dem Schluss , dass es einfach aus systematischen Gru?nden nicht funktioniertNMT aber letztendlich kam ich zu dem Schluss , dass das einfach nicht aus systematischen Gru?nden funktionierte%PE ich musste aber einsehen , dass das aus systematischen Gru?nden nicht funktioniertTable 6: MT output and post-edit examples showing common types of reordering errors.7 ConclusionsWe analysed the output of four state-of-the-art MTsystems that participated in the English-to-Germantask of the IWSLT 2015 evaluation campaign.
Ourselected runs were produced by three phrase-basedMT systems and a neural MT system.
The analysisleveraged high quality post-edits of the MT outputs,which allowed us to profile systems with respect toreliable measures of post-editing effort and transla-tion error types.The outcomes of the analysis confirm that NMThas significantly pushed ahead the state of the art,especially in a language pair involving rich morphol-ogy prediction and significant word reordering.
Tosummarize our findings: (i) NMT generates outputsthat considerably lower the overall post-edit effortwith respect to the best PBMT system (-26%); (ii)NMT outperforms PBMT systems on all sentencelengths, although its performance degrades fasterwith the input length than its competitors; (iii) NMTseems to have an edge especially on lexically richtexts; (iv) NMT output contains less morphology er-rors (-19%), less lexical errors (-17%), and substan-tially less word order errors (-50%) than its closestcompetitor for each error type; (v) concerning wordorder, NMT shows an impressive improvement inthe placement of verbs (-70% errors).While NMT proved superior to PBMT with re-spect to all error types that were investigated, ouranalysis also pointed out some aspects of NMT thatdeserve further work, such as the handling of longsentences and the reordering of particular linguisticconstituents requiring a deep semantic understand-ing of text.
Machine translation is definitely not asolved problem, but the time is finally ripe to tackleits most intricate aspects.AcknowledgmentsFBK authors were supported by the CRACKER,QT21 and ModernMT projects, which receivedfunding from the European Union?s Horizon 2020programme under grants No.
645357, 645452 and645487.
AB was funded in part by the NWO underprojects 639.022.213 and 612.001.218.265ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In Proc.
of ICLR, SanDiego, US-CA.Alexandra Birch, Miles Osborne, and Phil Blunsom.2010.
Metrics for MT evaluation: evaluating reorder-ing.
Machine Translation, 24(1):15?26.Alexandra Birch.
2011.
Reordering Metrics for Statisti-cal Machine Translation.
Ph.D. thesis, School of In-formatics, University of Edinburgh, UK.Arianna Bisazza and Marcello Federico.
2013.
Effi-cient solutions for word reordering in German-Englishphrase-based statistical machine translation.
In Proc.of WMT, Sofia, Bulgaria.Eduardo Blanco and Dan Moldovan.
2011.
Semanticrepresentation of negation using focus detection.
InProc.
of ACL-HLT, Portland, US-OR.Ondr?ej Bojar, Rajen Chatterjee, Christian Federmann,Barry Haddow, Matthias Huck, Chris Hokamp, PhilippKoehn, Varvara Logacheva, Christof Monz, MatteoNegri, Matt Post, Carolina Scarton, Lucia Specia, andMarco Turchi.
2015.
Findings of the 2015 workshopon statistical machine translation.
In Proc.
of WMT,Lisbon, Portugal.Ondrej Bojar.
2011.
Analyzing error types in English-Czech machine translation.
The Prague Bulletin ofMathematical Linguistic, (95):63?76.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
WIT3: Web Inventory of Transcribedand Translated Talks.
In Proc.
of EAMT, Trento, Italy.Mauro Cettolo, Jan Niehues, Sebastian Stu?ker, LuisaBentivogli, Roldano Cattoni, and Marcello Federico.2015.
The IWSLT 2015 evaluation campaign.
InProc.
of IWSLT, Da Nang, Vietnam.Kyunghyun Cho, Bart van Merrie?nboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014a.
On the proper-ties of neural machine translation: encoder?decoderapproaches.
In Proc.
of SSST-8, Doha, Qatar.Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014b.
Learning phrase repre-sentations using RNN encoder?decoder for statisticalmachine translation.
In Proc.
of EMNLP, Doha, Qatar.Joke Daems, Lieve Macken, and Sonia Vandepitte.
2014.On the origin of errors: a fine-grained analysis of MTand PE errors and their relationship.
In Proc.
of LREC,Reykjavik, Iceland.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameterizationof IBM model 2.
In Proc.
of NACL-HLT, Atlanta, US-GA.Mireia Farru?s Cabeceran, Marta Ruiz Costa-Jussa`,Jose?
Bernardo Marin?o Acebal, and Jose?
Adria?nRodr?
?guez Fonollosa.
2010.
Linguistic-based evalu-ation criteria to identify statistical machine translationerrors.
In Proc.
of EAMT, Saint-Raphae?l, France.Marcello Federico, Matteo Negri, Luisa Bentivogli, andMarco Turchi.
2014.
Assessing the impact of transla-tion errors on machine translation quality with mixed-effects models.
In Proc.
of EMNLP, Doha, Qatar.Mark Fishel, Ondrej Bojar, and Maja Popovic?.
2012.Terra: a collection of translation error-annotated cor-pora.
In Proc.
of LREC, Istanbul, Turkey.Mary Flanagan.
1994.
Error classification for MT evalu-ation.
In Proc.
of AMTA, Columbia, US-MD.Thomas Franc?ois and Ce?drick Fairon.
2012.
An ?AIreadability?
formula for French as a foreign language.In Proc.
of EMNLP-CoNLL, Jeju Island, Korea.C?aglar Gu?lc?ehre, Orhan Firat, Kelvin Xu, KyunghyunCho, Lo?
?c Barrault, Huei-Chi Lin, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2015.
On us-ing monolingual corpora in neural machine translation.CoRR, abs/1503.03535.Thanh-Le Ha, Jan Niehues, Eunah Cho, Mohammed Me-diani, and Alex Waibel.
2015.
The KIT translationsystems for IWSLT 2015.
In Proc.
of IWSLT, DaNang, Vietnam.Matthias Huck and Alexandra Birch.
2015.
The Edin-burgh machine translation systems for IWSLT 2015.In Proc.
of IWSLT, Da Nang, Vietnam.Ann Irvine, John Morgan, Marine Carpuat, Hal Daume?III, and Dragos Munteanu.
2013.
Measuring machinetranslation errors in new domains.
Transactions ofthe Association for Computational Linguistics, 1:429?440.Se?bastien Jean, Kyunghyun Cho, Roland Memisevic, andYoshua Bengio.
2015a.
On using very large targetvocabulary for neural machine translation.
In Proc.
ofACL-IJCNLP, Beijing, China.Se?bastien Jean, Orhan Firat, Kyunghyun Cho, RolandMemisevic, and Yoshua Bengio.
2015b.
Montrealneural machine translation systems for WMT15.
InProc.
of WMT, Lisbon, Portugal.Laura Jehl, Patrick Simianer, Julian Hitschler, and Ste-fan Riezler.
2015.
The Heidelberg university English-German translation system for IWSLT 2015.
In Proc.of IWSLT, Da Nang, Vietnam.Maarit Koponen.
2012.
Comparing human perceptionsof post-editing effort with post-editing operations.
InProc.
of WMT, Montre?al, Canada.Arle Lommel, Aljoscha Burchardt, Maja Popovic?, KimHarris, Eleftherios Avramidis, and Hans Uszkoreit.2014.
Using a new analytic measure for the annota-tion and analysis of MT errors on real data.
In Proc.of EAMT, Dubrovnik, Croatia.266Minh-Thang Luong and Christopher D Manning.
2015.Stanford neural machine translation systems for spo-ken language domains.
In Proc.
of IWSLT, Da Nang,Vietnam.Thang Luong, Hieu Pham, and Christopher D. Manning.2015.
Effective approaches to attention-based neuralmachine translation.
In Proc.
of EMNLP, Lisbon, Por-tugal.Graham Neubig, Makoto Morishita, and Satoshi Naka-mura.
2015.
Neural Reranking Improves SubjectiveQuality of Machine Translation: NAIST at WAT2015.In Proc.
of WAT2015, Kyoto, Japan.Maja Popovic?
and Hermann Ney.
2011.
Towards au-tomatic error analysis of machine translation output.Computational Linguistics, 37(4):657?688.Maja Popovic?, Eleftherios Avramidis, Aljoscha Bur-chardt, Sabine Hunsicker, Sven Schmeier, CindyTscherwinka, David Vilar, and Hans Uszkoreit.
2013.Learning from human judgments of machine transla-tion output.
In Proc.
of MT Summit, Nice, France.Maja Popovic?.
2011.
Hjerson: an open source tool forautomatic error classification of machine translationoutput.
The Prague Bulletin of Mathematical Linguis-tic, (96):59?68.Jean Pouget-Abadie, Dzmitry Bahdanau, Bart van Mer-rienboer, Kyunghyun Cho, and Yoshua Bengio.
2014.Overcoming the curse of sentence length for neuralmachine translation using automatic segmentation.
InProc.
of SSST-8, Doha, Qatar.Nicholas Ruiz and Marcello Federico.
2014.
Complexityof spoken versus written language for machine trans-lation.
In Proc.
of EAMT, Dubrovnik, Croatia.Rico Sennrich, Martin Volk, and Gerold Schneider.
2013.Exploiting synergies between open resources for Ger-man dependency parsing, POS-tagging, and morpho-logical analysis.
In Proc.
of RANLP, Hissar, Bulgaria.Matthew Snover, Bonnie Dorr, Rich Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
InProc.
of AMTA, Boston, US-MA.Sara Stymne and Lars Ahrenberg.
2012.
On the practiceof error analysis for machine translation evaluation.
InProc.
of LREC, Istanbul, Turkey.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural networks.In Proc.
of NIPS, Montre?al, Canada.Christoph Tillmann, Stephan Vogel, Hermann Ney,Alexander Zubiaga, and Hassan Sawaf.
1997.
Ac-celerated DP based search for statistical translation.
InProc.
of Eurospeech, Rhodes, Greece.David Vilar, Jia Xu, Luis Fernando d?Haro, and HermannNey.
2006.
Error analysis of statistical machine trans-lation output.
In Proc.
of LREC, Genoa, Italy.Daniel Zeman, Mark Fishel, Jan Berka, and Ondrej Bo-jar.
2011.
Addicter: what is wrong with my transla-tions?
The Prague Bulletin of Mathematical Linguis-tic, (96):79?88.267
