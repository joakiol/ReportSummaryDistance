Proc.
of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 14?18,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsLearning the hyperparameters to learn morphologyStella FrankILCC, School of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKsfrank@inf.ed.ac.ukAbstractWe perform hyperparameter inferencewithin a model of morphology learn-ing (Goldwater et al., 2011) and findthat it affects model behaviour drastically.Changing the model structure successfullyavoids the unsegmented solution, but re-sults in oversegmentation instead.1 IntroductionBayesian models provide a sound statisticalframework in which to explore aspects of languageacquisition.
Explicitly specifying the causal andcomputational structure of a model enables the in-vestigation of hypotheses such as the feasibility oflearning linguistic structure from the available in-put (Perfors et al., 2011), or the interaction of dif-ferent linguistic levels (Johnson, 2008a).
However,these models can be sensitive to small changes in(hyper-)parameters settings.
Robustness in this re-spect is important, since positing specific parame-ter values is cognitively implausible.In this paper we revisit a model of morphol-ogy learning presented by Goldwater and col-leagues in Goldwater et al.
(2006) and Goldwa-ter et al.
(2011) (henceforth GGJ).
This modeldemonstrated the effectiveness of non-parametricstochastic processes, specifically the Pitman-YorProcess, for interpolating between types and to-kens.
Language learners are exposed to tokens,but many aspects of linguistic structure are lexical;identifying which tokens belong to the same lexi-cal type is crucial.
Surface form is not always suf-ficient, as in the case of ambiguous words.
More-over, morphology in particular is influenced byvocabulary-level type statistics (Bybee, 1995), soit is important for a model to operate on both lev-els: token statistics from realistic (child-directed)input, and type-level statistics based on the tokenanalyses.The GGJ model learns successfully given fixedhyperparameter values in the Pitman-Yor Process.However, we show that when these hyperparam-eters are inferred, it collapses to a token-basedmodel with a trivial morphology.
In this paperwe discuss the reasons for this problematic be-haviour, which are relevant for other models basedon Pitman-Yor Processes with discrete base dis-tributions, common in natural language tasks.
Weinvestigate some potential solutions, by chang-ing the way morphemes are generated within themodel.
Our results are mixed; we avoid the hyper-parameter problem, but learn overly compact mor-pheme lexicons.2 The Pitman-Yor ProcessThe Pitman-Yor Process G ?
PYP(a, b,H0) (Pit-man and Yor, 1997; Teh, 2006) generates distribu-tions over the space of the base distribution H0,with the hyperparameters a and b governing theextent of the shift from H0.
Draws from G havevalues from H0, but with probabilities given bythe PYP.
For example, in a unigram PYP languagemodel with observed words, H0may be a uni-form distribution over the vocabulary, U(1T).
ThePYP shifts this distribution to the power-law dis-tribution over tokens found in natural language,allowing words to have much higher (and lower)than uniform probability.
We will continue usingthe language model example in this section, sincethe subsequent morphology model is effectively acomplex unigram language model in which wordtypes correspond to morphological analyses.
Inour presentation, we pay particular attention to therole of the hyperparameter a, since this value gov-erns the power-law behaviour of the PYP (Buntineand Hutter, 2010).When G is marginalised out, the result is thePYP Chinese Restaurant Process, which is a use-ful representation of the distribution of observa-tions (word tokens) to values from H0(types).
In14this restaurant, customers (tokens) arrive and areseated at one of a potentially infinite number oftables.
Each table receives a dish (type) from thebase distribution when the first customer is seatedthere; thereafter all subsequent customers adoptthe same dish.
The probability of customer zibe-ing seated at a table k depends on the number ofcustomers already seated at that table nk.
Popu-lar tables will attract more customers, generating aZipfian distribution over customers at tables.This Zipfian/power-law behaviour can be simi-lar to that of the natural language data, and is theprincipal motivation behind using the PYP.
How-ever, it is only valid for the distribution of cus-tomers to tables.
When the base distribution is dis-crete ?
as in our language model example andthe morphology model ?
the same dish may beserved at multiple tables.
In most cases, the dis-tribution of interest is generally that of customers(tokens) to dishes (types), rather than to tables,suggesting a preference for a setting in which eachdish appears at few tables.
This is dependent on a(constrained to be 0 ?
a < 1), and to a lesser ex-tent on b: If a is small, each dish will be served ata single table, resulting in the type-token and thetable-customer power-laws matching.
If a is near1, however, the probability of more than a singlecustomer being seated at a table is small, and thedistribution of dishes being eaten by the customerswill match the base distribution, rather than beingadapted by the caching mechanism of the PYP.The expected number of tables K grows asO(Na) (see Buntine and Hutter (2010) for an ex-act formulation).
The number of word types inthe data gives us a minimum number of tables,K ?
T .
When a is small (less than 0.5), the num-ber of expected tables is significantly less than thenumber of types in a non-trivial dataset, suggest-ing a lower bound for values of a.In our language model, the posterior probabilityof assigning a wordwito a table k with dish `kandnkprevious customers is:p(wi= k|w1.
.
.
wi?1, a, b) ?
(1){(nk?
a)I(wi= `k) if 1 ?
k ?
K(Ka+ b)H0(wi) if k = K + 1where I(wi= `k) returns 1 if the token and thedish match, and 0 otherwise.
We see that in orderto prefer assigning customers to already occupiedtables, we need H0(w)(Ka+ b) < nk?
a. GivenK ?
T , and setting H0=1T, we can approxi-mate this with1T(Ta + b) < nk?
a.
From thiswe obtain a <12(nk?bT), which indicates that inorder for tables with a single customer (nk= 1) toattract further customers, a must be smaller than0.5.
Thus, there is a tension between the numberof tables required by the data and our desire toreuse tables.
One solution is to fix a to an arbi-trary, sufficiently small value, as GGJ do in theirexperiments.
In contrast, in this paper we infer aand b along with the other parameters, and changethe other free variable, the base distribution H0.3 MorphologyThe morphology model introduced by GGJ has abase distribution that generates not simply wordtypes, as in the language model example, but mor-phological analyses.
These are relatively simple,consisting of stem+suffix segmentation and a clus-ter membership.
The probability of a word is thesum of the probability of all cluster c, stem s, suf-fix f tuples:H0(w) =?
(c,s,f)p(c)p(s|c)p(f |c)I(w = s.f)(2)with the stems and the suffixes being gener-ated from cluster-specific distributions.
In theGGJ model, all three distributions (cluster, stem,suffix) are finite conjugate symmetric Dirichlet-Multinomial (DirMult) distributions.
We retain theDirMult over clusters, but change the morpheme-generating distributions.The DirMult is equivalent to a Dirichlet Processprior (DP) with a finite base distribution; we usethis representation because it allows us to replacethe base distributions flexibly.
A DP(?,H0) is alsoequivalent to a PYP with a = 0, and thus also canbe represented with a Chinese Restaurant Process,but in this case we sum over all tables to obtain thepredictive probability of a (say) stem:p(s|?s, HS) =ms+ ?sHS?s?ms?+ ?s(3)Note that the counts msrefer to stems generatedwithin the base distribution, not to token countswithin the PYP.The original GGJ model, ORIG, is equivalent tosetting HSfor stems to U(1S), and likewise HF=U(1F), where S and F are the number of possiblestems and suffixes in the dataset (i.e., all possibleprefix and suffix strings, including a null string).15There are two difficulties with this model.Firstly, it assumes a closed vocabulary and re-quires setting S and F in advance, by looking atthe data.
As a cognitive model, this is awkward,since it assumes a fixed, relatively small numberof possible morphemes.Secondly, when the PYP hyperparameters areinferred, a is set to be nearly 1, resulting in a modelwith as many tokens as tables.
This behaviour isdue to the interaction between vocabulary size andbase distribution probabilities outlined in the pre-vious section: this base distribution assigns rel-atively high probability to words, so new tableshave high probability; as the number of tables in-creases (from its fairly large minimum), the op-timal a for this table configuration also increases,resulting in convergence at the token-based model.We investigate two alternate base distributionover stems and suffixes, both of which extend thespace of possible morphemes, thereby loweringthe overall probability of the observed words.DP-CHAR generates morphemes by first gener-ating a length l ?
Poisson(?).
Charactersare then drawn from a uniform distribution,c0...l?
U(1/|Chars|).
A morpheme?s prob-ability decreases exponentially by length, re-sulting in a strong preference for shorter mor-phemes.DP-UNI simply extends the original uniform dis-tribution to s and f ?
U(1/1e6), in effectmoving probability mass to a large numberof unseen morphemes.
It is thus similar toDP-CHAR without the length preference.4 InferenceWe follow the same inference procedure as GGJ,using Gibbs sampling.
The sampler iterates be-tween inferring each token?s table assignment andresampling the table labels (see GGJ for details).Within the morphology base distribution, theprior for the DirMult over clusters is set to ?k=0.5.
To replicate the original DirMult model1, weset ?s= 0.001S and ?f= 0.001F .
In the othermodels, ?s= ?f= 1.
Within DP-CHAR, ?
= 6for stems, 0.5 for suffixes.1In this model, the predictive posterior is defined asp(s|?, S) =ms+?m.+S?, using an alternate definition of ?.Eve (Orth.)
Ornat (Orth.
)a Tables/Type a Tables/TypeORIG 0.96 21.2 0.97 10.64DP-CHAR 0.46 1.4 0.56 1.17DP-UNI 0.81 7.3 0.70 2.33Table 1: Final values for a on the orthographic En-glish and Spanish datasets, as well as the averagenumber of tables for each word type.
The 95%confidence interval across three runs is ?
0.01.
(Phonological Eve is similar to Orthographic Eve.
)4.1 Sampling HyperparametersWe sample PYP a and b hyperparameters usinga slice sampler2.
Previous work with this modelhas always fixed these values, generally findingsmall a to be optimal and b to have little effect.In experiments with fixed hyperparameters, we seta = b = 0.1.To sample the hyperparameters, we place vaguepriors over them: a ?
Beta(1, 1) and b ?Gamma(10, 0.1).
The slice sampler samples a newvalue for a and b after every 10 iterations of Gibbssampling.5 Experiments5.1 DatasetsOur datasets consist of the adult utterancesfrom two morphologically annotated corpora fromCHILDES, an English corpus, Eve (Brown, 1973),and a Spanish corpus, Ornat (Ornat, 1994).
Mor-phology is marked by a grammatical suffix on thestem, e.g.
doggy-PL.
Words marked with irregularmorphology are unsegmented.The two languages, while related, have differ-ing degrees of affixation: the English Eve corpusconsists of 63 315 tokens (5% suffixed) and 1 988types (28% suffixed); the Ornat corpus has 43 796tokens (23% suffixed) and 3 157 types (50% suf-fixed).
The English corpus has 17 gold suffixtypes, while Spanish has 72.We also use the phonologically encoded Evedataset used by GGJ.
This dataset does not ex-actly correspond to the orthographic version, dueto discrepancies in tokenisation, so we are unableto evaluate this dataset quantitatively.2Mark Johnson?s implementation, available athttp://web.science.mq.edu.au/?mjohnson/Software.htm16Eve (Orth.)
Ornat (Orth.)
Eve (Phon.
)% Seg |L| VM % Seg |L| VM % Seg |L|Gold 5 23 (5)ORIG Fix 7 1680 46.42(10.8) 14 2488 46.63(2.7) 17 1619ORIG Inf 1 1893 9.94(1.0) 4 2769 17.80(3.7) 1 1984DP-CHAR Fix 52 1331 15.33(0.3) 83 1828 35.76(1.1) 47 1289DP-CHAR Inf 50 1330 16.15(0.4) 85 1824 36.47(0.5) 33 1317DP-UNI Fix 38 1394 17.28(1.7) 51 1874 39.58(0.8) 36 1392DP-UNI Inf 15 1574 31.54(3.1) 31 1983 42.48(1.1) 21 1500Table 2: Final morphology results.
?Fix?
refers to models with fixed PYP hyperparameters (a = b = 0.1),while ?Inf?
models have inferred hyperparameters.
% Seg shows the percentage of tokens that have a non-null suffix, while |L| is the size of the morpheme lexicon.
VM is shown with 95% confidence intervals.5.2 ResultsFor each setting, we report the average over threeruns of 1000 iterations of Gibbs sampling withoutannealing, using the last iteration for evaluation.Table 1 shows what happens when hyperpa-rameters are inferred: ORIG finds a token-basedsolution, with as many tables as tokens, whileDP-CHAR is the opposite, with a small a allowingfor just over one table for each word type.
DP-UNIis between these two extremes.
b is consistentlybetween 1 and 3, confirming it has little effect.The effect of the hyperparameters can be seenin the morphology results, shown in Table 2.DP-CHAR is robust across hyperparameter val-ues, finding the same type-based solution withfixed and inferred hyperparameters, while theother models have very different results dependingon the hyperparameter settings.
ORIG with fixedhyperparameters performs best, with the highestVM score (a clustering measure, Rosenberg andHirschberg (2007)) and a level of segmentationclose to the correct one.
However, with inferredhyperparameters, this model severely underseg-ments: it finds the unsegmented maximum likeli-hood solution, where all tokens are generated fromthe stem distribution (Goldwater, 2007).The models with alternate base distributions goto the other extreme, oversegmenting the corpus.As generating new morphemes becomes less prob-able, the pressure to find the most compact mor-pheme lexicon grows.
This leads to oversegmen-tation due to many spurious suffixes.
The lengthpenalty in DP-CHAR exacerbates this problem, butit can be seen in the DP-UNI solutions as well,particularly when hyperparameters are fixed to en-courage a type-based solution.6 ConclusionThe base distribution in the original GGJ modelassigned a relatively high probability to unseenmorphemes, allowing the model to generate newanalyses for seen words instead of reusing oldanalyses and leading to undersegented token-based solutions.
The alternative base distributionsproposed here were effective in finding type-basedsolutions.
However, these over-segmented solu-tions clearly do not match the true morphology,indicating that the model structure is inadequate.One reason may be that the model structureis overly simple.
The model is faced with an ar-guably more difficult task than a human learner,who has access to semantic, syntactic, and phono-logical cues.
Adding these types of informationhas been shown to help morphology learning insimilar models (Johnson, 2008b; Sirts and Gold-water, 2013; Frank et al., 2013).Similarly, the morphological ambiguity that iscaptured by a model operating over tokens (andignored in better-performing models that allowonly a single analysis for each word type: Poonet al.
(2009); Lee et al.
(2011); Sirts and Alum?ae(2012)) can often be disambiguated using seman-tic and syntactic information.
A model that gener-ates a single analysis per meaningful (semanticallyand syntactically distinct) word-form could avoidthe potential problems of spurious re-generationseen in the original GGJ model as well as theconverse problem of under-generation in our al-ternatives.
Such a model might also map onto thehuman lexicon (which demonstrably avoids bothproblems) in a more realistic way.17ReferencesRoger Brown.
A first language: The earlystages.
Harvard University Press, Cambridge,MA, 1973.Wray Buntine and Marcus Hutter.
A Bayesianview of the Poisson-Dirichlet process.
2010.URL arXiv:1007.0296.Joan Bybee.
Regular morphology and the lexi-con.
Language and Cognitive Processes, 10:425?455, 1995.Stella Frank, Frank Keller, and Sharon Goldwater.Exploring the utility of joint morphological andsyntactic learning from child-directed speech.In Proceedings of the 18th Conference on Em-pirical Methods in Natural Language Process-ing (EMNLP), 2013.Sharon Goldwater.
Nonparametric Bayesian Mod-els of Lexical Acquisition.
PhD thesis, BrownUniversity, 2007.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
Interpolating between types and to-kens by estimating power-law generators.
InAdvances in Neural Information ProcessingSystems 18, 2006.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
Producing power-law distributionsand damping word frequencies with two-stagelanguage models.
Journal of Machine LearningResearch, 12:2335?2382, 2011.Mark Johnson.
Using Adaptor Grammars to iden-tify synergies in the unsupervised acquisition oflinguistic structure.
In Proceedings of the 46thAnnual Meeting of the Association for Compu-tational Linguistics (ACL), 2008a.Mark Johnson.
Unsupervised word segmentationfor Sesotho using Adaptor Grammars.
In Pro-ceedings of the Tenth Meeting of ACL SpecialInterest Group on Computational Morphologyand Phonology, pages 20?27, June 2008b.Yoong Keok Lee, Aria Haghighi, and ReginaBarzilay.
Modeling syntactic context improvesmorphological segmentation.
In Proceedings ofFifteenth Conference on Computational NaturalLanguage Learning (CONLL), 2011.S.
Lopez Ornat.
La adquisicion de la lengua es-pagnola.
Siglo XXI, Madrid, 1994.Amy Perfors, Joshua B. Tenenbaum, and TerryRegier.
The learnability of abstract syntacticprinciples.
Cognition, 118(3):306 ?
338, 2011.Jim Pitman and Marc Yor.
The two-parameterPoisson-Dirichlet distribution derived from astable subordinator.
Annals of Probability, 25(2):855?900, 1997.Hoifung Poon, Colin Cherry, and KristinaToutanova.
Unsupervised morphologicalsegmentation with log-linear models.
InProceedings of the Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL), 2009.Andrew Rosenberg and Julia Hirschberg.
V-measure: A conditional entropy-based externalcluster evaluation measure.
In Proceedings ofthe 12th Conference on Empirical Methods inNatural Language Processing (EMNLP), 2007.Kairit Sirts and Tanel Alum?ae.
A hierarchicalDirichlet process model for joint part-of-speechand morphology induction.
In Proceedings ofthe Conference of the North American Chapterof the Association for Computational Linguis-tics (NAACL), 2012.Kairit Sirts and Sharon Goldwater.
Minimally-supervised morphological segmentation usingadaptor grammars.
Transactions of the Associa-tion for Computational Linguistics, 1:231?242,2013.Yee Whye Teh.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Pro-ceedings of the 44th Annual Meeting of the As-sociation for Computational Linguistics (ACL),Sydney, 2006.18
