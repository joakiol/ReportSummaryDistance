An Acquis i t ion Model  for both Choosing and ResolvingAnaphora  in Conjoined Mandar in  Chinese SentencesBenjamin L. ChenApplication Software DepartmentComputer and Communication Research LaboratoriesIndustrial Technology Research InstituteW300/CCL, #1 RA~D Rd.
I,Hsinchu, 30045, Taiwan, R. O. C.e-mail: blchen@jaguar .ccl.it ri.org.twVon-Wun SooDepartment of Computer ScienceNational Tsing-Hua UniversityHsinchu, 30043, Taiwan, R. O. C.e-mail: eoo@es.nthu.edu.twAbst rac tAnaphoric reference is an important linguistic phe-nomenon to understand the discourse structure andcontent.
In Chinese natural language processing,there are both the problems of choosing and resolv-ing anaphora.
In Mandarin Chinese, several inguistshave attempted to propose criteria to ezplain the phe-nomenon of anaphora but with controversial results.On the other hand, search-based computational tech-niques for resolving anaphora are neither the best wayto resolve Chinese anaphora nor to facilitate choosinganaphora.
Thus, to facilitate both choosing and resolv-ing anaphora with accuracy and efficiency, we proposea case-based learning model G-UNIMEM to automat-ically acquire anaphorie regularity from a sample setof training sentences i, which are annotated with alist of features.
The regularity acquired from trainingwas then tested and compared with other approachesin both choosing and resolving anaphora.Keywords:  anaphor ic  reference,  semant icro les(case),  natura l  l anguage acquis i t ion,  ease-based learn ing.i IntroductionIn discourse, there may be anaphora in two consec-utive sentences.
When anaphora ppear in a pair ofconsecutive sentences, the two consecutive sentencesare called conjoined sentences.
In real life conver-sation, we frequently choose and resolve anaphorato understand the utterances.
There are primarilythree types of anaphora in Mandarin Chinese: zero(ellipsis), pronominal (using pronoun) and nominalanaphora\[4\].
Let's take the conjoined Chinese sen-tencez in (B) to illustrate the phenomenon.
The con-1This paper is partially supported" by the Minis-ter of Economic Affairs, R.O.C.
under the project no.33H3100 at ITRI and by National Science Councial ofR.O.C.
under the grant no.
NSC81-0408-E007-02 atNational Tsing Hua University.
The authors also wantto thank Dr. Martha Pollack for valuable commentsduring 1991 UCSC linguistic institute.joined sentence in (C) is the English translation of theChinese sentences m (B).
(B) Yueh-haa sheng-bing.
\[ \] i-thinghui-chia-le.John got sick \[ \] alreadygone home(C) Because John .as sick, he has gone home.Because the anaphora in (B) is a zero anaphora ndthere is no zero anaphora in English, the antecedentof zero anaphora in (B) must be resolved first beforechoosing an appropriate pronominal anaphora in Chi-nese to English translation.
In the translation from(C) to (B), it is not good to directly translate an En-glish pronoun to a Chinese pronoun.
A better wayis to resolve the anaphora ire (C) and then choose anappropriate type of anaphora in Chinese.In natural anguage processing, better esults eemsto be attainable if rich linguistic or domain knowl-edge is available.
However it generally costs muchand doesn't seem to be realistic.
The same situa-tion applies for resolving and choosing anaphora inMandarin Chinese.
If we only used search-based ap-proaches(those that merely used heuristic and algo-rithmic methods without much linguistic knowledge),the performance was limited.
However, when we in-tended to adopt linguistic knowledge, we found lin-guists' theories tended to be controversial and lesscomputable.
Thus, it motivated us to pursue an ac-quisition model that could acquire linguistic regularityfrom corpora and then used the regularity to resolveand choose anaphora.2 Review of previousapproaches2.1 Search-based approachesBoth history list \[1\] and Hobbs's naive syntactic.algorithm \[7\] are search-bused approaches for re~lv-mg anaphora.
However, it's not quite obvious to tellwhich was better than the other with only few exam-pies.
Thus, we collected 120 testing instances to testthem.
Those instances were selected from linguists'ACTES DE COLING-92, NANTES, 23-28 Aotrr 1992 2 7 4 PROC.
OF COLING.92, NANTES, AUO.
23-28, 1992examples, textbooks, essays and novels.
Half of themcontained zero anaphora the other pronominal.The result showed that the correct number was111(92.5%) with Hobbs's syntactic algorithm and87(72.5%) with the history list approach if firstmatched were selected.
There was 109(90.8%) cor-rect for history list if the last matched were selected.It seemed that both approaches were applicable to re-solve anaphora, tiowever, when there are several NPswith the same semantic features, both approaches mayget into troubles.
\]~harthermore, both cannot be usedto choose anaphora.2.2 L ingu isCs  c r i te r iaAmong linguists' works \[31 \[51 \[ l l l  \[121 \[141, Tai'scriteria \[14\] was applicable to both choose and resolveanaphora.
Others' suffered from difficulties of extract-ing features or resolving anaphora.
Table 1 shows 4co-references for Tai's citeria, which are all applica-ble when co-referred NPs are human.
For example,consider the following conjoined sentences:Tai:\[ Lao Zhang \] dao-le Meiguoyihou.
\[ \] jiac-lehen-duo pengyou.John came U.S.A after \[\] mademany friendsSince John came to the U.S.A., he has made manyfriends.The subject in the first sentence is human and co-referred by the subject in the second sentence, so thisis a subject-subject o-reference.
According to Table1, zero anaphora is preferred to the pronominal oneand nominal anaphora is not permitted in this exam-ple.
Though Tal didn't propose the criteria for resolv-ing anaphora, it was possible to get these criteria justby transforming the choosing criteria in reverse order.After Tai's criteria were applied to choose and re-solve anaphora on the 120 testing instances, we gotthe success numbers 86(71.7%) and 65(54.2%) respec-tively.
The results failed to meet our satisfaction.Through above paragraphs, it appears that search-based methods have their limitations due to lack ofenough linguistisc knowledge and Tai's criteria seemsto be applicable to both choose and resolve anaphora.It might be that Tai's criteria were too general tolead to a high success rate.
More reliable methodto acquire regularity might be required to promotethe success rate.
We hypothesized the regularity ofanaphora could be accounted by causal relations be-tween the features in the conjoined sentences and theantecedents.
In the following section, an acquisitionmodel is introduced.3 G-UNIMEM:  A Case-BasedLearning ModelIn natural language acquisition problem, the re-striction of positive-only examples \[2\] has prohibitedmany machine learning models as a feasible naturallanguage model.
However, a case-baaed learning ap-proach such as Lebowitz's UNIMEM \[9\] \[I0\] seems tobe a candidate due to its capability to form conceptsincrementally from a rich input domain.
Neverthe-less, to apply UNIMEM directly to the acquisition ofanaphoric regularity in Mandarin Chinese is still notsufficient.
We have therefore modified UNIMEM intoG-UNIMEM.G-UNIMEM, a modified version of UNIMEM, isan incremental learning system thatuses GBM(Geueralized-based Memory) to generalizeconcepts from a large set of training instances.
Theprogram was implemented in Quintus PROLOG andon SUN workstation.G-UNIMEM differs from UNIMEM in two respects.Firstly, if a drinker got drunk many times after tak-ing either whiskey and water or brandy and water,he would induce that water made him drunk withUNIMEM.
This is intuitively incorrect.
Whereas,with G-UNIMEM, he would induce that whiskey andwater, brandy and water or water would cause himdrunk.
In this case, G-UNIMEM retains the possiblecausal accounts without committing to erroneous con-clusion.
Secondly, G-UNIMEM can extract explicitcausal rules from memory hierarchy.Similar to UNIMEM, G-UNIMEM organizes inputtraining instances into a memory hierarchy accordingto the frequencies of features.
However, its goal isto explicitly express the generalized causal relation-ships between two specified types of features: causefeatures and goal features.
Since there may be incon-sistency due to lack of cause features, further refine-meat is needed to obtain consistent causal relations.Thus, there are four different modules in G-UNIMEMto complete different functions in order to achieve thispurpose.3.1 The  c lass i f ie rThe classifier is the first module that processesall training instances for G-UNIMEM.
Its function isclose to UNIMEM that organizes a hierarchy structureto incrementally accommodate a training instance andat the same time generalize the features based on sim-ilarities among training instances.
The forming hier-archy is organized as either a g-c-hierarchy or a c-g-hierarchy depending on the setup of system, which isdefined in Definition 1.
In Appendix A we show thebasic classifier algorithm.Def in i t ion 1 A g-c-hierarchy is the hierarchy thatevery generalized goal feature resides in a GEN-NODEand there is no generalized cause feature that residesbetween the root node and this GEN-NODE.
A c-g-hierarchy doesn't allow any generalized goal feature toreside in the GEN-NODE between the root node andany GEN-NODE where generalized cause features re-side.Figure 1 and Figure 2 show the forming g-c-hierarchy and c-g-hierarchy respectively after 13 anno-tated training sentences are entered into G-UNIMEM.Generally, g-c-hierarchy would be chosen since it re-tained all possible causal accounts.
For example, thedrinker with g-c-hierarchy would induce that whiskeyand water, brandy and water or water would causehim drunk; whereas, he would induce whiskey andAc'rY.s DE COLING-92, NANTES, 23-28 AOU'r 1992 2 7 5 FROC.
OI' COLING-92, NAhrrES, AUG. 23-28, 1992water, brandy and water with e-g-hierarchy.
The c-g-hierarchy is more efficient since no rules are neededto be generated.
Fig.
3 and Fig.
4 show the updatingof a GBM before and after inserting a new traininginstance.3.2 The  ru le  generatorOnce a hierarchy has been constructed by the clas-sifter, the causal rules can be extracted.
The rule gen-erator module serves as the role to extract causal rulesfrom the hierarchy.
It generates all causal rules fromthe hierarchy as the regularity is retrieved for predic-tions.In Fig.
6, if a testing instance is given for choos-ing anaphora with a query feature list \[ (g,type(*?
)),(g,ante(theme)), (c,fl(theme)), (c,anaphor(theme)),(c,s2(obj)), (c,p(pv))\], the retrieval process is searchedwith a post-order traverse, namely, in the order se-quence of the node number 1, 2, 3, 4, 5 and 6.
Sincethere may be more than one candidate, the system canbe setup to select either the first or the most specificone.
If the first one is preferred, type(nil) is yieldedas the prediction.
If the most specific answer is pre-ferred, all possible rules will be tried and the one withthe most number of contingent features matched willbe the answer(i.e, type(pronoun) ).The sample rules generated from Fig.
1 are shownin Fig.
5.
Before generating rules, the GBM is ad-justed so that all children of a GEN-NODE are or-dered according to their confidence scores of features.Then all rules are generated in a post-order traversal.3.3 The  ru le  f i l te rThe rule filter removes those rules that are ill-formed and useless.
For example, the causal rule 5in Fig.
5 has no causes which is not a well-formedrule.
It also detects conflicting rules.
Conflicting rulesare those that have different goal feature descriptions,which are accounted by the same cause.
For example,the rule I and rule 6 in Fig.
5 are conflicting.
Theserules will be detected in this module and then to beresolved by the feature selector.3.4 The  feature selectorAny two conflicting rules are resolved by the fea-ture selector through augmenting the two rules withmutual exclusive contingent cause features, which areprepared in advance.
Dominant features were usedin initial regularity acquisition stage; whereas contin-g ent features were used in feature selection stage.
The ominant features uch as goal features are assumed tobe those that must be present in every anaphoric rule.Contingent features are optional.
Fig.
6. shows theGBM with g-c hierarchy after feature selection pro-c?08.4 Tests  us ing sentences  anno-ta ted  with  mixed featuresWe trained G-UNIMEM with 30, 60, 90, 120 in-stances using those features mentioned by Tai, andused all the 120 instances as testing instances.
Itshowed that the approach using Tai's criteria was notpromising.
There are two reasons.
First, none of thesuccess rates was as high as those using the history listapproach or IIobbs's algorithm.
Second, many con-flicting rules remained conflicting due to either thatno further features from feature selection were avail-able or too many specific training leading to too manyspecific rules.
These factors decreased the success rate.4.1 Se lec t ing  mixed  featuresSince Tal's features were not sufficient, more se-mantic features were considered.
Among several lin-guists' works, we tentatively selected some computa-tional feasible syntactic and semantic features fromdifferent sources \[3\] \[5\] \[11\] [12\] [13\] [14\] [15\] as in Ta-ble 2.
An example with annotated features is shownbelow.
Tile notation \[ \] represents zero anaphora.
(C)\[Lao zheng\]i qu-le ji-ge \[nurenli.\[ \]j hen hui zuo-cai.John married a woman t \] wetlcan cook.agent theme agenthm sub,bynondefiniteJohn married a woman, and the woman cookedwell.The training feature list for the sentences (C)is :\[(g ante(theme)), (g,type(nil)), (c,fl(agent)),(c,f2(theme)), (c,anaphor(agent)), (c,p(bv)),(c,s2(sub)), (c,h(hm)) (c d(nondefinite))where the notations g and c represent goa and causefeatures respectively.4.2 Test ing  us ing  mixed  featuresAfter semantic features has been determined, wetrained G-UNIMEM with 30, 60, 90, 120 instancesand used all the 120 instances as testing instances eachtime.We hypothesized to choose semantic roles(i.e.ease) as dominant cause features.
The features uch asante(CASE), type(X), anaphor(CASE) and fi (CASE)are dominant features and the number of fi is vari-ant.
The hypothesis was motivated by Sidner \[13\] whoused semantic roles to determine focus and resolve def-inite anaphora.
The others such as h(Hm), p(POS),s2(SYN); d(D), con(s) belong to contingent features.4.3 The  exper imenta l  resu l t sIt is interesting that the success numbers in Ta-ble 3 increased with the number of training instances.Finally, our results showed that experiments with c-g-hierarchy had a little high accuracy rates (95.8%for resolving and 90.8% for choosing anaphora with120 training instances) than thoee with g-e-hierarchy.Both accuracy rates were higher than those with TaPscriteria \[14\].
Thus, G-UNIMEM with semantic rolesas dominant features promised much higher accuracyrate.In Appendix B we show some sample rules acquiredin Horn-like clauses.
After examination, either theagent or ~heme of first sentence is most likely toAcrEs DE COLING-92, NANTES, 2.3-28 AOOT 1992 2 7 6 PROC.
OF COLING-92.
NANTES, AUG. 23-28, 1992act as antecedents of anaphora.
Tiffs phenomenon isin coincidence with the investigation on anaphora bySidner.
That is, the agent often appeared as actorfocus and theme as default focus .
This is similar toTai's criteria but is in more compact interpretation.5 D iscuss ionThere are two concerns in implementing G-UNIMEM:(1) The feature set : Is the assignment of dominantfeatures and contingent features objective?
If thereis any contingent feature in the assignment that obvi~ously improves the accuracy rate, it shonld be assignedas dominant feature.
We use statistical methods \[8\] toanalyze if contingent features actually improve accu-racy rates.
If there is no obvious improvement withcontingent features, the division of dominant and corr-tingent features is acceptable.We made the null hypothesis "G-UNIMEM with c-g-hierarchy doesn't have obvious improvement withcontingent features" and the alternative hypothesis"G-UNIMEM with c-g-hierarchy has obvious improve-ment with contingent features".
We titan got twotest values from test statistics: tl = 0.8472 and t2< 0.
Both test statistic.q were less than t~ = .05 (=1.734 with d.f.
= 18).
Thus, the null hypothesis "G-UNIMEM with c-g-lfierarchy doesn't have obvious im-provement with contingent features" was not rejected,which justified that G-UNIMEM using semantic rolesas dominant features was valid.
(2)The sample size : Compared with actual inguis-tic domain, the 120 training and testing instances aresmall.
A large corpus is desirable to test the system'sperformance.
If it becomes available, our resnlts wouldbe more objective and reliable.6 Conc lus ionWe have illustrated a way of using machine learn-ing techniques to acquire anaphoric regularity in con-joined Mandarin Chinese sentences.
The regularitywas used to both choose and resolve anaphora withconsiderable accuracy.
Table 4 shows a comparisonbetween different approaches.In comparison to other approaches, tire proposal ofusing G-UNIMEM as the acquisition model and usingsemantic roles as dominant features is practical andserves multiple purposes.References\[1\] James  A l len  (1987), Natural Language Under-standing, The Benjamin/Cummings PublishingCo.\[2\] Rober t  Berwick  (1986), Learning FromPositive-only Examples: The Subjective Princi-ple and Three Case Studies.
In R.Michalsk i ,J. Carbonel l ,  and T.
Mitchell(cal.)
MachineLearning: An Artificial Intelligence ApproachVol.
\[I, Chapter 21, Morgan Kanfmann Publish-ers, Inc.1131\[4\]\[6\]P ing  Chen (1984), A Discourse Analysis ofThird Person Zero Anaphora in Chinese, repro-duced by Indianan University Linguistics Club,Bloomington, Indiana.P ing  Chert (1987), A Discourse Approach toZero Anaph0ra in Chinese, ghonggua Ynwen,Beijiug.Chauncey  C. Chu (1983), Definiteness, Pre-suppositimt, Topic and Focus in Mandarin Chi-ne~e, Student Book company, Taipei, Taiwan.John  H, Gennar i ,  Pa t  Lmlgley and Doug.F isher  (1989), Models of Incremental ConceptFormation, Artificial Inielligence 40.\[7\] J .
l l obbs  (1978), Resolving Pronoun References,Lingua 44, North-tlolland Publishing Co.\[8\]\[9\]\[10\]R ichard  Johnson  and Cour t  Bhat -tacharyya  (1985), Statistics: Principles andMethods, Chapter 6 (Section ,I), 9, 10, 11, Pub-lished by Hohn Wiley & Sons.Michael  Lebowitz  (1986), Concept Learningin ~ Rich Input Domain: Generalization-basedMemory.
Ira It.
M idra lsk i ,  J. Carbonel l ,and T.
Mitcholl(ed.)
Machine Learning: AnArtificial \[nlelligence Approach Vol.
1I, Chapter8, Morgan Kaufmann Publishers, Inc.Michael  Lebowitz  (1986), Integrated Learning:Cmttrolling Explanation, Cognitive Science 10.\[11\] Cher ry  lug  Li (1985), Participant anapho~ inMandarin Chinese, University of Florida Press.\[12\] Met Du Li (1986), Anapho,~c Structure of Chi-nese, Student Book CO., Taipei, Taiwan.\[131\[1,11\[ix\]C. S idner  (1983), Focusing in tim Compre ~hens\[on of Definite Anaphora, Iu ComputationalModels of Discourse, M. Brady and R.
Berwick,eds., MIT Press.J.
l I .
Tat (1978), Auaphoric Constraints inMandarin Chinese Narrative Discourse.
In J.H inds  (ed.)
Anaphora in Discourse, LinguisticResearch, Edmonton, Alberta.T ing -Ch i  chanties Tang (1975), A CaseGrammar Classification of Chinese Verbs, Ha\[-Gun Book Company, Taipei, Taiwaa.A(:Iq~S I)E COLING-92, NANTES, 23-28 Aolrr 1992 2 7 7 PRO{:.
ov COLIN(L92, NANTES, AUG. 23-28, 1992g,type(nil)):11 1' (g,type(pronoun)):2 I \ \[ (g,ante(theme)):2i\ I (c,anaphor(theme)):2(g,ante(theme)):7 \ \ [  ic,fl(thenm)): 2(c,fl(theme)):7 \ [ \ ~c ,anaphor(t heine) : 7 i \.
(g,ante(agent)):4 \[/ I  (c,fl(agent)):4 "--l~x~tc,anaphor(theme)):2 (c,f2(theme)):2 i\[ , .\] I (c,anaphor(agent)):~Fig.
1.
A g-c-hierarchy of GBM(g,ante(theme)):3inst:\[(g,type(pronoun))\] jFig.
4 A new GBM after inserting a newinstance \[(g,type(pronoun)),(g,ante(theme))\]1: \[(g,type(nil)),(g,ante(theme))\] :-\[(c,:fl(gheme)),(c,anaphor(theme))\]2: \[(g,type(nil)),(g,aute(agent))\]:-\[(c,:fl(agent)),(c,anaphor(theme))\]3: \[(g,type(nil)),(g,ante(agen?
))J:-\[(c,fl(agent)),(c,:f2(theme)),(?, a.aaphor (agent) )'14: \[(g,type(nil)), (g,ante(agent))\] :-\[(c,f l(agent))\]5: \[(g,type(nil))\] :- \ [ \ ](c,fl(theme)):9 \] I (c'fl(agent)):4 16: \[(g,type(pronotm) ), (g, a.ate (gheme))\] :-c,amaphor(theme)):9\[ / \[(c,:f 1(theme)) (c,aaaphor(th~e))\](g,ante(theme)):9 \[ Fig.
5 The sample rules generated :from\[ (c,anaphor(theme)):2 " Fig.
1.\[ (g,type(nil)):2~g,ante(agent)):2 ,3...... (c,anaphor(agent)):~lg,type(pr?n?un))i~ (c,f2(theme)):2 \[(g,type(nil)):2 |g,type(nil)):7 \] (g,ante(agent)):2 ,.
\[Fig.
2.
A c-g-hierarchy of GBM(g,type(nil)):2(g,ante(theme)):2 1Fig.
3 A GBM with two training instancesI/c,anaphor/theme/):  ' / /k (c,anaphor\[theme)):2 \ (g,type(nil)):2 \2 ~ \] (g,ante(agent)):2 \(g,type(pronoun)):2' 5 (c,anaphor(agent)):~(g,ante(theme)):2 (c,f2(theme)):2 |(c,s2(obj)):2 (g,type(nil)):2 /(c,p(pv)):2 (g,ante(agent)):2 \]1(g,type(nit)):7(g,ante(theme)):7 1Fig.
6 The c-g hierarchy after conflictresolution from Fig.
2AcrEs DE COLING-92.
NANTES.
23-28 AO0r 1992 2 7 8 PROC.
OF COLING-92, NANTES.
AUG. 23-28, 1992Table 1: TaPs criteria for choosing anaphoraI .
.
.
.
wneos_e \] zero--5-~ro~ n onoun~rouou~-~~_nom,na l  jI Not  permi t ted  \[ nominal \] zero \[ ~ -  ~- - - - ze -m \]n o ~ .
u j ~ ~ J .
c~-#&Tgf&__ ~ ~ i~pre~rre~ g87T ~ m : e s  and efe e ces~ .
feature  ~ notat ion  .
.
.
.semant ic  antecedent ante CASE)~semanttc role \[ ti(CASE)' ~  I - -anaphoriCXSE) - -~an or n o ~ ~ ~ n ( n o n h m ~ - -syntact ic  anaphora \[ type(X)1___ subject .... \[ sz(suz~, ...I position of anap\ [ i -o~- -p~gv~o~- -\] I bv: before verb; pv: post-verb\ [ _____ .~f in i te  .
\[ d(uondefinite) __I co.ne~or I c?n/s)notat ion  :'GAS rE-rE-rE-rE-rE-rE-~g-presents a variable for a semantic role a n ~order in a sequence of roles.Tans= C h m ~- -  ru~-lgnumberAccuracy rate(for resolving)Accuracy rate(for choosing~ g  number(after resolving)Table 3: Comparison of G-UNIMEM using ~ against Tai's criteriaGroup  Cand idate  E 3 ~Y-~"qS"0- ~ - -~- 'E~20 " ~  TaP~4- -choice cr i ter ia30**specific 58/62 87/96 104/110 114/120=95.0%, , one  58/62 86/96 104/110 111/120=:92.5%trst\] '?/~ 53/60 83/99-~ 95/109 109/120:-'90.8?70- ~TT2ffg"TE=71.7~U-ospecific 56/65 83/100 88/110 101/120=84.2%4 - - - - -~*~- -notat ion:  * ."
for choos in~r~ ~g;  accuracy ~ g ~ ' c c ~ s -that have applicable rules; none,  in column 2 means no contingent features are used.Table 4: Comparison of different approach~MethOds ~ _ ~ o s ~1.
- -h i s to ry  list L~___~_~e search\[" l lobbs"s syntact ic  - -  ~ ' - - - - - / ' - ~ - ~ ~ e ~ y - ~ - y ~\].
al~;orithm ~ J~ s  mter,'g----- c oo~ pre , c t ~ ~\ [ .~ Chen's criteria \[ eho~ pl_~.dlc~ not easy J\] G-UNIMEM & dominant \] choose & rel ive \] predict \] easy JI features \] ~ _ ~  JAcrEs DE COLING-92, NANTES, 23-28 noun" 1992 2 7 9 PROC.
OF COL1NG -92, NANTES, AUG. 23 -28, 1992Appendix A.
The basic classifier algorithminput: The current node N of the concept hierarchy.The name I of an unclassified instance.The set of I's unaccounted features F.Results: The concept hierarchy that classifies the instance.Top-level call: classif ier( Top-node, I, F)Variables: N, N', C and NC are nodes in the hierarchy.G, H, and K are sets of features.J is an instance stored on a node.P~ is a variable of set.Classif ier(N, 1, F).Let G be the set of features tores in N.Let H be the features in F that match features in G.Let K1 be the features in F that do not match features in G.Let K2 be the features in G that do not match features in H.Let H', KI '  and K2' be the sets of features after Adjust(H,K1,Ki,H',KI ' ,Ki ' )/* adjust goal and cause features forg-c-hierarchy or c-g-hierarchy */if  N is not the root node,then  i f  H is empty set / *  no features match */then  re turn  Falseelse i f  both H' and KI '  are not empty setsthen  ~/*  split node N */spht N into N' and NC where NC is a child of N';N' contains features in H' with confidence scoresand I as a instance with features KI';each confidence score in tl '  is increased by 1;the remaining features and instances belong to NC;re turn  Spl i t .
)else if It' and H are equal /*  all features match */then  increase ach confidence score in N by 1.for each child C of node N/*  continue match remaining features */call Classifier(C, I KI') and collect returns to the set It,i f  any Classifier(C, I KI ') call return True or Split then break.if  1% is \[False \] /* All trials fail, try to do generalization */then  for each instance J of node Ncall Generalize(N, J, I, KI ')  and collect returns to the set 1%,if any Generallze(N, J, I, KI ') call return True then break.if  t t  is \[ False \ ] / *  All trials fail, insert I as an instance of N */then  store I as an instance of node N with features KI'.re turn  True.Appendix B.
Sample r t l l en  of regularity ~ith high probability of appearancein Horn-like clauses\[an~;e (agent ) ,  gype(n i l ) \ ]  : - \ [anaphor(agent)  , f2(theme) , f t  (agent) \ ]\[ante(agent) ,type(nil)\] :- \[anaphor (agent) ,f l(agent)\]\[ante(theme) ,type(nil)\] :- \[p(bv).
s2 (sub) ,d(nondefinite), anaphor (agent), f I (agent), f2 (theme)\]\[ante(theme).
type(nil)\] :- \[h(nonhm), anaphor (theme).
f i (agent), fi(theme)\]\[ante(theme) .type(nil)\] : -  \[anaphor (theme) ,fl (theme)\]\[ante(ar E) .type(pronoun)\] :- \[anaphor(agent), f2(pred) , fl(arg)\]\[ant ?
(agent).
type (pronoun) \] : - \[h(hm).
d(definite), con(s) 0anaphor(agent).
f2 (theme), f i (agent)\]\[unt e (art) ,type(pronoun)\] :- \[k(hm), anaphor (a~ent), f 2(pred) ,f l(arg)\] ..\[ante(theme).
type(pronoun)\] :- \[s2(obj ) ,p(pv) ,d(definite),anaphor(theme) ,f I (agent) ,f2(theme)J\[ant e (art) .type(pronoun)\] :- \[h(hm), anaphor (theme).
f2 (pred) ,f I (arg)\]Acrf~s DE COLING-92, NAN'IT.S, 23-28 AoU'r 1992 2 8 0 PROC.
OF COLING-92, NANTES.
AUG. 23-28, 1992
