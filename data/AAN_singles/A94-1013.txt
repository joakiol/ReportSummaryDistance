Adaptive Sentence Boundary DisambiguationDavid D. PalmerCS Division, 387 Soda Hall #1776University of California, BerkeleyBerkeley, CA 94720-1776dpalmer@ cs.
berkeley, eduMarti A.
Hears tXerox PARC3333 Coyote Hill RdPalo Alto, CA 94304hearst @parc.xeroz.
cornAbstractLabeling of sentence boundaries i a nec-essary prerequisite for many natural an-guage processing tasks, including part-of-speech tagging and sentence alignment.End-of-sentence punctuation marks areambiguous; to disambiguate them mostsystems use brittle, special-purpose regularexpression grammars and exception rules.As an alternative, we have developed an ef-ficient, trainable algorithm that uses a lex-icon with part-of-speech probabilities anda feed-forward neural network.
This workdemonstrates the feasibility of using priorprobabilities of part-of-speech assignments,as opposed to words or definite part-of-speech assignments, as contextual infor-mation.
After training for less than oneminute, the method correctly labels over98.5% of sentence boundaries ina corpus ofover 27,000 sentence-boundary marks.
Weshow the method to be efficient and easilyadaptable to different ext genres, includ-ing single-case texts.1 In t roduct ionLabeling of sentence boundaries is a necessaryprerequisite for many natural language process-ing (NLP) tasks, including part-of-speech tagging(Church, 1988), (Cutting et al, 1991), and sen-tence alignment (Gale and Church, 1993), (Kayand RSscheisen, 1993).
End-of-sentence punctuationmarks are ambiguous; for example, a period can de-note an abbreviation, the end of a sentence, or both,as shown in the examples below:(1) The group included Dr. J.M.
Freeman and T.Boone Pickens Jr.(2) "This issue crosses party lines and crossesphilosophical lines!"
said Rep. John Rowland(R., Conn.).Riley (1989) determined that in the Tagged Browncorpus (Francis and Kucera, 1982) about 90% of pe-78riods occur at the end of sentences, 10% at the endof abbreviations, and about 0.5% as both abbrevi-ations and sentence delimiters.
Note from example(2) that exclamation points and question marks arealso ambiguous, ince they too can appear at loca-tions other than sentence boundaries.Most robust NLP systems, e.g., Cutting et al(1991), find sentence delimiters by tokenizing thetext stream and applying a regular expression gram-mar with some amount of look-ahead, an abbrevia-tion list, and perhaps a list of exception rules.
Theseapproaches are usually hand-tailored to the particu-lar text and rely on brittle cues such as capitalizationand the number of spaces following asentence delim-iter.
Typically these approaches use only the tokensimmediately preceding and following the punctua-tion mark to be disambiguated.
However, more con-text can be necessary, such as when an abbreviationappears at the end of a sentence, as seen in (3a-b):(3a) It was due Friday by 5 p.m. Saturday would betoo late.
(3b) She has an appointment at 5 p.m. Saturday toget her car fired.or when punctuation occurs in a subsentence withinquotation marks or parentheses, asseen in Example(2).Some systems have achieved accurate boundarydetermination by applying very large manual effort.For example, at Mead Data Central, Mark Wassonand colleagues, over a period of 9 staff months, de-veloped a system that recognizes special tokens (e.g.,non-dictionary terms such as proper names, legalstatute citations, etc.)
as well as sentence bound-aries.
From this, Wasson built a stand-alone bound-ary recognizer in the form of a grammar convertedinto finite automata with 1419 states and 18002transitions (excluding the lexicon).
The resultingsystem, when tested on 20 megabytes of news andcase law text, achieved an accuracy of 99.7% atspeeds of 80,000 characters per CPU second on amainframe computer.
When tested against upper-case legal text the algorithm still performed verywell, achieving accuracies of 99.71% and 98.24% ontest data of 5305 and 9396 periods, respectively.
Itis not likely, however, that the results would be thisstrong on lower-case data.
1Humphrey and Zhou (1989) report using a feed-forward neural network to disambiguate periods, al-though they use a regular grammar to tokenize thetext before training the neural nets, and achieve anaccuracy averaging 93~.
2Riley (1989) describes an approach that uses re-gression trees (Breiman et al, 1984) to classify sen-tence boundaries according to the following features:Probability\[word preceding "."
occurs at end ofsentence\]Probability\[word following "."
occurs at begin-ning of sentence\]Length of word preceeding " "Length of word after ".
"Case of word preceeding ".
": Upper, Lower,Cap, NumbersCase of word following ". '
:  Upper, Lower Cap,NumbersPunctuation after "."
(if any)Abbreviation class of words with ".
"The method uses information about one word ofcontext on either side of the punctuation mark andthus must record, for every word in the lexicon, theprobability that it occurs next to a sentence bound-ary.
Probabilities were compiled from 25 millionwords of pre-labeled training data from a corpus ofAP newswire.
The results were tested on the Browncorpus achieving an accuracy of 99.8%.
3Miiller (1980) provides an exhaustive analysisof sentence boundary disambiguation as it relatesto lexical endings and the identification of wordssurrounding a punctuation mark, focusing on textwritten in English.
This approach makes multi-ple passes through the data and uses large wordlists to determine the positions of full stops.
Ac-curacy rates of 95-98% are reported for this methodtested on over 75,000 scientific abstracts.
(In con-trast to Riley's Brown corpus statistics, Mfiller re-ports sentence-ending to abbreviation ratios rangingfrom 92.8%/7.2% to 54.7%/45.3%.
This implies aneed for an approach that can adapt flexibly to thecharacteristics of different ext collections.
)Each of these approaches has disadvantages toovercome.
We propose that a sentence-boundarydisambiguation algorithm have the following char-acteristics:1All information about Mead's system is courtesy ofa personal communication with Mark Wasson.2Accuracy results were obtained courtesy of a per-sonal communication with Joe Zhou.~Time for training was not reported, nor was theamount of the Brown corpus against which testing wasperformed; we assume the entire Brown corpus was used.79?
The approach should be robust, and shouldnot require a hand-built grammar or special-ized rules that depend on capitalization, mul-tiple spaces between sentences, etc.
Thus, theapproach should adapt easily to new text genresand new languages.?
The approach should train quickly on a smalltraining set and should not require excessivestorage overhead.?
The approach should be very accurate and ef-ficient enough that it does not noticeably slowdown text preprocessing.?
The approach should be able to specify "noopinion" on cases that are too difficult to dis-ambiguate, rather than making underinformedguesses.In the following sections we present an approachthat meets each of these criteria, achieving perfor-mance close to solutions that require manually de-signed rules, and behaving more robustly.
Section2 describes the algorithm, Section 3 describes omeexperiments that evaluate the algorithm, and Sec-tion 4 summarizes the paper and describes futuredirections.2 Our  So lu t ionWe have developed an efficient and accurate auto-matic sentence boundary labeling algorithm whichovercomes the limitations of previous olutions.
Themethod is easily trainable and adapts to new texttypes without requiring rewriting of recognitionrules.
The core of the algorithm can be stated con-cisely as follows: the part-of-speech probabilities ofthe tokens surrounding a punctuation mark are usedas input to a feed-forward neural network, and thenetwork's output activation value determines whatlabel to assign to the punctuation mark.The straightforward approach to using contextualinformation is to record for each word the likelihoodthat it appears before or after a sentence bound-ary.
However, it is expensive to obtain probabilitiesfor likelihood of occurrence of all individual tokensin the positions urrounding the punctuation mark,and most likely such information would not be use-ful to any subsequent processing steps in an NLPsystem.
Instead, we use probabilities for the part-of-speech categories of the surrounding tokens, thusmaking training faster and storage costs negligiblefor a system that must in any case record these prob-abilities for use in its part-of-speech tagger.This approach appears to incur a cycle: becausemost part-of-speech taggers require pre-determinedsentence boundaries, entence labeling must be donebefore tagging.
But if sentence labeling is done be-fore tagging, no part-of-speech assignments are avail-able for the boundary-determination algorithm.
In-stead of assigning a single part-of-speech to eachword, our algorithm uses ~he prior probabilities ofall parts-of-speech for that word.
This is in contrastto Riley's method (Riley, 1989) which requires prob-abilities to be found for every lexical item (since itrecords the number of times every token has beenseen before and after a period).
Instead, we suggestmaking use of the unchanging prior probabilities foreach word already stored in the system's lexicon.The rest of this section describes the algorithm inmore detail.2.1 Ass ignment  o f  Descr ip torsThe first stage of the process is lexical analysis,which breaks the input text (a stream of characters)into tokens.
Our implementation uses a slightly-modified version of the tokenizer from the PARTSpart-of-speech tagger (Church, 1988) for this task.A token can be a sequence of alphabetic haracters,a sequence of digits (numbers containing periods act-ing as decimal points are considered a single token),or a single non-alphanumeric character.
A lookupmodule then uses a lexicon with part-of-speech tagsfor each token.
This lexicon includes informationabout the frequency with which each word occurs aseach possible part-of-speech.
The lexicon and thefrequency counts were also taken from the PARTStagger, which derived the counts from the Browncorpus (Francis and Kucera, 1982).
For the wordadult, for example, the lookup module would returnthe tags " J J /2 NN/24," signifying that the word oc-curred 26 times in the Brown corpus - twice as anadjective and 24 times as a singular noun.The lexicon contains 77 part-of-speech tags, whichwe map into 18 more general categories (see Figure1).
For example, the tags for present ense verb, pastparticiple, and modal verb all map into the moregeneral "verb" category.
For a given word and cate-gory, the frequency of the category is the sum of thefrequencies of all the tags that are mapped to thecategory for that word.
The 18 category frequen-cies for the word are then converted to probabilitiesby dividing the frequencies for each category by thetotal number of occurrences of the word.For each token that appears in the input stream, adescriptor array is created consisting of the 18 prob-abilities as well as two additional flags that indicateif the word begins with a capital etter and if it fol-lows a punctuation mark.2.2 The  Role  o f  the  Neura l  NetworkWe accomplish the disambiguation of punctuationmarks using a feed-forward neural network trainedwith the back propagation algorithm (Hertz et al,1991).
The network accepts as input k ?
20 inputunits, where k is the number of words of context sur-rounding an instance of an end-of-sentence punctua-tion mark (referred to in this paper as "k-context"),and 20 is the number of elements in the descrip-tor array described in the previous ubsection.
The80input layer is fully connected to a hidden layer con-sisting of j hidden units with a sigmoidal squashingactivation function.
The hidden units in turn feedinto one output unit which indicates the results ofthe function.
4The output of the network, a single value between0 and 1, represents the strength of the evidence thata punctuation mark occurring in its context is in-deed the end of the sentence.
We define two ad-justable sensitivity thresholds to and tl, which areused to classify the results of the disambiguation.If the output is less than to, the punctuation markis not a sentence boundary; if the output is greaterthan or equal to Q, it is a sentence boundary.
Out-puts which fall between the thresholds cannot bedisambiguated by the network and are marked ac-cordingly, so they can be treated specially in laterprocessing.
When to : t l ,  every punctuation markis labeled as either a boundary or a non-boundary.To disambiguate a punctuation mark in a k-context, a window of k+l  tokens and their descriptorarrays is maintained as the input text is read.
Thefirst k/2 and final k/2 tokens of this sequence repre-sent the context in which the middle token appears.If the middle token is a potential end-of-sentencepunctuation mark, the descriptor arrays for the con-text tokens are input to the network and the outputresult indicates the appropriate label, subject o thethresholds to and t 1.Section 3 describes experiments which vary thesize of k and the number of hidden units.2.3 Heur i s t i csA connectionist network can discover patterns in theinput data without using explicit rules, but the in-put must be structured to allow the net to recognizethese patterns.
Important factors in the effective-ness of these arrays include the mapping of part-of-speech tags into categories, and assignment of parts-of-speech to words not explicitly contained in thelexicon.As previously described, we map the part-of-speech tags in the lexicon to more general categories.This mapping is, to an extent, dependent on therange of tags and on the language being analyzed.In our experiments, when all verb forms in Englishare placed in a single category, the results are strong(although we did not try alternative mappings).
Wespeculate, however, that for languages like German,4The context of a punctuation mark can be thoughtof as the sequence of tokens preceding and following it.Thus this network can be thought of roughly as a Time-Delay Neural Network (TDNN) (Hertz et al, 1991),since it accepts a sequence of inputs and is sensitive topositional information wRhin the sequence.
However,since the input information is not really shifted witheach time step, but rather only presented to the neu-ral net when a punctuation mark is in the center of theinput stream, this is not technically a TDNN.noun verb article modifierconjunction pronoun preposition proper nounnumber comma or semicolon left parentheses right parenthesesnon-punctuation character possessive colon or dash abbreviationsentence-ending punctuation othersFigure 1: Elements of the Descriptor Array assigned to each incoming token.the verb forms will need to be separated from eachother, as certain forms occur much more frequentlyat the end of a sentence than others do.
Similarissuse may arise in other languages.Another important consideration is classificationof words not present in the lexicon, since most textscontain infrequent words.
Particularly important isthe ability to recognize tokens that are likely to beabbreviations or proper nouns.
M/iller (1980) givesan argument for the futility of trying to compile anexhaustive list of abbreviations in a language, thusimplying the need to recognize unfamiliar abbrevi-ations.
We implement several techniques to accom-plish this.
For example, we attempt o identify ini-tials by assigning an "abbreviation" tag to all se-quences of letters containing internal periods andno spaces.
This finds abbreviations like "J.R." and"Ph.D." Note that the final period is a punctuationmark which needs to be disambiguated, and is there-fore not considered part of the word.A capitalized word is not necessarily a propernoun, even when it appears omewhere other than ina sentence's initial position (e.g., the word "Amer-ican" is often used as an adjective).
We requirea way to assign probabilities to capitalized wordsthat appear in the lexicon but are not registered asproper nouns.
We use a simple heuristic: we splitthe word's probabilities, assigning a 0.5 probabilitythat the word is a proper noun, and dividing theremaining 0.5 according to the proportions of theprobabilities of the parts of speech indicated in thelexicon for that word.Capitalized words that do not appear in the lex-icon at all are generally very likely to be propernouns; therefore, they are assigned a proper nounprobability of 0.9, with the remaining 0.1 probabil-ity distributed equally among all the other parts-of-speech.
These simple assignment rules are effectivefor English, but would need to be slightly modifiedfor other languages with different capitalization rules(e.g., in German all nouns are capitalized).3 Exper iments  and ResultsWe tested the boundary labeler on a large bodyof text containing 27,294 potential sentence-endingpunctuation marks taken from the Wall Street Jour-nal portion of the ACL/DCI collection (Church andLiberman, 1991).
No preprocessing was performedon the test text, aside from removing unnecessaryheaders and correcting existing errors.
(The sen-81tence boundaries in the WSJ text had been previ-ously labeled using a method similar to that used inPARTS and is described in more detail in (Liber-man and Church, 1992); we found and correctedseveral hundred errors.)
We trained the weights inthe neural network with a back-propagation algo-rithm on a training set of 573 items from the samecorpus.
To increase generalization of training, aseparate cross-validation set (containing 258 itemsalso from the same corpus) was also fed throughthe network, but the weights were not trained onthis set.
When the cumulative rror of the items inthe cross-validation set reached a minimum, train-ing was stopped.
Training was done in batch modewith a learning rate of 0.08.
The entire training pro-cedure required less than one minute on a HewlettPackard 9000/750 Workstation.
This should be con-trasted with Riley's algorithm which required 25 mil-lion words of training data in order to compile prob-abilities.If we use Riley's statistics presented in Section1, we can determine a lower bound for a sentenceboundary disambiguation algorithm: an algorithmthat always labels a period as a sentence boundarywould be correct 90% of the time; therefore, anymethod must perform better than 90%.
In our ex-periments, performance was very strong: with bothsensitivity thresholds et to 0.5, the network methodwas successful in disambiguating 98.5% of the punc-tuation marks, mislabeling only 409 of 27,294.
Theseerrors fall into two major categories: (i)"false posi-tive": the method erroneously abeled a punctuationmark as a sentence boundary, and (ii) "false nega-tive": the method did not label a sentence boundaryas such.
See Table 1 for details.224 (54.8%) false positives185 (45.2%) false negatives409 total errors out of 27,294 itemsTable 1: Results of testing on 27,294 mixed-caseitems; to -- tl -- 0.5, 6-context, 2 hidden units.The 409 errors from this testing run can be de-composed into the following groups:37.6% false positive at an abbreviation within atitle or name, usually because the wordfollowing the period exists in the lexiconwith other parts-of-speech (Mr. Gray, Col.North, Mr. Major, Dr. Carpenter, Mr.Sharp).
Also included in this group areitems such as U.S. Supreme Court or U.S.Army, which are sometimes mislabeled be-cause U.S. occurs very frequently at theend of a sentence as well.22.5% false negative due to an abbreviation atthe end of a sentence, most frequently Inc.,Co., Corp., or U.S., which all occur withinsentences as well.11.0% false positive or negative due to a sequenceof characters including a punctuation markand quotation marks, as this sequence canoccur both within and at the end of sen-tences.9.2% false negative resulting from an abbrevia-tion followed by quotation marks; relatedto the previous two types.9.8% false positive or false negative resultingfrom presence of ellipsis (...), which can oc-cur at the end of or within a sentence.9.9% miscellaneous errors, including extraneouscharacters (dashes, asterisks, etc.
), un-grammatical sentences, misspellings, andparenthetical sentences.The results presented above (409 errors) are ob-tained when both to and tl are set at 0.5.
Adjust-ing the sensitivity thresholds decreases the numberof punctuation marks which are mislabeled by themethod.
For example, when the upper threshold isset at 0.8 and the lower threshold at 0.2, the networkplaces 164 items between the two.
Thus when thealgorithm does not have enough evidence to classifythe items, some mislabeling can be avoided, sWe also experimented with different context sizesand numbers of hidden units, obtaining the resultsshown in Tables 2 and 3.
All results were found usingthe same training set of 573 items, cross-validationset of 258 items, and mixed-case test set of 27,294items.
The "Training Error" is one-half the sum ofall the errors for all 573 items in the training set,where the "error" is the difference between the de-sired output and the actual output of the neural net.The "Cross Error" is the equivalent value for thecross-validation set.
These two error figures give anindication of how well the network learned the train-ing data before stopping.We observed that a net with fewer hidden unitsresults in a drastic decrease in the number of falsepositives and a corresponding increase in the numberof false negatives.
Conversely, increasing the numberof hidden units results in a decrease of false negatives(to zero) and an increase in false positives.
A net-work with 2 hidden units produces the best overallerror rate, with false negatives and false positivesnearly equal.From these data we concluded that a context ofsix surrounding tokens and a hidden layer with two5We will report on results of varying the thresholdsin future work.units worked best for our test set.After converting the training, cross-validation a dtest texts to a lower-case-only format and retraining,the network was able to successfully disambiguate96.2% of the boundaries in a lower-case-only testtext.
Repeating the procedure with an upper-case-only format produced a 97.4% success rate.
Unlikemost existing methods which rely heavily on capital-ization information, the network method is reason-ably successful at disambiguating single-case texts.4 D iscuss ion  and  Future  WorkWe have presented an automatic sentence boundarylabeler which uses probabilistic part-of-speech infor-mation and a simple neural network to correctly dis-ambiguate over 98.5% of sentence-boundary punctu-ation marks.
A novel aspect of the approach is itsuse of prior part-of-speech probabilities, rather thanword tokens, to represent he context surroundingthe punctuation mark to be disambiguated.
Thisleads to savings in parameter estimation and thustraining time.
The stochastic nature of the input,combined with the inherent robustness of the con-nectionist network, produces robust results.
The al-gorithm is to be used in conjunction with a part-of-speech tagger, and so assumes the availability ofa lexicon containing prior probabilities of parts-of-speech.
The network is rapidly trainable and thusshould be easily adaptable to new text genres, and isvery efficient when used in its labeling capacity.
Al-though the systems of Wasson and Riley (1989) re-port slightly better error rates, our approach as theadvantage of flexibility for application to new textgenres, small training sets (and hence fast trainingtimes), (relatively) small storage requirements, andlittle manual effort.
Futhermore, additional experi-mentation may lower the error rate.Although our results were obtained using an En-glish lexicon and text, we designed the boundarylabeler to be equally applicable to other languages,assuming the accessibility of lexical part-of-speechfrequency data (which can be obtained by runninga part-of-speech tagger over a large corpus of text,if it is not available in the tagger itself) and an ab-breviation list.
The input to the neural network isa language-independent set of descriptor arrays, sotraining and labeling would not require recoding fora new language.
The heuristics described in Section2 may need to be adjusted for other languages inorder to maximize the efficacy of these descriptorarrays.Many variations remain to be tested.
We plan to:(i) test the approach on French and perhaps Ger-man, (ii) perform systematic studies on the effectsof asymmetric context sizes, different part-of-speechcategorizations, different hresholds, and larger de-scriptor arrays, (iii) apply the approach to texts withunusual or very loosely constrained markup formats,82Context Training Training Cross Testing TestingSize Epochs Error Error Errors Error (%)4-context 1731 1.52 2.36 1424 5.22%6-context 218 0.75 2.01 409 1.50%8-context 831 0.043 1.88 877 3.21%Table 2: Results of comparing context sizes (2 hidden units).# Hidden Training Training Cross TestingUnits Epochs Error Error Errors1 623 1.05 1.61 7212 216 1.08 2.18 4093 239 0.39 2.27 4354 350 0.27 1.42 1343TestingError (%)2.64%1.50%1.59%4.92%Table 3: Results of comparing hidden layer sizes (6-context).
Training was done on 573 items, using a crossvalidation set of 258 items.and perhaps even to other markup recognition prob-lems, and (iv) compare the use of the neural net withmore conventional tools such as decision trees andHidden Markov Models.Acknowledgements  The authors would like toacknowledge valuable advice, assistance, and en-couragement provided by Manuel F?hndrich, HaymHirsh, Dan Jurafsky, Terry Regier, and JeanetteFigueroa.
We would also like to thank Ken Churchfor making the PARTS data available, and Ido Da-gan, Christiane Hoffmann, Mark Liberman, JanPedersen, Martin RSscheisen, Mark Wasson, andJoe Zhou for assistance in finding references and de-termining the status of related work.
Special thanksto Prof. Franz Guenthner for introducing us to theproblem.The first author was sponsored by a GAANN fel-lowship; the second author was sponsored in partby the Advanced Research Projects Agency underGrant No.
MDA972-92-J-1029 with the Corporationfor National Research Initiatives (CNRI) and in partby the Xerox Palo Alto Research Center (PARC).ReferencesLeo Breiman, Jerome H. Friedman, Richard Olshen, andCharles J.
Stone.
1984.
Classification and regressiontrees.
Wadsworth International Group, Belmont, CA.Kenneth W. Church and Mark Y. Liberman.
1991.
Astatus report on the ACL/DCI.
In The Proceedings ofthe 7th Annual Conference of the UW Centre for theNew OED and Tezt Research: Using Corpora, pages84-91, Oxford.Kenneth W. Church.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.
In Sec-ond Conference on Applied Natural Language Process-ing, pages 136-143, Austin, TX.Doug Cutting, Julian Kupiec, Jan Pedersen, and Pene-lope Sibun.
1991.
A practical part-of-speech tagger.83In The 3rd Conference on Applied Natural LanguageProcessing, Trento, Italy.W.
Francis and H. Kucera.
1982.
Frequency Analysis ofEnglish Usage.
Houghton Mifl\]in Co., New York.William A. Gale and Kenneth W. Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational Linguistics, 19(1):75-102.John Hertz, Anders Krogh, and Richard G. Palmer.1991.
Introduction to the theory of neural compu-tation.
Santa Fe Institute studies in the sciences ofcomplexity.
Addison-Wesley Pub.
Co., Redwood City,CA.Susanne M. Humphrey.
1989.
Research on interactiveknowledge-based indexing: The medindex prototype.In Symposium on Computer Applications in MedicalCare, pages 527-533.Martin Kay and Martin R6schelsen.
1993.
Text-translation alignment.
Computational Linguistics,19(i):121-142.Mark Y. Liberman and Kenneth W. Church.
1992.
Textanalysis and word pronunciation i  text-to-speech syn-thesis.
In Sadaoki Furui and Man Mohan Sondhi, edi-tors, Advances in Speech Signal Processing, pages 791-831.
Marcel Dekker, Inc.Hans Miiller, V. Amerl, and G. Natalls.
1980.Worterkennungsverfahren als Grundlage einer Unl-versalmethode zur automatlschen Segmentierung yonTexten in Sgtze.
Ein Verfahren zur maschineUenSatzgrenzenbestimmung im Englischen.
Sprache undDatenverarbeitung, 1.Michael D. Riley.
1989.
Some applications of tree-basedmodelling to speech and language indexing.
In Pro-ceedings of the DARPA Speech and Natural LanguageWorkshop, pages 339-352.
Morgan Kaufrnann.
