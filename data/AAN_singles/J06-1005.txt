Automatic Discovery of Part?Whole RelationsRoxana Girju?University of Illinois atUrbana-ChampaignAdriana Badulescu?Language Computer CorporationDan Moldovan?Language Computer CorporationAn important problem in knowledge discovery from text is the automatic extraction of semanticrelations.
This paper presents a supervised, semantically intensive, domain independent ap-proach for the automatic detection of part?whole relations in text.
First an algorithm is describedthat identifies lexico-syntactic patterns that encode part?whole relations.
A difficulty is that thesepatterns also encode other semantic relations, and a learning method is necessary to discriminatewhether or not a pattern contains a part?whole relation.
A large set of training examples havebeen annotated and fed into a specialized learning system that learns classification rules.
Therules are learned through an iterative semantic specialization (ISS) method applied to nounphrase constituents.
Classification rules have been generated this way for different patterns suchas genitives, noun compounds, and noun phrases containing prepositional phrases to extractpart?whole relations from them.
The applicability of these rules has been tested on a test corpusobtaining an overall average precision of 80.95% and recall of 75.91%.
The results demonstratethe importance of word sense disambiguation for this task.
They also demonstrate that differentlexico-syntactic patterns encode different semantic information and should be treated separatelyin the sense that different clarification rules apply to different patterns.1.
IntroductionThe identification of semantic relations in text is at the core of Natural LanguageProcessing and many of its applications.
Detecting semantic relations between varioustext segments, such as phrases, sentences, and discourse spans, is important for auto-matic text understanding (Rosario, Hearst, and Fillmore 2002; Lapata 2002; Morris andHirst 2004).
Furthermore, semantic relations represent the core elements in the organi-zation of lexical semantic knowledge bases intended for inference purposes.
Recently,there has been a renewed interest in text semantics as evidenced by the international?
Computer Science Department, University of Illinois at Urbana-Champaign, Urbana, IL 61801, E-mail:girju@uiuc.edu.?
Language Computer Corporation, 1701 N. Collins Blvd.
Suite 2000, Richardson, TX 75080, E-mail:adriana@languagecomputer.com.?
Language Computer Corporation, 1701 N. Collins Blvd.
Suite 2000, Richardson, TX 75080, E-mail:moldovan@languagecomputer.com.Submission received: 21 October 2003; revised submission received: 7 March 2005; accepted forpublication: 1 August 2005.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 32, Number 1participation in the Senseval 3 Semantic Roles competition,1 the associated workshops,2and numerous other workshops.An important semantic relation for many applications is the part?whole relation, ormeronymy.
Let us notate the part?whole relation as PART(X, Y), where X is part of Y. Forexample, the compound nominal door knob contains the part?whole relation PART(knob,door).
Part?whole relations occur frequently in text and are expressed by a variety oflexical constructions as illustrated in the text below.
(1) The car?s mail messenger is busy at work in the mail car as the train movesalong.
Through the open side door of the car, moving scenery can be seen.The worker is alarmed when he hears an unusual sound.
He peeksthrough the door?s keyhole leading to the tender and locomotive cab and seesthe two bandits trying to break through the express car door.3There are several part?whole relations in this text: 1) the mail car is part of the train,2) the side door is part of the car, 3) the keyhole is part of the door, 4) the cab is part of thelocomotive, 5) the tender is part of the train, 6) the locomotive is part of the train, 7) the dooris part of the car, and 8) the car is part of the express train (in the compound noun expresscar door).This paper provides a supervised, knowledge-intensive method for the automaticdetection of part?whole relations in English texts.
Based on a set of positive (encodingmeronymy) and negative (not encoding meronymy) training examples provided andannotated by us, the algorithm creates a decision tree and a set of rules that classifynew data.
The rules produce semantic conditions that the noun constituents matchedby the patterns must satisfy in order to exhibit a part?whole relation.
For the dis-covery of classification rules we used C4.5 decision tree learning (Quinlan 1993).
Thelearned function is represented by a decision tree transformed into a set of if?thenrules.
The decision tree learning searches a complete hypothesis space from simple tocomplex hypotheses until it finds a hypothesis consistent with the data.
Its bias is apreference for the shorter tree that places high information gain attributes closer to theroot.For training purposes we used WordNet, and the LA Times (TREC9)4 and SemCor1.75 text collections.
From these we formed a large corpus of 27,963 negative examplesand 29,134 positive examples of well distributed subtypes of part?whole relationshipswhich provided a comprehensive set of classification rules.
The rules were tested on1 http://www.senseval.org/senseval3.2 The Computational Lexical Semantics Workshop at the 2004 Human Language Technology(HLT/NAACL) conference; the first and second Workshops on Text Meaning and Interpretation at theHLT/NAACL-03 and the 2004 Association for Computational Linguistics conference (ACL), respectively;the first and second Workshops on Multiword Expressions at ACL 2003 and 2004; the ACL 2005Workshop on Deep Lexical Acquisition.3 This example is an excerpt from a review of the 1903 movie ?The Great Train Robbery?
(http://filmsite.org/grea.html).4 TREC 9 is a text collection provided by NIST for the Question Answering competition (TREC-QA) at theTExt Retrieval Conference in 2000.
It contains 3 GBytes of news articles from the Wall Street Journal,Financial Times, LA Times, Financial Report, AP Newswire, San Jose Mercury News, and ForeignBroadcast Information Center from 1989 to 1994.5 The SemCor collection (Miller et al, 1993) is a subset of the Brown Corpus and consists of 352 newsarticles distributed into three sets in which the nouns, verbs, adverbs, and adjectives have been manuallytagged with their corresponding WordNet senses and part-of-speech tags using Brill?s tagger (1995).84Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relationstwo different text collections (LA Times and Wall Street Journal) obtaining an overallaverage precision of 80.95% and recall of 75.91%.In this paper we do not distinguish between situations when whole objects consistof parts that are always present, or parts that are only sometimes present.
For example,it might be relatively easy to pin down the parts of a car (e.g., four wheels, one engine,as ever present parts of a car irrespective of its type) as compared to enumerating allthe components of a sandwich (e.g., two layers of cheese and/or salami, two slices of bread,that depend on the type of sandwich).
In our experiments we focus only on part?wholeinstances that are mentioned in the corpus employed and on those provided by general-purpose lexical knowledge bases such as WordNet,6 whether the parts are just sometimesconstituents of the entity considered or are always present.
We do not check for thevalidity of these instances (e.g., whether the instance ?wood is part of a sandwich?
is trueor not).
Based on a large training corpus of positive and negative part?whole examples,our system infers what type of objects are parts and wholes.
Also, our system does nottake into consideration modality information such as knowledge about the possibility,certainty, or probability of existence of part?whole relations.The paper is organized as follows.
Section 2 presents a summary of previouswork on meronymy from several perspectives.
Section 3 gives a detailed classifica-tion of the lexico-syntactic patterns used to express meronymy in English texts anda procedure for finding these patterns.
Section 4 describes a method for learningsemantic classification rules, while Section 5 shows the results obtained for discov-ering the part?whole relations by applying the classification rules on two distincttest corpora.
Section 6 comments on the method?s limitations and extensions, andSection 7 discusses the relevance of the task to NLP applications.
Conclusions areoffered in Section 8.2.
Previous Work on MeronymyHistorically, part?whole or meronymy relations have played an important role in lin-guistics, philosophy, and psychology mainly because a clear understanding of part?whole relations requires a deep interaction of logic, semantics, and pragmatics as theyprovide the tools needed for our understanding of the world.
The part?whole rela-tion has been considered a fundamental ontological relation since the atomists (Plato,Aristotle, and the Scholastics).
They were the first to give a systematic characterizationof parts and wholes, the relation between them, and the inheritance properties of thisrelation.
However, most of the investigations of part?whole relations have been madesince the beginning of the 20th century.The logical/philosophical studies of meronymy were concerned with formal theo-ries of parts (mereologies), wholes, and their relation in the context of formal ontology.This school of thought advocates a single, universal, and transitive part-of relation usedfor modeling various domains such as time and space.Simons (1986) criticized this standard extensional view and proposed a moreadequate account that offers an axiomatic representation of the part-of relation as astrict partial-ordering relation.
The axioms considered were: existence (if A is a part ofB then both A and B exist), asymmetry (if A is a part of B then B is not a part of A),supplementarity (if A is a part of B then B has a part C disjoint of A), and transitivity (if6 For example, in WordNet 1.7 the only part listed for the concept sandwich is bread.85Computational Linguistics Volume 32, Number 1A is a part of B and B is a part of C then A is a part of C).
In 1991, Simons (1991) addedtwo more axioms: extensionality (objects with the same parts are identical) and existenceof mereological sum (for any number of objects there exists a whole that consists exactlyof those objects).Linguistics and cognitive psychology researchers focused on different part?wholerelations and their role as semantic primitives.
Since there are different ways in whichsomething can be expressed as part of something else, many researchers have claimedthat meronymy is a complex relation that ?should be treated as a collection of relations,not as a single relation?
(Iris, Litowitz, and Evens 1988).Based on psycholinguistic experiments and the way in which the parts contributeto the structure of the wholes, Winston, Chaffin, and Hermann (1987) determinedsix types of part?whole relations: (1) COMPONENT?INTEGRAL OBJECT, (2) MEMBER?COLLECTION, (3) PORTION?MASS, (4) STUFF?OBJECT, (5) FEATURE?ACTIVITY, and (6)PLACE?AREA.They also proposed three relation elements ( functional, homeomerous, and separable) tofurther classify the six types of meronymy relations.
The functional relational elementindicates that the part has a function with respect to its whole, whereas homeomerousmeans that the part is identical to the other parts making up the whole.
The separablerelational element shows that the part can be separated from the whole.
For example,the relation wheel?car is a COMPONENT?INTEGRAL part?whole relation that is functional,non-homeomerous and separable.
This means that the wheel has a specific function withrespect to the car, does not resemble the other parts of the car, and can be separated fromthe car.The COMPONENT?INTEGRAL relation is the relation between components and theobjects to which they belong.
Integral objects have a structure, their components are sep-arable and have a functional relation with their wholes.
For example, kitchen?apartmentand aria?opera are COMPONENT?INTEGRAL relations.The MEMBER?COLLECTION relation represents membership in a collection.
Mem-bers are parts, but they cannot be separated from their collections and do not play anyfunctional role with respect to their whole.
For example, soldier?army, professor?faculty,and tree?forest are MEMBER?COLLECTION relations.PORTION?MASS captures the relations between portions and masses, extensive ob-jects, and physical dimensions.
The parts are separable and similar to each other andto the wholes which they comprise, and do not play any functional role with respect totheir whole.
For example, slice?pie and meter?kilometer are PORTION?MASS relations.The STUFF?OBJECT category encodes the relations between an object and the stuffof which it is partly or entirely made.
The parts are not similar to the wholes thatthey comprise, cannot be separated from the whole, and have no functional role.
Forexample, steel?car and alcohol?wine are STUFF?OBJECT relations.The FEATURE?ACTIVITY relation captures the semantic links within features orphases of various activities or processes.
The parts have a functional role, but they arenot similar or separable from the whole.
For example, paying?shopping and chewing?eating are FEATURE?ACTIVITY relations.PLACE?AREA captures the relation between areas and special places and locationswithin them.
The parts are similar to their wholes, but they are not separable from them.For example, oasis?desert and Guadalupe Mountains National Park?Texas are PLACE?AREArelations.In this paper we use the Winston, Chaffin, and Hermann classification as a criterionfor building the training corpus to provide a wide coverage of such subtypes of part?whole relations.86Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsIn computational linguistics, although a considerable amount of work has beendone on semantic relation detection,7 the work most similar to the task of identifyingpart?whole semantic relations is that of Hearst (1992) and Berland and Charniak (1999).Hearst developed a method for the automatic acquisition of hypernymy relations byidentifying a set of frequently used and mostly unambiguous lexico-syntactic patterns.For example, countries, such as England indicates a hypernymy relation between thewords countries and England.
In her paper, she mentions that she tried applying thesame method to meronymy, but without much success, as the patterns detected alsoexpressed other semantic relations.
This is consistent with our study of part?wholelexico-syntactic patterns presented in this paper.In 1999, Berland and Charniak applied statistical methods to a very large corpus8to find part?whole relations.
Using Hearst?s method, they focused on a small set ofgenitive patterns and a list of six seeds representing whole objects (book, building, car,hospital, plant, and school).
Their system?s output was an ordered list of possible partsaccording to some statistical metrics (e.g., the log-likelihood metric (Dunning 1993)).Although the training corpus used is very large, the coverage of the algorithm is smalldue to the limited number of patterns used and the small number of wholes allowed.Moreover, certain words, such as those ending in -ing, -ness, or -ity, were ruled out.Their accuracy is 55% for the first 50 ranked parts and 70% for the first 20 rankedparts.
As a baseline, they considered as potential parts the head nouns immediatelysurrounding the target whole object and ranked them based on the same statisticalmetric.
The baseline accuracy was 8%.While Berland and Charniak?s method focuses solely on identifying parts given awhole, our task targets the identification of both parts and wholes.Hearst, and Berland and Charniak observed that for ambiguous whole words, suchas plant, the method produces the weakest part list of the six seeds considered.
Althoughthey don?t provide a one-to-one comparison, Berland and Charniak mention that theirmethod outperforms Hearst?s pattern matching algorithm mainly due to the very largecorpus used.
However, neither approach addresses the pattern ambiguity problem,i.e., patterns such as genitives that can express different semantic relations in differentcontexts (the dress of silk encodes a part?whole relation, but the dress of my girl does not).The ambiguity of these patterns explains our rationale for choosing an approach basedon a machine learning method to discover discriminating rules automatically.3.
Lexico-Syntactic Patterns that Express MeronymyThe automatic discovery of any semantic relation must start with a thorough un-derstanding of the lexical and syntactic forms used to express that relation.
Sincethere are many ways in which something can be part of something else, there is avariety of lexico-syntactic structures that can express a meronymy semantic relation.7 Besides the work on semantic roles (Charniak 2000; Gildea and Jurafsky 2002; Thompson, Levy, andManning 2003), considerable interest has been shown in the automatic interpretation of various nounphrase-level constructions, such as noun compounds.
The focus here is to determine the semanticrelations that link the two noun constituents.
The best-performing noun compound interpretationsystems have employed either symbolic (Finin 1980; Vanderwende 1994) or statistical techniques(Pustejovsky, Bergler, and Anick 1993; Lauer and Dras 1994; Lapata 2002) relying on rather ad hoc,domain-specific, hand-coded semantic taxonomies, or on statistical patterns in a large corpus ofexamples, respectively.8 The North American News Corpus (NANC) of 1 million words.87Computational Linguistics Volume 32, Number 1There are unambiguous lexical expressions that always convey a part?whole relation.For example:(2) The substance consists of three ingredients.
(3) The cloud was made of dust.
(4) Iceland is a member of NATO.In these cases the simple detection of the patterns leads to the discovery of part?wholerelations.On the other hand, there are many ambiguous expressions that are explicit butconvey part?whole relations only in some contexts.
The detection of meronymy in thesecases is based on extracting semantic features of constituents and checking whether ornot these features match the classification rules.
For example, The horn is part of the car ismeronymic whereas He is part of the game is not.In the case of meronymy, since there are numerous unambiguous and ambiguouspatterns, we devised a method to find these patterns and rank them in the order oftheir frequency of use.
Our intention is to detect the most frequently occurring patternsthat express meronymy and provide an algorithm for their automatic detection anddisambiguation in text.3.1 An Algorithm for Finding Lexico-Syntactic PatternsIn order to identify lexico-syntactic forms that express part?whole relations and de-termine their distribution over a very large corpus, we used the following algorithminspired by Hearst?s (1998) work:Step 1.
Pick pairs of concepts Ci, Cj among which there is a part?whole relationFor this task, we used the information provided by WordNet 1.7 (Fellbaum 1998).In WordNet, the nouns are organized into nine hierarchies, each hierarchy beingidentified by its corresponding root concept: {abstraction}, {act}, {entity}, {event},{group}, {phenomenon}, {possession}, {psychological feature}, and {state}.
The nounsare grouped in concepts or synsets; a concept consisting of a list of synonymousword senses.
For example, {mother#1, female parent#1} is a WordNet concept.
Besidesconcepts, WordNet contains 11 semantic relations: HYPONYMY (IS?A), HYPERNYMY(REVERSE IS?A), MERONYMY (PART?WHOLE), HOLONYMY (REVERSE PART?WHOLE),ENTAIL, CAUSE?TO, ATTRIBUTE, PERTAINYMY, ANTONYMY, SYNSET (SYNONYMY), andSIMILARITY.
The part?whole relations in WordNet are further classified into threebasic types: MEMBER-OF (e.g., UK#1 IS-MEMBER-OF NATO#1), STUFF-OF (e.g., carbon#1IS-STUFF-OF coal#1), and PART-OF (e.g., leg#3 IS-PART-OF table#2) which includes all theother part?whole relations described in the Winston, Chaffin and Hermann (WCH)classification.Since the part and whole concepts provided by WordNet can belong to almostany WordNet noun hierarchy, we randomly selected 100 pairs of part?whole conceptsthat were well distributed over all nine WordNet noun hierarchies, the three WordNetmeronymic relations, and the six types of part?whole relations of WCH.
Two annotatorswith computational linguistic knowledge classified the WordNet meronymic relationsinto the WCH?s six part?whole types.
According to our annotations, the MEMBER-88Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsOF WordNet relations correspond to Winston, Chaffin, and Hermann?s MEMBER?COLLECTION relations, STUFF-OF relations correspond to WCH?s STUFF?OBJECT rela-tions, and the PART-OF correspond to the other four WCH relations.
The annotatorsobtained a 100% agreement in mapping the MEMBER-OF to MEMBER?COLLECTION,STUFF-OF to STUFF?OBJECT.
The PART-OF relations were mapped to the other four typesof WCH relations with an average agreement of 98%.
A third judge (one of the authors)checked the correctness of all the mappings and decided on the non-agreed instances.This mapping ensures that the 100 general-purpose WordNet pairs cover most of thepossible types of part?whole relations in text.Table 1 shows only 50 pairs from the set of 100 WordNet part?whole pairs andtheir distribution among the WordNet hierarchies and the part?whole types providedby WordNet and the WCH taxonomy.
For example, the pair Bucharest#1?Romania#1is a PART-OF relation in WordNet, but based on the Winston, Chaffin, and Hermannclassification it can be further classified as a more specific meronymy relation, PLACE?AREA.For the purpose of this research, we lumped together all part?whole types in theclassification of Winston et al9 However, the method presented in the paper is applica-ble to extracting subtypes of part?whole relations; separate annotations for each typewould be necessary.Step 2.
Search a corpus and extract lexico-syntactic patterns that link a pair of part?whole conceptsFor each pair of part?whole noun concepts determined above, search the Internet orany other large collection of documents and retain only the sentences containing thatpair.
Since our intention is to demonstrate that the automatic procedure proposed hereis domain independent, we chose two distinct text collections: SemCor 1.7 and theLA Times from TREC-9.
From each collection we randomly selected 10,000 sentences,which were searched for the pair of concepts selected.
Since the LA Times collection isnot word-sense disambiguated, we searched for sentences containing the pair of nounswithout considering their senses.
Out of these sentences, only some contained the part?whole pairs selected in Step 1.
We manually inspected these sentences and picked onlythose in which the pairs involved meronymy.
For example, the sentence I can feel myfingers and close my hand contains the meronymic pair finger?hand, but in this context therelationship is not expressed.
From these sentences we manually extracted meronymiclexico-syntactic patterns.
Table 2 shows for each collection the number of sentencesused, the number of sentences that contain the studied concept pairs, the number of sen-tences that contain part?whole relations, and the number of unique patterns discoveredfrom those sentences.
Seven of the unique patterns occurred in both SemCor and the LATimes.In order to extract the patterns from the SemCor collection we used its gold standardword sense annotations to our advantage and looked for the occurrences of concepts(word with the sense) in the corpus.
This explains the large difference between thenumber of sentences discovered in the two corpora.
The SemCor patterns thus extracteddid not need manual validation, since the noun concept pairs were always in a part?whole relation.9 The WCH categories were also used by the annotators to better distinguish between positive andnegative examples.89Computational Linguistics Volume 32, Number 1Table1Thelistoffiftyselectedpart?wholerelationpairsusedasinputforthelexico-syntacticpatternidentificationprocedure.WNTypeisthepart?wholetypefromWordNetandWCHTypeisthepart?wholetypefromtheWinston,ChaffinandHermanntaxonomy.PartconceptWholeconceptThePartTheWholeWNTypeWCHTypeHierarchyHierarchyactplayabstractionabstractionPART-OFPORTION?MASSairwindentityphenomenonSTUFF-OFSTUFF?OBJECTartillerybatteryentitygroupMEMBER-OFMEMBER?COLLECTIONbodyguardguardentitygroupMEMBER-OFMEMBER?COLLECTIONBucharestRomaniaentityentityPART-OFPLACE?AREAcellulosepaperentityentitySTUFF-OFSTUFF?OBJECTcheweatingentityphenomenonPART-OFFEATURE?ACTIVITYchorussonggroupentityPART-OFPORTION?MASScomputercomputernetworkentityentityPART-OFCOMPONENT?INTEGRALdoorcarentityentityPART-OFCOMPONENT?INTEGRALepilepticseizureepilepsyeventstatePART-OFFEATURE?ACTIVITYexecutivegovernmentgroupgroupMEMBER-OFMEMBER?COLLECTIONfightwarentityactPART-OFFEATURE?ACTIVITYfingerhandentityentityPART-OFCOMPONENT?INTEGRALfootlegentityentityPART-OFCOMPONENT?INTEGRALgentlebreezeBeaufortscalephenomenonabstractionMEMBER-OFMEMBER?COLLECTIONGibraltarEuropegroupentityPART-OFPLACE?AREAhandarmentityentityPART-OFCOMPONENT?INTEGRALinchfootabstractionabstractionPART-OFPORTION?MASSironsteelentityentitySTUFF-OFSTUFF?OBJECTkneelegentityentityPART-OFCOMPONENT?INTEGRALlegchairentityentityPART-OFCOMPONENT?INTEGRALletteralphabetabstractionabstractionMEMBER-OFMEMBER?COLLECTIONliquidassetscapitalpossessionpossessionPART-OFMEMBER?COLLECTIONlockdoorentityentityPART-OFCOMPONENT?INTEGRAL90Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable1(cont.
)PartconceptWholeconceptThePartTheWholeWNTypeWCHTypeHierarchyHierarchymemberorganizationentitygroupMEMBER-OFMEMBER?COLLECTIONmemorycomputerentityentityPART-OFCOMPONENT?INTEGRALmetacarpushandentityentityPART-OFCOMPONENT?INTEGRALmeterkilometerabstractionabstractionPART-OFPORTION?MASSmiddleageadulthoodabstractionabstractionPART-OFFEATURE?ACTIVITYmyocardialinfarctheartattackstateeventPART-OFFEATURE?ACTIVITYnumberseriesabstractionentityMEMBER-OFMEMBER?COLLECTIONoxygenairentityentitySTUFF-OFSTUFF?OBJECTpalmhandentityentityPART-OFCOMPONENT?INTEGRALpavementroadentityentitySTUFF-OFSTUFF?OBJECTpawcatentityentityPART-OFCOMPONENT?INTEGRALpeopleworldgroupgroupMEMBER-OFMEMBER?COLLECTIONpromenadeballentitygroupPART-OFFEATURE?ACTIVITYRomaniaEuropeentityentityPART-OFPLACE?AREARomanianRomaniaentityentityMEMBER-OFMEMBER?COLLECTIONSaharaAfricaentityentityPART-OFPLACE?AREAshowerbathentityentityPART-OFCOMPONENT?INTEGRALsnowsnowballentityentitySTUFF-OFSTUFF?OBJECTsymptomdiseasepsychfeaturestatePART-OFFEATURE?ACTIVITYtympanumearentityentityPART-OFCOMPONENT?INTEGRALvolumesetentitygroupMEMBER-OFMEMBER?COLLECTIONwatericeentityentitySTUFF-OFSTUFF?OBJECTWaterlooBelgiumentityentityPART-OFPLACE?AREAwindowcarentityentityPART-OFCOMPONENT?INTEGRALwingangelentitypsychfeaturePART-OFCOMPONENT?INTEGRAL91Computational Linguistics Volume 32, Number 13.2 Taxonomy of Part?Whole PatternsFrom the 535 part?whole relations detected from the 20,000 SemCor and LA Timessentences, 493 (92.15%) were expressed by phrase-level patterns and 42 (7.85%) bysentence-level patterns.
Overall, there were 42 unique meronymic lexico-syntactic pat-terns, of which 31 were phrase-level patterns and 11 sentence-level patterns.Recall our notation for the part?whole relation PART(X, Y), where X is part of Y.a.
Phrase-level patternsHere, the part and whole concepts are included in the same phrase.
For example, for thepattern NPX PPY the noun phrase that contains the part and the prepositional phrasethat contains the whole are found in the same noun phrase.
The engine in the car is aninstance of this pattern where X is the part (engine) and Y is the whole (car).b.
Sentence-level patternsIn these constructions, the part?whole relation is intrasentential.
The patterns containspecific verbs and the part and the whole can be found inside noun phrases or prepo-sitional phrases that contain specific prepositions.
A frequent such pattern is NPY verbNPX, where NPX is the noun phrase that contains the part, NPY is the noun phrase thatcontains the whole and the verb is restricted (see Table 2 of Appendix A).
For instance,the cars have doors is an instance of this pattern.An extension of this pattern is NPX verb NPZ PPY, with NPZ containing the wordspart or member.
An example is: The engine is a part of the car; NPX ?
the engine, PPY ?
ofthe car, and the verb ?
to be.In some instances the meronymic constructions contained combinations, conjunc-tions and/or disjunctions, of parts and wholes.
For example, NPX1X2 PPY (e.g., wheelsand engine of a car) is a form of the pattern NPX PPY.
This observation enabled us to gen-eralize the list of patterns.
A summary of phrase-level and sentence-level meronymicpatterns along with their extensions and generalizations is provided in Appendix A.Based on our observations of the corpus used for the pattern identification proce-dure and based on the results obtained by others (Evens et al 1980), we have concludedthat the lexico-syntactic patterns encoding meronymy can be classified according totheir semantic similarity and frequency of occurrence into the clusters presented inTable 3.
The clusters contain lexico-syntactic patterns that have similar semantic be-havior.
We also noticed that more than a half of cluster 4?s patterns are very rare;for example, X branch of Y; In Y, X1 verb X2; or In Y packed to X.
Overall, this clustercovers less than 7% of the part?whole patterns discovered.
Thus, for the purpose ofthis research we considered only the first three clusters of lexico-syntactic patternsexpressing meronymy.Table 2Number of sentences and patterns containing the 100 part?whole pairs in each text collectionconsidered.Collection Number of Number of sentences Number of sentences Number ofsentences containing the pairs containing patternspart?whole relationsSemCor 10,000 87 48 12LA Times 10,000 1,988 487 3092Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable 3Clusters of lexico-syntactic patterns classified based on their semantic similarity and theirfrequency of occurrence in the 20,000 sentence corpus used in the part?whole patternidentification procedure.Cluster Patterns Freq.
Coverage ExamplesC1.
genitives NPX of NPY eyes of the babyand NPY?s NPX 282 52.71% girl?s mouthverb to have NPY have NPX The table has four legs.C2.
noun NPXY 86 16.07% door knobcompounds NPYX turkey pieC3.
preposition NPY PPX 133 24.86% A bird without wings cannot fly.NPX PPY A room in the house.C4.
other others 34 6.36% The Supreme Court is a branch ofthe Government.This pattern classification criterion is justified, in part, by our desire to verifywhether or not the automatic approach proposed here is generally applicable notonly for the genitive cluster patterns (cluster 1) (Girju, Badulescu, and Moldovan2003), but also for more complex types, such as noun compounds (cluster 2) andprepositional constructions (cluster 3).
Our intuition that the proposed patterns havedifferent semantic behavior, and thus have to be treated separately in distinct clusters,is partially justified by a linguistic analysis summarized in Section 3.3 and supportedby our empirical results from Section 5.3.
In the remainder of the paper, we refer tothese clusters as the genitives (cluster 1), noun compounds (cluster 2), preposition(cluster 3), and other (cluster 4) clusters.We also noticed that some patterns, such as the genitive and preposition clusters,prefer the part and the whole in a certain position.
For example, in of?genitives the partis mostly in the first position (modifier), and the whole in the second (head) (e.g., door ofthe car), while in s?genitives the positions are reversed (e.g., car?s door).
The verb to haverequires parts in the second position, while noun compounds have a preference for themin the second position (e.g., car has door and car door, respectively).
In the prepositioncluster patterns, for the preposition in the part is usually in the first position (e.g., doorin the car) and for the preposition with the positions are reversed (e.g., car with four doors).However, there are also exceptions.
For instance, in some of?genitives the part canoccupy the second position (e.g., flock of birds) and in some noun compounds it canbe present in the first position (e.g., ham sandwich).
In the corpus used for patternidentification these exceptions are rare.
Therefore, we will not consider the patterns NPYof NPX and NPXY in our experiments.
If such examples are encountered, the part andthe whole concepts are wrongly identified, representing one source of errors.Berland and Charniak (1999) also used Hearst?s algorithm to find part?whole pat-terns.
However, they focused only on the first five patterns that occur frequently in theircorpus.
These patterns are subsumed by our clusters as shown in Table 4.
They noticedthat the last three patterns are ambiguous and decided to use only the first two in theirexperiments.3.3 The Ambiguity of Part?Whole Lexico-Syntactic PatternsFrom the list of lexico-syntactic patterns thus extracted, we noticed that some of thesepart?whole constructions always refer to meronymy, but most of them are ambiguous,93Computational Linguistics Volume 32, Number 1Table 4The patterns used by Berland and Charniak and the corresponding cluster patterns used by us.Berland and Charniak patterns Our cluster patterns ExampleNNwhole ?s NNpart NPY ?s NPX girl?s mouthNNpart of (the|a) (JJ|NN) NNwhole NPX of NPY eyes of the babyNNpart in (the|a) (JJ|NN) NNwhole NPX PPY ball in red boxNNparts of NNwholes NPX of NPY doors of carsNNparts in NNwholes NPX PPY quotations in articlesin the sense that they express a part?whole relation only in some particular contexts andonly between specific pairs of nouns.
For example, NP1 is member of NP2 always refersto meronymy, but this is not true for NP1 has NP2.
In most cases, the verb to have has thesense of to possess, and only in some particular contexts refers to meronymy.Table 5 presents a summary of some of the most frequent part?whole lexico-syntactic patterns we observed, classified based on their ambiguity.Below we discuss further the ambiguities encountered in the patterns of the firstthree clusters.The Semantic Ambiguity of Genitive ConstructionsIn English there are two kinds of genitives: the s-genitive and the of-genitive.
A char-acteristic of the genitives is that they are very ambiguous, as the constructions can begiven various interpretations (Moldovan and Badulescu 2005).
For instance, genitivescan encode relations such as PART?WHOLE (Mary?s hand), POSSESSION (Mary?s car),KINSHIP (Mary?s sister), PROPERTY/ATTRIBUTE HOLDER (Mary?s beauty), DEPICTION?DEPICTED (Mary?s painting ?
if it depicts her), SOURCE-FROM (Mary?s birth city), orTable 5Examples of meronymic expressions based on their ambiguity.Types of Positive Examples (part?whole) Negative Examples (not part?whole)Part?WholeExpressionsUnambiguous The parts of an airplane includethe engine, ..The substance consists ofthree ingredients.One of the air?s constituents isoxygen.The cloud was made of dust.Iceland is a member of NATO.Ambiguous The horn is part of the car.
He is part of the game (PARTICIPANT?EVENT)The table has four legs.
Kate has four cats.
(POSSESSION)The girl?s mouth is sensual.
Mary?s brother is cute.
(KINSHIP)The eyes of the baby are blue.
The dress of my niece is blue.
(POSSESSION)Each door knob was made of silver.
Dallas is a modern Texas city.
(LOCATION)It was the girl with blue eyes.
The woman with triplets received a lotof attention.
(KINSHIP)94Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsMAKE-PRODUCE (Mary?s novel ?
if Mary wrote it).
Thus, any attempt to interpretgenitive constructions has to deal with the semantic analysis of the two noun con-stituents.
Sometimes world knowledge or more contextual information is necessary toidentify the correct semantic relation (e.g., Mary?s novel might mean the novel writtenby Mary, read by Mary, or dreamed about by Mary).The Semantic Ambiguity of the Verb To HaveAccording to WordNet 1.7, the verb to have in transitive constructions has 21 differentsenses, such as to possess, feature, need, get, undergo, be confronted with, accept, sufferfrom, and many others.
Although the senses enumerated in WordNet represent a ratherdisparate set with no well defined semantic connection among them, the verb to havecan participate in many different semantic structures and has been studied extensivelyin the linguistics community (Freeze 1992; Schafer 1995; Jensen and Vikner 1996).The semantic relations encoded by the verb to have are quite similar to those realizedby genitive constructions.
Some researchers (Jensen and Vikner 1996) offered a detailedanalysis for the purpose of capturing the most important semantic features of the verbto have.
Their hypothesis is based on the idea that, semantically, the verb to have hasa sense of its own derived from the semantic interpretation of the close context orthe sentence in which it occurs.
Let?s consider the following sentences: (a) Kate has a sister(KINSHIP), (b) Kate has a cat (POSSESSION), and (c) Kate has green eyes (PART?WHOLE).
Themeaning of the verb to have in these situations is derived from the semantic informationencoded in both the subject and the object.The Semantic Ambiguity of Noun CompoundsNoun compounds (NCs) are noun sequences of the type N1 N2 .. Nn that have aparticular meaning as a whole.
NCs have been studied intensively in linguistics (Levi1978), psycholinguistics (Downing 1977), and computational linguistics (Spa?rck Jones1983; Lauer and Dras 1994; Rosario and Hearst 2001) for a long time.
The interpretationof NCs focuses on the detection and classification of a comprehensive set of semanticrelations between the noun constituents.
This task has proved to be very difficult due tothe complex semantic aspect of noun compounds:1.
NCs have implicit semantic relations: for example, spoon handle(PART?WHOLE).2.
NCs?
interpretation is knowledge intensive and can be idiosyncratic: Forexample, GM car (in order to correctly interpret this compound we have toknow that GM is a car-producing company).3.
There can be many possible semantic relations between a given pair ofword constituents.
For example, linen bag can mean bag made of linen(PART?WHOLE), as well as bag for linen (PURPOSE).4.
Interpretation of NCs can be highly context-dependent.
For example, applejuice seat can be defined as ?seat with apple juice on the table in front of it?
(Downing 1977).The Semantic Ambiguity of Prepositional ConstructionsIn English and various other natural languages, prepositions play a very important roleboth syntactically and semantically in the phrases, clauses, and sentences in which they95Computational Linguistics Volume 32, Number 1occur.
Semantically speaking, prepositional constructions can encode various seman-tic relations, their interpretations being provided most of the time by the underlyingcontext.
For instance, in the following examples the preposition with encodes differentsemantic relations: (a) It was the girl with blue eyes (MERONYMY), (b) The baby with thered ribbon is cute (POSSESSION), and (c) The woman with triplets received a lot of attention(KINSHIP).The variety and ambiguity of these constructions show the complexity and impor-tance of our task.
We have seen that the interpretation of these constructions dependsheavily on the meaning of the two noun constituents.
To get the meaning of the nounswe rely on a word sense disambiguation system that takes into consideration surround-ing contexts of the words.4.
A Machine Learning Algorithm for Automatic Discovery of Classification Rules4.1 ApproachIn this section we propose a method for the automatic discovery of rules that discrimi-nate whether or not a selected pattern instance is meronymic.
First a corpus is preparedand patterns from clusters C1?C3 are identified.
The approach relies on the assumptionthat the semantic relation between two noun constituents representing the part and thewhole can be detected based on nouns?
semantic features.This procedure applies to ambiguous constructions.
The unambiguous construc-tions don?t have to be processed since they lead unmistakably to part?whole relations.The system learns automatically classification rules that check semantic features ofnoun constituents.
The classification rules are learned through an iterative semantic spe-cialization (ISS) procedure applied on the noun constituents?
semantic features providedby the WordNet lexical knowledge base (Fellbaum 1998).
ISS starts by mapping thetraining noun pairs to the corresponding top WordNet noun concepts using hypernymychains.
Then, it builds a learning tree by recursively splitting the training corpus intounambiguous and ambiguous examples based on the semantic information providedby the WordNet noun hierarchies.
The learning tree is built top-down, one level at atime, each level corresponding to a specialization iteration.
The internal nodes representsets of ambiguous examples at various levels of specialization, while the leaves containunambiguous examples.
The ambiguous examples are further specialized with next-level WordNet concepts.
The process is repeated recursively until there are no moreambiguous examples.
For each set of unambiguous positive and negative examplesat each level in the downward descent, we apply Quinlan?s C4.5 algorithm and learnclassification rules of the form if X is/is not of a WordNet semantic class A and Y is/is not ofWordNet semantic class B, then the instance is/is not a part?whole relation.4.2 Preprocessing Part?Whole Lexico-Syntactic PatternsSince our discovery procedure is based on the semantic information provided by Word-Net, we need to preprocess the noun phrases (NPs) extracted by the three clustersconsidered and identify the potential part and the whole concepts.
For each NP we keeponly the largest sequence of words (from left to right) defined in WordNet.
For example,from the noun phrase brown carving knife the procedure retains only carving knife, sincethis concept is defined in WordNet.
For each such sequence of words, we manuallyannotate it with its WordNet sense in context.
For the example above we annotatedthe noun phrase with sense #1 (carving knife#1), since in that context it had sense #1 in96Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsWordNet (for this concept WordNet lists only one sense, defined as ?a large knife usedto carve cooked meat?).
Table 6 shows a few examples of patterns from different clustersand the results of this preprocessing step.4.3 Building the Training CorpusIn order to learn the classification rules, we used the SemCor 1.7 and TREC 9 textcollections, and the part?whole information provided by WordNet.
From the SemCorcollection we selected 19,000 sentences.
Another 100,000 sentences were randomly ex-tracted from the LA Times articles of TREC 9.
As SemCor 1.7 is already annotated withpart-of-speech tags and WordNet senses, we part-of-speech tagged only the LA Timescollection using Brill?s tagger (1995).
A corpus ?A?
was thus created from the selectedsentences of each text collection.
Each sentence in this corpus was then parsed using thesyntactic parser developed by Charniak (2000).
Focusing only on sentences containingthe lexico-syntactic patterns in each cluster C1?C3, we manually annotated nouns in thepatterns with their corresponding WordNet senses (with the exception of those fromSemCor), as shown in Section 4.2, and marked all candidate instances that encoded apart?whole relation as positives, and negatives otherwise.
In the corpus, 66% of theannotated instances were PART-OF relations, 14% STUFF-OF, and 20% MEMBER-OF.Moreover, WordNet 1.7 contains 27,636 part?whole relations linking various nounconcepts.
As this information is very valuable for training purposes, we tried to seewhich of the selected patterns match these pairs.
For each WordNet part?whole pair,we formed inflected queries (to capture singular and plural instances) and searchedthe Web, the largest on-line general purpose text collection, using Altavista.
From thefirst 100 retrieved documents, we selected and syntactically parsed only those sen-tences containing pairs within cluster patterns.
We manually validated those instancesand registered which cluster(s) of patterns could extract the pair.
All these sentencesformed a second corpus, corpus ?B?.
For instance, for the pair door#4?car#1 we searchedAltavista for documents containing both words car and door.
Then, we retrieved all thesentences that contained the two words in at least one of the target patterns.
As a result,we obtained sentences containing the pair of words linked by patterns such as door ofcar, car?s door, car has door, car with four doors, car door, etc.Overall, the 27,636 WordNet pairs were linked by the genitive cluster patterns,while the noun compound and preposition clusters extracted only some subsets of thesepairs.
Some part?whole pairs were linked by patterns that belong to more than onecluster.
For instance, door knob is a pair that usually belongs to the noun compoundcluster, but it can also be selected by the genitive cluster (e.g., knob of the door) and thepreposition cluster (e.g., the door with the iron knob).Table 6Examples of identifying the potential Part and Whole concepts for different clusters.Cluster Example Potential Part(X) and Positive orWhole(Y) concepts negative exampleC1.
genitives the door of the car the [door#4]X of the [car#1]Y positivemy friend?s car my [ friend#1]Y ?s [car#1]X negativeC2.
noun compounds car door company [car door#1]X [company#1]Y negative[car#1]X [door#4]Y positiveC3.
prepositions window from the car [window#2]X from the [car#1]Y positive97Computational Linguistics Volume 32, Number 1Corpus ?B?
was used only to convince us that the part?whole pairs selected fromWordNet were representative, ie., present in the patterns considered.
Indeed corpus?B?
pairs were found in at least one of the cluster patterns.
While corpus ?A?
consistsof positive and negative examples from LA Times and SemCor collections, corpus?B?
contains only positive instances as they are WordNet part?whole pair concepts.Moreover, although corpus ?B?
has a different distribution than corpus ?A?, the nounpairs from WordNet are general-purpose and always encode a part?whole relation.Table 7 shows the statistics for the positive and negative training examples for eachcluster.
In the genitive cluster, for example, there were 18,936 such pattern instances, ofwhich 325 encoded part?whole relations, while 18,611 did not.
Thus, for the genitivecluster we used a training corpus of 27,961 positive examples (325 pairs of concepts ina part?whole relation extracted from corpus ?A?
and 27,636 extracted from WordNetas selected pairs) and 18,611 negative examples (the non-part?whole relations extractedfrom corpus ?A?
).4.4 Inter-Annotator AgreementThe part?whole relation discovery procedure proposed in this paper was trained andtested on a large corpus of human annotated examples (a part of the LA Times collectionfor both training and testing, and a part of the Wall Street Journal (WSJ) collection fortesting).
The annotators, two researchers in computational semantics, decided whetheran example pair encoded a part?whole relation or not.
The examples were disam-biguated in context: the annotators were given the pairs and the sentence in which theyoccurred.
The two annotators?
task was to determine the correct senses of the two nounconstituents and then decide if the relation is meronymic or not.
A third researcherdecided on the non-agreed word senses and relations.
The annotators were also pro-vided with the list of subtypes of meronymy relations proposed by (Winston, Chaffin,and Hermann 1987) as a guideline for detecting part?whole relations.
If an examplecontained one of the six meronymy subtypes, the annotators tagged that example aspositive (part?whole); otherwise they tagged it as a negative example.The annotators?
agreement was measured using the kappa statistic (Siegel andCastellan 1988), one of the most frequently used measures of inter-annotator agreementfor classification tasks:K =Pr(A) ?
Pr(E)1 ?
Pr(E) , (1)where Pr(A) is the proportion of times the raters agree and Pr(E) is the probability ofagreement by chance.Table 7Training corpora statistics for each of the three clusters considered.Positive examples Negative examplesCluster from WordNet as from Corpus ?A?
from Corpus ?A?evidenced by corpus ?B?C1.
genitives 27,636 325 18,611C2.
noun compounds 142 625 6,601C3.
prepositions 111 295 2,75198Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsThe K coefficient is 1 if there is total agreement among the annotators, and 0 ifthere is no agreement other than that expected to occur by chance.
This coefficientmeasures how well annotators agree at identifying both positive and negative instancesof meronymic relations.Table 8 shows the inter-annotator agreement on the part?whole classification taskfor each of the three clusters considered in both training and test phases of the part?whole relation discovery procedure.On average, the K coefficient is close to 0.85, showing a good level of agreement,for all clusters in the training and test data.
This can be explained by the instructionsthe annotators received prior to annotation and by their expertise in lexical semantics.The results also show that even for more productive genitive and noun compoundexamples, the sentence-level context was enough to disambiguate the examples mostof the time.4.5 Iterative Semantic Specialization (ISS) LearningIterative Semantic Specialization Learning is an iterative process that learns a decisiontree and classification rules by mapping the semantic features of the noun pairs tothe WordNet noun hierarchies.
The procedure starts with a generalized version of thetraining examples as pairs of top WordNet noun concepts using hypernymy chains.
Theexamples are then split into unambiguous and ambiguous.
The ambiguous examplesare further specialized with next-level WordNet concepts.
The process is repeated re-cursively until there are no more ambiguous examples.
For each set of unambiguouspositive and negative examples at each level in the downward descent, we applyQuinlan?s C4.5 algorithm and learn classification rules.
As will be shown in Section 5.3,the algorithm is applied separately to each of the three clusters considered for optimalresults.The Iterative Semantic Specialization (ISS) Learning AlgorithmInput: Positive and negative meronymic examples of pairs of concepts.
The concepts areWordNet words semantically disambiguated in context (tagged with their correspond-ing WordNet senses).Output: Classification rules in the form of semantic selectional restrictions on the modifierand head concepts using WordNet IS?A hierarchy information.Table 8The inter-annotator agreement on the part?whole classification task for each of the three clustersconsidered in both training and test phases of the part?whole relation discovery procedure.Corpus Cluster Kappa agreementLA Times genitives 0.878(training and testing) noun compounds 0.826prepositions 0.811genitives 0.880WSJ (testing) noun compounds 0.862prepositions 0.83699Computational Linguistics Volume 32, Number 1Step 1.
Generalizing the training examplesInitially, the training corpus consists of examples that have the format ?part#sense;whole#sense; target?, where target can be either Yes or No, depending whether the relationbetween the part and whole is meronymy or not: for example, ?aria#1, opera#1, Yes?.From this initial set of examples an intermediate corpus was created by expanding eachexample using the following format:?part#sense, class part#sense, whole#sense, class whole#sense; target?,where class part and class whole correspond to the WordNet top semantic classes ofthe part and whole concepts, respectively.
For instance, the previous example becomes?aria#1, entity#1, opera#1, abstraction#6, Yes?.From this intermediate corpus a generalized set of training examples is built, retain-ing only the semantic classes and the target value.
At this point, the generalized trainingcorpus contains three types of examples:1.
Positive examples ?X hierarchy#sense, Y hierarchy#sense, Yes?2.
Negative examples ?X hierarchy#sense, Y hierarchy#sense, No?3.
Ambiguous examples ?X hierarchy#sense, Y hierarchy#sense, Yes/No?The third situation occurs when the training corpus contains both positive andnegative examples for the same hierarchy types.
For example, both the relationships?apartment#1, woman#1, No?
and ?hand#1, woman#1, Yes?
are mapped into the moregeneral type ?entity#1, entity#1, Yes/No?.
However, the first example is negative (a POS-SESSION relation), while the second one is a positive example.Step 2.
Learning classification rules for unambiguous examplesFor the unambiguous examples in the generalized training corpus (those that are eitherpositive or negative), rules are determined using C4.5.
In this context, the features arethe components of the relation (the part and, respectively the whole) and the values ofthe features are the corresponding WordNet semantic classes (the furthest ancestor inWordNet of the corresponding concept).With the first two types of examples, the unambiguous ones, a new training corpuswas created on which we applied C4.5 using a 10-fold cross validation.
The corpusis split in ten permutations, 9/10 training and 1/10 testing, and the output is rep-resented by 10 sets of rules and default values generated from these unambiguousexamples.The rules obtained are if?then rules with the part and whole noun semantic sensesas preconditions.
The default value is the most probable value for the target value andis used to classify unseen instances of that type when no other rule applies.
It can beeither Yes or No, corresponding to the possible values of the target attribute (part?wholerelation or not).The rules were ranked according to their frequency of occurrence and averageaccuracy obtained for each particular set.
In order to use the best rules, we decidedto keep only those that had a frequency above a threshold (occurring in at least 7 of the10 sets of rules) and an average accuracy greater than or equal to 50%.In order to minimize the redundancies that may occur during the learning process,rules with the same classification value as the default value are ignored.
The idea is thatthe default rule incorporates all the rules with the same target value.For instance, after running C4.5 on the unambiguous set for the abstraction#6?abstraction#6 example, we obtained a list of five rules and a default value No, as shown100Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relationsin Table 9.
Rules 1 and 5 were discarded as they were incorporated into the defaultclass.
Rules 3 and 4 were also discarded as their frequency did not pass the threshold of7.
Thus, rule 2 remains the only applicable rule.After filtering the rules that have the default value or do not pass the frequency andaccuracy thresholds, there might be cases in which the set of remaining rules is empty.Step 3.
Specializing ambiguous examplesSince C4.5 cannot be applied to ambiguous examples, we recursively specialize them toeliminate the ambiguity.
The specialization procedure is based on the IS?A informationprovided by WordNet.
Initially, each semantic class represented the root of one of thenoun hierarchies in WordNet.
By specialization, the semantic class is replaced with thecorresponding hyponym for that particular sense, i.e., the concept immediately belowin the hierarchy.For this task, we again considered the intermediate training corpus of examples.For instance, the examples ?leg#2, entity#1, bee#1, entity#1, Yes?
and ?beehive#1,entity#1, bee#1, entity#1, No?
that caused the ambiguity ?entity#1, entity#1, Yes/No?, werereplaced with ?leg#2, thing#12, bee#1, organism#1, Yes?
and ?beehive#1, object#1, bee#1,organism#1, No?, respectively.
This intermediate example is thus generalized in the lessambiguous examples ?thing#12, organism#1, Yes?
and ?object#1, organism#1, No?.
This way,we specialize the ambiguous examples with more specific values for the attributes.
Thespecialization process for this particular example is shown in Figure 1.Although this specialization procedure eliminates a proportion of the ambigu-ous examples, there is no guarantee it will work for all the ambiguous examples ofthis type.
This is because the specialization splits the initial hierarchy into smallerdistinct subhierarchies, with the examples distributed over this new set of subhier-archies.
For the examples described above, the procedure eliminates the ambiguitythrough specialization of the semantic classes into new ones: thing#12?organism#1 andobject#1?organism#1.However, not all the examples can be disambiguated after only one specialization.For the examples ?leg#2, bee#1, Yes?
and ?world#7, bee#1, No?, the procedure generalizesabstraction#6?abstraction#6 into the ambiguous example ?entity#1, entity#1, Yes/No?
andthen specializes it in the ambiguous example ?part#7, organism#1, Yes/No?.
After onespecialization the ambiguity still remains.Steps 2 and 3 are repeated until there are no more ambiguous examples.
The generalarchitecture of this procedure is shown in Figure 2.Table 9The list of rules for the iteration generated by the unambiguous subset of the ambiguousexample ?abstraction#6, abstraction#6, yes/no?.
?Yes?
means part?whole relation, while ?No?means non-part?whole relation.
The global default target value of this unambiguous nodeis No.
Note that rules 3 and 4 are discarded as their frequency is below 7, and rules 1 and5 were also discarded as incorporated in the default class No.Rule no.
Part Class Whole Class Target Accuracy value Frequency1 measure#3 abstraction#6 No 92.51 92 time#5 abstraction#6 Yes 79.21 93 abstraction#6 time#5 Yes 85.70 14 abstraction#6 measure#3 Yes 63.00 15 abstraction#6 attribute#2 No 93.00 1Default No101Computational Linguistics Volume 32, Number 1Figure 1The specialization of examples ?leg#2, entity#1, bee#1, entity#1, Yes?, ?beehive#1, entity#1, bee#1,entity#1, No?, and ?world#7, entity#1, bee#1, entity#1, No?
with the corresponding WordNetsemantic classes.We observed that after the first generalization, 99.72% of the examples were am-biguous.
After each specialization, the percentage decreases.
For instance, after one levelof specialization, 97.36% of the examples for entity#1?entity#1, 96.05% for abstraction#6?abstraction#6, and 97.56% for entity#1?group#1 were ambiguous.Table 10 presents a sample of the iterations produced by the program to specializethe genitive cluster ambiguous example abstraction#6?abstraction#6.
Each indentationcorresponds to a specialization iteration.The training corpus considered for this research required on average 2.5 and at mostfive levels of specialization.The next section describes the construction of classification rules, the experiments,and the results obtained.Figure 2Diagram of the ISS system.102Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable 10A sample iteration produced by the ISS procedure for the genitive clusterabstraction#6?abstraction#6 ambiguous example.
The italicized examples are unambiguous.abstraction#6?abstraction#6attribute#2?attribute#2attribute#2?measure#3attribute#2?relation#1relation#1?attribute#2measure#3?measure#3measure#3?relation#1time#5?time#5relation#1?time#5relation#1?relation#1magnitude relation#1?magnitude relation#1part#1?part#1social relation#1?part#1communication#2?language unit#1social relation#1?social relation#1communication#2?communication#2signal#1?message#2signal#1?written communication#1written communication#1?written communication#1writing#2?writing#2message#2?written communication#15.
Formulating Classification Rules and Applying them to DiscoverPart?Whole Relations5.1 Building the Learning TreeThe ISS learning procedure presented in the previous section builds a learning tree byrecursively splitting the training corpus into unambiguous and ambiguous examples,based on the semantic information provided by the WordNet noun hierarchies.
Thelearning tree is built top-down, one level at a time, each level corresponding to aspecialization iteration.
The internal nodes represent ambiguous examples at variouslevels of specialization, while the leaves contain sets of unambiguous examples.
Forinstance, Figure 3 shows the learning tree corresponding to the specialization fromTable 10.Initially, the learning tree contains only a dummy root node that provides noinformation.
After the generalization done in step 1 of the ISS learning procedure,all the initial examples are mapped into corresponding pairs of top noun semanticclasses in WordNet and split into unambiguous and ambiguous sets based on theirtarget function.
All these new sets of examples form the first level of the learningtree.The learning tree has two types of nodes: unambiguous nodes, corresponding to thesets of unambiguous examples from each iteration (e.g., nodes 1.1, 1.3.1, and 1.4.1 fromFigure 3) and ambiguous nodes, corresponding to each ambiguous example from eachiteration (e.g., nodes 1.2, 1.3, 1.4, and 1.4.2 from Figure 3).
Each node has associated withit a pair {R, D} representing a set of rules and a default value.
The set of rules representsthe rules to be used for classifying the new instances and the default value representsthe target value (Yes if an instance is a part?whole relation and No if the instance is103Computational Linguistics Volume 32, Number 1Figure 3A snapshot of the learning subtree abstraction#6?abstraction#6 on which the combination andpropagation algorithm is exemplified.
Each node has an associated set of rules and a defaultvalue.
The rule number references are for the ?No.?
column from Table 11.not a part?whole relation) that should be returned if none of the rules classify the newinstances.After learning the classification rules in Step 2 of the ISS procedure, all the unam-biguous nodes have default values and some have rules.5.2 Formulating the Classification RulesIn order to generate an overall set of classification rules, we traverse the learning treein a bottom-up fashion, applying the rules generated at each level in this order.
Therationale of this approach is that the rules closer to the bottom are more specific,and thus more accurate.
At each level, the idea is to combine the rules associatedwith each sibling node and propagate the result to the parent.
The combination andpropagation steps are applied recursively until the root is reached.
The combinationphase guarantees that the rules to be combined are applied in a particular order at eachlevel.Figure 4 shows a typical tree corresponding to one iteration of the ISS procedure onwhich we will explain the combination and propagation algorithm.
Node L representsan internal node containing an ambiguous example.
Through specialization, the learn-ing procedure generated a set of unambiguous examples represented by the leaf LU, anda sequence of n ambiguous examples represented by the internal nodes LA1 , LA2 , .. LAn.104Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable 11The rules and default value learned for the genitive cluster for the abstraction#6?abstraction#6ambiguous example.
?Val.?
is the target value, ?Acc.?
is the rules?
accuracy, and ?Fr.?
is theiroccurrence frequency.
The numbering style used in the ?No.?
column is intended to indicaterules at different specialization levels.No.
Part Class Whole Class Val.
Acc.
Fr.
Exampleabstraction#6 abstraction#6 No glory#2?past#11 linear measure#3 measure#3 Yes 63 9 centimeter#1?decimeter#12 communication#2 communication#2 Yes act#3?play#12.1 written comm.#1 written comm.#1 No text#1?act#32.1.1 writing#2 writing#2 Yes New Testament#1?Bible#12.1.1.1 matter#6 No 79.98 9 text#1?act#32.2 indication#1 message#2 No 73.25 10 copy#1?recommendation#12.3 message#2 communication#2 No 79.72 8 irony#1?play#13 time#5 abstraction#6 Yes 79.21 9 carboniferous#1?paleozoic#1Default No glory#2?past#1The values associated with the ambiguous nodes (rules and default values) aregenerated through propagation from lower levels.Rule combination and propagation algorithm:Input: Pairs of rules and associated default values for each unambiguous and ambigu-ous node: {RU, DU}, {RA1 , DA1}, {RA2 , DA2}, ..{RAn , DAn};Output: A pair of rules and default value for parent node: {RL, DL}.Step 1.
Propagating the default value to the parent node: DL ?
DUThe default value of the unambiguous examples (DU) will be directly propagatedto the parent as the global default value of the subtree L (DL).
For example, the defaultvalue for the unambiguous node 1.1 from Figure 3 is No and it will propagate to theparent node abstraction#6?abstraction#6 (node 1 in Figure 3).If there is no unambiguous node LU (and therefore default value DU), the defaultvalue for the first ambiguous example is propagated to L. For instance, for the am-biguous node 1.4.2 (social relation#1?social relation#1), there were no unambiguous ex-Figure 4A part of the learning tree generated by the ISS learning procedure.
The pairs of rules anddefault value associated with the parent node are generated through propagation of thecombined pairs of rules and default values of the children.105Computational Linguistics Volume 32, Number 1amples; and therefore the default value from the node 1.4.2.1 (written communication#2?written communication#2) will be used.Step 2.
Propagating the rules from an ambiguous node with the same default valueto the parent node: RL ?
{RAi |DAi = DL, 1 ?
i ?
n}The ambiguous nodes are the first to be tested.
All the rules associated with theambiguous nodes having the same default value as the global one are applied with thehighest priority.
For instance, all the ambiguous nodes for abstraction#6?abstraction#6(nodes 1.2?1.8 in Figure 3) received a default value of No through propagation fromtheir descendents.
Since the default value for this node is No, it will receive all theirrules (Rules 1 and 2 from Table 12).Step 3.
Propagating the rules from an ambiguous node with the opposite defaultvalue to the parent node: RL ?
RL ?
{if Aj then RAj ?
DAj |DAj = DL, 1 ?
j ?
n}The remaining ambiguous nodes have associated with them a different defaultvalue (a non-default value).
Since the two nodes have opposite default values, thedefault value (DAj ) needs to be used when the rules for the child node (Aj) do not hold.Therefore, a new rule, specific to the example Aj, needs to be created, for handling allthe instances of Aj: if Aj then RAj ?
DAj .For example, the ambiguous node 1.4.2.1.2 (written communication#1?writtencommunication#1) has the default value No.
Its only ambiguous node (node 1.4.2.1.2.2 :writing#2?writing#2) has the default value Yes.
Therefore, a specific rule (Rule 2.1.1 fromTable 12) needs to be created for the example (Part=writing#2 and Whole=writing#2),Table 12The list of rules obtained for the ambiguous example abstraction#6?abstraction#6 for the genitivecluster.1 if Part is linear measure#3 and Whole Class is measure#3then It is a part?whole relation2 if Part is communication#2 and Whole is communication#2then2.1 if Part is written communication#1 and Whole is written communication#1then2.1.1 if Part is writing#2 and Whole is writing#2then2.1.1.1 if Part is matter#6then It is not a part?whole relationelse It is a part?whole relationelse It is not a part?whole relationelse2.2 if Part is indication#1 and Whole is message#2then It is not a part?whole relationelse2.3 if Part is message#2 and Whole is communication#2then It is not a part?whole relationelse It is a part?whole relation3 if Part is time#5 and Whole is abstraction#6then It is a part?whole relation106Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relationsthat applies its rule (Rule 2.1.1.1: if Part=matter#6 then No) and returns its default value(a non-default value) for the other cases:if Part=writing#2 and Whole=writing#2 - the ambiguous example Ajthenif Part=matter#6 then No (Is not part?whole) - the rules RAjelse Yes (Is part?whole) - the non-default value DAjStep 4.
Propagating the rules learned from an unambiguous node to the parent node:RL ?
RL ?
RULast, the rules learned from the unambiguous examples propagate to the parentnode.
They are applied last, since they are more general than the other rules.
Forexample, after running C4.5 on the unambiguous set for the abstraction#6?abstraction#6ambiguous example (Node 1 in Figure 3), and eliminating the non-satisfactory rules(see Table 9), we obtained only Rule 3: if Part is time#5 and Whole is abstraction#6 thenYes and the default value No (see Table 11).
The rule is propagated to the parent nodeabstraction#6?
abstraction#6 and applied last.In the end, the rules learned from the unambiguous examples are propagated to theparent node L. The procedure repeats until the top node of the tree is reached.
After thecombination and propagation procedure finishes, the root node contains the completeset of rules.
The default value is added as a last rule, for classifying the instances thatare not captured by the rules.A sample of the rules obtained using the ISS procedure for the genitive clusteris shown in Table 11 in the order in which they were applied and propagated to theabstraction#6?abstraction#6 node.
Table 12 shows a translation of these rules into if?then?else rules.The meaning of a rule Part Class Whole Class Val is if Part is Part Class and Whole isWhole Class, then It is a part?whole relation (Val.
= Yes) or not (Val.
= No).
For example,Rule 1 is if Part is a linear measure#3 and Whole is a measure#3, then It is a part?wholerelation.5.3 Classification Rules for Each ClusterIn this section we present the classification rules learned for each cluster using the ISSlearning procedure.
We also performed various experiments to study the similaritiesand differences among clusters, especially to determine whether or not the classifica-tion rules learned for a particular cluster can be applied with high accuracy to otherclusters.A.
Experiments with the genitive clusterThe most frequently used set of part?whole lexico-syntactic patterns is representedby the genitive cluster.
Tables 13 shows some of the classification rules learned for thiscluster by the ISS learning procedure in the order provided by the combination andpropagation algorithm.
The full list of classification rules is shown in Tables 1 and 2from Appendix B.
The unambiguous set at level 1 of the learning tree did not generateany rules.
The rule labeled Default in Table 13 shows the learning tree global defaultvalue (No).
The tables of classification rules show only the frequency and accuracy ofthe rules generated at the unambiguous nodes.107Computational Linguistics Volume 32, Number 1Table13Asampleoftheruleslearnedforthegenitivecluster.ThefulllistisprovidedinTable1,AppendixB.?Val.?meanstargetvalue(NoorYes),?Acc.?istherules?accuracy,and?Fr.?isthefrequencywithwhichtheyoccurred.Thenumberingstyleusedinthe?No.
?columnisintendedtoindicateruleslearnedatdifferentspecializationlevels.No.PartClassWholeClassVal.Acc.Fr.Exampleabstraction#6abstraction#6Noglory#2?past#11linearmeasure#3measure#3Yes639centimeter#1?decimeter#1abstraction#6entity#1Noage#1?earth#14shape#2artifact#1Yespoint#8?knife#24.1shape#2structure#1No67.6210diameter#2?plug#14.2shape#2surface#1No67.6210square#1?pegboard#1abstraction#6group#1Nohistory#3?regiment#19abstraction#6biologicalgroup#1Yes92.4410year#3?montia#110relation#1arrangement#2Yes79.409mediumfrequency#1?electromagneticspectrum#1abstraction#6phenomenon#1Nocause#2?death#211shape#2physicalphenomenon#1Yesdewdrop#1?dew#1abstraction#6psychologicalfeature#1Noamount#1?work#412measure#3structure#3Yes95.6410August#1?Gregoriancalendar#1entity#1phenomenon#1Nokeeper#2?flame#113point#2physicalphenomenon#1Yesstormcenter#3?storm#114object#1process#2Yesferricoxide#1?rust#3phenomenon#1entity#1No18process#2organism#1Yesmeiosis#1?anapsid#1108Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable13(cont.
)No.PartClassWholeClassVal.Acc.Fr.Example18.1process#2person#1No76.708growth#2?child#2phenomenon#1phenomenon#1Noinfluence#4?action#619naturalphenomenon#1naturalphenomenon#1Yesmeteor#1?meteorshower#1possession#2entity#1Nocost#1?home#220territory#2entity#1Yes69.849unitedstatesvirginislands#1?virginislands#1psychologicalfeature#1psychologicalfeature#1No22knowledgedomain#1knowledgedomain#1Yesagrology#1?agronomy#123entity#1entity#1Yesdoor#4?car#123.1causalagent#1causalagent#1Nolethaldose#1?opium#123.2causalagent#1location#1Notaxidriver#1?LosAngeles#123.3causalagent#1object#1Nodose#1?malathion#123.4point#2bodyofwater#1No94.4610headwaters#1?nile#123.5region#1bodyofwater#1No91.7910eastside#1?river#123.6line#11region#3Nodirection#1?park#123.6.1admindistrict#1admindistrict#1YesAlaska#1?UnitedStates#123.DefaultYesdoor#4?car#126group#1group#1Yesgenusamoeba#1?amoebida#126.1socialgroup#1people#1Nodictatorship#1?proletariat#126.2group#1people#1No83.868demi-monde#1?highsociety#126.3arrangement#2collection#1No82.2210classification#2?family#426.4socialgroup#1collection#1No82.2210circle#2?law#226.DefaultYesgenusamoeba#1?amoebida#1DefaultNo109Computational Linguistics Volume 32, Number 1Overall, for the genitive cluster the ISS procedure obtained 27 complex sets ofclassification rules.B.
Experiments with the noun compound clusterTaking into consideration the results already obtained for the genitive cluster, thereare three possible approaches for detecting part?whole relations using the Y X and X Ypatterns:a.
[C1] Use the classification rules obtained for the genitive cluster.b.
[C1 + C2] Determine new classification rules collectively for the genitiveand noun compound clusters (Y?s X; X of Y; Y have X; and Y X).c.
[C2] Determine classification rules only for the noun compound cluster(Y X; X Y).Table 14 shows the results obtained for the noun compound cluster using thesethree approaches.
As one can observe, the best approach is to use only the classificationrules generated by the noun compound cluster training examples.
The recall increasessignificantly when new classification rules are learned for both the genitive and nouncompound clusters, while the precision jumps considerably when the classification rulesare learned only from the noun compound cluster examples.
These statistics indicatethat the genitive and noun compound clusters encode different semantic information,and consequently should be treated separately.Table 15 shows the classification rules learned only for the noun compound cluster.C.
Experiments with the preposition clusterTaking into consideration the results obtained for the previous two clusters, thereare five possible approaches for detecting part?whole relations using X prep Y and Yprep X patterns:a.
[C1] Use the classification rules obtained for the genitive cluster.b.
[C2] Use the classification rules obtained for the noun compound cluster.c.
[C1 + C3] Determine new classification rules for all the patterns in thegenitive and preposition clusters (Y?s X; X of Y; and Y have X; Y prep X;and X prep Y).Table 14The results obtained for each of the three approaches for the Y X; X Y patterns applied on the LATimes test corpus.Results Genitives Genitives + Noun compounds Noun compounds(C1) (C1 + C2) (C2)Precision 48.43% 52.98% 79.02%Recall for cluster 58.08% 73.46% 75.33%F-measure 52.82% 61.56% 77.13%110Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable15Thesemanticclassificationruleslearnedforthenouncompoundcluster.?Val.?meanstargetvalue(NoorYes),?Acc.?istherules?accuracy,and?Fr.
?isthefrequencywithwhichtheyoccurred.No.PartClassWholeClassVal.Acc.Fr.Exampleabstraction#6abstraction#6Noart#4?advertising#11timeperiod#1timeperiod#1Yesafternoon#1?Wednesday#12message#2writtencommunication#1Yesindex#4?backmatter#13writtencommunication#1message#2Yeszipcode#1?address#6abstraction#6entity#1Noaddress#1?restaurant#14communication#2musicalcomposition#1Yes507lyric#1?ballad#15writtencommunication#1creation#2Yes507zipcode#1?address#6abstraction#6psychologicalfeature#1Notheorem#1?decomposition#16attribute#2information#3Yes507head#10?abscess#1act#2group#1Noconsolidation#2?school#17act#2people#1Yespresident#6?class#1entity#1abstraction#6Nobook#1?recipe#18surface#1communication#2Yes67.6210head#27?coin#18.1horizontalsurface#1Nodais#1?medal#1entity#1entity#1Noadvocate#1?child#19object#1bodyofwater#1Yeswater#1?pond#110covering#2instrumentality#3Yes96.5010roof#?car#111way#6structure#1Yes90.6610stairway#1?building#112opening#10artifact#1Yes84.3010window#2?bus#113covering#2structure#1Yes82.2210roof#1?building#114artifact#1covering#2Yes67.0910top#11?roof#115instrumentality#3covering#2Yeslock#1?lid#216instrumentality#3instrumentality#3Yesaccelerator#1?car#116.1conveyance#3instrumentality#3No93.5310alarm#2?seismograph#116.2furnishings#1instrumentality#3No83.5610stand#4?magazine#116.3means#2instrumentality#3No76.7910magazine#1?telescope#1111Computational Linguistics Volume 32, Number 1Table15(cont.
)No.PartClassWholeClassVal.Acc.Fr.Example16.4equipment#1instrumentality#3No72.7310recorder#1?pen#117region#1location#1Yes67.6210boundary#1?city#118region#3district#1Yes508city#?California#19region#1organism#1Yes508crown#8?tree#1entity#1group#1Nomine#1?navy#120causalagent#1people#1Yes509administrator#1?school#121organism#1people#1Yes509secretary#2?press#122organism#1socialgroup#1Yeschancellor#1?university#123plant#2socialgroup#1No67.6210rice#1?U.S.#1group#1group#1Nogovernment#1?military#124socialgroup#1set#5Yes508leader#1?party#1DefaultNoofficer#1?narcotic#1112Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable 16The results obtained for each of the five approaches for the Y prep X and X prep Y patterns in thepreposition cluster applied on the LA Times test corpus.
C1 refers to the genitive cluster, C2 tothe noun compound cluster, and C3 to the preposition cluster.Results C1 C2 C1 + C3 C2 + C3 C3 C1 + C2 + C3Precision 46.98% 61.54% 4.81% 36.36% 82.56% 40.74%Recall for cluster 61.37% 54.55% 8.26% 36.36% 62.83% 15.06%F-measure 53.25% 57.84% 6.18% 36.36% 71.36% 22.78%d.
[C2 + C3] Determine new classification rules for all the patterns in thenoun compound and preposition clusters (Y X, Y prep X and X prep Y).e.
[C3] Determine classification rules only for the preposition cluster patterns(Y prep X and X prep Y patterns).f.
[C1 + C2 + C3] Determine new classification rules for all the patterns in allthree clusters (Y?s X; X of Y; X Y; Y X, and Y have X; Y prep X and X prep Y).Table 16 shows the results obtained for the preposition cluster patterns in each ofthe five approaches used.
One can observe that the preposition cluster alone providesthe best results over all other combinations.
These statistics are also consistent with theresults obtained for the noun compound cluster experiments.
The best approach is touse only the classification rules generated by the preposition cluster training examples.Table 17 shows the classification rules learned only for the preposition clusterpatterns.5.4 Results for Discovering Part?Whole RelationsIn order to test the classification rules for the extraction of part?whole relations, weselected two different text collections: the LA Times news articles from TREC 9 and theWall Street Journal (WSJ) articles from Treebank2.10 From each collection we randomlyselected 10,000 sentences that formed two distinct test corpora.
This corpus wasparsed and disambiguated using a state-of-the-art domain independent Word SenseDisambiguation system that has an accuracy of 71% when disambiguating nouns intexts (Novischi et al 2004).
In cases in which the noun constituents were not in WordNet,we used an in-house Named Entity Recognizer (NERD) that has a 96% F-measure onMUC6 data.The part?whole relations extracted by the ISS system were validated by com-paring them with the gold standard for the test set obtained through inter-annotatoragreement.We define the precision, recall, and F-measure performance metrics in this context:Precision =Number of correctly retrieved relationsNumber of relations retrieved(2)10 Treebank2 is a text collection developed at UPenn consisting of a million words of 1989 Wall StreetJournal material.113Computational Linguistics Volume 32, Number 1Table17Thesemanticclassificationruleslearnedfortheprepositioncluster.?Val.?meanstargetvalue(NoorYes),?Acc.?istherules?accuracy,and?Fr.
?isthefrequencywithwhichtheyoccurred.No.PartClassWholeClassVal.Acc.Fr.Exampleabstraction#6abstraction#6Noforce#7?past#11statement#1speech#2Yesannouncement#1?newsconference#12signal#1message#2Yesletter#2?alphabet#13writing#2writing#2Yesaddendum#1?backmatter#14linearmeasure#1measure#3Yes508inch#1?foot#1entity#1entity#1Nochild#2?husband#15artifact#1artifact#1Yesdoor#1?room#15.1creation#2artifact#1No93.1710book#1?audiocassette#15.2artifact#1creation#2No87.7410charcoal#2?watercolor#15.3fabric#1artifact#1No82.2210knit#1?tie#15.4artifact#1float#4No82.2010outboardmotor#1?raft#15.5artifact#1excavation#3No76.7910adit#1?mine#15.6artifact#1surface#1No68.1310cargocontainer#1?maindeck#15.7line#18artifact#1No67.6210rope#1?walkway#15.8way#6way#6No67.6210path#2?door#25.9container#1furnishings#1No81.4210bottle#1?wardrobe#16naturalobject#1naturalobject#1Yespistil#1?flower#17region#1object#1Yes67.0910foot#3?shoe#18region#3artifact#1Yes507seat#1?hall#39part#4object#1Yes507autoaccessory#1?car#1entity#1group#1Noweapon#1?troop#210causalagent#1socialgroup#1Yesmember#1?association#1group#1group#1Nodelegation#1?Washington#311people#1socialgroup#1Yesyouth#2?highschool#1DefaultNoautomobile#1?garage#1114Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsRecall =Number of correctly retrieved relationsNumber of correct relations(3)F ?
measure = 21Precision +1Recall(4)Tables 18 and 19 show the overall results obtained by the ISS system on the WallStreet Journal (WSJ) and on the LA Times collections of news articles, respectively.
Theresults obtained for each cluster are summarized in Tables 1 and 2 in Appendix C.Overall, on the WSJ test set the system obtained 82.87% precision and 79.09% recallon these three clusters.
Besides the 373 relations corresponding to the three clusters, 33other meronymy relations (406 ?
373) were found in the corpus corresponding to part?whole lexico-syntactic patterns that were not studied in this paper, giving us a globalpart?whole relation coverage (recall) of 72.66%.The ISS system?s results were compared to four baseline measures.
Baseline1 showsthe results obtained by the system with no word sense disambiguation (WSD), usingonly sense#1 (the most frequent sense in WordNet) for the pair of concepts.
In Baseline2,the system considered WSD and applied the specialization algorithm, but ran C4.5 onlyonce on all the unambiguous sets of specialized training examples representing all theleaves of the learning tree.
Baseline3 shows the results obtained without generalizingthe concepts; and Baseline4 shows the results obtained with automatic word sensedisambiguation (WSD) on the training corpus as opposed to the manual word sensedisambiguation used for ISS training.From the baselines?
results for both the WSJ and LA Times text collections, one cansee the importance of the WSD and IS?A generalization/specialization features to theextraction of the part?whole relations.Table 18The number of part?whole relations obtained and the accuracy in the WSJ collection.Results Baseline1 Baseline2 Baseline3 Baseline4 ISS(No WSD) (One learning) (No generalization) (Using WSD Systemfor training)Precision 7.72% 7.73% 15.71% 53.57% 82.87%Pattern recall 24% 43% 2.95% 27.87% 79.09%Relation recall 10.81% 19.37% 2.71% 25.86% 72.66%F-measure 3.56% 6.02% 4.97% 36.67% 82.05%Table 19The number of part?whole relations obtained and the accuracy in the LA Times collection.Results Baseline1 Baseline2 Baseline3 Baseline4 ISS(No WSD) (One learning) (No generalization) (Using WSD Systemfor training)Precision 2.10% 3.24% 24.34% 48.22% 79.03%Pattern recall 11.61% 42.86% 6.25% 20.61% 85.30%Relation recall 3.02% 11.16% 5.80% 30.05% 79.15%F-measure 11.68% 13.1% 9.98% 28.88% 80.94%115Computational Linguistics Volume 32, Number 1Figure 5 shows the learning curve where the classifier is trained on an incrementallyincreasing number of training data instances.
The learning curve was determined byapplying the training rules obtained through specialization on the LA Times test corpusannotated with automatic WSD.
If for 1,000 positive and 1,000 negative examples theF-measure is only 35%, for 5,000 it increases to 70%, for 10,000 to 74%, for 15,000 to77%, and it stabilizes at 87% for 20,000 examples.
The learning curve shows that the ISSsystem obtains an F-measure of about 75% with only 16.8% of the training data.5.5 Comparison with Previous WorkIn this section we compare our work with two other approaches most similar to our taskof part?whole semantic relation detection.Berland and Charniak (1999) limit their approach to single words denoting someentities that have recognizable parts, such as car and building.
As they also observe,this approach causes errors, such as the detection of conditioner is part of car instead ofair conditioner is part of car.
Our system is considerably more knowledge intensive, butmore general in the sense that it relies on WordNet and NERD to detect both singleword and multiple word concepts in context.
Moreover, their system was tested onlyon a working list of predefined highly probable wholes for their corpus based on thegenitive syntactic patterns.
In contrast, the ISS system can disambiguate any pair ofconcepts, provided they are in WordNet or can be classified by NERD.In order to eliminate a part of the data ambiguities, Berland and Charniak applyan ad hoc filtering procedure to eliminate those instances that represent properties orqualities of objects, such as those ending in -ing, -ity, and -ness.
Our procedure is generalenough to treat both positive and negative example instances.Using the genitive patterns they find parts of a predefined list of wholes from alarge text collection.
Our method, however, determines if two noun concepts are in apart?whole relation or not.
By generalizing the method to all the parts and wholes fromour testing corpus, the accuracy of the system will fall.
On the other hand, to be ableto test the system on their six whole concepts we would need thousands of positiveand negative examples for each such word.
For instance, for the word book, BerlandFigure 5The learning curve for the number of learning examples.116Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relationsand Charniak had almost 2,000 examples for the top 50 ranked parts.
Unfortunately,in our LA Times testing corpus we couldn?t find more than ten parts for each of theirproposed whole objects.
Therefore, we are unable to replicate their work using our textcollection.ISS algorithm is based on an iterative semantic specialization method that allowsus to go deeper into the semantic complexity problem of the patterns considered.
Tothe best of our knowledge, ISS is the only noun-phrase interpretation system thatuses word sense disambiguation.
One other noun compound interpretation sys-tem, SENS (Vanderwende 1995), used IS?A generalizations, and considered only thefirst sense of the noun constituents.
The current state-of-the-art approaches in auto-matic detection of semantic roles (Gildea and Jurafsky 2002) have tried to use lexico-semantic hierarchies, such as WordNet, to generalize from lexical noun features.However, they also rely on the first sense listed for each noun occurring in the train-ing data.
Our experiments indicate the importance of WSD in extracting part?wholesemantic relations.6.
Limitations and ExtensionsThe difficulty of detecting part?whole relations is due to a variety of factors rangingfrom syntactic analysis, to semantic and pragmatic information.
In this section weanalyze the sources of errors occurring in our experiments and present some possibleimprovements.To arrive at an interpretation of the pair of words selected by the cluster patterns, itis first necessary to identify that both words are nouns, and not other parts of speech.For example, if Brill?s tagger mis-tags an adjective or verb as a noun, then the ISS systemwill also be affected.Our classification rule learning approach is based on the WordNet semantic classesof the two concepts that represent the part and the whole, respectively.
Thus, if theWSD system fails to annotate the concepts with the correct senses, the ISS systemcan generate wrong semantic classes, which leads to wrong conclusions.
For example,the WordNet concept end has 14 senses corresponding to 6 semantic classes (entity,abstraction, event, psychological feature, state, and act) (see Table 20).
However, not allthe senses refer to a part?whole relation (e.g., senses 4, 6, 8, 9, 11, and 14 do not).Some senses corresponding to both positive and negative examples are mapped intothe same semantic class (e.g., senses 7 and 8).
In this case, the classification error willnot affect the final result as it is eliminated in the specialization phase.
However, whena part?whole sense of end is mapped erroneously into a semantic class that is represent-ative of negative examples, then the error might propagate to the final classificationrule.For some words, WordNet does not have all their senses.
For example, the conceptsimport and export are not listed in WordNet as denoting the act of importing/exportingcommodities from a foreign country.
Thus, relations such as import of sweater and exportof milk are mis-classified.
Similar examples are participant and beneficiary for whichWordNet lists only the senses corresponding to people and not to other entities, suchas countries (e.g., a country can be one of the participants at a NATO meeting).When a noun is too specific to be found in WordNet, we rely on a named entityrecognizer (NERD).
NERD identifies people, organizations, and other information ex-traction categories and annotates them accordingly.
However, NERD doesn?t alwaysprovide the correct annotation.
For example, in the phrase attorney of York, it identifies117Computational Linguistics Volume 32, Number 1Table 20The semantic classes and part?whole status for all the senses of the concept end in WordNet.Sense Semantic Class Part?Whole ExamplesNo.
Relation?1 entity Yes end of line2 abstraction Yes end of year3 event Yes end of movie4 psychological feature No the end justifies the means5 psychological feature Yes end of section6 state No glorious end of the experiment(sense of destruction, death)7 entity Yes end of box8 entity No end hold the pass9 entity No both ends wrote encouraging thoughts10 entity Yes end of town11 act No -12 abstraction Yes In the end of the presentation13 entity Yes the end of the cloth14 act No the end from the line of scrimmageYork as a name of a person and tags it with sense#1.
However, York#1 is defined inWordNet as the House of York, the English royal house that reigned from 1461 to 1485.Consequently, the ISS system will consider York#1 a group instead of an entity, yieldingan erroneous result.The WSD tool identifies noun compounds and annotates them with the correspond-ing WordNet sense.
For instance in the sentence ?...
by/IN simply/RB/1 redesigning/VBG/1how/WRB/1 a/DT car door/NN/1 is/VBZ assembled/VBN/1?
the system annotated theconcept car door with its WordNet sense (sense 1).
This way, the ISS system considers thetwo words as a concept and not as a noun compound encoding a part?whole relation.The majority of noun compounds from the test corpus are names of people (e.g., AndreaWest, Mr. Moore), dates (e.g., Oct 12, Monday afternoon), names of institutions (e.g., Bankof America, Planters Corp., Research Inc., Johnson & Johnson), or numbers (e.g., six days, fiveyears).
After analyzing the ambiguous pairs of nouns in noun compound instances, wenoticed that only a few of them were positive examples.
This error can be easily fixedby disabling the labeling of noun compounds with word senses.Another class of errors involves the position of part and whole concepts.
For exam-ple, the part?whole instance band#1 of people#1 is detected by the pattern NPX of NPYand the system classifies erroneously band as part, and people as whole.
One way toovercome this is to further classify the patterns based on selectional restrictions on theirconstituent nouns (e.g., group nouns in of?genitives have different positions for the partand whole concepts).We present in Table 21 the types of errors and their frequency of occurrence for eachcluster and overall.Although our approach takes context into account through the use of word sensedisambiguation, it does so in a limited way, without access to the general discourseand pragmatic context within which a pair of nouns is embedded.
Various researchers(Spa?rck Jones 1983; Lascarides and Copestake 1998; Lapata 2002) showed that theinterpretation of noun compounds, for example, may be influenced by discourse andpragmatic knowledge.
For instance, the discourse context provided by the following118Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable 21Error types statistics measured on the Wall Street Journal corpus for the ISS system.ClustersError Type Genitives Noun Preposition All Error (%)compoundsWSD System 57 36 26 119 53.85%NERD module 11 5 9 25 11.31%Brill?s tagger 7 4 8 19 8.6%Missing WordNet sense 10 1 7 18 8.14%Part and Whole identification 5 10 0 15 6.79%Classification rules 7 2 5 14 6.33%Unseen examplesNoun compound annotation 0 9 2 11 4.98%Total 97 67 57 221 100%sentences prefers the PURPOSE interpretation (bag for cotton clothes) of the noun com-pound cotton bag over the PART?WHOLE meaning (bag made of cotton) (cf.
(Lapata 2002)):(5) Mary sorted her clothes into various bags made from plastic.
(6) She put her skirt into the cotton bag.11Encoding discourse knowledge is thus necessary.
However, this is an open researchproblem and involves considerable manual annotation effort.Furthermore, our experiments focused on the detection of part?whole relationsin compositional constructions.
A more general approach would consider lexicalizedinstances as well.
Pragmatic knowledge is particularly important for the interpretationof lexicalized constructions, such as soap opera.
The meaning of lexicalized instances isusually captured by semantic lexicons and dictionaries.Finally, the approach presented here can be extended to other semantic relationsencoded by the cluster patterns considered.
The only part?whole elements used in thisalgorithm were the patterns and the examples.
Thus the learning and the validationprocedures are generally applicable and we intend to generalize the method for thedetection of other semantic relations, such as KINSHIP and PURPOSE.
So far, we haveobtained encouraging results for a list of 35 general-purpose semantic relations encodedby genitives (Moldovan and Badulescu 2005), by noun compounds (Girju et al 2005),and different noun phrase-level patterns including genitives, noun compounds, and thepreposition patterns (Moldovan et al 2004).The drawback of the method presented here, as for other very precise learningmethods, is that the number of training examples needs to be very large.
If a certainclass of negative or positive examples is not seen in the training data (and therefore it isnot captured by the classification rules), the system cannot classify its instances.
Thus,the larger and more diverse the training data, the better the classification rules.11 These sentences were introduced in (Lapata 2002).119Computational Linguistics Volume 32, Number 1Table 22The components of the AH?64A Apache Helicopter found on Web documents.AH?64A Apache HelicopterHellfire air-to-surface missilemillimeter wave seeker70mm Folding Fin Aerial rocket30mm Cannon cameraarmamentsGeneral Electric 1700-GE engine4-rail launchersfour-bladed main rotoranti-tank laser guided missileLongbow millimetre wave fire control radarintegrated radar frequency interferometerrotating turrettandem cockpitKevlar seats7.
Importance to NLP ApplicationsSince part?whole semantic relations occur frequently in text and have been recognizedas fundamental ontological relations since ancient times, their discovery is paramountfor applications such as Question Answering, automatic ontology construction, textualinferencing, and others.
For questions like What parts does General Electric manufacture?,What are the components of X, What is Y made of?, and many more, the discovery of part?whole relations is necessary to assemble the right answer.The concepts and part?whole relations acquired from a collection of documentscan be useful in answering difficult questions that normally can not be handled basedsolely on keyword matching and proximity.
As the level of difficulty increases, QuestionAnswering systems need richer semantic resources, including ontologies and largerknowledge bases.
Consider the question What does the AH?64A Apache helicopter consistof?
For questions like this, the system must extract all the components the war helicopterhas.
Unless an ontology of such army attack helicopter parts exists in the knowledgebase, which in an open domain situation is highly unlikely, the system must first acquirefrom the document collection all the pieces the helicopter is made of.
These partscan be scattered all over the text collection, so the Question Answering system has togather together these partial answers into a single and concise hierarchy of parts.
Thistechnique is called answer fusion (Girju 2001).Using a state-of-the-art Question Answering system (Moldovan et al 2002) adaptedfor answer fusion and including the ISS system as a module, the question presentedabove was answered by searching the Internet (the website for the Defence Industries?army at www.army-technology.com).
The QA system started with the question focushelicopter and extracted and disambiguated all the meronymy relations using the ISSmodule.
Table 22 shows the taxonomic ontology created for this question (presentingall the parts of a whole).For example, the relation ?AH?64 Apache helicopter has part Hellfire air-to-surfacemissile?
was determined from the sentence AH?64 Apache helicopter has a Longbow-millimetre wave fire control radar and a Hellfire air-to-surface missile.
Only the heads of thenoun phrases were considered as they occur in WordNet (i.e., helicopter and air-to-surfacemissile, respectively).120Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsOntologies12 are used more and more as means to boost the accuracy of naturallanguage application systems (Moldovan and Girju 2001).
Semantically richer ontolo-gies can be built by incorporating more semantic relations in addition to the traditionalIS?A relation.
Part?whole is an excellent example of such relations.
Recently, Tatuand Moldovan (2005) have shown that semantic relations such as part?whole can becombined with other relations using a semantic calculus for the purpose of improvingthe performance of a textual inference system.8.
ConclusionsIn this paper we presented a supervised, knowledge-intensive approach to the auto-matic detection of part?whole relations encoded by the three most frequent clusters ofsyntactic constructions: (1) genitives and NP have NP clauses, (2) noun compounds, and(3) other NP PP phrases.
The detection of the part?whole relations is difficult due tothe highly ambiguous nature of the syntactic constructions, as they can encode otherrelations than meronymy.Our method for detection of part?whole relations discovers semi-automatically thepart?whole lexico-syntactic patterns and learns automatically the semantic classifica-tion rules needed for the disambiguation of these patterns.
We defined the task as abinary classification problem and used an approach that relies on the assumption thatthe semantic relation between two constituent nouns representing the part and thewhole can be detected based on the components?
semantic classification rules.
The clas-sification rules are learned automatically through an iterative semantic specialization (ISS)procedure applied on the noun constituents?
semantic classes provided by WordNet.We successfully combined the results of decision tree learning with the WordNet IS-Ahierarchy specialization for more accurate learning.
We proved the method is domainindependent.The classification rules learned by our method and listed in several tables can beeasily implemented to extract part?whole relations from text.
However, to apply theserules a word sense disambiguation system for nouns is necessary.Our experiments revealed the importance of word sense disambiguation and Word-Net IS?A specialization.
We have directly compared and contrasted the results of oursystem with a variety of baselines and have shown impressive results.
Combination ofword sense disambiguation information with IS-A semantic information in WordNetyields better performance over either WSD or IS-A specialization alone.Our experiments also showed that the three cluster patterns considered are not al-ternative ways of encoding part?whole information.
This observation is very importantfor various text understanding applications.Moreover, the approach presented can be extended to other semantic relations sincethe learning procedures are generally applicable and yield good results for sufficientlylarge training corpora.12 Gartner Group identified Ontologies as one of the leading IT technologies, ranked 3rd in its list of top 10technologies forecast for 2005.121Computational Linguistics Volume 32, Number 1Appendix A: Experiments with Meronymic PatternsTables 1, 2, and 3 present a summary of phrase-level and sentence-level meronymicpatterns and their possible extensions.Table 1The phrase-level patterns determined with the pattern identification procedure in Section 3.?Fr.?
means frequency.No.
Pattern Fr.
Example1 NPX PPY 173 door of his car?
PPY starts with of the executive of the new government2 NPX PPY 61 people in the world?
PPY starts with in3 NPX PPY 14 They organized the executive branch of government?
NPX ends with branch?
PPY begins with of4 NPX PPY 9 oxygen from air?
PPY starts with from people from all over the world5 NPX PPY 6 people throughout the world?
PPY starts with throughout6 NPX PPY 5 window at the rear of the building?
PPY starts with at7 NPX PPY 16 five fingers on one hand?
PPY starts with on8 NPX PPY 3 the door to the house?
PPY starts with to9 NPX PPY 2 people around the world?
PPY starts with around10 NPX PPY 1 people all over the world?
PPY starts with all over11 X and other Z of Y 1 in Romania and the other countries of Eastern Europe12 NPX PPY 1 severe ligament damage to the left knee?
PPY starts with to?
NPX ends with damage13 NPX PPY 1 pavement onto streets?
PPY starts with onto14 NPX PPY 1 windows outside the court building?
PPY starts with outside15 NPY PPX 7 the organization with 120 members?
PPX starts with with The spiders with 6 legs are dangerous.16 NPY PPX 4 They amputate his leg above the knee.?
PPX starts with above17 NPY?s NPX 71 car?s engineorganization?s membership18 PPX PPY 2 in an abdomen, in a reclining torso?
PPX starts with in?
PPY starts with in19 PPY PPX 1 on her car, on her window?
PPX starts with on?
PPY starts with on20 NPX, NPY 10 Bucharest, Romaniain Atlanta ,Ga.21 PPY WHPX 4 The club whose membership?
WHPX starts with whose122Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable 1(cont.)No.
Pattern Fr.
Example22 NPX1X2 PPY 8 between the executive and legislative branchesof government?
NPX1X2 ends with branches?
NPX1X2 contains and or or?
PPY begins with of23 NPX1X2 PPY 1 the memory and other features of IBM-compatiblepersonal computers.?
NPX1X2 contains and or or?
PPY begins with of24 NPY (NPX1X2) 1 the three states of Southern New England(Massachusetts, Connecticut, and Rhode Island)?
NPX1X2 contains and or or25 NPX (NPY) 1 red-bellied snake (Storeria)26 NPY NPX 42 He sell car doors27 NPZ?NPX NPY 26 a one?act ballet28 NPY NPX NPZ 12 faulty garage door lockcomputer memory chipfour?door compact car29 NPX NPY 3 membership organizationpower window buildings30 NPX?NPY NPZ 3 a play?act universe123Computational Linguistics Volume 32, Number 1Table2Thesentence-levelpatternsdeterminedwiththepatternidentificationprocedureinSection3.?Fr.
?meansfrequency.No.PatternFr.Example1NPYverbNPX18Acarhaswheels.-verbs:carry,combine,comprehend,comprise,Thecakecontainsfreshfruits.consist,contain,enclose,feature,have,hold,Anycarincludesasparetire.holdin,house,include,incorporate,inherit,Thepatientreceivedanewheart.integrate,receive,retain,subsume2NPZverbNPXPPY13Theyconstructedthecarfromengine,doors,wheels.-PPYstartswithin,into,asorfromThepriceincludesamembershipinagoodclub.-verbs:assemble,build,buildin,carry,combine,Thesystemadministratorconnectthecomputerscompose,compound,comprehend,comprise,intoacomputernetwork.connect,consist,construct,contain,coordinate,Thestateusesthesesoldiersasthemainarmy.create,embrace,enclose,enter,fabricate,feature,Thecolonelorganizedthesoldiersintoanelitearmy.file,form,have,hold,holdin,house,include,Theuserinsertsthefileintohisdirectory.incorporate,infix,inherit,insert,integrate,Theprogrammerincludesthemainprocedureinintroduce,join,link,make,manufacture,merge,theCsourcefile.observe,organize,overlap,receive,retain,subsume,unify,unite,use3NPZverbPPXPPY2Theydraggedhimoutofthecar-PPYstartswiththroughthroughthewindow.-verbs:drag,exit,leave4NPXverbNPY1Thememberjoinedtheorganizationin1976.-verbs:accommodate,add,admit,affiliate,Theoxygencomposestheair.appertain,be,bear,belong,buildin,colligate,Themaninfiltratestheorganizationin1999.compose,compound,confine,constitute,dwell,embrace,encompass,fallin,form,gettogether,infiltrate,inhere,involve,join,letin,lie,liein,make,makeup,pertain,rejoin,repose,represent,reside,rest,signup,take124Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable2(cont.)No.PatternFr.Example5NPXverbPPY1Thecytoplasminhereinacell.-verbs:attachedto,inherein(PPYstartswithinorto)6NPXverbNPZPPY2Theengineisapartofacar.-PPYstartswithofAroseisamemberofgenusRosa.-NPZispartormember7NPXNPYverb1Theheadlightsthecarhadwereblue.-verbs:carry,combine,comprehend,comprise,consist,contain,enclose,feature,have,hold,holdin,house,include,incorporate,inherit,integrate,receive,retain,subsume8NPY1verbNPXtoNPY21Themandonatedoneofhiskidneystohissister.-verbs:donate,give9InPPY,NPX1verbNPX21Inacar,thecarbodycoverstheengine.-verbs:coverinNPYpackedtoNPX1...inacarpackedtothewindowswithpersonalbelongings...10NPZPPXverbNPTPPY1InfantmortalityinRomaniaisnowthehighest?PPXstartswithininEurope.
?NPTcontainsanadjectivesuperlative-verb:be125Computational Linguistics Volume 32, Number 1Table 3Extensions for lexico-syntactic patterns discovered in the 20,000 sentence corpus used in thepattern identification procedure in Section 3.No.
Pattern Example1 NPY PPX A bird without wings cannot fly.- PPY starts with without2 NPX PPY X inside Y The walls inside the building had better colors.- PPY starts with inside126Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsAppendixB:SemanticClassificationRulesfortheGenitiveClusterTables1and2showthefulllistofsemanticclassificationruleslearnedforthegenitiveclusterfromalltheambiguousnodes.Table1Thesemanticclassificationruleslearnedforthegenitiveclusterallambiguousnodes(defaultvalueNo).?Val.?isthetargetvalue,?Acc.?istherules?accuracy,and?Fr.?istheiroccurrencefrequency.Theindentationsinthe?No.
?columnrefertorulesatdifferentspecializationlevels.No.PartClassWholeClassVal.Acc.Fr.Exampleabstraction#6abstraction#6Noglory#2?past#11linearmeasure#3measure#3Yes639centimeter#1?decimeter#12communication#2communication#2Yesact#3?play#12.1writtencommunication#1writtencommunication#1Notext#1?act#32.1.1writing#2writing#2YesNewTestament#1?Bible#12.1.1.1matter#6No79.989text#1?act#32.2indication#1message#2No73.2510copy#1?recommendation#12.3message#2communication#2No79.728irony#1?play#12.4time#5abstraction#6Yes79.219carboniferous#1?paleozoic#1abstraction#6entity#1Noage#1?earth#14shape#2artifact#1Yespoint#8?knife#24.1shape#2structure#1No67.6210diameter#2?plug#14.2shape#2surface#1No67.6210square#1?pegboard#15measure#3object#1Yesdrumstick#1?bird#25.1definitequantity#1artifact#1Nodozen#1?videotape#15.2indefinitequantity#1artifact#1Nolot#1?throttle#15.3linearmeasure#1artifact#1Nomile#1?quarters#15.4systemofmeasurement#1object#1No82.268bandwidth#1?receiver#15.5relativequantity#1object#1No79.728nothing#1?refrigerator#16position#7artifact#1Yescircle#6?theater#16.1placement#1No67.6210density#2?pattern#37writtencommunication#1instrumentality#3Yes85.7010by-line#1?writingarm#18shape#2location#1Yes66.5610point#8?arrowhead#1abstraction#6group#1Nohistory#3?regiment#1127Computational Linguistics Volume 32, Number 1Table1(cont.
)No.PartClassWholeClassVal.Acc.Fr.Example9abstraction#6biologicalgroup#1Yes92.4410year#3?montia#110relation#1arrangement#2Yes79.409mediumfrequency#1?electromagneticspectrum#1abstraction#6phenomenon#1Nocause#2?death#211shape#2physicalphenomenon#1Yesdewdrop#1?dew#1abstraction#6psychologicalfeature#1Noamount#1?work#412measure#3structure#3Yes95.6410August#1?Gregoriancalendar#1entity#1phenomenon#1Nokeeper#2?flame#113point#2physicalphenomenon#1Yesstormcenter#3?storm#114object#1process#2Yesferricoxide#1?rust#3event#1entity#1No15periodicevent#1object#1Yes66.5610wave#3?waveguide#1event#1event#1Norerun#1?televisionshow#116happening#1periodicevent#1Yes58.128flood#6?floodtide#2phenomenon#1abstraction#6Noomission#3?pronoun#117atmosphericphenomenon#1communication#2Yesgentlebreeze#1?beaufortscale#1phenomenon#1entity#1No18process#2organism#1Yesmeiosis#1?anapsid#118.1process#2person#1No76.708growth#2?child#2phenomenon#1phenomenon#1Noinfluence#4?action#619naturalphenomenon#1naturalphenomenon#1Yesmeteor#1?meteorshower#1possession#2entity#1Nocost#1?home#220territory#2entity#1Yes69.849unitedstatesvirginislands#1?virginislands#1possession#2possession#2Noliquidassets#1?capital#121assets#1transferredproperty#1Yescut#6?loot#1psychologicalfeature#1psychologicalfeature#1No22knowledgedomain#1knowledgedomain#1Yesagrology#1?agronomy#1128Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsTable2ThesemanticclassificationruleslearnedforthegenitiveclusterfromalltheambiguousnodeswithdefaultvalueYes.?Val.?meanstargetvalue(NoorYes),?Acc.?istherules?accuracy,and?Fr.?isthefrequencywithwhichtheyoccurred.Thenumberingstyleusedinthe?No.
?columnisintendedtoindicateruleslearnedatdifferentspecializationlevels.No.PartClassWholeClassVal.Acc.Fr.Example23entity#1entity#1Yesdoor#4?car#123.1causalagent#1causalagent#1Nolethaldose#1?opium#123.2causalagent#1location#1Notaxidriver#1?LosAngeles#123.3causalagent#1object#1Nodose#1?malathion#123.4point#2bodyofwater#1No94.4610headwaters#1?nile#123.5region#1bodyofwater#1No91.7910eastside#1?river#123.6line#11region#3Nodirection#1?park#123.7geographicpoint#1region#3No89.439corner#4?washington#123.8point#2geographicalarea#1No79.989stockexchange#1?istanbul#123.9district#1district#1Nocommonwealth#1?puertorico#123.9.1admindistrict#1admindistrict#1YesAlaska#1?UnitedStates#123.10location#1object#1YesRomania#1?Europe#123.10.1location#1naturalobject#1No89.5510neighborhood#1?earth#123.10.2area#1naturalobject#1No508corner#1?earth#123.10.3location#1person#1No87.9010birthplace#1?Nixon#123.11object#1causalagent#1Nobottle#1?prescriptiondrug#123.12part#4location#1No507map#1?Vietnam#123.13creation#2extremity#4Noset#4?edge#123.14facility#1region#3No93.7410railwaystation#1?Beijing#123.15creation#2region#3No87.9010miniature#2?warsaw#123.16equipment#1admindistrict#1Noreactor#2?iraq#123.17naturalobject#1point#2Noheadland#1?top#323.18block#1buildingmaterial#1No95.3210slab#1?concrete#123.19artifact#1pavingmaterial#1No91.7610slab#1?concrete#123.20creation#2structure#1No95.3210art#1?music-hall#123.21commodity#1instrumentality#3Noshipment#1?capacitor#123.22flap#1clothing#1Nohem#1?dress#123.23representation#2creation#2No67.6210spectacle#2?scenery#1129Computational Linguistics Volume 32, Number 1Table2(cont.
)No.PartClassWholeClassVal.Acc.Fr.Example23.24design#4covering#2No67.6210colors#1?paint#123.25land#3island#1No91.7910continent#1?Atlantis#123.26appendage#3instrumentality#3Nostock#7?artillery#123.27object#1part#7Noslab#1?fat#223.28object#1unit#6Noaddition#1?sodiumnitrate#123.29organism#1causalagent#1Nosupplier#1?cocaine#123.30organism#1location#1Noambassador#1?iraq#123.31organism#1object#1Noauthor#1?book#123.32organism#1organism#1Noassassin#1?Kennedy#123.33thing#12entity#1No94.6410something#1?America#123.34bodyofwater#1entity#1No68.188sea#1?interaction#123.DefaultYesdoor#4?car#124entity#1group#1Yesacademician#1?academy#224.1entity#1system#1No87.9010river#1?ecosystem#124.2artifact#1gathering#1Norostrum#1?congress#224.DefaultYesacademician#1?academy#225entity#1possession#2YesTuamotuArchipelago#1?FrenchPolynesia#125.1organism#1possession#2No85.508manager#1?investmentfunds#125.2causalagent#1possession#2No85.308buyer#1?lifeinsurance#125.DefaultYesTuamotuArchipelago#1?FrenchPolynesia#126group#1group#1Yesgenusamoeba#1?amoebida#126.1socialgroup#1people#1Nodictatorship#1?proletariat#126.2group#1people#1No83.868demi-monde#1?highsociety#126.3arrangement#2collection#1No82.2210classification#2?family#426.4socialgroup#1collection#1No82.2210circle#2?law#226.DefaultYesgenusamoeba#1?amoebida#1DefaultNo130Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsAppendix C: Performance Results per ClusterTables 1 and 2 show the performance results obtained for each cluster considered on LATimes and WSJ test corpora.Table 1The number of part?whole relations obtained and the accuracy for each cluster and for all theclusters in the WSJ collection.Results Genitives Noun compounds Preposition Allcluster cluster cluster clustersISS systemNumber of patterns 1514 1217 1081 3812Number of correctly retrieved 161 85 49 295relationsNumber of relations retrieved 202 90 64 356Number of correct relations 167 141 65 373for the pattern(s)Number of correct relations 406Precision 79.70% 94.44% 76.56% 82.87%Recall for cluster(s) 96.41% 60.28% 75.38% 79.09%Coverage 72.66%F-measure 86.87% 77.13% 71.36% 82.05%Baseline1 ?
No WSDPrecision 6.04% 50% 45.45% 7.72%Recall for the pattern(s) 36% 3.57% 22.73% 24%Coverage 10.81%F-measure 2.12% 7.41% 23.26% 3.56%Baseline2 ?
One learningPrecision 7.18% 37.5% 20% 7.73%Recall for the pattern(s) 78% 10.71% 4.54% 43%Coverage 19.37%F-measure 6% 10% 3.57% 6.02%Baseline3 ?
No GeneralizationPrecision 28.21% 0% 0% 15.71%Recall for the pattern(s) 6.59% 0% 0% 2.95%Coverage 2.71%F-measure 10.68% 1.12% 0% 4.97%Baseline4 ?
WSD using system for trainingPrecision 62.42% 50.74% 59.05% 53.57%Recall for the pattern(s) 92.40% 68.67% 54.87% 27.87%Coverage 25.86%F-measure 74.51% 58.36% 56.88% 36.67%131Computational Linguistics Volume 32, Number 1Table 2The number of part?whole relations obtained and the accuracy for each cluster and for all theclusters in the LA Times collection.Results Genitives Noun compounds Preposition Allcluster cluster cluster clustersISS systemNumber of patterns 4106 3442 2577 10125Number of correctly retrieved 321 113 71 505relationsNumber of relations retrieved 410 143 86 639Number of correct relations 329 150 113 592for the pattern(s)Number of correct relations 638Precision 78.29% 79.02% 82.56% 79.03%Recall for cluster(s) 97.57% 75.33% 62.83% 85.30%Coverage 79.15%F-measure 87.26% 73.59% 75.97% 80.94%Baseline1 ?
No WSDPrecision 1.16% 33.33% 38.46% 2.10%Recall for the pattern(s) 12.07% 4.17% 16.67% 11.61%Coverage 3.02%F-measure 10.34% 6.66% 30.3% 11.68%Baseline2 ?
One learningPrecision 3.12% 12.5% 3.84% 3.24%Recall for the pattern(s) 77.59% 8.33% 3.33% 42.86%Coverage 11.16%F-measure 13.15% 16.66% 7.4% 13.1%Baseline3 ?
No GeneralizationPrecision 34.29% 3.33% 0% 24.34%Recall for the pattern(s) 10.94% 0.67% 0% 6.25%Coverage 5.80%F-measure 16.59% 1.12% 0% 9.98%Baseline4 ?
WSD using system for trainingPrecision 52.8% 54% 39.81% 48.22%Recall for the pattern(s) 79.04% 57.45% 63.08% 20.61%Coverage 30.05%F-measure 63.31% 55.67% 48.81% 28.88%132Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsAcknowledgmentsWe would like to thank Matthew Jones forhis help in providing the gold-standardannotations for the training and test corporaused in this research.
We are gratefulfor the constructive comments made byRobert Dale and anonymous reviewersthat helped considerably clarify andimprove the presentation.
This work waspartially supported by the AdvancedResearch and Development Activity/Disruptive Technology Office.ReferencesBerland, Matthew and Eugene Charniak.1999.
Finding parts in very large corpora.In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics(ACL 1999), pages 57?64, University ofMaryland.Brill, Eric.
1995.
Transformation-basederror-driven learning and naturallanguage processing: A case study inpart-of-speech tagging.
ComputationalLinguistics, 21(4):543?566.Charniak, Eugene.
2000.
Amaximum-entropy-inspired parser.
InProceedings of the 1st Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL 2000),pages 132?139, Seattle, WA.Downing, Pamela.
1977.
On the creation anduse of English compound nouns.
Language,53(4):810?842.Dunning, Ted.
1993.
Accurate methods forthe statistics of surprise and coincidence.Computational Linguistics, 19:61?74.Evens, Martha W., Bonnie C. Litowitz,Judith A. Markowitz, Raoul N. Smith, andOswald Werner.
1980.
Lexical-semanticrelations: A comparative survey.
LinguisticResearch, pages 187?219.Fellbaum, Christiane.
1998.
WordNet?AnElectronic Lexical Database.
MIT Press,Cambridge, MA.Finin, Timothy W. 1980.
The SemanticInterpretation of Compound Nominals.
Ph.D.thesis, University of Illinois atUrbana-Champaign.Freeze, Ray.
1992.
Existentials and otherlocatives.
Language, 68:553?595.Gildea, Daniel and Daniel Jurafsky.2002.
Automatic labeling of semanticroles.
Computational Linguistics, 28(3):245?288.Girju, Roxana.
2001.
Answer fusion withon-line ontology development.
InProceedings of the 2nd Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL 2001) -Student Research Workshop, pages 23?28,Pittsburgh, PA.Girju, Roxana, Adriana Badulescu, andDan Moldovan.
2003.
Learning semanticconstraints for the automatic discovery ofpart-whole relations.
In Proceedings of the3rd Human Language Technology Conference/4th Meeting of the North American Chapter ofthe Association for Computational LinguisticsConference (HLT-NAACL 2003),pages 80?87, Edmonton, Canada.Girju, Roxana, Dan Moldovan, Marta Tatu,and Daniel Antohe.
2005.
On the semanticsof noun compounds.
Computer Speech andLanguage?Special Issue on MultiwordExpressions (in press).Hearst, Marti.
1992.
Acquisition of hyponymsfrom large text corpora.
In Proceedingsof the 14th International Conference onComputational Linguistics (COLING-92),pages 539?545, Nantes, France.Hearst, Marti.
1998.
Automated discoveryof WordNet relations.
In ChristianeFellbaum, editor, An Electronic LexicalDatabase and Some of Its Applications.MIT Press, Cambridge, MA, pages 131?151.Iris, Madelyn, Bonnie Litowitz, and MarthaEvens.
1988.
Problems with part-wholerelation.
In M. W. Evens, editor, RelationalModels of the Lexicon: RepresentingKnowledge in Semantic Networks.Cambridge University Press, Cambridge,pages 261?288.Jensen, Per Anker and Carl Vikner.
1996.
Thedouble nature of the verb have.
LAMBDA,21:25?37.Kingsbury, Paul, Martha Palmer, and MitchMarcus.
2002.
Adding semantic annotationto the Penn Treebank.
In Proceedings ofthe 2nd Human Language TechnologyConference (HLT 2002), pages 252?256,San Diego, CA.Lapata, Mirella.
2002.
The disambiguation ofnominalisations.
Computational Linguistics,28(3):357?388.Lascarides, Alex and Ann Copestake.
1998.Pragmatics and word meaning.
Journalof Linguistics, 34(2):387?414.Lauer, Mark and Mark Dras.
1994.
Aprobabilistic model of compound nouns.In Proceedings of the 7th Australian JointConference on Artificial Intelligence,pages 474?481, Armidale, Australia.Levi, Judith.
1978.
The Syntax and Semanticsof Complex Nominals.
Academic Press,New York.133Computational Linguistics Volume 32, Number 1Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English:The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Moldovan, Dan and Adriana Badulescu.2005.
A semantic scattering modelfor the automatic interpretation ofgenitives.
In Proceedings of HumanLanguage Technology Conference andConference on Empirical Methods inNatural Language Proceesing (HLT/EMNLP 2005), pages 891?898,Vancouver, BC, Canada.Moldovan, Dan, Adriana Badulescu,Marta Tatu, Daniel Antohe, and RoxanaGirju.
2004.
Models for the semanticclassification of noun phrases.
InProceedings of the Human LanguageTechnology Conference (HLT-NAACL)2004, Computational Lexical SemanticsWorkshop, Boston, MA.Moldovan, Dan and Roxana Girju.
2001.
Aninteractive tool for the rapid developmentof knowledge bases.
InternationalJournal on Artificial Intelligence Tools,10(1?2):65?86.Moldovan, Dan, Sanda Harabagiu, RoxanaGirju, Paul Morarescu, Finley Lacatusu,Adrian Novischi, Adriana Badulescu,and Orest Bolohan.
2002.
LCC tools forquestion answering.
In Proceedingsof the 11th Meeting of the Text RetrievalConference (TREC 2002), pages 388?397,Gaithersburg, MD.Morris, Jane and Graeme Hirst.
2004.Non-classical lexical semantic relations.In Proceedings of the 4th Human LanguageTechnology Conference / of the 5th Meetingof the North American Chapter of theAssociation for Computational Linguistics(HLT-NAACL 2004) - Workshop onComputational Lexical Semantics,pages 46?51, Boston, MA.Novischi, Adrian, Dan Moldovan, PaulParker, Adriana Badulescu, and BobHauser.
2004.
LCC?s WSD systems forSenseval 3.
In Proceedings of Senseval 3(ACL 2004), Barcelona, Spain.Pustejovsky, James, Sabine Bergler, andPeter Anick.
1993.
Lexical semantictechniques for corpus analysis.Computational Linguistics, 19(2):331?358.Quinlan, Ross.
J.
1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann,San Francisco, CA.Resnik, Philip.
1996.
Selectional constraints:An information-theoretic model and itscomputational realization.
Cognition,61:127?159.Resnik, Philip and Marti Hearst.
1993.Structural ambiguity and conceptualrelations.
In Proceedings of the 31stMeeting of the Association forComputational Linguistics (ACL 1993)-1st Workshop on Very Large Corpora:Academic and Industrial Perspectives,pages 58?64, Ohio State University,Columbus, OH.Rosario, Barbara and Marti Hearst.
2001.Classifying the semantic relations innoun compounds via a domain-specificlexical hierarchy.
In Proceedings of theConference on Empirical Methods inNatural Language Processing (EMNLP 2001),pages 82?90, Pittsburgh, PA.Rosario, Barbara, Marti Hearst, and CharlesFillmore.
2002.
The descent of hierarchy,and selection in relational semantics.
InProceedings of the 40th Annual Meetingof the Association for ComputationalLinguistics, pages 247?254, Universityof Pennsylvania.Schafer, Robin.
1995.
The SLP/ILPdistinction in have-predication.
InM.
Simons and T. Galloway, editors,Proceedings from Semantics andLinguistic Theory V. Cornell UniversityDepartment of Linguistics, pages 292?309,Ithaca.Siegel, Sidney and John Castellan.
1988.Nonparametric Statistics for the BehavioralScience.
McGraw-Hill, New York.Simons, Peter.
1987.
Parts.
A Studyin Ontology.
Clarendon Press, Oxford.Simons, Peter.
1991.
Part/whole II:Mereology since 1900.
In H. Burkhardtand B. Smith, editors, Handbook ofMetaphysics and Ontology.
Philosophia,Munich, pages 672?675.Spa?rck Jones, K. 1983.
Compound nouninterpretation problems.
In F. Fallside andW.
A.
Woods, editors, Computer SpeechProcessing.
Prentice-Hall, Englewood Cliffs,NJ, pages 363?380.Tatu, Marta and Dan Moldovan.
2005.A semantic approach to recognizingtextual entailmant.
In Proceedings ofHuman Language Technology Conferenceand Conference on Empirical Methods inNatural Language Processing (HLT/EMNLP2005), pages 371?378, Vancouver,BC, Canada.Thompson, Cynthia A., Roger Levy, andChristopher Manning.
2003.
A generativemodel for Framenet semantic rolelabeling.
In Proceedings of the 14th134Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole RelationsEuropean Conference on Machine Learning(ECML 2003), pages 397?408,Cavtat-Dubrovnik, Croatia.Vanderwende, Lucy.
1994.
Algorithm forautomatic interpretation of nounsequences.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics (COLING 1994), pages 782?788,Kyoto, Japan.Vanderwende, Lucy.
1995.
The Analysis ofNoun Sequences using SemanticInformation Extracted from On-LineDictionaries.
Ph.D. thesis, GeorgetownUniversity.Winston, Morton, Roger Chaffin, andDouglas Hermann.
1987.
A taxonomyof part-whole relations.
Cognitive Science,11(4):417?444.135
