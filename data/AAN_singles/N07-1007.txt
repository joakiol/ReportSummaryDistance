Proceedings of NAACL HLT 2007, pages 49?56,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsGenerating Case Markers in Machine TranslationKristina Toutanova  Hisami SuzukiMicrosoft ResearchOne Microsoft Way, Redmond WA 98052 USA{hisamis,kristout}@microsoft.comAbstractWe study the use of rich syntax-basedstatistical models for generating gram-matical case for the purpose of machinetranslation from a language which doesnot indicate case explicitly (English) to alanguage with a rich system of surfacecase markers (Japanese).
We propose anextension of n-best re-ranking as amethod of integrating such models into astatistical MT system and show that thismethod substantially outperforms stan-dard n-best re-ranking.
Our best perform-ing model achieves a statistically signifi-cant improvement over the baseline MTsystem according to the BLEU metric.Human evaluation also confirms the re-sults.1 IntroductionGeneration of grammatical elements such as in-flectional endings and case markers is an impor-tant component technology for machine transla-tion (MT).
Statistical machine translation (SMT)systems, however, have not yet successfully in-corporated components that generate grammati-cal elements in the target language.
Most state-of-the-art SMT systems treat grammatical ele-ments in exactly the same way as content words,and rely on general-purpose phrasal translationsand target language models to generate these ele-ments (e.g., Och and Ney, 2002; Koehn et al,2003; Quirk et al, 2005; Chiang, 2005; Galley etal., 2006).
However, since these grammaticalelements in the target language often correspondto long-range dependencies and/or do not haveany words corresponding in the source, they maybe difficult to model, and the output of an SMTsystem is often ungrammatical.For example, Figure 1 shows an output fromour baseline English-to-Japanese SMT system ona sentence from a computer domain.
The SMTsystem, trained on this domain, produces a natu-ral lexical translation for the English word patchas correction program, and translates replaceinto passive voice, which is more appropriate inJapanese.
1  However, there is a problem in thecase marker assignment: the accusative markerwo, which was output by the SMT system, iscompletely inappropriate when the main verb ispassive.
This type of mistake in case marker as-signment is by no means isolated in our SMTsystem: a manual analysis showed that 16 out of100 translations had mistakes solely in the as-signment of case markers.
A better model of caseassignment could therefore improve the qualityof an SMT system significantly.S: The patch replaces the .dll file.O: ????????.dll?????????????
?shuusei puroguramu-wo    .dll fairu-ga   okikae-raremasucorrection program-ACC dll file-NOM replace-PASSC: ????????.dll?????????????
?shuusei puroguramu-de    .dll fairu-ga   okikae-raremasucorrection program-with dll file-NOM replace-PASSFigure 1: Example of SMT (S: source; O: output ofMT; C: correct translation)In this paper, we explore the use of a statisti-cal model for case marker generation in  English-to-Japanese SMT.
Though we focus on the gen-eration of case markers in this paper, there aremany other surface grammatical phenomena thatcan be modeled in a similar way, so any SMTsystem dealing with morpho-syntactically diver-gent language pairs may benefit from a similarapproach to modeling grammatical elements.
Ourmodel uses a rich set of syntactic features of boththe source (English) and the target (Japanese)sentences, using context which is broader thanthat utilized by existing SMT systems.
We showthat the use of such features results in very highcase assignment quality and also leads to a nota-ble improvement in MT quality.Previous work has discussed the building ofspecial-purpose classifiers which generate gram-matical elements such as prepositions (Haji?
et al2002), determiners (Knight and Chander, 1994)and case markers (Suzuki and Toutanova, 2006)with an eye toward improving MT output.
How-1There is a strong tendency to avoid transitive sentenceswith an inanimate subject in Japanese.49ever, these components have not actually beenintegrated in an MT system.
To our knowledge,this is the first work to integrate a grammaticalelement production model in an SMT system andto evaluate its impact in the context of end-to-end MT.A common approach of integrating new mod-els with a statistical MT system is to add them asnew feature functions which are used in decod-ing or in models which re-rank n-best lists fromthe MT system (Och et al, 2004).
In this paperwe propose an extension of the n-best re-rankingapproach, where we expand n-best candidate listswith multiple case assignment variations, anddefine new feature functions on this expandedcandidate set.
We show that expanding the n-bestlists significantly outperforms standard n-best re-ranking.
We also show that integrating our caseprediction model improves the quality of transla-tion according to BLEU (Papineni et al, 2002)and human evaluation.2 BackgroundIn this section, we provide necessary backgroundof the current work.2.1 Task of case marker predictionOur definition of the case marker prediction taskfollows Suzuki and Toutanova (2006).
That is,we assume that we are given a source Englishsentence, and its translation in Japanese whichdoes not include case markers.
Our task is to pre-dict all case markers in the Japanese sentence.We determine the location of case marker in-sertion using the notion of bunsetsu.
A bunsetsuconsists of one content (head) word followed byany number of function words.
We can thereforesegment any sentence into a sequence of bun-setsu by using a part-of-speech (POS) tagger.Once a sentence is segmented into bunsetsu, itis trivial to determine the location of case mark-ers in a sentence: each bunsetsu can have at mostone case marker, and the position of the casemaker within a phrase is predictable, i.e., therightmost position before any punctuation marks.The sentence in Figure 1 thus has the followingbunsetsu analysis (denoted by square brackets),with the locations of potential case marker inser-tion indicated by ?:[??'correction'?][?????'program'?][.dll?][????'file'?][????????'replace-PASS'??
]For each of these positions, our task is to predictthe case marker or to predict NONE, which meansthat the phrase does not have a case marker.The case markers we used for the predictiontask are the same as those defined in Suzuki andToutatnova (2006), and are summarized in Table1: in addition to the case markers in a strict sense,the topic marker wa is also included as well asthe combination of a case marker plus the topicmarker for the case markers with the column+wa checked in the table.
In total, there are 18case markers to predict: ten simple case markers,the topic marker wa, and seven case+wa combi-nations.
The case prediction task is therefore a19-fold classification task: for each phrase, weassign one of the 18 case markers or NONE.2.2 Treelet translation systemWe constructed and evaluated our case predic-tion model in the context of a treelet-based trans-lation system, described in Quirk et al (2005).2In this approach, translation is guided by treelettranslation pairs, where a treelet is a connectedsubgraph of a dependency tree.A sentence is translated in the treelet systemas follows.
The input sentence is first parsed intoa dependency structure, which is then partitionedinto treelets, assuming a uniform probability dis-tribution over all partitions.
Each source treelet isthen matched to a treelet translation pair, the col-lection of which will form the target translation.The target language treelets are then joined toform a single tree, and the ordering of all thenodes is determined, using the method describedin Quirk et al (2005).Translations are scored according to a linearcombination of feature functions:( ) ( )j jjscore t f t?= ?
(1)2Though this paper reports results in the context of a treeletsystem, the model is also applicable to other syntax-basedor phrase-based SMT systems.case markers grammatical functions +wa?ga subject; object?
wo object; path?no genitive; subject?ni dative object, location??
?kara source?
?to quotative, reciprocal, as?
?de location,instrument, cause??
e goal, direction ??
?made goal (up to, until)??
?yori source, comparison target?
?wa TopicTable 1.
Case markers to be predicted50where j are the model parameters and fj(t) is thevalue of the feature function j on the candidate t.There are ten feature functions in the treelet sys-tem, including log-probabilities according to in-verted and direct channel models estimated byrelative frequency, lexical weighting channelmodels following Vogel et al (2003), a trigramtarget language model, an order model, wordcount, phrase count, average phrase size func-tions, and whole-sentence IBM Model 1 log-probabilities in both directions (Och et al 2004).The weights of these models are determined us-ing the max-BLEU method described in Och(2003).
As we describe in Section 4, the caseprediction model is integrated into the system asan additional feature function.The treelet translation model is estimated us-ing a parallel corpus.
First, the corpus is word-aligned using GIZA++ (Och and Ney, 2000);then the source sentences are parsed into a de-pendency structure, and the dependency is pro-jected onto the target side following the heuris-tics described in Quirk et al (2005).
Figure 2shows an example of an aligned sentence pair: onthe source (English) side, POS tags and worddependency structure are assigned (solid arcs);the word alignments between English and Japa-nese words are indicated by the dotted lines.
Onthe target (Japanese) side, projected word de-pendencies (solid arcs) are available.
Additionalannotations in Figure 2, namely the POS tags andthe bunsetsu dependency structure (bold arcs) onthe target side, are derived from the treelet sys-tem to be used for building a case predictionmodel, which we describe in Section 3.2.3 DataAll experiments reported in this paper are runusing parallel data from a technical (computer)domain.
We used two main data sets: train-500K,consisting of 500K sentence pairs which we usedfor training the baseline treelet system as well asthe case prediction model, and a disjoint set ofthree data sets, lambda-1K, dev-1K and test-2K,which are used to integrate and evaluate the caseprediction model in an end-to-end MT scenario.Some characteristics of these data sets are givenin Table 2.
We will refer to this table as we de-scribe our experiments in later sections.# sentpairs# of words(average sent length in words)data setEnglish Japanesetrain-500K 500K 7,909,198(15.81)9,379,240(18.75)lambda-1K 1,000 15,219(15.2) 20,660 (20.7)dev-1K 1,000 15,397(15.4) 21,280 (21.3)test-2K 2,000 30,198(15.1) 41,269 (20.6)Table 2: Data set characteristics3 Statistical Models for Case Predictionin MT3.1 Case prediction modelOur model of case marker prediction closely fol-lows our previous work of case prediction in anon-MT context (Suzuki and Toutanova, 2006).The model is a multi-class log-linear (maximumentropy) classifier using 19 classes (18 casemarkers and NONE).
It assigns a probability dis-tribution over case marker assignments given asource English sentence, all non-case markerwords of a candidate Japanese translation, andadditional annotation information.
Let t denote aJapanese translation, s a corresponding sourcesentence, and A additional annotation informa-tion such as alignment, dependency structure,and POS tags (such as shown in Figure 2).
Letrest(t) denote the sequence of words in t exclud-ing all case markers, and case(t) a case markingassignment for all phrases in t. Our case markingmodel estimates the probability of a case as-signment given all other information:),),(|)(( AstresttcasePcaseThe probability of a complete case assignment isa product over all phrases of the probability ofthe case marker of the phrase given all contextfeatures used by the model.
Our model assumesthat the case markers in a sentence are independ-ent of each other given the input features.
Thisindependence assumption may seem strong, butthe results presented in our previous work (Su-zuki and Toutanova, 2006) showed that a jointmodel did not result in large improvements overa local one in predicting case markers in a non-MT context.Figure 2.
Aligned English-Japanese sentence pair513.2 Model features and feature selectionThe features of our model are similar to the onesdescribed in Suzuki and Toutanova (2006).
Themain difference is that in the current model weapplied a feature selection and induction algo-rithm to determine the most useful features andfeature combinations.
This is important for un-derstanding what sources of information are im-portant for predicting grammatical elements, butare currently absent from SMT systems.
Weused 490K sentence pairs for training the caseprediction model, which is a subset of the train-500K set of Table 2.
We divided the remaining10K sentences for feature selection (5K-feat) andfor evaluating the case prediction models on ref-erence translations (5K-test, discussed in Section3.3).
The paired data is annotated using thetreelet translation system: as shown in Figure 2,we have source and target word dependencystructure, source language POS and word align-ment directly from the aligned treelet structure.Additionally, we used a POS tagger of Japaneseto assign POS to the target sentence as well as toparse the sentence into bunsetsu (indicated bybrackets in Figure 2), using the method describedin Section 2.1.
We then compute bunsetsu de-pendency structure on the target side (indicatedby bold arcs in Figure 2) based on the word de-pendency structure projected from English.
Weapply this procedure to annotate a paired corpus(in which case the Japanese sentence is a refer-ence translation) as well as translations generatedby the SMT system (which may potentially beill-formed).We derived a large set of possible featuresfrom these annotations.
The features are repre-sented as feature templates, such as "HeadwordPOS=X", which generate a set of binary featurescorresponding to different instantiations of thetemplate, such as "Headword POS=NOUN".
Weapplied an automatic feature selection and induc-tion algorithm to the base set of templates.The feature selection algorithm considers theoriginal templates as well as arbitrary (bigramand trigram) conjunctions of these templates.The algorithm performs forward stepwise featureselection, choosing templates which result in thehighest increase in model accuracy on the 5K-feat set mentioned above.
The algorithm is simi-lar to the one described in McCallum (2003).The application of this feature selection pro-cedure gave us 17 templates, some of which areshown in Table 3, along with example instantia-tions for the phrase headed by saabisu ?service?from Figure 2.
Conjunctions are indicated by &.Note that many features that refer to POS andsyntactic (parent) information are selected, onboth the target and source sides.
We also notethat the context required by these features ismore extensive than what is usually availableduring decoding in an SMT system due to a limitimposed on the treelet or phrase size.
For exam-ple, our model uses word lemma and POS tags ofup to six words (previous word, next word, wordin position +2, head word, previous head wordand parent word), which covers more contextthan the treelet system we used (the system im-poses the treelet size limit of four words).
Thismeans that the case model can make use of muchricher information from both the source and tar-get than the baseline MT system.
Furthermore,our model makes better use of the context bycombining the contributions of multiple sourcesof knowledge using a maximum entropy model,rather than using the relative frequency estimateswith a very limited amount of smoothing, whichare used by most state-of-the art SMT systems.3.3 Performance on reference translationsBefore discussing the integration of the case pre-diction model with the MT system, we present anevaluation of the model on the task of predictingthe case assignment of reference translations.This performance constitutes an upper bound onthe model?s performance in MT, because in ref-erence translations, the word choice and the wordorder are perfect.Table 4 summarizes the results of the refer-ence experiments on the 5K-test set using twometrics: accuracy, which denotes the percentageof phrases for which the respective modelguessed the case marker correctly, and BLEUscore against the reference translation.
For com-Features ExampleWords in position  ?1 and +2 kono,moodoHeadword & previous headword saabisu&konoParent word kaishiAligned word  serviceParent of word aligned to headword startedNext word POS NOUNNext word & next word POS seefu&NNHeadword POS NOUNParent headword POS VNAligned to parent word POS & next wordPOS & prev word POSVERB&NN&andParent POS of word aligned to headword VERBAligned word POS & headword POS &prev word POSNN&NN&ADNPOS of word aligned to headword NOUNTable 3: Features for the case prediction model52parison, we also include results from two base-lines: a frequency-based baseline, which alwaysassigns the most likely class (NONE), and a lan-guage model (LM) baseline, which is one of thestandard methods of generating grammaticalelements in MT.
We trained a word-trigram LMusing the CMU toolkit (Clarkson and Rosenfeld,1997) on the same 490K sentences which weused for training the case prediction model.Table 4 shows that our model performs sub-stantially better than both baselines: the accuracyof the frequency-based baseline is 59%, and anLM-based model improves it to 87.2%.
In con-trast, our model achieves an accuracy of 95%,which is a 60% error reduction over the LMbaseline.
It is also interesting to note that as theaccuracy goes up, so does the BLEU score.These results show that our best model canvery effectively predict case markers when theinput to the model is clean, i.e., when the inputhas correct words in correct order.
Next, we seethe impact of applying this model to improve MToutput.4 Integrating Case Prediction Models inMTIn the end-to-end MT scenario, we integrate ourcase assignment model with the SMT system andevaluate its contribution to the final MT output.As a method of integration with the MT sys-tem, we chose an n-best re-ranking approach,where the baseline MT system is left unchangedand additional models are integrated in the formof feature functions via re-ranking of n-best listsfrom the system.
Such an approach has beentaken by Och et al (2004) for integrating sophis-ticated syntax-informed models in a phrase-based SMT system.
We also chose this approachfor ease of implementation: as discussed in Sec-tion 3.2, the features we use in our case modelextend over long distance, and are not readilyavailable during decoding.
Though a tighter inte-gration with the decoding process is certainlyworth exploring in the future, we have taken anapproach here that allows fast experimentation.Within the space of n-best re-ranking, wehave considered two variations: the standard n-best re-ranking method, and our significantlybetter performing extension.
These are now dis-cussed in turn.4.1 Method 1: Standard n-best re-rankingThis method is a straightforward application ofthe n-best re-ranking approach described in Ochet al (2004).
As described in Section 2.2, ourbaseline SMT system is a linear model whichweighs the values of ten feature functions.
Tointegrate a case prediction model, we simply addit to the linear model as an 11th feature function,whose value is the log-probability of the caseassignment of the candidate hypothesis t accord-ing to our model.
The weights of all feature func-tions are then re-estimated using max-BLEUtraining on the n-best list of the lambda-1K set inTable 2.
As we show in Section 5, this re-rankingmethod did not result in good performance.4.2 Method 2: Re-ranking of expandedcandidate listsA drawback of the previous method is that in ann-best list, there may not be sufficiently manycase assignment variations of existing hypothe-ses.
If this is the case, the model cannot be effec-tive in choosing a hypothesis with a good caseassignment.
We performed a simple experimentto test this.
We took the first (best) hypothesis tfrom the MT system and generated the top 40case variations t?
of t, according to the case as-signment model.
These variations differ from tonly in their case markers.
We wanted to seewhat fraction of these new hypotheses t?
oc-curred in a 1000-best list of the MT system.
Inthe dev-1K set of Table 2, the fraction of newcase variations of the first hypothesis occurringin the 1000-best list of hypotheses was 0.023.This means that only less than one (2.3% of 40 =0.92) case variant of the first hypothesis is ex-pected to be found in the 1000-best list, indicat-ing that even an n-best list for a reasonably largen (such as 1000) does not contain enough candi-dates varying in case marker assignment.In order to allow more case marking candi-dates to be considered, we propose the followingmethod to expand the candidate translation list:for each translation t in the n-best list of the base-line SMT system, we also consider case assign-ment variations of t. For simplicity, we chose toconsider the top k case assignment variations ofeach hypothesis according to our case model,3for 1 ?
k ?
40.43From a computational standpoint, it is non-trivial to con-Model ACC BLEUBaseline (frequency) 58.9 40.0Baseline (490K LM) 87.2 83.6Log-linear model 94.9 93.0Table 4: Accuracy (%) and BLEU score for caseprediction when given correct context (referencetranslations) on the 5K-test set53After we expand the translation candidate set,we compute feature functions for all candidatesand train a linear model which chooses from thislarger set.
While some features (e.g., word countfeature) are easy to recompute for a new candi-date, other features (e.g., treelet phrase transla-tion probability) are difficult to recompute.
Wehave chosen to recompute only four features ofthe baseline model:  the language model feature,the word count feature, and the direct and reversewhole-sentence IBM Model 1 features,  assum-ing that the values of the other baseline modelfeatures for a casing variation t?
of t are the sameas their values for t. In addition, we added thefollowing four feature functions, specificallymeant to capture the extent to which the newlygenerated case marking variations differ from theoriginal baseline system hypotheses they are de-rived from: Generated: a binary feature with a value of 0for original baseline system candidates, and avalue of 1 for newly generated candidates. Number NONE?non-NONE: the count of casemarkers changed from NONE to non-NONEwith respect to an original translation candi-date. Number non-NONE?NONE: the count of casemarkers changed from non-NONE to NONE. Number non-NONE?non-NONE: the count ofcase markers changed from non-NONE to an-other non-NONE case marker.Note that these newly defined features all have avalue of 0 for original baseline system candidates(i.e., when k=0) and therefore would have noeffect in Method 1.
Therefore, the only differ-ence between our two methods of integration isthe presence or absence of case-expanded candi-date translations.5 Experiments and Results5.1 Data and settingsFor our end-to-end MT experiments, we usedthree datasets in Table 2 that are disjoint fromthe train-500K data set.
They consist of sourceEnglish sentences and their top 1000 candidatetranslations produced by the baseline SMT sys-sider all possible case assignment variations of a hypothesis:even though the case assignment score for a sentence islocally decomposable, there are still global dependencies inthe linear model from Equation (1) due to the reversewhole-sentence IBM model 1 score used as a feature func-tion.4Our results indicate that additional case variations wouldnot be helpful.tem.
These datasets are the lambda-1K set fortraining the weights   of the linear model fromEquation (1), the dev-1K set for model selection,and the test-2K set for final testing includinghuman evaluation.5.2 ResultsThe results for the end-to-end experiments on thedev-1K set are summarized in Table 5.
The tableis divided into four sections.
The first section(row) shows the BLEU score of the baselineSMT system, which is equivalent to the 1-bestre-ranking scenario with no case expansion.
TheBLEU score for the baseline was 37.99.
In thetable, we also show the oracle BLEU scores foreach model, which are computed by greedily se-lecting the translation in the candidate list withthe highest BLEU score.5The second section of Table 5 corresponds tothe results obtained by Method 1, i.e., the stan-dard n-best re-ranking, for n = 20, 100, and 1000.Even though the oracle scores improve as n isincreased, the actual performance improves onlyslightly.
These results show that the strategy ofonly including the new information as features ina standard n-best re-ranking scenario does notlead to an improvement over the baseline.In contrast, Method 2 obtains notable im-provements over the baseline.
Recall that we ex-pand the n-best SMT candidates with their k-bestcase marking variations in this method, and re-5A modified version of BLEU was used to compute sen-tence-level BLEU in order to select the best hypothesis persentence.
The table shows corpus-level BLEU on the result-ing set of translations.Models #MThypotheses#caseexpan-sionsBLEU OracleBLEUBaseline 1 0 37.99 37.9920 0 37.83 41.79Method 1 100 0 38.02 42.791000 0 38.08 43.141 1 38.18 38.75Method 2 1 10 38.42 40.511 20 38.54 41.151 40 38.41 41.7420 10 38.91 45.3220 20 38.72 45.94Method 2 20 40 38.78 46.56100 10 38.73 46.87100 20 38.64 47.47100 40 38.74 47.96Table 5.
Results of end-to-end experiments on thedev-1K set54train the model parameters on the resulting can-didate lists.
For the values n=1 and k=1 (whichwe refer to as 1best-1case), we observe a smallBLEU gain of .19 over the baseline.
Even thoughthis is not a big improvement, it is still betterthan the improvement of standard n-best re-ranking with a 1000-best list.
By consideringmore case marker variations (k = 10, 20 and 40),we are able to gain about a half BLEU point overthe baseline.
The fact that using more case varia-tions performs better than using only the bestcase assignment candidate proposed by the casemodel suggests that the proposed approach,which integrates the case prediction model as afeature function and retrains the weights of thelinear model, works better than using the caseprediction model as a post-processor of the MToutput.The last section of the table explores combi-nations of the values for n and k. Considering 20best SMT candidates and their top 10 case varia-tions gave the highest BLEU score on the dev-1K set of 38.91, which is an 0.92 BLEU pointsimprovement over the baseline.
Consideringmore case variations (20 or 40), and more SMTcandidates (100) resulted in a similar but slightlylower performance in BLEU.
This is presumablybecause the case model does affect the choice ofcontent words as well, but this influence is lim-ited and can be best captured when using a smallnumber (n=20) of baseline system candidates.Based on these results on the dev-1K set, wechose the best model (i.e., 20-best-10case) andevaluated it on the test-2K set against the base-line.
Using the pair-wise statistical test designdescribed in Collins et al (2005), the BLEU im-provement (35.53 vs. 36.29) was statisticallysignificant (p < .01) according to the Wilcoxonsigned-rank test.5.3 Human evaluationThese results demonstrate that the proposedmodel is effective at improving the translationquality according to the BLEU score.
In this sec-tion, we report the results of human evaluation toensure that the improvements in BLEU lead tobetter translations according to human evaluators.We performed human evaluation on the20best-10case (n=20, k=10) and 1best-40case(n=1, k=40) models against the baseline usingour final test set, the test-2K data.
The perform-ance in BLEU of these models on the full test-2Kdata was 35.53 for the baseline, 36.09 for the1best-40case model, and 36.29 for the 20best-10case model, respectively.In our human evaluation, two annotators wereasked to evaluate a random set of 100 sentencesfor which the models being compared produceddifferent translations.
The judges were asked tocompare two translations, the baseline outputfrom the original SMT system and the outputchosen by the system augmented with the casemarker generation component.
Each judge wasasked to run two separate evaluations along dif-ferent evaluation criteria.
In the evaluation offluency, the judges were asked to decide whichtranslation is more readable/grammatical, ignor-ing the reference translation.
In the evaluation ofadequacy, they were asked to judge which trans-lation more correctly reflects the meaning of thereference translation.
In either setting, they werenot given the source sentence.Table 6 summarizes the results of the evalua-tion of the 20best-10case model.
The table showsthe results along two evaluation criteria sepa-rately, fluency on the left and adequacy on theright.
The evaluation results of Annotator #1 areshown in the columns, while those of Annotator#2 are in the rows.
Each grid in the table showsthe number of sentences the annotators classifiedas the proposed system output better (S), thebaseline system better (B) or the translations areof equal quality (E).
Along the diagonal (in bold-face) are the judgments that were agreed on bythe two annotators: both annotators judged theoutput of the proposed system to be more fluentin 27 translations, less fluent in 9 translations;they judged that our system output was moreadequate in 17 translations and less adequate in 9translations.
Our system output was thus judgedbetter under both criteria, though according to asign test, the improvement is statistically signifi-cant (p < .01) in fluency, but not in adequacy.One of the reasons for this inconclusive resultis that human evaluation may be very difficultand can be unreliable when evaluating very dif-ferent translation candidates, which happens of-ten when comparing the results of models thatconsider n-best candidates where n>1, as is thecase with the 20best-10case model.
In Table 6,Fluency AdequacyAnnotator #1 Annotator #1S B E S B ES 27 1 8 17 0 9B 1 9 16 0 9 12Anno-tator#2 E 7 4 27 9 8 36Table 6.
Results of human evaluation comparing20best-10case vs. baseline.
S: proposed system is bet-ter; B: baseline is better; E: of equal quality55we can see that the raw agreement rate betweenthe two annotators (i.e., number of agreed judg-ments over all judgments) is only 63% (27+9+27/100) in fluency and 62% (17+9+36/100) in ade-quacy.
We therefore performed an additionalhuman evaluation where translations being com-pared differ only in case markers: the baseline vs.the 1best-40case model output.
The results areshown in Table 7.This evaluation has a higher rate of agreement,74% for fluency and 71% for adequacy, indicat-ing that comparing two translations that differonly minimally (i.e., in case markers) is morereliable.
The improvements achieved by ourmodel are statistically significant in both fluencyand adequacy according to a sign test; in particu-lar, it is remarkable that on 42 sentences, thejudges agreed that our system was better in flu-ency, and there were no sentences on which thejudges agreed that our system caused degradation.This means that the proposed system, whenchoosing among candidates differing only in casemarkers, can improve the quality of MT outputin an extremely precise manner, i.e.
making im-provements without causing degradations.6 ConclusionWe have described a method of using a casemarker generation model to improve the qualityof English-to-Japanese MT output.
We haveshown that the use of such a model contributes toimproving MT output, both in BLEU and humanevaluation.
We have also proposed an extensionof n-best re-ranking which significantly outper-formed standard n-best re-ranking.
This methodshould be generally applicable to integratingmodels which target specific phenomena intranslation, and for which an extremely large n-best list would be needed to cover enough vari-ants of the phenomena in question.Our model improves the quality of generatedcase markers in an extremely precise manner.We believe this result is significant, as there aremany phenomena in the target language of MTthat may be improved by using special-purposemodels, including the generation of articles, aux-iliaries, inflection and agreement.
We plan toextend and generalize the current approach tocover these phenomena in morphologically com-plex languages in general in the future.ReferencesClarkson, P.R.
and R. Rosenfeld.
1997.
StatisticalLanguage Modeling Using the CMU-CambridgeToolkit.
In ESCA Eurospeech, pp.
2007-2010.Collins, M., P. Koehn and I. Ku?erov?.
2005.
ClauseRestructuring for Statistical Machine Translation.In ACL, pp.531-540.Chiang, D. 2005.
A Hierarchical Phrase-based Modelfor Statistical Machine Translation.
In ACL.Galley, M., J. Graehl, K. Knight, D. Marcu, S.DeNeefe, W. Wang and I. Thayer.
2006.
ScalableInference and Training of Context-Rich SyntacticTranslation Models.
In ACL.Koehn, P., F. J. Och and D. Marcu.
2003.
StatisticalPhrase-based Translation.
In HLT-NAACL.Haji?, J., M. ?mejrek, B. Dorr, Y. Ding, J. Eisner, D.Gildea, T. Koo, K. Parton, G. Penn, D. Radev andO.
Rambow.
2002.
Natural Language Generationin the Context of Machine Translation.
Technicalreport, Center for Language and Speech Process-ing, Johns Hopkins University 2002 Summer Work-shop Final Report.Knight, K. and I. Chander.
1994.
Automatic Postedit-ing of Documents.
In AAAI.McCallum, A.
2003.
Efficiently inducing features ofconditional random fields.
In UAI.Och, F. J.
2003.
Minimum Error-rate Training forStatistical Machine Translation.
In ACL.Och, F. J. and H. Ney.
2000.
Improved StatisticalAlignment Models.
In ACL.Och, F. J. and H. Ney.
2002.
Discriminative Trainingand Maximum Entropy Models for Statistical Ma-chine Translation.
In ACL 2002.Och, F. J., D. Gildea, S. Khudanpur, A. Sarkar, K.Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith,K.
Eng, V. Jain, Z. Jin and D. Radev.
2004.
ASmorgasbord of Features for Statistical MachineTranslation.
In NAACL.Papineni, K., S. Roukos, T. Ward and W.J.
Zhu.
2002.BLEU: A Method for Automatic Evaluation ofMachine Translation.
In ACL.Quirk, C., A. Menezes and C. Cherry.
2005.
Depend-ency Tree Translation: Syntactically InformedPhrasal SMT.
In ACL.Suzuki, H. and K. Toutanova.
2006.
Learning to Pre-dict Case Markers in Japanese.
In ACL-COLING.Vogel, S., Y. Zhang, F. Huang, A. Tribble, A.Venugopal, B. Zhao and A. Waibel.
2003.
TheCMU Statistical Machine Translation System.
InProceedings of the MT Summit.Fluency AdequacyAnnotator #1 Annotator #1S B E S B ES 42 0 9 30 1 9B 1 0 7 0 9 7Anno-tator#2 E 7 2 32 9 3 32Table 7.
Results of human evaluation comparing1best-40case vs. baseline56
