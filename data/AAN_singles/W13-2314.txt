Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 112?121,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsAnnotating Anaphoric Shell Nouns with their AntecedentsVarada KolhatkarDepartment of Computer ScienceUniversity of Torontovarada@cs.toronto.eduHeike ZinsmeisterInstitut fu?r Maschinelle SprachverarbeitungUniversita?t Stuttgartzinsmeis@ims.stuttgart-uni.deGraeme HirstDepartment of Computer ScienceUniversity of Torontogh@cs.toronto.eduAbstractAnaphoric shell nouns such as this is-sue and this fact conceptually encapsulatecomplex pieces of information (Schmid,2000).
We examine the feasibility of anno-tating such anaphoric nouns using crowd-sourcing.
In particular, we present ourmethodology for reliably annotating an-tecedents of such anaphoric nouns and thechallenges we faced in doing so.
We alsoevaluated the quality of crowd annotationusing experts.
The results suggest thatmost of the crowd annotations were goodenough to use as training data for resolv-ing such anaphoric nouns.1 IntroductionAnaphoric shell nouns (ASNs) such as this fact,this possibility, and this issue are common in allkinds of text.
They are called shell nouns be-cause they provide nominal conceptual shells forcomplex chunks of information representing ab-stract concepts such as fact, proposition, and event(Schmid, 2000).
An example is shown in (1).
(1) Despite decades of education and widespread courseofferings, the survival rate for out-of-hospital car-diac arrest remains a dismal 6 percent or lessworldwide.This fact prompted the American Heart Associationlast November to simplify the steps of CPR to makeit easier for lay people to remember and to encour-age even those who have not been formally trainedto try it when needed.Here, the ASN this fact encapsulates the clausemarked in bold from the preceding paragraph.ASNs play an important role in organizing a dis-course.
First, they are used metadiscursively totalk about the current discourse.
In (1), the au-thor characterizes the information presented in thecontext by referring to it as a fact ?
a thing thatis indisputably the case.
Second, they are used ascohesive devices in a discourse.
In (1), for exam-ple, this fact on the one hand refers to the propo-sition marked in bold, and on the other, faces for-ward and serves as the starting point of the follow-ing paragraph.
Finally, as Schmid (2000) pointsout, like conjunctions so and however, ASNsmay function as topic boundary markers and topicchange markers.Despite their importance, ASNs have not re-ceived much attention in Computational Linguis-tics.
Although there has been some effort to anno-tate certain anaphors with similar properties, i.e.,demonstratives and the pronoun it (Byron, 2003;Artstein and Poesio, 2006), in contrast to ordi-nary nominal anaphora, there are not many anno-tated corpora available that could be used to studyASNs.
Indeed, many questions of annotation ofASNs must still be answered.
For example, theextent to which native speakers themselves agreeon the resolution of such anaphors, i.e., on the pre-cise antecedents, remains unclear.An essential first step in this field of researchis therefore to clearly establish the extent of inter-annotator agreement on antecedents of ASNs asa measure of feasibility of the task.
In this pa-per, we describe our methodology for annotatingASNs using crowdsourcing, a cheap and fast wayof obtaining annotation.
We also describe how weevaluated the feasibility of the task and the qualityof the annotation, and the challenges we faced indoing so, both with regard to the task itself and thecrowdsourcing platform we use.
The results sug-gest that most of the crowd-annotations were goodenough to use as training data for ASN resolution.1122 Related workThere exist only few annotated corpora ofanaphora with non-nominal antecedents (Dipperand Zinsmeister, 2011).
The largest one of these,the ARRAU corpus (Poesio and Artstein, 2008),contains 455 anaphors pointing to non-nominalantecedents, but only a few instances are ASNs.Kolhatkar and Hirst (2012) annotated antecedentsof the same type as we do, but restricted their ef-forts to the ASN this issue.1 In addition, there arecorpora annotated with event anaphora in whichverbal instances are identified as proxies for non-nominal antecedents (Pradhan et al 2007; Chenet al 2011; Lee et al 2012).For the task of identifying non-nominal an-tecedents as free spans of text, there is no standardway of reporting inter-annotator agreement.
Somestudies report only observed percentage agree-ment with results in the range of about 0.40?0.55 (Vieira et al 2002; Dipper and Zinsmeis-ter, 2011).
The studies differed with respect tonumber of annotators, types of anaphors, and lan-guage of the corpora.
Artstein and Poesio (2006)discuss Krippendorff?s alpha for chance-correctedagreement.
They considered antecedent strings asbags of words and computed the degree of differ-ence between them by different distance measures(e.g.
Jaccard, Dice).
The bag-of-words approachis rather optimistic in the sense that even two non-overlapping strings are very likely to share at leasta few words.
Kolhatkar and Hirst (2012) followeda different approach by using Krippendorff?s uni-tizing alpha (u?)
which considers the longest com-mon subsequence of different antecedent options(Krippendorff, 2013).
They reported high chance-corrected u?
of 0.86 for two annotators but in avery restricted domain.There has been some prior effort to annotateanaphora and coreference using Games with aPurpose as a method of crowdsourcing (Chamber-lain et al 2009; Hladka?
et al 2009).
Another, lesstime-consuming approach of crowdsourcing is us-ing platforms such as Amazon Mechanical Turk2.It has been shown that crowdsourced data can suc-cessfully be used as training data for NLP tasks(Hsueh et al 2009).1Another data set reported in the literature could havebeen relevant for us: Botley?s (2006) corpus contained about462 ASN instances signaled by shell nouns; but this data isno longer available (S. Botley, p.c.
).2https://mturk.com/mturk/Class Description Examplesfactual states of affairs fact, reasonlinguistic linguistic acts question, reportmental thoughts and ideas issue, decisionmodal subjective judgements possibility, trutheventive events act, reactioncircumstantial situations situation, wayTable 1: Schmids categorization of shell nouns.The nouns in boldface are used in this research.3 The Anaphoric Shell Noun CorpusOur goal is to obtain annotated data for ASN an-tecedents that could be used to train a supervisedmachine learning system to resolve ASNs.
Forthat, we created the Anaphoric Shell Noun (ASN)corpus.Schmid (2000) provides a list of 670 Englishnouns which are frequently used as shell nouns.He divides them into six broad semantic classes:factual, mental, linguistic, modal, circumstantial,and eventive.
Table 1 shows this classification,along with example shell nouns for each category.To begin with, we considered articles contain-ing occurrences of these 670 shell nouns from theNew York Times (NYT) corpus (about 711,046occurrences).3 To create a corpus of a manage-able size for annotation, we considered first 10highly frequent shell nouns distributed across eachof Schmid?s shell noun categories from Table 1and extracted ASN instances by searching for thepattern {this shell noun} in these articles.4To examine the feasibility of the annotation, wesystematically annotated sample data ourselves,which contained about 15 examples of each ofthese 10 highly frequent shell nouns.
The anno-tation process revealed that not all ASN instancesare easy to resolve.
The instances with shell nounsfrom the circumstantial and eventive categories, inparticular, had very long and unclear antecedents.So we excluded these categories in this researchand work with six shell nouns from the other fourcategories: fact, reason, issue, decision, question,and possibility.
To create the ASN corpus, weextracted about 500 instances for each of thesesix shell nouns.
After removing duplicates andinstances with a non-abstract sense (e.g., this is-3http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2008T194Schmid (2000) provides patterns for anaphoric shellnouns, and this-NP is the most prominent pattern amongthem.113sue with a publication-related sense), we were leftwith 2,822 ASN instances.4 ASN Annotation ChallengesASN antecedent annotation is a complex task, asit involves deeply understanding the discourse andinterpreting it.
Here we point out two main chal-lenges associated with the task.What to annotate?
The question of ?what to an-notate?
as mentioned by Fort et al(2012) is notstraightforward for ASN antecedents, as the no-tion of markables is complex compared to ordi-nary nominal anaphora: the units on which theannotation work should focus are heterogeneous.5Moreover, due to this heterogeneous nature of an-notation units, there is a huge number of mark-ables (e.g., all syntactic constituents given by asyntactic parse tree).
So there are many optionsto choose from, while only a few units are actu-ally to be annotated.
Moreover, there is no one-to-one correspondence between the syntactic typeof an antecedent and the semantic type of its refer-ent (Webber, 1991).
For instance, a semantic typesuch as fact can be expressed with different syn-tactic shapes such as a clause, a verb phrase, or acomplex sentence.
Conversely, a syntactic shape,such as a clause, can function as several semantictypes, including fact, proposition, and event.Lack of the notion of the right answer It is notobvious how to define clear and detailed annota-tion guidelines to create a gold-standard corpusfor ASN antecedent annotation due to our limitedunderstanding of the nature and interpretation ofsuch antecedents.
The notion of the right answeris not well-defined for ASN antecedents.
Indeedmost people will be hard-pressed to say whetheror not to include the clause Despite decades ofeducation and widespread course offerings in theantecedent of this fact in example (1).
The mainchallenge is to identify the conditions when twodifferent candidates for annotation should be con-sidered as representing essentially the same con-cept, which raises deep philosophical issues thatwe do not propose to solve in this paper.
For ourpurposes, we believe, this challenge could onlybe possibly tackled by the requirements of down-stream applications of ASN resolution.5Occasionally, ASN antecedents are non-contiguousspans of text, but in this work, we ignore them for simplicity.5 Annotation MethodologyConsidering the difficulties of ASN annotationdiscussed above, there were two main challengesinvolved in the annotation process: first, to find an-notators who can annotate data reliably with min-imal guidelines, and second, to design simple an-notation tasks that will elicit data useful for ourpurposes.
Now we discuss how we dealt withthese challenges.Crowdsourcing We wanted to examine to whatextent non-expert native speakers of English withminimal annotation guidelines would agree onASN antecedents.
We explored the possibility ofusing crowdsourcing, which is an effective way toobtain annotations for natural language research(Snow et al 2008).
In particular, we explored theuse of CrowdFlower6, a crowdsourcing platformthat in turn uses various worker channels such asAmazon Mechanical Turk.
CrowdFlower offers anumber of features.First, it offers a number of integrated quality-control mechanisms.
For instance, it throws goldquestions randomly at the annotators, and anno-tators who do not answer them correctly are notallowed to continue.
To further minimize spam-mers, it also offers a training phase before the ac-tual annotation.
In this phase, every annotator ispresented with a few gold questions.
Only thoseannotators who get the gold questions right get ad-mittance to do the actual annotation.Second, CrowdFlower chooses a unique answerfor each annotation unit based on the majority voteof the trusted annotators.
For each annotator, itassigns a trust level based on how she performson the gold examples.
The unique answer is com-puted by adding together the trust scores of an-notators, and then picking the answer with thehighest sum of trusts (CrowdFlower team, p.c.
).It also assigns a confidence score (denoted as chenceforth) for each answer, which is a normal-ized score of the summation of the trusts.
For ex-ample, suppose annotators A, B, and C with trustlevels 0.75, 0.75, and 1.0 give answers no, yes, yesrespectively for a particular instance.
Then the an-swer yes will score 1.75 and answer no will score0.75 and yes will be chosen as the crowd?s answerwith c = 0.7 (i.e., 1.75/(1.75 + 0.75)).
We usethese confidence scores in our analysis of inter-annotator agreement below.6http://crowdflower.com/114Finally, CrowdFlower also provides detailed an-notation results including demographic informa-tion and trustworthiness of each annotator.Design of the annotation tasks With the help ofwell-designed gold examples, CrowdFlower canget rid of spammers and ensures that only reliableannotators perform the annotation task.
But theannotation task must be well-designed in the firstplace to get a good quality annotation.
Followingthe claim in the literature that with crowdsourc-ing platforms simple tasks do best (Madnani et al2010; Wang et al 2012), we split our annotationtask into two relatively simple sequential annota-tion tasks.
First, identifying the broad region of theantecedent, i.e., not the precise antecedent but theregion where the antecedent lies, and second, iden-tifying the precise antecedent, given the broad re-gion of the antecedent.
Now we will discuss eachof our annotation tasks in detail.5.1 CrowdFlower experiment 1The first annotation task was about identifying thebroad region of ASN antecedents without actu-ally pinpointing the precise antecedents.
We de-fined the broad region as the sentence containingthe ASN antecedent, as the shell nouns we havechosen tend to have antecedents that lie withina single sentence.
We designed a CrowdFlowerexperiment where we presented to the annotatorsASNs from the ASN corpus with three precedingparagraphs as context.
Sentences in the vicinityof ASNs were each labelled: four sentences pre-ceding the anaphor, the sentence containing theanaphor, and two sentences following the anaphor.This choice was based on our pilot annotation:the antecedents very rarely occur more than foursentences away from the anaphor.
The annota-tion task was to pinpoint the sentence in the pre-sented text that contained the antecedent for theASN and selecting the appropriate sentence labelas the correct answer.
If no labelled sentence in thepresented text contained the antecedent, we sug-gested to the annotators to select None.
If the an-tecedent spanned more than one sentence, then wesuggested to them to select Combination.
We alsoprovided a link to the complete article from whichthe text was drawn in case the annotators wantedto have a look at it.Settings We asked for 8 judgements per instanceand paid 8 cents per annotation unit.
Our jobcontained in total 2,822 annotation units with 168gold units.
As we were interested in the ver-dict of native speakers of English, we limited theallowed demographic region to English-speakingcountries.5.2 CrowdFlower experiment 2This annotation task was about pinpointing theexact antecedent text of the ASN instances.
Wedesigned a CrowdFlower experiment, where wepresented to the annotators ASN instances fromthe ASN corpus with highlighted ASNs and thesentences containing the antecedents, the outputof experiment 1.
One way to pinpoint the ex-act antecedent string is to ask the annotators tomark free spans of text within the antecedent sen-tence, similar to Byron (2003) and Artstein andPoesio (2006).
However, CrowdFlower quality-control mechanisms require multiple-choice an-notation labels.
So we decided to display a setof labelled candidates to the annotators and askthem to choose the answer that best representsthe ASN antecedent.
A practical requirement ofthis approach is that the number of options to bedisplayed be only a handful in order to make ita feasible task for online annotation.
But as wenoted in Section 4, the number of markables forASN antecedents is large.
If, for example, we de-fine markables as all syntactic constituents givenby the Stanford parser7, there are on average 49.5such candidates per sentence in the ASN corpus.
Itis not practical to display all these candidates andto ask CrowdFlower annotators to choose one an-swer from this many options.
Also, some potentialcandidates are clearly not appropriate candidatesfor a particular shell noun.
For instance, the NPconstituent the survival rate in example (1) is notan appropriate candidate for the shell noun fact asgenerally facts are propositions.
So the question iswhether it is possible to restrict this set of candi-dates by discarding unlikely ones.To deal with this question, we used super-vised machine learning methods trained on easy,non-anaphoric unlabelled examples of shell nouns(e.g., the fact that X).
In this paper, we will focuson the annotation and will treat these methods as ablack box.
In brief, the methods reduce the largesearch space of ASN antecedent candidates to asize that is manageable for crowdsourcing anno-tation, without eliminating the most likely candi-7http://nlp.stanford.edu/software/lex-parser.shtml115dates.
We displayed the 10 most-likely candidatesgiven by these methods.
In addition, we made surenot to display two candidates with only a negli-gible difference.
For example, given two candi-dates, X and that X, which differ only with respectto the introductory that, we chose to display onlythe longer candidate that X.In a controlled annotation, with detailed guide-lines, such difficulties of selecting between minorvariations could be avoided.
However, such de-tailed annotation guidelines still have to be devel-oped.Settings As in experiment 1, we asked for 8judgements per instance and paid 6 cents per anno-tation unit.
But for this experiment we consideredonly 2,323 annotation units with 151 gold units,only high-confidence units (c ?
0.5) from experi-ment 1.
This task turned out to be a suitable taskfor crowdsourcing as it offered a limited numberof options to choose from, instead of asking theannotators to mark arbitrary spans of text.6 AgreementOur annotation tasks pose difficulties in measur-ing inter-annotator agreement both in terms of thetask itself and the platform used for annotation.
Inthis section, we describe our attempt to computeagreement for each of our annotation tasks and thechallenges we faced in doing so.6.1 CrowdFlower experiment 1Recall that in this experiment, annotators identifythe sentence containing the antecedent and selectthe appropriate sentence label as their answer.
Weknow from our pilot annotation that the distribu-tion of such labels is skewed: most of the ASN an-tecedents lie in the sentence preceding the anaphorsentence.
We observed the same trend in the re-sults of this experiment.
In the ASN corpus, thecrowd chose the preceding sentence 64% of thetime, the same sentence 13% of the time, and long-distance sentences 23% of the time.8 Consider-ing the skewed distribution of labels, if we use tra-ditional agreement coefficients, such as Cohen?s?
(1960) or Krippendorff?s ?
(2013), expectedagreement is very high, which in turn results in alow reliability coefficient (in our case ?
= 0.61)that does not necessarily reflect the true reliabilityof the annotation (Artstein and Poesio, 2008).8This confirms Passonneau?s (1989) observation that non-nominal antecedents tend to be close to the anaphors.F R I D Q P allc < .5 8 8 36 21 13 7 16.5?
c < .6 6 6 13 8 7 5 8.6?
c < .8 24 25 31 31 22 27 27.8?
c < 1.
22 23 11 14 19 25 18c = 1.
40 38 9 26 39 36 31Average c .83 .82 .61 .72 .80 .83 .76Table 2: CrowdFlower confidence distribution forCrowdFlower experiment 1.
Each column showsthe distribution in percentages for confidence ofannotating antecedents of that shell noun.
The fi-nal row shows the average confidence of the dis-tribution.
Number of ASN instances = 2,822.
F= fact, R = reason, I = issue, D = decision, Q =question, P = possibility.One way to measure the reliability of the data,without taking chance correction into account, isto consider the distribution of the ASN instanceswith different levels of CrowdFlower confidence.Table 2 shows the percentages of instances in dif-ferent confidence level bands for each shell nounas well as for all instances.
For example, for theshell noun fact, 8% of the total number of this factinstances were annotated with c < 0.5.
As wecan see, most of the instances of the shell nounsfact, reason, question, and possibility were anno-tated with high confidence.
In addition, most ofthem occurred in the band 0.8 ?
c ?
1.
Thereare relatively few instances with low confidencefor these nouns, suggesting the feasibility of re-liable antecedent annotation for these nouns.
Bycontrast, the mental nouns issue and decision hada large number of low-confidence (c < 0.5) in-stances, bringing in the question of reliability ofantecedent annotation of these nouns.Given these results with different confidencelevels, the primary question is what confidencelevel should be considered acceptable?
For ourtask, we required that at least four trusted anno-tators out of eight annotators should agree on ananswer for it to be acceptable.9 We will talk aboutacceptability later in Section 7.6.2 CrowdFlower experiment 2Recall that this experiment was about identifyingthe precise antecedent text segment given the sen-tence containing the antecedent.
It is not clearwhat the best way to measure the amount of such9We chose this threshold after systematically examininginstances with different confidence levels.116Jaccard DiceDo De ?
Do De ?A&P .53 .95 .45 .43 .94 .55Our results .47 .96 .51 .36 .92 .61Table 3: Agreement using Krippendorff?s ?
forCrowdFlower experiment 2.
A&P = Artstein andPoesio (2006).agreement is.
Agreement coefficients such as Co-hen?s ?
underestimate the degree of agreement forsuch annotation, suggesting disagreement even be-tween two very similar annotated units (e.g., twotext segments that differ in just a word or two).We present the agreement results in three differentways: Krippendorff?s ?
with distance metrics Jac-card and Dice (Artstein and Poesio, 2006), Krip-pendorff?s unitizing alpha (Krippendorff, 2013),and CrowdFlower confidence values.Krippendorff?s ?
using Jaccard and Dice Tocompare our agreement results with previous ef-forts to annotate such antecedents, following Art-stein and Poesio (2006), we computed Krippen-dorff?s ?
using distance metrics Jaccard and Dice.The general form of coefficient ?
is:?
= 1?
DoDe(1)where Do and De are observed and expected dis-agreements respectively.
?
= 1 indicates perfectreliability and u?
= 0 indicates the absence of re-liability.
When u?
< 0, either the sample sizeis very small or the disagreement is systematic.Table 3 shows the agreement results.
Our agree-ment results are comparable to Artstein and Poe-sio?s agreement results.
They had 20 annotatorsannotating 16 anaphor instances with segment an-tecedents, whereas we had 8 annotators annotat-ing 2,323 ASN instances.
As Artstein and Poesiopoint out, expected disagreement in case of suchantecedent annotation is close to maximal, as thereis little overlap between segment antecedents ofdifferent anaphors and therefore ?
pretty much re-flects the observed agreement.Krippendorff?s unitizing ?
(u?)
FollowingKolhatkar and Hirst (2012), we use u?
for measur-ing reliability of the ASN antecedent annotationtask.
This coefficient is appropriate when the an-notators work on the same text, identify the unitsin the text that are relevant to the given researchF R I D Q P allc < .5 11 17 32 31 14 28 21.5?
c < .6 12 12 19 23 9 19 15.6?
c < .8 36 33 34 32 30 36 33.8?
c < 1.
24 22 10 10 21 13 18c = 1.
17 16 5 3 26 4 13Average c .74 .71 .60 .59 .77 .62 .68Table 4: CrowdFlower confidence distribution forCrowdFlower experiment 2.
Each column showsthe distribution in percentages for confidence ofannotating antecedents of that shell noun.
The fi-nal row shows the average confidence of the dis-tribution.
Number of ASN instances = 2,323.
F= fact, R = reason, I = issue, D = decision, Q =question, P = possibility.question, and then label the identified units (Krip-pendorff, p.c.).
The general form of coefficientu?
is the same as in equation 1.
In our context,the annotators work on the same text, the ASN in-stances.
We define an elementary annotation unit(the smallest separately judged unit) to be a wordtoken.
The annotators identify and locate ASNantecedents for the given anaphor in terms of se-quences of elementary annotation units.u?
incorporates the notion of distance betweenstrings by using a distance function which is de-fined as the square of the distance between thenon-overlapping tokens in our case.
The distanceis 0 when the annotated units are exactly the same,and is the summation of the squares of the un-matched parts if they are different.
We computeobserved and expected disagreement as explainedby Krippendorff (2013, Section 12.4).
For ourdata, u?
was 0.54.10 u?
was lower for the men-tal nouns issue and decision and the modal nounpossibility compared to other shell nouns.CrowdFlower confidence results We also ex-amined different confidence levels for ASN an-tecedent annotation.
Table 4 gives confidence re-sults for all instances and for each noun.
In con-trast with Table 2, the instances are more evenlydistributed here.
As in experiment 1, the men-tal nouns issue and decision had many low con-fidence instances.
For the modal noun possibility,it was easy to identify the sentence containing theantecedent, but pinpointing the precise antecedent10Note that u?
reported here is just an approximation ofthe actual agreement as in our case the annotators chose anoption from a set of predefined options instead of markingfree spans of text.117turned out to be difficult.Now we discuss the nature of disagreement inASN annotation.Disagreement in experiment 1 There were twoprimary sources of disagreement in experiment 1.First, the annotators had problems agreeing on theanswer None.
We instructed them to choose Nonewhen the sentence containing the antecedent wasnot labelled.
Nonetheless, some annotators chosesentences that did not precisely contain the actualantecedent but just hinted at it.
Second, sometimesit was hard to identify the precise antecedent sen-tence as the antecedent was either present in theblend of all labelled sentences or there were multi-ple possible answers, as shown in example (2).
(2) Any biography of Thomas More has to answer onefundamental question.
Why?
Why, out of all themany ambitious politicians of early Tudor England,did only one refuse to acquiesce to a simple pieceof religious and political opportunism?
What was itabout More that set him apart and doomed him to aspectacularly avoidable execution?The innovation of Peter Ackroyd?s new biography ofMore is that he places the answer to this questionoutside of More himself.Here, the author formulates the question in a num-ber of ways and any question mentioned in thepreceding text can serve as the antecedent of theanaphor this question.Hard instances Low agreement can indicatedifferent problems: unclear guidelines, poor-quality annotators, or difficult instances (e.g., notwell understood linguistic phenomena) (Artsteinand Poesio, 2006).
We can rule out the pos-sibility of poor-quality annotators for two rea-sons.
First, we consider 8 diverse annotatorswho work independently.
Second, we use Crowd-Flower?s quality-control mechanisms and henceallow only trustworthy annotators to annotate ourtexts.
Regarding instructions, we take inter-annotator agreement as a measure for feasibility ofthe task, and hence we keep the annotation instruc-tion as simple as possible.
This could be a sourceof low agreement.
The third possibility is hard in-stances.
Our results show that the mental nounsissue and decision had many low-confidence in-stances, suggesting the difficulty associated withthe interpretation of these nouns (e.g., the veryidea of what counts as an issue is fuzzy).
The shellnoun decision was harder because most of its in-stances were court-decision related articles, whichwere in general hard to understand.Different strings representing similar conceptsAs noted in Section 4, the main challenge with theASN annotation task is that different antecedentcandidates might represent the same concept andit is not trivial to incorporate this idea in the anno-tation process.
When five trusted annotators iden-tify the antecedent as but X and three trusted anno-tators identify it as merely X, since CrowdFlowerwill consider these two answers to be two com-pletely different answers, it will give the answerbut X a confidence of only about 0.6. u?
or ?
withJaccard and Dice will not consider this as a com-plete disagreement; however, the coefficients willregister it as a difference.
In other words, the dif-ference functions used with these coefficients donot respond to semantics, paraphrases, and othersimilarities that humans might judge as inconse-quential.
One way to deal with this problem wouldbe clustering the options that reflect essentially thesame concepts before measuring the agreement.Some of these problems could also be avoided byformulating instructions for marking antecedentsso that these differences do not occur in the iden-tified antecedents.
However, crowdsourcing plat-forms require annotation guidelines to be clear andminimal, which makes it difficult to control the an-notation variations.7 Evaluation of Crowd AnnotationCrowdFlower experiment 2 resulted in 1,810 ASNinstances with c > 0.5.
The question is how goodare these annotations from experts?
point of view.To examine the quality of the crowd annotationwe asked two judges A and B to evaluate the ac-ceptability of the crowd?s answers.
The judgeswere highly-qualified academic editors: A, a re-searcher in Linguistics and B, a translator with aPh.D.
in History and Philosophy of Science.
Fromthe crowd-annotated ASN antecedent data, werandomly selected 300 instances, 50 instances pershell noun.
We made sure to choose instances withborderline confidence (0.5 ?
c < 0.6), mediumconfidence (0.6 ?
c < 0.8), and high confidence(0.8 ?
c ?
1.0).
We asked the judges to ratethe acceptability of the crowd-answers based onthe extent to which they provided interpretation ofthe corresponding anaphor.
We gave them fouroptions: perfectly (the crowd?s answer is perfectand the judge would have chosen the same an-tecedent), reasonably (the crowd?s answer is ac-ceptable and is close to their answer),118Judge BP R I N TotalJudge AP 171 44 11 7 233R 12 27 7 4 50I 2 4 6 1 13N 1 2 0 1 4Total 186 77 24 13 300Table 5: Evaluation of ASN antecedent annota-tion.
P = perfectly, R = reasonably, I = implicitly,N = not at allimplicitly (the crowd?s answer only implicitlycontains the actual antecedent), and not at all (thecrowd?s answer is not in any way related to theactual antecedent).11 Moreover, if they did notmark perfectly, we asked them to provide their an-tecedent string.
The two judges worked on the taskindependently and they were completely unawareof how the annotation data was collected.Table 5 shows the confusion matrix of the rat-ings of the two judges.
Judge B was stricter thanJudge A.
Given the nature of the task, it wasencouraging that most of the crowd-antecedentswere rated as perfectly by both judges (72% byA and 62% by B).
Note that perfectly is rather astrong evaluation for ASN antecedent annotation,considering the nature of ASN antecedents them-selves.
If we weaken the acceptability criteria andconsider the antecedents rated as reasonably to bealso acceptable antecedents, 84.6% of the total in-stances were acceptable according to both judges.Regarding the instances marked implicitly, mostof the times the crowd?s answer was the closesttextual string of the judges?
answer.
So we againmight consider instances marked implicitly as ac-ceptable answers.For a very few instances (only about 5%) eitherof the judges marked not at all.
This was a posi-tive result and suggests success of different stepsof our annotation procedure: identifying broad re-gion, identifying the set of most likely candidates,and identifying precise antecedent.
As we can seein Table 5, there were 7 instances where the judgeA rated perfectly while the judge B rated not at all,i.e., completely contradictory judgements.
Whenwe looked at these examples, they were rather hardand ambiguous cases.
An example is shown in (3).The whether clause marked in the preceding sen-11Before starting the actual annotation, we carried out atraining phase with 30 instances, which gave an opportunityto the judges to ask questions about the task.tence is the crowd?s answer.
One of our judgesrated this answer as perfectly, while the other ratedit as not at all.
According to her the correct an-tecedent is whether Catholics who vote for Mr.Kerry would have to go to confession.
(3) Several Vatican officials said, however, that any suchtalk has little meaning because the church does nottake sides in elections.
But the statements by severalAmerican bishops that Catholics who vote for Mr.Kerry would have to go to confession have raisedthe question in many corners about whether this isan official church position.The church has not addressed this question publiclyand, in fact, seems reluctant to be dragged into thefight...?There was no notable relation between the an-notator?s rating and the confidence level: many in-stances with borderline confidence were markedperfectly or reasonably, suggesting that instanceswith c ?
0.5 were reasonably annotated instances,to be used as training data for ASN resolution.8 ConclusionIn this paper, we addressed the fundamental ques-tion about feasibility of ASN antecedent annota-tion, which is a necessary step before developingcomputational approaches to resolve ASNs.
Wecarried out crowdsourcing experiments to get na-tive speaker judgements on ASN antecedents.
Ourresults show that among 8 diverse annotators whoworked independently with a minimal set of an-notation instructions, usually at least 4 annotatorsconverged on a single ASN antecedent.
The re-sult is quite encouraging considering the nature ofsuch antecedents.We asked two highly-qualified judges to in-dependently examine the quality of a sample ofcrowd-annotated ASN antecedents.
According toboth judges, about 95% of the crowd-annotationswere acceptable.
We plan to use this crowd-annotated data (1,810 instances) as training datafor an ASN resolver.
We also plan to distribute theannotations at a later date.AcknowledgementsWe thank the CrowdFlower team for their respon-siveness and Hans-Jo?rg Schmid for helpful dis-cussions.
This material is based upon work sup-ported by the United States Air Force and the De-fense Advanced Research Projects Agency underContract No.
FA8650-09-C-0179, Ontario/Baden-Wu?rttemberg Faculty Research Exchange, and theUniversity of Toronto.119ReferencesRon Artstein and Massimo Poesio.
2006.
Identify-ing reference to abstract objects in dialogue.
InProceedings of the 10th Workshop on the Semanticsand Pragmatics of Dialogue, pages 56?63, Potsdam,Germany.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596, December.Simon Philip Botley.
2006.
Indirect anaphora: Testingthe limits of corpus-based linguistics.
InternationalJournal of Corpus Linguistics, 11(1):73?112.Donna K. Byron.
2003.
Annotation of pronouns andtheir antecedents: A comparison of two domains.Technical report, University of Rochester.
ComputerScience Department.Jon Chamberlain, Udo Kruschwitz, and Massimo Poe-sio.
2009.
Constructing an anaphorically anno-tated corpus with non-experts: Assessing the qualityof collaborative annotations.
In Proceedings of the2009 Workshop on The People?s Web Meets NLP:Collaboratively Constructed Semantic Resources,pages 57?62, Suntec, Singapore, August.
Associa-tion for Computational Linguistics.Bin Chen, Jian Su, Sinno Jialin Pan, and Chew LimTan.
2011.
A unified event coreference resolu-tion by integrating multiple resolvers.
In Proceed-ings of 5th International Joint Conference on Nat-ural Language Processing, pages 102?110, ChiangMai, Thailand, November.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37.Stefanie Dipper and Heike Zinsmeister.
2011.
Anno-tating abstract anaphora.
Language Resources andEvaluation, 69:1?16.Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.2012.
Modeling the complexity of manual anno-tation tasks: a grid of analysis.
In 24th Inter-national Conference on Computational Linguistics,pages 895?910.Barbora Hladka?, Jir???
M?
?rovsky?, and Pavel Schlesinger.2009.
Play the language: Play coreference.
In Pro-ceedings of the Association of Computational Lin-guistics and International Joint Conference on Nat-ural Language Processing 2009 Conference ShortPapers, pages 209?212, Suntec, Singapore, August.Association for Computational Linguistics.Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.2009.
Data quality from crowdsourcing: A study ofannotation selection criteria.
In Proceedings of theNAACL HLT 2009 Workshop on Active Learning forNatural Language Processing, pages 27?35, Boul-der, Colorado, June.
Association for ComputationalLinguistics.Varada Kolhatkar and Graeme Hirst.
2012.
Resolving?this-issue?
anaphora.
In Proceedings of the 2012Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 1255?1265, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Klaus Krippendorff.
2013.
Content Analysis: AnIntroduction to Its Methodology.
Sage, ThousandOaks, CA, third edition.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entity andevent coreference resolution across documents.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages489?500, Jeju Island, Korea, July.
Association forComputational Linguistics.Nitin Madnani, Jordan Boyd-Graber, and PhilipResnik.
2010.
Measuring transitivity using un-trained annotators.
In Proceedings of the NAACLHLT 2010 Workshop on Creating Speech and Lan-guage Data with Amazon?s Mechanical Turk, pages188?194, Los Angeles, June.
Association for Com-putational Linguistics.Rebecca J. Passonneau.
1989.
Getting at discourse ref-erents.
In Proceedings of the 27th Annual Meetingof the Association for Computational Linguistics,pages 51?59, Vancouver, British Columbia, Canada,June.
Association for Computational Linguistics.Massimo Poesio and Ron Artstein.
2008.
Anaphoricannotation in the ARRAU corpus.
In Proeedingsof the Sixth International Conference on LanguageResources and Evaluation (LREC?08), Marrakech,Morocco, May.
European Language Resources As-sociation (ELRA).Sameer S. Pradhan, Lance A. Ramshaw, Ralph M.Weischedel, Jessica MacBride, and Linnea Micci-ulla.
2007.
Unrestricted coreference: Identifyingentities and events in OntoNotes.
In Proceedings ofthe International Conference on Semantic Comput-ing, pages 446?453, September.Hans-Jo?rg Schmid.
2000.
English Abstract Nouns AsConceptual Shells: From Corpus to Cognition.
Top-ics in English Linguistics 34.
De Gruyter Mouton,Berlin.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Ng.
2008.
Cheap and fast ?
but is itgood?
evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 254?263, Honolulu, Hawaii, Oc-tober.
Association for Computational Linguistics.Renata Vieira, Susanne Salmon-Alt, CarolineGasperin, Emmanuel Schang, and Gabriel Othero.2002.
Coreference and anaphoric relations of120demonstrative noun phrases in multilingual corpus.In Proceedings of the 4th Discourse Anaphora andAnaphor Resolution Conference (DAARC 2002),pages 385?427, Lisbon, Portugal, September.Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan.2012.
Perspectives on crowdsourcing annotationsfor natural language processing.
In Language Re-sources and Evaluation, volume in press, pages 1?23.
Springer.Bonnie Lynn Webber.
1991.
Structure and ostension inthe interpretation of discourse deixis.
In Languageand Cognitive Processes, pages 107?135.121
