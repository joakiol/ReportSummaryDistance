Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 22?31, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsCoarse to Fine Grained Sense Disambiguation in WikipediaHui ShenSchool of EECSOhio UniversityAthens, OH 45701, USAhui.shen.1@ohio.eduRazvan BunescuSchool of EECSOhio UniversityAthens, OH 45701, USAbunescu@ohio.eduRada MihalceaDepartment of CSEUniversity of North TexasDenton, TX 76203, USArada@cs.unt.eduAbstractWikipedia articles are annotated by volunteercontributors with numerous links that connectwords and phrases to relevant titles.
Linksto general senses of a word are used concur-rently with links to more specific senses, with-out being distinguished explicitly.
We presentan approach to training coarse to fine grainedsense disambiguation systems in the presenceof such annotation inconsistencies.
Experi-mental results show that accounting for anno-tation ambiguity in Wikipedia links leads tosignificant improvements in disambiguation.1 Introduction and MotivationThe vast amount of world knowledge available inWikipedia has been shown to benefit many typesof text processing tasks, such as coreference res-olution (Ponzetto and Strube, 2006; Haghighi andKlein, 2009; Bryl et al 2010; Rahman and Ng,2011), information retrieval (Milne, 2007; Li et al2007; Potthast et al 2008; Cimiano et al 2009),or question answering (Ahn et al 2004; Kaisser,2008; Ferrucci et al 2010).
In particular, the usercontributed link structure of Wikipedia has beenshown to provide useful supervision for trainingnamed entity disambiguation (Bunescu and Pasca,2006; Cucerzan, 2007) and word sense disambigua-tion (Mihalcea, 2007; Ponzetto and Navigli, 2010)systems.
Articles in Wikipedia often contain men-tions of concepts or entities that already have a cor-responding article.
When contributing authors men-tion an existing Wikipedia entity inside an article,they are required to link at least its first mention tothe corresponding article, by using links or pipedlinks.
Consider, for example, the following Wikisource annotations: The [[capital city|capital]] ofGeorgia is [[Atlanta]].
The bracketed strings iden-tify the title of the Wikipedia articles that describethe corresponding named entities.
If the editor wantsa different string displayed in the rendered text, thenthe alternative string is included in a piped link, af-ter the title string.
Based on these Wiki processingrules, the text that is rendered for the aforementionedexample is: The capital of Georgia is Atlanta.Since many words and names mentioned inWikipedia articles are inherently ambiguous, theircorresponding links can be seen as a useful sourceof supervision for training named entity and wordsense disambiguation systems.
For example,Wikipedia contains articles that describe possiblesenses of the word ?capital?, such as CAPITAL CITY,CAPITAL (ECONOMICS), FINANCIAL CAPITAL, orHUMAN CAPITAL, to name only a few.
When dis-ambiguating a word or a phrase in Wikipedia, a con-tributor uses the context to determine the appropriateWikipedia title to include in the link.
In the exam-ple above, the editor of the article determined thatthe word ?capital?
was mentioned with the politicalcenter meaning, consequently it was mapped to thearticle CAPITAL CITY through a piped link.In order to useWikipedia links for training aWSDsystem for a given word, one needs first to define asense repository that specifies the possible meaningsfor that word, and then use the Wikipedia links tocreate training examples for each sense in the repos-itory.
This approach might be implemented usingthe following sequence of steps:22In global climate models, the state and properties of the [[atmosphere]] are specified at a number of discrete locationsGeneral = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A ?
A(S) ?
AEThe principal natural phenomena that contribute gases to the [[Atmosphere of Earth|atmosphere]] are emissions from volcanoesGeneral = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A ?
A(S) ?
AEAn aerogravity assist is a spacecraft maneuver designed to change velocity when arriving at a body with an [[atmosphere]]General = ATMOSPHERE; Specific = ATMOSPHERE ?
generic; Label = A ?
A(G)Assuming the planet?s [[atmosphere]] is close to equilibrium, it is predicted that 55 Cancri d is covered with water cloudsGeneral = ATMOSPHERE; Specific = ATMOSPHERE OF CANCRI ?
missing; A ?
A(G)Figure 1: Coarse and fine grained sense annotations in Wikipedia (bold).
The proposed hierarchical Label (right).A(S) = ATMOSPHERE (S), A(G) = ATMOSPHERE (G), A = ATMOSPHERE, AE = ATMOSPHERE OF EARTH.1.
Collect all Wikipedia titles that are linked fromthe ambiguous anchor word.2.
Create a repository of senses from all titles thathave sufficient support in Wikipedia i.e., titlesthat are referenced at least a predefined min-imum number of times using the ambiguousword as anchor.3.
Use the links extracted for each sense in therepository as labeled examples for that senseand train a WSD model to distinguish betweenalternative senses of the ambiguous word.Taking the word ?atmosphere?
as an example, thefirst step would result in a wide array of titles,ranging from the general ATMOSPHERE and its in-stantiations ATMOSPHERE OF EARTH or ATMO-SPHERE OF MARS, to titles as diverse as ATMO-SPHERE (UNIT), MOOD (PSYCHOLOGY), or AT-MOSPHERE (MUSIC GROUP).
In the second step,the most frequent titles for the anchor word ?at-mosphere?
would be assembled into a repository R= {ATMOSPHERE, ATMOSPHERE OF EARTH, AT-MOSPHERE OF MARS, ATMOSPHERE OF VENUS,STELLAR ATMOSPHERE, ATMOSPHERE (UNIT),ATMOSPHERE (MUSIC GROUP)}.
The classifiertrained in the third step would use features ex-tracted from the context to discriminate betweenword senses.This Wikipedia-based approach to creating train-ing data for word sense disambiguation has a ma-jor shortcoming.
Many of the training examples ex-tracted for the title ATMOSPHERE could very wellbelong to more specific titles such as ATMOSPHEREOF EARTH or ATMOSPHERE OF MARS.
Wheneverthe word ?atmosphere?
is used in a context with thesense of ?a layer of gases that may surround a ma-terial body of sufficient mass, and that is held inplace by the gravity of the body,?
the contributorhas the option of adding a link either to the title AT-MOSPHERE that describes this general sense of theword, or to the title of an article that describes theatmosphere of the actual celestial body that is re-ferred in that particular context, as shown in the first2 examples in Figure 1.
As shown in bold in Fig-ure 1, different occurrences of the same word maybe tagged with either a general or a specific link, anambiguity that is pervasive in Wikipedia for wordslike ?atmosphere?
that have general senses that sub-sume multiple, popular specific senses.
There doesnot seem to be a clear, general rule underlying thedecision to tag a word or a phrase with a generalor specific sense link in Wikipedia.
We hypothesizethat, in some cases, editors may be unaware that anarticle exists in Wikipedia for the actual referenceof a word or for a more specific sense of the word,and therefore they end up using a link to an articledescribing the general sense of the word.
There isalso the possibility that more specific articles are in-troduced only in newer versions of Wikipedia, andthus earlier annotations were not aware of these re-cent articles.
Furthermore, since annotating wordswith the most specific sense available in Wikipediamay require substantial cognitive effort, editors mayoften choose to link to a general sense of the word, achoice that is still correct, yet less informative thanthe more specific sense.2 Annotation Inconsistencies in WikipediaIn order to get a sense of the potential magnitudeof the general vs. specific sense annotation ambi-guity, we extracted all Wikipedia link annotations23for the words ?atmosphere?, ?president?, ?game?,?dollar?, ?diamond?
and ?Corinth?, and createda special subset from those that were labeled byWikipedia editors with the general sense links AT-MOSPHERE, PRESIDENT, GAME, DOLLAR, DIA-MOND, and CORINTH, respectively.
Then, for eachof the 7,079 links in this set, we used the contextto manually determine the corresponding more spe-cific title, whenever such a title exists in Wikipedia.The statistics in Tables 1 and 2 show a significantoverlap between the general and specific sense cate-gories.
For example, out of the 932 links from ?at-mosphere?
to ATMOSPHERE that were extracted intotal, 518 were actually about the ATMOSPHERE OFEARTH, but the user linked them to the more generalsense category ATMOSPHERE.
On the other hand,there are 345 links to ATMOSPHERE OF EARTH thatwere explicitly made by the user.
We manually as-signed general links (G) whenever the word is usedwith a generic sense, or when the reference is notavailable in the repository of titles collected for thatword because either the more specific title does notexist in Wikipedia or the specific title exists, but itdoes not have sufficient support ?
at least 20 linkedanchors ?
in Wikipedia.
We grouped the more spe-cific links for any given sense into a special cate-gory suffixed with (S), to distinguish them from thegeneral links (generic use, or missing reference) thatwere grouped into the category suffixed with (G).For many ambiguous words, the annotation in-consistencies appear when the word has sensesthat are in a subsumption relationship: the ATMO-SPHERE OF EARTH is an instance of ATMOSPHERE,whereas a STELLAR ATMOSPHERE is a particulartype of ATMOSPHERE.
Subsumed senses can beidentified automatically using the category graph inWikipedia.
The word ?Corinth?
is an interestingcase: the subsumption relationship between AN-CIENT CORINTH and CORINTH appears because ofa temporal constraint.
Furthermore, in the case ofthe word ?diamond?, the annotation inconsistenciesare not caused by a subsumption relation betweensenses.
Instead of linking to the DIAMOND (GEM-STONE) sense, Wikipedia contributors often link tothe related DIAMOND sense indicating the mineralused in the gemstone.A supervised learning algorithm that uses the ex-tracted links for training aWSD classification modelatmosphere SizeATMOSPHERE 932Atmosphere (S) 559Atmosphere of Earth 518Atmosphere of Mars 19Atmosphere of Venus 9Stellar Atmosphere 13Atmosphere (G) 373ATMOSPHERE OF EARTH 345ATMOSPHERE OF MARS 37ATMOSPHERE OF VENUS 26STELLAR ATMOSPHERE 29ATMOSPHERE (UNIT) 96ATMOSPHERE (MUSIC GROUP) 104president SizePRESIDENT 3534President (S) 989Chancellor (education) 326President of the United States 534President of the Philippines 42President of Pakistan 27President of France 22President of India 21President of Russia 17President (G) 2545CHANCELLOR (EDUCATION) 210PRESIDENT OF THE UNITED STATES 5941PRESIDENT OF THE PHILIPPINES 549PRESIDENT OF PAKISTAN 192PRESIDENT OF FRANCE 151PRESIDENT OF INDIA 86PRESIDENT OF RUSSIA 101Table 1: Wiki (CAPS) and manual (italics) annotations.to distinguish between categories in the sense repos-itory assumes implicitly that the categories, andhence their training examples, are mutually disjoint.This assumption is clearly violated for words like?atmosphere,?
consequently the learned model willhave a poor performance on distinguishing betweenthe overlapping categories.
Alternatively, we cansay that sense categories like ATMOSPHERE are illdefined, since their supporting dataset contains ex-amples that could also belong to more specific sensecategories such as ATMOSPHERE OF EARTH.We see two possible solutions to the problem ofinconsistent link annotations.
In one solution, spe-cific senses are grouped together with the subsuminggeneral sense, such that all categories in the result-ing repository become disjoint.
For ?atmosphere?,the general category ATMOSPHERE would be aug-mented to contain all the links previously annotated24dollar SizeDOLLAR 379Dollar (S) 231United States dollar 228Canadian dollar 3Australian dollar 1Dollar (G) 147UNITED STATES DOLLAR 3516CANADIAN DOLLAR 420AUSTRALIAN DOLLAR 124DOLLAR SIGN 290DOLLAR (BAND) 30DOLLAR, CLACKMANNANSHIRE 30game SizeGAME 819Game (S) 99Video game 55PC game 44Game (G) 720VIDEO GAME 312PC GAME 24GAME (FOOD) 232GAME (RAPPER) 154diamond SizeDIAMOND 716Diamond (S) 221Diamond (gemstone) 221Diamond (G) 495DIAMOND (GEMSTONE) 71BASEBALL FIELD 36MUSIC RECORDING SALES CERT.
36Corinth SizeCORINTH 699Corinth (S) 409Ancient Corinth 409Corinth (G) 290ANCIENT CORINTH 92CORINTH, MISSISSIPPI 72Table 2: Wiki (CAPS) and manual (italics) annotations.as ATMOSPHERE, ATMOSPHERE OF EARTH, AT-MOSPHERE OF MARS, ATMOSPHERE OF VENUS,or STELLAR ATMOSPHERE.
This solution isstraightforward to implement, however it has thedisadvantage that the resulting WSD model willnever link words to more specific titles in Wikipedialike ATMOSPHERE OF MARS.Another solution is to reorganize the originalsense repository into a hierarchical classificationscheme such that sense categories at each classifi-cation level become mutually disjoint.
The resultingWSD system has the advantage that it can make finegrained sense distinctions for an ambiguous word,despite the annotation inconsistencies present in thetraining data.
The rest of this paper describes a feasi-ble implementation for this second solution that doesnot require any manual annotation beyond the linksthat are already provided by Wikipedia volunteers.3 Learning for Coarse to Fine GrainedSense DisambiguationFigure 2 shows our proposed hierarchical classifica-tion scheme for disambiguation, using ?atmosphere?as the ambiguous word.
Shaded leaf nodes showthe final categories in the sense repository for eachword, whereas the doted elliptical frames on thesecond level in the hierarchy denote artificial cate-gories introduced to enable a finer grained classifi-cation into more specific senses.
Thick dotted ar-rows illustrate the classification decisions that aremade in order to obtain a fine grained disambigua-tion of the word.
Thus, the word ?atmosphere?is first classified to have the general sense ATMO-SPHERE, i.e.
?a layer of gases that may surround amaterial body of sufficient mass, and that is held inplace by the gravity of the body?.
In the first so-lution, the disambiguation process would stop hereand output the general sense ATMOSPHERE.
In thesecond solution, the disambiguation process contin-ues and further classifies the word to be a referenceto ATMOSPHERE OF EARTH.
To get to this finalclassification, the process passes through an inter-mediate binary classification level where it deter-mines whether the word has a more specific sensecovered in Wikipedia, corresponding to the artificialcategory ATMOSPHERE (S).
If the answer is no, thesystem stops the disambiguation process and out-puts the general sense category ATMOSPHERE.
Thisbasic sense hierarchy can be replicated dependingon the existence of even finer sense distinctions inWikipedia.
For example, Wikipedia articles describ-ing atmospheres of particular stars could be used tofurther refine STELLAR ATMOSPHERE with two ad-ditional levels of the type Level 2 and Level 3.
Over-all, the proposed disambiguation scheme could beused to relabel the ATMOSPHERE links in Wikipediawith more specific, and therefore more informative,senses such as ATMOSPHERE OF EARTH.
In gen-eral, the Wikipedia category graph could be usedto automatically create hierarchical structures for re-25Figure 2: Hierarchical disambiguation scheme, from coarse to fine grained senses.lated senses of the same word.Training word sense classifiers for Levels 1 and 3is straightforward.
For Level 1, Wikipedia links thatare annotated by users as ATMOSPHERE, ATMO-SPHERE OF EARTH, ATMOSPHERE OF MARS, AT-MOSPHERE OF VENUS, or STELLAR ATMOSPHEREare collected as training examples for the generalsense category ATMOSPHERE.
Similarly, links thatare annotated as ATMOSPHERE (UNIT) and ATMO-SPHERE (MUSIC GROUP) will be used as trainingexamples for the two categories, respectively.
Amulticlass classifier is then trained to distinguish be-tween the three categories at this level.
For Level 3,a multiclass classifiers is trained on Wikipedia linkscollected for each of the 4 specific senses.For the binary classifier at Level 2, we coulduse as training examples for the category ATMO-SPHERE (G) all Wikipedia links that were anno-tated as ATMOSPHERE, whereas for the categoryATMOSPHERE (S) we could use as training exam-ples all Wikipedia links that were annotated specif-ically as ATMOSPHERE OF EARTH, ATMOSPHEREOF MARS, ATMOSPHERE OF VENUS, or STELLARATMOSPHERE.
A traditional binary classificationSVM could be trained on this dataset to distinguishbetween the two categories.
We call this approachNaive SVM, since it does not account for the fact thata significant number of the links that are annotatedby Wikipedia contributors as ATMOSPHERE shouldactually belong to the ATMOSPHERE (S) category ?about 60% of them, according to Table 1.
Instead,we propose treating all ATMOSPHERE links as unla-beled examples.
If we consider the specific links inATMOSPHERE (S) to be positive examples, then theproblem becomes one of learning with positive andunlabeled examples.3.1 Learning with positive and unlabeledexamplesThis general type of semi-supervised learning hasbeen studied before in the context of tasks suchas text classification and information retrieval (Leeand Liu, 2003; Liu et al 2003), or bioinformat-ics (Elkan and Noto, 2008; Noto et al 2008).
Inthis setting, the training data consists of positive ex-amples x ?
P and unlabeled examples x ?
U .Following the notation of Elkan and Noto (2008),we define s(x) = 1 if the example is positive ands(x) = ?1 if the example is unlabeled.
The truelabel of an example is y(x) = 1 if the exampleis positive and y(x) = ?1 if the example is neg-ative.
Thus, x ?
P ?
s(x) = y(x) = 1 andx ?
U ?
s(x) = ?1 i.e., the true label y(x) of anunlabeled example is unknown.
For the experimentsreported in this paper, we use our implementationof two state-of-the-art approaches to Learning withPositive and Unlabeled (LPU) examples: the BiasedSVM formulation of Lee and Liu (2003) and theWeighted Samples SVM formulation of Elkan andNoto (2008).
The original version of Biased SVMwas designed to maximize the product between pre-cision and recall.
In the next section we describe a26modification to the Biased SVM approach that canbe used to maximize accuracy, a measure that is of-ten used to evaluate WSD performance.3.1.1 The Biased SVMIn the Biased SVM formulation (Lee and Liu,2003; Liu et al 2003), all unlabeled examples areconsidered to be negative and the decision functionf(x) = wT?
(x) + b is learned using the standardsoft-margin SVM formulation shown in Figure 3.minimize: 12?w?2 + CP?x?P?x + CU?x?U?xsubject to: s(x) (wT?
(x) + b) ?
1?
?x?x ?
0, ?x ?
P ?
UFigure 3: Biased SVM optimization problem.The capacity parameters CP and CU control howmuch we penalize errors on positive examples vs. er-rors on unlabeled examples.
Since not all unlabeledexamples are negative, one would want to select ca-pacity parameters satisfying CP > CU , such thatfalse negative errors are penalized more than falsepositive errors.
In order to find the best capacity pa-rameters to use during training, the Biased SVM ap-proach runs a grid search on a separate developmentdataset.
This search is aimed at finding values forthe parameters CP and CU that maximize pr, theproduct between precision p = p(y = 1|f = 1) andrecall r = p(f = 1|y = 1).
Lee and Liu (2003)show that maximizing the pr criterion is equivalentwith maximizing the objective r2/p(f = 1), whereboth r = p(f = 1|y = 1) and p(f = 1) can be es-timated using the trained decision function f(x) onthe development dataset.Maximizing the pr criterion in the original BiasedSVM formulation was motivated by the need to opti-mize the F measure in information retrieval settings,where F = 2pr(p+ r).
In the rest of this section weshow that classification accuracy can be maximizedusing only positive and unlabeled examples, an im-portant result for problems where classification ac-curacy is the target performance measure.The accuracy of a binary decision function f(x)is, by definition, acc = p(f = 1|y = 1) + p(f =?1|y = ?1).
Since the recall is r = p(f = 1|y =1), the accuracy can be re-written as:acc = r + 1?
p(f = 1|y = ?1) (1)Using Bayes?
rule twice, the false positive termp(f = 1|y = ?1) can be re-written as:p(f = 1|y = ?1) = p(f = 1)p(y = ?1|f = 1)p(y = ?1)= p(f = 1)p(y = ?1) ?
(1?
p(y = 1|f = 1))= p(f = 1)p(y = ?1) ?p(f = 1)p(y = ?1) ?p(y = 1)p(f = 1|y = 1)p(f = 1)= p(f = 1)?
p(y = 1)?
rp(y = ?1) (2)Plugging identity 2 in Equation 1 leads to:acc = 1 + r + r ?
p(y = 1)?
p(f = 1)p(y = ?1)= 1 + r ?
p(f = 1)p(y = ?1) (3)Since p(y = ?1) can be assimilated with a con-stant, Equation 3 implies that maximizing accu-racy is equivalent with maximizing the criterionr ?
p(f = 1), where both the recall r and p(f = 1)can be estimated on the positive and unlabeled ex-amples from a separate development dataset.In conclusion, one can use the original BiasedSVM formulation to maximize r2/p(f = 1), whichhas been shown by Lee and Liu (2003) to maximizepr, a criterion that has a similar behavior with theF-measure used in retrieval applications.
Alterna-tively, if the target performance measure is accuracy,we can choose instead to maximize r ?
p(f = 1),which we have shown above to correspond to accu-racy maximization.3.1.2 The Weighted Samples SVMElkan and Noto (2008) introduced two ap-proaches for learning with positive and unlabeleddata.
Both approaches are based on the assumptionthat labeled examples {x|s(x) = 1} are selected atrandom from the positive examples {x|y(x) = 1}i.e., p(s = 1|x, y = 1) = p(s = 1|y = 1).
Theirbest performing approach uses the positive and unla-beled examples to train two distinct classifiers.
First,the dataset P ?
U is split into a training set and avalidation set, and a classifier g(x) is trained on the27labeling s to approximate the label distribution i.e.g(x) = p(s = 1|x).
The validation set is then usedto estimate p(s = 1|y = 1) as follows:p(s=1|y=1) = p(s=1|x, y=1) = 1|P |?x?Pg(x) (4)The second and final classifier f(x) is trained on adataset of weighted examples that are sampled fromthe original training set as follows:?
Each positive example x ?
P is copied as apositive example in the new training set withweight p(y = 1|x, s = 1) = 1.?
Each unlabeled example x ?
U is duplicatedinto two training examples in the new dataset:a positive example with weight p(y = 1|x, s =0) and a negative example with weight p(y =?1|x, s = 0) = 1?
p(y = 1|x, s = 0).Elkan and Noto (2008) show that the weights abovecan be derived as:p(y=1|x, s=0) = 1?p(s=1|y=1)p(s=1|y=1) ?p(s=1|x)1?p(s=1|x) (5)The output of the first classifier g(x) is used toapproximate the probability p(s = 1|x), whereasp(s = 1|y = 1) is estimated using Equation 4.The two classifiers g and f are trained usingSVMs and a linear kernel.
Platt scaling is used withthe first classifier to obtain the probability estimatesg(x) = p(s = 1|x), which are then converted intoweights following Equations 4 and 5, and used dur-ing the training of the second classifier.4 Experimental EvaluationWe ran disambiguation experiments on the 6 am-biguous words atmosphere, president, dollar, game,diamond andCorinth.
The correspondingWikipediasense repositories have been summarized in Tables 1and 2.
All WSD classifiers used the same set of stan-dard WSD features (Ng and Lee, 1996; Stevensonand Wilks, 2001), such as words and their part-of-speech tags in a window of 3 words around the am-biguous keyword, the unigram and bigram contentwords that are within 2 sentences of the current sen-tence, the syntactic governor of the keyword, andits chains of syntactic dependencies of lengths up totwo.
Furthermore, for each example, a Wikipediaspecific feature was computed as the cosine similar-ity between the context of the ambiguous word andthe text of the article for the target sense or reference.The Level1 and Level3 classifiers were trained us-ing the SVMmulti component of the SVMlight pack-age.1 TheWSD classifiers were evaluated in a 4-foldcross validation scenario in which 50% of the datawas used for training, 25% for tuning the capacityparameter C, and 25% for testing.
The final accu-racy numbers, shown in Table 3, were computed byaveraging the results over the 4 folds.
Since the wordpresident has only one sense on Level1, no classifierneeded to be trained for this case.
Similarly, wordsdiamond andCorinth have only one sense on Level3.atmosphere president dollarLevel1 93.1% ?
94.1%Level3 85.6% 82.2% 90.8%game diamond CorinthLevel1 82.9% 95.5% 92.7%Level3 92.9% ?
?Table 3: Disambiguation accuracy at Levels 1 & 3.The evaluation of the binary classifiers at the sec-ond level follows the same 4-fold cross validationscheme that was used for Level1 and Level3.
Themanual labels for specific senses and references inthe unlabeled datasets are always ignored duringtraining and tuning and used only during testing.We compare the Naive SVM, Biased SVM, andWeighted SVM in the two evaluation settings, usingfor all of them the same train/development/test splitsof the data and the same features.
We emphasizethat our manual labels are used only for testing pur-poses ?
the manual labels are ignored during train-ing and tuning, when the data is assumed to containonly positive and unlabeled examples.
We imple-mented the Biased SVM approach on top of the bi-nary SVMlight package.
TheCP andCU parametersof the Biased SVM were tuned through the c and jparameters of SVMlight (c = CU and j = CP /CU ).Eventually, all three methods use the developmentdata for tuning the c and j parameters of the SVM.However, whereas the Naive SVM tunes these pa-rameters to optimize the accuracy with respect to thenoisy label s(x), the Biased SVM tunes the same pa-rameters to maximize an estimate of the accuracy or1http://svmlight.joachims.org28F-measure with respect to the true label y(x).
TheWeighted SVM approach was implemented on topof the LibSVM2 package.
Even though the originalWeighted SVM method of Elkan and Noto (2008)does not specify tuning any parameters, we noticedit gave better results when the capacity c and weightj parameters were tuned for the first classifier g(x).Table 4 shows the accuracy results of the threemethods for Level2, whereas Table 5 shows the F-measure results.
The Biased SVM outperforms theNaive SVM on all the words, in terms of both ac-curacy and F-measure.
The most dramatic increasesare seen for the words atmosphere, game, diamond,and Corinth.
For these words, the number of pos-itive examples is significantly smaller compared tothe total number of positive and unlabeled examples.Thus, the percentage of positive examples relative tothe total number of positive and unlabeled examplesis 31.9% for atmosphere, 29.1% for game, 9.0% fordiamond, and 11.6% for Corinth.
The positive to to-tal ratio is however significantly larger for the othertwo words: 67.2% for president and 91.5% for dol-lar.
When the number of positive examples is large,the false negative noise from the unlabeled datasetin the Naive SVM approach will be relatively small,hence the good performance of Naive SVM in thesecases.
To check whether this is the case, we havealso run experiments where we used only half ofthe available positive examples for the word presi-dent and one tenth of the positive examples for theword dollar, such that the positive datasets becamecomparable in size with the unlabeled datasets.
Theresults for these experiments are shown in Tables 4and 5 in the rows labeled presidentS and dollarS .
Asexpected, the difference between the performance ofNaive SVM and Biased SVM gets larger on thesesmaller datasets, especially for the word dollar.The Weighted SVM outperforms the Naive SVMon five out of the six words, the exception being theword president.
Comparatively, the Biased SVMhas a more stable behavior and overall results in amore substantial improvement over the Naive SVM.Based on these initial results, we see the BiasedSVM as the method of choice for learning with pos-itive and unlabeled examples in the task of coarse tofine grained sense disambiguation in Wikipedia.2http://www.csie.ntu.edu.tw/?cjlin/libsvmWord NaiveSVM BiasedSVM WeightedSVMatmosphere 39.9% 79.6% 75.0%president 91.9% 92.5% 89.5%dollar 96.0% 97.0% 97.1%game 83.8% 87.1% 84.6%diamond 70.2% 74.5% 75.1%Corinth 46.2% 75.1% 51.9%presidentS 88.1% 90.6% 87.4%dollarS 70.3% 84.9% 70.6%Table 4: Disambiguation accuracy at Level2.Word NaiveSVM BiasedSVM WeightedSVMatmosphere 30.5% 86.0% 83.2%president 94.4% 95.0% 92.8%dollar 97.9% 98.4% 98.5%game 75.1% 81.8% 77.5%diamond 8.6% 53.5% 46.3%Corinth 15.3% 81.2% 68.0%presidentS 90.0% 92.4% 89.5%dollarS 77.9% 91.2% 78.2%Table 5: Disambiguation F-measure at Level2.In a final set of experiments, we compared thetraditional flat classification approach and our pro-posed hierarchical classifier in terms of their over-all disambiguation accuracy.
In these experiments,the sense repository contains all the leaf nodes asdistinct sense categories.
For example, the wordatmosphere would correspond to the sense repos-itory R = {ATMOSPHERE (G), ATMOSPHERE OFEARTH, ATMOSPHERE OF MARS, ATMOSPHEREOF VENUS, STELLAR ATMOSPHERE, ATMO-SPHERE (UNIT), ATMOSPHERE (MUSIC GROUP)}.The overall accuracy results are shown in Table 6and confirm the utility of using the LPU frameworkin the hierarchical model, which outperforms the tra-ditional flat model, especially on words with low ra-tio of positive to unlabeled examples.atmosphere president dollarFlat 52.4% 89.4% 90.0%Hierarchical 79.7% 91.0% 90.1%game diamond CorinthFlat 83.6% 65.7% 42.6%Hierarchical 87.2% 76.8% 72.1%Table 6: Flat vs. Hierarchical disambiguation accuracy.295 Future WorkAnnotation inconsistencies in Wikipedia were cir-cumvented by adapting two existing approaches thatuse only positive and unlabeled data to train binaryclassifiers.
This binary classification constraint ledto the introduction of the artificial specific (S) cat-egory on Level2 in our disambiguation framework.In future work, we plan to investigate a direct exten-sion of learning with positive and unlabeled data tothe case of multiclass classification, which will re-duce the number of classification levels from 3 to 2.We also plan to investigate the use of unsupervisedtechniques in order to incorporate less popular refer-ences of a word in the hierarchical classification.ConclusionWe presented an approach to training coarse to finegrained sense disambiguation systems that treatsannotation inconsistencies in Wikipedia under theframework of learning with positive and unlabeledexamples.
Furthermore, we showed that the true ac-curacy of a decision function can be optimized us-ing only positive and unlabeled examples.
For test-ing purposes, we manually annotated 7,079 links be-longing to six ambiguous words 3.
Experimentalresults demonstrate that accounting for annotationambiguity in Wikipedia links leads to consistent im-provements in disambiguation accuracy.
The man-ual annotations were only used for testing and wereignored during training and development.
Conse-quently, the proposed framework of learning withpositive and unlabeled examples for sense disam-biguation could be applied on the entire Wikipediawithout any manual annotations.
By augmentinggeneral sense links with links to more specific ar-ticles, such an application could have a significantimpact on Wikipedia itself.AcknowledgmentsThis work was supported in part by the Na-tional Science Foundation IIS awards #1018613 and#1018590, and an allocation of computing time fromthe Ohio Supercomputer Center.3Data and code will be made publicly available.ReferencesD.
Ahn, V. Jijkoun, G. Mishne, K. Muller, M. de Ri-jke, and S. Schlobach.
2004.
Using Wikipedia at theTREC QA track.
In Proceedings of the 13th Text Re-trieval Conference (TREC 2004).Volha Bryl, Claudio Giuliano, Luciano Serafini, andKateryna Tymoshenko.
2010.
Using backgroundknowledge to support coreference resolution.
In Pro-ceedings of the 2010 conference on ECAI 2010: 19thEuropean Conference on Artificial Intelligence, pages759?764, Amsterdam, The Netherlands.Razvan Bunescu and Marius Pasca.
2006.
Using ency-clopedic knowledge for named entity disambiguation.In Proceesings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-06), pages 9?16, Trento, Italy.Philipp Cimiano, Antje Schultz, Sergej Sizov, PhilippSorg, and Steffen Staab.
2009.
Explicit versus la-tent concept models for cross-language information re-trieval.
In International Joint Conference on ArtificialIntelligence (IJCAI-09, pages 1513?1518, Pasadena,CA, july.S.
Cucerzan.
2007.
Large-scale named entity disam-biguation based on Wikipedia data.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 708?716.Charles Elkan and Keith Noto.
2008.
Learning clas-sifiers from only positive and unlabeled data.
InProceedings of the 14th ACM SIGKDD internationalconference on Knowledge discovery and data mining,KDD ?08, pages 213?220.David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,James Fan, David Gondek, Aditya Kalyanpur, AdamLally, J. William Murdock, Eric Nyberg, John M.Prager, Nico Schlaefer, and Christopher A. Welty.2010.
Building watson: An overview of the deepqaproject.
AI Magazine, 31(3):59?79.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1152?1161, Singapore, August.M.
Kaisser.
2008.
The QuALiM question answeringdemo: Supplementing answers with paragraphs drawnfrom Wikipedia.
In Proceedings of the ACL-08 Hu-man Language Technology Demo Session, pages 32?35, Columbus, Ohio.Wee Sun Lee and Bing Liu.
2003.
Learning with pos-itive and unlabeled examples using weighted logisticregression.
In Proceedings of the Twentieth Interna-tional Conference on Machine Learning (ICML, pages448?455, Washington, DC, August.30Y.
Li, R. Luk, E. Ho, and K. Chung.
2007.
Improv-ing weak ad-hoc queries using Wikipedia as externalcorpus.
In Proceedings of the 30th Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 797?798,Amsterdam, Netherlands.Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, andPhilip S. Yu.
2003.
Building text classifiers using pos-itive and unlabeled examples.
In Proceedings of theThird IEEE International Conference on Data Mining,ICDM ?03, pages 179?186, Washington, DC, USA.R.
Mihalcea.
2007.
Using Wikipedia for automatic wordsense disambiguation.
In Human Language Technolo-gies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pages 196?203, Rochester, New York, April.D.
Milne.
2007.
Computing semantic relatedness usingWikipedia link structure.
In Proceedings of the NewZealand Computer Science Research Student Confer-ence, Hamilton, New Zealand.Hwee Tou Ng and H. B. Lee.
1996.
Integrating multipleknowledge sources to disambiguate word sense: Anexemplar-based approach.
In Proceedings of the 34thAnnual Meeting of the Association for ComputationalLinguistics (ACL-96), pages 40?47, Santa Cruz, CA.Keith Noto, Milton H. Saier, Jr., and Charles Elkan.2008.
Learning to find relevant biological articleswithout negative training examples.
In Proceedings ofthe 21st Australasian Joint Conference on Artificial In-telligence: Advances in Artificial Intelligence, AI ?08,pages 202?213.Simone Paolo Ponzetto and Roberto Navigli.
2010.Knowledge-rich word sense disambiguation rivalingsupervised systems.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1522?1531, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Simone Paolo Ponzetto and Michael Strube.
2006.
Ex-ploiting semantic role labeling, wordnet and wikipediafor coreference resolution.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics, pages 192?199.M.
Potthast, B. Stein, and M. A. Anderka.
2008.Wikipedia-based multilingual retrieval model.
In Pro-ceedings of the 30th European Conference on IR Re-search, Glasgow.Altaf Rahman and Vincent Ng.
2011.
Coreference res-olution with world knowledge.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies -Volume 1, pages 814?824, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Mark Stevenson and YorickWilks.
2001.
The interactionof knowledge sources in word sense disambiguation.Computational Linguistics, 27(3):321?349, Septem-ber.31
