Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 958?967,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBootstrapping Semantic Analyzers from Non-Contradictory TextsIvan Titov Mikhail KozhevnikovSaarland UniversitySaarbru?cken, Germany{titov|m.kozhevnikov}@mmci.uni-saarland.deAbstractWe argue that groups of unannotated textswith overlapping and non-contradictorysemantics represent a valuable source ofinformation for learning semantic repre-sentations.
A simple and efficient infer-ence method recursively induces joint se-mantic representations for each group anddiscovers correspondence between lexicalentries and latent semantic concepts.
Weconsider the generative semantics-text cor-respondence model (Liang et al, 2009)and demonstrate that exploiting the non-contradiction relation between texts leadsto substantial improvements over natu-ral baselines on a problem of analyzinghuman-written weather forecasts.1 IntroductionIn recent years, there has been increasing inter-est in statistical approaches to semantic parsing.However, most of this research has focused on su-pervised methods requiring large amounts of la-beled data.
The supervision was either given inthe form of meaning representations aligned withsentences (Zettlemoyer and Collins, 2005; Ge andMooney, 2005; Mooney, 2007) or in a some-what more relaxed form, such as lists of candidatemeanings for each sentence (Kate and Mooney,2007; Chen and Mooney, 2008) or formal repre-sentations of the described world state for eachtext (Liang et al, 2009).
Such annotated resourcesare scarce and expensive to create, motivating theneed for unsupervised or semi-supervised tech-niques (Poon and Domingos, 2009).
However,unsupervised methods have their own challenges:they are not always able to discover semanticequivalences of lexical entries or logical forms or,on the contrary, cluster semantically different oreven opposite expressions (Poon and Domingos,2009).
Unsupervised approaches can only rely ondistributional similarity of contexts (Harris, 1968)to decide on semantic relatedness of terms, but thisinformation may be sparse and not reliable (Weedsand Weir, 2005).
For example, when analyzingweather forecasts it is very hard to discover in anunsupervised way which of the expressions among?south wind?, ?wind from west?
and ?southerly?denote the same wind direction and which are not,as they all have a very similar distribution of theircontexts.
The same challenges affect the problemof identification of argument roles and predicates.In this paper, we show that groups of unanno-tated texts with overlapping and non-contradictorysemantics provide a valuable source of informa-tion.
This form of weak supervision helps todiscover implicit clustering of lexical entries andpredicates, which presents a challenge for purelyunsupervised techniques.
We assume that eachtext in a group is independently generated froma full latent semantic state corresponding to thegroup.
Importantly, the texts in each group donot have to be paraphrases of each other, as theycan verbalize only specific parts (aspects) of thefull semantic state, yet statements about the sameaspects must not contradict each other.
Simulta-neous inference of the semantic state for the non-contradictory and semantically overlapping docu-ments would restrict the space of compatible hy-potheses, and, intuitively, ?easier?
texts in a groupwill help to analyze the ?harder?
ones.1As an illustration of why this weak supervi-sion may be valuable, consider a group of twonon-contradictory texts, where one text mentions?2.2 bn GBP decrease in profit?, whereas anotherone includes a passage ?profit fell by 2.2 billionpounds?.
Even if the model has not observed1This view on this form of supervision is evocative of co-training (Blum and Mitchell, 1998) which, roughly, exploitsthe fact that the same example can be ?easy?
for one modelbut ?hard?
for another one.958Current?temperature?is?about?70F,Cith?high?of?around?75F?amd?lowof?around?64uOvercast,Rain?is?quite?possible?tonight,as?t-storms?arruSouth?wind?of?around?19?mph.Cuu r euA?slight?chance?of?showers?Mostly?cloudy,?
?with?a?high?near?75.South?wind?between?15?and?20?mph,Chance?of?precipitation?is?30%.with?gusts?as?high?as?30?mph.and?thunderstorms?after?noon.Thunderstorms?and?pouring?are?possiblrthroughout?the?day,with?precipitation?chance?of?about?25%.possibly?growing?up?to?75?F?during?the?day,as?south?wind?blows?at?about?20?mphuThe?sky?is?heavyuIt?is?70?F?now,erntr?mep?raiesnrabao70F,anshabaogfanmdaba5lfanrmhaba5w6Csh4Os?iesnrbo70Ffnv4r bc6Rpqeiesnrbo70Ffanshbwfanmdb0-fanrmhb0l6t?rSst1verhesm9iesnrbo70Ffnshb0wfnmdb.0fnrmhb 0o6eAph4r?MAmhSriesnrbo70Ffnv4rbSAmhSr6y?rr2shR3mshMAmhSriesnrbF57.wfnv4rb776q9rreMAmhSriesnrb%o70F%fnv4rb776qTkSvIr?iesnrbo70FfpST reb5l7Fww6Csh4ctrr4iesnrbo70F,an shbFgfnmdb00fnrmhbF-fapSTrebFw70w6?mshMAmhSriesnrbo70Ffnv4rbSAmhSr6Csh4MAs99iesnrbo70Ffnshbwfnmdbwfnrmhbw6aaaaaaaaaaaaaaaauuuuuuFigure 1: An example of three non-contradictory weather forecasts and their alignment to the semanticrepresentation.
Note that the semantic representation (the block in the middle) is not observable intraining.the word ?fell?
before, it is likely to align thesephrases to the same semantic form because of sim-ilarity of their arguments.
And this alignmentwould suggest that ?fell?
and ?decrease?
refer tothe same process, and should be clustered together.This would not happen for the pair ?fell?
and ?in-crease?
as similarity of their arguments would nor-mally entail contradiction.
Similarly, in the exam-ple mentioned earlier, when describing a forecastfor a day with expected south winds, texts in thegroup can use either ?south wind?
or ?southerly?to indicate this fact but no texts would verbalizeit as ?wind from west?, and therefore these ex-pressions will be assigned to different semanticclusters.
However, it is important to note that thephrase ?wind from west?
may still appear in thetexts, but in reference to other time periods, un-derlying the need for modeling alignment betweengrouped texts and their latent meaning representa-tion.As much of the human knowledge is re-described multiple times, we believe that non-contradictory and semantically overlapping textsare often easy to obtain.
For example, considersemantic analysis of news articles or biographies.In both cases we can find groups of documents re-ferring to the same events or persons, and thoughthey will probably focus on different aspects andhave different subjective passages, they are likelyto agree on the core information (Shinyama andSekine, 2003).
Alternatively, if such groupings arenot available, it may still be easier to give each se-mantic representation (or a state) to multiple an-notators and ask each of them to provide a tex-tual description, instead of annotating texts withsemantic expressions.
The state can be communi-cated to them in a visual or audio form (e.g., asa picture or a short video clip) ensuring that theirinterpretations are consistent.Unsupervised learning with shared latent se-mantic representations presents its own chal-lenges, as exact inference requires marginalizationover possible assignments of the latent semanticstate, consequently, introducing non-local statisti-cal dependencies between the decisions about thesemantic structure of each text.
We propose a sim-ple and fairly general approximate inference algo-rithm for probabilistic models of semantics whichis efficient for the considered model, and achievesfavorable results in our experiments.In this paper, we do not consider modelswhich aim to produce complete formal meaningof text (Zettlemoyer and Collins, 2005; Mooney,2007; Poon and Domingos, 2009), instead focus-ing on a simpler problem studied in (Liang et al,2009).
They investigate grounded language ac-quisition set-up and assume that semantics (worldstate) can be represented as a set of records eachconsisting of a set of fields.
Their model seg-ments text into utterances and identifies records,fields and field values discussed in each utter-ance.
Therefore, one can think of this problem asan extension of the semantic role labeling prob-lem (Carreras and Marquez, 2005), where predi-cates (i.e.
records in our notation) and their ar-guments should be identified in text, but here ar-guments are not only assigned to a specific role(field) but also mapped to an underlying equiv-alence class (field value).
For example, in theweather forecast domain field sky cover should getthe same value given expressions ?overcast?
and?very cloudy?
but a different one if the expres-959sions are ?clear?
or ?sunny?.
This model is hardto evaluate directly as text does not provide in-formation about all the fields and does not neces-sarily provide it at the sufficient granularity level.Therefore, it is natural to evaluate their modelon the database-text alignment problem (Snyderand Barzilay, 2007), i.e.
measuring how well themodel predicts the alignment between the text andthe observable records describing the entire worldstate.
We follow their set-up, but assume that in-stead of having access to the full semantic statefor every training example, we have a very smallamount of data annotated with semantic states anda larger number of unannotated texts with non-contradictory semantics.We study our set-up on the weather forecastdata (Liang et al, 2009) where the original textualweather forecasts were complemented by addi-tional forecasts describing the same weather states(see figure 1 for an example).
The average overlapbetween the verbalized fields in each group of non-contradictory forecasts was below 35%, and morethan 60% of fields are mentioned only in a singleforecast from a group.
Our model, learned from100 labeled forecasts and 259 groups of unanno-tated non-contradictory forecasts (750 texts in to-tal), achieved 73.9% F1.
This compares favorablywith 69.1% shown by a semi-supervised learningapproach, though, as expected, does not reach thescore of the model which, in training, observed se-mantics states for all the 750 documents (77.7%F1).The rest of the paper is structured as follows.In section 2 we describe our inference algorithmfor groups of non-contradictory documents.
Sec-tion 3 redescribes the semantics-text correspon-dence model (Liang et al, 2009) in the context ofour learning scenario.
In section 4 we provide anempirical evaluation of the proposed method.
Weconclude in section 5 with an examination of ad-ditional related work.2 Inference with Non-ContradictoryDocumentsIn this section we will describe our inferencemethod on a higher conceptual level, not speci-fying the underlying meaning representation andthe probabilistic model.
An instantiation of thealgorithm for the semantics-text correspondencemodel is given in section 3.2.Statistical models of parsing can often be re-garded as defining the probability distribution ofmeaning m and its alignment a with the giventext w, P (m,a,w) = P (a,w|m)P (m).
Thesemantics m can be represented either as a logicalformula (see, e.g., (Poon and Domingos, 2009)) oras a set of field values if database records are usedas a meaning representation (Liang et al, 2009).The alignment a defines how semantics is verbal-ized in the text w, and it can be represented bya meaning derivation tree in case of full semanticparsing (Poon and Domingos, 2009) or, e.g., bya hierarchical segmentation into utterances alongwith an utterance-field alignment in a more shal-low variation of the problem.
In semantic parsing,we aim to find the most likely underlying seman-tics and alignment given the text:(m?, a?)
= argmaxm,aP (a,w|m)P (m).
(1)In the supervised case, where a and m are observ-able, estimation of the generative model parame-ters is generally straightforward.
However, in asemi-supervised or unsupervised case variationaltechniques, such as the EM algorithm (Demp-ster et al, 1977), are often used to estimate themodel.
As common for complex generative mod-els, the most challenging part is the computationof the posterior distributions P (a,m|w) on theE-step which, depending on the underlying modelP (m,a,w), may require approximate inference.As discussed in the introduction, our goal is tointegrate groups of non-contradictory documentsinto the learning procedure.
Let us denote byw1,...,wK a group of non-contradictory docu-ments.
As before, the estimation of the poste-rior probabilities P (mi,ai|w1 .
.
.wK) presentsthe main challenge.
Note that the decision aboutmi is now conditioned on all the texts wj ratherthan only on wi.
This conditioning is exactly whatdrives learning, as the information about likely se-mantics mj of text j affects the decision aboutchoice of mi:P (mi|w1,...,wK) ?
?aiP (ai,wi|mi)??
?m?i,a?iP (mi|m?i)P (m?i,a?i,w?i), (2)where x?i denotes {xj : j 6= i}.
P (mi|m?i)is the probability of the semantics mi given allthe meanings m?i.
This probability assigns zeroweight to inconsistent meanings, i.e.
such mean-960ings (m1,...,mK) that ?Ki=1mi is not satisfiable,2and models dependencies between components inthe composite meaning representation (e.g., argu-ments values of predicates).
As an illustration, inthe forecast domain it may express that clouds, andnot sunshine, are likely when it is raining.
Note,that this probability is different from the probabil-ity that mi is actually verbalized in the text.Unfortunately, these dependencies between miand wj are non-local.
Even though the dependen-cies are only conveyed via {mj : j 6= i} the spaceof possible meaningsm is very large even for rela-tively simple semantic representations, and, there-fore, we need to resort to efficient approximations.One natural approach would be to use a formof belief propagation (Pearl, 1982; Murphy et al,1999), where messages pass information aboutlikely semantics between the texts.
However, thisapproach is still expensive even for simple mod-els, both because of the need to represent distribu-tions over m and also because of the large numberof iterations of message exchange needed to reachconvergence (if it converges).An even simpler technique would be to parsetexts in a random order conditioning each mean-ing m?k for k ?
{1,...,K} on all the previous se-mantics m?<k = m?1,...,m?k?1:m?k = argmaxmkP (wk|mk)P (mk|m?<k).Here, and in further discussion, we assume thatthe above search problem can be efficiently solved,exactly or approximately.
However, a major weak-ness of this algorithm is that decisions about com-ponents of the composite semantic representation(e.g., argument values) are made only on the ba-sis of a single text, which first mentions the cor-responding aspects, without consulting any futuretexts k?
> k, and these decisions cannot be revisedlater.We propose a simple algorithm which aims tofind an appropriate order of the greedy inferenceby estimating how well each candidate semanticsm?k would explain other texts and at each step se-lecting k (and m?k) which explains them best.The algorithm, presented in figure 23, con-structs an ordering of texts n = (n1,..., nK)2Note that checking for satisfiability may be expensive orintractable depending on the formalism.3We slightly abuse notation by using set operations withthe lists n and m?
as arguments.
Also, for all the documentindices j we use j /?
S to denote j ?
{1,...,K}\S.1: n := (), m?
:= ()2: for i := 1 : K ?
1 do3: for j /?
n do4: m?j := argmaxmj P (mj ,wj |m?
)5: end for6: ni := argmaxj /?n P (m?j ,wj |m?)???k/?n?
{j}maxmk P (mk,wk|m?, m?j)7: m?i := m?ni8: end for9: nK := {1,...,K}\n10: m?K := argmaxmnK P (mnK ,wnK |m?
)Figure 2: The approximate inference algorithm.and corresponding meaning representations m?
=(m?1,...,m?K), where m?k is the predicted mean-ing representation of text wnk .
It starts with anempty ordering n = () and an empty list of mean-ings m?
= () (line 1).
Then it iteratively pre-dicts meaning representations m?j conditioned onthe list of semantics m?
= (m?1,...,m?i?1) fixedon the previous stages and does it for all the re-maining texts wj (lines 3-5).
The algorithm se-lects a single meaning m?j which maximizes theprobability of all the remaining texts and excludesthe text j from future consideration (lines 6-7).Though the semantics mk (k /?
n?
{j}) used inthe estimates (line 6) can be inconsistent with eachother, the final list of meanings m?
is guaranteedto be consistent.
It holds because on each iterationwe add a single meaning m?ni to m?
(line 7), andm?ni is guaranteed to be consistent with m?, as thesemantics m?ni was conditioned on the meaningm?
during inference (line 4).An important aspect of this algorithm is that un-like usual greedy inference, the remaining (?fu-ture?)
texts do affect the choice of meaning rep-resentations made on the earlier stages.
As soonas semantics m?k are inferred for every k, we findourselves in the set-up of learning with unalignedsemantic states considered in (Liang et al, 2009).The induced alignments a1,...,aK of semanticsm?
to texts w1,...,wK at the same time inducealignments between the texts.
The problem of pro-ducing multiple sequence alignment, especially inthe context of sentence alignments, has been ex-tensively studied in NLP (Barzilay and Lee, 2003).In this paper, we use semantic structures as a pivotfor finding the best alignment in the hope that pres-ence of meaningful text alignments will improvethe quality of the resulting semantic structures byenforcing a form of agreement between them.9613 A Model of SemanticsIn this section we redescribe the semantics-textcorrespondence model (Liang et al, 2009) with anextension needed to model examples with latentstates, and also explain how the inference algo-rithm defined in section 2 can be applied to thismodel.3.1 Model definitionLiang et al (2009) considered a scenario whereeach text was annotated with a world state, eventhough alignment between the text and the statewas not observable.
This is a weaker form ofsupervision than the one traditionally consideredin supervised semantic parsing, where the align-ment is also usually provided in training (Chen andMooney, 2008; Zettlemoyer and Collins, 2005).Nevertheless, both in training and testing theworld state is observable, and the alignment andthe text are conditioned on the state during infer-ence.
Consequently, there was no need to modelthe distribution of the world state.
This is differ-ent for us, and we augment the generative story byadding a simplistic world state generation step.As explained in the introduction, the worldstates s are represented by sets of records (see theblock in the middle of figure 1 for an example ofa world state).
Each record is characterized by arecord type t ?
{1,..., T}, which defines the set offields F (t).
There are n(t) records of type t andthis number may change from document to docu-ment.
For example, there may be more than a sin-gle record of type wind speed, as they may referto different time periods but all these records havethe same set of fields, such as minimal, maximaland average wind speeds.
Each field has an asso-ciated type: in our experiments we consider onlycategorical and integer fields.
We write s(t)n,f = vto denote that n-th record of type t has field f setto value v.Each document k verbalizes a subset of the en-tire world state, and therefore semantics mk ofthe document is an assignment to |mk| verbalizedfields: ?|mk|q=1 (s(tq)nq ,fq= vq), where tq, nq, fq arethe verbalized record types, records and fields, re-spectively, and vq is the assigned field value.
Theprobability of meaning mk then equals the prob-ability of this assignment with other state vari-ables left non-observable (and therefore marginal-ized out).
In this formalism checking for con-tradiction is trivial: two meaning representationsFigure 3: The semantics-text correspondencemodel with K documents sharing the same latentsemantic state.contradict each other if they assign different val-ues to the same field of the same record.The semantics-text correspondence model de-fines a hierarchical segmentation of text: first, itsegments the text into fragments discussing differ-ent records, then the utterances corresponding toeach record are further segmented into fragmentsverbalizing specific fields of that record.
An exam-ple of a segmented fragment is presented in fig-ure 4.
The model has a designated null-recordwhich is aligned to words not assigned to anyrecord.
Additionally there is a null-field in eachrecord to handle words not specific to any field.In figure 3 the corresponding graphical model ispresented.
The formal definition of the model fordocuments w1,...,wK sharing a semantic state isas follows:?
Generation of world state s:?
For each type ?
?
{1,..., T} choose a number ofrecords of that type n(?)
?
Unif(1,..., nmax).?
For each record s(?
)n , n ?
{1, .., n(?)}
choosefield values s(?
)nf for all fields f ?
F(?)
from thetype-specific distribution.?
Generation of the verbalizations, for each documentwk, k ?
{1,...,K}:4?
Record Types: Choose a sequence of verbalizedrecord types t = (t1,..., t|t|) from the first-orderMarkov chain.?
Records: For each type ti choose a verbalizedrecord ri from all the records of that type: l ?Unif(1,..., n(?
)), ri := s(ti)l .?
Fields: For each record ri choose a sequence ofverbalized fields f i = (fi1,..., fi|fi|) from thefirst-order Markov chain (fij ?
F (ti)).?
Length: For each field fij , choose length cij ?Unif(1,..., cmax).?
Words: Independently generate cij words fromthe field-specific distribution P (w|fij , rifij ).4We omit index k in the generative story and figure 3 tosimplify the notation.962Figure 4: A segmentation of a text fragment into records and fields.Note that, when generating fields, the Markovchain is defined over fields and the transition pa-rameters are independent of the field values rifij .On the contrary, when drawing a word, the distri-bution of words is conditioned on the value of thecorresponding field.The form of word generation distributionsP (w|fij , rifij ) depends on the type of the fieldfi,j .
For categorical fields, the distribution ofwords is modeled as a distinct multinomial foreach field value.
Verbalizations of numerical fieldsare generated via a perturbation on the field valuerifij : the value rifij can be perturbed by eitherrounding it (up or down) or distorting (up or down,modeled by a geometric distribution).
The param-eters corresponding to each form of generation areestimated during learning.
For details on theseemission models, as well as for details on model-ing record and field transitions, we refer the readerto the original publication (Liang et al, 2009).In our experiments, when choosing a worldstate s, we generate the field values independently.This is clearly a suboptimal regime as often thereare very strong dependencies between field val-ues: e.g., in the weather domain many recordtypes contain groups of related fields defining min-imal, maximal and average values of some param-eter.
Extending the method to model, e.g., pair-wise dependencies between field values is rela-tively straightforward.As explained above, semantics of a textm is de-fined by the assignment of state variables s. Anal-ogously, an alignment a between semantics mand a text w is represented by all the remaininglatent variables: by the sequence of record typest = (t1,..., t|t|), choice of records ri for each ti,the field sequence f i and the segment length cijfor every field fij .3.2 Learning and inferenceWe select the model parameters ?
by maximiz-ing the marginal likelihood of the data, wherethe data D is given in the form of groups w ={w1,...,wK} sharing the same latent state:5max?
?w?D?sP (s)?k?r,f ,cP (r,f , c,wk|s, ?
).To estimate the parameters, we use theExpectation-Maximization algorithm (Dempsteret al, 1977).
When the world state is observ-able, learning does not require any approxima-tions, as dynamic programming (a form of theforward-backward algorithm) can be used to in-fer the posterior distribution on the E-step (Lianget al, 2009).
However, when the state is latent,dependencies are not local anymore, and approxi-mate inference is required.We use the algorithm described in section 2 (fig-ure 2) to infer the state.
In the context of thesemantics-text correspondence model, as we dis-cussed above, semantics m defines the subset ofadmissible world states.
In order to use the algo-rithm, we need to understand how the conditionalprobabilities of the form P (m?|m) are computed,as they play the key role in the inference proce-dure (see equation (2)).
If there is a contradiction(m?
?m) then P (m?|m) = 0, conversely, if m?is subsumed by m (m ?
m?)
then this proba-bility is 1.
Otherwise, P (m?|m) equals the prob-ability of new assignments ?|m?\m|q=1 (s(t?q)n?q ,f ?q= v?q)(defined by m?\m) conditioned on the previouslyfixed values of s (given by m).
Summarizing,when predicting the most likely semantics m?j(line 4), for each span the decoder weighs alter-natives of either (1) aligning this span to the pre-viously induced meaning m?, or (2) aligning it toa new field and paying the cost of generation of itsvalue.The exact computation of the most probable se-mantics (line 4 of the algorithm) is intractable, andwe have to resort to an approximation.
Insteadof predicting the most probable semantics m?j wesearch for the most probable pair (a?j , m?j), thusassuming that the probability mass is mostly con-centrated on a single alignment.
The alignment aj5For simplicity, we assume here that all the examples areunlabeled.963is then discarded and not used in any other compu-tations.
Though the most likely alignment a?j fora fixed semantic representation m?j can be foundefficiently using a Viterbi algorithm, computingthe most probable pair (a?j , m?j) is still intractable.We use a modification of the beam search algo-rithm, where we keep a set of candidate meanings(partial semantic representations) and compute analignment for each of them using a form of theViterbi algorithm.As soon as the meaning representations m?
areinferred, we find ourselves in the set-up studiedin (Liang et al, 2009): the state s is no longerlatent and we can run efficient inference on theE-step.
Though some fields of the state s maystill not be specified by m?, we prohibit utterancesfrom aligning to these non-specified fields.On the M-step of EM the parameters are es-timated as proportional to the expected marginalcounts computed on the E-step.
We smooth thedistributions of values for numerical fields withconvolution smoothing equivalent to the assump-tion that the fields are affected by distortion in theform of a two-sided geometric distribution withthe success rate parameter equal to 0.67.
We useadd-0.1 smoothing for all the remaining multino-mial distributions.4 Empirical EvaluationIn this section, we consider the semi-supervisedset-up, and present evaluation of our approach onon the problem of aligning weather forecast re-ports to the formal representation of weather.4.1 ExperimentsTo perform the experiments we used a subsetof the weather dataset introduced in (Liang etal., 2009).
The original dataset contains 22,146texts of 28.7 words on average, there are 12types of records (predicates) and 36.0 records perforecast on average.
We randomly chose 100texts along with their world states to be used asthe labeled data.6 To produce groups of non-contradictory texts we have randomly selected asubset of weather states, represented them in a vi-sual form (icons accompanied by numerical and6In order to distinguish from completely unlabeled exam-ples, we refer to examples labeled with world states as la-beled examples.
Note though that the alignments are not ob-servable even for these labeled examples.
Similarly, we callthe models trained from this data supervised though full su-pervision was not available.symbolic parameters) and then manually anno-tated these illustrations.
These newly-producedforecasts, when combined with the original texts,resulted in 259 groups of non-contradictory texts(650 texts, 2.5 texts per group).
An example ofsuch a group is given in figure 1.The dataset is relatively noisy: there are incon-sistencies due to annotation mistakes (e.g., numberdistortions), or due to different perception of theweather by the annotators (e.g., expressions suchas ?warm?
or ?cold?
are subjective).
The overlapbetween the verbalized fields in each group wasestimated to be below 35%.
Around 60% of fieldsare mentioned only in a single forecast from agroup, consequently, the texts cannot be regardedas paraphrases of each other.The test set consists of 150 texts, each corre-sponding to a different weather state.
Note thatduring testing we no longer assume that docu-ments share the state, we treat each document inisolation.
We aimed to preserve approximately thesame proportion of new and original examples aswe had in the training set, therefore, we combined50 texts originally present in the weather datasetwith additional 100 newly-produced texts.
We an-notated these 100 texts by aligning each line to oneor more records,7 whereas for the original texts thealignments were already present.
Following Lianget al (2009) we evaluate the models on how wellthey predict these alignments.When estimating the model parameters, we fol-lowed the training regime prescribed in (Liang etal., 2009).
Namely, 5 iterations of EM with a basicmodel (with no segmentation or coherence mod-eling), followed by 5 iterations of EM with themodel which generates fields independently and,at last, 5 iterations with the full model.
Onlythen, in the semi-supervised learning scenarios,we added unlabeled data and ran 5 additional it-erations of EM.Instead of prohibiting records from crossingpunctuation, as suggested by Liang et al (2009),in our implementation we disregard the words notattached to specific fields (attached to the null-field, see section 3.1) when computing spans ofrecords.
To speed-up training, only a single recordof each type is allowed to be generated when run-ning inference for unlabeled examples on the E-7The text was automatically tokenized and segmented intolines, with line breaks at punctuation characters.
Informationabout the line breaks is not used during learning and infer-ence.964P R F1Supervised BL 63.3 52.9 57.6Semi-superv BL 68.8 69.4 69.1Semi-superv, non-contr 78.8 69.5 73.9Supervised UB 69.4 88.6 77.9Table 1: Results (precision, recall and F1) on theweather forecast dataset.step of the EM algorithm, as it significantly re-duces the search space.
Similarly, though we pre-served all records which refer to the first time pe-riod, for other time periods we removed all therecords which declare that the corresponding event(e.g., rain or snowfall) is not expected to happen.This preprocessing results in the oracle recall of93%.We compare our approach (Semi-superv, non-contr) with two baselines: the basic supervisedtraining on 100 labeled forecasts (Supervised BL)and with the semi-supervised training which disre-gards the non-contradiction relations (Semi-supervBL).
The learning regime, the inference proce-dure and the texts for the semi-supervised baselinewere identical to the ones used for our approach,the only difference is that all the documents weremodeled as independent.
Additionally, we reportthe results of the model trained with all the 750texts labeled (Supervised UB), its scores can beregarded as an upper bound on the results of thesemi-supervised models.
The results are reportedin table 1.4.2 DiscussionOur training strategy results in a substantiallymore accurate model, outperforming both the su-pervised and semi-supervised baselines.
Surpris-ingly, its precision is higher than that of the modeltrained on 750 labeled examples, though admit-tedly it is achieved at a very different recall level.The estimation of the model with our approachtakes around one hour on a standard desktop PC,which is comparable to 40 minutes required totrain the semi-supervised baseline.In these experiments, we consider the problemof predicting alignment between text and the cor-responding observable world state.
The directevaluation of the meaning recognition (i.e.
se-mantic parsing) accuracy is not possible on thisdataset, as the data does not contain informationwhich fields are discussed.
Even if it would pro-value top words0-25 clear, small, cloudy, gaps, sun25-50 clouds, increasing, heavy, produce, could50-75 cloudy, mostly, high, cloudiness, breezy75-100 amounts, rainfall, inch, new, possiblyTable 2: Top 5 words in the word distribution forfield mode of record sky cover, function words andpunctuation are omitted.vide this information, the documents do not ver-balize the state at the necessary granularity levelto predict the field values.
For example, it is notpossible to decide to which bucket of the field skycover the expression ?cloudy?
refers to, as it has arelatively uniform distribution across 3 (out of 4)buckets.
The problem of predicting text-meaningalignments is interesting in itself, as the extractedalignments can be used in training of a statisti-cal generation system or information extractors,but we also believe that evaluation on this prob-lem is an appropriate test for the relative compar-ison of the semantic analyzers?
performance.
Ad-ditionally, note that the success of our weakly-supervised scenario indirectly suggests that themodel is sufficiently accurate in predicting seman-tics of an unlabeled text, as otherwise there wouldbe no useful information passed in between se-mantically overlapping documents during learningand, consequently, no improvement from sharingthe state.8To confirm that the model trained by our ap-proach indeed assigns new words to correct fieldsand records, we visualize top words for the fieldcharacterizing sky cover (table 2).
Note that thewords ?sun?, ?cloudiness?
or ?gaps?
were not ap-pearing in the labeled part of the data, but seem tobe assigned to correct categories.
However, cor-relation between rain and overcast, as also notedin (Liang et al, 2009), results in the wrong assign-ment of the rain-related words to the field valuecorresponding to very cloudy weather.5 Related WorkProbably the most relevant prior work is an ap-proach to bootstrapping lexical choice of a gen-eration system using a corpus of alternative pas-8We conducted preliminary experiments on synthetic datagenerated from a random semantic-correspondence model.Our approach outperformed the baselines both in predicting?text?-state correspondence and in the F1 score on the pre-dicted set of field assignments (?text meanings?
).965sages (Barzilay and Lee, 2002), however, in theirwork all the passages were annotated with un-aligned semantic expressions.
Also, they as-sumed that the passages are paraphrases of eachother, which is stronger than our non-contradictionassumption.
Sentence and text alignment hasalso been considered in the related context ofparaphrase extraction (see, e.g., (Dolan et al,2004; Barzilay and Lee, 2003)) but this priorwork did not focus on inducing or learning se-mantic representations.
Similarly, in informationextraction, there have been approaches for pat-tern discovery using comparable monolingual cor-pora (Shinyama and Sekine, 2003) but they gener-ally focused only on discovery of a single patternfrom a pair of sentences or texts.Radev (2000) considered types of potential rela-tions between documents, including contradiction,and studied how this information can be exploitedin NLP.
However, this work considered primarilymulti-document summarization and question an-swering problems.Another related line of research in machinelearning is clustering or classification with con-straints (Basu et al, 2004), where supervision isgiven in the form of constraints.
Constraints de-clare which pairs of instances are required to beassigned to the same class (or required to be as-signed to different classes).
However, we are notaware of any previous work that generalized thesemethods to structured prediction problems, as triv-ial equality/inequality constraints are probably toorestrictive, and a notion of consistency is requiredinstead.6 Summary and Future WorkIn this work we studied the use of weak supervi-sion in the form of non-contradictory relations be-tween documents in learning semantic represen-tations.
We argued that this type of supervisionencodes information which is hard to discover inan unsupervised way.
However, exact inferencefor groups of documents with overlapping seman-tic representation is generally prohibitively expen-sive, as the shared latent semantics introduces non-local dependences between semantic representa-tions of individual documents.
To combat it, weproposed a simple iterative inference algorithm.We showed how it can be instantiated for thesemantics-text correspondence model (Liang etal., 2009) and evaluated it on a dataset of weatherforecasts.
Our approach resulted in an improve-ment over the scores of both the supervised base-line and of the traditional semi-supervised learn-ing.There are many directions we plan on inves-tigating in the future for the problem of learn-ing semantics with non-contradictory relations.
Apromising and challenging possibility is to con-sider models which induce full semantic represen-tations of meaning.
Another direction would beto investigate purely unsupervised set-up, thoughit would make evaluation of the resulting methodmuch more complex.
One potential alternativewould be to replace the initial supervision with aset of posterior constraints (Graca et al, 2008) orgeneralized expectation criteria (McCallum et al,2007).AcknowledgementsThe authors acknowledge the support of the Excel-lence Cluster on Multimodal Computing and Inter-action (MMCI).
Thanks to Alexandre Klementiev,Alexander Koller, Manfred Pinkal, Dan Roth, Car-oline Sporleder and the anonymous reviewers fortheir suggestions, and to Percy Liang for answer-ing questions about his model.ReferencesRegina Barzilay and Lillian Lee.
2002.
Bootstrap-ping lexical choice via multiple-sequence align-ment.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 164?171.Regina Barzilay and Lillian Lee.
2003.
Learningto paraphrase: An unsupervised approach usingmultiple-sequence alignment.
In Proceedings of theConference on Human Language Technology andNorth American chapter of the Association for Com-putational Linguistics (HLT-NAACL).Sugatu Basu, Arindam Banjeree, and RaymondMooney.
2004.
Active semi-supervision for pair-wise constrained clustering.
In Proc.
of the SIAMInternational Conference on Data Mining (SDM),pages 333?344.A.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In COLT: Pro-ceedings of the Workshop on Computational Learn-ing Theory, Morgan Kaufmann Publishers, pages209?214.Xavier Carreras and Lluis Marquez.
2005.
Introduc-tion to the conll-2005 shared task: Semantic role la-beling.
In Proceedings of CoNLL-2005, Ann Arbor,MI USA.966David L. Chen and Raymond L. Mooney.
2008.
Learn-ing to sportcast: A test of grounded language acqui-sition.
In Proc.
of International Conference on Ma-chine Learning, pages 128?135.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithms.
Journal of the Royal Statistical So-ciety.
Series B (Methodological), 39(1):1?38.P.
Diaconis and B. Efron.
1983.
Computer-intensivemethods in statistics.
Scientific American, pages116?130.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.In Proceedings of the Conference on ComputationalLinguistics (COLING), pages 350?356.Ruifang Ge and Raymond J. Mooney.
2005.
A sta-tistical semantic parser that integrates syntax andsemantics.
In Proceedings of the Ninth Confer-ence on Computational Natural Language Learning(CONLL-05), Ann Arbor, Michigan.Joao Graca, Kuzman Ganchev, and Ben Taskar.
2008.Expectation maximization and posterior constraints.Advances in Neural Information Processing Systems20 (NIPS).Zellig Harris.
1968.
Mathematical structures of lan-guage.
Wiley.Rohit J. Kate and Raymond J. Mooney.
2007.
Learn-ing language semantics from ambigous supervision.In Association for the Advancement of Artificial In-telligence (AAAI), pages 895?900.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less super-vision.
In Proc.
of the Annual Meeting of the Asso-ciation for Computational Linguistics and Interna-tional Joint Conference on Natural Language Pro-cessing (ACL-IJCNLP).Andrew McCallum, Gideon Mann, and GregoryDruck.
2007.
Generalized expectation criteria.Technical Report TR 2007-60, University of Mas-sachusetts, Amherst, MA.Raymond J. Mooney.
2007.
Learning for semanticparsing.
In Proceedings of the 8th InternationalConference on Computational Linguistics and Intel-ligent Text Processing, pages 982?991.Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.1999.
Loopy belief propagation for approximate in-ference: An empirical study.
In Proc.
of Uncertaintyin Artificial Intelligence (UAI), pages 467?475.Judea Pearl.
1982.
Reverend bayes on inference en-gines: A distributed hierarchical approach.
In Proc.of the National Conference on Artificial Intelligence(AAAI), pages 133?136.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, (EMNLP-09).Dragomir Radev.
2000.
A common theory of infor-mation fusion from multiple text sources step one:Cross-document structure.
In 1st SIGdial Workshopon Discourse and Dialogue, pages 74?83.Yusuke Shinyama and Satoshi Sekine.
2003.
Para-phrase acquisition for information extraction.
InProceedings of Second International Workshop onParaphrasing (IWP2003), pages 65?71.Benjamin Snyder and Regina Barzilay.
2007.Database-text alignment via structured multilabelclassification.
In Proceedings of International JointConference on Artificial Intelligence (IJCAI-05),pages 1713?1718.J.
Weeds and W. Weir.
2005.
Co-occurrence retrieval:A flexible framework for lexical distributional simi-larity.
Computational Linguistics, 31(4):439?475.Luke Zettlemoyer and Michael Collins.
2005.
Learn-ing to map sentences to logical form: Structuredclassification with probabilistic categorial grammar.In Proceedings of the Twenty-first Conference onUncertainty in Artificial Intelligence, Edinburgh,UK, August.967
