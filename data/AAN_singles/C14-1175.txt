Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1854?1864, Dublin, Ireland, August 23-29 2014.Rediscovering Annotation Projection for Cross-Lingual Parser InductionJ?org TiedemannUppsala UniversityDepartment of Linguistics and Philologyfirstname.lastname@lingfil.uu.seAbstractPrevious research on annotation projection for parser induction across languages showed onlylimited success and often required substantial language-specific post-processing to fix inconsis-tencies and to lift the performance onto a useful level.
Model transfer was introduced as anotherquite successful alternative and much research has been devoted to this paradigm recently.
In thispaper, we revisit annotation projection and show that the previously reported results are mainlyspoiled by the flaws of evaluation with incompatible annotation schemes.
Lexicalized parserscreated on projected data are especially harmed by such discrepancies.
However, recently de-veloped cross-lingually harmonized annotation schemes remove this obstacle and restore theabilities of syntactic annotation projection.
We demonstrate this by applying projection strate-gies to a number of European languages and a selection of human and machine-translated data.Our results outperform the simple direct transfer approach by a large margin and also pave theroad to cross-lingual parsing without gold POS labels.1 IntroductionLinguistic resources and tools exist only for a minority of the world?s languages.
However, many NLPapplications require robust tools and the development of language-specific resources is expensive andtime consuming.
Many of the common tools are based on data-driven techniques and they often re-quire strong supervision to achieve reasonable results for real world applications.
Fully unsupervisedtechniques are not a good alternative yet for tasks like data-driven syntactic parsing and, therefore, cross-lingual learning has been proposed as a possible solution to quickly create initial tools for otherwiseunsupported languages (Ganchev and Das, 2013).In syntactic parsing, two main strategies have been explored in cross-lingual learning: annotation pro-jection and model transfer.
The first strategy relies on parallel corpora and automatic word alignmentthat make it possible to map linguistics annotation from a source language to a new target language(Yarowsky et al., 2001; Hwa et al., 2005; T?ackstr?om et al., 2013a).
The basic idea is that existing toolsand models are used to process the source side of a parallel corpus and that projection heuristics guidedby alignment can be used to transfer the automatic annotation to the target language text.
Using theprojected annotation assuming that it is sufficiently correct, models can then be trained for the targetlanguage.
However, directly projecting syntactic structure results in a rather poor performance whenapplied to resources that were developed separately for individual languages (Hwa et al., 2005).
Exten-sive additional post-processing in form of transformation rules is required to achieve reasonable scores.Furthermore, incompatible tagsets make it impossible to directly transfer labeled annotation to a newlanguage and previous literature on cross-lingual parsing via annotation projection is, therefore, boundto the evaluation of unlabeled attachment scores (UAS).
Less frequent, but also possible, is the scenariowhere the source side of the corpus contains manual annotation (Agi?c et al., 2012).
This addresses theproblem created by projecting noisy annotations, but it presupposes parallel corpora with manual anno-tation, which are rarely available.
Additionally, the problem of incompatible annotation still remains.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1854The second strategy, model transfer instead relies on universal features and the transfer of modelparameters from one language to another.
The main idea is to reduce the need of language-specificinformation, e.g.
using delexicalized parsers that ignore lexical information.
Drawing from a harmonizedPOS tagset (Petrov et al., 2012), transfer models have been used for a variety of languages.
The advantageover annotation projection approaches is that no parallel data is required (at least in the basic settings)and that training can be performed on gold standard annotation.
However, it requires a common featurerepresentation across languages (McDonald et al., 2013), which can be a strong bottleneck.
There arealso several extensions to improve the performance of transfer models.
One idea is to use multiplesource languages to increase the statistical ground for the learning process (McDonald et al., 2011;Naseem et al., 2012), a strategy that can also be used in the case of annotation projection.
Another ideais to enhance models by cross-lingual word clusters (T?ackstr?om et al., 2012) and to use target languageadaptation techniques with prior knowledge of language properties and their relatedness when usingmultiple sources in training (T?ackstr?om et al., 2013b).
Based on the success of these techniques, modeltransfer has dominated recent research on cross-lingual learning.In this paper, we return to annotation projection as a powerful tool for porting syntactic parsers to newlanguages.
Building on the availability of cross-lingually harmonized data sets, we show that projectionperforms well and outperforms direct transfer models by a large margin in contrast to previous findingson projection with incompatible treebanks.
In the following, we first revisit the projection algorithmsproposed earlier and discuss issues with transferring labels across languages.
After that we report ex-perimental results with various settings using human translations and machine-translated data.
Finally,we also look at parsing results without gold standard POS labeling, which is ultimately required whenporting parsers to new languages that lack appropriate resources.2 Syntactic Annotation ProjectionHwa et al.
(2005) propose a direct projection algorithm for syntactic dependency annotation.
The algo-rithm defines several heuristics to map source side annotations to target languages using word alignmentsin a parallel corpus.
The main difficulties with the projection arise with none-one-to-one links and un-aligned tokens.
Each of the following alignment types are adressed by the algorithm separately:one-to-one: Copy relations R(si, sj) between source words siand sjto relations R(tx, ty)if siis aligned to txand sjis aligned to tyand nothing else.unaligned source: Create an empty (dummy) word in the target language sentence that takesall relations (incoming and outgoing arcs) of the unaligned source language word.one-to-many: Create an empty target word tzthat acts as the parent of all aligned targetwords tx, .., ty.
Remove the alignments between siand tx, .., tyand align sito the newempty word tzinstead.many-to-one: Delete all alignments between si, .., sjand txexcept the link between the headof si, .., sjand tx.many-to-many: Perform the rule for one-to-many alignments first and then perform the rulefor many-to-one alignments.unaligned target: Remove all unaligned target words.In contrast to Hwa et al.
(2005), we are also interested in labeled attachment and the projection of POSannotation.
Therefore, we copy labels through the alignment using the heuristics listed above.
Figure 1illustrates some of the cases discussed.
There are some important implications due to the treatment ofcomplex alignment types.
The direct projection algorithm frequently creates dummy nodes and relationsthat have no correspondence in the source language.
Here, we need to make some decisions on how toproject the annotation from source to target sentences.First of all, we decided to name all additional tokens created by the algorithm with the same stringDUMMY.
An alternative would be to invent unique names for each newly created token within eachsentence but this would blow up the vocabulary and would not add useful information to the data.1855src1    src2   src3    src4trg1    trg2    trg3DUMMYlabel 1label 2label 3label 1label 2label 3pos1      pos2      pos3      pos4pos2      pos1      pos3        pos4src1    src2   src3    src4trg1    trg2    trg3label 1label 2label 3label 1label 2pos2      pos1      pos4pos1      pos2      pos3      pos4src1    src2    src3trg1    trg2     trg3    trg4DUMMYdummydummylabel 2label 2label 1label 1pos1      dummy   dummy     pos2      pos3pos1      pos2      pos3Figure 1: Annotation projection heuristics for special alignment types: Unaligned source words (leftimage), many-to-one alignments (center), one-to-many alignments (right image).The second problem is related to the auxiliary relations that are created when treating one-to-manyalignments.
In these cases, multiple words are attached to newly created dummy nodes.
However, nocorresponding labels exist in the source language that would allow us to infer appropriate labels forthese additional attachments.
One possibility would be to use a specific label from the existing setof dependency relations, for example ?mwe?.
However, one-to-many alignments do not always referto proper multi-word expressions but often represent other grammatical or structural differences likethe relation between the English preposition ?of?
which is linked together with the determiner ?the?to the German determiner ?der?
in sentences like ?Resumption OF THE session?
translated to German?Wiederaufnahme DER Sitzung?.
Therefore, we decided to label these additional dependency with a newunique label dummy instead of selecting an existing one.Yet another problem arises with the projection of POS annotation.
Similar to the labeling of depen-dency relations, we have to decide how to transfer POS tags to the target language in cases of one-to-many alignments.
In our implementation, we transfer the source language label only to the newlycreated dummy node which dominates all target language words linked to the source language wordin the projected dependency tree.
The daughter nodes, however, obtain the label dummy even as theirPOS annotation.
Alternatively, we may project the POS tag to all linked tokens according to the originalalignment but our guiding principle is to resolve link ambiguity first using the heuristics in the directprojection algorithm and then to transfer annotation.src1    src2   src3    src4trg1    trg2    trg3label 1label 2label 3pos1      pos2      pos3      pos4?src1    src2   src3    src4trg1    trg2    trg3label 1label 2DUMMYdummydummylabel 2label 3label 3DUMMYlabel 1pos1      dummy     pos2      pos4           pos3pos1      pos2      pos3      pos4Figure 2: A complex example for annotation projection with many-to-many word alignments.Finally, we also need to look at the interaction between the various projection heuristics.
Figure 2illustrates a complex case with many-to-many word alignments.
Resolving the alignment ambiguity isnot entirely straightforward.
In our implementation, we start by looking at all one-to-many alignmentsand resolve them according to the definitions of the projection algorithm.
In our example, this createsa DUMMY node that dominates target words trg1 and trg2 and links between src1 and (trg1,trg2) aredeleted.
We label the new relations with dummy.
The next step considers many-to-one alignments,1856DET DET NOUN VERB ADP NOUN CONJ ADP DET NOUN ADJ .Tous ses produits sont de qualite?
et d?
une fraicheur exemplaires .All his products are high- quality DUMMY and DUMMY a cold mullet DUMMY copies .DET DET NOUN VERB DUMMY ADP NOUN CONJ ADP DET DUMMY DUMMY NOUN ADJ .detpossnsubjrootadpmodadpobjccconjdetadpobjamodpdetpossnsubjrootadpmodadpobjccconjdetDUMMYDUMMYadpobjamodDUMMYpFigure 3: A complete projection example from a translated treebank including transitive relations overa DUMMY node that can safely be collapsed (which also removes the non-projectivity of the projectedtree).
The resulting relation between quality and high- will be labeled as adpobj.
Note that projectionerrors appear due to the ambiguous alignments between de qualit?e and high- quality.
Boxes indicatephrases that are translated as units by the SMT engine.which, using the remaining links, is source words (src2,src3) aligned to trg2.
According to the algorithmwe delete the link between src3 and trg2 (because src2 dominates src3 in the source language tree) andproceed.
This, however, creates an unaligned source language word (src3), which we treat in the nextstep.
The unaligned token gives rise to the second DUMMY word, which is attached to trg3 as the resultof the alignment between src4 and trg3 and the relation between src4 and src3.
Finally, we can mapall other relations according to the one-to-one alignment rule.
This, however, creates a conflict with thealready existing dummy relation between the first DUMMY word and trg2.
Mapping according to theone-to-one rule turns the relation around and attaches the DUMMY word to trg2 and labels the relationwith label 1.
Now, we could remove the second DUMMY node according to the rule about unalignedtarget language words.
However, this rule should not apply to these special nodes as they may play acrucial role to keep elements connected in the final target language tree.Another difficult case, which is not illustrated here, is when many-to-one alignments need to be re-solved but the aligned source language words are siblings in the syntactic tree and no unique head canbe identified.
In our implementation, we randomly pick a node but more linguistically informed guesseswould probably be better.
Yet another difficult decision is the placement of the DUMMY nodes.
Wedecided to put them next to the head node they attach to.
Other heuristics are possible and all placementsgreatly influence the projectivity of the resulting tree structure.
One final adjustment that we apply isthe removal of unary productions over DUMMY nodes.
We collaps all relations that run with single at-tachments via DUMMY nodes to reduce the number of these uninformative tokens.
This may also havepositive effects on projectivity as we can see in the example in Figure 3.3 Machine-Translated TreebanksAnother strategy for annotation projection is based on automatic translation.
Machine translation modelscan be used to create synthetic parallel data for projecting annotations from one language to another(Tiedemann et al., 2014).
Recent advances in machine translation (MT) are now making this a realisticalternative.
The use of direct treebank translation instead of existing parallel corpora has several impor-tant advantages.
First of all, we skip the use of an error-prone annotation step when producing the sourcelanguage side of the training data.
Starting with a noisy source language annotation, we accumulate twosources of errors in annotation projection.
However, with direct translation we can start with the goldstandard annotation provided in the original treebank.
Furthermore, we avoid problems of domain shiftswhich is typically the case when applying a parser trained on one domain to texts (a parallel corpus in1857DELEXICALIZEDDE EN ES FR SVDE 62.71 43.20 46.09 46.09 50.64EN 46.62 77.66 55.65 56.46 57.68ES 44.03 46.73 68.21 57.91 53.82FR 43.91 46.75 59.65 67.51 52.01SV 50.69 49.13 53.62 51.97 70.22MCDONALD ET AL.
(2013)DE EN ES FR SVDE 64.84 47.09 48.14 49.59 53.57EN 48.11 78.54 56.86 58.20 57.04ES 45.52 47.87 70.29 63.65 53.09FR 45.96 47.41 62.56 73.37 52.25SV 52.19 49.71 54.72 54.96 70.90Table 1: Baselines ?
labeled attachment score (LAS) for delexicalized transfer parsing; results of Mc-Donald et al.
(2013) included for reference.our case) coming from another domain.
Finally, we can also assume that machine translation producesoutput which is closer to the original text than most human translations will be in any parallel corpus.Even if this may sound as a disadvantage, for projection this is preferred.
Being close to the originalsource makes it easier to map annotation from one language to another as we expect a lower degree ofgrammatical and structural divergences that originate in the linguistic freedom human translators canapply.
Furthermore, common statistical MT models inherently provide alignments between words andphrases, which removes the requirement to apply yet another error-prone alignment step on the paral-lel data.
In the experiments below we, therefore, explore the translation strategy as yet another way ofapplying annotation projection.4 ExperimentsIn the following, we show our experimental results using annotation projection in several cross-lingualscenarios.
However, we start by presenting a delexicalized baseline, which is, to our knowledge, theonly previous model that has been presented for labeled dependency parsing across languages usingthe recently created Universal Treebank.
We will use this baseline as reference point even though ourprojection models are not directly comparable with delexicalized direct transfer models.
Note that allresults below are computed on the held-out test data sections of the Universal Treebank if not statedotherwise.4.1 Delexicalized BaselinesMcDonald et al.
(2013) present the Universal Treebank that comes with a harmonized syntactic anno-tation scheme across six languages.
This data set enables cross-lingual learning of labeled dependencyparsing models.
McDonald et al.
(2013) propose delexicalized models as a simple baseline for modeltransfer and present encouraging labeled attachment scores (LAS) especially for closely related lan-guages.
As a reference, we have created similar baseline models using the same data set but a slightlydifferent setup, which is compatible with the experiments we present later.
Table 1 summarizes the scoresin terms of LAS for all language pairs in the data set.1In our setup, we apply MaltParser (Nivre et al.,2006) and optimize feature models and learning parameters using MaltOptimizer (Ballesteros and Nivre,2012).
For all cross-lingual experiments (columns represent target languages we test on), we always usethe same feature model and parameters as we have found for the source language treebank.
Contrastingour models with the scores from McDonald et al.
(2013), we can see that they are comparable with somedifferences that are due to the tools and learning parameters they apply which are along the lines ofZhang and Nivre (2011).4.2 Annotation Projection with Human TranslationsOur first batch of projection experiments considers parallel data taken from the well-known Europarlcorpus, which is frequently used in research on statistical machine translation (SMT).
It contains largequantities of translated proceedings from the European Parliament for all but one language (namely1Note that we include punctuation in our evaluation.
Ignoring punctuation leads to slightly higher scores but we do notreport those numbers here.1858UAS on CoNLL dataDE EN ES SVDE ?
41.60 47.89 58.80EN 49.67 ?
51.44 58.66ES 46.14 37.78 ?
52.53SV 57.99 51.57 57.25 ?UAS on Universal Treebank dataDE EN ES SVDE ?
56.21 65.18 70.27EN 63.17 ?
68.02 70.40ES 61.98 56.16 ?
71.06SV 64.78 58.93 69.15 ?Table 2: Unlabeled attachment scores for projected treebank models; comparing CoNLL data to Univer-sal Treebank data for evaluation.Korean) that are included in the Universal Treebank v1.
The entire corpus (version 7) contains overtwo million sentences in each language and we use increasing amounts of the corpus to investigate theimpact on cross-lingual parser induction.
The corpus comes with automatic sentence alignments andis quite clean with respect to translation quality and sentence alignment accuracy.
It is, therefore, wellsuited for our initial experiments with annotation projection even though the domain does not necessarilymatch the one included in the treebank test sets.Another important prerequisite for annotation projection is word alignment.
Following the typicalsetup, we rely on automatic word alignment produced by models developed for statistical machine trans-lation.
Similar to Hwa et al.
(2005), we apply GIZA++ (Och and Ney, 2003) to align the corpus for alllanguage pairs in all translation directions using IBM model 4 Viterbi alignments.
In contrast to Hwa etal.
(2005), we then use symmetrization heuristics to combine forward and backward alignments, whichis common practice in the SMT community.
In particular, we apply the popular grow-diag-final-andheuristics as implemented in the Moses toolbox (Koehn et al., 2007).Let us first look at unlabeled attachment scores to compare results that can be achieved with harmo-nized annotation in contrast to the ones that we can see on the cross-lingually incompatible data from theCoNLL shared task (Buchholz and Marsi, 2006).
Table 2 lists the scores that we obtain when applyingour implementation of the direct projection algorithm.2As expected, the performance on the CoNLLdata is rather poor, which confirms the findings of Hwa et al.
(2005) even though our scores are signifi-cantly above their results without post-correction.
The scores on the Universal Treebank data, however,are up to about 20 UAS points higher than the corresponding results on CoNLL data but without any ofthe extensive post-processing transformations proposed by Hwa et al.
(2005).LAS on Universal Treebank dataDE EN ES FR SVDE ?
49.44 56.58 58.75 61.04EN 56.59 ?
60.07 62.78 62.15ES 54.04 47.90 ?
65.50 61.45FR 53.93 51.23 65.03 ?
58.71SV 56.13 49.18 60.82 62.00 ?data set: 40,000 sentences4850525456586062640  5  10  15  20  25  30  35  40nr of sentences (in thousands)deesfrsvFigure 4: Annotation projection on Europarl data: LAS for induced parser models.
The Figure to theright plots the learning curves for increasing training data for projections from English to the otherlanguages.Moreover, the real power of the harmonized annotation in the Universal Treebank comes from the pos-sibility to obtain attachment labels.
The table in Figure 4 shows the labeled attachment scores obtainedfor training on 40,000 sentences3of each language pair.
Next to the table in Figure 4 we also show the2We leave out French in this comparison as there is no French treebank in the CoNLL data.3Note that there may be repeated sentences in the data.1859with original source side annotationDE EN ES FR SVDE ?
53.02 54.96 58.20 59.65EN 52.93 ?
61.25 64.58 63.82ES 50.88 50.28 ?
66.17 60.48FR 50.46 53.95 65.46 ?
59.05SV 53.69 51.51 60.58 60.19 ?jackknifing for source side annotationDE EN ES FR SVDE ?
50.27 54.91 56.00 57.91EN 52.65 ?
61.28 63.86 63.72ES 49.19 50.04 ?
64.43 59.65FR 49.37 53.25 64.41 ?
57.78SV 54.83 50.25 60.27 60.04 ?Table 3: Cross-lingual parsing results (LAS) using translated treebanks (phrase-based model) and DCA-based annotation projection.
The table to the left contrasts the result with two-sample jackknifing experi-ments where the source side dependencies are created by automatically parsing each half of the treebankusing a model trained on the other half of the training data.learning curves for increasing amounts of training data using the example of data projected from Englishto other languages.
The figure illustrates that the LAS levels out at around 10,000 - 20,000 sentences andthis trend is essentially the same for all other languages as well.4.3 Annotation Projection with Synthetic Machine-Translated DataThe next possibility we would like to explore is the use of synthetic parallel data.
Annotating paralleldata with a statistical parser may lead to quite a lot of noise especially when the domain does not matchthe original training data.
Starting with noisy source language annotations, the projection algorithm maytransfer errors to the target language that can cause problems for the target language parsing model in-duced from that data.
Using machine translation and the original source language treebanks, we avoidthis kind of error propagation.
Furthermore, we suspect that human translations are more difficult toalign on the word level then machine translated data which are inherently based on word alignments and,therefore, tend to be more literal and consistent (Carpuat and Simard, 2012).
Using statistical MT asour translation model, we can also obtain such alignment as a given output from the decoding process,which makes it unnecessary to run yet another error-prone process such as automatic word alignment.Furthermore, the treebank data is too small to be used alone with generative statistical alignment mod-els.
Concatenating the data with larger parallel data would help but domain mismatches may, again,negatively influence the alignment performance.In the following, we show the cross-lingual scores obtained by translating all treebanks in the UniversalTreebank to all other languages.
We leave out Korean here again, because no SMT training data isincluded in Europarl for that language.
The translation models are trained on the entire Europarl corpususing a standard setup for phrase-based SMT and the Moses toolbox for training, tuning and decoding(Koehn et al., 2007).
For tuning we use MERT (Och, 2003) and the newstest 2012 data provided by theannual workshop on statistical machine translation.4and for language modeling, we use a combination ofEuroparl and News data provided from the same source.
The language model is a standard 5-gram modelestimated from the monolingual data using modified Kneser-Ney smoothing without pruning (applyingKenLM tools (Heafield et al., 2013)).Table 3 summarizes the labeled attachment scores obtained with our projection approach on syntheticmachine-translated data.
The main observation we can make here is that this approach is very robustwith respect to the noise introduced by the translation engine.
Automatic translation is a difficult task onits own but we still achieve results that are similar to the ones from the projection approach on humantranslated data.
Note that our training data is now much smaller5compared to the data sizes used inSection 4.2 and, still, we outperform those models in several cases.
This seems to prove that it can bea clear advantage to start with gold annotations in the source language and to have a close alignmentbetween source and target language.
An indication for this effect is illustrated by the contrastive jack-knifing experiments shown in Table 3.
The scores are generally lower with two minor exceptions.
Note4http://www.statmt.org/wmt145Most treebanks includes 2,000-5,000 sentences, except English with about 40,000 sentences.1860that this experiment does not cover domain shift problems.
Another trend that can be seen in our resultsis that some languages such as German are more difficult to translate to (which can be confirmed by theSMT literature) leading to lower cross-lingual parsing performance.4.4 The Impact of Word AlignmentCrucial for the success of annotation projection is the quality of the word alignment used to map in-formation from the source to the target language.
Not only alignment errors cause problems but alsoambiguous alignments can lead to projection difficulties as we have discussed before.
In the previoussections, we relied on symmetrized word alignments that are common in the SMT community, whichare based on Viterbi alignments created by the final IBM model 4 in the typical training pipeline.
Eventhough this is a reasonable setup for training phrase-based SMT models (as presented in the previoussection), the chosen symmetrization heuristics (grow-diag-final-and) may not be well suited for accurateannotation projection.
In particular, it is known that these heuristics focus on recall and tend to addmany additional links that may not be useful for our projection task and even lead to some confusion asdepicted in the example in Figure 3.In order to investigate the impact of word alignment, we, therefore, decided to look at other sym-50556065English Spanish French SwedishLASSource: German50556065German Spanish French SwedishLASSource: English50556065German English French SwedishLASSource: Spanish50556065German English Spanish SwedishLASSource: French50556065German English Spanish FrenchLASSource: Swedishtrg-to-srcsrc-to-trgintersectgrowgdfaFigure 5: The impact of word alignment symmetrization on projection and parsing accuracy.
src-to-trgand trg-to-src refer to the original directional Viterbi word alignments created by IBM model 4 in bothdirections; intersect refers to the intersection of both IBM 4 alignments; grow and gdfa (grow-diag-final-and) refer to popular symmetrization heuristics used in the SMT community.1861metrization heuristics and their effect on projection and the quality of the parser model trained on theprojected data.
For this, we return to the setup of projecting annotations on human translations usingthe Europarl corpus with the same settings as described in Section 4.2 (using 40,000 sentences for theprojection).
We now compare five different word alignments based on IBM model 4 trained on the entirecorpus for each language pair.
First of all, we look at the original directional word alignment from sourceto target language and vice versa.
We then include the intersection of these two directional link sets torepresent a symmetrization heuristics that produces very sparse but high precision word alignments.
Fi-nally, we also consider the grow heuristics that adds adjacent alignment points coming from the union ofdirectional alignment links to the sparse intersection of the same.
In this way, the resulting word align-ment covers most words while keeping precision at a rather high level.
All of these alignment types arethen contrasted with the grow-diag-final-and heuristics that we use in our default setup.Figure 5 plots the parsing performance across languages based on the projection with the variousalignment techniques listed above.
A general observation is that the differences are rather small in mostcases.
Projecting annotation using the direct correspondence assumption seems to be quite robust withrespect to alignment noise.
In our experiments, no specific tendencies can be identified that wouldallow to draw immediate conclusions and to give clear recommendations for our task.
Somewhat sur-prisingly we can see that the recall-oriented alignment heuristics (grow-diag-final-and) actually performquite well in many cases, leading either to the best performing model or to one that is very close to thebest result.
However, in some cases, these models fall behind the ones based on alignment intersec-tions (for instance Spanish-English) or directional word alignments (for example for Spanish-German,French-English, Swedish-German).
A striking difference can be seen in the annotations projected toGerman.
There, the target-to-source alignment performs pretty well and outperforms in two cases allother alignment types in the down-stream task.
Furthermore, the intersection falls far behind in three ofthese cases, which indicates that both alignment directions are probably very different from each otherleading to a very sparse word alignment when intersecting them.
One possible reason for the success ofthe directional alignment might be that it favors the mapping to a compounding language such as Ger-man that frequently requires many-to-one links.
However, the same effect cannot be seen for the othercompounding language in our test set, Swedish.4.5 Parsing Without Golden POS LabelsFor a truly unsupported language, it does not make sense to assume a high quality POS tagger.
Neverthe-less, most cross-lingual experiments test their performance on data with human annotated golden POSlabels.
This is similar to the tradition of monolingual parsing where test accuracy is measured with per-fect tokenization and completely correct POS annotation.
In practice, this would not be realistic wherenew data needs to be parsed without proper tagging and unambiguous tokenization.Direct transfer models are even more dependent on POS labels as those are the only source of infor-mation they can work with when making attachment decisions.
Annotation projection approaches, onthe other hand, are able to transfer POS information as well, which allows to train tagger models onprojected data.
In this section, we would like to test the feasibility of such an idea to see if we can trulyport a parser to a new language without additional assumptions.The first step is to train tagger models on our projected data sets.
For this, we use the translatedtreebanks and a simple word-by-word translation approach in which we translate single-word-phrasesonly in our standard SMT model.
The word-by-word translation model assures that we do not contam-inate the data with DUMMY nodes and labels even though the translation quality lags behind the morepowerful phrase-based models with larger translation options.
We train standard Markov taggers withsuffix backoff using HunPos (Hal?acsy et al., 2007) on each of the projected training data sets from theUniversal Treebank.
Table 4 summarizes the performance of all tagger models tested on the test sets inthe treebank.
The tagger all use the same universal POS tagset with its 12 labels as used in the UniversalTreebank (Petrov et al., 2012).
As we can see, the performance of those taggers is not great but stillrather informative with overall accuracy values around 80%.
The drop from source data to projected datais about 10-15 absolute points, which is, however, quite dramatic.
Assuming that this is the best we can1862POS DE EN ES FR SVDE 95.24 73.15 69.31 72.41 79.01EN 82.04 97.56 79.91 81.23 84.44ES 77.27 77.43 95.37 83.97 78.26FR 80.99 78.74 88.47 95.08 79.62SV 78.40 71.45 70.11 66.77 95.86DELEXICALIZED MODELSLAS DE EN ES FR SVDE ?
33.38 34.37 36.59 39.15EN 36.55 ?
45.53 47.71 48.92ES 35.07 39.87 ?
51.40 42.95FR 35.89 40.40 51.55 ?
40.30SV 37.87 39.80 43.62 41.61 ?TRANSLATED TREEBANK MODELSLAS DE EN ES FR SVDE ?
41.29 42.16 46.26 46.79EN 42.24 ?
50.54 53.63 53.78ES 38.61 43.70 ?
57.58 47.01FR 42.65 48.37 57.78 ?
45.55SV 41.37 42.34 49.38 46.00 ?Table 4: Top (POS): Accuracy of POS tagging models trained on translated treebanks (word-by-wordmodel).
Bottom (LAS): Cross-lingual parser models tested on automatically POS tagged test sets.
Thedelexicalized baseline (left) and the translated treebank model using word-by-word translation (right).achieve for the target language, we now have to look at the parsing performance when relying on suchnoisy annotation.Firstly, we look at the delexicalized baselines.
The bottom-left part of Table 4 lists the labeled at-tachment scores when gold POS labels are replaced with automatic tags created by the correspondingprojection tagger.
The drop is huge and the original scores that were well above 50-70% go down tonot more than 30-40% LAS.
Clearly, this was to be expected as proper POS labeling is crucial for thesemodels.
Let us now look at the annotation projection approach using a translated treebank as our paralleldata set.
Table 4 on the bottom-right lists the corresponding labeled attachment scores with automaticPOS tags.
As expected, the performance is considerably lower than with golden POS labels, which arestill the most informative features in those models.
However, the performance remains in a range ofabove 40-50% LAS.
Clearly, the lexical features help to keep the performance up at a higher level thanthe delexicalized baselines.
We believe, that this difference can be crucial when porting language toolsto new languages and that the models can be further optimized to rely less on golden POS tags.5 ConclusionsIn this paper we revisit annotation projection for cross-lingual parser induction.
We show that annotationcan successfully be transfered to target languages if the annotation is harmonized across languages.Despite previous negative results on diverse treebanks we demonstrate that direct projection works verywell for a number of languages and outperforms direct delexicalized transfer models by a large margin.The approach is also quite robust with respect to word alignment.
Furthermore, we show that machinetranslation can be a useful alternative for this strategy and that projected data can also be used to inducebasic information such as POS labels in combination with syntactic parser models.AcknowledgementsThis work was supported by the Swedish Research Council (Vetenskapsr?adet), project 2012-916.
I wouldalso like to thank Joakim Nivre,?Zeljko Agi?c and the anonymous reviewers for helpful comments andsuggestions.References?Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2012.
Slovene-Croatian Treebank Transfer Using BilingualLexicon Improves Croatian Dependency Parsing.
In Proceedings of IS-LTC 2012, pages 5?9.1863Miguel Ballesteros and Joakim Nivre.
2012.
MaltOptimizer: An Optimization Tool for MaltParser.
In Proceed-ings of EACL 2012, pages 58?62.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X Shared Task on Multilingual Dependency Parsing.
InProceedings of CoNLL 2006, pages 149?164.Marine Carpuat and Michel Simard.
2012.
The trouble with smt consistency.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages 442?449, Montr?eal, Canada.Kuzman Ganchev and Dipanjan Das.
2013.
Cross-Lingual Discriminative Learning of Sequence Models withPosterior Regularization.
In Proceedings of EMNLP 2013, pages 1996?2006.P?eter Hal?acsy, Andr?as Kornai, and Csaba Oravecz.
2007.
Poster paper: Hunpos ?
an open source trigram tagger.In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion VolumeProceedings of the Demo and Poster Sessions, pages 209?212, Prague, Czech Republic.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn.
2013.
Scalable Modified Kneser-NeyLanguage Model Estimation.
In Proceedings of ACL 2013, pages 690?696.Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak.
2005.
Bootstrapping Parsers viaSyntactic Projection across Parallel Texts.
Natural Language Engineering, 11(3):311?325.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens, Christopher J. Dyer, Ond?rej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open Source Toolkit for Statistical Machine Translation.
In Proceedings ofACL 2007, pages 177?180.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.
Multi-Source Transfer of Delexicalized Dependency Parsers.In Proceedings of EMNLP 2011, pages 62?72.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu Castell?o, and JungmeeLee.
2013.
Universal Dependency Annotation for Multilingual Parsing.
In Proceedings of ACL 2013, pages92?97.Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012.
Selective Sharing for Multilingual DependencyParsing.
In Proceedings of ACL 2012, pages 629?637.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
MaltParser: A Data-Driven Parser-Generator for DependencyParsing.
In Proceedings of LREC 2006, pages 2216?2219.Franz Josef Och and Hermann Ney.
2003.
A Systematic Comparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?52.Franz Josef Och.
2003.
Minimum Error Rate Training in Statistical Machine Translation.
In Proceedings of ACL2003, pages 160?167.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.
A Universal Part-of-Speech Tagset.
In Proceedings ofLREC 2012, pages 2089?2096.Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszkoreit.
2012.
Cross-lingual Word Clusters for Direct Transferof Linguistic Structure.
In Proceedings of NAACL 2012, pages 477?487.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre.
2013a.
Token and TypeConstraints for Cross-lingual Part-of-speech Tagging.
Transactions of the Association for Computational Lin-guistics, 1:1?12.Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013b.
Target Language Adaptation of DiscriminativeTransfer Parsers.
In Proceedings of NAACL 2013, pages 1061?1071.J?org Tiedemann,?Zeljko Agi?c, and Joakim Nivre.
2014.
Treebank translation for cross-lingual parser induction.In Proceedings of the 18th Conference Natural Language Processing and Computational Natural LanguageLearning (CoNLL), Baltimore, Maryland, USA.David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001.
Inducing Multilingual Text Analysis Tools viaRobust Projection Across Aligned Corpora.
In Proceedings of HLT 2011, pages 1?8.Yue Zhang and Joakim Nivre.
2011.
Transition-based Dependency Parsing with Rich Non-local Features.
InProceedings of ACL 2011, pages 188?193.1864
