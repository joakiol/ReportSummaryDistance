Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 217?224, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Salience Driven Approach to Robust Input Interpretation inMultimodal Conversational SystemsJoyce Y. Chai                  Shaolin QuComputer Science and EngineeringMichigan State UniversityEast Lansing, MI 48824{jchai@cse.msu.edu,  qushaoli@cse.msu.edu}AbstractTo improve the robustness in multimodalinput interpretation, this paper presents a newsalience driven approach.
This approach isbased on the observation that, duringmultimodal conversation, information fromdeictic gestures (e.g., point or circle) on agraphical display can signal a part of thephysical world (i.e., representation of thedomain and task) of the application which issalient during the communication.
This salientpart of the physical world will prime whatusers tend to communicate in speech and inturn can be used to constrain hypotheses forspoken language understanding, thusimproving overall input interpretation.
Ourexperimental results have indicated thepotential of this approach in reducing worderror rate and improving concept identificationin multimodal conversation.1 IntroductionMultimodal conversational systems promote morenatural and effective human machine communicationby allowing users to interact with systems throughmultiple modalities such as speech and gesture(Cohen et al, 1996; Johnston et al, 2002; Pieracciniet al, 2004).
Despite recent advances, interpretingwhat users communicate to the system is still asignificant challenge due to insufficient recognition(e.g., speech recognition) and understanding (e.g.,language understanding) performance.
Significantimprovement in the robustness of multimodalinterpretation is crucial if multimodal systems are tobe effective and practical for real world applications.Previous studies have shown that, in multimodalconversation, multiple modalities tend to complementeach other (Cassell et al 1994).
Fusing two or moremodalities can be an effective means of reducingrecognition uncertainties, for example, throughmutual disambiguation (Oviatt 1999).
Forsemantically-rich modalities such as speech and pen-based gesture, mutual disambiguation usuallyhappens at the fusion stage where partial semanticrepresentations from individual modalities aredisambiguated and combined into an overallinterpretation (Johnston 1998, Chai et al, 2004a).One problem is that some critical but low probabilityinformation from individual modalities (e.g.,recognized alternatives with low probabilities) maynever reach the fusion stage.
Therefore, this paperaddresses how to use information from one modality(e.g., deictic gesture) to directly influence thesemantic processing of another modality (e.g., spokenlanguage understanding) even before the fusion stage.In particular we present a new salience drivenapproach that uses gesture to influence spokenlanguage understanding.
This approach is based onthe observation that, during multimodal conversation,information from deictic gestures (e.g., point orcircle) on a graphical interface can signal a part of thephysical world (i.e., representation of the domain andtask) of the application which is salient during thecommunication.
This salient part of the physicalworld will prime what users tend to communicate inspeech and thus in turn can be used to constrainhypotheses for spoken language understanding.
Inparticular, this approach incorporates a notion ofsalience from deictic gestures into language modelsfor spoken language processing.
Our experimentalresults indicate the potential of this approach inreducing word error rate and improving conceptidentification from spoken utterances.217In the following sections, we first introduce thecurrent architecture for multimodal interpretation.Then we describe our salience driven approach andpresent empirical results.23Input InterpretationInput interpretation is the identification of semanticmeanings in user inputs.
In multimodal conversation,user inputs can come from multiple channels (e.g.,speech and gesture).
Thus, most work on inputinterpretation is based on semantic fusion thatincludes individual recognizers and a sequentialintegration processes as shown in Figure 1.
In thisapproach, a system first creates possible partialmeaning representations from recognized hypotheses(e.g., N-best lists) independently of other modalities.For example, suppose a user says ?what is the priceof this painting?
and at the same time points to aposition on the screen.
The partial meaningrepresentations from the speech input and the gestureinput are shown in (a-b) in Figure 1.
The system usesthe partial meaning representations to disambiguateeach other and combines compatible partialrepresentations together into an overall semanticrepresentation as in Figure1(c).In this architecture, the partial semanticrepresentations from individual modalities are crucialfor mutual disambiguation during multimodal fusion.The quality of partial semantic representationsdepends on how individual modalities are processed.For example, if the speech input is recognized as?what is the prize of this pant?, then the partialrepresentation from the speech input will not becreated in the first place.
Without a candidate partialrepresentation, it is not likely for multimodal fusionto reach an overall meaning of the input given thislate fusion architecture.Thus, a problem with the semantics-based fusionapproach is that information from multiple modalitiesis only used during the fusion stage to disambiguateor combine partial semantic representations.
This lateuse of information from other sources in thepipelined process can cause the loss of some lowprobability information (e.g., recognized alternativeswith low probabilities which did not make it to the N-best list) which could be very crucial in terms of theoverall interpretation.
It is desirable to useinformation from multiple sources at an earlier stagebefore partial representations are created fromindividual modalities.
For example, in ((Bangaloreand Johnston 2000), a finite-state approach wasapplied to tightly couple multimodal languageprocessing (e.g., gesture and speech) and speechrecognition to improve recognition hypotheses.
Tofurther address this issue, in this paper, we present asalience driven approach that particularly appliesgesture information (e.g., pen-based deictic gestures)to robust spoken language understanding beforemultimodal fusion.Related Work on Salience ModelingWe first give a brief overview on the notion ofsalience and how salience modeling is applied inearlier work on natural language and multimodallanguage processing.Linguistic salience describes the accessibility ofentities in a speaker/hearer?s memory and itsimplication in language production andinterpretation.
Many theories on linguistic saliencehave been developed, including how the salience ofentities affects the form of referring expressions as inthe Givenness Hierarchy (Gundel et al, 1993) andthe local coherence of discourse as in the CenteringTheory (Grosz et al, 1995).
Salience modeling isused for both language generation and languageinterpretation; the latter is more relevant to our work.Most salience-based interpretation has focused onreference resolution for both linguistic referringexpressions (e.g., pronouns) (Lappin and Leass 1995)and multimodal expressions (Hul et al 1995;Eisenstein and Christoudias 2004).Speech Input Gesture InputSpeechRecognitionLanguageUnderstandingGestureRecognizerMultimodalFusionSemantic RepresentationGestureUnderstandingSemantic Representation Semantic RepresentationWhat is the price of this painting Point to a position on the screenIntent: AskType: PaintingAspect: PriceType: PaintingId: P23Intent: AskType: PaintingAspect: PriceId: P23Type: WallId: W1(a) (b)(c)Figure 1: Semantics-based multimodal interpretationVisual salience considers an object salient whenit attracts a user?s visual attention more than others.The cause of such attention depends on many factorsincluding user intention, familiarity, and physicalcharacteristics of objects.
For example, an object maybe salient when it has some properties the others donot have, such as it is the only one that is highlighted,or the only one of a certain size, category, or color218(Landragin et al, 2001).
Visual salience can also beuseful in input interpretation, for example, formultimodal reference resolution (Kehler 2000) andcross-modal coreference interpretation (Byron et al,2005).We believe that salience modeling should gobeyond reference resolution.
Our view is that thesalience not only affects the use of referringexpressions (and thus is useful for interpretingreferring expressions), but also influences thelinguistic context of the referring expressions.
Thespoken utterances that contain these expressions tendto describe information relating to the salient objects(e.g., properties or actions).
Therefore, our goal inthis paper is to take salience modeling a step furtherfrom reference resolution, towards overall languageunderstanding.44.1A Salience Driven ApproachThe new salience driven approach is based on thecognitive theory of Conversation Implicature (Grice1975) and earlier empirical findings of user speechand gesture behavior in multimodal conversation(Oviatt 1999).
The theory of ConversationImplicature (Grice 1975) states that speakers tend tomake their contribution as informative as is required(for the current purpose of communication) and notmake their contribution more informative than isrequired.
In the context of multimodal conversationthat involves speech and pen-based gesture, thistheory indicates that users most likely will not makeany unnecessary deictic gestures unless thosegestures help in communicating users?
intention.
Thisis especially true since gestures usually take an extraeffort from a user.
When a pen-based gesture isintentionally delivered by a user, the informationconveyed is often a crucial component ininterpretation (Chai et al, 2005).SpeechRecognitionLanguageUnderstandingPhysical world representationsaliente1 e2 e3 ??
?.P(e)discourseSpeechGestureGestureRecognitionGestureUnderstandingMultimodal    FusionSemantic  RepresentationFigure 2: The salience driven approach: the saliencedistribution calculated from gesture is used to tailorlanguage models for spoken language understandingSpeech and gesture also tend to complement eachother.
For example, when a speech utterance isaccompanied by a deictic gesture (e.g., point orcircle) on a graphical display, the speech input tendsto issue commands or inquiries about properties ofobjects, and the deictic gestures tend to indicate theobjects of interest.
In addition, as shown in (Oviatt1999), the deictic gestures often occur before spokenutterances.
Our previous work (Chai et al, 2004b)also showed that 85% of time gestures occurredbefore corresponding speech units.
Therefore,gestures can be used as an earlier indicator toanticipate the content of communication in thesubsequent spoken utterances.OverviewThe general idea of the salience based approach isshown in Figure 2.
For each application domain,there is a physical world representation that capturesdomain knowledge (details are described later).
Adeictic gesture can activate several objects on thegraphical display.
This activation will signal adistribution of objects that are salient.
The salientobjects are mapped to the physical worldrepresentation to indicate a salient part ofrepresentation that includes relevant properties ortasks related to the salient objects.
This salient part ofthe physical world is likely to be the potential contentof the spoken communication, and thus can be usedto tailor language models for spoken languageunderstanding.
This process is shown in the middleshaded box of Figure 2.
It bridges gestureunderstanding and language understanding at a stagebefore multimodal fusion.
Note that the use ofgesture information can be applied at different stages:during speech recognition to generate hypotheses orpost processing of recognized hypotheses beforelanguage understanding.
In this paper, we focus onthe latter.The physical world representation includes thefollowing components:?
Domain Model.
This component captures therelevant knowledge about the domain includingdomain objects, properties of the objects, relationsbetween objects, and task models related to objects.Previous studies have shown that domain knowledge219can be used to improve spoken languageunderstanding (Wai et al 2001).
Currently, we applya frame-based representation where a framerepresents an object (or a type of object) in thedomain and frame elements represent attributes andtasks related to the objects.
Each frame element isassociated with a semantic tag which indicates thesemantic content of that element.
In the future, thedomain model might also include knowledge aboutthe interface, for example, visual properties andspatial relations between objects on the interface.w1 wn??
?
?Timet2 t3 tn)(ePnt)|(3tgeP)( 3tnt?
)( 2tnt?
)( 1tnt?wi wi+1t1)|(2tgeP)|( 1tgePFigure 3: Salience modeling: the salience distributionat time tn is calculated by a joint effect of gesturesthat happen before tn.
?
Domain Grammar.
This component specifiesgrammar and vocabularies used to process languageinputs.
There are two types of representation.
Thefirst type is a semantics-based context free grammarwhere each non-terminal symbol represents asemantic tag (indicating semantic information such asthe semantic type of an object, etc).
Each word (i.e.,the terminal symbol) in the lexicon relates to one ormore semantic tags.
Some of these semantic tags aredirectly linked to the frame elements in the domainmodel since they represent certain properties or tasks.This grammar was manually developed.4.2The second type of representation is based onannotated user spoken utterances.
The data areannotated in terms of relevant semantic information(i.e., using semantic tags) in the utterance and theintended objects of interest (which are directly linkedto the domain model).
Based on the annotated data,N-grams can be learned to represent the dependencyof language in our domain.Based on the physical world representation, ourapproach supports the following operations:Salience modeling.
This operation calculates asalience distribution of entities in the physical world.In our current investigation, we limit the scope ofentities to a closed set of objects from our physicalworld representation since the system has knowledgeabout those objects.
These entities could havedifferent salience values depending on whether theyare visible on the graphical display, gestured by auser, or mentioned in the prior conversation.
In thispaper, we focus on the salience modeling usinggesture information only.Salience driven language understanding.
Thisoperation maps the salience distribution to thephysical world representation and uses the salientworld to influence spoken language understanding.Note that, in this paper, we are not concerned withacoustic models for speech recognition, but rather weare interested in the use of the salience distribution toprime language models and facilitate languageunderstanding.Salience ModelingWe use a vector er to represent entities in the physicalworld representation.
For each entity e ekr?
, we useto represent its salience value at time tn.
Forall the entities, we use P)( kt eP n)(entv  to represent a saliencedistribution at time tn.
Figure 3 shows a sequence ofwords with corresponding gestures that occur at t1, t2,and t3.
As shown in Figure 3, the salience distributionat any given time tn is influenced by a joint effectfrom this sequence of gestures that happen before tnetc.
Depending on its time of occurrence, eachgesture may have a different impact on the saliencedistribution at time tn.
Note that although eachgesture may have a short duration, here we onlyconsider the beginning time of a gesture.
Therefore,for an entity ek, its salience value at time tn iscomputed as follows:11( ) ( | )( )( ) ( |n i inn i imt t k tit k mt t te e i)g P e gP eg P e g??=?
==??
?r(1)In Equation (1), m (m ?
1) is the number ofgestures that have occurred before tn.
The differentimpact of a gesture g  at time ti that contributes tothe salience distribution at time tn is represented asthe weightit)(in tt g?
in Equation (1).
Currently, wecalculate the weight depending on the temporaldistance as follows:)(]2000)(exp[)( inintt ttttgin???=?
(2)Equation (2) indicates that at a given time tn(measured in milliseconds), the closer a gesture (at ti)is to the time tn, the higher impact this gesture has onthe salience distribution (Chai et al, 2004b).It is worth mentioning that a deictic gesture on thegraphic display (e.g., pointing and circling) couldhave ambiguous interpretation by itself.
For example,220given an interface, a point or a circle on the screencould result in selection of different entities withdifferent probabilities.
Therefore, in Equation (1),is the selection probability which indicatesthe likelihood of selecting an entity e given a gestureat time ti.
This selection probability is calculated by afunction of the distance between the location of theentity and the focus point of the recognized gestureon the display (Chai et al, 2004a).
A normalizationfactor is incorporated to ensure that the summation ofselection probabilities over all possible entities addsup to one.
( | )itP e gWhen no gesture is involved in a given input, thesalience distribution at any given time is a uniformdistribution.
If one or more gestures are involved,then Equation (1) is used to calculate the saliencedistribution.4.3P WSalience Driven Spoken LanguageUnderstandingThe salience distribution of entities identified basedon the gesture information (as described above) isused to constrain hypotheses for languageunderstanding.
More specifically, for each onset of aspoken word at time t (i.e., the beginning time stampof a spoken word), the salience distribution at t canbe calculated based on a sequence of gestures thathappen before t by Equation (1).
This saliencedistribution can then be used to prime languagemodels for spoken language processing.Language ModelingWe first give a brief background of languagemodeling.
Given an observed speech utterance O, thegoal of speech recognition is to find a sequence ofwords W* so that W P ,where P(O|W) is the acoustic model and P(W) is thelanguage model.
In traditional speech recognitionsystems, the acoustic model provides the probabilityof observing the acoustic features given hypothesizedword sequences and the language model provides theprobability of a sequence of words.
The languagemodel is computed as follows:* arg max ( | ) ( )O W=)|()...|()|()()( 112131211?= nnn wwPwwwPwwPwPwPUsing the Markov assumption, the language modelcan be approximated by a bigram model as in:?=?=niiin wwPwP111 )|()(To improve the speech understanding results forspoken language interfaces, many systems haveapplied a loosely-integrated approach whichdecouples the language model from the acousticmodel (Zue et al, 1991, Harper et al, 2000).
Thisallows the development of powerful language modelsindependent of the acoustic model, for example,utilizing topics of the utterances (Gildea andHofmann 1999), syntactic or semantic labels(Heeman 1999), and linguistic structures (Chelba andJelinek 2000, Wang and Harper 2002).
Recently, wehave seen work on language understanding based onenvironment (Schuler 2003) and language modelingusing visual context (Roy and Mukherjee 2005).
Oursalience driven approach is inspired by this earlierwork.
Here, we do not address the acoustic model ofspeech recognition, but rather incorporate thesalience distribution for language modeling.
Inparticular, our focus is on investigating the effect ofincorporating additional information from othermodalities (e.g., gesture) with traditional languagemodels.Primed Language ModelThe calculated salience distribution is used to primethe language model.
More specifically, we use aclass-based bigram model from (Brown et al 1992):)|()|()|( 11 ??
= iiiiii ccPcwPwwP                 (3)In Equation (3), ci is the class of the word wi,which could be a syntactic class or a semantic class.is the class transition probability, whichreflects the grammatical formation of utterances.is the word class probability whichmeasures the probability of seeing a word wi given aclass ci.
The class-based N-gram model can makebetter use of limited training data by clustering wordsinto classes.
A number of researchers have shownthat the class-based N-gram model can successfullyimprove the performance of speech recognition(Jelinek 1990, Heeman 1999, Kneser and Ney 1993,Samuelsson and Reichl, 1999).
)|( 1?ii ccP)|( ii cwPIn our approach, the ?class?
used in the class-based bigram model comes from combined semanticand functional classes designed for our domain.
Forexample, ?this?
is tagged as Demonstrative, and?price?
is tagged as AttrPrice.
As shown in Equation(3), there are two types of parameter estimation.
Interms of the class transition probability, as in earlierwork, we directly use the annotated data.
In terms ofthe word class distribution, we incorporate the notionof salience.
We use the salience distribution todynamically adjust the world class probabilityas follows: )|( ii cwP221)()|()|,()|( ktee kikiiii ePecPecwPcwPik?
?=v(4) Userindex# ofInputs# inputsw/o gestureBaselineWER1 21 0 0.2872 31 0 0.3353 27 0 0.3994 10 0 0.6805 8 1 0.2006 36 0 0.3877 18 0 0.2508 25 1 0.2789 23 0 0.48210 11 0 0.11711 16 3 0.255Table 1: Related information about the evaluationdata: user type, the number of turns per user, and thebaseline word recognition rate.In Equation (4), P  is the salience value for anentity  at time ti (the onset of the spoken word wi),which can be calculated by Equation (1).
Equation(4) indicates that only information associated with thesalient entities is used to estimate the word classdistribution.
In other words, the word classprobability favors the salient physical world asindicated by the salience distribution)( kt eike)(ePitv .
Morespecifically, at time  ti, given a semantic class ci, thechoice of word ?wi?
is dependent on the salientphysical world at the moment, which is representedas the salience distribution )(ePitv at time ti.
For all wi,the summation of this word class probability is equalto one.
Furthermore, given an entity ,and  are not dependent on time ti, but ratheron the domain and the use of language expressions.Therefore they can be estimated based on the trainingdata that are annotated in terms of semanticinformation and the intended objects of interest (asdiscussed in Section 4.1).
Since the annotated data isvery limited, the sparse data can become a problemfor the maximum likelihood estimation.
Therefore, asmoothing technique based on the Katz backoffmodel (Katz, 1987) is applied.
For example, tocalculate , if a word wi has one or moreoccurrences in the training data associated with theclass ci and the entity , then its count is discountedby a fraction in the maximum likelihood estimation.If wi does not occur, then this approach backs off tothe domain grammar and redistributes the remainingprobability mass uniformly among words in thelexicon that are linked with class ci and entity e .ke )| ki eck,( iwP)| keP,( iwP( ic)| kekeic5EvaluationWe evaluated the salience model during postprocessing recognized hypotheses.
Given possiblehypotheses from a speech recognizer, we use thesalience-based language model to identify the mostlikely sequence of words.
The salience distributionbased on gesture was used to favor words that areconsistent with the attention indicated by gestures.The data collected from our previous user studieswere used in our evaluation (Chai et al, 2004b).
Inthese studies, users interacted with our multimodalinterface using both speech and deictic gestures tofind information about real estate properties.
Inparticular, each user was asked to accomplish fivetasks.
Each of these tasks required the user to retrievedifferent types of information from our interface.
Forexample, one task was to find the least expensivehouse in the most populated town.
The data wererecorded from eleven subjects including five non-native speakers and six native speakers.
Each user?svoice was individually trained before the study.
Table1 shows the relevant information about the data suchas the total number of inputs (or turns) from eachsubject, the number of speech alone inputs withoutany gesture, and the baseline recognition resultswithout using salience-based post processing in termsof the word error rate (WER).
In total, we havecollected 226 user inputs with an average of eightwords per spoken utterance1.
As shown in Table 1,the majority of inputs consisted of both speech andgesture.
Since currently we only use gesture0.10.20.30.40.50.60.71 2 3 4 5 6 7 8 9 10 11User indexWordErrorRateBaseline Salience driven modelFigure 5: Comparison of the baseline and the resultfrom post-processing in terms of WER1 The difference between the number of user inputs reportedhere and that in (Chai et al, 2004b) was caused by the situa-tion where one intended user input (which was the unit forcounting in our previous work) was split into a couple turns(which constitute the new counts here).222information in salience modeling, our approach willnot affect speech only inputs.To train the salience-based model, we applied theleave-one-out approach.
The data from each user washeld out as the testing data and the remaining userswere used as the training data to acquire relevantprobability estimations in Equation (3) and (4).Figure 5 shows the comparison results betweenthe baseline and the salience-based model in terms ofword error rate (WER).
The word error rate as aresult of salience-based post processing issignificantly better than that from the baselinerecognizer (t = 4.75, p < 0.001).
The average WERreduction is about 12%.We further evaluated how the salience basedmodel affects the final understanding results.
This isbecause an improvement in WER may not directlylead to an improvement in understanding.
We appliedour semantic grammar on a sequence of wordsresulting from both the baseline and the salience-based post-processing to identify key concepts.
Intotal, there were 686 concepts from the transcribedspeech utterances.
Table 2 shows the evaluationresults.
Precision measures the percentage of correctlyidentified concepts out of the total number ofconcepts identified based on a sequence of words.Recall measures the percentage of correctly identifiedconcepts out of the total number of intended conceptsfrom user?s utterance.
F-measurement combinesprecision and recall together as follows:1,RecallPrecisionRecallPrecision)1(22=+?
?+= ???
whereF .Table 2 shows that, on average, the conceptidentification based on the word sequence resultingfrom the salience-based approach performs betterthan the baseline in terms of both precision andrecall.
Figure 6 provides two examples to show thedifference between the baseline recognition and thesalience-based post processing.The evaluation reported here is only an initial stepbased on a limited domain.
The small scale in thenumber of objects and the vocabulary size can onlydemonstrate the potential of the salience-basedapproach to a limited degree.
To further understandthe advantages and issues of this approach, we arecurrently working on a more complex domain withricher concepts and relations, as well as largervocabularies.It is worth mentioning that the goal of this work isto explore whether salience modeling based on othermodalities (e.g., gesture) can be used to primetraditional language models to facilitate spokenlanguage processing.
The salience driven approachbased on additional modalities can be combined withmore sophisticated language modeling (e.g., betterparameter estimation) in the future.Example 1:Transcription: What is the population of this townBaseline recognition: What is the publisher of this timeSalience-based processing: what is the population of this townExample 2:Transcription: How much is this gray houseBaseline recognition: How much is this great houseSalience-based processing: How much is this gray houseFigure 6: Examples of utterances with baseline recogni-tion and improved recognition from the salience-basedprocessing.User # Baseline Salience-basedPrecision 80.3% 84.6%Recall 75.7% 83.8%F-measure 77.9% 84.2%Table2.
Overall concept identification comparisonbetween the baseline and the salience driven model.6 Conclusion and Future WorkThis paper presents a new salience driven approachto robust input interpretation in multimodalconversational systems.
This approach takesadvantage of rich information from multiplemodalities.
Information from deictic gestures is usedto identify a part of the physical world that is salientat a given point of communication.
This salient partof the physical world is then used to prime languagemodels for spoken language understanding.
Ourexperimental results have shown the potential of thisapproach in reducing word error rate and improvingconcept identification from spoken utterances in ourapplication.
Although currently we have onlyinvestigated the use of gesture information in saliencemodeling, the salience driven approach can beextended to include other modalities (e.g., eye gaze)and information (e.g., conversation context).
Ourfuture work will specifically investigate how tocombine information from multiple sources insalience modeling and how to apply the saliencemodels in different early stages of processing.223AcknowledgementThis work was supported by a CAREER grant IIS-0347548from the National Science Foundation.
The authors would liketo thank anonymous reviewers for their helpful comments andsuggestions.ReferencesBangalore, S. and Johnston, M. 2000.
Integrating MultimodalLanguage Processing with Speech Recognition.
InProceedings of ICSLP.Brown, P., Della Pietra, V. J., deSouza, P. V., Lai, J.
C, andMercer, R. L. 1992.
Class-based n-gram models of naturallanguage.
Computational Linguistics, 18(4):467-479.Byron, D., Mampilly, T., Sharma, V., and Xu, T. 2005.
UtilizingVisual Attention for Cross-Modal Coreference Interpretation.Spring Lecture Notes in Computer Science: Proceedings ofContext-05, page 83-96.Cassell, J., Stone, M., Douville, B., Prevost, S., Achorn, B.,Steedman, M., Badler, N., and Pelachaud, C. 1994.
Modelingthe interaction between speech and gesture.
Cognitive ScienceSociety.Chai, J. Y., Prasov, Z., Blaim, J., and Jin, R. 2005.
LinguisticTheories in Efficient Multimodal Reference Resolution: anEmpirical Investigation.
The 10th International Conference onIntelligent User Interfaces (IUI-05), pp.
43-50, San Diego,CA.Chai, J. Y., Hong, P., Zhou, M. X, and Prasov, Z.
2004b.Optimization in Multimodal Interpretation.
In Proceedings ofACL,  pp.
1-8, Barcelona, Spain.Chai, J. Y., Hong, P., and Zhou, M.  2004a.
A ProbabilisticApproach to Reference Resolution in Multimodal UserInterfaces.
Proceedings of 9th International Conference onIntelligent User Interfaces (IUI-04), pp.
70-77, Madeira,Portugal.Chelba, C. and Jelinek, F. 2000.
Structured language modeling.Computer Speech and Language, 14(4):283?332.Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J.;Smith, I., Chen, L., and Clow, J.
1996.
Quickset: MultimodalInteraction for Distributed Applications.
Proceedings of ACMMultimedia, 31?
40.Eisenstein J. and Christoudias.
C. 2004.
A salience-basedapproach to gesture-speech alignment.
In Proceedings ofHLT/NAACL?04.Gildea, D. and Hofmann, T. 1999.
Topic-based language modelsusing EM.
In Proceedings of Eurospeech.Griffin, Z. M. 2001.
Gaze durations during speech reflect wordselection and phonological encoding.
Cognition 82, B1-B14.Grosz, B. J., Joshi, A. K., and Weinstein, S. 1995.
Centering: Aframework for modeling the local coherence of discourse.Computational Linguistics, 21(2).Grice, H. P. Logic and Conversation.
1975.
In Cole, P., andMorgan, J., eds.
Speech Acts.
New York, New York:Academic Press.
41-58.Gundel, J. K., Hedberg, N., and Zacharski, R. 1993.
CognitiveStatus and the Form of Referring Expressions in Discourse.Language 69(2):274-307.Harper, M.., White, C., Wang, W., Johnson, M., and Helzerman,R.
2000.
The Effectiveness of Corpus-Induced DependencyGrammars for Post-processing Speech.
Proceedings of theNorth American Association for Computational Linguistics,102-109.Heeman.
P. 1999.
POS tags and decision trees for languagemodeling.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Process (EMNLP).Huls, C., Bos, E., and Classen, W. 1995.
Automatic ReferentResolution of Deictic and Anaphoric Expressions.Computational Linguistics, 21(1):59-79.Jelinek, F. 1990.
Self-organized language modeling for speechrecognition.
In Waibel, A. and Lee, K. F. (Eds).
Readings inSpeech Recognition, pp.
450-506.Johnston, M. 1998.
Unification-based Multimodal parsing,Proceedings of COLING-ACL.Johnston, M.,  Bangalore, S.,  Visireddy G., Stent, A., Ehlen, P.,Walker, M., Whittaker, S., and Maloor, P. 2002.
MATCH: AnArchitecture for Multimodal Dialog Systems, in Proceedingsof the 40th ACL, Philadelphia, pp.
376-383.Katz, S. M. 1987.
Estimation of probabilities from sparse data forthe language model component of a speech recognizer.
IEEETransactions on Acoustics, Speech, and Signal Processing,35(3).Kehler, A.
2000.
Cognitive Status and Form of Reference inMultimodal Human-Computer Interaction, Proceedings ofAAAI?01.Kneser, R. and Ney, H. 1993.
Improved clustering techniques forclass-based statistical language modeling.
In Eurospeech?93,pp.
973-976.Landragin, F., Bellalem, N., and Romary, L. 2001.
VisualSalience and Perceptual Grouping in Multimodal Interactivity.In: First International Workshop on Information Presentationand Natural Multimodal Dialogue, Verona, Italy, pp.
151-155.Lappin, S., and Leass, H. 1994.
An algorithm for pronominalanaphora resolution.
Computational Linguistics, 20(4).Oviatt, S. 1999.
Mutual Disambiguation of Recognition Errors ina Multimodal Architecture.
In Proceedings of CHI.Pieraccini, R., Dayandhi, K., Bloom, J., Dahan, J.-G., Phillips, M.,Goodman, B. R., Prasad, K. V., 2004.
MultimodalConversational Systems for Automobiles, Communications ofthe ACM, Vol.
47, No.
1, pp.
47-49Roy, D. and Mukherjee, N. 2005.
Towards Situated SpeechUnderstanding: Visual Context Priming of Language Models.Computer Speech and Language, 19(2): 227-248.Samuelsson, C. and Reichl, W. 1999.
A class-based LanguageModel for Large Vocabulary Speech Recognition Extractedfrom Part-of-Speech Statistics.
In IEEE ICASSP?99.Schuler, W. 2003.
Using model-theoretic semantic interpretationto guide statistical parsing and word recognition in a spokenlanguage interface.
In  Proceedings of ACL, Sapporo, Japan.Wai, C., Pierraccinni, R., and Meng, H. 2001.
A DynamicSemantic Model for Rescoring Recognition Hypothesis.Proceedings of the ICASSP.Wang, W. and Harper.
M. 2002.
The superARV language model:In Investigating the effectiveness of tightly integratingmultiple knowledge sources.
In Proceedings  of EMNLP, 238?247.Zue, V., Glass, J., Goodine, D., Leung, H., Phillips, M., Polifroni,J., and Seneff, S. 1991.
Integration of Speech Recognition andNatural Language Processing in the MIT Voyager System.Proceedings of the ICASSP.224
