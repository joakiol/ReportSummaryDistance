Proceedings of the EACL 2009 Student Research Workshop, pages 61?69,Athens, Greece, 2 April 2009. c?2009 Association for Computational LinguisticsA Comparison of Merging Strategies for Translation of GermanCompoundsSara StymneDepartment of Computer and Information ScienceLinko?ping University, Swedensarst@ida.liu.seAbstractIn this article, compound processing fortranslation into German in a factored sta-tistical MT system is investigated.
Com-pounds are handled by splitting them priorto training, and merging the parts aftertranslation.
I have explored eight mergingstrategies using different combinations ofexternal knowledge sources, such as wordlists, and internal sources that are carriedthrough the translation process, such assymbols or parts-of-speech.
I show thatfor merging to be successful, some internalknowledge source is needed.
I also showthat an extra sequence model for part-of-speech is useful in order to improve theorder of compound parts in the output.The best merging results are achieved by amatching scheme for part-of-speech tags.1 IntroductionIn German, as in many other languages, com-pounds are normally written as single words with-out spaces or other word boundaries.
Compoundscan be binary, i.e., made up of two parts (1a), orhave more parts (1b).
There are also coordinatedcompound constructions (1c).
In a few cases com-pounds are written with a hyphen (1d), often whenone of the parts is a proper name or an abbrevia-tion.
(1) a. Regierungskonferenzintergovernmental conferenceb.
Fremdsprachenkenntnisseknowledge of foreign languagesc.
See- und Binnenha?fensea and inland portsd.
Kosovo-KonfliktKosovo conflicte.
Vo?lkermordgenocideGerman compounds can have English trans-lations that are compounds, written as separatewords (1a), other constructions, possibly with in-serted function words and reordering (1b), or sin-gle words (1e).
Compound parts sometimes havespecial compound forms, formed by addition ortruncations of letters, by umlaut or by a combi-nation of these, as in (1a), where the letter -s isadded to the first part, Regierung.
For an overviewof German compound forms, see Langer (1998).Compounds are productive and common in Ger-man and other Germanic languages, which makesthem problematic for many applications includ-ing statistical machine translation.
For translationinto a compounding language, fewer compoundsthan in normal texts are often produced, which canbe due to the fact that the desired compounds aremissing in the training data, or that they have notbeen aligned correctly.
Where a compound is theidiomatic word choice in the translation, aMT sys-tem can instead produce separate words, genitiveor other alternative constructions, or only translateone part of the compound.The most common way to integrate compoundprocessing into statistical machine translation is tosplit compounds prior to training and translation.Splitting of compounds has received a lot of focusin the literature, both for machine translation, andtargeted at other applications such as informationretrieval or speech recognition.When translating into a compounding languagethere is a need to merge the split compounds af-ter translation.
In order to do this we have toidentify which words that should be merged intocompounds, which is complicated by the fact thatthe translation process is not guaranteed to pro-duce translations where compound parts are kepttogether.61In this article I explore the effects of merging ina factored phrase-based statistical machine trans-lation system.
The system uses part-of-speech asan output factor.
This factor is used as a knowl-edge source for merging and to improve wordorder by using a part-of-speech (POS) sequencemodel.There are different knowledge sources formerging.
Some are external, such as frequencylists of words, compounds, and compound parts,that could be compiled at split-time.
It is alsopossible to have internal knowledge sources, thatare carried through the translation process, suchas symbols on compound parts, or part-of-speechtags.
Choices made at split-time influence whichinternal knowledge sources are available at merge-time.
I will explore and compare three markupschemes for compound parts, and eight merg-ing algorithms that use different combinations ofknowledge sources.2 Related WorkSplitting German compounds into their parts priorto translation has been suggested by many re-searchers.
Koehn and Knight (2003) presented anempirical splitting algorithm that is used to im-prove translation from German to English.
Theysplit all words in all possible places, and consid-ered a splitting option valid if all the parts are ex-isting words in a monolingual corpus.
They al-lowed the addition of -s or -es at all splittingpoints.
If there were several valid splitting optionsthey chose one based on the number of splits, thegeometric mean of part frequencies or based onalignment data.
Stymne (2008) extended this al-gorithm in a number of ways, for instance by al-lowing more compound forms.
She found that fortranslation into German, it was better to use thearithmetic mean of part frequencies than the geo-metric mean.
Using the mean of frequencies canresult in no split, if the compound is more frequentthan its parts.Merging has been much less explored than split-ting since it is common only to discuss translationfrom compounding languages.
However, Popovic?et al (2006) used merging for translation into Ger-man.
They did not mark compound parts in anyway, so the merging is based on two word lists,with compound parts and full compounds foundat split-time.
All words in the translation outputthat were possible compound parts were mergedwith the next word if it resulted in a known com-pound.
They only discussed merging of binarycompounds.
The drawback of this method is thatnovel compounds cannot be merged.
Neverthe-less, this strategy led to improved translation mea-sured by three automatic metrics.In a study of translation between English andSwedish, Stymne and Holmqvist (2008) suggesteda merging algorithm based on part-of-speech,which can be used in a factored translation sys-tem with part-of-speech as an output factor.
Com-pound parts had special part-of-speech tags basedon the head of the compound, and merging wasperformed if that part-of-speech tag matched thatof the following word.
When compound formshad been normalized the correct compound formwas found by using frequency lists of parts andwords compiled at split-time.
This method canmerge unseen compounds, and the tendency tomerge too much is reduced by the restriction thatPOS-tags need to match.
In addition coordinatedcompounds were handled by the algorithm.
Thisstrategy resulted in improved scores on automaticmetrics, which were confirmed by an error analy-sis.Koehn et al (2008) discussed treatment of hy-phened compounds in translation into German bysplitting at hyphens and treat the hyphen as a sep-arate token, marked by a symbol.
The impact ontranslation results was small.There are also other ways of using compoundprocessing to improve SMT into German.
Popovic?et al (2006) suggested using compound splittingto improve alignment, or to merge English com-pounds prior to training.Some work has discussed merging of not onlycompounds, but of all morphs.
Virpioja et al(2007) merged translation output that was splitinto morphs for Finnish, Swedish and Danish.They marked split parts with a symbol, andmerged every word in the output which had thissymbol with the next word.
If morphs weremisplaced in the translation output, they weremerged anyway, possibly creating non-existentwords.
This system was worse than the baselineon Bleu (Papineni et al, 2002), but an error analy-sis showed some improvements.El-Kahlout and Oflazer (2006), discuss merg-ing of morphs in Turkish.
They also markmorphs with a symbol, and in addition normal-ize affixes to standard form.
In the merging62phase, surface forms were generated followingmorphographemic rules.
They found that morphswere often translated out of order, and that merg-ing based purely on symbols gave bad results.
Toreduce this risk, they constrained splitting to allowonly morphologically correct splits, and by group-ing some morphemes.
This lead to less orderingproblems in the translation output and gave im-provements over the baseline.Compound recombination have also been ap-plied to German speech recognition, e.g.
by(Berton et al, 1996), who performed a lexicalsearch to extend the word graph that is output bythe speech recogniser.3 Compound ProcessingGerman compounds are split in the training dataand prior to translation.
After translation, the partsare merged to form full compounds.
The knowl-edge sources available to the merging process de-pend on which information is carried through thetranslation process.The splitting algorithm of Stymne (2008) willbe used throughout this study.
It is slightly mod-ified such that only the 10 most common com-pound forms from a corpus study of Langer (1998)are allowed, and the hyphen in hyphened com-pounds is treated as a compound form, analogousto adding for instance the letter s to a part.The annotation of compound parts influencesthe merging process.
Choices have to be madeconcerning the form, markup and part-of-speechof compound parts.
For the form two optionshave been considered, keeping the original com-pound form, or normalizing it so that it coincideswith a normal word.
Three types of marking havebeen investigated, no marking at all (unmarked), amarking symbol that is concatenated to all partsbut the last (marked), or using a separate sym-bol between parts (sepmarked).
The sepmarkedscheme has different symbols for parts of coordi-nated compounds than for other compounds.
Partsare normalized in the unmarked and sepmarkedschemes, but left in their compound form in themarked scheme, since the symbol separates themfrom ordinary words in any case.There is also the issue of which part-of-speechtag to use for compound parts.
The last part of thecompound, the head, always has the same part-of-speech tag as the full compound.
Two schemesare explored for the other parts.
For the markedand unmarked system, a part-of-speech tag that isderived from that of the last part of the word isused.
For the sepmarked scheme the most com-mon part-of-speech tag of the part from the taggedmonolingual corpus is used.In summary, the three markup schemes use thefollowing combinations, exemplified by the resultof splitting the word begru?
?enswert (welcome, lit-erally worth to welcome)?
Unmarked: no symbol, normalization, spe-cial POS-tagsbegru?
?en ADJ-PART wert ADJ?
Marked: symbol on parts, no normalization,special POS-tagsbegru?
?ens# ADJ-PART wert ADJ?
Sepmarked: symbol as separate token, nor-malization, ordinary POS-tagsbegru?
?en VV @#@ COMP wert ADJ3.1 MergingThere is no guarantee that compound parts appearin a correct context in the translation output.
Thisfact complicates merging, since there is a generalchoice between only merging those words that weknow are compounds, and merging all occurrencesof compound parts, which will merge unseen com-pounds, but probably also merge parts that do notform well-formed compounds.
There is also theissue of parts possibly being part of coordinatedcompounds.The internal knowledge sources that can be usedfor merging depends on the markup scheme used.The available internal sources are markup sym-bols, part-of-speech tags, and the special tags forcompound parts.
The external resources are fre-quency lists of words, compounds and parts, pos-sibly with normalization, compiled at split-time.For the unmarked and sepmarked scheme, re-verse normalization, i.e., mapping normalizedcompound parts into correct compound forms, hasto be applied in connection with merging.
As inStymne and Holmqvist (2008), all combinationsof compound forms that are known for each partare looked up in the word frequency list, and themost frequent combination is chosen.
If there areno known combinations, the parts are combinedfrom left to right, at each step choosing the mostfrequent combination.Three main types of merging algorithms are in-vestigated in this study.
The first group, inspired63Name Descriptionword-list Merges all tokens that have been seen as compound parts with the next part if it results in a knownword, from the training corpusword-list + head-pos As word-list, but only merges words where the last part is a noun, adjective or verbcompound-list As word-list, but for known compounds from split-time, not for all known wordssymbol Merges all tokens that are marked with the next tokensymbol + head-pos As symbol, but only merges words where the last part is a noun, adjective or verbsymbol + word-list A mix of symbol and word-list, where marked compounds are merged, if it results in a known wordPOS-match Merges all tokens with a compound part-of-speech tag, if the tag match the tag of the next tokenPOS-match + coord As POS-match, but also adds a hyphen to parts that are followed by the conjunction und (and)Table 1: Merging algorithmsby Popovic?
et al (2006), is based only on exter-nal knowledge sources, frequency lists of wordsor compounds, and of parts, compiled at split-time.
Novel compounds cannot be merged bythese algorithms.
The second group uses sym-bols to guide merging, inspired by work on mor-phology merging (Virpioja et al, 2007).
In theunmarked scheme where compound parts are notmarked with symbols, the special POS-tags areused to identify parts instead1.
The third groupis based on special part-of-speech tags for com-pounds (Stymne and Holmqvist, 2008), and merg-ing is performed if the part-of-speech tags match.This group of algorithms cannot be applied to thesepmarked scheme.In addition a restriction that the head of thecompound should have a compounding part-of-speech, that is, a noun, adjective, or verb, and arule to handle coordinated compounds are used.By using these additions and combinations of themain algorithms, a total of eight algorithms are ex-plored, as summarized in Table 1.
For all algo-rithms, compounds can have an arbitrary numberof parts.If there is a marked compound part that cannotbe combined with the next word, in any of the al-gorithms, the markup is removed, and the part isleft as a single word.
For the sepmarked system,coordinated compounds are handled as part of thesymbol algorithms, by using the special markupsymbol that indicates them.3.2 Merging PerformanceTo give an idea of the potential of the merging al-gorithms, they are evaluated on the split test refer-ence corpus, using the unmarked scheme.
The cor-pus has 55580 words, of which 4472 are identifiedas compounds by the splitting algorithm.
Of these4160 are known from the corpus, 245 are novel,1For the marked scheme using POS-tags to identify com-pound parts is equivalent to using symbols.and 67 are coordinated.
For the methods basedon symbols or part-of-speech, this merging task istrivial, except for reverse normalization, since allparts are correctly ordered.Table 2 shows the number of errors.
The POS-match algorithm with treatment of coordinationmakes 55 errors, 4 of which are due to coordinatedcompounds that does not use und as the conjunc-tion.
The other errors are due to errors in the re-verse normalization of novel compounds, whichhas an accuracy of 79% on this text.
The POS-match and symbol algorithms make additional er-rors on coordinated compounds.
The head-posrestriction blocks compounds with an adverb ashead, which gave better results on translation data,but increased the errors on this evaluation.
Theword list method both merges many words thatare not compounds, and do not merge any novelcompounds.
Using a list of compounds instead ofwords reduces the errors slightly.4 System DescriptionThe translation system used is a factored phrase-based translation system.
In a factored transla-tion model other factors than surface form canbe used, such as lemma or part-of-speech (Koehnand Hoang, 2007).
In the current system part-of-speech is used only as an output factor in the targetlanguage.
Besides the standard language model asequence model on part-of-speech is used, whichcan be expected to lead to better word order in thetranslation output.
There are no input factors, sono tagging has to be performed prior to translation,only the training corpus needs to be tagged.
In ad-dition, the computational overhead is small.
Onepossible benefit gained by using part-of-speech asan output factor is that ordering, both in general,and of compound parts, can be improved.
This hy-pothesis is tested by trying two system setups, withand without the part-of-speech sequence model.In addition part-of-speech is used for postprocess-64wlist wlist+head-pos clist symbol symbol+head-pos symbol+wlist POS-match POS-match+coord2393 1656 2257 118 205 330 118 55Table 2: Number of merging errors on the split reference corpusTokens TypesEnglish baseline 15158429 63692Germanbaseline 14356051 184215marked 15674728 93746unmarked 15674728 81806sepmarked 17007929 81808Table 3: Type and token counts for the 701157sentence training corpusing, both for uppercasing German nouns and as aknowledge source for compound merging.The tools used are the Moses toolkit (Koehn etal., 2007) for decoding and training, GIZA++ forword alignment (Och and Ney, 2003), and SRILM(Stolcke, 2002) for language models.
A 5-grammodel is used for surface form, and a 7-grammodel is used for part-of-speech.
To tune featureweights minimum error rate training is used (Och,2003), optimized against the Neva metric (Fors-bom, 2003).
Compound splitting is performed onthe training corpus, prior to training.
Merging isperformed after translation, both for test, and in-corporated into the tuning step.4.1 CorpusThe system is trained and tested on the Europarlcorpus (Koehn, 2005).
The training corpus is fil-tered to remove sentences longer than 40 wordsand with a length ratio of more than 1 to 7.
The fil-tered training corpus contains 701157 sentences.500 sentences are used for tuning and 2000 sen-tences for testing2.
The German side of the train-ing corpus is part-of-speech tagged using TreeTag-ger (Schmid, 1994).The German corpus has nearly three times asmany types, i.e., unique tokens, as the English cor-pus despite having a somewhat lower token count,as shown for the training corpus in Table 3.
Com-pound splitting drastically reduces the number oftypes, to around half or less, even though it is stilllarger than for English.
Marking on parts gives15% more types than no marking.2The test set is test2007 from the ACL 2008 Workshop onStatistical Machine Translation, http://www.statmt.org/wmt08/shared-task.html5 EvaluationTwo types of evaluation are performed.
The in-fluence of the different merging algorithms on theoverall translation quality is evaluated, using twoautomatic metrics.
In addition the performanceof the merging algorithms are analysed in somemore detail.
In both cases the effect of the POSsequence model is also discussed.
Even when thePOS sequence model is not used, part-of-speechis carried through the translation process, so that itcan be used in the merging step.5.1 Evaluation of TranslationTranslations are evaluated on two automatic met-rics: Bleu (Papineni et al, 2002) and PER, posi-tion independent error-rate (Tillmann et al, 1997).Case-sensitive versions of the metrics are used.PER does not consider word order, it evaluatesthe translation as a bag-of-word, and thus the sys-tems without part-of-speech sequence models canbe expected to do well on PER.
Note that PER isan error-rate, so lower scores are better, whereashigher scores are better for Bleu.These metrics have disadvantages, for instancebecause the same weight is given to all tokens,both to complex compounds, and to functionwords such as und (and).
Bleu has been criticized,see e.g.
(Callison-Burch et al, 2006; Chiang et al,2008).Table 4 and 5 shows the translation results usingthe different merging algorithms.
For the systemswith POS sequence models the baseline performsslightly better on Bleu, than the best systems withmerging.
Without the POS sequence model, how-ever, merging often leads to improvements, by upto 0.48 Bleu points.
For all systems it is advanta-geous to use the POS sequence model.For the baseline, the PER scores are higherfor the system without a POS sequence model,which, compared to the Bleu scores, confirmsthe fact that word order is improved by the se-quence model.
The systems with merging arebetter than the baseline with the POS sequencemodel.
In all cases, however, the systems withmerging performs worse when not using a POSsequence model, indicating that the part-of-speech65with POS-model without POS-modelunmarked sepmarked marked unmarked sepmarked markedword-list 17.93 17.66 18.92 17.70 17.29 18.69word-list + head-pos 19.34 19.07 19.60 19.13 18.63 19.38compound-list 18.94 17.77 18.13 18.56 17.40 17.86symbol 20.02 19.57 20.03 19.66 19.14 19.79symbol + head-pos 20.02 19.55 20.01 19.75 19.12 19.78symbol + word-list 20.03 19.72 20.02 19.76 19.29 19.79POS-match 20.12 ?
20.03 19.84 ?
19.80POS-match + coord 20.10 ?
19.97 19.85 ?
19.80Table 4: Translation results for Bleu.
Baseline with POS: 20.19, without POS: 19.66.
Results that arebetter than the baseline are marked with bold face.with POS-model without POS-modelunmarked sepmarked marked unmarked sepmarked markedword-list 29.88 28.64 28.19 30.27 29.94 28.71word-list + head-pos 27.49 26.07 27.26 27.78 27.22 27.84compound-list 26.92 27.99 29.25 27.46 29.07 29.74symbol 27.21 26.13 26.95 27.70 27.40 27.61symbol + head-pos 27.11 26.10 26.92 27.34 27.35 27.54symbol + word-list 26.86 25.54 26.80 27.15 26.72 27.39POS-match 26.99 ?
26.93 27.17 ?
27.53POS-match + coord 27.10 ?
26.93 27.28 ?
27.53Table 5: Translation results for PER.
Baseline with POS: 27.22, without POS: 26.49.
Results that arebetter than the baseline are marked with bold face.sequence model improves the order of compoundparts.When measured by PER, the best results whenusing merging are achieved by combining sym-bols and word lists, but when measured by Bleu,the POS-based algorithms are best.
The simplersymbol-based methods, often have similar scores,and in a few cases even better.
Adding treatmentof coordinated compounds to the POS-match al-gorithm changes scores marginally in both direc-tions.
The word list based methods, however, gen-erally give bad results.
Using the head-pos restric-tion improves it somewhat and using a compoundlist instead of a word list gives different results inthe different markup schemes, but is still worsethan the best systems.
This shows that some kindof internal knowledge source, either symbols orpart-of-speech, is needed in order for merging tobe successful.On both metrics, the marked and unmarked sys-tem perform similarly.
They are better than thesepmarked system on Bleu, but the sepmarked sys-tem is a lot better on PER, which is an indicationof that word order is problematic in the sepmarkedsystem, with its separate tokens to indicate com-pounds.5.2 Evaluation of MergingThe results of the different merging algorithms areanalysed to find the number of merges and the typeand quality of the merges.
In addition I investigatethe effect of using a part-of-speech model on themerging process.Table 6 shows the reduction of words3 achievedby applying the different algorithms.
The wordlist based method produces the highest numberof merges in all cases, performing many mergeswhere the parts are not recognized as such by thesystem.
The number of merges is greatly reducedby the head-pos restriction.
An investigation of theoutput of the word list based method shows thatit often merges common words that incidentallyform a new word, such as bei (at) and der (the)to beider (both).
Another type of error is due toerrors in the corpus, such as the merge of umwelt(environment) and und (and), which occurs in thecorpus, but is not a correct German word.
Thesetwo error types are often prohibited by the head-pos restrictions.
The compound list method avoidsthese errors, but it does not merge compounds thatwere not split by the splitting algorithm, due to ahigh frequency, giving a very low number of splitsin some cases.
There are small differences be-tween the POS-match and symbol algorithms.
Notusing the POS sequence model results in a highernumber of merges for all systems.A more detailed analysis was performed of the3The reduction of words is higher than the number of pro-duced compounds, since each compound can have more thantwo parts.66with POS-model without POS-modelunmarked sepmarked marked unmarked sepmarked markedword-list 5275 5422 4866 5897 5589 5231word-list + head-pos 4161 4412 4338 4752 4601 4661compound-list 4460 4669 3253 5116 4850 3534symbol 4431 4712 4332 5144 4968 4702symbol + head-pos 4323 4671 4279 4832 4899 4594symbol + word-list 4178 4436 4198 4753 4656 4530POS-match 4363 ?
4310 4867 ?
4618POS-match + coord 4361 ?
4310 4865 ?
4618Table 6: Reduction of number of words by using different merging algorithmswith POS-model without POS-modelunmarked sepmarked marked unmarked sepmarked markedKnown 3339 3594 3375 3747 3762 3587NovelGood 168 176 105 104 245 93Bad 20 97 8 10 64 7CoordinatedGood 43 43 42 42 37 44Bad 9 9 3 22 7 5Single partGood 6 ?
5 136 ?
33Bad 11 ?
16 52 ?
46Total 3596 3919 3554 4113 4115 3815Table 7: Analysis of merged compoundscompounds parts in the output.
The result of merg-ing them are classified into four groups: mergedcompounds that are known from the training cor-pus (2a) or that are novel (2b), parts that werenot merged (2c), and parts of coordinated com-pounds (2d).
They are classified as bad if the com-pound/part should have been merged with the nextword, does not fit into its context, or has the wrongform.
(2) a. Naturschutzpolitiknature protection policyb.
UN-FriedensplanUN peace planc.
* West- zulassenwest allowd.
Mittel- und OsteuropaCentral and Eastern EuropeFor the unmarked and sepmarked systems, theclassification was based on the POS-match con-straint, where parts are not merged if the POS-tagsdo not match.
POS-match cannot be used for thesepmarked scheme, which has standard POS-tags.Table 7 shows the results of this analysis.
Themajority of the merged compounds are knownfrom the training corpus for all systems.
Thereis a marked difference between the two systemsthat use POS-match, and the sepmarked systemthat does not.
The sepmarked system found thehighest number of novel compounds, but also havethe highest error rate for these, which shows thatit is useful to match POS-tags.
The other two sys-tems find fewer novel compounds, but also makefewer mistakes.
The marked system has more er-rors for single parts than the other systems, mainlybeacuse the form of compound parts were not nor-malized.
Very few errors are due to reverse nor-malization.
In the unmarked system with a POSsequence model, there were only three such errors,which is better than the results on split data in Sec-tion 3.2.Generally the percentage of bad parts or com-pounds is lower for the systems with a POS se-quence model, which shows that the sequencemodel is useful for the ordering of compoundparts.
The number of single compound parts isalso much higher for the systems without a POSsequence model.
80% of the merged compoundsin the unmarked system are binary, i.e., have twoparts, and the highest number of parts in a com-pound is 5.
The pattern for the other systems issimilar.All systems produce fewer compounds than the4472 in the German reference text.
However, theremight also be compounds in the output, that werenot split and merged.
These numbers are not di-rectly comparable to the baseline system, and ap-plying the POS-based splitting algorithm to trans-lation output would not give a fair comparison.An indication of the number of compounds in atext is the number of long words.
In the referencetext there are 351 words with at least 20 characters,67which will be used as the limit for long words.
Amanual analysis showed that all these words arecompounds.
The baseline system produces 209long words.
The systems with merging, discussedabove, all produce more long words than the base-line, but less than the reference, between 263 and307, with the highest number in the marked sys-tem.
The trend is the same for the systems with-out a POS sequence model, but with slightly fewerlong words than for the systems with merging.6 DiscussionThe choice of merging method has a large impacton the final translation result.
For merging to besuccessful some internal knowledge source, suchas part-of-speech or symbols is needed.
The pureword list based method performed the worst ofall systems on both metrics in most cases, whichwas not surprising, considering the evaluation ofthe merging algorithms on split data, where it wasshown that the word-list based methods mergedmany parts that were not compounds.The combination of symbols and word lists gavegood results on the automatic metrics.
An advan-tage of this method is that it is applicable for trans-lation systems that do not use factors.
However,it has the drawback that it does not merge novelcompounds, and finds fewer compounds than mostother algorithms.
The error analysis shows thatmany valid compounds are discarded by this algo-rithm.
A method that both find novel compounds,and that works well is that based on POS-match.In its current form it needs a decoder that can han-dle factored translation models.
It would, how-ever, be possible to use more elaborate symbolswith part-of-speech information, which would al-low a POS-matching scheme, without the need offactors.The error analysis of merging performanceshowed that merging works well, especially forthe two schemes where POS-matching is possi-ble, where the proportion of errors is low.
Italso showed that using a part-of-speech sequencemodel was useful in order to get good results,specifically since it increased the number of com-pound parts that were placed correctly in the trans-lation output.The sepmarked scheme is best on the PER met-ric it is worse on Bleu, and the error analysisshows that it performs worse on merging than theother systems.
This could probably be improvedby the use of special POS-tags and POS-matchingfor this scheme as well.
It is hard to judge whichis best of the unmarked and marked scheme.
Theyperform similarly on the metrics, and there is noclear difference in the error analysis.
The un-marked scheme does produce a somewhat highernumber of novel compounds, though.
A disadvan-tage of the marked scheme is that the compoundform is kept for single parts.
A solution for thiscould be to normalize parts in this scheme as well,which could improve performance, since reversenormalization performance is good on translationdata.The systems with splitting and merging havemore long words than the baseline, which indi-cates that they are more successful in creatingcompounds.
However, they still have fewer longwords than the reference text, indicating the needof more work on producing compounds.7 Conclusion and Future WorkIn this study I have shown that the strategy usedfor merging German compound parts in transla-tion output influences translation results to a largeextent.
For merging to be successful, it needssome internal knowledge source, carried throughthe translation process, such as symbols or part-of- speech.
The overall best results were achievedby using matching for part-of-speech.One factor that affects merging, which was notexplored in this work, is the quality of splitting.If splitting produces less erroneously split com-pounds than the current method, it is possiblethat merging also can produce better results, eventhough it was not clear from the error analysis thatbad splits were a problem.
A number of more ac-curate splitting strategies have been suggested fordifferent tasks, see e.g.
Alfonseca et al (2008),that could be explored in combination with merg-ing for machine translation.I have compared the performance of differentmerging strategies in one language, German.
Itwould be interesting to investigate these meth-ods for other compounding languages as well.
Ialso want to explore translation between two com-pounding languages, where splitting and mergingwould be performed on both languages, not onlyon one language as in this study.68ReferencesEnrique Alfonseca, Slaven Bilac, and Stefan Phar-ies.
2008.
Decompounding query keywords fromcompounding languages.
In Proceedings of ACL-08: HLT, Short Papers, pages 253?256, Columbus,Ohio.Andre?
Berton, Pablo Fetter, and Peter Regel-Brietzmann.
1996.
Compound words in large-vocabulary German speech recognition systems.
InProceedings of the Fourth International Conferenceon Spoken Language Processing (ICSLP), pages1165?1168, Philadelphia, Pennsylvania, USA.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of BLEU inmachine translation research.
In Proceedings of the11th Conference of EACL, pages 249?256, Trento,Italy.David Chiang, Steve DeNeefe, Yee Seng Chan, andHwee Tou Ng.
2008.
Decomposability of transla-tion metrics for improved evaluation and efficient al-gorithms.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 610?619, Honolulu, Hawaii.I?lknur Durgar El-Kahlout and Kemal Oflazer.
2006.Initial explorations in English to Turkish statisticalmachine translation.
In HLT-NAACL 2006: Pro-ceedings of the Workshop on Statistical MachineTranslation, pages 7?14, New York, NY.Eva Forsbom.
2003.
Training a super model look-alike: featuring edit distance, n-gram occurrence,and one reference translation.
In Proceedings ofthe Workshop on Machine Translation Evaluation:Towards Systemizing MT Evaluation, pages 29?36,New Orleans, Louisiana.Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In Proceedings of the Joint Con-ference on Empirical Methods in Natural languageProcessing and Computational Natural LanguageLearning, pages 868?876, Prague, Czech Republic.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In Proceedings ofthe 10th Conference of EACL, pages 187?193, Bu-dapest, Hungary.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACL,demonstration session, Prague, Czech Republic.Philipp Koehn, Abhishek Arun, and Hieu Hoang.2008.
Towards better machine translation quality forthe German-English language pairs.
In Proceedingsof the Third Workshop on Statistical Machine Trans-lation, pages 139?142, Columbus, Ohio.Philipp Koehn.
2005.
Europarl: a parallel corpus forstatistical machine translation.
In Proceedings ofMT Summit X, Phuket, Thailand.Stefan Langer.
1998.
Zur Morphologie und Seman-tik von Nominalkomposita.
In Tagungsband der4.
Konferenz zur Verarbeitung natu?rlicher Sprache(KONVENS), pages 83?97, Bonn, Germany.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of ACL, pages 160?167, Sap-poro, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting of the ACL, pages 311?318, Philadelphia, Pennsylvania.Maja Popovic?, Daniel Stein, and Hermann Ney.
2006.Statistical machine translation of German compoundwords.
In Proceedings of FinTAL ?
5th Interna-tional Conference on Natural Language Process-ing, pages 616?624, Turku, Finland.
Springer Ver-lag, LNCS.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, Manchester, UK.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the Sev-enth International Conference on Spoken LanguageProcessing (ICSLP), pages 901?904, Denver, Col-orado.Sara Stymne and Maria Holmqvist.
2008.
Process-ing of Swedish compounds for phrase-based statis-tical machine translation.
In Proceedings of the Eu-ropean Machine Translation Conference (EAMT08),pages 180?189, Hamburg, Germany.Sara Stymne.
2008.
German compounds in factoredstatistical machine translation.
In Aarne Ranta andBengt Nordstro?m, editors, Proceedings of GoTAL ?6th International Conference on Natural LanguageProcessing, pages 464?475, Gothenburg, Sweden.Springer Verlag, LNCS/LNAI.C.
Tillmann, S. Vogel, H. Ney, A. Zubiaga, andH.
Sawaf.
1997.
Accelerated DP based search forstatistical translation.
In Proceedings of the 5 th Eu-ropean Conference on Speech Communication andTechnology, pages 2667?2670, Rhodes, Greece.Sami Virpioja, Jaako J.Va?yrynen, Mathias Creutz, andMarkus Sadeniemi.
2007.
Morphology-aware sta-tistical machine translation based on morphs in-duced in an unsupervised manner.
In Proceedings ofMT Summit XI, pages 491?498, Copenhagen, Den-mark.69
