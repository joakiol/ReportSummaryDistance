Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 6?14,Prague, June 2007. c?2007 Association for Computational LinguisticsWord Similarity Metrics and Multilateral ComparisonBrett KesslerWashington University in St. Louisbkessler@wustl.eduAbstractPhylogenetic analyses of languages need toexplicitly address whether the languages un-der consideration are related to each other atall.
Recently developed permutation tests al-low this question to be explored by testingwhether words in one set of languages aresignificantly more similar to those in anotherset of languages when paired up by seman-tics than when paired up at random.
Sevendifferent phonetic similarity metrics are im-plemented and evaluated on their effective-ness within such multilateral comparisonsystems when deployed to detect genetic re-lations among the Indo-European and Uraliclanguage families.1 IntroductionBecause the historical development of languages isanalogous to the evolution of organisms, linguistsand biologists have been able to share much of theircladistic theory and practice.
But in at least onerespect, linguists are at a disadvantage.
While allcellular organisms on Earth are patently related toeach other, no such assumption can be made for lan-guages.
It is possible that languages were inventedmultiple times, so that the proper cladistic analy-sis of all human languages comprises a forest ratherthan a single tree.
Therefore historical linguists un-dertaking a cladistic analysis ?
more often referredto as subgrouping ?
have to ask a question that rarelyarises at all in biology: Are the entities for which Iam undertaking to draw a family tree related to eachother in the first place?The question of whether two or more languagesare related is addressed by looking at characters thatdiffer between languages and asking whether ob-served similarities in those characters are so greatas to lead to the conclusion that the languages havea common ancestor.
Researchers have investigatedmany types of characters for this purpose, includ-ing fairly abstract ones such as the structure ofparadigms, but the most commonly used charac-ters have been the individual morphemes of the lan-guage.
Morphemes are associations between stringsof phones and specific language functions such aslexical meanings or more general grammatical prop-erties.
Crucially, those associations are arbitraryto a very great extent.
Knowing that a ?tree?
is/strom/ in Czech will not help one figure out that itis /ets/ in Hebrew; nor should Hebrew speakers con-fronted with two Czech lexical morphemes, such as/strom/ vs /Hrad/, be able to guess which one means?tree?
and which one means ?castle?.
An implica-tion of this arbitrariness is that if one pairs mor-phemes by meaning between two languages, thatset of pairs should not have any systematic phoneticproperty that would not be obtained if morphemeswere paired up without regard to meaning.
Thus, ifone does observe some systematic phonetic propertyacross the semantically paired morphemes, one canconclude that there is some historical contingencythat gave those languages that property.
Namely, onecan conclude that at one time the languages sharedthe same morpheme for at least some of the mean-ings, either because of borrowing or because of de-scent from a common ancestor.The most straightforward application of this prin-6ciple is to see whether the morphemes for the sameconcept in two different languages appear unusu-ally similar to each other.
Anyone seeing that themorpheme for ?all?
was /?Al:/ in Old English and/al:/ in Old High German, that ?animal?
was /de:or/and /tior/, respectively, and that ?back?
was /hryd:Z/vs.
/hruk:/, and so forth, might well conclude thatthe languages were related to each other, as indeedthey were.
Unfortunately, the universal propertiesof language mean that even unrelated morphemeshave something in common; it is not always obviouswhether the amount of similarity between seman-tically matched morphemes is significantly greaterthan that between semantically mismatched mor-phemes.
For nearly two centuries now, the standardrecourse in case of doubt has been the comparativemethod.
One counts how many times the same pairof sounds match up in semantically matched mor-phemes; for example, Old English /d/ often corre-sponds to Old High German /t/.
A large number ofrecurrent sound correspondences appearing in sev-eral positions in a large number of different wordshas been considered proof that languages are related.This method is more sophisticated than eyeballingsimilarities, not least because it recognizes the effectof phonetic apomorphies ?
sound changes ?
suchas the change of /d/ to /t/ in Old High German.
Thestandard methodology gives no concrete guidance asto how many recurrent sound correspondences con-stitute proof.
However, there have been attemptsto recast the comparative method in terms of mod-ern statistical theory and experimental methodology,providing clearcut quantification of the magnitudeand significance of the evidence that languages arerelated (see Kessler, 2001, for recent developmentsand a summary of earlier work).One drawback to recent statistical adaptations ofthe comparative method is that they have been lim-ited to comparing two languages at a time.
It hasbeen claimed, however, most prominently by Green-berg (e.g., 1993), that when one wishes to testwhether a large set of languages are related, con-ducting a series of bilateral tests loses power: theremay be information contained in a pattern of rela-tions across three or more languages that is not man-ifest in the bilateral partitioning of the set of lan-guages.
Greenberg?s approach to multilateral com-parison was a step backward to the days before thedevelopment of the comparative method (Poser &Campbell, 1992).
By his own account, he simplyeyed the data and apparently never failed to concludethat languages were related.Most linguists have rejected Greenberg?s ap-proach and many have written detailed refutations(e.g., Campbell, 1988; Matisoff, 1990; Ringe, 1996;Salmons, 1992).
But Kessler and Lehtonen (2006)believed that multilateral comparison could be validand advantageous if applied with some statisticalrigour.
Adapting Greenberg?s basic approach, theydeveloped a methodology that involved computingphonetic similarity between semantically matchedmorphemes across several languages at a time.
Thiswas different from the comparative method, becauserecurrent sound correspondences were not sought:large numbers of recurrences are not typically foundacross large numbers of languages.
However, itis conceptually straightforward to aggregate sim-ilarity measures across morphemes in many lan-guages.
Crucially, the similarity across semanticallymatched morphemes was compared to that obtainedacross semantically mismatched morphemes.
Thus,this application of multilateral comparison is basedon the same principles about sound?meaning ar-bitrariness on which the comparative method wasbased.
Because the similarity computations werecompletely algorithmic and applied to data collectedin an unbiased fashion, the new methodology pro-vided a way to reliably quantify and test the signif-icance of phonetic similarity as evidence for histor-ical connections between two sets of multiple lan-guages.
Kessler and Lehtonen demonstrated that themethod was powerful enough to detect the relation-ship between 11 Indo-European languages and thatbetween 4 Uralic languages, but it did not detect anyconnection between those two families.The core of the multilateral comparison method-ology is the phonetic similary metric.
To my knowl-edge, Greenberg never specified any particular met-ric.
However, many different phonetic comparisonalgoriths have been proposed for many purposes,including this task of looking for similarities be-tween words (reviewed in Kessler, 2005); in partic-ular, Baxter and Manaster Ramer (2000) and Oswalt(1998) developed algorithms expressly for investi-gating language relatedness, though only in bilat-eral tests.
In this paper I explore several different7phonetic comparison algorithms and evaluate howwell they perform in Kessler and Lehtonen?s (2006)multilateral comparison task for Indo-European andUralic.2 Multilateral ComparisonThe basic multilateral algorithm is described inKessler and Lehtonen (2006); here I give a sum-mary of the relevant facts.
For each of 15 languages,we collected all of the words expressing conceptsin the Swadesh (1952) list of 200 concepts.
How-ever, words were discarded if they violated the keyassumptions discussed in the introduction.
For ex-ample, onomatopoeia and sound symbolism wouldviolate the assumption of arbitrariness: languagescould easily come up with similar words for thesame concept if they both resorted to natural asso-ciations between sounds and their meanings.
Gram-matical words were rejected because they tend tohave certain phonetic properties in common acrosslanguages, such as shortness; this also violates arbi-trariness.
Loanwords were discarded in order to fo-cus on genetic relationships rather than other typesof historical connection.In addition to rejecting some words outright, wetagged others for their relative suitability for a histor-ical analysis.
The concepts themselves were scoredfor how much confidence other researchers haveplaced in their suitability for glottochronologicalstudies.
Some of the contribution to this score wasquite subjective; other parts of it were derived fromstudies of retention rates: how long words express-ing the concept tend to survive before being replacedby other words.
The words were stripped down totheir root morpheme, and then tagged for how con-cordant that root meaning is with the target concept;for example, if a word for ?dirty?
literally means ?un-clean?, the root ?clean?
does not express the concept?dirty?
very well.
None of the conditions indicatedby these suitability measures invalidates the use ofa word, but low retention rates and complex seman-tic composition mean the word has a lower chanceof being truly old and consequently of being a verygood datum in a comparison of languages suspectedof being only distantly related.
These suitabilityscores were combined for each word in each lan-guage.
Then, in any given comparison between lan-guages, the suitability scores for each concept wereaggregated across words, and the 100 concepts withthe best rankings were selected for actual compari-son.
This technique both ensures the availability of areasonably large amount of data and also attempts toensure that the words themselves will be reasonablyprobative without biasing the test in either direction.In any single multilateral test, it is assumed thatwe have a single specific hypothesis: whether onegroup of one or more languages is related to anothergroup of one or more languages.
The approach takentherefore is to determine for each concept how dif-ferent the words in one group are to the words inthe other group.
If there are more than one wordin each group, then all crosspairs are computed andtheir average is taken.
This approach applies bothto the situation where there are multiple languagesin a group and multiple words for a given language.These averages are then summed across all 100 con-cepts, giving a single distance measure: a score ofhow different the two groups of languages are fromeach other.It is important to note, however, that this dis-tance measure is not meaningful in itself.
Sets oflanguages could get relatively low distance mea-sures just because their phonological inventoriesand phonotactics are very similar to each other?s;such typological similarity is not, however, strongevidence for historical connectedness between lan-guages.
Rather, what is needed is a relative compar-ison: how dissimilar would the words be across thetwo sets of languages if they were not matched bysemantics?
This is computed by randomly match-ing concepts in one set of languages with conceptsin another set of languages and recomputing thesum of the dissimilarity measures.
Each such rear-rangement may give a different total distance, whichmay not be representative, so this procedure is done100,000 times and the distance is averaged acrossall those iterations, yielding a very close estimate ofthe phonetic difference between words that are notmatched on semantics.
From this one can computethe proportion by which the semantically matcheddistance is less than the semantically mismatcheddistance.
This proportion is the magnitude m ofthe evidence in favour of the hypothesis that setsof languages are related to each other.
At the sametime that the magnitude is computed, one can also8compute the significance level of the hypothesis,by counting what proportion of the 100,000 rear-rangements has a total distance score that is at leastas small as that between the semantically matchedwords.
That number estimates how likely it is thatthe attested amount of evidence would have oc-curred by chance, given the phonology of the setsof languages.
This paper follows the usual con-vention in the social sciences of considering signifi-cance levels, p, below .05 as being reasonably com-fortable.While each individual test can tell the probabil-ity that two sets of languages are related, specificstudies may seek to find out which of three or moresets of languages are related.
To investigate that, anearest-neighbour hierarchical clustering is used.
Ineach cycle of the procedure, comparisons are madebetween all pairs of sets of languages to see whichpairs have significant evidence (p < .05) of beingrelated.
Of those, the pair with the highest magni-tude m are combined to form a new, larger, set oflanguages.
The cycles repeat until all languages aregrouped into one large set, or no pair of sets havesufficiently significant evidence of being related.3 Phonetic Distance MetricsPhonetic distance metrics can be evaluated on sev-eral different principles.
The ultimate goal is thatthey should result in p values that are very low whenlanguages are related and high when they are not re-lated.
Unfortunately, that goal is only partly evalu-able.
There are no two languages known for surenot to be related; otherwise there would be no mono-geneticists.
The best one can test for ism values thatcorrelate well with our incomplete knowledge of thedegree of relatedness between languages.Beyond basic engineering goals of simplicity andefficiency, therefore, a good algorithm should givea relatively low distance score for words or lan-guages known to be related.
To the extent possible,it should take minimal account of phonetic featuresthat change quickly over time, and weight moreheavily features that tend to be stable over time.It is perhaps less obvious that a phonetic dis-tance metric should be based on features that arewidespread, both across the languages of the worldand within individual languages.
To take a clearlyabsurd example, a bad metric would give a distanceof 0 if two words agree in whether or not they con-tained a click, and 1 otherwise.
For the vast major-ity of languages, all word pairs would be assigned adistance 0, because neither word has a click.
Sucha metric would find no evidence that any pair ofclickless languages are related, because the distanceof the semantically matched pairs would be no lessthan the distance of the mismatched pairs.
Simi-larly, even if a feature is found in both languages,it should be neither too common nor too rare.
Forexample, many languages have a contrast betweenlateral and central sounds, but lateral sounds tendto be vastly less common than central sounds.
Ametric that compares sounds based on central/lateraldistinctions may again end up finding little probativeevidence.
This observation may seem commonplacefor statisticians, but is worth pointing out becausethe tradition in historical linguistics has always beento look for pieces of evidence that are individuallyspectacular for their rarity, such as a pair of wordswhose first five sounds are all identical.
It is greatto report such evidence when it is found, but badto demand such evidence in advance, because typi-cally any specific type of spectacular evidence willnot show up even for related languages.
In a statis-tical analysis it is much better to look for commonpieces of evidence to ensure that their distributionin any particular study will be typical and thereforereasonably conducive to a reliable quantitative anal-ysis.A much more subtle danger is that a poorly cho-sen phonetic distance metric might be influenced byparts of the phonology that are not as completely ar-bitrary as one might like them to be.
Because the ar-bitrariness hypothesis is almost always observed tobe applicable in practice, and because it has attainedthe status of dogma, linguists do not know all thereis to know about conditions in which the associationbetween sound and meaning may not be entirely ar-bitrary and the ways in which that non-arbitrarinessmay repeat across languages, spuriously indicatingthat languages are related.
However, one strong con-tender for non-arbitrariness is word length.
It ap-pears to be true that words that are longer in onelanguage tend to be longer in another.
If a pho-netic distance metric is sensitive to word length, itcould indicate that semantically matched words are9more or less similar than mismatched words, just be-cause their length is similar.
This study attemptsto minimize that effect by discarding grammaticalwords, which tend to be systematically shorter thanlexical words.
It also reduces words to their rootmorpheme, in part because crosslinguistic tenden-cies favouring longer words are probably due largelyto a tendency to use more morphemes when buildinglower-frequency concepts.
Nevertheless, even thesesteps are not proof against matching-length effects,and so it would be better for phonetic distance met-rics not to be sensitive to word length.3.1 Candidate MetricsSeven different phonetic distance metrics were eval-uated for this study.C1-place.
The phonetic distance metric used byKessler and Lehtonen (2006) was based on the ob-servations that in language change, consonants tendto be more stable than vowels, the front of the wordtends to be more stable than the end of the word,and place of articulation tends to be more stable thanother features.
Consequently it is based on the placefeature of the first consonants (C1) found in the com-paranda; only if a comparandum has no consonant atall is its first vowel used instead.
Places of articula-tion are assigned integer values from 0 (lips) to 10(postvelar), and candidate phones are assigned a listof these values, which allows for secondary and dou-ble articulation.
The phonetic distance between twosounds is the smallest absolute difference betweenthe crosswise pairings of those place values.
In ad-dition, half a point is added if the two sounds are notidentical.
For example, when comparing the Old En-glish word for ?child?, /tSild/, with the correspondingOld High German word, /kind/, the algorithm wouldextract the first consonants, /tS/ and /k/; assign thepostalveolar /tS/ a place value of 4 and the velar /k/a value of 9; and report the difference plus an extra0.5 for being non-identical: 5.5.P1-Dolg.
Baxter and Manaster Ramer (2000), ina demonstration of bilateral comparison, used aphonetic distance metric adapted fom Dolgopol-sky (1986).
Dolgopolsky grouped sounds into 10classes, which were defined by a combination ofplace and manner of articulation.
Two sounds wereconsidered to have a distance of 0 between them ifthey fell in the same class; otherwise the distancewas 1.
Instead of using the first consonant in theword, the first phoneme (P1) is used instead, but allvowels are put in the same class.
Dolgopolsky?s ideawas to group together sounds that tend to changeinto each other over time; thus one class containsboth velar stops and postalveolar affricates, becausethe sound change [k]?
[tS] is common.
Thus in theexample of /tSild/ vs. /kind/, the reported distancewould be 0.C1-Dolg and P1-place.
These metrics were intro-duced in order to factor apart the two main differ-ences between C1-place and P1-Dolg.
C1-Dolg usesDolgopolsky classes but operates on the first conso-nant, if any, rather than on an initial vowel.
P1-placeuses the place comparison metrics of C1-place, butalways operates on the first phoneme, even if it is avowel.
So many morphemes begin with a consonantthat this is often a distinction without a difference,as in the ?child?
example.
But note how in compar-ing Old English /?
:G/ with Latin /o:w/, both ?egg?,the P1 versions would compare /?
:/ with /o:/, for adistance of 3.5 by the P1-place metric (palatal vs.velar vowels) and 0 by the P1-Dolg metric (all vow-els are in the same class); whereas the C1 metricswould compare /G/ with /w/, for a distance of 0.5 byC1-place (both sounds have velar components) and1 by C1-Dolg.P1-voice.
This metric is designed to be as sim-ple as possible.
Two words have a distance of 0 iftheir first phones agree in voicing, 1 if they disagree.Breathy voice was counted as voiced.
The idea hereis that phonation contrast is reasonably universal,and it is a relatively simple matter to partition allknown phones into two sets.C*-DolgSeq.
In the comparative method, the bestevidence for genetic relatedness is considered tobe the presence of several words that contain mul-tiple sounds that all evince recurrent sound cor-respondences.
In particular, multiple consonantmatches between words are often sought as partic-ularly probative evidence.
This metric implementsthis desideratum by lining up all the consonants (C*)in the words sequentially (hence Seq).
Each suchpair of aligned consonants contributes a distance of1 to the cumulative distance between the words if the10consonants are not in the same Dolgopolsky class.If the one word has more consonants than the otherword, alignment begins at the beginning of the word,and the extra consonants at the end are ignored.
Toavoid making this metric sensitive to word length,the total distance is divided by the number of conso-nant pairs.
Continuing the ?child?
example, /tS/ and/k/ contribute 0 because they are in the same Dol-gopolsky class; /l/ and /n/ contribute 1 because theyare in different classes; and /d/ and /d/ contribute 0;the sum 1 is averaged across 3 comparisons to givea score of 0.33.C*-DolgCross.
Although the C*-DolgSeq metricattempts to exploit information from multiple con-sonants in each pair of words, it fails to exploit allpossible information.
The extra consonants at theend of the longer word are ignored.
Further, there isthe possibility that the sequential alignment wouldfail under some fairly common situations.
For ex-ample, if in one language a consonant is deleted orvocalized, the later consonants will not be alignedcorrectly.
To address this issue, this metric exam-ines all crosswise pairs of consonants and reportstheir average Dolgopolsky metric.
In the example,/tS/ is compared to /k/ (0), /n/ (1), and /d/ (1); /l/ iscompared to /k/ (1), /n/ (1), and /d/ (1); and /d/ iscompared to /k/ (1), /n/ (1), and /d/ (0).
Thus themetric is 7/9, or 0.78.3.2 TestData from 15 languages were used.
These languageswere selected to give a reasonably wide range ofvariation in their relatedness to each other.
Elevenof the languages were Indo-European, and four wereUralic.
Within both of those families there aresubclades that are noticeably more closely relatedto each other than to other languages in the samefamily.
The Indo-European set contains four Ger-manic languages (Old English, Old High German,Gothic and Old Norse) and two Balto-Slavic lan-guages (Lithuanian and Old Church Slavonic); allthe other languages are traditionally considered asbelonging to separate branches of Indo-European:Latin, Albanian, Greek, Latin, Old Irish, and San-skrit.
The Uralic set contains three languages thatsubgroup in a clade called Finno-Ugric (Finnish,Hungarian, and Mari), which is rather distinct fromthe Samoyedic branch, which contains Nenets.
Sev-eral linguists believe that the Indo-European andUralic languages are related to each other (e.g.,Bomhard, 1996; Greenberg, 2000; Kortlandt, 2002),though this hypothesis is far from being universallyaccepted.
For each of the 15 languages, transla-tion equivalents were found for each of the Swadesh200 concepts, as described in Kessler and Lehtonen(2006).The multilateral comparison algorithm describedabove was performed once with each of the above-described phonetic distance metrics.
Each of theanalyses comprised a complete hierarchical clus-tering of all 15 languages.
For each metric, themain concern was whether a multilateral analysisperformed with it would group together languagesknown to be related, however remotely.
A secondquestion was what similarity magnitudes would bereported for languages known to be related.
In gen-eral one would expect a good phonetic distance met-ric to yield high magnitudes and low p values forlanguages known to be related, and that, all thingsbeing equal, magnitudes should increase the moreclosely related the languages are.A large amount of information is available abouteach run of the program.
The algorithm begins byperforming bilateral comparisons for each pair oflanguages, and it might be somewhat interesting tocompare those 105 data points across each of theseven metrics.
Perhaps more interesting and decid-edly more succinct is to focus on the numbers foreach of the major clades described above (Table 1).Because almost all of the runs of the program cre-ated clusters that contained exactly the languages ineach of the clades named in the column headers, itwas possible to show the m value reported by theprogram when that cluster was formed: the degreeof similarity between the two subsclusters that werejoined to form the cluster in question.
For exam-ple, when the algorithm using the C1-place metricjoined Old Norse up with a cluster containing OldEnglish, Old High German, and Gothic, it reportedan m value of .65 between those two groups.
Be-cause of the nature of the clustering algorithm, thisrepresents the weakest link within the clade: in gen-eral, the similarity between languages in each ofthose two subclades will be higher than this number.A striking feature of Table 1 is the stability of11Metric Germanic Balto- Indo- Finno- Uralic Indo-Slavic European Ugric UralicC1-place .65** .43** .12** .23** .09* .00C1-Dolg .65** .42** .12** .26** .09** .02*C*-DolgCross .22** .14** .05** .10** .05** .01C*-DolgSeq .57** .37** .09** .22** .07** .02*P1-Dolg .66** .41** .13** .25** .10** .02P1-place .66** .45** .13** .31** .09* -.01P1-voice .68** .57** (.19) .37** (.05) (.05)Table 1: Similarity Magnitudes Reported for Each Linguistic Clade.
*p < .05.
**p < .001.
Numbers arethe m values reported when the clade is constructed via clustering.
If the algorithm does not posit the cladeas a cluster, table reports in parentheses the average m reported for each pair of languages in the clade.the algorithm across different phonetic distance met-rics.
All of them constructed the relatively easy sub-clades (Germanic, Balto-Slavic, and Finno-Ugric),reporting very strong significance values.
All ofthem except P1-voice constructed Indo-Europeanand Uralic, which are both fairly difficult to identify;in fact P1-voice nearly did so, except that it mis-classed Nenets with the Indo-European languages.All of them assigned very low similarity magnitudesto a proposed Indo-Uralic grouping: that is, theyfound very little similarity between Indo-Europeanand Uralic words for the same concept.
Further-more, the magnitudes for the various clades are allranked in the same order.
As one would hope, thesubclades within each family are given much higherm values than the families themselves.In direct comparisons between comparable ver-sion of the place metric and the Dolgopolsky metric(C1-place vs. C1-Dolg and P1-place vs. P1-Dolg),no very consistent patterns emerge.
But the Dolgo-polsky metrics tend to reveal the Uralic family withmuch higher significance levels than do the othermeasures, and they are also the only metrics thatever posit an Indo-Uralic clade at acceptable signif-icance levels (C1-Dolg at p = .04; C*-DolgSeq atp = .02).
An optimistic explanation is that the Dol-gopolsky classes are better at finding subtle evidenceof language relatedness, and that this may be dueto their being constructed eclectically.
Sounds wereclaimed to have been grouped into classes based onthe frequency with which they are known to developinto each other in the course of language change(Dolgopolsky, 1986:35), not based on any a prioriprinciple; place of articulation clearly is a consid-eration, but there are many other factors involved.For example, one group comprises the coronal ob-struents, except that sibilant fricatives are in a sep-arate group of their own, and sibilant affricates aregrouped with the velars.
One might expect a systembased on empirical data to perform better than onebased on a monothetic property such as place of par-ticulation.
However, it must also be cautioned thatDolgopolsky did not explain how he gathered thestatistics upon which his classes are based.
Sincethe classes were introduced in a paper designed toshow that Indo-European and Uralic, among otherfamilies, are related to each other, it is possible thatthe statistics were informed at least in part by pat-terns he perceived between those language families.There is therefore some small cause to be concernedthat Dolgopolsky classes may be, if only inadver-tently, somewhat tuned to the Indo-Uralic data andtherefore not completely unbiased with respect tothe research question.A more consistent trend in the table is that themetrics that attempt to incorporate more informa-tion about the comparanda return lower similaritymagnitudes.
The C*-DolgSeq metric, which alignsthe consonants and reports the average distanceacross all the pairs.
gave substantially lower num-bers than the metrics that analyze single phonemes.This observation applies even more strongly to theC*-DolgCross metric, which reported magnitudes athird the size of other measures.
The result is notunexpected.
It is common knowledge that initialconsonants tend to be more stable than other conso-12nants in the word; incorporating non-initial conso-nants into the metric means that a higher proportionof the data the metric looks at will be more dissimi-lar.
This being the case, it may seem surprising thatC*-DolgSeq and C*-DolgCross showed essentiallythe same connections between languages as did theother metrics, and at strong significance levels.
Eventhough the similarity levels are close to backgroundlevels (those of semantically unmatched pairs), theyare still measurably above background levels; the pvalues are only concerned with whether the matcheddata is more similar than the unmatched data, not byhow much they are different.P1-voice was introduced to experiment with ametric that takes the other approach: instead of in-corporating more material into the measure, it incor-porates less.
Being based on a single binary pho-netic feature, P1-voice is arguably the most mini-mal metric possible.
Perhaps not unexpectedly, ithas the opposite effect of that of C*-DolgSeq andC*-DolgCross: m measures are raised.
At the sametime, this metric too appears to reveal the known re-lations between languages.
The several gaps in thetable are due to a single odd choice that the algo-rithm made: it concluded that the Uralic languageNenets was quite similar to the Germanic languages,at least with respect to whether the first sound isvoiced in semantically matched words.
Presumablythis connection was just a chance accident; indeed,saying that one is working with significance levelsof .05 is another way of saying that one is willing totolerate such errors about 5% of the time.4 ConclusionsThe evaluation of the methodology across 15 lan-guages did not provide overwhelming evidencefavouring one type of phonetic distance metric overanother.
Perhaps, by a small margin, the strongestresults are obtained by comparing what Dolgopol-sky classes the first consonants ?
or, equally well,the first phonemes?
of the words fall into, but noth-ing seriously warns the researcher away from otherapproaches.Conceivably further experiments with other datasets will reveal strengths and weaknesses of differentmetrics more convincingly.
Until such time, how-ever, it may be most useful to choose phonetic dis-tance metrics primarily on theoretical, if not philo-sophical, criteria.
Metrics that look at many parts ofthe word have the advantage of not missing infor-mation, even if it turns up in unusual places.
It isnot unknown for a branch of a language family to dosomething unusual like drop all initial consonants;in such an event, all the single-phoneme metrics ex-plored here would fail entirely.
One does not reallywish to change one?s metric for different sets of lan-guages, because if one has the freedom to fish fordifferent metrics until a test succeeds, one can alm-sot certainly ?
and spuriously ?
prove that almostall languages are related.
So there is some advan-tage to having a metric that covers all the bases.
Butthe similarity measures returned under such circum-stances do tend to be small, and although such re-duction inm did not seem to have any deleterious ef-fect in the present experiment, it is not unreasonableto worry that weak similarity measures may causeproblems in some data sets.
Further, the more of aword one is looking at, the more likely it is that onewill inadvertently encode length information into themetric.The main conclusion to be drawn from this studyis that the basic methodology is very hospitable toa variety of phonetic distance metrics and performsadequately and stably with any reasonable met-ric.
Unlike parametric methods, this randomization-based methodology does not require the researcherto develop new formulas to compute strength andsignificance values for each new distance metric.The simple expedient of randomly rearranging thedata a large number of times and recomputing thedistance metric for each rearrangement provides themost literal and straightforward way of applying thekey insight of the arbitrariness hypothesis: the pho-netic similarity of semantically matched words willbe no greater than that of semantically mismatchedones, unless some historical contingency such as de-scent from a common language is involved.ReferencesWilliam Baxter and Alexis Manaster Ramer.
2000.
Be-yond lumping and splitting: probabilistic issues inhistorical linguistics.
In Time Depth in HistoricalLinguistics, eds.
C. Renfrew, A.
McMahon., and L.Trask.
McDonald Institute for Archaeological Re-search, Cambridge, England.
167?188.13Allan R. Bomhard.
1996.
Indo-European and theNostratic Hypothesis.
SIGNUM Desktop Publishing,Charleston, SC.Lyle Campbell.
1988. Review of Greenberg (1987).Language 64:591?615.Aaron B. Dolgopolsky.
1986.
A probabilistic hypothe-sis concerning the oldest relationships among the lan-guage families of northern Eurasia.
In Typology, Re-lationship, and Time: A Collection of Papers on Lan-guage Change and Relationship by Soviet Linguists,eds.
V. V. Shevoroshkin and T. L. Markey.
Karoma,Ann Arbor, MI.
27?50.Joseph H. Greenberg.
1993.
Observations concerningRinge?s Calculating the Factor of Chance in LanguageComparison.
Proceedings of the American Philosoph-ical Society, 137, 79?89.Joseph H. Greenberg.
2000.
Indo-European and its Clos-est Relatives: the Eurasiatic Language Family: Gram-mar.
Stanford University Press, Stanford, CA.Brett Kessler.
2001.
The Significance of Word Lists.Center for the Study of Language and Information,Stanford, CA.Brett Kessler.
2005.
Phonetic comparison algorithms.Transactions of the Philological Society 103:243?260.Brett Kessler and Annukka Lehtonen.
2006.
Multilateralcomparison and significance testing of the Indo-Uralicquestion.
Phylogenetic Methods and the Prehistory ofLanguages, eds.
P. Forster and C. Renfrew.
McDon-ald Institute for Archaeological Research, Cambridge,England.
33?42.Frederik Kortlandt.
2002.
The Indo-Uralic verb.
InFinno-Ugrians and Indo-Europeans: Linguistic andLiterary Contacts.
Shaker, Maastricht, 217?227.James A. Matisoff.
1990.
On megalocomparison.
Lan-guage 66:106?120.Robert L. Oswalt.
1998.
A probabilistic evaluation ofNorth Eurasiatic Nostratic.
In Nostratic: Sifting theEvidence., eds.
J. C. Salmons and B.D.
Joseph.
Ben-jamins, Amsterdam.
199?216.William J. Poser and Lyle Campbell.
1992.
Indo-European practice and historical methodology.
InProceedings of the Eighteenth Annual Meeting ofthe Berkeley Linguistics Society, eds.
L. A. Buszard-Welcher, L. Wee, and W. Weigel.
Berkeley LinguisticsSociety, Berkeley, CA.
214?236.Donald A. Ringe.
1996.
The mathematics of ?Amerind?.Diachronica 13:135?154.Joseph Salmons.
1992.
A look at the data for a global et-ymology: *Tik ?finger?.
In Explanation in HistoricalLinguistics, eds.
G.W.
Davis and G.K. Iverson.
Ben-jamins, Amsterdam, 207?228.Morris Swadesh.
1952.
Lexico-statistic dating of pre-historic ethnic contacts.
Proceedings of the AmericanPhilosophical Society 96:452?463.14
