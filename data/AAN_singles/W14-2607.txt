Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 32?41,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsSemantic Role Labeling of Emotions in TweetsSaif M. Mohammad, Xiaodan Zhu, and Joel MartinNational Research Council CanadaOttawa, Ontario, Canada K1A 0R6{saif.mohammad,xiaodan.zhu,joel.martin}@nrc-cnrc.gc.caAbstractPast work on emotion processing has fo-cused solely on detecting emotions, andignored questions such as ?who is feelingthe emotion (the experiencer)??
and ?to-wards whom is the emotion directed (thestimulus)??.
We automatically compile alarge dataset of tweets pertaining to the2012 US presidential elections, and anno-tate it not only for emotion but also forthe experiencer and the stimulus.
We thendevelop a classifier for detecting emotionthat obtains an accuracy of 56.84 on aneight-way classification task.
Finally, weshow how the stimulus identification taskcan also be framed as a classification task,obtaining an F-score of 58.30.1 IntroductionDetecting emotions in text has a number of ap-plications including tracking sentiment towardspoliticians, movies, and products (Pang and Lee,2008), identifying what emotion a newspaperheadline is trying to evoke (Bellegarda, 2010),developing more natural text-to-speech systems(Francisco and Gerv?as, 2006), detecting how peo-ple use emotion-bearing-words and metaphors topersuade and coerce others (for example, in pro-paganda) (K?ovecses, 2003), tracking response tonatural disasters (Mandel et al., 2012), and soon.
With the rapid proliferation of microblogging,there is growing amount of emotion analysis re-search on newly available datasets of Twitter posts(Mandel et al., 2012; Purver and Battersby, 2012;Mohammad, 2012b).
However, past work has fo-cused solely on detecting emotional state.
It hasignored questions such as ?who is feeling the emo-tion (the experiencer)??
and ?towards whom is theemotion directed (the stimulus)?
?.In this paper, we present a system that analyzestweets to determine who is feeling what emotion,and towards whom.
We use tweets from the 2012US presidential elections as our dataset, since weexpect political tweets to be particularly rich inemotions.
Further, the dataset will be useful forapplications such as determining political align-ment of tweeters (Golbeck and Hansen, 2011;Conover et al., 2011b), identifying contentiousissues (Maynard and Funk, 2011), detecting theamount of polarization in the electorate (Conoveret al., 2011a), and so on.Detecting the who, what, and towards whomof emotions is essentially a semantic role-labelingproblem (Gildea and Jurafsky, 2002).
The seman-tic frame for ?emotions?
in FrameNet (Baker et al.,1998) is shown in Table 1.
In this work, we fo-cus on the roles of Experiencer, State, and Stim-ulus.
Note, however, that the state or emotion isoften not explicitly present in text.
Other rolessuch as Reason, Degree, and Event are also of sig-nificance, and remain suitable avenues for futurework.We automatically compile a large dataset of2012 US presidential elections using a small num-ber of hand-chosen hashtags.
Next we annotatethe tweets for Experiencer, State, and Stimulusby crowdsourcing to Amazon?s Mechanical Turk.1We analyze the annotations to determine the dis-tributions of different types of roles, and show thatthe dataset is rich in emotions.
We develop a clas-sifier for emotion detection that obtains an accu-racy of 56.84.
We find that most of the tweetsexpress emotions of the tweeter, and only a feware indicative of the emotions of someone else.Finally, we show how the stimulus identificationtask can be framed as a classification task that cir-cumvents more complicated problems of detectingentity mentions and coreferences.
Our supervisedclassifier obtains an F-score of 58.30 on this task.1https://www.mturk.com/mturk/welcome32Table 1: The FrameNet frame for emotions.
The three roles investigated in this paper are shown in bold.Role DescriptionCore:Event The Event is the occasion or happening that Experiencers in a certain emotional state participate in.Experiencer The Experiencer is the person or sentient entity that experiences or feels the emotions.Expressor The body part, gesture, or other expression of the Experiencer that reflects his or her emotional state.State The State is the abstract noun that describes a more lasting experience by the Experiencer.Stimulus The Stimulus is the person, event, or state of affairs that evokes the emotional response in the Experiencer.Topic The Topic is the general area in which the emotion occurs.
It indicates a range of possible Stimulus.Non-Core:Circumstances The Circumstances is the condition(s) under which the Stimulus evokes its response.Degree The extent to which the Experiencer?s emotion deviates from the norm for the emotion.Empathy target The Empathy target is the individual or individuals with which the Experiencer identifies emotionally.Manner Any way the Experiencer experiences the Stimulus which is not covered by more specific frame elements.Parameter The Parameter is a domain in which the Experiencer experiences the Stimulus.Reason The Reason is the explanation for why the Stimulus evokes a certain emotional response.2 Related WorkOur work here is related to emotion analysis, se-mantic role labeling (SRL), and information ex-traction (IE).Much of the past work on emotion detectionfocuses on emotions argued to be the most ba-sic.
For example, Ekman (1992) proposed six ba-sic emotions?joy, sadness, anger, fear, disgust,and surprise.
Plutchik (1980) argued in favorof eight?Ekman?s six, surprise, and anticipation.Many of the automatic systems use affect lexi-cons pertaining to these basic emotions such asthe NRC Emotion Lexicon (Mohammad and Tur-ney, 2010), WordNet Affect (Strapparava and Val-itutti, 2004), and the Affective Norms for EnglishWords.2Affect lexicons are lists of words and as-sociated emotions.Emotion analysis techniques have been appliedto many different kinds of text (Mihalcea and Liu,2006; Genereux and Evans, 2006; Neviarouskayaet al., 2009; Mohammad, 2012a).
More recentlythere has been work on tweets as well (Bollenet al., 2011; Tumasjan et al., 2010; Mohammad,2012b).
Bollen et al.
(2011) measured tension,depression, anger, vigor, fatigue, and confusionin tweets.
Tumasjan et al.
(2010) study Twitteras a forum for political deliberation.
Mohammad(2012b) developed a classifier to identify emotionsusing tweets with emotion word hashtags as la-beled data.
However, none of this work exploresthe many semantic roles of emotion.Semantic role labeling (SRL) identifies seman-tic arguments and roles with regard to a predicate2http://www.purl.org/net/NRCEmotionLexiconhttp://csea.phhp.ufl.edu/media/anewmessage.htmlin a sentence (Gildea and Jurafsky, 2002; M`arquezet al., 2008; Palmer et al., 2010).
More recently,there has also been some work on semantic rolelabeling of tweets for verb and nominal predi-cates (Liu et al., 2012; Liu et al., 2011).
Thereexists work on extracting opinions and the top-ics of opinions, however most of it if focused onopinions about product features (Popescu and Et-zioni, 2005; Zhang et al., 2010; Kessler and Ni-colov, 2009).
For example, (Kessler and Nicolov,2009) identifies semantic relations between sen-timent expressions and their targets for car anddigital-camera reviews.
However, there is no workon semantic role labeling of emotions in tweets.We use many of the ideas developed in the senti-ment analysis work and apply them to detect thestimulus of emotions in the electoral tweets data.Our work here is also related to template fillingin information extraction (IE), for example as de-fined in MUC (Grishman, 1997), which extractsinformation (entities) from a document to fill outa pre-defined template, such as the date, location,target, and other information about an event.3 Challenges of Semantic Role Labelingof Emotions in TweetsSemantic role labeling of emotions in tweets posescertain unique challenges.
Firstly, there are manydifferences between tweets and linguistically well-formed texts, such as written news (Liu et al.,2012; Ritter et al., 2011).
Tweets are often lesswell-formed?they tend to be colloquial, havemisspellings, and have non-standard tokens.
Thus,methods depending heavily on deep language un-derstanding such as syntactic parsing (Kim andHovy, 2006) are less reliable.33Secondly, in a traditional SRL system, an ar-gument frame is a cohesive structure with strongdependencies between the arguments.
Thus it isoften beneficial to develop joint models to identifythe various elements of a frame (Toutanova et al.,2005).
However, these assumptions are less viablewhen dealing with emotions in tweets.
For exam-ple, there is no reason to believe that people with acertain name will have the same emotions towardsthe same entities.
On the other hand, if we makeuse of information beyond the target tweet to inde-pendently identify the political leanings of a per-son, then that information can help determine theperson?s emotions towards certain entities.
How-ever, that is beyond the scope of this paper.
Thuswe develop independent classifiers for identifyingexperiencer, state, and stimulus.Often, the goal in SRL and IE template fillingis the labeling of text spans in the original text.However, emotions are often not explicitly statedin text.
Thus we develop a system that assigns anemotion to a tweet even though that emotion is notexplicitly mentioned.
The stimulus of the emo-tion may also not be mentioned.
Consider Happyto see #4moreyears come into reality.
The stimu-lus of the emotion joy is to see #4moreyears comeinto reality.
However, the tweet essentially con-veys the tweeter?s joy towards Barack Obama be-ing re-elected as president.
One may argue thatthe true stimulus here is Barack Obama.
Thus it isuseful to normalize mentions and resolve the co-reference, for example, all mentions of Barack H.Obama, Barack, Obama, and #4moreyears shouldbe directed to the same entity.
Thus, we ground(in the same sense as in language grounding) theemotional arguments to the predefined entities.Through our experiments we show the target of anemotion in political tweets is often one among ahandful of entities.
Thus we develop a classifier toidentify which of these pre-chosen entities is thestimulus in a given tweet.4 Data Collection and Annotation4.1 Identifying Electoral TweetsWe created a corpus of tweets by polling the Twit-ter Search API, during August and September2012, for tweets that contained commonly knownhashtags pertaining to the 2012 US presidentialelections.
Table 2 shows the query terms weused.
Apart from 21 hashtags, we also collectedtweets with the words Obama, Barack, or Rom-Table 2: Query terms used to collect tweets per-taining to the 2012 US presidential elections.#4moreyears #Barack #campaign2012#dems2012 #democrats #election#election2012 #gop2012 #gop#joebiden2012 #mitt2012 #Obama#ObamaBiden2012 #PaulRyan2012 #president#president2012 #Romney #republicans#RomneyRyan2012 #veep2012 #VP2012Barack Obama Romneyney.
We used these additional terms because theyare names of the two presidential candidates, andthe probability that these words were used to referto somebody else in tweets posted in August andSeptember of 2012 was low.The Twitter Search API was polled every fourhours to obtain new tweets that matched the query.Close to one million tweets were collected, whichwe will make freely available to the research com-munity.
The query terms which produced the high-est number of tweets were those involving thenames of the presidential candidates, as well as#election2012, #campaign, #gop, and #president.We used the metadata tag ?iso language code?to identify English tweets.
Since this tag is not al-ways accurate, we also discarded tweets that didnot have at least two valid English words.
Weused the Roget Thesaurus as the English word in-ventory.3This step also helps discard very shorttweets and tweets with a large proportion of mis-spelled words.
Since we were interested in deter-mining the source and target of emotions in tweets,we decided to focus on original tweets as opposedto retweets.
We discarded retweets, which can eas-ily be identified through the presence of RT, rt, orRt in the tweet (usually in the beginning of thepost).
Finally, there remained close to 170,000original English tweets.4.2 Annotating Emotions by CrowdsourcingWe used Amazon?s Mechanical Turk service tocrowdsource the annotation of the electoral tweets.We randomly selected about 2,000 tweets, each bya different Twitter user.
We set up two question-naires on Mechanical Turk for the tweets.
The firstquestionnaire was used to determine the numberof emotions in a tweet and also whether the tweetwas truly relevant to the US politics.3www.gutenberg.org/ebooks/1068134Questionnaire 1: Emotions in the US election tweetsTweet: Mitt Romney is arrogant as hell.Q1.
Which of the following best describes the emotions inthis tweet??
This tweet expresses or suggests an emotional attitudeor response to something.?
This tweet expresses or suggests two or more contrast-ing emotional attitudes or responses.?
This tweet has no emotional content.?
There is some emotion here, but the tweet does not giveenough context to determine which emotion it is.?
It is not possible to decide which of the above optionsis appropriate.Q2.
Is this tweet about US politics and elections??
Yes, this tweet is about US politics and elections.?
No, this tweet has nothing to do with US politics oranybody involved in it.These questionnaires are called HITs (Human In-telligence Tasks) in Mechanical Turk parlance.
Weposted 2042 HITs corresponding to 2042 tweets.We requested responses from at least three anno-tators for each HIT.
The response to a HIT by anannotator is called an assignment.
In MechanicalTurk, an annotator may provide assignments for asmany HITs as they wish.
Thus, even though onlythree annotations are requested per HIT, dozensof annotators contribute assignments for the 2,042tweets.The tweets that were marked as having oneemotion were chosen for annotation by the Ques-tionnaire 2.
We requested responses from at leastfive annotators for each of these HITs.
Below isan example:Questionnaire 2:Who is feeling what, and towards whom?Tweet: Mitt Romney is arrogant as hell.Q1.
Who is feeling or who felt an emotion?Q2.
What emotion?
Choose one of the options from belowthat best represents the emotion.?
anger or annoyance or hostility or fury?
anticipation or expectancy or interest?
disgust or dislike?
fear or apprehension or panic or terror?
joy or happiness or elation?
sadness or gloominess or grief or sorrow?
surprise?
trust or likeTable 3: Questionnaire 1: Percentage of tweetsin each category of Q1.
Only those tweets thatwere annotated by at least two annotators were in-cluded.
A tweet belongs to category X if it is an-notated with X more often than all other categoriescombined.
There were 1889 such tweets in total.Percentageof tweetssuggests an emotional attitude 87.98suggests two contrasting attitudes 2.22no emotional content 8.21some emotion; not enough context 1.32unknown; not enough context 0.26all 100.0Q3.
Towards whom or what?After performing a small pilot annotationeffort, we realized that the stimulus in most ofthe electoral tweets was one among a handfulof entities.
Thus we reformulated question 3 asshown below:Q3.
What best describes the target of the emotion??
Barack Obama and/or Joe Biden?
Mitt Romney and/or Paul Ryan?
Some other individual?
Democratic party, democrats, or DNC?
Republican party, republicans, or RNC?
Some other institution?
Election campaign, election process, or elections?
The target is not specified in the tweet?
None of the above4.3 Annotation AnalysesFor each annotator and for each question, we cal-culated the probability with which the annotatoragreed with the response chosen by the majorityof the annotators.
We identified poor annotators asthose that had an agreement probability more thantwo standard deviations away from the mean.
Allannotations by these annotators were discarded.We determine whether a tweet is to be assigneda particular category based on strong majorityvote.
That is, a tweet belongs to category X ifit was annotated by at least three annotators andonly if at least half of the annotators agreed witheach other.
Percentage of tweets in each of the fivecategories of Q1 are shown in Table 3.
Observethat the majority category for Q1 is ?suggests anemotion?
?87.98% of the tweets were identifiedas having an emotional attitude.35Table 4: Questionnaire 2: Percentage of tweetsin the categories of Q2.
Only those tweets thatwere annotated by at least three annotators wereincluded.
A tweet belongs to category X if it isannotated with X more often than all other cate-gories combined.
There were 965 such tweets.PercentageEmotion of tweetsanger 7.41anticipation 5.01disgust 47.75fear 1.98joy 6.58sadness 0.83surprise 6.37trust 24.03all 100.00Responses to Q2 showed that a large majority(95.56%) of the tweets were relevant to US pol-itics and elections.
This shows that the hashtagsshown earlier in Table 2 were effective in identify-ing political tweets.As mentioned earlier, only those tweets thatwere marked as having an emotion (with highagreement) were annotated further through Ques-tionnaire 2.Responses to Q1 of Questionnaire 2 revealedthat in the vast majority of the cases (99.825%),the tweets contains emotions of the tweeter.
Thedata did include some tweets that referred to emo-tions of others such as Romney, GOP, and pres-ident, but these instances are rare.
Tables 4 and5 give the distributions of the various options forQuestions 2, and 3 of Questionnaire 2.
Table 4shows that disgust (49.32%) is by far the mostdominant emotion in the tweets of 2012 US pres-idential elections.
The next most prominent emo-tion is that of trust (23.73%).
About 61% of thetweets convey negative emotions towards some-one or something.
Table 5 shows that the stimulusof emotions was often one of the two presidentialcandidates (close to 55% of the time)?Obama:29.90%, Romney: 24.87%.4.3.1 Inter-Annotator AgreementWe calculated agreement statistics on the full setof annotations, and not just on the annotations witha strong majority as described in the previous sec-tion.
Table 6 shows inter-annotator agreement(IAA) for the questions?the average percentage oftimes two annotators agree with each other.
An-other way to gauge agreement is by calculatingthe average probability with which an annotatorTable 5: Questionnaire 2: Percentage of tweets inthe categories of Q3.
A tweet belongs to categoryX if it is annotated with X more often than all othercategories combined.
There were 973 such tweets.PercentageWhom of tweetsBarack Obama and/or Joe Biden 29.90Mitt Romney and/or Paul Ryan 24.87Some other individual 5.03Democratic party, democrats, or DNC 2.46Republican party, republicans, or RNC 8.42Some other institution 1.23Election campaign or process 4.93The target is not specified in the tweet 1.95None of the above 21.17all 100.00Table 6: Agreement statistics: inter-annotatoragreement (IAA) and average probability ofchoosing the majority class (APMS).IAA APMSQuestionnaire 1:Q1 78.02 0.845Q2 96.76 0.974Questionnaire 2:Q1 52.95 0.731Q2 59.59 0.736Q3 44.47 0.641picks the majority class.
The last column in Ta-ble 6 shows the average probability of picking themajority class (APMS) by the annotators (highernumbers indicate higher agreement).
Observe thatthere is high agreement on determining whether atweet has an emotion or not, and on determiningwhether the tweet is related to the 2012 US pres-idential elections or not.
The questions in Ques-tionnaire 2 pertaining to the experiencer, state, andstimulus were less straightforward and tend to re-quire more context than just the target tweet fora clear determination, but yet the annotations hadmoderate agreement.4.4 Access to the dataAll of the data is made freely available through thefirst author?s website:http://www.purl.org/net/PoliticalTweets2012It includes: (1) the complete set of tweets collectedfrom the Twitter API with hashtags shown in Ta-ble 2, (2) the subset of English tweets, (3) Ques-tionnaires 1 and 2, (4) and tweets annotated as perQuestionnaires 1 and 2.365 Automatically Detecting SemanticRoles of Emotions in TweetsSince in most instances (99.83%) the experiencerof emotions in a tweet is the tweeter, we focuson automatically detecting the other two semanticroles: the emotional state and the stimulus.Due to the unique challenges of semantic rolelabeling of emotions in tweets described earlierin the paper, we treat the detection of emotionalstate and stimulus as two subtasks for whichwe train state-of-the-art support vector machine(SVM) classifiers.
SVM is a learning algorithmproved to be effective on many classification tasksand robust on large feature spaces.
In our ex-periments, we exploited several different classi-fiers and found SVM outperforms others such asmaximum-entropy models (i.e., logistic regres-sion).
We also tested the most popular kernelssuch as the polynomial and RBF kernels with dif-ferent parameters in stratified ten-fold cross val-idation.
We found that a simple linear kernelyielded the best performance.
We used the Lib-SVM package (Chang and Lin, 2011).As mentioned earlier, there is fair amount ofwork on emotion detection in non-tweet texts(Boucouvalas, 2002; Holzman and Pottenger,2003; Ma et al., 2005; John et al., 2006; Mihalceaand Liu, 2006; Genereux and Evans, 2006; Amanand Szpakowicz, 2007; Tokuhisa et al., 2008;Neviarouskaya et al., 2009) as well as on tweets(Kim et al., 2009; Tumasjan et al., 2010; Bollen etal., 2011; Mohammad, 2012b; Choudhury et al.,2012; Wang et al., 2012).
In the experiments be-low we draw from various successfully used fea-tures described in these papers.
More specifically,the system we use builds on the classifier and fea-tures used in two previous systems: (1) the sys-tem described in (Mohammad, 2012b) which wasshown to perform significantly better than someother previous systems on the news paper head-lines corpus and the system described in (Moham-mad et al., 2013) which ranked first (among 44participating teams) in a 2013 SemEval competi-tion on detecting sentiment in tweets).The goal of the experiments in this section isto apply a state-of-the art emotion detection sys-tem on the electoral tweets data.
We want toset up baseline performance for emotion detec-tion on this new dataset and also validate the databy showing that automatic classifiers can obtainresults that are greater than random and major-ity baselines.
In Section 5.2, we apply the SVMclassifier and various features for the first time onthe task of detecting the stimulus of an emotion intweets.
In each experiment, we report results often-fold stratified cross-validation.5.1 Detecting emotional state5.1.1 FeaturesWe included the following features for detectingemotional state in tweets.Word n-grams: We included unigrams (singlewords) and bigrams (two-word sequences) intoour feature set.
All words were stemmed withPorter?s stemmer (Porter, 1980).Punctuations: number of contiguous sequences ofexclamation marks, question marks, or a combina-tion of them.Elongated words: the number of words with thefinal character repeated 3 or more times (soooo,mannnnnn, etc).
(Elongated words have been usedsimilarly in (Brody and Diakopoulos, 2011).
)Emoticons: presence/absence of positive and neg-ative emoticons.
The emoticon and its polar-ity were determined through a regular expres-sion adopted from Christopher Potts?
tokenizingscript.4Emotion Lexicons: We used the NRC word?emotion association lexicon (Mohammad and Tur-ney, 2010) to check if a tweet contains emo-tional words.
The lexicon contains human anno-tations of emotion associations for about 14,200word types.
The annotation includes whethera word is positive or negative (sentiments), andwhether it is associated with the eight basic emo-tions (joy, sadness, anger, fear, surprise, antici-pation, trust, and disgust).
If a tweet has threewords that have associations with emotion joy,then the LexEmo emo joy feature takes a valueof 3.
We also counted the number of wordswith regard to the Osgood?s (Osgood et al., 1957)semantic differential categories (LexOsg) builtfor Wordnet (LexOsg wn) and General Inquirer(LexOsg gi).
To reduce noise, we only consid-ered the words that have an adjective or adverbsense in Wordnet.Negation features: We examined tweets to deter-mine whether they contained negators such as no,not, and shouldn?t.
An additional feature deter-mined whether the negator was located close to an4http://sentiment.christopherpotts.net/tokenizing.html37Table 7: Results for emotion detection.Accuracyrandom baseline 30.26majority baseline 47.75automatic SVM system 56.84upper bound 69.80Table 8: The accuracies obtained with one of thefeature groups removed.
The number in bracketsis the difference with the all features score.
Thebiggest drop is shown in bold.Difference fromExperiment Accuracy all featuresall features 56.84 0all - ngrams 53.35 -3.49all - word ngrams 54.44 -2.40all - char.
ngrams 56.32 -0.52all - lexicons 54.34 -2.50all - manual lex.
55.17 -1.67all - auto lex.
55.38 -1.46all - negation 55.80 -1.04all - encodings (elongated words, emoticons, punctns.,uppercase) 56.82 -0.02emotion word (as determined by the emotion lex-icon) in the tweet and in the dependency parse ofthe tweet.
The list of negation words was adoptedfrom Christopher Potts?
sentiment tutorial.5Position features: We included a set of positionfeatures to capture whether the feature terms de-scribed above appeared at the beginning or the endof the tweet.
For example, if one of the first fiveterms in a tweet is a joy word, then the featureLexEmo joy begin was triggered.Combined features Though non-linear modelslike SVM (with non-linear kernels) can cap-ture interactions between features, we explic-itly combined some of our features.
For ex-ample, we concatenated all emotion categoriesfound in a given tweet.
If the tweet containedboth surprise and disgust words, a binary feature?LexEmo surprise disgust?
was triggered.
Also,if a tweet contained more than one joy wordand no other emotion words, then the featureLexEmo joy only was triggered.5.1.2 ResultsTable 7 shows the results.
We included two base-lines here: the random baseline corresponds to asystem that randomly guesses the emotion of atweet, whereas the majority baseline assigns all5http://sentiment.christopherpotts.net/lingstruc.htmltweets to the majority category (disgust).
Sincethe data is significantly skewed towards disgust,the majority baseline is relative high.The automatic system obtained by the classi-fier in identifying the emotions (56.84), which issignificantly higher than the majority baseline.
Itshould be noted that the highest scores in the Se-mEval 2013 task of detecting sentiment analysis oftweets was around 69% (Mohammad et al., 2013).That task even though related involved only threeclasses (positive, negative, and neutral).
Thus it isnot surprising that for an 8-way classification task,the performance is somewhat lower.The upper bound of the task here is not 100%?human annotators do not always agree with eachother.
To estimate the upper bound we can expectan automatic system to achieve, for each tweet werandomly sampled an human annotation from itsmultiple annotations and treated it as a system out-put.
We compare it with the majority categorychosen from the remaining human annotations forthat tweet.
Such sampling is conducted over alltweets and then evaluated.
The results table showsthis upper bound.Table 8 shows results of ablation experiments?the accuracies obtained with one of the featuregroups removed.
The higher the drop in per-formance, the more useful is that feature.
Ob-serve that the ngrams are the most useful fea-tures, followed by the emotion lexicons.
Most ofthe gain from ngrams come through word ngrams,but character ngrams provide small gains as well.Both the manual and automatic sentiment lexi-cons were found to be useful to a similar degree.Paying attention to negation was also beneficial,whereas emotional encodings such as elongatedwords, emoticons, and punctuations did not helpmuch.
It is possible that much of the discrimi-nating information they might have is already pro-vided by unigram and character ngram features.5.2 Detecting emotion stimulusAs discussed earlier, instead of detecting and la-beling the original text spans, we ground the emo-tion stimulus directly to the predefined entities.This allows us to circumvent mention detectionand co-reference resolution on linguistically lesswell-formed text.
We treat the problem as a classi-fication task, in which we classify a tweet into oneof the categories defined in Table 5.
We believethat a similar approach is also possible in other38Table 9: Results for detecting stimulus.P R Frandom baseline 16.45 20.87 18.39majority baseline 34.45 38.00 36.14automatic rule-based system 43.47 55.15 48.62automatic SVM system 57.30 59.32 58.30upper bound 82.87 81.36 82.11domains such as natural disaster tweets and epi-demic surveillance tweets.
We perform a ten-foldstratified cross-validation.5.2.1 FeaturesWe used the features below for detecting emotionstimulus:Word ngrams: Same as described earlier foremotional state.Lexical features: We collected lexicons thatcontain a variety of words and phrases describingthe categories in Table 5.
For example, the Re-publican party may be called as ?gop?
or ?GrandOld Party?
; all such words or phrases are all putinto the lexicon called ?republican?.
We countedhow many words in a given tweet are from each ofthese lexicons.Hashtag features: Hashtags related to the U.S.election were collected.
We organized them intodifferent categories and use them to further smooththe sparseness.
For example, ?#4moreyear?
and?#obama?
are put into the same hashtag lexiconand any occurrence of such hashtags in a tweettriggers the feature ?hashtag obama generalized?,indicating that this is a general version of hashtagrelated to president Barack Obama.Position features: Same as described earlier foremotional state.Combined features As discussed earlier, we ex-plicitly combined some of the above features.
Forexample, we first concatenate all lexicon and hash-tag categories found in a given tweet?if the tweetcontains both the general hashtag of ?obama?and ?romney?, a binary feature ?Hashtag generalobama romney?
takes the value of 1.5.2.2 ResultsTable 9 shows the results obtained by the system.Overall, the system obtains an F-measure of 58.30.The table also shows upper-bound and baselinescalculated just as described earlier for the emo-tional state category.
We added results for anadditional baseline, rule-based system, here thatchose the stimulus to be: Obama if the tweet hadthe terms obama or #obama; Romney if the tweethad the terms romney or #romney; Republicans ifthe tweet had the terms republican, republicans,or #republicans; Democrats if the tweet had theterms democrats, democrat, or #democrats; andCampaign if the tweet had the terms #election or#campaign.
If two or more of the above rules aretriggered in the same tweet, then a label is chosenat random.
This rule-based system based on hand-chosen features obtains an F-score of 48.62, show-ing that there are sufficiently many tweets wherekey words alone are not sufficient to disambiguatethe true stimulus.
Observe that the SVM-based au-tomatic system performs markedly better than themajority baseline and also the rule-based systembaseline.6 Conclusions and Future WorkIn this paper, we framed emotion detection as a se-mantic role labeling problem, focusing not just onemotional state but also on experiencer and stimu-lus.
We chose tweets about the 2012 US presiden-tial elections as our target domain.
We automati-cally compiled a large dataset of these tweets usinghashtags, and annotated them first for presence ofemotions, and then for the different semantic rolesof emotions.
All of the data is made freely avail-able.We found that a large majority of these tweets(88.1%) carry some emotional attitude towardssomeone or something.
Further, tweets that con-vey disgust are twice as prevalent than those thatconvey trust.
We found that most tweets expressemotions of the tweeter themselves, and the stim-ulus is often one among a few handful of entities.We developed a classifier for emotion detectionthat obtained an accuracy of 56.84 on an eight-way classification task.
Finally, we showed howthe stimulus identification task can be framed asa classification task in which our system outper-forms competitive baselines.Our future work involves exploring the use ofmore tweets from the same user to determine theirpolitical leanings, and use that as an additional fea-ture in emotion detection.
We are also interested inautomatically identifying other semantic roles ofemotions such as degree, reason, and empathy tar-get (described in Table 1).
We believe that a moresophisticated sentiment analysis applications anda better understanding of affect require the deter-mination of semantic roles of emotion.39ReferencesSaima Aman and Stan Szpakowicz.
2007.
Identifyingexpressions of emotion in text.
In Vclav Matou?sekand Pavel Mautner, editors, Text, Speech and Dia-logue, volume 4629 of Lecture Notes in ComputerScience, pages 196?205.
Springer Berlin / Heidel-berg.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 36th Annual Meeting of the Associa-tion for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics -Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA.Association for Computational Linguistics.Jerome Bellegarda.
2010.
Emotion analysis using la-tent affective folding and embedding.
In Proceed-ings of the NAACL-HLT 2010 Workshop on Compu-tational Approaches to Analysis and Generation ofEmotion in Text, Los Angeles, California.Johan Bollen, Alberto Pepe, and Huina Mao.
2011.Modeling public mood and emotion: Twitter senti-ment and socio-economic phenomena.
In The Inter-national AAAI Conference on Weblogs and SocialMedia (ICWSM), Barcelona, Spain.Anthony C. Boucouvalas.
2002.
Real time text-to-emotion engine for expressive internet commu-nication.
Emerging Communication: Studies onNew Technologies and Practices in Communication,5:305?318.Samuel Brody and Nicholas Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
: usingword lengthening to detect sentiment in microblogs.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 562?570, Stroudsburg, PA, USA.Association for Computational Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.Munmun De Choudhury, Scott Counts, and MichaelGamon.
2012.
Not all moods are created equal!
ex-ploring human emotional states in social media.
InThe International AAAI Conference on Weblogs andSocial Media (ICWSM).M D Conover, J Ratkiewicz, M Francisco, B Gonc,A Flammini, and F Menczer.
2011a.
Political po-larization on Twitter.
Networks, 133(26):89?96.Michael D Conover, Bruno Goncalves, JacobRatkiewicz, Alessandro Flammini, and FilippoMenczer.
2011b.
Predicting the political alignmentof Twitter users.
In IEEE Third InternationalConference on Privacy Security Risk and Trust andIEEE Third International Conference on SocialComputing, pages 192?199.
IEEE.Paul Ekman.
1992.
An argument for basic emotions.Cognition and Emotion, 6(3):169?200.Virginia Francisco and Pablo Gerv?as.
2006.
Auto-mated mark up of affective information in englishtexts.
In Petr Sojka, Ivan Kopecek, and Karel Pala,editors, Text, Speech and Dialogue, volume 4188 ofLecture Notes in Computer Science, pages 375?382.Springer Berlin / Heidelberg.Michel Genereux and Roger Evans.
2006.
Distin-guishing affective states in weblogs.
In AAAI-2006Spring Symposium on Computational Approaches toAnalysing Weblogs, pages 27?29, Stanford, USA.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Jennifer Golbeck and Derek Hansen.
2011.
Com-puting political preference among twitter followers.In Proceedings of the SIGCHI Conference on Hu-man Factors in Computing Systems, CHI ?11, pages1105?1108, New York, NY.
ACM.Ralph Grishman.
1997.
Information extraction: Tech-niques and challenges.
In SCIE, pages 10?27.Lars E. Holzman and William M. Pottenger.
2003.Classification of emotions in internet chat: An appli-cation of machine learning using speech phonemes.Technical report, Leigh University.David John, Anthony C. Boucouvalas, and Zhe Xu.2006.
Representing emotional momentum withinexpressive internet communication.
In Proceed-ings of the 24th IASTED international conference onInternet and multimedia systems and applications,pages 183?188, Anaheim, CA.
ACTA Press.Jason S. Kessler and Nicolas Nicolov.
2009.
Targetingsentiment expressions through supervised ranking oflinguistic configurations.
In 3rd Int?l AAAI Confer-ence on Weblogs and Social Media (ICWSM 2009).S.
Kim and E. Hovy.
2006.
Extracting opinions, opin-ion holders, and topics expressed in online news me-dia text.
In Proceedings of the Workshop on Senti-ment and Subjectivity in Text, pages 1?8.Elsa Kim, Sam Gilbert, Michael J. Edwards, and Er-hardt Graeff.
2009.
Detecting sadness in 140characters: Sentiment analysis of mourning MichaelJackson on twitter.Zolt?an K?ovecses.
2003.
Metaphor and Emotion: Lan-guage, Culture, and Body in Human Feeling (Stud-ies in Emotion and Social Interaction).
CambridgeUniversity Press.X.
Liu, K. Li, M. Zhou, and Z. Xiong.
2011.
En-hancing semantic role labeling for tweets using self-training.
In AAAI.X.
Liu, Z. Fu, F. Wei, and M. Zhou.
2012.
Collectivenominal semantic role labeling for tweets.
In AAAI.Chunling Ma, Helmut Prendinger, and MitsuruIshizuka.
2005.
Emotion estimation and reasoningbased on affective textual interaction.
In J. Tao andR.
W. Picard, editors, First International Conferenceon Affective Computing and Intelligent Interaction(ACII-2005), pages 622?628, Beijing, China.40Benjamin Mandel, Aron Culotta, John Boulahanis,Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.2012.
A demographic analysis of online sentimentduring Hurricane Irene.
In Proceedings of the Sec-ond Workshop on Language in Social Media, LSM?12, pages 27?36, Stroudsburg, PA. Association forComputational Linguistics.Llu?
?s M`arquez, Xavier Carreras, Kenneth C.Litkowski, and Suzanne Stevenson.
2008.
Se-mantic role labeling: an introduction to the specialissue.
Computational Linguistics, 34(2):145?159.Diana Maynard and Adam Funk.
2011.
Automaticdetection of political opinions in tweets.
gateacuk,7117:81?92.Rada Mihalcea and Hugo Liu.
2006.
A corpus-based approach to finding happiness.
In AAAI-2006Spring Symposium on Computational Approaches toAnalysing Weblogs, pages 139?144.
AAAI Press.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: Us-ing mechanical turk to create an emotion lexicon.
InProceedings of the NAACL-HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, LA, California.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedingsof the seventh international workshop on Seman-tic Evaluation Exercises (SemEval-2013), Atlanta,Georgia, USA.Saif Mohammad.
2012a.
Portable features for clas-sifying emotional text.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 587?591, Montr?eal,Canada.Saif M. Mohammad.
2012b.
#Emotional tweets.
InProceedings of the First Joint Conference on Lexi-cal and Computational Semantics - Volume 1: Pro-ceedings of the main conference and the shared task,and Volume 2: Proceedings of the Sixth Interna-tional Workshop on Semantic Evaluation, SemEval?12, pages 246?255, Stroudsburg, PA.Alena Neviarouskaya, Helmut Prendinger, and Mit-suru Ishizuka.
2009.
Compositionality principle inrecognition of fine-grained emotions from text.
InProceedings of the Third International Conferenceon Weblogs and Social Media, pages 278?281, SanJose, California.C.E.
Osgood, Suci G., and P. Tannenbaum.
1957.The measurement of meaning.
University of IllinoisPress.Martha Palmer, Daniel Gildea, and Nianwen Xue.2010.
Semantic role labeling.
Synthesis Lectureson Human Language Technologies, 3(1):1?103.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1?2):1?135.Robert Plutchik.
1980.
A general psychoevolutionarytheory of emotion.
Emotion: Theory, research, andexperience, 1(3):3?33.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InProceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, pages 339?346, Stroudsburg, PA,USA.M.
Porter.
1980.
An algorithm for suffix stripping.Program, 14:130?137.Matthew Purver and Stuart Battersby.
2012.
Ex-perimenting with distant supervision for emotionclassification.
In Proceedings of the 13th Confer-ence of the European Chapter of the Associationfor Computational Linguistics, EACL ?12, pages482?491, Stroudsburg, PA. Association for Compu-tational Linguistics.A.
Ritter, S. Clark, Mausam, and O. Etzioni.
2011.Named entity recognition in tweets: An experimen-tal study.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing, pages 1524?1534.Carlo Strapparava and Alessandro Valitutti.
2004.Wordnet-Affect: An affective extension of WordNet.In Proceedings of the 4th International Conferenceon Language Resources and Evaluation (LREC-2004), pages 1083?1086, Lisbon, Portugal.Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.2008.
Emotion classification using massive exam-ples extracted from the web.
In Proceedings of the22nd International Conference on ComputationalLinguistics - Volume 1, COLING ?08, pages 881?888, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semanticrole labeling.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, pages 589?596, Stroudsburg, PA. Associationfor Computational Linguistics.Andranik Tumasjan, Timm O Sprenger, Philipp GSandner, and Isabell M Welpe.
2010.
Predictingelections with Twitter : What 140 characters revealabout political sentiment.
Word Journal Of The In-ternational Linguistic Association, pages 178?185.Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,and Amit P. Sheth.
2012.
Harnessing twitter?big data?
for automatic emotion identification.
InProceedings of the 2012 ASE/IEEE InternationalConference on Social Computing, SOCIALCOM-PASSAT ?12, pages 587?592, Washington, DC,USA.
IEEE Computer Society.Lei Zhang, Bing Liu, Suk Hwan Lim, and EamonnO?Brien-Strain.
2010.
Extracting and ranking prod-uct features in opinion documents.
In Proceedingsof the 23rd International Conference on Compu-tational Linguistics: Posters, COLING ?10, pages1462?1470, Stroudsburg, PA, USA.
Association forComputational Linguistics.41
