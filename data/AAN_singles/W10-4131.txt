A Boundary-Oriented Chinese Segmentation Method Using N-Gram Mutual InformationLing-Xiang Tang1, Shlomo Geva1, Andrew Trotman2, Yue Xu11Faculty of Science and TechnologyQueensland University of TechnologyBrisbane, Australia{l4.tang,s.geva,yue.xu}@qut.edu.au2Department of Computer ScienceUniversity of OtagoDunedin, New Zealandandrew@cs.otago.ac.nzAbstractThis paper describes our participationin the Chinese word segmentation taskof CIPS-SIGHAN 2010.
We imple-mented an n-gram mutual information(NGMI) based segmentation algorithmwith the mixed-up features from unsu-pervised, supervised and dictionary-based segmentation methods.
This al-gorithm is also combined with a simplestrategy for out-of-vocabulary (OOV)word recognition.
The evaluation forboth open and closed training showsencouraging results of our system.
Theresults for OOV word recognition inclosed training evaluation were how-ever found unsatisfactory.1 IntroductionChinese segmentation has been an interestingresearch topic for decades.
Lots of delicatemethods are used for providing good Chinesesegmentation.
In general, on the basis of therequired human effort, Chinese word segmen-tation approaches can be classified into twocategories: supervised and unsupervised.Particularly, supervised segmentation meth-ods can achieve a very high precision on thetargeted knowledge domain with the help oftraining corpus?the manually segmented textcollection.
On the other hand, unsupervisedmethods are suitable for more general Chinesesegmentation where there is no or limitedtraining data available.
The resulting segmen-tation accuracy with unsupervised methodsmay not be very satisfying, but the human ef-fort for creating the training data set is not ab-solutely required.In the Chinese word segmentation task ofCIPS-SIGHAN 2010, the focus is on the per-formance of Chinese segmentation on cross-domain text.
There are in total two types ofevaluations: closed and open.
We participatedin both closed and open training evaluationtasks and both simplified and traditionalChinse segmentation subtasks.
For the closedtraining evaluation, the provided resource forsystem training is limited and using externalresources such as trained segmentation soft-ware, corpus, dictionaries, lexicons, etc areforbidden; especially, human-encoded rulesspecified in the segmentation algorithm are notallowed.For the bakeoff of this year, we imple-mented a boundary-oriented NGMI-based al-gorithm with the mixed-up features from su-pervised, unsupervised and dictionary-basedmethods for the segmentation of cross-domaintext.
In order to detect words not in the trainingcorpus, we also used a simple strategy for out-of-vocabulary word recognition.2 A Boundary-Oriented SegmentationMethod2.1 N-Gram Mutual InformationIt is a challenge to segment text that is out-of-domain for supervised methods which aregood at the segmentation for the text that hasbeen seen segmented before.
On the other hand,unsupervised segmentation methods could helpto discover words even if they are not in vo-cabularies.
To conquer the goal of segmentingtext that is out-of-domain and to take advan-tage of the training corpus, we use n-gram mu-tual information (NGMI)(Tang et al, 2009) ?an unsupervised boundary-oriented segmenta-tion method and make it trainable for cross-domain text segmentation.As an unsupervised segmentation approach,NGMI is derived from the character-based mu-tual information(Sproat & Shih, 1990), butunlike its ancestor it can additionally recognisewords longer than two characters.
Generally,mutual information is used to measure the as-sociation strength of two adjoining entities(characters or words) in a given corpus.
Thestronger association, the more likely it is thatthey should be together.
The association scoreMI for the adjacent two entities (x and y) iscalculated as:(1)where freq(x) is the frequency of entity x oc-curring in the given corpus; freq(xy) is the fre-quency of entity xy (x followed by y) occurringin the corpus; N is the size of entities in thegiven corpus; p(x) is an estimate of the prob-ability of entity x occurring in corpus, calcu-lated as freq(x)/N.NGMI separates words by choosing themost probable boundaries in the unsegmentedtext with the help of a frequency table of n-gram string patterns.
Such a frequency tablecan be built from any selected text.The main concept of NGMI is to find theboundaries between words by combining con-textual information rather than looking for justwords.
Any place between two Chinese char-acters could be a possible boundary.
To findthe most rightful ones, boundary confidence(BC) is introduced to measure the confidenceof having words separated correctly.
In otherwords, BC measures the association level ofthe left and right characters around each possi-ble boundary to decide whether the boundaryshould be actually placed.For any input string, suppose that we have:(2)The boundary confidence of a possibleboundary ( | )  is defined as:(3)where L and R are the adjoining left and rightsegments with up to two characters from eachside of the boundary ( | ) and            ,; and NGMImin is calculated as:(4)Basically, NGMImin considers mutual informa-tion of k (k=2, or k= 4) pairs of segmentsaround the boundary; the one with the lowestvalue is used as the score of boundary confi-dence.
Those segment pairs used in NGMImincalculation are named adjoining segment pairs(ASPs).
Each ASP consists of a pair of adjoin-ing segments.For the boundary confidence of the bounda-ries at the beginning or end of an input string,we can retrieve only one character from oneside of the boundary.
So for these two kinds ofboundaries differently we have:(5)(6)For any possible boundary the lower confi-dence score it has, the more likely it is an ac-tual boundary.
A threshold then can be set todecide whether a boundary should be placed.So even without a lexicon, it is still probable tosegment text with a certain precision whichjust simply means the suggested words are allout-of-vocabulary.
Hence, NGMI can be sub-sequently used for OOV word recognition.2.2 Supervised NGMIThe idea of making NGMI trainable is to turnthe segmented text into a word based fre-quency table.
It is a table that records onlywords, adjoining word pairs and their frequen-cies.
For example, given a piece of trainingtext ?
?A B C E B C A B?
(where A, B, C andE are n-gram Chinese words), its frequencytable should look like the following:A|B 2B|C 2C|E 1C|A 1A 2B 3C 2E 1Also, when doing the boundary confidencecomputation, any substrings (children) of thewords (parents) in this table are set to have thesame frequency as their parents?.3 Segmentation System Design3.1 Frequency Table and Its AlignmentIn order to resolve ambiguity and also recog-nise OOV terms, statistical information of n-gram string patterns in test files should be col-lected.
There are in total two groups of fre-quency information used in the segmentation.One is from the training data, recording thefrequency information of the actual words andthe adjoining word pairs; the other is from theunsegmented text, containing frequency infor-mation of all possible n-gram patterns.However, the statistical data collected fromthe unsegmented test file contains many noisepatterns.
It is necessary to remove those noisepatterns from the table to avoid negative im-pact on the final BC computation.
Therefore,an alignment of the pattern frequencies ob-tained from the test file is performed to reducenoise.The frequency alignment is conducted in afew steps.
First, build a frequency table of allstring patterns for the unsegmented text includ-ing those having a frequency of one.
Second,the frequency table is sorted by the frequencyand the length of the patterns.
Longer patternshave a higher ranking than the shorter ones; forpatterns of same length the ones having higherfrequency are ranked higher than those havinglower.
Next, starting from the beginning of thetable where the longest and the most frequentpattern have the highest ranking, retrieve onerecord each time and remove from the table allits sub-patterns which have the same frequencyas its parent?s.After such a frequency alignment is done,two frequency tables are merged into one andready for the final boundary confidence calcu-lation.3.2 SegmentationIn the training and the system testing stages,the segmentation results using boundary confi-dence alone for word disambiguation werefound unsatisfactory.
Trying to achieve as highperformance as possible, the overall wordsegmentation for the bakeoff is done by usinga hybrid algorithm which is a combination ofNGMI for general word segmentation, and thebackward maximum match (BMM) method forthe final word disambiguation.Since it is common for a Chinese documentcontaining various types of characters: Chi-nese, digit, alphabet and characters from otherlanguages, segmentation needs to be consid-ered for two particular forms of Chinesewords: 1) words containing non-Chinese char-acters such as numbers or letters; and 2) wordscontaining purely Chinese characters.In order to simplify the process of overallsegmentation, boundaries are automaticallyadded to the places in which Chinese charac-ters precede non-Chinese characters.
Addition-ally, for words containing numbers or letters,we only search those begin with numbers orletters and end with Chinese character(s)against the given lexicons.
If the search fails,the part with all non-Chinese characters re-mains the same and a boundary is added be-tween the non-Chinese character and the Chi-nese character.For example, to segment a sentence????????????????
?, itconsists of there following main steps:?
First, because of ??
|?
?, only ????????
requires initial segmentation.?
Next, find a matched word ??????
?in a given lexicon.?
Last, segment ?????
?.So the critical part of the segmentation algo-rithm is to segment strings with purely Chinesecharacters.By already knowing the actual word infor-mation (i.e.
a vocabulary from the labelledtraining data), it can be set in our algorithmthat when computing BCs each possibleboundary is assigned with a score falling inone of the following four BC categories:?
INSEPARATABLE?
THRESHOLD?
normal boundary confidence score?
ABSOLUTE-BOUNDARYINSEPARATABLE means the charactersaround the possible boundary are a part of anactual word; ABSOLUTE-BOUNDARYmeans the adjoining segments pairs are notseen in any words or string patterns.THRESHOLD is a threshold value that isgiven to a possible boundary for which onlyone of ASPs can be found in the word pair ta-ble, and its length is greater than two.After finishing all BC computations for aninput string, it then can be broken down intosegments separated by the boundaries having aBC score that is lower than or equals to thethreshold value.
For each segment, it can bechecked if it is a word in the vocabulary or if itis an OOV term using an OOV judgementformula that will be discussed in Section 3.3.
Ifa segment is not a word or an OOV term, itmeans there is an ambiguity in that segment.For example, given a sentence ??????
?,the substring ?????
inside the sentence canbe either segmented into ???
| ??
or ??
|??
?.To disambiguate it, a segment is dividedinto two chunks at the place having the lowestBC score.
If one of the chunks is a word orOOV term, this two-chunk breaking-down op-eration continues on the remaining non-wordchunk until both divided chunks are words, ornone of them is a word or an OOV term.
Afterthis recursive operation is finished, if there arestill non-word chunks left they will be furthersegmented using the BMM method.The overall segmentation algorithm for anall-Chinese string can be summarised as fol-lows:1) Compute BC for each possible boundary.2) Input string becomes segments that areseparated by the boundaries having alow BC score (not higher than thethreshold).3) For each remaining non-word segmentresulting from step 2, it gets recursivelybroken down into two chunks at theplace having the lowest BC among thissegment based on the scores from step 1.This breaking-down-into-two-chunkloop continues on the non-word chunk ifthe other is a word or an OOV term;otherwise, all the remaining non-wordchucks are further segmented using thebackward maximum match method.3.3 OOV Word RecognitionWe use a simple strategy for OOV word detec-tion.
It is assumed that an n-gram string patterncan be qualified as an OOV word if it repeatsfrequently within only a short span of text or afew documents.
So to recognise OOV words,the statistical data extracted from the unseg-mented text needs to contain not only patternfrequency information but also document fre-quency information.
However, the documentsin the test data are boundary-less.
To obtaindocument frequencies for string patterns, weseparate test files into a set of virtual docu-ments by splitting them according size.
Thesize of the virtual document (VDS) is adjust-able.For a given non-word string pattern S, wethen can compute its probability of being anOOV term by using:(7)where tf is the term frequency of the string pat-tern S; df is the virtual document frequency ofthe string pattern.
Then S is considered anOOV candidate, if it satisfies:(8)where OOV_THRES is an adjustable thresholdvalue used to filter out the patterns with lowerprobability of being OOV words.
However,using this strategy could have side effects onthe segmentation performance because not allthe suggested OOV words could be correct.4 Experiments4.1 Experimental EnvironmentOS GNU/Linux 2.6.32.11-99.fc12.x86_64CPUIntel(R) Core(TM)2 Duo CPUE6550  @ 2.33GHzMEM 8G memoryBUILD DEBUG build without optimisationTable 1.
Software and Hardware Environ-ment.The information of operating system andhardware used in the experiments is given inTable 1.4.2 Parameters SettingsParameter ValueN # of words in training corpusTHRESHOLD log(1/N)VDS 10,000bytesOOV_THRES 2.3Table 2.
System settings used in both closedand open training evaluation.Table 2 shows the parameters used in the sys-tem for segmentation and OOV recognition.4.3 Closed and Open TrainingFor both closed and open training evaluations,the algorithm and parameters used for segmen-tation and OOV detection are exactly the same.This is true except for an extra dictionary - cc-cedict(MDBG) being used in the open trainingevaluation.4.4 Segmentation EfficiencySUBTASK DOMAIN TIMEsimplified(closed)A 2m19.841sB 2m1.405sC 1m57.819sD 1m54.375ssimplified(open)A 3m52.726sB 3m20.907sC 3m10.398sD 3m22.866straditional(closed)A 2m33.448sB 2m56.056sC 3m7.103sD 3m14.286straditional A 3m14.595s(open) B 3m41.634sC 3m53.839sD 4m10.099sTable 3.
The execution time of segmenta-tions for four different domains in bothsimplified and traditional Chinese subtasks.Table 3 shows the execution time of all tasksfor generating the segmentation outputs.
Theexecution time listed in the table includes thetime for loading the training frequency table,building the frequency table from the test file,and producing the actual segmentation results.5 Evaluation5.1 Segmentation ResultsSimplified ChineseTask R P F1 ROOV RROOV RRIVA (c) 0.907 0.862 0.884 0.069 0.206 0.959A (o) 0.869 0.873 0.871 0.069 0.657 0.885B (c) 0.876 0.844 0.86 0.152 0.457 0.951B (o) 0.859 0.878 0.868 0.152 0.668 0.893C (c) 0.885 0.804 0.842 0.110 0.218 0.967C (o) 0.865 0.846 0.855 0.110 0.559 0.903D (c) 0.904 0.865 0.884 0.087 0.321 0.960D (o) 0.853 0.850 0.851 0.087 0.438 0.893Traditional ChineseTask R P F1 ROOV RROOV RRIVA (c) 0.864 0.789 0.825 0.094 0.105 0.943A (o) 0.804 0.722 0.761 0.094 0.234 0.863B (c) 0.868 0.85 0.859 0.094 0.316 0.926B (o) 0.789 0.736 0.761 0.094 0.35 0.834C (c) 0.871 0.815 0.842 0.075 0.115 0.932C (o) 0.811 0.74 0.774 0.075 0.254 0.856D (c) 0.875 0.834 0.854 0.068 0.169 0.926D (o) 0.811 0.753 0.781 0.068 0.235 0.853Table 4.
The segmentation results for fourdomains in both closed and open trainingevaluations.
(c) ?
closed; (o) ?
open;  A - Lit-erature; B ?
Computer; C ?
Medicine; D ?Finance.
ROOV is the OOV rate in the testfile.In the Chinese word segmentation task ofCIPS-SIGHAN 2010, the system performanceis measured by five metrics: recall (R), preci-sion (P), F-measure (F1), recall rate of OOVwords (RROOV), and recall rate of words in vo-cabulary (RRIV).The official results of our system for bothopen and closed training evaluation are givenin Table 4.
The recall rates, precision values,and F1-scores of all tasks show promising re-sults of our system in the segmentation forcross-domain text.
However, the gaps betweenour scores and the bakeoff bests also suggestthat there is still plenty of room for perform-ance improvements in our system.The OOV recall rates (RRoov) showed inTable 4 demonstrate that the OOV recognitionstrategy used in our system can achieve a cer-tain level of OOV word discovery in closedtraining evaluation.
The overall result for theOOV word recognition is not very satisfactoryif comparing it with the best result from otherbakeoff participants.
But for the open trainingevaluation the OOV recall rate picked up sig-nificantly, which indicates that the extra dic-tionary - cc-cedict covers a fair amount ofterms for various domains.5.2 Possible Further ImprovementsDue to finishing the implementation of oursegmentation system in a short time, we be-lieve that there might be many program bugswhich had negative effects on our system andleaded to producing results not as expected.
Inan analysis of the segmentation outputs, wordsstarting with numbers were found incorrectlysegmented because of the different encodingsused in the training and test files for digits.Moreover, the disambiguation in breakingdown a non-word segment which contains atleast an n-gram word could lead to an all-single-character-word segmentation.
Thisshould certainly be avoided.Also, the current OOV word recognitionstrategy may detect a few good OOV words,but also introduces incorrect segmentationconsistently through the whole input text ifOOV words are mistakenly identified.
If thisOOV word recognition used in our system canbe further improved, it can help to alleviate theproblem of performance deterioration.For the open training, if language rules canbe encoded in both word segmentation andOOV word recognition, it certainly is anotherbeneficial method to improve the overall preci-sion and recall rate.6 ConclusionsIn this paper, we describe a novel hybridboundary-oriented NGMI-based segmentationmethod, which combines a simple strategy forOOV word recognition.
The evaluation resultsshow reasonable performance of our system incross-domain text segmentation even with thenegative effects from system bugs and theOOV word recognition strategy.
It is believedthat the segmentation system can be improvedby fixing the existing program bugs, and hav-ing a better OOV word recognition strategy.Performance can also be further improved byincorporating language or domain specificknowledge into the system.ReferencesMDBG.
CC-CEDICT download.
fromhttp://www.mdbg.net/chindict/chindict.php?page=cc-cedictSproat, Richard, and Chilin Shih.
1990.
A statisticalmethod for finding word boundaries in Chinesetext.
Computer Processing of Chinese &amp;Oriental Languages, 4(4): 336-351.Tang, Ling-Xiang, Shlomo Geva, Yue Xu, andAndrew Trotman.
2009.
Word Segmentation forChinese Wikipedia Using N-Gram MutualInformation.
Paper presented at the 14thAustralasian Document Computing Symposium(ADCS 2009).
