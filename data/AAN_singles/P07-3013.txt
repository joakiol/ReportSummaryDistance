Proceedings of the ACL 2007 Student Research Workshop, pages 73?78,Prague, June 2007. c?2007 Association for Computational LinguisticsAnnotating and Learning Compound Noun SemanticsDiarmuid O?
Se?aghdhaUniversity of Cambridge Computer Laboratory15 JJ Thomson AvenueCambridge CB3 0FDUnited Kingdomdo242@cl.cam.ac.ukAbstractThere is little consensus on a standard ex-perimental design for the compound inter-pretation task.
This paper introduces well-motivated general desiderata for semanticannotation schemes, and describes such ascheme for in-context compound annotationaccompanied by detailed publicly availableguidelines.
Classification experiments on anopen-text dataset compare favourably withpreviously reported results and provide asolid baseline for future research.1 IntroductionThere are a number of reasons why the interpreta-tion of noun-noun compounds has long been a topicof interest for NLP researchers.
Compounds oc-cur very frequently in English and many other lan-guages, so they cannot be avoided by a robust se-mantic processing system.
Compounding is a veryproductive process with a highly skewed type fre-quency spectrum, and corpus information may bevery sparse.
Compounds are often highly ambigu-ous and a large degree of ?world knowledge?
seemsnecessary to understand them.
For example, know-ing that a cheese knife is (probably) a knife forcutting cheese and (probably) not a knife made ofcheese (cf.
plastic knife) does not just require anability to identify the senses of cheese and knife butalso knowledge about what one usually does withcheese and knives.
These factors combine to yielda difficult problem that exhibits many of the chal-lenges characteristic of lexical semantic process-ing in general.
Recent research has made signifi-cant progress on solving the problem with statisti-cal methods and often without the need for manu-ally created lexical resources (Lauer, 1995; Lapataand Keller, 2004; Girju, 2006; Turney, 2006).
Thework presented here is part of an ongoing projectthat treats compound interpretation as a classifica-tion problem to be solved using machine learning.2 Selecting an Annotation SchemeFor many classification tasks, such as part-of-speechtagging or word sense disambiguation, there is gen-eral agreement on a standard set of categories thatis used by most researchers.
For the compoundinterpretation task, on the other hand, there is lit-tle agreement and numerous classification schemeshave been proposed.
This hinders meaningful com-parison of different methods and results.
One musttherefore consider how an appropriate annotationscheme should be chosen.One of the problems is that it is not immedi-ately clear what level of granularity is desirable, oreven what kind of units the categories should be.Lauer (1995) proposes a set of 8 prepositions thatcan be used to paraphrase compounds: a cheeseknife is a knife FOR cheese but a kitchen knife isa knife (used) IN a kitchen.
An advantage of thisapproach is that preposition-noun co-occurrencescan efficiently be mined from large corpora usingshallow techniques.
On the other hand, interpret-ing a paraphrase requires further disambiguation asone preposition can map onto many semantic rela-tions.1 Girju et al (2005) and Nastase and Szpakow-icz (2003) both present large inventories of seman-1The interpretation of prepositions is itself the focus of aSemeval task in 2007.73tic relations that describe noun-noun dependencies.Such relations provide richer semantic information,but it is harder for both humans and machines toidentify their occurrence in text.
Larger invento-ries can also suffer from class sparsity; for exam-ple, 14 of Girju et al?s 35 relations do not occur intheir dataset and 7 more occur in less than 1% ofthe data.
Nastase and Szpakowicz?
scheme mitigatesthis problem by the presence of 5 supercategories.Each of these proposals has its own advantagesand drawbacks, and there is a need for principled cri-teria for choosing one.
As the literature on semanticannotation ?best practice?
is rather small,2 I deviseda novel set of design principles based on empiricaland theoretical considerations:1.
The inventory of informative categories shouldaccount for as many compounds as possible2.
The category boundaries should be clear andcategories should describe a coherent concept3.
The class distribution should not be overlyskewed or sparse4.
The concepts underlying the categories shouldgeneralise to other linguistic phenomena5.
The guidelines should make the annotation pro-cess as simple as possible6.
The categories should provide useful semanticinformationThese intuitively appear to be desirable principlesfor any semantic annotation scheme.
The require-ment of class distribution balance is motivated bythe classification task.
Where one category domi-nates, the most-frequent-class baseline can be diffi-cult to exceed and care must be taken in evaluationto consider macro-averaged performance as well asraw accuracy.
It has been suggested that classifierstrained on skewed data may perform poorly on mi-nority classes (Zhang and Oles, 2001).
Of course,this is not a justification for conflating concepts withlittle in common, and it may well be that the naturaldistribution of data is inherently skewed.There is clearly a tension between these criteria,and only a best-fit solution is possible.
However, itwas felt that a new scheme might satisfy them moreoptimally than existing schemes.
Such a proposal2One relevant work is Wilson and Thomas (1997).Relation Distribution ExampleBE 191 (9.55%) steel knifeHAVE 199 (9.95%) street nameIN 308 (15.40%) forest hutINST 266 (13.30%) rice cookerACTOR 236 (11.80%) honey beeABOUT 243 (12.15%) fairy taleREL 81 (4.05%) camera gearLEX 35 (1.75%) home secretaryUNKNOWN 9 (0.45%) simularity crystalMISTAG 220 (11.00%) blazing fireNONCOMP 212 (10.60%) [real tennis] clubTable 1: Sample class frequenciesnecessitates a method of evaluation.
Not all the cri-teria are easily evaluable.
It is difficult to prove gen-eralisability and usefulness conclusively, but it canbe maximised by building on more general work onsemantic representation; for example, the guidelinesintroduced here use a conception of events and par-ticipants compatible with that of FrameNet (Bakeret al, 1998).
Good results on agreement and base-line classification will provide positive evidence forthe coherence and balance of the classes; agreementmeasures can confirm ease of annotation.In choosing an appropriate level of granularity, Iwished to avoid positing a large number of detailedbut rare categories.
Levi?s (1978) set of nine se-mantic relations was used as a starting point.
Thedevelopment process involved a series of revisionsover six months, aimed at satisfying the six criteriaabove and maximising interannotator agreement inannotation trials.
The nature of the decisions whichhad to be made is exemplified by the compound carfactory, whose standard referent seems to qualify asFOR, CAUSE, FROM and IN in Levi?s scheme (andcauses similar problems for the other schemes I amaware of).
Likewise there seems to be no princi-pled way to choose between a locative or purposivelabel for dining room.
Such examples led to bothredefinition of category boundaries and changes inthe category set; for example, FOR was replaced byINST and AGENT, which are independent of purpo-sivity.
This resulted in the class inventory shown inTable 1 and a detailed set of annotation guidelines.33The guidelines are publicly available at http://www.cl.cam.ac.uk/?do242/guidelines.pdf.74The scheme?s development is described at length inO?
Se?aghdha (2007b).Many of the labels are self-explanatory.
AGENTand INST(rument) apply to sentient and non-sentient participants in an event respectively, withties (e.g., stamp collector) being broken by a hier-archy of coarse semantic roles.
REL is an OTHER-style category for compounds encoding non-specificassociation.
LEX(icalised) applies to compoundswhich are semantically opaque without prior knowl-edge of their meanings.
MISTAG and NON-COMP(ound) labels are required to deal with se-quences that are not valid two-noun compounds buthave been identified as such due to tagging errorsand the simple data extraction heuristic described inSection 3.1.
Coverage is good, as 92% of valid com-pounds in the dataset described below were assignedone of the six main semantic relations.3 Annotation Experiment3.1 DataA simple heuristic was used to extract noun se-quences from the 90 million word written part of theBritish National Corpus.4 The corpus was parsedusing the RASP parser5 and all sequences of twocommon nouns were extracted except those adjacentto another noun and those containing non-alphabeticcharacters.
This yielded almost 1.6 million tokenswith 430,555 types.
2,000 unique tokens were ran-domly drawn for use in annotation and classificationexperiments.3.2 MethodTwo annotators were used: the current author andan annotator experienced in lexicography but with-out any special knowledge of compounds or any rolein the development of the annotation scheme.
In allthe trials described here, each compound was pre-sented alongside the sentence in which it was foundin the BNC.
The annotators had to assign one of thelabels in Table 1 and the rule that licensed that la-bel in the annotation guidelines.
For example, thecompound forest hut in its usual sense would be an-notated IN,2,2.1.3.1 to indicate the semantic4http://www.natcorp.ox.ac.uk/5http://www.informatics.susx.ac.uk/research/nlp/rasp/relation, the direction of the relation (it is a hut ina forest, not a forest in a hut) and that the label islicensed by rule 2.1.3.1 in the guidelines (N1/N2 isan object spatially located in or near N2/N1).6 Twotrial batches of 100 compounds were annotated tofamiliarise the second annotator with the guidelinesand to confirm that the guidelines were indeed us-able for others.
The first trial resulted in agreementof 52% and the second in agreement of 73%.
Theresult of the second trial, corresponding to a Kappabeyond-chance agreement estimate (Cohen, 1960)of ??
= 0.693, was very impressive and it was de-cided to proceed to a larger-scale task.
500 com-pounds not used in the trial runs were drawn fromthe 2,000-item set and annotated.3.3 Results and AnalysisAgreement on the test set was 66.2% with ??
= 0.62.This is less than the score achieved in the secondtrial run, but may be a more accurate estimator of thetrue population ?
due to the larger sample size.
Onthe other hand, the larger dataset may have causedannotator fatigue.
Pearson standardised residuals(Haberman, 1973) were calculated to identify themain sources of disagreement.7 In the context ofinter-annotator agreement one expects these residu-als to have large positive values on the agreement di-agonal and negative values in all other cells.
Amongthe six main relations listed at the top of Table 1,a small positive association was observed betweenINST and ABOUT, indicating that borderline topicssuch as assessment task and gas alarm were likelyto be annotated as INST by the first annotator andABOUT by the second.
It seems that the guidelinesmight need to clarify this category boundary.It is clear from analysis of the data that the REL,LEX and UNKNOWN categories show very lowagreement.
They all have low residuals on the agree-ment diagonal (that for UNKNOWN is negative) andnumerous positive entries off it.
REL and LEX arealso the categories for which it is most difficult to6The additional information provided by the direction andrule annotations could be used to give a richer classificationscheme but has not yet been used in this way in my experiments.7The standardised residual of cell ij is calculated aseij =nij ?
p?i+p?+j?p?i+p?+j(1?
p?i+)(1?
p?+j)where nij is the observed value of cell ij and p?i+, p?+j are rowand column marginal probabilities estimated from the data.75provide clear guidelines.
On the other hand, theMISTAG and NONCOMP categories showed goodagreement, with slightly higher agreement residu-als than the other categories.
To get a rough ideaof agreement on the six categories used in the clas-sification experiments described below, agreementwas calculated for all items which neither annota-tor annotated with any of REL, LEX, UNKNOWN,MISTAG and NONCOMP.
This left 343 items withagreement of 73.6% and ??
= 0.683.3.4 DiscussionThis is the first work I am aware of where com-pounds were annotated in their sentential context.This aspect is significant, as compound meaning isoften context dependent (compare school manage-ment decided.
.
.
and principles of school manage-ment) and in-context interpretation is closer to thedynamic of real-world language use.
Context canboth help and hinder agreement, and it is not clearwhether in- or out-of-context annotation is easier.Previous work has given out-of-context agree-ment figures for corpus data.
Kim and Bald-win (2005) report an experiment using 2,169 com-pounds taken from newspaper text and the categoriesof Nastase and Szpakowicz (2003).
Their annota-tors could assign multiple labels in case of doubtand were judged to agree on an item if their anno-tations had any label in common.
This less strin-gent measure yielded agreement of 52.31%.
Girjuet al (2005) report agreement for annotation usingboth Lauer?s 8 prepositional labels (??
= 0.8) andtheir own 35 semantic relations (??
= 0.58).
Thesefigures are difficult to interpret as annotators wereagain allowed assign multiple labels (for the prepo-sitions this occurred in ?almost all?
cases) and themultiply-labelled items were excluded from the cal-culation of Kappa.
This entails discarding the itemswhich are hardest to classify and thus most likely tocause disagreement.Girju (2006) has recently published impressiveagreement results on a related task.
This involvedannotating 2,200 compounds extracted from an on-line dictionary, each presented in five languages, andresulted in a Kappa score of 0.67.
This task mayhave been facilitated by the data source and its mul-tilingual nature.
It seems plausible that dictionaryentries are more likely to refer to familiar conceptsthan compounds extracted from a balanced corpus,which are frequently context-dependent coinages orrare specialist terms.
Furthermore, the translationsof compounds in Romance languages often pro-vide information that disambiguates the compoundmeaning (this aspect was the main motivation for thework) and translations from a dictionary are likelyto correspond to an item?s most frequent meaning.A qualitative analysis of the experiment describedabove suggests that about 30% of the disagreementscan confidently be attributed to disagreement aboutthe semantics of a given compound (as opposed tohow a given meaning should be annotated).84 SVM Learning with Co-occurrence Data4.1 MethodThe data used for classification was taken from the2,000 items used for the annotation experiment, an-notated by a single annotator.
Due to time con-straints, this annotation was done before the secondannotator had been used and was not changed af-terwards.
All compounds annotated as BE, HAVE,IN, INST, AGENT and ABOUT were used, giving adataset of 1,443 items.
All experiments were run us-ing Support Vector Machine classifiers implementedin LIBSVM.9 Performance was measured via 5-foldcross-validation.
Best performance was achievedwith a linear kernel and one-against-all classifica-tion.
The single SVM parameter C was estimatedfor each fold by cross-validating on the training set.Due to the efficiency of the linear kernel the optimi-sation, training and testing steps for each fold couldbe performed in under an hour.I investigated what level of performance couldbe achieved using only corpus information.
Featurevectors were extracted from the written BNC foreach modifier and head in the dataset under thefollowing conditions:w5, w10: Each word within a window of 5 or 10words on either side of the item is a feature.Rbasic, Rmod, Rverb, Rconj: These feature sets8For example, one annotator thought peat boy referred to aboy who sells peat (AGENT) while the other thought it referredto a boy buried in peat (IN).9http://www.csie.ntu.edu.tw/?cjlin/libsvm76use the grammatical relation output of the RASPparser run over the written BNC.
The Rbasic featureset conflates information about 25 grammaticalrelations; Rmod counts only prepositional, nominaland adjectival noun modification; Rverb countsonly relations among subjects, objects and verbs;Rconj counts only conjunctions of nouns.
In eachcase, each word entering into one of the targetrelations with the item is a feature and only thetarget relations contribute to the feature values.Each feature vector counts the target word?s co-occurrences with the 10,000 words that most fre-quently appear in the context of interest over the en-tire corpus.
Each compound in the dataset is rep-resented by the concatenation of the feature vectorsfor its head and modifier.
To model aspects of co-occurrence association that might be obscured byraw frequency, the log-likelihood ratio G2 was usedto transform the feature space.104.2 Results and AnalysisResults for these feature sets are given in Table 2.The simple word-counting conditions w5 and w10perform relatively well, but the highest accuracy isachieved by Rconj.
The general effect of the log-likelihood transformation cannot be stated categor-ically, as it causes some conditions to improve andothers to worsen, but the G2-transformed Rconj fea-tures give the best results of all with 54.95% ac-curacy (53.42% macro-average).
Analysis of per-formance across categories shows that in all casesaccuracy is lower (usually below 30%) on the BEand HAVE relations than on the others (often above50%).
These two relations are least common in thedataset, which is why the macro-averaged figures areslightly lower than the micro-averaged accuracy.4.3 DiscussionIt is interesting that the conjunction-based featuresgive the best performance, as these features are alsothe most sparse.
This may be explained by the factthat words appearing in conjunctions are often tax-onomically similar (Roark and Charniak, 1998) andthat taxonomic information is particularly useful for10This measure is relatively robust where frequency countsare low and consistently outperformed other association mea-sures in the empirical evaluation of Evert (2004).Raw G2Accuracy Macro Accuracy Macrow5 52.60% 51.07% 51.35% 49.93%w10 51.84% 50.32% 50.10% 48.60%Rbasic 51.28% 49.92% 51.83% 50.26%Rmod 51.35% 50.06% 48.51% 47.03%Rverb 48.79% 47.13% 48.58% 47.07%Rconj 54.12% 52.44% 54.95% 53.42%Table 2: Performance of BNC co-occurrence datacompound interpretation, as evidenced by the suc-cess of WordNet-based methods (see Section 5).In comparing reported classification results, it isdifficult to disentangle the effects of different data,annotation schemes and classification methods.
Theresults described here should above all be taken todemonstrate the feasibility of learning using a well-motivated annotation scheme and to provide a base-line for future work on the same data.
In terms ofmethodology, Turney?s (2006) Vector Space Modelexperiments are most similar.
Using feature vec-tors derived from lexical patterns and frequencies re-turned by a Web search engine, a nearest-neighbourclassifier achieves 45.7% accuracy on compoundsannotated with 5 semantic classes.
Turney improvesaccuracy to 58% with a combination of query ex-pansion and linear dimensionality reduction.
Thismethod trades off efficiency for accuracy, requiringmany times more resources in terms of time, stor-age and corpus size than that described here.
Lap-ata and Keller (2004) obtain accuracy of 55.71% onLauer?s (1995) prepositionally annotated data usingsimple search engine queries.
Their method has theadvantage of not requiring supervision, but it cannotbe used with deep semantic relations.5 SVM Classification with WordNet5.1 MethodThe experiments reported in this section make a ba-sic use of the WordNet11 hierarchy.
Binary featurevectors are used whereby a vector entry is 1 if theitem belongs to or is a hyponym of the synset corre-sponding to that feature, and 0 otherwise.
Each com-pound is represented by the concatenation of twosuch vectors, for the head and modifier.
The same11http://wordnet.princeton.edu/77classification method is used as in Section 4.5.2 Results and DiscussionThis method achieves accuracy of 56.76% andmacro-averaged accuracy of 54.59%, slightly higherthan that achieved by the co-occurrence features.Combining WordNet and co-occurrence vectors bysimply concatenating the G2-transformed Rconjvector and WordNet feature vector for each com-pound gives a further boost to 58.35% accuracy(56.70% macro-average).These results are higher than those reported forsimilar approaches on open-text data (Kim andBaldwin, 2005; Girju et al, 2005), though the samecaveat applies about comparison.
The best results(over 70%) reported so far for compound inter-pretation use a combination of multiple lexical re-sources and detailed additional annotation (Girju etal., 2005; Girju, 2006).6 Conclusion and Future DirectionsThe annotation scheme described above has beentested on a rigorous multiple-annotator task andachieved superior agreement to comparable resultsin the literature.
Further refinement should be possi-ble but would most likely yield diminishing returns.In the classification experiments, my goal was tosee what level of performance could be gained byusing straightforward techniques so as to providea meaningful baseline for future research.
Goodresults were achieved with methods that rely nei-ther on massive corpora or broad-coverage lexicalresources, though slightly better performance wasachieved using WordNet.
An advantage of resource-poor methods is that they can be used for the manylanguages where compounding is common but suchresources are limited.The learning approach described here only cap-tures the lexical semantics of the individual con-situents.
It seems intuitive that other kinds of corpusinformation would be useful; in particular, contextsin which the head and modifier of a compound bothoccur may make explicit the relations that typicallyhold between their referents.
Kernel methods for us-ing such relational information are investigated in O?Se?aghdha (2007a) with promising results, and I amcontinuing my research in this area.ReferencesCollin Baker, Charles Fillmore, and John Lowe.
1998.The Berkeley FrameNet project.
In Proc.
ACL-COLING-98, pages 86?90, Montreal, Canada.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20:37?46.Stefan Evert.
2004.
The Statistics of Word Cooccur-rences: Word Pairs and Collocations.
Ph.D. thesis,Universita?t Stuttgart.Roxana Girju, Dan Moldovan, Marta Tatu, and DanielAntohe.
2005.
On the semantics of noun compounds.Computer Speech and Language, 19(4):479?496.Roxana Girju.
2006.
Out-of-context noun phrase seman-tic interpretation with cross-linguistic evidence.
InProc.
CIKM-06, pages 268?276, Arlington, VA.Shelby J. Haberman.
1973.
The analysis of residuals incross-classified tables.
Biometrics, 29(1):205?220.Su Nam Kim and Timothy Baldwin.
2005.
Automaticinterpretation of noun compounds using WordNet sim-ilarity.
In Proc.
IJCNLP-05, pages 945?956, Jeju Is-land, Korea.Mirella Lapata and Frank Keller.
2004.
The Web as abaseline: Evaluating the performance of unsupervisedWeb-based models for a range of NLP tasks.
In Proc.HLT-NAACL-04, pages 121?128, Boston, MA.Mark Lauer.
1995.
Designing Statistical LanguageLearners: Experiments on Compound Nouns.
Ph.D.thesis, Macquarie University.Judith N. Levi.
1978.
The Syntax and Semantics of Com-plex Nominals.
Academic Press, New York.Vivi Nastase and Stan Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Proc.
IWCS-5,Tilburg, Netherlands.Brian Roark and Eugene Charniak.
1998.
Noun-phrase co-occurrence statistics for semi-automatic se-mantic lexicon construction.
In Proc.
ACL-COLING-98, pages 1110?1106, Montreal, Canada.Diarmuid O?
Se?aghdha.
2007a.
Co-occurrence contextsfor corpus-based noun compound interpretation.
InProc.
of the ACL Workshop A Broader Perspective onMultiword Expressions, Prague, Czech Republic.Diarmuid O?
Se?aghdha.
2007b.
Designing and evaluatinga semantic annotation scheme for compound nouns.
InProc.
Corpus Linguistics 2007, Birmingham, UK.Peter D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Andrew Wilson and Jenny Thomas.
1997.
Semantic an-notation.
In R. Garside, G. Leech, and A. McEnery,editors, Corpus Annotation.
Longman, London.Tong Zhang and Frank J. Oles.
2001.
Text categorizationbased on regularized linear classification methods.
In-formation Retrieval, 4(1):5?31.78
