Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1416?1421,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsType-Driven Incremental Semantic Parsing with Polymorphism?Kai ZhaoGraduate CenterCity University of New Yorkkzhao.hf@gmail.comLiang HuangQueens College and Graduate CenterCity University of New Yorkliang.huang.sh@gmail.comAbstractSemantic parsing has made significant progress, butmost current semantic parsers are extremely slow(CKY-based) and rather primitive in representation.We introduce three new techniques to tackle theseproblems.
First, we design the first linear-timeincremental shift-reduce-style semantic parsing al-gorithm which is more efficient than conventionalcubic-time bottom-up semantic parsers.
Second, ourparser, being type-driven instead of syntax-driven,uses type-checking to decide the direction of reduc-tion, which eliminates the need for a syntactic gram-mar such as CCG.
Third, to fully exploit the powerof type-driven semantic parsing beyond simple types(such as entities and truth values), we borrow fromprogramming language theory the concepts of sub-type polymorphism and parametric polymorphism toenrich the type system in order to better guide theparsing.
Our system learns very accurate parses inGEOQUERY, JOBS and ATIS domains.1 IntroductionMost existing semantic parsing efforts employ a CKY-style bottom-up parsing strategy to generate a meaningrepresentation in simply typed lambda calculus (Zettle-moyer and Collins, 2005; Lu and Ng, 2011) or its variants(Wong and Mooney, 2007; Liang et al, 2011).
Althoughthese works led to fairly accurate semantic parsers, thereare two major drawbacks: efficiency and expressiveness.First, as many researches in syntactic parsing (Nivre,2008; Zhang and Clark, 2011) have shown, compared tocubic-time CKY-style parsing, incremental parsing canachieve comparable accuracies while being linear-time,and orders of magnitude faster in practice.
We thereforeintroduce the first incremental parsing algorithm for se-mantic parsing.
More interestingly, unlike syntactic pars-ing, our incremental semantic parsing algorithm, beingstrictly type-driven, directly employs type checking toautomatically determine the direction of function applica-tion on-the-fly, thus reducing the search space and elimi-?We thank the reviewers for helpful suggestions.
We are also grate-ful to Luke Zettelmoyer, Yoav Artzi, and Tom Kwiatkowski for pro-viding data.
This research is supported by DARPA FA8750-13-2-0041(DEFT), NSF IIS-1449278, and a Google Faculty Research Award.nating the need for a syntactic grammar such as CCG toexplicitly encode the direction of function application.However, to fully exploit the power of type-driven in-cremental parsing, we need a more sophisticated typesystem than simply typed lambda calculus.
Compare thefollowing two phrases:(1) the governor of New York(2) the mayor of New YorkIf we know that governor is a function from state to per-son, then the first New York can only be of type state; sim-ilarly knowing mayor maps city to person disambiguatesthe second New York to be of type city.
This can not bedone using a simple type system with just entities andbooleans.Now let us consider a more complex question whichwill be our running example in this paper:(3) What is the capital of the largest state by area?Since we know capital takes a state as input, we expectthe largest state by area to return a state.
But does largestalways return a state type?
Notice that it is polymorphic,for example, largest city by population, or largest lakeby perimeter.
So there is no unique type for largest: itsreturn type should depend on the type of its first argu-ment (city, state, or lake).
This observation motivates usto introduce the powerful mechanism of parametric poly-morphism from programming languages into natural lan-guage semantics for the first time.For example, we can define the type of largest to be atemplatelargest : ('a?t)?('a?i)?
'awhere 'a is a type variable that can match any type (forformal details see ?3).
Just like in functional program-ming languages such as ML or Haskell, type variablescan be bound to a real type (or a range of types) duringfunction application, using the technique of type infer-ence.
In the above example, when largest is applied tocity, we know that type variable 'a is bound to type city(or its subtype), so largest would eventually return a city.We make the following contributions:?
We design the first linear-time incremental semanticparsing algorithm (?2), which is much more efficientthan the existing semantic parsers that are cubic-time and CKY-based.1416?
In line with classical Montague theory (Heim andKratzer, 1998), our parser is type-driven instead ofsyntax-driven as in CCG-based efforts (Zettlemoyerand Collins, 2005; Kwiatkowski et al, 2011; Krish-namurthy and Mitchell, 2014) (?2.3).?
We introduce parametric polymorphism into nat-ural language semantics (?3), along with propertreatment of subtype polymorphism, and implementHindley-Milner style type inference (Pierce, 2005,Chap.
10) during parsing (?3.3).1?
We adapt the latent-variable max-violation percep-tron training from machine translation (Yu et al,2013), which is a perfect fit for semantic parsing dueto its huge search space (?4).2 Type-Driven Incremental ParsingWe start with the simplest meaning representation (MR),untyped lambda calculus, and introduce typing and theincremental parsing algorithm for it.
Later in ?3, we addsubtyping and type polymorphism to enrich the system.2.1 Meaning Representation with TypesThe untyped MR for the running example is:Q: What is the capital of the largest state by area?MR: (capital (argmax state size))Note the binary function argmax(?, ?)
is a higher-orderfunction that takes two other functions as input: the firstargument is a ?domain?
function that defines the set tosearch for, and second argument is an ?evaluation?
func-tion that returns a integer for an element in that domain.The simply typed lambda calculus (Heim and Kratzer,1998; Lu and Ng, 2011) augments the system with types,including base types (entities e, truth values t, or num-bers i), and function types (e.g., e?t).
So capital is oftype e?e, state is of type e?t, and size is of type e?i.The argmax function is of type (e?t)?
(e?i)?e.2Thesimply typed MR is now written as(capital :e?e (argmax :(e?t)?
(e?i)?estate :e?t size :e?i))).2.2 Incremental Semantic Parsing: An ExampleSimilar to a standard shift-reduce parser, we maintaina stack and a queue.
The queue contains words to be1There are three kinds of polymorphisms in programming lan-guages: parametric (e.g., C++ templates), subtyping, and ad-hoc (e.g.,operator overloading).
See Pierce (2002, Chap.
15) for details.2Note that the type notation is always curried, i.e., we repre-sent a binary function as a unary function that returns another unaryfunction.
Also the type notation is always right-associative, so(e?t)?
((e?i)?e) is also written as (e?t)?
(e?i)?e.pattern ?-expression templates, simple types (?2.2)JJS ?P : (e?t)?
(e?i)?e .
PNN ?P :e?e .
P ; ?P :e?t .
P ; ?P :e?i .
Ppattern ?-expression templates, polymorphic types (?3.3)JJS ?P : ('a?t)?('a?i)?
'a .
PNN ?P :'b?
'c .
PTable 1: POS-based meaning representation templates used inthe running example (see Figure 1).
The polymorphic typesgreatly simplifies the representation for common nouns (NN).parsed, while the stack contains subexpressions of the fi-nal MR, each of which is a valid typed lambda expres-sion.
At each step, the parser choose to shift or reduce,but unlike standard shift-reduce parser, there is also athird possible action, skip, skipping a semantically vacu-ous word (e.g., ?the?, ?of?, ?is?, etc.).
For example, thefirst three words of the example question ?What is the ...?are all skipped (steps 1?3 in Figure 1 (left)).The parser then shifts the next word, ?capital?, fromthe queue to the stack.
But unlike incremental syn-tactic parsing where the word itself is moved onto thestack, here we need to find a grounded predicate in theGeoQuery domain for the current word.
Triggered bythe POS tag NN of word ?capital?, the template ?P :e?e .
P is fetched from a predefined MR templates setlike Table 1.
In its outermost lambda abstraction, variableP needs to be grounded on-the-fly before we push the ex-pression onto the stack.
We find a predicate capital :e?ein the GEOQUERY domain applicable to the MR tem-plate.
After the application, we push the result onto thestack (step 4).Next, words ?of the?
are skipped (steps 5?6).
For thenext word ?largest?, argmax : (e?t)?
(e?i)?e is ap-plied to the MR template triggered by its POS tag JJS inTable 1, and the stack becomes (step 7)capital :e?e argmax : (e?t)?
(e?i)?e.At this step we have two expressions on the stack andwe could attempt to reduce.
But type checking fails be-cause for left reduce, argmax expects an argument (its?domain?
function) of type (e?t) which is different fromcapital?s type (e?e), so is the case for right reduce.So we have to shift again.
This time for word ?state?
:state :e?t.
The stack becomes:capital :e?e argmax : (e?t)?
(e?i)?e state :e?t.2.3 Type-Driven ReduceAt this step we can finally perform a reduce action,since the top two expressions on the stack pass the type-checking for rightward function application (a partial ap-plication): argmax expects an (e?t) argument, which isexactly the type of state.
So we conduct a right-reduce,applying argmax on state, which results in(argmax state) : (e?i)?e1417step action stack after action (simple type) stack after action (subtyping+polymorphism)0 - ?
?1?3 skip ?
?4 shcapitalcapital:e?e capital:st?ct7 shlargestcapital:e?e argmax:(e?t)?
(e?i)?e capital:st?ct argmax : ('a?t)?('a?i)?
'a8 shstatecapital:e?e argmax:(e?t)?
(e?i)?e state:e?t capital:st?ct argmax : ('a?t)?('a?i)?
'a state :st?t9 reycapital:e?e (argmax state):(e?i)?e capital:st?ct (argmax state) : (st?i)?st ?11 shareacapital:e?e (argmax state):(e?i)?e size:e?i capital:st?ct (argmax state) : (st?i)?st size : lo?i12 reycapital:e?e (argmax state size):e capital:st?ct (argmax state size) :st ?13 rey(capital (argmax state size)):e (capital (argmax state size)) :ctFigure 1: Type-driven Incremental Semantic Parsing (TISP) with (a) simple types and (b) subtyping+polymorphism on the examplequestion: ?what is the capital of the largest state by area??.
Steps 5?6 and 10 are skip actions and thus omitted.
The stack and queuein each row are the results after each action.
?
: Type variable 'a is binded to st.
?
: From Eq.
4, st <: lo ?
(lo?i)<: (st?i).while the stack becomes (step 9)capital :e?e (argmax state) : (e?i)?eNow if we want to continue reduction, it does not typecheck for either left or right reduction, so we have to shiftagain.
So we move on to shift the final word ?area?
withpredicate: size :e?i and the stack becomes (step 11):capital :e?e (argmax state) : (e?i)?e size :e?i.Now we can do two consecutive right reduces supportedby type checking (step 12, 13) and get the final result:(capital (argmax state size)) :e.Here we can see the novelty of our shift-reduce parser:its decisions are largely driven by the type system.
Whenwe attempt a reduce, at most one of the two reduceactions (left, right) is possible thanks to type check-ing, and when neither is allowed, we have to shift (orskip).
This observation suggests that our incrementalparser is more deterministic than those syntactic incre-mental parsers where each step always faces a three-waydecision (shift, left-reduce, right-reduce).
We also notethat this type-checking mechanism, inspired by the clas-sical type-driven theory in linguistics (Heim and Kratzer,1998), eliminates the need for an explicit encoding ofdirection as in CCG, which makes our formalism muchsimpler than the synchronous syntactic-semantic onesin most other semantic parsing efforts (Zettlemoyer andCollins, 2005; Zettlemoyer and Collins, 2007; Wong andMooney, 2007).33 Subtype and Parametric PolymorphismsCurrently in simply typed lambda calculus representationfunction capital can apply to any entity type, for examplecapital(boston), which should have been disallowed bythe type checker.
So we need a more sophisticated systemthat helps ground with refined types, which will in turnhelp type-driven parsing.3We need to distinguish between two concepts: a) ?direction of re-duction?
: f(g) or g(f).
Obviously at any given time, between the toptwo (unarized) functions f and g on the stack, at most one reduction ispossible.
b) ?order of arguments?
: f(x, y) or f(y, x).
For predicatessuch as loc : lo?lo?t the order does matter.
Our parser can not dis-tinguish this purely via types, nor can CCG via its syntactic categories.In practice, it is decided by features such as the voice of the verb.top (root)i (integer)lo (location)nu (nature unit)lk (lake)rv (river)au (admin.
unit)ct (city)st (state)t (boolean)Figure 2: Type hierarchy for GEOQUERY (slightly simplified).3.1 Semantics with Subtype PolymorphismWe first augment the meaning representation with a do-main specific type hierarchy.
For example Figure 2 showsa (slightly simplified) version of the type hierarchy forGEOQUERY domain.
We use <: to denote the (tran-sitive, reflexive, and antisymmetric) subtyping relationbetween types; for example in GEOQUERY, st <: lo.Each constant in the GEOQUERY domain is welltyped.
For example, there are states (michigan:st), cities(nyc:ct), rivers (mississippi:rv), and lakes (tahoe:lk).Similarly each predicate is also typed.
For example,we can query the length of a river, len:rv?i, or the pop-ulation of some administrative unit, population:au?i.Notice that population(?)
can be applied to both statesand cities, since they are subtypes of administrative unit,i.e., st <: au and ct <: au.
This is because, as inJava and C++, a function that expects a certain type canalways take an argument of a subtype.
For example,we can query whether two locations are adjacent, usingnext_to:lo?
(lo?t), as the next_to(?, ?)
function can beapplied to two states, or to a river and a city, etc.Before we move on, there is an important consequenceof polymorphism worth mentioning here.
For the types ofunary predicates such as city(?)
and state(?)
that charac-terize its argument, we define theirs argument types to bethe required type, i.e., city : ct?t, and state : st?t.
Thismight look a little weird since everything in the domainof those functions are always mapped to true; i.e., f(x)is either undefined or true, and never false for such f ?s.This is different from classical simply-typed Montaguesemantics (Heim and Kratzer, 1998) which defines suchpredicates as type top?t so that city(mississippi :st) re-turns false.
The reason for our design is, again, due to1418subtyping and polymorphism: capital takes a state typeas input, so argmax must returns a state, and therefore itsfirst argument, the state function, must have type st?tso that the matched type variable 'a will be bound to st.This more refined design will also help prune unneces-sary argument matching using type checking.3.2 Semantics with Parametric PolymorphismThe above type system works smoothly for first-orderfunctions (i.e., predicates taking atomic type arguments),but the situation with higher-order functions (i.e., predi-cates that take functions as input) is more involved.
Whatis the type of argmax in the context ?the capital of largeststate ...??
One possibility is to define it to be as general aspossible, as in the simply typed version (and many con-ventional semantic parsers):argmax : (top?t)?
(top?i)?top.But this actually no longer works for our sophisticatedtype system for the following reason.Intuitively, remember that capital:st?ct is a functionthat takes a state as input, so the return type of argmaxmust be a state or its subtype, rather than top which is asupertype of st.
But we can not simply replace top byst, since argmax can also be applied in other scenariossuch as ?the largest city?.
In other words, argmax is apolymorphic function, and to assign a correct type for itwe have to introduce type variables:argmax : ('a?t)?('a?i)?
'a,where type variable 'a is a place-holder for ?any type?.3.3 Parsing with Subtype Polymorphism andParametric PolymorphismWe modify the previous parsing algorithm to accommo-date subtyping and polymorphic types.
Figure 1 (right)shows the derivation of the running example using thenew parsing algorithm.
Below we focus on the differ-ences brought by the new algorithm.Note that we also modified the MR templates as in Ta-ble 1.
The new MR templates are more general due to thepolymorphism from type variables.
For example, now weuse only one MR template ?P :'b?
'c .
P to replace thethree NN MR templates for simple types.In step 4, unlike capital : e?e, we shift the predicatecapital : st?ct; in step 7, we shift the polymorphic ex-pression for ?largest?
: argmax : ('a?t)?('a?i)?
'a.And after the shift in step 8, the stack becomescapital :st?ct argmax : ('a?t)?('a?i)?
'a state :st?tAt step 9, in order to apply argmax onto state : st?t,we simply bind type variable 'a to type st, results in(argmax state) : (st?i)?st.After the shift in step 11, the stack becomes:capital :st?ct (argmax state) : (st?i)?st size : lo?i.Can we still apply right reduce here?
According to sub-typing requirement (?3.1), we want lo?i <: st?i tohold, knowing that st <: lo.
Luckily, there is a rule aboutfunction types in type theory that exactly fits here:A <: BB?C <: A?C(4)which states the input side is reversed (contravariant).This might look counterintuitive, but the intuition is that,it is safe to allow the function size : lo?i to be used in thecontext where another type st?i is expected, since in thatcontext the argument passed to size will be state type (st),which is a subtype of location type (lo) that size expects,which in turn will not surprise size.
See the classical typetheory textbook (Pierce, 2002, Chap.
15.2) for details.Several works in literature (Zettlemoyer and Collins,2005; Zettlemoyer and Collins, 2007; Wong and Mooney,2007; Kwiatkowski et al, 2013) employ some primitivetype hierarchies and parse with typed lambda calculus.However, simply introducing subtyped predicates with-out polymorphism will cause type checking failures inhandling high-order functions, as we discussed above.4 Training: Latent Variable PerceptronWe follow the latent variable max-violation perceptronalgorithm of Yu et al (2013) for training.
This algorithmis based on the ?violation-fixing?
framework of Huang etal.
(2012) which is tailored to structured learning prob-lems with abundant search errors such as parsing or ma-chine translation.The key challenge in the training is that, for each ques-tion, there might be many different unknown derivationsthat lead to its annotated MR, which is known as thespurious ambiguity.
In our task, the spurious ambigu-ity is caused by how the MR templates are chosen andgrounded during the shift step, and the different reduceorders that lead to the same result.
We treat this unknowninformation as latent variable.More formally, we denote D(x) to be the set of allpartial and full parsing derivations for an input sentencex, and mr(d) to be the MR yielded by a full derivationd.
Then we define the sets of (partial and full) referencederivations as:goodi(x, y)?= {d ?
D(x) | |d| = i,?full derivation d?s.t.d is a prefix of d?,mr(d?)
= y},Those ?bad?
partial and full derivations that do not leadto the annotated MR can be defined as:badi(x, y)?= {d ?
D(x) | d 6?
goodi(x, y), |d| = i}.At step i, the best reference partial derivation isd+i(x, y)?= argmaxd?goodi(x,y)w ??
(x, d), (5)1419GEOQUERY JOBS ATISSystem P R F1 P R F1 P R F1Z&C?05 96.3 79.3 87.0 97.3 79.3 87.4 - - -Z&C?07 91.6 86.1 88.8 - - - 85.8 84.6 85.2UBL 94.1 85.0 89.3 - - - 72.1 71.4 71.7FUBL 88.6 88.6 88.6 - - - 82.8 82.8 82.8TISP (st) 89.7 86.8 88.2 76.4 76.4 76.4 - - -TISP 92.9 88.9 90.9 85.0 85.0 85.0 84.7 84.2 84.4Table 2: Performances (precision, recall, and F1) of variousparsing algorithms on GEOQUERY, JOBS, and ATIS datasets.TISP with simple types are marked ?st?.while the Viterbi partial derivation isd?i(x, y)?= argmaxd?badi(x,y)w ??
(x, d), (6)where ?
(x, d) is the defined feature set for derivation d.In practice, to compute Eq.
6 exactly is intractable, andwe resort to beam search.
Following Yu et al (2013),we then find the step i?with the maximal score differ-ence between the best reference partial derivation and theViterbi partial derivation:i?
?= argmaxiw ???
(x, d+i(x, y), d?i(x, y)),and do update w ?
w + ??
(x, d+i?
(x, y), d?i?
(x, y))where ??
(x, d, d?
)?= ?
(x, d)??
(x, d?
).We also use minibatch parallelization of Zhao andHuang (2013); in practice we use 24 cores.5 ExperimentsWe implement our type-driven incremental semanticparser (TISP) using Python, and evaluate its perfor-mance on GEOQUERY, JOBS, and ATIS datasets.Our feature design is inspired by the very effectiveWord-Edge features in syntactic parsing (Charniak andJohnson, 2005) and MT (He et al, 2008).
From eachparsing state, we collect atomic features including thetypes and the leftmost and rightmost words of the spanof the top 3 MR expressions on the stack, the top 3 wordson the queue, the grounded predicate names and the IDof the MR template used in the shift action.
We use bud-get scheme similar to (Yu et al, 2013) to alleviate theoverfitting problem caused by feature sparsity.
We get84 combined feature templates in total.
Our final systemcontains 62 MR expression templates, of which 33 aretriggered by POS tags, and 29 are triggered by specificphrases.In the experiments, we use the same training, develop-ment, and testing data splits as Zettlemoyer and Collins(2005) and Zettlemoyer and Collins (2007).For evaluation, we follow Zettlemoyer and Collins(2005) to use precision and recall:Precision =# of correctly parsed questions# of successfully parsed questions,top (root)i (integer)jb (job)qa (qualification)pa (platform)ar (area)ye (year)t (boolean)Figure 3: Type hierarchy for JOBS domain (slightly simplified).Recall =# of correctly parsed questions# of questions.5.1 Evaluation on GEOQUERY DatasetWe first evaluate TISP on GEOQUERY dataset.In the training and evaluating time, we use a very smallbeam size of 16, which gives us very fast decoding.
Inserial mode, our parser takes ?
83s to decode the 280sentences (2,147 words) in the testing set, which means?0.3s per sentence, or ?0.04s per word.We compare the our accuracy performance with ex-isting methods (Zettlemoyer and Collins, 2005; Zettle-moyer and Collins, 2007; Kwiatkowski et al, 2010;Kwiatkowski et al, 2011) in Table 2.
Given that all othermethods use CKY-style parsing, our method is well bal-anced between accuracy and speed.In addition, to unveil the helpfulness of our type sys-tem, we train a parser with only simple types.
(Table 2)In this setting, the predicates only have primitive typesof location lo, integer i, and boolean t, while the con-stants still keep their types.
It still has the type system,but it is weaker than the polymorphic one.
Its accuracyis lower than the standard one, mostly caused by that thetype system can not help pruning the wrong applicationslike (population:au?i mississippi:rv).5.2 Evaluations on JOBS and ATIS DatasetsWe also evaluate the performance of our parser on JOBSand ATIS datasets.
Figure 3 shows the type hierarchy forJOBS.
We omit the type hierarchy for ATIS due to spaceconstraint.
Note that ATIS contains more than 5,000 ex-amples and is a lot larger than GEOQUERY and JOBS.We show the results in Table 2.
In JOBS, we achievesvery good recall, but the precision is not as good as Zettle-moyer and Collins (2005), which is actually because weparsed a lot more sentences.
Also, TISP with simple typesis still weaker than the one with subtyping and parametricpolymorphisms.
For ATIS, our performance is very closeto the state-of-the-art.6 ConclusionWe have presented an incremental semantic parser thatis guided by a powerful type system of subtyping andpolymorphism.
This polymorphism greatly reduced thenumber of templates and effectively pruned search spaceduring the parsing.
Our parser is competitive with state-of-the-art accuracies, but, being linear-time, is faster thanCKY-based parsers in theory and in practice.1420ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
In Pro-ceedings of ACL, pages 173?180, Ann Arbor, Michigan,June.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.
Improvingstatistical machine translation using lexicalized rule selec-tion.
In Proceedings of COLING, pages 321?328, Manch-ester, UK, August.Irene Heim and Angelika Kratzer.
1998.
Semantics in Genera-tive Grammar.
Blackwell Publishing.Liang Huang, Suphan Fayong, and Yang Guo.
2012.
Structuredperceptron with inexact search.
In Proceedings of NAACL.Jayant Krishnamurthy and Tom M Mitchell.
2014.
Jointsyntactic and semantic parsing with combinatory categorialgrammar.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, andMark Steedman.
2010.
Inducing probabilistic ccg grammarsfrom logical form with higher-order unification.
In Proceed-ings of the 2010 conference on empirical methods in natu-ral language processing, pages 1223?1233.
Association forComputational Linguistics.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, andMark Steedman.
2011.
Lexical generalization in ccg gram-mar induction for semantic parsing.
In Proceedings ofEMNLP, EMNLP ?11.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettle-moyer.
2013.
Scaling semantic parsers with on-the-fly on-tology matching.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.
Learningdependency-based compositional semantics.
In Associationfor Computational Linguistics (ACL), pages 590?599.Wei Lu and Hwee Tou Ng.
2011.
A probabilistic forest-to-string model for language generation from typed lambda cal-culus expressions.
In Proceedings of EMNLP.Joakim Nivre.
2008.
Algorithms for deterministic incrementaldependency parsing.
Computational Linguistics, 34(4):513?553.Benjamin C. Pierce.
2002.
Types and Programming Lan-guages.
MIT Press.Benjamin Pierce, editor.
2005.
Advanced Topics in Types andProgramming Languages.
MIT Press.Yuk Wah Wong and Raymond J Mooney.
2007.
Learning syn-chronous grammars for semantic parsing with lambda calcu-lus.
In Annual Meeting-Association for computational Lin-guistics, volume 45, page 960.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013.
Max-violation perceptron and forced decoding for scalable mttraining.
In Proceedings of EMNLP 2013.Luke Zettlemoyer and Michael Collins.
2005.
Learning tomap sentences to logical form: Structured classification withprobabilistic categorial grammars.
In Proceedings of UAI.Luke S Zettlemoyer and Michael Collins.
2007.
Online learn-ing of relaxed ccg grammars for parsing to logical form.
InIn Proceedings of EMNLP-CoNLL-2007.
Citeseer.Yue Zhang and Stephen Clark.
2011.
Shift-reduce ccg parsing.In Proceedings of ACL.Kai Zhao and Liang Huang.
2013.
Minibatch and paralleliza-tion for online large margin structured learning.
In Proceed-ings of NAACL 2013.1421
