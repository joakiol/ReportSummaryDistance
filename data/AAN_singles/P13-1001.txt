Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1?10,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Shift-Reduce Parsing Algorithm for Phrase-basedString-to-Dependency TranslationYang LiuState Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and TechnologyTsinghua University, Beijing 100084, Chinaliuyang2011@tsinghua.edu.cnAbstractWe introduce a shift-reduce parsingalgorithm for phrase-based string-to-dependency translation.
As the algorithmgenerates dependency trees for partialtranslations left-to-right in decoding, itallows for efficient integration of bothn-gram and dependency language mod-els.
To resolve conflicts in shift-reduceparsing, we propose a maximum entropymodel trained on the derivation graph oftraining data.
As our approach combinesthe merits of phrase-based and string-to-dependency models, it achieves significantimprovements over the two baselines onthe NIST Chinese-English datasets.1 IntroductionModern statistical machine translation approachescan be roughly divided into two broad categories:phrase-based and syntax-based.
Phrase-based ap-proaches treat phrase, which is usually a sequenceof consecutive words, as the basic unit of trans-lation (Koehn et al, 2003; Och and Ney, 2004).As phrases are capable of memorizing local con-text, phrase-based approaches excel at handlinglocal word selection and reordering.
In addition,it is straightforward to integrate n-gram languagemodels into phrase-based decoders in which trans-lation always grows left-to-right.
As a result,phrase-based decoders only need to maintain theboundary words on one end to calculate languagemodel probabilities.
However, as phrase-based de-coding usually casts translation as a string con-catenation problem and permits arbitrary permuta-tion, it proves to be NP-complete (Knight, 1999).Syntax-based approaches, on the other hand,model the hierarchical structure of natural lan-guages (Wu, 1997; Yamada and Knight, 2001;Chiang, 2005; Quirk et al, 2005; Galley et al,2006; Liu et al, 2006; Huang et al, 2006;Shen et al, 2008; Mi and Huang, 2008; Zhanget al, 2008).
As syntactic information can beexploited to provide linguistically-motivated re-ordering rules, predicting non-local permutationis computationally tractable in syntax-based ap-proaches.
Unfortunately, as syntax-based de-coders often generate target-language words in abottom-up way using the CKY algorithm, inte-grating n-gram language models becomes moreexpensive because they have to maintain targetboundary words at both ends of a partial trans-lation (Chiang, 2007; Huang and Chiang, 2007).Moreover, syntax-based approaches often sufferfrom the rule coverage problem since syntac-tic constraints rule out a large portion of non-syntactic phrase pairs, which might help decodersgeneralize well to unseen data (Marcu et al,2006).
Furthermore, the introduction of non-terminals makes the grammar size significantlybigger than phrase tables and leads to higher mem-ory requirement (Chiang, 2007).As a result, incremental decoding with hierar-chical structures has attracted increasing attentionin recent years.
While some authors try to inte-grate syntax into phrase-based decoding (Galleyand Manning, 2008; Galley and Manning, 2009;Feng et al, 2010), others develop incremental al-gorithms for syntax-based models (Watanabe etal., 2006; Huang and Mi, 2010; Dyer and Resnik,2010; Feng et al, 2012).
Despite these success-ful efforts, challenges still remain for both direc-tions.
While parsing algorithms can be used toparse partial translations in phrase-based decod-ing, the search space is significantly enlarged sincethere are exponentially many parse trees for expo-nentially many translations.
On the other hand, al-though target words can be generated left-to-rightby altering the way of tree transversal in syntax-based models, it is still difficult to reach full rulecoverage as compared with phrase table.1zongtong jiang yu siyue lai lundun fangwenThe President will visit London in Aprilsource phrase target phrase dependency categoryr1 fangwen visit {} fixedr2 yu siyue in April {1?
2} fixedr3 zongtong jiang The President will {2?
1} floating leftr4 yu siyue lai lundun London in April {2?
3} floating rightr5 zongtong jiang President will {} ill-formedFigure 1: A training example consisting of a (romanized) Chinese sentence, an English dependencytree, and the word alignment between them.
Each translation rule is composed of a source phrase, atarget phrase with a set of dependency arcs.
Following Shen et al (2008), we distinguish between fixed,floating, and ill-formed structures.In this paper, we propose a shift-reduce parsingalgorithm for phrase-based string-to-dependencytranslation.
The basic unit of translation in ourmodel is string-to-dependency phrase pair, whichconsists of a phrase on the source side and a depen-dency structure on the target side.
The algorithmgenerates well-formed dependency structures forpartial translations left-to-right using string-to-dependency phrase pairs.
Therefore, our approachis capable of combining the advantages of bothphrase-based and syntax-based approaches:1. compact rule table: our rule table is a subsetof the original string-to-dependency gram-mar (Shen et al, 2008; Shen et al, 2010) byexcluding rules with non-terminals.2.
full rule coverage: all phrase pairs, bothsyntactic and non-syntactic, can be used inour algorithm.
This is the same with Moses(Koehn et al, 2007).3. efficient integration of n-gram languagemodel: as translation grows left-to-right inour algorithm, integrating n-gram languagemodels is straightforward.4.
exploiting syntactic information: as theshift-reduce parsing algorithm generates tar-get language dependency trees in decoding,dependency language models (Shen et al,2008; Shen et al, 2010) can be used to en-courage linguistically-motivated reordering.5.
resolving local parsing ambiguity: as de-pendency trees for phrases are memorizedin rules, our approach avoids resolving localparsing ambiguity and explores in a smallersearch space than parsing word-by-word onthe fly in decoding (Galley and Manning,2009).We evaluate our method on the NIST Chinese-English translation datasets.
Experiments showthat our approach significantly outperforms bothphrase-based (Koehn et al, 2007) and string-to-dependency approaches (Shen et al, 2008) interms of BLEU and TER.2 Shift-Reduce Parsing for Phrase-basedString-to-Dependency TranslationFigure 1 shows a training example consisting ofa (romanized) Chinese sentence, an English de-pendency tree, and the word alignment betweenthem.
Following Shen et al (2008), string-to-dependency rules without non-terminals can beextracted from the training example.
As shownin Figure 1, each rule is composed of a sourcephrase and a target dependency structure.
Shen etal.
(2008) divide dependency structures into twobroad categories:1. well-formed(a) fixed: the head is known or fixed;20 ?
?
?
?
?
?
?1 S r3 [The President will] ?
?
?
?
?
?
?2 S r1 [The President will] [visit] ?
?
?
?
?
?
?3 Rl [The President will visit] ?
?
?
?
?
?
?4 S r4 [The President will visit] [London in April] ?
?
?
?
?
?
?5 Rr [The President will visit London in April] ?
?
?
?
?
?
?step action rule stack coverageFigure 2: Shift-reduce parsing with string-to-dependency phrase pairs.
For each state, the algorithmmaintains a stack to store items (i.e., well-formed dependency structures).
At each step, it chooses oneaction to extend a state: shift (S), reduce left (Rl), or reduce right (Rr).
The decoding process terminateswhen all source words are covered and there is a complete dependency tree in the stack.
(b) floating: sibling nodes of a commonhead, but the head itself is unspecifiedor floating.
Each of the siblings must bea complete constituent.2.
ill-formed: neither fixed nor floating.We further distinguish between left and rightfloating structures according to the position ofhead.
For example, as ?The President will?
is theleft dependant of its head ?visit?, it is a left floatingstructure.To integrate the advantages of phrase-basedand string-to-dependency models, we propose ashift-reduce algorithm for phrase-based string-to-dependency translation.Figure 2 shows an example.
We describe a state(i.e., parser configuration) as a tuple ?S, C?
whereS is a stack that stores items and C is a cover-age vector that indicates which source words havebeen translated.
Each item s ?
S is a well-formeddependency structure.
The algorithm starts withan empty state.
At each step, it chooses one of thethree actions (Huang et al, 2009) to extend a state:1. shift (S): move a target dependency structureonto the stack;2. reduce left (Rl): combine the two items onthe stack, st and st?1 (t ?
2), with the root ofst as the head and replace them with a com-bined item;3. reduce right (Rr): combine the two items onthe stack, st and st?1 (t ?
2), with the rootof st?1 as the head and replace them with acombined item.The decoding process terminates when all sourcewords are covered and there is a complete depen-dency tree in the stack.Note that unlike monolingual shift-reduceparsers (Nivre, 2004; Zhang and Clark, 2008;Huang et al, 2009), our algorithm does not main-tain a queue for remaining words of the input be-cause the future dependency structure to be shiftedis unknown in advance in the translation scenario.Instead, we use a coverage vector on the sourceside to determine when to terminate the algorithm.For an input sentence of J words, the number ofactions is 2K ?
1, where K is the number of rulesused in decoding.
1 There are always K shifts and1Empirically, we find that the average number of stacksfor J words is about 1.5 ?
J on the Chinese-English data.3[The President] [will] [visit][The President] [will] [visit] [London][The President] [will] [visit London][The President] [will visit London][The President] [will visit][The President will visit][The President will visit] [London][The President will visit London]SRrRlRlRlRlSRrFigure 3: Ambiguity in shift-reduce parsing.st?1 st legal action(s)yes Sh yes Sl yes Sr noh h yes S, Rl, Rrh l yes Sh r yes Rrl h yes Rll l yes Sl r nor h nor l nor r noTable 1: Conflicts in shift-reduce parsing.
st andst?1 are the top two items in the stack of a state.We use ?h?
to denote fixed structure, ?l?
to de-note left floating structure, and ?r?
to denote rightfloating structure.
It is clear that only ?h+h?
is am-biguous.K ?
1 reductions.It is easy to verify that the reduce left and re-duce right actions are equivalent to the left adjoin-ing and right adjoining operations defined by Shenet al (2008).
They suffice to operate on well-formed structures and produce projective depen-dency parse trees.Therefore, with dependency structures presentin the stacks, it is possible to use dependency lan-guage models to encourage linguistically plausiblephrase reordering.3 A Maximum Entropy BasedShift-Reduce Parsing ModelShift-reduce parsing is efficient but suffers fromparsing errors caused by syntactic ambiguity.
Fig-ure 3 shows two (partial) derivations for a depen-dency tree.
Consider the item on the top, the algo-rithm can either apply a shift action to move a newitem or apply a reduce left action to obtain a big-ger structure.
This is often referred to as conflictin the shift-reduce dependency parsing literature(Huang et al, 2009).
In this work, the shift-reduceparser faces four types of conflicts:1. shift vs. shift;2. shift vs. reduce left;3. shift vs. reduce right;4. reduce left vs. reduce right.Fortunately, if we distinguish between left andright floating structures, it is possible to rule outmost conflicts.
Table 1 shows the relationshipbetween conflicts, dependency structures and ac-tions.
We use st and st?1 to denote the top two4[The President will visit London][in April]DT NNP MD VB NNP IN INtype feature templatesUnigram c Wh(st) Wh(st?1)Wlc(st) Wrc(st?1) Th(st)Th(st?1) Tlc(st) Trc(st?1)Bigram Wh(st) ?Wh(st?1) Th(St) ?
Th(st?1) Wh(st) ?
Th(st)Wh(st?1) ?
Th(st?1) Wh(st) ?Wrc(st?1) Wh(st?1) ?Wlc(st)Trigram c ?Wh(st) ?W (st?1) c ?
Th(st) ?
Th(st?1) Wh(st) ?Wh(st?1) ?
Tlc(st)Wh(st) ?Wh(st?1) ?
Trc(st?1) Th(st) ?
Th(st?1) ?
Tlc(st) Th(st) ?
Th(st?1) ?
Trc(st?1)Figure 4: Feature templates for maximum entropy based shift-reduce parsing model.
c is a booleanvalue that indicate whether all source words are covered (shift is prohibited if true), Wh(?)
and Th(?
)are functions that get the root word and tag of an item, Wlc(?)
and Tlc(?)
returns the word and tag ofthe left most child of the root, Wrc(?)
amd Trc(?)
returns the word and tag of the right most child of theroot.
Symbol ?
denotes feature conjunction.
In this example, c = true, Wh(st) = in, Th(st) = IN,Wh(st?1) = visit, Wlc(st?1) = London.items in the stack.
?h?
stands for fixed struc-ture, ?l?
for left floating structure, and ?r?
for rightfloating structure.
If the stack is empty, the onlyapplicable action is shift.
If there is only one itemin the stack and the item is either fixed or left float-ing, the only applicable action is shift.
Note that itis illegal to shift a right floating structure onto anempty stack because it will never be reduced.
Ifthe stack contains at least two items, only ?h+h?is ambiguous and the others are either unambigu-ous or illegal.
Therefore, we only need to focus onhow to resolve conflicts for the ?h+h?
case (i.e.,the top two items in a stack are both fixed struc-tures).We propose a maximum entropy model to re-solve the conflicts for ?h+h?
: 2P?
(a|c, st, st?1) =exp(?
?
h(a, c, st, st?1))?a exp(?
?
h(a, c, st, st?1))where a ?
{S,Rl, Rr} is an action, c is a booleanvalue that indicates whether all source words arecovered (shift is prohibited if true), st and st?1are the top two items on the stack, h(a, c, st, st?1)is a vector of binary features and ?
is a vector offeature weights.Figure 4 shows the feature templates used in ourexperiments.
Wh(?)
and Th(?)
are functions thatget the root word and tag of an item, Wlc(?)
andTlc(?)
returns the word and tag of the left mostchild of the root, Wrc(?)
and Trc(?)
returns the2The shift-shift conflicts always exist because there areusually multiple rules that can be shifted.
This can be re-volved using standard features in phrase-based models.word and tag of the right most child of the root.In this example, c = true, Wh(st) = in, Th(st) =IN, Wh(st?1) = visit, Wlc(st?1) = London.To train the model, we need an ?oracle?
or gold-standard action sequence for each training exam-ple.
Unfortunately, such oracle turns out to benon-unique even for monolingual shift-reduce de-pendency parsing (Huang et al, 2009).
The situ-ation for phrase-based shift-reduce parsing aggra-vates because there are usually multiple ways ofsegmenting sentence into phrases.To alleviate this problem, we introduce a struc-ture called derivation graph to compactly repre-sent all derivations of a training example.
Figure 3shows a (partial) derivation graph, in which a nodecorresponds to a state and an edge corresponds toan action.
The graph begins with an empty stateand ends with the given training example.More formally, a derivation graph is a directedacyclic graph G = ?V,E?
where V is a set ofnodes and E is a set of edges.
Each node v cor-responds to a state in the shift-reduce parsing pro-cess.
There are two distinguished nodes: v0, thestaring empty state, and v|V |, the ending com-pleted state.
Each edge e = (a, i, j) transits nodevi to node vj via an action a ?
{S,Rl, Rr}.To build the derivation graph, our algorithmstarts with an empty state and iteratively extendsan unprocessed state until reaches the completedstate.
During the process, states that violate thetraining example are discarded.
Even so, there arestill exponentially many states for a training exam-ple, especially for long sentences.
Fortunately, we5Algorithm 1 Beam-search shift-reduce parsing.1: procedure PARSE(f )2: V ?
?3: ADD(v0, V[0])4: k ?
05: while V[k] 6= ?
do6: for all v ?
V[k] do7: for all a ?
{S,Rl, Rr} do8: EXTEND(f , v, a, V)9: end for10: end for11: k ?
k + 112: end while13: end procedureonly need to focus on ?h+h?
states.
In addition,we follow Huang et al (2009) to use the heuristicof ?shortest stack?
to always prefer Rl to S.4 DecodingOur decoder is based on a linear model (Och,2003) with the following features:1. relative frequencies in two directions;2. lexical weights in two directions;3. phrase penalty;4. distance-based reordering model;5. lexicaized reordering model;6. n-gram language model model;7. word penalty;8. ill-formed structure penalty;9. dependency language model;10. maximum entropy parsing model.In practice, we extend deterministic shift-reduce parsing with beam search (Zhang andClark, 2008; Huang et al, 2009).
As shown in Al-gorithm 1, the algorithm maintains a list of stacksV and each stack groups states with the same num-ber of accumulated actions (line 2).
The stack listV initializes with an empty state v0 (line 3).
Then,the states in the stack are iteratively extended un-til there are no incomplete states (lines 4-12).
Thesearch space is constrained by discarding any statethat has a score worse than:1. ?
multiplied with the best score in the stack,or2.
the score of b-th best state in the stack.As the stack of a state keeps changing during thedecoding process, the context information neededto calculate dependency language model and max-imum entropy model probabilities (e.g., root word,leftmost child, etc.)
changes dynamically as well.As a result, the chance of risk-free hypothesis re-combination (Koehn et al, 2003) significantly de-creases because complicated contextual informa-tion is much less likely to be identical.Therefore, we use hypergraph reranking(Huang and Chiang, 2007; Huang, 2008), whichproves to be effective for integrating non-localfeatures into dynamic programming, to alleviatethis problem.
The decoding process is dividedinto two passes.
In the first pass, only standardfeatures (i.e., features 1-7 in the list in thebeginning of this section) are used to producea hypergraph.
3 In the second pass, we use thehypergraph reranking algorithm (Huang, 2008) tofind promising translations using additional de-pendency features (i.e., features 8-10 in the list).As hypergraph is capable of storing exponentiallymany derivations compactly, the negative effect ofpropagating mistakes made in the first pass to thesecond pass can be minimized.To improve rule coverage, we follow Shen etal.
(2008) to use ill-formed structures in decoding.If an ill-formed structure has a single root, it cantreated as a (pseudo) fixed structure; otherwise it istransformed to one (pseudo) left floating structureand one (pseudo) right floating structure.
We usea feature to count how many ill-formed structuresare used in decoding.5 ExperimentsWe evaluated our phrase-based string-to-dependency translation system on Chinese-English translation.
The training data consistsof 2.9M pairs of sentences with 76.0M Chinesewords and 82.2M English words.
We used theStanford parser (Klein and Manning, 2003) toget dependency trees for English sentences.
Weused the SRILM toolkit (Stolcke, 2002) to train a3Note that the first pass does not work like a phrase-baseddecoder because it yields dependency trees on the target side.A uniform model (i.e., each action has a fixed probability of1/3) is used to resolve ?h+h?
conflicts.6MT02 (tune) MT03 MT04 MT05system BLEU TER BLEU TER BLEU TER BLEU TERphrase 34.88 57.00 33.82 57.19 35.48 56.48 32.52 57.62dependency 35.23 56.12 34.20 56.36 36.01 55.55 33.06 56.94this work 35.71??
55.87??
34.81?
?+ 55.94?
?+ 36.37??
55.02?
?+ 33.53??
56.58?
?Table 2: Comparison with Moses (Koehn et al, 2007) and a re-implementation of the bottom-up string-to-dependency decoder (Shen et al, 2008) in terms of uncased BLEU and TER.
We use randomiza-tion test (Riezler and Maxwell, 2005) to calculate statistical significance.
*: significantly better thanMoses (p < 0.05), **: significantly better than Moses (p < 0.01), +: significantly better than string-to-dependency (p < 0.05), ++: significantly better than string-to-dependency (p < 0.01).features BLEU TERstandard 34.79 56.93+ depLM 35.29?
56.17?
?+ maxent 35.40??
56.09?
?+ depLM & maxent 35.71??
55.87?
?Table 3: Contribution of maximum entropy shift-reduce parsing model.
?standard?
denotes us-ing standard features of phrase-based system.Adding dependency language model (?depLM?
)and the maximum entropy shift-reduce parsingmodel (?maxent?)
significantly improves BLEUand TER on the development set, both separatelyand jointly.4-gram language model on the Xinhua portion ofthe GIGAWORD coprus, which contians 238MEnglish words.
A 3-gram dependency languagemodel was trained on the English dependencytrees.
We used the 2002 NIST MT Chinese-English dataset as the development set and the2003-2005 NIST datasets as the testsets.
Weevaluated translation quality using uncased BLEU(Papineni et al, 2002) and TER (Snover et al,2006).
The features were optimized with respectto BLEU using the minimum error rate trainingalgorithm (Och, 2003).We chose the following two systems that areclosest to our work as baselines:1.
The Moses phrase-based decoder (Koehn etal., 2007).2.
A re-implementation of bottom-up string-to-dependency decoder (Shen et al, 2008).All the three systems share with the same target-side parsed, word-aligned training data.
The his-togram pruning parameter b is set to 100 andrules coverage BLEU TERwell-formed 44.87 34.42 57.35all 100.00 35.71??
55.87?
?Table 4: Comparison of well-formed and ill-formed structures.
Using all rules significantlyoutperforms using only well-formed structures.BLEU and TER scores are calculated on the de-velopment set.phrase table limit is set to 20 for all the three sys-tems.
Moses shares the same feature set with oursystem except for the dependency features.
For thebottom-up string-to-dependency system, we in-cluded both well-formed and ill-formed structuresin chart parsing.
To control the grammar size, weonly extracted ?tight?
initial phrase pairs (i.e., theboundary words of a phrase must be aligned) assuggested by (Chiang, 2007).
For our system, weused the Le Zhang?s maximum entropy modelingtoolkit to train the shift-reduce parsing model afterextracting 32.6M events from the training data.
4We set the iteration limit to 100.
The accuracy onthe training data is 90.18%.Table 2 gives the performance of Moses, thebottom-up string-to-dependency system, and oursystem in terms of uncased BLEU and TERscores.
From the same training data, Mosesextracted 103M bilingual phrases, the bottom-up string-to-dependency system extracted 587Mstring-to-dependency rules, and our system ex-tracted 124M phrase-based dependency rules.
Wefind that our approach outperforms both baselinessystematically on all testsets.
We use randomiza-tion test (Riezler and Maxwell, 2005) to calculatestatistical significance.
As our system can take fulladvantage of lexicalized reordering and depen-4http://homepages.inf.ed.ac.uk/lzhang10/maxent.html730.5031.0031.5032.0032.5033.0033.5034.0034.500  2  4  6  8  10  12BLEUdistortion limitthis workMosesFigure 5: Performance of Moses and our systemwith various distortion limits.dency language models without loss in rule cov-erage, it achieves significantly better results thanMoses on all test sets.
The gains in TER are muchlarger than BLEU because dependency languagemodels do not model n-grams directly.
Comparedwith the bottom-up string-to-dependency system,our system outperforms consistently but not sig-nificantly in all cases.
The average decoding timefor Moses is 3.67 seconds per sentence, bottom-up string-to-dependency is 13.89 seconds, and oursystem is 4.56 seconds.Table 3 shows the effect of hypergraph rerank-ing.
In the first pass, our decoder uses standardphrase-based features to build a hypergraph.
TheBLEU score is slightly lower than Moses with thesame configuration.
One possible reason is thatour decoder organizes stacks with respect to ac-tions, whereas Moses groups partial translationswith the same number of covered source words instacks.
In the second pass, our decoder reranksthe hypergraph with additional dependency fea-tures.
We find that adding dependency languageand maximum entropy shift-reduce models consis-tently brings significant improvements, both sepa-rately and jointly.We analyzed translation rules extracted from thetraining data.
Among them, well-formed struc-tures account for 43.58% (fixed 33.21%, float-ing left 9.01%, and floating right 1.36%) and ill-formed structures 56.42%.
As shown in Table4, using all rules clearly outperforms using onlywell-formed structures.Figure 5 shows the performance of Moses andour system with various distortion limits on thedevelopment set.
Our system consistently outper-forms Moses in all cases, suggesting that addingdependency helps improve phrase reordering.6 Related WorkThe work of Galley and Manning (2009) is clos-est in spirit to ours.
They introduce maximumspanning tree (MST) parsing (McDonald et al,2005) into phrase-based translation.
The systemis phrase-based except that an MST parser runs toparse partial translations at the same time.
Onechallenge is that MST parsing itself is not incre-mental, making it expensive to identify loops dur-ing hypothesis expansion.
On the contrary, shift-reduce parsing is naturally incremental and canbe seamlessly integrated into left-to-right phrase-based decoding.
More importantly, in our workdependency trees are memorized for phrases ratherthan being generated word by word on the fly indecoding.
This treatment might not only reducedecoding complexity but also potentially revolvelocal parsing ambiguity.Our decoding algorithm is similar to Gimpeland Smith (2011)?s lattice parsing algorithm as wedivide decoding into two steps: hypergraph gener-ation and hypergraph rescoring.
The major differ-ence is that our hypergraph is not a phrasal lat-tice because each phrase pair is associated witha dependency structure on the target side.
Inother words, our second pass is to find the Viterbiderivation with addition features rather than pars-ing the phrasal lattice.
In addition, their algorithmproduces phrasal dependency parse trees while theleaves of our dependency trees are words, makingdependency language models can be directly used.Shift-reduce parsing has been successfully usedin phrase-based decoding but limited to addingstructural constraints.
Galley and Manning (2008)propose a shift-reduce algorithm to integrate a hi-erarchical reordering model into phrase-based sys-tems.
Feng et al (2010) use shift-reduce parsingto impose ITG (Wu, 1997) constraints on phrasepermutation.
Our work differs from theirs by go-ing further to incorporate linguistic syntax intophrase-based decoding.Along another line, a number of authors havedeveloped incremental algorithms for syntax-based models (Watanabe et al, 2006; Huang andMi, 2010; Dyer and Resnik, 2010; Feng et al,2012).
Watanabe et al (2006) introduce an Early-style top-down parser based on binary-branchingGreibach Normal Form.
Huang et al (2010), Dyer8and Resnik (2010), and Feng et al (2012) use dot-ted rules to change the tree transversal to gener-ate target words left-to-right, either top-down orbottom-up.7 ConclusionWe have presented a shift-reduce parsing al-gorithm for phrase-based string-to-dependencytranslation.
The algorithm generates depen-dency structures incrementally using string-to-dependency phrase pairs.
Therefore, our ap-proach is capable of combining the advantages ofboth phrase-based and string-to-dependency mod-els, it outperforms the two baselines on Chinese-to-English translation.In the future, we plan to include more con-textual information (e.g., the uncovered sourcephrases) in the maximum entropy model to re-solve conflicts.
Another direction is to adaptthe dynamic programming algorithm proposed byHuang and Sagae (2010) to improve our string-to-dependency decoder.
It is also interesting to com-pare with applying word-based shift-reduce pars-ing to phrase-based decoding similar to (Galleyand Manning, 2009).AcknowledgmentsThis research is supported by the 863 Programunder the grant No 2012AA011102 and No.2011AA01A207, by the Singapore National Re-search Foundation under its International Re-search Centre @ Singapore Funding Initiative andadministered by the IDM Programme Office, andby a Research Fund No.
20123000007 from Ts-inghua MOE-Microsoft Joint Laboratory.ReferencesDavid Chiang.
2005.
A hiearchical phrase-basedmodel for statistical machine translation.
In Proc.of ACL 2005.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Chris Dyer and Philip Resnik.
2010.
Context-free re-ordering, finite-state translation.
In Proc.
of NAACL2010.Yang Feng, Haitao Mi, Yang Liu, and Qun Liu.2010.
An efficient shift-reduce decoding algorithmfor phrased-based machine translation.
In Proc.
ofCOLING 2010.Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn.2012.
Left-to-right tree-to-string decoding with pre-diction.
In Proc.
of EMNLP 2012.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proc.
of EMNLP 2008.Michel Galley and Christopher D. Manning.
2009.Quadratic-time dependency parsing for machinetranslation.
In Proc.
of ACL 2009.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of ACL 2006.Kevin Gimpel and Noah A. Smith.
2011.
Quasi-synchronous phrase dependency grammars for ma-chine translation.
In Proc.
of EMNLP 2011.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proc.
of ACL 2007.Liang Huang and Haitao Mi.
2010.
Efficient incre-mental decoding for tree-to-string translation.
InProc.
of EMNLP 2010.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProc.
of ACL 2010.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
of AMTA 2006.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proc.
of EMNLP 2009.Liang Huang.
2008.
Forest reranking: Discrimina-tive parsing with non-local features.
In Proc.
of ACL2008.Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proc.
of ACL 2003.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
ComputationalLinguistics.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
ofNAACL 2003.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
of ACL 2007.9Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of ACL 2006.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
Spmt: Statistical machinetranslation with syntactified target language phrases.In Proc.
of EMNLP 2006.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Proc.
of EMNLP 2005.HaitaoMi and Liang Huang.
2008.
Forest-based trans-lation.
In Proc.
of ACL 2008.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In Proc.
of ACL 2004 Work-shop Incremental Parsing: Bringning Engineeringand Cognition Together.Franz Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4).Franz Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
of ACL 2003.Kishore Papineni, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proc.
of ACL 2002.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal smt.
In Proc.
of ACL 2005.S.
Riezler and J. Maxwell.
2005.
On some pitfallsin automatic evaluation and significance testing formt.
In Proc.
of ACL 2005 Workshop on Intrinsic andExtrinsic Evaluation Measures for Machine Trans-lation and/or Summarization.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProc.
of ACL 2008.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2010.String-to-dependency statistical machine transla-tion.
Computational Linguistics, 36(4).Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
of AMTA 2006.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proc.
of ICSLP 2002.Taro Watanabe, Hajime Tsukuda, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proc.
of ACL 2006.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proc.
of ACL2001.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam search.
In Proc.
of EMNLP 2008.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A treesequence alignment-based tree-to-tree translationmodel.
In Proc.
of ACL 2008.10
