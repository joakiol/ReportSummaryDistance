2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752?761,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Concept-to-text Generation with HypergraphsIoannis Konstas and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABi.konstas@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractConcept-to-text generation refers to the task ofautomatically producing textual output fromnon-linguistic input.
We present a joint modelthat captures content selection (?what to say?
)and surface realization (?how to say?)
inan unsupervised domain-independent fashion.Rather than breaking up the generation pro-cess into a sequence of local decisions, we de-fine a probabilistic context-free grammar thatglobally describes the inherent structure of theinput (a corpus of database records and textdescribing some of them).
We represent ourgrammar compactly as a weighted hypergraphand recast generation as the task of finding thebest derivation tree for a given input.
Experi-mental evaluation on several domains achievescompetitive results with state-of-the-art sys-tems that use domain specific constraints, ex-plicit feature engineering or labeled data.1 IntroductionConcept-to-text generation broadly refers to the taskof automatically producing textual output from non-linguistic input (Reiter and Dale, 2000).
Depend-ing on the application and the domain at hand, theinput may assume various representations includ-ing databases of records, expert system knowledgebases, simulations of physical systems and so on.Figure 1 shows input examples and their correspond-ing text for three domains, air travel, sportscastingand weather forecast generation.A typical concept-to-text generation system im-plements a pipeline architecture consisting of threecore stages, namely text planning (determining thecontent and structure of the target text), sentenceplanning (determining the structure and lexical con-tent of individual sentences), and surface realiza-tion (rendering the specification chosen by the sen-tence planner into a surface string).
Traditionally,these components are hand-engineered in order togenerate high quality text, however at the expenseof portability and scalability.
It is thus no surprisethat recent years have witnessed a growing interestin automatic methods for creating trainable genera-tion components.
Examples include learning whichdatabase records should be present in a text (Duboueand McKeown, 2002; Barzilay and Lapata, 2005)and how these should be verbalized (Liang et al,2009).
Besides concentrating on isolated compo-nents, a few approaches have emerged that tackleconcept-to-text generation end-to-end.
Due to thecomplexity of the task, most models simplify thegeneration process, e.g., by creating output that con-sists of a few sentences, thus obviating the need fordocument planning, or by treating sentence planningand surface realization as one component.
A com-mon modeling strategy is to break up the genera-tion process into a sequence of local decisions, eachlearned separately (Reiter et al, 2005; Belz, 2008;Chen and Mooney, 2008; Angeli et al, 2010; Kimand Mooney, 2010).In this paper we describe an end-to-end gen-eration model that performs content selection andsurface realization jointly.
Given a corpus ofdatabase records and textual descriptions (for someof them), we define a probabilistic context-freegrammar (PCFG) that captures the structure of thedatabase and how it can be rendered into natural752FlightFrom Tophoenix new yorkSearchType Whatquery flightDayDay Dep/Arsunday departureList flights from phoenix to new york on sundayTemperatureTime Min Mean Max06:00-21:00 9 15 21Wind SpeedTime Min Mean Max06:00-21:00 15 20 30Cloud Sky CoverTime Percent (%)06:00-09:00 25-5009:00-12:00 50-75Wind DirectionTime Mode06:00-21:00 SCloudy, with a low around 10.
South wind around 20 mph.PassFrom Topink3 pink7Bad PassFrom Topink7 purple3Turn OverFrom Topink7 purple3pink3 passes the ball to pink7(b)(a)(c)Figure 1: Input-output examples for (a) query generation in the air travel domain, (b) weather forecast generation, and(c) sportscasting.language.
This grammar represents a set of treeswhich we encode compactly using a weighted hy-pergraph (or packed forest), a data structure that de-fines a probability (or weight) for each tree.
Gen-eration then boils down to finding the best deriva-tion tree in the hypergraph which can be done effi-ciently using the Viterbi algorithm.
In order to en-sure that our generation output is fluent, we intersectour grammar with a language model and performdecoding using a dynamic programming algorithm(Huang and Chiang, 2007).Our model is conceptually simpler than previousapproaches and encodes information about the do-main and its structure globally, by considering theinput space simultaneously during generation.
Ouronly assumption is that the input must be a set ofrecords essentially corresponding to database-liketables whose columns describe fields of a certaintype.
Experimental evaluation on three domains ob-tains results competitive to the state of the art with-out using any domain specific constraints, explicitfeature engineering or labeled data.2 Related WorkOur work is situated within the broader class ofdata-driven approaches to content selection and sur-face realization.
Barzilay and Lapata (2005) focuson the former problem which they view as an in-stance of collective classification (Barzilay and La-pata, 2005).
Given a corpus of database recordsand texts describing some of them, they learn a con-tent selection model that simultaneously optimizeslocal label assignments and their pairwise relations.Building on this work, Liang et al (2009) present ahierarchical hidden semi-Markov generative modelthat first determines which facts to discuss and thengenerates words from the predicates and argumentsof the chosen facts.A few approaches have emerged more recentlythat combine content selection and surface realiza-tion.
Kim and Mooney (2010) adopt a two-stage ap-proach: using a generative model similar to Liang etal.
(2009), they first decide what to say and then ver-balize the selected input with WASP?1, an existinggeneration system (Wong and Mooney, 2007).
Incontrast, Angeli et al (2010) propose a unified con-tent selection and surface realization model whichalso operates over the alignment output producedby Liang et al (2009).
Their model decomposesinto a sequence of discriminative local decisions.They first determine which records in the databaseto talk about, then which fields of those recordsto mention, and finally which words to use to de-scribe the chosen fields.
Each of these decisionsis implemented as a log-linear model with featureslearned from training data.
Their surface realiza-tion component is based on templates that are au-tomatically extracted and smoothed with domain-specific constraints in order to guarantee fluent out-put.
Other related work (Wong and Mooney, 2007;Lu and Ng, 2011).
has focused on generating naturallanguage sentences from logical form (i.e., lambda-expressions) using mostly synchronous context-freegrammars (SCFGs).753Similar to Angeli et al (2010), we also presentan end-to-end system that performs content selec-tion and surface realization.
However, rather thanbreaking up the generation task into a sequence oflocal decisions, we optimize what to say and howto say simultaneously.
We do not learn mappingsfrom a logical form, but rather focus on input whichis less constrained, possibly more noisy and with alooser structure.
Our key insight is to convert theset of database records serving as input to our gen-erator into a PCFG that is neither hand crafted nordomain specific but simply describes the structureof the database.
The approach is conceptually sim-ple, does not rely on discriminative training or anyfeature engineering.
We represent the grammar andits derivations compactly as a weighted hypergraphwhich we intersect with a language model in orderto generate fluent output.
This allows us to easilyport surface generation to different domains withouthaving to extract new templates or enforce domainspecific constraints.3 Problem FormulationWe assume our generator takes as input a set ofdatabase records d and produces text w that verbal-izes some of these records.
Each record r ?
d has atype r.t and a set of fields f associated with it.
Fieldshave different values f .v and types f .t (i.e., in-teger or categorical).
For example, in Figure 1b,wind speed is a record type with four fields: time,min, mean, and max.
The values of these fields are06:00-21:00, 15, 20, and 30, respectively; the typeof time is categorical, whereas all other fields areintegers.During training, our algorithm is given a cor-pus consisting of several scenarios, i.e., databaserecords paired with texts like those shown in Fig-ure 1.
In the weather forecast domain, a scenario cor-responds to weather-related measurements of tem-perature, wind, speed, and so on collected for a spe-cific day and time (e.g., day or night).
In sportscast-ing, scenarios describe individual events in the soc-cer game (e.g., passing or kicking the ball).
In the airtravel domain, scenarios comprise of flight-relateddetails (e.g., origin, destination, day, time).
Our goalthen is to reduce the tasks of content selection andsurface realization into a common probabilistic pars-ing problem.
We do this by abstracting the struc-ture of the database (and accompanying texts) intoa PCFG whose probabilities are learned from train-ing data.1 Specifically, we convert the database intorewrite rules and represent them as a weighted di-rected hypergraph (Gallo et al, 1993).
Instead oflearning the probabilities on the PCFG, we directlycompute the weights on the hyperarcs using a dy-namic program similar to the inside-outside algo-rithm (Li and Eisner, 2009).
During testing, we aregiven a set of database records without the corre-sponding text.
Using the trained grammar we com-pile a hypergraph specific to this test input and de-code it approximately via cube pruning (Chiang,2007).The choice of the hypergraph framework is moti-vated by at least three reasons.
Firstly, hypergraphscan be used to represent the search space of mostparsers (Klein and Manning, 2001).
Secondly, theyare more efficient and faster than the common CYKparser-based representation for PCFGs by a factorof more than ten (Huang and Chiang, 2007).
Andthirdly, the hypergraph representation allows us tointegrate an n-gram language model and perform de-coding efficiently using k-best Viterbi search, opti-mizing what to say and how to say at the same time.3.1 Grammar DefinitionOur model captures the inherent structure of thedatabase with a number of CFG rewrite rules, ina similar way to how Liang et al (2009) defineMarkov chains in the different levels of their hierar-chical model.
These rules are purely syntactic (de-scribing the intuitive relationship between records,records and fields, fields and corresponding words),and could apply to any database with similar struc-ture irrespectively of the semantics of the domain.Our grammar is defined in Table 1 (rules (1)?
(9)).Rule weights are governed by an underlying multi-nomial distribution and are shown in square brack-ets.
Non-terminal symbols are in capitals and de-1An alternative would be to learn a SCFG between thedatabase input and the accompanying text.
However, this wouldinvolve considerable overhead in terms of alignment (as thedatabase and the text do not together constitute a clean parallelcorpus, but rather a noisy comparable corpus), as well as gram-mar training and decoding using state-of-the art SMT methods,which we manage to avoid with our simpler approach.7541.
S?
R(start) [Pr = 1]2.
R(ri.t)?
FS(r j,start) R(r j.t) [P(r j.t |ri.t) ??]3.
R(ri.t)?
FS(r j,start) [P(r j.t |ri.t) ??]4.
FS(r,r.
fi)?
F(r,r.
f j) FS(r,r.
f j) [P( f j | fi)]5.
FS(r,r.
fi)?
F(r,r.
f j) [P( f j | fi)]6.
F(r,r.
f )?W(r,r.
f ) F(r,r.
f ) [P(w |w?1,r,r.
f )]7.
F(r,r.
f )?W(r,r.
f ) [P(w |w?1,r,r.
f )]8.
W(r,r.
f )?
?
[P(?
|r,r.
f , f .t, f .v)]9.
W(r,r.
f )?
g( f .v)[P(g( f .v).mode |r,r.
f , f .t = int)]Table 1: Grammar rules and their weights shown insquare brackets.note intermediate states; the terminal symbol ?corresponds to all words seen in the training set,and g( f .v) is a function for generating integer num-bers given the value of a field f .
All non-terminals,save the start symbol S, have one or more features(shown in parentheses) that act as constraints, sim-ilar to number and gender agreement constraints inaugmented syntactic rules.Rule (1) denotes the expansion from the startsymbol S to record R, which has the special ?start?record type (hence the notation R(start)).
Rule (2)defines a chain between two consecutive records,i.e., going from a source record ri to a target r j.Here, FS(r j,r j. f ) represents the set of fields of thetarget r j, following the source record R(ri).For example, the rule R(skyCover1.t) ?FS(temperature1,start)R(temperature1.t) canbe interpreted as follows.
Given that we havetalked about skyCover1, we will next talk abouttemperature1 and thus emit its corresponding fields.R(temperature1.t) is a non-terminal place-holderfor the continuation of the chain of records, andstart in FS is a special boundary field betweenconsecutive records.
The weight of this rule is thebigram probability of two records conditioned ontheir record type, multiplied with a normalizationfactor ?.
We have also defined a null record typei.e., a record that has no fields and acts as asmoother for words that may not correspond to aparticular record.
Rule (3) is simply an escape rule,so that the parsing process (on the record level) canfinish.Rule (4) is the equivalent of rule (2) at thefield level, i.e., it describes the chaining oftwo consecutive fields fi and f j. Non-terminalF(r,r.
f ) refers to field f of record r. Forexample, the rule FS(windSpeed1,min) ?F(windSpeed1,max)FS(windSpeed1,max), spec-ifies that we should talk about the field max ofrecord windSpeed1, after talking about the fieldmin.
Analogously to the record level, we have alsoincluded a special null field type for the emissionof words that do not correspond to a specific recordfield.
Rule (6) defines the expansion of field F toa sequence of (binarized) words W, with a weightequal to the bigram probability of the current wordgiven the previous word, the current record, andfield.
This is an attempt at capturing contextualdependencies between words over and above tointegrating a language model during decoding (seeSection 3.3).Rules (8) and (9) define the emission of words andinteger numbers from W, given a field type and itsvalue.
Rule (8) emits a single word from the vocabu-lary of the training set.
Its weight defines a multino-mial distribution over all seen words, for every valueof field f , given that the field type is categorical orthe special null field.
Rule (9) is identical but forfields whose type is integer.
Function g( f .v) gener-ates an integer number given the field value, usingeither of the following six ways (Liang et al, 2009):identical to the field value, rounding up or roundingdown to a multiple of 5, rounding off to the clos-est multiple of 5 and finally adding or subtractingsome unexplained noise.2 The weight is a multino-mial over the six generation function modes, giventhe record field f .3.2 Hypergraph ConstructionSo far we have defined a probabilistic grammarthat captures the structure of a database d withrecords and fields as intermediate non-terminals, andwords w (from the associated text) as terminals.
Us-ing this grammar and the CYK parsing algorithm,we could obtain the top scoring derivation of recordsand fields for a given input (i.e., a sequence of2The noise is modeled as a geometric distribution.755S0,7R0,2(start)R0,1(start)?
?
?FS0,1(skyCover1,start)R1,1(skyCover1.t)R1,1(temp1.t)FS0,1(temp1,start)?
?
?F0,1(skyCover1,%)FS1,1(skyCover1,%)F0,1(skyCover1,time)FS1,1(skyCover1,time)W0,1(skyCover1,%)W0,1(skyCover1,time)FS1,2(temp1,start)R2,2(temp1.t)FS1,2(skyCover1,start)R2,2(skyCover1.t)?
?
?sunnyF1,2(temp1,min)FS2,2(temp1,min)W0,1(temp1,min) g0,1(min,v=10)F1,2(temp1,max)FS2,2(temp1,max)W0,1(temp1,max) g0,1(max,v=20)withFigure 2: Partial hypergraph representation for the sentence ?Sunny with a low around 30 .?
For the sake of readability,we show a partial span on the first two words without weights on the hyperarcs.words) as well as the optimal segmentation of thetext, provided we have a trained set of weights.
Theinside-outside algorithm is commonly used for esti-mating the weights of a PCFG.
However, we firsttransform the CYK parser and our grammar intoa hypergraph and then compute the weights usinginside-outside.
Huang and Chiang (2005) define aweighted directed hypergraph as follows:Definition 1 An ordered hypergraph H is a tuple?N,E, t,R?, where N is a finite set of nodes, Eis a finite set of hyperarcs and R is the set ofweights.
Each hyperarc e ?
E is a triple e =?T (e),h(e), f (e)?, where h(e) ?
N is its head node,T (e) ?
N?
is a set of tail nodes and f (e) is a mono-tonic weight function R|T (e)| to R and t ?
N is a tar-get node.Definition 2 We impose the arity of a hyperarc to be|e| = |T (e)| = 2, in other words, each head node isconnected with at most two tail nodes.Given a context-free grammar G = ?N,T,P,S?
(where N is the set of variables, T the set of ter-minals, P the set of production rules, and S ?
N thestart symbol) and an input string w, we can map thestandard weighted CYK algorithm to a hypergraphas follows.
Each node [A, i, j] in the hypergraphcorresponds to non-terminal A spanning words wito w j of the input.
Each rewrite rule A?
BC in P,with three free indices i < j < k, is mapped tothe hyperarc ?
((B, i, j),(C, j,k)) ,(A, i,k), f ?, wheref = f ((B, i, j)) f ((C, j,k)) ?Pr(A?
BC).3 The hy-3Similarly, rewrite rules of type A?
B are mapped to thehyperarc ?
(B, i, j),(A, i, j), f ?, with f = f ((B, i, j)) ?Pr(A?
B).pergraph can be thus viewed as a compiled latticeof the corresponding chart graph.
Figure 2 showsan example hypergraph for a grammar defined ondatabase input similar to Figure (1b).In order to learn the weights on the hyperarcs weperform the following procedure iteratively in anEM fashion (Li and Eisner, 2009).
For each train-ing scenario we build its hypergraph representation.Next, we perform inference by calculating the in-side and outside scores of the hypergraph, so as tocompute the posterior distribution over its hyperarcs(E-step).
Finally, we collectively update the posteri-ors on the parameters-weights, i.e., rule probabilitiesand emission multinomial distributions (M-step).3.3 DecodingIn the framework outlined above, parsing an inputstring w (given some learned weights) boils downto traversing the hypergraph in a particular order.
(Note that the hypergraph should be acyclic, whichis always guaranteed by the grammar in Table 1).
Ingeneration, our aim is to verbalize an input scenariofrom a database d (see Figure 1).
We thus find thebest text by maximizing:argmaxwP(w |d) = argmaxwP(w) ?P(d |w) (1)where P(d |w) is the decoding likelihood for a se-quence of words w, P(w) is a measure of the qual-ity of each output (given by a language model),and P(w |d) the posterior of the best output fordatabase d. Note that calculating P(d |w) requiresdeciding on the output length |w|.
Rather than set-756ting w to a fixed length, we rely on a linear regres-sion predictor that uses the counts of each recordtype per scenario as features and is able to producevariable length texts.In order to perform decoding with an n-gram lan-guage model, we adopt Huang and Chiang?s (2007)dynamic-programming algorithm for SCFG-basedsystems.
Each node in the hypergraph is split intoa set of compound items, namely +LM items.
Each+LM item is of the form (na?b), where a and b areboundary words of the generation string, and ?
is aplace-holder symbol for an elided part of that string,indicating a sub-generation part ranging from a to b.An example +LM deduction of a single hyperarc ofthe hypergraph in Figure 2 using bigrams is:(2)FS1,2(temp1,start)low : (w1,g1),R2,2(temp1.t)around?degrees : (w2,g2)R1,1(skyCover1.t)low?degrees : (w,g1g2)w = w1 +w2 + ew +Plm(around | low) (3)where w1,w2 are node weights, g1,g2 are the corre-sponding sub-generations, ew is the weight of the hy-perarc and w the weight of the resulting +LM item.Plm and (na?b) are defined as in Chiang (2007) in ageneric fashion, allowing extension to an arbitrarysize of n-gram grammars.Naive traversal of the hypergraph bottom-upwould explore all possible +LM deductions alongeach hyperarc, and would increase decoding com-plexity to an infeasible O(2nn2), assuming a trigrammodel and a constant number of emissions at the ter-minal nodes.
To ensure tractability, we adopt cubepruning, a popular approach in syntax-inspired ma-chine translation (Chiang, 2007).
The idea is to use abeam-search over the intersection grammar coupledwith the cube-pruning heuristic.
The beam limits thenumber of derivations for each node, whereas cube-pruning further limits the number of +LM items con-sidered for inclusion in the beam.
Since f (e) in Def-inition 1 is monotonic, we can select the k-best itemswithout computing all possible +LM items.Our decoder follows Huang and Chiang (2007)but importantly differs in the treatment of leaf nodesin the hypergraph (see rules (8) and (9)).
In theSCFG context, the Viterbi algorithm consumes ter-minals from the source string in a bottom-up fashionand creates sub-translations according to the CFGrule that holds each time.
In the concept-to-textgeneration context, however, we do not observe thewords; instead, for each leaf node we emit the k-bestwords from the underlying multinomial distribution(see weights on rules (8) and (9)) and continue build-ing our sub-generations bottom-up.4 Experimental DesignData We used our system to generate soccer com-mentaries, weather forecasts, and spontaneous utter-ances relevant to the air travel domain (examplesare given in Figure 1).
For the first domain weused the dataset of Chen and Mooney (2008), whichconsists of 1,539 scenarios from the 2001?2004Robocup game finals.
Each scenario contains on av-erage |d|= 2.4 records, each paired with a short sen-tence (5.7 words).
This domain has a small vocabu-lary (214 words) and simple syntax (e.g., a transitiveverb with its subject and object).
Records in thisdataset (henceforth ROBOCUP) were aligned man-ually to their corresponding sentences (Chen andMooney, 2008).
Given the relatively small size ofthis dataset, we performed cross-validation follow-ing previous work (Chen and Mooney, 2008; An-geli et al, 2010).
We trained our system on threeROBOCUP games and tested on the fourth, averagingover the four train/test splits.For weather forecast generation, we used thedataset of Liang et al (2009), which consists of29,528 weather scenarios for 3,753 major US cities(collected over four days).
The vocabulary in thisdomain (henceforth WEATHERGOV) is comparableto ROBOCUP (345 words), however, the texts arelonger (|w| = 29.3) and more varied.
On average,each forecast has 4 sentences and the content selec-tion problem is more challenging; only 5.8 out ofthe 36 records per scenario are mentioned in the textwhich roughly corresponds to 1.4 records per sen-tence.
We used 25,000 scenarios from WEATHER-GOV for training, 1,000 scenarios for developmentand 3,528 scenarios for testing.
This is the same par-tition used in Angeli et al (2010).For the air travel domain we used the ATIS dataset(Dahl et al, 1994), consisting of 5,426 scenar-ios.
These are transcriptions of spontaneous utter-ances of users interacting with a hypothetical on-757WEATHERGOV ATIS ROBOCUP1-BEST Near 57.
Near 57.
Near 57.
Near 57.
Near57.
Near 57.
Near 57.
Near 57.
Near 57.Near 57.
Near 57.
South wind.What what what what flightsfrom Denver Phoenix Pink9 to to Pink7 kicksk-BESTAs high as 23 mph.
Chance of precipitationis 20.
Breezy, with a chance of showers.Mostly cloudy, with a high near 57.
Southwind between 3 and 9 mph.Show me the flights fromDenver to PhoenixPink9 passes back to Pink7ANGELIA chance of rain or drizzle, with a high near57.
South wind between 3 and 9 mph.Show me the flights leavefrom Nashville to PhoenixPink9 kicks to Pink7HUMANA slight chance of showers.
Mostly cloudy,with a high near 58.
South wind between 3and 9 mph, with gusts as high as 23 mph.Chance of precipitation is 20%.List flights from Denver toPhoenixPink9 passes back to Pink7Table 2: System output on WEATHERGOV, ATIS, and ROBOCUP (1-BEST, k-BEST, ANGELI) and correspondinghuman-authored text (HUMAN).line flight booking system.
We used the datasetintroduced in Zettlemoyer and Collins (2007)4 andautomatically converted their lambda-calculus ex-pressions to attribute-value pairs following the con-ventions adopted by Liang et al (2009).
For ex-ample, the scenario in Figure 1(a) was initiallyrepresented as: ?x.
f light(x) ?
f rom(x, phoenix) ?to(x,new york)?day(x,sunday).5 In contrast to thetwo previous datasets, ATIS has a much richer vo-cabulary (927 words); each scenario correspondsto a single sentence (average length is 11.2 words)with 2.65 out of 19 record types mentioned on av-erage.
Following Zettlemoyer and Collins (2007),we trained on 4,962 scenarios and tested on ATISNOV93 which contains 448 examples.Model Parameters Our model has two parame-ters, namely the number of k grammar derivationsconsidered by the decoder and the order of thelanguage model.
We tuned k experimentally onheld-out data taken from WEATHERGOV, ROBOCUP,and ATIS, respectively.
The optimal value was k=15for WEATHERGOV, k=25 for ROBOCUP, and k = 404The original corpus contains user utterances of single dia-logue turns which would result in trivial scenarios.
Zettlemoyerand Collins (2007) concatenate all user utterances referring tothe same dialogue act, (e.g., book a flight), thus yielding morecomplex scenarios with longer sentences.5The resulting dataset and a technical report describ-ing the mapping procedure in detail are available fromhttp://homepages.inf.ed.ac.uk/s0793019/index.php?page=resourcesfor ATIS.
For the ROBOCUP domain, we used a bi-gram language model which was considered suffi-cient given that the average text length is small.
ForWEATHERGOV and ATIS, we used a trigram languagemodel.System Comparison We evaluated two configu-rations of our system.
A baseline that uses the topscoring derivation in each subgeneration (1-BEST)and another version which makes better use of ourdecoding algorithm and considers the best k deriva-tions (i.e., 15 for WEATHERGOV, 40 for ATIS, and25 for ROBOCUP).
We compared our output to An-geli et al (2010) whose approach is closest to oursand state-of-the-art on the WEATHERGOV domain.For ROBOCUP, we also compare against the best-published results (Kim and Mooney, 2010).Evaluation We evaluated system output automat-ically, using the BLEU modified precision score(Papineni et al, 2002) with the human-written textas reference.
In addition, we evaluated the gener-ated text by eliciting human judgments.
Participantswere presented with a scenario and its correspond-ing verbalization and were asked to rate the latteralong two dimensions: fluency (is the text grammat-ical and overall understandable?)
and semantic cor-rectness (does the meaning conveyed by the text cor-respond to the database input?).
The subjects used afive point rating scale where a high number indicatesbetter performance.
We randomly selected 12 doc-758ROBOCUP WEATHERGOV ATISSystem BLEU BLEU BLEU1-BEST 10.79 8.64 11.85k-BEST 30.90 33.70 29.30ANGELI 28.70 38.40 26.77KIM-MOONEY 47.27 ?
?Table 3: BLEU scores on ROBOCUP (fixed content se-lection), WEATHERGOV, and ATIS.uments from the test set (for each domain) and gen-erated output with our models (1-BEST and k-BEST)and Angeli et al?s (2010) model (see Figure 2 forexamples of system output).
We also included theoriginal text (HUMAN) as gold standard.
We thusobtained ratings for 48 (12 ?
4) scenario-text pairsfor each domain.
The study was conducted over theInternet using WebExp (Keller et al, 2009) and wascompleted by 114 volunteers, all self reported nativeEnglish speakers.5 ResultsWe conducted two experiments on the ROBOCUP do-main.
We first assessed the performance of our gen-erator (k-BEST) on joint content selection and sur-face realization and obtained a BLEU score of 24.88.In comparison, the baseline?s (1-BEST) BLEU scorewas 8.01.
In a second experiment we forced thegenerator to use the gold-standard records from thedatabase.
This was necessary in order to comparewith previous work (Angeli et al, 2010; Kim andMooney, 2010).6 Our results are summarized in Ta-ble 3.
Overall, our generator performs better thanthe baseline and Angeli et al (2010).
We observea substantial increase in performance compared tothe joint content selection and surface realizationsetting.
This is expected as the generator is facedwith an easier task and there is less scope for error.Our model does not outperform Kim and Mooney(2010), however, this is not entirely surprising astheir model requires considerable more supervision(e.g., during parameter initialization) and includes apost-hoc re-ordering component.6Angeli et al (2010) and Kim and Mooney (2010) fix con-tent selection both at the record and field level.
We let our gen-erator select the appropriate fields, since these are at most twoper record type and this level of complexity can be easily tack-led during decoding.ROBOCUP WEATHERGOV ATISSystem F SC F SC F SC1-BEST 2.47??
2.33??
1.82??
2.05??
2.40??
2.46?
?k-BEST 4.31?
3.96?
3.92?
3.30?
4.01 3.87ANGELI 4.03??
3.70??
4.26?
3.60?
3.56??
3.33?
?HUMAN 4.47?
4.37?
4.61?
4.03?
4.10 4.01Table 4: Mean ratings for fluency (F) and semantic cor-rectness (SC) on system output elicited by humans onROBOCUP, WEATHERGOV, and ATIS (?
: sig.
diff.
fromHUMAN; ?
: sig.
diff.
from k-BEST.
)With regard to WEATHERGOV, our generator im-proves over the baseline but lags behind Angeli etal.
(2010).
Since our system emits words based ona language model rather than a template, it displaysmore freedom in word order and lexical choice, andis thus penalized by BLEU when creating output thatis overly distinct from the reference.
On ATIS, ourmodel outperforms both the baseline and Angeli etal.
This is the most challenging domain with re-gard to surface realization with a vocabulary largerthan ROBOCUP and WEATHERGOV by factors of 2.7and 4.3, respectively.The results of our human evaluation study areshown in Table 3.
We carried out an Analysis ofVariance (ANOVA) to examine the effect of systemtype (1-BEST, k-BEST, ANGELI, and HUMAN) on thefluency and semantic correctness ratings.
Meansdifferences were compared using a post-hoc Tukeytest.
On ROBOCUP, our system (k-BEST) is signif-icantly better than the baseline (1-BEST) and AN-GELI both in terms of fluency and semantic correct-ness (a < 0.05).
On WEATHERGOV, our generatorperforms comparably to ANGELI on fluency and se-mantic correctness (the differences in the means arenot statistically significant); 1-BEST is significantlyworse than 15-BEST and ANGELI (a < 0.05).
OnATIS, k-BEST is significantly more fluent and seman-tically correct than 1-BEST and ANGELI (a < 0.01).There was no statistically significant difference be-tween the output of our system and the original ATISsentences.In sum, we observe that taking the k-best deriva-tions into account boosts performance (the 1-BESTsystem is consistently worse).
Our model is on parwith ANGELI on WEATHERGOV but performs betteron ROBOCUP and ATIS when evaluated both auto-759matically and by humans.
In general, a large part ofour output resembles the human text, which demon-strates that our simple language model yields coher-ent sentences (without any template engineering), atleast for the domains under consideration.6 ConclusionsWe have presented an end-to-end generation systemthat performs both content selection and surface re-alization.
Central to our approach is the encodingof generation as a parsing problem.
We reformulatethe input (a set of database records and text describ-ing some of them) as a PCFG and show how to findthe best derivation using the hypergraph framework.Despite its simplicity, our model is able to obtainperformance comparable to the state of the art.
Weargue that our approach is computationally efficientand viable in practical applications.
Porting the sys-tem to a different domain is straightforward, assum-ing a database and corresponding (unaligned) text.As long as the database is compatible with the struc-ture of the grammar in Table 1, we need only retrainto obtain the weights on the hyperarcs and a domainspecific language model.Our model takes into account the k-best deriva-tions at decoding time, however inspection of theseshows that it often fails to select the best one.
Inthe future, we plan to remedy this by using forestreranking, a technique that approximately reranksa packed forest of exponentially many derivations(Huang, 2008).
We would also like to scale ourmodel to more challenging domains (e.g., productdescriptions) and to enrich our generator with somenotion of discourse planning.
An interesting ques-tion is how to extend the PCFG-based approach ad-vocated here so as to capture discourse-level docu-ment structure.Acknowledgments We are grateful to Percy Liangand Gabor Angeli for providing us with their codeand data.
We would also like to thank Luke Zettle-moyer and Tom Kwiatkowski for sharing their ATISdataset with us and Frank Keller for his feedback onan earlier version of this paper.ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approach togeneration.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 502?512, Cambridge, MA.Regina Barzilay and Mirella Lapata.
2005.
Collec-tive content selection for concept-to-text generation.In Proceedings of Human Language Technology andEmpirical Methods in Natural Language Processing,pages 331?338, Vancouver, British Columbia.Anja Belz.
2008.
Automatic generation ofweather forecast texts using comprehensive probabilis-tic generation-space models.
Natural Language Engi-neering, 14(4):431?455.David L. Chen and Raymond J. Mooney.
2008.
Learn-ing to sportscast: A test of grounded language acqui-sition.
In Proceedings of International Conference onMachine Learning, pages 128?135, Helsinki, Finland.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Deborah A. Dahl, Madeleine Bates, Michael Brown,William Fisher, Kate Hunicke-Smith, David Pallett,Christine Pao, Alexander Rudnicky, and ElizabethShriberg.
1994.
Expanding the scope of the atis task:the atis-3 corpus.
In Proceedings of the Workshop onHuman Language Technology, pages 43?48, Plains-boro, NJ.Pablo A. Duboue and Kathleen R. McKeown.
2002.Content planner construction via evolutionary algo-rithms and a corpus-based fitness function.
In Pro-ceedings of International Natural Language Genera-tion, pages 89?96, Ramapo Mountains, NY.Giorgio Gallo, Giustino Longo, Stefano Pallottino, andSang Nguyen.
1993.
Directed hypergraphs and appli-cations.
Discrete Applied Mathematics, 42:177?201.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the 9th International Work-shop on Parsing Technology, pages 53?64, Vancouver,British Columbia.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 144?151,Prague, Czech Republic.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-08: HLT, pages 586?594, Columbus, Ohio.Frank Keller, Subahshini Gunasekharan, Neil Mayo, andMartin Corley.
2009.
Timing accuracy of Web ex-periments: A case study using the WebExp softwarepackage.
Behavior Research Methods, 41(1):1?12.760Joohyun Kim and Raymond Mooney.
2010.
Generativealignment and semantic parsing for learning from am-biguous supervision.
In Proceedings of the 23rd Con-ference on Computational Linguistics, pages 543?551,Beijing, China.Dan Klein and Christopher D. Manning.
2001.
Parsingand hypergraphs.
In Proceedings of the 7th Interna-tional Workshop on Parsing Technologies, pages 123?134, Beijing, China.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proceedings ofthe 2009 Conference on Empirical Methods in Natu-ral Language Processing, pages 40?51, Suntec, Sin-gapore.Percy Liang, Michael Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In roceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 91?99, Suntec, Singapore.Wei Lu and Hwee Tou Ng.
2011.
A probabilistic forest-to-string model for language generation from typedlambda calculus expressions.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 1611?1622, Edinburgh,UK.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia.Ehud Reiter and Robert Dale.
2000.
Building naturallanguage generation systems.
Cambridge UniversityPress, New York, NY.Ehud Reiter, Somayajulu Sripada, Jim Hunter, and IanDavy.
2005.
Choosing words in computer-generatedweather forecasts.
Artificial Intelligence, 167:137?169.Yuk Wah Wong and Raymond Mooney.
2007.
Gener-ation by inverting a semantic parser that uses statis-tical machine translation.
In Proceedings of the Hu-man Language Technology and the Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 172?179, Rochester, NY.Luke Zettlemoyer and Michael Collins.
2007.
Onlinelearning of relaxed CCG grammars for parsing to logi-cal form.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 678?687, Prague, Czech Republic.761
