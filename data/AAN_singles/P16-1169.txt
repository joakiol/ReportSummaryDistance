Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1791?1801,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLearning Concept Taxonomies from Multi-modal DataHao Zhang1, Zhiting Hu1, Yuntian Deng1, Mrinmaya Sachan1,Zhicheng Yan2, Eric P. Xing11Carnegie Mellon University,2UIUC{hao,zhitingh,yuntiand,mrinmays,epxing}@cs.cmu.eduAbstractWe study the problem of automaticallybuilding hypernym taxonomies from tex-tual and visual data.
Previous works intaxonomy induction generally ignore theincreasingly prominent visual data, whichencode important perceptual semantics.Instead, we propose a probabilistic modelfor taxonomy induction by jointly leverag-ing text and images.
To avoid hand-craftedfeature engineering, we design end-to-endfeatures based on distributed representa-tions of images and words.
The modelis discriminatively trained given a smallset of existing ontologies and is capableof building full taxonomies from scratchfor a collection of unseen conceptual labelitems with associated images.
We evalu-ate our model and features on the WordNethierarchies, where our system outperformsprevious approaches by a large gap.1 IntroductionHuman knowledge is naturally organized as se-mantic hierarchies.
For example, in WordNet(Miller, 1995), specific concepts are categorizedand assigned to more general ones, leading to asemantic hierarchical structure (a.k.a taxonomy).A variety of NLP tasks, such as question answer-ing (Harabagiu et al, 2003), document cluster-ing (Hotho et al, 2002) and text generation (Biranand McKeown, 2013) can benefit from the con-ceptual relationship present in these hierarchies.Traditional methods of manually constructingtaxonomies by experts (e.g.
WordNet) and interestcommunities (e.g.
Wikipedia) are either knowl-edge or time intensive, and the results have lim-ited coverage.
Therefore, automatic induction oftaxonomies is drawing increasing attention in both(a) InputSeafishShark RaySeafishRayShark?seafish, such as sharks and rays??
?shark and ray are a group of seafish??
?either ray or shark lives in ??
(b) Outputvisual similaritywordvec closenessFigure 1: An overview of our system.
(a) Input: acollection of label items, represented by text andimages; (b) Output: we build a taxonomy fromscratch by extracting features based on distributedrepresentations of text and images.NLP and computer vision.
On one hand, a num-ber of methods have been developed to build hi-erarchies based on lexical patterns in text (Yangand Callan, 2009; Snow et al, 2006; Kozareva andHovy, 2010; Navigli et al, 2011; Fu et al, 2014;Bansal et al, 2014; Tuan et al, 2015).
Theseworks generally ignore the rich visual data whichencode important perceptual semantics (Bruni etal., 2014) and have proven to be complemen-tary to linguistic information and helpful for manytasks (Silberer and Lapata, 2014; Kiela and Bot-tou, 2014; Zhang et al, 2015; Chen et al, 2013).On the other hand, researchers have built visual hi-erarchies by utilizing only visual features (Griffinand Perona, 2008; Yan et al, 2015; Sivic et al,2008).
The resulting hierarchies are limited in in-terpretability and usability for knowledge transfer.Hence, we propose to combine both visualand textual knowledge to automatically build tax-onomies.
We induce is-a taxonomies by su-pervised learning from existing entity ontologieswhere each concept category (entity) is associatedwith images, either from existing dataset (e.g.
Im-ageNet (Deng et al, 2009)) or retrieved from theweb using search engines, as illustrated in Fig 1.Such a scenario is realistic and can be extended toa variety of tasks; for example, in knowledge base1791construction (Chen et al, 2013), text and imagecollections are readily available but label relationsamong categories are to be uncovered.
In large-scale object recognition, automatically learningrelations between labels can be quite useful (Denget al, 2014; Zhao et al, 2011).Both textual and visual information provide im-portant cues for taxonomy induction.
Fig 1 il-lustrates this via an example.
The parent cate-gory seafish and its two child categories sharkand ray are closely related as: (1) there is ahypernym-hyponym (is-a) relation between thewords ?seafish?
and ?shark?/?ray?
through text de-scriptions like ?...seafish, such as shark and ray...?,?...shark and ray are a group of seafish...?
; (2)images of the close neighbors, e.g., shark andray are usually visually similar and images ofthe child, e.g.
shark/ray are similar to a sub-set of images of seafish.
To effectively capturethese patterns, in contrast to previous works thatrely on various hand-crafted features (Chen et al,2013; Bansal et al, 2014), we extract features byleveraging the distributed representations that em-bed images (Simonyan and Zisserman, 2014) andwords (Mikolov et al, 2013) as compact vectors,based on which the semantic closeness is directlymeasured in vector space.
Further, we developa probabilistic framework that integrates the richmulti-modal features to induce ?is-a?
relations be-tween categories, encouraging local semantic con-sistency that each category should be visually andtextually close to its parent and siblings.In summary, this paper has the following con-tributions: (1) We propose a novel probabilisticBayesian model (Section 3) for taxonomy induc-tion by jointly leveraging textual and visual data.The model is discriminatively trained and can bedirectly applied to build a taxonomy from scratchfor a collection of semantic labels.
(2) We de-sign novel features (Section 4) based on general-purpose distributed representations of text and im-ages to capture both textual and visual relationsbetween labels.
(3) We evaluate our model andfeatures on the ImageNet hierarchies with two dif-ferent taxonomy induction tasks (Section 5).
Weachieve superior performance on both tasks andimprove the F1score by 2x in the taxonomy con-struction task, compared to previous approaches.Extensive comparisons demonstrate the effective-ness of integrating visual features with languagefeatures for taxonomy induction.
We also providequalitative analysis on our features, the learnedmodel, and the taxonomies induced to provide fur-ther insights (Section 5.3).2 Related WorkMany approaches have been recently developedthat build hierarchies purely by identifying eitherlexical patterns or statistical features in text cor-pora (Yang and Callan, 2009; Snow et al, 2006;Kozareva and Hovy, 2010; Navigli et al, 2011;Zhu et al, 2013; Fu et al, 2014; Bansal et al,2014; Tuan et al, 2014; Tuan et al, 2015; Kielaet al, 2015).
The approaches in Yang and Callan(2009) and Snow et al (2006) assume a startingincomplete hierarchy and try to extend it by in-serting new terms.
Kozareva and Hovy (2010) andNavigli et al (2011) first find leaf nodes and thenuse lexical patterns to find intermediate terms andall the attested hypernymy links between them.
In(Tuan et al, 2014), syntactic contextual similarityis exploited to construct the taxonomy, while Tuanet al (2015) go one step further to consider trusti-ness and collective synonym/contrastive evidence.Different from them, our model is discriminativelytrained with multi-modal data.
The works of Fuet al (2014) and Bansal et al (2014) use similarlanguage-based features as ours.
Specifically, in(Fu et al, 2014), linguistic regularities betweenpretrained word vectors (Mikolov et al, 2013)are modeled as projection mappings.
The trainedprojection matrix is then used to induce pairwisehypernym-hyponym relations between words.
Ourfeatures are partially motivated by Fu et al (2014),but we jointly leverage both textual and visual in-formation.
In Kiela et al (2015), both textual andvisual evidences are exploited to detect pairwiselexical entailments.
Our work is significantly dif-ferent as our model is optimized over the wholetaxonomy space rather than considering only wordpairs separately.
In (Bansal et al, 2014), a struc-tural learning model is developed to induce a glob-ally optimal hierarchy.
Compared with this work,we exploit much richer features from both text andimages, and leverage distributed representationsinstead of hand-crafted features.Several approaches (Griffin and Perona, 2008;Bart et al, 2008; Marsza?ek and Schmid, 2008)have also been proposed to construct visual hier-archies from image collections.
In (Bart et al,2008), a nonparametric Bayesian model is devel-oped to group images based on low-level features.1792In (Griffin and Perona, 2008) and (Marsza?ek andSchmid, 2008), a visual taxonomy is built to ac-celerate image categorization.
In (Chen et al,2013), only binary object-object relations are ex-tracted using co-detection matrices.
Our work dif-fers from all of these as we integrate textual withvisual information to construct taxonomies.Also of note are several works that integratetext and images as evidence for knowledge baseautocompletion (Bordes et al, 2011) and zero-shot recognition (Gan et al, 2015; Gan et al, ;Socher et al, 2013).
Our work is different be-cause our task is to accurately construct multi-level hyponym-hypernym hierarchies from a set of(seen or unseen) categories.3 Taxonomy Induction ModelOur model is motivated by the key observation thatin a semantically meaningful taxonomy, a cate-gory tends to be closely related to its children aswell as its siblings.
For instance, there exists ahypernym-hyponym relation between the name ofcategory shark and that of its parent seafish.
Be-sides, images of shark tend to be visually simi-lar to those of ray, both of which are seafishes.Our model is thus designed to encourage such lo-cal semantic consistency; and by jointly consider-ing all categories in the inference, a globally opti-mal structure is achieved.
A key advantage of themodel is that we incorporate both visual and tex-tual features induced from distributed representa-tions of images and text (Section 4).
These fea-tures capture the rich underlying semantics andfacilitate taxonomy induction.
We further distin-guish the relative importance of visual and tex-tual features that could vary in different layersof a taxonomy.
Intuitively, visual features wouldbe increasingly indicative in the deeper layers, assub-categories under the same category of specificobjects tend to be visually similar.
In contrast,textual features would be more important wheninducing hierarchical relations between the cate-gories of general concepts (i.e.
in the near-rootlayers) where visual characteristics are not neces-sarily similar.3.1 The ProblemAssume a set of N categories x ={x1, x2, .
.
.
, xN}, where each category xnconsists of a text term tnas its name, as wellas a set of images in= {i1, i2, .
.
.
}.
Our goalis to construct a taxonomy tree T over thesecategories1, such that categories of specific objecttypes (e.g.
shark) are grouped and assigned tothose of general concepts (e.g.
seafish).
As thecategories in x may be from multiple disjointtaxonomy trees, we add a pseudo category x0asthe hyper-root so that the optimal taxonomy is en-sured to be a single tree.
Let zn?
{1, .
.
.
, N} bethe index of the parent of category xn, i.e.
xznisthe hypernymic category of xn.
Thus the problemof inducing a taxonomy structure is equivalent toinferring the conditional distribution p(z|x) overthe set of (latent) indices z = {z1, .
.
.
, zn}, basedon the images and text.3.2 ModelWe formulate the distribution p(z|x) through amodel which leverages rich multi-modal features.Specifically, let cnbe the set of child nodes of cat-egory xnin a taxonomy encoded by z.
Our modelis defined aspw(z,pi|x,?)
?
p(pi|?)N?n=1?xn?
?cnpingw(xn, xn?, cn\xn?
)(1)where gw(xn, xn?, cn\xn?
), defined asgw(xn, xn?, cn\xn?)
= exp{w>d(xn?)fn,n?,cn\xn?
},measures the semantic consistency between cate-gory xn?, its parent xnas well as its siblings in-dexed by cn\xn?.
The function gw(?)
is loglin-ear with respect to fn,n?,cn\xn?, which is the fea-ture vector defined over the set of relevant cate-gories (xn, xn?, cn\xn?
), with cn\xn?being the setof child categories excluding xn?
(Section 4).
Thesimple exponential formulation can effectively en-courage close relations among nearby categoriesin the induced taxonomy.
The function has com-bination weights w = {w1, .
.
.
,wL}, where L isthe maximum depth of the taxonomy, to capturethe importance of different features, and the func-tion d(xn?)
to return the depth of xn?in the currenttaxonomy.
Each layer l (1 ?
l ?
L) of the tax-onomy has a specific wlthereby allowing varyingweights of the same features in different layers.The parameters are learned in a supervised man-ner.
In eq 1, we also introduce a weight pinfor eachnode xn, in order to capture the varying popular-ity of different categories (in terms of being a par-ent category).
For example, some categories like1We assume T to be a tree.
Most existing taxonomies aremodeled as trees (Bansal et al, 2014), since a tree helps sim-plify the construction and ensures that the learned taxonomyis interpretable.
With minor modifications, our model alsoworks on non-tree structures.1793plant can have a large number of sub-categories,while others such as stone have less.
We model pias a multinomial distribution with Dirichlet prior?
= (?1, .
.
.
, ?N) to encode any prior knowledgeof the category popularity2; and the conjugacy al-lows us to marginalize out pi analytically to getpw(z|x,?)
??p(pi|?)N?n=1?xn?
?cnpingw(xn, xn?, cn\xn?)dpi??n?
(qn+ ?n)?xn?
?cngw(xn, xn?, cn\xn?
)(2)where qnis the number of children of category xn.Next, we describe our approach to infer the ex-pectation for each zn, and based on that selecta particular taxonomy structure for the categorynodes x.
As z is constrained to be a tree (i.e.
cyclewithout loops), we include with eq 2, an indicatorfactor 1(z) that takes 1 if z corresponds a tree and0 otherwise.
We modify the inference algorithmappropriately to incorporate this constraint.Inference.
Exact inference is computationally in-tractable due to the normalization constant of eq 2.We therefore use Gibbs Sampling, a procedure forapproximate inference.
Here we present the sam-pling formula for each zndirectly, and defer thedetails to the supplementary material.
The sam-pling procedure is highly efficient because the nor-malization term and the factors that are irrelevantto znare cancelled out.
The formula isp(zn=m|z\zn, ?)
?
1(zn= m,z\zn) ?
(q?nm+ ?m)??xn??cm?
{xn}gw(xm, xn?, cm?
{xn})?xn?
?cm\xngw(xm, xn?, cm\xn),(3)where qmis the number of children of categorym; the superscript?n denotes the number exclud-ing xn.
Examining the validity of the taxonomystructure (i.e.
the tree indicator) in each samplingstep can be computationally prohibitive.
To han-dle this, we restrict the candidate value of znineq 3, ensuring that the new znis always a tree.Specifically, given a tree T , we define a structureoperation as the procedure of detaching one nodexnin T from its parent and appending it to anothernode xmwhich is not a descendant of xn.Proposition 1.
(1) Applying a structure operationon a tree T will result in a structure that is stilla tree.
(2) Any tree structure over the node set xthat has the same root node with tree T can beachieved by applying structure operation on T afinite number of times.2?
could be estimated using training data.The proof is straightforward and we omit it dueto space limitations.
We also add a pseudo nodex0as the fixed root of the taxonomy.
Hence byinitializing a tree-structured state rooted at x0andrestricting each updating step as a structure opera-tion, our sampling procedure is able to explore thewhole valid tree space.Output taxonomy selection.
To apply the modelto discover the underlying taxonomy from a givenset of categories, we first obtain the marginals of zby averaging over the samples generated througheq 3, then output the optimal taxonomy z?by find-ing the maximum spanning tree (MST) using theChu-Liu-Edmonds algorithm (Chu and Liu, 1965;Bansal et al, 2014).Training.
We need to learn the model parame-ters wlof each layer l, which capture the rela-tive importance of different features.
The modelis trained using the EM algorithm.
Let `(xn) bethe depth (layer) of category xn; and?z (siblings?cn) denote the gold structure in training data.
Ourtraining algorithm updates w through maximumlikelihood estimation, wherein the gradient of wlis (see the supplementary materials for details):?wl=?n:`(xn)=l{f(xz?n, xn,?cn\xn)?Ep[f(xzn, xn, cn\xn)]} ,which is the net difference between gold featurevectors and expected feature vectors as per themodel.
The expectation is approximated by col-lecting samples using the sampler described aboveand averaging them.4 FeaturesIn this section, we describe the feature vector fused in our model, and defer more details in thesupplementary material.
Compared to previoustaxonomy induction works which rely purely onlinguistic information, we exploit both perceptualand textual features to capture the rich spectrum ofsemantics encoded in images and text.
Moreover,we leverage the distributed representations of im-ages and words to construct compact and effec-tive features.
Specifically, each image i is repre-sented as an embedding vector vi?
Raextractedby deep convolutional neural networks.
Such im-age representation has been successfully appliedin various vision tasks.
On the other hand, thecategory name t is represented by its word em-bedding vt?
Rb, a low-dimensional dense vec-tor induced by the Skip-gram model (Mikolov et1794al., 2013) which is widely used in diverse NLP ap-plications too.
Then we design f(xn, xn?, cn\xn?
)based on the above image and text representations.The feature vector f is used to measure the localsemantic consistency between category xn?and itsparent category xnas well as its siblings cn\xn?.4.1 Image FeaturesSibling similarity.
As mentioned above, closeneighbors in a taxonomy tend to be visually simi-lar, indicating that the embedding of images of sib-ling categories should be close to each other in thevector space Ra.
For a category xnand its imageset in, we fit a Gaussian distribution N (vin,?n)to the image vectors, where vin?
Rais the meanvector and ?n?
Ra?ais the covariance matrix.For a sibling category xmof xn, we define the vi-sual similarity between xnand xmasvissim(xn, xm)=[N (vim;vin,?n)+N (vin;vim,?m)]/2which is the average probability of the mean im-age vector of one category under the Gaussian dis-tribution of the other.
This takes into account notonly the distance between the mean images, butalso the closeness of the images of each category.Accordingly, we compute the visual similarity be-tween xn?and the set cn\xn?by averaging:vissim(xn?, cn\xn?)
=?xm?cn\xn?vissim(xn?, xm)|cn| ?
1.We then bin the values of vissim(xn?, cn\xn?
)and represent it as an one-hot vector, which consti-tutes f as a component named as siblings image-image relation feature (denoted as S-V13).Parent prediction.
Similar to feature S-V1, wealso create the similarity feature between the im-age vectors of the parent and child, to measuretheir visual similarity.
However, the parent node isusually a more general concept than the child, andit usually consists of images that are not necessar-ily similar to its child.
Intuitively, by narrowingthe set of images to those that are most similar toits child improves the feature.
Therefore, differentfrom S-V1, when estimating the Gaussian distri-bution of the parent node, we only use the top Kimages with highest probabilities under the Gaus-sian distribution of the child node.
We empiricallyshow in section 5.3 that choosing an appropriateK consistently boosts the performance.
We namethis feature as parent-child image-image relationfeature (denoted as PC-V1).3S: sibling, PC: parent-child, V: visual, T: textual.Further, inspired by the linguistic regularities ofword embedding, i.e.
the hypernym-hyponym re-lationship between words can be approximated bya linear projection operator between word vectors(Mikolov et al, 2013; Fu et al, 2014), we design asimilar strategy to (Fu et al, 2014) between im-ages and words so that the parent can be ?pre-dicted?
given the image embedding of its childcategory and the projection matrix.
Specifically,let (xn, xn?)
be a parent-child pair in the trainingdata, we learn a projection matrix ?
which min-imizes the distance between ?vin?(i.e.
the pro-jected mean image vector vin?of the child) andvtn(i.e.
the word embedding of the parent):?
?= argmin?1N?n??vin??
vtn?22+ ???
?1,where N is the number of parent-child pairs in thetraining data.
Once the projection matrix has beenlearned, the similarity between a child node xn?and its parent xnis computed as ??vin??
vtn?,and we also create an one-hot vector by binningthe feature value.
We call this feature as parent-child image-word relation feature (PC-V2).4.2 Word FeaturesWe briefly introduce the text features employed.More details about the text feature extractioncould be found in the supplementary material.Word embedding features.d PC-V1, We in-duce features using word vectors to measure bothsibling-sibling and parent-child closeness in textdomain (Fu et al, 2014).
One exception is that, aseach category has only one word, the sibling sim-ilarity is computed as the cosine distance betweentwo word vectors (instead of mean vectors).
Thiswill produce another two parts of features, parent-child word-word relation feature (PC-T1) and sib-lings word-word relation feature (S-T1).Word surface features.
In addition to theembedding-based features, we further leveragelexical features based on the surface forms ofchild/parent category names.
Specifically, weemploy the Capitalization, Ends with, Contains,Suffix match, LCS and Length different features,which are commonly used in previous worksin taxonomy induction (Yang and Callan, 2009;Bansal et al, 2014).5 ExperimentsWe first disclose our implementation details insection 5.1 and the supplementary material for bet-1795ter reproducibility.
We then compare our modelwith previous state-of-the-art methods (Fu et al,2014; Bansal et al, 2014) with two taxonomy in-duction tasks.
Finally, we provide analysis on theweights and taxonomies induced.5.1 Implementation DetailsDataset.
We conduct our experiments on the Im-ageNet2011 dataset (Deng et al, 2009), whichprovides a large collection of category items(synsets), with associated images and a label hi-erarchy (sampled from WordNet) over them.
Theoriginal ImageNet taxonomy is preprocessed, re-sulting in a tree structure with 28231 nodes.Word embedding training.
We train word em-bedding for synsets by replacing each word/phrasein a synset with a unique token and then us-ing Google?s word2vec tool (Mikolov et al,2013).
We combine three public available cor-pora together, including the latest Wikipedia dump(Wikipedia, 2014), the One Billion Word Lan-guage Modeling Benchmark (Chelba et al, 2013)and the UMBC webbase corpus (Han et al, 2013),resulting in a corpus with total 6 billion tokens.The dimension of the embedding is set to 200.Image processing.
we employ the ILSVRC12pre-trained convolutional neural networks (Si-monyan and Zisserman, 2014) to embed each im-age into the vector space.
Then, for each categoryxnwith images, we estimate a multivariate Gaus-sian parameterized by Nxn= (?xn,?xn), andconstrain ?xnto be diagonal to prevent overfitting.For categories with very few images, we only es-timate a mean vector ?xn.
For nodes that do nothave images, we ignore the visual feature.Training configuration.
The feature vector is aconcatenation of 6 parts, as detailed in section 4.All pairwise distances are precomputed and storedin memory to accelerate Gibbs sampling.
The ini-tial learning rate for gradient descent in the M stepis set to 0.1, and is decreased by a fraction of 10every 100 EM iterations.5.2 Evaluation5.2.1 Experimental SettingsWe evaluate our model on three subtrees sampledfrom the ImageNet taxonomy.
To collect the sub-trees, we start from a given root (e.g.
consumergoods) and traverse the full taxonomy using BFS,and collect all descendant nodes within a depth h(number of nodes in the longest path).
We vary hTrees Tree A Tree B Tree CSynset ID 12638 19919 23733Name consumer goods animal food, nutrienth = 4 187 207 572h = 5 362 415 890h = 6 493 800 1166h = 7 524 1386 1326Table 1: Statistics of our evaluation set.
The bot-tom 4 rows give the number of nodes within eachheight h ?
{4, 5, 6, 7}.
The scale of the threesrange from small to large, and there is no overlap-ping among them.to get a series of subtrees with increasing heightsh ?
{4, 5, 6, 7} and various scales (maximally1326 nodes) in different domains.
The statisticsof the evaluation sets are provided in Table 1.To avoid ambiguity, all nodes used in ILSVRC2012 are removed as the CNN feature extractor istrained on them.We design two different tasks to evaluate ourmodel.
(1) In the hierarchy completion task, werandomly remove some nodes from a tree and usethe remaining hierarchy for training.
In the testphase, we infer the parent of each removed nodeand compare it with groundtruth.
This task is de-signed to figure out whether our model can suc-cessfully induce hierarchical relations after learn-ing from within-domain parent-child pairs.
(2)Different from the previous one, the hierarchyconstruction task is designed to test the gener-alization ability of our model, i.e.
whether ourmodel can learn statistical patterns from one hi-erarchy and transfer the knowledge to build a tax-onomy for another collection of out-of-domain la-bels.
Specifically, we select two trees as the train-ing set to learn w. In the test phase, the model isrequired to build the full taxonomy from scratchfor the third tree.We use Ancestor F1as our evaluation metric(Kozareva and Hovy, 2010; Navigli et al, 2011;Bansal et al, 2014).
Specifically, we measureF1= 2PR/(P +R) values of predicted ?is-a?
re-lations where the precision (P) and recall (R) are:P =|isapredicted?
isagold||isapredicted|, R =|isapredicted?
isagold||isagold|.We compare our method to two previouslystate-of-the-art models by Fu et al (2014) andBansal et al (2014), which are closest to ours.1796Method h = 4 h = 5 h = 6 h = 7Hierarchy CompletionFu2014 0.66 0.42 0.26 0.21Ours (L) 0.70 0.49 0.45 0.37Ours (LV) 0.73 0.51 0.50 0.42Hierarchy ConstructionFu2014 0.53 0.33 0.28 0.18Bansal2014 0.67 0.53 0.43 0.37Ours (L) 0.58 0.41 0.36 0.30Ours (LB) 0.68 0.55 0.45 0.40Ours (LV) 0.66 0.52 0.42 0.34Ours (LVB - E) 0.68 0.55 0.44 0.39Ours (LVB) 0.70 0.57 0.49 0.43Table 2: Comparisons among different variants ofour model, Fu et al (2014) and Bansal et al (2014)on two tasks.
The ancestor-F1scores are reported.5.2.2 ResultsHierarchy completion.
In the hierarchy comple-tion task, we split each tree into 70% nodes fortraining and 30% for test, and experiment withdifferent h. We compare the following three sys-tems: (1) Fu20144(Fu et al, 2014); (2) Ours (L):Our model with only language features enabled(i.e.
surface features, parent-child word-word re-lation feature and siblings word-word relation fea-ture); (3) Ours (LV): Our model with both lan-guage features and visual features5.
The aver-age performance on three trees are reported at Ta-ble 2.
We observe that the performance gradu-ally drops when h increases, as more nodes areinserted when the tree grows higher, leading to amore complex and difficult taxonomy to be ac-curately constructed.
Overall, our model outper-forms Fu2014 in terms of the F1score, even with-out visual features.
In the most difficult case withh = 7, our model still holds an F1score of 0.42(2?
of Fu2014), demonstrating the superiority ofour model.Hierarchy construction.
The hierarchy construc-tion task is much more difficult than hierarchycompletion task because we need to build a taxon-omy from scratch given only a hyper-root.
For thistask, we use a leave-one-out strategy, i.e.
we trainour model on every two trees and test on the third,and report the average performance in Table 2.
Wecompare the following methods: (1) Fu2014, (2)Ours (L), and (3) Ours (LV), as described above;(4) Bansal2014: The model by Bansal et al (2014)4We tried different parameter settings for the number ofclusters C and the identification threshold ?, and reported thebest performance we achieved.5In the comparisons to (Fu et al, 2014) and (Bansal etal., 2014), we simply set K = ?, i.e.
we use all availableimages of the parent category to estimate the PC-V1 feature.retrained using our dataset; (5) Ours (LB): By ex-cluding visual features, but including other lan-guage features from Bansal et al (2014); (6) Ours(LVB): Our full model further enhanced with allsemantic features from Bansal et al (2014); (7)Ours (LVB - E): By excluding word embedding-based language features from Ours (LVB).As shown, on the hierarchy construction task,our model with only language features still outper-forms Fu2014 with a large gap (0.30 compared to0.18 when h = 7), which uses similar embedding-based features.
The potential reasons are two-fold.First, we take into account not only parent-childrelations but also siblings.
Second, their methodis designed to induce only pairwise relations.
Tobuild the full taxonomy, they first identify all pos-sible pairwise relations using a simple threshold-ing strategy and then eliminate conflicted relationsto obtain a legitimate tree hierarchy.
In contrast,our model is optimized over the full space of alllegitimate taxonomies by taking the structure op-eration in account during Gibbs sampling.When comparing to Bansal2014, our modelwith only word embedding-based features under-performs theirs.
However, when introducing vi-sual features, our performance is comparable (p-value = 0.058).Furthermore, if we discard visualfeatures but add semantic features from Bansal etal.
(2014), we achieve a slight improvement of0.02 over Bansal2014 (p-value = 0.016), whichis largely attributed to the incorporation of wordembedding-based features that encode high-levellinguistic regularity.
Finally, if we enhance ourfull model with all semantic features from Bansalet al (2014), our model outperforms theirs by agap of 0.04 (p-value < 0.01), which justifies ourintuition that perceptual semantics underneath vi-sual contents are quite helpful.5.3 Qualitative AnalysisIn this section, we conduct qualitative studies toinvestigate how and when the visual informationhelps the taxonomy induction task.Contributions of visual features.
To evaluatethe contribution of each part of the visual fea-tures to the final performance, we train our modeljointly with textual features and different combi-nations of visual features, and report the ancestor-F1scores.
As shown in Table 3.
When incorporat-ing the feature S-V1, the performance is substan-tially boosted by a large gap at all heights, show-1797S-V1 PC-V1 PC-V2 h = 4 h = 5 h = 6 h = 70.58 0.41 0.36 0.30X 0.63 0.48 0.40 0.32X 0.61 0.44 0.38 0.31X 0.60 0.42 0.37 0.31X X 0.65 0.52 0.41 0.33X X X 0.66 0.52 0.42 0.34Table 3: The performance when different combi-nations of visual features are enabled.ing that visual similarity between sibling nodesis a strong evidence for taxonomy induction.
Itis intuitively plausible, as it is highly likely thattwo specific categories share a common (and moregeneral) parent category if similar visual contentsare observed between them.
Further, adding thePC-V1 feature gains us a better improvement thanadding PC-V2, but both minor than S-V1.Compared to that of siblings, the visual similar-ity between parents and children does not stronglyholds all the time.
For example, images of Terres-trial animal are only partially similar to those ofFeline, because the former one contains the laterone as a subset.
Our feature captures this type of?contain?
relation between parents and children byconsidering only the top-K images from the par-ent category that have highest probabilities underthe Gaussian distribution of the child category.
Tosee this, we vary K while keep all other settings,and plot the F1scores in Fig 2.
We observe atrend that when we gradually increase K, the per-formance goes up until reaching some maximal; Itthen slightly drops (or oscillates) even when moreimages are available, which confirms with our fea-ture design that only top images should be consid-ered in parent-child visual similarity.Overall, the three visual features complementeach other, and achieve the highest performancewhen combined.Visual representations.
To investigate how theimage representations affect the final performance,we compare the ancestor-F1 score when differ-ent pre-trained CNNs are used for visual fea-ture extraction.
Specifically, we employ both theCNN-128 model (128 dimensional feature with15.6% top-5 error on ILSVRC12) and the VGG-16 model (4096 dimensional feature with 7.5%top-5 error) by Simonyan and Zisserman (2014),but only observe a slight improvement of 0.01 onthe ancestor-F1 score for the later one.Relevance of textual and visual features v.s.depth of tree.
Compared to Bansal et al (2014),h = 4 h = 5h = 6 h = 7Ancester-F1K /100Figure 2: The Ancestor-F1scores changes overK (number of images used in the PC-V1 feature)at different heights.
The values in the x-axis areK/100; K =?
means all images are used.Figure 3: Normalized weights of each feature v.s.the layer depth.a major difference of our model is that differ-ent layers of the taxonomy correspond to differentweightswl, while in (Bansal et al, 2014) all layersshare the same weights.
Intuitively, introducinglayer-wisew not only extends the model capacity,but also differentiates the importance of each fea-ture at different layers.
For example, the imagesof two specific categories, such as shark and ray,are very likely to be visually similar.
However,when the taxonomy goes from bottom to up (spe-cific to general), the visual similarity is graduallyundermined ?
images of fish and terrestrial ani-mal are not necessarily similar any more.
Hence,it is necessary to privatize the weightsw for differ-ent layers to capture such variations, i.e.
the visualfeatures become more and more evident from shal-low to deep layers, while the textual counterparts,which capture more abstract concepts, relativelygrow more indicative oppositely from specific togeneral.To visualize the variations across layers, foreach feature component, we fetch its correspond-1798millipedeinvertebrate critteranimalcaterpillardomestic animalstarfishchordatearrowwormarthropod nematodetrichinaplanarian polypechinodermannelidwormtussockcaterpillartentcaterpillarcephalochordate scavengerlarvaceansagittastockerlancelet archiannelidlarvafoodstuffmeal, repastfood, nutrientnutrimentliquiddietdietaryingredientflour grainbeefstewcows?milkjuice waterboiledeggbarley springwaterstewfishstewdiaryproductwheatsoybeanmealwheatflourhard- boiledeggbrunch breakfastwaterdrinkingwaterjuiceFigure 4: Excerpts of the prediction taxonomies, compared to the groundturth.
Edges marked as red andgreen are false predictions and unpredicted groundtruth links, respectively.ing block in w as V .
Then, we average |V | andobserve how its values change with the layer depthh.
For example, for the parent-child word-wordrelation feature, we first fetch its correspondingweights V from w as a 20 ?
6 matrix, where 20is the feature dimension and 6 is the number oflayers.
We then average its absolute values6incolumn and get a vector v with length 6.
After`2normalization, the magnitude of each entry inv directly reflects the relative importance of thefeature as an evidence for taxonomy induction.Fig 3(b) plots how their magnitudes change withh for every feature component averaged on threetrain/test splits.
It is noticeable that for both word-word relations (S-T1, PC-T1), their correspondingweights slightly decrease as h increases.
On thecontrary, the image-image relation features (S-V1,PC-V1) grows relatively more prominent.
The re-sults verify our conjecture that when the categoryhierarchy goes deeper into more specific classes,the visual similarity becomes relatively more in-dicative as an evidence for taxonomy induction.Visualizing results.
Finally, we visualize someexcerpts of our predicted taxonomies, as comparedto the groundtruth in Fig 4.6We take the absolute value because we only care aboutthe relevance of the feature as an evidence for taxonomy in-duction, but note that the weight can either encourage (posi-tive) or discourage (negative) connections of two nodes.6 ConclusionIn this paper, we study the problem of automat-ically inducing semantically meaningful concepttaxonomies from multi-modal data.
We propose aprobabilistic Bayesian model which leverages dis-tributed representations for images and words.
Wecompare our model and features to previous oneson two different tasks using the ImageNet hier-archies, and demonstrate superior performance ofour model, and the effectiveness of exploiting vi-sual contents for taxonomy induction.
We furtherconduct qualitative studies and distinguish the rel-ative importance of visual and textual features inconstructing various parts of a taxonomy.AcknowledgementsWe would like to thank anonymous reviewers fortheir valuable feedback.
We would also like tothank Mohit Bansal for helpful suggestions.
Wethank NVIDIA for GPU donations.
The work issupported by NSF Big Data IIS1447676.ReferencesMohit Bansal, David Burkett, Gerard de Melo, and DanKlein.
2014.
Structured learning for taxonomy in-duction with belief propagation.Evgeniy Bart, Ian Porteous, Pietro Perona, and Max1799Welling.
2008.
Unsupervised learning of visual tax-onomies.
In CVPR.Or Biran and Kathleen McKeown.
2013.
Classifyingtaxonomic relations between pairs of wikipedia arti-cles.Antoine Bordes, Jason Weston, Ronan Collobert, andYoshua Bengio.
2011.
Learning structured embed-dings of knowledge bases.
In Conference on Artifi-cial Intelligence, number EPFL-CONF-192344.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.2014.
Multimodal distributional semantics.Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,Thorsten Brants, Phillipp Koehn, and Tony Robin-son.
2013.
One billion word benchmark for measur-ing progress in statistical language modeling.
arXivpreprint arXiv:1312.3005.Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta.2013.
Neil: Extracting visual knowledge from webdata.
In CVPR.Yoeng-Jin Chu and Tseng-Hong Liu.
1965.
Onshortest arborescence of a directed graph.
ScientiaSinica.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, KaiLi, and Li Fei-Fei.
2009.
Imagenet: A large-scalehierarchical image database.
In CVPR.Jia Deng, Nan Ding, Yangqing Jia, Andrea Frome,Kevin Murphy, Samy Bengio, Yuan Li, HartmutNeven, and Hartwig Adam.
2014.
Large-scale ob-ject classification using label relation graphs.
InECCV.Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, HaifengWang, and Ting Liu.
2014.
Learning semantic hier-archies via word embeddings.
In ACL.Chuang Gan, Yi Yang, Linchao Zhu, Deli Zhao, andYueting Zhuang.
Recognizing an action using itsname: A knowledge-based approach.
InternationalJournal of Computer Vision, pages 1?17.Chuang Gan, Ming Lin, Yi Yang, Yueting Zhuang, andAlexander G Hauptmann.
2015.
Exploring seman-tic inter-class relationships (SIR) for zero-shot ac-tion recognition.
In AAAI.Gregory Griffin and Pietro Perona.
2008.
Learningand using taxonomies for fast visual categorization.In CVPR.Lushan Han, Abhay Kashyap, Tim Finin, James May-field, and Jonathan Weese.
2013.
Umbc ebiquity-core: Semantic textual similarity systems.
Atlanta,Georgia, USA.Sanda M Harabagiu, Steven J Maiorano, and Marius APasca.
2003.
Open-domain textual question answer-ing techniques.
Natural Language Engineering.Andreas Hotho, Alexander Maedche, and SteffenStaab.
2002.
Ontology-based text document clus-tering.Douwe Kiela and L?eon Bottou.
2014.
Learning imageembeddings using convolutional neural networks forimproved multi-modal semantics.
In EMNLP.Douwe Kiela, Laura Rimell, Ivan Vulic, and StephenClark.
2015.
Exploiting image generality for lexicalentailment detection.
In ACL.Zornitsa Kozareva and Eduard Hovy.
2010.
Asemi-supervised method to learn and construct tax-onomies using the web.
In EMNLP.Marcin Marsza?ek and Cordelia Schmid.
2008.
Con-structing category hierarchies for visual recognition.In ECCV.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In NIPS.George A Miller.
1995.
Wordnet: a lexical databasefor english.
Communications of the ACM.Roberto Navigli, Paola Velardi, and Stefano Faralli.2011.
A graph-based algorithm for inducing lexicaltaxonomies from scratch.
In IJCAI.Carina Silberer and Mirella Lapata.
2014.
Learn-ing grounded meaning representations with autoen-coders.
In ACL.Karen Simonyan and Andrew Zisserman.
2014.
Verydeep convolutional networks for large-scale imagerecognition.
arXiv preprint arXiv:1409.1556.Josef Sivic, Bryan C Russell, Andrew Zisserman,William T Freeman, and Alexei A Efros.
2008.
Un-supervised discovery of visual object class hierar-chies.
In CVPR.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In ACL.Richard Socher, Milind Ganjoo, Christopher D Man-ning, and Andrew Ng.
2013.
Zero-shot learningthrough cross-modal transfer.
In Advances in neuralinformation processing systems, pages 935?943.Luu Anh Tuan, Jung-jae Kim, and Ng See Kiong.2014.
Taxonomy construction using syntactic con-textual evidence.
In EMNLP.Luu Anh Tuan, Jung-jae Kim, and Ng See Kiong.2015.
Incorporating trustiness and collective syn-onym/contrastive evidence into taxonomy construc-tion.Wikipedia.
2014. https://dumps.wikimedia.org/enwiki/20141208/.1800Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vi-gnesh Jagadeesh, Dennis DeCoste, Wei Di, andYizhou Yu.
2015.
Hd-cnn: Hierarchical deepconvolutional neural networks for large scale visualrecognition.
In ICCV.Hui Yang and Jamie Callan.
2009.
A metric-basedframework for automatic taxonomy induction.
InACL-IJCNLP.Hao Zhang, Gunhee Kim, and Eric P. Xing.
2015.
Dy-namic topic modeling for monitoring market com-petition from online text and image data.
In KDD.Bin Zhao, Fei Li, and Eric P Xing.
2011.
Large-scalecategory structure aware image categorization.
InNIPS.Xingwei Zhu, Zhao-Yan Ming, Xiaoyan Zhu, and Tat-Seng Chua.
2013.
Topic hierarchy construction forthe organization of multi-source user generated con-tents.
In SIGIR.1801
