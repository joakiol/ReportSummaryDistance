Automatic Discovery of Attribute Words fromWeb DocumentsKosuke Tokunaga, Jun?ichi Kazama, and Kentaro TorisawaJapan Advanced Institute of Science and Technology (JAIST),Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan{kosuke-t, kazama, torisawa}@jaist.ac.jpAbstract.
We propose a method of acquiring attribute words for a widerange of objects from Japanese Web documents.
The method is a simpleunsupervised method that utilizes the statistics of words, lexico-syntacticpatterns, and HTML tags.
To evaluate the attribute words, we also es-tablish criteria and a procedure based on question-answerability aboutthe candidate word.1 IntroductionKnowledge about how we recognize objects is of great practical importance formany NLP tasks.
Knowledge about attributes, which tells us from what view-points objects are usually understood or described, is one of such type of knowl-edge.
For example, the attributes of car objects will be weight, engine, steeringwheel, driving feel, and manufacturer.
In other words, attributes are items whosevalues we want to know when we want to know about the object.
More analyti-cally, we tend to regard A as an attribute for objects of class C when A worksas if function v = A(o), o ?
C where v is necessary to us to identify o (especiallyto distinguish o from o?
(= o) ?
C).
Therefore, obvious applications of attributesare ones such as summarization [1,2] and question-answering [3].
Moreover, theycan be useful as features in word clustering [4] or machine learning.
Althoughthe knowledge base for attributes can be prepared manually (e.g., WordNet [5]),problems are cost and coverage.
To overcome these, we propose a method thatautomatically acquires attribute knowledge from the Web.To acquire the attributes for a given class, C (e.g., car), the proposed methodfirst downloads documents that contain class label C (e.g., ?car?)
from the Web.1We extract the candidates of attribute words from these documents and scorethem according to the statistics of words, lexico-syntactic patterns, and HTMLtags.
Highly scored words are output as attributes for the class.
Lexico-syntacticpatterns and other statistics have been used in other lexical knowledge acquisi-tion systems [3,4,6,7,8].
We specifically used lexico-syntactic patterns involvingthe Japanese postposition ?no?
as used in [8] such as ?C no A?
where A is anattribute word, which is almost equivalent to pattern ?A of C?
used in [7] to1 We use C to denote both the class and its class label (the word representing theclass).
We also use A to denote both the attribute and the word representing it.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
106?118, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Automatic Discovery of Attribute Words from Web Documents 107find part-whole relations.
Novel features of our method are its use of Web searchengines to focus on documents highly relevant to the class and its use of statisticsconcerning attribute words and surrounding HTML tags.One of the difficulties in studying attribute knowledge is that there are nostandard definitions of attributes, or criteria for evaluating obtained attributes.In this paper, we propose a simple but effective definition of attributes thatmatches our motivation and applications, i.e., whether we can ask a questionabout the attribute and whether there is an answer to that question (questionanswerability).
For example, one can ask as ?Who is the manufacturer of thiscar?
?, and someone might answer ?Honda?, because we want to know the manu-facturer when we concerned about cars.
We designed a procedure for evaluatingattributes based on this idea.
As the literature points out [9,10], attributes caninclude many types of relations such as property (e.g., weight), part-of (e.g.,engine), telic (e.g., driving feel), and agentive (e.g., manufacturer).
However, weignored type distinctions in this study.
First, because attributes are useful evenif the type is not known, and second, because defining attributes as one of thesetypes and evaluating them only complicates the evaluation process, making theresults unstable.
The use of linguistic tests to define attributes is not that new.Woods [11] devised a test on whether we can say ?The A of o is v.?
Although wefollowed this procedure, we focused more on attributes that are important forour understanding of an object by using question-answerability as our criterion.2 Acquisition Method2.1 Basic Observations on AttributesOur method is based on the following three observations.1.
Attributes tend to occur in documents that contain the class label and notin other documents.2.
Attributes tend to be emphasized by the use of certain HTML tags or occuras items in HTML itemizations or tables in Web documents.3.
Attributes tend to co-occur with the class label in specific lexico-syntacticpatterns involving the postposition ?no.
?2.2 Extraction of Candidate WordsTo acquire the attributes of class C, we first download documents that containclass label C using a Web search engine, according to the first observation.
Werefer to this set of documents as a local document set (LD(C)).
All the nounsappearing in the local document set are regarded as candidates of attributewords.
Here, the nouns are words tagged as ?proper nouns?, ?sahen nouns?
(nouns that can become a verb with the suffix ?suru?
), ?location?, or ?unknown?
(e.g., words written in katakana) by a Japanese morphological analyzer, JUMAN[12].
Note that we restricted ourselves to single word attributes in this study.The obtained candidate words are scored in the next step.108 K. Tokunaga, J. Kazama, and K. TorisawaTable 1.
Lexico-syntactic patterns for attribute acquisition.
(We added possible En-glish translations for the patterns in parenthesis).C no A ha (A of C [verb]) C no A de (by A of C) C no A e (to A of C)C no A ga (A of C [verb]) C no A made (even/until A of C) C no AA(A of C,)C no A wo ([verb] A of C) C no A kara (from A of C)C no A ni (at/in A of C) C no A yori (from/than A of C)2.3 Ranking of Candidate WordsWe rank the candidate words according to a score that reflects the observationsdescribed in Sect.
2.1.
The overall score takes the following form.V (C, A) = n(C, A) ?
f(C, A) ?
t(C, A) ?
dfidf(C, A), (1)where A is the candidate word to be scored and C is the class.
n(C, A) andf(C, A) are scores concerning lexico-syntactic patterns.
t(C, A) is a score con-cerning the statistics of HTML tags to reflect the second observation.
Finally,dfidf(C, A) is the score related to word statistics.
This reflects the first obser-vation.
By multiplying these sub-scores, we expect that they will complementeach other.
We will explain the details on these sub-scores in the following.As previously mentioned, we use lexico-syntactic patterns including the Japa-nese postposition ?no?
as clues.
The patterns take the form ?C no A POST ?where POST is a Japanese postposition or a punctuation mark.2 The actualpatterns used are listed in Table 1.
Score n(C, A) is the number of times C andA co-occur in these patterns in the local document set LD(C).Score f(C, A) requires more explanation.
Roughly, f(C, A) is the number oftimes C and A co-occur in the patterns without the last postposition (i.e., pat-tern ?C no A?)
collected from 33 years of parsed newspaper articles.3 Note thatpattern matching was done against the parsed dependency structures.4 The rea-son this score was used in addition to n(C, A) was to obtain more reliable scoresby increasing the number of documents to be matched.
This may sound contra-dictory to the fact that the Web is the largest corpus in the world.
However,we found that we could not obtain all the documents that contained the classlabel because existing commercial Web search engines return URLs for a verysmall fraction of matched documents (usually up to about 1,000 documents).Although we could use hit counts for the patterns, we did not do this to avoidoverloading the search engine (each class has about 20,000 candidate words).Score t(C, A) is the number of times A appears in LD(C) surrounded byHTML tags.
More precisely, we count the number of times A appears in theform: ?<tag1>A<tag2>?
where the number of characters between HTML tags2 Note that there are actually no spaces between words in Japanese.
The spaces arefor easier understanding.3 Yomiuri newspaper 1987?2001, Mainichi newspaper 1991?1999, and Nikkei newspa-per 1983?1990; 3.01 GB in total.
We used a Japanese dependency parser [13].4 The differences from n(C, A) were introduced to reuse the existing parsed corpus.Automatic Discovery of Attribute Words from Web Documents 109<B>???????</B><BR>??<BR>??
400g???
2?????????
2??????????
1.5<BR>????
1.5???????
1?????????????<P>????<BR>??????????<P>???<BR><OL><LI>???????????????Fig.
1.
Example HTML document(i.e., the length of A) is 20 at maximum.
The tags (<tag1> and <tag2>) can beeither a start tag (e.g., <A>) or an end tag (e.g., </A>).
This score is intendedto give high values for words that are emphasized or occur in itemizations ortables.
For example, in the HTML document in Fig.
1, the words ????????
(Thai-curry)?, ???
(ingredient)?, ?????
(spice)?, ???????????
(coriander, cumin)?, and ????
(recipe)?
are counted.Finally, dfidf(C, A), which reflects the first observation, is calculated as:dfidf(C, A) = df(A, LD(C)) ?
idf(A), idf(A) = log |G|df(A,G) .df(A, X) denotes the number of documents where A appears in documents X .G is a large set of randomly collected Web documents, which we call the globaldocument set.
We derived this score from a similar score, which was used in [14]to measure the association between a hypernym and hyponyms.3 Evaluation CriteriaThis section presents the evaluation criteria based on question-answerability (QAtests).
Based on the criteria, we designed an evaluation procedure where theevaluators were asked to answer either by yes or no to four tests at maximum,i.e., a hyponymy test (Sect.
3.4), a QA test (Sect.
3.1) and a suffix augmentedQA test (Sect.
3.2) followed by a generality test (Sect.
3.3).3.1 Question-Answerability TestBy definitions we used, attributes are what we want to know about the object.Therefore, if A is an attribute of objects of class C, we can arrange questions(consisting of A and C) that require the values for A as the answer.
Then someoneshould be able to answer the questions.
For example, we can ask ?Who is thedirector of this movie??
because director is an attribute of movie.
The answermight be someone such as ?Stanley Kubrick.?
We designed the QA test shown inFig.
2 to assess the correctness of attribute A for class C based on this criterion.Several points should be noted.
First, since the value for the attribute is actuallydefined for the object instance (i.e., v = A(o), o ?
C), we should qualify classlabel C using ?kono (this)?
to refer to an object instance of class C.Second, since we cannot know what question is possible for A beforehand,we generate all the question types listed in Fig.
2 and ask whether any of themare acceptable.110 K. Tokunaga, J. Kazama, and K. TorisawaAre any of the following questions grammatically correct, natural, and answerable?1.
??
C ?
A???
(kono C no A ha nani?/What is the A of this C?)2.
??
C ?
A???
(kono C no A ha dare?/Who is the A of this C?)3.
??
C ?
A????
(kono C no A ha itu?/When is the A of this C?)4.
??
C ?
A????
(kono C no A ha doko?/Where is the A of this C?)5.
??
C ?
A????
(kono C no A ha dore?/Which is the A of this C?)6.
??
C ?
A?????
(kono C no A ha ikutu?/How many is the A of this C?)7.
??
C ?
A????
(kono C no A ha dou?/How much is the A of this C?)Fig.
2.
Question-answerability TestThird, the question should be natural as well as grammatically correct.
Nat-uralness was explained to the evaluators as positively determining whether thequestion can be their first choice in usual conversations.
In our point of view, at-tributes should be important items for people in describing objects.
We assumedthat attributes that conformed to the naturalness criterion would be such impor-tant attributes.
For example, stapler is not an attribute of company in our sense,although almost all companies own staplers.
Our naturalness criterion can re-flect this observation since the question ?What is the stapler of this company?
?is unnatural as a first question when talking about a company, and thereforewe can successfully conclude that stapler is not an attribute.
Note that Woods?linguistic test [11] (i.e., whether ?the attribute of an object is a value?
can bestated or not) cannot reject stapler since it does not have the naturalness re-quirement (e.g., we can say ?the stapler of [used by] SONY is Stapler-X?
).5 Inaddition, note that such importances can be assessed more easily in the QA test,since questioners basically ask what they think is important at least at the timeof utterance.
However, we cannot expect such an implication even though thedeclarative sentence is acceptable.Finally, the answer to the question does not necessarily need to be written inlanguage.
For example, values for attributes such as map, picture, and blueprintcannot be written as language expressions but can be represented by other media.Such attributes are not rare since we obtain attributes from the Web.3.2 Suffix Augmented QA TestSome attributes that are obtained can fail the QA test even if they are correct,especially when the surface form is different from the one they actually mean.This often occurs since Japanese is very elliptic and our method is restricted tosingle word attributes.
For example, the word seito (students) can be used torepresent the attribute seito suu (number of students) as in the sentence below.kono gakko no seito ha 500 ninthis school of students is 500 NUM(The number of students of this school is 500.
)5 Stapler might be an important attribute of companies for stationery sellers.
However,we focus on attributes that are important for most people in most situations.Automatic Discovery of Attribute Words from Web Documents 111?
?
(number of)???
(method for) ?
(name of)??
(-er)?
???
([amount of] time of) ??
(time of)????
(period of) ??
(location of)?
??
(amount of money for) ??
(degree of)???
(state of)?
???
(nominalized adjectives e.g., ?height of?
?prettiness of?)Fig.
3.
Allowed augmentationThese attributes whose parts are elided (e.g., seito representing seito suu) arealso useful since they are actually used in sentences as in the above example.Therefore, they should be assessed as correct attributes in some way.
Althoughthe most appropriate question for seito representing seito suu is (6) in Fig.
2,it is unfortunately ungrammatical since ikutu cannot be used for the number ofpersons.
Therefore, seito representing seito suu will fail the QA test.6In Japanese, most of the elided parts can be restored by adding appropriatesuffixes (as ?suu?
(number of) in the previous example) or by adding ?no?
+nominalized adjectives.
Thus, when the attribute word failed the first QA test,we asked the evaluators to re-do the QA test by choosing an appropriate suffixor a nominalized adjective from the list of allowed augmentations and adding itto the end of the evaluated word.
Figure 3 lists the allowed augmentations.7,83.3 Generality TestAlthough our primal aim was to acquire the attributes for a given class, i.e., ,to find attributes that are common to all the instances of the class, we found,in preliminary experiments, that some uncommon (but interesting) attributeswere assessed as correct according to the QA test depending on the evaluator.An example is subtitle for the class movie.
Strictly speaking, subtitle is notan attribute of all movies, since all movies do not necessarily have subtitles.For example, only foreign films have subtitles in Japan.
However, we think thisattribute is also useful in practice for people who have a keen interest in foreignfilms.
Thus, the evaluators were asked whether the attribute was common formost instances of the class when the attribute was judged to be correct in theQA test.
We call attributes that passed this generality test general attributes,and those that failed but passed the QA test relaxed attributes (note that generalattributes is a subset of relaxed attributes).
We compare the accuracies for therelaxed and general attributes in the experiments.6 Seito (representing students) might pass the QA test with question type (2) in Fig.2.
However, this is not always the case since some evaluators will judge the questionto be unnatural.7 Postposition ?no (of)?
before the suffix is also allowed to be added if it makes thequestion more natural.8 The problem here might not occur if we used many more question types in the firstQA test.
However, we did not do this to keep the first QA test simple.
With the samemotivation, we kept the list of allowed suffixes short (only general and importantsuffixes).
The uncovered cases were treated by adding nominalized adjectives.112 K. Tokunaga, J. Kazama, and K. Torisawa3.4 Hyponymy TestFinally, we should note that we designed the evaluation procedure so that theevaluators could be asked whether candidate A is a hyponym of C before the QAtests.
If A is a hyponym of C, we can skip all subsequent tests since A cannotbe an attribute of C. We added this test because the output of the system oftencontains hyponyms and these tend to cause confusion in the QA tests sinceexpression ?C no A?
is natural even when A is a hyponym of C (e.g., ?anime noDragon Ball (Dragon Ball [of/the] anime)?
).4 Experiments4.1 Experimental SettingWe first selected 32 word classes from 1,589 classes acquired from the Web withan automatic hypernym-hyponym acquisition method [14].
Here, we regarded thehypernym as the class label.
Since our purpose was just to evaluate our methodfor classes from the Web, we selected classes that were obtained successfully.
Werandomly chose the 22 classes listed in Table 2 for human evaluation from these32 classes.9 The hyponyms were used to help the evaluators to disambiguate themeaning of class labels (if ambiguity existed).To collect LD(C), we used the Web search engine goo (http://www.goo.ne.jp).The size of LD(C) was 857 documents (URLs) on class average.
There wereabout 20, 000 candidate words on class average.
As global document set G re-quired for the calculation of dfidf(C, A), we used 1.0?106 randomly downloadedWeb documents.Table 2.
Classes used in evaluation??
(city), ???
(museum), ??
(national holiday), ??
(police), ??
(facility), ??
(university),??
(newspaper), ??
(garbage), ??
(shrine), ?
(bird), ??
(hospital), ??
(plant), ?
(river), ???
(elementary school), ?
(music tune), ???
(library), ??
(branch office), ???
(web site), ?
(town), ????
(sensor), ??
(training), ???
(car)We output the top 50 attributes for each class ranked with our proposedmethod and with alternative methods that were used for comparison.
We gath-ered outputs for all the methods, removing duplication (i.e., taking the set union)to achieve efficient evaluation, and re-sorted them randomly to ensure that theassessment was unbiased.
Four human evaluators assessed these gathered at-tributes class-by-class in four days using a GUI tool implementing the evaluationprocedure described in Sect.
3.
There were a total of 3, 678 evaluated attributes.Using the evaluation results, we re-constructed the evaluations for the top 50 foreach method.
The kappa value [15], which indicates inter-evaluator agreement,was 0.533 for the general attribute case and 0.593 for the relaxed attribute case.According to [15], these kappa values indicate ?moderate?
agreement.9 This selection was due to time/cost limitations.Automatic Discovery of Attribute Words from Web Documents 1134.2 Accuracy of Proposed MethodFigure 4 has accuracy graphs for the proposed method for relaxed attributes.The graph on the left shows per-evaluator precision when the top n (repre-sented by x axis) attributes were output.
The precision is the average over allclasses.
Although we cannot calculate the actual recall, the x axis correspondsto approximate recall.
We can see that ranking with the proposed method hasa positive correlation with human evaluation, although the assessments variedgreatly depending on the evaluator.
The graph on the right shows curves foraverage (with standard deviation), 3-consensus, and 4-consensus precision.
3-consensus (4-consensus) is precision where the attribute is considered correct byat least three (four) evaluators.
Figure 5 has graphs for the general attributecase the same as for the relaxed case.
Although there is a positive correlationbetween ranking with the proposed method and human evaluators, the precisionwas, not surprisingly, lower than that for the relaxed case.
In addition, the lowerkappa value (0.533 compared to 0.593 for the relaxed case) indicated that thegenerality test was harder than the QA tests.The accuracy of the proposed method was encouraging.
Although we cannoteasily determine which indicator is appropriate, if we use the majority rule (3-0.550.60.650.70.750.80.850.90.9515  10  15  20  25  30  35  40  45  50PrecisionRank (recall)Evaluator1Evaluator2Evaluator3Evaluator40.550.60.650.70.750.80.850.90.9515  10  15  20  25  30  35  40  45  50PrecisionRank (recall)Average3-consensus4-consensusFig.
4.
Accuracy of relaxed attributes0.40.50.60.70.80.915  10  15  20  25  30  35  40  45  50PrecisionRank (recall)Evaluator1Evaluator2Evaluator3Evaluator40.40.50.60.70.80.915  10  15  20  25  30  35  40  45  50PrecisionRank (recall)Average3-consensus4-consensusFig.
5.
Accuracy of general attributes114 K. Tokunaga, J. Kazama, and K. TorisawaTable 3.
Top 20 attributes of several classes obtained by proposed methodClasses Attributes?????(bird)??
(picture)[4/4] ??
(name)[4/2] ??
(sort)[4/4] ????
(illustration)[3/3] ??
(characteristics)[4/4] ??
(disease)[4/2] ??
(life)[4/4] ??
(topic)[3/2] ??
(relation)[0/0] ????
(image)[4/4] ?
(nest)[4/4] ???
(song)[4/4] ?
(shape)[4/4] ??
(info.
)[4/4] ??
(world)[0/0] ?
(song)[4/4] ??
(animal)[0/0] ???
(page)[3/2] ??
(ecology)[4/4] ?
(wing)[4/4]??????(hospital)??????
(home page)[4/1] ??
(facility)[3/3] ??
(info.
)[4/4] ??
(intro.
)[4/4] ??
(info.
desk)[4/4] ??
(authorization)[3/3] ??
(name)[4/2] ??
(doctor)[4/4] ???
(psychiatry)[4/2] ??
(reputation)[4/4] ??
(handling)[4/4] ??
(phone)[2/2] ??
(medical care)[4/4] ??
(treatment)[4/4] ??
(medical service)[3/3] ??
(function)[3/3] ??
(director)[4/4] ??
(valuation)[4/4] ??
(medical examination)[4/4] ???
(page)[2/2]??
(admin.
)[4/3] ??
(part)[1/1]??????(plant)??
(name)[4/2] ??
(species)[4/4] ??
(picture)[4/4] ??
(seed)[4/4] ??
(cultivation)[4/3] ??
(observa-tion)[4/3] ??
(characteristics)[4/4] ??
(explanation)[4/4] ??
(image)[4/4] ??
(surveillance)[4/3] ???
(data)[4/4] ??
(evolution)[3/3] ??
(description)[4/4] ???
(list)[2/2] ?
(leaf)[4/3] ??
(preserva-tion)[2/2] ????
(design)[1/1] ??
(growth)[4/4]?????(river)??
(water level)[4/4] ??
(upstream)[4/4] ??
(name)[4/2] ??
(environment)[4/4] ??
(water qual-ity)[4/4] ??
(history)[4/4] ??
(head stream)[4/4] ??
(picture)[4/4] ?
(water)[4/4] ??
(surface)[4/4] ??
(location)[4/4] ??
(current)[4/4] ??
(waterside)[4/4] ??
(river head)[4/4] ??
(four seasons)[3/3] ??
(characteristics)[4/4] ?
(inside)[1/1] ???
(streamside)[4/4] ??
(nature)[4/4] ????
(babbling)[4/4]?????(elementaryschool)??
(activity)[4/4] ????
(efforts)[4/3] ???
(athletic meeting)[4/4] ???
(child)[4/4] ??????
(homepage)[4/0] ??
(head teacher)[4/4] ??
(classroom)[4/4] ??
(school song)[4/4] ??
(student)[4/4] ??
(school building)[4/4] ??
(event)[4/4] ??
(learning)[3/3] ??
(feeding service)[4/3] ???
(page)[2/2] ???
(gym)[4/4] ??
(class)[3/3] ???
(mail)[0/0] ??
(grade)[1/1] ???
(opening ceremony)[4/4] ??(music)[2/2]?????
(music tune)??
(lyrics)[4/1] ????
(title)[4/2] ??
(performance)[4/4] ???
(list)[0/0] ????
(image)[4/4] ??
(lyrics writing)[4/1] ??
(musical score)[4/4] ??
(name)[4/2] ??
(content)[3/3] ????
(genre)[4/4] ??(info.
)[4/4] ????
(point)[4/4] ??
(world)[1/1] ?????
(melody)[4/4] ??
(end)[3/2] ??
(title)[4/2]?
(inside)[0/0] ??
(composition)[4/4] ???
(theme)[4/4] ???
(data)[4/2]?????(library)??
(source material)[4/4] ??????
(home page)[4/2] ???
(page)[3/1] ??
(history)[4/4] ??
(establish-ment)[4/4] ????
(system)[4/4] ??
(book stock)[4/4] ???
(copy)[2/2] ?
(book)[4/4] ??
(location)[4/4]??
(use)[4/4] ????
(service)[4/4] ??????
(database)[4/3] ??
(book)[4/4] ??
(newspaper)[4/4] ??
(close)[4/4] ??
(catalog)[3/3] ??
(display)[4/2] ??
(facility)[2/2] ??
(info.)[4/4]?????(town)??
(population)[4/4]??
(history)[4/4]??????
(home page)[4/0]??
(sightseeing)[4/4]??
(info.)[3/3]??
(finance)[4/4] ??
(facility)[4/4] ???
(heritage)[4/2] ??
(environment)[4/4] ??
(hot spring)[3/1]??
(topic)[3/2] ??
(four seasons)[3/3] ????
(event)[4/3] ???
(library)[4/3] ??
(culture)[4/4] ??
(landscape)[4/4] ????
(symbol)[4/3] ??
(industry)[4/3] ??
(agriculture)[4/2] ??
(town council)[3/3]??????(sensor)??
(info.
)[4/4] ??
(sensitivity)[4/3] ??
(sort)[4/3] ??
(position)[4/4] ????
(install)[4/4] ??
(devel-opment)[4/4] ??
(accuracy)[4/4] ???
(size)[4/4] ??
(specification)[4/4] ??
(temperature)[2/1] ???
(data)[4/4] ???
(set)[4/4] ??
(install)[4/4] ??
(function)[4/4] ??
(technology)[4/4] ??
(feature)[4/4]???
(page)[3/3] ??
(height)[3/2] ??
(adoption)[3/3] ??
(application)[4/4]??????(training)??
(content)[4/4] ??
(purpose)[4/4] ??
(practice)[4/4] ???
(theme)[4/3] ?????
(program)[4/4] ??
(lecturer)[4/4] ??
(plan)[4/4] ??
(name)[4/2] ????
(menu)[4/4] ??
(report)[4/4] ??
(target)[4/4]??
(outcome)[4/4] ??
(satisfaction)[2/2] ?
(place/atmosphere)[3/3] ???
(state of existence)[2/2] ??
(detail)[4/4] ??
(opportunity)[1/1] ??
(capacity)[4/4] ??
(participation)[4/4] ??
(other)[0/0]consensus in our case) employed in [7], the proposed method obtained relaxedattributes with 0.852 precision and general attributes with 0.727 precision for thetop 20 outputs.
Table 3 lists the top 20 attributes obtained with the proposedmethod for several classes.
The numeral before (after) ?/?
is the number ofevaluators who judged the attribute as correct as a relaxed (general) attribute.We can see that many interesting attributes were obtained.4.3 Effect of ScoresIn this analysis, we assessed the effect that sub-scores in Eq.
(1) had on theacquisition accuracy by observing the decrease in precision when we removedeach score from Eq.
(1).
First, we could observe a positive effect for most scoresin terms of the precision averaged over evaluators.
Moreover, interestingly, thetendency of the effect was very similar for all evaluators, even though the as-sessments varied greatly depending on the evaluator as the previous experimentshowed.
Due to space limitations, we will only present the latter analysis here.Automatic Discovery of Attribute Words from Web Documents 115-0.2-0.15-0.1-0.0500.050.15  10  15  20  25  30  35  40  45  50DifferenceRank (recall)Proposed - pattern (web)Proposed - pattern (news)Proposed - tagProposed - dfidf-0.3-0.25-0.2-0.15-0.1-0.0500.050.15  10  15  20  25  30  35  40  45  50DifferenceRank (recall)Proposed - pattern (web)Proposed - pattern (news)Proposed - tagProposed - dfidfFig.
6.
Effect of scores.
Left: relaxed attribute.
Right: general attribute.We calculated the change in precision ?per evaluator?, and then calculated theaveraged change, i.e., the change averaged over evaluators.
Figure 6 plots theaveraged change and standard deviations.
The effect of n(C, A) is representedby ?Proposed - pattern (web)?, that of f(C, A) by ?Proposed - pattern (news)?,that of t(C, A) by ?Proposed - tag?, and that of dfidf(C, A) by ?Proposed -dfidf?.
In the relaxed attribute case, we can see that most of the scores were ef-fective at almost all ranks regardless of the evaluator (negative difference meanspositive effect).
The effect of f(C, A) and t(C, A) was especially remarkable.
Al-though n(C, A) has a similar curve to f(C, A), the effect is weaker.
This maybe caused by the difference in the number of documents available (As we previ-ously described, we currently cannot obtain a large number of documents fromthe Web).
The effect dfidf(C, A) had was two-fold.
This contributed positivelyat lower ranks but it contributed negatively at higher ranks (around the top1-5).
In the general attribute case, the positive effect became harder to observealthough the tendency was similar to the relaxed case.
However, we can see thatf(C, A) still contributed greatly even in this case.
The effect of t(C, A), on theother hand, seems to have weakened greatly.4.4 Effect of HypernymIf we have a hypernym-hyponym knowledge base, we can also collect the localdocument set by using the hyponyms in the class as the keywords for the searchengine instead of using the class label (hypernym).
In this experiment, we com-pared the proposed method with this alternative.
We collected about the samenumber of documents for the alternative method as for the proposed method tofocus on the quality of collected documents.
We used hyponyms with the alter-native method instead of class label C in patterns for n(C, A) (thus n(Hs, A) tobe precise).
f(C, A) was unchanged.
Figure 7 plots the results in the same wayas for the previous analysis (i.e., difference from the proposed method).
We cansee that the class label is better than hyponyms for collecting local documentsat least in the current setting.116 K. Tokunaga, J. Kazama, and K. Torisawa-0.1-0.08-0.06-0.04-0.0200.020.045  10  15  20  25  30  35  40  45  50DifferenceRank (recall)Hyponym-0.1-0.08-0.06-0.04-0.0200.020.045  10  15  20  25  30  35  40  45  50DifferenceRank (recall)HyponymFig.
7.
Effect of hypernyms.
Left: relaxed case.
Right: general case.5 Discussion5.1 Related WorkSeveral studies have attempted to acquire attributes or attribute-value pairs[1,3,7,8,16].
Yoshida [1] proposed a method of integrating tables on the Web.Although his method consequently acquired attributes, he did not evaluate theaccuracy of attributes.
Yoshida et al [16] proposed a method of identifyingattribute-value pairs in Web documents.
However, since this method only iden-tified the attributes obtained with the method in [1], the coverage might bebounded by the coverage of tables for attributes.
Moreover, these methods didnot utilize the statistics for words or lexico-syntactic patterns as ours did.
Taka-hashi et al [8] extracted triples (object, attribute, value) from newspaper articlesusing lexico-syntactic patterns and statistical scores.
However, they focused onlyon proper nouns and selected the attribute candidates manually.
Freishmann etal.
[3] extracted attribute-value pairs with a high degree of precision by filteringthe candidates extracted with lexico-syntactic patterns by using a model learnedwith supervised learning.
Although this approach is promising, their method waslimited to person names and we must prepare training data to apply the methodto other types of objects.5.2 Future DirectionsClues based on QA tests.
The current ranking, Eq.
(1), does not exploit theobservation behind the criteria in Sect.
3.
Only the lexico-syntactic patterns ?Cno A?
slightly reflect the criteria.
Higher accuracy might be achieved by usingpatterns that directly reflect the QA tests, e.g., statistics from FAQ lists.
Thehyponym tests in Sect.
3.4 can also be reflected if we use a hyponymy database.In addition, it is not surprising that the proposed method was not efficient atacquiring general attributes since the score was not meant for that (althoughthe use of class labels might be a contributing factor, ambiguous class labelsAutomatic Discovery of Attribute Words from Web Documents 117cause problems at the same time).
The hyponym database might be exploitedto measure the generality of attributes.Full use of the Web.
The current method cannot use all Web documentsdue to limitations with search engines.
The more Web documents we have, themore useful the score n(C, A).
We are currently planning to prepare our ownnon-restricted Web repository.
Using this, we would also like to elaborate on thecomparison described in Sect.
4.4 between the use of hypernyms (class labels)and hyponyms (instance words) in collecting the local document set.Assessment of Coverage.
Currently, the actual recall with the proposedmethod is unknown.
It will be important to estimate how many attributes areneeded for practical applications, e.g., by manually analyzing the use of pattern?C no A?
exhaustively for a certain class, C. In addition, since we selected classesthat were successfully obtained with a hyponymy acquisition method, we cannotdeny the possibility that the proposed method has been evaluated for the classesfor which reliable statistics can easily be obtained.
Thus, the evaluation of moredifficult (e.g., more infrequent) classes will be an important future work.Type Acquisition.
What types of questions and what types of suffix augmen-tations are possible for a given attribute (i.e., the type of attribute value) mightalso be useful, e.g., in value extraction and in determining type of the attribute(in the sense of ?property or part-of?).
This was left for the evaluators to chosearbitrarily in this study.
We would like to extract such knowledge from the Webusing similar techniques such as word statistics and lexico-syntactic patterns.6 ConclusionWe presented a method of acquiring attributes that utilizes statistics on words,lexico-syntactic patterns, and HTML tags.
We also proposed criteria and anevaluation procedure based on question-answerability.
Using the procedure, weconducted experiments with four human evaluators.
The results revealed thatour method could obtain attributes with a high degree of precision.References1.
Yoshida, M.: Extracting attributes and their values from web pages.
In: Proc.
ofthe ACL 2002 Student Research Workshop.
(2002) 72?772.
Yoshida, M., Torisawa, K., Tsujii, J.: Integrating tables on the world wide web.Transactions of the Japanese Society for Artificial Intelligence 19 (2004) 548?5603.
Fleischman, M., Hovy, E., Echihabi, A.: Offline strategies for online questionanswering: Answering questions before they are asked.
In: Proc.
of ACL 2003.
(2003) 1?74.
Almuhareb, A., Poesio, M.: Attribute-based and value-based clustering: An eval-uation.
In: Proc.
of EMNLP 2004.
(2004) 158?1655.
Fellbaum, C., ed.
: WordNet: An electronic lexical database.
The MIT Press (1998)118 K. Tokunaga, J. Kazama, and K. Torisawa6.
Hearst, M.A.
: Automatic acquisition of hyponyms from large text corpora.
In:Proc.
of COLING ?92.
(1992) 539?5457.
Berland, M., Charniak, E.: Finding parts in very large corpora.
In: Proc.
of ACL?99.
(1999)8.
Takahashi, T., Inui, K., Matsumoto, Y.: Automatic extraction of attribute relationsfrom text (in Japanese).
IPSJ, SIG-NLP.
NL-164 (2004) 19?249.
Guarino, N.: Concepts, attributes and arbitrary relations: some linguistic and on-tological criteria for structuring knowledge base.
Data and Knowledge Engineering(1992) 249?26110.
Pustejovsky, J.: The Generative Lexicon.
The MIT Press (1995)11.
Woods, W.A.
: What?s in a Link: Foundations for Semantic Networks.
In: Repre-sentation and Understanding: Studies in Cognitive Science.
Academic Press (1975)12.
Kurohashi, S., Nagao, M.: Japanese morphological analysis system JUMAN version3.61 manual (1999)13.
Kanayama, H., Torisawa, K., Mitsuishi, Y., Tsujii, J.: A hybrid Japanese parserwith hand-crafted grammar and statistics.
In: Proc.
of COLING 2000.
(2000)411?41714.
Shinzato, K., Torisawa, K.: Acquiring hyponymy relations from web documents.In: Proc.
of HLT-NAACL04.
(2004) 73?8015.
Landis, J.R., Koch, G.G.
: The measurement of observer agreement for categorialdata.
Biometrics 33 (1977) 159?17416.
Yoshida, M., Torisawa, K., Tsujii, J.: Chapter 10 (Extracting Attributes and TheirValues from Web Pages).
In: Web Document Analysis.
World Scientific (2003)
