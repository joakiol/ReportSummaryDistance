Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 15?22,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPUnsupervised Induction of Sentence Compression RulesJo?ao CordeiroCLT and BioinformaticsUniversity of Beira InteriorCovilh?a, Portugaljpaulo@di.ubi.ptGa?el DiasCLT and BioinformaticsUniversity of Beira InteriorCovilh?a, Portugalddg@di.ubi.ptPavel BrazdilLIAADUniversity of PortoPorto, Portugalpbrazdil@liaad.up.ptAbstractIn this paper, we propose a new unsu-pervised approach to sentence compres-sion based on shallow linguistic process-ing.
For that purpose, paraphrase extrac-tion and alignment is performed over webnews stories extracted automatically fromthe web on a daily basis to provide struc-tured data examples to the learning pro-cess.
Compression rules are then learnedthrough the application of Inductive LogicProgramming techniques.
Qualitative andquantitative evaluations suggests that thisis a worth following approach, whichmight be even improved in the future.1 IntroductionSentence compression, simplification or summa-rization has been an active research subject dur-ing this decade.
A set of approaches involvingmachine learning algorithms and statistical mod-els have been experimented and documented in theliterature and several of these are described next.1.1 Related WorkIn (Knight & Marcu, 2002) two methods wereproposed, one is a probabilistic model - thenoisy channel model - where the probabili-ties for sentence reduction (P{Scompress|S)}1) are estimated from a training set of 1035(Sentence, Sentencecompress) pairs, manuallycrafted, while considering lexical and syntacti-cal features.
The other approach learns syntac-tic tree rewriting rules, defined through four op-erators: SHIFT, REDUCE DROP and ASSIGN.Sequences of these operators are learned from thetraining set, and each sequence defines a complete1In the original paper the P (t|s) notation is used, where tis the sentence in the target language and s the original sen-tence in the source language.transformation from an original sentence to thecompressed version.In the work of (Le Nguyen & Ho, 2004)two sentence reduction algorithms were also pro-posed.
The first one is based on template-translation learning, a method inherited from themachine translation field, which learns lexicaltransformation rules2, by observing a set of 1500(Sentence, Sentencereduced) pair, selected froma news agency and manually tuned to obtain thetraining data.
Due to complexity difficulties foundfor the application of this big lexical ruleset, theyproposed an improvement where a stochastic Hid-den Markov Model is trained to help in the deci-sion of which sequence of possible lexical reduc-tion rules should be applied to a specific case.An unsupervised approach was included in thework of (Turner & Charniak, 2005), where train-ing data are automatically extracted from the PennTreebank corpus, to fit a noisy channel model,similar to the one used by (Knight & Marcu,2002).
Although it seems an interesting approachto provide new training instances, it still be depen-dent upon data manually labeled.More recently, the work of (Clarke & Lapata,2006) devise a different and quite curious ap-proach, where the sentence compression task isdefined as an optimization goal, from an IntegerProgramming problem.
Several constraints are de-fined, according to language models, linguistic,and syntactical features.
Although this is an unsu-pervised approach, without using any paralel cor-pus, it is completely knowledge driven, like a setof crafted rules and heuristics incorporated into asystem to solve a certain problem.1.2 Our ProposalIn this paper, we propose a new approach tothis research field, which follows an unsupervisedmethodology to learn sentence compression rules2Those rules are named there as template-reduction rules.15based on shallow linguistic processing.
We de-signed a system composed of four main stepsworking in pipeline, where the first three are re-sponsible for data extraction and preparation andin the last one the induction process takes place.The first step gathers web news stories from re-lated news events collected on a daily basis fromwhich paraphrases are extracted.
In the secondstep, word alignment between two sentences ofa paraphrase is processed.
In the third step, spe-cial regions from these aligned paraphrases, calledbubbles, are extracted and conveniently prepro-cessed to feed the induction process.
The wholesequence is schematized in figure 1.Figure 1: The Pipeline Architecture.The induction process generates sentence re-duction rules which have the following generalstructure: Lcond?Xcond?Rcond?
suppress(X).This means that the sentence segment X willbe eliminated if certain conditions hold over left(L), middle (X) and right (R) segments3.
InFigure 2, we present seven different rules whichhave been automatically induced from our archi-tecture.
These rules are formed by the conjunc-tion of several literals, and they define constraintsunder which certain sentence subparts may bedeleted, therefore compressing or simplifying thesentence.
The X symbol stands for the segment3For the sake of simplicity and compact representation,we will omit the rule consequent, which is always the same(??
suppress(X)?
), whenever a rule is presented.Z(X)= 1 ?
Lc= NP ?X1= JJ ?R1= IN (1)Z(X)= 1 ?
Lc= NP ?X1= RB ?R1= IN (2)Z(X)= 2 ?
L1= and ?X1= the ?R1= JJ (3)Z(X)= 2 ?
L1= the ?X2= of ?R1= NN (4)Z(X)= 2 ?
L1= the ?Xc= NP ?R1= NN (5)Z(X)= 3 ?
Lc= PP ?X1= the ?Rc= NP (6)Z(X)= 3 ?
Lc= NP ?X1= and ?R2= V B (7)Figure 2: Learned Sentence Compression Rules.to be dropped, L(?
)and R(?
)are conditions overthe left and right contexts respectively.
The nu-meric subscripts indicate the positions4where asegment constraint holds and the c subscript standsfor a syntactic chunk type.
The Z(?
)function com-putes the length of a given segment, by countingthe number of words it contains.
For instance, thefirst rule means that a word5will be eliminated ifwe have a NP (Noun Phrase) chunk in the leftcontext, and a preposition or subordinating con-junction, in the right context (R1= IN ).
The rulealso requires that the elimination word must be anadjective, as we have X1= JJ .This rule would be applied to the following seg-ment6[NP mutual/jj funds/nns information/nn][ADJP available/jj] [PP on/in] [NPreuters.com/nn]and would delete the word available giving riseto the simplified segment:[NP mutual/jj funds/nns information/nn][PP on/in] [NP reuters.com/nn].Comparatively to all existing works, we proposein this paper a framework capable to extract com-pression rules in a real world environment.
More-over, it is fully unsupervised as, at any step of theprocess, examples do not need to be labeled.In the remaining of the paper, we will presentthe overall architecture which achieves precision4The position starts with 1 and is counted from left toright, on the word segments, except for the left context, whereit is counted reversely.5As we have Z(X)= 1, the candidate segment size toeliminate is equal to one.6The segment is marked with part-of-speech tags (POS)and chunked with a shallow parser.
Both transformationswere made with the OpenNLP toolkit.16values up to 85.72%, correctness up to 4.03 in 5and utility up to 85.72%.2 Data PreparationCreating relevant training sets, with some thou-sands examples is a difficult task, as well as is themigration of such a system to process other lan-guages.
Therefore, we propose an unsupervisedmethodology to automatically create a training setof aligned paraphrases, from electronically avail-able texts on the web.
This step is done throughstep one and step two of Figure 1, and the detailsare described in the next two subsections.2.1 Paraphrase ExtractionOur system collects web news stories on a dailybasis, and organized them into clusters, whichare exclusively related to different and uniqueevents, happening each day: ?a company acqui-sition?, ?a presidential speech?, ?a bomb attack?,etc.
Usually, such clusters contain near 30 smallor medium news articles, collected from differ-ent media sources.
This environment proves to bevery fruitful for paraphrase extraction, since wehave many sentences conveying similar informa-tion yet written in a different form.A few unsupervised metrics have been appliedto automatic paraphrase identification and extrac-tion (Barzilay & Lee, 2003; Dolan et al, 2004).However, these unsupervised methodologies showa major drawback by extracting quasi-exact oreven exact match pairs of sentences as they relyon classical string similarity measures such as theEdit Distance in the case of (Dolan et al, 2004)and Word N-gram Overlap for (Barzilay & Lee,2003).
Such pairs are useless for our purpose,since we aim to identify asymmetrical paraphrasepairs to be used for sentence compression ruleinduction, as explained in (Cordeiro et al, Oct2007).
There we proposed a new metric, theSumo-Metric, specially designed for asymmetricalentailed pairs identification, and proved better per-formance over previous established metrics, evenin the specific case when tested with the MicrosoftParaphrase Research Corpus (Dolan et al, 2004),which contains mainly symmetrical cases.
For agiven sentence pair, having each sentence x andy words, and with ?
exclusive links between thesentences, the Sumo-Metric is defined in Equation8 and 9.S(Sa, Sb) =8>><>>:S(x, y, ?)
if S(x, y, ?)
< 1.00 if ?
= 0e?k?S(x,y,?
)otherwise(8)whereS(x, y, ?)
= ?
log2(x?)
+ ?
log2(y?)
(9)with ?, ?
?
[0, 1] and ?+ ?
= 1.We have shown (Cordeiro et al, Oct 2007) thatSumo-Metric outperforms all state-of-the-art met-rics over all tested corpora and allows to identify-ing similar sentences with high probability to beparaphrases.
In Figure 3, we provide the readerwith an example of an extracted paraphrase.
(1) To the horror of their fans, Miss Balland Arnaz were divorced in 1960.
(2) Ball and Arnaz divorced in 1960.Figure 3: An Assymetrical Paraphrase2.2 Paraphrase AlignmentFrom a corpus of asymmetrical paraphrases, wethen use biology-based gene alignment algorithmsto align the words contained in each of the twosentences within each paraphrase.
For that pur-pose, we implemented two well established algo-rithms, one identifying local alignments (Smith& Waterman, 1981) and the other one computingglobal alignments (Needleman & Wunsch, 1970).We also proposed a convenient dynamic strategy(Cordeiro et al, 2007), which chooses the bestalignment algorithm to be applied to a specificcase at runtime.The difference between local and global se-quence alignments is illustrated below, where weuse letters, instead of words, to better fit our paperspace constraints.
Suppose that we have the fol-lowing two sequences: [D,H,M,S,T,P,R,Q,I,S]and [T,P,Q,I,S,D,H,S] a global alignmentwould produce the following pair.D H M S T P R Q I S _ _ __ _ _ _ T P _ Q I S D H SFor the same two sequences, a local alignmentstrategy could generate two or more aligned sub-sequences as follows.17|D H M S| |T P R Q I S||D H _ S| |T P _ Q I S|Hence, at this stage of the process, we end with acorpus of aligned7asymmetrical paraphrases.
InFigure 4, we present the alignment of the para-phrase of Figure 3.
(1) To the horror of their fans ,(2) __ ___ ______ __ _____ ____ _(1) Miss Ball and Arnaz were divorced in 1960.
(2) ____ Ball and Arnaz ____ divorced in 1960.Figure 4: An Aligned ParaphraseThe next section describes how we use thisstructured data to extract instances which are go-ing to feed a learning system.3 Bubble ExtractionIn order to learn rewriting rules, we have focusour experiences on a special kind of data, se-lected from the corpus of aligned sentences, andwe named this data as Bubbles8.
Given two wordaligned sentences, a bubble is a non-empty seg-ment aligned with an empty segment of the othersentence of the paraphrase, sharing a ?strong?
con-text.
In Figure 5, we show different examples ofbubbles.the situation here in chicago with the workersthe situation ____ in chicago with the workersobama talks exclusively with tom brokaw on meetobama talks ___________ with tom brokaw on meetBall and Arnaz were divorced in 1960Ball and Arnaz ____ divorced in 1960america is in the exact same seat as sweigert andamerica is in ___ _____ same seat as sweigert andafter a while at the regents park gym, the presidentafter a while at ___ _______ ____ gym, the presidentFigure 5: Examples of BubblesTo extract a bubble, left and right contexts ofequally aligned words must occur, and the proba-bility of such extraction depends on the contextssize as well as the size of the region aligned withthe empty space.
The main idea is to eliminatecases where the bubble middle sequence is toolarge when compared to the size of left and rightcontexts.
More precisely, we use the condition in7By ?aligned?
we mean, from now on, word alignmentbetween paraphrase sentence pairs.8There are other possible regions to explore, but due tothe complexity of this task, we decided to initially work onlywith bubblesEquation 10 to decide whether a bubble should beextracted or not.Z(L)?
Z(X)+ Z(R)?
0 (10)whereL andR stand for the left and right contexts,respectively, and X is the middle region.
The Z(?
)function computes the length of a given segment,in terms of number of words.
For example, in thefirst and last examples of Figure 5, we have: 2 ?1+5 = 6 ?
0 and 4?3+4 = 5 ?
0.
In this case,both bubbles will be extracted.
This condition isdefined to prevent from extracting eccentric cases,as the ones shown in the examples shown in Figure6, where the conditions respectively fail: 0 ?
8 +3 = ?5 < 0 and 1?
7 + 2 = ?4 < 0.To the horror of their fans , Miss Ball and Arnaz__ ___ ______ __ _____ ____ _ ____ Ball and Arnazwill vote __ ___ _______ ____ __ _____ __ friday .____ vote on the amended bill as early as friday .Figure 6: Examples of Rejected BubblesIndeed, we favor examples with high commoncontexts and few deleted words to enhance the in-duction process.So far, we only consider bubbles where themiddle region is aligned with a void segment(Xtransf??
?).
However, more general transforma-tions will be investigated in the future.
Indeed, anytransformation Xtransf??
Y , where Y 6= ?, havingZ(X)> Z(Y ), may be a relevant compression ex-ample.Following this methodology, we obtain a hugeset of examples, where relevant sentence transfor-mations occur.
To have an idea about the amountof data we are working with, from a set of 30 daysweb news stories (133.5 MB of raw text), we iden-tified and extracted 596678 aligned paraphrases,from which 143761 bubbles were obtained.In the next section, we show how we exploreInductive Logic Programming (ILP) techniques togeneralize regularities and find conditions to com-press sentence segments.4 The Induction of Compression RulesMany different algorithms exist to induce knowl-edge from data.
In this paper, we use InductiveLogic Programming (ILP) (Muggleton, 1991) andit was a choice based on a set of relevant fea-tures like: the capacity to generate symbolic and18relational knowledge; the possibility to securelyavoid negative instances; the ability to mix differ-ent types of attribute and to have more control overthe theory search process.Unlike (Clarke & Lapata, 2006), we aim atinducing human understandable knowledge, alsoknown as symbolic knowledge.
For that pur-pose, ILP satisfies perfectly this goal by produc-ing clauses based on first order logic.
Moreover,most of the learning algorithms require a com-plete definition and characterization of the featureset, prior to the learning process, where any at-tribute must be specified.
This is a conceptual bot-tleneck to many learning problems such as ours,since we need to combine different types of at-tributes i.e.
lexical, morpho-syntactic and syntac-tical.
With ILP, we only need to define a set of pos-sible features and the induction process will searchthroughout this set.4.1 The Aleph SystemThe Aleph system(Srinivasan, 2000) is an empir-ical ILP system, initially designed to be a pro-totype for exploring ILP ideas.
It has become aquite mature ILP implementation, used in manyresearch projects, ranging form Biology to NLP.
Infact, Aleph is the successor of several and ?moreprimitive?
ILP systems, like: Progol (Muggleton,1999), FOIL (Quinlan, 1990), and Indlog (Cama-cho, 1994), among others, and may be appropri-ately parametrized to emulate any of those oldersystems.One interesting advantage in Aleph is the possi-bility to learn exclusively from positive instances,contrarily to what is required by most learning sys-tems.
Moreover, there is theoretical research work(Muggleton, 1996) demonstrating that the increasein the learning error tend to be negligible with theabsence of negative examples, as the number oflearning instances increases.
This is a relevantissue, for many learning domains, and speciallyours, where negative examples are not available.4.2 Learning InstancesIn our problem, we define predicates that charac-terize possible features to be considered during theinduction process.
Regarding the structure of ourlearning instances (bubbles), we define predicateswhich restrict left and right context sequences aswell as the aligned middle sequence.
In particu-lar, we limit the size of our context sequences toa maximum of three words and, so far, only usebubbles in which the middle sequence has a max-imum length of three9words.
The notion of con-texts from bubbles is clarified with the next exam-ple.L2 L1 X1 X2 X3 R1 R2 R3 R4L2 L1 __ __ __ R1 R2 R3 R4For such a case, we consider [L1, L2] as the leftcontext, [R1, R2, R3] as the right context, and[X1, X2, X3] as the aligned middle sequence.Such an example is represented with a Prolog termwith arity 5 (bub/5) in the following manner:bub(ID, t(3,0), [L1,L2],[X1,X2,X3]--->[],[R1,R2,R3]).The ID is the identifier of the sequence instance,t/2 defines the ?transformation dimension?, inthis case from 3 words to 0.
The third and fiftharguments are lists with the left and right con-texts, respectively, and the fourth argument con-tains the list with the elements deleted from themiddle sequence.
It is important to point out thatevery Li, XiandRiare structures with 3 elementssuch as word/POS/Chunk.
For example, theword president would be represented by theexpanded structure president/nn/np.4.3 Feature SpaceAs mentioned previously, with an ILP system, andin particular with Aleph, the set of attributes isdefined through a set of conditions, expressed inthe form of predicates.
These predicates are thebuilding blocks that will be employed to constructrules, during the induction process.
Hence, our at-tribute search space is defined using Prolog pred-icates, which define the complete set of possibil-ities for rule body construction.
In our problem,we let the induction engine seek generalizationconditions for the bubble main regions (left, mid-dle, and right).
Each condition may be from oneof the four types: dimensional, lexical, POS, andchunk.
Dimensional conditions simply expressthe aligned sequence transformation dimensional-ity.
Lexical conditions impose a fixed position tomatch a given word.
The POS condition is similarto the lexical one, but more general, as the positionmust match a specific part-of-speech tag.
Likely,chunk conditions bind a region to be equal to aparticular chunk type.
For example, by looking9They represent 83.47% from the total number of ex-tracted bubbles.19at Figure 2, the attentive reader may have noticedthat these three conditions are present in rule 7.
Interms of Aleph declaration mode, these conditionsare defined as follows.
:- modeh(1,rule(+bub)).
:- modeb(1,transfdim(+bub,n(#nat,#nat))).
:- modeb(3,chunk(+bub,#side,#chk)).
:- modeb(*,inx(+bub,#side,#k,#tword)).
:- determination(rule/1,transfdim/2).
:- determination(rule/1,chunk/3).
:- determination(rule/1,inx/4).The inx/4 predicate defines lexical and POStype conditions, the chunk/3 predicate de-fines chunking conditions and the transfdim/2predicate defines the transformation dimension-ality, which is in the form transfdim(N,0)with N>0, according to the kind of bubbles we areworking with.4.4 The Rule Value FunctionThe Aleph system implements many differentevaluation10functions which guide the theorysearch process, allowing the basic procedure fortheory construction to be altered.
In order to bet-ter fit to our problem, we define a new evaluationfunction calculated as the geometrical mean be-tween the coverage percentage and the rule sizevalue, as shown in Equation 11 whereR is the can-didate rule and Cov(R) is the proportion of posi-tive instances covered by R and the LV (?)
func-tion defines the rule value in terms of its length,returning a value in the [0, 1] interval.V alue(R) =pCov(R)?
LV (R) (11)The V alue(?)
function guides the inductionprocess, by preferring not too general rules havingmaximum possible coverage value.
As shown inFigure 7, the V alue(?)
function gives preferencesto rules with 3, 4 and 5 literals.5 ResultsThe automatic evaluation of a system is always thebest way to do it, due to its objectivity and scal-ability.
However, in many cases it is unfeasiblefor several practical reasons, like the unavailabilityof data or the difficulty to prepare an appropriate10In the Aleph terminology, this function is named as the?cost?
function, despite the fact that it really computes thevalue in the sense that the grater the value, the more likely itis to be chosen.0102030405060708090100102550906040201 2 3 4 5 6 7noclausesvalueFigure 7: Rule length value functiondataset.
Some supervised learning approach usemanually labeled test sets to evaluated their sys-tems.
However, these are small test sets, for exam-ple, (Knight & Marcu, 2002) use a set of 1035 sen-tences to train the system and only 32 sentencesto test it, which is a quite small test set.
As aconsequence, it is also important to propose morethrough evaluation.
In order to assess as clearlyas possible the performance of our methodologyon large datasets, we propose a set of qualitativeand quantitative evaluations based on three differ-ent measures: Utility, Ngram simplification andCorrectness.5.1 EvaluationA relevant issue, not very commonly discussed, isthe Utility of a learned theory.
In real life prob-lems, people may be more interested in the vol-ume of data processed than the quality of the re-sults.
Maybe, between a system which is 90%precise and processes only 10% of data, and a sys-tem with 70% precision, processing 50% of data,the user would prefer the last one.
The Utilitymay be a stronger than the Recall measure, usedfor the evaluation of supervised learning systems,because the later measures how many instanceswere well identified or processed from the test setonly, and the former takes into account the wholeuniverse.
For example, in a sentence compres-sion system, it is important to know how manysentences would be compressed, from the wholepossible set of sentences encountered in electronicnews papers, or in classical literature books, orboth.
This is what we mean here by Utility.The Ngram-Simplification methodology is anautomatic extrinsic test, performed to perceivehow much a given sentence reduction rulesetwould simplify sentences in terms of syntacticalcomplexity.
The answer is not obvious at firstsight, because even smaller sentences can contain20more improbable syntactical subsequences thantheir uncompressed versions.
To evaluate the syn-tactical complexity of a sentence, we use a 4 ?gram model and compute a relative11sequenceprobability as defined in Equation 12 where~W =[t1, t2, ..., tm] is the sequence of part-of-speechtags for a given sentence with size m.P{~W} =?m?nYk=nP{tk| tk?1, ..., tk?n}?1m(12)The third evaluation is qualitative.
We measurethe quality of the learned rules when applied tosentence reduction.
The objective is to assess howcorrect is the application of the reduction rules.This evaluation was made through manual annota-tion for a statistically representative random sam-ple of compressed sentences.
A human judgedthe adequacy and Correctness of each compres-sion rule to a given sentence segment, in a scalefrom 1 to 5, where 1 means that it is absolutely in-correct and inadequate, and 5 that the compressionrule fits perfectly to the situation (sentence) beinganalyzed.To perform our evaluation, a sample of 300 sen-tences were randomly extracted, where at least onecompression rule had been applied.
This eval-uation set may be subdivided into three subsets,where 100 instances came from rules with Z(X)=1 (BD1), 100 from rules with Z(X)= 2 (BD2),and the other 100 from rules with Z(X)= 3(BD3).
Another random sample, also with 100cases has been extracted to evaluate our base-line(BL) which consists in the direct application ofthe bubble set to make compressions.
This meansthat no learning process is performed.
Instead, westore the complete bubble set as if they were rulesby themselves (in the same manner as (Le Nguyen& Ho, 2004) do).Table 1 compiles the comparative resultsfor Correctness, Precision, Utility and Ngram-simplification for all datasets.
In particular,Ngram-simplification in percentage is the pro-portion of test cases where P{reduced(~W )} ?P{~W}.Table 1 provides evidence of the improvementachieved with the induction rules, in comparisonwith the base line, on each test parameter: Cor-rectness, Utility and Ngram-simplification.
Con-11Because it is raised to the inverse power of m, which isthe number of words in the sentence.Parameter BL BD1 BD2 BD3Correctness: 2.93 3.56 4.03 4.01Precision: 58.60% 71.20% 80.60% 80.20%Utility: 8.65% 32.67% 85.72% 26.86%NG-Simpl: 47.39% 89.33% 90.03% 89.23%Table 1: Results with Four Evaluation Parameters.sidering the three experiences, BD1, BD2, andBD3, as a unique evaluation run, we obtained amean Correctness quality of 3.867 (i.e.
77.33%Precision), a mean Utility of 48.45%, and a meanNgram-simplification equal to 89.53%, which aresignificantly better than the base line.Moreover, best results overall are obtained forBD2 with 80.6% Precision, 85.72% Utility and90.03% Ngram-simplification which means thatwe can expect a reduction of two words with highquality for a great number of sentences.
In partic-ular, Figure 2 shows examples of learned rules.5.2 Time ComplexityIn the earlier12days of ILP, the computation timespent by their systems was a serious difficult ob-stacle, disabling its implementation for real lifeproblems.
However, nowadays these time ef-ficiency issues have been overcome, opening awide range of application possibilities, for manyproblems, from Biology to Natural Language Pro-cessing.
The graph in figure 8, shows that evenwith considerable big datasets, our learning sys-tem (based on Aleph) evidences acceptable feasi-ble computation time.02040608010012014012270530120010 20 30 40 50 60103bubblessecondsFigure 8: Time spent during the induction process,for datasets with size expressed in thousands ofbubbles.To give an idea about the size of an inducedrule set, and taking as an example the learned rules12In the 1990-2000 decade.21with Z(X)= 2, these were learned from a datasetcontaining 37271 t(2, 0) bubbles, and in the final5806 sentence reduction rules were produced.6 Conclusion and Future DirectionsSentence Compression is an active research topic,where several relevant contributions have recentlybeen proposed.
However, we believe that manymilestones still need to be reached.
In this pa-per, we propose a new framework in the form ofa pipeline, which processes huge sets of web newsarticles and retrieves compression rules in an un-supervised way.
For that purpose, we extract andalign paraphrases, explore and select specific textcharacteristics called bubbles and finally induce aset of logical rules for sentence reduction in a real-world environment.
Although we have only con-sidered bubbles having Z(X)?
3, a sentence mayhave a compression length greater than this value,since several compression rules may be applied toa single sentence.Our results evidence good practical applicabil-ity, both in terms of Utility, Precision and Ngram-simplification.
In particular, we assess results upto 80.6% Precision, 85.72% Utility and 90.03%Ngram-simplification for reduction rules of twoword length.
Moreover, results were compared toa base line set of rules produced without learningand the difference reaches a maximum improve-ment using Inductive Logic Programming of 22%.AcknowledgmentsThis work was supported by the VIPACCESSproject - Ubiquitous Web Access for Visually Im-paired People.
Funding Agency: Fundac?
?ao paraa Ci?encia e a Tecnologia (Portugal).
Reference:PTDC/PLP/72142/2006.ReferencesBarzilay R. and Lee L.. 2003.
Learning to paraphrase:An unsupervised approach using multiple-sequencealignment.
In HLT-NAACL 2003: Main Proceed-ings, pages 16?23.Camacho R. 1994.
Learning stage transition rules withIndlog.
Gesellschaft f?ur Mathematik und Datenver-arbeitung MBH., Volume 237 of GMD- Studien, pp.273-290.Clarke J., and Lapata M. 2006.
Constraint-based Sen-tence Compression: An Integer Programming Ap-proach.
21st International Conference on Compu-tational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics.Cordeiro J. and Dias G. and Cleuziou G. 2007.
Bi-ology Based Alignments of Paraphrases for Sen-tence Compression.
In Proceedings of the Workshopon Textual Entailment and Paraphrasing (ACL-PASCAL / ACL2007), Prague, Czech Republic.Cordeiro J. and Dias G. and Brazdir P. October2007.
New Functions for Unsupervised Asymmet-rical Paraphrase Detection.
In Journal of Software.,Volume:2, Issue:4, Page(s): 12-23.
Academy Pub-lisher.
Finland.
ISSN: 1796-217X.Dolan W.B.
and Quirck C. and Brockett C. 2004.
Un-supervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
In Pro-ceedings of 20th International Conference on Com-putational Linguistics (COLING 2004).Knight K. and Marcu D. 2002.
Summarization be-yond sentence extraction: A probabilistic approachto sentence compression.
Artificial Intelligence,139(1):91-107.Muggleton S. 1991.
Inductive Logic Programming.New Generation Computing, 8 (4):295-318.Muggleton S. 1996.
Learning from positive data.
Pro-ceedings of the Sixth International Workshop on In-ductive Logic Programming (ILP-96), LNAI 1314,Berlin, 1996.
Springer-Verlag.Muggleton S. 1999.
Inductive logic programming: Is-sues, results and the challenge of learning languagein logic.
Artificial Intelligence, 114 (1-2), 283?296.Le Nguyen M., Horiguchi S., A. S., and Ho B. T. 2004.Example-based sentence reduction using the hiddenmarkov model.
ACM Transactions on Asian Lan-guage Information Processing (TALIP), 3(2):146-158.Needleman SB, Wunsch CD.
1970.
A general methodapplicable to the search for similarities in the aminoacid sequence of two proteins.
Journal of MolecularBiology, 48 (3): 443?53.Quinlan J. R. 1990.
Learning Logical Deinitions fromRelations.
Machine Learning., 5 (3), 239-266.
33,39, 41.Smith TF, Waterman MS. 1981.
Identification of Com-mon Molecular Subsequences.
Journal of Molecu-lar Biology, 147: 195?197.Srinivasan A.
2000.
The Aleph Manual, TechnicalReport.
Computing Laboratory, Oxford University,UK.Turner J, Charniak E. 2005.
Supervised and Unsuper-vised Learning for Sentence Compression.
Proceed-ings of the 43rd Annual Meeting of the ACL, pages290-297.22
