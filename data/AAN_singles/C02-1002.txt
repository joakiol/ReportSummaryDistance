A cheap and fast way to build useful translation lexiconsDan TUFISRomanian Academy Centre for Artificial Intelligence13, ?13 Septembrie?Bucharest, ROMANIA, RO-74311tufis@racai.roAbstractThe paper presents a statistical approach toautomatic building of translation lexiconsfrom parallel corpora.
We briefly describethe pre-processing steps, a baseline iterativemethod, and the actual algorithm.
Theevaluation for the two algorithms ispresented in some detail in terms ofprecision, recall and processing time.
Weconclude by briefly presenting some of ourapplications of the multilingual lexiconsextracted by the method described herein.IntroductionThe scientific and technological advancement inmany domains is a constant source of new termcoinage and therefore keeping up withmultilingual lexicography in such areas is verydifficult unless computational means are used.Translation lexicons, based on translationequivalence relation are lexical knowledgesources, which can be extracted from paralleltexts (even from comparable texts), with verylimited human resources.
The translationlexicons appear to be quite different from thecorresponding printed lexicons, meant for thehuman users.
There are well known reasons forthese differences and we will not discuss theissue here, but exactly these differences makethem very useful (in spite of inherent noisecontent) in many computer-based applications.We will discuss some of our experiments basedon automatically extracted multilingual lexicons.Most modern approaches to automatic extractionof translation equivalents rely on statisticaltechniques and roughly fall into two categories.The hypotheses-testing methods such as Galeand Church (1991), Smadja et al (1996),Tiedmann (1998), Ahrenberg (2000), Melamed(2001) etc.
use a generative device that producesa list of translation equivalence candidates(TECs), extracted from corresponding segmentsof the parallel texts (translation units-TU), eachof them being subject to an independencestatistical test.
The TECs that show anassociation measure higher than expected underthe independence assumption are assumed to betranslation-equivalence pairs (TEPs).
The TEPsare extracted independently one of another andtherefore the process might be characterized as alocal maximization (greedy) one.
The estimatingapproach such as Brown et al (1993), Kay andR?scheisen (1993), Kupiec (1993), Hiemstra(1997) etc.
is based on building from data astatistical bitext model, the parameters of whichare to be estimated according to a given set ofassumptions.
The bitext model allows for globalmaximization of the translation equivalencerelation, considering not individual translationequivalents but sets of translation equivalents(sometimes called assignments).
There are prosand cons for each type of approach, some ofthem discussed in Hiemstra (1997).Our translation equivalents extraction processmay be characterized as a ?hypotheses testing?approach and does not need a pre-existingbilingual lexicon for the considered languages.
Ifsuch a lexicon exists it can be used to eliminatespurious candidate translation equivalence pairsand thus to speed up the process and increase itsaccuracy.1 Assumptions, preprocessing and a baselineThere are several underlying assumptions onemay consider in keeping the computationalcomplexity of a translation lexicon extractionalgorithm as low as possible.
None of thesehopotheses is true in general, but the situationswhere they are not observed are rare enough sothat ignoring the exceptions would not produce asignificant number of errors and would not losetoo many useful translations.
The assumptionswe made were the following:?
a lexical token in one half of the translationunit (TU) corresponds to at most one non-emptylexical unit in the other half of the TU; this is the1:1 mapping assumption which underlines thework of many other researchers (Ahrenberg et al(2000), Brew and McKelvie (1996), Hiemstra(1996), Kay and R?scheisen (1993), Tiedmann(1998), Melamed (2001) etc);?
a polysemous lexical token, if used severaltimes in the same TU, is used with the samemeaning; this assumption is explicitly used byGale and  Church (1991), Melamed (2001) andimplicitly by all the previously mentionedauthors;?
a lexical token in one part of a TU can bealigned to a lexical token in the other part of theTU only if the two tokens have compatible types(part-of-speech); in most cases, compatibilityreduces to the same POS, but it is also possibleto define other compatibility mappings (e.g.participles or gerunds in English are quite oftentranslated as adjectives or nouns in Romanianand vice-versa);?
although the word order is not an invariant oftranslation, it is not random either (Ahrenberg etal (2000)); when two or more candidatetranslation pairs are equally scored, the onecontaining tokens which are closer in relativeposition are preferred.The proper extraction of translationequivalents requires special pre-processing:?
sentence alignment; we used a slightlymodified version of CharAlign described byGale and Church (1993) .?
tokenization; the segmenter we used (MtSeg,developed by P. di Cristo for the MULTEXTproject: http://www.lpl.univ-aix.fr/projects/multext/MtSeg/), may process multiword expressions assingle lexical tokens.
The segmenter comes withtokenization resources for several WesternEuropean languages, further enhanced in theMULTEXT-EAST project (Dimitrova et al(1998), Erjavec et al(1998), Tufis et al(1998))with corresponding resources for Bulgarian,Czech, Estonian, Hungarian, Romanian andSlovene.?
tagging and lemmatization; we used a tieredtagging with combined language modelsapproach (Tufis (1999, 2000)) based on aBrants?s TnT tagger.After the sentence alignment, tagging andlemmatization, the first step is to compute a listof translation equivalence candidates (TECL).This list contains several sub-lists, one for eachPOS considered in the extraction procedure.Each POS-specific sub-list contains several pairsof tokens <tokenS tokenT> of the correspondingPOS that appeared in the same TUs.
TECLcontains a lot of noise and many TECs are veryimprobable.
In order to eliminate much of thisnoise, the most unlikely candidates are  filteredout of  TECL.
The filtering is based on scoringthe association between the tokens in a TEC.For the ranking of the TECs and their filteringwe experimented with 4 scoring functions: MI(pointwise mutual information), DICE, LL(loglikelihood), and ?2 (chi-square).
Afterempirical tests we decided to use LL test withthe threshold value set to 9.Our baseline algorithm, BASE, is a very simpleiterative algorithm, very fast and can beenhanced in many ways.
It has some similaritiesto the iterative algorithm presented in Ahrenberget al(2000) but unlike it, our algorithm avoidscomputing various probabilities (or better saidprobability estimates) and scores (t-score).
Ateach iteration step, the pairs that pass theselection  (see below) will be removed fromTECL so that this list is shortened after each stepand eventually may be emptied.
Based onTECL, for each POS a Sm*Tn contingency tableis constructed, with Sm the number of tokentypes in the first part of the bitext (call it source)and Tn the number of token types in the otherpart of the bitext (call it target).
Source tokentypes index the rows of the table and the targettoken types (of the same POS) index thecolumns.
Each cell (i,j) contains the number ofoccurrences in TECL of the <TSi, TTj> TEC.Equations below express the selection condition:{ }3n               )2(&)n(n)n(n qp, | TTTP (1)ijpj ijiq ijTj Sik????
?><=This is the key idea of the iterative extractionalgorithm: it expresses the requirement that inorder to select a TEC <TSi, TTj> as a translationequivalence pair, the number of associations ofTSi with TTj must be higher than (or at least equalto) any other TTp (p?j).
The same holds for theother way around.
All the pairs selected in TPkare removed (the respective counts are zeroed).If TSi is translated in more than one way the restof translations will be found in subsequent steps(if frequent enough).
The most used translationof a token TSi will be found first.
The equation(2) represents a frequency relevance threshold,necessary in order to diminish the influence ofdata sparseness.2 An improved algorithm (BETA)One of the main deficiencies of the BASEalgorithm is that it is sensitive to what Melamed(2001) calls indirect associations.
If <TSi, TTj>has a high association score and TTj collocateswith TTk, it might very well happen that <TSi,TTk> gets also a high association score.Although, as observed by Melamed (2001), ingeneral, the indirect associations have lowerscores than the direct (correct) associations, theycould receive higher scores than many correctpairs and this will not only generate wrongtranslation equivalents, but will eliminate fromfurther considerations several correct pairs,deteriorating the procedure?s recall.
To weakenthis sensitivity, the BASE algorithm had toimpose that the number of occurrences of a TECbe at least 3, thus filtering out more than 50% ofall the possible TECs.
Still, because of theindirect association effect, in spite of a verygood precision (more than 98%) out of theconsidered pairs another approximately 50%correct pairs were missed.
The BASE algorithmhas this deficiency because it looks on theassociation scores globally, and does not checkwithin the TUs if the tokens making the indirectassociation are still there.To diminish the influence of the indirectassociations and consequently removing thefrequency threshold, we modified the BASEalgorithm so that the maximum score is notconsidered globally but within each of the TUs.This brings BETA closer to the competitivelinking algorithm described in Melamed (2001).The competing pairs are only the TECsgenerated from the current TU and the one withthe best score is the first selected.
Based on the1:1 mapping hypothesis, any TEC containing thetokens in the winning pair are discarded.
Then,the next best scored TEC in the current TU isselected and again the remaining pairs thatinclude one of the two tokens in the selected pairare discarded.
The multiple-step control inBASE, where each TU was scanned severaltimes (equal to the number of iteration steps) isnot necessary anymore.
The BETA algorithmwill see each TU unit only once but the TU isprocessed until no further TEPs can be reliablyextracted or TU is emptied.
This modificationimproves both the precision and recall incomparison with the BASE algorithm.
Inaccordance with the 1:1 mapping hypothesis,when two or more TEC pairs of the same TUshare the same token and they are equallyscored, the algorithm has to make a decision andchoose only one of them.
If there exists a seedlexicon and one of the competitors is in thislexicon it will be the winner.
Otherwise,decision is made based on two heuristics: stringsimilarity scoring and relative distance.The similarity measure we used, COGN(TS, TT),is very similar to the XXDICE score described inBrew and McKevie (1996).The threshold for the COGN(TS, TT) test wasempirically set to 0.42.
This value depends onthe pair of languages in the considered bitext.The actual implementation of the COGN testconsiders a language dependent normalizationstep, which strips some suffixes, discards thediacritics and reduces some consonant doublingetc.
This normalization step was hand written,but, based on available lists of cognates, it couldbe automatically induced.The second filtering condition, DIST(TS, TT) isbased on the difference between the relativepositions in the TU of the TS and TTrespectively.
The threshold for the DIST(TS, TT)was set to 2.The COGN(TS, TT) filter is stronger thanDIST(TS, TT), so that the TEC with the highestsimilarity score is the preferred one.
If thesimilarity score is irrelevant, the weaker filterDIST(TS, TT) gives priority to the pairs with thesmallest relative distance between theconstituent tokens.3 BASE and BETA EvaluationsWe conducted experiments on the "1984"multilingual corpus (Dimitrova et al(1998))containing 6 translations of the English original.This corpus was developed within the Multext-East project, published on a CD-ROM (Erjavecet al(1998)) and recently improved within theCONCEDE project.
The newer version isdistributed by TRACTOR (www.tractor.de).Each monolingual part of the corpus (Bulgarian-Bg, Czech-Cz, Estonian-Et, Hungarian-Hu,Romanian-Ro and Slovene-Si) was tokenized,lemmatized, tagged and sentence aligned to theEnglish hub.The evaluation protocol specified that all thetranslation pairs be judged in context, so that ifone pair is found to be correct in at least onecontext, then it should be judged as correct.
Theevaluation was done for both BASE and BETAalgorithms but on different scales.
The BASEalgorithm was run on all the 6 bitexts with theEnglish hub and native speakers of the secondlanguage in the bitexts (with good command ofEnglish) validated 4 of the 6 bilingual lexicons.The lexicons contained all parts of speechdefined in the MULTEXT-EAST lexiconspecifications (Erjavec et al(1998)) except forinterjections, particles and residuals.The BETA algorithm was ran on the Romanian-English bitext, but at the time of this writing theevaluation was finalized only for the nominaltranslation pairs.3.1 BASE EvaluationFor validation purposes we limited the number ofiteration steps to 4.
The extracted dictionariescontain adjectives (A), conjunctions (C),determiners (D), numerals (M), nouns (N),pronouns (P), adverbs (R), prepositions (S) andverbs (V).
The precision (Prec) was computed asthe number of correct TEPs divided by the totalnumber of extracted TEPs.
The recall (consideredfor the non-English language in the bitext) wascomputed two ways: the first one, Rec*, whichtook into account only the tokens processed bythe algorithm (those that appeared at least threetimes).
The second one, Rec, took into accountall the tokens irrespective of their frequencycounts.
Rec* is defined as the number of sourcelemma types in the correct TEPs divided by thenumber of lemma types in the source languagewith at least 3 occurrences.
Rec is defined as thenumber of source lemma types in the correctTEPs divided by the number of lemma types inthe source language.The rationale for showing Rec* is to estimate theproportion of the missed considered tokens.
Thismight be of interest when precision is of utmostimportance.
When the threshold of minimal 3occurrences is considered, the algorithmprovides a high precision and a good recall(Rec*).
The evaluation was fully done for Et, Huand Ro and partially for Si (the first step wasfully evaluated while the rest were evaluatedfrom randomly selected pairs).The results after 4 iteration steps are shown inthe table below for the Et-En, Hu-En, Ro-En andSi-En lexicons.
From the 6 bilingual lexicons wealso derived a 7-language lexicon (2862 entries),with English as a search hub (seehttp://www.racai.ro/~tufis/BilingualLexicons/AutomaticallyExtractedBilingualLexicons.html).Et-En Hu-En Ro-En Si-EnEntriesPrec/Rec*Rec191196.2/57.918.8193596.9/56.919.3222798.4/58.825.2164698.7/57.922.7Table 1: BASE evaluation for all POS and 4iteration stepsTo facilitate the comparison with the evaluationof the BETA algorithm we ran the BASEalgorithm for extracting the noun translation pairsfrom the Romanian-English bitext.
The nounextraction had the second worst accuracy (theworst was the adverb), and therefore weconsidered that an in-depth evaluation of thiscase would be more informative than a globalevaluation.
We set no limit for the number ofsteps and lowered the occurrence threshold to 2.The program stopped after 10 steps with anumber of 1900 extracted translation pairs, out ofwhich 126 were wrong.
Compared with the 4steps run the precision decreased to 93.4%, butboth Rec* (70.1%) and Rec  (39.8%) improved.In the 10-step run of the BASE algorithm, theextracted noun pairs covered 85.83% of thenouns in the Romanian part of the bitext.We should mention that in spite of the generalpractice in computing recall for bilingual lexiconextraction task (be it Rec* or Rec) this is only anapproximation of the real recall.
The reason forthis approximation is that in order to computethe real recall one should have a gold standardwith all the words aligned by human evaluators.In general such a gold standard bitext is notavailable and the recall is either approximated asabove, or is evaluated on a small sample and theresult is taken to be more or less true for all thebitext.3.2   BETA EvaluationThe BETA algorithm preserves the simplicity ofthe BASE algorithm but it significantlyimproves its recall (Rec) at the expense of someloss in precision (Prec).
Its evaluation was donefor the Romanian-English bitext, without a seedlexicon and only with respect to the lexicon ofnouns.
The filtering condition in case of ties wasthe following:max(COGN(TjS,TjT)?0.4)?min(DIST(TjS,TjT)?2)The results show that the Rec (72.7%) almostdoubled compared with the best Rec obtained bythe BASE algorithm for nouns (39.9%).However, the price for these significantimprovements was a serious deterioration of thePrec (78.3% versus 93.4%).Noun types in text 3435No.
entries 4023Correct entries 3149Types in correct entries 2496Prec/Rec 78.3/72.7Table2: BETA evaluation for the Ro-EN lexiconof nouns; both COGN and DIST filters usedThe analysis of the wrong translation pairsrevealed that most of them were hapax pairs(pairs appearing only once) and they wereselected because the DIST measure enabledthem, so we considered that this filter is notdiscriminative enough for hapaxes.
On the otherhand for the non-hapax pairs the DIST conditionwas successful in more than 85% of the cases.Therefore, we decided that the additional DISTfiltering condition be preserved for non-hapaxcompetitors only.Although 166 erroneous TEPs were removed,144 good TEP were lost.
Prec improved  (81.0%versus 78.3%) but Rec depreciated (69.0%versus 72.7%).The BASE algorithm allows for trading offbetween Prec and Rec by means of the numberof iteration steps.Noun types in text 3435No.
entries 3713Correct entries 3007Types in correct entries 2371Prec/Rec 81.0/69.0Table3: BETA evaluation for the Ro-ENlexicon of nouns; only COGN filter usedThe BETA algorithm allows for similar tradingoff between Prec and Rec by means of theCOGN and DIST thresholds and obviously bymeans of an occurrence threshold.
For instancewhen BETA was set to ignore the hapax pairs,its Prec was 96.1% (better then the BASEprecision 93.4%) Rec* was 96.4% (BASE with10 iterations had a Rec* of 70.1%) and Rec was60.0% (BASE with 10 iterations had a Rec of39.8%).4 Partial translationsAs the alignment model used by the translationequivalence extraction is based on the 1:1mapping hypothesis, inherently it will findpartial translations for those cases where one ormore words in one language must be translatedby two or more words in the other language.Although we used a tokenizer aware ofcompounds in the two languages, its resourceswere obviously partial.
In the extracted nounlexicon, the evaluators found 116 partialtranslations (3.86%).
In this section we willdiscuss one way to recover the correcttranslations for the partial ones, discovered byour 1:1 mapping-based extraction program.First, from each part of the bitext a set ofpossible collocations was extracted by a simplemethod called ?repeated segments?
analysis.Any sequence of two or more tokens thatappears more than once is retained.Additionally, the tags attached to the wordsoccurring in a repeated segment must observethe syntactic patterns characterizing most of thereal collocations.
For the noun lexicon weconsidered only forms of <head-noun(functional_word) modifier> as Romanianpatterns and <modifier (functional_word) head-noun> as English patterns.
If all the contentwords contained in a repeated segment havetranslation equivalents, then the repeatedsegment is discarded as not being relevant for apartial translation.
Otherwise, the repeatedsegment is stored in the lexicon as a translationfor the translation of its head-noun.
This simpleprocedure managed to recover 62 partialtranslations and improve other 12 (still partial,but better).5 ImplementationThe extraction programs, both BASE andBETA, are written in Perl and run underpractically any platform (Perl implementationsexist not only for UNIX/LINUX but also forWindows, and MACOS).
Although, as onereviewer rightfully noticed, the speed is notreally relevant for such an algorithm, evaluationof the current speed shows that, the approachbeing computationally very cheap,  there is roomfor adding more sophisticated ?associationfunctions?
without too much concern for theoverall response time.
Table 4 shows the BASErunning time for each bitext in the "1984"parallel corpus (4 steps, all POS considered).Bitext Bg-EnCz-EnEt-EnHu-EnRo-En4         28stepsSi-EnExtractiontime (sec)181 148 139 220 183 415 157Table 4:BASE extraction time for each of thebilingual lexicons (all POS)The running time for extraction of the nounRomanian-English lexicon (Cygwin UNIXemulator for Windows on a PII/233Mhz with 96MB RAM) for BASE was 103 seconds while forBETA was 234 seconds.A quite similar approach to our BASE algorithm(also implemented in Perl) is presented inAhrenberg et al(2000) and for a novel of abouthalf the length of Orwell's "1984" theiralgorithm needed 55 minutes on a Ultrasparc1Workstation with 320 MB RAM.
They used afrequency threshold of 3 and the best resultsreported are 92.5% precision and 54.6% recall(our Rec*).
For a computer manual containingabout 45% more tokens than our corpus, theiralgorithm needed 4.5 hours with the best resultsbeing 74.94% precision and 67,3% recall (Rec*).The BETA algorithm is closer to Melamed?sextractor, although our program is greedier andnever returns to a visited translation unit.6 Applications and further workWe used the multilingual lexicon, mentionedbefore, for a sense discrimination exercisedescribed in Erjavec et al(2001) where thecriterion for sense clustering was the way thedifferent occurrences of an English word in the?1984?
parallel corpus were translated in theother 6 languages.
The experiment carried oninvolved 91 highly ambiguous English nounsand was extremely encouraging; new results aredescribed in Erjavec&el all (2002).Another application of the translation lexiconswas in the BALKANET project aimed atdeveloping wordnets for Balkan languages,Romanian included.
The translation lexiconswere used both in building from scratch, but in aharmonized way, the synsets for the baseconcepts and also for cross-lingual validation onrunning text (this was again the ?1984?
novel) ofthe interlingual index (ILI) mapping of thesebasic concepts.
Considering that 4 languages inthe BALKANET are represented in the ?1984?parallel corpus we plan to take advantage of theILI mapping for further refinement of the word-sense discrimination method mentioned aboveand add cluster labeling.
The obvious languageindependent labeling is based on ILI-recordnumbers.The experiments reported here were evaluatedon European language.
A new experiment hasbeen preliminarily evaluated for an extract of500 sentences Chinese-English form a parallelcorpus of juridical texts.
The experiment wasfocused on noun translations extraction, used anLL-score threshold set to 9 and no conflictresolution method for the competitivetranslations.
We had two result sets:RS1: contains translations which haven?tcompetitors (that is whenever there werecompeting translations for the same wordnone of them was selected)RS2:  differs from DS1 by the inclusion inthe output lexicon of all the competingtranslations.It is obvious that if 1:1 mapping hypothesis istrue, for any competing translations included inRS2 only 1 is correct and all the others areerrors.
Therefore the precision for RS2 is muchless than for RS1.The results of this experiment are shown inTable 5 and they show that without making adecision on the competing translations we eitherloose many good translations (RS1) or include alot of noise (RS2).Result set # extr.
pairs precision recallRS1 187 93.04% 33.6%RS2 545 49.9% 98.1%Table 5: BETA results for CN-EN experimentFurther work will address the issue of definingadequate heuristics for filtering out competingcandidates.AcknowledgementsOur thanks go to Chang Baobao who evaluatedthe Chinese-English results as well as toWolfgang Teubert who launched the CN-ENexperiment in the framework of the Europeanconcerted action TELRI.ReferencesAhrenberg, L., M. Anderson, M. Merke (2000) Aknowledge-lite approach to word alignment, In?Parallel Text Processing?.
V?ronis, J.
(ed).
Text,Speech and Language Technology Series, KluwerAcademic Publishers, pp.
97-116Brew, C., McKelvie, D. (1996) Word-pair extractionfor lexicography http:///www.ltg.ed.ac.uk/~chrisbr/papers/nemplap96Brown, P., Pietra, S. A. Della Pietra, V. J. DellaPietra, and R. L. Mercer (1993).
The mathematicsof statistical machine translation: parameterestimation.
In Computational Linguistics 19/2, pp.263-311.Dimitrova, L, T. Erjavec, N. Ide, H. Kaalep, V.Petkevic, D. Tufis (1998) Multext-East: Paralleland Comparable Corpora and Lexicons for SixCentral and East European Languages.
InProceedings of COLING, Montreal, Canada, pp.315-319.Gale, W., K.W.
Church (1991) Identifying wordcorrespondences in parallel texts.
In Proceedingsof the 4th DARPA Workshop on Speech andNatural Language,  pp.
152-157.Gale, W.A., K.W.
Church (1993) A Program forAligning Sentences in Bilingual Corpora inComputational Linguistics, 19/1, pp.
75-102Erjavec, T., Lawson A., Romary, L. (1998) East MeetWest: A Compendium of Multilingual Resources.TELRI-MULTEXT EAST CD-ROM, ISBN: 3-922641-46-6.Erjavec T., Ide N., Tufis, D.(2001) Automatic SenseTagging Using Parallel Corpora.
In Proceedingsof the 6th Natural Language Processing Pacific RimSymposium, Tokyo, Japan,  pp.
212-219Hiemstra, D. (1997) Deriving a bilingual lexicon forcross language information retrieval.
InProceedings of Gronics, pp.
21-26Kay, M., R?scheisen M. (1993) Text-TranslationAlignment.
In Computational Linguistics, 19/1, pp.121-142Kupiec, J.
(1993) An algorithm for finding nounphrase correspondences in bilingual corpora.
InProceedings of the 31st Annual Meeting of theACL, pp.
17-22Melamed, D (2001) Empitical ethods for ExploitingParallel Texts.
The MIT Press.
Cambridge,Massachusetts, London, England, 195 p.Smadja,F., K. R. McKeown, V. Hatzivassiloglou(1996) Translating collocations for bilinguallexicons: A statistical approach.
In ComputationalLinguistics, 22/1, pp.
1-38Tiedemann, J.
(1998) Extraction of TranslationEquivalents from Parallel Corporahttp://stp.ling.uu.se /~joergTufis, D.(1999) Tiered Tagging and CombinedClassifiers.
In Text, Speech and Dialogue, F.Jelinek, E. N?th (eds), Lecture Notes in ArtificialIntelligence 1692, Springer,  pp.
29-33Tufis, D., Ide, N. Erjavec, T (1998) StandardizedSpecifications, Development and Assessment ofLarge Morpho-Lexical Resources for Six Centraland Eastern European Languages.
In Proceedingsof LREC, Granada, Spain,   pp.
233-240Tufis, D.(2000) Using a Large Set of Eagles-compliant Morpho-Syntactic Descriptors as aTagset for Probabilistic Tagging.
In Proceedings ofthe LREC, Athens, Greece, pp.
1105 -1112Tufis, D., Barbu, A.
(2001) Extracting multilinguallexicons from parallel corpora.
In Proceedings ofthe ACH/ALLC, New York University, pp.122-124Tufis D., Barbu A.
(2001) Computational BilingualLexicography: Automatic Extraction of TranslationDictionaries.
In International Journal on Scienceand Technology of Information, RomanianAcademy, ISSN 1453-8245, 4/3-4,  pp.325-352.Erjavec T., Ide N., Tufis D. (2002) Sensediscrimination with Parallel Corpora.
Proceedingsof the SIGLEX Workshop on Word SenseDisambiguation: Recent Successes and FutureDirections.
ACL2002, July, Philadelphia
