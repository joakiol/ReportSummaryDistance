Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 831?839,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTopic Models for Image Annotation and Text IllustrationYansong Feng and Mirella LapataSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKY.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractImage annotation, the task of automaticallygenerating description words for a picture,is a key component in various image searchand retrieval applications.
Creating imagedatabases for model development is, however,costly and time consuming, since the key-words must be hand-coded and the processrepeated for new collections.
In this workwe exploit the vast resource of images anddocuments available on the web for develop-ing image annotation models without any hu-man involvement.
We describe a probabilisticmodel based on the assumption that imagesand their co-occurring textual data are gener-ated by mixtures of latent topics.
We show thatthis model outperforms previously proposedapproaches when applied to image annotationand the related task of text illustration despitethe noisy nature of our dataset.1 IntroductionRecent years have witnessed the rapid growth of im-age collections available for searching and browsingover the Internet.
Although image search engines arestill in their infancy, initial research suggests that thedeployed algorithms are not very accurate (Hawkinget al, 1999).
Given a query, search engines retrieverelevant pictures by analyzing the image caption (ifit exists), textual descriptions found adjacent to theimage, and other text-related factors such as the filename of the image.
However, since they do not an-alyze the actual content of the images, search en-gines cannot be used to retrieve pictures from unan-notated collections.
The ability to perform the an-notation task automatically would be of significantpractical import for many image-based applications.Besides search and retrieval, other examples includebrowsing support (e.g., by clustering images intogroups that are visually and semantically coherent)and story picturing (i.e., automatically suggestingimages to illustrate text).Automatic image annotation is a popular task incomputer vision; a large number of approaches havebeen proposed in the literature under many distinctlearning paradigms.
These range from supervisedclassification (Smeulders et al, 2000; Vailaya et al,2001) to instantiations of the noisy-channel model(Duygulu et al, 2002), to clustering (Barnard et al,2002), and methods inspired by information retrieval(Feng et al, 2004; Lavrenko et al, 2003).
Despitedifferences in application and formulation, all thesemethods essentially attempt to learn the correlationbetween image features and words from examplesof annotated images.
The Corel database has beenextensively used as a testbed for the developmentand evaluation of image annotation models.
It is acollection of stock photographs, divided into themes(e.g., tigers, sunsets) each of which are associatedwith keywords (e.g., sun, sea) that are consideredappropriate descriptors for all images belonging tothe same theme.Unfortunately, the Corel database is not represen-tative of the size or content of real-world image col-lections.
It has a small number of themes with manyclosely related images which in turn share keyworddescriptions.
It is therefore relatively easy to learnthe associations between images and keywords anddo well on annotation and retrieval tasks (Tang andLewis, 2007; Westerveld and de Vries, 2003).
Anappealing alternative is the use of resources whereimages and their annotations co-occur naturally.
Ex-amples include images found in news documents,consumer photo collections, Wikipedia articles, il-lustrated stories and so on.
The key idea here is totreat the words in the surrounding text as annota-tions for the image.
These annotations are undoubt-831edly noisy, but plenty and cost-free.
Moreover, thecollateral text is often longer and more informativein comparison to the few keywords reserved for eachimage in Corel.In this paper we propose a probabilistic imageannotation model that learns to automatically labelimages under such noisy conditions.
We use thedatabase created in Feng and Lapata (2008) whichconsists of news articles, images, and their cap-tions.
Our model exploits the redundancy inherentin this multimodal dataset by assuming that imagesand their surrounding text are generated by a sharedset of latent variables or topics.
Specifically, we de-scribe documents and images by a common multi-modal vocabulary consisting of textual words andvisual terms (visiterms).
Due to polysemy and syn-onymy many words in this vocabulary will refer tothe same underlying concept.
Using Latent DirichletAllocation (LDA, Blei and Jordan 2003), a proba-bilistic model of text generation, we represent visualand textual meaning jointly as a probability distribu-tion over a set of topics.
Our annotation model takesthese topic distributions into account while findingthe most likely keywords for an image and its asso-ciated document.
We also show how the model canbe straightforwardly modified to perform automatictext illustration.1 The task is routinely performed bynews writers who often have to search large imagecollections in order to find suitable pictures for theirtext.
Here, the model takes a document as input andsuggests images that match its content.
Experimen-tal results on both tasks bring improvements overcompetitive models.2 Related WorkA variety of learning methods have been applied tothe image annotation task.
These generally fall un-der two broad categories.
Supervised methods de-fine annotation as a classification task, e.g., by as-suming a one-to-one correspondence between vo-cabulary words and classes or by grouping severalwords into a single class (see Chai and Hung 2008for an overview).
Unsupervised approaches attemptto discover the underlying connections between vi-sual features and words, typically by introducinglatent variables.
Standard latent semantic analysis(LSA) and its probabilistic variant (PLSA) havebeen applied to this task (Hofmann, 2001; Monayand Gatica-Perez, 2007; Pan et al, 2004).
More so-phisticated models estimate the joint distribution ofwords and regional image features, whilst treating1We use the terms ?text illustration?
and ?story picturing?interchangeably throughout the paper.annotation as a problem of statistical inference in agraphical model (Barnard et al, 2002; Blei and Jor-dan, 2003; Wang et al, 2009).Irrespectively of the underlying model or task athand, much work has focused how to best representthe visual and textual modalities in order to exploittheir synergy.
Several approaches attempt to renderimages more word-like, by reducing the dimension-ality of the image feature space (Bosch et al, 2008;Fei-Fei and Perona, 2005) or by learning a singlerepresentation for both visual and textual features(Monay and Gatica-Perez, 2007; Zhao and Grosky,2003).Our own work approaches the image annotation(and related story picturing) task from a slightly dif-ferent angle.
We train and test our model on im-ages that contain implicit (and thus noisy) annota-tions that have not been specifically created for ourtask.
On account of this, our model has access toknowledge sources other than the image and its key-words (i.e., the news article containing the imagewe wish to annotate).
In Feng and Lapata (2008)we addressed this problem with a modified ver-sion of the continuous relevance annotation model(Lavrenko et al, 2003).
Unlike other unsupervisedapproaches where a set of latent variables is in-troduced, each defining a joint distribution on thespace of keywords and image features, the relevancemodel captures the joint probability of images andannotated words directly, without requiring an inter-mediate clustering stage (i.e., each annotated imagein the training set is treated as a latent variable).
Wemodified this model so as to exploit the informa-tion present in the document in two ways.
First, inestimating the conditional probability of a keywordgiven an image, we also considered its likelihood inthe collateral document.
Secondly, we used an LDAmodel (trained on the document collection) to prunefrom the model?s output words that are not represen-tative of the document?s topics.The proposed approach differs from Fengand Lapata (2008) in three important respects:(a) document-based information is an integral partof our model as we predict caption words given theimage and its accompanying document (b) LDA isno longer a post-processing step; our model relies onLDA to infer meaningful topics that capture the co-occurrence of visual features and words; (c) beyondimage annotation, we show how the same frame-work can be applied to story picturing (Joshi et al,2006), a task which has received less attention in theliterature.In terms of model structure, Blei and Jordan832(2003) andMonay and Gatica-Perez (2007) are clos-est to our work.
The first model, known as corre-spondence LDA (CorrLDA), has been successfullyemployed for modeling annotated images in theCorel domain.
CorrLDA also uses the notion of topicto model the generation of images and their captions.In this model, the visual modality drives the defini-tion of the latent space to which the textual modalityis linked.
The second model is based on PLSA andlearns a representation similar to ours consisting oftextual and visual features.
It is also trained usingcaptioned images from the Corel database.
We workwith noisier and larger datasets.
Our model exploitsthe captions accompanying the images as well astheir surrounding documents.
As a result, we obtaina similar number of textual and visual words; theseare often imbalanced in the Corel database, wherevisual words are nearly 50 times more than textualwords.
The different nature of our data dictates theuse of a model where the visual and textual modal-ity are given equal importance in defining the latentspace.3 Problem FormulationIn this section we give a brief description of the im-age database we employ and also define the imageannotation and story picturing tasks we are attempt-ing here.
As mentioned earlier, we use the datasetcreated in Feng and Lapata (2008).2 It contains3,361 articles that have been downloaded from theBBC News website3.
Each article contains a newsimage which in turn is associated with a caption.The images are usually 203 pixels wide and 152 pix-els high.
The average caption length is 5.35 tokens,and the average document length 133.85 tokens.
Thecaptions vocabulary is 2,167 words and the docu-ment vocabulary is 6,253.
The vocabulary sharedbetween captions and documents is 2,056 words.In contrast to the Corel database, this dataset con-tains more complex images (with many objects) andhas a larger vocabulary (Corel?s vocabulary is ap-proximately 300 words).
An example of an abridgeddatabase entry is shown in Figure 1.Due to the non-standard nature of the databasewe assume that the caption and news article de-scribe the content of the image either directly or in-directly.
It also follows that we may not be able toname all objects depicted in the image.
Now, giventhese constraints our goal is twofold.
Firstly, we willperform image annotation.
Our model is trained on2Available from http://homepages.inf.ed.ac.uk/s0677528/data.html.3http://news.bbc.co.uk/A woman from EastSussex who bought anemu egg sold as a nov-elty food item on afarm on the Isle ofWight has managed tohatch it into a chick.
Osborne the emu will growGillian Stone, from to over 6ft tallBexhill, who breeds chickens, brought home three largegreen emu eggs from a holiday and put them in an incu-bator in her kitchen.
Two turned out to be infertile, butafter 52 days little Osborne hatchedTable 1: Each entry in the BBC News database contains adocument, an image, and its caption (shown in boldface).document-image-caption tuples like the one shownin Table 1.
During testing, we must infer the cap-tion for an image.
Secondly, we use the same datasetto perform automatic text illustration.
During train-ing, the model has access to the same collection ofimage-caption-document tuples.
During testing, weare given a document and must find the images thatbest illustrate it.4 Image and Document RepresentationWords and images represent distinct modalities, im-ages live in a continuous feature space, whereaswords are discrete.
Yet, both modalities on somelevel capture the same underlying concepts as theyare used to describe the same objects.
A commonfirst step to all previous methods is the segmenta-tion of the image into regions, using either a fixed-grid layout or an image segmentation algorithm.
Re-gions are usually described by a standard set of fea-tures including color, texture, and shape which aretreated as continuous vectors (e.g., Barnard et al2002; Blei and Jordan 2003) or in quantized form(e.g., Duygulu et al 2002).
Through this process,the low-level image features are made to resembleword-like units.Here, we go one step further and represent eachimage by a bag of visual words, thereby convert-ing visual features from a continuous onto a discretespace.
In order to do this we use the Scale InvariantFeature Transform (SIFT) algorithm (Lowe, 1999).The general idea behind the algorithm is to firstsample an image with the difference-of-Gaussianspoint detector at different scales and locations.
Im-portantly, this detector is, to some extent, invariant totranslation, scale, rotation and illumination changes.Each detected region is represented with a SIFT de-scriptor which is a histogram of edge directions at833different locations.
SIFT descriptors are quantizedusing the K-means clustering algorithm to obtain adiscrete set of visual terms (visiterms) which formour visual vocabulary Vocv.
Each entry in this vo-cabulary stands for a group of image regions whichare similar in content or appearance and assumed tooriginate from similar objects.
More formally, eachimage I is expressed in a bag-of-words format vec-tor, [wv1,wv2, ...,wvL ], where wvi = n only if I has nregions labeled with vi.Since visual and textual modalities have now thesame status?they are both represented as bags-of-words?we can also represent any image-caption-document tuple jointly as a mixed document dMix.The underlying assumption is that the two modali-ties express the same meaning which, as we explainbelow, can be operationalized as a probability distri-bution over a set of topics.5 ModelingLatent Dirichlet Allocation For ease of exposi-tion, we first describe the basics of Latent DirichletAllocation (LDA; Blei et al 2003), a probabilisticmodel of text generation and then move on to dis-cuss our models which make use of probabilities es-timated by LDA.LDA can be represented as a three level hierarchi-cal Bayesian model.
Given a corpus consisting of Mdocuments, Blei et al (2003) define the generativeprocess for a document d as follows:1.
Choose ?|??
Dir(?)2.
For n ?
1,2, ...,N :(a) Choose topic zn|??Mult(?
)(b) Choose a word wn|zn,?1:K ?Mult(?zn)The mixing proportion over topics ?
is drawn froma Dirichlet prior with parameters ?
whose role is tocreate a smoothed topic distribution.
Once ?
and ?are sampled, then each document is generated ac-cording to the topic proportions z1:K and word prob-abilities over topics ?.
The probability of a docu-ment d in a corpus is defined as:P(d|?,?)=Z?P(?|?)(N?n=1?zkP(zk|?)P(wn|zk,?
))d?Computing the posterior distribution P(?,z|d,?,?
)of the hidden variables given a document is in-tractable in general.
However, a variety of approx-imate inference algorithms have been proposed inthe literature including variational inference whichour model adopts (Blei et al, 2003).
In this case,training an LDA model on a document collectionwill give two sets of parameters, the word proba-bilities given topics, p(w|z1:K), and the topic pro-portions given documents, P(z1:K |d).
The latter aredocument-specific, whereas the former represent theset of topics learned from the document collection.Given a trained model, it is possible to do infer-ence on an unseen document dnew:p(w|dnew) ?K?k=1P(w|zk)?k?Kj=1 ?
j(1)where P(w|z1:K) are word probabilities over top-ics z1:K estimated during model training, and ?1:Kare variational Dirichlet parameters obtained duringinference on the new document (and can be consid-ered as the posteriors of topic proportions over doc-uments).Image Annotation In the standard image annota-tion setting, a hypothetical model is given an image Iand a set of keywordsW , and must find the subsetWI(WI ?W ) which appropriately describes image I:W ?I = argmaxWP(W |I) (2)The keywords are usually assumed to be condition-ally independent on each other, so Equation (2) sim-plifies to:W ?I = argmaxW?w?WP(w|I) (3)Each entry in our database is an image-caption-document tuple (I,C,D).
In this setting, we mustfind the subset of keywords WI which appropriatelydescribe image I with the help of the accompanyingdocument D:W ?I = argmaxWtP(Wt |I,D) (4)Here, Wt denotes a set of textual words (we use thesubscript t to discriminate from the visual wordswhich are not part of the model?s output).
We alsoassume that the keywords are conditionally indepen-dent of each other:W ?I =argmaxWtP(Wt |I,D)=argmaxWt?wt?WtP(wt |I,D) (5)Since I and D are represented jointly as the con-catenation of textual and visual terms, we may intu-itively simplify the problem and use the mixed doc-ument representation dMix directly in estimating theconditional probabilities P(wt |I,D):P(wt |I,D) ?
P(wt |dMix) (6)834Substituting Equation (6) into (5) yields:W ?I ?
argmaxWt?wt?WtP(wt |dMix) (7)As mentioned earlier, we assume that the image andits associated text are generated by a mixture oflatent topics which we infer using LDA.
Specifi-cally, we select the number of topics K and applythe LDA algorithm to a corpus consisting of doc-uments {dMix} in order to obtain the multimodalword distributions over topics P(w|z1:K), and theestimated posterior of the topic proportions overdocuments P(z1:K |dMix).
We infer the topic pro-portions P(z1:K |dMixnew) on a new document-imagepair dMixnew approximately using Equations (1)and (7):4W ?I ?
argmaxWt?wt?WtP(wt |dMix) (8)= argmaxWt?wt?WtK?k=1P(wt |zk)P(zk|dMix)?
argmaxWt?wt?WtK?k=1P(wt |zk)?k?Kj=1 ?
jwhere P(wt |zk) are obtained during training, and ?1:Kare inferred on the image-document test pair.However, note that for an unseen image dI and ac-companying document dD, the estimated topic pro-portions are solely based on variational inferencewhich is an approximate algorithm.
In order to ren-der the model more robust, we smooth the topic pro-portions P(z1:K |dMix) with probabilities based on asingle modality:P?
(z1:K |dMix) ?
(9)q1P(z1:K |dMix)+q2P(z1:K |dD)+q3P(z1:K |dI)where P(z1:K |dD) and P(z1:K |dI) are inferred on dDand dI , respectively, and q1, q2, q3 are smoothingparameters (which we tune experimentally on held-out data); q3 is a shorthand for (1?q1?q2).In sum, calculating P(Wt |I,D) boils down to es-timating the probabilities P(wt |dMix) according toEquations (8) and (9) which we obtain using theLDA model.
We train LDA on the document col-lection {dMix} and use inference to obtain the topicdistributions of unseen image-document pairs.
In theend, we obtain a ranked list of textual words wt , then-best of which are the annotations for image I.4During training, the model has access to all three elements(I,C,D), so the mixed document dMix is the concatenation ofthe visual terms and words in the caption and document.
Dur-ing testing, the model is given an image and its accompanyingdocument, so dMix contains words based on I and D, but notC.Text Illustration Previous text illustration modelsare based on Corel-like databases with manual im-age descriptions (Barnard and Forsyth, 2001; Bleiand Jordan, 2003) or instance-based learning usingcomplex learning schemes (Joshi et al, 2006).
Here,we present a relatively simple model, again underthe topic mixture framework.Given a test document D and a candidate imagedatabase I1...N with captionsC, we must find the im-age or images which best describe the document.We can simply compute the probability of each vi-sual term in the vocabulary given D by marginaliz-ing over the document topics z1:K :P(wv|D) = ?z1:KP(wv|zk)P(zk|dD) (10)where wv is a visual term and P(wv|zk) the probabil-ity of wv given topic zk (as estimated on the trainingset).Equation (10) delivers a ranked list of visual termsaccording to a given document.
We could multiplythese probabilities together mirroring Equation (7),however this is not reliable.
In contrast to textualwords, for which we may infer whether they arelinguistically meaningful (e.g., by resorting to theirpart of speech), there is no easy way of knowingwhich visual words are important.
Relying solely onfrequency is not reliable either, as frequent visitermsmay simply represent features common in all images(e.g., most images have some white color).
To avoida bias towards frequent but potentially irrelevant vi-sual words, we output a fixed number of visual termsand select the image with the highest overlap as thecorrect illustration.6 Experimental SetupIn this section we discuss our experimental designfor assessing the performance of the models pre-sented above.
We give details on our training pro-cedure and parameter estimation, describe our fea-tures, and present the baseline methods used forcomparison with our models.Data We evaluated the image annotation and textillustration tasks on the dataset described in Sec-tion 3.
Documents and captions were part-of-speechtagged and lemmatized with Tree Tagger (Schmid,1994).
We excluded from the vocabulary low fre-quency words (appearing fewer than five times)and words other than nouns, verbs, and adjectives.For the image annotation task we follow the datasplit used in Feng and Lapata (2008).
The trainingset contains 2,881 image-caption-document tuples;240 tuples are reserved for development and 240 for835testing.
Our text illustration experiments, used 2,881image-caption-document tuples for training.
For thepurposes of simulating a real story picturing engineenvironment, we created a large image pool of 450image-caption pairs and tested on 300 of them.Model Parameters For each image we ex-tracted 150 (on average) SIFT features.
These werequantized into a discrete set of visual terms us-ing K-means.
We varied K from 100 to 2,000.
Wetrained the LDA topic model on the multimodal doc-ument collection {dMix} and varied the number oftopics from 15 to 1,000.
The hyperparameter ?
wasinitialized to 0.1; the ?
probabilities were initial-ized randomly.
The maximum number of iterationsfor variational inference was set to 1,000.
We tunedthe smoothing parameters q1, q2, and q3 (see Equa-tion (9)) on the development set.
The best valueswere q1 = 0.84, q2 = 0.12, and q3 = 0.04 (for bothtasks).
As the number of visual terms and topics areinterrelated we exhaustively examined all possiblecombinations on the development set.
We obtainedbest results on image annotation with 1,000 topicsand 750 visual terms.
On text illustration the best pa-rameters were 1,000 topics and 2,000 visual terms.Baselines For the image annotation experiments,we compared our model against the following base-lines.
Firstly, we trained a vanilla LDA model onthe document collection without taking the im-ages into account.
This model estimates P(wt |D) =?Kk=1P(wt |zk)P(zk|D), the probability of textualword wt given text document D. We assume that themost probable words are the captions for the accom-panying image.
Our second baseline is the extendedrelevance model (Feng and Lapata, 2008) that alsotakes the document into account but crucially as-sumes that the process of generating the images isindependent from the process of generating its key-words.We also compared our approach with twoclosely related latent variable models (developed forimage-caption pairs), a PLSA-based model (Monayand Gatica-Perez, 2007) and CorrLDA (Blei andJordan, 2003).
The former model is an asymmet-ric version of PLSA; it estimates the topic structuresolely from the textual modality and keeps it fixedfor the visual modality.
The technique is similar tofolding-in (Hofmann, 2001), the standard PLSA pro-cedure for inference in unseen documents and al-lows modeling an image as a mixture of latent top-ics that is defined only by one modality (in thiscase the caption words).
CorrLDA first generatesimage regions from a Gaussian multinomial distri-bution parametrized with Dirichlet priors.
Then, foreach annotation word, it uniformly selects a regionfrom the image and generates a word according tothe topic used to generate that region.
We optimizedthe parameters for both models on the developmentset.
For CorrLDA, we followed the mean-field vari-ational inference strategy proposed in Blei (2004).The optimal number of topics for PLSA, was 200(with 2000 visual terms) and for CorrLDA 50.For the text illustration experiments, the pro-posed model was compared with three baselines.The first one is essentially word overlap.
We se-lect the image whose caption has the largest num-ber of words in common with the test document.The second one is a straightforward implementa-tion of the vector space model (Salton and McGill,1983) where documents and captions are repre-sented by vectors whose components correspond toterm-document co-occurrences.
We followed com-mon practice in weighting terms by their tf-idf val-ues, and used the cosine similarity measure to findthe image whose caption is most similar to thetest document.
Our third baseline uses a text-basedLDA model to estimate document-caption similar-ity probabilistically, through topic sharing.
The im-ages most relevant to a document are found by max-imizing the conditional probability of the candi-date captions C given the document dD: P(C|dD) =?wc?C?Kk=1P(wc|zk)P(zk|dD) (where wc are the cap-tion words, P(wc|zk) the conditional distribution ofeach wc given a topic zk, and P(zk|dD) the condi-tional distribution of zk given dD, the document wewish to illustrate.Evaluation In the image annotation task wefollow the evaluation methodology proposed inDuygulu et al (2002).
We are given an un-annotatedimage I and asked to automatically produce then-best keywords.
For all models discussed here, wereport results with the top 10 annotation words us-ing precision, recall and F1.
In the text illustrationtask, we are given a test document d and a poolof candidate images I1...N with captions C1...N .
Themodel is expected to find an image from the can-didate pool that matches the test document.
We useequation (10) to output a ranked list of MI visualterms.
The image having the highest overlap withthe top 30 visual terms is selected as the illustrationfor the test document.
All illustration models wereevaluated using top 1 accuracy, which is the percent-age of successfully matched image-document pairsin the test set.836Model Top 10Precision Recall F1CorrLDA 5.33 11.80 7.36TxtLDA 7.30 16.90 10.20PLSA 10.26 22.60 14.12ExtRel 14.70 27.90 19.80MixLDA 16.30 33.10 21.60Table 2: Automatic image annotation results.7 ResultsOur results on the image annotation task are summa-rized in Table 2.
Here, we compare our own model(MixLDA) which is trained on both visual and tex-tual information against an LDA model based solelyon textual information (TxtLDA), an extended ver-sion of the Continuous Relevance model that alsoexploits collateral document information (ExtRel;Feng and Lapata 2008), a PLSA model that prior-itizes the textual over visual modality (Monay andGatica-Perez, 2007), and CorrLDA (Blei and Jor-dan, 2003) which does the opposite.
We performedsignificance testing on F1 using stratified shuffling(Noreen, 1989), an instance of assumption-free ap-proximative randomization testing.Let us first discuss the performance of TxtLDAand MixLDA.
These two models are closely related?
they both rely on the probabilities P(wt |d) togenerate the image keywords ?
save one importantdifference.
MixLDA uses a concatenated represen-tation of words and visual features assuming thatthe two modalities have equal importance in defin-ing the latent space, whereas TxtLDA considers onlythe textual modality.
Our results show that MixLDAoutperforms TxtLDA in terms of precision (by 9%),recall (by 16.2%).
MixLDA improves F1 by 11.4%,and the difference is significant (p < 0.01).PLSA significantly (p < 0.01) improves uponTxtLDA.
The key difference is the visual informa-tion which makes up (to a certain extent) for thelack of richer textual data.
Interestingly, CorrLDAperforms significantly (p < 0.01) worse than bothPLSA and TxtLDA.
Recall that in CorrLDA wordtopic assignments are drawn from the image regionswhich are in turn drawn from a Gaussian distribu-tion.
Although this modeling choice delivers bet-ter results on the simpler Corel dataset, it does notseem able to capture the characteristics of our im-ages which are noisier and more complex.
More-over, CorrLDA assumes that annotation keywordsmust correspond to image regions.
This assumptionis too restrictive in our setting where a single key-TxtLDAAfghanistan, Taleban,soldier, British, zone,kill, force, Microsoft,troop, NATOpolice, Burgess, time,letter, crash, case,death, operation,investigation, jailMixLDAAfghanistan, troop,Blair, British, NATO,helicopter, soldier,support, operation,commanderDiana, police, case,crash, Princess, re-port, death, inquest,Paris, BurgessCaptionTroops need moreChinook helicopters tocarry out operationsPrincess Diana died ina car crash in Paris in1997Figure 1: Annotations generated by the TxtLDA andMixLDA models.
Words in bold face indicate exactmatches.
The original captions are in the last row.word may refer to many objects or persons in animage (e.g.,the word badminton is used to collec-tively describe an image depicting players, shuttle-cocks, and rackets).
As an aside, it is interesting tonote, that neither PLSA nor CorrLDA achieve betterresults, when modified to take the captions and asso-ciated documents into account.
PLSA scores are inthe same ballpark (see Table 2), whereas CorrLDAperforms worse, F1 decreases by 2%.The extended relevance model improves consid-erably upon TxtLDA, CorrLDA, and PLSA but issignificantly worse (p < 0.01) than MixLDA.
Onthe surface, MixLDA seems similar to ExtRel, bothmodels take advantage of visual and textual informa-tion.
ExtRel smooths the conditional probability of aword given an image with the conditional probabil-ity of the same word given the document and uses anLDA model (trained on the document collection) toremove non-topical keywords from the model?s out-put.
MixLDA is conceptually simpler, LDA is theactual model rather than a post-processing step, andexploits the synergy between visual and textual in-formation more directly.
Topics are created based onboth modalities which are treated on an equal foot-ing.
Compared to ExtRel, MixLDA improves pre-cision by 1.6%, recall by 5.2% and the overall F1by 1.8%.Figure 1 illustrates examples of annotations gen-837Model AccuracyTxtLDA 31.0Overlap 31.3VectorSpace 38.7MixLDA 57.3Table 3: Text Illustration results.erated by TxtLDA and MixLDA for two images.
Forcomparison, we also show the goldstandard imagecaptions.
Note that TxtLDA fails to generate anywords relating to the objects shown in the image.It finds primarily words relating to the topics of theassociated articles such as troops and crash.
On thecontrary, MixLDA is more successful at identifyingthe depicted objects, since it takes visual informa-tion into account.Table 3 presents our results on the automatictext illustration task.
Here, we compare our mul-timodal topic model (MixLDA) against three text-based baselines, namely word overlap (Overlap)a standard vector space model (VectorSpace), andTxtLDA.
We examined whether differences in per-formance are statistically significant using a ?2 test.As can be seen, MixLDA significantly (p < 0.01)outperforms these models by a wide margin (accu-racy is 57.3% for MixLDA vs. 31.0% for TxtLDA,38.7% for the vector space model, and 31.3% forword overlap).
These results are encouraging giventhe simplicity of our model.
They also indicate thatsubstantial mileage can be gained by taking into ac-count the visual modality.Figure 2 shows the three best illustrations foundby MixLDA and VectorSpace (incidentally, Overlapdelivered the same ranking as VectorSpace).
The im-ages are presented in ranked order, i.e., the first im-age was given a higher score than the second one,etc.
The document discusses Smart 1 Probe, a lunarsatellite about to end its mission by crashing ontothe moon?s surface.
MixLDA identifies an image de-picting this satellite.
The second best picture is alsorelevant, it resembles the moon?s surface.
The Vec-torSpace model does not find any related images, thefirst one is a DNA image, the second one depictspolicemen at a crime scene and the third one BenNevis, the highest mountain in the British Isles.8 ConclusionsIn this paper we have presented a probabilistic ap-proach for automatic image annotation and text il-lustration.
Our model postulates that visual termsand words are generated by common (hidden) top-VectorSpaceMixLDAEurope?s lunar satellite, the Smart 1 probe, isabout to end its mission by crashing onto theMoon?s surface.
It will be a spectacular endfor the robot which has spent the past 16 monthstesting innovative and miniaturized space tech-nologies.
Smart 1 has also produced detailedmaps of the Moon?s chemical make-up.Figure 2: Top-3 illustrations for document in bottom row.ics and is trained on a dataset consisting of imagesavailable on the Internet, their captions, and associ-ated news articles.
The annotations are implicit andthe dataset is representative of the scale, diversity,and difficulty of real-world image collections.
Over-all, our results show that the model is robust to thenoise inherent in such data.
It improves upon com-petitive approaches that prioritize one modality overthe other or exploit them indirectly.
We also showthat with minor modifications the model can be usedto automatically illustrate a document with an appro-priate image.
Our method shows promise for multi-modal search and image retrieval and other applica-tions which have been traditionally text-based.
Aninteresting future direction involves generating ac-tual sentence descriptions rather than isolated key-words.
Another relevant application is summariza-tion.
Our results suggest that taking visual informa-tion into account could potentially create more fo-cused and accurate summaries.The model presented here could be further im-proved in two ways.
Firstly, we could allow an in-finite number of topics and develop a nonparamet-ric version that learns how many topics are optimal.Secondly, our model is based on word unigrams, andin this sense takes very little linguistic knowledgeinto account.
Recent developments in topic model-ing could potentially rectify this, e.g., by assumingthat each word is generated by a distribution thatcombines document-specific topics and parse-tree-specific syntactic transitions (Boyd-Graber and Blei,2009).838ReferencesBarnard, K., P. Duygulu, D. Forsyth, N. de Freitas,D.
Blei, andM.
Jordan.
2002.
Matching words and pic-tures.
Journal of Machine Learning Research 3:1107?1135.Barnard, K. and D. Forsyth.
2001.
Learning the semanticsof words and pictures.
In Proceedings of the 8th Inter-national Conference on Computer Vision.
Vancouver,BC, pages 408?415.Blei, D. 2004.
Probabilistic Models of Text and Images.Ph.D.
thesis, U.C.
Berkeley, Division of Computer Sci-ence.Blei, D. and M. Jordan.
2003.
Modeling annotated data.In Proceedings of the 26th Annual International ACMSIGIR Conference.
Toronto, ON, pages 127?134.Blei, D., A. Ng, and M. Jordan.
2003.
Latent Dirich-let alocation.
Journal of Machine Learning Research3:993?1022.Bosch, A., A. Zisserman, and X. Munoz.
2008.
Sceneclassification using a hybrid generative/discriminativeapproach.
IEEE Transactions on Pattern Analysis andMachine Intelligence 30(4):712?727.Boyd-Graber, J. and D. Blei.
2009.
Syntactic topicmodels.
In Proceedings of the 22nd Conference onAdvances in Neural Information Processing Systems.MIT, Press, Cambridge, MA, pages 185?192.Chai, C. and C. Hung.
2008.
Automatically annotatingimages with keywords: A review of image annotationsystems.
Recent Patents on Computer Science 1:55?68.Duygulu, P., K. Barnard, J. de Freitas, and D. Forsyth.2002.
Object recognition as machine translation:Learning a lexicon for a fixed image vocabulary.
InProceedings of the 7th European Conference on Com-puter Vision.
Copenhagen, Danemark, pages 97?112.Fei-Fei, L. and P. Perona.
2005.
A Bayesian hierarchi-cal model for learning natural scene categories.
InProceedings of the 2005 IEEE Computer Society Con-ference on Computer Vision and Pattern Recognition.IEEE Computer Society Washington, DC, volume 2,pages 524?531.Feng, S., V. Lavrenko, and R. Manmatha.
2004.
Mul-tiple Bernoulli relevance models for image and videoannotation.
In Proceedings of the International Con-ference on Computer Vision and Pattern Recognition.Washington, DC, pages 1002?1009.Feng, Y. and M. Lapata.
2008.
Automatic image annota-tion using auxiliary text information.
In Proceedingsof ACL-08: HLT .
Columbus, OH, pages 272?280.Hawking, D., N. Craswell, P. Thistlewaite, and D. Har-man.
1999.
Results and challenges in web search eval-uation.
Computer Networks 31(11):1321?1330.Hofmann, T. 2001.
Unsupervised learning by proba-bilistic latent semantic analysis.
Machine Learning41(2):177?196.Joshi, D., J.Z.
Wang, and J. Li.
2006.
The story picturingengine?a system for automatic text illustration.
ACMTransactions on Multimedia Computing, Communica-tions, and Applications 2(1):68?89.Lavrenko, V., R. Manmatha, and J. Jeon.
2003.
A modelfor learning the semantics of pictures.
In Proceedingsof the 17th Conference on Advances in Neural Infor-mation Processing Systems.
MIT, Press, Cambridge,MA.Lowe, D. 1999.
Object recognition from local scale-invariant features.
In Proceedings of InternationalConference on Computer Vision.
IEEE Computer So-ciety, pages 1150?1157.Monay, F. and D. Gatica-Perez.
2007.
Modeling semanticaspects for cross-media image indexing.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence29(10):1802?1817.Pan, J., H. Yang, P. Duygulu, and C. Faloutsos.
2004.
Au-tomatic image captioning.
In Proceedings of the 2004International Conference on Multimedia and Expo.Taipei, pages 1987?1990.Salton, G. and M.J. McGill.
1983.
Introduction to Mod-ern Information Retrieval.
McGraw-Hill, New York.Schmid, H. 1994.
Probabilistic part-of-speech tagging us-ing decision trees.
In Proceedings of the InternationalConference on New Methods in Language Processing.Manchester, UK, pages 44?49.Smeulders, A. W., M. Worring, S. Santini, A. Gupta, andR.
Jain.
2000.
Content-based image retrieval at theend of the early years.
IEEE Transactions on PatternAnalysis and Machine Intelligence 22(12):1349?1380.Tang, J. and P. H. Lewis.
2007.
A study of quality is-sues for image auto-annotation with the Corel data-set.IEEE Transactions on Circuits and Systems for VideoTechnology 17(3):384?389.Vailaya, A., M. Figueiredo, A. Jain, and H. Zhang.
2001.Image classification for content-based indexing.
IEEETransactions on Image Processing 10:117?130.Wang, C., D. Blei, and L. Fei-Fei.
2009.
Simultaneousimage classification and annotation.
In Proceedings ofCVPR.
Miami, FL, pages 1903?1910.Westerveld, T. and A. P. de Vries.
2003.
Experimentalevaluation of a generative probabilistic image retrievalmodel on ?easy?
data.
In Proceedings of the SIGIRMultimedia Information Retrieval Workshop.
Toronto,ON.Zhao, R. and W. I. Grosky.
2003.
Video shot detectionusing color anglogram and latent semantic indexing:From contents to semantics.
In B. Furht and O. Mar-ques, editors, Handbook of Video Databases: Designand Applications, CRC Press, pages 371?392.839
