Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1348?1353,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsUsing Topic Modeling to Improve Prediction of Neuroticism and Depressionin College StudentsPhilip ResnikUniversity of MarylandCollege Park, MD 20742resnik@umd.eduAnderson GarronUniversity of MarylandCollege Park, MD 20742agarron@cs.umd.eduRebecca ResnikMindwell Psychology Bethesda5602 Shields DriveBethesda, MD 20817drrebeccaresnik@gmail.comAbstractWe investigate the value-add of topic model-ing in text analysis for depression, and for neu-roticism as a strongly associated personalitymeasure.
Using Pennebaker?s Linguistic In-quiry and Word Count (LIWC) lexicon to pro-vide baseline features, we show that straight-forward topic modeling using Latent Dirich-let Allocation (LDA) yields interpretable, psy-chologically relevant ?themes?
that add valuein prediction of clinical assessments.1 IntroductionIn the United States, where 25 million adults peryear suffer a major depressive episode (NAMI,2013), identifying people with mental health prob-lems is a key challenge.
For clinical psychologists,language plays a central role in diagnosis: manyclinical instruments fundamentally rely on what is,in effect, manual coding of patient language.
Au-tomating language assessment in this domain poten-tially has enormous impact, for two reasons.
First,conventional clinical assessments for affective dis-orders (e.g.
The Minnesota Multiphasic Personal-ity Inventory, MMPI) are based on norm-referencedself-report, and therefore depend on patients?
will-ingness and ability to report symptoms.
However,some individuals are motivated to underreport symp-toms to avoid negative consequences (e.g.
activeduty soldiers, parents undergoing child custody eval-uations), and others lack the self awareness to reportaccurately.1 Second, many people ?
e.g.
those with-1As evidenced by the fact that assessments such as the MMPI-2-RF include validity scales to detect, e.g., defensiveness, atyp-out adequate insurance or in rural areas ?
cannot ac-cess a clinician qualified to perform a psychologicalevaluation (Sibelius, 2013; APA, 2013).
There isenormous value in inexpensive screening measuresthat could be administered in primary care and bysocial workers and other providers.We take as a starting point the well knownlexicon-driven methods of Pennebaker and col-leagues (LIWC, Pennebaker and King (1999)),which relate language use to psychological vari-ables, and improve on them straightforwardly usingtopic modeling (LDA, Blei et al(2003)).
First, weshow that taking automatically derived topics intoaccount improves prediction of neuroticism (emo-tional instability, John and Srivastava (1999)), asmeasured by correlation with widely used clinicalinstruments, when compared with lexically-basedprediction alone.
Neuroticism is of particular inter-est as a personality measure because higher scoreson neuroticism scales are consistent with increaseddistress and more difficulty coping; individuals withhigh levels of neuroticism may also be at higher riskof psychiatric problems categorized as Axis I in theDSM-IV (Association, 2000), including the inter-nalizing disorders (depression, anxiety).2 Second,we show a similar correlation improvement resultfor prediction of depression, adding improvementical responses, and overly positive self-portrayals (Tellegen etal., 2003).2The Diagnostic and Statistical Manual of Mental Disorders is awidely used organization of mental health conditions; it servedas the standard for diagnosis from 1994 until the release ofthe (quite controversial) DSM-5 in May, 2013.
Axis I in theDSM-IV includes all the major diagnostic categories, exclud-ing mental retardation and personality disorders.1348on precision (with no decrease in recall), as wellas comparison with human performance by clinicalpsychologists.
Third, we show that LDA has iden-tified meaningful, population-specific themes thatgo beyond the pre-defined LIWC categories and arepsychologically relevant.2 Predicting neuroticism2.1 Experimental frameworkData.
We utilize a collection of 6,459 stream-of-consciousness essays collected from college stu-dents by Pennebaker and King (1999) between 1997and 2008, averaging approximately 780 words each.Students were asked to think about their thoughts,sensations, and feelings in the moment and ?writeyour thoughts as they come to you?.Each essay is accompanied by metadata for itsauthor, which includes scores for the Big-5 person-ality traits (John and Srivastava, 1999): agreeable-ness, conscientiousness, extraversion, neuroticism,and openness.
Because Big-5 assessment can bedone using a variety of different survey instruments(John et al 2008), and different instruments wereused from year to year, we treat the data from eachyear as an independent dataset.Any author missing any Big-5 attribute was ex-cluded.
Essays were tokenized using NLTK (Birdet al 2009) and lowercased, eliminating those thatfailed tokenization because of encoding issues.
Thisresulted in a dataset containing 4,777 essays with as-sociated Big-5 metadata.LIWC features.
For each document, we calcu-late the number of observed words in each of Pen-nebaker and King?s 64 LIWC categories.
These in-clude, among others, syntactic categories (e.g.
pro-nouns, verbs), affect categories (e.g.
negative emo-tion words, anger words), semantic categories (e.g.causation words, cognitive words), and topical cat-egories (e.g.
health, religion).
For instance, theanger category contains 190 word patterns specify-ing, for example, words descriptive of contexts in-volving anger (e.g.
brutal, hostile, shoot) and wordsthat would be used by someone when angry (e.g.bullshit, hate, moron).
We also explored includ-ing essays?
average sentence length and total word-count, as an initial proxy for language complexity,which often figures into psychological assessments.However, results adding these features to LIWC didnot differ significantly from LIWC alone, and forbrevity we do not report them.3LDA features.
We use vanilla LDA as imple-mented in the Mallet toolkit (McCallum, 2002), de-veloping a k-topic model on just training documents,and using the posterior topic distribution for eachtraining and test document as a set of k features.Mallet?s stoplist and default parameters were usedfor burn-in, lag, number of iterations, priors, etc.Details on train/test splits and number k of topicsappear below.LIWC+LDA features.
The union of the LIWCfeatures (one feature per category) and LDA features(one feature per topic).Prediction.
We utilize linear regression in theWEKA toolkit (Hall et al 2009), estimated on train-ing documents, to predict the neuroticism score as-sociated with the author of each test document.42.2 ResultsTable 1 shows the quality of prediction via linearregression, averaged over the eleven datasets, 1997through 2008, using Pearson correlation (r) as theevaluation metric.
For each year, we used 10-foldcross-validation to ensure proper separation of train-ing and test data.
We experimented with LDA us-ing 20, 30, 40, and 50 topics.5A first thing to observe is that the multiple re-gressions using all LIWC categories produce muchstronger correlations with neuroticism than the indi-vidual category correlations reported by Pennebakerand King.6 There the strongest individual corre-lations with neuroticism for any LIWC categoriesare .16 (negative emotion words) and -.13 (positiveemotion words), though it should be noted that theirgoal was to validate their categories as a meaningful3Using richer measures of complexity, e.g.
Pakhomov et al(2011), is a topic for future work.4In previous work we have found that multiple linear regressionis competitive with more complicated techniques such as SVMregression, though we plan to explore the latter in future work.5Full year-by-year data appears in supplemental materi-als at http://umiacs.umd.edu/?resnik/papers/emnlp2013-supplemental/.6The comparison is not perfect, since they used Big-5 data col-lected between 1993 and 1998, and we also eliminated somefiles during preprocessing.1349Feature set LIWC LDA20 LIWC+LDA20 LDA30 LIWC+LDA30 LDA40 LIWC+LDA40 LDA50 LIWC+LDA50Average r 0.413 0.384 0.430 0.407 0.442* 0.420 0.459** 0.440 0.443*Table 1: Prediction quality for neuroticism, for alternative feature sets (Pearson?s r).
*p< .03, **p < .02way to explore personality differences, not predic-tion.As noted in Table 1, paired t-tests (df=10, ?
=.05) establish that, in comparing the cross-year av-erages, augmenting LIWC with topic features im-proves average correlation significantly over usingLIWC features alone for 30, 40, and 50 topics.LDA features alone do not improve significantlyover LIWC.3 Predicting Depression3.1 Experimental frameworkData.
We use essays collected by Rude et al(2004) similarly to ?2.1; in this case, students wereasked to ?describe your deepest thoughts and feel-ings about being in college?.
Each essay is accom-panied by the author?s Beck Depression Inventoryscore (BDI).
BDI (Beck et al 1961) is a widelyused 21-question instrument that correlates stronglywith ratings of depression by psychiatrists.
Follow-ing Rude et al we treat BDI ?
14 as the thresholdfor a positive instance of depression.7 Text prepro-cessing was done as in ?2.1, with 124 documents intotal averaging around 390 words each.Training/test split.
Because only 12 of 124 au-thors met the BDI ?
14 threshold, we did not splitrandomly, lest the test sample include too few pos-itive instances to be useful.
Instead we included arandom 6 of the 12 above-threshold cases, plus 24more items sampled at random, to create a 30-itemtest set.
To form the training set from the comple-mentary items, we added two more copies of eachpositive instance to help address class imbalance(Batista et al 2004) and, following Rude et al weexcluded items with with BDI of 0 or 1 as potentiallyinvalid.Human comparison.
We created a set of experthuman results for comparison by asking three prac-ticing clinical psychologists to review the test doc-uments and rate whether or not the author is suffer-7Each question contributes a score value from 0 to 3, so BDIscores range from 0 to 63.ing depression.8 They were asked to ?decide howat-risk this person might be for depression?, assign-ing 0 (no significant concerns), 1 (mild concerns, butdoes not require futher evaluation), or 2 (requires at-tention, refer for further evaluation).
Following rec-ommended practice for cases where different labelsare not equally distinct from each other (Artsteinand Poesio, 2008), we evaluate inter-coder agree-ment using Krippendorff?s ?
; our ?, computed forordinal data, is 0.722.Features.
We ran 50-topic LDA on the 4,777 es-says from ?2.1 plus the BDI training items, using theposterior topic distributions as features as in ?2.1.As in ?2.1, the LIWC features comprised one countper LIWC category, and LIWC+LDA features werethe union of the two.3.2 ResultsRegression on LIWC features alone achieved r =.288, and adding topic features improved this sub-stantially to r = .416.
Treating BDI ?
14 as thethreshold for positive instances (i.e.
that an author isdepressed), Table 2 shows that adding topic featuresimproves precision without harming recall.
Auto-matic prediction is more conservative than humanratings, trading recall for precision to achieve com-parable F-measure on this test set.98These psychologists all have doctoral degrees, are licensed,and spend significant time primarily in assessment and diag-nosis of psychological disorders.
None were familiar with thespecifics of this study.9A reviewer observes, correctly, that in a scenario where a sys-tem is providing preliminary screenings to aid psychologists,the precision/recall tradeoff demonstrated here would poten-tially be undesirable, since a presumed goal would be to notmiss any cases, even at the risk of some false positives.
Wenote, however, that the real world is unfortunately replete withsituations where there is significant cost or social/professionalstigma associated with interventions or follow-up testing; insuch situations it might be high precision that is desirable.These are challenging questions, and the ability to trade offprecision versus recall more flexibly is a topic we are inter-ested in investigating in future work.1350P R F1LIWC .43 .50 .46LIWC+LDA .50 .50 .50Rater 1 .38 .83 .52Rater 2 .33 .83 .47Rater 3 .33 .66 .44Table 2: Prediction quality for depression.4 Qualitative themesIn order to explore the relevance of the themesuncovered by LDA, the third author, a practicingclinical psychologist, reviewed the 50 LDA cate-gories created in ?3.1.
Each category, representedby its 20 highest-probability words, was given areadable description.
Then, for each category, shewas asked: ?If you were conducting a clinical in-terview, would observing these themes in a patient?sresponses make you more (less) likely to feel that thepatient merited further evaluation for depression?
?Table 3 shows the seven topics selected as par-ticularly indicative of further evaluation.10 Thesecapture population-specific properties in ways thatLIWC cannot ?
for example, although LIWC doeshave a body category, it does not have a categorythat corresponds to somatic complaints, which oftenco-occur with depression.
Similarly, some words re-lated to energy level, e.g.
tired, would be capturedin LIWC?s body, bio, and/or health category, butthe LDA theme corresponding to low energy or lackof sleep, another potential depression cue, containswords that make sense there only in context (e.g.
to-morrow, late).
Other themes, such as the one labeledHOMESICKNESS, are clearly relevant (potentially in-dicative of an adjustment disorder), but even morespecific to the population and context.5 Related WorkThe application of NLP to psychological variableshas seen a recent uptick in community activity.
Onerecent shared task brings together research on theBig-5 personality traits (Celli et al 2013; Kosin-ski et al 2013), and another involved research onidentification of emotion in suicide notes (Pestian etal., 2012).
Other examples include NLP research on10All 50 can be found in the supplemental materials athttp://umiacs.umd.edu/?resnik/papers/emnlp2013-supplemental/.autistic spectrum disorders (Van Santen et al 2010;Prudhommeaux et al 2011; Lehr et al 2013) anddementia (Pakhomov et al 2011; Lehr et al 2012;Roark et al 2011).With regard to depression, Neuman et al(2012)develop a corpus-based ?depression lexicon?
andproduce promising screening results, and De Choud-hury et al(2013) predict social network behav-ior changes related to post-partum depression.
Nei-ther, however, evaluates using formal instrumentsfor clinical assessment.Related investigations involving LDA includeZhai et al(2012), who use LIWC to provide pri-ors for corpus-specific emotion categories; Stark etal.
(2012), who combine LIWC and LDA-basedfeatures in classification of social relationships; andSchwartz et al(2013), who use lexical and topic-based features in Twitter to predict life satisfaction.6 ConclusionsIn this paper, we have aimed for a small, fo-cused contribution, investigating the value-add oftopic modeling in text analysis for depression, andfor neuroticism as a strongly associated personal-ity measure.
Our contribution here is not techni-cal: corpus-specific topics/themes are anticipated byZhai et al(2012), and Stark et al(2012) employtopic-based features for prediction in a supervisedsetting.
Rather, our contribution here has been toshow that topic models can get us beyond the LIWCcategories to relevant, population-specific themesrelated to neuroticism and depression, and to sup-port that claim using evaluation against formal clin-ical assessments.
More data (e.g.
Kosinski et al(2013)) and more sophisticated models (e.g.
super-vised LDA, Blei and McAuliffe (2008), and exten-sions such as Nguyen et al(2013)) will be the keyto further progress.AcknowledgmentsWe are grateful to Jamie Pennebaker for the LIWClexicon and for allowing us to use data from Pen-nebaker and King (1999) and Rude et al(2004),to the three psychologists who kindly took the timeto provide human ratings, and to our reviewers forhelpful comments.
This work has been supported inpart by NSF grant IIS-1211153.1351VEGETATIVE/ENERGY LEVEL sleep tired night bed morning class early tomorrow wake late asleep long hours day sleeping nap today fall staytimeSOMATIC hurts sick eyes hurt cold head tired back nose itches hate stop starting water neck hand stomach feels kind soreNEGATIVE/TROUBLE COPING don(?t) hate doesn care didn(?t) understand anymore feel isn(?t) stupid make won(?t) wouldn talk scared wantedwrong mad stop shouldn(?t)ANGER/FRUSTRATION hate damn stupid sucks hell shit crap man ass god don blah thing bad suck doesn fucking fuck freaking realHOMESICKNESS home miss friends back school family weekend austin parents college mom lot boyfriend left houston visit weekswait high homesickEMOTIONAL STRESS feel feeling thinking makes make felt feels things nervous scared lonely feelings afraid moment happy worrycomfortable stress excited guiltyANXIETY feel happy things lot sad good makes bad make hard mind happen crazy cry day worry times talk great wantedTable 3: LDA-induced themes related to depression.References[APA2013] APA.
2013.
The criticalneed for psychologists in rural america.http://www.apa.org/about/gr/education/rural-need.aspx, Downloaded September 16, 2013.
[Artstein and Poesio2008] Ron Artstein and MassimoPoesio.
2008.
Inter-coder agreement for computa-tional linguistics.
Comput.
Linguist., 34(4):555?596,December.
[Association2000] American Psychiatric Association.2000.
Diagnostic and Statistical Manual of MentalDisorders, 4th Edition, Text Revision (DSM-IV-TR).American Psychiatric Association, 4th edition, July.
[Batista et al004] Gustavo E. A. P. A. Batista,Ronaldo C. Prati, and Maria Carolina Monard.2004.
A study of the behavior of several methods forbalancing machine learning training data.
SIGKDDExplor.
Newsl., 6(1):20?29, June.
[Beck et al961] Aaron T Beck, Calvin H Ward, MockMendelson, Jeremiah Mock, and J1 Erbaugh.
1961.An inventory for measuring depression.
Archives ofgeneral psychiatry, 4(6):561.
[Bird et al009] Steven Bird, Ewan Klein, and EdwardLoper.
2009.
Natural language processing withPython.
O?Reilly.
[Blei and McAuliffe2008] David Blei and Jon McAuliffe.2008.
Supervised topic models.
In J.C. Platt,D.
Koller, Y.
Singer, and S. Roweis, editors, Advancesin Neural Information Processing Systems 20, pages121?128.
MIT Press, Cambridge, MA.
[Blei et al003] David M Blei, Andrew Y Ng, andMichael I Jordan.
2003.
Latent dirichlet alcation.The Journal of Machine Learning Research, 3:993?1022.
[Celli et al013] Fabio Celli, Fabio Pianesi, David Still-well, and Michal Kosinski.
2013.
Computational per-sonality recognition (shared task) workshop.
In In-ternational Conference on Weblogs and Social Media.AAAI.
[De Choudhury et al013] Munmun De Choudhury,Scott Counts, and Eric Horvitz.
2013.
Predictingpostpartum changes in emotion and behavior viasocial media.
In Proceedings of the 2013 ACM annualconference on Human factors in computing systems,pages 3267?3276.
ACM.
[Hall et al009] Mark Hall, Eibe Frank, GeoffreyHolmes, Bernhard Pfahringer, Peter Reutemann, andIan H Witten.
2009.
The weka data mining software:an update.
ACM SIGKDD Explorations Newsletter,11(1):10?18.
[John and Srivastava1999] Oliver P John and Sanjay Sri-vastava.
1999.
The big five trait taxonomy: History,measurement, and theoretical perspectives.
Handbookof personality: Theory and research, 2:102?138.
[John et al008] O. P. John, L. P. Naumann, and C. J.Soto.
2008.
Paradigm shift to the integrative big fivetrait taxonomy: History, measurement, and conceptualissues.
In Handbook of personality: Theory and re-search, pages 114?158.
[Kosinski et al013] Michal Kosinski, David Stillwell,and Thore Graepel.
2013.
Private traits and attributesare predictable from digital records of human behav-ior.
Proceedings of the National Academy of Sciences,110(15):5802?5805.
[Lehr et al012] Maider Lehr, Emily TuckerPrud?hommeaux, Izhak Shafran, and Brian Roark.2012.
Fully automated neuropsychological assess-ment for detecting mild cognitive impairment.
InINTERSPEECH.
[Lehr et al013] Maider Lehr, Izhak Shafran, EmilyPrudhommeaux, and Brian Roark.
2013.
Discrimi-native joint modeling of lexical variation and acous-tic confusion for automated narrative retelling assess-ment.
In Proceedings of NAACL-HLT, pages 211?220.
[McCallum2002] Andrew Kachites McCallum.
2002.Mallet: A machine learning for language toolkit.http://mallet.cs.umass.edu.
[NAMI2013] NAMI.
2013.
Ma-jor depression fact sheet, April.http://www.nami.org/Template.cfm?Section=depression.
[Neuman et al012] Yair Neuman, Yohai Cohen, DanAssaf, and Gabbi Kedma.
2012.
Proactive screen-ing for depression through metaphorical and automatic1352text analysis.
Artif.
Intell.
Med., 56(1):19?25, Septem-ber.
[Nguyen et al013] Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2013.
Lexical andhierarchical topic regression.
In Neural InformationProcessing Systems.
[Pakhomov et al011] Serguei Pakhomov, Dustin Cha-con, Mark Wicklund, and Jeanette Gundel.
2011.Computerized assessment of syntactic complexity inalzheimers disease: a case study of iris murdochs writ-ing.
Behavior Research Methods, 43(1):136?144.
[Pennebaker and King1999] James W Pennebaker andLaura A King.
1999.
Linguistic styles: language useas an individual difference.
Journal of personality andsocial psychology, 77(6):1296.
[Pestian et al012] John P Pestian, Pawel Matykiewicz,Michelle Linn-Gust, Brett South, Ozlem Uzuner, JanWiebe, K Bretonnel Cohen, John Hurdle, ChristopherBrew, et al2012.
Sentiment analysis of suicidenotes: A shared task.
Biomedical Informatics Insights,5(Suppl.
1):3.
[Prudhommeaux et al011] Emily T Prudhommeaux,Brian Roark, Lois M Black, and Jan van Santen.2011.
Classification of atypical language in autism.ACL HLT 2011, page 88.
[Roark et al011] Brian Roark, Margaret Mitchell, J Ho-som, Kristy Hollingshead, and Jeffrey Kaye.
2011.Spoken language derived measures for detecting mildcognitive impairment.
Audio, Speech, and LanguageProcessing, IEEE Transactions on, 19(7):2081?2090.
[Rude et al004] Stephanie Rude, Eva-Maria Gortner,and James Pennebaker.
2004.
Language use of de-pressed and depression-vulnerable college students.Cognition & Emotion, 18(8):1121?1133.
[Schwartz et al013] H. Andrew Schwartz, Johannes C.Eichstaedt, Margaret L. Kern, Lukasz Dziurzynski,Megha Agrawal, Gregory J.
Park, Shrinidhi K. Lak-shmikanth, Sneha Jha, Martin E. P. Seligman, andLyle Ungar.
2013.
Characterizing geographic varia-tion in well-being using tweets.
In Seventh Interna-tional AAAI Conference on Weblogs and Social Media(ICWSM 2013).
[Sibelius2013] Kathleen Sibelius.
2013.
Increas-ing access to mental health services, April.http://www.whitehouse.gov/blog/2013/04/10/increasing-access-mental-health-services.
[Stark et al012] Anthony Stark, Izhak Shafran, and Jef-frey Kaye.
2012.
Hello, who is calling?
: can wordsreveal the social nature of conversations?
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 112?119.Association for Computational Linguistics.
[Tellegen et al003] A. Tellegen, Y.S.
Ben-Porath, J.L.McNulty, P.A.
Arbisi, and B. Graham, J.R.and Kaem-mer.
2003.
The MMPI-2 Restructured ClinicalScales: Development, validation, and interpretation.Minneapolis, MN: University of Minnesota Press.
[Van Santen et al010] Jan PH Van Santen, Emily TPrud?hommeaux, Lois M Black, and MargaretMitchell.
2010.
Computational prosodic markers forautism.
Autism, 14(3):215?236.
[Zhai et al012] Ke Zhai, Jordan Boyd-Graber, NimaAsadi, and Mohamad L. Alkhouja.
2012.
Mr. lda:a flexible large scale topic modeling package usingvariational inference in mapreduce.
In Proceedings ofthe 21st international conference on World Wide Web,WWW ?12, pages 879?888, New York, NY, USA.ACM.1353
