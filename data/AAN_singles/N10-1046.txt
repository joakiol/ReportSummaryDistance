Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 325?328,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Comparative Study of Word Co-occurrence for Term Clusteringin Language Model-based Sentence RetrievalSaeedeh MomtaziSpoken Language SystemsSaarland Universitysaeedeh.momtazi@lsv.uni-saarland.deSanjeev KhudanpurCenter for Languageand Speech ProcessingJohns Hopkins Universitykhudanpur@jhu.eduDietrich KlakowSpoken Language SystemsSaarland Universitydietrich.klakow@lsv.uni-saarland.deAbstractSentence retrieval is a very important part ofquestion answering systems.
Term clustering,in turn, is an effective approach for improvingsentence retrieval performance: the more simi-lar the terms in each cluster, the better the per-formance of the retrieval system.
A key step inobtaining appropriate word clusters is accurateestimation of pairwise word similarities, basedon their tendency to co-occur in similar con-texts.
In this paper, we compare four differ-ent methods for estimating word co-occurrencefrequencies from two different corpora.
The re-sults show that different, commonly-used con-texts for defining word co-occurrence differsignificantly in retrieval performance.
Using anappropriate co-occurrence criterion and corpusis shown to improve the mean average preci-sion of sentence retrieval form 36.8% to 42.1%.1 Corpus-Driven Clustering of TermsSince the search in Question Answering (QA) is con-ducted over smaller segments of text than in docu-ment retrieval, the problems of data sparsity and ex-act matching become more critical.
The idea of usingclass-based language model by applying term clus-tering, proposed by Momtazi and Klakow (2009), isfound to be effective in overcoming these problems.Term clustering has a very long history in natu-ral language processing.
The idea was introducedby Brown et al (1992) and used in different appli-cations, including speech recognition, named entitytagging, machine translation, query expansion, textcategorization, and word sense disambiguation.
Inmost of the studies in term clustering, one of severalwell-know notions of co-occurrence?appearing inthe same document, in the same sentence or follow-ing the same word?has been used to estimate termsimilarity.
However, to the best of our knowledge,none of them explored the relationship between dif-ferent notions of co-occurrence and the effectivenessof their resulting clusters in an end task.In this research, we present a comprehensive studyof how different notions of co-occurrence impact re-trieval performance.
To this end, the Brown algo-rithm (Brown et al, 1992) is applied to pairwise wordco-occurrence statistics based on different definitionsof word co-occurrence.
Then, the word clusters areused in a class-based language model for sentenceretrieval.
Additionally, impact of corpus size and do-main on co-occurrence estimation is studied.The paper is organized as follows.
In Section 2,we give a brief description of class-based languagemodel for sentence retrieval and the Brown wordclustering algorithm.
Section 3 presents differentmethods for estimating the word co-occurrence.
InSection 4, experimental results are presented.
Fi-nally, Section 5 summarizes the paper.2 Term Clustering Method and ApplicationIn language model-based sentence retrieval, the prob-ability P (Q|S) of generating query Q conditioned ona candidate sentence S is first calculated.
Thereaftersentences in the search collection are ranked in de-scending order of this probability.
For word-basedunigram, P (Q|S) is estimated asP (Q|S) =?i=1...MP (qi|S), (1)where M is the number of query terms, qi denotes theith query term in Q, and S is the sentence model.325For class-based unigrams, P (Q|S) is computedusing only the cluster labels of the query terms asP (Q|S) =?i=1...MP (qi|Cqi , S)P (Cqi |S), (2)where Cqi is the cluster that contains qi andP (qi|Cqi , S) is the emission probability of theith query term given its cluster and the sen-tence.
P (Cqi |S) is analogous to the sentence modelP (qi|S) in (1), but is based on clusters instead ofterms.
To calculate P (Cqi |S), each cluster is con-sidered an atomic entity, with Q and S interpreted assequences of such entities.In order to cluster lexical items, we use the al-gorithm proposed by Brown et al(1992), as imple-mented in the SRILM toolkit (Stolcke, 2002).
The al-gorithm requires an input corpus statistics in the form?w,w?, fww?
?, where fww?
is the number of times theword w?
is seen in the context w. Both w and w?
areassumed to come from a common vocabulary.
Be-ginning with each vocabulary item in a separate clus-ter, a bottom-up approach is used to merge the pair ofclusters that minimizes the loss in Average Mutual In-formation (AMI) between the word cluster Cw?
andits context cluster Cw.
Different words seen in thesame contexts are good candidates for merger, as aredifferent contexts in which the same words are seen.While originally proposed with bigram statistics,the algorithm is agnostic to the definition of co-occurrence.
E.g.
if ?w,w??
are verb-object pairs,the algorithm clusters verbs based on their selectionalpreferences, if fww?
is the number of times w and w?appear in the same document, it will produce seman-tically (or topically) related word-clusters, etc.Several notions of co-occurrence have been usedin the literature to cluster words, as described next.3 Notions of Word Co-occurrenceCo-occurrence in a DocumentIf two content words w and w?
are seen in thesame document, they are usually topically related.
Inthis notion of co-occurrence, how near or far awayfrom each other they are in the document is irrele-vant, as is their order of appearance in the document.Document-wise co-occurrence has been successfullyused in many NLP applications such as automaticthesaurus generation (Manning et al, 2008)Statistics of document-wise co-occurrence may becollected in two different ways.
In the first case,fww?
= fw?w is simply the number of documents thatcontain both w and w?.
This is usually the notionused in ad hoc retrieval.
Alternatively, we may wantto treat each instance of w?
in a document that con-tains an instance of w to be a co-occurrence event.Therefore if w?
appears three times in a documentthat contains two instances of w, the former methodcounts it as one co-occurrence, while the latter as sixco-occurrences.
We use the latter statistic, since weare concerned with retrieving sentence sized ?docu-ments,?
wherein a repeated word is more significant.Co-occurrence in a SentenceSince topic changes sometimes happen within asingle document, and our end task is sentence re-trieval, we also investigate the notion of word co-occurrence in a smaller segment of text such as asentence.
In contrast to the document-wise model,sentence-wise co-occurrence does not consider wholedocuments, and only concerns itself with the numberof times that two words occur in the same sentence.Co-occurrence in a Window of TextThe window-wise co-occurrence statistic is an evennarrower notion of context, considering only terms ina window surrounding w?.
Specifically, a window ofa fixed size is moved along the text, and fww?
is setas the number of times both w and w?
appear in thewindow.
Since the window size is a free parameter,different sizes may be applied.
In our experiments weuse two window sizes, 2 and 5, that have been studiedin related research (Church and Hanks, 1990).Co-occurrence in a Syntactic RelationshipAnother notion of word similarity derives fromhaving the same syntactic relationship with the con-text w. This syntax-wise co-occurrence statistic issimilar to the sentence-wise co-occurrence, in thatco-occurrence is defined at the sentence level.
How-ever, in contrast to the sentence-wise model, w andw?
are said to co-occur only if there is a syntactic re-lation between them in that sentence.
E.g., this typeof co-occurrence can help cluster nouns that are usedas objects of same verb, such as ?tea?, ?water?, and?cola,?
which all are used with the verb ?drink?.To gather such statistics, all sentences in the corpusmust be syntactically parsed.
We found that a depen-dency parser is an appropriate tool for our goal: it326directly captures dependencies between words with-out the mediation of any virtual (nonterminal) nodes.Having all sentences in the parsed format, fww?
is de-fined as the number of times that the words w and w?have a parent-child relationship of any syntactic typein the dependency parse tree.
For our experiments weuse MINIPAR (Lin, 1998) to parse the whole corpusdue to its robustness and speed.4 Sentence Retrieval Experiments4.1 Derivatives of the TREC QA Data SetsThe set of questions from the TREC 2006 QA track1was used as the test data to evaluate our models,while the TREC 2005 set was used for development.The TREC 2006 QA task contains 75 question-series, each on one topic, for a total of 403 factoidquestions which is used as queries for sentence re-trieval.
For sentence-level relevance judgments, theQuestion Answer Sentence Pair corpus of Kaisserand Lowe (2008) was used.
All the documentsthat contain relevant sentences are from the NISTAQUAINT1 corpus.QA systems typically employ sentence retrieval af-ter initial, high quality document retrieval.
To simu-late this, we created a separate search collection foreach question using all sentences from all documentsrelevant to the topic (question-series) from which thequestion was derived.
On average, there are 17 rel-evant documents per topic, many not relevant to thequestion itself: they may be relevant to another ques-tion.
So the sentence search collection is realistic,even if somewhat optimistic.4.2 Corpora for Term ClusteringWe investigated two different corpora2, AQUAINT1and Google n-grams, to obtain word co-occurrencestatistics for term clustering.
Based on this we canalso evaluate the impact of corpus size and corpusdomain on the result of term clustering.AQUAINT1 consists of English newswire text ex-tracted from the Xinhua, the New York Times and theAssociated Press Worldstream News Services.The Google n-gram counts were generated frompublicly accessible English web pages.
Since there is1See http://trec.nist.gov.2See catalog numbers LDC2002T31 and LDC2006T13 re-spectively at http://www.ldc.upenn.edu/Catalog.Corpus Co-occurrence # Word PairsAQUAINT1 document 368,109,133AQUAINT1 sentence 104,084,473AQUAINT1 syntax 12,343,947AQUAINT1 window-5 46,307,650AQUAINT1 window-2 14,093,661Google n-grams window-5 12,005,479Google n-grams window-2 328,431,792Table 1: Statistics for different notions of co-occurrence.no possibility of extracting document-wise, sentence-wise or syntax-wise co-occurrence statistics from theGoogle n-gram corpus, we only collect window-wisestatistics to the extent available in the corpus.Table 1 shows the number of word pairs extractedfrom the two corpora with different definitions of co-occurrence.
The statistics only include word pairsfor which both constituent words are present in the35,000 word vocabulary of our search collection.4.3 Sentence Retrieval Results and DiscussionSentence retrieval performance for term clusteringusing different definitions of word co-occurrence isshown in Figure 1.
Since the Brown algorithm re-quires specifying the number of clusters, tests wereconducted for 50, 100, 200, 500, and 1000 clustersof the term vocabulary.
The baseline system is theword-based sentence retrieval model of Equation (1).Figure 1(a) shows the Mean Average Precision(MAP) for class-based sentence retrieval of Equation(2) using clusters based on different co-occurrencestatistics from AQUAINT1.
Note that(i) the best result achieved by sentence-wise co-occurence is better the best result of document-wise, perhaps due to more local and relevant in-formation that it captures;(ii) all the results achieved by syntax-wise co-occurrence are better than sentence-wise, indi-cating that merely co-occurring in a sentenceis not very indicative of word similarity, whilerelations extracted from syntactic structure im-prove system performance significantly;(iii) window-2 significantly outperforms all othernotions of co-occurrence; i.e., the bigram statis-tics achieve the best clustering results.
In com-parison, window-5 has the worst results, withperformance very close to baseline.Although window-5 co-occurrence has been reported32750 5000.350.360.370.380.390.400.410.420.43document sentence window2 window5 syntax base?linelog?of?number?of?clustersMAP50 5000.350.360.370.380.390.400.410.420.43AQUAINT?window2 Google?window2 Google?window5 base?linelog?of?number?of?clustersMAP(b)(a)Figure 1: MAP of sentence retrieval for different word co-occurrence statistics from AQUAINT1 and Google n-grams.to be effective in other applications, it is not helpfulin sentence retrieval.Figure 1(b) shows the MAP for class-based sen-tence retrieval of Equation (2) when window-wiseco-occurrence statistics from the Google n-grams areused.
For better visualization, we repeated the MAPresults using AQUAINT1 window-2 co-occurrencestatistics from Figure 1(a) in 1(b).
Note that(iv) window-2 co-occurrence statistics significantlyoutperform window-5 for the Google n-grams,consistent with results from AQUAINT1;(v) Google n-gram window-2 co-occurrence statis-tics consistently result in better MAP thanAQUAINT window-2.The last result indicates that even though the Googlen-grams are from a different (and much broader) do-main than the test data, they significantly improve thesystem performance due to sheer size.
Finally(vi) Google n-gram window-2 MAP curve is flatterthan AQUAINT window-2; i.e., performance isnot very sensitive to the number of clusters.The best overall result is from Google window-2co-occurrence statistics with 100 clusters, achiev-ing 42.1% MAP while the best result derivedfrom AQUAINT1 is 41.7% MAP for window-2 co-occurrence with 100 clusters, and the MAP of theword-based model (baseline) is 36.8%.5 Concluding RemarksWe compared different notions of word co-occurrence for clustering terms, using document-wise, sentence-wise, window-wise, and syntax-wiseco-occurrence statistics derived from AQUAINT1.We found that different notions of co-occurrence sig-nificantly change the behavior of a sentence retrievalsystem, in which window-wise model with size 2achieves the best result.
In addition, Google n-gramswere used for window-wise model to study the im-pact of corpus size and domain on the clustering re-sult.
The result showed that although the domain ofthe Google n-grams is dissimilar to the test set, itoutperforms models derived from AQUAINT1 due tosheer size.AcknowledgmentsSaeedeh Momtazi is funded by the German researchfoundation DFG through the International ResearchTraining Group (IRTG 715).ReferencesP.F.
Brown, V.J.D.
Pietra, P.V.
Souza, J.C. Lai, and R.L.Mercer.
1992.
Class-based n-gram models of naturallanguage.
Computational Linguistics, 18(4):467?479.K.W.
Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational Linguistics, 16(1):22?29.M.
Kaisser and J.B. Lowe.
2008.
Creating a researchcollection of question answer sentence pairs with Ama-zon?s mechanical turk.
In Proc.
of LREC.D.
Lin.
1998.
Dependency-based evaluation of MINI-PAR.
In Proc.
of the Evaluation of Parsing SystemsWorkshop.C.D.
Manning, P. Raghavan, and H. Schu?tze.
2008.
Intro-duction to Information Retrieval.
Cambridge Univer-sity Press.S.
Momtazi and D. Klakow.
2009.
A word clusteringapproach for language model-based sentence retrievalin question answering systems.
In Proc.
of ACM CIKM.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proc.
of ICSLP.328
