Coling 2010: Poster Volume, pages 979?987,Beijing, August 2010Filling Knowledge Gaps in Text for Machine ReadingAnselmo Pe?as Eduard HovyUNED NLP & IR Group USC Information Sciences Instituteanselmo@lsi.uned.es hovy@isi.eduAbstractTexts are replete with gaps, informationomitted since authors assume a certainamount of background knowledge.
We de-fine the process of enrichment that fillsthese gaps.
We describe how enrichmentcan be performed using a BackgroundKnowledge Base built from a large corpus.We evaluate the effectiveness of variousopenly available background knowledgebases and we identify the kind of informa-tion necessary for enrichment.1 Introduction: Knowledge GapsAutomated understanding of connected text re-mains an unsolved challenge in NLP.
In contrastto systems that harvest information from largecollections of text, or that extract only certainpre-specified kinds of information from singletexts, the task of extracting and integrating allinformation from a single text, and building acoherent and relatively complete representationof its content, is still beyond current capabilities.A significant obstacle is the fact that text al-ways omits information that is important, but thatpeople recover effortlessly.
Authors leave outinformation that they assume is known to theirreaders, since its inclusion (under the Griceanmaxim of minimality) would carry an additional,often pragmatic, import.
The problem is that sys-tems cannot perform the recovery since they lackthe requisite background knowledge and inferen-tial machinery to use it.In this research we address the problem of au-tomatically recovering such omitted informationto ?plug the gaps?
in text.
To do so, we describethe background knowledge required as well as aprocedure of enrichment, which recognizeswhere gaps exist and fills them out using appro-priate background knowledge as needed.
We de-fine enrichment as:Def: Enrichment is the process of adding ex-plicitly to a text?s representation the informationthat is either implicit or missing in the text.Central to enrichment is the source of the newknowledge.
The use of Proposition Stores asBackground Knowledge Bases (BKB) have beenargued to be useful for improving parsing, co-reference resolution, and word sense disambigua-tion (Clark and Harrison 2009).
We argue herethat Proposition Stores are also useful forEnrichment and show how in Section 4.
Howev-er, we show in Section 5 that current BKB re-sources such as TextRunner (Banko et al 2007)and DART (Clark and Harrison 2009) are notideal for enrichment purposes.
In some casesthere is a lack in normalization.
But the most im-portant shortcoming is the lack in answeringabout instances, their possible classes, how theyrelate to propositions, and how different proposi-tions are related through them.
We propose easyto achieve extensions in this direction.
We testthis hypothesis building our own PropositionStore with the proposed extensions, and compareit with them for enrichment in the US footballdomain.To perform enrichment, we begin with an ini-tial simple text representation and a PropositionStores as a background knowledge base.
We ex-ecute a simple formalized procedure to select andattach appropriate elements from the BKB to theentities and implicit relations present in the initialtext representation.
Surprisingly, we find thatsome quite simple processing can be effective ifwe are able to contextualize the text under inter-pretation.We describe in Section 2 our textual represen-tations and in Section 3 the process of buildingthe Proposition Store.
Enrichment is described inSection 4, and an evaluation and comparison isperformed in Section 5.2 Text RepresentationThe initial, shallow, text representation must cap-ture the first impression of what is going on inthe text, possibly (unfortunately) losing somefragments.
After the first impression, in accordwith the purpose of the reading, we ?contextual-979ize?
each sentence, expanding its initial represen-tation with the relevant related backgroundknowledge in our base.During this process of making explicit the im-plicit semantic relations it will become apparentwhether we need to recover some of the missedelements, whether we need to expand some oth-ers, etc.
So the process is identified with thegrowing of the context until deeper interpretationis possible.
This approach resembles some well-known theories such as the Theory of Relevance(Sperber and Wilson, 1995).
The particular me-thod we envisage is related to Interpretation asAbduction (Hobbs et al 1993).How can the initial information be representedso as to enable the context to grow into an inter-pretation?
We hypothesize that:1.
Behind certain syntactic dependencies thereare semantic relations.2.
In the case of dependencies between nouns,this semantic relation can be made more ex-plicit using verbs and/or prepositions.
Theknowledge base (our Proposition Store) musthelp us find them.We look for a semantic representation closeenough to the syntactic representation we canobtain from the dependency graph.
The mainsyntactic dependencies we want to represent inorder to enable enrichment are:1.
Dependencies between nouns such as noun-noun compounds (nn) or possessive (poss).2.
Dependencies between nouns and verbs,such as subject and object relations.3.
Prepositions having two nouns as arguments.Then the preposition becomes the label forthe relation, being the object of the preposi-tion the target of the relation.We collapse the syntactic dependencies be-tween verb, subject, and object into a single se-mantic relation.
Since we are assuming that theverb is the more explicit expression of a semanticrelation, we fix this in the initial representation.The subject will be the source of the relation andthe object will be the target of the relation.
Whenthe verb has more arguments we consider its ex-pansion as a new node as referred in Section 4.4.Figure 1 shows the initial minimal representa-tion for the sentence we will use for our discus-sion: ?San Francisco's Eric Davis intercepted aSteve Walsh pass on the next series to set up aseven-yard Young touchdown pass to BrentJones?.
Notice that some pieces of the text aremissing in the initial representation of the text, asfor example ?on the next series?
or ?seven-yard?.3 Background Knowledge BaseWe will use a Proposition Stores as a Back-ground Knowledge Base (BKB).
We built it froma collection of 30,826 New York Times articlesabout US football, similar to the kind of texts wewant to interpret.
We parsed the collection usinga standard dependency parser (Marneffe andManning, 2008; Klein and Maning, 2003) and,after collapsing some syntactic dependencies,obtained 3,022,305 raw elements in the BKB.3.1 Types of elements in the BKBWe distinguish three kinds of elements in ourBackground Knowledge Base: Entities, Proposi-tions, and Lexical relations.
All three have asso-ciated their frequency in the reference collection.Entities: We distinguish between entity classesand entity instances:1.
Entity classes: Entity classes are denoted bynouns.
We don?t restrict classes to any par-ticular predefined set.
In addition, we intro-duce two special classes: Person and Group.These two classes are related to the use ofpronouns in text.
Pronouns ?I?, ?he?
and?she?
are linked to class Person.
Pronouns?we?
and ?they?
are linked to class Group.For example, the occurrence of the pronoun?he?
in ?He threw a pass?
would produce anadditional count of the proposition ?per-son:throw:pass?.set up toYoungBrentJones touchdownpass2Figure 1.
Initial text representation.nnSteveWalshEricDavispass1interceptnn nnSan Franciscoposs9802.
Entity Instances: Entity instances are indi-cated by proper nouns.
Proper nouns areidentified by the part of speech tagging.Some of these instances will participate inthe ?has-instance?
relation (see below).When they participate in a proposition theyproduce proposition instances.Propositions: Following Clark and Harrison(2009) we call propositions the tuples of wordsthat have some determined pattern of syntacticrelations among them.
We focus on NVN,NVNPN and NPN proposition types.
For exam-ple, a NVNPN proposition is a full instantiationof: Subject:Verb:Object:Prep:Complement.The first three elements are the subject, theverb and the direct object.
Fourth is the preposi-tion that attaches the PP complement to the verb.For simplicity, indirect objects are considered asa Complement with the preposition ?to?.The following are the most frequent NVNpropositions in the BKB ordered by frequency.NVN 2322 'NNP':'beat':'NNP'NVN 2231 'NNP':'catch':'pass'NVN 2093 'NNP':'throw':'pass'NVN 1799 'NNP':'score':'touchdown'NVN 1792 'NNP':'lead':'NNP'NVN 1571 'NNP':'play':'NNP'NVN 1534 'NNP':'win':'game'NVN 1355 'NNP':'coach':'NNP'NVN 1330 'NNP':'replace':'NNP'NVN 1322 'NNP':'kick':'goal'The ?NNP?
tag replaces specific proper nouns(instances) found in the proposition.When a sentence has more than one comple-ment, a new occurrence is counted for each com-plement.
For example, given the sentence?Steve_Walsh threw a pass to Brent_Jones in thefirst quarter?, we would add a count to each ofthe following propositions:Steve_Walsh:throw:passSteve_Walsh:throw:pass:to:Brent_JonesSteve_Walsh:throw:pass:in:quarterNotice that we include only the heads of thenoun phrases in the propositions.We call proposition classes the propositionsthat only involve instance classes (e.g., ?per-son:throw:pass?
), and proposition instancesthose that involve at least one entity instance(e.g., ?Steve_Walsh:throw:pass?
).Proposition instances are useful for the track-ing of a entity instance.
For example,?'Steve_Walsh':'supplant':'John_Fourcade':'as':'quarterback'?.
When a proposition instance isfound, it is stored also as a proposition class re-placing the proper nouns by a special word(NNP) to indicate the presence of an entity in-stance.
The enrichment of the text is based on theuse of most frequent proposition classes.Lexical Relations: We make use of very generalpatterns considering appositions and copulaverbs (detected by the Stanford parser) in orderto extract ?is?, and ?has-instance?
relations:1.
Is: between two entity classes.
They denote akind of identity between both entity classes,but not in any specific hierarchical relationsuch as hyponymy.
Neither is a relation ofsynonymy.
As a result, it is somehow a kindof underspecified relation that groups thosemore specific.
For example, if we ask theBKB what a ?receiver?
is, the most frequentrelations are:290 'person':is:'receiver'29 'player':is:'receiver'16 'pick':is:'receiver'15 'one':is:'receiver'14 'receiver':is:'target'8 'end':is:'receiver'7 'back':is:'receiver'6 'position':is:'receiver'The number indicates the frequency of therelation in the collection.2.
Has-instance: between an entity class and anentity instance.
For example, if we ask forinstances of team, the top instances withmore support in the collection are:192 'team':has-instance:'Jets'189 'team':has-instance:'Giants'43 'team':has-instance:'Eagles'40 'team':has-instance:'Bills'36 'team':has-instance:'Colts'35 'team':has-instance:'Miami'But we can ask also for the possible classes ofan instance.
For example, all the entity classesfor ?Eric_Davis?
are:12 'cornerback':has-instance:'Eric_Davis'1 'hand':has-instance:'Eric_Davis'1 'back':has-instance:'Eric_Davis'We still work on other lexical relations suchas ?part-of?
and ?is-value-of?.
For example, themost frequent ?is-value-of?
relations are:5178 '[0-9]-[0-9]':is-value-of:'lead'3996 '[0-9]-[0-9]':is-value-of:'record'2824 '[0-9]-[0-9]':is-value-of:'loss'1225 '[0-9]-[0-9]':is-value-of:'season'9814 Enrichment operationsThe goal of the following enrichment operationsis to make explicit what kind of semantic rela-tions and entity classes are involved in the text.4.1 Fusion of nodesSometimes, the syntactic dependency ties two ormore words that form a single concept.
This isthe case with multiword terms such as ?tightend?, ?field goal?, ?running back?, etc.
In thesecases, the meaning of the compound is beyondthe syntactic dependency.
Thus, we shouldn?tlook for its explicit meaning.
Instead, we fuse thenodes into a single one.The question is whether the fusion of thewords into a single expression allows or not theconsideration of possible paraphrases.
For exam-ple, in the case of ?field:nn:goal?, we don?t findother ways to express the concept in the BKB.However, in the case of ?touchdown:nn:pass?
wecan find, for example, ?pass:for:touchdown?
asignificant amount of times, and we want to iden-tify them as equivalent expressions.4.2 Building context for instancesSuppose we wish to determine what kind of enti-ty ?Steve Walsh?
is in the context of the syntacticdependency ?Steve_Walsh:nn:pass?.
First, welook into the BKB for the possible entity classesof Steve_Walsh previously found in the collec-tion.
In this particular case, the most frequentclass is ?quarterback?
:40 'quarterback':has-instance:'Steve_Walsh'2 'junior':has-instance:'Steve_Walsh'But what happens if we see ?Steve_Walsh?
forthe first time?
Then we need to take into accountthe classes shared by other instances in the samesyntactic context.
The most frequent are ?Mari-no?, ?Kelly?, ?Elway?, etc.
From them we areable to infer the most plausible class for the newentity.
In our example, quarterback:20 'quarterback':has-instance:'Marino'6 'passer':has-instance:'Marino'?17 'quarterback':has-instance:'Kelly'6 'passer':has-instance:'Kelly'?16 'quarterback':has-instance:'Elway'9 'player':has-instance:'Elway'4.3 Building context for dependenciesNow we want to determine the meaning behindsuch syntactic dependencies as:?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?,?Young:nn:pass?
or ?pass:to:Brent_Jones?.We have two ways for adding more meaningto these syntactic dependencies: find the mostappropriate prepositions to describe them, andfind the most appropriate verbs.
Whether one, theother, or both is useful has to be determined dur-ing the reasoning system development.Finding the prepositionsSeveral types of propositions in the BKB involveprepositions.
The most relevant are NPN andNVNPN.
In the case of ?touchdown:nn:pass?,?for?
is clearly the best interpretation:NPN 712 'pass':'for':'touchdown'NPN 24 'pass':'include':'touchdown'In the case of ?Steve_Walsh:nn:pass?
and?Young:nn:pass?, since we know they are quar-terbacks, we can ask for all the prepositions be-tween ?pass?
and ?quarterback?
:NPN 23 'pass':'from':'quarterback'NPN 14 'pass':'by':'quarterback'If we don?t have any evidence on the instanceclass, and we know only that they are instances,the pertinent query to the BKB obtains:NPN 1305 'pass':'to':'NNP'NPN 1085 'pass':'from':'NNP'NPN 147 'pass':'by':'NNP'In the case of ?Young:nn:pass?
(in ?Youngpass to Brent Jones?
), there exists already thepreposition ?to?
(?pass:to:Brent_Jones?
), so themost promising choice becomes the second,?pass:from:Young?, which has one order ofmagnitude more occurrences than its successor.In the case of ?Steve_Walsh:nn:pass?
(in ?EricDavis intercepted a Steve Walsh pass?)
we canuse additional information: we know that ?Er-ic_Davis:intercept:pass?.
So, we can try to findthe appropriate preposition using NVNPN propo-sitions in the following way:?Eric_Davis:intercept:pass:P:Steve_Walsh?Asking the BKB about the propositions thatinvolve two instances with ?intercept?
and?pass?, we obtain:NVNPN 48 'NNP':'intercept':'pass':'by':'NNP'982NVNPN 26 'NNP':'intercept':'pass':'at':'NNP'NVNPN 12 'NNP':'intercept':'pass':'from':'NNP'We could also query the BKB with the classeswe have already found for ?Eric_Davis?
(cor-nerback, player, person):NVNPN 11 'person':'intercept':'pass':'by':'NNP'NVNPN 4 'person':'intercept':'pass':'at':'NNP'NVNPN 2 'person':'intercept':'pass':'in':'NNP'NVNPN 2 'person':'intercept':'pass':'against':'NNP'NVNPN 1 'cornerback':'intercept':'pass':'by':'NNP'All these queries accumulate evidence over thepreposition ?by?
(?pass:by:Steve_Walsh?
).Finding the verbsThe next exercise is to find a verb able to givemeaning to syntactic dependencies such as?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?,?Young:nn:pass?
or ?pass:to:Brent_Jones?.We can ask the BKB what instances (NNP) dowith passes.
The most frequent propositions are:NVN 2241 'NNP':'catch':'pass'NVN 2106 'NNP':'throw':'pass'NVN 844 'NNP':'complete':'pass'NVN 434 'NNP':'intercept':'pass'?NVNPN 758 'NNP':'throw':'pass':'to':'NNP'NVNPN 562 'NNP':'catch':'pass':'for':'yard'NVNPN 338 'NNP':'complete':'pass':'to':'NNP'NVNPN 255 'NNP':'catch':'pass':'from':'NNP'Considering the evidence of ?Brent_Jones?
beinginstance of ?end?
(tight end), if we ask the BKBabout the most frequent relations between ?end?and ?pass?
we find:NVN 28 'end':'catch':'pass'NVN 6 'end':'drop':'pass'So, in this case, the BKB suggests that thesyntactic dependency ?pass:to:Brent_Jones?means ?Brent Jones is an end catching a pass?.Or in other words, that ?Brent_Jones?
has a roleof ?catch-ER?
with respect to ?pass?.If we want to accumulate more evidence onthis we can consider NVNPN propositions in-cluding ?touchdown?.
We only find evidence forthe most general classes (NNP and person):NVNPN 189 NNP:'catch':'pass':'for':'touchdown'NVNPN 26 NNP:'complete':'pass':'for':'touchdown'NVNPN 84 person:catch:pass:for:touchdownNVNPN 18 person:complete:pass:for:touchdownThis means that when we have ?touchdown?, wedon?t have counts for the second option?Brent_Jones:drop:pass?, while ?catch?
be-comes stronger.In the case of ?Steve_Walsh:nn:pass?
we hy-pothesize that ?Steve_Walsh?
is a ?quarterback?.Asking the BKB about the most plausible rela-tion between a quarterback and a pass we find:NVN 98 'quarterback':'throw':'pass'NVN 27 'quarterback':'complete':'pass'Again, if we take into account that it is a?touchdown:nn:pass?, then only the second op-tion ?Steve_Walsh:complete:pass?
is consistentwith the NVNPN propositions.
So, in this case,the BKB suggests that the syntactic dependency?Steve_Walsh:nn:pass?
means ?Steve_Walsh is aquarterback completing a pass?.Finally, with respect to ?touchdown:nn:pass?,we can ask about the verbs that relate them:NVN 14 'pass':'set_up':'touchdown'NVN 6 'pass':'score':'touchdown'NVN 5 'pass':'produce':'touchdown'Figure 2 shows the resulting enrichment afterthe process described.4.4 Expansion of relationsSometimes, the sentence shows a verb with morethan two arguments.
In our example, we have?Eric_David:intercept:pass:on:series?.
In thesecases, relations can be expanded into new nodes.Following our example, the new node is theeventuality of ?intercept?
(?intercept-ION?),?Eric_Davis?
is the ?intercept-ER?
and ?pass?
isthe ?intercept-ED?.
Then, the missing informa-tion is attached to the new node (see Figure 3).In addition, we can proceed with the expan-sion of the context considering this new node.For example, we are working with the hypothesisthat ?Steve_Walsh?
is an instance of ?quarter-back?
and thus, its most plausible relations with?pass?
are ?throw?
and ?complete?.
However,now we can ask about the most frequent relationbetween ?quarterback?
and a nominalization ofcatch (28)drop (6)throw (98)complete (27)by forhas-instance (12) has-instance (33)toquarterback endYoung Brent Jones touchdownpassFigure 2.
Enrichment of the noun phrase: ?Youngtouchdown pass to Brent Jones?983?intercept?.
The most frequent proposition is?quarterback:throw:interception?, supported 35times in the collection.
In this way, we have in-ferred that the nominalization for the eventualityof intercept is interception (in our documents).Two further actions are possible: reinforce thehypothesis of ?throw:pass?
instead of ?com-plete:pass?
and add the hypothesis that?Steve_Walsh:throw:interception?.Finally, notice that since ?set_up?
doesn?tneed to accommodate more arguments, we canmaintain the collapsed edge.4.5 Constraining the interpretationsSome of the inferences being performed are localin the sense that they involve only an entity and arelation.
However, these local inferences must becoherent both with the sentence and the completedocument.
To ensure this coherence we can useadditional information as a way to constrain dif-ferent hypotheses.
In section 4.3 we showed theuse of NVNPN propositions to constrain NVNones.
Another example is the case of ?Er-ic_Davis:intercept:pass?.
We can ask the BKBfor the entity classes that participate in such kindof proposition:NVN 75 'person':'intercept':'pass'NVN 14 'cornerback':'intercept':'pass'NVN 11 'defense':'intercept':'pass'NVN 8 'safety':'intercept':'pass'NVN 7 'group':'intercept':'pass'So the local inference for the kind of entity?Eric_Davis?
is (cornerback) must be coherentwith the fact that it intercepted a pass.
In thiscase ?cornerback?
and ?person?
are properlyreinforced.
In some sense, we are using theseadditional constrains as selectional preferences.5 EvaluationProperly evaluating the enrichment process isvery difficult.
Ideally, one would compare theoutput of an enrichment engine?a text graphfully fleshed out with additional knowledge?toa gold-standard graph containing all relevant in-formation explicitly, and measure Recall andPrecision of the links added by enrichment.
Butsince we have no gold standard examples, and itis unclear how much knowledge should be in-cluded manually if one were to try to build some,two options remain: extrinsic evaluations andmeasuring the utility of the BKB in providingknowledge.
We are in the process of performingan extrinsic evaluation, by measuring how muchQA about the text read improves using theenriched representation.
We report here the re-sults of comparing the utility, for enrichmentpurposes, of two other publicly available back-ground knowledge bases: DART (Clark and Har-rison, 2009) and TextRunner (Banko et al 2007).5.1 Ability to answer about instancesAs shown in our examples, BKBs need the abili-ty to answer about instances and their classes.The BKBs don?t need to be completely popu-lated, but at least have enough instance-class at-tachments in order to allow analogy.Neither DART nor TextRunner allow askingabout possible classes for a particular instance.This is out of the scope of TextRunner.
InDART, instances are replaced by one of threebasic categories (person, place, organization).Although storing the original proper nouns at-tached to the assigned class would bestraightforward, these three general classes arenot enough to support inference.
This leads us tothe next ability.5.2 Ability to discover new classes andrelationsWhile quarterbacks throw passes, ends usuallycatch or drop them.
As we have shown in ourexamples, classifying them as ?person?
or even?player?
is not specific enough for enrichment.Using a predefined set of entity classes doesn?tseem a good approach for midterm goals.
First,human abstraction is not correlated with the ap-propriate granularity level that enable recoveringintercept-ERthrow (98)complete (27)onhas-instance (17) possinterceptquarterback San FranciscoSteveWalshEricDavisintercept-IONpassFigure 3.
Expansion of ?intercept?
relationintercept-EDthrow (35)seriesnn984of relevant background knowledge.
Second, an-notation will be needed for training.In our Proposition Stores, we count simplywhat is explicitly said in the texts about our in-stances.
This seems correlated to an appropriatelevel of granularity.
Furthermore, an instance canbe attached to several classes that can be compat-ible (quarterback, player, person, leader, veteran,etc.).
Frequencies tell us the classes we have toconsider in the first place in order to find a cohe-rent interpretation of the text.5.3 Ability to constrain interpretationand accumulate evidenceEnrichment must be guided by the coherence ofthe ensuing interpretation.
For this reason BKBsmust allow different types of queries over thesame elements.
The aim is to constrain as muchas possible the relations we recover to the onesthat give a coherent interpretation of the text.As shown in our example, we require the abili-ty to ask different syntactic contexts/structures(NN, NVNPN, etc.
), not only NVN (subject-verb-object).
Achieving this is very difficult forapproaches that don?t use parsing.5.4 Ability to digest enough knowledgeadapted to the domainNone of the abilities discussed above are relevantif the BKB doesn?t contain enough knowledgeabout the domain in which we want to enrichdocuments.
To evaluate, we ran three simplequeries related to the US football domain in or-der to assess the suitability of the BKBs forenrichment: What do quarterbacks do withpasses?
What do persons do with passes?
Whointercepts passes?
Table 1 shows the results ob-tained with DART, TextRunner and our BKB.Although DART is a general domain BKBbuilt using parsing, its approach doesn?t allowone to process enough information to answer thefirst question (first row in Table 1).
A web scaleresource such as TextRunner is better suited forthis purpose.
However, results show its lack ofnormalization.
On the other hand, our BKB isable to return a clean and relevant answer.The second question (second row) shows theability of the three BKBs to deal with a basicabstraction needed for inference.
Since TextRun-ner doesn?t perform any kind of processing overentities or pronouns, it doesn?t recover relevantknowledge for this question in the football do-main.
In addition, the table shows the need fordomain adaptation: most of the TextRunner rela-tions, such as ?person:gets:pass?
or ?per-son:bought:pass?, refer to different domains.DART shows the same effect: the first two en-tries (?person:make:pass?, ?person:take:pass?
)belong to different domains.DART1 TextRunner2 BKB (Football)(no results) (~200) threw(~100) completed(36) to throw(26) has thrown(19) makes(19) has(18) fires(99) throw(25) complete(7) have(5) attempt(5) not-throw(4) toss(3) release(47) make(45) take(36) complete(30) throw(25) let(23) catch(1) make(1) expect(22) gets(17) makes(10) has(10) receives(7) who has(7) must have(6) acting on(6) to catch(6) who buys(5) bought(5) admits(5) gives(824) catch(546) throw(256) complete(136) have(59) intercept(56) drop(39) not-catch(37) not-throw(36) snare(27) toss(23) pick off(20) run(13) person(6) person/place/ organi-zation(2) full-back(1) place(30) Early(26) Two plays(24) fumble(20) game(20) ball(17) Defensively(75) person(14) cornerback(11) defense(8) safety(7) group(5) linebackerTable 1.
Comparison of DART, TextRunner and ourBKB for the following queries (rows): (1) quarter-back:X:pass, (2) person:X:pass, (3) X:intercept:pass.Frequencies are in parentheses.Finally, the third question is aimed at recover-ing possible agents (those that intercept passes inour case).
Again, as shown in DART, the re-duced set of classes given by the entity recogniz-er is not enough for the football domain.
Buthaving no classes (TextRunner) is even worse,showing its orientation to discovering relationsrather than to generalizing and answering abouttheir possible arguments.
Our approach is able todiscover plausible agent-classes for the query.Other queries related to the football domainshow the same behavior.1 Available at http://userweb.cs.utexas.edu/users/pclark/dart/2 After aggregating partial results for each cluster usinghttp://www.cs.washington.edu/research/textrunner/9856 Related WorkOur approach lies between macro-reading andOpen Information Extraction (OIE).
Macro-reading (Mitchell et al 2009) is a different taskfrom ours; it seeks to populate ontologies.
Hereconcepts and relations are predefined by the on-tology.OIE (Banko et al 2007) does not use a prede-fined set of semantic classes and relations and isaimed at web scale.
For this reason the frame-work does not include a complete NLP pipeline.The resulting lack of term normalization and ab-sence of domain adaptation (e.g., the query per-son:X:pass return throw but also buy) makes theresults less relevant to single-document reading.When, as with DART, the complete NLP pipe-line is applied over a general corpus, the amountof information to be processed has to be limiteddue to computational cost.
Ultimately, too littleknowledge remains for working in a specificdomain.
For example, asking DART about?quarterback:X:pass?
produces no results.Our approach takes advantage of both worlds,ensuring that enough amounts of documents re-lated to the domain will be processed with acomplete NLP pipeline.
Doing so providescleaner and canonical representations (our propo-sitions) and even higher counts than TextRunnerfor our domain.
This level of processing will bescalable in the midterm; various people including(Huang and Sagae, 2010) are working in lineartime parsers with state-of-the-art performance.Another intermediate point between a collec-tion of domain documents and the general web,reached by restricting processing to the results ofa web query, is explored in IE-on-demand (Se-kine 2006; Shinyama and Sekine 2006).
Howev-er, they use a predefined set of entity classes,preventing from discovering the appropriate gra-nularity level that enables retrieval of relevantbackground knowledge.
We do not predefine theconcepts/classes and relations, but discover themfrom what it is explicitly said in the collection.The process of building the BKB describedhere is closely related to DART (Clark and Har-rison, 2009) which in turn is related to KNEXT(Van Durme and Schubert, 2008).
Perhaps themost important extension we performed is theinclusion of lexical relations (like ?has-instance?)
that activate more powerful uses ofthe Proposition Stores.7 Conclusions and Future WorkIn building a BKB, limiting oneself to a specificdomain provides some powerful benefits.
Ambi-guity is reduced inside the domain, makingcounts in propositions more accurate.
Also, fre-quency distributions of propositions differ fromone domain to another.
For example, the list ofthe most frequent NVN propositions in our BKB(see Section 3.1) is, by itself, an indication of themost salient and important events specifically inthe US football domain.
Furthermore, the amountof text required to build the BKB is reduced sig-nificantly allowing processing such as parsing.The task of inferring omitted but necessary in-formation is a significant part of automated textinterpretation.
In this paper we show that evensimple kinds of information, gleaned relativelystraightforwardly from a parsed corpus, can bequite useful.
Though they are still lexical andnot even starting to be semantic, propositionsconsisting of verbs as relations between nounsseem to provide a surprising amount of utility.
Itremains a research problem to determine whatkinds and levels of knowledge are most useful inthe long run.In the paper, we discuss only the propositionsthat are grounded in instantial statements aboutplayers and events.
But for true learning byreading, a system has to be able to recognizewhen the input expresses general rules, and toformulate such input as axioms or inferences.
Inaddition is the significant challenge of generaliz-ing certain kinds of instantial propositions toproduce inferences.
At which point, for exam-ple, should the system decide that ?all footballplayers have teams?, and how should it do so?This remains a topic for future work.A further topic of investigation is the time atwhich expansion should occur.
Doing so at ques-tion time, in the manner of traditional task-oriented back-chaining inference, is the obviouschoice, but some limited amount of forwardchaining at reading time seems appropriate too,especially if it can significantly assist with textprocessing tasks, in the manner of expectation-driven understanding.Finally, as discussed above, the evaluation ofintrinsic evaluation procedures remains to be de-veloped.986AcknowledgmentsWe are grateful to Hans Chalupsky and DavidFarwell for their comments and input for thiswork.
We acknowledge the builders of TextRun-ner and DART for their willingness to make theirresources openly available.This work has been partially supported by theSpanish Government through the "Programa Na-cional de Movilidad de Recursos Humanos delPlan Nacional de I+D+i 2008-2011?
(GrantPR2009-0020).
Research supported in part byAir Force Contract FA8750-09-C-0172 under theDARPA Machine Reading Program.ReferencesBanko, M., Cafarella, M., Soderland, S., Broadhead,M., Etzioni, O.
2007.
Open Information Extrac-tion from the Web.
IJCAI 2007.Barker, K. 2007.
Building Models by Reading Texts.Invited talk at the AAAI 2007 Spring Symposiumon Machine Reading, Stanford University.Clark, P. and Harrison, P. 2009.
Large-scale extrac-tion and use of knowledge from text.
The FifthInternational Conference on Knowledge Capture(K-CAP 2009).http://www.cs.utexas.edu/users/pclark/dart/Hobbs, J.R., Stickel, M., Appelt, D. and Martin, P.,1993.
Interpretation as Abduction.
Artificial In-telligence, Vol.
63, Nos.
1-2, pp.
69-142.http://www.isi.edu/~hobbs/interp-abduct-ai.pdfHuang, L. and Sagae, K. 2010.
Dynamic Program-ming for Linear-Time Shift-Reduce Parsing.ACL 2010.Klein, D. and Manning, C.D.
2003.
Accurate Unlexi-calized Parsing.
Proceedings of the 41st Meetingof the Association for Computational Linguistics,pp.
423-430Marneffe, M. and Manning, C.D.
2008.
The Stanfordtyped dependencies representation.
In COLING2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.Mitchell, T. M., Betteridge, J., Carlson, A., Hruschka,E., and Wang, R. Populating the Semantic Webby Macro-reading Internet Text.
The SemanticWeb - ISWC 2009.
LNCS Volume 5823.
Sprin-ger-Verlag.Sekine, S. 2006.
On Demand Information Extraction.COLING 2006.Shinyama, Y. and Sekine, S. 2006.
Preemptive Infor-mation Extraction using Unrestricted RelationDiscovery.
HLT-NAACL 2006.Sperber, D. and Wilson, D. 1995.
Relevance: Com-munication and cognition (2nd ed.)
Oxford,Blackwell.Van Durme, B., Schubert, L. 2008.
Open KnowledgeExtraction through Compositional LanguageProcessing.
Symposium on Semantics in Systemsfor Text Processing, STEP 2008.987
