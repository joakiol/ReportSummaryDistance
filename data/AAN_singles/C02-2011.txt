Semantic Case Role Detection for Information ExtractionRik DE BUSSER and Roxana ANGHELUTA and Marie-Francine MOENSInterdisciplinary Centre for Law and ITKatholieke Universiteit LeuvenTiensestraat 41B-3000 Leuven, Belgiumrik.debusser, roxana.angheluta, marie-france.moens@law.kuleuven.ac.beAbstractIf information extraction wants to make itsresults more accurate, it will have to resortincreasingly to a coherent implementation ofnatural language semantics.
In this paper, wewill focus on the extraction of semantic caseroles from texts.
After setting the essentialtheoretical framework, we will argue that itis possible to detect case roles on the basisof morphosyntactic and lexical surfacephenomena.
We will give a conciseoverview of our methodology and of apreliminary test that seems to confirm ourhypotheses.IntroductionInformation extraction (IE) from texts currentlyreceives a large research interest.
Traditionally,it has been associated with the ?
often verbatim?
extraction of domain-specific informationfrom free text (Riloff & Lorenzen 1999).
Inputdocuments are scanned for very specific relevantinformation elements on a particular topic,which are used to fill out empty slots in apredefined frame.
Other types of systems try toacquire this knowledge automatically bydetecting reoccurring lexical and syntacticinformation from manually annotated exampletexts (e.g.
Soderland 1999).Most of these techniques are inherently limitedbecause they exclude natural language semanticsas much as possible.
This is understandable forreasons of efficiency and genericity but itrestricts the algorithms' possibilities and itdisregards the fact that ?
at least in free text ?
IEhas much to do with identifying semantic roles.In most of these systems, case role detection as agoal in itself has been treated in a rather trivialway.
Our research will try to provide asystematic approach to case role detection as anindependent extraction task.
Using notions fromsystemic-functional grammar and presupposinga possible mapping between morphosyntacticproperties and functional role patterns, we willdevelop a general model for case role extraction.The idea is to learn domain-independent caserole patterns from a tagged corpus, which arethen (automatically) specialized to particulardomain-dependent case role sets and which canbe reassigned to previously unseen text.
In thispaper, we will focus on the first part of this task.For IE, an accurate and speedy detection offunctional case roles is of major importance,since they describe events (or states) andparticipants to these events and thus allow foridentifying real-world entities, their propertiesand interactions between them.1 Theoretical settingOne of the earliest and most notable accounts oncase roles is without any doubt CharlesFillmore's groundbreaking article (Fillmore1968).
His most fundamental argument is thatthe notion of case is not so much connected tomorphosyntactic surface realisation as tosyntactico-semantic categories in the deepstructure of a language.
Particular constellationsof case roles determine distinctive functionalpatterns, a considerable part of which (accordingto Fillmore) is likely to be universally valid.This deep-structure case system can be realizedin the surface structure by means of a set oflanguage-dependent transformation rules (seeFillmore 1968).
As a consequence there has tobe a regular mapping between the case systemand its surface realization ?
which includes casemarkers, word order, grammatical roles, etc.
Forour research, we will disregard thetransformational dimension in Fillmore's theorybut we will nevertheless assume that there is atleast some degree of correspondence betweenthe case role system underlying a language andits (1) morphosyntax, (2) relative word order and(3) lexicon.In Halliday's systemic-functional grammar(Halliday 1994; Halliday & Matthiessen 1999),functional patterns that are part of the language'sdeep structure are organized as figures, i.e.configurations of case roles which consist of:1.
A nuclear process, which is typically realizedby a verb phrase.
Processes express an eventor state as it is distinctly perceived by thelanguage user.2.
A limited number of participants, which areinherent to the process and are typicallyrealized by noun phrases.
They represententities or abstractions that participate in theprocess.3.
An in theory unlimited number ofcircumstantial elements.
Circumstances arein most cases optional and are typicallyrealized by prepositional or adverbialphrases.
They allocate the process and itsparticipants in a temporal, spatial, causal, ?context.Processes are classified into types and subtypes,each having its particular participantcombinations.
We discern four main processtypes: Material, Mental, Verbal and Relational(Halliday 1994).
Figure 1 is an example of averbal process, the Sayer being the participant'doing' the process and the Receiver the one towhom the (implicit) verbal message is directed.Invesco in merger talks with AIM ManagementSayer Verbal Process ReceiverFigure 1 ?
Example of a verbal processSince these main types (and some secondaryones) correspond to universal experiential modi,it is to be expected that they will have a certainuniversal validity, i.e.
that they are in some wayor another present in all languages of the world.For our preliminary experiments, we use areduced version of the case role model proposedby Halliday (1994, p. 106-175), as it is aconsistent, well-developed and relatively simplesystem, which makes it very suitable for testingthe validity of our assumptions.
For actualapplications, we will replace it by a moreelaborate variant, most likely Bateman'sGeneralized Upper Model (Bateman 1990;Bateman et al in progress).
Bateman's model isfiner-grained than Halliday's; it is to a largeextent language-independent; and it has beenspecifically developed for implementation intoNLP systems (see Bateman et al in progress).2 Our approachGiven the framework outlined above, weconsider case role detection to be a standardclassification task.
In pattern classification oneattempts to learn certain patterns or rules fromclassified examples and to use them forclassifying previously unseen instances (Hand1997).
In our case, a class is a concatenation ofcase roles that constitute one particular process(i.e.
the deep structure figure) and the patternitself is to be derived from the morphosyntacticand lexical properties corresponding to thatprocess (its surface realisation).Taking that point-of-view, individualrealisations of figures ?
roughly correspondingto stripped-down clauses ?
are translated intofixed-length sets of lexical and morphosyntacticfeatures (word order is implicitly encoded) and afunctional classification is manually assigned tothem.
For each verb the classification algorithmthen attempts to match all functional patterns toone or a few relevant sets of distinctive features.The latter are translated into patterns that can beused to match an occurrence in a text to aparticular constellation of case roles.The entire learning process consists of five mainsteps:1.
Preprocessing2.
Annotation3.
Feature selection4.
Training of the classifier5.
Translation into rulesIn the preprocessing phase, the input text istagged, lemmatised and chunked.
The output isstandardized and passed to the annotation tool,in which the user is asked to assign case rolepatterns to individual clauses.
For now, we willonly take into account processes, participantsand circumstantial elements of Extent andLocation.In a next step, individual training examples ?each example corresponding to one figure ?
areconverted to a fixed-length feature vector.
Foreach phrase, the lexical and morphosyntacticfeatures of the head and of the left and rightcontext boundaries (i.e.
the first and the lasttoken of the strings pre- and postmodifying thehead) are automatically extracted from thetagged text and added to the vector.
This enablesus to align corresponding features quiteaccurately without having to resort to anycomplex form of phrasal analysis.
Although thisreduction of the context of the head word mayseem to be counter-intuitive from a grammaticalpoint-of-view, our initial tests indicate that itdoes capture most constructions that are relevantto the extraction task.Feature selection is necessary for two mainreasons.
Firstly, it is impossible to take intoaccount all lexical and morphosyntactic features,since that would boost the time-complexity,incorporate many irrelevant features and bringdown accuracy when a limited set of trainingexamples is available.
Secondly, naturallanguage utterances have the uncanny habit ofbeing of variable length.
The latter aspect isproblematic not only because classificationalgorithms usually expect a clearly delineatedset of features, but also because it is crucial toalign examples in order to comparecorrespondent features.In our test setting, we will constrain the maximalnumber of case roles per figure to four.
Sinceeach case role is transformed into a set of 10features, a figure will be translated into a 40-dimensional feature vector (see Figure 2).As a result, a particular constellation of caseroles is treated as one pattern in which each roleand each of its relevant features has a fixedposition.
We expect this vector representation tobe relevant in most languages apart from freeword order languages.
Currently, our modelfocuses on English.In the fourth step, the classifier is trained todiscriminate features that are distinctive for eachprocess type associated with a particular verb.These features are again translated into rules thatcan be used for reassigning case roles that havebeen learned to previously unseen text.This is necessary because the variable length offigures and ?
within figures ?
of phrases isbound to cause difficulties when applying thepatterns that were learned to new sentences.Rules have the advantage over feature vectors inthat they allow us to use head-centredstretching: when figures are assigned topreviously unseen sentences and no pattern canimmediately be matched, the nearest equivalentaccording to the head of the figure will beassigned; the rest of the pattern will be allocatedby shifting the left and right context of the headtowards the left and right sentence boundaries.
Asimilar approach will be used for matchingindividual roles to phrases.3 An experimentBefore engaging in the laboursome task ofbuilding a set of tools and tagging an entirecorpus, we decided to test the practical validityof our ideas on a small scale on the verb be.
Wemanually constructed a limited set of trainingexamples (76 occurrences) from the new Reuterscorpus (CD-rom, Reuters Corpus.
Volume 1:English Language, 1996-08-20 to 1997-08-19)and processed it with the C4.5 classificationalgorithm (Quinlan 1993).Figure 3 gives an overview of the process.
Thetagged text1 (step 1) is translated into a set of1 For our first experiment we used TnT(http://www.coli.uni-sb.de/~thorsten/tnt/).
In ourFigure 2 ?
The feature setfeatures (step 2).
A functional pattern is used asthe class corresponding to the feature set (thelast entry in step 2).
The classifier extracts oneor more distinctive features (step 3), which arein turn transposed into a rule (step 4) that is usedin case role assignment.Figure 3 ?
Schematic illustration of the experimentInitial results are encouraging.
The evaluationcomponent of C4.5 revealed an error rate of9.2% when reapplying its rule extractions on thetraining data.
Given the limited amount of data,these results are reasonable.
Manual applicationof the rules (from step 4) to new text confirmedtheir natural look-and-feel.
We are currentlytesting the approach with larger amounts oftraining and testing data.
Most of the currenterrors are caused by the limited amount oftraining data in our experiment: in a number ofcases there was only one instance of a particularfigure.4 Discussion and future improvementsAlthough most shortcomings that arose in ourpresent set-up can be settled relatively easily, anumber of issues still remains to be resolved.From a theoretical angle, the most urgentproblem is the underspecification of the materialdomain (or the disagreement on exactly howmaterial processes ought to be subclassified).Unfortunately, most verb meanings are materialpresent experiment, it is replaced by LT POS(http://www.ltg.ed.ac.uk/~mikheev/software.html).We manually lemmatised the tokens, but we arecurrently using a lemmatizer based on WordNet.and distinctions in the material domain tend tobe rather crucial in most IE applications.Two major implementational difficulties arerelated to circumstantial elements.
Sincecircumstances are normally not inherent to theprocess, they do not tend to have a fixed positionin a figure.
In addition, no formal parametersexist to distinguish obligatory circumstancesfrom optional ones.
Since it would be absurd toencode all variation in separate patterns, it istempting just to add empty slots at the mostpredictable positions where circumstances couldappear, but that would still not tell apart optionaland obligatory circumstances and it would be arather ad hoc solution.
We are currentlyinvestigating whether both problems might bedealt with by encoding the relative position ofcase roles explicitly.In our current information society, it willbecome increasingly important to extractinformation on well-specified events or entitiesfrom documents.
Case role detection willprovide a way to do this by integrating 'real'semantics into the systems withoutoverburdening the algorithms.
For instance, inour example analysis (Figure 1) we canimmediately identify two entities involved in acommunicative action, one that does the talking('Invesco') and one that is being talked to ('AIMmanagement').
An immediate application of caserole detection is straightforward IE, whichtypically attempts to extract specific informationfrom a text.
However, the algorithm could alsobe used for optimising information retrievalapplications, in the construction of knowledgebases, in questioning-answering systems or incase-based reasoning.
Actually, for real naturallanguage understanding a highly accurate modelfor interpreting case roles in some form will beunavoidable.A major advantage of our approach is that thepattern base resulting from it will containsemantic information and yet be fully domain-independent.
In a next stage of our research, wewill try to specialize the generic case rolesautomatically to domain-dependent ones.
At firstsight, this two-step approach might appearcumbersome, but it will enable us to easilyexpand the pattern base while reusing the hard-won patterns.5 Related researchHistorically, case role detection has its roots inframe-based approaches to IE (e.g.
Schank &Abelson 1977).
The main problem here is that tobuild case frames one needs prior knowledge onwhich information exactly one wants to extract.In recent years, different solutions have beenoffered to automatically generate those framesfrom annotated examples (e.g.
Riloff &Schmelzenbach 1998, Soderland 1999) or byusing added knowledge (e.g.
Harabagiu &Maiorano 2000).
Many of those approacheswere very successful but most of them have atendency to blend syntactic and semanticconcepts and they still have to be trained onindividual domains.Some very interesting research on case framedetection has been done by Gildea (Gildea 2000,Gildea 2001).
He uses statistical methods tolearn case frames from parsed examples fromFrameNet (Johnson et al 2001).ConclusionThere is a definite need for case role analysis inIE and in natural language processing in general.In this article, we have tried to argue that genericcase role detection is possible by using shallowtext analysis methods.
We outlined ourfunctional framework and presented a modelthat considers case role pattern extraction to be astandard classification task.
Our main focus forthe near future will be on automating as manyaspects of the annotation process as possible andon the construction of the case role assignmentalgorithm.
In these tasks, the emphasis will beon genericity and reusability.AcknowledgementsWe would like to thank the Institute for thePromotion of Innovation by Science andTechnology in Flanders (IWT-Flanders) for theresearch funding.ReferencesBateman J.
(1990) Upper Modelling: a generalorganization of knowledge for natural languageprocessing.
In "Proceedings of the InternationalLanguage Generation Workshop", Pittsburgh, June1990.Bateman, J.
(in progress) The Generalized UpperModel 2.0. http://www.darmstadt.gmd.de/publish/komet/gen-um/newUM.html.
Checked 15 February2002.Fillmore Ch.
(1968) The case for case.
In "Universalsin Linguistic Theory", E. Bach & R.T. Harms, ed.,Holt, Rinehart and Winston, New York, pp.
1-88.Gildea D. (2000) Automatic labeling of semanticroles.
Qualifying exam proposal, University ofCalifornia, January 2000, 21 p.Gildea D. (2001) Statistical Language UnderstandingUsing Frame Semantics.
PhD.
dissertation,University of California at Berkeley, 2001, 109 p.Halliday M.A.K.
(1994) An introduction to functionalgrammar.
Arnold, London, 434 p.Halliday M.A.K.
and Matthiessen C. (1999)Construing Experience Through Meaning.
ALanguage-Based Approach to Cognition.
Cassell,London, 657 p.Hand D. (1997) Construction and Assessment ofClassification Rules.
Chichester: John Wiley &Sons, Chichester, 214 p.Harabagiu S. and Maiorano S. (2000) Acquisition oflinguistic patterns for knowledge-basedinformation extraction.
In "Proceedings of LREC-2000", Athens, June 2000.Johnson C. et al (2001).
The FrameNet Project:Tools for Lexicon Building.
http://www.icsi.berkeley.edu/~framenet/book.pdf.
Checked 15February 2002.Quinlan J.
(1993) C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, 302 p.Riloff E. and Lorenzen J.
(1999) Extraction-basedtext categorization: generating domain-specificrole relationships automatically.
In "NaturalLanguage Information Retrieval", T.
Strzalkowski,ed., Kluwer Academic Publishers, Dordrecht, pp.167-195.Riloff E. and Schelzenbach M. (1998) An empiricalapproach to conceptual case frame acquisition.
In"Proceedings of the Sixth Workshop on Very largeCorpora", Montreal, Canada, August 1998.Schank R. and Abelson R. (1977) Scripts, Plans,Goals and Understanding.
An Inquiry into HumanKnowledge Structures.
Erlbaum, Hillsdale, NJ,248p.Soderland S. (1999) Learning information extractionrules for semi-structured and free text.
In MachineLearning 34, 1/3, pp.
233-272.
