Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 443?449, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUMCC_DLSI-(SA): Using a ranking algorithm and informal featuresto solve Sentiment Analysis in TwitterYoan Guti?rrez, Andy Gonz?lez,Roger P?rez, Jos?
I. AbreuUniversity of Matanzas, Cuba{yoan.gutierrez, roger.perez,jose.abreu}@umcc.cu,andy.gonzalez@infonet.umcc.cuAntonio Fern?ndez Orqu?n,Alejandro Mosquera, Andr?sMontoyo, Rafael Mu?ozUniversity of Alicante, Spainantonybr@yahoo.com,{amosquera, montoyo,rafael}@dlsi.ua.esFranc CamaraIndependent ConsultantUSAinfo@franccamara.comAbstractIn this paper, we describe the developmentand performance of the supervised systemUMCC_DLSI-(SA).
This system uses corporawhere phrases are annotated as Positive,Negative, Objective, and Neutral, to achievenew sentiment resources involving worddictionaries with their associated polarity.
Asa result, new sentiment inventories areobtained and applied in conjunction withdetected informal patterns, to tackle thechallenges posted in Task 2b of the Semeval-2013 competition.
Assessing the effectivenessof our application in sentiment classification,we obtained a 69% F-Measure for neutral andan average of 43% F-Measure for positiveand negative using Tweets and SMSmessages.1 IntroductionTextual information has become one of the mostimportant sources of data to extract useful andheterogeneous knowledge from.
Texts can providefactual information, such as: descriptions, lists ofcharacteristics, or even instructions to opinion-based information, which would include reviews,emotions, or feelings.
These facts have motivateddealing with the identification and extraction ofopinions and sentiments in texts that requirespecial attention.Many researchers, such as (Balahur et al 2010;Hatzivassiloglou et al 2000; Kim and Hovy,2006; Wiebe et al 2005) and many others havebeen working on this and related areas.Related to assessment Sentiment Analysis (SA)systems, some international competitions havetaken place.
Some of those include: Semeval-2010(Task 18: Disambiguating Sentiment AmbiguousAdjectives 1 ) NTCIR (Multilingual OpinionAnalysis Task (MOAT 2)) TASS 3  (Workshop onSentiment Analysis at SEPLN workshop) andSemeval-2013 (Task 2 4  Sentiment Analysis inTwitter) (Kozareva et al 2013).In this paper, we introduce a system for Task 2b) of the Semeval-2013 competition.1.1 Task 2 DescriptionIn participating in ?Task 2: Sentiment Analysis inTwitter?
of Semeval-2013, the goal was to take agiven message and its topic and classify whether ithad a positive, negative, or neutral sentimenttowards the topic.
For messages conveying, both apositive and negative sentiment toward the topic,the stronger sentiment of the two would end up asthe classification.
Task 2 included two sub-tasks.Our team focused on Task 2 b), which providestwo training corpora as described in Table 3, andtwo test corpora: 1) sms-test-input-B.tsv (with2094 SMS) and 2) twitter-test-input-B.tsv (with3813 Twit messages).The following section shows some backgroundapproaches.
Subsequently, in section 3, wedescribe the UMCC_DLSI-(SA) system that wasused in Task 2 b).
Section 4 describes theassessment of the obtained resource from theSentiment Classification task.
Finally, theconclusion and future works are presented insection 5.2 BackgroundThe use of sentiment resources has proven to be anecessary step for training and evaluating  systemsthat implement sentiment analysis, which also1 http://semeval2.fbk.eu/semeval2.php2 http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/3 http://www.daedalus.es/TASS/4http://www.cs.york.ac.uk/semeval-2013/task2/443include fine-grained opinion mining (Balahur,2011).In order to build sentiment resources, severalstudies have been conducted.
One of the first is therelevant work by (Hu and Liu, 2004) using lexiconexpansion techniques by adding synonymy andantonym relations provided by WordNet(Fellbaum, 1998; Miller et al 1990) Another oneis the research described by (Hu and Liu, 2004;Liu et al 2005) which obtained an OpinionLexicon compounded by a list of positive andnegative opinion words or sentiment words forEnglish (around 6800 words).A similar approach has been used for buildingWordNet-Affect (Strapparava and Valitutti, 2004)which expands six basic categories of emotion;thus, increasing the lexicon paths in WordNet.Nowadays, many sentiment and opinionmessages are provided by Social Media.
To dealwith the informalities presented in these sources, itis necessary to have intermediary systems thatimprove the level of understanding of themessages.
The following section offers adescription of this phenomenon and a tool to trackit.2.1 Text normalizationSeveral informal features are present in opinionsextracted from Social Media texts.
Some researchhas been conducted in the field of lexicalnormalization for this kind of text.
TENOR(Mosquera and Moreda, 2012) is a multilingualtext normalization tool for Web 2.0 texts with anaim to transform noisy and informal words intotheir canonical form.
That way, they can be easilyprocessed by NLP tools and applications.
TENORworks by identifying out-of-vocabulary (OOV)words such as slang, informal lexical variants,expressive lengthening, or contractions using adictionary lookup and replacing them by matchingformal candidates in a word lattice using phoneticand lexical edit distances.2.2 Construction of our own SentimentResourceHaving analyzed the examples of SA described insection 2, we proposed building our own sentimentresource (Guti?rrez et al 2013) by adding lexicaland informal patterns to obtain classifiers that candeal with Task 2b of Semeval-2013.
We proposedthe use of a method named RA-SR (using RankingAlgorithms to build Sentiment Resources)(Guti?rrez et al 2013) to build sentiment wordinventories based on senti-semantic evidenceobtained after exploring text with annotatedsentiment polarity information.
Through thisprocess, a graph-based algorithm is used to obtainauto-balanced values that characterize sentimentpolarities, a well-known technique in SentimentAnalysis.
This method consists of three key stages:(I) Building contextual word graphs; (II) Applyinga ranking algorithm; and (III) Adjusting thesentiment polarity values.These stages are shown in the diagram in Figure 1,which the development of sentimental resourcesstarts off by giving four corpora of annotatedsentences (the first with neutral sentences, thesecond with objective sentences, the third withpositive sentences, and the last with negativesentences).Figure 1.
Resource walkthrough developmentprocess.2.3 Building contextual word graphsInitially, text preprocessing is performed byapplying a Post-Tagging tool (using Freeling(Atserias et al 2006) tool version 2.2 in this case)to convert all words to lemmas 5 .
After that, allobtained lists of lemmas are sent to RA-SR, thendivided into four groups: neutral, objective,positive, and negative candidates.
As the first set5 Lemma denotes canonic form of the words.Phr se 3Phrase 2W1 W2 W3 W4W5 W3 W2W3 W4 W5 W6W1W7Phrase 1 PositvePhrasesW5 W6 W8 W9W8 W9 W7W6 W9 W10 W11W6W1 W8NegativePhrasesPhrase 3Phrase 2Phrase 1PositiveWordsNegativeWordsW1 W2 W3 W4W5W6 W7W5W6W7W8W9W10W11(I)(II) Reinforcing wordsWeight = 1(II) (II)(I)W ight =1W ight =1Weight =1Weight =1W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11(III)W1Default Weight = 1/N Default Weight = 1/NW1 W2 W3W4W5Phrase 3Phrase 2W1 W2 W3 W4W5 W3 W2W3 W1 W2 W4W1W5Phrase 1 NeutralPhrasesW1 W6 W7 W8W8 W7 W3W6 W8 W7 W5W5W5 W2ObjectivePhrasesPhrase 3Phrase 2Phrase 1(II)W1 W2 W3 W4 W5 W6 W7 W8(II)W1 W2 W3W5W6 W7W8Default Weight = 1/N(I)(I)Default Weight = 1/N444of results, four contextual graphs areobtained:  ???
?,   ????
, ???
?,  and ????
, whereeach graph includes the words/lemmas from theneutral, objective, positive and negative sentencesrespectively.
These graphs are generated afterconnecting all words for each sentence intoindividual sets of annotated sentences inconcordance with their annotations (???
, ???
,??
?, ???
).Once the four graphs representing neutral,objective, positive and negative contexts arecreated, we proceed to assign weights to applygraph-based ranking techniques in order to auto-balance the particular importance of each vertex ?
?into ???
?, ???
?, ????
and ???
?.As the primary output of the graph-based rankingprocess, the positive, negative, neutral, andobjective values are calculated using the PageRankalgorithm and normalized with equation (1).
For abetter understanding of how the contextual graphwas built see (Guti?rrez et al 2013).2.4 Applying a ranking algorithmTo apply a graph-based ranking process, it isnecessary to assign weights to the vertices of thegraph.
Words involved into ???
?, ???
?, ???
?and ????
take the default of 1/N as their weightto define the weight of ?
vector, which is used inour proposed ranking algorithm.
In the case wherewords are identified on the sentiment repositories(see Table 4) as positive or negative, in relation totheir respective graph, a weight value of 1 (in arange [0?1] ) is assigned.
?
represents themaximum quantity of words in the current graph.After that, a graph-based ranking algorithm isapplied in order to structurally raise the graphvertexes?
voting power.
Once the reinforcementvalues are applied, the proposed ranking algorithmis able to increase the significance of the wordsrelated to these empowered vertices.The PageRank (Brin and Page, 1998)adaptation, which was popularized by (Agirre andSoroa, 2009) in Word Sense Disambiguationthematic, and which has obtained relevant results,was an inspiration to us in our work.
The mainidea behind this algorithm is that, for each edgebetween ?i and ?j in graph ?, a vote is made from?
i to ?
j.
As a result, the relevance of ?
j isincreased.On top of that, the vote strength from ?
to ?depends on ????
relevance.
The philosophy behindit is that, the more important the vertex is, themore strength the voter would have.
Thus,PageRank is generated by applying a randomwalkthrough from the internal interconnection of?
, where the final relevance of ??
represents therandom walkthrough probability over ?
, andending on ?
?.In our system, we apply the followingconfiguration: dumping factor ?
= 0.85 and, likein (Agirre and Soroa, 2009) we used 30 iterations.A detailed explanation about the PageRankalgorithm can be found in (Agirre and Soroa,2009)After applying PageRank, in order to obtainstandardized values for both graphs, we normalizethe rank values by applying the equation (1),where ???(??)
obtains the maximum rank valueof ??
vector (rankings?
vector).???
= ???/???(??)
(1)2.5 Adjusting the sentiment polarity valuesAfter applying the PageRank algorithm on????,????
, ????
and ????
, having normalized theirranks, we proceed to obtain a final list of lemmas(named ?? )
while avoiding repeated elements.??
is represented by ???
lemmas, which wouldhave, at that time, four assigned values: Neutral,Objective, Positive, and Negative, all of whichcorrespond to a calculated rank obtained by thePageRank algorithm.At that point, for each lemma from ?
?,  thefollowing equations are applied in order to selectthe definitive subjectivity polarity for each one:???
=  {???
?
???
;  ???
> ??
?0                ; ?????????(2)???
=  {???
?
???
;  ???
> ??
?0                ; ?????????
(3)Where ???
is the Positive value and ???
theNegative value related to each lemma in ?
?.In order to standardize again the ???
and ??
?values and making them more representative in a[0?1] scale, we proceed to apply a normalizationprocess over the ???
and ???
values.From there, based on the objective featurescommented by (Baccianella et al 2010), weassume the same premise to establish analternative objective value of the lemmas.Equation (4) is used for that:??????
= 1 ?
|???
?
??
?| (4)Where ??????
represents the alternativeobjective value.445As a result, each word obtained in the sentimentresource has an associated value of: positivity(???
, see equation (2)), negativity (???
, seeequation (3)), objectivity(????_??
?,  obtained byPageRank over ????
and normalized withequation (1)), calculated-objectivity (?????
?, nowcited as ???_???????? )
and neutrality (???
,obtained by PageRank over ????
and normalizedwith equation (1)).3  System DescriptionThe system takes annotated corpora as input fromwhich two models are created.
One model iscreated by using only the data provided atSemeval-2013 (Restricted Corpora, see Table 3),and the other by using extra data from otherannotated corpora (Unrestricted Corpora, seeTable 3).
In all cases, the phrases are pre-processed using Freeling 2.2 pos-tagger (Atseriaset al 2006) while a dataset copy is normalizedusing TENOR (described in section 2.1).The system starts by extracting two sets offeatures.
The Core Features (see section 3.1) arethe Sentiment Measures and are calculated for astandard and normalized phrase.
The SupportFeatures (see section 3.2) are based on regularities,observed in the training dataset, such asemoticons, uppercase words, and so on.The supervised models are created using Weka6and a Logistic classifier, both of which the systemuses to predict the values of the test dataset.
Theselection of the classifier was made after analyzingseveral classifiers such as: Support VectorMachine, J48 and REPTree.
Finally, the Logisticclassifier proved to be the best by increasing theresults around three perceptual points.The test data is preprocessed in the same waythe previous corpora were.
The same process offeature extraction is also applied.
With theaforementioned features and the generated models,the system proceeds to classify the final values ofPositivity, Negativity, and Neutrality.3.1 The Core FeaturesThe Core Features is a group of measures based onthe resource created early (see section 2.2).
Thesystem takes a sentence preprocessed by Freeling2.2 and TENOR.
For each lemma of the analyzedsentence, ???
, ???
, ???_????????
,  ????_??
?,6 http://www.cs.waikato.ac.nz/and ???
are calculated by using the respectiveword values assigned in RA-SR.
The obtainedvalues correspond to the sum of the correspondingvalues for each intersecting word between theanalyzed sentence (lemmas list) and the obtainedresource by RA-SR. Lastly, the aforementionedattributes are normalized by dividing them by thenumber of words involved in this process.Other calculated attributes are: ???_?????
,???_?????
, ???_????????_?????
,???_????_?????
and ???_?????.
These attributescount each involved iteration for each feature type( ???
, ???
, ????_???
, ??????
and ??
?respectively, where the respective value may begreater than zero.Attributes ???
and cnn are calculated bycounting the amount of lemmas in the phrasescontained in the Sentiment Lexicons (Positive andNegative respectively).All of the 12 attributes described previously arecomputed for both, the original, and thenormalized (using TENOR) phrase, totaling 24attributes.
The Core features are described next.Feature Name Description??
?Sum of respective value of each word.??????_????????????_?????????_????
?Counts the words where its respective valueis greater than zero???_????????_????????_?????????_???_????????_????????
(to positive) Counts the words contained in theSentiment Lexicons for their respectivepolarities.???
(to negative)Table 1.
Core Features3.2 The Support FeaturesThe Support Features is a group of measures basedon characteristics of the phrases, which may helpwith the definition on extreme cases.
The emotPosand emotNeg values are the amount of Positiveand Negative Emoticons found in the phrase.
Theexc and itr are the amount of exclamation andinterrogation signs in the phrase.
The followingtable shows the attributes that represent thesupport features:Feature Name Description??????
?Counts the respective Emoticons??????????
(exclamation marks (?!?
))Counts the respective marks???
(question marks (???))?????_?????
Counts the uppercase words?????_???
Sums the respective values of theUppercase words ?????_????????_???_?????_???
(to Counts the Uppercase words446positivity) contained in their respectiveGraph ?????_???_?????_???(tonegativity)?????_???_?????_????
(topositivity)Counts the Uppercase wordscontained in the SentimentLexicons 7 for their respectivepolarity?????_???_?????_????
(tonegativity)???????_?????
Counts the words with repeatedchars???????_???
Sums the respective values of thewords with repeated chars ???????_??????????_???_?????_????
(innegative lexical resource )Counts the words with repeatedchars contained in the respectivelexical resource ???????_???_?????_????
(inpositive lexical resource )???????_???_?????_???
(inpositive graph )Counts the words with repeatedchars contained in the respectivegraph ???????_???_?????_???
(innegative graph )Table 2.
The Support Features4 EvaluationIn the construction of the sentiment resource, weused the annotated sentences provided by thecorpora described in Table 3.
The resources listedin Table 3 were selected to test the functionality ofthe words annotation proposal with subjectivityand objectivity.
Note that the shadowed rowscorrespond to constrained runs corpora: tweeti-b-sub.dist_out.tsv 8  (dist), b1_tweeti-objorneu-b.dist_out.tsv 9  (objorneu), twitter-dev-input-B.tsv10 (dev).The resources from Table 3 that includeunconstrained runs corpora are: all the previouslymentioned ones, Computational-intelligence11 (CI)and stno12 corpora.The used sentiment lexicons are from theWordNetAffect_Categories13 and opinion-words14files as shown in detail in Table 4.Some issues were taken into account throughoutthis process.
For instance, after obtaining acontextual graph ?, factotum words are present inmost of the involved sentences (i.e., verb ?to be?
).This issue becomes very dangerous after applyingthe PageRank algorithm because the algorithm7 Resources described in Table 4.8Semeval-2013 (Task 2.
Sentiment Analysis in Twitter,subtask b).9Semeval-2013 (Task 2.
Sentiment Analysis in Twitter,subtask b).10 http://www.cs.york.ac.uk/semeval-2013/task2/11A sentimental corpus obtained applying techniquesdeveloped by GPLSI department.
See(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel)12NTCIR Multilingual Opinion Analysis Task (MOAT)http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/13 http://wndomains.fbk.eu/wnaffect.html14 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.htmlstrengthens the nodes possessing many linkedelements.
For that reason, the subtractions ???
????
and ???
?
???
are applied, where the mostfrequent words in all contexts obtain high values.The subtraction becomes a dumping factor.As an example, when we take the verb ?to be?,before applying equation (1), the verb achieves thehighest values in each subjective context graph(????
and ????)
namely, 9.94 and 18.67 rankvalues respectively.
These values, once equation(1) is applied, are normalized obtaining both???
=  1 and ???
=  1 in a range [0...1].
At theend, when the following steps are executed(Equations (2) and (3)), the verb ?to be?achieves ???
= 0 , ???
= 0  andtherefore  ??????
= 1 .
Through this example, itseems as though we subjectively discarded wordsthat appear frequently in both contexts (Positiveand Negative).Corpus N P O NeuObjor NeuUnk TC UCdist 176 368 110 34 - - 688 X Xobjorneu 828 1972 788 1114 1045 - 5747 X Xdev 340 575 - 739 - - 1654 X XCI 6982 6172 - - - - 13154  Xstno15 1286 660 - 384 - 10000 12330  XT 9272 9172 898 1532 1045 10000 31919Table 3.
Corpora used to apply RA-SR.
Positive (P),Negative (N), Objective (Obj/O), Unknow (Unk), Total(T), Constrained (C), Unconstrained (UC).Sources P N TWordNet-Affects_Categories(Strapparava and Valitutti, 2004)629 907 1536opinion-words(Hu and Liu, 2004; Liu et al 2005)2006 4783 6789Total 2635 5690 8325Table 4.
Sentiment Lexicons.
Positive (P), Negative(N) and Total (T).Precision (%) Recall (%) Total (%)C Inc P  N  Neu P N Neu Prec Rec F1Run1 8032 1631 80,7 83,8 89,9 90,9 69,5 86,4 84,8 82,3 82,9Run2 19101 4671 82,2 77,3 89,4 80,7 81,9 82,3 83,0 81,6 80,4Table 5.
Training dataset evaluation using cross-validation (Logistic classifier (using 10 folds)).Constrained (Run1), Unconstrained (Run2), Correct(C),Incorrect (Inc).4.1 The training evaluationIn order to assess the effectiveness of our trainedclassifiers, we performed some evaluation tests.Table 5 shows relevant results obtained afterapplying our system to an environment (specificdomain).
The best results were obtained with the15 NTCIR Multilingual Opinion Analysis Task (MOAT)http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/447restricted corpus.
The information used to increasethe knowledge was not balanced or perhaps is ofpoor quality.4.2 The test evaluationThe test dataset evaluation is shown in Table 6,where system results are compared with the bestresults in each case.
We notice that the constrainedrun is better in almost every aspect.
In the fewcases where it was lower, there was a minimaldifference.
This suggests that the information usedto increase our Sentiment Resource wasunbalanced (high difference between quantity oftagged types of annotated phrases), or was of poorquality.
By comparing these results with the onesobtained by our system on the test dataset, wenotice that on the test dataset, the results fell in themiddle of the effectiveness scores.
After seeingthese results (Table 5 and Table 6), we assumedthat our system performance is better in acontrolled environment (or specific domain).
Tomake it more realistic, the system must be trainedwith a bigger and more balanced dataset.Table 6 shows the results obtained by oursystem while comparing them to the best results ofTask 2b of Semeval-2013.
In Table 5, we can seethe difference between the best systems.
They arethe ones in bold and underlined as target results.These results have a difference of around 20percentage points.
The grayed out ones correspondto our runs.Precision (%) Recall (%) TotalRuns C Inc P N Neu P N Neu Prec Rec F 11_tw 2082 1731 60,9 46,5 52,8 49,8 41,4 64,1 53,4 51,8 49,31_tw_cnd 2767 1046 81,4 69,7 67,7 66,7 60,4 82,6 72,9 69,9 69,02_tw 2026 1787 58,0 42,2 42,2 52,2 43,9 57,4 47,4 51,2 49,02_tw_ter 2565 1248 71,1 54,6 68,6 74,7 59,4 63,1 64,8 65,7 64,91_sms 1232 862 43,9 46,1 69,5 55,9 31,7 68,9 53,2 52,2 43,41_sms_cnd 1565 529 73,1 55,4 85,2 73,0 75,4 75,3 71,2 74,5 68,52_sms 1023 1071 38,4 31,4 68,3 60,0 38,3 47,8 46,0 48,7 40,72_sms_ava 1433 661 60,9 49,4 81,4 65,9 63,7 71,0 63,9 66,9 59,5Table 6.
Test dataset evaluation using official scores.Corrects(C), Incorrect (Inc).Table 6 run descriptions are as follows:?
UMCC_DLSI_(SA)-B-twitter-constrained(1_tw),?
NRC-Canada-B-twitter-constrained(1_tw_cnd),?
UMCC_DLSI_(SA)-B-twitter-unconstrained(2_tw),?
teragram-B-twitter-unconstrained (2_tw_ter),?
UMCC_DLSI_(SA)-B-SMS-constrained(1_sms),?
NRC-Canada-B-SMS-constrained(1_sms_cnd), UMCC_DLSI_(SA)-B-SMS-unconstrained (2_sms),?
AVAYA-B-sms-unconstrained (2_sms_ava).As we can see in the training and testingevaluation tables, our training stage offered morerelevant scores than the best scores in Task2b(Semaval-2013).
This means that we need toidentify the missed features between both datasets(training and testing).For that reason, we decided to check how manywords our system (more concretely, our SentimentResource) missed.
Table 7 shows that our systemmissed around 20% of the words present in the testdataset.hits miss miss (%)twitter 23807 1591 6,26%sms 12416 2564 17,12%twitter nonrepeat   2426 863 26,24%sms norepeat 1269 322 20,24%Table 7.
Quantity of words used by our system overthe test dataset.5 Conclusion and further workBased on what we have presented, we can say thatwe could develop a system that would be able tosolve the SA challenge with promising results.
Thepresented system has demonstrated electionperformance on a specific domain (see Table 5)with results over 80%.
Also, note that our system,through the SA process, automatically buildssentiment resources from annotated corpora.For future research, we plan to evaluate RA-SRon different corpora.
On top of that, we also planto deal with the number of neutral instances andfinding more words to evaluate the obtainedsentiment resource.AcknowledgmentsThis research work has been partially funded bythe Spanish Government through the projectTEXT-MESS 2.0 (TIN2009-13391-C04),"An?lisis de Tendencias Mediante T?cnicas deOpini?n Sem?ntica" (TIN2012-38536-C03-03)and ?T?cnicas de Deconstrucci?n en laTecnolog?as del Lenguaje Humano?
(TIN2012-31224); and by the Valencian Government throughthe project PROMETEO(PROMETEO/2009/199).448ReferencesAgirre, E. and A. Soroa.
Personalizing PageRank forWord Sense Disambiguation.
Proceedings of the12th conference of the European chapter of theAssociation for Computational Linguistics (EACL-2009), Athens, Greece, 2009.Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L.Padr?
and M. Padr?.
FreeLing 1.3: Syntactic andsemantic services in an opensource NLP library.Proceedings of LREC'06, Genoa, Italy, 2006.Baccianella, S.; A. Esuli and F. Sebastiani.SENTIWORDNET 3.0: An Enhanced LexicalResource for Sentiment Analysis and OpinionMining.
7th Language Resources and EvaluationConference, Valletta, MALTA., 2010.
2200-2204 p.Balahur, A.
Methods and Resources for SentimentAnalysis in Multilingual Documents of DifferentText Types.
Department of Software and ComputingSystems.
Alacant, Univeristy of Alacant, 2011.
299.p.Balahur, A.; E. Boldrini; A. Montoyo and P. Martinez-Barco.
The OpAL System at NTCIR 8 MOAT.Proceedings of NTCIR-8 Workshop Meeting,Tokyo, Japan., 2010.
241-245 p.Brin, S. and L. Page The anatomy of a large-scalehypertextual Web search engine Computer Networksand ISDN Systems, 1998, 30(1-7): 107-117.Fellbaum, C. WordNet.
An Electronic LexicalDatabase.
University of Cambridge, 1998. p. TheMIT Press.Guti?rrez, Y.; A. Gonz?lez; A. F. Orqu?n; A. Montoyoand R. Mu?oz.
RA-SR: Using a ranking algorithm toautomatically building resources for subjectivityanalysis over annotated corpora.
4th Workshop onComputational Approaches to Subjectivity,Sentiment & Social Media Analysis (WASSA 2013),Atlanta, Georgia, 2013.Hatzivassiloglou; Vasileios and J. Wiebe.
Effects ofAdjective Orientation and Gradability on SentenceSubjectivity.
International Conference onComputational Linguistics (COLING-2000), 2000.Hu, M. and B. Liu.
Mining and Summarizing CustomerReviews.
Proceedings of the ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining (KDD-2004), USA, 2004.Kim, S.-M. and E. Hovy.
Extracting Opinions, OpinionHolders, and Topics Expressed in Online NewsMedia Text.
In Proceedings of workshop onsentiment and subjectivity in text at proceedings ofthe 21st international conference on computationallinguistics/the 44th annual meeting of the associationfor computational linguistics (COLING/ACL 2006),Sydney, Australia, 2006.
1-8 p.Kozareva, Z.; P. Nakov; A. Ritter; S. Rosenthal; V.Stoyonov and T. Wilson.
Sentiment Analysis inTwitter.
in:  Proceedings of the 7th InternationalWorkshop on Semantic Evaluation.
Association forComputation Linguistics, 2013.Liu, B.; M. Hu and J. Cheng.
Opinion Observer:Analyzing and Comparing Opinions on the Web.Proceedings of the 14th International World WideWeb conference (WWW-2005), Japan, 2005.Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross andK.
Miller.
Five papers on WordNet.
PrincentonUniversity, Cognositive Science Laboratory, 1990.Mosquera, A. and P. Moreda.
TENOR: A LexicalNormalisation Tool for Spanish Web 2.0 Texts.
in:Text, Speech and Dialogue - 15th InternationalConference (TSD 2012).
Springer, 2012.Strapparava, C. and A. Valitutti.
WordNet-Affect: anaffective extension of WordNet.
Proceedings of the4th International Conference on Language Resourcesand Evaluation (LREC 2004), Lisbon, 2004.
1083-1086 p.Wiebe, J.; T. Wilson and C. Cardie.
AnnotatingExpressions of Opinions and Emotions in Language.Kluwer Academic Publishers, Netherlands, 2005.449
