Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 624?633,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsHow to make words with vectors:Phrase generation in distributional semanticsGeorgiana Dinu and Marco BaroniCenter for Mind/Brain SciencesUniversity of Trento, Italy(georgiana.dinu|marco.baroni)@unitn.itAbstractWe introduce the problem of generationin distributional semantics: Given a distri-butional vector representing some mean-ing, how can we generate the phrase thatbest expresses that meaning?
We mo-tivate this novel challenge on theoreticaland practical grounds and propose a sim-ple data-driven approach to the estimationof generation functions.
We test this ina monolingual scenario (paraphrase gen-eration) as well as in a cross-lingual set-ting (translation by synthesizing adjective-noun phrase vectors in English and gener-ating the equivalent expressions in Italian).1 IntroductionDistributional methods for semantics approximatethe meaning of linguistic expressions with vectorsthat summarize the contexts in which they occurin large samples of text.
This has been a very suc-cessful approach to lexical semantics (Erk, 2012),where semantic relatedness is assessed by compar-ing vectors.
Recently these methods have beenextended to phrases and sentences by means ofcomposition operations (see Baroni (2013) for anoverview).
For example, given the vectors repre-senting red and car, composition derives a vectorthat approximates the meaning of red car.However, the link between language and mean-ing is, obviously, bidirectional: As message recip-ients we are exposed to a linguistic expression andwe must compute its meaning (the synthesis prob-lem).
As message producers we start from themeaning we want to communicate (a ?thought?
)and we must encode it into a word sequence (thegeneration problem).
If distributional semanticsis to be considered a proper semantic theory, thenit must deal not only with synthesis (going fromwords to vectors), but also with generation (fromvectors to words).Besides these theoretical considerations, phrasegeneration from vectors has many useful applica-tions.
We can, for example, synthesize the vectorrepresenting the meaning of a phrase or sentence,and then generate alternative phrases or sentencesfrom this vector to accomplish true paraphrasegeneration (as opposed to paraphrase detection orranking of candidate paraphrases).Generation can be even more useful when thesource vector comes from another modality or lan-guage.
Recent work on grounding language in vi-sion shows that it is possible to represent imagesand linguistic expressions in a common vector-based semantic space (Frome et al, 2013; Socheret al, 2013).
Given a vector representing an im-age, generation can be used to productively con-struct phrases or sentences that describe the im-age (as opposed to simply retrieving an existingdescription from a set of candidates).
Translationis another potential application of the generationframework: Given a semantic space shared be-tween two or more languages, one can compose aword sequence in one language and generate trans-lations in another, with the shared semantic vectorspace functioning as interlingua.Distributional semantics assumes a lexicon ofatomic expressions (that, for simplicity, we taketo be words), each associated to a vector.
Thus,at the single-word level, the problem of genera-tion is solved by a trivial generation-by-synthesisapproach: Given an arbitrary target vector, ?gener-ate?
the corresponding word by searching throughthe lexicon for the word with the closest vector tothe target.
This is however unfeasible for largerexpressions: Given n vocabulary elements, thisapproach requires checking nkphrases of lengthk.
This becomes prohibitive already for relativelyshort phrases, as reasonably-sized vocabularies donot go below tens of thousands of words.
Thesearch space for 3-word phrases in a 10K-wordvocabulary is already in the order of trillions.
In624this paper, we introduce a more direct approach tophrase generation, inspired by the work in com-positional distributional semantics.
In short, werevert the composition process and we proposea framework of data-induced, syntax-dependentfunctions that decompose a single vector into avector sequence.
The generated vectors can thenbe efficiently matched against those in the lexiconor fed to the decomposition system again to pro-duce longer phrases recursively.2 Related workTo the best of our knowledge, we are the first toexplicitly and systematically pursue the generationproblem in distributional semantics.
Kalchbrennerand Blunsom (2013) use top-level, composed dis-tributed representations of sentences to guide gen-eration in a machine translation setting.
More pre-cisely, they condition the target language modelon the composed representation (addition of wordvectors) of the source language sentence.Andreas and Ghahramani (2013) discuss thethe issue of generating language from vectors andpresent a probabilistic generative model for distri-butional vectors.
However, their emphasis is onreversing the generative story in order to derivecomposed meaning representations from word se-quences.
The theoretical generating capabilities ofthe methods they propose are briefly exemplified,but not fully explored or tested.Socher et al (2011) come closest to our targetproblem.
They introduce a bidirectional language-to-meaning model for compositional distributionalsemantics that is similar in spirit to ours.
How-ever, we present a clearer decoupling of synthesisand generation and we use different (and simpler)training methods and objective functions.
More-over, Socher and colleagues do not train separatedecomposition rules for different syntactic config-urations, so it is not clear how they would be ableto control the generation of different output struc-tures.
Finally, the potential for generation is onlyaddressed in passing, by presenting a few caseswhere the generated sequence has the same syn-tactic structure of the input sequence.3 General frameworkWe start by presenting the familiar synthesis set-ting, focusing on two-word phrases.
We then in-troduce generation for the same structures.
Fi-nally, we show how synthesis and generation oflonger phrases is handled by recursive extensionof the two-word case.
We assume a lexicon L,that is, a bi-directional look-up table containing alist of words Lwlinked to a matrix Lvof vectors.Both synthesis and generation involve a trivial lex-icon look-up step to retrieve vectors associated towords and vice versa: We ignore it in the exposi-tion below.3.1 SynthesisTo construct the vector representing a two-wordphrase, we must compose the vectors associatedto the input words.
More formally, similarly toMitchell and Lapata (2008), we define a syntax-dependent composition function yielding a phrasevector ~p:~p = fcompR(~u,~v)where ~u and ~v are the vector representations asso-ciated to words u and v. fcompR: Rd?
Rd?
Rd(for d the dimensionality of vectors) is a compo-sition function specific to the syntactic relation Rholding between the two words.1Although we are not bound to a specific com-position model, throughout this paper we use themethod proposed by Guevara (2010) and Zanzottoet al (2010) which defines composition as appli-cation of linear transformations to the two con-stituents followed by summing the resulting vec-tors: fcompR(~u,~v) = W1~u+W2~v.
We will furtheruse the following equivalent formulation:fcompR(~u,~v) = WR[~u;~v]where WR?
Rd?2dand [~u;~v] is the vertical con-catenation of the two vectors (using Matlab no-tation).
Following Guevara, we learn WRusingexamples of word and phrase vectors directly ex-tracted from the corpus (for the rest of the pa-per, we refer to these phrase vectors extractednon-compositionally from the corpus as observedvectors).
To estimate, for example, the weightsin the WAN(adjective-noun) matrix, we use thecorpus-extracted vectors of the words in tuplessuch as ?red, car, red.car?, ?evil, cat, evil.cat?,etc.
Given a set of training examples stacked intomatrices U , V (the constituent vectors) and P (thecorresponding observed vectors), we estimate WRby solving the least-squares regression problem:1Here we make the simplifying assumption that all vec-tors have the same dimensionality, however this need not nec-essarily be the case.625minWR?Rd?2d?P ?WR[U ;V ]?
(1)We use the approximation of observed phrasevectors as objective because these vectors can pro-vide direct evidence of the polysemous behaviourof words: For example, the corpus-observed vec-tors of green jacket and green politician reflecthow the meaning of green is affected by its occur-rence with different nouns.
Moreover, it has beenshown that for two-word phrases, despite theirrelatively low frequency, such corpus-observedrepresentations are still difficult to outperform inphrase similarity tasks (Dinu et al, 2013; Turney,2012).3.2 GenerationGeneration of a two-word sequence from a vec-tor proceeds in two steps: decomposition of thephrase vectors into two constituent vectors, andsearch for the nearest neighbours of each con-stituent vector in Lv(the lexical matrix) in orderto retrieve the corresponding words from Lw.Decomposition We define a syntax-dependentdecomposition function:[~u;~v] = fdecompR(~p)where ~p is a phrase vector, ~u and ~v are vectors as-sociated to words standing in the syntactic relationR and fdecompR: Rd?
Rd?
Rd.We assume that decomposition is also a lineartransformation, W?R?
R2d?d, which, given an in-put phrase vector, returns two constituent vectors:fdecompR(~p) = W?R~pAgain, we can learn from corpus-observed vectorsassociated to tuples of word pairs and the corre-sponding phrases by solving:minW?R?R2d?d?
[U ;V ]?W?RP?
(2)If a composition function fcompRis available, analternative is to learn a function that can best revertthis composition.
The decomposition function isthen trained as follows:minW?R?R2d?d?
[U ;V ]?W?RWR[U ;V ]?
(3)where the matrix WRis a given compositionfunction for the same relation R. Training withobserved phrases, as in eq.
(2), should be betterat capturing the idiosyncrasies of the actual dis-tribution of phrases in the corpus and it is morerobust by being independent from the availabilityand quality of composition functions.
On the otherhand, if the goal is to revert as faithfully as possi-ble the composition process and retrieve the orig-inal constituents (e.g., in a different modality or adifferent language), then the objective in eq.
(3) ismore motivated.Nearest neighbour search We retrieve the near-est neighbours of each constituent vector ~u ob-tained by decomposition by applying a searchfunction s:NN~u= s(~u, Lv, t)where NN~uis a list containing the t nearestneighours of ~u from Lv, the lexical vectors.
De-pending on the task, t might be set to 1 to retrievejust one word sequence, or to larger values to re-trieve t alternatives.
The similarity measure usedto determine the nearest neighbours is another pa-rameter of the search function; we omit it here aswe only experiment with the standard cosine mea-sure (Turney and Pantel, 2010).23.3 Recursive (de)compositionExtension to longer sequences is straightforwardif we assume binary tree representations as syn-tactic structures.
In synthesis, the top-levelvector can be obtained by applying composi-tion functions recursively.
For example, thevector of big red car would be obtained as:fcompAN(~big, fcompAN(~red, ~car)), where fcompANis the composition function for adjective-nounphrase combinations.
Conversely, for generation,we decompose the phrase vector with fdecompAN.The first vector is used for retrieving the nearestadjective from the lexicon, while the second vec-tor is further decomposed.In the experiments in this paper we assume thatthe syntactic structure is given.
In Section 7, wediscuss ways to eliminate this assumption.2Note that in terms of computational efficiency, cosine-based nearest neighbour searches reduce to vector-matrixmultiplications, for which many efficient implementationsexist.
Methods such as locality sensitive hashing can be usedfor further speedups when working with particularly large vo-cabularies (Andoni and Indyk, 2008).6264 Evaluation settingIn our empirical part, we focus on noun phrasegeneration.
A noun phrase can be a single noun ora noun with one or more modifiers, where a mod-ifier can be an adjective or a prepositional phrase.A prepositional phrase is in turn composed of apreposition and a noun phrase.
We learn two com-position (and corresponding decomposition) func-tions: one for modifier-noun phrases, trained onadjective-noun (AN) pairs, and a second one forprepositional phrases, trained on preposition-noun(PN) combinations.
For the rest of this section wedescribe the construction of the vector spaces andthe (de)composition function learning procedure.Construction of vector spaces We test twotypes of vector representations.
The cbow modelintroduced in Mikolov et al (2013a) learns vec-tor representations using a neural network archi-tecture by trying to predict a target word given thewords surrounding it.
We use the word2vec soft-ware3to build vectors of size 300 and using a con-text window of 5 words to either side of the target.We set the sub-sampling option to 1e-05 and esti-mate the probability of a target word with the neg-ative sampling method, drawing 10 samples fromthe noise distribution (see Mikolov et al (2013a)for details).
We also implement a standard count-based bag-of-words distributional space (Turneyand Pantel, 2010) which counts occurrences of atarget word with other words within a symmetricwindow of size 5.
We build a 300Kx300K sym-metric co-occurrence matrix using the top mostfrequent words in our source corpus, apply posi-tive PMI weighting and Singular Value Decompo-sition to reduce the space to 300 dimensions.
Forboth spaces, the vectors are finally normalized tounit length.4For both types of vectors we use 2.8 billion to-kens as input (ukWaC + Wikipedia + BNC).
TheItalian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC.5A word token is a word-form + POS-tag string.
We extract both word vec-tors and the observed phrase vectors which are3Available at https://code.google.com/p/word2vec/4The parameters of both models have been chosen withoutspecific tuning, based on their observed stable performance inprevious independent experiments.5Corpus sources: http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.ukrequired for the training procedures.
We sanity-check the two spaces on MEN (Bruni et al, 2012),a 3,000 items word similarity data set.
cbow sig-nificantly outperforms count (0.80 vs. 0.72 Spear-man correlations with human judgments).
countperformance is consistent with previously reportedresults.6(De)composition function training The train-ing data sets consist of the 50K most frequent?u, v, p?
tuples for each phrase type, for example,?red, car, red.car?
or ?in, car, in.car?.7We con-catenate ~u and ~v vectors to obtain the [U ;V ] ma-trix and we use the observed ~p vectors (e.g., thecorpus vector of the red.car bigram) to obtain thephrase matrix P .
We use these data sets to solvethe least squares regression problems in eqs.
(1)and (2), obtaining estimates of the compositionand decomposition matrices, respectively.
For thedecomposition function in eq.
(3), we replace theobserved phrase vectors with those composed withfcompR(~u,~v), where fcompRis the previously esti-mated composition function for relation R.Composition function performance Since theexperiments below also use composed vectors asinput to the generation process, it is important toprovide independent evidence that the composi-tion model is of high quality.
This is indeed thecase: We tested our composition approach on thetask of retrieving observed AN and PN vectors,based on their composed vectors (similarly to Ba-roni and Zamparelli (2010), we want to retrieve theobserved red.car vector using fcompAN(red, car)).We obtain excellent results, with minimum accu-racy of 0.23 (chance level <0.0001).
We also teston the AN-N paraphrasing test set used in Dinuet al (2013) (in turn adapting Turney (2012)).The dataset contains 620 ANs, each paired witha single-noun paraphrase (e.g., false belief/fallacy,personal appeal/charisma).
The task is to rankall nouns in the lexicon by their similarity to thephrase, and return the rank of the correct para-phrase.
Results are reported in the first row of Ta-ble 1.
To facilitate comparison, we search, likeDinu et al, through a vocabulary containing the20K most frequent nouns.
The count vectors re-sults are similar to those reported by Dinu and col-leagues for the same model, and with cbow vec-6See Baroni et al (2014) for an extensive comparison ofthe two types of vector representations.7For PNs, we ignore determiners and we collapse, for ex-ample, in.the.car and in.car occurrences.627Input Output cbow countA?N N 11 171N A, N 67,29 204,168Table 1: Median rank on the AN-N set of Dinu etal.
(2013) (e.g., personal appeal/charisma).
Firstrow: the A and N are composed and the closestN is returned as a paraphrase.
Second row: theN vector is decomposed into A and N vectors andtheir nearest (POS-tag consistent) neighbours arereturned.tors we obtain a median rank that is considerablyhigher than that of the methods they test.5 Noun phrase generation5.1 One-step decompositionWe start with testing one-step decomposition bygenerating two-word phrases.
A first straightfor-ward evaluation consists in decomposing a phrasevector into the correct constituent words.
For thispurpose, we randomly select (and consequently re-move) from the training sets 200 phrases of eachtype (AN and PN) and apply decomposition op-erations to 1) their corpus-observed vectors and2) their composed representations.
We generatetwo words by returning the nearest neighbours(with appropriate POS tags) of the two vectorsproduced by the decomposition functions.
Ta-ble 2 reports generation accuracy, i.e., the pro-portion of times in which we retrieved the cor-rect constituents.
The search space consists ofthe top most frequent 20K nouns, 20K adjec-tives and 25 prepositions respectively, leading tochance accuracy <0.0001 for nouns and adjectivesand <0.05 for prepositions.
We obtain relativelyhigh accuracy, with cbow vectors consistently out-performing count ones.
Decomposing composedrather than observed phrase representations is eas-ier, which is to be expected given that composedrepresentations are obtained with a simpler, lin-ear model.
Most of the errors consist in generat-ing synonyms (hard case?difficult case, true cost?
actual cost) or related phrases (stereo speak-ers?omni-directional sound).Next, we use the AN-N dataset of Dinu andcolleagues for a more interesting evaluation ofone-step decomposition.
In particular, we reversethe original paraphrasing direction by attemptingto generate, for example, personal charm fromcharisma.
It is worth stressing the nature of theInput Output cbow countA.N A, N 0.36,0.61 0.20,0.41P.N P, N 0.93,0.79 0.60,0.57A?N A, N 1.00,1.00 0.86,0.99P?N P, N 1.00,1.00 1.00,1.00Table 2: Accuracy of generation models at re-trieving (at rank 1) the constituent words ofadjective-noun (AN) and preposition-noun (PN)phrases.
Observed (A.N) and composed repre-sentations (A?N) are decomposed with observed-(eq.
2) and composed-trained (eq.
3) functions re-spectively.paraphrase-by-generation task we tackle here andin the next experiments.
Compositional distri-butional semantic systems are often evaluated onphrase and sentence paraphrasing data sets (Bla-coe and Lapata, 2012; Mitchell and Lapata, 2010;Socher et al, 2011; Turney, 2012).
However,these experiments assume a pre-compiled list ofcandidate paraphrases, and the task is to rankcorrect paraphrases above foils (paraphrase rank-ing) or to decide, for a given pair, if the twophrases/sentences are mutual paraphrases (para-phrase detection).
Here, instead, we do not as-sume a given set of candidates: For example, inN?AN paraphrasing, any of 20K2possible com-binations of adjectives and nouns from the lexiconcould be generated.
This is a much more challeng-ing task and it paves the way to more realistic ap-plications of distributional semantics in generationscenarios.The median ranks of the gold A and N of theDinu set are shown in the second row of Table1.
As the top-generated noun is almost always,uninterestingly, the input one, we return the nextnoun.
Here we report results for the more moti-vated corpus-observed training of eq.
(2) (unsur-prisingly, using composed-phrase training for thetask of decomposing single nouns leads to lowerperformance).Although considerably more difficult than theprevious task, the results are still very good, withmedian ranks under 100 for the cbow vectors (ran-dom median rank at 10K).
Also, the dataset pro-vides only one AN paraphrase for each noun, outof many acceptable ones.
Examples of generatedphrases are given in Table 3.
In addition to gen-erating topically related ANs, we also see nounsdisambiguated in different ways than intended in628Input Output Goldreasoning deductive thinking abstract thoughtjurisdiction legal authority legal powerthunderstorm thundery storm electrical stormfolk local music common peoplesuperstition old-fashioned religion superstitious notionvitriol political bitterness sulfuric acidzoom fantastic camera rapid growthreligion religious religion religious beliefTable 3: Examples of generating ANs from Ns us-ing the data set of Dinu et al (2013).the gold standard (for example vitriol and folk inTable 3).
Other interesting errors consist of de-composing a noun into two words which both havethe same meaning as the noun, generating for ex-ample religion?
religious religions.
We observemoreover that sometimes the decomposition re-flects selectional preference effects, by generat-ing adjectives that denote typical properties of thenoun to be paraphrased (e.g., animosity is a (po-litical, personal,...) hostility or a fridge is a (big,large, small,...) refrigerator).
This effect could beexploited for tasks such as property-based conceptdescription (Kelly et al, 2012).5.2 Recursive decompositionWe continue by testing generation through recur-sive decomposition on the task of generating noun-preposition-noun (NPN) paraphrases of adjective-nouns (AN) phrases.
We introduce a dataset con-taining 192 AN-NPN pairs (such as pre-electionpromises?
promises before election), which wascreated by the second author and additionally cor-rected by an English native speaker.
The data setwas created by analyzing a list of randomly se-lected frequent ANs.
49 further ANs (with adjec-tives such as amazing and great) were judged notNPN-paraphrasable and were used for the experi-ment reported in Section 7.
The paraphrased sub-set focuses on preposition diversity and on includ-ing prepositions which are rich in semantic contentand relevant to paraphrasing the AN.
This has ledto excluding of, which in most cases has the purelysyntactic function of connecting the two nouns.The data set contains the following 14 preposi-tions: after, against, at, before, between, by, for,from, in, on, per, under, with, without.8NPN phrase generation involves the applica-tion of two decomposition functions.
In the first8This dataset is available at http://clic.cimec.unitn.it/composesstep we decompose using the modifier-noun rule(fdecompAN).
We generate a noun from the headslot vector and the ?adjective?
vector is further de-composed using fdecompPN(returning the top nounwhich is not identical to the previously generatedone).
The results, in terms of top 1 accuracy andmedian rank, are shown in Table 4.
Examples aregiven in Table 5.For observed phrase vector training, accuracyand rank are well above chance for all constituents(random accuracy 0.00005 for nouns and 0.04 forprepositions, corresponding median ranks: 10K,12).
Preposition generation is clearly a more diffi-cult task.
This is due at least in part to their highlyambiguous and broad semantics, and the way inwhich they interact with the nouns.
For exam-ple, cable through ocean in Table 5 is a reason-able paraphrase of undersea cable despite the goldpreposition being under.
Other than several caseswhich are acceptable paraphrases but not in thegold standard, phrases related in meaning but notsynonymous are the most common error (overcastskies ?
skies in sunshine).
We also observe thatoften the A and N meanings are not fully separatedwhen decomposing and ?traces?
of the adjectiveor of the original noun meaning can be found inboth generated nouns (for example nearby school?
schools after school).
To a lesser degree, thismight be desirable as a disambiguation-in-contexteffect as, for example, in underground cavern, insecret would not be a context-appropriate para-phrase of underground.6 Noun phrase translationThis section describes preliminary experimentsperformed in a cross-lingual setting on the taskof composing English AN phrases and generatingItalian translations.Creation of cross-lingual vector spaces Acommon semantic space is required in order tomap words and phrases across languages.
Thisproblem has been extensively addressed in thebilingual lexicon acquisition literature (Haghighiet al, 2008; Koehn and Knight, 2002).
We opt fora very simple yet accurate method (Klementiev etal., 2012; Rapp, 1999) in which a bilingual dictio-nary is used to identify a set of shared dimensionsacross spaces and the vectors of both languages areprojected into the subspace defined by these (Sub-space Projection - SP).
This method is applicableto count-type vector spaces, for which the dimen-629Input Output Training cbow countA?N N, P, N observed 0.98(1),0.08(5.5),0.13(20.5) 0.82(1),0.17(4.5),0.05(71.5)A?N N, P, N composed 0.99(1),0.02(12), 0.12(24) 0.99(1),0.06(10), 0.05(150.5)Table 4: Top 1 accuracy (median rank) on the AN?NPN paraphrasing data set.
AN phrases are com-posed and then recursively decomposed into N, (P, N).
Comma-delimited scores reported for first noun,preposition, second noun in this order.
Training is performed on observed (eq.
2) and composed (eq.
3)phrase representations.Input Output Goldmountainous region region in highlands region with mountainsundersea cable cable through ocean cable under seaunderground cavern cavern through rock cavern under groundinterdisciplinary field field into research field between disciplinesinter-war years years during 1930s years between warspost-operative pain pain through patient pain after operationpre-war days days after wartime days before warintergroup differences differences between intergroup differences between minoritiessuperficial level level between levels level on surfaceTable 5: Examples of generating NPN phrases from composed ANs.sions correspond to actual words.
As the cbow di-mensions do not correspond to words, we align thecbow spaces by using a small dictionary to learna linear map which transforms the English vectorsinto Italian ones as done in Mikolov et al (2013b).This method (Translation Matrix - TM) is applica-ble to both cbow and count spaces.
We tune the pa-rameters (TM or SP for count and dictionary size5K or 25K for both spaces) on a standard task oftranslating English words into Italian.
We obtainTM-5K for cbow and SP-25K for count as opti-mal settings.
The two methods perform similarlyfor low frequency words while cbow-TM-5K sig-nificantly outperforms count-SP-25K for high fre-quency words.
Our results for the cbow-TM-5Ksetting are similar to those reported by Mikolov etal.
(2013b).Cross-lingual decomposition training Train-ing proceeds as in the monolingual case, this timeconcatenating the training data sets and estimatinga single (de)-composition function for the two lan-guages in the shared semantic space.
We train bothon observed phrase representations (eq.
2) and oncomposed phrase representations (eq.
3).Adjective-noun translation dataset We ran-domly extract 1,000 AN-AN En-It phrase pairsfrom a phrase table built from parallel movie sub-titles, available at http://opus.lingfil.uu.se/ (OpenSubtitles2012, en-it) (Tiedemann,2012).Input Output cbow countA?N(En) A,N (It) 0.31,0.59 0.24,0.54A?N (It) A,N(En) 0.50,0.62 0.28,0.48Table 6: Accuracy of En?It and It?En phrasetranslation: phrases are composed in source lan-guage and decomposed in target language.
Train-ing on composed phrase representations (eq.
(3))(with observed phrase training (eq.
2) results are?50% lower).Results are presented in Table 6.
While inthese preliminary experiments we lack a properterm of comparison, the performance is very goodboth quantitatively (random < 0.0001) and quali-tatively.
The En?It examples in Table 7 are repre-sentative.
In many cases (e.g., vicious killer, roughneighborhood) we generate translations that arearguably more natural than those in the gold stan-dard.
Again, some differences can be explainedby different disambiguations (chest as breast, asin the generated translation, or box, as in the gold).Translation into related but not equivalent phrasesand generating the same meaning in both con-stituents (stellar star) are again the most signifi-cant errors.
We also see cases in which this has thedesired effect of disambiguating the constituents,such as in the examples in Table 8, showing thenearest neighbours when translating black tie andindissoluble tie.630Input Output Goldvicious killer assassino feroce (ferocious killer) killer pericolosospectacular woman donna affascinante (fascinating woman) donna eccezionalehuge chest petto grande (big chest) scrigno immensorough neighborhood zona malfamata (ill-repute zone) quartiere difficilemortal sin peccato eterno (eternal sin) pecato mortalecanine star stella stellare (stellar star) star caninaTable 7: En?It translation examples (back-translations of generated phrases in parenthesis).black tiecravatta (tie) nero (black)velluto (velvet) bianco (white)giacca (jacket) giallo (yellow)indissoluble tiealleanza (alliance) indissolubile (indissoluble)legame (bond) sacramentale (sacramental)amicizia (friendship) inscindibile (inseparable)Table 8: Top 3 translations of black tie and indis-soluble tie, showing correct disambiguation of tie.7 Generation confidence and generationqualityIn Section 3.2 we have defined a search functions returning a list of lexical nearest neighbours fora constituent vector produced by decomposition.Together with the neighbours, this function cannaturally return their similarity score (in our case,the cosine).
We call the score associated to thetop neighbour the generation confidence: if thisscore is low, the vector has no good match in thelexicon.
We observe significant Spearman cor-relations between the generation confidence of aconstituent and its quality (e.g., accuracy, inverserank) in all the experiments.
For example, for theAN(En)?AN(It) experiment, the correlations be-tween the confidence scores and the inverse ranksfor As and Ns, for both cbow and count vectors,range between 0.34 (p < 1e?28) and 0.42.
Inthe translation experiments, we can use this to au-tomatically determine a subset on which we cantranslate with very high accuracy.
Table 9 showsAN-AN accuracies and coverage when translatingonly if confidence is above a certain threshold.Throughout this paper we have assumed that thesyntactic structure of the phrase to be generated isgiven.
In future work we will exploit the corre-lation between confidence and quality for the pur-pose of eliminating this assumption.
As a concreteexample, we can use confidence scores to distin-guish the two subsets of the AN-NPN dataset in-troduced in Section 5: the ANs which are para-phrasable with an NPN from those that do notEn?It It?EnThr.
Accuracy Cov.
Accuracy Cov.0.00 0.21 100% 0.32 100%0.55 0.25 70% 0.40 63%0.60 0.31 32% 0.45 37%0.65 0.45 9% 0.52 16%Table 9: AN-AN translation accuracy (both A andN correct) when imposing a confidence threshold(random: 1/20K2).Figure 1: ROC of distinguishing ANs para-phrasable as NPNs from non-paraphrasable ones.have this property.
We assign an AN to the NPN-paraphrasable class if the mean confidence of thePN expansion in its attempted N(PN) decomposi-tion is above a certain threshold.
We plot the ROCcurve in Figure 1.
We obtain a significant AUC of0.71.8 ConclusionIn this paper we have outlined a framework forthe task of generation with distributional semanticmodels.
We proposed a simple but effective ap-proach to reverting the composition process to ob-tain meaningful reformulations of phrases througha synthesis-generation process.For future work we would like to experimentwith more complex models for (de-)compositionin order to improve the performance on the taskswe used in this paper.
Following this, we631would like to extend the framework to handlearbitrary phrases, including making (confidence-based) choices on the syntactic structure of thephrase to be generated, which we have assumedto be given throughout this paper.In terms of applications, we believe that the lineof research in machine translation that is currentlyfocusing on replacing parallel resources with largeamounts of monolingual text provides an inter-esting setup to test our methods.
For example,Klementiev et al (2012) reconstruct phrase ta-bles based on phrase similarity scores in seman-tic space.
However, they resort to scoring phrasepairs extracted from an aligned parallel corpus, asthey do not have a method to freely generate these.Similarly, in the recent work on common vectorspaces for the representation of images and text,the current emphasis is on retrieving existing cap-tions (Socher et al, 2014) and not actual genera-tion of image descriptions.From a more theoretical point of view, our workfills an important gap in distributional semantics,making it a bidirectional theory of the connec-tion between language and meaning.
We can nowtranslate linguistic strings into vector ?thoughts?,and the latter into their most appropriate linguis-tic expression.
Several neuroscientific studies sug-gest that thoughts are represented in the brain bypatterns of activation over broad neural areas, andvectors are a natural way to encode such patterns(Haxby et al, 2001; Huth et al, 2012).
Someresearch has already established a connection be-tween neural and distributional semantic vectorspaces (Mitchell et al, 2008; Murphy et al, 2012).Generation might be the missing link to power-ful computational models that take the neural foot-print of a thought as input and produce its linguis-tic expression.AcknowledgmentsWe thank Kevin Knight, Andrew Anderson,Roberto Zamparelli, Angeliki Lazaridou, NghiaThe Pham, Germ?an Kruszewski and Peter Tur-ney for helpful discussions and the anonymous re-viewers for their useful comments.
We acknowl-edge the ERC 2011 Starting Independent ResearchGrant n. 283554 (COMPOSES).ReferencesAlexandr Andoni and Piotr Indyk.
2008.
Near-optimalhashing algorithms for approximate nearest neigh-bor in high dimensions.
Commun.
ACM, 51(1):117?122, January.Jacob Andreas and Zoubin Ghahramani.
2013.
Agenerative model of vector space semantics.
InProceedings of the Workshop on Continuous VectorSpace Models and their Compositionality, pages 91?99, Sofia, Bulgaria.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
Asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof ACL, To appear, Baltimore, MD.Marco Baroni.
2013.
Composition in distributionalsemantics.
Language and Linguistics Compass,7(10):511?522.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of EMNLP, pages546?556, Jeju Island, Korea.Elia Bruni, Gemma Boleda, Marco Baroni, andNam Khanh Tran.
2012.
Distributional semanticsin Technicolor.
In Proceedings of ACL, pages 136?145, Jeju Island, Korea.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
General estimation and evaluation of com-positional distributional semantic models.
In Pro-ceedings of ACL Workshop on Continuous VectorSpace Models and their Compositionality, pages 50?58, Sofia, Bulgaria.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: A survey.
Language andLinguistics Compass, 6(10):635?653.Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-gio, Jeff Dean, Marc?Aurelio Ranzato, and TomasMikolov.
2013.
DeViSE: A deep visual-semanticembedding model.
In Proceedings of NIPS, pages2121?2129, Lake Tahoe, Nevada.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of GEMS, pages 33?37,Uppsala, Sweden.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL,pages 771?779, Columbus, OH, USA, June.James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai,Jennifer Schouten, and Pietro Pietrini.
2001.
Dis-tributed and overlapping representations of facesand objects in ventral temporal cortex.
Science,293:2425?2430.632Alexander Huth, Shinji Nishimoto, An Vu, and JackGallant.
2012.
A continuous semantic space de-scribes the representation of thousands of object andaction categories across the human brain.
Neuron,76(6):1210?1224.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, Seattle, October.
Associa-tion for Computational Linguistics.Colin Kelly, Barry Devereux, and Anna Korhonen.2012.
Semi-supervised learning for automatic con-ceptual property extraction.
In Proceedings of the3rd Workshop on Cognitive Modeling and Computa-tional Linguistics, pages 11?20, Montreal, Canada.Alexandre Klementiev, Ann Irvine, Chris Callison-Burch, and David Yarowsky.
2012.
Toward sta-tistical machine translation without parallel corpora.In Proceedings of EACL, pages 130?140, Avignon,France.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InIn Proceedings of ACL Workshop on UnsupervisedLexical Acquisition, pages 9?16, Philadelphia, PA,USA.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
http://arxiv.org/abs/1301.3781/.Tomas Mikolov, Quoc Le, and Ilya Sutskever.
2013b.Exploiting similarities among languages for Ma-chine Translation.
http://arxiv.org/abs/1309.4168.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL, pages 236?244, Columbus, OH.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,Kai-Min Chang, Vincente Malave, Robert Mason,and Marcel Just.
2008.
Predicting human brain ac-tivity associated with the meanings of nouns.
Sci-ence, 320:1191?1195.Brian Murphy, Partha Talukdar, and Tom Mitchell.2012.
Selecting corpus-semantic models for neu-rolinguistic decoding.
In Proceedings of *SEM,pages 114?123, Montreal, Canada.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated english and germancorpora.
In Proceedings of the 37th annual meet-ing of the Association for Computational Linguisticson Computational Linguistics, ACL ?99, pages 519?526.
Association for Computational Linguistics.Richard Socher, Eric Huang, Jeffrey Pennin, AndrewNg, and Christopher Manning.
2011.
Dynamicpooling and unfolding recursive autoencoders forparaphrase detection.
In Proceedings of NIPS, pages801?809, Granada, Spain.Richard Socher, Milind Ganjoo, Christopher Manning,and Andrew Ng.
2013.
Zero-shot learning throughcross-modal transfer.
In Proceedings of NIPS, pages935?943, Lake Tahoe, Nevada.Richard Socher, Quoc Le, Christopher Manning, andAndrew Ng.
2014.
Grounded compositional se-mantics for finding and describing images with sen-tences.
Transactions of the Association for Compu-tational Linguistics.
In press.J?org Tiedemann.
2012.
Parallel data, tools and inter-faces in opus.
In Proceedings of the Eight Interna-tional Conference on Language Resources and Eval-uation (LREC?12), Istanbul, Turkey.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Peter Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.Fabio Zanzotto, Ioannis Korkontzelos, FrancescaFalucchi, and Suresh Manandhar.
2010.
Estimat-ing linear models for compositional distributionalsemantics.
In Proceedings of COLING, pages 1263?1271, Beijing, China.633
