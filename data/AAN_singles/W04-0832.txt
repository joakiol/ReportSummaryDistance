SENSEVAL Automatic Labeling of Semantic Roles using Maximum EntropyModelsNamhee KwonInformation Science InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292nkwon@isi.eduMichael FleischmanMessachusetts Institute ofTechnology77 Massachusetts AveCambridge, MA 02139mbf@mit.eduEduard HovyInformation Science InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292hovy@isi.eduAbstractAs a task in SensEval-3, Automatic Labelingof Semantic Roles is to identify frameelements within a sentence and tag them withappropriate semantic roles given a sentence, atarget word and its frame.
We applyMaximum Entropy classification with featuresets of syntactic patterns from parse trees andofficially attain 80.2% precision and 65.4%recall.
When the frame element boundariesare given, the system performs 86.7%precision and 85.8% recall.1 IntroductionThe Automatic Labeling of Semantic Roles trackin SensEval-3 focuses on identifying frameelements in sentences and tagging them with theirappropriate semantic roles based on FrameNet1.For this task, we extend our previous work(Fleischman et el., 2003) by adding a sentencesegmentation step and by adopting a few additionalfeature vectors for Maximum Entropy Model.Following the task definition, we assume the frameand the lexical unit of target word are knownalthough we have assumed only the target word isknown in the previous work.2 ModelWe separate the problem of FrameNet tagginginto three subsequent processes: 1) sentencesegmentation 2) frame element identification, and3) semantic role tagging.
We assume the frameelement (FE) boundaries match the parseconstituents, so we segment a sentence based onparse constituents.
We consider steps 2) and 3) asclassification problems.
In frame elementidentification, we use a binary classifier todetermine if each parse constituent is a FE or not,while, in semantic role tagging, we classify each1 http://www.icsi.berkeley.edu/~framenetidentified FE into its appropriate semantic role.2Figure 1 shows the sequence of steps.He fastened the panel from an old radio to the headboard wi thsticky tape and tied the driving wheel to Pete 's cardboard boxwi th st ring(He) (fastened the panel from an old radio to the headboardwi th sticky tape) (and) (t ied) (the driving wheel) ( to Pete 'scardboard box) (wi th s tring)(He) (the driving wheel) (to Pete 's cardboard box) (wi th  string)Agen t I tem Goal Connecto rInput sentence1) Sentence Segmentation: choose the highestconsti tuen ts while separating targe t word2) Frame Element Identification: apply MEclassification to classify each segment in t o classes ofFE (frame element), T (target), NO (none) then ext ractiden ti fied f rame elements3) Semantic Role Tagging: apply ME classification toclassify each FE Into classes of various semantic rolesOutput role: Agent (He), I tem ( the driving wheel),Goal ( to Pete?s cardboard box), Connector (wi th string)Fig.
1.
The sequence of steps on a sample sentencehaving a target word ?tied?.We train the ME models using the GISalgorithm (Darroch and Ratcliff, 1972) asimplemented in the YASMET ME package (Och,2002).
We use the YASMET MEtagger (Bender etal.
2003) to perform the Viterbi search forchoosing the most probable tag sequence for asentence using the probabilities computed duringtraining.
Feature weights are smoothed usingGaussian priors with mean 0 (Chen and Rosenfeld,1999).2.1 Sentence SegmentationWe segment a sentence into a sequence of non-overlapping constituents instead of all individualconstituents.
There are a number of advantages toapplying sentence segmentation before FE2 We are currently ignoring null instantiations.Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsboundary identification.
First, it allows us toutilize sentence-wide features for FE identification.The sentence-wide features, containing dependentinformation between frame element such as thepreviously identified class or the syntactic pattern,have previously been shown to be powerfulfeatures for role classification (Fleischman et al,2003).
Further, it allows us to reduce the numberof candidate constituents for FE, which reduces theconvergence time in training.The constituents are derived from a syntacticparse tree3.
Although we need to consider allcombinations of various level constituents in aparse tree, we know the given target word shouldbe a separate segment because a target word is nota part of other FEs.4  Since most frame elementstend to be in higher levels of the parse tree, wedecide to use the highest constituents (the parseconstituents having the maximum number ofwords) while separating the target word.
Figure 2shows an example of the segmentation for anactual sentence in FrameNet with the target word?tied?.He tied the to Pete boxPRP VBD DT TO NP NNVPVPVPNPNPPPNPS?sCCandPPfastened the panelfrom an old radioto the headboardwith sticky tape?.VBGdriving wheelNNPOSNNPNNcardboardINwithNPNNstringFig.
2.
A sample sentence segmentation: ?tied?
isthe target predicate, and the shaded constituentrepresents each segment.However, this segmentation reduces the FEcoverage of constituents (the number ofconstituents matching frame elements).
In Table 1,?individual constituents?
means a list of allconstituents, and ?Sentence segmentation?
means asequence of non-overlapping constituents that aretaken in our work.
We can regard 85.8% as theaccuracy of the parser.3 We use Charniak parser :http://www.cs.brown.edu/people/ec/#software4 Although 17% of constituents are both a target and aframe element, there is no case that a target is a part of aframe element.Method Number of constituentsFE coverage(%)Individualconstituents  342,245 85.8Sentencesegmentation 66,401 79.5Table 1.
FE coverage for the test set.2.2 Frame Element IdentificationFrame element identification is executed forsegments to classify into the classes on FE, Target,or None.
When a constituent is both a target and aframe element, we set it as a frame element whentraining because we are interested in identifyingframe elements not a target.The initial features are adopted from (Gildea andJuraksky 2002) and (Fleischman, Kwon, and Hovy2003), and a few additional features are also used.The features are:?
Target predicate (target): The target is theprincipal lexical item in a sentence.?
Target lexical name (lexunit): The formallexical name of target predicate is the string ofthe original form of target word andgrammatical type.
For example, when thetarget is ?tied?, the lexical name is ?tie.v?.?
Target type (ltype): The target type is a partof lexunit representing verb, noun, or adjective.(e.g.
?v?
for a lexunit ?tie.v?)?
Frame name (frame): The semantic frame isdefined in FrameNet with corresponding target.?
Constituent path (path): From the syntacticparse tree of a sentence, we extract the pathfrom each constituent to the target predicate.The path is represented by the nodes throughwhich one passes while traveling up the treefrom the constituent and then down throughthe governing category to the target word.
Forexample, ?the driving wheel?
in the sentenceof Figure 2 has the path, NP?VP?VBD.?
Partial path (ppath): The partial path is avariation of path, and it produces the same pathas above if the constituent is under the same?S?
as target word, if not, it gives ?nopath?.?
Syntactic Head (head): The syntactic head ofeach constituent is obtained based on MichaelCollins?s heuristic method5.
When the head isa proper noun, ?proper-noun?
substitutes forthe real head.
The decision as to whether thehead is a proper noun is made based on thepart of speech tags used in the parse tree.5 http://www.ai.mit.edu/people/mcollins/papers/heads?
Phrase Type (pt): The syntactic phrase type(e.g., NP, PP) of each constituent is alsoextracted from the parse tree.
It is not thesame as the manually defined PT in FrameNet.?
Logical Function (lf): The logical functions ofconstituents in a sentence are simplified intothree values: external argument, objectargument, other.
When the constituent?sphrase type is NP, we follow the links in theparse tree from the constituent to the ancestorsuntil we meet either S or VP.
If the S is foundfirst, we assign external argument to theconstituent, and if the VP is found, we assignobject argument.
Otherwise, other is assigned.?
Position (pos): The position indicates whethera constituent appears before or after the targetpredicate.?
Voice (voice): The voice of a sentence (active,passive) is determined by a simple regularexpression over the surface form of thesentence.?
Previous class (c_n): The class information ofthe nth-previous constituent (Target, FE, orNone) is used to exploit the dependencybetween constituents.
During training, thisinformation is provided by simply looking atthe true class of the constituent occurring n-positions before the target element.
Duringtesting, the hypothesized classes are used forViterbi search.Feature Set Example Functionsf(c, lexunit) f(c, tie.v) = 1f(c, pt, pos, voice) f(c, NP,after,active) = 1f(c, pt, lf) f(c, ADVP,obj) = 1f(c, pt_-1, lf_-1) f(c, VBD_-1, other_-1) = 1f(c, pt_1, lf_1) f(c, PP_1, other_1) = 1f(c, head) f(c, wheel) = 1f(c, head, frame) f(c, wheel, Attaching) = 1f(c, path) f(c, NP?VP?VBD) = 1f(c, path_-1) f(c, VBD_-1) = 1f(c, path_1) f(c, PP?VP?VBD_1) = 1f(c, target) f(c, tied) = 1f(c, ppath) f(c, NP?VP?VBD) = 1f(c, ppath, pos) f(c,NP?VP?VBD, after) = 1f(c, ppath_-1, pos_-1) f(c, VBD_-1,after) = 1f(c ,ltype,  ppath) f(c, v, NP?VP?VBD) = 1f(c ,ltype,  path) f(c, v, NP?VP?VBD) = 1f(c ,ltype,  path_-1) f(c, v,VBD_-1) = 1f(c  frame) f(c, Attaching) = 1f(c, frame, c_-1) f(c, Attaching, T_-1) = 1f(c,frame, c_-2,c_-1) f(c, Attaching,NO_-2,T_-1)=1Table 2.
Feature sets used in ME frame elementidentification.
Example functions of ?the drivingwheel?
from the sample sentence in Fig.2.The combinations of these features that are usedin the ME model are shown in Table 2.
Thesefeature sets contain the previous or nextconstituent?s features, for example, pt_-1represents the previous constituent?s phrase typeand lf_1 represents the next constituent?s logicalfunction.2.3 Semantic Role ClassificationSemantic role classification is executed only forthe constituents that are classified into FEs in theprevious FE identification phase by employingMaximum Entropy classification.In addition to the features in Section 2.2, twomore features are applied.?
Order (order): The relative position of aframe element in a sentence is given.
Forexample, the sentence from Figure 2 has fourframe elements, where the element ?He?
hasorder 0, while ?with string?
has order 3.?
Syntactic pattern (pat): The sentence levelsyntactic pattern is generated from the parsetree by considering the phrase type and logicalfunctions of each frame element in thesentence.
In the example sentence in Figure 2,?He?
is an external argument Noun Phrase,?tied?
is a target predicate, and ?the drivingwheel?
is an object argument Noun Phrase.Thus, the syntactic pattern associated with thesentence is [NP-ext, target, NP-obj, PP-other,PP-other].Table 3 shows the list of feature sets used for theME role classification.Feature Setf(r, lexunit) f(r, pt, lf)f(r, target) f(r, pt_-1, lf_-1)f(r, pt, pos, voice) f(r, pt_1, lf_1)f(r, head) f(r, order, syn)f(r, head, lexunit) f(r, lexunit, order, syn)f(r, head, frame) f(r, pt, pos, voice, lexunit)f(r, frame, r_-1) f(r, frame, r_-2,r_-1)f(r, frame,r_-3, r_-2,r_-1)Table 3.
Feature sets used in role classification.3 ResultsSensEval-3 provides the following data set:training set (24,558 sentences/ 51,323 frameelements/ 40 frames), and test set (8,002 sentences/16,279 frame elements/ 40 frames).
We submit twosets to SensEval-3, one (test A) is the output of allabove processes (identifying frame elements andtagging them given a sentence), and the other (testB) is to tag semantic roles given frame elements.For test B, we attempt the role classification for allframe elements including frame elements notmatching the parse tree constituents.
Althoughthere are frame elements that have two differentsemantic roles, we ignore those cases and assignone semantic role per frame element.
Thisexplains why test B shows 99% attempted frameelements.
The attempted number for test A is thenumber of frame elements identified by our system.Table 4 shows the official scores for these tests.Test Prec.
Overlap Recall AttemptedTest A 80.2 78.4 65.4 81.5Test B 86.7 86.6 85.8 99.0Table 4.
Final SensEval-3 scores for the test set.In the official evaluation, the precision and recallare calculated by counting correct roles thatoverlap even in only one word with the referenceset.
Overlap score shows how much of an actualFE is identified as an FE not penalizing wronglyidentified part.
Since this evaluation is so lenient,we perform another evaluation to check exactmatches.FE boundaryIdentificationFE boundaryIdentification &Role labeling MethodPrec Rec Prec RecTest A 80.3 66.1 71.1 58.5Test B 100.0 99.0 86.7 85.8Table 5.
Exact match scores for the test set.4 Discussion and ConclusionDue to time limit, we?ve not done manyexperiments with different feature sets orthresholds in ME classification.
We expect thatrecall will increase with lower thresholdsespecially in lenient evaluation and the finalperformance will increase by optimizingparameters.ReferencesO.
Bender, K. Macherey, F.J. Och, and H. Ney.2003.
Comparison of Alignment Templatesand Maximum Entropy Models for NaturalLanguage Processing.
Proc.
of EACL-2003.Budapest, Hungary.A.
Berger, S. Della Pietra and V. Della Pietra,1996.
A Maximum Entropy Approach to NaturalLanguage Proc.
of Computational Linguistics,vol.
22, no.
1.E.
Charniak.
2000.
A Maximum-Entropy-InspiredParser.
Proc.
of NAACL-2000, Seattle, USA.S.F.
Chen and R. Rosenfeld.
1999.
A GaussianPrior for Smoothing Maximum Entropy Models.Technical Report CMUCS-99-108, CarnegieMellon University.J.
N. Darroch and D. Ratcliff.
1972.
GeneralizedIterative Scaling for Log-Linear Models.
Annalsof Mathematical Statistics, 43:1470-1480.C.Fillmore 1976.
Frame Semantics and the Natureof Language.
Annals of the New York Academyof Science Conference on the Origin andDevelopment of Language and Speech, Volume280 (pp.
20-32).M.
Fleischman, N. Kwon, and E. Hovy.
2003.Maximum Entropy Models for FrameNetClassification.
Proc.
of Empirical Methods inNatural Language Processing conference(EMNLP), 2003.
Sapporo, Japan.D.
Gildea and D. Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
ComputationalLinguistics, 28(3) 245-288 14.F.J.
Och.
2002.
Yet Another Maxent Toolkit:YASMET www-i6.informatik.rwth-aachen.de/Colleagues/och/.C.
Thompson, R. Levy, and C. Manning.
2003.
AGenerative Model for FrameNet Semantic RoleLabeling.
Proc.
of the Fourteenth EuropeanConference on Machine Learning, 2003.
Croatia.
