Last WordsEmpiricism Is Not a Matter of FaithTed Pedersen?University of Minnesota, Duluth1.
The Sad Tale of the Zigglebottom Tagger?Hurrah, this is it!?
you exclaim as you set down the most recent issue of ComputationalLinguistics.
?This Zigglebottom Tagger is exactly what I need!?
A gleeful smile crossesyour face as you imagine how your system will improve once you replace your taggerfrom graduate school with the clearly superior Zigglebottom method.
You rub yourhands together and page through the article looking for a way to obtain the tagger,but nothing is mentioned.
That doesn?t dampen your enthusiasm, so you search theWeb, but still nothing turns up.
You persist though; those 17 pages of statisticallysignificant results really are impressive.
So you e-mail Zigglebottom asking for thetagger.Some days, or perhaps weeks, later, you get a hesitant reply saying: ?We?re planningto release a demo version soon, stay tuned .
.
.
?
Or perhaps: ?We don?t normally do this,but we can send you a copy (informally) once we clean it up a bit .
.
.
?
Or maybe: ?Wecan?t actually give you the tagger, but you should be able to re-implement it from thearticle.
Just let us know if you have any questions .
.
.
?Still having faith, and lacking any better alternative, you decide to re-implement theZigglebottom Tagger.
Despite three months of on-and-off effort, the end result providesjust the same accuracy as your old tagger, which is nowhere near that reported in thearticle.
Feeling sheepish, you conclude you must have misunderstood something, ormaybe there?s a small detail missing from the article.
So you contact Zigglebottom againand explain your predicament.
He eventually responds: ?We?ll look into this right awayand get back to you .
.
.
?A year passes.
You have the good fortune to bump into Zigglebottom at the AnnualMeeting of the Association for Computational Linguistics (ACL).
You angle for a seatnext to him during a night out, and you buy him a few beers before you politelyresume your quest for the tagger.
Finally, he confesses rather glumly: ?My studentPifflewhap was the one who did the implementation and ran the experiments, and ifhe?d only respond to my e-mail I could ask him to tell you how to get it working, buthe?s graduated now and is apparently too busy to reply.
?After a fewmore beers, Zigglebottom finally agrees to give you the tagger: ?I?ll sendyou the version of the code I have, no promises though!?
And true to his word, what hesends is incomplete and undocumented.
It doesn?t compile easily, and it?s engineeredso that a jumble of programs must be run in an undisclosed kabalistic sequence knownonly to (perhaps) the elusive Pifflewhap.
You try your best to make it work every now?
Department of Computer Science, 1114 Kirby Drive, University of Minnesota, Duluth, MN 55812, USA.E-mail: tpederse@d.umn.edu.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 3and then for a few months, but eventually you give up, and go back to using the sameold tagger you used before.2.
The Paradox of Faith-Based EmpiricismThe tale of the Zigglebottom Tagger is one of disappointment, not just for you but alsofor Zigglebottom himself.
While his work achieved publication, it must gnaw at hisscientific conscience that he can?t reproduce his own results.
The fact that you can?treproduce those results either raises questions, but those are resolved with a shrug ofyour shoulders and by giving the benefit of the doubt to Zigglebottom.
He?s not a fraud;there?s just some crucial detail that is neither recorded in the article nor in the software,which can?t be installed and run in any case.The problem here is not the Zigglebottom article; as a community we accept thatour publications don?t provide enough space to describe our elaborate 21st centuryempirical methods in sufficient detail to allow for re-implementation and reproductionof results.
This is true despite the generous page allowances in Computational Linguisticsand even more so in our much more constrained conference proceedings.What?s really missing is the software that produced the results that convinced thereviewers the article should be published.
This is particularly troubling given the highlyempirical nature of the work reported in so many of our publications.
We publishpage after page of experimental results where apparently small differences determinethe perceived value of the work.
In this climate, convenient reproduction of resultsestablishes a vital connection between authors and readers.Our community expects published papers to be rigorously reviewed and madeavailable via open access as soon as possible (e.g., via the ACL Anthology1).
We expectthe supporting corpora and lexical resources will be made available even if at some cost(e.g., via the Linguistic Data Consortium2).
Yet, we do not have the same expectationsregarding our software.
While we have table after table of results to pore over, weusually don?t have access to the software that would allow us to reproduce those results.This cuts to the core of whether we are engaged in science, engineering, or theology:Scientists reproduce results; engineers build impressive and enduring artifacts; andtheologians muse about what they believe but can?t see or prove.Before you judge the analogy with theology as being too harsh, conduct the follow-ing experiment.
Randomly select one of your own publications from a year or two agoand think about what would be involved in reproducing the results.
How long wouldit take, assuming you would be able to do it?
If you can?t reproduce those results, whydo you believe them?
Why should your readers?Our inability to reproduce results leads to a debilitating paradox, where we asreviewers and readers accept highly empirical results on faith.
We do this routinely,to the point where we seem to have given up on the idea of being able to reproduceresults.
This is the natural consequence of faith-based empiricism, and the only way tofight that movement is with a little bit of heresy.
Let?s not accept large tables of empiricalresults on faith, let?s insist that we be able to reproduce them exactly and conveniently.Let?s insist that we are scientists first and foremost, and agree that this means that wemust be able to reproduce each other?s results.1 www.aclweb.org/anthology/.2 www.ldc.upenn.edu/.466Pedersen Empiricism Is Not a Matter of Faith3.
A Heretic?s Guide to ReproducibilityIn many cases the failure to release software that allows results to be reproduced isnot a conscious decision, but rather unintentional fallout from how we manage projectsand set priorities in guiding our careers.
What follows are a few simple ideas that anyresearcher can adopt to make it much easier (and more likely) to produce software thatcan not only be released but that will allow users to reproduce results with minimaleffort.
As more of us use and release such software, our expectations as a communitywill rise, and we?ll eventually see software releases as a natural part of the publicationprocess, much as we now view data sharing.3.1 Release Early, Release OftenThe single greatest barrier to releasing software is that we don?t think about doing itearly enough.
It?s only when we get that first e-mail asking for the implementation of amethod discussed in Computational Linguistics that the issue arises, and by then it?s toolate.
At that point the task of converting our code into a well-documented and easy touse package is often nearly impossible.Beyond difficulties caused by poor documentation, the passage of time, andturnover in project members, there can even be legal concerns.
When projects do notplan to release software, it?s often the case that system development will include stagesbased on helter-skelter cutting and pasting of code from other sources.
The effect of thisis to erase all traces of the origin of that code and the terms under which it was madeavailable.
Once you have gone down this route, it?s very hard to consider releasing theresulting software.However, if you plan from the start to distribute your software, you will inevitablybe guided by considerations that are important to your potential audience.
You willchoose licenses, hardware platforms, and programming languages that avoid any obvi-ous barriers to distribution and use.
You will develop an infrastructure of Web services,software repositories, andmailing lists that will evolve with your project.
Youwill avoidhaphazard development methodologies that lead to disorganized and impossible-to-maintain code.
The prospect of having actual external users of your software willinspire a discipline and orderliness on your development and deployment processesthat will likely result in much better software than if you developed it for internal useonly.It is true that releasing software that is both usable and reliable requires a stronghand to guide system development, and that?s a skill that many researchers don?t thinkthey have.
However, it?s really quite simple to develop.
All you must do is play the partof a demanding yet naive client from time to time from the very start of the project.Insist that the code be easy to install and use and that the results that come from itbe easy to understand and absolutely reproducible.
If the project is too large for youto play this role yourself, assign it to one or more members of your team, and makesure they play the part as if they are a new user encountering the system for the firsttime.If you do this from the beginning of a project it takes surprisingly little time, andyou end up with much better documentation and software, and a system that can beeasily and conveniently used to reproduce results both by outside users and by yourselfafter the passage of some time.467Computational Linguistics Volume 34, Number 33.2 Measure Your Career in Downloads and UsersResearchers sometimes fall into the trap of seeing software and reproduction of resultsas frills, and not essential components in their career development: ?Asmuch as I wouldlike to, I don?t have the time to produce distributable code.
Besides, my promotion willbe based on publications and grants, not software releases .
.
.
?This suggests that you can either spend your time creating and releasing software,or you can spend it writing grant proposals and papers, but not both.
This overlooksa very happy side-effect that comes from creating releasable code?you will be moreefficient in producing newwork of your own since you can easily reproduce and extendyour own results.There is also a danger that this attitudewill evolve over time into a self-perpetuatingcycle: ?I?ve worked on this for X years, why would I just give it away??
This ignores thefact that ?giving it away?
will make it easier for others to use your work, because if youdon?t make your code available, who is really going to spend years re-implementingwhat you did?Webber (2007) draws attention to the amount of time our community wastes inwriting and reviewing papers that are rejected and eventually abandoned.
In a similarvein, we should all think about the time we cost our community when we don?t releasesoftware and make anyone who is interested in using or validating our work do theirown implementation.If software is released publicly under one of the standard licenses that protects yourcopyright (e.g., the GNU General Public License3 or the Mozilla Public License4) thenthere is little danger of your work being misappropriated, and you will build a reservoirof good will within our community.
Most users don?t want to steal from you; theysimply want to use your code to build their own system while giving you all the creditthat is your due.
As your software acquires a following, you can use that as a foundationfor offering tutorials andworkshops and othermeans of dissemination that will increaseyour visibility in the research community, thereby enhancing the credibility and impactof the work you have done.3.3 Ensure Project Survivability By Releasing SoftwareReleased software can allow your project to sustain itself despite turnover in personneland the passage of time.
There is no greater satisfaction than opening up a softwarerelease that has not been used for a few years and immediately being able to startproducing meaningful results, without having to reverse engineer it or trace throughcode line by line.
The more time passes, the more you become just like every otherpotential user of your software; so, as you are creating it, remember that in a few yearsyour memory of all the details that now seem so obvious will have faded, and you willbe grateful for a job well done, and that will translate into time saved as you begin touse that software again.Imagine meeting with a new project member and being able to say: ?Go downloadthis software, read the documentation, install it, run the script that reproduces our ACLexperiments, and thenwe can start talking tomorrow about how you are going to extendthat work .
.
.
?
This lowers the bar for entry to your project for new colleagues, and saves3 www.gnu.org/copyleft/gpl.html.4 www.mozilla.org/MPL/.468Pedersen Empiricism Is Not a Matter of Faithyour existing team considerable time when introducing a new member to the work ofyour group.Although youwon?t spendmuch time thinking about it at the start of a project, yourstudents will graduate, post-docs will move on, employees will resign, and you mighteven find a better job somewhere.
Having publicly released software helps clarify whatrights former project members have once they have left a project.
This is a painfullymurky area, and it can lead to many misunderstandings and bad feelings that take timeand energy to deal with as they arise.That confusion can also cause former colleagues to distance themselves from aproject simply because they feel they don?t have the right to participate, and in factin some cases they may not even have access to or copies of the very system they spentall those months or years working on.
This difficult situation is absolutely avoided ifyou release the software: Your former colleagues will have exactly the same rights asanyone else.
They can remain a part of the community of users, testers, and developers,and can often provide valuable continuity in a project even if they have moved to a newproject or organization.
The same is true for you.
Suppose you move from the academicworld to a position in industry: If your project code has already been released prior tothis move, then you can safely continue to use it without fear of losing control of it toyour new employer.3.4 Make The World A Better PlaceFinally, although this viewpoint may seem quaint or naive, a great deal of our research isfunded by public tax dollars, by people who make ten dollars an hour waiting tables orstanding behind a counter in a convenience store for 12 hours at a time.
We are fortunateto do what we do: even if it takes many hours and causes great personal stress, in theend the work is challenging and satisfying, and compared to how most people in theworld live and work, we are leading charmed and privileged lives.Although most taxpayers won?t have much interest in reading our papers and run-ning our code, they ought to have that opportunity.
And who knows, maybe when theirchildren take a Computational Linguistics or Artificial Intelligence class they will runacross a piece of our publicly available code that will cause them to pause and think, andmaybe inspire them to try something new or different, maybe even make them thinkabout becoming one of our community.
It?s not the most likely scenario, but it seemslike we really ought to try to give back as much as we can to the greater public good.4.
What should Computational Linguistics Do?We seem as a community to have accepted a very curious state of affairs.
As reviewersand readers of Computational Linguistics and the proceedings of ACL conferences, we in-sist upon extensive, rigorous, and fine-grained evaluations, where the difference in per-formance between competing methods is sometimes rather small.
However, we don?texpect to be able to reproduce these results or modify these experiments in any way.With the rise of search engines as a source of linguistic data, we may have evenreached a point where we don?t expect our data to be reproducible due to the arbitraryresults they provide.
Kilgarriff (2007) argues, ?Googleology is bad science,?
to whichwe would simply add ?because it is not reproducible.
?But instead of insisting upon reproducibility, we tell ourselves to think aboutthe bigger picture, to focus on the ideas and not the software, as those are just ?im-plementation issues.?
This is a debilitating paradox, because results must be supported469Computational Linguistics Volume 34, Number 3experimentally with great precision and detail and are judged according to harsh em-pirical standards, but we as readers and reviewers are asked to accept that these resultsare accurate and reproducible on faith.If we believe in empirical methods and the value of comparisons and experimentalstudies, then we must also believe in having access to the software that produced thoseresults as a necessary and essential part of the evidentiary process.
Without that we areasked to re-implement methods that are often too complicated and underspecified forthis to be possible, or to accept the reported results as a matter of faith.There are two courses of action open to us.
One is to back away from the verystringent standards that focus on evaluation and comparisons of empirical results; toapproach things more with a focus on bigger ideas, and less on statistically significantempirical results.
This is not necessarily a bad thing, and might address concerns suchas those raised by Chuch (2005) about very conservative reviewing in our field and theresulting tendency to prefer incremental improvements.However, the other path is to accept (and in fact insist) that highly detailed empiricalstudies must be reproducible to be credible, and that it is unreasonable to expect thatreproducibility be possible based on the description provided in a publication.
Thus,releasing software that makes it easy to reproduce and modify experiments should bean essential part of the publication process, to the point where we might one day onlyaccept for publication articles that are accompanied by working software that allows forimmediate and reliable reproduction of results.AcknowledgmentsI would like to thank Robert Dale forsuggesting this topic, and for his manyhelpful comments and suggestions.ReferencesChuch, Kenneth.
2005.
Reviewing thereviewers.
Computational Linguistics,31(4):575?578.Kilgarriff, Adam.
2007.
Googleology is badscience.
Computational Linguistics,33(1):147?151.Webber, Bonnie.
2007.
Breaking news:Changing attitudes and practices.Computational Linguistics, 33(4):607?611.470
