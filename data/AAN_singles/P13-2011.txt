Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 58?62,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAn Empirical Study on Uncertainty Identification in Social Media ContextZhongyu Wei1, Junwen Chen1, Wei Gao2,Binyang Li1, Lanjun Zhou1, Yulan He3, Kam-Fai Wong11The Chinese University of Hong Kong, Shatin, N.T., Hong Kong2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar3School of Engineering & Applied Science, Aston University, Birmingham, UK{zywei,jwchen,byli,ljzhou,kfwong}@se.cuhk.edu.hkwgao@qf.org.qa, y.he@cantab.netAbstractUncertainty text detection is importantto many social-media-based applicationssince more and more users utilize socialmedia platforms (e.g., Twitter, Facebook,etc.)
as information source to produceor derive interpretations based on them.However, existing uncertainty cues are in-effective in social media context becauseof its specific characteristics.
In this pa-per, we propose a variant of annotationscheme for uncertainty identification andconstruct the first uncertainty corpus basedon tweets.
We then conduct experimentson the generated tweets corpus to study theeffectiveness of different types of featuresfor uncertainty text identification.1 IntroductionSocial media is not only a social network tool forpeople to communicate but also plays an importantrole as information source with more and moreusers searching and browsing news on it.
Peoplealso utilize information from social media for de-veloping various applications, such as earthquakewarning systems (Sakaki et al, 2010) and freshwebpage discovery (Dong et al, 2010).
How-ever, due to its casual and word-of-mouth pecu-liarities, the quality of information in social me-dia in terms of factuality becomes a premier con-cern.
Chances are there for uncertain informationor even rumors flooding in such a context of freeform.
We analyzed a tweet dataset which includes326,747 posts (Details are given in Section 3) col-lected during 2011 London Riots, and result re-veals that at least 18.91% of these tweets bear un-certainty characteristics1.
Therefore, distinguish-ing uncertain statements from factual ones is cru-cial for users to synthesize social media informa-tion to produce or derive reliable interpretations,1The preliminary study was done based on a manually de-fined uncertainty cue-phrase list.
Tweets containing at leastone hedge cue were treated as uncertain.and this is expected helpful for applications likecredibility analysis (Castillo et al, 2011) and ru-mor detection (Qazvinian et al, 2011) based onsocial media.Although uncertainty has been studied theoret-ically for a long time as a grammatical phenom-ena (Seifert and Welte, 1987), the computationaltreatment of uncertainty is a newly emerging areaof research.
Szarvas et al (2012) pointed out that?Uncertainty - in its most general sense - can beinterpreted as lack of information: the receiver ofthe information (i.e., the hearer or the reader) can-not be certain about some pieces of information?.In recent years, the identification of uncertaintyin formal text, e.g., biomedical text, reviews ornewswire, has attracted lots of attention (Kilicogluand Bergler, 2008; Medlock and Briscoe, 2007;Szarvas, 2008; Light et al, 2004).
However, un-certainty identification in social media context israrely explored.Previous research shows that uncertainty identi-fication is domain dependent as the usage of hedgecues varies widely in different domains (Moranteand Sporleder, 2012).
Therefore, the employmentof existing out-of-domain corpus to social mediacontext is ineffective.
Furthermore, compared tothe existing uncertainty corpus, the expression ofuncertainty in social media is fairly different fromthat in formal text in a sense that people usu-ally raise questions or refer to external informa-tion when making uncertain statements.
But, nei-ther of the uncertainty expressions can be repre-sented based on the existing types of uncertaintydefined in the literature.
Therefore, a different un-certainty classification scheme is needed in socialmedia context.In this paper, we propose a novel uncertaintyclassification scheme and construct the first uncer-tainty corpus based on social media data ?
tweetsin specific here.
And then we conduct experi-ments for uncertainty post identification and studythe effectiveness of different categories of featuresbased on the generated corpus.582 Related workWe introduce some popular uncertainty corporaand methods for uncertainty identification.2.1 Uncertainty corpusSeveral text corpora from various domains havebeen annotated over the past few years at differentlevels (e.g., expression, event, relation, sentence)with information related to uncertainty.Sauri and Pustejovsky (2009) presented a cor-pus annotated with information about the factu-ality of events, namely Factbank, which is con-structed based on TimeBank2 containing 3,123 an-notated sentences from 208 news documents with8 different levels of uncertainty defined.Vincze et al (2008) constructed the BioSocpecorpus, which consists of medical and biologicaltexts annotated for negation, uncertainty and theirlinguistic scope.
This corpus contains 20,924 sen-tences.Ganter et al (2009) generated WikipediaWeasels Corpus, where Weasel tags in Wikipediaarticles is adopted readily as labels for uncertaintyannotation.
It contains 168,923 unique sentenceswith 437 weasel tags in total.Although several uncertainty corpora exist,there is not a uniform set of standard for uncer-tainty annotation.
Szarvas et al (2012) normal-ized the annotation of the three corpora aforemen-tioned.
However, the context of these corporais different from that of social media.
Typically,these documents annotated are grammatically cor-rect, carefully punctuated, formally structured andlogically expressed.2.2 Uncertainty identificationPrevious work on uncertainty identification fo-cused on classifying sentences into uncertainor definite categories.
Existing approaches aremainly based on supervised methods (Light etal., 2004; Medlock and Briscoe, 2007; Medlock,2008; Szarvas, 2008) using the annotated corpuswith different types of features including Part-Of-Speech (POS) tags, stems, n-grams, etc..Classification of uncertain sentences was con-solidated as a task in the 2010 edition of CoNLLshared task on learning to detect hedge cuesand their scope in natural language text (Farkaset al, 2010).
The best system for Wikipediadata (Georgescul, 2010) employed Support VectorMachine (SVM), and the best system for biolog-ical data (Tang et al, 2010) adopted Conditional2http://www.timeml.org/site/timebank/timebank.htmlRandom Fields (CRF).In our work, we conduct an empirical study ofuncertainty identification on tweets dataset and ex-plore the effectiveness of different types of fea-tures (i.e., content-based, user-based and Twitter-specific) from social media context.3 Uncertainty corpus for microblogs3.1 Types of uncertainty in microblogsTraditionally, uncertainty can be divided intotwo categories, namely Epistemic and Hypothet-ical (Kiefer, 2005).
For Epistemic, there are twosub-classes Possible and Probable.
For Hypotheti-cal, there are four sub-classes including Investiga-tion, Condition, Doxastic andDynamic.
The detailof the classification is described as below (Kiefer,2005):Epistemic: On the basis of our world knowledgewe cannot decide at the moment whether thestatement is true or false.Hypothetical: This type of uncertainty includesfour sub-classes:?
Doxastic: Expresses the speaker?s be-liefs and hypotheses.?
Investigation: Proposition under inves-tigation.?
Condition: Proposition under condi-tion.?
Dynamic: Contains deontic, disposi-tional, circumstantial and buletic modal-ity.Compared to the existing uncertainty corpora,social media authors enjoy free form of writing.In order to study the difference, we annotated asmall set of 827 randomly sampled tweets accord-ing to the scheme of uncertainty types above, inwhich we found 65 uncertain tweets.
And then,we manually identified all the possible uncertaintweets, and found 246 really uncertain ones out ofthese 827 tweets, which means that 181 uncertaintweets are missing based on this scheme.
We havethe following three salient observations:?
Firstly, there is no tweet found with the type ofInvestigation.
We find people seldom use wordslike ?examine?
or ?test?
(indicative words of In-vestigation category) when posting tweets.
Oncethey do this, the statement should be consideredas highly certain.
For example, @dobibid I havetested the link, it is fake!?
Secondly, people frequently raise questionsabout some specific topics for confirmation whichexpresses uncertainty.
For example, @ITVCentral59Can you confirm that Birmingham children?s hos-pital has/hasn?t been attacked by rioters??
Thirdly, people tend to post message with exter-nal information (e.g., story from friends) which re-veals uncertainty.
For example, Friend who worksat the children?s hospital in Birmingham says theriot police are protecting it.Based on these observations, we propose a vari-ant of uncertainty types in social media contextby eliminating the category of Investigation andadding the category of Question and External un-der Hypothetical, as shown in Table 3.1.
Notethat our proposed scheme is based on Kiefer?swork (2005) which was previously extended tonormalize uncertainty corpora in different genresby Szarvas et al (2012).
But we did not try theseextended schema for specific genres since even themost general one (Kiefer, 2005) was proved un-suitable for social media context.3.2 Annotation resultThe dataset we annotated was collected from Twit-ter using Streaming API during summer riotsin London during August 6-13 2011, including326,747 tweets in total.
Search criteria includehashtags like #ukriots, #londonriots, #prayforlon-don, and so on.
We further extracted the tweetsrelating to seven significant events during the riotidentified by UK newspaper The Guardian fromthis set of tweets.
We annotated all the 4,743 ex-tracted tweets for the seven events3.Two annotators were trained to annotate thedataset independently.
Given a collection oftweets T = {t1, t2, t3...tn}, the annotation task isto label each tweet ti as either uncertain or cer-tain.
Uncertainty assertions are to be identifiedin terms of the judgements about the author?s in-tended meaning rather than the presence of uncer-tain cue-phrase.
For those tweets annotated as un-certain, sub-class labels are also required accord-ing to the classification indicated in Table 3.1 (i.e.,multi-label is allowed).The Kappa coefficient (Carletta, 1996) indi-cating inter-annotator agreement was 0.9073 forthe certain/uncertain binary classification and was0.8271 for fine-grained annotation.
The conflictlabels from the two annotators were resolved by athird annotator.
Annotation result is displayed inTable 3.2, where 926 out of 4,743 tweets are la-beled as uncertain accounting for 19.52%.
Ques-tion is the uncertainty category with most tweets,followed by External.
Only 21 tweets are labeled3http://www.guardian.co.uk/uk/interactive/2011/dec/07/london-riots-twitterTweet# 4743Uncertainty# 926Epistemic Possible# 16Probable# 129HypotheticalCondition# 71Doxastic# 48Dynamic# 21External# 208Question# 488Table 2: Statistics of annotation resultas Dynamic and all of them are buletic modal-ity4 which shares similarity with Doxastic.
There-fore, we consider Dynamic together with Domes-tic in the error analysis for simplicity.
Duringthe preliminary annotation, we found that uncer-tainty cue-phrase is a good indicator for uncer-tainty tweets since tweets labeled as uncertain al-ways contain at least one cue-phrase.
Therefore,annotators are also required identify cue-phraseswhich trigger the sense of uncertainty in the tweet.All cue-phrases appearing more than twice are col-lected to form a uncertainty cue-phrase list.4 Experiment and evaluationWe aim to identify those uncertainty tweets fromtweet collection automatically based on machinelearning approaches.
In addition to n-gram fea-tures, we also explore the effectiveness of threecategories of social media specific features includ-ing content-based, user-based and Twitter-specificones.
The description of the three categories offeatures is shown in Table 4.
Since the length oftweet is relatively short, we therefore did not carryout stopwords removal or stemming.Our preliminary experiments showed that com-bining unigrams with bigrams and trigrams gavebetter performance than using any one or two ofthese three features.
Therefore, we just report theresult based on the combination of them as n-gramfeatures.
Five-fold cross validation is used forevaluation.
Precision, recall and F-1 score of un-certainty category are used as the metrics.4.1 Overall performanceThe overall performance of different approachesis shown in Table 4.1.
We used uncertainty cue-phrase matching approach as baseline, denotedby CP.
For CP, we labeled tweets containing atleast one entry in uncertainty cue-phrase list (de-scribed in Section 3) as uncertain.
All the otherapproaches are supervised methods using SVMbased on different feature sets.
n-gram stands forn-gram feature set, C means content-based featureset, U denotes user-based feature set, T represents4Proposition expresses plans, intentions or desires.60Category Subtype Cue Phrase ExampleEpistemic Possible, etc.
may, etc.
It may be raining.Probable likely, etc.
It is probably raining.HypotheticalCondition if, etc.
If it rains, we?ll stay in.Doxastic believe, etc.
He believes that the Earth is flat.Dynamic hope, etc.
fake picture of the london eye on fire... i hopeExternal someone said, etc.
Someone said that London zoo was attacked.Question seriously?, etc.
Birmingham riots are moving to the children hospital?!
seriously?Table 1: Classification of uncertainty in social media contextCategory Name DescriptionContent-basedLength Length of the tweetCue Phrase Whether the tweet contains a uncertainty cueOOV Ratio Ratio of words out of vocabularyTwitter-specificURL Whether the tweet contains a URLURL Count Frequency of URLs in corpusRetweet Count How many times has this tweet been retweetedHashtag Whether the tweet contains a hashtagHashtag Count Number of Hashtag in tweetsReply Is the current tweet a reply tweetRtweet Is the current tweet a retweet tweetUser-basedFollower Count Number of follower the user ownsList Count Number of list the users ownsFriend Count Number of friends the user ownsFavorites Count Number of favorites the user ownsTweet Count Number of tweets the user publishedVerified Whether the user is verifiedTable 3: Feature list for uncertainty classificationApproach Precision Recall F-1CP 0.3732 0.9589 0.5373SVMn?gram 0.7278 0.8259 0.7737SVMn?gram+C 0.8010 0.8260 0.8133SVMn?gram+U 0.7708 0.8271 0.7979SVMn?gram+T 0.7578 0.8266 0.7907SVMn?gram+ALL 0.8162 0.8269 0.8215SVMn?gram+Cue Phrase 0.7989 0.8266 0.8125SVMn?gram+Length 0.7372 0.8216 0.7715SVMn?gram+OOV Ratio 0.7414 0.8233 0.7802Table 4: Result of uncertainty tweets identificationTwitter-specific feature set and ALL is the combi-nation of C, U and T.Table 4.1 shows that CP achieves the best recallbut its precision is the lowest.
The learning basedmethods with different feature sets give some sim-ilar recalls.
Compared to CP, SVMn?gram in-creases the F-1 score by 43.9% due to the salientimprovement on precision and small drop of re-call.
The performance improves in terms of pre-cision and F-1 score when the feature set is ex-panded by adding C, U or T onto n-gram, where+C brings the highest gain, and SVMn?gram+ALLperforms best in terms of precision and F-1 score.We then study the effectiveness of the threecontent-based features, and result shows that thepresence of uncertain cue-phrase is most indica-tive for uncertainty tweet identification.4.2 Error analysisWe analyze the prediction errors based onSVMn?gram+ALL.
The distribution of errors interms of different types of uncertainty is shownType Poss.
Prob.
D.&D.
Cond.
Que.
Ext.Total# 16 129 69 71 488 208Error# 11 20 18 11 84 40% 0.69 0.16 0.26 0.15 0.17 0.23Table 5: Error distributionsin Table 4.2.
Our method performs worst on thetype of Possible and on the combination of Dy-namic and Doxastic because these two types havethe least number of samples in the corpus and theclassifier tends to be undertrained without enoughsamples.5 Conclusion and future workIn this paper, we propose a variant of classificationscheme for uncertainty identification in social me-dia and construct the first uncertainty corpus basedon tweets.
We perform uncertainty identificationexperiments on the generated dataset to explorethe effectiveness of different types of features.
Re-sult shows that the three categories of social mediaspecific features can improve uncertainty identifi-cation.
Furthermore, content-based features bringthe highest improvement among the three and thepresence of uncertain cue-phrase contributes mostfor content-based features.In future, we will explore to use uncertaintyidentification for social media applications.6 AcknowledgementThis work is partially supported by General Re-search Fund of Hong Kong (No.
417112).61ReferencesJean Carletta.
1996.
Assessing agreement on classi-fication tasks: the kappa statistic.
Computationallinguistics, 22(2):249?254.Carlos Castillo, Marcelo Mendoza, and BarbaraPoblete.
2011.
Information credibility on twitter.In Proceedings of the 20th International Conferenceon World Wide Web, pages 675?684.Anlei Dong, Ruiqiang Zhang, Pranam Kolari, JingBai, Fernando Diaz, Yi Chang, Zhaohui Zheng, andHongyuan Zha.
2010.
Time is of the essence: im-proving recency ranking using twitter data.
In Pro-ceedings of the 19th International Conference onWorld Wide Web, pages 331?340.
ACM.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The conll-2010 shared task: learning to detect hedges and theirscope in natural language text.
In Proceedings ofthe 14th Conference on Computational Natural Lan-guage Learning?Shared Task, pages 1?12.
Associ-ation for Computational Linguistics.Viola Ganter and Michael Strube.
2009.
Findinghedges by chasing weasels: Hedge detection usingwikipedia tags and shallow linguistic features.
InProceedings of the ACL-IJCNLP 2009, pages 173?176.
Association for Computational Linguistics.Maria Georgescul.
2010.
A hedgehop over a max-margin framework using hedge cues.
In Proceed-ings of the 14th Conference on Computational Natu-ral Language Learning?Shared Task, pages 26?31.Association for Computational Linguistics.Ferenc Kiefer.
2005.
Lehetoseg es szuk-segszeruseg[Possibility and necessity].
Tinta Kiado,Budapest.H.
Kilicoglu and S. Bergler.
2008.
Recognizing spec-ulative language in biomedical research articles: alinguistically motivated perspective.
BMC bioinfor-matics, 9(Suppl 11):S10.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The language of bioscience: Facts, specula-tions, and statements in between.
In Proceedingsof BioLink 2004 workshop on linking biological lit-erature, ontologies and databases: tools for users,pages 17?24.B.
Medlock and T. Briscoe.
2007.
Weakly supervisedlearning for hedge classification in scientific litera-ture.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages992?999.Ben Medlock.
2008.
Exploring hedge identification inbiomedical literature.
Journal of Biomedical Infor-matics, 41(4):636?654.Roser Morante and Caroline Sporleder.
2012.
Modal-ity and negation: An introduction to the special is-sue.
Computational Linguistics, 38(2):223?260.Vahed Qazvinian, Emily Rosengren, Dragomir RRadev, and Qiaozhu Mei.
2011.
Rumor has it:Identifying misinformation in microblogs.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 1589?1599.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes twitter users: real-timeevent detection by social sensors.
In Proceedingsof the 19th International Conference on World WideWeb, pages 851?860.
ACM.R.
Saur??
and J. Pustejovsky.
2009.
Factbank: A cor-pus annotated with event factuality.
Language Re-sources and Evaluation, 43(3):227?268.Stephan Seifert and Werner Welte.
1987.
A basic bib-liography on negation in natural language, volume313.
Gunter Narr Verlag.Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,Gyo?rgy Mo?ra, and Iryna Gurevych.
2012.
Cross-genre and cross-domain detection of semantic uncer-tainty.
Computational Linguistics, 38(2):335?367.Gyo?rgy Szarvas.
2008.
Hedge classification inbiomedical texts with a weakly supervised selectionof keywords.
In Proceedings of 46th Annual Meet-ing of the Association for Computational Linguis-tics.Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,and Shixi Fan.
2010.
A cascade method for detect-ing hedges and their scope in natural language text.In Proceedings of the 14th Conference on Compu-tational Natural Language Learning?Shared Task,pages 13?17.
Association for Computational Lin-guistics.V.
Vincze, G. Szarvas, R. Farkas, G. Mo?ra, andJ.
Csirik.
2008.
The bioscope corpus: biomedicaltexts annotated for uncertainty, negation and theirscopes.
BMC bioinformatics, 9(Suppl 11):S9.62
