Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 174?177,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPA general scheme for broad-coverage multimodal annotationPhilippe BlacheLaboratoire Parole et LangageCNRS & Aix-Marseille Universite?sblache@lpl-aix.frAbstractWe present in this paper a formal andcomputational scheme in the perspectiveof broad-coverage multimodal annotation.We propose in particular to introducethe notion of annotation hypergraphs inwhich primary and secondary data are rep-resented by means of the same structure.This paper addresses the question of resourcesand corpora for natural human-human interaction,in other words broad-coverage annotation of natu-ral data.
In this kind of study, most of domainshave do be taken into consideration: prosody,pragmatics, syntax, gestures, etc.
All these dif-ferent domains interact in order to build an un-derstandable message.
We need then large mul-timodal annotated corpora of real data, preciselyannotated for all domains.
Building this kind ofresource is a relatively new, but very active re-search domain, illustrated by the number of work-shops (cf.
(Martin, 2008)), international initia-tives, such as MUMIN (Allwood, 2005), anno-tation tools such as NITE NXT (Carletta, 2003),Anvil (Kipp, 2001), etc.1 A characterization of primary dataDifferent types of primary data constitute the basisof an annotation: speech signal, video input, wordstrings, images, etc.
But other kinds of primarydata also can be used, for example in the perspec-tive of semantic annotations such as concepts, ref-erences, types, etc.
Such data are considered to beatomic in the sense that they are not built on topof lower level data.
When looking more closely atthese kinds of data, several characteristics can beidentified:- Location: primary data is usually localized withrespect to a timeline or a position: gestures canbe localized into the video signal, phonemes intothe speech one, words into the string or objectsinto a scene or a context.
Two different kinds oflocalisation are used: temporal and spatial.
In thefirst case, a data is situated by means of a timeinterval whereas spatial data are localised in termsof relative or absolute positions.- Realization: primary data usually refer to con-crete (or physical) objects: phonemes, gestures,referential elements into a scene, etc.
However,other kinds of primary data can be abstract suchas concepts, ideas, emotions, etc.- Medium: The W3C recommendation EMMA(Extensible Multi-Modal Annotations) proposes todistinguish different medium: acoustic, tactile andvisual.
This classification is only relevant for datacorresponding to concrete objects.- Production: the study of information structureshows the necessity to take into account accessi-bility of the objects: some data are directly acces-sible from the signal or the discourse, they have anexistence or have already been mentioned.
In thiscase, they are said to be ?produced?.
For example,gestures, sounds, physical objects fall in this cate-gory.
On the other hand, other kinds of data are de-duced from the context, typically the abstract ones.They are considered as ?accessible?.In the remaining of the paper, we propose thefollowing definition:Primary data: atomic objects that cannot be de-composed.
They represent possible constituent ontop of which higher level objects can be built.
Pri-mary data does not require any interpretation tobe identified, they are of direct access.This primary data typology is given in fig-ure (1).
It shows a repartition between concretevs.
abstract objects.
Concrete objects are usu-ally those taken into account in corpus annotation.As a consequence, annotation usually focuses onspeech and gestures, which narrows down the setof data to those with a temporal localization.
How-ever, other kinds of data cannot be situated in the174Phonemes Words Gestures Discourse referents Synsets Physical objectsProduced + + + +/- - +Accessible - - - +/- + -Concrete + + + +/- - +Abstract - - - +/- - +Temporal + + + +/- - -Spatial - - +/- +/- - +Acoustic + +/- - - - -Visual - - + +/- - +Tactile - - +/- +/- - +Figure 1: Primary data descriptiontimeline (e.g.
objects in the environment of thescene) nor spatially (e.g.
abstract data).We need to propose a more general approachof data indexing that has to distinguish on theone hand between temporal and spatial localiza-tion and on the other hand between data that canbe located and data that cannot.2 Graph representation: nodes andedges semanticsOne of the most popular linguistic annotation rep-resentation is annotation graphs (Bird, 2001) inwhich nodes are positions whereas edges bear lin-guistic information.
This representation is elabo-rated on the basis of a temporal anchoring, eventhough it is also possible to represent other kindsof anchoring.
Several generic annotation formathas been proposed on top of this representation,such as LAF and its extension GrAF (cf.
(Ide,2007)).
In these approaches, edges to their turncan be interpreted as nodes in order to build higherlevel information.
One can consider the result asan hypergraph, in which nodes can be subgraphs.In order to explore farther this direction, we pro-pose a more general interpretation for nodes thatare not only positions in the input: nodes are com-plex objects that can be referred at different lev-els of the representation, they encode all annota-tions.
In order to obtain an homogeneous repre-sentations, the two node types used in hypergraphs(nodes and hypernodes) share the same informa-tion structure which relies on the following points:- Index: using an index renders possible to repre-sent any kind of graphs, not only trees.
They giveto nodes the possibility of encoding any kind ofinformation.- Domain: prosody, semantics, syntax, gesture,pragmatics, etc.
It is important to indicate as pre-cisely as possible this information, eventually bymeans of sub-domains- Location: annotations generally have a spatial ora temporal situation.
This information is optional.- Features: nodes have to bear specific linguisticindications, describing its properties.Hypernodes bear, on top of this information,the specification of the subgraph represented byits constituents and their relations.
We propose toadd another kind of information in the hypernodestructure:?
Relations: secondary data are built on topof primary one.
They can be represented bymeans of a set of properties (constituency,linearity, coreference, etc.)
implemented asedges plus the basic characteristics of a node.A secondary data is then graph with a label,these two elements composing an hypernode.The distinction between node and hypernodesmakes it possible to give a homogeneous repre-sentation of primary and secondary data.3 An XML representation of annotationhypergraphsWe propose in this section an XML encoding ofthe scheme presented above.3.1 Atomic nodesThe first example of the figure (2) illustrates therepresentation of a phoneme.
The node is indexed,making its reference possible in higher level struc-tures.
Its label corresponds to the tag that would beindicated in the annotation.
Other elements com-plete the description: the linguistic domain (speci-fied by the attributes type and sub-type), the speci-fication of the medium, the object localization (bymeans of anchors).
In this example, a phonemebeing part of the acoustic signal, the anchor is tem-poral and use an explicit timeline reference.The same kind of representation can be givenfor transcription tokens (see node n21 in figure(2)).
The value of the node is the orthographicform.
It is potentially aligned on the signal, andthen represented with a temporal anchoring.
Such175<node ID="n1" label="u"><domain type="phonetics" subtype="phoneme"medium="acoustic"/><anchor type="temporal" start="285" end="312"/></node><node ID="n21" label="book"><domain type="transcription" subtype="token"/><anchor type="temporal" start="242" end="422"/></node><node ID="n24" label="N"><domain type=" morphosyntax" subtype="word"/><anchor type="temporal" start="242" end="422"/><features ms="ncms---"/></node><node ID="n3" label="deictic"><domain type="gestures" subtype="hand"/><anchor type="temporal" start="200" end="422"/><features hand="right" deictic type="space"object="ref object"/></node><node ID="n4" label="discourse-referent"><domain type="semantics" subtype="discourse universe"medium="visual"/><anchoring type="spatial" x="242" y="422" z="312"/><features isa="book" color="red" /></node>Figure 2: XML encoding of atomic nodesanchoring makes it possible to align the ortho-graphic transcription with the phonetic one.
In thecase of written texts, temporal bounds would bereplaced by the positions in the texts, which couldbe interpreted as an implicit temporal anchoring.The next example presented in node n24 illus-trates the representation of part-of-speech nodes.The domain in this case is morphosyntax, its sub-type is ?word?.
In this case too, the anchoring istemporal, with same bounds as the correspondingtoken.
In this node, a feature element is added,bearing the morpho-syntactic description.The atomic node described in node n3 repre-sents another physical object: a deictic gesture.
Itsdomain is gesture and its subtype, as proposed forexample in the MUMIN scheme (see (Allwood,2005)) is the part of the body.
The anchoring isalso temporal and we can observe in this exam-ple a synchronization of the gesture with the token?book?.The last example (node n4) presents an atomicnode describing a physical object present in thescene (a book on a shelf of a library).
It belongs tothe semantics domain as a discourse referent and isanchored spatially by its spatial coordinates.
Onecan note that anchoring can be absolute (as in theexamples presented here) or relative (situating theobject with respect to other ones).3.2 RelationsRelations are represented in the same way asnodes.
They are of different types, such as con-stituency, linearity, syntactic dependency, seman-tic specification, etc.
and correspond to a certaindomain.
The example r1 in figure (3) illustrates aspecification relation between a noun (noden21, described above) and its determiner (noden20).
Non-oriented binary relations also occur,for example cooccurrency.
Relations can be ex-pressed in order to represent a set of objects.
Thenext example (relation r2) presents the case ofthree constituents of an higher-level object (thecomplete description of which being given in thenext section).Finally, the alignment between objects is speci-fied by two different values: strict when they haveexactly the same temporal or spatial marks; fuzzyotherwize.3.3 HypernodesHypernodes encode subgraphs with the possibilityof being themselves considered as nodes.
Theirstructure completes the atomic node with a set ofrelations.
Hypernodes encode different kinds ofobjects such as phrases, constructions, referentialexpressions, etc.
The first example represents aNP.
The node is indexed, bears a tag, a domain, ananchoring and features.
The set of relations spec-ifies two types of information.
First, the NP nodehas three constituents: n20 (for example a deter-miner), n22 (for example an adjective) and n24(the noun described in the previous section).
Thealignment is said to be strict which means that theright border of the first element and the left borderof the last one have to be the same.
The resultingstructure is an hypernode describing the differentcharacteristics of the NP by means of features andrelations.The second example illustrates the case of a ref-erential expression.
Let?s imagine the situationwhere a person points out at a book on a shelf,saying ?The book will fall down?.
In terms of in-formation structure, the use of a definite NP is pos-sible because the referent is accessible from thephysical context: the alignment of the NP (n50)and the deictic gesture (n3, see previous section)makes the coreference possible.
This construc-tion results in a discourse referent bringing to-gether all the properties of the physical object (n3)and that of the object described in the discourse176<relation id="r1" label="specification"><domain type="syntax" subtype="oriented rel"/><edge from="n20" to="n24"></relation><relation id="r2" label="constituency"><domain type="syntax" subtype="set rel"/><node list><node id="n20"/> <node id="n22"/> <node id="n24"/></node list><alignment type="strict"/></relation>Figure 3: XML encoding of relations<node ID="n50" label="NP"><domain type="syntax" subtype="phrase"/><anchor type="temporal" start="200" end="422"/><features cat="NP" agr="ms" sem type="ref"/><relations><relation id="r1" type="constituency"><domain type="syntax" subtype="set rel"/><node list><node id="n20"/> <node id="n22"/> <node id="n24"/></node list><alignment type="strict"/></relation><relation id="r2" type="specification"><domain type="syntax" subtype="oriented rel"/><edge from="n20" to="n24"></relation></relations></node><node ID="n51" label="ref expression"><domain type="semantics" subtype="discourse referent"/><features referent="book?"
color="red" /><relations><relation id="r3" type="constituency"><domain type="semantics" type="set rel"/><node list><node id="n50"/> <node id="n3"/> <node id="n4"/></node list><alignment type="fuzzy"/></relation><relation id="r4" type="pointing"><domain type="gesture" type="oriented rel"/><edge from="n3" to="n4"><alignment type="strict"/></relation></relations></node>Figure 4: XML encoding of hypernodes(n50).
In this expression, the alignment betweenthe objects is fuzzy, which is the normal situationwhen different modalities interact.
The second re-lation describes the pointing action, implementingthe coreference between the noun phrase and thephysical object.
This representation indicates thethree nodes as constituents.4 ConclusionUnderstanding the mechanisms of natural interac-tion requires to explain how the different modal-ities interact.
We need for this to acquire multi-modal data and to annotate them as precisely aspossible for all modalities.
Such resources haveto be large enough both for theoretical and com-putational reasons: we need to cover as broadlyas possible the different phenomena and give thepossibility to use machine learning techniques inorder to produce a new generation of multimodalannotation tools.
However, neither such resource,and a fortiori such tools, already exist.
One reason,besides the cost of the annotation task itself whichis still mainly manual for multimodal information,is the lack of a general and homogeneous anno-tation scheme capable of representing all kinds ofinformation, whatever its origin.We have presented in this paper the basis ofsuch a scheme, proposing the notion of annota-tion hypergraphs in which primary as well as sec-ondary data are represented by means of the samenode structure.
This homogeneous representationis made possible thanks to a generic descriptionof primary data, identifying four types of basic in-formation (index, domain, location, features).
Wehave shown that this scheme can be directly repre-sented in XML, resulting in a generic multimodalcoding scheme.ReferencesAllwood J., L. Cerrato, L. Dybkjaer, & al.
(2005) ?TheMUMIN Multimodal Coding Scheme?, NorFAyearbookBird S., M. Liberman (2001) ?A formal frameworkfor linguistic annotation?
Speech Communication,ElsevierCarletta, J., J. Kilgour, and T. O?Donnell (2003) ?TheNITE Object Model Library for Handling StructuredLinguistic Annotation on Multimodal Data Sets?
inprocs of the EACL Workshop on Language Technol-ogy and the Semantic WebIde N. & K. Suderman (2007) ?GrAF: A Graph-basedFormat for Linguistic Annotations?, in proceed-ings of the Linguistic Annotation Workshop at theACL?07 (LAW-07)Kipp M. (2001) ?Anvil-a generic annotation tool formultimodal dialogue?
in procs of 7th EuropeanConference on Speech Communication and Tech-nologyMartin, J.-C., Paggio, P., Kipp, M., Heylen, D. (2008)Proceedings of the Workshop on Multimodal Cor-pora : From Models of Natural Interaction to Sys-tems and Applications (LREC?2008)177
