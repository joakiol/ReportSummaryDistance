Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 318?322,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUsing Context Vectors in Improving a Machine Translation Systemwith Bridge LanguageSamira Tofighi Zahabi       Somayeh Bakhshaei       Shahram KhadiviHuman Language Technology LabAmirkabir University of TechnologyTehran, Iran{Samiratofighi,bakhshaei,khadivi}@aut.ac.irAbstractMapping phrases between languages astranslation of each other by using an intermediatelanguage (pivot language) may generatetranslation pairs that are wrong.
Since a word or aphrase has different meanings in differentcontexts, we should map source and targetphrases in an intelligent way.
We propose apruning method based on the context vectors toremove those phrase pairs that connect to eachother by a polysemous pivot phrase or by weaktranslations.
We use context vectors to implicitlydisambiguate the phrase senses and to recognizeirrelevant phrase translation pairs.Using the proposed method a relativeimprovement of 2.8 percent in terms of BLEUscore is achieved.1 IntroductionParallel corpora as an important component of astatistical machine translation system areunfortunately unavailable for all pairs oflanguages, particularly in low resource languagesand also producing it consumes time and cost.So, new ideas have been developed about how tomake a MT system which has lower dependencyon parallel data like using comparable corporafor improving performance of a MT system withsmall parallel corpora or making a MT systemwithout parallel corpora.
Comparable corporahave segments with the same translations.
Thesesegments might be in the form of words, phrasesor sentences.
So, this extracted information canbe added to the parallel corpus or might be usedfor adaption of the language model or translationmodel.Comparable corpora are easily availableresources.
All texts that are about the same topiccan be considered as comparable corpora.Another idea for solving the scarce resourceproblem is to use a high resource language as apivot to bridge between source and targetlanguages.
In this paper we use the bridgetechnique to make a source-target system and wewill prune the phrase table of this system.
InSection 2, the related works of the bridgeapproach are considered, in Section 3 theproposed approach will be explained and it willbe shown how to prune the phrase table usingcontext vectors, and experiments on German-English-Farsi systems will be presented inSection 4.2 Related WorksThere are different strategies of bridgetechniques to make a MT system.
The simplestway is to build two MT systems in two sides: onesystem is source-pivot and the other is pivot-target system, then in the translation stage theoutput of the first system is given to the secondsystem as an input and the output of the secondsystem is the final result.
The disadvantage ofthis method is its time consuming translationprocess, since until the first system?s output isnot ready; the second system cannot start thetranslation process.
This method is calledcascading of two translation systems.In the other approach the target side of thetraining corpus of the source-pivot system isgiven to the pivot-target system as its input.
Theoutput of the pivot-target system is parallel withthe source side of the training corpus of thesource-pivot system.
A source-to-target systemcan be built by using this noisy parallel corpuswhich in it each source sentence is directlytranslated to a target sentence.
This method iscalled the pseudo corpus approach.Another way is combining the phrase tables ofthe source-pivot and pivot-target systems todirectly make a source-target phrase table.
Thiscombination is done if the pivot phrase is318identical in both phrase tables.
Since one phrasehas many translations in the other language, alarge phrase table will be produced.
This methodis called combination of phrase tables approach.Since in the bridge language approach twotranslation systems are used to make a finaltranslation system, the errors of these twotranslation systems will affect the final output.Therefore in order to decrease the propagation ofthese errors, a language should be chosen aspivot which its structure is similar to the sourceand target languages.
But even by choosing agood language as pivot there are some othererrors that should be handled or decreased suchas the errors of ploysemous words and etc.For making a MT system using pivot languageseveral ideas have been proposed.
Wu and Wang(2009) suggested a cascading method which isexplained in Section 1.Bertoldi (2008) proposed his method inbridging at translation time and bridging attraining time by using the cascading method andthe combination of phrase tables.Bakhshaei (2010) used the combination ofphrase tables of source-pivot and pivot-targetsystems and produced a phrase table for thesource-target system.Paul (2009) did several experiments to showthe effect of pivot language in the finaltranslation system.
He showed that in some casesif training data is small the pivot should be moresimilar to the source language, and if trainingdata is large the pivot should be more similar tothe target language.
In Addition, it is moresuitable to use a pivot language that its structureis similar to both of source and target languages.Saralegi (2011) showed that there is nottransitive property between three languages.
Somany of the translations produced in the finalphrase table might be wrong.
Therefore forpruning wrong and weak phrases in the phrasetable two methods have been used.
One methodis based on the structure of source dictionariesand the other is based on distributional similarity.Rapp (1995) suggested his idea about theusage of context vectors in order to find thewords that are the translation of each other incomparable corpora.In this paper the combination of phrase tablesapproach is used to make a source-target system.We have created a base source-target system justsimilar to previous works.
But the contribution ofour work compared to other works is that herewe decrease the size of the produced phrase tableand improve the performance of the system.
Ourpruning method is different from the method thatSaralegi (2011) has used.
He has pruned thephrase table by computing distributionalsimilarity from comparable corpora or by thestructure of source dictionaries.
Here we usecontext vectors to determine the concept ofphrases and we use the pivot language tocompare source and target vectors.3 ApproachFor the purpose of showing how to create apruned phrase table, in Section 3.1 we willexplain how to create a simple source-to-targetsystem.
In Section 3.2 we will explain how toremove wrong and weak translations in thepruning step.
Figure 1 shows the pseudo code ofthe proposed algorithm.In the following we have used theseabbreviations: f, e stands for source and targetphrases.
pl, src-pl, pl-trg, src-trg respectivelystand for pivot phrase, source-pivot phrase table,pivot-target phrase table and source-targetphrase table.3.1 Creating source-to-target systemAt first, we assume that there is transitiveproperty between three languages in order tomake a base system, and then we will show indifferent ways that there is not transitive propertybetween three languages.Figure 1.
Pseudo code for proposed methodfor each source phrase fpls = {translations of f in src-pl }for each pl in plsEs ={ translations of pl in pl-trg }for each e in Esp(e|f) =p(pl|f)*p(e|pl) and add (e,f) to src-trgcreate source-to-destination system with src-trgcreate context vector V for each source phrase fusing source corporacreate context vector V?
for each target phrase eusing target corporaconvert Vs to pivot language vectors using src-plsystemconvert V?
s to pivot language vectors using pl-trgsystemfor each f in src-trgEs = {translations of f in src-trg}For each e in Es calculate similarity of its contextvector with f context vectorSelect k top similar as translations of fdelete other translations of f in src-trg319For each phrase f in src-pl phrase table, all thephrases pl which are translations of f, areconsidered.
Then for each of these pls everyphrase e from the pl-trg phrase table that aretranslations of pl, are found.
Finally f is mappedto all of these es in the new src-trg phrase table.The probability of these new phrases iscalculated using equation (1) through thealgorithm that is shown in figure 1.
( | ) ( | ) ( | )p e f p pl f p e pl= ?
(1)A simple src-trg phrase table is created bythis approach.
Pl phrases might be ploysemousand produce target phrases that have differentmeaning in comparison to each other.
Theconcept of some of these target phrases aresimilar to the corresponding source phrase andthe concept of others are irrelevant to the sourcephrase.The language model can ignore some of thesewrong translations.
But it cannot ignore thesetranslations if they have high probability.Since the probability of translations iscalculated using equation (1), therefore wrongtranslations have high probability in three cases:first when p(pl|f) is high, second when p(e|pl) ishigh and third when p(pl|f) and p(e|pl) are high.In the first case pl might be a good translationfor f and refers to concept c, but pl and e refer toconcept ??
so mapping f to e as a translation ofeach other is wrong.
The second case is similarto the first case but e might be a good translationfor pl.
The third case is also similar to the firstcase, but pl is a good translation for both f and e.The pruning method that is explained inSection 3.2, tries to find these translations anddelete them from the src-trg phrase table.3.2 Pruning methodTo determine the concept of each phrase (p) inlanguage L at first a vector (V) with length N iscreated.
Each element of V is set to zero and N isthe number of unique phrases in language L.In the next step all sentences of the corpus inlanguage L are analyzed.
For each phrase p if poccurs with ??
in the same sentence the elementof context vector ?
that corresponds to ??
ispulsed by 1.
This way of calculating contextvectors is similar to Rapp (1999), but here thewindow length of phrase co-occurrence isconsidered a sentence.
Two phrases areconsidered as co-occurrence if they occur in thesame sentence.
The distance between them doesnot matter.
In other words phrase ?
might be atthe beginning of the sentence while ??
being atthe end of the sentence, but they are consideredas co-occurrence phrases.For each source (target) phrase its contextvector should be calculated within the source(target) corpus as shown in figure 1.The number of unique phrases in the source(target) language is equal to the number ofunique source (target) phrases in the src-trgphrase table that are created in the last Section.So, the length of source context vectors is mand the length of target context vectors is n.These variables (m and n) might not be equal.
Inaddition to this, source vectors and target vectorsare in two different languages, so they are notcomparable.One method to translate source context vectorsto target context vectors is using an additionalsource-target dictionary.
But instead here, sourceand target context vectors are translated to pivotcontext vectors.
In other words if source contextvectors have length m and target context vectorshave length n, they are converted to pivotcontext vectors with length z.
The variable z isthe number of unique pivot phrases in src-pl orpl-trg phrase tables.To map the source context vector?
(?1, ?2, ?
, ??)
to the pivot context vector, weuse a fixed size vector ?1?.
Elements of vector?1?
= (?1,?2, ?
, ??)
are the unique phrasesextracted from src-pl or pl-trg phrase tables.?1?
= (?1,?2, ?
, ??)
= (0, 0, ?
, 0)In the first step ?
?s are set to 0.
For eachelement, ?
?, of vector S if ??
> 0 it will betranslated to k pivot phrases.
These phrases arethe output of k-best translations of ??
by usingthe src-pl phrase table.
{ }1 1 2( , ,..., )phrase takeibkls V v v v??
?
?
?=src pl?For each element ?
?of ?1??
its correspondingelement ?
of ?1?
which are equal, will be found,then the amount of ?
will be increased by ??.?
??
?
?1??
????
(?
?
?1?)
?
?
= ?????(?)
?
???(?)
+ ?
?Using K-best translations as middle phrases isfor reducing the effect of translation errors thatcause wrong concepts.
This work is done foreach target context vector.
Source and targetcontext vectors will be mapped to identicallength vectors and are also in the same language(pivot language).Now source and target contextvectors are comparable, so with a simplesimilarity metric their similarity can becalculated.Here we use cosine similarity.
The similaritybetween each source context vector and each320target context vector that are translations of thesource phrase in src-trg, are calculated.
Foreach source phrase, the N-most similar targetphrases are kept as translations of the sourcephrase.
These translations are also similar incontext.
Therefore this pruning method deletesirrelevant translations form the src-trg phrasetable.
The size of the phrase table is decreasedvery much and the system performance isincreased.
Reduction of the phrase table size isconsiderable while its performance is increased.4 ExperimentsIn this work, we try to make a German-Farsisystem without using parallel corpora.
We useEnglish language as a bridge between Germanand Farsi languages because English language isa high resource language and parallel corpora ofGerman-English and English-Farsi are available.We use Moses0F1 (Koehn et al, 2007) as the MTdecoder and IRSTLM1F2 tools for making thelanguage model.
Table 1 shows the statistics ofthe corpora that we have used in ourexperiments.
The German-English corpus is fromVerbmobil project (Ney et al, 2000).
Wemanually translate 22K English sentences toFarsi to build a small Farsi-English-Germancorpus.
Therefore, we have a small English-German corpus as well.With the German-English parallel corpus andan additional German-English dictionary with118480 entries we have made a German-English(De-En) system and with English-Farsi parallelcorpus we have made a German-Farsi (En-Fa)system.
The BLEU score of these systems areshown in Table 1.Now, we create a translation system bycombining phrase tables of De-En and En-Fasystems.
Details of creating the source-targetsystem are explained in Section 3.1.
The size ofthis phrase table is very large because ofploysemous and some weak translations.Sentences BLEUGerman-English 58,073 40.1English-Farsi 22,000  31.6Table 1.
Information of two parallel systems thatare used in our experiments.The size of the phrase table is about 55.7 MB.Then, we apply the pruning method that we1Available under the LGPL fromhttp://sourceforge.net/projects/mosesdecoder/2Available under the LGPL fromhttp://hlt.fbk.eu/en/irstlmexplained in Section 3.2.
With this method onlythe phrases are kept that their context vectors aresimilar to each other.
For each source phrase the35-most similar target translations are kept.
Thenumber of phrases in the phrase table isdecreased dramatically while the performance ofthe system is increased by 2.8 percent BLEU.The results of these experiments are shown inTable 2.
The last row in this table is the result ofusing small parallel corpus to build German-Farsi system.
We observe that the pruningmethod has gain better results compared to thesystem trained on the parallel corpus.
This ismaybe because of some translations that aremade in the parallel system and do not haveenough training data and their probabilities arenot precise.
But when we use context vectors tomeasure the contextual similarity of phrases andtheir translations, the impact of these trainingsamples are decreased.
In Table 3, two wrongphrase pairs that pruning method has removedthem are shown.BLEU # of phrasesBase bridge system 25.1   500,534Pruned system 27.9   26,911Parallel system 27.6   348,662Table 2.
The MT results of the base system, thepruned system and the parallel system.German phrase WrongtranslationCorrecttranslationvorschlagen , wir ???????
??????
??
???
?um neun Uhrmorgens????
??
???
?
?Table 3.
Sample wrong translations that theprunning method removed them.In Table 4, we extend the experiments withtwo other methods to build German-Farsi system using English as bridging language.We see that the proposed method obtainscompetitive result with the pseudo parallelmethod.System BLEU size (MB)Phrase tables combination 25.1 55.7Cascade method 25.2 NAPseudo parallel corpus  28.2 73.2Phrase tables comb.+prune 27.9 3.0Table 4.
Performance results of different ways ofbridging321Now, we run a series of significance tests tomeasure the superiority of each method.
In thefirst significance test, we set the pruned systemas our base system and we compare the result ofthe pseudo parallel corpus system with it, thesignificance level is 72%.
For anothersignificance test we set the combined phrasetable system without pruning as our base systemand we compare the result of the pruned systemwith it, the significance level is 100%.
In the lastsignificance test we set the combined phrasetable system without pruning as our base systemand we compare the result of the pseudo systemwith it, the significance level is 99%.
Therefore,we can conclude the proposed method obtainsthe best results and its difference with pseudoparallel corpus method is not significant.5 Conclusion and future workWith increasing the size of the phrase table, theMT system performance will not necessarilyincrease.
Maybe there are wrong translationswith high probability which the language modelcannot remove them from the best translations.By removing these translation pairs, theproduced phrase table will be more consistent,and irrelevant words or phrases are much less.
Inaddition, the performance of the system will beincreased by about 2.8% BLEU.In the future work, we investigate how to use theword alignments of the source-to-pivot andpivot-to-target systems to better recognize goodtranslation pairs.ReferencesSomayeh Bakhshaei, Shahram Khadivi, and NoushinRiahi.
2010.
Farsi-German statistical machinetranslation through bridge language.Telecommunications (IST) 5th InternationalSymposium on, pages 557-561.Nicola Bertoldi, Madalina Barbaiani, MarcelloFederico, and Roldano Cattoni.
2008.
Phrase-Based Statistical Machine Translation with PivotLanguage.
In Proc.
Of IWSLT, pages 143-149,Hawaii, USA.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In proc.
ofEMNLP, pages 388-395, Barcelona, Spain.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisC.
Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, WadeShen, Christine Moran,RichardZens, Chris Dyer, Ondrei Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
of ACL Demonstration Session, pages 177?180, Prague.Hermann Ney, Franz.
J. Och, Stephan Vogel.
2000.Statistical Translation of Spoken Dialogues in theVerbmobil System.
In Workshop on Multi LingualSpeech Communication, pages 69-74.Michael Paul, Hirofumi Yamamoto, Eiichiro Sumita,and Satoshi  Nakamura.
2009.
On the Importanceof Pivot Language Selection for StatisticalMachine Translation.
In proc.
Of NAACL HLT,pages 221-224, Boulder, Colorado.Reinhard Rapp.
1995.
Identifying Word Translationsin Non-Parallel Texts.
In proc.
Of ACL, pages 320-322, Stroudsburg, PA, USA.Reinhard Rapp.
1999.
Automatic Identification ofWord Translations from Unrelated English andGerman Corpora.
In Proc.
Of ACL, pages 519-525,Stroudsburg, PA, USA.Xabeir Saralegi, Iker Manterola, and Inaki S. Vicente,2011.Analyzing Methods for Improving Precisionof Pivot Based Bilingual Dictionaries.
In proc.
ofthe EMNLP, pages 846-856, Edinburgh, Scotland.Masao Utiyama and Hitoshi Isahara.
2007.
Acomparison of pivot methods for phrase-basedSMT.
InProc.
of HLT, pages 484-491, New York,US.Hua Wu and Haifeng Wang.
2007.
Pivot LanguageApproach for Phrase-Based SMT.
In Proc.
of ACL,pages 856-863, Prague, Czech Republic.322
