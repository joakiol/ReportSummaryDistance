Syntactic Ambiguity Resolution Using A Discriminationand Robustness Oriented Adaptive Learning AlgorithmTung-Hui Chiang, Yi-Chung Lin and Keh-Yih SuDepartment of Electrical EngineeringNational Tsing Hua UniversityHsinchu, Taiwan 300, R.O.C.E-Mail: thchiang@ ee.nthu.edu.twTopic Area: Computational Methods (Statistical), Application (NLP)AbstractIn this paper, a discrimination and robusmessoriented adaptive learning procedure is proposed todeal with the task of syntactic ambiguity resolution.Owing to the problem of insufficient training dataand approximation error introduced by the languagemodel, traditional statistical approaches, which re-solve ambiguities by indirectly and implicitly usingmaximum likelihood method, fail to achieve highperformance in real applications.
The proposedmethod remedies these problems by adjusting theparameters to maximize the accuracy rate directly.To make the proposed algorithm robust, the possi-ble variations between the training corpus and thereal tasks are als0 taken into consideration by en-larging the separation margin between the correctcandidate and its competing members.
Significantimprovement has been observed in the test.
Theaccuracy rate of syntactic disambiguation is raisedfrom 46.0% to 60.62% by using this novel approach.1.
IntroductionAmbiguity resolution has long been the focus innatural anguage processing.
Many rule-based ap-proaches have been proposed in the past, However,when applying such approaches tolarge scale appli-cations, they usually fail to offer satisfactory pertbr-mance.
As a huge amount of fine-grained knowl-edge is required to solve the ambiguity problem, itis quite difficult for rule-based approach to acquirethe huge and fine-grained knowledge, and maintainconsistency among them by human \[Su 90a\].Probabilistic approaches attack these problemsby providing a more objective measure on the pref-erence to a given interpretation.
Then, these ap-proaches acquire huge and fine grained knowledge,or parameters in statistic terms from the corpus au-tomatically.
The uncertainty problem in linguisticphenomena is resolved on a more solid basis if aprobabilistic approach is adopted.
Moreover, theknowledge acquired by the statistical method is al-ways consistent because the knowledge is acquiredby jointly considering all the data in the corpus atthe same time.
Hence, the time for knowledge ac-quisition and the cost to maintain consistency aresignificantly reduced by adopting those probabilis-tic approaches.To resolve the problems resulting from syntac-tic ambiguities, a unified statistical pproach for am-biguity resolution has been proposed by Su \[Su 88,92b\].
In that approach, all knowledge sources, in-cluding lexical, syntactic and semantic knowledge,are encoded by a unifiedprobabilistic s ore functionwith a uniform formulation.
This uniform proba-bilistic score function has been successfully appliedin spoken language processing \[Su 90b, 91b, 92a\]and machine translation systems \[Chen 91\] to in-tegrate different knowledge sources for ambiguityresolution.In implementing this unified probabilistic scorefunction, values of score functions are estimatedfrom the data in the training corpus.
However, dueto the problem of insufficiency of training data andincompleteness of model knowledge, the statisticalvariations between the training corpus and the realapplication are usually not covered by this approach.Therefore, the performance in the testing set some-times gets poor in the real application.To enhance the capability of discriminationand robustness of those proposed score function,a discrimination-oriented adaptive learning is pro-posed in this paper.
And then, the robustness ofthisproposed adaptive learning procedure is enhancedby enlarging the margin between the correct candi-date and its confusing candidates to achieve maxi-mum separation between different candidates.Since the implementation f this adaptive learn-ing procedure is based on the uniform probabilisticscore function, we will first briefly review the uni-fied probabilistic score function.
Readers who areACTES DE COLING-92, NANTas, 23-28 AO~r 1992 3 5 2 PROC.
OF COLING-92, NANTES, AuG, 23-28.
1992interested in the details about the uniform proba-bilistic score function please refer \[Chen 91, Su91b, 92a, 92b\].2.
Overview of Uniform ProbabilisticScore Function2.1, General DefinitionA Score Function for a given syntactic tree, saySynj,  is defined as follows:S~or~ (s~,,~)_-- v (%, , .
L~,  IG ' )  (\])where w~ ~ is the input word sequence, w~ ~ ={Wl,W2, '" ,w, ,} ,  and Lcx j ,  the correspond-ing lexical string, i.e., part of speech sequence{cjl , c j2, .
.
.
, cj,~ }.
By applying the multiplicationtheorem of probability, l '(Sun), Lexj I w?)
can be re-stated as follows.P (Syn:, l,e~j I G')The two components, ~,, (.
','yn D and s~,~ ( Lexj ), mthc above formula are called syntactic Score Func-tion and Lexieal Score Function, respectively.
Theoriginal score function, i.e., P(Sy%,Lexjlw~'), isthen called Integrated Score Function.Next, we assume the information, from theword .sequence w\] ~, required for syntactic ambigu-ity resolution, has percolated to the lexical inter-pretation Lez j .
Also, only little additional infor-mation can be provided from w\] ~ for the task ofdisambiguating syntactic interpretation ~yuj afterthe lexical interpretation Lex j  is given.
Thus, ritesyntactic score can be approximated as shown int~t.(3):.
'~.,~,, (Syn:)= l '(Sy% I Lex~,w'~ ') ~ I'(Sy% I Le%).
(3)The integrated score function P(Sy%, Lez~\]wi') isthen approximated as follows.P (Synj, Lex~ I w~') (4)-~ P(Sy% I Lez,) ?
P(Lex, I u,;').Such a formulation "allows us to use both lexicaland syntactic knowledge in assigning preferencemeasure m a syntactic tree.
in the real computation,log operation is used to convert the operations ofmultiplication to the operations of addition.
Thefollowing equation shows the final form in the realapplication.log P ( Syrtj, Lex~ I w\[') = log S,u, ( Sy% )+log S~,, (Lex~ ) .(5)2.2.
Lexical Score F'unctionLet ck~, denote the k-th sequence of tile lexicalcategory, or part of speech, co~xesl)onding to theword sequence w~L The Lexical Score Function canbe expressed as follows \[Chen 91, Su 92b\]:= f i  e (~,  I~ki ...... ;'), (6)i= lwhere ck~ is rite lexic',d category of w i. Sev-eral forms lGars 87, Chur 88, Su 92b\] tor1' (~k, I~q .
.
.
.
.
p) were propo~d to simplify thecomputanon.
For example, IChur 88\] approximated"(~k IG .
.
.
.
.
.
; ' )by \[P(~k, I%,-,\] ?
t'(ck, 1~,)\] , Ageneral nonlinear smoothing l'onn \[Chen 91\] de-scribed in Eq.
(7) is adopted in this paper:u (P (~, I ~ .
.
.
.
.
.
,)) (7)~ Ag(P(, :k.  \ ]w, ) )+( l  .~)9 (Ck, \[ .
.
.
.
.
),where A is the lexical weight (A = 0.6 is used in thecurrent setup), and 9 is a transform function (log (.
)is used in this paper).
Hence, given both Eq.
(6) and(7), the lollowing tormula is derived:log (St~ ( Lr, z~,))n= ~ {Atog P(~,l,,,) + (~ -~)  o~?
(~ I .
.
.
.
.
)}J= l(8)It is noted that the above generalized form reducedto the formulation of \[Chur 88\] when file transtonnfunction is log function and A is 0.5.2.3.
Syntactic Score FunctionTo show the computing mechanism for the syn-tactic score, we take the syntax tree in Fig.1 as anexample.
The syntax tree is decomposed into anumber of phrase levels.
Each phrase level (alsocalled a sententialform) consists of a set of sym-bols (terminal or nontenninal) which can derive allthe temfin',d symbols in a sentence.
Let label t i inFig.l be the time index lot each state transition of aLR parser, and Li be the i-fit phrase level.
Thus, atransition from phrase level L i to phrase level Li+ 1is equivalent to a redue action at time ti.A ACIIONO C l.Tffi {B  C ..... A ............. $ ""~ I .
-  {II I ' .
O \ ]  ...... ....................b I~ F G IA .
\ [B  C3.
C41 ...... 11 ............ CA ....I I I  I I1 17" 14 ~ 1\['21 II~ C2' C3' C4I ..... 1\[~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.C I C 2 C~ 124 L I~ 1121.
Ca, C~, C4I  ..... D '  ........... C2""  ... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
C1 ....Figure I The dr, composition of a syntax treehito phrase levels.ACfES DE COLING-92, NANTF.S, 23-28 AOtTr 1992 3 5 3 Pgoc.
OF COLING-92, NANri~s, AUG. 23-28, 1992The syntactic score of the syntax tree in Fig.lis then defined asS,y, (svna) =- P(Ls,Lr, ... ,L, I L~)8 8: ( ) I I e  L, I L~-'i=~ i=2(9)where syn A is the parse tree, and LI through L 8represent different phrase levels.
Note that the prod-uct terms in the last formula correspond to the right-most derivation sequence in a general LR parser \[Su91c\], with left and right contexts taken into account.Therefore, such a formulation is especially usefulfor a generalized LR parsing algorithm, in whichcontext-sensitive processing power is desirable.Although the context-sensitive model in theabove equation provides the ability to deal withintra-level context-sensitivity, it falls to catch inter-level correlation.
In addition, the formulation ofEq.
(9) gives rise to the normalization problem forambiguous yntax trees with different number ofnodes.
An alternative to relieve this problem isto compact multiple highly correlated phrase levelsinto one in evaluating the syntactic scores.
Tileformulation is expressed as follows \[Su 91c\]:S,v, (sv,,a)Word Category(part of speech)1 pron (pronoun)n (noun)vi (intransitive rb) saw vt (txansitive rb)art (article)a prep (preposition)man n (noun)log P(elw)-0.22-0.39-0.52-0.16-0.02-1.300Table 1 Categories for words and their logword-to-category scores.In Table 1, the log word-to-category score,log (P  (e \] to)), for each word is estimated from thetraining corpus by calculating their relative frequen-cies.
For exanrple, in the training corpus, the word'T '  is used as pronoun for 60 times, and 40 timesas noun.
Then, the log word-to-category scores canbe calculated as follows.60 lOglo l" (prou l {I} ) = loglo ( 6-'~--~-~ ) = --0.22, (,o) IOg,o P(u I (1}) = loan ~ = -0 .39 .
(11)In this example, there are 2"2"2"1=8 possible dif-ferent ways to assign lexical categories to the inputP(L~,LT,L~ \[ L~) ?
P(Lr~ I L4) x P(L4,L~ I L~) ?
P(L7 I LQsentence.
When these 8 possible lexical ,sequencesP(La I Lr,) ?
P(Ln I L4) ?
P(L4 I L~) ?
I' (L~, I Lt)(10)Because thc number of shifts, i.e., the number ofterms in Eq.
(lO), is always the same for all am-biguous syntax trees, the normalization problemis then resolved.
Moreover.
it provides a wayto consider both intra-level context-sensitivity andinter-level correlation of the underlying context-freegrammar.
With such a score function, the capabil-ity of context-sensitive parsing (in probability sense)can be achieved with a context-free grammar.3.
Discrimination and Robustness OrientedAdaptive Learning3.1.
Concepts of Adaptive LearningThe general idea of adaptive learning is to ad-just the model parameters (in this paper, they arelexical scores and syntactic scores) to achieve thedesired criterion (in our case, it is to minimize theerror rate).
To explain clearly how the adaptivelearning works, we take the sentence "1 saw a man.
"as an example.
The lexical category (i.e,, part ofspeech) and its corresponding log score for eachword are listed in Table 1.are parsed, only four of them are accepted by ourparser.
They are listed as follows:1. pron vt art n2.
n vt art n3.
pron vi prep n4.
n vi prep n.The syntactic scores of different parse trees arethen calculated according to Eq.(10).
A parse treecorresponding to the lexical sequence "\[pron vt artn\]" is drawn below as an example.sNP VPpron v NPUlt  nThe log syntactic scores for those four grammaticalinputs are computed and listed in Table 2.AcrEs DE COLING-92, NANTES, 23-28 Aot3"r 1992 3 5 4 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992Input Lexlcal Sequence log syntactic sco~e\[pron vt art n\] (-0.7)+(41.3)+(-0.3)+(-0.2) = -I.5\[, vt art n\] (41.2)+(-0.3)+(-0.3)+(-0.2) = -1.0\[prot, vi prep n\] (-0.7)+(-0.7)+(-0.4)+(-0.3) = 2 1\[n vi prep ,1 (-0.2) +(-0.7)+(-0.4)+(-0.3) = -1.6"fable 2 log syntactic scores of the grammaticalInput lexlcal sequences.Accord ing to Eq.
(5), the total log integratedscore ( log Slez+logSsvn)  for each parsed sentencehypothesis is calculated.
For example,  the log lexi-cal score for "I / \[pron\] saw/\[vt\] a/\[art\] man/ In \ ] "  = (-0.22-0.16-0.02-0)= -0.4.
Finally, the log integratedscores for the above grammat ica l  inputs are listedas follows:candidate-1, log integrated score = (-0.40-1.5 = -1.90) :l/Ipron\] saw/\[vt\] a/\[art\] man/In\]candidate -2. logintegrated score = ( 0.57-1.0 = -1.57) : l/in\]saw/\[vt\] a/\[art\] maa/\]nlcandidate-3, log integrated score = (-2.04-2.1 = 4.71) :I/\[pron\] saw/lvi\] a/\[prep\] man/\[n\]candidate -4. log integrated score = (-2.21-1.6 = -3.81) : l/\[n\]saw/\[vii a/\[prepl man/\[nlAmong these four candidates, the candidate 1 isregarded as the desired selection by linguists.
Sinceour decision criterion will select the candidate whichhas the highest integrated score, i.e., the second one;I/\[n\] ~w/ \ [v t \ ]  a/\[art\] man/\[n\] ,  it results in a decisionerror in this case.To remedy this error, adaptive learning proce-dure is adopted to adjust the score values iteratively,including lexical and syntactic scores, until the inte-grated score o f  the correct candidate (i.e., candidate1) raises to the highest rank.
In this paper, parame-ters which are adjusted by adaptive learning proce-dure are those log scores, including log P (c/q I wi),l ogP(ek , \ ]ck , - i  ~ and logP(L i l L~- l ) .
Theamount  of  adjutstment in each iteration depends onthe misclassihcation distance.
Misclassification dis-tance is defined as the difference between the scoreo f  the top candidate and that o f  the correct one.
(In the above example,  distance = (score o f  cor-rect candidate)-(score o f top candidate) = (-1.90)-(-1.57)= -0.33).
From iteration to iteration, theparameters (both lexical and syntactic scores) areadjusted so that the integrated score of  the correctcandidate is increased, and the integrated score ofthe wrong candidate is decreased at the same time.The learning procedure for a sentence is stoppedwhen the candidate o f  this sentence is correctly se-lected.
To make the explanation of  this adaptivelearning procedure clear, we assume lexical scoresare unchanged during learning.
That  is, only theparanteters of  the syntactic scores are adjusted.
Thedetails o f  adaptive leanl ing for adjusting syntacticscores are listed as follows:hdtlal 'lzutioncandidate -1. zx syntactic score = \[-0.7 -0.3 -0.3 -0.2\] = -1.5.log integrated score = -1.9;candidate .2. "
syntactic score = \[-0.2 -0.3 -0.3 -0.2\] = -1.0,log integrated score = -1.57;candidate -3. syntactic score = \[-0.7 -0.7 -0.4 -0.3\] = -2.1.log integrated score = .4.71;catudidate -4. syntactic score = \[-0.2 -0.7 -0.4 -0.3\] = -1.6.log integrated score = -3.81;Iteration Icandidate -1.
~ syntactic score = \[-0.5 .0.3 -0.3 -0.2\] = -1.3,log integrated score = -1.7;candidate -2. "
syntactic score = \[-0.3 -0.3 -0.3 .0.2\] = -1.1,log integrated score = -1.67;candidate -3. syntactic score = \[-0.5 -0.7 -0.4 ~0.3\] =-1.9,log integrated score = -3.94;candidate -4. syntactic score = \[-0.3 -0.7 -0.4 -0.3\] = -1.7,1o 8 integrated score = -3.91;Iteration 2candidate -1. e," syntactic score = \[-0.2 -0.3 -0.3 -0.2\] = -1.0,log integrated score = -1.4;(stop learning)candidate -2. syntactic score = \[-0.6 -0.3 -0.3 -0.2\] = -1.4,log integrated score = -1.97;candidate -3. syntactic score = \[-0.2 -0.7 -0.4 -0.31 = -1.6,log integrated score = -3.64;candidate 5. syntactic score = \[41.6 -0.7 -0.4 -0.3\] = 2.0,log integrated score = -4.21;(where * denotes the top candidate, and A denotes the desiredcandidate)It is clear that after the second iteration, paranaetershave been adjusted so that the desired candidate(i.e., candidate 1) would be selected.3.2.
P rocedure  o f  D isc r iminat ion  Learn ingSince correct decision only depends upon cor-rect rank ordering of  the integrated scores for allambiguit ies, not their real value, a discrimination-oriented approach should directly pursue correctrank ordering.
To der ive the discr imination func-tion, the probabil ity scores ment ioned above are firstjointly considered.
Then,  a discrimination-orientedfunction, namely g (.
), is defined as a measurementof  above mentioned score functions, so that it canwell p re~rve  the correct rank ordering \[Su 91a\].Here, 9 ( ' )  is chosen as the weighted sum of  logACRES DE COLING-92, NANTES, 23-28 aofrr 1992 3 5 5 PROC.
OV COLING-92, NANTES.
AUO.
23-28, 1992lexical and log syntactic scores, i.e.,a (sv,,k)= wl~,  .
log S l ,x  (Le~:k) + W,vn ' log S .~, .
(Sun~:)u. .
.
.
.
.....
:.
)+ .
.
.
.
.i= ln n(12)where .h,, (i) = logP (ckA%; .
.
.
.
1'), and A,v,, (i) =logP(LdLi- :) .
Both stand for the log lexical scoreand the log syntactic score of the i-th word for thek-th syntactic ambiguity, respectively.
In addition,Wle z and Wsyn correspond to the weights of lexicaland syntactic scores, respectively.If the parse tree of a sentence is misselected,the parameters (i.e., the lexical and the syntacticscores) are adjusted via the proposed adaptive learn-ing procedure.
Otherwise, no parameters would beadjusted.
When misselection occurs, the misclassi-fication distance, dso is less than zero.
This mis-classification distance is defined as the differencebetween the log integrated .score of the correct can-didate and that of the top one.
A specific term ofthe syntactic score components in the (t+l)-th itera-A(t+l) lion of the correct candidate, say sv,* (j), wouldbe adjusted as follows:{ ~('+'~ ' ~ - L~).
(i) + ~(.
;),, (j) a,.
< o, ,v- t3)- ' - (13)At the same time, the term of the syntactic scorecomponents of the top candidate would be adjustedaccording to the following formulas:fa(.~+,,')(i) x(') "" ~a(') = ,~s) -  ,~,,(j), a,~<o,  (I,1)where AA~ n ( j ) is  the amount of adjustment.
Thisvalue is represented aszx~i~).
O) = "d  .
.
.
.
.
.
.I=\] " leX t l J  + iOst/n "(15)where do is a constant which stands for a windowsize, and e is the learning constant for controllingthe speed of convergence.
The learning rule foradjusting the lexical scores can be represented in asimilar manner.
Notice that only the parameters ofthe top candidate and those of the correct candidatewould be adjusted when misselections occur.
Thoseparameters of other wrong candidates would not beadjusted in this adaptive leaming procedure.
FromEq.
(13), (14) and (15), it is clear that the score ofthe correct candidate will increase and that of wrongcandidate will decrease from iteration to iterationuntil the correct candidate is selected.
For thepurpose of clarity, the detailed derivations of theabove adaptive learning procedure will not be givenhem.
Interested readers can contact he authors fordetails.3.3.
Robustness I suesSince it is easy to improve the performance in atraining set by adopting amodel with a large numberof parameters, the error rate measured in the train-ing set frequently turns out to be over-optimistic.Moreover, the parameters e tinlated from the train-ing corpus may be quite differ from that obtainedfrom the real applications.
These phenomena mayoccur due to the factors of finite sampling size, stylemismatch, or domain mismatch, etc.
To achieve abetter perlbrmance in the real application, one mustdeal with the possible mismatch of parameters, orstatistical variation between the training corpus andthe real application.
One way to achieve this goal isto enlarge the inter-class distance to achieve maxi-mum separation \[Su 91a\] between the cot:ect can-didate and the other candidates.
That is, this ap-proach provides a tolerance zone between differentcandidates for allowing possible data scattering inthe real application.Traditional adaptive learning methods \[Amar67, Kata 90\] stop adjusting parameters once the in-put pattern has been correctly classified.
However,if we stop adjusting parameters under the condi-tion that the observations are correctly classified inthe training corpus, the distance between the cor-rect candidate and other ambiguities may still betoo small.
Thus, it is vulnerable to deal with pos-sible modeling errors and statistical variations be-tween the training corpus and the real application.Su \[Su 91a\] has proposed a robust learning proce-dure which continues to enlarge the margin betweenthe correct candidate and the top one, even if thesyntax tree of the sentence has been correctly se-leeted.
That is, the parameters will not be adjustedonly if the distance between the correct candidateand the others has exceeded a given threshold.
Thelearning rules in Eq.
(13), (14) are then modified asfollows.If dsc _< 6, where 6 is a preset margin, the syntacticscore in the (t+l) iteration tor the correct candidateis adjusted according to the following formulas:( ,~( t+ l )  l l )  (t)~,~, I : )  = .t,~., OI + A; , .~.
O),  a,, < ~, (16)...... O) ,x~,'~),, 0) ,  ot/,~T,o,~.And, the syntactic score of the top candidate isadjusted as follows:~L~,t, '~Ol -~"~ O l - ,ax~, ;LO)  a,~<~, - -,v .
.
.
.
(17) ) .
{t+l),., - -  1(t) La'~" t31-  ",~, O), otherwtse.ACa-ES DE COLING-92, NANTES, 23-28 xot:rr 1992 3 5 6 I'ROC.
OF COLING-92, NANTES, AUG. 23-28, 19924.
SimulationsThe following experiments are conducted to ili-vestigate the advmltage of the proposed iscrimina-tion and robustness oriented adaptive leanfing pro_cedure.
In the experiments, 4,000 sentences, whichare extracted from IBM tectmical manuals are firstassociated with their conesponding correct categorysequences and correct parsed trees by linguists.
Thecorpus are then partitioned into a training corpusof 3,2(X) sentences and a test set of 800 sentences.Next, the lexical and syntactic probabilities are es-timated from the data in the training corpus.
Af-terwards, the sentences in the test set a~e used toevaluate rite perlomtance of the proposed algorithmusing the estimated lexical and syntactic probabili-ties.
This integrated score timction apploach usingthe estimated probabilities is considelcd as the base-line system.
Performances of discrimination ufi~ented adaptive learnings with and widtout robust-ness enhancement are then evaluated.
The accuracyrate of the syntactic ambiguity resolution for thetraining corpus and rile test set are summarized inTable 3.
(Note that the top candidate is selectedt?om all po~ible parses allowed by tile gmntmarsof the system; therefore, the baseline perlonnance isevaluated under a highly ambiguous environment.
)I laseline+ Ila.sie version oflearning+ Robust  version ofLearning'able 3 Accuracy rate (InTraining I Test SetCorDus I79.75 I 46.
(X)95.50 I 56.8896.03 I 60.62%) of syntactic disanl|flguatlunTable 3 shows that syntax tree accuracy ~ateis improved from 46% to 56.88% using the basicversion of discrimination oriented adaptive learningprocedure.
This significant iiuprovement shows thesuperiority of the adaptive learning procedure fordealing with the disambiguation task.
l:unhenuore,when the rt)bust versiou of leanfiug proccdure isadopted, the perfommnce is iml)roved further (flora56.88% to 60.62%).
It means that the rohuslnessof the learning procedure is indeed enhanced by en-larging the distance between the correct candidateand other candidates.
Moreover, not only the ac-curacy rate of syntax trec is improved using adap-tive learning, but al~ that of lexical sequence isimproved.
In this paper, a lexical sequence is ~e-garded as "correct" only if all the lexical categoriesin a sentence perfectly match those selected by lin-guists.
In other words, we are measuring "sentenceaccurac3: rate" in contract tu "word accuracy rate"as adopled ill \[Chur 88, Gal~ 87\].
Table 4 showsthat file basic version of adaptive leaming proce-dule ilnproves the sentence accuracy rate of lexi-col sequences a|xmt 5% (from 77.12% to 82.38%).Again, with the robust version of learning, the ac-curate rate of lexical sequences i  greatly enbanced.I TrahdngCorpu~Baseline 91.41 ~ _ ~+ It asit i version of learn big 98.91 8~_2.3~\]+ ltohust version of 98.53 1Learning L _ _Table 4 ~'-;entencc accuracy rate (in %) of lexlcal sequencesThe behavior of cach iteration of the adaptivelearning process is shown in Figure 2.
Through ob-serving this figure, we can conclude that if the ro-bustness i sues are not considered uring learning,the performance oftile test set would decrease as thetraining t)~ocess goes on.
This is the phenomena ofriver-tuning, llowever, by h~rcing the learning pro-cedure to continue unlil the separation between thecorrect candidate and the top one exceeds the de-sired margin, the performmlce of the test set can bet'utther improved, and no degradation phenomenonis observed.I ............. = ; : :  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.l:lgurc 2 Syntax t~e accuracy rate versus Iteratkms forbasic and r~lbusl verslou of adaptive l arningAcrEs DE COLING-92, NAbrrl,:s, 23 28 Ao(rr 1992 3 5 7 l'lto?
:, oi: C()l,IN(;-92, N^yn~s, AUG. 23-28, 19925.
SummaryBecause of insufficient raining data, and ap- \[Su 88\]proximation error introduced by the language model,traditional statistical approaches, which resolve am-biguities by indirectly and implicitly using maxi-mum likelihood method, fail to achieve high per-formance in real applications.
To overcome these \[Su 90a\]problems, adaptive learning is proposed to pursuethe goal of minimizing discrimination error directly.The performance of syntactic ambiguity resolutionis significantly improved using the discrimination \[Su 90b\]oriented analysis.
In addition, the sentence ac-curacy rate of the lexical sequences i also im-proved.
Moreover, the performance is further en-hanced by using the robust version of learning pro-cedure, which enlargeds the margin between thecorrect candidate and its candidates.
The final re-suits show that using the basic version of learning, \[Su 91b\]the syntax tree selection accuracy rate is improvedabout 10% (from 46% to 56.88%), and the total im-provement is over 14% using robust version learn-ing.
Also, the sentence accuracy rate for lexicalsequences i improved from 77.12% to 82.38 and87.88% using the basic and robust version of leam-ing procedure, respectively.Reference\[Amar 67\]\[Chen 91\]\[Chur 88\]\[Gars 87\]\[Kata 90\]\[Su 91atAmari S., "A theory of adaptive patternclassifiers," IEEE Trans.
on ElectronicComputers, Vol.
EC-16, pp.
299-307, \[Su 91c\]June 1967.Chen S.-C., J.-S. Chang, J.-N. Wang,and K.-Y.
Su, "ArchTran: A Corpus-based Statistics-oriented English-ChineseMachine Translation System," Proc.
of \[Su 92a\]Machine Translation Summit II1.
Washing-ton, D. C., U.S.A., July 1-3, 1991.Church, K., "A Stochastic Parts Programand Noun Phrase for Unrestricted Text,"ACL Proceedings of 2nd Conference onApplied Natural Language Processing,pp.136-143, Austin, Texas, U.S.A., 9-12Feb.
1988.Garside, R., G., Leech, and G., Samp- \[Su 92b\]son, "The Computational Analysis of Eng-lish: A Corpus-Based Approach," Lon-don: Longman.S.Katagiri.
C.H.
Lee, "A General-ized Probabilistic Decent Method," Proc.Acous.
Sco.
of Japan, 2-p-6, pp.141-142, Nagoya, Sept 1990.Su K.-Y., and J.-S. Chang, "Semanticand Syntactic Aspects of Score Function,"Proc.
COLING-88, Vol.2, pp.
642-644,12th Int.
Conf.
on Comput.
Linguistic,Budapest, Hurgay, 22-27, Aug. 1988.Su, K.-Y., and J.-S Chang, 1990.
"SomeKey Issues in Designing MT Systems,"Machi'ne Translation, vol.
5, no.
4, pp.265-300, 1990.Su K.-Y., T.-H. Chlang and Y.-C. Lin, "AUnified Probabilistic Score Function forIntegrating Speech and Language Infor-mation in Spoken Language Processing,"Proceeding of 1990 International Con-ference on Spoken Language Processing,pp.901-904, Kobe, Japan, 19-22 Nov.1990.Keh-Yih Su, Tung-Hui Chiang and Yi-Chung Lin, "A Robustness and Discrim-ination Orientexl Score Function for Inte-grating Speech and Language Processing,"Proceeding of the 2rid European Confer-ence on Speech Communication a d Tech-nology, Genova, Italy, pp.
207-210, Sep.24-26 1991.Su K.-Y., and C.-H. Lee, "Robustness andDiscrimination Oriented Speech Recogni-tion Using Weighted HMM and SubspaceProjection Approaches," Proc.
ICASSP-91, pp.541-544, Toranto, Canada, 14-17May, 1991.Su K.-Y., J.-N. Wang, M.-H. Su, and J.-S. Chang, "GLR Parsing with Scoring,"in Tomita, Masaru (ed.
), Generalized LRParsing, Chapter 7, pp.93-112, KluwerAcademic Publisher 1991..Keh-Yih Su, Tung-Hui Chiang and Yi-Chung Lin, "An Unified Framework toIn-corporate Speech and Language Informa-tion in Spoken Language Processing" toappear in the Proceeding of IEEE Inter-national Conference on Acoustics, Speechand Signal Processing, ICASSP-92, SanFrancisco, California, U.S.A., March 23-26 1992.Sn K.-Y., J.-S. Chang and Y.-C. Lin, "AUnified Approach to Disambiguation Us-ing A Uniform Formulation ofProhabilis-tic Score Function," in preparation.ACRES DE COLING-92, NANTES, 23-28 AOUT 1992 3 5 8 Prtoc.
OF COLING-92, NANTES, AUG. 23-28, 1992
