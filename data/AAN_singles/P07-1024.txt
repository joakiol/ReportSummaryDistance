Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 184?191,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsOptimizing Grammars for Minimum Dependency LengthDaniel GildeaComputer Science Dept.University of RochesterRochester, NY 14627David TemperleyEastman School of MusicUniversity of RochesterRochester, NY 14604AbstractWe examine the problem of choosing wordorder for a set of dependency trees so asto minimize total dependency length.
Wepresent an algorithm for computing the op-timal layout of a single tree as well as anumerical method for optimizing a gram-mar of orderings over a set of dependencytypes.
A grammar generated by minimizingdependency length in unordered trees fromthe Penn Treebank is found to agree surpris-ingly well with English word order, suggest-ing that dependency length minimization hasinfluenced the evolution of English.1 IntroductionDependency approaches to language assume that ev-ery word in a sentence is the dependent of one otherword (except for one word, which is the global headof the sentence), so that the words of a sentence forman acyclic directed graph.
An important principle oflanguage, supported by a wide range of evidence, isthat there is preference for dependencies to be short.This has been offered as an explanation for numer-ous psycholinguistic phenomena, such as the greaterprocessing difficulty of object relative clauses ver-sus subject relative clauses (Gibson, 1998).
Depen-dency length minimization is also a factor in ambi-guity resolution: listeners prefer the interpretationwith shorter dependencies.
Statistical parsers makeuse of features that capture dependency length (e.g.an adjacency feature in Collins (1999), more explicitlength features in McDonald et al (2005) and Eisnerand Smith (2005)) and thus learn to favor parses withshorter dependencies.In this paper we attempt to measure the extent towhich basic English word order chooses to minimizedependency length, as compared to average depen-dency lengths under other possible grammars.
Wefirst present a linear-time algorithm for finding theordering of a single dependency tree with shortesttotal dependency length.
Then, given that word or-der must also be determined by grammatical rela-tions, we turn to the problem of specifying a gram-mar in terms of constraints over such relations.
Wewish to find the set of ordering constraints on depen-dency types that minimizes a corpus?s total depen-dency length.
Even assuming that dependency treesmust be projective, this problem is NP-complete,1but we find that numerical optimization techniqueswork well in practice.
We reorder unordered depen-dency trees extracted from corpora and compare theresults to English in terms of both the resulting de-pendency length and the strings that are produced.The optimized order constraints show a high degreeof similarity to English, suggesting that dependencylength minimization has influenced the word orderchoices of basic English grammar.2 The Dependency Length PrincipleThis idea that dependency length minimization maybe a general principle in language has been dis-cussed by many authors.
One example concerns the1English has crossing (non-projective) dependencies, butthey are believed to be very infrequent.
McDonald et al (2005)report that even in Czech, commonly viewed as a non-projectivelanguage, fewer than 2% of dependencies violate the projectiv-ity constraint.184well-known principle that languages tend to be pre-dominantly ?head-first?
(in which the head of eachdependency is on the left) or ?head-last?
(where itis on the right).
Frazier (1985) suggests that thismight serve the function of keeping heads and de-pendents close together.
In a situation where eachword has exactly one dependent, it can be seen thata ?head-first?
arrangement achieves minimal depen-dency length, as each link has a length of one.We will call a head-first dependency ?right-branching?
and a head-last dependency ?left-branching?
; a language in which most or all de-pendencies have the same branching direction is a?same-branching?
language.Another example of dependency length mini-mization concerns situations where a head has mul-tiple dependents.
In such cases, dependency lengthwill be minimized if the shorter dependent is placedcloser to the head.
Hawkins (1994) has shown thatthis principle is reflected in grammatical rules acrossmany languages.
It is also reflected in situations ofchoice; for example, in cases where a verb is fol-lowed by a prepositional phrase and a direct objectNP, the direct object NP will usually be placed first(closer to the verb) but if it is longer than the PP, itis often placed second.While one might suppose that a ?same-branching?
language is optimal for dependency-length minimization, this is not in fact the case.
Ifa word has several dependents, placing them allon the same side causes them to get in the way ofeach other, so that a more ?balanced?
configuration?
with some dependents on each side ?
has lowertotal dependency length.
It is particularly desirablefor one or more one-word dependent phrases to be?opposite-branching?
(in relation to the prevailingbranching direction of the language); opposite-branching of a long phrase tends to cause a longdependency from the head of the phrase to theexternal head.Exactly this pattern has been observed by Dryer(1992) in natural languages.
Dryer argues that,while most languages have a predominant branch-ing direction, phrasal (multi-word) dependents tendto adhere to this prevailing direction much moreconsistently than one-word dependents, which fre-quently branch opposite to the prevailing directionof the language.
English reflects this pattern quite?w0 w1 w2 w3 w4 w5 w6 w7 w8Figure 1: Separating a dependency link into twopieces at a subtree boundary.strongly: While almost all phrasal dependents areright-branching (prepositional phrases, objects ofprepositions and verbs, relative clauses, etc.
), some1-word categories are left-branching, notably deter-miners, noun modifiers, adverbs (sometimes), andattributive adjectives.This linguistic evidence strongly suggests thatlanguages have been shaped by principles of de-pendency length minimization.
One might won-der how close natural languages are to being op-timal in this regard.
To address this question, weextract unordered dependency graphs from Englishand consider different algorithms, which we call De-pendency Linearization Algorithms (DLAs), for or-dering the words; our goal is to find the algorithmthat is optimal with regard to dependency lengthminimization.
We begin with an ?unlabeled?
DLA,which simply minimizes dependency length withoutrequiring consistent ordering of syntactic relations.We then consider the more realistic case of a ?la-beled?
DLA, which is required to have syntacticallyconsistent ordering.Once we find the optimal DLA, two questions canbe asked.
First, how close is dependency length inEnglish to that of this optimal DLA?
Secondly, howsimilar is the optimal DLA to English in terms of theactual rules that arise?3 The Optimal Unlabeled DLAFinding linear arrangements of graphs that minimizetotal edge length is a classic problem, NP-completefor general graphs but with an O(n1.6) algorithm fortrees (Chung, 1984).
However, the traditional prob-lem description does not take into account the pro-jectivity constraint of dependency grammar.
Thisconstraint simplifies the problem; in this section weshow that a simple linear-time algorithm is guaran-teed to find an optimal result.A natural strategy would be to apply dynamic pro-gramming over the tree structure, observing that to-185tal dependency length of a linearization can be bro-ken into the sum of links below any node w in thetree, and the sum of links outside the node, by whichwe mean all links not connected to dependents of thenode.
These two quantities interact only through theposition of w relative to the rest of its descendants,meaning that we can use this position as our dy-namic programming state, compute the optimal lay-out of each subtree given each position of the headwithin the subtree, and combine subtrees bottom-upto compute the optimal linearization for the entiresentence.This can be further improved by observing thatthe total length of the outside links depends on theposition of w only because it affects the length ofthe link connecting w to its parent.
All other outsidelinks either cross above all words under w, and de-pend only on the total size of w?s subtree, or are en-tirely on one side of w?s subtree.
The link from w toits parent is divided into two pieces, whose lengthsadd up to the total length of the link, by slicing thelink where it crosses the boundary from w?s subtreeto the rest of the sentence.
In the example in Fig-ure 1, the dependency from w1 to w6 has total lengthfive, and is divided in to two components of length2.5 at the boundary of w1?s subtree.
The length ofthe piece over w?s subtree depends on w?s positionwithin that subtree, while the other piece does notdepend on the internal layout of w?s subtree.
Thusthe total dependency length for the entire sentencecan be divided into:1. the length of all links within w?s subtree plusthe length of the first piece of w?s link to itsparent, i.e.
the piece that is above descendantsof w.2.
the length of the remaining piece of w?s link toits parent plus the length of all links outside w.where the second quantity can be optimized in-dependently of the internal layout of w?s subtree.While the link from w to its parent may point eitherto the right or left, the optimal layout for w?s subtreegiven that w attaches to its left must be the mirrorimage of the optimal layout given that w attaches toits right.
Thus, only one case need be considered,and the optimal layout for the entire sentence canbe computed from the bottom up using just one dy-namic programming state for each node in the tree.We now go on to show that, in computing the or-dering of the di children of a given node, not all di!possibilities need be considered.
In fact, one cansimply order the children by adding them in increas-ing order of size, going from the head outwards,and alternating between adding to the left and rightedges of the constituent.The first part of this proof is the observation that,as we progress from the head outward, to either theleft or the right, the head?s child subtrees must beplaced in increasing order of size.
If any two ad-jacent children appear with the smaller one furtherfrom the head, we can swap the positions of thesetwo children, reducing the total dependency lengthof the tree.
No links crossing over the two chil-dren will change in length, and no links within ei-ther child will change.
Thus only the length of thelinks from the two children will change, and as thelink connecting the outside child now crosses over ashorter intermediate constituent, the total length willdecrease.Next, we show that the two longest children mustappear on opposite sides of the head in the optimallinearization.
To see this, consider the case whereboth child i (the longest child) and child i ?
1 (thesecond longest child) appear on the same side of thehead.
From the previous result, we know that i ?
1and i must be the outermost children on their side.If there are no children on the other side of the head,the tree can be improved by moving either i or i ?1 to the other side.
If there is a child on the otherside of the head, it must be smaller than both i andi?
1, and the tree can be improved by swapping theposition of the child from the other side and childi?
1.Given that the two largest children are outermostand on opposite sides of the head, we observe thatthe sum of the two links connecting these childrento the head does not depend on the arrangement ofthe first i ?
2 children.
Any rearrangement that de-creases the length of the link to the left of the headmust increase the length of the link to the right ofthe head by the same amount.
Thus, the optimal lay-out of all i children can be found by placing the twolargest children outermost and on opposite sides, thenext two largest children next outermost and on op-186Figure 2: Placing dependents on alternating sidesfrom inside out in order of increasing length.posite sides, and so on until only one or zero chil-dren are left.
If there are an odd number of children,the side of the final (smallest) child makes no differ-ence, because the other children are evenly balancedon the two sides so the last child will have the samedependency-lengthening effect whichever side it ison.Our pairwise approach implies that there aremany optimal linearizations, 2?i/2?
in fact, but onesimple and optimal approach is to alternate sides asin Figure 2, putting the smallest child next to thehead, the next smallest next to the head on the op-posite side, the next outside the first on the first side,and so on.So far we have not considered the piece of the linkfrom the head to its parent that is over the head?ssubtree.
The argument above can be generalized byconsidering this link as a special child, longer thanthe longest real child.
By making the special childthe longest child, we will be guaranteed that it willbe placed on the outside, as is necessary for a projec-tive tree.
As before, the special child and the longestreal child must be placed outermost and on oppo-site sides, the next two longest children immediatelywithin the first two, and so on.Using the algorithm from the previous section, itis possible to efficiently compute the optimal de-pendency length from English sentences.
We takesentences from the Wall Street Journal section ofthe Penn Treebank, extract the dependency trees us-ing the head-word rules of Collins (1999), considerthem to be unordered dependency trees, and lin-earize them to minimize dependency length.
Au-tomatically extracting dependencies from the Tree-bank can lead to some errors, in particular withcomplex compound nouns.
Fortunately, compoundnouns tend to occur at the leaves of the tree, and thehead rules are reliable for the vast majority of struc-tures.Results in Table 1 show that observed depen-dency lengths in English are between the minimumDLA LengthOptimal 33.7Random 76.1Observed 47.9Table 1: Dependency lengths for unlabeled DLAs.achievable given the unordered dependencies andthe length we would find given a random order-ing, and are much closer to the minimum.
This al-ready suggests that minimizing dependency lengthhas been a factor in the development of English.However, the optimal ?language?
to which Englishis being compared has little connection to linguis-tic reality.
Essentially, this model represents a freeword-order language: Head-modifier relations areoriented without regard to the grammatical relationbetween the two words.
In fact, however, word orderin English is relatively rigid, and a more realistic ex-periment would be to find the optimal algorithm thatreflects consistent syntactic word order rules.
Wecall this a ?labeled?
DLA, as opposed to the ?unla-beled?
DLA presented above.4 Labeled DLAsIn this section, we consider linearization algorithmsthat assume fixed word order for a given grammat-ical relation, but choose the order such as to mini-mize dependency length over a large number of sen-tences.
We represent grammatical relations simplyby using the syntactic categories of the highest con-stituent headed by (maximal projection of) the twowords in the dependency relation.
Due to sparsedata concerns, we removed all function tags such asTMP (temporal), LOC (locative), and CLR (closelyrelated) from the treebank.
We made an exceptionfor the SBJ (subject) tag, as we thought it importantto distinguish a verb?s subject and object for the pur-poses of choosing word order.
Looking at a head andits set of dependents, the complete ordering of all de-pendents can be modeled as a context-free grammarrule over a nonterminal alphabet of maximal projec-tion categories.
A fixed word-order language willhave only one rule for each set of nonterminals ap-pearing in the right-hand side.Searching over all such DLAs would be exponen-tially expensive, but a simple approximation of the187Dep.
len.
/DLA % correct orderrandom 76.1 / 40.5extracted from optimal 61.6 / 55.4weights from English 50.9 / 82.2optimized weights 42.5 / 64.9Table 2: Results for different methods of lineariz-ing unordered trees from section 0 of the Wall StreetJournal corpus.
Each result is given as average de-pendency length in words, followed by the percent-age of heads (with at least one dependent) having alldependents correctly ordered.optimal labeled DLA can found using the followingprocedure:1.
Compute the optimal layout of all sentences inthe corpus using the unlabeled DLA.2.
For each combination of a head type and a setof child types, count the occurrences of eachordering.3.
Take the most frequent ordering for each set asthe order in the new DLA.In the first step we used the alternating procedurefrom the previous section, with a modification forthe fixed word-order scenario.
In order to makethe order of a subtree independent of the directionin which it attaches to its parent, dependents wereplaced in order of length on alternating sides of thehead from the inside out, always starting with theshortest dependent immediately to the left of thehead.Results in Table 2 (first two lines) show that aDLA using rules extracted from the optimal layoutmatches English significantly better than a randomDLA, indicating that dependency length can be usedas a general principle to predict word order.4.1 An Optimized Labeled DLAWhile the DLA presented above is a good deal bet-ter than random (in terms of minimizing dependencylength), there is no reason to suppose that it is opti-mal.
In this section we address the issue of findingthe optimal labeled DLA.If we model a DLA as a set of context-free gram-mar rules over dependency types, specifying a fixedordering for any set of dependency types attachingto a given head, the space of DLAs is enormous, andthe problem of finding the optimal DLA is a diffi-cult one.
One way to break the problem down isto model the DLA as a set of weights for each typeof dependency relation.
Under this model the wordorder is determined by placing all dependents of aword in order of increasing weight from left to right.This reduces the number of parameters of the modelto T , if there are T dependency types, from T k ifa word may have up to k dependents.
It also al-lows us to naturally capture statements such as ?anoun phrase consists of a determiner, then (possi-bly) some adjectives, the head noun, and then (pos-sibly) some prepositional phrases?, by, for example,setting the weight for NP?DT to -2, NP?JJ to -1, and NP?PP to 1.
We assume the head itselfhas a weight of zero, meaning negatively weighteddependents appear to the head?s left, and positivelyweighted dependents to the head?s right.4.1.1 A DLA Extracted from EnglishAs a test of whether this model is adequate torepresent English word order, we extracted weightsfor the Wall Street Journal corpus, used them to re-order the same set of sentences, and tested how oftenwords with at least one dependent were assigned thecorrect order.
We extracted the weights by assign-ing, for each dependency relation in the corpus, aninteger according to its position relative to the head,-1 for the first dependent to the left, -2 for the sec-ond to the left, and so on.
We averaged these num-bers across all occurrences of each dependency type.The dependency types consisted of the syntactic cat-egories of the maximal projections of the two wordsin the dependency relation.Reconstructing the word order of each sentencefrom this weighted DLA, we find that 82% of allwords with at least one dependent have all depen-dents ordered correctly (third line of Table 2).
Thisis significantly higher than the heuristic discussed inthe previous section, and probably as good as can beexpected from such a simple model, particularly inlight of the fact that there is some choice in the wordorder for most sentences (among adjuncts for exam-ple) and that this model does not take the lengths of188the individual constituents into account at all.We now wish to find the set of weights that min-imize the dependency length of the corpus.
Whilethe size of the search space is still too large to searchexhaustively, numerical optimization techniques canbe applied to find an approximate solution.4.1.2 NP-CompletenessThe problem of finding the optimum weightedDLA for a set of input trees can be shown to be NP-complete by reducing from the problem of finding agraph?s minimum Feedback Arc Set, one of the 21classic problems of Karp (1972).
The input to theFeedback Arc Set problem is a directed graph, forwhich we wish to find an ordering of vertices suchthat the smallest number of edges point from later toearlier vertices in the ordering.
Given an instance ofthis problem, we can create a set of dependency treessuch that each feedback arc in the original graphcauses total dependency length to increase by one,if we identify each dependency type with a vertexin the original problem, and choose weights for thedependency types according to the vertex order.24.1.3 Local SearchOur search procedure is to optimize one weight ata time, holding all others fixed, and iterating throughthe set of weights to be set.
The objective functiondescribing the total dependency length of the corpusis piecewise constant, as the dependency length willnot change until one weight crosses another, caus-ing two dependents to reverse order, at which pointthe total length will discontinuously jump.
Non-differentiability implies that methods based on gra-dient ascent will not apply.
This setting is reminis-cent of the problem of optimizing feature weightsfor reranking of candidate machine translation out-puts, and we employ an optimization technique sim-ilar to that used by Och (2003) for machine trans-lation.
Because the objective function only changesat points where one weight crosses another?s value,the set of segments of weight values with differentvalues of the objective function can be exhaustivelyenumerated.
In fact, the only significant points arethe values of other weights for dependency typeswhich occur in the corpus attached to the same head2We omit details due to space.Test DataTraining Data WSJ SwbdWSJ 42.5 / 64.9 12.5 / 63.6Swbd 43.9 / 59.8 12.2 / 58.7Table 3: Domain effects on dependency length min-imization: each result is formatted as in Table 2.as the dependency being optimized.
We build a ta-ble of interacting dependencies as a preprocessingstep on the data, and then when optimizing a weight,consider the sequence of values between consecu-tive interacting weights.
When computing the totalcorpus dependency length at a new weight value, wecan further speed up computation by reordering onlythose sentences in which a dependency type is used,by building an index of where dependency types oc-cur as another preprocessing step.This optimization process is not guaranteed tofind the global maximum (for this reason we callthe resulting DLA ?optimized?
rather than ?opti-mal?).
The procedure is guaranteed to converge sim-ply from the fact that there are a finite number ofobjective function values, and the objective functionmust increase at each step at which weights are ad-justed.We ran this optimization procedure on section 2through 21 of the Wall Street Journal portion of thePenn Treebank, initializing all weights to randomnumbers between zero and one.
This initializationmakes all phrases head-initial to begin with, and hasthe effect of imposing a directional bias on the re-sulting grammar.
When optimization converges, weobtain a set of weights which achieves an averagedependency length of 40.4 on the training data, and42.5 on held-out data from section 0 (fourth lineof Table 2).
While the procedure is unsupervisedwith respect to the English word order (other thanthe head-initial bias), it is supervised with respect todependency length minimization; for this reason wereport all subsequent results on held-out data.
Whilerandom initializations lead to an initial average de-pendency length varying from 60 to 73 with an aver-age of 66 over ten runs, all runs were within ?.5 ofone another upon convergence.
When the order ofwords?
dependents was compared to the real wordorder on held-out data, we find that 64.9% of words189Training Sents Dep.
len.
/ % correct order100 13.70 / 54.38500 12.81 / 57.751000 12.59 / 58.015000 12.34 / 55.3310000 12.27 / 55.9250000 12.17 / 58.73Table 4: Average dependency length and rule accu-racy as a function of training data size, on Switch-board data.with at least one dependent have the correct order.4.2 Domain VariationWritten and spoken language differ significantly intheir structure, and one of the most striking differ-ences is the much greater average sentence lengthof formal written language.
The Wall Street Journalis not representative of typical language use.
Lan-guage was not written until relatively recently in itsdevelopment, and the Wall Street Journal in particu-lar represents a formal style with much longer sen-tences than are used in conversational speech.
Thechange in the lengths of sentences and their con-stituents could make the optimized DLA in terms ofdependency length very different for the two genres.In order to test this effect, we performed exper-iments using both the Wall Street Journal (written)and Switchboard (conversational speech) portions ofthe Penn Treebank, and compared results with dif-ferent training and test data.
For Switchboard, weused the first 50,000 sentences of sections 2 and 3 asthe training data, and all of section 4 as the test data.We find relatively little difference in dependencylength as we vary training data between written andspoken English, as shown in Table 3.
For the ac-curacy of the resulting word order, however, train-ing on Wall Street Journal outperforms Switchboardeven when testing on Switchboard, perhaps becausethe longer sentences in WSJ provide more informa-tion for the optimization procedure to work with.4.3 Learning CurveHow many sentences are necessary to learn a goodset of dependency weights?
Table 4 shows resultsfor Switchboard as we increase the number of sen-tences provided as input to the weight optimizationprocedure.
While the average dependency length onLabel Interpretation WeightS?NP verb - object NP 0.037S?NP-SBJ verb - subject NP -0.022S?PP verb - PP 0.193NP?DT object noun - determiner -0.070NP-SBJ?DT subject noun - determiner -0.052NP?PP obj noun - PP 0.625NP-SBJ?PP subj noun - PP 0.254NP?SBAR obj noun - rel.
clause 0.858NP-SBJ?SBAR subject noun - rel.
clause -0.110NP?JJ obj noun - adjective 0.198NP-SBJ?JJ subj noun - adjective -0.052Table 5: Sample weights from optimized DLA.
Neg-atively weighted dependents appear to the left oftheir head.held-out test data slowly decreases with more data,the percentage of correctly ordered dependents isless well-behaved.
It turns out that even 100 sen-tences are enough to learn a DLA that is nearly asgood as one derived from a much larger dataset.4.4 Comparing the Optimized DLA to EnglishWe have seen that the optimized DLA matches En-glish text much better than a random DLA and thatit achieves only a slightly lower dependency lengththan English.
It is also of interest to compare theoptimized DLA to English in more detail.
Firstwe examine the DLA?s tendency towards ?opposite-branching 1-word phrases?.
English reflects thisprinciple to a striking degree: on the WSJ test set,79.4 percent of left-branching phrases are 1-word,compared to only 19.4 percent of right-branchingphrases.
The optimized DLA also reflects this pat-tern, though somewhat less strongly: 75.5 percent ofleft-branching phrases are 1-word, versus 36.7 per-cent of right-branching phrases.We can also compare the optimized DLA to En-glish with regard to specific rules.
As explained ear-lier, the optimal DLA?s rules are expressed in theform of weights assigned to each relation, with pos-itive weights indicating right-branching placement.Table 5 shows some important rules.
The middlecolumn shows the syntactic situation in which therelation normally occurs.
We see, first of all, thatobject NPs are to the right of the verb and subjectNPs are to the left, just like in English.
PPs are alsothe right of verbs; the fact that the weight is greaterthan for NPs indicates that they are placed further tothe right, as they normally are in English.
Turning190to the internal structure of noun phrases, we see thatdeterminers are to the left of both object and sub-ject nouns; PPs are to the right of both object andsubject nouns.
We also find some differences withEnglish, however.
Clause modifiers of nouns (theseare mostly relative clauses) are to the right of objectnouns, as in English, but to the left of subject nouns;adjectives are to the left of subject nouns, as in En-glish, but to the right of object nouns.
Of course,these differences partly arise from the fact that wetreat NP and NP-SBJ as distinct whereas Englishdoes not (with regard to their internal structure).5 ConclusionIn this paper we have presented a dependency lin-earization algorithm which is optimized for mini-mizing dependency length, while still maintainingconsistent positioning for each grammatical relation.The fact that English is so much lower than therandom DLAs in dependency length gives suggeststhat dependency length minimization is an importantgeneral preference in language.
The output of theoptimized DLA also proves to be much more similarto English than a random DLA in word order.
An in-formal comparison of some important rules betweenEnglish and the optimal DLA reveals a number ofstriking similarities, though also some differences.The fact that the optimized DLA?s orderingmatches English on only 65% of words shows, notsurprisingly, that English word order is determinedby other factors in addition to dependency lengthminimization.
In some cases, ordering choices inEnglish are underdetermined by syntactic rules.
Forexample, a manner adverb may be placed either be-fore the verb or after (?He ran quickly / he quicklyran?).
Here the optimized DLA requires a consistentordering while English does not.
One might supposethat such syntactic choices in English are guided atleast partly by dependency length minimization, andindeed there is evidence for this; for example, peopletend to put the shorter of two PPs closer to the verb(Hawkins, 1994).
But there are also other factors in-volved ?
for example, the tendency to put ?given?discourse elements before ?new?
ones, which hasbeen shown to play a role independent of length(Arnold et al, 2000).In other cases, the optimized DLA allows morefine-grained choices than English.
For example, theoptimized DLA treats NP and NP-SBJ as different;this allows it to have different syntactic rules for thetwo cases ?
a possibility that it sometimes exploits,as seen above.
No doubt this partly explains why theoptimized DLA achieves lower dependency lengththan English.Acknowledgments This work was supported byNSF grants IIS-0546554 and IIS-0325646.ReferencesJ.
E. Arnold, T. Wasow, T. Losongco, and R. Ginstrom.2000.
Heaviness vs. newness: the effects of structuralcomplexity and discourse status on constituent order-ing.
Language, 76:28?55.F.
R. K. Chung.
1984.
On optimal linear arrangements oftrees.
Computers and Mathematics with Applications,10:43?60.Michael John Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Matthew Dryer.
1992.
The Greenbergian word order cor-relations.
Language, 68:81?138.Jason Eisner and Noah A. Smith.
2005.
Parsing withsoft and hard constraints on dependency length.
InProceedings of the International Workshop on ParsingTechnologies (IWPT), pages 30?41.Lyn Frazier.
1985.
Syntactic complexity.
In D. Dowty,L.
Karttunen, and A. Zwicky, editors, Natural Lan-guage Parsing: Psychological, Computational, andTheoretical Perspectives, pages 129?189.
CambridgeUniversity Press, Cambridge.Edward Gibson.
1998.
Linguistic complexity: Localityof syntactic dependencies.
Cognition, 68:1?76.John Hawkins.
1994.
A Performance Theory of Orderand Constituency.
Cambridge University Press, Cam-bridge, UK.Richard M. Karp.
1972.
Reducibility among combina-torial problems.
In R. E. Miller and J. W. Thatcher,editors, Complexity of Computer Computations, pages85?103.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT/EMNLP.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of ACL-03.191
