Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 602?610,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsHierarchical Bayesian Domain AdaptationJenny Rose Finkel and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{jrfinkel|manning}@cs.stanford.eduAbstractMulti-task learning is the problem of maxi-mizing the performance of a system across anumber of related tasks.
When applied to mul-tiple domains for the same task, it is similar todomain adaptation, but symmetric, rather thanlimited to improving performance on a targetdomain.
We present a more principled, betterperforming model for this problem, based onthe use of a hierarchical Bayesian prior.
Eachdomain has its own domain-specific parame-ter for each feature but, rather than a constantprior over these parameters, the model insteadlinks them via a hierarchical Bayesian globalprior.
This prior encourages the features tohave similar weights across domains, unlessthere is good evidence to the contrary.
Weshow that the method of (Daume?
III, 2007),which was presented as a simple ?prepro-cessing step,?
is actually equivalent, exceptour representation explicitly separates hyper-parameters which were tied in his work.
Wedemonstrate that allowing different values forthese hyperparameters significantly improvesperformance over both a strong baseline and(Daume?
III, 2007) within both a conditionalrandom field sequence model for named en-tity recognition and a discriminatively traineddependency parser.1 IntroductionThe goal of multi-task learning is to improve perfor-mance on a set of related tasks, when provided with(potentially varying quantities of) annotated data foreach of the tasks.
It is very closely related to domainadaptation, a far more common task in the naturallanguage processing community, but with two pri-mary differences.
Firstly, in domain adaptation thedifferent tasks are actually just different domains.Secondly, in multi-task learning the focus is on im-proving performance across all tasks, while in do-main adaptation there is a distinction between sourcedata and target data, and the goal is to improve per-formance on the target data.
In the present work wefocus on domain adaptation, but like the multi-tasksetting, we wish to improve performance across alldomains and not a single target domains.
The worddomain is used here somewhat loosely: it may referto a topical domain or to distinctions that linguistsmight term mode (speech versus writing) or regis-ter (formal written prose versus SMS communica-tions).
For example, one may have a large amountof parsed newswire, and want to use it to augmenta much smaller amount of parsed e-mail, to build ahigher quality parser for e-mail data.
We also con-sider the extension to the task where the annotationis not the same, but is consistent, across domains(that is, some domains may be annotated with moreinformation than others).This problem is important because it is omni-present in real life natural language processing tasks.Annotated data is expensive to produce and limitedin quantity.
Typically, one may begin with a con-siderable amount of annotated newswire data, someannotated speech data, and a little annotated e-maildata.
It would be most desirable if the aggregatedtraining data could be used to improve the perfor-mance of a system on each of these domains.From the baseline of building separate systemsfor each domain, the obvious first attempt at domainadaptation is to build a system from the union of thetraining data, and we will refer to this as a secondbaseline.
In this paper we propose a more principled,formal model of domain adaptation, which not onlyoutperforms previous work, but maintains attractive602performance characteristics in terms of training andtesting speed.
We also show that the domain adapta-tion work of (Daume?
III, 2007), which is presentedas an ad-hoc ?preprocessing step,?
is actually equiv-alent to our formal model.
However, our representa-tion of the model conceptually separates some of thehyperparameters which are not separated in (Daume?III, 2007), and we found that setting these hyperpa-rameters with different values from one another wascritical for improving performance.We apply our model to two tasks, named entityrecognition, using a linear chain conditional randomfield (CRF), and dependency parsing, using a dis-criminative, chart-based model.
In both cases, wefind that our model improves performance over bothbaselines and prior work.2 Hierarchical Bayesian DomainAdaptation2.1 MotivationWe call our model hierarchical Bayesian domainadaptation, because it makes use of a hierarchicalBayesian prior.
As an example, take the case ofbuilding a logistic classifier to decide if a word ispart of a person?s name.
There will be a param-eter (weight) for each feature, and usually there isa zero-mean Gaussian prior over the parameter val-ues so that they don?t get too large.1 In the stan-dard, single-domain, case the log likelihood of thedata and prior is calculated, and the optimal pa-rameter values are found.
Now, let?s extend thismodel to the case of two domains, one containingAmerican newswire and the other containing Britishnewswire.
The data distributions will be similar forthe two domains, but not identical.
In our model,we have separate parameters for each feature in eachdomain.
We also have a top level parameter (alsoto be learned) for each feature.
For each domain,the Gaussian prior over the parameter values is nowcentered around these top level parameters insteadof around zero.
A zero-mean Gaussian prior is thenplaced over the top level parameters.
In this ex-ample, if some feature, say word=?Nigel,?
only ap-pears in the British newswire, the correspondingweight for the American newswire will have a sim-ilar value.
This happens because the evidence inthe British domain will push the British parameter1This can be regarded as a Bayesian prior or as weight reg-ularization; we adopt the former perspective here.to have a high value, and this will in turn influencethe top-level parameter to have a high value, whichwill then influence the American newswire to havea high value, because there will be no evidence inthe American data to override the prior.
Conversely,if some feature is highly indicative of isName=truefor the British newswire, and of isName=false forthe American newswire, then the British parameterwill have a high (positive) value while the Americanparameter will have a low (negative) value, becausein both cases the domain-specific evidence will out-weigh the effect of the prior.2.2 Formal ModelOur domain adaptation model is based on a hierar-chical Bayesian prior, through which the domain-specific parameters are tied.
The model is verygeneral-purpose, and can be applied to any discrim-inative learning task for which one would typicallyput a prior with a mean over the parameters.
We willbuild up to it by first describing a general, single-domain, discriminative learning task, and then wewill show how to modify this model to constructour hierarchical Bayesian domain adaptation model.In a typical discriminative probabilistic model, thelearning process consists of optimizing the log con-ditional likelihood of the data with respect to the pa-rameters, Lorig(D ;?).
This likelihood function cantake on many forms: logistic regression, a condi-tional Markov model, a conditional random field, aswell as others.
It is common practice to put a zero-mean Gaussian prior over the parameters, leading tothe following objective, for which we wish to findthe optimal parameter values:argmax?
(Lorig(D ;?)??i?2i2?
2)(1)From a graphical models perspective, this looks likeFigure 1(a), where ?
is the mean for the prior (in ourcase, zero), ?
2 is the variance for the prior, ?
are theparameters, or feature weights, and D is the data.Now we will extend this single-domain model intoa multi-domain model (illustrated in Figure 1(b)).Each feature weight ?i is replicated once for eachdomain, as well as for a top-level set of parame-ters.
We will refer to the parameters for domaind as ?d , with individual components ?d,i, the top-level parameters as ?
?, and all parameters collec-tively as ?
.
All of the power of our model stemsfrom the relationship between these sets of param-603??
?DN???
??
?d ?dDdN M???
??
?txt ?txt ?sp ?sp?d ?d ?d ?dDd Dd(a) (b) (c)Figure 1: (a) No domain adaptation.
The model parameters, ?
, are normally distributed, with mean ?
(typically zero)and variance ?2.
The likelihood of the data,D , is dependent on the model parameters.
The form of the data distributiondepends on the underlying model (e.g., logistic regression, or a CRF).
(b) Our hierarchical domain adaptation model.The top-level parameters, ?
?, are normally distributed, with mean ?
(typically zero) and variance ?2?
.
There is a platefor each domain.
Within each plate, the domain-specific parameters, ?d are normally distributed, with mean ??
andvariance ?2d .
(c) Our hierarchical domain adaptation model, with an extra level of structure.
In this example, thedomains are further split into text and speech super-domains, each of which has its own set of parameters (?txt and ?txtfor text and ?sp and ?sp for speech).
?d is normally distributed with mean ?txt if domain d is in the text super-domain,and ?sp if it is in the speech super-domain.eters.
First, we place a zero-mean Gaussian priorover the top level parameters ??.
Then, these toplevel parameters are used as the mean for a Gaussianprior placed over each of the domain-specific param-eters ?d .
These domain-specific parameters are thenthe parameters used in the original conditional loglikelihood functions for each domain.
The domain-specific parameter values jointly influence an appro-priate value for the higher-level parameters.
Con-versely, the higher-level parameters will largely de-termine the domain-specific parameters when thereis little or no evidence from within a domain, but canbe overriden by domain-specific evidence when itclearly goes against the general picture (for instanceLeeds is normally a location, but within the sportsdomain is usually an organization (football team)).The beauty of this model is that the degree of in-fluence each domain exerts over the others, for eachparameter, is based on the amount of evidence eachdomain has about that parameter.
If a domain hasa lot of evidence for a feature weight, then that evi-dence will outweigh the effect of the prior.
However,when a domain lacks evidence for a parameter theopposite occurs, and the prior (whose value is deter-mined by evidence in the other domains) will have agreater effect on the parameter value.To achieve this, we modify the objective func-tion.
We now sum over the log likelihood for all do-mains, including a Gaussian prior for each domain,but which is now centered around ?
?, the top-levelparameters.
Outside of this summation, we have aGaussian prior over the top-level parameters whichis identical to the prior in the original model:Lhier(D ;?)
= (2)?d(Lorig(Dd ;?d)?
?i(?d,i ???,i)22?
2d)??i(??,i)22?
2?where ?
2d and ?
2?
are variances on the priors overthe parameters for all the domains, as well as thetop-level parameters.
The graphical models repre-sentation is shown in Figure 1(b).One potential source of confusion is with respectto the directed or undirected nature of our domainadaptation model, and the underlying model of thedata.
Our hierarchical Bayesian domain adaptationmodel is directed, as illustrated in Figure 1.
How-ever, somewhat counterintuitively, the underlying(original) model of the data can be either directedor undirected, and for our experiments we use undi-604rected, conditional random field-based models.
Thedirected domain adaptation model can be viewedas a model of the parameters, and those parameterweights are used by the underlying data model.
InFigure 1, the entire data model is represented by asingle node, D , conditioned on the parameters, ?
or?d .
The form of that model can then be almost any-thing, including an undirected model.From an implementation perspective, the objec-tive function is not much more difficult to implementthan the original single-domain model.
For all of ourexperiments, we optimized the log likelihood usingL-BFGS, which requires the function value and par-tial derivatives of each parameter.
The new partialderivatives for the domain-specific parameters (butnot the top-level parameters) utilize the same par-tial derivatives as in the original model.
The onlychange in the calculations is with respect to the pri-ors.
The partial derivatives for the domain-specificparameters are:?Lhier(D ;?)?
?d,i= ?Ld(Dd ,?d)?
?d,i ?
?d,i ???,i?
2d(3)and the derivatives for the top level parameters ?
?are:?Lhier(D ;?)???,i=(?d?
?,i ??d,i?
2d)?
??,i?
2?
(4)This function is convex.
Once the optimal param-eters have been learned, the top level parameterscan be discarded, since the runtime model for eachdomain is the same as the original (single-domain)model, parameterized by the parameters learned forthat domain in the hierarchical model.
However, itmay be useful to retain the top-level parameters foruse in adaptation to further domains in the future.In our model there are d extra hyper-parameterswhich can be tuned.
These are the variances ?
2d foreach domain.
When this value is large then the priorhas little influence, and when set high enough will beequivalent to training each model separately.
Whenthis value is close to zero the prior has a strong in-fluence, and when it is sufficiently close to zero thenit will be equivalent to completely tying the param-eters, such that ?d1,i = ?d2,i for all domains.
Despitehaving many more parameters, for both of the taskson which we performed experiments, we found thatour model did not take much more time to train thata baseline model trained on all of the data concate-nated together.2.3 Model GeneralizationThe model as presented thus far can be viewedas a two level tree, with the top-level parametersat the root, and the domain-specific ones at theleaves.
However, it is straightforward to generalizethe model to any tree structure.
In the generalizedversion, the domain-specific parameters would stillbe at the leaves, the top-level parameters at the root,but new mid-level parameters can be added basedon beliefs about how similar the various domainsare.
For instance, if one had four datasets, two ofwhich contained speech data and two of which con-tained newswire, then it might be sensible to havetwo sets of mid-level parameters, one for the speechdata and one for the newswire data, as illustrated inFigure 1(c).
This would allow the speech domainsto influence one another more than the newswire do-mains, and vice versa.2.4 Formalization of (Daume?
III, 2007)As mentioned earlier, our model is equivalent to thatpresented in (Daume?
III, 2007), and can be viewedas a formal version of his model.2 In his presenta-tion, the adapation is done through feature augmen-tation.
Specifically, for each feature in the originalversion, a new version is created for each domain, aswell as a general, domain-independent version of thefeature.
For each datum, two versions of each orig-inal feature are present: the version for that datum?sdomain, and the domain independent one.The equivalence between the two models can beshown with simple arithmetic.
Recall that the loglikelihood of our model is:?d(Lorig(Dd ;?d)?
?i(?d,i ???,i)22?
2d)??i(??,i)22?
2?We now introduce a new variable ?d = ?d ??
?, andplug it into the equation for log likelihood:?d(Lorig(Dd ;?d +??)??i(?d,i)22?
2d)??i(??,i)22?
2?The result is the model of (Daume?
III, 2007), wherethe ?d are the domain-specific feature weights, and?d are the domain-independent feature weights.
Inhis formulation, the variances ?
2d = ?
2?
for all do-mains d.This separation of the domain-specific and inde-pendent variances was critical to our improved per-formance.
When using a Gaussian prior there are2Many thanks to David Vickrey for pointing this out to us.605two parameters set by the user: the mean, ?
(usu-ally zero), and the variance, ?
2.
Technically, eachof these parameters is actually a vector, with an en-try for each feature, but almost always the vectorsare uniform and the same parameter is used for eachfeature (there are exceptions, e.g.
(Lee et al, 2007)).Because Daume?
III (2007) views the adaptation asmerely augmenting the feature space, each of hisfeatures has the same prior mean and variance, re-gardless of whether it is domain specific or indepen-dent.
He could have set these parameters differently,but he did not.3 In our presentation of the model,we explicitly represent different variances for eachdomain, as well as the top level parameters.
Wefound that specifying different values for the domainspecific versus domain independent variances sig-nificantly improved performance, though we foundno gains from using different values for the differ-ent domain specific variances.
The values were setbased on development data.3 Named Entity RecognitionFor our first set of experiments, we used a linear-chain, conditional random field (CRF) model,trained for named entity recognition (NER).
The useof CRFs for sequence modeling has become stan-dard so we will omit the model details; good expla-nations can be found in a number of places (Laffertyet al, 2001; Sutton and McCallum, 2007).
Our fea-tures were based on those in (Finkel et al, 2005).3.1 DataWe used three named entity datasets, from theCoNLL 2003, MUC-6 and MUC-7 shared tasks.CoNLL is British newswire, while MUC-6 andMUC-7 are both American newswire.
ArguablyMUC-6 and MUC-7 should not count as separatedomains, but because they were annotated sepa-rately, for different shared tasks, we chose to treatthem as such, and feel that our experimental resultsjustify the distinction.
We used the standard trainand test sets for each domain, which for CoNLL cor-responds to the (more difficult) testb set.
For detailsabout the number of training and test words in eachdataset, please see Table 1.One interesting challenge in dealing with bothCoNLL and MUC data is that the label sets differ.3Although he alludes to the potential for something similarin the last section of his paper, when discussing the kerneliza-tion interpretation of his approach.# Train # TestWords WordsMUC-6 165,082 15,032MUC-7 89,644 64,490CoNLL 203,261 46,435Table 1: Number of words in the training and test sets foreach of the named entity recognition datasets.CoNLL has four classes: person, organization, lo-cation, and misc.
MUC data has seven classes: per-son, organization, location, percent, date, time, andmoney.
They overlap in the three core classes (per-son, organization, and location), but CoNLL hasone additional class and MUC has four additionalclasses.The differences in the label sets led us to performtwo sets of experiments for the baseline and hier-archical Bayesian models.
In the first set of exper-iments, at training time, the model allows any la-bel from the union of the label sets, regardless ofwhether that label was legal for the domain.
At testtime, we would ignore guesses made by the modelwhich were inconsistent with the allowed labels forthat domain.4 In the second set of experiments, werestricted the model at training time to only allowlegal labels for each domain.
At test time, the do-main was specified, and the model was once againrestricted so that words would never be tagged witha label outside of that domain?s label set.3.2 Experimental Results and DiscussionIn our experiments, we compared our model to sev-eral strong baselines, and the full set of results is inTable 2.
The models we used were:TARGET ONLY.
Trained and tested on only the datafor that domain.ALL DATA.
Trained and tested on data from all do-mains, concatenated into one large dataset.ALL DATA*.
Same as ALL DATA, but restrictedpossible labels for each word based on domain.DAUME07.
Trained and tested using the same tech-nique as (Daume?
III, 2007).
We note that theypresent results using per-token label accuracy,while we used the more standard entity preci-sion, recall, and F score (as in the CoNLL 2003shared task).4We treated them identically to the background symbol.
So,for instance, labelling a word a date in the CoNLL data had noeffect on the score.606Named Entity RecognitionModel Precision Recall F1MUC-6TARGET ONLY 86.74 80.10 83.29ALL DATA* 85.04 83.49 84.26ALL DATA 86.00 82.71 84.32DAUME07* 87.83 83.41 85.56DAUME07 87.81 82.23 85.46HIER BAYES* 88.59 84.97 86.74HIER BAYES 88.77 85.14 86.92MUC-7TARGET ONLY 81.17 70.23 75.30ALL DATA* 81.66 76.17 78.82ALL DATA 82.20 70.91 76.14DAUME07* 83.33 75.42 79.18DAUME07 83.51 75.63 79.37HIER BAYES* 82.90 76.95 79.82HIER BAYES 83.17 77.02 79.98CoNLLTARGET ONLY 85.55 84.72 85.13ALL DATA* 86.34 84.45 85.38ALL DATA 86.58 83.90 85.22DAUME07* 86.09 85.06 85.57DAUME07 86.35 85.26 85.80HIER BAYES* 86.33 85.06 85.69HIER BAYES 86.51 85.13 85.81Table 2: Named entity recognition results for each of themodels.
With the exception of the TARGET ONLY model,all three datasets were combined when training each ofthe models.DAUME07*.
Same as DAUME07, but restrictedpossible labels for each word based on domain.HIER BAYES.
Our hierarchical Bayesian domainadaptation model.HIER BAYES*.
Same as HIER BAYES, but re-stricted possible labels for each word based onthe domain.For all of the baseline models, and for the toplevel-parameters in the hierarchical Bayesian model,we used ?
= 1.
For the domain-specific parameters,we used ?d = 0.1 for all domains.The HIER BAYES model outperformed all base-lines for both of the MUC datasets, and tied withthe DAUME07 for CoNLL.
The largest improvementwas on MUC-6, where HIER BAYES outperformedDAUME07*, the second best model, by 1.36%.
Thisimprovement is greater than the improvement madeby that model over the ALL DATA* baseline.
To as-sess significance we used a document-level pairedt-test (over all of the data combined), and found thatHIER BAYES significantly outperformed all of thebaselines (not including HIER BAYES*) with greaterthan 95% confidence.For both the HIER BAYES and DAUME07 mod-els, we found that performance was better for thevariant which did not restrict possible labels basedon the domain, while the ALL DATA model did ben-efit from the label restriction.
For H IER BAYES andDAUME07, this result may be due to the structureof the models.
Because both models have domain-specific features, the models likely learned that theselabels were never actually allowed.
However, whena feature does not occur in the data for a particulardomain, then the domain-specific parameter for thatfeature will have positive weight due to evidencepresent in the other domains, which at test time canlead to assigning an illegal label to a word.
Thisinformation that a word may be of some other (un-known to that domain) entity type may help preventthe model from mislabeling the word.
For example,in CoNLL, nationalities, such as Iraqi and Ameri-can, are labeled as misc.
If a previously unseen na-tionality is encountered in the MUC testing data, theMUC model may be tempted to label is as a location,but this evidence from the CoNLL data may preventthat, by causing it to instead be labeled misc, a labelwhich will subsequently be ignored.In typical domain adaptation work, showing gainsis made easier by the fact that the amount of train-ing data in the target domain is comparatively small.Within the multi-task learning setting, it is morechallenging to show gains over the ALL DATA base-line.
Nevertheless, our results show that, so long asthe amount of data in each domain is not widely dis-parate, it is possible to achieve gains on all of thedomains simultaneously.4 Dependency Parsing4.1 Parsing ModelWe also tested our model on an untyped dependencyparsing task, to see how it performs on a more struc-turally complex task than sequence modeling.
Toour knowledge, the discriminatively trained depen-dency model we used has not been previously pub-lished, but it is very similar to recent work on dis-criminative constituency parsing (Finkel and Man-ning, 2008).
Due to space restrictions, we cannotgive a complete treatment of the model, but will givean overview.607We built a CRF-based model, optimizing the like-lihood of the parse, conditioned on the words andparts of speech of the sentence.
At the heart ofour model is the Eisner dependency grammar chart-parsing algorithm (Eisner, 1996), which allows forefficient computation of inside and outside scores.The Eisner algorithm, originally designed for gen-erative parsing, decomposes the probability of a de-pendency parse into the probabilities of each attach-ment of a dependent to its parent, and the proba-bilities of each parent stopping taking dependents.These probabilities can be conditioned on the child,parent, and direction of the dependency.
We useda slight modification of the algorithm which allowseach probability to also be conditioned on whetherthere is a previous dependent.
While the unmodifiedversion of the algorithm includes stopping probabil-ities, conditioned on the parent and direction, theyhave no impact on which parse for a particular sen-tence is most likely, because all words must eventu-ally stop taking dependents.
However, in the modi-fied version, the stopping probability is also condi-tioned on whether or not there is a previous depen-dent, so this probability does make a difference.While the Eisner algorithm computes locally nor-malized probabilities for each attachment decision,our model computes unnormalized scores.
Froma graphical models perspective, our parsing modelis undirected, while the original model is directed.5The score for a particular tree decomposes the sameway in our model as in the original Eisner model,but it is globally normalized instead of locally nor-malized.
Using the inside and outside scores we cancompute partial derivatives for the feature weights,as well as the value of the normalizing constantneeded to determine the probability of a particularparse.
This is done in a manner completely analo-gous to (Finkel and Manning, 2008).
Partial deriva-tives and the function value are all that is needed tofind the optimal feature weights using L-BFGS.6Features are computed over each attachment andstopping decision, and can be conditioned on the5The dependencies themselves are still directed in bothcases, it is just the underlying graphical model used to computethe likelihood of a parse which changes from a directed modelto an undirected model.6In (Finkel and Manning, 2008) we used stochastic gradientdescent to optimize our weights because our function evaluationwas too slow to use L-BFGS.
We did not encounter this problemin this setting.parent, dependent (or none, if it is a stopping deci-sion), direction of attachment, whether there is a pre-vious dependent in that direction, and the words andparts of speech of the sentence.
We used the samefeatures as (McDonald et al, 2005), augmented withinformation about whether or not a dependent is thefirst dependent (information they did not have).4.2 DataFor our dependency parsing experiments, we usedLDC2008T04 OntoNotes Release 2.0 data (Hovyet al, 2006).
This dataset is still in development,and includes data from seven different domains, la-beled for a number of tasks, including PCFG trees.The domains span both newswire and speech frommultiple sources.
We converted the PCFG treesinto dependency trees using the Collins head rules(Collins, 2003).
We also omitted the WSJ portionof the data, because it follows a different annotationscheme from the other domains.7 For each of theremaining six domains, we aimed for an 75/25 datasplit, but because we divided the data using the pro-vided sections, this split was fairly rough.
The num-ber of training and test sentences for each domainare specified in the Table 3, along with our results.4.3 Experimental Results and DiscussionWe compared the same four domain adaptationmodels for dependency parsing as we did for thenamed entity experiments, once again setting ?
=1.0 and ?d = 0.1.
Unlike the named entity experi-ments however, there were no label set discrepenciesbetween the domains, so only one version of eachdomain adaptation model was necessary, instead ofthe two versions in that section.Our full dependency parsing results can be foundin Table 3.
Firstly, we found that DAUME07, whichhad outperformed the ALL DATA baseline for thesequence modeling task, performed worse than the7Specifically, all the other domains use the ?new?
PennTreebank annotation style, whereas the WSJ data is still in the?traditional?
annotation style, familiar from the past decade?swork in Penn Treebank parsing.
The major changes are inhyphenation and NP structure.
In the new annotation style,many hyphenated words are separated into multiple tokens, witha new part-of-speech tag given to the hyphens, and leftward-branching structure inside noun phrases is indicated by use ofa new NML phrasal category.
The treatment of hyphenatedwords, in particular, makes the two annotation styles inconsis-tent, and so we could not work with all the data together.608Dependency ParsingTraining Testing TARGET ALL HIERRange # Sent Range # Sent ONLY DATA DAUME07 BAYESABC 0?55 1195 56?69 199 83.32% 88.97% 87.30% 88.68%CNN 0?375 5092 376?437 1521 85.53% 87.09% 86.41% 87.26%MNB 0?17 509 18?25 245 77.06% 86.41% 84.70% 86.71%NBC 0?29 552 30?39 149 76.21% 85.82% 85.01% 85.32%PRI 0?89 1707 90?112 394 87.65% 90.28% 89.52% 90.59%VOA 0?198 1512 199?264 383 89.17% 92.11% 90.67% 92.09%Table 3: Dependency parsing results for each of the domain adaptation models.
Performance is measured as unlabeledattachment accuracy.baseline here, indicating that the transfer of infor-mation between domains in the more structurallycomplicated task is inherently more difficult.
Ourmodel?s gains over the ALL DATA baseline arequite small, but we tested their significance using asentence-level paired t-test (over all of the data com-bined) and found them to be significant at p < 10?5.We are unsure why some domains improved whileothers did not.
It is not simply a consequence oftraining set size, but may be due to qualities of thedomains themselves.5 Related WorkWe already discussed the relation of our work to(Daume?
III, 2007) in Section 2.4.
Another piece ofsimilar work is (Chelba and Acero, 2004), who alsomodify their prior.
Their work is limited to two do-mains, a source and a target, and their algorithm hasa two stage process: First, train a classifier on thesource data, and then use the learned weights fromthat classifier as the mean for a Gaussian prior whentraining a new model on just the target data.Daume?
III and Marcu (2006) also took a Bayesianapproach to domain adaptation, but structured theirmodel in a very different way.
In their model, it isassumed that each datum within a domain is either adomain-specific datum, or a general datum, and thendomain-specific and general weights were learned.Whether each datum is domain-specific or generalis not known, so they developed an EM based algo-rithm for determining this information while simul-taneously learning the feature weights.
Their modelhad good performance, but came with a 10 to 15times slowdown at training time.
Our slowest de-pendency parser took four days to train, making thismodel close to infeasible for learning on that data.Outside of the NLP community there has beenmuch similar work making use of hierarchicalBayesian priors to tie parameters across multiple,similar tasks.
Evgeniou et al (2005) present a sim-ilar model, but based on support vector machines,to predict the exam scores of students.
Elidan etal.
(2008) make us of an undirected Bayesian trans-fer hierarchy to jointly model the shapes of differ-ent mammals.
The complete literature on relatedmulti-task learning is too large to fully discuss here,but we direct the reader to (Baxter, 1997; Caruana,1997; Yu et al, 2005; Xue et al, 2007).
For a moregeneral discussion of hierarchical priors, we recom-mend Chapter 5 of (Gelman et al, 2003) and Chap-ter 12 of (Gelman and Hill, 2006).6 Conclusion and Future WorkIn this paper we presented a new model for domainadaptation, based on a hierarchical Bayesian prior,which allows information to be shared between do-mains when information is sparse, while still allow-ing the data from a particular domain to override theinformation from other domains when there is suf-ficient evidence.
We outperformed previous workon a sequence modeling task, and showed improve-ments on dependency parsing, a structurally morecomplex problem, where previous work failed.
Ourmodel is practically useful and does not require sig-nificantly more time to train than a baseline modelusing the same data (though it does require morememory, proportional to the number of domains).
Inthe future we would like to see if the model could beadapted to improve performance on data from a newdomain, potentially by using the top-level weightswhich should be less domain-dependent.AcknowledgementsThe first author is supported by a Stanford GraduateFellowship.
We also thank David Vickrey for hishelpful comments and observations.609ReferencesJ.
Baxter.
1997.
A bayesian/information theoretic modelof learning to learn via multiple task sampling.
In Ma-chine Learning, volume 28.R.
Caruana.
1997.
Multitask learning.
In Machine Learn-ing, volume 28.Ciprian Chelba and Alex Acero.
2004.
Adaptation of amaximum entropy capitalizer: Little data can help alot.
In EMNLP 2004.M.
Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguistics,29(4):589?637.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In Conference of the Association for Computa-tional Linguistics (ACL), Prague, Czech Republic.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of the 16th International Conference on Compu-tational Linguistics (COLING-96), Copenhagen.Gal Elidan, Benjamin Packer, Geremy Heitz, and DaphneKoller.
2008.
Convex point estimation using undi-rected bayesian transfer hierarchies.
In UAI 2008.T.
Evgeniou, C. Micchelli, and M. Pontil.
2005.
Learn-ing multiple tasks with kernel methods.
In Journal ofMachine Learning Research.Jenny Rose Finkel and Christopher D. Manning.
2008.Efficient, feature-based conditional random field pars-ing.
In ACL/HLT-2008.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In ACL 2005.Andrew Gelman and Jennifer Hill.
2006.
Data AnalysisUsing Regression and Multilevel/Hierarchical Models.Cambridge University Press.A.
Gelman, J.
B. Carlin, H. S. Stern, and DonaldD.
B. Rubin.
2003.
Bayesian Data Analysis.
Chapman& Hall.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In HLT-NAACL 2006.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML 2001.Su-In Lee, Vassil Chatalbashev, David Vickrey, andDaphne Koller.
2007.
Learning a meta-level prior forfeature relevance from multiple related tasks.
In ICML?07: Proceedings of the 24th international conferenceon Machine learning.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In ACL 2005.Charles Sutton and Andrew McCallum.
2007.
An in-troduction to conditional random fields for relationallearning.
In Lise Getoor and Ben Taskar, editors, Intro-duction to Statistical Relational Learning.
MIT Press.Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krish-napuram.
2007.
Multi-task learning for classificationwith dirichlet process priors.
J. Mach.
Learn.
Res., 8.Kai Yu, Volker Tresp, and Anton Schwaighofer.
2005.Learning gaussian processes from multiple tasks.
InICML ?05: Proceedings of the 22nd international con-ference on Machine learning.610
