Non-Verbal Cues for Discourse StructureJustine Cassell?, Yukiko I.
Nakano?, Timothy W. Bickmore?,Candace L.
Sidner?, and Charles Rich?
?MIT Media Laboratory20 Ames StreetCambridge, MA 02139{justine, yukiko, bickmore}@media.mit.edu?Mitsubishi Electric Research Laboratories201 BroadwayCambridge, MA 02139{sidner, rich}@merl.comAbstractThis paper addresses the issue ofdesigning embodied conversationalagents that exhibit appropriate postureshifts during dialogues with humanusers.
Previous research has noted theimportance of hand gestures, eye gazeand head nods in conversationsbetween embodied agents and humans.We present an analysis of humanmonologues and dialogues thatsuggests that postural shifts can bepredicted as a function of discoursestate in monologues, and discourse andconversation state in dialogues.
On thebasis of these findings, we haveimplemented an embodiedconversational agent that usesCollagen in such a way as to generatepostural shifts.1.
IntroductionThis paper provides empirical support for therelationship between posture shifts anddiscourse structure, and then derives analgorithm for generating posture shifts in ananimated embodied conversational agent fromdiscourse states produced by the middlewarearchitecture known as Collagen [18].
Othernonverbal behaviors have been shown to becorrelated with the underlying conversationalstructure and information structure of discourse.For example, gaze shifts towards the listenercorrelate with a shift in conversational turn(from the conversational participants?perspective, they can be seen as a signal that thefloor is available).
Gestures correlate withrhematic content in accompanying language(from the conversational participants?perspective, these behaviors can be seen as asignal that accompanying speech is of highinterest).
A better understanding of the role ofnonverbal behaviors in conveying discoursestructures enables improvements in thenaturalness of embodied dialogue systems, suchas embodied conversational agents, as well ascontributing to algorithms for recognizingdiscourse structure in speech-understandingsystems.
Previous work, however, has notaddressed major body shifts during discourse,nor has it addressed the nonverbal correlates oftopic shifts.2.
BackgroundOnly recently have computational linguistsbegun to examine the association of nonverbalbehaviors and language.
In this section wereview research by non-computational linguistsand discuss how this research has beenemployed to formulate algorithms for naturallanguage generation or understanding.About three-quarters of all clauses in descriptivediscourse are accompanied by gestures [17], andwithin those clauses, the most effortful part ofgestures tends to co-occur with or just before thephonologically most prominent syllable of theaccompanying speech [13].
It has been shownthat when speech is ambiguous or in a speechsituation with some noise, listeners rely ongestural cues [22] (and, the higher the noise-to-signal ratio, the more facilitation by gesture).Even when gestural content overlaps withspeech (reported to be the case in roughly 50%of utterances, for descriptive discourse), gestureoften emphasizes information that is alsofocused pragmatically by mechanisms likeprosody in speech.
In fact, the semantic andpragmatic compatibility in the gesture-speechrelationship recalls the interaction of words andgraphics in multimodal presentations [11].On the basis of results such as these, severalresearchers have built animated embodiedconversational agents that ally synthesizedspeech with animated hand gestures.
Forexample, Lester et al [15] generate deicticgestures and choose referring expressions as afunction of the potential ambiguity andproximity of objects referred to.
Rickel andJohnson [19]'s pedagogical agent produces adeictic gesture at the beginning of explanationsabout objects.
Andr?
et al [1] generate pointinggestures as a sub-action of the rhetorical actionof labeling, in turn a sub-action of elaborating.Cassell and Stone [3] generate either speech,gesture, or a combination of the two, as afunction of the information structure status andsurprise value of the discourse entity.Head and eye movement has also been examinedin the context of discourse and conversation.Looking away from one?s interlocutor has beencorrelated with the beginning of turns.
From thespeaker?s point of view, this look away mayprevent an overload of visual and linguisticinformation.
On the other hand, during theexecution phase of an utterance, speakers lookmore often at listeners.
Head nods and eyebrowraises are correlated with emphasized linguisticitems ?
such as words accompanied by pitchaccents [7].
Some eye movements occurprimarily at the ends of utterances and atgrammatical boundaries, and appear to functionas synchronization signals.
That is, one mayrequest a response from a listener by looking atthe listener, and suppress the listener?s responseby looking away.
Likewise, in order to offer thefloor, a speaker may gaze at the listener at theend of the utterance.
When the listener wants thefloor, s/he may look at and slightly up at thespeaker [10].
It should be noted that turn takingonly partially accounts for eye gaze behavior indiscourse.
A better explanation for gazebehavior integrates turn taking with theinformation structure of the propositionalcontent of an utterance [5].
Specifically, thebeginning of themes are frequently accompaniedby a look-away from the hearer, and thebeginning of rhemes are frequently accompaniedby a look-toward the hearer.
When thesecategories are co-temporaneous with turnconstruction, then they are strongly predictive ofgaze behavior.Results such as these have led researchers togenerate eye gaze and head movements inanimated embodied conversational agents.Takeuchi and Nagao, for example, [21] generategaze and head nod behaviors in a ?talking head.
?Cassell et al [2] generate eye gaze and headnods as a function of turn taking behavior, headturns just before an utterance, and eyebrowraises as a function of emphasis.To our knowledge, research on posture shiftsand other gross body movements, has not beenused in the design or implementation ofcomputational systems.
In fact, although anumber of conversational analysts andethnomethodologists have described postureshifts in conversation, their studies have beenqualitative in nature, and difficult to reformulateas the basis of algorithms for the generation oflanguage and posture.
Nevertheless, researchersin the non-computational fields have discussedposture shifts extensively.
Kendon [13] reportsa hierarchy in the organization of movementsuch that the smaller limbs such as the fingersand hands engage in more frequent movements,while the trunk and lower limbs changerelatively rarely.A number of researchers have noted thatchanges in physical distance during interactionseem to accompany changes in the topic or inthe social relationship between speakers.
Forexample Condon and Osgton [9] have suggestedthat in a speaking individual the changes inthese more slowly changing body parts occur atthe boundaries of the larger units in the flow ofspeech.
Scheflen (1973) also reports thatposture shifts and other general bodymovements appear to mark the points of changebetween one major unit of communicativeactivity and another.
Blom & Gumperz (1972)identify posture changes and changes in thespatial relationship between two speakers asindicators of what they term "situational  shifts"-- momentary changes in the mutual rights andobligations between  speakers accompanied byshifts in language style.
Erickson (1975)concludes that proxemic shifts seem to bemarkers of 'important' segments.
In his analysisof college counseling interviews, they occurredmore frequently than any other coded indicatorof segment changes, and were therefore the bestpredictor of new segments in the data.Unfortunately, in none of these studies arestatistics provided, and their analyses rely onintuitive definitions of discourse segment or?major shift?.
For this reason, we carried outour own empirical study.3.
Empirical StudyVideotaped ?pseudo-monologues?
and dialogueswere used as the basis for the current study.
In?pseudo-monologues,?
subjects were asked todescribe each of the rooms in their home, thengive directions between four pairs of locationsthey knew well (e.g., home and the grocerystore).
The experimenter acted as a listener, onlyproviding backchannel feedback (head nods,smiles and paraverbals such as "uh-huh").
Fordialogues, two subjects were asked to generatean idea for a class project that they would bothlike to work on, including: 1) what they wouldwork on; 2) where they would work on it(including facilities, etc.
), and 3) when theywould work on it.
Subjects stood in bothconditions and were told to perform their tasksin 5-10 minutes.
The pseudo-monologuecondition (pseudo- because there was in fact aninterlocutor, although he gave backchannelfeedback only and never took the turn) allowedus to investigate the relationship betweendiscourse structure and posture shiftindependent of turn structure.
The two taskswere constructed to allow us to identify exactlywhere discourse segment boundaries would beplaced.The video data was transcribed and coded forthree features: discourse segment boundaries,turn boundaries, and posture shifts.
A discoursesegment is taken to be an aggregation ofutterances and sub-segments that convey thediscourse segment purpose, which is anintention that leads to the segment initiation[12].
In this study we chose initially to look athigh-level discourse segmentation phenomenarather than those discourse segments embeddeddeeper in the discourse.
Thus, the time points atwhich the assigned task topics were startedserved as segmentation points.
Turn boundarieswere coded (for dialogues only) as the point intime in which the start or end of an utterance co-occurred with a change in speaker, but excludingbackchannel feedback.
Turn overlaps werecoded as open-floor time.
We defined a postureshift as a motion or a position shift for a part ofthe human body, excluding hands and eyes(which we have dealt with in other work).Posture shifts were coded with start and endtime of occurrence (duration), body part in play(for this paper we divided the body at thewaistline and compared upper body vs. lowerbody shifts), and an estimated energy level ofthe posture shift.
Energy level was normalizedfor each subject by taking the largest postureshift observed for each subject as 100% andcoding all other posture shift energies relative tothe 100% case.
Posture shifts that occurred aspart of gesture or were clearly intentionallygenerated (e.g., turning one's body while givingdirections) were not coded.4.
ResultsData from seven monologues and five dialogueswere transcribed, and then coded and analyzedindependently by two raters.
A total of 70.5minutes of data was analyzed (42.5 minutes ofdialogue and 29.2 minutes of monologue).
Atotal of 67 discourse segments were identified(25 in the dialogues and 42 in the monologues),which constituted 407 turns in the dialogue data.We used the instructions given to subjectsconcerning the topics to discuss as segmentationboundaries.
In future research, we will addressthe smaller discourse segmentation.
For postureshift coding, raters coded all posture shiftsindependently, and then calculated reliability onthe transcripts of one monologue (5.2 minutes)and both speakers from one dialogue (8.5minutes).
Agreement on the presence of anupper body or lower body posture shift in aparticular location (taking location to be a 1-second window that contains all of or a part of aposture shift) for these three speakers was 89%(kappa = .64).
For interrater reliability of thecoding of energy level, a Spearman?s rhorevealed a correlation coefficient of .48 (p<.01).4.1 AnalysisPosture shifts occurred regularly throughout thedata (an average of 15 per speaker in bothpseudo-monologues and dialogues).
This,together with the fact that the majority of timewas spent within discourse segments and withinturns (rather than between segments), led us tonormalize our posture shift data for comparisonpurposes.
For relatively brief intervals (inter-discourse-segment and inter-turn) normalizationby number of inter-segment occurrences wassufficient (ps/int), however, for long intervals(intra-discourse segment and intra-turn) weneeded to normalize by time to obtainmeaningful comparisons.
For this normalizationmetric we looked at posture-shifts-per-second(ps/s).
This gave us a mean average of .06posture shifts/second (ps/s) in the monologues(SD=.07), and .07 posture shifts/second in thedialogues (SD=.08).Table 4.1.1.
Posture WRT Discourse SegmentsOur initial analysis compared posture shiftsmade by the current speaker within discoursesegments (intra-dseg) to those produced at theboundaries of discourse segments (inter-dseg).
Itcan be seen (in Table 4.1.1) that posture shiftsoccur an order of magnitude more frequently atdiscourse segment boundaries than withindiscourse segments in both monologues anddialogues.
Posture shifts also tend to be moreenergetic at discourse segment boundaries(F(1,251)=10.4; p<0.001).Table 4.1.2 Posture Shifts WRT Turnsps/s ps/int energyinter-turn 0.140 0.268 0.742intra-turn 0.022  0.738Initially, we classified data as being inter- orintra-turn.
Table 4.1.2 shows that turn structuredoes have an influence on posture shifts;subjects were five times more likely to exhibit ashift at a boundary than within a turn.Table 4.1.3 Posture by Discourse and Turn Breakdownps/s ps/intinter-dseg/start-turn 0.562 0.542inter-dseg/mid-turn 0.000 0.000inter-dseg/end-turn 0.130 0.125intra-dseg/start-turn 0.067 0.135intra-dseg/mid-turn 0.041intra-dseg/end-turn 0.053 0.107An interaction exists between turns anddiscourse segments such that discourse segmentboundaries are ten times more likely to co-occurwith turn changes than within turns.
Both turnand discourse structure exhibit an influence onposture shifts, with discourse having the mostpredictive value.
Starting a turn while starting anew discourse segment is marked with a postureshift roughly 10 times more often than whenstarting a turn while staying within discoursesegment.
We noticed, however, that postureshifts appeared to congregate at the beginningsor ends of turn boundaries, and so oursubsequent analyses examined start-turns, mid-turns and end-turns.
It is clear from these resultsthat posture is indeed correlated with discoursestate, such that speakers generate a posture shiftwhen initiating a new discourse segment, whichis often at the boundary between turns.In addition to looking at the occurrence andenergy of posture shifts we also analyzed thedistributions of upper vs. lower body shifts andthe duration of posture shifts.
Speaker upperbody shifts were found to be used morefrequently at the start of turns (48%) than at themiddle of turns (36%) or end of turns (18%)(F(2,147)=5.39; p<0.005), with no significantMonologues Dialoguesps/s ps/int energy ps/s ps/int energyinter-dseg0.340 0.837 0.832 0.332 0.533 0.844intra-dseg0.039 0.701 0.053  0.723dependence on discourse structure.
Finally,speaker posture shift duration was found tochange significantly as a function of both turnand discourse structure (see Figure 4.1.3).
At thestart of turns, posture shift duration isapproximately the same whether a new topic isintroduced or not (2.5 seconds).
However, whenending a turn, speakers move significantlylonger (7.0 seconds) when finishing a topic thanwhen the topic is continued by the otherinterlocutor (2.7 seconds) (F(1,148)=17.9;p<0.001).Figure 4.1.1 Posture Shift Duration by DSeg and Turn5.
SystemIn the following sections we discuss how theresults of the empirical study were integratedalong with Collagen into our existent embodiedconversational agent, Rea.5.1 System ArchitectureRea is an embodied conversational agent thatinteracts with a user in the real estate agentdomain [2].
The system architecture of Rea isshown in Figure 5.1.
Rea takes input from amicrophone and two cameras in order to sensethe user?s speech and gesture.
The UMinterprets and integrates this multimodal inputand outputs a unified semantic representation.The Understanding Module then sends theoutput to Collagen as the Dialogue Manager.Collagen, as further discussed below, maintainsthe state of the dialogue as shared between Reaand a user.
The Reaction Module decides Rea?snext action based on the discourse statemaintained by Collagen.
It also assignsinformation structure to output utterances so thatgestures can be appropriately generated.
Thesemantic representation of the action, includingverbal and non-verbal behaviors, is sent to theGeneration Module which generates surfacelinguistic expressions and gestures, including aset of instructions to achieve synchronizationbetween animation and speech.
Theseinstructions are executed by a 3D animationrenderer and a text-to-speech system.
Table 5.1shows the associations between discourse andconversational state that Rea is currently able tohandle.
In other work we have discussed howRea deals with the association betweeninformation structure and gesture [6].
In thefollowing sections, we focus on Rea?sgeneration of posture shifts.Table 5.1:Discourse functions & non-verbalbehavior cuesDiscourselevel info.Functions non-verbalbehavior cuesDiscoursestructurenew segment Posture_shiftturn giving eye_gaze &(stop_gesturinghand_gesture)turn keeping (look_awaykeep_gesture)Conversationstructureturn taking eye_gaze &posture_shiftInformationstructureemphasizeinformationeye_gaze &beat_andother_hand_gstsDSEGmidendintra inter87654321startUnderstandingModuleDialogueManager(Collagen)ReactionModule (RM)AnimationRendererText toSpeechSpeechRecognitionVisionProcessingMicrophone CameraAnimation SpeechGeneration ModuleSentenceRealizerGestureComponentFigure5.1: System architecture5.2 The Collagen dialogue managerCollagenTM is JAVA middleware for buildingCOLLAborative interface AGENts to work withusers on interface applications.
Collagen isdesigned with the capability to participate incollaboration and conversation, based on [12],[16].
Collagen updates the focus stack andrecipe tree using a combination of the discourseinterpretation algorithm of [16] and planrecognition algorithms of [14].
It takes as inputuser and system utterances and interface actions,and accesses a library of recipes describingactions in the domain.
After updating thediscourse state, Collagen makes three resourcesavailable to the interface agent: focus ofattention (using the focus stack), segmentedinteraction history (of completed segments) andan agenda of next possible actions created fromthe focus stack and recipe tree.5.3 Output GenerationThe Reaction Module works as a contentplanner in the Rea architecture, and also playsthe role of an interface agent in Collagen.
It hasaccess to the discourse state and the agendausing APIs provided by Collagen.
Based on theresults reported above, we describe here howRea plans her next nonverbal actions using theresources that Collagen maintains.The empirical study revealed that posture shiftsare distributed with respect to discourse segmentand turn boundaries, and that the form of aposture shift differs according to these co-determinants.
Therefore, generation of postureshifts in Rea is determined according to thesetwo factors, with Collagen contributinginformation about current discourse state.5.3.1 Discourse structure informationAny posture shift that occurs between the end ofone discourse segment and the beginning of thenext is defined as an inter-discourse segmentposture shift.
In order to elaborate differentgeneration rules for inter- vs. intra-discoursesegments, Rea judges (D1) whether the nextutterance starts a new topic, or contributes to thecurrent discourse purpose, (D2) whether thenext utterance is expected to finish a segment.First, (D1) is calculated by referring to the focusstack and agenda.
In planning a next action, Reaaccesses the goal agenda in Collagen and getsthe content of her next utterance.
She alsoaccesses the focus stack and gets the currentdiscourse purpose that is shared between her andthe user.
By comparing the current purpose andthe purpose of her next utterance, Rea can judgewhether the her next utterance contributes to thecurrent discourse purpose or not.
For example, ifthe current discourse purpose is to find a houseto show the user (FindHouse), and the nextutterance that Rea plans to say is as follows,(1) (Ask.What (agent Propose.What (user FindHouse<city ?>)))Rea says: "What kind of transportation access do youneed?
"then Rea uses Collagen APIs to compare thecurrent discourse purpose (FindHouse) to thepurpose of utterance (1).
The purpose of thisutterance is to ask the value of the transportationparameter of FindHouse.
Thus, Rea judges thatthis utterance contributes to the currentdiscourse purpose, and continues the samediscourse segment (D1 = continue).
On theother hand, if Rea?s next utterance is aboutshowing a house,(2) (Propose.Should (agent ShowHouse (joint123ElmStreet))Rea says: "Let's look at 123 Elm Street.
"then this utterance does not directly contributeto the current discourse purpose because it doesnot ask a parameter of FindHouse, and itintroduces a new discourse purpose ShowHouse.In this case, Rea judges that there is a discoursesegment boundary between the previousutterance and the next one (D1 = topic change).In order to calculate (D2), Rea looks at the plantree in Collagen, and judges whether the nextutterance addresses the last goal in the currentdiscourse purpose.
If it is the case, Rea expectsto finish the current discourse segment by thenext utterance (D1 = finish topic).
As forconversational structure, Rea needs to know;(T1) whether Rea is taking a new turn with thenext utterance, or keeping her current turn forthe next utterance, (T2) whether Rea?s nextutterance requires that the user respond.First, (T1) is judged by referring to the dialoguehistory1.
The dialogue history stores both systemutterances and user utterances that occurred inthe dialogue.
In the history, each utterance isstored as a logical form based on an artificialdiscourse language [20].
As shown above inutterance (1), the first argument of the actionindicates the speaker of the utterance; in thisexample, it is ?agent?.
The turn boundary can beestimated by comparing the speaker of theprevious utterance with the speaker of the nextutterance.
If the speaker of the previousutterance is not Rea, there is a turn boundarybefore the next utterance (T1 = take turn).
If thespeaker of the previous utterance is Rea, thatmeans that Rea will keep the same turn for thenext utterance (T1 = keep turn).Second, (T2) is judged by looking at the type ofRea?s next utterance.
For example, when Reaasks a question, as in utterance (1), Rea expectsthe user to answer the question.
In this case, Reamust convey to the user that the system gives upthe turn (T2 = give up turn).5.3.2 Deciding and selecting a posture shiftCombining information about discoursestructure (D1, D2) and conversation structure(T1, T2), the system decides on posture shifts1We currently maintain a dialogue history in Rea eventhough Collagen has one as well.
This is in order to storeand manipulate the information to generate hand gesturesand assign intonational accents.
This information will beintegrated into Collagen in the near future.for the beginning of the utterance and the end ofthe utterance.
Rea decides to do or not to do aposture shift by calling a probabilistic functionthat looks up the probabilities in Table 5.3.1.A posture shift for the beginning of the utteranceis decided based on the combination of (D1) and(T1).
For example, if the combined factorsmatch Case (a), the system decides to generate aposture shift with 54% probability for thebeginning of the utterance.
Note that in Case(d), that is, Rea keeps the turn without changinga topic, we cannot calculate a per intervalposture shift rate.
Instead, we use a posture shiftrate normalized for time.
This rate is used in theGenerationModule, which calculates theutterance duration and generates a posture shiftduring the utterance based on this posture shiftrate.
On the other hand, ending posture shiftsare decided based on the combination of (D2)and (T2).For example, if the combined factors matchCase (e), the system decides to generate aposture shift with 0.04% probability for theending of the utterance.
When Rea does decideto activate a posture shift, she then needs tochoose which posture shift to perform.
Ourempirical data indicates that the energy level ofthe posture shift differs depending on whetherthere is a discourse segment boundary or not.Moreover the duration of a posture shift differsdepending on the place in a turn: start-, mid-, orend-turn.Posture shift selection Place of aposture shift CaseDiscoursestructureinformationConversationstructureinformationPosture shiftdecisionprobability energy duration body partatopicchange  take turn 0.54/int high defaultupper &lowerb topicchange keep turn 0 - - -c continue take turn 0.13/int low default upper or lowerbeginning ofthe utterancedD1continueT1keep turn 0.14/sec low short lowerefinishtopic give turn 0.04/int high long lower End of theutterancefD2continueT2give turn 0.11/int low default lowerTable 5.3.1:Posture Decision Probabilities for DialogueBased on these results, we define posture shiftselection rules for energy, duration, and bodypart.
The correspondence with discourseinformation is shown in Table 5.3.1.
Forexample, in Case (a), the system selects aposture shift with high energy, using both upperand lower body.
After deciding whether or notRea should shift posture and (if so) choosing akind of posture shift, Rea sends a command tothe Generation Module to generate a specifickind of posture shift within a specific timeduration.Posture shiftselectionCaseDiscoursestructureinformationPostureshiftdecisionprobability energyg change topic 0.84/int highhD1continue 0.04/sec lowPosture shifts for pseudo-monologues can bedecided using the same mechanism as that fordialogue, but omitting conversation structureinformation.
The probabilities are given intable Table 5.3.2.
For example, if Rea changesthe topic with her next utterance, a posture shiftis generated 84% of the time with high-energymotion.
In other cases, the system randomlygenerates low-energy posture shifts 0.04 timesper second.6.
ExampleFigure 6.1 shows a dialogue between Rea andthe user, and shows how Rea decides to generateposture shifts.
This dialogue consists of twomajor segments: finding a house (dialogue), andshowing a house (pseudo-monologue).
Based onthis task structure, we defined plan recipes forCollagen.
The first shared discourse purpose[goal: HaveConversation] is introduced by theuser before the example.
Then, in utterance (1),the user introduces the main part of theconversation [goal: FindHouse].The next goal in the agenda, [goal:IdentifyPreferredCity], should beaccomplished to identify a parameter value for[goal: FindHouse].
This goal directlycontributes to the current purpose, [goal:FindHouse].
This case is judged to be a turnboundary within a discourse segment (Case (c)),and Rea decides to generate a posture shift at thebeginning of the utterance with 13% probability.If Rea decides to shift posture she selects a lowenergy posture shift using either upper or lowerbody.
In addition to a posture shift at thebeginning of the utterance, Rea may also chooseto generate a posture shift to end the turn.
Asutterance (2) expects the user to take the turn,and continue to work on the same discoursepurpose, this is Case (f).
Thus, the systemgenerates an end utterance posture shift 11% ofthe time.
If generated, a low energy  postureshift is chosen.
If a beginning and/or endingposture shifts are generated, they are sent to theGM, which calculates the schedule of thesemultimodal events and generates them.In utterance (25), Rea introduces a newdiscourse purpose [goal : ShowHouse].
Rea,using a default rule, decides to take the initiativeon this goal.
At this point, Rea accesses thediscourse state and confirms that a new goal isabout to start.
Rea judges this case as adiscourse segment boundary and also a turnboundary (Case (a)).
Based on this information,Rea selects a high energy posture shift.
Anexample of Rea?s high energy posture shift isshown on the right in Figure 5.2.As a subdialogue of showing a house, in adiscourse purpose [goal : DiscussFeature], Reakeeps the turn and continues to describe thehouse.
We handle this type of interaction as apseudo-monologue.
Therefore, we can use tableTable 5.3.2 for deciding on posture shifts here.In utterance (27), Rea starts the discussion aboutthe house, and takes the initiative.
This is judgedas Case (g), and a high energy body motion isgenerated 84% of the time.Table 5.3.2: Posture Decision Probabilities: Monologue7.
Conclusion and Further workWe have demonstrated a clear relationshipbetween nonverbal behavior and discourse state,and shown how this finding can be incorporatedinto the generation of language and nonverbalbehaviors for an embodied conversational agent.Speakers produce posture shifts at 53% ofdiscourse segment boundaries, more frequentlythan they produce those shifts discoursesegment-internally, and with more motionenergy.
Furthermore, there is a relationshipbetween discourse structure and conversationalstructure such that when speakers initiate a newsegment at the same time as starting a turn (themost frequent case by far), they are more likelyto produce a posture shift; while when they enda discourse segment and a turn at the same time,their posture shifts last longer than when thesecategories do not co-occur.Although this paper reports results from alimited number of monologues and dialogues,the findings are promising.
In addition, theypoint the way to a number of future directions,both within the study of posture and discourse,and more generally within the study of non-verbal behaviors in computational linguistics.Figure 6.2: Rea demonstrating a low and high energyposture shiftFirst, given the relationship betweenconversational and information structure in [5],a natural next step is to examine the three-wayrelationship between discourse state,conversational structure (turns), and informationstructure (theme/rheme).
For the moment, wehave demonstrated that posture shifts may signalboundaries of units; do they also signal theinformation content of units?
Next, we need tolook at finer segmentations of the discourse, tosee whether larger and smaller discoursesegments are distinguished through non-verbalmeans.
Third, the question of listener posture isan important one.
We found that a number ofposture shifts were produced by the participantwho was not speaking.
More than half of theseshifts were produced at the same time as aspeaker shift, suggesting a kind of mirroring.
Inorder to interpret these data, however, a moresensitive notion of turn structure is required, asone must be ready to define when exactlyspeakers and listeners shift roles.
Also, ofcourse, evaluation of the importance of suchnonverbal behaviors to user interaction isessential.
In a user study of our earlier Gandalfsystem [4], users rated the agent's languageskills significantly higher under test conditionsin which Gandalf deployed conversationalbehaviors (gaze, head movement and limitedgesture) than when these behaviors weredisabled.
Such an evaluation is also necessaryfor the Rea-posture system.
But, moregenerally, we need to test whether generatingposture shifts of this sort actually serves as asignal to listeners, for example to initiative[Finding a house] < dialogue>(1)U: I?m looking for a house.
(2)R:  (c)   Where do you want to live?
(f)  (3)U: I like Boston.
(4)R:  (c) (d)  What kind of transportationaccess do you need?
(f)(5)U: I need T access.?.
(23)R:  (c) (d)  How much storage space doyou need?
(f)(24)U: I need to have a storage place in thebasement.
(25)R:  (a) (d)Let?s look at 123 Elm Street.
(f)(26)U: OK.[Discuss a feature of the house](27)R:  (g)  Let's discuss a feature of this place.
(28)R:  (h)  Notice the hardw ood flooring in theliving room.
(29)R:  (h)  Notice the jacuzzi.
(30)R:  (h) Notice the remodeled kitchen[Showing a house] <Pseudo-monologue>Figure 6.1: Example dialoguestructure in task and dialogue [8].
Theseevaluations form part of our future researchplans.8.
AcknowledgementsThis research was supported by MERL, FranceTelecom, AT&T, and the other generous sponsors ofthe MIT Media Lab.
Thanks to the other members ofthe Gesture and Narrative Language Group, inparticular Ian Gouldstone and Hannes Vilhj?lmsson.9.
REFERENCES[1] Andre, E., Rist, T., & Muller, J., Employing AImethods to control the behavior of animatedinterface agents, Applied Artificial Intelligence,vol.
13, pp.
415-448, 1999.
[2] Cassell, J., Bickmore, T., Billinghurst, M.,Campbell, L., Chang, K., Vilhjalmsson, H., &Yan, H., Embodiment in ConversationalInterfaces: Rea, Proc.
of CHI 99, Pittsburgh, PA,ACM, 1999.
[3] Cassell, J., Stone, M., & Yan, H., Coordinationand context-dependence in the generation ofembodied conversation, Proc.
INLG 2000,Mitzpe Ramon, Israel, 2000.
[4] Cassell, J. and Thorisson, K. R., The Power of aNod and a Glance: Envelope vs. EmotionalFeedback in Animated Conversational Agents,Applied Art.
Intell., vol.
13, pp.
519-538, 1999.
[5] Cassell, J., Torres, O., & Prevost, S., TurnTaking vs. Discourse Structure: How Best toModel Multimodal Conversation., in MachineConversations, Y. Wilks, Ed.
The Hague:Kluwer, 1999, pp.
143-154.
[6] Cassell, J., Vilhj?lmsson, H., & Bickmore, T.,BEAT: The Behavior Expression AnimationToolkit, Proc.
of SIGGRAPH, ACM Press,2001.
[7] Chovil, N., Discourse-Oriented Facial Displaysin Conversation, Research on Language andSocial Interaction, vol.
25, pp.
163-194, 1992.
[8] Chu-Carroll, J.
& Brown, M., Initiative inCollaborative Interactions - Its Cues and Effects,Proc.
of AAAI Spring 1997 Symp.
onComputational Models of Mixed Initiative,1997.
[9] Condon, W. S. & Osgton, W. D., Speech andbody motion synchrony of the speaker-hearer, inThe perception of language, D. Horton & J.Jenkins, Eds.
NY: Academic Press, 1971, pp.150-184.
[10] Duncan, S., On the structure of speaker-auditorinteraction during speaking turns, Language inSociety, vol.
3, pp.
161-180, 1974.
[11] Green, N., Carenini, G., Kerpedjiev, S., & Roth,S, A Media-Independent Content Language forIntegrated Text and Graphics Generation, Proc.of Workshop on Content Visualization andIntermedia Representations at COLING andACL '98, 1998.
[12] Grosz, B.
& Sidner, C., Attention, Intentions,and the Structure of Discourse, ComputationalLinguistics, vol.
12, pp.
175-204, 1986.
[13] Kendon, A., Some Relationships between BodyMotion and Speech, in Studies in DyadicCommunication, A. W. Siegman and B. Pope,Eds.
Elmsford, NY: Pergamon Press, 1972, pp.177-210.
[14] Lesh, N., Rich, C., & Sidner, C., Using PlanRecognition in Human-Computer Collaboration,Proc.
of the Conference on User Modelling,Banff, Canada, NY: Springer Wien, 1999.
[15] Lester, J., Towns, S., Callaway, C., Voerman, J.,& FitzGerald, P., Deictic and EmotiveCommunication in Animated PedagogicalAgents, in Embodied Conversational Agents, J.Cassell, J. Sullivan, et.
al, Eds.
Cambridge: MITPress, 2000.
[16] Lochbaum, K., A Collaborative Planning Modelof Intentional Structure, ComputationalLinguistics, vol.
24, pp.
525-572, 1998.
[17] McNeill, D., Hand and Mind: What GesturesReveal about Thought.
Chicago, IL/London,UK: The University of Chicago Press, 1992.
[18] Rich, C. & Sidner, C. L., COLLAGEN: ACollaboration Manager for Software InterfaceAgents, User Modeling and User-AdaptedInteraction, vol.
8, pp.
315-350, 1998.
[19] Rickel, J.
& Johnson, W. L., Task-OrientedCollaboration with Embodied Agents in VirtualWorlds, in Embodied Conversational Agents, J.Cassell, Ed.
Cambridge, MA: MIT Press, 2000.
[20] Sidner, C., An Artificial Discourse Language forCollaborative Negotiation, Proc.
of 12th Intnl.Conf.
on Artificial Intelligence (AAAI), Seattle,WA, MIT Press, 1994.
[21] Takeuchi, A.
& Nagao, K., Communicativefacial displays as a new conversational modality,Proc.
of InterCHI '93, Amsterdam, NL, ACM,1993.
[22] Thompson, L. and Massaro, D., Evaluation andIntegration of Speech and Pointing Gesturesduring Referential Understanding, Journal ofExperimental Child Psychology, vol.
42, pp.144-168, 1986.
