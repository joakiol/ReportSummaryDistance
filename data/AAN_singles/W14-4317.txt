Proceedings of the SIGDIAL 2014 Conference, pages 123?132,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsDialogue Act Modeling for Non-Visual Web AccessVikas AshokDept of Computer ScienceStony Brook UniversityStony Brook , New YorkYevgen BorodinCharmtech Labs LLCCEWIT SBU R & D ParkStony Brook , New YorkSvetlana StoyanchevAT&T Labs ResearchNew York City, New York(While at Columbia University)I V RamakrishnanCharmtech Labs LLCCEWIT SBU R & D ParkStony Brook , New Yorkvganjiguntea@cs.sunysb.edu, borodin@charmtechlabs.com,sstoyanchev@cs.columbia.edu, ram@charmtechlabs.comAbstractSpeech-enabled dialogue systems have thepotential to enhance the ease with whichblind individuals can interact with the Webbeyond what is possible with screen read-ers - the currently available assistive tech-nology which narrates the textual contenton the screen and provides shortcuts tonavigate the content.
In this paper, wepresent a dialogue act model towards de-veloping a speech enabled browsing sys-tem.
The model is based on the corpusdata that was collected in a wizard-of-ozstudy with 24 blind individuals who wereassigned a gamut of browsing tasks.
Thedevelopment of the model included exten-sive experiments with assorted feature setsand classifiers; the outcomes of the exper-iments and the analysis of the results arepresented.1 IntroductionThe Web is the ?go-to?
computing infrastructurefor participating in our fast-paced digital society.It has the potential to provide an even greater ben-efit to blind people who once required human as-sistance with many of their activities.
Accordingto the American Federation for the Blind, thereare 21.5 million Americans who have vision loss,of whom 1.5 million are computer users (AFB,2013).Blind users employ screen readers as the as-sistive technology to interact with digital con-tent (e.g.., JAWS (Freedom-Scientific, 2014) andVoiceOver (Apple-Inc., 2013)).
Screen readers se-rially narrate the content of the screen using text-to-speech engines and enable users to navigate inthe content using keyboard shortcuts and touch-screen gestures.Navigating content-rich web pages and con-ducting online transactions spanning multiplepages requires using shortcuts and this can getquite cumbersome and tedious.
Specifically, inonline shopping a user typically browses throughproduct categories, searches for products, addsproducts to cart, logs into his/her account, and fi-nally makes a payment.
All these steps requirescreen-reader users listen through a lot of content,fill forms, and find links and buttons that have to beselected to get through these steps.
If users do notwant to go through all content on the page, theyhave to remember and use a number of differentshortcuts.
Beginner users often use the ?Down?key to go through the page line by line, listeningto all content on the way (Borodin et al., 2010).Now suppose that blind users were to tell theweb browser what they wanted to accomplish andlet the browsing application automatically deter-mine what has to be clicked, fill out forms, helpfind products, answer questions, breeze throughcheckout, and wherever possible, relieve the userfrom doing all the mundane and tedious low-leveloperations such as clicking, typing, etc.
The abil-ity to carry out a dialogue with the web browser ata higher level has the potential to overcome thelimitations of shortcut-based screen reading andthus offers a richer and more productive user ex-perience for blind people.The first step toward building a dialogue-basedsystem is the understanding of what users couldsay and dialogue act modeling.
Although di-alogue act modeling is a well-researched topic(with details provided in related work - Section2), it has remained unexplored in the context ofweb accessibility for blind people.
The commer-cial speech-based applications have been aroundfor a while and new ones continue to emerge at arapid pace; however, these are mainly stand-alone(e.g.., Apple?s Siri) domain specific systems thatare not connected to web browsers, which pre-cludes dialogue-based interaction with the Web.Current spoken input modules integrated with web123browsers are limited to certain specific functional-ities such as search (e.g.., Google?s voice search)or are used as a measure of last resort (e.g.., Sirisearching for terms online).In this paper, we made a principal step towardsbuilding a dialogue-based assistive web browsingsystem for blind people; specifically, we built adialogue act model for non-visual access to theWeb.
The contributions of this paper include:1) a unique dialogue corpus for non-visual web ac-cess, collected during the wizard-of-oz user studyconducted with 24 blind participants (Section 3);2) the design of a suitable dialogue act scheme(Section 3); 3) experimentation with classifiers ca-pable of identifying the dialogue acts associatedwith utterances based on combinations of lexi-cal/syntactic, contextual, and task-related featuresets (Section 4); 4) investigation of the impor-tance of each feature set with respect to classifi-cation performance to assess whether simple lex-ical/syntactic features are sufficient for obtainingan acceptable performance (Section 5).2 Related WorkWhile previous research addressed spoken dia-logue interfaces for a domain-specific websites,such as news or movie search (Ferreras andCarde?noso-Payo, 2005; Wang et al., 2014), dia-logue interface to generic web sites is a novel task.Spoken dialogue systems (SDS) can be classifiedby the type of initiative: system, user, or mixedinitiative (Lee et al., 2010).
In a system-initiativeSDS, a system guides a user through a series ofinformation gathering and information presentingprompts.
In a user-initiative system, a user caninitiate and steer the interaction.
Mixed-initiativesystems allow both system and user-initiated ac-tions.Dialogue systems also differ in the types of di-alogue manager: finite state based, form based,or agent based (Lee et al., 2010), (Chotimongkol,2008).
Finite state and form filling systems areusually system-initiative.
These systems have afixed set of dialogue states and finite set of possi-ble user commands that map to system actions.
Incontrast, a speech-enabled browsing system pro-posed in this work is an agent-based system.
Theset of actions of this system correspond to user ac-tions during web browsing.
The domain of possi-ble user commands at each point of the dialoguedepends on the current web page that is viewed bya user.
The dialogue state in a voice browsing sys-tem is compiled at run-time as the user can visitany web page.While a users dialogue acts in a form-basedor finite state system depends primarily on a di-alogue state, in an agent-based system with user-initiative, the space of users dialogue acts at eachdialogue state is open.
To determine dialoguemanager action, it is essential for the system toidentify users intent or dialogue act.
In thiswork, we address dialogue act modelling for open-domain voice web browsing as a proof of conceptfor the system.Dialogue act (DA) annotation schemes for spo-ken dialogue systems follow theories on speechacts originally developed by Searle (1975).
Anumber of DA annotation schemes have been de-veloped previously (Core and Allen, 1997), (Car-letta et al., 1997).
Several of dialogue taggingschemes strive to provide domain-independence(Core and Allen, 1997), (Bunt, 2011).Bunt (2011) developed a NIST standardizeddomain-independent annotation scheme which in-corporates elements from the previously devel-oped annotation schemes.
It is a hierarchicalmulti-dimensional annotation scheme.
Each func-tional segment (part of an utterance correspond-ing to a DA) can have a general purpose function,such as Inform, Propositional Question, Yes/NoQuestion, and a dimension-specific function in anynumber of 10 defined dimensions, such as Task,Feedback, or Time management.In the analysis of human-computer dialogues, itis common to adopt DA annotation schemes to suitspecific domains.
Generic domain-independentschemes are geared towards the analysis of nat-ural human-human dialogue and provide rich an-notation structure that can cover complexity ofnatural dialogue.
Domain-specific dialogues usea subset of the generic dialogue structure.
Forexample, Ohtake et al.
(2009) developed a DAscheme for tourist-guide domain motivated by ageneric annotation scheme (Ohtake et al., 2010),and Bangalore and Stent (2009) created a dialoguescheme for a catalogue product ordering dialoguesystem.
In our work we design DA scheme forWeb-Browsing domain motivated by the DAMSL(Core and Allen, 1997) schema for task-orienteddialogue.We used a Wizard-of-Oz (WOZ) approach tocollect an initial dataset of spoken voice com-124Task ?u?dShopping 121 16Email 92 16Flight 180 16Hotel 179 16Job 76 16Admission 144 16Overall 792 96Table 1: Corpus details.
?u- number of utterances,?d- number of dialogs.mands by both blind and sighted users.
WOZ iscommonly used before building a dialogue system(Chotimongkol, 2008), (Ohtake et al., 2009), (Es-kenazi et al., 1999).In previous work on dialogue modelling, Stol-cke et al.
(2000) used HMM approach to predictdialogue acts in a switchboard human-human di-alogue corpus achieving 65% accuracy.
Rangara-jan Sridhar et al.
(2009) applied a maximum en-tropy classifier on the Switchbord corpus.
Usinga combination of lexical, syntactic, and prosodicfeatures, the authors achieve accuracy of 72%on that corpus.
Following the work of Rangara-jan Sridhar et al.
(2009), we use supervised classi-fication approach to determine dialogue act on theannotated corpus of human-wizard web-browsingdialogues.3 Corpus and AnnotationIn this section, we describe the corpus and theassociated dialogue act scheme.
The corpus wascollected using a WOZ user study with 24 blindparticipants.
Exactly 50% of the participants indi-cated that they were very comfortable with screenreaders, while the remaining 50% said they werenot comfortable with computers.
We will refer tothem as ?experts?
and ?beginners?
respectively.The study required each participant to completea set of typical web browsing tasks (shopping,sending an email, booking a flight, reserving a ho-tel room, searching for a job and applying for uni-versity admission) using unrestricted speech com-mands ranging from simple commands such as?click the search button?, to complex commandssuch as ?buy this product?.
Unknown to the partic-ipants, these commands were executed by a wiz-ard and appropriate responses were narrated usinga screen reader.
The dialogs were effective; al-most every participant was able to complete eachassigned task by engaging in a dialogue with thewizarded interface.As shown in Table 1, the corpus consists of atotal of 96 dialogs collected during the executionof 6 tasks and captures approximately 22 hours ofspeech with a total of 792 user utterances and 774system utterances.
There is exactly 1 dialogue pertask for any given participant.
Each user turn con-sists of a single command that is usually a sim-ple sentence or phrase.
Each system turn is eithernarration of webpage content or information re-quest for the purpose of either form filling or dis-ambiguation.
Therefore, each dialogue turn wastreated as a single utterance and every utterancewas identified with a single associated dialogueact.The corpus was manually annotated with dia-logue act labels and the labeling scheme was ver-ified by measuring the inter-annotator agreement.The rest of this section describes the annotationscheme.3.1 Dialogue Act AnnotationThe dialogue act annotation scheme was inspiredby the DAMSL scheme (Core and Allen, 1997)for task oriented dialogue.
The proposed schemewas also influenced by extended DAMSL tagset(Stolcke et al., 2000) and the DIT++ annotationscheme (Bunt, 2011).
We customized the annota-tion scheme to suit the non-visual web access do-main, thereby making it more relevant to our cor-pus and tasks.Table 2 lists the dialogue acts for both userand system utterances.
The user dialogue acttagset consists of labels representing task relatedrequests (Command-Intention, Command-Task,Command-Multiple, Command-Navigation), in-quiries (Question-Task, Help-Task) and informa-tion input (Information-Task), whereas the systemDA tagset contains labels representing informa-tion requests (Prompt), answers to user inquiries(Question-Answer, Help-Response) and other sys-tem responses (Short-Response, Long-Response,etc.)
to user commands.Inter-rater agreement values for different tasksin the corpus are presented in Table 3.
The ?
val-ues for all tasks are above 0.80, which accordingto Fleiss?
guidelines (Fleiss, 1973), indicates ex-cellent inter-rater reliability on the DA annotation.Therefore, the DA tagset is generic enough to beapplicable for a wide varity of tasks that can beperformed on the web.
Note that the dialogue actscheme was specially designed for non-visual web125User dialogue ActsDialogue Act Description FrequencyCommand-Intention Indication of user?s intention or end goal, e.g.
I wish to buy a Bluetooth speaker 0.117Command-Task Basic action commands like click, select, enter, etc.
0.072Command-Multiple Complex commands requiring an execution plan comprising a sequence of basiccommands, e.g.
buy this product, book this room, etc.0.162Command-Navigation Commands directing the movement of cursor like go to, stop, next etc.
0.136Information-Task Information required for completing a task, e.g.
departure date/return date in-formation for flight booking task, first name, phone number, etc.0.442Question-Task Task specific questions like What is the cheapest flight?, What is the basicsalary?, etc.0.041Self-Talk Utterances not directed towards the system, e.g.
hmmm, what should I do next?
0.002Help-Task Request for help when the user wishes to speak with the experimenter, e.g.
Help,what does that mean?0.024System dialogue Actsdialogue Act Description FrequencyPrompt Request for information from user to complete a task, e.g.
First Name, text boxblank0.460Short-Response A short response to a user command, e.g.
description of product, brief details offlight, acknowledgements, etc.0.198Long-Response A lengthy response to a user command, e.g.
Narration of entire page, list ofsearch results, etc.0.120Keyboard-Response Response to user keyboard actions 0.072Article-Response Narration of an article 0.034Question-Answer Response to a user question regarding task (non-help) 0.044No-Response No response for some navigation commands like Stop 0.041Help-Response Response to a help request from the user 0.026Table 2: dialogue acts for non-visual Web accessaccess.
Insofar as sighted people are concerned,a more elaborate scheme would be required sincetheir utterances are dominated by visual cues, afact that was confirmed by a parallel user studywith sighted participants on the same set of webtasks that were used in the wizard-of-oz study.4 FeaturesThis section describes the different feature setsthat we experimented with for our classificationtasks.
The vector representation for training theDA classifiers integrates several types of features(Table 4): unigrams (U ) and syntactic features(S), context related features (C), task related fea-tures (T ), presence of words anywhere in anutterance(P) and presence of words at the begin-ning of an utterance(B).
The last two feature setsare similar to the ones used in Boyer et al.
(2010).Task ?Shopping 0.865Email 0.829Flight 0.894Hotel 0.848Job 0.824Admission 0.800Table 3: Inter-rater agreement measured in termsof Cohen?s ?
for all tasks in the corpus.The feature sets C, P , B and S are specific tothe domain of non-visual web access and werehand-crafted based on the following three factors:knowledge of the browsing behavior of blind usersreported in previous studies, e.g.
(Borodin et al.,2010); manual analysis of the corpus; mitigate theeffect of noise that is usually present in standardlexical/syntactic feature sets such as n-grams andparse tree rules.
Each of the features in C, P , Band S were crafted to have a close correspondenceto some dialogue act.
For example, pnavis closelytied to the Command-Navigation dialogue act.4.1 UnigramsUnigrams (U in Table 4) are one of the commonlyused lexical features for training dialogue act clas-sifiers (e.g.
(Boyer et al., 2010), (Stolcke et al.,2000), (Rangarajan Sridhar et al., 2009)).
Encod-ing unigrams as features is based on the obser-vation that some words appear more frequentlyin certain dialogue acts compared to other di-alogue acts.
For example, approximately 73%of ?want?
occur in the Command-Intention DA,100% of ?skip?
occur in the Command-NavigationDA and approximately 92% of ?select?
occurin the Command-Task DA.
Word-DA correctionscan also be automatically identified using SVMclassifers trained on unigram features.
Table 5126Overall Feature SetUNIGRAMS (U )Feature Description Binaryu Unigrams NPRESENCE OF WORDS IN COMMANDS (P )piyouThe utterance contains either I or you YphelpThe utterance contains the word help YphelpqThe utterance contains words usually associated with help requests.
E.g., how, am I, etc.
YpprevThe immediately preceding system DA is Prompt and the utterance contains words alsopresent in this immediately preceding system utteranceYpintentThe utterance contains words , need, desire, prefer, like and their synonyms YpbrowserThe utterance contains words also present in the web browser tab title.
E.g., email, job YphtmlThe utterance contains references to HTML elements.
E.g., form, box, link, page, etc.
YpbasicThe utterance contains a verb representing basic operations on a web page.
E.g., click, edit.
YpnbasicThe utterance contains a verb not related to basic web page operations; a verb usuallyassociated with task or domain related actions.
E.g.
send, open, compose, etc.YpnavThe utterance contains words related to cursor movement.
E.g., go to, continue, next, etc.
YpquestionThe utterance contains words usually associated with questions.
E.g., what, when, why YSYNTACTIC STRUCTURE OF COMMANDS (S)snpThe utterance is a noun phrase with atleast two words YsnounThe utterance consists of a single noun YsbasicThe utterance consists of a single verb representing basic web page operations.
E.g., click,edit, erase, select, etc.YsnbasicThe utterance consists of a single verb representing task or domain related actions.
e.g.send, open, compose, order, etc.YCONTEXT RELATED FEATURES (C)cfirstThe utterance is the first command to be issued when a new website is loaded in the browser Ycpreviousdialogue act of the immediately preceding system utterance NPOSITION OF WORDS IN COMMANDS (B)bnavThe utterance begins with word(s) related to cursor movement.
e.g.
go to, continue, etc.
YbquestionThe utterance begins with a word that is usually associated with a question.
E.g., what,when, where, why, etc.YbiThe utterance begins with the personal pronoun I. YbhelpqThe utterance begins with word(s) usually associated with help requests.
E.g., how, am I YTASK RELATED FEATURES (T )tnameName of the task associated with the utterance NTable 4: Feature set for user dialogue act classification.
The complete list of words associated with eachfeature in P and B is provided in Appendix A.presents few such correlations.
Note that some ofthe words in Table 5 are task-specific (noise); aconsequence of using a small dataset.4.2 Presence of Words in CommandsIn constract to unigram features that take intoaccount all possible word-DA correlations, thepresence-of-word features (P in Table 4) are lim-ited to certain specific words that have strong cor-relations with the DA types.
For each featurep ?
P , if the presence of certain specific wordsassociated with p occur in an utterance, then p isset to true.
The set of words for every p that cor-responds to some dialogue act d was contructedby determining the discriminatory words for d us-ing simple statistical analysis of the corpus (e.g.relative frequencies of words) as well as by an ex-amination of the weights of different words learntby the SVM classifier trained on a developmentdataset using unigram features alone.
e.g.., thewords continue and skip occur much more fre-quently in Command-Navigation than in other di-alogue acts (see Table 5) and hence are includedin pnav.
Note however that not all discrimina-tory words in Table 5 were used.
Only genericwords, independent of any specific task, were se-lected (see Appendix A for details).4.3 Syntactic Structure of CommandsThe binary syntactic features (S in Table 4) wereautomatically extracted using the Stanford parser(Klein and Manning, 2003).
As in word-DAcorrelations, some of the syntactic structure-DAcorrelations were also identified by a manual in-127Dialogue Act Discriminatory WordsCommand-Intention want, compose, book, for, look, email, find, an, ac-counting, Stanford, a, airplane, message, I, music,get, ticket, positions, need, bluetooth, jobs, newCommand-Task repeat, choose, delete, select, link, edit, enter,erase, clear, fill, in, click, third, at, body, box,again, blue, thatCommand-Multiple play, read, senior, send, reviews, Harlem, artists,study, submit, details, law, description, Kitaro,mornings, availability, apply, construction, pay,reservations, proceed, it, this, availableComand-Navigation skip, next, previous, go, page, finish, stop, item,continue, back, line, before, box, first, second, to,top, home, part, wouldInformation-Task JFK, customer, no, August, July, USA, October,Kahalui, October30th, anytime, coach, today, non-stop, movies, YorkQuestion-Task price, time, fare, layover, times, is, what?s, any-thing, cheaper, best, flight, airline, complete, one-stop, departure, cards, price, much, cost, weekly.Help-Task help, do, mean, does, say, can, supposed, some-thing, how, use, voice, have, apply, reservation, by,address, give, getTable 5: Top discriminative unigrams based onweights from SVM classifier.vestigation of the corpus.
For example, 82.1%of single noun-only utterrances (snoun) have theDA Information-Task, 76.2% of ?basic?
verb-onlyutterances (sbasic) have the DA Command-Taskand 83.3% of ?non-basic?
verb-only utterances(snbasic) have the DA Command-Multiple.4.4 Context Related FeaturesThe local context (C in Table 4) provides valuablecues to identify the dialogue act associated witha user utterance.
It was observed during the studythat user utterance is influenced to a large extent bythe immediately preceding system utterance.
Forexample, 89.95% of all user utterances immedi-ately following the system Prompt were observedto be Information-Task.
In addition, most of thetime (probability 87.5%), the first utterance issuedfor a task was Command-Intention.4.5 Position-of-Word in CommandsDesign of feature set B in Table 4 was inspired byan analysis of the corpus which revealed that cer-tain dialogue acts are characterized by the pres-ence of certain words at the beginning of the cor-responding utterances.
For example, 93.4% ofall Command-Navigation utterances begin with acursor-movement related word (e.g.
next, previ-ous, etc.
see Appendix A for the complete list).4.6 Task Related FeaturesSince it is possible for different tasks to exhibit dif-ferent feature vector patterns for the same dialogueact, incorporating task name (T in Table 4) as anadditional feature may therefore improve classifi-Group CompositionG1 UG2 P ?
B ?
SG3 C ?
B ?
SG4 C ?
P ?
SG5 C ?
P ?
BG6 C ?
P ?
B ?
SG7 C ?
P ?
B ?
S ?
TG8 C ?
P ?
B ?
S ?
UTable 6: Feature groups.cation performance by exploiting these variations(if any) between tasks.5 Classification ResultsAll classification tasks were performed using theWEKA toolkit (Hall et al., 2009).
The classifica-tion experiments were done using Support VectorMachine (frequently used for benchmarking), J48Decision Tree (appropriate for a small size mostlybinary feature set) and Random Forest classifiers.The model parameters for all classifiers were opti-mized for maximum performance.In addition, experiments were also performedto assess the utility of each feature set (Table 4).Specifically, the performance of classifiers withdifferent combinations (Groups 1-8 in Table 6) offeature sets was evaluated to assess the importanceof each individual feature set.
We primarily fo-cussed on domain-specific feature sets (P , B, Cand S).
Observe that group G6 differs from anyof G2 ?
G5 by exactly one feature set.
This letsus to assess the individual utility of P , B, C andS .
In addition, we also extended G6 by includingU (G7) and T (G8) to determine if there was anynoticeable improvement in performance.
G1 withonly unigram features serves as a baseline.
All re-ported results (Table 7) are based on 5-fold crossvalidation: 632 instances for training and 158 in-stances for testing.
Table 7 presents the classifica-tion results for different feature groups.
The DASelf-Talk was excluded from classification due toinsufficient number (2) of data points.5.1 Classification PerformanceOverall Performance: As seen in Table 7, thetree-based classifiers (J48 and RF) performed bet-ter than SVM in a majority of the feature groups(6 out of 8).
The random forest classifier yieldedthe best performance (91% Precision, 90% Recall)for feature group G6, whereas the G3-SVM com-bination had the lowest performance (69% Preci-sion, 67% Recall).
However, all groups includ-128Performance of Feature GroupsG1 G2 G3 G4 G5 G6 G7 G8DA MODEL P R P R P R P R P R P R P R P RCISVM .83 .80 .84 .95 .71 .95 .91 .96 .82 .90 .91 .95 .89 .96 .89 .94J48 .74 .74 .83 .90 .80 .93 .84 .95 .81 .93 .83 .95 .85 .93 .91 .95RF .76 .74 .81 .90 .85 .94 .88 .90 .80 .87 .84 .93 .88 .89 .87 .95CTSVM .87 .73 .86 .81 .93 .30 .89 .87 .84 .81 .89 .83 .89 .81 .92 .88J48 .80 .64 .80 .70 1.0 .28 .88 .79 .80 .70 .85 .75 .83 .87 .86 .67RF .72 .58 .84 .89 .81 .26 .88 .89 .85 .85 .79 .93 .77 .78 .88 .80CMSVM .73 .65 .77 .58 .36 .30 .78 .64 .78 .59 .78 .64 .80 .62 .79 .78J48 .74 .36 .78 .79 .68 .87 .83 .59 .81 .78 .76 .83 .81 .80 .76 .87RF .79 .56 .80 .81 .68 .83 .80 .59 .82 .79 .81 .83 .80 .82 .76 .89CNSVM .89 .84 .93 .87 .96 .82 .67 .96 .94 .87 .96 .89 .94 .87 .90 .92J48 .89 .65 .95 .95 .96 .92 .65 .93 .95 .95 .95 .92 .92 .93 .87 .90RF .82 .86 .94 .94 .95 .92 .66 .95 .95 .95 .95 .95 .94 .93 .91 .88ITSVM .70 .89 .82 .93 .70 .81 .81 .79 .82 .93 .82 .93 .82 .94 .85 .90J48 .54 .93 .96 .97 .94 .97 .80 .82 .96 .97 .97 .96 .96 .97 .94 .94RF .65 .93 .98 .98 .95 .97 .81 .82 .97 .98 .98 .97 .98 .98 .97 .92QTSVM .66 .46 .87 .27 .90 .30 .80 .30 .62 .31 .80 .31 .70 .33 .85 .49J48 .44 .36 .62 .33 .80 .23 .90 .30 .53 .34 .62 .31 .56 .47 .93 .32RF .63 .36 .65 .31 .61 .39 .78 .27 .54 .35 .83 .39 .68 .51 .87 .33HTSVM .77 .71 .73 .65 .80 .45 .79 .63 .63 .67 .78 .63 .72 .64 .92 .76J48 .86 .79 .80 .57 .80 .33 .81 .60 .70 .50 .81 .55 .55 .52 .93 .91RF .85 .70 .79 .65 .78 .33 .75 .60 .74 .67 .90 .48 .67 .67 .90 .80OverallSVM .77 .76 .83 .82 .69 .67 .80 .79 .82 .82 .84 .83 .84 .83 .85 .85J48 .70 .66 .88 .88 .87 .85 .80 .78 .88 .88 .89 .88 .88 .89 .87 .86RF .74 .73 .90 .90 .86 .85 .80 .79 .89 .89 .91 .90 .90 .89 .88 .87Table 7: Classification Results.
The overall performance is the weighted average over all dialogue acts.Notation: J48-Decision Tree, RF-Random Forest, SVM-Support Vector Machine, P-Precision, R-Recall,CI-Command-Intention, CT-Command-Task, CM-Command-Multiple, CN-Command-Navigation, IT-Information-Task, QT-Question-Task, HT-Help-Task.
The best performances for each DA are high-lighted in bold.ing G3 did better than G1 with tree-based clas-sifiers.
G1 was consistently outperformed by theother groups.Performance on dialogue acts: In 6/8 featuregroups, the performance of SVM with respect toIT dialogue act was significantly worse than thatof tree-based classifiers.
However, SVM producedconsistently good results (> 80% in most cases)for the CI and CT dialogue acts.
All classifiersperformed very well in case of CN dialogue act(> 80% for 7/8 groups).
However, none of theclassifiers performed well in case of QT.5.2 Importance of feature setsFrom Table 7, it can be inferred that contextualfeatures (C) do not contribute to improving overallclassification performance.
In particular, for eachclassifier, the difference in overall performancebetween groups G2 (excluding C) and G6 (includ-ing C) is very small (worst case: 1% differencein both P and R).
However, inclusion of C signifi-cantly improved the classification performance ofRF for QT and CI dialogue acts (18% improve-ment in P, 8% improvement in R for QT, 3% im-provement in both P and R for CI).
Even in case ofJ48, where group G6 yields the best performance,Dialogue Act Discriminatory RulesCommand-Intention?
cfirst?
?bnav?
?phtml?
?snoun?
cfirst?
?bnav?
phtml?
piyou?
?cfirst??bnav?pintent??pnav??pquestionCommand-Task?
?cfirst??bnav??pintent??phelpq?pbasic??pnbasic?
?cfirst??bnav??pintent??phelpq?pbasic?pnbasic?
phtmlCommand-Multiple?
?cfirst??bnav??pintent??phelpq??pbasic??pnbasic?
cprevious= [h|k|l|n] ?
?phtml??pquestion?
?cfirst??bnav??pintent??phelpq??pbasic?pnbasic?
cprevious= [?p]Comand-Navigation?
cfirst?
bnav?
cfirst?
?bnav?
phtml?
?piyou?
?cfirst?
bnav?
?snp?
?cfirst?
bnav?
snp?
cprevious= [s|a]Information-Task?
?cfirst??bnav??pintent??phelpq??pbasic??pnbasic?
cprevious= [p]?
?cfirst??bnav??pintent??phelpq??pbasic?pnbasic?
cprevious= [p] ?
?piyouQuestion-Task?
?cfirst??bnav??pintent??phelpq??pbasic??pnbasic?
cprevious= [h|k|l|n] ?
?phtml?pquestion?
?cfirst??bnav??pintent??phelpq??pbasic?
?pnbasic?cprevious= [q|s|a]??pnav??phtml?
?snounHelp-Task ?
?cfirst??bnav??pintent?phelpq?piyou?
?biTable 8: A select sample of J48 rules (conf ?0.75 and descending order of support) for groupG6.
Notation: ?cfirststands for cfirst= falseand cfirststands for cfirst= true.129Utterance Actual DA Predicted DA Comments?Continue to booking it?
Command-Multiple Command-NavigationThis utterance was issued while performing the book a hotel room task.
Thiscommand essentially is the same as ?book it?.
The presence of a navigationrelated verb continue at the beginning caused the classifiers to incorrectly classifyit as Command-Navigation.
?I am looking to check inon July 23rd?Information-Task Command-IntentionThis utterance was in response to a system prompt for check-in date while per-forming the book a hotel room task.
The presence of first person nominativepronoun ?I?
caused the classifiers to categorize it as Command-Intention.
?What does that mean??
Help-Task Question-TaskThis utterance was directed towards the experimenter and therefore it was anno-tated as Help-Task.
However, the absence of the keyword help and the presenceof a Wh-word what at the beginning of the command caused the classifiers toincorrectly classify this command as Question-Task.
?Best available price??
?Ok, return time???Price???Layover?
?Question-TaskCommand-MultipleInformationThe absence of Question related words like Wh-words, is, etc.
at the beginningcoupled with the fact that these commands are noun phrases caused the classifiersto incorrectly classify them as either Command-Multiple or Information.Table 9: A few incorrectly classified utterances.contextual features were found to be a componentof some of the high-confidence, high-support J48rules (Table 8) for CI and QT.
Similar claims canalso be made for syntactic features(S), where al-though there is not much difference in overall per-formance between groups G5 and G6 (Worst Case:2% drop in P, 1% drop in R), improvements wereobserved in case of RF for QT and CI dialogueacts (29% improvement in P, 4% improvement inR for QT, 4% improvement in P, 6% improvementin R for CI).Excluding either word-existential features (P)or word-position related features (B), however,caused a significant drop in overall performance(Worst case: 15% drop in P, 16% drop in R with-out P , 11% drop in both P and R without B).
Ta-ble 8 further highlights the importance of featureset P , since over 50% of the high performing J48rules (Table 8) have at least one feature of type Pwith true as their truth values.It can be seen in Table 7 that adding either un-igrams or task-name to the existing feature set ofG6 does not affect the overall performance.
How-ever, the use of unigram features improved re-sults of all the classifiers for the HT DA.
No suchDA specific improvements were seen with task-name as an added feature to G6.
This suggeststhat the feature values of G6 for all DAs are task-independent.5.3 Prediction ErrorsIt is clear from Table 7 that the prediction accu-racies of CM, QT and HT are not nearly as goodas those of other dialogue acts.
Table 9 providessome insights into this issue via illustrative exam-ples from the corpus.Notice that the errors in case of CI, CM and HTare mostly related to choice of words used in theutterances, whereas mistakes in the prediction ofQT are mainly due to inadequate information orthe incompleteness of the utterances.
Therefore, itis recommended that the speech enabled web dia-logue systems enforce a constraint requiring usersto express their complete thoughts in each of theircorresponding utterances.6 ConclusionExperiments with the dialogue act model de-scribed in the paper indicate that with a small setof simple lexical/syntactic features it is possibleto achieve a high overall dialogue act recogni-tion accuracy (over 90% precision and recall) us-ing simple and well-known tree-based classifierssuch as decision trees and random forests.
It ishence possible to build speech-enabled dialogue-based assistive web browsing systems with lowcomputational overhead that, inturn, can result inlow latency response times - a critical requirementfrom a usability perspective for blind users.
Fi-nally, a dialogue model for non-visual web access,such as the one described in this paper, can be thekey driver of goal-oriented web browsing - a nextgeneration assistive technology that will empowerblind users to stay focused on high-level browsingtasks, while the system does all of the low-leveloperations such as clicking on links, filling forms,etc., necessary to accomplish the tasks.AcknowledgementsResearch reported in this publication was sup-ported by the National Eye Institute of the Na-tional Institutes of Health under award number1R43EY21962-1A1.
We would like to thankLighthouse Guild International and Dr. WilliamSeiple in particular for helping conduct user stud-ies.130ReferencesAFB.
2013.
Facts and figures on american adultswith vision loss.
http://www.afb.org/info/blindness-statistics/adults/facts-and-figures/235, January.Apple-Inc. 2013.
Voiceover for os x. http://www.apple.com/accessibility/osx/voiceover/.Srinivas Bangalore and Amanda J Stent.
2009.
In-cremental parsing models for dialog task structure.In Proceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 94?102.
Association for Compu-tational Linguistics.Yevgen Borodin, Jeffrey P Bigham, Glenn Dausch, andIV Ramakrishnan.
2010.
More than meets the eye:a survey of screen-reader browsing strategies.
InProceedings of the 2010 International Cross Dis-ciplinary Conference on Web Accessibility (W4A),page 13.
ACM.Kristy Elizabeth Boyer, Eun Young Ha, RobertPhillips, Michael D Wallis, Mladen A Vouk, andJames C Lester.
2010.
Dialogue act modeling ina complex task-oriented domain.
In Proceedingsof the 11th Annual Meeting of the Special InterestGroup on Discourse and Dialogue, pages 297?305.Association for Computational Linguistics.Harry Bunt.
2011.
Multifunctionality in dialogue.Computer Speech & Language, 25(2):222?245.Jean Carletta, Stephen Isard, Gwyneth Doherty-Sneddon, Amy Isard, Jacqueline C Kowtko, andAnne H Anderson.
1997.
The reliability of a dia-logue structure coding scheme.
Computational lin-guistics, 23(1):13?31.Ananlada Chotimongkol.
2008.
Learning the structureof task-oriented conversations from the corpus of in-domain dialogs.
Ph.D. thesis, SRI International.Mark G Core and James Allen.
1997.
Coding dialogswith the damsl annotation scheme.
In AAAI fall sym-posium on communicative action in humans and ma-chines, pages 28?35.
Boston, MA.Maxine Eskenazi, Alexander I Rudnicky, Karin Gre-gory, Paul C Constantinides, Robert Brennan,Christina L Bennett, and Jwan Allen.
1999.
Datacollection and processing in the carnegie melloncommunicator.
In EUROSPEECH.C?esar Gonz?alez Ferreras and Valent?
?n Carde?noso-Payo.2005.
Development and evaluation of a spoken di-alog system to access a newspaper web site.
In IN-TERSPEECH, pages 857?860.J.L.
Fleiss.
1973.
Statistical methods for rates andproportions Rates and proportions.
Wiley.Freedom-Scientific.
2014.
Screen read-ing software from freedom scientific.http://www.freedomscientific.com/products/fs/jaws-product-page.asp.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD explorations newsletter, 11(1):10?18.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.Cheongjae Lee, Sangkeun Jung, Kyungduk Kim,Donghyeon Lee, and Gary Geunbae Lee.
2010.
Re-cent approaches to dialog management for spokendialog systems.
JCSE, 4(1):1?22.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Kiyonori Ohtake, Teruhisa Misu, Chiori Hori, HidekiKashioka, and Satoshi Nakamura.
2009.
Annotat-ing dialogue acts to construct dialogue systems forconsulting.
In Proceedings of the 7th Workshop onAsian Language Resources, pages 32?39.
Associa-tion for Computational Linguistics.Kiyonori Ohtake, Teruhisa Misu, Chiori Hori, HidekiKashioka, and Satoshi Nakamura.
2010.
Dialogueacts annotation for nict kyoto tour dialogue corpusto construct statistical dialogue systems.
In LREC.Yury Puzis, Yevgen Borodin, Rami Puzis, and IV Ra-makrishnan.
2013.
Predictive web automation as-sistant for people with vision impairments.
In Pro-ceedings of the 22nd international conference onWorld Wide Web, pages 1031?1040.
InternationalWorld Wide Web Conferences Steering Committee.Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,and Shrikanth Narayanan.
2009.
Combining lexi-cal, syntactic and prosodic cues for improved onlinedialog act tagging.
Computer Speech & Language,23(4):407?422.John R Searle.
1975.
Indirect speech acts.
Syntax andsemantics, 3:59?82.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational linguistics, 26(3):339?373.Lu Wang, Larry Heck, and Dilek Hakkani-Tur.
2014.Leveraging semantic web search and browse ses-sions for multi-turn spoken dialog systems.131A List of Words Predictive of DialogueActsTable 10 lists all the words associated withpresence-of-word (P) and position-of-word (B)related features (Table 4) used in this work.
No-tice that all words specified in Table 10 are task-independent.
This ensures that the proposed fea-ture set is generic enough to be applicable for awide variety of tasks on the web.
The proposedlist of words can be easily extended by adding syn-onyms, which can be obtained automatically frompublicly available sources like WordNet (Miller,1995).Features Predictive WordspiyouI, youphelphelpphelpq, bhelpqhow, can, do, am Ipprevdynamically determined at runtimepintentwant, like, would, need, preferpbrowserdynamically determined at runtimephtmlbody, page, form, box, field, search, link, button,list, dropdownpbasicclear, select, fill, delete, click, edit, erase, submit,repeat, choose, enter, checkpnbasicany verb not in the pbasiclist abovepnav, bnavskip, go to, next, first, last, back, continue, previ-ous, stop, go back, finish, home pagepquestion, bquestionwhat, where, why, when, howTable 10: Complete list of predictive words forfeatures in P and B of Table 4.As explained earlier, the words in Table 10 wereselected by performing simple statistical analysisof corpus and also by examining the word-weightsproduced by the SVM classifier trained on uni-gram features alone.
In other words, some of thewords in Table 10 were borrowed from Table 5that lists discriminatory unigrams for different di-alogue acts.
Note that the task-dependent words(e.g.
?Stanford?, ?airplane?, etc.)
in Table 5 wereignored while constructing Table 10.B Sample Dialogue in the CorpusTable 11 presents an example of a dialogue thatwas collected during the execution of the Shop-ping task by a participant in the Wizard-Of-Ozstudy.
For deeper understanding, the wizard ac-tions for every user utterance are also listed.Subject: ?I want to buy CD?Wizard: [enter ?CDs?
in the search box, clicks ?Go?, andnarrate search results one by one]Screen reader: Verbatim 97458 700 MB 80 Minute .
.
.
Disc CD-Rby Verbatim link .
.
.Subject: ?Search for new age items?Wizard: [searches for ?new age items?, clicks on results]Screen reader: Age of Wushu - Free Amazon .
.
.
Game Connectlink by Snail Games USA .
.
.Subject: ?stop?Wizard: [presses pause shortcut]Subject: ?new age music CDs?Wizard: [searches for ?new music CDs?
]Screen reader: The ultimate Most Relaxing New Age .
.
.
.Subject: ?new age music CDs by Kitaro?Wizard: [Searches for ?new music cds by Kitaro?
]Screen reader: Most Relaxing New Age Music link by Kitaro,List 8 items, $10.87 link, order in the next 3 hours.
.
.
Tenku link by Kitaro .
.
.Subject: ?sample recordings?Wizard: [follows the current link]Screen reader: Page loadingWizard: [plays the first sample track]Screen reader: [audio sample]Subject: ?next?Wizard: [plays the next sample track]Screen reader: [audio sample]Subject: ?next?Wizard: [plays the next sample track]Screen reader: [audio sample]Subject: ?go back?Wizard: [presses back button]Screen reader: page loading, [repeats the visited link]Subject: ?next CD?Wizard: [clicks the title of the next item in search result]Screen Reader: Ancient link by Kitaro .
.
.
$14.98 link .
.
.Subject: ?listen to audio?Wizard: [follows link]Screen Reader: Page loadingWizard: [plays the next sample track]Screen reader: [audio sample]Subject: ?next?Wizard: [plays the next sample track]Screen reader: [audio sample]Subject: ?buy this cd?Wizard: [clicks ?Add to cart?
button, then clicks ?Proceedto Checkout?
button]Screen reader: [reads out all captions]Table 11: An example dialogue from corpus alongwith associated wizard actions.132
