Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1943?1948,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsCombining Supervised and Unsupervised Ensemblesfor Knowledge Base PopulationNazneen Fatema Rajani and Raymond J. MooneyDepartment Of Computer ScienceThe University of Texas at Austinnrajani@cs.utexas.edu, mooney@cs.utexas.eduAbstractWe propose an algorithm that combines su-pervised and unsupervised methods to ensem-ble multiple systems for two popular Knowl-edge Base Population (KBP) tasks, Cold StartSlot Filling (CSSF) and Tri-lingual Entity Dis-covery and Linking (TEDL).
We demonstratethat it outperforms the best system for bothtasks in the 2015 competition, several ensem-bling baselines, as well as a state-of-the-artstacking approach.
The success of our tech-nique on two different and challenging prob-lems demonstrates the power and generality ofour combined approach to ensembling.1 IntroductionEnsembling multiple systems is a well known stan-dard approach to improving accuracy in several ma-chine learning applications (Dietterich, 2000).
En-sembles have been applied to parsing (Hendersonand Brill, 1999), word sense disambiguation (Ped-ersen, 2000), sentiment analysis (Whitehead andYaeger, 2010) and information extraction (IE) (Flo-rian et al, 2003; McClosky et al, 2012).
Recently,using stacking (Wolpert, 1992) to ensemble sys-tems was shown to give state-of-the-art results onslot-filling and entity linking for Knowledge BasePopulation (KBP) (Viswanathan et al, 2015; Ra-jani and Mooney, 2016).
Stacking uses supervisedlearning to train a meta-classifier to combine multi-ple system outputs; therefore, it requires historicaldata on the performance of each system.
Rajani andMooney (2016) use data from the 2014 iteration ofthe KBP competition for training and then test on thedata from the 2015 competition, therefore they canonly ensemble the shared systems that participatedin both years.However, we would sometimes like to ensem-ble systems for which we have no historical perfor-mance data.
For example, due to privacy, some com-panies may be unwilling to share their performanceon arbitrary training sets.
Simple methods such asvoting permit ?unsupervised?
ensembling, and sev-eral more sophisticated methods have also been de-veloped for this scenario (Wang et al, 2013).
How-ever, such methods fail to exploit supervision forthose systems for which we do have training data.Therefore, we present an approach that utilizes su-pervised and unsupervised ensembling to exploit theadvantages of both.
We first use unsupervised en-sembling to combine systems without training data,and then use stacking to combine this ensembledsystem with other systems with available trainingdata.Using this new approach, we demonstrate newstate-of-the-art results on two NIST KBP challengetasks: Cold Start Slot-Filling (CSSF)1 and the Tri-lingual Entity Discovery and Linking (TEDL) (Jiet al, 2015).
Our approach outperforms the bestsystem as well as other state-of-the-art ensemblingmethods on both tasks in the most recent 2015 com-petition.
There is one previous work on ensemblingsupervised and unsupervised models using graph-based consensus maximization (Gao et al, 2009),however we show that it does not do as well as ourstacking method.1http://www.nist.gov/tac/2015/KBP/ColdStart/guidelines.html19432 Overview of KBP Tasks2.1 Cold Start Slot Filling (CSSF)The goal of CSSF is to collect information (fills)about specific attributes (slots) for a set of entities(queries) from a given corpus.
The query entitiescan be a person, organization, or geo-political entity(PER/ORG/GPE).
The input is a set of queries alongwith a text corpus in which to look for information.The output is a set of slot fills for each query.
Sys-tems must also provide provenance in the form ofdocid:startoffset-endoffset, where docid specifies asource document and the offsets demarcate the textin this document supporting the filler.
Systems mayalso provide a confidence score to indicate their cer-tainty in the extracted information.2.2 Tri-lingual Entity Discovery and Linking(TEDL)The first step in the TEDL task is to discover all en-tity mentions in a corpus with English, Spanish andChinese documents.
The entities can be a person, or-ganization or geo-political entity (PER/ORG/GPE)and in 2015 two more entity types were introduced?
facility and location (FAC/LOC).
The extractedmentions are then linked to an existing English KB(a version of FreeBase) entity via its ID.
If there isno KB entry for an entity, systems are expected tocluster all the mentions for that entity using a NILID.
The output for the task is a set of extracted men-tions, each with a string, its provenance in the cor-pus, and a corresponding KB ID if the system couldsuccessfully link the mention, or else a mention clus-ter with a NIL ID.
Systems can also provide a confi-dence score for each mention.3 Ensembling AlgorithmFigure 1 illustrates our system which trains a finalmeta-classifier for combining multiple systems us-ing confidence scores and other auxiliary featuresdepending on the task.3.1 Supervised Ensembling ApproachFor the KBP systems that are common betweenyears, we use the stacking method of Viswanathanet al (2015) for these shared systems.The meta-classifier makes a binary decision foreach distinct output represented as a key-value pair.Sup	System	1Sup	System	2Sup	System	NUnsup	System	1Trained		Meta-classifierAuxiliary	Features	conf	1	conf	2conf	NUnsup	System	2	 Aggregated	confUnsup	System	M	 Accept?Constrained	Op?iza?nFigure 1: Illustration of our approach to combine supervisedand unsupervised ensembles.The function of the key is to provide a handle for ag-gregating outputs that are common across systems.For the CSSF task, the key for ensembling multiplesystems is a query along with a slot type, for exam-ple, per:age of ?Barack Obama?
and the value is acomputed slot fill.
For TEDL, the key is the KB (orNIL) ID and the value is a mention, that is a spe-cific reference to an entity in the text.
The top halfof Figure 1 illustrates ensembling multiple systemswith historical training data using a supervised ap-proach.3.2 Unsupervised Ensembling ApproachOnly 38 of the 70 systems that participated in CSSF2015 also participated in 2014, and only 24 of the 34systems that participated in TEDL 2015 also partic-ipated in 2014 EDL.
Therefore, many KBP systemsin 2015 were new and did not have past training data.In fact, some of the new systems performed betterthan the shared systems, for example the hltcoe sys-tem did not participate in 2014 but was ranked 4th inthe 2015 TEDL task (Ji et al, 2015).
Thus, for im-proving recall and performance in general, it is cru-cial to use systems without historical training data,which we call unsupervised systems.
To achievethis end, we first ensemble such systems using anunsupervised technique.
Frequently, the confidencescores provided by systems are not well-calibratedprobabilities.
So in order to calibrate the confidencescores across unsupervised systems, we use the con-strained optimization approach proposed by Wanget al (2013).
The idea is to aggregate the raw confi-dence values produced by individual KBP systems,1944to arrive at a single aggregated confidence value foreach key-value pair.
The constraints ensure that theaggregated confidence score is close to the raw scoreas well as proportional to the agreement among sys-tems on a value for a given key.
Thus for a givenkey, if a system?s value is also produced by multi-ple other systems, it would have a higher score thanif it were not produced by any other system.
Theauthors use the inverse ranking of the average pre-cision previously achieved by individual systems asthe weights in their algorithm.
However since weuse this approach for systems with no historical per-formance data, we use uniform weights across allunsupervised systems for both the tasks.We use the slot type for the CSSF task and en-tity type for the TEDL task to define the constraintson the values.
The output from the constrained op-timization approach for both tasks is a set of key-values with aggregated confidence scores across allunsupervised systems which go directly into thestacker as shown in the lower half of Figure 1.
Us-ing the aggregation approach as opposed to directlyusing the raw confidence scores allows the classifierto meaningfully compare confidence scores acrossmultiple systems although they are produced by verydiverse systems.Another unsupervised ensembling method wetried in place of the constrained optimization ap-proach is the Bipartite Graph based Consensus Max-imization (BGCM) approach of Gao et al (2009).BGCM is presented as a way of combining super-vised and unsupervised models, so we compare it toour stacking approach to combining supervised andunsupervised systems, as well as an alternative ap-proach to ensembling just the unsupervised systemsbefore passing their output to the stacker.
BGCMperforms an optimization over a bipartite graph ofsystems and outputs, where the objective functionfavors the smoothness of the label assignments overthe graph, as well as penalizing deviations from theinitial labeling provided by supervised models.3.3 Combining Supervised and UnsupervisedWe propose a novel approach to combine the afore-mentioned supervised and unsupervised methods us-ing a stacked meta-classifier as the final arbiter foraccepting a given key-value.
The outputs from thesupervised and unsupervised systems are fed intothe stacker in a consistent format such that there is aunique input key-value pair.
Most KBP teams sub-mit multiple variations of their system.
Before en-sembling, we first combine multiple runs of the sameteam into one.
Of the 38 CSSF systems from 10teams for which we have 2014 data for training andthe 32 systems from 13 teams that do not have train-ing data, we combine the runs of each team into oneto ensure diversity of the final ensemble.
For the slotfills that were common between the runs of a giventeam, we compute an average confidence value, andthen add any additional fills that are not commonbetween runs.
Thus, we obtained 10 systems (onefor each team) for which we have supervised datafor training stacking.
Similarly, we combine the 24TEDL systems from 6 teams that have 2014 trainingdata and 10 systems from 4 teams that did not havetraining data into one per team.
Thus using the no-tation in Figure 1, for TEDL, N = 6 and M = 4while for CSSF, N = 10 and M = 13.The unsupervised method produces aggregated,calibrated confidence scores which go directly intoour final meta-classifier.
We treat this combinationas a single system called the unsupervised ensemble.We add the unsupervised ensemble as an additionalsystem to the stacker, thus giving us a total of N+1,that is 11 CSSF and 7 TEDL systems.
Once we haveextracted the auxiliary features for each of the N su-pervised systems and the unsupervised ensemble forboth years, we train the stacker on 2014 systems,and test on the 2015 systems.
The unsupervised en-semble for each year is composed of different sys-tems, but hopefully the stacker learns to combine ageneric unsupervised ensemble with the supervisedsystems that are shared across years.
This allowsthe stacker to arbitrate the final correctness of a key-value pair, combining systems for which we have nohistorical data with systems for which training datais available.
To learn the meta-classifier, we use anL1-regularized SVM with a linear kernel (Fan et al,2008) (other classifiers gave similar results).3.4 Post-processingOnce we obtain the decisions on each key-valuepair from the stacker, we perform some final post-processing.
For CSSF, each list-valued slot fill thatis classified as correct is included in the final output.For single-valued slot fills, if they are multiple fills1945Methodology Precision Recall F1Combined stacking and constrained optimization with auxiliary features 0.4679 0.4314 0.4489Top ranked SFV system in 2015 (Rodriguez et al, 2015) 0.4930 0.3910 0.4361Stacking using BGCM instead of constrained optimization 0.5901 0.3021 0.3996BGCM for combining supervised and unsupervised systems 0.4902 0.3363 0.3989Stacking with auxiliary features described in (Rajani and Mooney, 2016) 0.4656 0.3312 0.3871Ensembling approach described in (Viswanathan et al, 2015) 0.5084 0.2855 0.3657Top ranked CSSF system in 2015 (Angeli et al, 2015) 0.3989 0.3058 0.3462Oracle Voting baseline (3 or more systems must agree) 0.4384 0.2720 0.3357Constrained optimization approach described in (Wang et al, 2013) 0.1712 0.3998 0.2397Table 1: Results on 2015 Cold Start Slot Filling (CSSF) task using the official NIST scorerMethodology Precision Recall F1Combined stacking and constrained optimization 0.686 0.624 0.653Stacking using BGCM instead of constrained optimization 0.803 0.525 0.635BGCM for combining supervised and unsupervised outputs 0.810 0.517 0.631Stacking with auxiliary features described in (Rajani and Mooney, 2016) 0.813 0.515 0.630Ensembling approach described in (Viswanathan et al, 2015) 0.814 0.508 0.625Top ranked TEDL system in 2015 (Sil et al, 2015) 0.693 0.547 0.611Oracle Voting baseline (4 or more systems must agree) 0.514 0.601 0.554Constrained optimization approach 0.445 0.176 0.252Table 2: Results on 2015 Tri-lingual Entity Discovery and Linking (TEDL) using official NIST scorer and CEAF metricthat were classified as correct for the same query andslot type, we include the fill with the highest meta-classifier confidence.For TEDL, for each entity mention link that isclassified as correct, if the link is a KB cluster IDthen we include it in the final output, but if the linkis a NIL cluster ID then we keep it aside until allmention links are processed.
Thereafter, we resolvethe NIL IDs across systems since NIL ID?s for eachsystem are unique.
We merge NIL clusters acrosssystems into one if there is at least one common en-tity mention among them.4 Experimental ResultsAll results were obtained using the official NISTscorers after the competitions ended.2 We compareto the purely supervised approach of Viswanathan etal.
(2015) using shared systems between 2014 and2015, and the constrained optimization approach ofWang et al (2013) using all 2015 systems.
We alsocompare to BGCM (Gao et al, 2009) in two ways.2http://www.nist.gov/tac/2015/KBP/ColdStart/tools.html,https://github.com/wikilinks/nelevalFirst, we use BGCM in place of the constrained op-timization approach to ensemble unsupervised sys-tems while keeping the rest of our pipeline the same.Secondly, we also compare to combining both su-pervised and unsupervised systems using BGCM in-stead of stacking.
We also include an ?oracle?
vot-ing ensembling baseline, which varies the thresholdon the number of systems that must agree to identifyan ?oracle?
threshold that results in the highest F1score for 2015.
We find that for CSSF a threshold of3, and for TEDL a threshold of 4, gives us the bestF1 score.Tables 1 and 2 show CSSF and TEDL results.Our full system, which combines supervised and un-supervised ensembling performed the best on bothtasks.
TAC-KBP also includes the Slot Filler Val-idation (SFV) task3 where the goal is to ensem-ble/filter outputs from multiple slot filling systems.The top ranked system in 2015 (Rodriguez et al,2015) does substantially better than many of theother ensembling approaches, but it does not do aswell as our best performing system.
The purely3http://www.nist.gov/tac/2015/KBP/SFValidation/index.html1946020000400006000080000100000120000140000Supervised	 Unsupervised	 Combina?onTEDLUnique	pairs	 Common	pairs05000100001500020000250003000035000Supervised	 Unsupervised	 Combina?onCSSFUnique	pairs	 Common	pairsFigure 2: Total number of unique and common input pairs contributed by the supervised and unsupervised systems to the combi-nation for the TEDL and CSSF tasks respectively.supervised approach of Viswanathan et al (2015)and the auxiliary features approach of Rajani andMooney (2016) performs substantially worse, al-though still outperforming the top-ranked individualsystem in the 2015 competition.
These approachesonly use the common systems from 2014, thus ig-noring approximately half of the systems.
The ap-proach of Wang et al (2013) performs very poorlyby itself; but when combined with stacking gives aboost to recall and thus the overall F1.
Note that allour combined methods have a substantially higherrecall.
The oracle voting baseline also performs verypoorly indicating that naive ensembling is not ad-vantageous.TEDL provides three different approaches tomeasuring accuracy: entity discovery, entity linking,and mention CEAF (Ji et al, 2015).
CEAF finds theoptimal alignment between system and gold stan-dard clusters, then evaluates precision and recallmicro-averaged.
We obtained similar results on allthree metrics and only include CEAF.
The purelysupervised stacking approach over shared systemsdoes not do as well as any of our combined ap-proaches even though it beats the best performingsystem (i.e.
IBM) in the 2015 competition (Sil etal., 2015).
The relative ranking of the approaches issimilar to those obtained for CSSF, proving that ourapproach is very general and improves performanceon two quite different and challenging problems.Even though it is obvious that the boost in ourrecall was because of adding the unsupervised sys-tems, it isn?t clear how many new key-value pairswere generated by these systems.
We thus evalu-ated the contribution of the systems ensembled usingthe supervised approach and those ensembled usingthe unsupervised approach, to the final combinationfor both the tasks.
Figure 2 shows the number ofunique as well as common key-value pairs that werecontributed by each of the approaches.
The uniquepairs are those that were produced by one approachbut not the other and the common pairs are thosethat were produced by both approaches.
The num-ber of unique pairs in the combination is the unionof unique pairs in the supervised and unsupervisedapproaches.
We found that approximately one thirdof the input pairs in the combination came from theunique pairs produced just by the unsupervised sys-tems for both the TEDL and CSSF tasks.
Only about15% and 22% of the total input pairs were commonbetween the two approaches for the TEDL and CSSFtasks respectively.
Our findings highlight the impor-tance of utilizing systems that do not have historicaltraining data.5 ConclusionWe presented results on two diverse KBP tasks,showing that a novel stacking-based approach to en-sembling both supervised and unsupervised systemsis very promising.
The approach outperforms thetop ranked systems from both 2015 competitions aswell as several other ensembling methods, achiev-ing a new state-of-the-art for both of these impor-tant, challenging tasks.
We found that adding theunsupervised ensemble along with the shared sys-tems specifically increased recall substantially.AcknowledgmentThis research was supported by the DARPA DEFTprogram under AFRL grant FA8750-13-2-0026.1947ReferencesGabor Angeli, Victor Zhong, Danqi Chen, Arun Cha-ganty, Jason Bolton, Melvin Johnson Premkumar,Panupong Pasupat, Sonal Gupta, and Christopher D.Manning.
2015.
Bootstrapped Self Training forKnowledge Base Population.
In Proceedings of theEighth Text Analysis Conference (TAC2015).T.
Dietterich.
2000.
Ensemble Methods in MachineLearning.
In J. Kittler and F. Roli, editors, FirstInternational Workshop on Multiple Classifier Sys-tems, Lecture Notes in Computer Science, pages 1?15.Springer-Verlag.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Radu Florian, Abe Ittycheriah, Hongyan Jing, and TongZhang.
2003.
Named entity recognition through clas-sifier combination.
In Proceedings of the seventh con-ference on Natural language learning at HLT-NAACL2003-Volume 4, pages 168?171.
Association for Com-putational Linguistics.Jing Gao, Feng Liang, Wei Fan, Yizhou Sun, and JiaweiHan.
2009.
Graph-based consensus maximizationamong multiple supervised and unsupervised models.In Advances in Neural Information Processing Sys-tems (NIPS2009), pages 585?593.John C. Henderson and Eric Brill.
1999.
ExploitingDiversity in Natural Language Processing: Combin-ing Parsers.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP99), pages 187?194, College Park, MD.Heng Ji, Joel Nothman, Ben Hachey, and Radu Florian.2015.
Overview of TAC-KBP2015 Tri-lingual EntityDiscovery and Linking.
In Proceedings of the EighthText Analysis Conference (TAC2015).David McClosky, Sebastian Riedel, Mihai Surdeanu, An-drew McCallum, and Christopher D Manning.
2012.Combining Joint Models for Biomedical Event Extrac-tion.
BMC Bioinformatics.Ted Pedersen.
2000.
A Simple Approach to Build-ing Ensembles of Naive Bayesian Classifiers for WordSense Disambiguation.
In North American Chap-ter of the Association for Computational Linguistics(NAACL2000), pages 63?69.Nazneen Fatema Rajani and Raymond J. Mooney.
2016.Stacking With Auxiliary Features.
ArXiv e-prints.Miguel Rodriguez, Sean Goldberg, and Daisy Zhe Wang.2015.
University of Florida DSR lab system for KBPslot filler validation 2015.
In Proceedings of theEighth Text Analysis Conference (TAC2015).Avirup Sil, Georgiana Dinu, and Radu Florian.
2015.The IBM systems for trilingual entity discovery andlinking at TAC 2015.
In Proceedings of the EighthText Analysis Conference (TAC2015).Vidhoon Viswanathan, Nazneen Fatema Rajani, YinonBentor, and Raymond J. Mooney.
2015.
StackedEnsembles of Information Extractors for Knowledge-Base Population.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics (ACL2015), pages 177?187, Beijing, China, July.I-Jeng Wang, Edwina Liu, Cash Costello, and ChristinePiatko.
2013.
JHUAPL TAC-KBP2013 Slot FillerValidation System.
In Proceedings of the Sixth TextAnalysis Conference (TAC2013).Matthew Whitehead and Larry Yaeger.
2010.
Sentimentmining using ensemble classification models.
In TarekSobh, editor, Innovations and Advances in ComputerSciences and Engineering.
SPRINGER, Berlin.David H. Wolpert.
1992.
Stacked generalization.
NeuralNetworks, 5:241?259.1948
