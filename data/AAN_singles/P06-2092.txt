Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 715?722,Sydney, July 2006. c?2006 Association for Computational LinguisticsATLAS ?
a new text alignment architectureBettina SchraderInstitute of cognitive ScienceUniversity of Osnabru?ck49069 Osnabru?ckbschrade@uos.deAbstractWe are presenting a new, hybrid align-ment architecture for aligning bilingual,linguistically annotated parallel corpora.It is able to align simultaneously at para-graph, sentence, phrase and word level,using statistical and heuristic cues, alongwith linguistics-based rules.
The systemcurrently aligns English and German texts,and the linguistic annotation used coversPOS-tags, lemmas and syntactic constitu-tents.
However, as the system is highlymodular, we can easily adapt it to new lan-guage pairs and other types of annotation.The hybrid nature of the system allowsexperiments with a variety of alignmentcues to find solutions to word alignmentproblems like the correct alignment of rarewords and multiwords, or how to aligndespite syntactic differences between twolanguages.First performance tests are promising, andwe are setting up a gold standard for athorough evaluation of the system.1 IntroductionAligning parallel text, i.e.
automatically settingthe sentences or words in one text into correspon-dence with their equivalents in a translation, is avery useful preprocessing step for a range of ap-plications, including but not limited to machinetranslation (Brown et al, 1993), cross-languageinformation retrieval (Hiemstra, 1996), dictionarycreation (Smadja et al, 1996) and induction ofNLP-tools (Kuhn, 2004).
Aligned corpora can bealso be used in translation studies (Neumann andHansen-Schirra, 2005).The alignment of sentences can be done suffi-ciently well using cues such as sentence length(Gale and Church, 1993) or cognates (Simard etal., 1992).
Word alignment, however, is almost ex-clusively done using statistics (Brown et al, 1993;Hiemstra, 1996; Vogel et al, 1999; Toutanova etal., 2002).Hence it is difficult to align so-called rareevents, i.e.
tokens with a frequency below 10.
Thisis a considerable drawback, as rare events makeup more than half of the vocabulary of any cor-pus.
Another problem is the correct alignment ofmultiword units like idioms.
Then, differences inword order are not modelled well by the statisticalalgorithms.In order to find solutions to these problems, wehave developed a hybrid alignment architecture: ituses statistical information extracted directly froma corpus, and rules or heuristics based on the lin-guistic information as given by the corpus?
anno-tation.
Additionally, it is not necessary to computesentence alignment prior to aligning at the wordlevel.
Instead, the system is capable of interac-tively and incrementally computing sentence andword alignment, along with alignment at the para-graph and phrase level.
The simultaneous align-ment at different levels of granularity imposes re-strictions on the way text alignment is computed:we are using a constrained best-first strategy forthis purpose.Although we are currently developing and test-ing the alignment system for the language pairEnglish-German, we have made sure that it caneasily be extended to new language pairs.
In fact,we are currently adding Swedish and French to theset of supported languages.First performance tests have been promising,and we are currently setting up a gold standard715of 242 manually aligned sentence pairs in Englishand German for a thorough evaluation.In the following, we give an overview on stan-dard approaches to sentence and word alignment,and discuss their advantages and shortcomings.Then, we describe the design of our alignment ar-chitecture.
In the next two sections, we are de-scribing the data on which we test our system, andour evaluation strategy.
Finally, we sum up anddescribe further work.2 Related workResearch on text alignment has largely focusedon aligning either sentences or words, i.e.
mostapproaches either compute which sentences of asource and a target language form a translationpair, or they use sentence alignment as a prepro-cessing step to align on the word level.Additionally, emphasis was laid on the devel-opment of language-independent algorithms.
Ide-ally, such algorithms would not be tailored to aligna specific language pair, but would be applicable toany two languages.
Language-independence hasalso been favoured with respect to linguistic re-sources in that alignment should do without e.g.using pre-existing dictionaries.
Hence there is adominance of purely statistical approaches.2.1 Sentence AlignmentSentence alignment strategies fall roughly intothree categories: length-based approaches (Galeand Church, 1991; Gale and Church, 1993) arebased on the assumption that the length propor-tions of a sentence and its translation are roughlythe same.
Anchor-based algorithms align sen-tences based on cues like corpus-specific markupand orthographic similarity (Simard et al, 1992).The third approach uses bilingual lexical informa-tion, e.g.
estimated from the corpus (Kay andRo?scheisen, 1993; Fung and Church, 1994; Fungand McKeown, 1994).Hybrid methods (Tschorn, 2002) combine thesestandard approaches such that the shortcomings ofone approach are counterbalanced by the strengthof another component: length-based methods arevery sensitive towards deletions in that a singleomission can cause the alignment to go on a wrongtrack from the point where it occurred to the endof the corpus.
Strategies that assume that ortho-graphic similarity entails translational equivalencerely on the relatedness of the language pair inquestion.
In closely-related languages like Englishand French, the amount of orthographically simi-lar words that share the same meaning is higherthan in unrelated languages like English and Chi-nese, were orthographic or even phonetic similar-ity may only indicate translational equivalence fornames.
Strategies that use system-external dictio-naries, finally, can only be used if a large-enoughdictionary exists for a specific language pair.2.2 Word AlignmentAligning below the sentence level is usually doneusing statistical models for machine translation(Brown et al, 1991; Brown et al, 1993; Hiemstra,1996; Vogel et al, 1999) where any word of thetarget language is taken to be a possible translationfor each source language word.
The probability ofsome target language word to be a translation ofa source language word then depends on the fre-quency with which both co-occur at the same orsimilar positions in the parallel corpus.The probabilities are estimated from the usingthe EM-algorithm1, and a Viterbi search is car-ried out to compute the most probable sequenceof word translation pairs.
Word order differencesbetween the two languages are modelled by usingstatistical weights, and multiword units are simi-larly treated.Another approach to word alignment is pre-sented by Tiedemann (2003), where alignmentprobabilities are computed using a combination offeatures like e.g.
co-occurrence, cognateness, syn-tactic category membership.
However, althoughthe alignment is partly based on linguistic fea-tures, its computation is entirely statistical.
Otherword alignment strategies (Toutanova et al, 2002;Cherry and Lin, 2003) have also begun to in-corporate linguistic knowledge.
Unfortunately,the basic, statistical, assumptions have not beenchanged, and hence no sufficient solution to theshortcomings of the early alignment models havebeen found.3 Shortcomings of the statisticalalignment approachesWhile sentence alignment can be done success-fully using a combination of the existing algo-rithms, word alignment quality suffers due tothree problematic phenomena: the amount of rare1see (Manning and Schu?tze, 1999), chapter 14.2.2 for ageneral introduction716words typically found in corpora, word order dif-ferences between the to languages to be aligned,and the existence of multiword units3.1 Rare WordsApproximately half of a corpus?
vocabulary con-sists of so-called hapax legomena, i.e.
types thatoccur exactly once in a text.
Most other words fallinto the range of so-called rare events, which wedefine here as types with occurrences between 2and 10.
Both hapax legomena and rare events ob-viously do not provide sufficient information forstatistical analysis.In the case of word alignment, it is easy to seethat they are hard to align: there is virtually no fre-quency or co-occurrence data with which to com-pute the alignment.
On the other hand, five to tenpercent of a corpus?
vocabulary consists of highlyfrequent words, i.e.
words with frequencies of100 or above.
These types have the advantage ofoccurring frequently enough for statistical analy-sis, however, as they occur at virtually every posi-tion in a corpus, they can correspond to anythingif alignment decisions are taken on the basis ofstatistics only.One solution to this problem would be to usestatistics-free rules for alignment, i.e.
rules thatare insensitive to the rarity or frequency of a word.However, this means that statistical models eitherhave to be abandoned completely, or that effort hasto be put in finding a means to combine both align-ment approaches into one single, hybrid system.An alternative would be to design a statisti-cal alignment model that is better suited for theZipfian frequency distributions in the source andtarget language texts.
Research in this direc-tion would greatly benefit from large amountsof high quality example alignments, e.g.
takenfrom the parallel treebanks that are currently be-ing built (Volk and Samuelsson, 2004; Neumannand Hansen-Schirra, 2005).3.2 Word Order DifferencesAnother problem that has been noticed as earlyas 1993 with the first research on word alignment(Brown et al, 1993) concerns the differences inword order between source and target language.While simple statistical alignment models likeIBM-1 (Brown et al, 1993) and the symmetricalignment approach by Hiemstra (1996) treat sen-tences as unstructured bags of words, the more so-phisticated IBM-models by Brown et al (1993)approximates word order differences using a sta-tistical distortion factor.
Vogel et al (1999), onthe other hand, treat word order differences as alocal phenomenon that can be modelled within awindow of no more than three words.
Recently,researchers like Cherry and Lin (2003) have be-gun to use syntactic analyses to guide and restrictthe word alignment process.The advantage of using available syntactic in-formation for word alignment is that it helps toovercome data sparseness: although a token maybe rare, its syntactic category may not, and hencethere may be sufficient statistical information toalign at the phrase level.
Subsequently, the phraselevel information can be used to compute align-ments for the tokens within the aligned phrases.The syntactic function of a token as modifier, headetc.
can equally simplify and guide the align-ment process considerably.
However, it is unclearwhether such an approach performs well for lan-guage pairs where syntactic and functional differ-ences are greater than between e.g.
English andFrench.3.3 Multiword alignmentLike syntactic differences, n:m correspondences,i.e.
alignments that involve multiword expres-sions, have soon been noted as being difficult forstatistical word alignment: Brown et al (1993)modelled fertility, as they called it, statistically inthe more sophisticated IBM-models.
Other ap-proaches adopt again a normalizing procedure: ina preprocessing step, multiwords are either rec-ognized as such and subsequently treated as ifthey were a single token (Tiedemann, 1999), or,reversely, the tokens they align to may be splitinto their components, with the components be-ing aligned to the parts of the corresponding mul-tiword expression on a 1:1 basis.The latter approach is clearly insufficient forword alignment quality: it assumes that composi-tionality holds for both the multiword unit and itstranslation, i.e.
that the meaning of the whole unitis made up of the meaning of its part.
This clearlyneed not be the case, and further problems arisewhen a multiword unit and its translation containdifferent numbers of elements.The former approach, i.e.
of recognizing mul-tiword units as such and treating them as a singletoken, depends on the kind of recognition proce-dure adopted, and on the way their alignment is717computed: if it is based on statistics, again, theapproach will hardly perform well for rare expres-sions.To sum up, aligning at the sentence level canbe done with success using a combination oflanguage-independent methods.
Word alignment,on the other hand, still leaves room for improve-ment: current models do not suffice to align rarewords and multiword units, and syntactic differ-ences between source and target languages, too,still present a challenge for most word alignmentstrategies.4 An alternative text alignment systemIn order to address these problems, we have de-signed an alternative text alignment system, calledATLAS, that computes text alignment based on acombination of linguistically informed rules andstatistical computation.
It takes a linguistically an-notated corpus as input2.
The output of the textalignment system consists of the corpus alignmentinformation and a bilingual dictionary.During the alignment process, hypotheses ontranslation pairs are computed by different align-ment modules, and assigned a confidence value.These hypotheses may be about paragraphs, sen-tences, words, or phrases.All hypotheses are reused to refine and com-plete the text alignment, and in a final filteringstep, implausible hypotheses are filtered out.
Theremaining hypotheses constitute the final overalltext alignment and are used to generate a bilingualdictionary (see figure 1 for an illustration).4.1 Core ComponentThe alignment process is controlled by a corecomponent: it manages all knowledge bases, i.e.?
information contained in a system-internaldictionary,?
corpus information like the positions of to-kens and their annotations, and?
the set of alignment hypotheses.2The linguistic annotation currently supported includeslemmas, parts of speech, and syntactic phrases, along withinformation on sentence or paragraph boundaries.
The an-notation may include sentence alignment information, and abilingual dictionary may be used, too.Additionally, the core component triggers the dif-ferent alignment modules depending on the type ofa hypothesis: if, for example, a hypothesis is abouta sentence pair, then the word alignment modulesof ATLAS are started in order to find translationpairs within the sentence pair.The alignment modules are run simultaneously,but independently of each other, i.e.
an alignmenthypothesis may be generated several times, basedon cues used by different alignment modules.
Aword pair e.g.
may be aligned based on ortho-graphic similarity by one module, and based onsyntactic information by another module.Each hypothesis is assigned a confidence valueby the alignment module that generated it, andthen returned to the core component.
The confi-dence value of each hypothesis is derived from i)its probability or similarity value, and ii) the con-fidence value of the parent hypothesis.The core component may change the confidencevalue of a hypothesis, e.g.
if it was generated mul-tiple times by different alignment modules, basedon different alignment cues.
This multiple gen-eration of the same hypothesis is taken as indica-tion that the hypothesis is more reliable than if ithad been generated by only one alignment mod-ule, and hence its confidence value is increase.The core component adds all new informationto its knowledge bases, and hands it over to appro-priate alignment modules for further computation.The process is iterated until no new hypothesesare found.
Then, the core component assemblesthe best hypotheses to compute a final hypothesisset: starting with the hypothesis that has the high-est confidence, each next-best hypothesis is testedwhether it fits into the final set; if there is a contra-diction between the hypotheses already in the setand the next-best, the latter is discarded from theknowledge base.
If not, then it is added to the finalset.
This process is iterated until all hypotheseshave been either added to the final hypothesis set,or have been discarded.Cleaning-up procedures ensure that corpusitems left unaligned are either aligned to null, orcan be aligned based on a process of elimina-tion: if two units a and b are contained within thesame textual unit, e.g.
within the same paragraph,and aligning them would not cause a contradictionwith the hypotheses in the final set, then they arealigned.
Finally, all remaining hypothesis are usedto generate the overall text alignment, and to com-718?
management of knowledge bases?
corpus,?
system-internal dictionary,?
set of hypotheses?
task management?
result filtering?
output generationparagraph alignment strategiessentence alignment strategiesword  alignment strategiesphrase alignment strategies... further alignment strategiesalignment modulesread corpuswrite alignmentinformationtrigger alignmentreceive hypothesescore componentFigure 1: A schema of the text alignment architecturepute a bilingual dictionary.4.2 Alignment modulesEach alignment module receives a parent hypoth-esis as input that covers certain units of the cor-pus, i.e.
a hypothesis on a sentence pair coversthose tokens along with their annotations that arecontained within the sentence pair.
It uses this in-formation to compute child hypotheses within theunits of the parent hypothesis, assigns each childhypothesis a confidence value that indicates howreliable it is, and returns the set of children hy-potheses to the core component.In the case of a statistics-based alignment mod-ule, the confidence value corresponds to the proba-bility with which a translation pair may be aligned.In other, non-statistical alignment modules, theconfidence value is derived from the similarityvalue computed for a specific translation pair.The alignment modules that are currently usedby our the system are modules for aligning sen-tences or paragraphs based on the strategies thathave been proposed in the literature (see overviewin section 2.1), but also strategies that we have ex-perimented with for aligning words based on lin-ear ordering, parts of speech, dictionary lookupetc (see section 5).
No statistical word align-ment procedure has yet been added to the sys-tem, but we are experimenting with using statisti-cal co-occurrence measures for deriving word cor-respondences.
One language independent align-ment strategy is based on inheritance: if two unitsa and b are aligned, then this information is usedto derive alignment hypotheses for the elementswithin a and b as well as for the textual units thatcontain a and b.5 Advantages of the hybrid architectureAs our alignment architecture is hybrid and henceneed not rely on statistial information alone, itcan be used to successfully address word align-ment problems.
Note that although linguisticallyinformed alignment strategies are used, the sys-tem is not restricted to statistics-free computation:it is still possible to compute word co-occurrencestatistics and derive alignment hypotheses.5.1 Rare wordsLinguistically-informed rules that compute align-ments based on corpus annotation, but not onstatistics, can be used to overcome data sparse-ness.
Syntactic categories e.g.
give reliable align-ment cues as lexical categories such as nouns andverbs are not commonly changed during the trans-lation process.
Even if category changes occur, itis likely that the categorial class stays the same.Ideally, a noun e.g.
will be translated as a noun, orif it is not, it is highly probable that it is translatedas an adjective or verb, but not as a functional classmember like a preposition.Likewise, dictionary lookup may be used, and isused by or system, to align words within sentencesor phrases.
We have also implemented a modulethat aligns sentences and words based on stringsimilarity constrained by syntactic categories: themodule exploits the part of speech annotation toalign sentences and words based on string simi-larity between nouns, adjectives, and verbs, thusmodifying the classic approach by Simard et al(1992).
The advantage of the modification is thatthe amount of cognates within lexical class wordswill be considerably higher than between prepo-sitions, determiners, etc., hence filtering by word719category yields good results.5.2 Word Order DifferencesAs ATLAS supports the alignment of phrases, mis-matches between the linear orderings of sourceand target language words become irrelevant.
Ad-ditionally, phrase alignment can considerably nar-row down the search space within which to findthe translation of a word.
If e.g.
a noun phrase hasalready been aligned to its equivalent in the otherlanguage, aligning its daughter nodes on the basisof their syntactic categories, without any furtherconstraints or statistical information, can be suffi-cient.Furthermore, if parts of the phrase can bealigned using the system-internal dictionary,aligning the remaining words could be done byprocess of elimination.5.3 Multiword alignmentMultiwords are traditionally hardest to align, onereason being that they are hard to recognize statis-tically.
With our text alignment system, however,it is possible to write i) language-specific rulesthat detect multiwords and define ii) a similar-ity measure that aligns the detected multiwords totheir translations.
This similarity measure may belanguage-pair specific, or it may be defined glob-ally, i.e.
it will be used for any language pair.We have already tested such a procedure foraligning English nominal multiwords with theirGerman translations: In this procedure, Englishnominals are detected based on their typical part-of-speech patterns, and aligned to German nounsif the two expressions are roughly of the samelength, counted in characters.
The results are en-couraging, indicating that nominals can be alignedreliably irrespective of their frequencies in the cor-pus (Schrader, 2006).6 DataAs development corpus, we are using Europarl,a corpus of European Parliament debates (Koehn,2005).
Europarl consists of roughly 30 million to-kens per language and is tokenized and sentence-aligned.
For the purposes of testing ATLAS, wehave POS-tagged and lemmatized the German,English, and French parts of the corpus using thefreely available tree-tagger (Schmid, 1994).
Addi-tionally, we have chunked the German and Englishtexts with an extension of this tool (Schmid, un-published).
Table 1 shows the number of tokensand types of the corpus for all three languages.It also shows the percentages of hapax legomena,rare events3, and all other types of the corpus.7 EvaluationFor evaluating of our text alignment system, weare currently setting up an English-German goldstandard: we have randomly chosen a debate pro-tocol of the Europarl corpus that contains approx-imately 100,000 tokens per language (see table 2),and we corrected its sentence alignment manually.The correction was done by two annotaters inde-pendently of each other, and remaining sentencealignment differences after the corrections wereresolved.In a second step, we have chosen 242 sentencepairs from this reference set to create a word align-ment gold standard.
Some sentence pairs of thisset have been chosen randomly, the others aretaken from two text passages in the protocol.
Wehad considered choosing sentence pairs that weredistributed randomly over the reference set, how-ever, we decided for taking complete text passagesin order to make manual annotation easier.
Thisway, the annotators can easily access the contextof a sentence pair to resolve alignment ambigui-ties.Additionally, we have created word align-ment guidelines based on those already given byMelamed (1998) and Merkel (1999).
We have an-notated all 242 sentence pairs twice, and annota-tion differences are currently being resolved.As this gold standard can only be used to eval-uate the performance of English-German wordalignment, we will also evaluate our system on theStockholm parallel treebank (Volk and Samuels-son, 2004).
Evaluating against this manually con-structed treebank has the advantage that we canevaluate phrase alignment quality, and that wecan gather evaluation data for the language pairsEnglish-Swedish and Swedish-German.We have decided to use the evaluation met-rics precision, recall and the alignment error rate(AER) proposed by Och and Ney (2000) in orderto compare results to those of other alignment sys-tems.3We define rare events here as types occurring 2 to 10times720Language Tokens Types Hapax Legomena Rare Events Frequent TypesEnglish 29.077,024 101,967 39,200 (38.44%) 35,608 (34.92%) 27,159 (26.64%)German 27.643,792 286,330 140,826 (49.18%) 98,126 (34.27%) 47,378 (16.55%)French 32.439,353 114,891 42,114 (36.66%) 41,194 (35.84%) 31,583 (27.49%)Table 1: Corpus characteristics of the Europarl corpusLanguage Tokens Types Hapax Legomena Rare Events Frequent TypesEnglish 111,222 7,657 3,474 (45.37%) 3,027 (39.53%) 1,156 (15.10%)German 91,054 11,237 6,336 (56.39%) 3,973 (35.36%) 928 ( 8.26%)Table 2: Characteristics of the evaluation suite8 SummarySumming up, we have presented a new textalignment architecture that makes use of multi-ple sources of information, partly statistical, partlylinguistics-based, to align bilingual, parallel texts.Its input is a linguistically annotated parallel cor-pus, and corpus annotation may include informa-tion on syntactic constituency, syntactic categorymembership, lemmas, etc.
Alignment is done onvarious levels of granularity, i.e.
the system alignssimultaneously at the paragraph, sentence, phrase,and word level.
A constrained best-first search isused to filter out errors, and the output of the sys-tem is corpus alignment information along with abilingual dictionary, generated on the basis of thetext alignment.As our system need not rely on statistics alone,the alignment of hapax legomena and other rareevents is not a problem.
Additionally, specificstrategies have been implemented, and further canbe added, to deal with various kinds of multiwordunits.
Finally, as the system allows phrase align-ment, it stands on equal footing with other phrasealignment approaches.Currently, the system is tested on the English-German parts of the Europarl corpus, but as it ishighly modular, it can easily be extended to newlanguage pairs, types of information, and differentalignment strategies.First performance test have been promising, andwe are setting up a gold standard alignment for athorough evaluation.9 Further workWe are currently adding Swedish and French to theset of supported languages, such that our systemwill be able to align all possible pairings with thelanguages German, English, French and Swedish.If possible, we want to conduct experiments thatinvolve further languages and additional kinds ofcorpus annotation, like e.g.
detailed morphologi-cal information as annotated e.g.
within the CroCoproject (Neumann and Hansen-Schirra, 2005).At the same time, we are constantly extend-ing the set of available alignment strategies, e.g.with strategies for specific syntactic categories orstrategies that compute alignments based on statis-tical co-occurrence.A first evaluation of our text alignment systemwill have been completed by autumn 2006, andwe plan to make our gold standard as well as ourguidelines available to the research community.AcknowledgementWe thank Judith Degen for annotation help withthe gold standard.ReferencesPeter F. Brown, Jennifer C. Lai, and Robert L. Mercer.1991.
Aligning sentences in parallel corpora.
InProceedings of the 29th Annual Meeting of the As-sociation for Computational Linguistics, pages 169?176, Berkeley, California, USA.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311.Colin Cherry and Dekang Lin.
2003.
A probabilitymodel to improve word alignment.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics, pages 88?95, Sapporo,Japan.Pascale Fung and Kenneth W. Church.
1994.
K-vec:a new approach for aligning parallel texts.
In Pro-ceedings of the 15th International Conference on721Computational Linguistics (COLING), pages 1096?1102, Kyoto, Japan.Pascale Fung and Kathleen McKeown.
1994.
Align-ing noisy parallel corpora across language groups:word pair feature matching by dynamic time warp-ing.
In Proceedings of the First Conference of theAssociation for Machine Translation in the Ameri-cas (AMTA-94), pages 81?88, Columbia, Maryland,USA.William A. Gale and Kenneth W. Church.
1991.
Aprogram for aligning sentences in bilingual corpora.In Proceedings of the 29th Annual Meeting of the As-sociation for Computational Linguistics, pages 177?184, Berkeley, California, USA.
Reprinted 1993 inComputational Linguistics.William A. Gale and Kenneth W. Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational Linguistics, 19(1):75?102.D.
Hiemstra.
1996.
Using statistical methods to createa bilingual dictionary.
Master?s thesis, UniversiteitTwente.Martin Kay and Martin Ro?scheisen.
1993.
Text-translation alignment.
Computational Linguistics,19(1):121?142.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Jonas Kuhn.
2004.
Exploiting parallel corpora formonolingual grammar induction ?
a pilot study.In Workshop proceedings of the 4th InternationalConference on Language Resources and Evaluation(LREC), pages 54?57, Lisbon, Portugal.
LRECWorkshop: The Amazing Utility of Parallel andComparable Corpora.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of statistical natural language process-ing.
MIT Press, Cambridge, Massachusetts, Lon-don.I.
Dan Melamed.
1998.
Annotation style guide forthe BLINKER project.
Technical Report 98-06, In-stitute for Research in Cognitive Science, Universityof Pennsylvania.Magnus Merkel.
1999.
Annotation style guide for thePLUG link annotator.
Technical report, Linko?pinguniversity, Linko?ping, March.
PLUG report.Stella Neumann and Silvia Hansen-Schirra.
2005.The CroCo project.
Cross-linguistic corpora for theinvestigateon of explicitation in translation.
InProceedings of the Corpus Linguistics Conference,Birmingham, UK.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting of the Association for Com-putational Linguistics, pages 440?447, Hong Kong,China.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In International Con-ference on New Methods in Language Processing,pages 44?49, Manchester, England.Helmut Schmid.
unpublished.
The IMS Chunker.
un-published manuscript.Bettina Schrader.
2006.
Non-probabilistic alignmentof rare German and English nominal expressions.
InTo appear in: Proceedings of the Fifth Language Re-sources and Evaluation Conference (LREC), Genoa,Italy.
to appear.Michel Simard, G. F. Foster, and P. Isabelle.
1992.Using cognates to align sentences in bilingual cor-pora.
In Proceedings of the Fourth Internationalconference on theoretical and methodological is-sues in Machine translation, pages 67?81, Montreal,Canada.Frank Smadja, Kathleen R. McKeown, and VasileiosHatzivassiloglou.
1996.
Translating collocations forbilingual lexicons: A statistical approach.
Compu-tational Linguistics, 22(1):1?38.Jo?rg Tiedemann.
1999.
Word alignment - step by step.In Proceedings of the 12th Nordic Conference onComputational Linguistics, pages 216?227, Trond-heim, Norway.Jo?rg Tiedemann.
2003.
Combining clues for wordalignment.
In Proceedings of the 10th Conference ofthe European Chapter of the ACL (EACL03), pages339 ?
346, Budapest, Hungary.Kristina Toutanova, H. Tolga Ilhan, and Christopher D.Manning.
2002.
Extensions to HMM-based sta-tistical word alignment models.
In Conference onEmpirical Methods in Natural Language Processing(EMNLP 2002), pages 87?94, Philadelphia, USA.Patrick Tschorn.
2002.
Automatically aligningEnglish-German parallel texts at sentence level us-ing linguistic knowledge.
Master?s thesis, Univer-sita?t Osnabru?ck.Stephan Vogel, Hermann Ney, and Christoph Till-mann.
1999.
HMM-based word alignment in sta-tistical translation.
In Proceedings of the Inter-national Conference on Computational Linguistics,pages 836?841, Copenhagen, Denmark.Martin Volk and Yvonne Samuelsson.
2004.
Boot-strapping parallel treebanks.
In Proceedings ofthe Workshop on Linguistically Interpreted Corpora(LINC) at COLING, Geneva, Switzerland.722
