Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 633?635,Dublin, Ireland, August 23-24, 2014.TeamZ: Measuring Semantic Textual Similarity for Spanish Using anOverlap-Based ApproachAnubhav GuptaUFR SLHSUniversit?e de Franche-Comt?eanubhav.gupta@edu.univ-fcomte.frAbstractThis paper presents an overlap-based ap-proach using bag of words and the SpanishWordNet to solve the STS-Spanish sub-task (STS-Es) of SemEval-2014 Task 10.Since bag of words is the most commonlyused method to ascertain similarity, theperformance is modest.1 IntroductionThe objective of STS-Es is to score a pair of sen-tences in Spanish on the scale of 0 (the two sen-tences are on different topics) to 4 (the two sen-tences are completely equivalent, as they mean thesame thing) (Agirre et al., 2014).
The textual sim-ilarity finds its utility in various NLP applicationssuch as information retrieval, text categorisation,word sense disambiguation, text summarisation,topic detection, etc.
(Besanc?on et al., 1999; Mi-halcea et al., 2006; Islam and Inkpen, 2008).The method presented in this paper calculatesthe similarity based on the number of words thatare common in two given sentences.
This ap-proach, being simplistic, suffers from variousdrawbacks.
Firstly, the semantically similar sen-tences need not have many words in common (Liet al., 2006).
Secondly, even if the sentences havemany words in common, the context in which theyare used can be different (Sahami and Heilman,2006).
For example, based on the bag of words ap-proach, the sentences in Table 1 would be scoredthe same:However, only sentences [2] and [3] mean thesame.Despite the flaws, this approach was used be-cause of the Basic Principle of Compositional-ity (Zimmermann, 2011), which states that theThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/No.
Spanish English1?El es listo.
He is clever.2?El est?a listo.
He is ready.3?El est?a preparado.
He is prepared.Table 1: Examples.meaning of a complex expression depends uponthe meaning of its components and the man-ner in which they are composed.
Furthermore,mainly nouns were considered in the bag of wordsbecause Spanish is an exocentric language, andnouns contain more specific, concrete semanticinformation than verbs (Michael Herslund, 2010;Michael Herslund, 2012).2 MethodologyThe training dataset provided for the task con-sisted of 65 pairs of sentences along with their cor-responding similarity scores.
There were two testsets: one consisted of 480 sentence pairs from anews corpus, and the other had 324 sentence pairstaken from Wikipedia.The approach consisted of learning the scoringwith the help of linear regression.
Two runs weresubmitted as solutions.
The first run used three-feature vectors, whereas the second one used four-feature vectors.
The features are the Jaccard in-dices for the lemmas, noun lemmas, synsets, andnoun subjects in each sentence pair.
For both runs,the sentence pairs were parsed using the TreeTag-ger (Schmid, 1994).
The TreeTagger was used be-cause it provides the part-of-speech tag and lemmafor each word of a sentence.Run 1 used these features:?
The fraction of lemmas that were commonbetween the two sentences.
In other words,the number of unique lemmas common be-tween the sentences divided by the total num-ber of unique lemmas of the two sentences.633?
The fraction of noun lemmas common be-tween the two sentences.?
The fraction of synsets common between thetwo sentences.
For each noun, its correspond-ing synset1was extracted from the Span-ish WordNet (spaWN) of the MultilingualCentral Repository2(MCR 3.0) (Gonzalez-Agirre et al., 2012).Run 2 employed one more feature in additionto the aforementioned, which was the fraction ofsynsets of noun subjects that were common foreach sentence pair.
The subject nouns were ex-tracted from the sentences after parsing them withthe MaltParser (Nivre et al., 2007).
Since the Tree-Tagger PoS tagset3differed from the EAGLES(Expert Advisory Group on Language Engineer-ing Standards) tagset4required by the MaltParser,rules were written to best translate the TreeTag-ger tags into EAGLES tags.
However, one-to-one mapping was not possible: EAGLES tags areseven characters long and encode number and gen-der, whereas TreeTagger tags do not.
For example,using the EAGLES tagset, the masculine singularcommon noun ?arbol ?tree?
is tagged as NCMS000,whereas the feminine singular common noun hoja?leaf?
is tagged as NCFS000; TreeTagger, on theother hand, tags both as NC.3 Results and ConclusionsTable 2 presents the performance, measured us-ing the Pearson correlation, of the approach.
Run1 achieved a weighted correlation of 0.66723 andranked 15th among 22 submissions to the task.Dataset Run 1 Run 2Training 0.83693 0.83773Wikipedia (Test) 0.61020 0.60425News (Test) 0.71654 0.70974Table 2: Performance of the Approach.Given that the approach relied mostly on bagof words, a modest performance was expected.The performance was also affected by the factthat the spaWN did not have synsets for most of1stored as synset offset in wei spa-30 variant.tsv2The resource can be obtained fromhttp://grial.uab.es/descarregues.php3http://www.cis.uni-muenchen.de/?schmid/tools/TreeTagger/data/spanish-tagset.txt4http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.htmlthe words.
Finally, converting TreeTagger tags tothose required by the MaltParser instead of usinga parser which annotates with EAGLES tags mayalso have contributed to the relatively low Run 2score.
However, the confidence intervals of thetwo runs obtained after bootstrapping overlapped.Thus, the difference between the two runs for boththe datasets is not statistically significant.AcknowledgementsI would like to thank Vlad Niculae,`Angels Catenaand Calvin Cheng for their inputs and feedback.ReferencesEneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWiebe.
2014.
?SemEval-2014 Task 10: MultilingualSemantic Textual Similarity.?
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014).
Dublin, Ireland.Romaric Besanc?on, Martin Rajman, and Jean-C?edricChappelier.
1999.
Textual Similarities Based on aDistributional Approach.
In Proceedings of the 10thInternational Workshop on Database & Expert Sys-tems Applications.
180?184.
DEXA ?99.
Washing-ton, DC, USA: IEEE Computer Society.Aitor Gonzalez-Agirre, Egoitz Laparra, and GermanRigau.
2012.
Multilingual Central Repository ver-sion 3.0: upgrading a very large lexical knowledgebase.
In Proceedings of the Sixth InternationalGlobal WordNet Conference (GWC ?12).Aminul Islam and Diana Inkpen.
2008.
Semantic TextSimilarity Using Corpus-Based Word Similarity andString Similarity.
ACM Transactions on KnowledgeDiscovery from Data 2 (2): 1?25.Michael Herslund.
2010.
Predicati e sostantivi comp-lessi.
In Language, Cognition and Identity, eds.
IrnKorzen and Emanuela Cresti.
1?9.
Strumenti per LaDidattica E La Ricerca.
Firenze University Press.Michael Herslund.
2012.
Structures lexicales et ty-pologie.
In S?emantique et lexicologie des languesd?Europe, eds.
Louis Begioni and Christine Brac-quenier.
35?52.
Rivages Linguistiques.
Presses Uni-versitaires de Rennes.Rada Mihalcea, Courtney Corley, and Carlo Strap-parava.
2006.
Corpus-Based and Knowledge-BasedMeasures of Text Semantic Similarity.
In Proceed-ings of the 21st National Conference on Artifi-cial Intelligence.
775?80.
AAAI?06.
Boston, Mas-sachusetts: AAAI Press.Yuhua Li, David McLean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
2006.
Sentence Simi-larity Based on Semantic Nets and Corpus Statistics.634IEEE Transactions on Knowledge and Data Engi-neering, 18 (8): 1138?50.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?uls?en Eryi?git, Sandra K?ubler, SvetovlasMarinov, and Erwin Marsi.
2007.
MaltParser: Alanguage-independent system for data-driven depen-dency parsing.
Natural Language Engineering, 13(2): 95?135.Mehran Sahami and Timothy D. Heilman.
2006.
AWeb-Based Kernel Function for Measuring the Sim-ilarity of Short Text Snippets.
In Proceedings of the15th International Conference on World Wide Web,377?86.
WWW ?06.
New York, NY, USA: ACM.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
Proceedings of Inter-national Conference on New Methods in LanguageProcessing.
Manchester, UK.Thomas Ede Zimmermann.
2011.
Model-theoreticsemantics.
Semantics.
An International Handbookof Natural Language Meaning.
edited by ClaudiaMaienborn, Klaus von Heusinger, and Paul Portner.Vol.
1.
Berlin, Boston: De Gruyter Mouton.635
