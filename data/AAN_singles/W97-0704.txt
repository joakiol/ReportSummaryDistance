Automated Text Summarization in SUMMARISTEduard Hovy and ChinYew LinInformatmn Sciences Instituteof the University of Southern California4676 Admiralty WayMarina del Rey, CA 90292-6695, U S Atel +1-310-822-1511fax +1-310-823-6714emafl {hovy,cyl}@ml eduAbstractSUMMARIST is an attempt to create a robustautomated text summanzaUon system, basedon the 'equation' summarization = topwMent:ficatwn + mterpretatwn + generatwnWe descnbe the system's arclutecture andprovide detmls of some of its modules1 Introduction1.1 Summary:  Extract or  Abstract?The task of a Summarizer ~s to produce asynopsis of any document (or set of documents)submitted to It These synopses may range from alist of isolated keywords that mdlcate the majorcontent of the document(s), through a hst ofindependent single sentences that express themajor content, all the way up to a coherent, fullyplanned and generated paragraph that compressesthe document The more sophmUcated a synopsis,the more effort it generally takes to produceSeveral exJstmg systems, mcludmg some Webbrowsers, claun to perform text summarizationHowever, even a cursory analysis of their outputshows that their so-called summaries are actuallyportions of the text, produced verbatim Whilethere is nothing wrong with such extracts, per se, atruly comprehensive and mformaUve textsummary fuses together various concepts of thetext Into a sinaller number of  concepts, to forman abstract We define extracts as consistmgwholly of pomons extracted verbatim from theongmal (they may be single words or wholepassages) and abstracts as conssstmg of novelphrasings describing the content of the original(which might be paraphrases or fully newlysynthesized text) Generally, producing anabstract requires tages of topic fusion and textgeneration not needed for extracts ' -L2  SUMMARISTOver the past two years we have beendeveloping .
the text summarization, system.SUMMARIST- In this paper, we describe itsstructure and provide de~ls on the evaluatedresults of two of its component modulesThe goal of SUMMARIST is to provide bothextracts and absWacts for arbitrary English (andlater, other-language) input text SUMMARISTcombines ymbolic world knowledge (embodied mWordNet, dicUonanes, and s~mxlar resources) withrobust NLP processing (using IR and statisticaltechniques) to overcome the problems endemic toeither approach alone These .problems arisebecause exmtmg robust NLP methods tend tooperate at the word level, and hence miss concept-level generalizations, which are provided bysymbolic world knowledge, whale on the otherhand symbolic knowledge is too difficult o acqmrem large enough scale to provide coverage androbustness.
For robust summarization, bothaspects are neededThe heart of abstract formation Is theinterpretation process performed to fuse conceptsThis step occurs in the middle of thesummarization procedure, to find the appropriateset of concepts in an Input text, an initial stage ofconcept identification and extraction is required,to produce the summary, a final stage ofgeneration Is needed Thus SUMMARIST IS basedon the following 'equatson'summanzauon = topic ,denttficat, on +mterpretatwn + generation18IIIIIIIiliII!iIIl!iIIIIIThis breakdown is motivated as follows1.
Identification" Select or filter the input todetermine the most important, central, topicsFor generahty we assume that a text can havemany (sub)-toplcs, and that the topic extractionprocess can be parametertzed to include more orfewer of  them to produce longer or shortersummaries2.
Interpretation Sunply aggregatmgtogether frequently mentmned portions of theinput text does not m itself make an abstractWhat are the central, most important, concepts mthe following story9John and Bdl wanted moneyThey bought ski-masks and gunsand stole an old car from anetghbor Wearing their ski-masks and wavmg their guns,the two entered the bank, andwithin minutes left the bankwith several bags of $100 bdlsThey drove away happy,throwing away the ski-masksand guns m a sidewalk trash canThey were never caughtThe popular method of sunple word countingwould indicate that the story is about sk|-masksand guns, both of which are mentmned threetimes, more than any other word Clearly,however, the story is about a robbery, and anysummary of It must menUon th|s fact Someprocess of interpreting the mdlwdual words as partof some encompassing concept is requued Onesuch process, word clustenng, ~s an essentmltechnique for topic =dent=ficaUon m IR Thistechmque would match the words "gun", "mask","money", "caught", "stole", etc, against he setof words that form the so-called signature for theword "robbery" Other, more soph|sttcated formsof word clustering and fusion are possible,mcludmg script matchmg, deductive reference, andconcept clustenng3.
Generat ion Two options exist either theoutput is a verbatim quotaUon of some portion(s)of the input, or ~t must be generated anew In theformer case, no generator is needed, but the outputis not lflcely to be htgh-quahty ext (although thismight be sufficient for the apphcatlon)2 The Structure of SUMMARISTFor each of the three steps of the  above'equation', SUMMARIST uses a mixture ofsymbolic world knowledge (from WordNvt andslmdar resources) and statistical or IR-basedtechniques Each stage employs everal different,complementary, methods (SUMMARIST willeventually contain several modules m each stage)To date, we have developed some methods foreach stage of processing, and are busy developingadditional methods and lmkmg them rote a singlesystem In the next sections we describe onemethod from each stage The overall architectureis shown m Figure 1Figure 1 Architecture of SUMMARIST19Is e ,  the title (TI) is the most hkely to bear topics, I2,1 Tonic Identification followed by the first sentence of paragraph 2, the~- ?
first sentence of paragraph 3, etc In contrast, for I ISeveral techmques for topic identification have the Wall Street Journal the OPP is Bbeen reported in the hterature, including methods r-r1 P1S1 P1S2 1based on Posmon \[Luhn 58, Edmundson 69\], Cue L- -, , .
, J IllPhrases \[Baxendale 58\], word frequency, and Evaluation.
We evaluated the OPP method m ElDiscourse Segmentation ~darcu 97\] various ways.
In one of them, coverage.is the m. .
.
.
.
.
t?action of the (human-supphed) keywords thatwe ae~oe nere just our  work on are included verbatim m the sentences elected IISUMMARIST s Positaon module.
This method under the nohcv (A random selectmn nohcvexploits the fact that m some genres, regularities would extra~ct sentences with a random di~._.butionof d~conrse structure _ and/or, methods of  of topics, a good position policy would extractexposmon mean mat certain sentence posmons rich ton c anna ~ t e I I  i-be___,~ sentences We measured _h_ Eltend to  carry more topic matenal than others .
.
.
.
.
.
r -  - .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
effectiveness of an OPP by taking cumulatively Iwe  aermea me upt:ma: J\["O..flllOn J"Oll~ ~U~I'*) as  mnr'~, n f  ItQ e~ntenc~.~ f i r s t  m~t th~ bile.
then  th~a list that indicates m what ordmal positaons in the t,-tl~'~l'~,~'p?gl""~'nd-~n'~n-" Oln'n--ra~rt'~"dot~'~-~text hlgh-topm-beanng sentences occur We th'~.-e~F~t'o~'muTtl~n-rd'kev'nl~ra~'~'~v~m'ate.
'"~"d Ideveloped a method of automatically trammg new ~n~ w"-~'n'an~w nf ' ;~c'~ea"~,~o-~-~ l"~,~'~'~v~OPPs, given a collection of genre-related texts ~v~r '~ l~"~'~l~ '~ '~ 'e"~-~'~nr"~e ~-hn~v'~ ,with keywords This work, descnbed in \[Lm and ~;~~.
9-'h~-,~l~'-~--~l~vn-hv-~v~d~w'~,~,~'- ~ ' ,~ ' , ,~  IIHo'vy 97a\], Is the first systematic study and t~ge~'~e~ ' t'h'e"'muit-'~-w"-o~ c 'ntn%ut'm-ns "(wm~do-~,evaluation ot  the Position method reported sizes 1 to 5) in the top ten sentence posmonsFor the Ziff-Davis corpus (13,000 newspaper (R10), the columns reach 95% over an extract o f  I Iarticles announcing computer products) we have 10 sentences (approx 15% of a typm.al Zlff-Dawsfound that the OPP ss text) an extremely encouraging resultIT1, P2S1, P3S1, P4S1, PIS1, P2S2,{P3S2, P4S2, P5S1, P1S2}, P6S1, \] U1|mOBII 06 u " '~" " .
.
.
.
.
.
.
.
.
_ .?
u4I I~03 ."
i - m??
'? '
: i0?
-" C~I 03 ~"  tO ?0 ~ ~0 O~ 0OPP POSIT IONSFigure 2 Coverage scores for top ten OPP sentence posiuons, window sizes 1 to 5.202.2 Topic Interpretation (ConceptFusion)The second step m the summarization processis that of concept interpretation In this step, acollection of extracted concepts are 'fused' intotheir one (or more) higher-level unifyingconcept(s) Concept fusion can be as simple aspart-whole construction, for example when wheel,chain, pedal, saddle, hght,.frame, and handlebarstogether fuse to bicycle Generally, though, R Ismore complex, ranging from dlrect concept/word?
clustering as used in IR \[Paice 90\] to scnptallybased inference as in scripts \[Schank and Abelson77\]Fusing topics into one or more characterizingconcepts m the most difficult step of automatedtext summarizatmn Here, too, a variety ofmethods can be employed All of them assocmte aset of concepts (the mdwators) with acharacteristic generahzatlon (the fuser or head)The challenge is to develop methods that workreliably and to construct a large enough collectmnof mdicator-fuser sets to achieve effective topicreductionSUMMARIST's topic interpretation methodscurrently include Concept Wavefiont \[Lm 95\] andConcept Signature \[Lm and Hovy 97b\]2.2.1 Concept Counting and theWave frontA topw is a particular subject hat we writeabout or d~cuss To identify the topics of texts,IR researchers make the assumptmn that the morea word Is used in a text, the more important i Is mthat text.
But although word frequency countingoperates robustly across different domains withoutrelying on stereotypical text structure or semanticmodels, they cannot handle synonyms,pronommahzation, and other forms ofcoreferentlahty Furthermore, word countingmisses conceptual generahzatmnsJohn bought some vegetables, fi'u:t,bread, and milk -4 John bought somegrocer:esThe word counting method must be extended torecognize that vegetables, frmt, etc, relate togrocerzes Recogmzing this inherent problem,people started using Amficml Intelhgencetechmques \[Jacobs 90, Mauldm 91\] and statisticaltechmques \[Salton et al94\] to incorporatesemantac relataons among words Following thistrend, we have developed a new way to ldenUfytopics by counting concepts instead of words, andgenemhzmg them using a concept generahzatlontaxonomy As approximation to such ahierarchy, we employ WordNet \[Miller et al90\](though we could have used any machine-readablethesaurus) for inter-concept relatedness links Inthe hmR case, when WordNet does not containthe words, this technique defaults to wordcountingAs described in \[Lm 95\], we locate the mostappropnate generalization somewhere mmiddle ofthe taxonomy by finding concepts on theinteresting wavefront, a set of nodes representingconcepts that each generalize a set ofapproximately equally strongly representedsubconcepts (ones that have no obvious dominantsubconcept to specmhze to)Evaluation: We selected 26 amcles about newcomputer products from BusmessWeek (1993-94)of average 750 words each For each text weextracted the eight sentences containing the mostinteresting concepts using the wavefronttechnique, and comparing them to the contents ofa professional's abstracts of these 26 texts froman onhne service We developed several weightingand scoring variations and tried various rauo anddepth parameter se~ngs for the algorithm Wealso implemented a random sentence selectmnalgorithm as a baseline comparisonThe average recall (R) and precision (P) valuesover the three sconng vanatmns were RffiO 32 andpro 35, when the system produces extracts of 8sentences" In comparison, the random selectionmethod had RffiO 18 and PffiO 22 precismn in thesame experimental setting While these R and Pvalues are not tremendous, they show thatsemantic knowledge--even as limited as thatmWordNet--does enable unprovements overtraditional IR word-based techniques However,the hm~tations of WordNet are serious drawbacksthere is no domain-specific knowledge, forexample to relate customer, waiter, cashier, food,and menu together with restaurant We thusdeveloped a second technique of conceptinterpretation, using category s:gnatures Wediscuss this next212.2.2 Interpretation using Signatures?
Can one automatically find a set of relatedwords that can collectwely be fused into a singleword9 To test hai?
.Idea we developed the ConceptSignature method \[Lm and Hovy 97b\] Wedefined a signature to be a list of word mdlcators,each with relatwe strength of associatmn, jointlyassociated with the signature head.To construct signatures automatically, we useda set of 30,000 texts from the Wall Street Journal(1987) The Journal editors have classified eachtext into one of 32 classes---AROspace, BNKmg,ENVironment, ?TELecommunications, etc Wecounted the occurrences of each content word(canonicalized morphologically to remove?
plurals,etc ), m the texts of a class, relative to the numberof tunes they occur m the whole corpus (this isthe standard tftdf method) We then selected thetop-sconng 300 terms for each category andcreated a signature with the category name as itshead The top terms of four example slgnaturesare shown m Figure 3 It is qmte easy todetermine the idenUty of the signature head justby mspecUng the top few signature mdlcatorsRANK ARO "?
BNK ENV TELt contract bank epa at&t2 air_force thnft waste network3 aircraft banking environmental fcc4 navy loan water cbs5 army.
mr ozone cable6 space deposit state bell7 missile board ? "
incinerator long-distance8 equipment fslic ?agency telephone9 mcdonnell fed clean telecomm10 northrop institution landfill mcl11 nasa federal hazardous mrt 2 pentagon fdlc acid_ram doctrine13 defense volcker standard service'14 receive henkel federal news15 boeing banker lake turnerFigure 3 Pomons of the signatures of several concepts?
SUMMARIST will use signatures for summarycreatmn as follows After the topic identificationmodule(s) ldentifyhes a set of words or concepts,the signature-based concept interpretation modulewdl iden~fy the most pertinent signaturessubsummg the topic words, and the signature'shead concept will then be used as the summarizingfuser concepts Matching the identified topicterms against all signature indicators involvesseveral problems, mcludmg takmg rote accountthe relative frequencies of occurrence andresolwng matches wRh muRlple signatures, andspecifying thresholds of acceptablhtyEvaluation.
First, however, we had toevaluate the quality of the signatures formed byour algorithm Recogmzmg the similarity ofsignature recognmon to document categorization,we  evaluated the effectiveness of each signature byseeing how well R serves as a selectmn criterion onnew texts As data we used a set of?
2,204prewously unseen WSJ news articles from 1988For each test text, we created a single-text'document signature' usmg the same ~f:dfmeasureas before, and then matched this documentsignature against he category signatures Theclosest match provided the class mto which thetext was categorized We tested four differentmatching functions, mcludmg a simple binarymatch (count 1 if a term match occurs, 0otherwise), curve-fit match (mimmtze thedifference m occurrence frequency of each termbetween document and concept signatures), andcosine match (mmma~ze the cosine angle in thehyperspace formed when each signature is viewedas a vector and each word frequency specifies thedistance along the dimension for that word)22IIIIIIIllIIIIIIIIlIThese matching functions all prowdedapproximately the same results The values forRecall and Precmion ' (R--0 756625 andP---0 69309375).
are very .
encouraging andcompare well wah recent IR results \[TREC 95\]Extending this work will reqmre the crealaon ofconcept signatures for hundreds, and eventuallythousands, of different topics needed for robustsummartzatlon We plan to mvestagate theeffectiveness of a varterty of methods for doingthis2.3 Summary GenerationThe final step in the summarization process hsto  generate the summary, conslstmg of the fusedconcepts, m Enghsh A range of posslbdmesoccurs here, from sunple concept printing tosophlsUcated sentence p!annmg and surface-formreahzat~on Although, as mentioned m Section 1,s~mple extract summaries reqmre no generattonstage, eventually SUMMARIST wdl contain threegeneration modules, assocmted as approprmte withthe various levels for various apphcatlons1 Topzc output Sometimes no summary Isreally needed, a simple hst of the summartzmgtopics ~s enough SUMMARIST wall print the fuserconcepts produced by stage 2 of the process,sorted by decreasing nnportance2 Phrase concatenatzon SUMMARIST wdlmclude a rudimentary generator that composesnoun phrase- and clause-stzed umts into stmplesentences It wdl extract the noun phrases andclauses from the mput text, by following hnksfrom the fuser concepts through the words .thatsupport hem back into the mput text3 Full sentence planmng and generattonSUMMARIST wdl employ the sentence plannerbeing bruit at ISI (m collaboration with theHealthDoe project from the Umverslty ofWaterloo) \[Hovy and Wanner 96\], together witha sentence generator such as Penman \[Penman 88,Matthlessen and Bateman 91\], FUF \[Eihadad 92\],or NitroGen \[Kmght and Hatmvassdoglou 95\] toproduce well-formed, fluent, summaries, takmg asinput the fuser concepts and their most closelyrelated concepts as Identified by SUMMARIST'stopic ldenUficatlon stage3 ConclusionAs outhned .in.
Section I, extract summariesreqmre only the stage of topic identification Byincluding modules to perform topic interpretationand summary generaUon, SUlVlMARIST will also beable to produce abstract summaries How well ~twdl do so ts a matter for future mvemgatlonAn important aspect to be addressed is thecombination of the outputs of various modules meach stage We plan to investigate differentapproaches, from a simple combination by votesto methods for automattcally training relattvestrengths of contributionAutomated summarmatlon LS sunultaneously anold topic--work on tt dates from the 1950's----anda new toplc--tt ts so difficult that mterestlngheadway can be made for many years to comeWe are excited about the posslbflmes offered bythe combination of semantic and statmtlcaltechmques m what is, qmte possibly, the mostcomplex task of all NLPReferences\[Baxendale 58\] Baxendale, PB 1958Machine-made index for techmcalhteraturePan experiment IBM Journal(354-361), October\[Edmundson 69\] Edmundson, H P 1968New methods m automaUc extraeuon In?, (264---285)\[Elhadad 92\] EIhadad, M 1992 UsingArgumentatton to Control LexlcalChmce A Functional Un~catton-BasedApproach Ph D dlssertatton, ColumbiaUmverslty\[Hovy and Wanner 96\] Hovy, E H and LWanner 1996 Managing SentencePlanning Reqmrements In Proceedmgso f  the Workshop on Planmng andGeneratton (with ECAI) Budapest,Hungary\[Jacobs 90\] Jacobs, P S and L F Rau 1990SCISOR Extracting mformauon fromon-hne news Commumcattons of theACM 33(11), (88-97)\[Kmght and Hatmvassdoglou 95\] Kmght, K-and V Hatmvassdoglou 1995 Two-level many-paths - generation InProceedings of the 33rd ACLConference, Boston, MA23\[Lm 95\] Lm, C Y .1995 TopicIdentlficalaon by Concept..,.
Generahzatlon.
I  Proceedmgs of the33rd ACL Conference, Boston, MA -~..\[Lln and Hovy 97a\] Lm, C Y and EHHovy 1997a Identifying Topics byPosRlon In Proceedmgs of the ApphedNatural Language ProcessmgConference, Washington, DC\[Lm and Hovy 97b\] Lm, CY and EHHovy .
1997b Automatic TextCategonzaaon A Concept-BasedApproach In prep\[Luhn 58\] Luhn, H P 1959 The automaticcreauon of hterature abstracts IBMJournal of Research and Development(159-165)\[Marcu 97\] Marcu, D 1997 The RhetoncalParsing of Natural Language TextsSubrmtted\[Matthlesseu and Bateman 91\] Matthlessen,CM.IM and JA Bateman 1991 TextGeneratwn and Systemtc-FuncuonalLmgulsucs London, England Prater\[Manldm 91\] Mauldm, ML 1991Conceptual lnformauon Retneval--ACase Study m Adapttve Parttal ParsmgKluwer Acadermc Pubhshers, Boston,MA\[McKeown and Radev 95\] McKeown, K Rand D R Radev 1995 Generatingsurnmanes of muluple news amcles InProceedings of the 18th InternauonalACM SIGIR Conference, (74-82),Seattle, WA ?\[1Vhller t ai 90\] Miller, G R BeckwRh, CFellbaum, D. Gross, and K Miller 1990Five papers on WordNet CSL Report 43,Cognmve Science Laboratory, PrincetonUmversRy, Princeton, NJ\[Pmce 90\] Pmce, C D 1990 Constructingliterature abstracts by computerTechniques and prospects InformatzonProcessing and Management, 26(1),(171-186)\[Penman 88\] The Penman Pruner, UserGrade, and Reference Manual 1988Unpubhshed documentatmn, USCInformation Sciences Insumte\[Salton et al 94\] Salton, G, J Allen, CBuckley, and A Smghal 1994AutomaUc analym, theme generauon,and summarization f machine-readabletexts SSczence 264, (1421-1426), June\[Schank and Abelson 77\] Schank, RC andR P Abelson 1977 Scripts, Plans,Goals, and Understanding LawrenceErlbaum Associates, Hlllsdale, N.l\[TREC 95\] Harman, D (ed) 1995Proceedings of the TREC Conference.24iIIIiIiIIIIIIIIIIII
