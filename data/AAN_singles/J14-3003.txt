A Random Walk?Based Model forIdentifying Semantic OrientationAhmed Hassan?Microsoft ResearchAmjad Abu-Jbara?
?University of MichiganWanchen Lu?University of MichiganDragomir Radev?University of MichiganAutomatically identifying the sentiment polarity of words is a very important task that hasbeen used as the essential building block of many natural language processing systems such astext classification, text filtering, product review analysis, survey response analysis, and on-linediscussion mining.
We propose a method for identifying the sentiment polarity of words thatapplies a Markov random walk model to a large word relatedness graph, and produces a polarityestimate for any given word.
The model can accurately and quickly assign a polarity sign andmagnitude to any word.
It can be used both in a semi-supervised setting where a training set oflabeled words is used, and in a weakly supervised setting where only a handful of seed words isused to define the two polarity classes.
The method is experimentally tested using a gold standardset of positive and negative words from the General Inquirer lexicon.
We also show how ourmethod can be used for three-way classification which identifies neutral words in addition topositive and negative words.
Our experiments show that the proposed method outperforms thestate-of-the-art methods in the semi-supervised setting and is comparable to the best reportedvalues in the weakly supervised setting.
In addition, the proposed method is faster and does notneed a large corpus.
We also present extensions of our methods for identifying the polarity offoreign words and out-of-vocabulary words.?
Microsoft Research, Redmond, WA, USA.
E-mail: hassanam@microsoft.com.
This research wasperformed while at the University of Michigan.??
Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA.E-mail: amjbara@umich.edu.?
Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA.E-mail: wanchlu@umich.edu.?
Department of Electrical Engineering & Computer Science and School of Information, University ofMichigan, Ann Arbor, MI, USA.
E-mail: radev@umich.edu.Submission received: 15 November 2011; revised submission received: 10 May 2013; accepted for publication:14 July 2013.doi:10.1162/COLI a 00192?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 31.
IntroductionIdentifying emotions and attitudes from unstructured text has a variety of possibleapplications.
For example, there has been a large body of work for mining productreputation on the Web (Morinaga et al.
2002; Turney 2002).
Morinaga et al.
(2002)have shown how product reputation mining helps with marketing and customer re-lation management.
The Google products catalog and many on-line shopping siteslike Amazon.com provide customers not only with comprehensive information andreviews about a product, but also with faceted sentiment summaries.
Such systems areall supported by a sentiment lexicon, some even in multiple languages.Another interesting application is mining on-line discussions.
An enormous num-ber of discussion groups exist on the Web.
Millions of users post content to these groupscovering pretty much every possible topic.
Tracking a participant attitude toward differ-ent topics and toward other participants is a very important task that makes use of sen-timent lexicons.
For example, Tong (2001) presented the concept of sentiment timelines.His system classifies discussion posts about movies as either positive or negative.
Thisis used to produce a plot of the number of positive and negative sentiment messagesover time.
All these applications would benefit from an automatic way of identifyingsemantic orientation of words.In this article, we study the task of automatically identifying the semantic orienta-tion of any word by analyzing its relations to other words, Automatically classifyingwords as positive, negative, or neutral enables us to automatically identify the polarityof larger pieces of text.
This could be a very useful building block for systems thatmine surveys, product reviews, and on-line discussions.
We apply a Markov randomwalk model to a large semantic relatedness graph, producing a polarity estimate forany given word.
Previous work on identifying the semantic orientation of words hasaddressed the problem as both a semi-supervised (Takamura, Inui, and Okumura 2005)and a weakly supervised (Turney and Littman 2003) learning problem.
In the semi-supervised setting, a training set of labeled words is used to train the model.
In theweakly supervised setting, only a handful of seeds are used to define the two polarityclasses.Our proposed method can be used both in a semi-supervised and in a weaklysupervised setting.
Empirical experiments on a labeled set of positive and negativewords show that the proposed method outperforms the state-of-the-art methods in thesemi-supervised setting.
The results in the weakly supervised setting are comparable tothe best reported values.
The proposed method has the advantages that it is faster anddoes not need a large training corpus.The rest of the article is structured as follows.
In Section 2, we review related workon word polarity and subjectivity classification and note applications of the randomwalk and hitting times framework.
Section 3 presents our method for identifying wordpolarity.
We describe how the proposed method can be extended to cover foreignlanguages in Section 4, and out-of-vocabulary words in Section 5.
Section 6 describesour experimental set-up.
We present our conclusions in Section 7.2.
Related Work2.1 Identifying Word PolarityHatzivassiloglou and McKeown (1997) proposed a method for identifying the wordpolarity of adjectives.
They extract all conjunctions of adjectives from a given corpus540Hassan et al.
A Random Walk?Based Model for Identifying Semantic Orientationand then they classify each conjunctive expression as either the same orientation suchas ?simple and well-received?
or different orientation such as ?simplistic but well-received.?
The result is a graph that they cluster into two subsets of adjectives.
Theyclassify the cluster with the higher average frequency as positive.
They created andlabeled their own data set for experiments.
Their approach works only with adjectivesbecause there is nothing wrong with conjunctions of nouns or verbs with oppositepolarities (?war and peace?, ?rise and fall?, etc.
).Turney and Littman (2003) identify word polarity by looking at its statistical asso-ciation with a set of positive/negative seed words.
They use two statistical measuresfor estimating association: Pointwise Mutual Information (PMI) and Latent SemanticAnalysis (LSA).
To get co-occurrence statistics, they submit several queries to a searchengine.
Each query consists of the given word and one of the seed words.
They use thesearch engine NEAR operator to look for instances where the given word is physicallyclose to the seed word in the returned document.
They present their method as an un-supervised method where a very small number of seed words are used to definesemantic orientation rather than train the model.
One of the limitations of their methodis that it requires a large corpus of text to achieve good performance.
They use sev-eral corpora; the size of the best performing data set is roughly one hundred billionwords (Turney and Littman 2003).Takamura et al.
(2005) propose using spin models for extracting semantic orienta-tion of words.
They construct a network of words using gloss definitions, thesaurus, andco-occurrence statistics.
They regard each word as an electron.
Each electron has a spinand each spin has a direction taking one of two values: up or down.
Two neighboringspins tend to have the same orientation from an energy point of view.
Their hypothesisis that as neighboring electrons tend to have the same spin direction, neighboring wordstend to have similar polarity.
They pose the problem as an optimization problem anduse the mean field method to find the best solution.
The analogy with electrons leadsthem to assume that each word should be either positive or negative.
This assumptionis not accurate because most of the words in the language do not have any semantic ori-entation.
They report that their method could get misled by noise in the gloss definitionand their computations sometimes get trapped in a local optimum because of its greedyoptimization flavor.Kamps et al.
(2004) construct a network based on WordNet (Miller 1995) synonymsand then use the shortest paths between any given word and the words ?good?
and?bad?
to determine word polarity.
They report that using shortest paths could be verynoisy.
For example, ?good?
and ?bad?
themselves are closely related in WordNet witha 5-long sequence ?good, sound, heavy, big, bad.?
A given word w may be moreconnected to one set of words (e.g., positive words); yet have a shorter path connectingit to one word in the other set.
Restricting seed words to only two words affects theiraccuracy.
Adding more seed words could help but it will make their method extremelycostly from the computation point of view.
They evaluate their method using onlyadjectives.Hu and Liu (2004) propose another method that uses WordNet.
They use WordNetsynonyms and antonyms to predict the polarity of words.
For any word whose polarityis unknown, they search WordNet and a list of seed labeled words to predict its polarity.They check if any of the synonyms of the given word has known polarity.
If so, theylabel it with the label of its synonym.
Otherwise, they check if any of the antonymsof the given word has known polarity.
If so, they label it with the opposite label ofthe antonym.
They continue in a bootstrapping manner until they label all possiblewords.541Computational Linguistics Volume 40, Number 32.2 Building Sentiment LexiconsA number of other methods try to build lexicons of polarized words.
Esuli andSebastiani (2005, 2006) use a textual representation of words by collating all the glossesof the word as found in some dictionary.
Then, a binary text classifier is trained usingthe textual representation and applied to new words.Kim and Hovy (2004) start with two lists of positive and negative seed words.
Word-Net is used to expand these lists.
Synonyms of positive words and antonyms of negativewords are considered positive, and synonyms of negative words and antonyms of posi-tive words are considered negative.
A similar method is presented in Andreevskaia andBergler (2006), where WordNet synonyms, antonyms, and glosses are used to iterativelyexpand a list of seeds.
The sentiment classes are treated as fuzzy categories where somewords are very central to one category, whereas others may be interpreted differently.Mohammad, Dunne, and Dorr (2009) utilize the marking theory, which states thatovertly marked words such as dishonest, unhappy, and impure tend to have negativesemantic orientations whereas their unmarked counterparts (honest, happy, and pure)tend to have positive semantic orientation.
They use a set of 11 antonym-generating affixpatterns to generate overtly marked words and their counterparts from the MacquarieThesaurus.
After obtaining a set of 2,600 seeds by the affix patterns, they expand thesentiment lexicon using a Roget-like thesaurus.
Their method does not require seedsentiment words or WordNet, but still needs a comprehensive thesaurus.
The idea ofthe marking theory is language-dependent and cannot be applied from one language toanother.Contrasting the dictionary based approaches that rely on resources such as Word-Net, Velikovich et al.
(2010) investigated the viability of learning sentiment lexiconssemi-automatically from the Web.
Kanayama and Nasukawa (2006) use syntactic fea-tures and context coherency (i.e., the tendency for same polarities to appear succes-sively) to detect polar clauses.2.3 Random Walk?Based MethodsClosest to our work in its methodology is probably the line of research on semi-supervised graphical methods for sentiment classification.
Rao and Ravichandran(2009) build a lexical graph similar to ours.
The graph is constructed of both unlabeledand labeled nodes, each node representing a word that can be either positive or neg-ative, and each edge representing some semantic relatedness that can be constructedusing resources like WordNet or other thesaurus.
They evaluate two semi-supervisedmethods: Mincut (including its variant, Randomized Mincut) and label propagation.The general idea of label propagation is defining a probability distribution over thepositive and negative classes for each node in the graph.
A Markov random walk isperformed on the graph to recover this distribution for the unlabeled nodes.Additionally, Rao and Ravichandra (2009) and Blair-Goldensohn et al.
(2008) use asimilar label propagation method on a lexical graph built from WordNet, where a smallset of words with known polarities are used as seeds.
Brody and Elhadad (2010) uselabel propagation over a graph constructed of adjectives only.Velikovich et al.
(2010) compare label propagation with a Web-based method andconclude that label propagation is not suitable when the whole Web is used as abackground corpus, because the constructed graph is very noisy and contains manydense subgraphs, unlike the lexical graph constructed from WordNet.542Hassan et al.
A Random Walk?Based Model for Identifying Semantic OrientationRandom walk?based methods have been studied in the context of many other NLPtasks.
For example, Kok and Brockett (2010) construct a graph from bilingual parallelcorpora, where each node represents a phrase and two nodes are connected by an edgeif they are aligned in a phrase table.
Then they compute hitting time of random walksto learn paraphrases.Our work is different from previous random walk methods in that it uses the meanhitting time as the criterion for assigning polarity labels.
Our experiments showed thatthis achieves better results than methods that use label propagation.2.4 Subjectivity AnalysisSubjectivity analysis is another research line that is closely related to our work.
Themain task in subjectivity analysis is to identify text that presents opinion as opposed toobjective text that present factual information (Wiebe 2000).
Text could be either words,phrases, sentences, or other chunks.
Wiebe et al.
(2001) list a number of applications ofsubjectivity analysis such as classifying e-mails and mining reviews.
For example, toanalyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed fromindividual sentences as nodes to determine whether a sentence is subjective or objective.Each node (sentence) has an individual subjectivity score obtained from a first-passclassifier using sentence features and linguistic knowledge.
Edges are weighted by asimilarity metric of how likely it is that the two sentences will be in the same subjectivityclass.
All sentences to be classified are represented as unlabeled nodes and the only twolabeled nodes represent the subjective and objective classes.
A Mincut algorithm is thenperformed on the constructed graph to obtain the subjectivity classes for individualsentences.
The authors also integrate the subjectivity classification of isolated sentencesto document level sentiment analysis.There are two main categories of work on subjectivity analysis.
In the first cate-gory, subjective words and phrases are identified without considering their context(Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008).
Inthe second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloffand Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005).
Wiebe andMihalcea (2006a) studied the association of word subjectivity and word sense.
Theyshowed that different subjectivity labels can be assigned to different senses of the sameword.
Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articlesfrom a wide variety of news sources manually annotated for opinions and other privatestates (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinionsand emotions in language.In addition, there has been a large body of work on labeling subjectivity of WordNetwords.
Wiebe and Mihalcea (2006b) label word senses in WordNet as subjective or ob-jective, utilizing the MPQA corpus.
They show that subjectivity information for Word-Net senses can improve word sense disambiguation tasks for subjectivity ambiguouswords.Su and Markert (2009) propose a semi-supervised minimum cut framework to labelword sense entries in WordNet with subjectivity information.
Their method requiresless training data other than the sense definitions and relational structure of WordNet.2.5 Word Polarity Classification for Foreign LanguagesWord sentiment and subjectivity has also been studied for languages other than English.Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity543Computational Linguistics Volume 40, Number 3lexicon based on an English lexicon, an on-line translation service, and Wordnet.Mihalcea and Banea (2007) use bilingual resources such as a bilingual dictionary or aparallel corpus to generate subjectivity analysis resources for foreign languages.
Raoand Ravichandran (2009) adapt their label propagation model to Hindi using HindiWordNet and French using a French thesaurus.3.
ApproachWe use a Markov random walk model to identify the polarity of words.
Assume thatwe have a network of words, some of which are labeled as either positive or nega-tive.
In this network, two words are connected if they are related.
Different sourcesof information are used to decide whether two words are related.
For example, thesynonyms of a word are all semantically related to it.
The intuition behind connect-ing semantically related words is that those words tend to have similar polarities.Now imagine a random surfer walking along the network starting from an unlabeledword w.The random walk continues until the surfer hits a labeled word.
If the word w ispositive then the probability that the random walk hits a positive word is higher, and ifw is negative then the probability that the random walk hits a negative word is higher.Thus, if the word w is positive then the average time it takes a random walk starting atw to hit a positive node should be much less than the average time it takes a randomwalk starting at w to hit a negative node.
If w doesn?t have a clear polarity and we wouldlike to say that it is neutral, we expect that the positive hitting time and negative hittingtime to not have a significant difference.We describe how we construct a word relatedness graph in Section 3.1.
The randomwalk model is described in Section 3.2.
Hitting time is defined in Section 3.3.
Finally,an algorithm for computing a sign and magnitude for the polarity of any given wordis described in Section 3.4.3.1 Network ConstructionWe construct a network where two nodes are linked if they are semantically related.Several sources of information are used as indicators of the relatedness of words.
Onesuch source is WordNet (Miller 1995).
WordNet is a large lexical database of English.Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms(synsets), each expressing a distinct concept (Miller 1995).
Synsets are interlinked bymeans of conceptual-semantic and lexical relations.The simplest approach is to connect words that occur in the same WordNet synset.We can collect all words in WordNet, and add links between any two words thatoccur in the same synset.
The resulting graph is a graph G(W, E) where W is a setof word/part-of-speech (POS) pairs for all the words in WordNet.
E is the set ofedges connecting each pair of synonymous words.
Nodes represent word/POS pairsrather than words because the part of speech tags are helpful in disambiguatingthe different senses for a given word.
For example, the word ?fine?
has two dif-ferent meanings, with two opposite polarities when used as an adjective and as anoun.Several other methods can be used to link words.
For example, we can use otherWordNet relations: hypernyms, similar to, and so forth.
Another source of links be-tween words is co-occurrence statistics from a corpus.
Following the method presented544Hassan et al.
A Random Walk?Based Model for Identifying Semantic Orientationin Hatzivassiloglou and McKeown (1997), we can connect words if they appear togetherin a conjunction in the corpus.
This method is only applicable to adjectives.
If twoadjectives are connected by ?and,?
it is highly likely that they have the same semanticorientation.
In all our experiments, we restricted the network to only WordNet relations.We study the effect of using co-occurrence statistics to connect words later at the end ofour experiments.
If more than one relation exists between any two words, the strengthof the corresponding edge is adjusted accordingly.3.2 Random Walk ModelImagine a random surfer walking along the word relatedness graph G. Starting from aword with unknown polarity i, it moves to a node j with probability Pij after the firststep.
The walk continues until the surfer hits a word with known polarity.
Seed wordswith known polarity act as an absorbing boundary for the random walk.
If we repeatthe number of random walks N times, the percentage of times in which the walk ends ata positive/negative word could be used as an indicator of its positive/negative polarity.The average time a random walk starting at w takes to hit the set of positive/negativenodes is also an indicator of its polarity.
This view is closely related to the partially la-beled classification with random walks approach in Szummer and Jaakkola (2002) andthe semi-supervised learning using harmonic functions approach in Zhu, Ghahramani,and Lafferty (2003).Let W be the set of words in our lexicon.
We construct a graph whose nodes V areall words in W. Edges E correspond to the relatedness between words.
We define thetransition probability Pt+1|t( j|i) from i to j by normalizing the weights of the edges outof node i, so:Pt+1|t( j|i) = Wij/?kWik (1)where k represents all nodes in the neighborhood of i. Pt+1|t( j|i) denotes the transitionprobability from node i at step t to node j at time step t + 1.
We note that the matrix ofweights Wij is symmetric whereas the matrix of transition probabilities Pt+1|t( j|i) is notnecessarily symmetric because of the node outdegree normalization.3.3 First-Passage TimeThe mean first-passage (hitting) time h(i|k) is defined as the average number of steps arandom walker, starting in state i 6= k, will take to enter state k for the first time (Norris1997).
Let G = (V, E) be a graph with a set of vertices V and a set of edges E. Considera subset of vertices S ?
V. Consider a random walk on G starting at node i 6?
S. Let Ntdenote the position of the random surfer at time t. Let h(i|S) be the average number ofsteps a random walker, starting in state i 6?
S, will take to enter a state k ?
S for the firsttime.
Let TS be the first-passage for any vertex in S.P(TS = t|N0 = i) =?j?Vpij ?
P(TS = t?
1|N0 = j) (2)545Computational Linguistics Volume 40, Number 3h(i|S) is the expectation of TS.
Hence:h(i|S) = E(TS|N0 = i)=??t=1t?
P(TS = t|N0 = i)=?
?t=1t?j?VpijP(TS = t?
1|N0 = j)=?j?V??t=1(t?
1)pijP(TS = t?
1|N0 = j)+?j?V?
?t=1pijP(TS = t?
1|N0 = j)=?j?Vpij?
?t=1tP(TS = t|N0 = j) + 1=?j?Vpij ?
h( j|S) + 1 (3)Hence the first-passage (hitting) time can be formally defined as:h(i|S) ={0 i ?
S?j?V pij ?
h( j|S) + 1 otherwise(4)3.4 Word Polarity CalculationBased on the description of the random walk model and the first-passage (hitting)time above, we now propose our word polarity identification algorithm.
We begin byconstructing a word relatedness graph and defining a random walk on that graph asdescribed above.
Let S+ and S?
be two sets of vertices representing seed words that arealready labeled as either positive or negative, respectively.For any given word w, we compute the hitting time h(w|S+) and h(w|S?)
for thetwo sets iteratively as described earlier.
The ratio between the two hitting times is thenused as an indication of how positive/negative the given word is.
This is useful incase we need to provide a confidence measure for the prediction.
This could be usedto allow the model to abstain from classifying words when the confidence level is low.It also means that our method can be easily extended from two-way classification (i.e.,positive or negative) to three-way classification (positive, negative, or neutral).
This canbe done by setting a threshold ?
on the ratio of positive and negative hitting time,and classifying a word to positive or negative only when the two hitting times havea significant difference; otherwise we classify it to neutral.When the relatedness graph is very large, computing hitting time as describedearlier may be very time consuming.
The graph constructed from the English WordNet546Hassan et al.
A Random Walk?Based Model for Identifying Semantic OrientationAlgorithm 1 3-class word polarity using random walks (parameter ?
: 0 < ?
< 1)Require: A word relatedness graph G1: Given a word w in V2: Define a random walk on the graph.
The transition probability between any twonodes i, and j is defined as: Pt+1|t( j|i) = Wij/?k Wik3: Start k independent random walks from w with a maximum number of steps m4: Stop when a positive word is reached5: Let h?
(w|S+) be the estimated value for h(w|S+)6: Repeat for negative words computing h?(w|S?
)7: if h?
(w|S+) ?
?h?(w|S?)
then8: Classify w as positive9: else if h?(w|S?)
?
?h?
(w|S+) then10: Classify w as negative11: else12: Classify w as neutral13: end ifand synsets contains 155,000 nodes and 117,000 edges.
To overcome this problem, wepropose a Monte Carlo?based algorithm (Algorithm 1) for estimating it.In the case of binary classification, where each word must be either positive ornegative, if h(w|S+) is greater than h(w|S?
), the word is classified as negative andpositive otherwise.
This can be achieved by setting parameter ?
= 1 in Algorithm 1.4.
Foreign Word PolarityAs we mentioned earlier, a large body of research has focused on identifying thesemantic orientation of words.
This work has almost exclusively dealt with English anduses several language-dependent resources.
When we try to apply these methods toother languages, we run into the problem of the lack of resources in other languageswhen compared with English.
For example, the General Inquirer lexicon (Stone et al.1966) has thousands of English words labeled with semantic orientation.
Most of theliterature has used it as a source of labeled seeds or for evaluation.
Such lexicons arenot readily available in other languages.As we showed earlier, WordNet (Miller 1995) has been used for this task.
How-ever, even though W have been built for other languages, their coverage is relativelylimited when compared to the English WordNet.
The current release of English Word-Net (WordNet 3.0) includes over 155K words and over 117K synsets.
Looking at theresources for other languages, the Arabic WordNet (Black et al.
2006; Elkateb et al.2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al.
2001; Narayanet al.
2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets inSpanish, 15K in German, and 22K in French, among other European languages.
In somecases, accuracy was traded for coverage.
For example, the current release of the JapaneseWordNet has 57K synsets but contains errors in as many as 5% of the entries.1In this section, we show how we can extend the methods presented earlier to predictthe semantic orientation of foreign words.
The proposed method is based on creating1 http://nlpwww.nict.go.jp/wn-ja/index.en.html.547Computational Linguistics Volume 40, Number 3a multilingual network of words that represents both English and foreign words.
Thenetwork has English?English connections, as well as Foreign?Foreign connections andEnglish?Foreign connections.
This allows us to benefit from the richness of the resourcesbuilt for the English language and at the same time utilize resources specific to foreignlanguages.
We define a random walk model over the multilingual network and pre-dict the semantic orientation of any given word by comparing the mean hitting timeof a random walk starting from it to a positive and a negative set of seed Englishwords.We use Arabic and Hindi in our experiments.
We compare the performance of sev-eral methods using the foreign language resources only, and the multilingual networkthat has both English and foreign words.
We show that bootstrapping from languageswith dense resources such as English is useful for improving the performance on otherlanguages with limited resources.4.1 Multilingual Word NetworkWe build a network G(V, E) where V = Ven ?
Vfr is the union of the sets of English andForeign words.
E is a set of edges connecting nodes in V. There are three types of connec-tions: English?English connections, Foreign?Foreign connections, and English?Foreignconnections.
For the English?English connections, we use the same methodology as inSection 3.Foreign?Foreign connections are created in a similar way to the English con-nections.
Some foreign languages have lexical resources based on the design of thePrinceton English WordNet.
For example: Euro WordNet (Vossen 1997), Arabic Word-Net (Black et al.
2006; Elkateb et al.
2006a, 2006b), and the Hindi WordNet (Jha et al.2001; Narayan et al.
2002).
We also use co-occurrence statistics similar to the work ofHatzivassiloglou and McKeown (1997).Finally, to connect foreign words to English words, we use a Foreign to English dic-tionary.
For every word in a list of foreign words, we look up its meaning in a dictionaryand add an edge between the foreign word and every other English word that appearedas a possible meaning for it.
If there is no comprehensive enough dictionary available,constructing a multilingual word network like a translation graph (Etzioni et al.
2007)may be a resolution.4.2 Foreign Word Semantic Orientation PredictionWe use the multilingual network described previously to predict the semantic orien-tation of words based on the mean hitting time to two sets of positive and negativeseeds.
Given two lists of seed English words with known polarity, we define two setsof nodes S+ and S?
representing those seeds.
For any given word w, we calculate themean hitting time between w and the two seed sets h(w|S+) and h(w|S?).
If h(w|S+)is greater than h(w|S?
), the word is classified as negative; otherwise it is classified aspositive.
We used the list of labeled seeds from Hatzivassiloglou and McKeown (1997)and Stone et al.
(1966).5.
Out-of-Vocabulary WordsWe observed that a significant portion of the text used on-line in discussions, comments,product reviews, and so on, contains words that are not defined in WordNet or in548Hassan et al.
A Random Walk?Based Model for Identifying Semantic Orientationstandard dictionaries.
We call these words Out-of-Vocabulary (OOV) words.
Table 6later in this article shows some OOV word examples.
To show the importance of OOVword polarity identification, we calculated the proportion of OOV words in threecorpora used for sentiment studies: a set of movie reviews, a set of on-line discussionsfrom a political forum, and a set of randomly sampled tweets.
For each word in thedata, we look it up in two standard English dictionaries, together containing 160Kunique words.
Table 1 shows the statistics.OOV words have a high chance of being polarized because people tend to useinformal language or special acronyms to emphasize their attitudes or impress theaudience.
Therefore, being able to automatically identify the polarity of OOV wordswill essentially benefit real-world applications.Consider the graph G(W, E) described in Section 3.1.
So far, the only resource we useto construct the graph is WordNet synsets.
The first step in our approach to OOV wordpolarity identification is to find the words in WordNet that are related to an OOVword.
Next, we add the OOV words to our graph by creating a new node for each OOVword and adding an edge between each OOV word and each of its related words.
Oncewe have constructed the extended network, we use the random walk model describedin Section 3.2 to predict the polarity of each OOV word.5.1 Mining OOV Word Relatedness from the WebThere are several alternative methods of linking words in the graph.
Agirre et al.
(2009)studied the strengths and weaknesses of different approaches to term similarity andrelatedness.
They noticed that lexicographical methods such as the WordNet suffer fromthe limitation of lexicon coverage, which is the case here with OOV words.
To overcomethis limitation, we use a Web-based distributional approach to find the set of relatedwords to each OOV word.
We perform a Web search using the OOV word as a searchquery and retrieve the top S search results.
We extract the textual content of the retrievedresults and tokenize it.
After removing all the stop words, we compute the number oftimes each word co-occurs with the OOV word in the same document.
We rank thewords based on their co-occurrence frequency and return the top R words as the set ofrelated words to the given OOV word.We experimented with three different variants of this approach.
In the first variant,the frequency values of the co-occurring words are normalized by the lengths of theTable 1Proportion of OOV words in some corpora used for real world applications.
(Numbers inparentheses exclude words whose first letters are capitalized because they are likely to refer tonamed entities.
)corpus source # of words Percentageof OOVMovie reviews 3, 411 customer reviews from IMDb for themovie The Dark Knight (2008)10.7 M (9.5 M) 5.3 (2.7)Political forum 23K sentences from www.politicalforum.comon various topics381 K (348 K) 8 (6)tweets 0.6M random English tweets from twitter.com.
(We count a tweet as in English if at least halfof the words are English dictionary words.Tags and symbols were removed.
)7.1 M (5.9 M) 30 (27)549Computational Linguistics Volume 40, Number 3documents that contributed to the count of each word.
The intuition here is that longerdocuments contain more words and hence the probability that a word in the thatdocument is related to the search query (i.e., the OOV word) is lower than when thedocument is shorter.In the second variant, we only consider the words that appear in the proximity ofthe OOV word (i.e., within d words around the OOV word) when we compute the co-occurrence frequency.
The intuition here is that words that appear near the OOV wordare more likely to be semantically related than the words that appear far away.In the third variant, instead of searching the entire Web, we limit the search tosocial text.
In the experiments described subsequently, we search for the OOV wordsin tweets posted on Twitter.2 The intuition here is that searching the entire Web is likelyto return results that do not necessarily contain opinionated text?particularly becausemany words have different senses.
In contrast, the text written in a social context is morelikely to carry sentiment and express emotions.
This helps us find better related wordsthat suit our task.5.2 Word Network Extension with OOV WordsTo extend the graph to include OOV words, we start with the graph G(W, E) constructedfrom WordNet synsets.
For each OOV word that does not exist in G, we create a newnode w. We set the part of speech of w to unspecified.
Then we use the Web-based methoddescribed in the previous section to find a set of words that are most related to w. Finally,we create a link between each OOV word and each of its related words.
To predict thepolarity of an OOV word, we use the same random walk model described earlier.6.
ExperimentsWe performed experiments on the gold-standard data set for positive/negative wordsfrom the General Inquirer lexicon (Stone et al.
1966).
The data set contains 4, 206 words,1, 915 of which are positive and 2, 291 of which are negative.
Some of the ambiguouswords were removed, as in Turney (2002) and Takamura, Inui, and Okumura (2005).Some examples of positive/negative words are listed in Table 2.We use WordNet (Miller 1995) as a source of synonyms and hypernyms for theword relatedness graph.
We used the Reuters Corpus, Volume 1 (Lewis et al.
2004) togenerate co-occurrence statistics in the experiments that used them.
We used 10-foldcross-validation for all tests.
We evaluate our results in terms of accuracy.
Statisticalsignificance was tested using a two-tailed paired t-test.
All reported results are statisti-cally significant at the 0.05 level.
We perform experiments varying the parameters andthe network.
We also look at the performance of the proposed method for differentparts of speech, and for different confidence levels.
We compare our method to theSemantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spinmodel described in Takamura, Inui, and Okumura (2005), the shortest path methoddescribed in Kamps et al.
(2004), a re-implementation of the label propagation andMincut methods described in Rao and Ravichandran (2009), and the bootstrappingmethod described in Hu and Liu (2004).2 http://www.twitter.com.550Hassan et al.
A Random Walk?Based Model for Identifying Semantic OrientationTable 2Examples of positive and negative words.Positive Negativeable adjective abandon verbacceptable adjective abuse verbadmire verb burglar nounamazing adjective chaos nouncareful adjective contagious adjectiveease noun corruption nounguide verb lie verbinspire verb reluctant adjectivetruthful adjective wrong adjective6.1 Comparison with Other MethodsThis method could be used in a semi-supervised setting where a set of labeled words areused and the system learns from these labeled nodes and from other unlabeled nodes.Under this setting, we compare our method to the spin model described in Takamura,Inui, and Okumura (2005).
Table 3 compares the performance using 10-fold cross val-idation.
The table shows that the proposed method outperforms the spin model.
Thespin model approach uses word glosses, WordNet synonym, hypernym, and antonymrelations, in addition to co-occurrence statistics extracted from corpus.
The proposedmethod achieves better performance by only using WordNet synonym, hypernym, andsimilar to relations.
Adding co-occurrence statistics slightly improved performance, andusing glosses did not help at all.We also compare our method to a re-implementation of the label propagation (LP)method.
Our method outperforms the LP method in both the 10-fold cross-validationset-up and when only 14 seeds are used.We also compare our method to the SO-PMI method.
Turney and Littman (2002)propose two methods for predicting the semantic orientation of words.
They useLatent Semantic Analysis (SO-LSA) and Pointwise Mutual Information (SO-PMI) formeasuring the statistical association between any given word and a set of 14 seedwords.
They describe this method as unsupervised because they only use 14 seedsas paradigm words that define the semantic orientation rather than train the model(Turney 2002).Table 3Accuracy for SO-PMI with different data set sizes, the spin model, the label propagation model,and the random walks model for 10-fold cross-validation and 14 seeds.?
CV 14 seedsSO-PMI (1?
107) ?
61.3SO-PMI (2?
109) ?
76.1SO-PMI (1?
1011) ?
82.8Spin Model 91.5 81.9Label Propagation 88.40 74.83Random Walks 93.1 82.1551Computational Linguistics Volume 40, Number 3The SO-PMI value can be calculated as follows:SO-PMI(w) = loghitsw,pos ?
hitsneghitsw,neg ?
hitspos(5)where w is a word with unknown polarity, hitsw,pos is the number of hits returned by acommercial search engine when the search query is the given word and the disjunctionof all positive seed words.
hitspos is the number of hits when we search for the disjunctionof all positive seed words.
hitsw,neg, and hitsneg are defined similarly.After Turney (2002), we use our method to predict semantic orientation of words inthe General Inquirer lexicon (Stone et al.
1966) using only 14 seed words.
The networkwe used contains only WordNet relations.
No glosses or co-occurrence statistics areused.
The results comparing the SO-PMI method with different data set sizes, the spinmodel, and the proposed method using only 14 seeds is shown in Table 3.
We observethat the random walk method outperforms SO-PMI when SO-PMI uses data sets ofsizes 1?
107 and 2?
109 words.
The performance of SO-PMI and the random walkmethods are comparable when SO-PMI uses a very large data set (1?
1011 words).
Theperformance of the spin model approach is also comparable to the other two methods.The advantages of the random walk method over SO-PMI is that it is faster and it doesnot need a very large corpus.
Another advantage is that the random walk method canbe used along with the labeled data from the General Inquirer lexicon (Stone et al.
1966)to get much better performance.
This is costly for the SO-PMI method because that willrequire the submission of almost 4,000 queries to a commercial search engine.We also compare our method with the bootstrapping method described in Hu andLiu (2004), and the shortest path method described in Kamps et al.
(2004).
We build anetwork using only WordNet synonyms and hypernyms.
We restrict the test set to theset of adjectives in the General Inquirer lexicon because our method is mainly interestedin classifying adjectives.The performance of the spin model, the bootstrapping method, the shortest pathmethod, the LP method, the Mincut method, and the random walk method for onlyadjectives is shown in Table 4.
We notice from the table that the random walk methodoutperforms the spin model, the bootstrapping method, the shortest path method,the LP method, and the Mincut method for adjectives.
The reported accuracy for theshortest path method only considers the words it could assign a non-zero orientationvalue.
If we consider all words, its accuracy will drop to around 61%.6.1.1 Varying Parameters.
As we mentioned in Section 3.4, we use a parameter m to putan upper bound on the length of random walks.
In this section, we explore the impactof this parameter on our method?s performance.Figure 1 shows the accuracy of the random walk method as a function of themaximum number of steps m as it varies from 5 to 50.
We use a network built fromWordNet synonyms and hypernyms only.
The number of samples k was set to 1, 000.Table 4Accuracy for adjectives only for the spin model, the bootstrap method, and the random walkmodel.Method Spin Model Bootstrap Shortest Path LP Mincut Random WalksAccuracy 83.6 72.8 68.8 84.8 73.8 88.8552Hassan et al.
A Random Walk?Based Model for Identifying Semantic OrientationFigure 1The effect of varying the maximum number of steps (m) on accuracy (k = 1,000).We perform 10-fold cross-validation using the General Inquirer lexicon.
We observethat the maximum number of steps m has very little impact on performance until itrises above 30.
At that point, the performance drops by no more than 1%, and then it nolonger changes as m increases.
An interesting observation is that the proposed methodperforms quite well with a very small number of steps (around 10).
We looked at thedata set to understand why increasing the number of steps beyond 30 negatively affectsperformance.
We found out that when the number of steps is very large compared withthe diameter of the graph, the random walk that starts at ambiguous words (which arehard to classify) have the chance of moving until it hits a node in the opposite class.That does not happen when the limit on the number of steps is smaller because thosewalks are then terminated without hitting any labeled nodes and are hence ignored.Next, we study the effect of the number of samples k on our method?s performance.As explained in Section 3.4, k is the number of samples used by the Monte Carloalgorithm to find an estimate for the hitting time.
Figure 2 shows the accuracy of therandom walks method as a function of the number of samples k. We use the sameFigure 2The effect of varying the number of samples (k) on accuracy.553Computational Linguistics Volume 40, Number 3settings as in the previous experiment.
The only difference is that we fix m at 15 andvary k from 10 to 20, 000 (note the logarithmic scale).
We notice that the performanceis badly affected when the value of k is very small (less than 100).
We also notice thatafter 1, 000, varying k has very little, if any, effect on performance.
This shows that theMonte Carlo algorithm for computing the random walks hitting time performs quitewell with values of the number of samples as small as 1, 000.The preceding experiments suggest that the parameter m has very little impacton the performance.
This suggests that the approach is fairly robust (i.e., it is quiteinsensitive to different parameter settings).6.1.2 Other Experiments.
We now measure the performance of the random walk methodwhen the system is allowed to abstain from classifying the words for which it has lowconfidence.
We regard the ratio between the hitting time to positive words and hittingtime to negative words as a confidence measure and evaluate the top words with thehighest confidence level at different values of threshold.
Figure 3 shows the accuracy for10-fold cross validation and for using only 14 seeds at different thresholds.
We noticethat the accuracy improves by abstaining from classifying the difficult words.
The figureshows that the top 60% words are classified with accuracy greater than 99% for 10-foldcross validation and 92% with 14 seed words.
This may be compared with the workdescribed in Takamura, Inui, and Okumura (2005), where they achieve the 92% levelwhen they only consider the top 1,000 words (28%).Figure 4 shows a learning curve displaying how the performance of both the pro-posed method and the LP method is affected with varying the labeled set size (i.e., thenumber of seeds).
We notice that the accuracy exceeds 90% when the training set sizerises above 20%.
The accuracy steadily increases as the size of labeled data increases.We also looked at the classification accuracy for different parts of speech in Figure 5.We notice that, in the case of 10-fold cross-validation, the performance is consistentacross parts of speech.
However, when we only use 14 seeds?all of which are ad-jectives, similar to Turney and Littman (2003)?we notice that the performance onadjectives is much better than other parts of speech.
When we use 14 seeds but replacesome of the adjectives with verbs and nouns such as love, harm, friend, enemy, the per-formance for nouns and verbs improves considerably at the cost of a small drop in theFigure 3Accuracy for words with high confidence measure.554Hassan et al.
A Random Walk?Based Model for Identifying Semantic OrientationFigure 4The effect of varying the number of seeds on accuracy.50556065707580859095100Adj Adv Noun VerbCV 14 Adj Seeds 14 SeedsFigure 5Accuracy for different parts of speech.performance on adjectives.
Finally, we tried adding edges to the network from glossesand co-occurrence statistics but we did not get any statistically significant improvement.Some of the words that were very weakly linked benefited from adding new typesof links and they were correctly predicted.
Others were misled by the noise and wereincorrectly classified.
We had a closer look at the results to find out what are the reasonsbehind incorrect predictions.
We found two main reasons.
First, some words have morethan one sense, possibly with different semantic orientations.
Disambiguating the senseof words given their context before trying to predict their polarity should solve thisproblem.
The second reason is that some words have very few connections in thethesaurus.
A possible solution to this might be to identify those words and add morelinks to them from glosses of co-occurrence statistics in the corpus.6.1.3 General Purpose Three-Way Classification.
The experiments described so far all usethe General Inquirer lexicon, which contains a well-established gold standard data setof positive and negative words.
However, in realistic applications, a general purpose555Computational Linguistics Volume 40, Number 3Table 5Accuracy for three classes on a general purpose list of 2,000 words.Class Positive Negative Neutral OverallAccuracy 68.0 82.1 80.6 77.9list of words will frequently have neutral words that don?t express sentiment polarity.To evaluate the effectiveness of the random walk method in distinguishing polarizedwords from neutral words, we constructed a data set of 2, 000 words randomly pickedfrom a standard English dictionary3 and hand labeled them with three classes: posi-tive, negative, and neutral.
Among the 2, 000 words, 494 were labeled positive,491 negative, and 1, 015 neutral.
The distribution among different parts of speech is532 adjectives, 335 verbs, 1, 051 nouns, and 82 others.We used the semi-supervised setting with the General Inquirer lexicon polarizedword list as the training set.
Because the 2, 000 test set has some portion of polarizedwords overlapping with the training set, we excluded the words that appear in the testset from the training set.
We performed Algorithm 2 in Section 3.4 with parameters?
= 0.8, m = 15, k = 1, 000.
The overall accuracy as well as the precision for each class isshown in Table 5.
We can see that the accuracy of the positive class is much lower thanthe negative class, due to the many positive words classified as neutral.
This meansthat the average confidence of negative words is higher than positive words.
One factorthat could have caused this is the bias originating from the training set.
Because thereare more negative seeds than positive ones, the constructed graph has an overall biastowards the negative class.6.2 Foreign WordsIn addition to the English data we described earlier, we constructed a labeled set of 300Arabic and 300 Hindi words for evaluation.
For every language, we asked two nativespeakers to examine a large amount of text and identify a set of positive and negativewords.
We also used an Arabic?English and a Hindi?English dictionary to generateForeign?English links.We compare our results with two baselines.
The first is the SO-PMI method de-scribed in Turney and Littman (2003).
We used the same seven positive and sevennegative seeds as Turney and Littman (2003).The second baseline constructs a network of only foreign words as described earlier.It uses mean hitting time to find the semantic association of any given word.
We used10-fold cross-validation for this experiment.
We will refer to this system as HT-FR.Finally, we build a multilingual network and use the hitting time as before to predictsemantic orientation.
We used the English words from Stone et al.
(1966) as seeds andthe labeled foreign words for evaluation.
We will refer to this system as HT-FR-EN.Figure 6 compares the accuracy of the three methods for Arabic and Hindi.
Wenotice that the SO-PMI and the hitting time?based methods perform poorly on bothArabic and Hindi.
This is clearly evident when we consider that the accuracy of the twosystems on English was 83%, and 93%, respectively (Turney and Littman 2003; Hassan3 Very infrequent words were filtered out by setting a threshold on the inverse document frequency of thewords in a corpus.556Hassan et al.
A Random Walk?Based Model for Identifying Semantic Orientation0102030405060708090100Arabic HindiSO-PMI HT - FR HT - FR+ ENFigure 6Accuracy of foreign word polarity identification.and Radev 2010).
This supports our hypothesis that state-of-the-art methods, designedfor English, perform poorly on foreign languages due to the limited amount of resourcesin them.
The figure also shows that the proposed method, which combines resourcesfrom both English and foreign languages, performs significantly better.
Finally, westudied how much improvement is achieved by including links between foreign wordsfrom global WordNets.
We found out that it improves the performance by 2.5% and 4%for Arabic and Hindi, respectively.6.3 OOV WordsWe created a labeled set of 300 positive and negative OOV words.
We asked a nativeEnglish speaker to examine a large number of threads posted on several on-line forumsand identify OOV words and label them with their polarities.
Some examples of posi-tive/negative OOV words are listed in Table 6.The baseline we use for OOV words is the SO-PMI method with the same 14 seedsas in Turney and Littman (2003).
The calculation of SO-PMI is given in Equation (5).We used the approach described in Section 5 to automatically label the words.
Weused the words of the General Inquirer lexicon as labeled seeds.
We set the maximumnumber of steps m to 15 and the number of samples k to 1, 000.
We experimented withTable 6Examples of positive and negative OOV words.Positive NegativeWord Meaning Word Meaningbeautimous beautiful and fabulous disastrophy a catastrophy and a disastergr8 great banjaxed ruinedbuffting attractive ijit idiot557Computational Linguistics Volume 40, Number 3Figure 7Accuracy of different methods in predicting OOV words polarity.the three variants we proposed for extracting the related words as described in Section 5.We give the experimental set-up for each variant here:1.
Search the entire Web (WS): We used Yahoo search4 to execute the searchqueries.
For each OOV word, we retrieve the top 500 results and use themto extract the related words.2.
Search the entire Web and limit the extraction of related words to theproximity of the OOV word (WSP): We fix the proximity of a givenOOV word to 15 words before and 15 words after the OOV word (weexperimented with different ranges but no significant changes wereobserved).3.
Limit the search to social content (SOC): We limit the search for OOVwords to tweets posted on Twitter.
We use the Twitter search APIto submit the search queries.
For each OOV word, we retrieve10,000 tweets.
Each tweet is maximum of 140 characters long.Figure 7 shows the results of the three methods compared with the baseline SO-PMI.The results show that extracting related words from tweets gives the best accuracy.
Thiscorroborated our intuition that using social content is more likely to provide sentiment-related words.
The baseline SO-PMI and WS obtain very similar accuracy.
This agreeswith the comparable performance of the two methods in the earlier experiment on theGeneral Inquirer lexicon.The three variant methods for obtaining related words have a tunable parameterR, the number of related words extracted for each OOV word.
We observe that Rhas a non-negligible effect on the prediction accuracy.
The results shown in Figure 8correspond to R = 90.
To better understand the impact of varying this parameter, we ranthe experiment that uses Twitter to extract related words several times using differentvalues for R. Figure 8 shows how the accuracy of polarity prediction changes as Rchanges.4 http://www.yahoo.com.558Hassan et al.
A Random Walk?Based Model for Identifying Semantic Orientation40%45%50%55%60%65%70%0 20 40 60 80 100 120 140 160 180AccuracyNumber of related wordsFigure 8The effect of varying the number of extracted related words on accuracy.7.
ConclusionsPredicting the semantic orientation of words is a very interesting task in natural lan-guage processing and it has a wide variety of applications.
We proposed a method forautomatically predicting the semantic orientation of words using random walks andhitting time.
The proposed method is based on the observation that a random walkstarting at a given word is more likely to hit another word with the same semanticorientation before hitting a word with a different semantic orientation.
The proposedmethod can be used in a semi-supervised setting, where a training set of labeled wordsis used, and in a weakly supervised setting, where only a handful of seeds is used todefine the two polarity classes.
We predict semantic orientation with high accuracy.The proposed method is fast, simple to implement, and does not need any corpus.
Wealso extended the proposed method to cover the problem of predicting the semanticorientation of foreign words.
All previous work on this task has almost exclusivelyfocused on English.
Applying off-the-shelf methods developed for English to otherlanguages does not work well because of the limited amount of resources availablein foreign languages compared with English.
We show that the proposed method canpredict the semantic orientation of foreign words with high accuracy and outperformsstate-of-the-art methods limited to using language specific resources.
Finally, we furtherextended the method to cover out-of-vocabulary words.
These words do not exist inWordNet and are not defined in the standard dictionaries of the language.
We proposedusing a Web-based approach to add the OOV words to our words network based onco-occurrence statistics, then use the same random walk model to predict the polar-ity.
We showed that this method can predict the polarity of OOV words with goodaccuracy.AcknowledgmentsThis research was funded by the Office of theDirector of National Intelligence (ODNI),Intelligence Advanced Research ProjectsActivity (IARPA), through the U.S. ArmyResearch Lab.
All statements of fact, opinion,or conclusions contained herein are those ofthe authors and should not be construed asrepresenting the official views or policies ofIARPA, the ODNI, or the U.S. Government.559Computational Linguistics Volume 40, Number 3ReferencesAgirre, Eneko, Enrique Alfonseca, KeithHall, Jana Kravalova, Marius Pas?ca, andAitor Soroa.
2009.
A study on similarityand relatedness using distributional andwordnet-based approaches.
In Proceedingsof Human Language Technologies: The 2009Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics, NAACL ?09, pages 19?27,Stroudsburg, PA.Andreevskaia, Alina and Sabine Bergler.2006.
Mining WordNet for fuzzysentiment: Sentiment tag extractionfrom WordNet glosses.
In EACL?06,pages 209?216.Banea, Carmen, Rada Mihalcea, andJanyce Wiebe.
2008.
A bootstrappingmethod for building subjectivity lexiconsfor languages with scarce resources.In LREC?08, pages 2,764?2,767.Black, W., S. Elkateb, H. Rodriguez,M.
Alkhalifa, P. Vossen, A. Pease, andC.
Fellbaum.
2006.
Introducing theArabic WordNet project.
In ThirdInternational WordNet Conference,pages 295?299.Blair-Goldensohn, Sasha, Tyler Neylon,Kerry Hannan, George A. Reis, RyanMcDonald, and Jeff Reynar.
2008.
Buildinga sentiment summarizer for local servicereviews.
In NLP in the InformationExplosion Era.Brody, Samuel and Noemie Elhadad.
2010.An unsupervised aspect-sentiment modelfor online reviews.
In Human LanguageTechnologies: The 2010 Annual Conferenceof the North American Chapter of theAssociation for Computational Linguistics,pages 804?812, Los Angeles, CA.Elkateb, S., W. Black, H. Rodriguez,M.
Alkhalifa, P. Vossen, A. Pease, andC.
Fellbaum.
2006a.
Building a WordNetfor Arabic.
In Fifth International Conferenceon Language Resources and Evaluation,pages 29?34.Elkateb, S., W. Black, P. Vossen, D. Farwell,H.
Rodriguez, A. Pease, and M. Alkhalifa.2006b.
Arabic WordNet and the challengesof Arabic.
In Arabic NLP/MT Conference,pages 15?24.Esuli, Andrea and Fabrizio Sebastiani.
2005.Determining the semantic orientationof terms through gloss classification.In CIKM?05, pages 617?624.Esuli, Andrea and Fabrizio Sebastiani.
2006.Sentiwordnet: A publicly available lexicalresource for opinion mining.
In LREC?06,pages 417?422.Etzioni, Oren, Kobi Reiter, Stephen Soderl,and Marcus Sammer.
2007.
Lexicaltranslation with application to imagesearch on the Web.
In Proceedings ofMachine Translation Summit XI.Hassan, Ahmed and Dragomir R. Radev.2010.
Identifying text polarity usingrandom walks.
In Proceedings of the 48thAnnual Meeting of the Association forComputational Linguistics, pages 395?403,Uppsala.Hatzivassiloglou, Vasileios and Kathleen R.McKeown.
1997.
Predicting the semanticorientation of adjectives.
In EACL?97,pages 174?181.Hatzivassiloglou, Vasileios and JanyceWiebe.
2000.
Effects of adjectiveorientation and gradability on sentencesubjectivity.
In COLING, pages 299?305.Hu, Minqing and Bing Liu.
2004.
Miningand summarizing customer reviews.In KDD?04, pages 168?177.Jha, S., D. Narayan, P. Pande, andP.
Bhattacharyya.
2001.
A WordNet forHindi.
In International Workshop on LexicalResources in Natural Language Processing.Jijkoun, Valentin and Katja Hofmann.
2009.Generating a non-English subjectivitylexicon: Relations that matter.
InProceedings of the 12th Conference of theEuropean Chapter of the ACL (EACL 2009),pages 398?405, Athens.Kamps, Jaap, Maarten Marx, Robert J.Mokken, and Maarten De Rijke.
2004.Using WordNet to measure semanticorientations of adjectives.
In Proceedingsof the 4th International Conference onLanguage Resources and Evaluation(LREC 2004), pages 1115?1118.Kanayama, Hiroshi and Tetsuya Nasukawa.2006.
Fully automatic lexicon expansionfor domain-oriented sentiment analysis.In EMNLP?06, pages 355?363.Kim, Soo-Min and Eduard Hovy.
2004.Determining the sentiment of opinions.In COLING, pages 1,367?1,373.Kok, Stanley and Chris Brockett.
2010.Hitting the right paraphrases in goodtime.
In Human Language Technologies:The 2010 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 145?153,Los Angeles, CA.Lewis, D. D., Y. Yang, T. Rose, and F. Li.2004.
Rcv1: A new benchmark collectionfor text categorization research.
Journal ofMachine Learning Research, 5:361?397.Mihalcea, Rada and Carmen Banea.
2007.Learning multilingual subjective language560Hassan et al.
A Random Walk?Based Model for Identifying Semantic Orientationvia cross-lingual projections.
In Proceedingsof the 45th Annual Meeting of the Associationof Computational Linguistics, pages 976?983.Miller, George A.
1995.
Wordnet: A lexicaldatabase for English.
Communications ofACM, 38(11):39?41.Mohammad, Saif, Cody Dunne, and BonnieDorr.
2009.
Generating high-coveragesemantic orientation lexicons from overtlymarked words and a thesaurus.
InProceedings of the 2009 Conference onEmpirical Methods in Natural LanguageProcessing: Volume 2, EMNLP ?09,pages 599?608, Stroudsburg, PA.Morinaga, Satoshi, Kenji Yamanishi, KenjiTateishi, and Toshikazu Fukushima.
2002.Mining product reputations on the Web.In KDD?02, pages 341?349.Narayan, Dipak, Debasri Chakrabarti,Prabhakar Pande, and P. Bhattacharyya.2002.
An experience in building the IndoWordNet?a WordNet for Hindi.
In FirstInternational Conference on Global WordNet.Nasukawa, Tetsuya and Jeonghee Yi.
2003.Sentiment analysis: Capturing favorabilityusing natural language processing.In K-CAP ?03: Proceedings of the 2ndInternational Conference on KnowledgeCapture, pages 70?77.Norris, J.
1997.
Markov Chains.
CambridgeUniversity Press.Pang, Bo and Lillian Lee.
2004.
A sentimentaleducation: Sentiment analysis usingsubjectivity summarization based onminimum cuts.
In Proceedings of the42nd Annual Meeting of the Associationfor Computational Linguistics, ACL ?04,Stroudsburg, PA.Popescu, Ana-Maria and Oren Etzioni.
2005.Extracting product features and opinionsfrom reviews.
In HLT-EMNLP?05,pages 339?346.Rao, Delip and Deepak Ravichandran.2009.
Semi-supervised polarity lexiconinduction.
In Proceedings of the 12thConference of the European Chapter of theACL (EACL 2009), pages 675?682, Athens.Riloff, Ellen and Janyce Wiebe.
2003.Learning extraction patterns forsubjective expressions.
In EMNLP?03,pages 105?112.Stone, Philip, Dexter Dunphy, MarchallSmith, and Daniel Ogilvie.
1966.
TheGeneral Inquirer: A Computer Approachto Content Analysis.
The MIT Press.Su, Fangzhong and Katja Markert.
2009.Subjectivity recognition on wordsenses via semi-supervised mincuts.In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference ofthe North American Chapter of the Associationfor Computational Linguistics, NAACL ?09,pages 1?9, Stroudsburg, PA.Szummer, Martin and Tommi Jaakkola.2002.
Partially labeled classification withMarkov random walks.
In NIPS?02,pages 945?952.Takamura, Hiroya, Takashi Inui, andManabu Okumura.
2005.
Extractingsemantic orientations of words usingspin model.
In ACL?05, pages 133?140.Tong, Richard M. 2001.
An operationalsystem for detecting and tracking opinionsin on-line discussion.
Workshop note,SIGIR 2001 Workshop on Operational TextClassification.Turney, Peter and Michael Littman.
2003.Measuring praise and criticism: Inferenceof semantic orientation from association.ACM Transactions on Information Systems,21:315?346.Turney, Peter D. 2002.
Thumbs up or thumbsdown?
: Semantic orientation applied tounsupervised classification of reviews.In ACL?02, pages 417?424.Velikovich, Leonid, Sasha Blair-Goldensohn,Kerry Hannan, and Ryan McDonald.
2010.The viability of Web-derived polaritylexicons.
In Human Language Technologies:The 2010 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 777?785,Los Angeles, CA.Vossen, P. 1997.
Eurowordnet: A multilingualdatabase for information retrieval.
InDELOS Workshop on Cross-LanguageInformation Retrieval, pages 5?7.Wiebe, Janyce.
2000.
Learning subjectiveadjectives from corpora.
In Proceedings ofthe Seventeenth National Conference onArtificial Intelligence and the TwelfthConference on Innovative Applications ofArtificial Intelligence, pages 735?740.Wiebe, Janyce, Rebecca Bruce, Matthew Bell,Melanie Martin, and Theresa Wilson.2001.
A corpus study of evaluative andspeculative language.
In Proceedings of theSecond SIGdial Workshop on Discourse andDialogue, pages 1?10.Wiebe, Janyce and Rada Mihalcea.2006a.
Word sense and subjectivity.In Proceedings of the 21st InternationalConference on Computational Linguisticsand the 44th Annual Meeting of theAssociation for Computational Linguistics,pages 1,065?1,072, Sydney.Wiebe, Janyce and Rada Mihalcea.
2006b.Word sense and subjectivity.
In Proceedings561Computational Linguistics Volume 40, Number 3of the 21st International Conference onComputational Linguistics and the 44thAnnual Meeting of the Association forComputational Linguistics, ACL-44,pages 1,065?1,072, Stroudsburg, PA.Wiebe, Janyce, Theresa Wilson, and ClaireCardie.
2005.
Annotating expressions ofopinions and emotions in language.Language Resources and Evaluation,39(2-3):165?210.Yu, Hong and Vasileios Hatzivassiloglou.2003.
Towards answering opinionquestions: Separating facts from opinionsand identifying the polarity of opinionsentences.
In EMNLP?03, pages 129?136.Zhu, Xiaojin, Zoubin Ghahramani, andJohn Lafferty.
2003.
Semi-supervisedlearning using Gaussian fields andharmonic functions.
In ICML?03,pages 912?919.562
