Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798?1803,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsJoint Emotion Analysis via Multi-task Gaussian ProcessesDaniel Beck?Trevor Cohn?Lucia Specia?
?Department of Computer Science, University of Sheffield, United Kingdom{debeck1,l.specia}@sheffield.ac.uk?Computing and Information Systems, University of Melbourne, Australiat.cohn@unimelb.edu.auAbstractWe propose a model for jointly predictingmultiple emotions in natural language sen-tences.
Our model is based on a low-rankcoregionalisation approach, which com-bines a vector-valued Gaussian Processwith a rich parameterisation scheme.
Weshow that our approach is able to learncorrelations and anti-correlations betweenemotions on a news headlines dataset.
Theproposed model outperforms both single-task baselines and other multi-task ap-proaches.1 IntroductionMulti-task learning (Caruana, 1997) has beenwidely used in Natural Language Processing.Most of these learning methods are aimed for Do-main Adaptation (Daum?e III, 2007; Finkel andManning, 2009), where we hypothesize that wecan learn from multiple domains by assuming sim-ilarities between them.
A more recent use ofmulti-task learning is to model annotator bias andnoise for datasets labelled by multiple annotators(Cohn and Specia, 2013).The settings mentioned above have one aspectin common: they assume some degree of posi-tive correlation between tasks.
In Domain Adap-tation, we assume that some ?general?, domain-independent knowledge exists in the data.
For an-notator noise modelling, we assume that a ?groundtruth?
exists and that annotations are some noisydeviations from this truth.
However, for some set-tings these assumptions do not necessarily holdand often tasks can be anti-correlated.
For thesecases, we need to employ multi-task methods thatare able to learn these relations from data andcorrectly employ them when making predictions,avoiding negative knowledge transfer.An example of a problem that shows this be-haviour is Emotion Analysis, where the goal is toautomatically detect emotions in a text (Strappa-rava and Mihalcea, 2008; Mihalcea and Strappa-rava, 2012).
This problem is closely related toOpinion Mining (Pang and Lee, 2008), with sim-ilar applications, but it is usually done at a morefine-grained level and involves the prediction of aset of labels (one for each emotion) instead of asingle label.
While we expect some emotions tohave some degree of correlation, this is usually notthe case for all possible emotions.
For instance, weexpect sadness and joy to be anti-correlated.We propose a multi-task setting for EmotionAnalysis based on a vector-valued Gaussian Pro-cess (GP) approach known as coregionalisation(?Alvarez et al., 2012).
The idea is to combine a GPwith a low-rank matrix which encodes task corre-lations.
Our motivation to employ this model isthree-fold:?
Datasets for this task are scarce and smallso we hypothesize that a multi-task approachwill results in better models by allowing atask to borrow statistical strength from othertasks;?
The annotation scheme is subjective and veryfine-grained, and is therefore heavily prone tobias and noise, both which can be modelledeasily using GPs;?
Finally, we also have the goal to learn amodel that shows sound and interpretablecorrelations between emotions.2 Multi-task Gaussian ProcessRegressionGaussian Processes (GPs) (Rasmussen andWilliams, 2006) are a Bayesian kernelisedframework considered the state-of-the-art forregression.
They have been recently used success-fully for translation quality prediction (Cohn andSpecia, 2013; Beck et al., 2013; Shah et al., 2013)1798and modelling text periodicities (Preotiuc-Pietroand Cohn, 2013).
In the following we give abrief description on how GPs are applied in aregression setting.Given an input x, the GP regression assumesthat its output y is a noise corrupted version of alatent function evaluation, y = f(x) + ?, where?
?
N (0, ?2n) is the added white noise and thefunction f is drawn from a GP prior:f(x) ?
GP(?
(x), k(x,x?
)), (1)where ?
(x) is the mean function, which is usuallythe 0 constant, and k(x,x?)
is the kernel or co-variance function, which describes the covariancebetween values of f at locations x and x?.To predict the value for an unseen input x?, wecompute the Bayesian posterior, which can be cal-culated analytically, resulting in a Gaussian distri-bution over the output y?:1y??
N (k?
(K + ?nI)?1yT, (2)k(x?,x?)?
kT?
(K + ?nI)?1k?
),where K is the Gram matrix corre-sponding to the covariance kernel evalu-ated at every pair of training inputs andk?= [?x1,x?
?, ?x2,x?
?, .
.
.
, ?xn,x??]
is thevector of kernel evaluations between the test inputand each training input.2.1 The Intrinsic Coregionalisation ModelBy extending the GP regression framework tovector-valued outputs we obtain the so-calledcoregionalisation models.
Specifically, we employa separable vector-valued kernel known as Intrin-sic Coregionalisation Model (ICM) (?Alvarez et al.,2012).
Considering a set of D tasks, we define thecorresponding vector-valued kernel as:k((x, d), (x?, d?))
= kdata(x,x?
)?Bd,d?, (3)where kdatais a kernel on the input points (herea Radial Basis Function, RBF), d and d?are taskor metadata information for each input and B ?RD?Dis the coregionalisation matrix, which en-codes task covariances and is symmetric and posi-tive semi-definite.A key advantage of GP-based modelling is itsability to learn hyperparameters directly from data1We refer the reader to Rasmussen and Williams (2006,Chap.
2) for an in-depth explanation of GP regression.by maximising the marginal likelihood:p(y|X,?)
=?fp(y|X,?, f)p(f).
(4)This process is usually performed to learn thenoise variance and kernel hyperparameters, in-cluding the coregionalisation matrix.
In order todo this, we need to consider how B is parame-terised.Cohn and Specia (2013) treat the diagonal val-ues of B as hyperparameters, and as a conse-quence are able to leverage the inter-task trans-fer between each independent task and the global?pooled?
task.
They however fix non-diagonal val-ues to 1, which in practice is equivalent to assum-ing equal correlation across tasks.
This can be lim-iting, in that this formulation cannot model anti-correlations between tasks.In this work we lift this restriction by adoptinga different parameterisation of B that allows thelearning of all task correlations.
A straightforwardway to do that would be to consider every corre-lation as an hyperparameter, but this can result ina matrix which is not positive semi-definite (andtherefore, not a valid covariance matrix).
To en-sure this property, we follow the method proposedby Bonilla et al.
(2008), which decomposes B us-ing Probabilistic Principal Component Analysis:B = U?UT+ diag(?
), (5)where U is an D ?
R matrix containing the Rprincipal eigenvectors and ?
is a R ?
R diago-nal matrix containing the corresponding eigenval-ues.
The choice of R defines the rank of U?UT,which can be understood as the capacity of themanifold with which we model the D tasks.
Thevector ?
allows for each task to behave more orless independently with respect to the global task.The final rank of B depends on both terms inEquation 5.For numerical stability, we use the incomplete-Cholesky decomposition over the matrix U?UT,resulting in the following parameterisation for B:B =?L?LT+ diag(?
), (6)where?L is a D ?R matrix.
In this setting, wetreat all elements of?L as hyperparameters.
Set-ting a larger rank allows more flexibility in mod-elling task correlations.
However, a higher numberof hyperparameters may lead to overfitting prob-lems or otherwise cause issues in optimisation due1799to additional non-convexities in the log likelihoodobjective.
In our experiments we evaluate this be-haviour empirically by testing a range of ranks foreach setting.The low-rank model can subsume the ones pro-posed by Cohn and Specia (2013) by fixing andtying some of the hyperparameters:Independent: fixing?L = 0 and ?
= 1;Pooled: fixing?L = 1 and ?
= 0;Combined: fixing?L = 1 and tying all compo-nents of ?
;Combined+: fixing?L = 1.These formulations allow us to easily replicatetheir modelling approach, which we evaluate ascompetitive baselines in our experiments.3 Experimental SetupTo address the feasibility of our approach, we pro-pose a set of experiments with three goals in mind:?
To find our whether the ICM is able to learnsensible emotion correlations;?
To check if these correlations are able to im-prove predictions for unseen texts;?
To investigate the behaviour of the ICMmodel as we increase the training set size.Dataset We use the dataset provided by the ?Af-fective Text?
shared task in SemEval-2007 (Strap-parava and Mihalcea, 2007), which is composedof 1000 news headlines annotated in terms of sixemotions: Anger, Disgust, Fear, Joy, Sadness andSurprise.
For each emotion, a score between 0 and100 is given, 0 meaning total lack of emotion and100 maximum emotional load.
We use 100 sen-tences for training and the remaining 900 for test-ing.Model For all experiments, we use a Radial Ba-sis Function (RBF) data kernel over a bag-of-words feature representation.
Words were down-cased and lemmatized using the WordNet lemma-tizer in the NLTK2toolkit (Bird et al., 2009).
Wethen use the GPy toolkit3to combine this kernelwith a coregionalisation model over the six emo-tions, comparing a number of low-rank approxi-mations.2http://www.nltk.org3http://github.com/SheffieldML/GPyBaselines and Evaluation We compare predic-tion results with a set of single-task baselines: aSupport Vector Machine (SVM) using an RBFkernel with hyperparameters optimised via cross-validation and a single-task GP, optimised via like-lihood maximisation.
The SVM models weretrained using the Scikit-learn toolkit4(Pedregosaet al., 2011).
We also compare our results againstthe ones obtained by employing the ?Combined?and ?Combined+?
models proposed by Cohn andSpecia (2013).
Following previous work in thisarea, we use Pearson?s correlation coefficient asevaluation metric.4 Results and Discussion4.1 Learned Task CorrelationsFigure 1 shows the learned coregionalisation ma-trix setting the initial rank as 1, reordering theemotions to emphasize the learned structure.
Wecan see that the matrix follows a block structure,clustering some of the emotions.
This pictureshows two interesting behaviours:?
Sadness and fear are highly correlated.
Angerand disgust also correlate with them, al-though to a lesser extent, and could be con-sidered as belonging to the same cluster.
Wecan also see correlation between surprise andjoy.
These are intuitively sound clustersbased on the polarity of these emotions.?
In addition to correlations, the modellearns anti-correlations, especially betweenjoy/surprise and the other emotions.
We alsonote that joy has the highest diagonal value,meaning that it gives preference to indepen-dent modelling (instead of pooling over theremaining tasks).Inspecting the eigenvalues of the learned ma-trix allows us to empirically determine its result-ing rank.
In this case we find that the model haslearned a matrix of rank 3, which indicates thatour initial assumption of a rank 1 coregionalisa-tion matrix may be too small in terms of modellingcapacity5.
This suggests that a higher rank isjustified, although care must be taken due to thelocal optima and overfitting issues cited in ?2.1.4http://scikit-learn.org5The eigenvalues were 592, 62, 86, 4, 3 ?
10?3and 9 ?10?5.1800Anger Disgust Fear Joy Sadness Surprise AllSVM 0.3084 0.2135 0.3525 0.0905 0.3330 0.1148 0.2603Single GP 0.1683 0.0035 0.3462 0.2035 0.3011 0.1599 0.3659ICM GP (Combined) 0.2301 0.1230 0.2913 0.2202 0.2303 0.1744 0.3295ICM GP (Combined+) 0.1539 0.1240 0.3438 0.2466 0.2850 0.2027 0.3723ICM GP (Rank 1) 0.2133 0.1075 0.3623 0.2810 0.3137 0.2415 0.3988ICM GP (Rank 5) 0.2542 0.1799 0.3727 0.2711 0.3157 0.2446 0.3957Table 1: Prediction results in terms of Pearson?s correlation coefficient (higher is better).
Boldface valuesshow the best performing model for each emotion.
The scores for the ?All?
column were calculated overthe predictions for all emotions concatenated (instead of just averaging over the scores for each emotion).Figure 1: Heatmap showing a learned coregional-isation matrix over the emotions.4.2 Prediction ResultsTable 1 shows the Pearson?s scores obtained inour experiments.
The low-rank models outper-formed the baselines for the full task (predictingall emotions) and for fear, joy and surprise sub-tasks.
The rank 5 models were also able to out-perform all GP baselines for the remaining emo-tions, but could not beat the SVM baseline.
Asexpected, the ?Combined?
and ?Combined+?
per-formed worse than the low-rank models, probablydue to their inability to model anti-correlations.4.3 Error analysisTo check why SVM performs better than GPs forsome emotions, we analysed their gold-standardscore distributions.
Figure 2 shows the smootheddistributions for disgust and fear, comparing thegold-standard scores to predictions from the SVMand GP models.
The distributions for the trainingset follow similar shapes.We can see that GP obtains better matchingscore distributions in the case when the gold-Figure 2: Test score distributions for disgust andfear.
For clarity, only scores between 0 and 50 areshown.
SVM performs better on disgust, while GPperforms better on fear.standard scores are more spread over the full sup-port of response values, i.e., [0, 100].
Since our GPmodel employs a Gaussian likelihood, it is effec-tively minimising a squared-error loss.
The SVMmodel, on the other hand, uses hinge loss, whichis linear beyond the margin envelope constraints.This affects the treatment of outlier points, whichattract quadratic cf.
linear penalties for the GPand SVM respectively.
Therefore, when train-ing scores are more uniformly distributed (whichis the case for fear), the GP model has to take thehigh scores into account, resulting in broader cov-erage of the full support.
For disgust, the scoresare much more peaked near zero, favouring the1801more narrow coverage of the SVM.More importantly, Figure 2 also shows that bothSVM and GP predictions tend to exhibit a Gaus-sian shape, while the true scores show an expo-nential behaviour.
This suggests that both mod-els are making wrong prior assumptions about theunderlying score distribution.
For SVMs, this isa non-trivial issue to address, although it is mucheasier for GPs, where we can use a different like-lihood distribution, e.g., a Beta distribution to re-flect that the outputs are only valid over a boundedrange.
Note that non-Gaussian likelihoods meanthat exact inference is no longer tractable, due tothe lack of conjugacy between the prior and likeli-hood.
However a number of approximate infer-ence methods are appropriate which are alreadywidely used in the GP literature for use with non-Gaussian likelihoods, including expectation prop-agation (Jyl?anki et al., 2011), the Laplace approx-imation (Williams and Barber, 1998) and MarkovChain Monte Carlo sampling (Adams et al., 2009).4.4 Training Set InfluenceWe expect multi-task models to perform better forsmaller datasets, when compared to single-taskmodels.
This stems from the fact that with smalldatasets often there is more uncertainty associatedwith each task, a problem which can be alleviatedusing statistics from the other tasks.
To measurethis behaviour, we performed an additional exper-iment varying the size of the training sets, whileusing 100 sentences for testing.Figure 3 shows the scores obtained.
As ex-pected, for smaller datasets the single-task mod-els are outperformed by ICM, but their perfor-mance become equivalent as the training set sizeincreases.
SVM performance tends to be slightlyworse for most sizes.
To study why we obtainedan outlier for the single-task model with 200 sen-tences, we inspected the prediction values.
Wefound that, in this case, predictions for joy, sur-prise and disgust were all around the same value.6For larger datasets, this effect disappears and thesingle-task models yield good predictions.5 Conclusions and Future WorkThis paper proposed an multi-task approach forEmotion Analysis that is able to learn correlations6Looking at the predictions for smaller datasets, we foundthe same behaviour, but because the values found were nearthe mean they did not hurt the Pearson?s score as much.Figure 3: Pearson?s correlation score according totraining set size (in number of sentences).and anti-correlations between emotions.
Our for-mulation is based on a combination of a GaussianProcess and a low-rank coregionalisation model,using a richer parameterisation that allows thelearning of fine-grained task similarities.
The pro-posed model outperformed strong baselines whenapplied to a news headline dataset.As it was discussed in Section 4.3, we planto further explore the possibility of using non-Gaussian likelihoods with the GP models.
An-other research avenue we intend to explore is toemploy multiple layers of metadata, similar to themodel proposed by Cohn and Specia (2013).
Anexample is to incorporate the dataset provided bySnow et al.
(2008), which provides multiple non-expert emotion annotations for each sentence, ob-tained via crowdsourcing.
Finally, another possi-ble extension comes from more advanced vector-valued GP models, such as the linear model ofcoregionalisation (?Alvarez et al., 2012) or hierar-chical kernels (Hensman et al., 2013).
These mod-els can be specially useful when we want to em-ploy multiple kernels to explain the relation be-tween the input data and the labels.AcknowledgementsDaniel Beck was supported by funding fromCNPq/Brazil (No.
237999/2012-9).
Dr.Cohn is the recipient of an Australian Re-search Council Future Fellowship (project numberFT130101105).ReferencesRyan Prescott Adams, Iain Murray, and David J. C.MacKay.
2009.
Tractable Nonparametric Bayesian1802Inference in Poisson Processes with Gaussian Pro-cess Intensities.
In Proceedings of ICML, pages 1?8,New York, New York, USA.
ACM Press.Mauricio A.?Alvarez, Lorenzo Rosasco, and Neil D.Lawrence.
2012.
Kernels for Vector-Valued Func-tions: a Review.
Foundations and Trends in Ma-chine Learning, pages 1?37.Daniel Beck, Kashif Shah, Trevor Cohn, and LuciaSpecia.
2013.
SHEF-Lite : When Less is More forTranslation Quality Estimation.
In Proceedings ofWMT13, pages 337?342.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.Edwin V. Bonilla, Kian Ming A. Chai, and ChristopherK.
I. Williams.
2008.
Multi-task Gaussian ProcessPrediction.
Advances in Neural Information Pro-cessing Systems.Rich Caruana.
1997.
Multitask Learning.
MachineLearning, 28:41?75.Trevor Cohn and Lucia Specia.
2013.
ModellingAnnotator Bias with Multi-task Gaussian Processes:An Application to Machine Translation Quality Es-timation.
In Proceedings of ACL.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of ACL.Jenny Rose Finkel and Christopher D. Manning.
2009.Hierarchical Bayesian Domain Adaptation.
In Pro-ceedings of NAACL.James Hensman, Neil D Lawrence, and Magnus Rat-tray.
2013.
Hierarchical Bayesian modelling ofgene expression time series across irregularly sam-pled replicates and clusters.
BMC Bioinformatics,14:252.Pasi Jyl?anki, Jarno Vanhatalo, and Aki Vehtari.
2011.Robust Gaussian Process Regression with a Student-t Likelihood.
Journal of Machine Learning Re-search, 12:3227?3257.Rada Mihalcea and Carlo Strapparava.
2012.
Lyrics,Music, and Emotions.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 590?599.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Foundations and Trends in In-formation Retrieval, 2(1?2):1?135.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Duborg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Daniel Preotiuc-Pietro and Trevor Cohn.
2013.
A tem-poral model of text periodicities using Gaussian Pro-cesses.
In Proceedings of EMNLP.Carl Edward Rasmussen and Christopher K. I.Williams.
2006.
Gaussian processes for machinelearning, volume 1.
MIT Press Cambridge.Kashif Shah, Trevor Cohn, and Lucia Specia.
2013.An Investigation on the Effectiveness of Features forTranslation Quality Estimation.
In Proceedings ofMT Summit XIV.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and Fast - Butis it Good?
: Evaluating Non-Expert Annotationsfor Natural Language Tasks.
In Proceedings ofEMNLP.Carlo Strapparava and Rada Mihalcea.
2007.SemEval-2007 Task 14 : Affective Text.
In Pro-ceedings of SEMEVAL.Carlo Strapparava and Rada Mihalcea.
2008.
Learningto identify emotions in text.
In Proceedings of the2008 ACM Symposium on Applied Computing.Christopher K. I. Williams and David Barber.
1998.Bayesian Classification with Gaussian Processes.IEEE Transactions on Pattern Analysis and MachineIntelligence, 20(12):1342?1351.1803
