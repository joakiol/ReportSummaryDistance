SRA: DESCRIPTION OF THE IE2SYSTEMUSED FOR MUC-7Chinatsu Aone, Lauren Halverson, Tom Hampton, Mila Ramos-SantacruzSRA International, Inc.4300 Fair Lakes CourtFairfax, VA 22033-4232faonec, laur, hamptont, milag@sra.comINTRODUCTIONIn our planning for MUC-7, we made the decision to concentrate our eorts in two areas: improvementsto our infrastructure, both in system architecture and the supporting set of tools; second, maximizing theperformance of each information extraction (IE) module, including the introduction of trainable learning-based modules.
We felt that achieving our performance goals in extraction depended on improvements inthese areas.
Specically, our goals when entering MUC-7 were to: Increase the accuracy in the Template Element (TE) task and the Template Relation (TR) task su-ciently for operational use, i.e., F-Measures of 85% and 80% respectively, Increase the accuracy and portability in the Scenario Template (ST) task signicantly.In order to achieve these goals, we took several important steps, which included: A new IE system developed in the past year Aexible, modular system architecture A set of annotation tools and development tools to speed up development, and enable higher accuracy A high-performance phrase and link tagger A hybrid, trainable discourse module.The result was a new high-performance information extraction system, Information Extraction En-gine (IE2).
It has aexible and modular architecture and allows optimal speed in the development of allsystem modules.In the end, this new system achieved the highest score in each of the three tasks we entered: TE, TR, andST, as shown in Table 1.
In particular, TE showed an operational performance level, while TR was almostat that level.
ST, by contrast, still presents fundamental problems yet to be solved by the community, butwe believe that we have demonstrated an eective strategy for enhancing performance on this task.1Recall Precision F-MeasureTE 86 87 86.76TR 67 86 75.63ST 42 65 50.79Table 1: SRA's Scores for TE, TR and STSYSTEM ARCHITECTUREAbout a year ago, we set out to develop a new IE system.
One of the rst things we worked on was thedesign of a new system architecture.
The two top requirements were modularity andexibility.
We wanteda modular architecture so that each module could be developed, tested, and improved independently.
Thismodular architecture not only speeds up the system development process but also provides the ability toreplace an existing module with a new one very easily.
We also wanted aexible workow so that developmentof a module in the latter stage of IE processing does not require processing input through all the previousmodules.
Thisexible workow not only cuts development time, but also enables simultaneous developmentof multiple modules without dependency problems.We chose SGML-marked up texts as the input and output of each module, which follows the spirit of theTIPSTER architecture.
This enabled us to spell out system interface requirements between two modulesmuch more clearly than previously possible, and reduced mis-communication between modules.
The IE2system is truly modular, and any module can be replaced as long as the new module follows the systeminterface requirements.
Its workow is alsoexible so that one can start processing at any point.
Forexample, the person who is testing the discourse module can just input the previous SGML output of theevent tagging module without re-processing any of the previous modules.
Figure 1 shows the new systemarchitecture.
We discuss each module in more detail in the next section.SYSTEM DESCRIPTIONSRA's IE2System used ve modules for the TE and TR tasks and six for the ST task, as shown inFigure 1.
The three core modules, namely NameTag, PhraseTag, and EventTag, use SRA's multilingualinformation extraction engine called TurboTag.Entity Name RecognitionFor MUC-7, we used NetOwl Extractor 3.0, a commercial entity name recognition software by IsoQuest,a subsidiary of SRA.
NetOwl Extractor 3.0 recognizes names of people, organizations, and places, as well astime and numeric expressions.
It was congured to follow MUC-7 NE guidelines.In addition to its name recognition capability, we used its sub-typing and name alias recognition capa-bilities.
Extractor 3.0 provides subtypes of organizations (e.g., company, government, military, etc.)
andplaces (e.g., country, province, city, etc.)
It also provides links among aliases of organizations (e.g., \TCI"for \Tele-Communications Inc."), people (e.g., \Goldstein" for \Irving Goldstein"), and places (e.g., \U.S.
"for \United States").Custom NameTagSRA developed custom patterns to perform additional name recognition using the TurboTag engine.
TheCustom NameTag tags artifact names (vehicle) necessary for the TE, TR and ST task.
It also performssemantic classication of vehicle names into AIR, GROUND, and WATER.
They are further classied into2Figure 1: IE2System Architecturesubtypes: plane, helicopter, space shuttle, rocket, missile, space probe, space station, satellite, and capsulefor AIR, car and tank for GROUND, and ship and submarine for WATER.During the formal test period, we also developed additional patterns to recognize missing names of people,organizations, and places for the launch domain.PhraseTagIn order to achieve operational-level accuracy in TE and TR, we decided that we needed a high-performance noun phrase tagger especially targeted to recognize noun phrases describing people (PNP),organizations (ENP), and artifacts (ANP).
Thus, we developed PhraseTag, which tags these three types ofNPs.
It recognizes not only simple NPs but also complex NPs with post modiers such as relative clauses,reduced relatives, and prepositional phrases.
In a blind test set, it achieves around 80% F-Measure, includingthe post-modier attachments.Additionally, PhraseTag adds local links between phrases with high accuracy.
During the design phaseof IE2, our analysis of MUC-6 data led us to believe that most links necessary to recognize for the TE andTR tasks are local.
That is, by targeting one's eort at good recognition of such linguistic structures asappositives (e.g., \Irving Goldstein, director general and chief executive of Intelsat") and copula sentences(e.g., \International Technology Underwriters of Bethesda, Maryland, is one insurer in this consortium"),one can achieve high accuracy in TE and TR.
As these structures are constrained to local contexts, it ismuch easier to recognize them with high accuracy than structures involving long-distance dependencies.PhraseTag recognizes four local links, namely employee of, location of, product of, and owner of.Links are registered in the SGML attributes AFFIL, LOC, MAKER and OWNER, respectively, as shownbelow.
The owner of relation is not a part of the TR task, but we used it in the ST task to extract ownersof vehicles and payloads.3 employee of<PNP AFFIL=1>an analyst at <ENTITY ID=1>ING Barings</ENTITY> in Mexico City</PNP> location of<ENP LOC=2><PLACE ID=2>Paris</PLACE> insurer</ENP> product of<ANP MAKER=3>a satellite built by <ENTITY ID=3>Loral Corp.</ENTITY> of New York forIntelsat</ANP> owner of<ANP OWNER=4>an <ENTITY ID=4>Intelsat</ENTITY> satellite</ANP>Note that in these examples, the unique identier (ID) of the ENTITY or PLACE is being written asattribute information on the surrounding noun phrase tags with the appropriate label.Other types of link, such as the parent/child-organization relationship (e.g., \ Apache Corp., which ownsa majority stake in Hudson Energy"), can be easily added to IE2.EventTagFor the ST task, we decided to take a bottom-up, minimalist approach in MUC-7 rather than using ageneral-purpose full noun phrase or sentence parser.
The EventTag module starts with a set of syntactically-oriented rule templates.
We show some examples in Table 2 and Table 3.Then, an IE developer lls in the templates using example phrases and sentences from the training texts.Scenario-specic NP macros, such as $Vehicle and $Payload, are used to ll Arg's, while scenario-specicverb lists (e.g., LauchTransitiveV, AttackV) and noun lists (e.g., LaunchN, AttackN) are used to ll verbsand nouns in the templates respectively.
Some examples from the launch event are shown in Table 4.Our plan was to develop and integrate a trainable rule generalization module under TIPSTER 3, andapply it to an initial set of manually-coded rules to signicantly boost ST accuracy, especially recall.
Unfor-tunately, this eort did not start early enough to be completed and integrated with EventTag.
However, weare currently making progress, and believe that the rule generalization module will improve the ST accuracysubstantially.Discourse ModuleIn the past year, we also developed a new discourse module for co-reference resolution that is bothtrainable and congurable.
We wanted an automatically trainable system to ease portability and achievehigh accuracy, as well as a congurable system to optimize the benet of discourse resolution for dierent IEtasks.
Our goal was to develop a discourse module which can improve specic IE tasks, rather than developa generic one which may perform well overall but may not necessarily increase the IE performance.
For this,we aimed at developing a high-precision discourse module.Template 1 Arg1 IntransitiveVTemplate 2 Arg1 IntransitiveV prep Arg2Template 3 Arg1 TransitiveV Arg2Template 4 Arg1 TransitiveV Arg2 prep Arg3Template 5 Arg1 PassiveV by Arg2Template 6 Arg1 PassiveVTable 2: ST Rule Template Examples (Verbs)4Template 7 Arg1 nounTemplate 8 noun of Arg1Template 9 noun of Arg1 prep Arg2Table 3: ST Rule Template Examples (Nouns)Template 7 $Vehicle + LaunchN \the Arian 5 launch"Template 8 LaunchN + of + $Payload \today's failed launch of a satellite"Template 1 $Vehicle + FailureIntransitiveV \A Chinese rocket exploded"Template 6 $Payload + LaunchPassiveV \a second satellite to be launched"Table 4: Launch RulesThe new Discourse Module employs three co-reference resolution strategies so that it can be conguredfor its best performance for dierent IE tasks.
The rule-based strategy uses the CLIPS engine.
We havedeveloped rules to resolve denite NPs (ANP, PNP, ENP) and singular personal pronouns (e.g., \he," \his,"\him").
The machine learning strategy uses C50, which is a decision tree implementation [4], to learn co-reference resolution automatically from a tagged corpus.
This is a re-implemented (in C++) and improvedversion of SRA's previous machine learning-based co-reference resolution module [2].
The third strategyis a hybrid method where the module rst applies the rule-based strategy to narrow down the possibleantecedents and then applies the machine learning strategy to order the possible antecedent candidates.The Discourse Module is also congurable so that one can select a set of anaphora types to resolve foreach task.
Currently the module resolves: name aliases (artifacts, people, organizations, places), denite NPs (ANP, ENP, PNP), and singular personal pronouns.Name alias resolution for people, organizations, and places is performed in addition to that performed byNetOwl Extractor 3.0 in order to increase recall.
Appositive links are always made by PhraseTag becauseof its local nature.For TE, TR, and ST, we resolved name alias and appositive anaphora.
However, we performed denite NPand pronoun resolution only for TR and ST because doing so did not increase the TE score in experimentingwith the formal training texts.
We think this is because the discourse module was trained on the dry-runtraining texts (i.e., aircraft crash domain), and the lack of training on the formal test domain (i.e., spacecraftlaunch domain) made the resolution of denite NPs and pronouns less accurate.
In fact, the module hadincreased the TE score on the dry-run test texts.This indicates the delicate trade o between recall and precision in IE.
For the TE task, the denite NPresolution should theoretically increase the recall of the DESCRIPTOR slot.
However, as most descriptors areactually found locally, either in appositives or copula constructions, it takes a very high-precision discoursemodule to improve the recall of this slot without hurting precision.
The discourse module with denite NPand pronoun resolution did however increase the ST score by 2 points in the formal test.The Discourse Module is still relatively new, and it needs to be trained on more texts.
We plan toperform additional experiments to make it a high-precision system that helps IE tasks.5<PERSON ID=5>Jeff Bantle</PERSON>, <PNP REF=5 AFFIL=12><ENTITY ID=12>NASA</ENTITY>'smission operations directorate representative for the shuttle flight</PNP>.Figure 2: Simplied Discourse Module outputTempGenSRA's new template generator, TempGen, is implemented in JAVA, and is considerably easier to congureand customize than our previous versions.
This module takes SGML output of the Discourse Module andmaps it into TE, TR or ST templates.
It uses an SGML-to-template mapping conguration le so thatmapping is more declarative and easier to customize.
For instance, for a phrase like \Je Bantle, NASA'smission operations directorate representative for the shuttleight," the Discourse Module produces thesimplied SGML output in Figure 2.TempGen takes this output and produces two TE templates (cf., Figure 3 and Figure 4), and one TRtemplate (cf., Figure 5).
The PNP's REF register holds the ID of the person the PNP refers to (i.e., JeBantle).
Thus, the PNP is used for ENT DESCRIPTOR of the TE template for \Je Bantle."
The PNP'sAFFIL register holds the person's aliation, and is used for the EMPLOYEE OF TR for the person (i.e.,Je Bantle) and its employer (i.e., NASA).For ST, TempGen integrates a Time Module, which interprets and normalizes time expressions accordingto the MUC-7 time guidelines.
TempGen also performs event merging.
While the Discourse Module takescare of the merging of noun phrases describing payloads and vehicles in the launch domain, the TempGenmakes decisions on whether or not to merge two launch events based on the consistency of payloads, vehicles,time, and location participating in the two events.
Event merging is a complex operation because the accuracyof the merging operation depends on various factors including: accuracy in the co-reference resolution of payloads and vehicles, correct interpretation of time phrases (e.g., \two days ago"), correct inference on whether two time/location expressions are consistent.
(e.g., \yesterday morning" vs. \on Wednesday," \Florida" vs. \Miami").For instance, in the example below, knowing that \Wednesday" and \tomorrow" are the same day iscrucial for event merging.\China plans to send a satellite into orbit Wednesday ...
In tomorrow's launch, a Long March3 rocket will carry an Apstar-1A satellite.
"Our plans to improve event merging include:<ENTITY-9601120403-13> :=ENT_NAME: Jeff BantleBantleENT_TYPE: PERSONENT_CATEGORY: PER_CIVENT_DESCRIPTOR: NASA's mission operations directorate representative forthe shuttle flightFigure 3: TE for Je Bantle6<ENTITY-9601120403-4> :=ENT_NAME: National Aeronautics and Space AdministrationNASAENT_TYPE: ORGANIZATIONENT_CATEGORY: ORG_GOVTFigure 4: TE for NASA<EMPLOYEE_OF-9601120403-44> :=PERSON: <ENTITY-9601120403-13>ORGANIZATION: <ENTITY-9601120403-4>Figure 5: TR for Je Bantle and NASA enhancing the Discourse Module for co-reference resolution of noun phrases and the Time Module fortime interpretation, incorporating event merging as a part of the trainable Discourse Module.ANNOTATION AND DEVELOPMENT TOOLSOne of the crucial aspects of developing a high-performance IE system is to have the right tools forthe right modules.
In general, IE development requires annotation tools for creating training examples anddevelopment environments for evaluating and debugging the output of a given IE module.
Having the righttools not only speeds up development signicantly, but also enables high accuracy.SRA has considerable experience in building annotation and development tools for multilingual informa-tion extraction.
Annotators of MUC-6, MUC-7, MET-1, and MET-2 used SRA's Named Entity Tool forNE annotation, and SRA's Discourse Tagging Tool for the Coref annotation [1].
SRA's Name Entity Toolcurrently works with Chinese, Japanese, Arabic, Thai, Russian, and all the Romance alphabet languages.In the past year, we built the following three new tools to support development for the TE, TR, and Coreftasks.
All the tools were implemented in JAVA to support multilingual, cross-platform capabilities.Annotation Tool: A GUI-based template-level annotation tool which can create TE, TR, and ST typetemplates.
The tool is easily congurable from the GUI, and can be integrated with IE2to functionalso as a post-editing tool.
Figure 6 shows the Annotation Tool.Template Tool: A GUI-based template-level development environment which enables inspection and de-bugging of complex template structures.
The tool is designed to allow an IE developer to run anyportion of the IE system from the GUI, and score the results using a scoring program for immediatefeedback.
Figure 7 shows the Template Tool.Discourse Debugger: A GUI-based discourse development environment which allows evaluation of com-plex co-reference links.
The tool lets an IE developer run the discourse module on any document ordocument sets, scores the results, and can display both the system-generated links and the links fromthe key graphically.
Figure 8 illustrates the Discourse Debugger.We believe that our ability to create training examples quickly with our annotation tools and evaluatesystem performance in an eective manner using our development environments contributed to our goodperformance in MUC-7.7Figure 6: Annotation ToolFigure 7: Template Tool8Figure 8: Discourse DebuggerEVALUATION RESULTSIE2performed very well on its three tasks (cf., Table 1).
In TE, it achieved operational quality,performing well above 85% F-M.
Both the TE and TR scores were statistically signicantly higher than thesecond best scores in each task.
TR was almost at the operational level.
In fact, the employee of relationscored a 87.59% F-M (Recall=82, Precision=94).
While ST did not achieve operational-level accuracy duringthe four-week time limit, it still achieved the highest score among the ST participants.
We believe that wehave an eective strategy for enhancing performance on this task, as described in the \Lessons Learned"Section.SYSTEM FACTSHEETSRA's TurboTag and the Discourse Module are written in C++, TempGen in JAVA.
In processing 100test texts on a SUN Ultra (167 MHz) with 128 MB of RAM, the system achieved the following performancegures: TE: 11 min., 17 sec.
(with Coref, add 5 min., 38 sec.
) TR: 18 min., 59 sec. ST: 19 min., 22 sec.We could increase throughput further easily by optimizing the TempGen code, distributing processes, and/orby performing a trade-o with development-timeexibility.9LESSONS LEARNEDLessons for TEOur system IE2performed very well in TE (Recall=86; Precision=87;F-M=86.76).
These results areespecially positive for two reasons.
First, the TE score shows a distinct improvement over that of MUC-6(80% F-M).
This is so in spite of the fact that the TE task is more complex in MUC-7 than in MUC-6.1Second, given the domain-independent design and implementation of our TE module, we believe that theseresults would carry over to other text types without much performance degradation.In our view, to go even higher { to achieve 90% F-M in TE { requires two things: Artifacts are a fairly new extraction target for the eld, at least by comparison with persons, organi-zations, and places.
In addition, there were far fewer artifact occurrences in the training texts than ofthe other name categories.
More training examples for artifacts will increase their yield signicantlyand improve the recognition of their descriptors. We can improve the recognition of long-distance links by increasing the accuracy of denite NP co-reference.
In future work, we will focus on trainable discourse.Lessons for TRIE2also did well in the TR task (Recall=67; Precision=86; F-M=75.63).
The most exciting sub-result wasin the employee of relation, which showed excellent performance (Recall=82; Precision=94; F-M=87.59).This piece of extraction technology is almost, but not quite, at the operational level.
To achieve TR of80 or above, we need: More training examples, particularly for the product of relation.
There were only 77 training examplesof this in the dry-run training texts. As with TE, we need improved discourse, specically better accuracy of denite NP and pronounco-reference.
This would improve performance on examples such as:{ \International Technology Underwriters" | \its chief executive" (employee of){ \Intelsat" | \the company's Washington headquarters" (location of)Lessons for STOur ST scores (Recall=42; Precision=65; F-M=50.79) reect the intrinsically dicult nature of the STtask.
We regard the following as the most important challenges to be overcome for ST to show improvementsup towards 75%. Rule Generalization: Knowing how to extend rule coverage beyond what is explicitly contained inthe training data is a critical goal to reach.
For example, while there were 3000 TE templates and 750TR templates in the dry-run training keys, there were only 80 launch events in the formal trainingkeys.
The current system has the intrinsic limitation that it encodes extraction rules from examplesin the training keys and from what the IE developer can intuit.
Corpus-based learning algorithms arerequired to extend the coverage of extraction rules much further while maintaining their accuracy.1MUC-7 requires the extraction of two additional elements: artifact TE templates and a descriptor for person TE templates.10 Event Merging: The elements of an ST event are frequently scattered over a text.
We need bettertechniques for understanding what and how to merge partial descriptions of the same event.
Enhancingthe current co-reference resolution of noun phrases will denitely help the merging of event arguments.In addition, we plan to incorporate event merging as a part of the Discourse Module, treating eventsalso as anaphora, and use a learning algorithm to acquire event merging rules from a corpus. Time Interpretation: Understanding the proper sequence of sub-events is critical for understandingthe structure of the overall event and for event merging in particular.
Fewer examples and less timedevoted to the Time Module made it less mature.
However, we are certain that we can improve thetime interpretation to increase the ST score signicantly.SUMMARY AND FUTURE DIRECTIONSThe IE eld has shown distinct progress in the last few years.
At MUC-6, several of the systems showedstrong results in name recognition.
At the current MUC, TE and TR have both been shown to be highlypracticable IE tasks.
TE in particular is already performing at an operational level.Very important to our success in MUC-7 was the quality of our new architecture, new tools, a newphrase/link tagging module, and an improved discourse module.
The new architecture emphasized modular-ity andexibility.
A modular architecture means that each module can be developed, tested, and improvedindependently of the other.
We also made the workow asexible as possible so that development of a\back-end" module does not require processing input through all the previous modules.However, to continue progress in extraction technology, we believe that the following track should befollowed: Continue development of learning-based, trainable modules:{ Automated learning of names and phrases.
Although name recognition is already perform-ing at a high accuracy level, this does not answer the question of how to port a name recognizerquickly and cost eectively to new languages or new text types.
In some cases a pattern matching-based name/phrase tagger is the best choice, while in other cases, a learning-based system is moresuitable.
SRA has already developed RoboTag, a decision-tree-based multilingual learning systemfor names, and has shown promising results [3].
We plan to continue enhancing RoboTag, andextend it to phrase recognition.
{ Trainable discourse module.
Discourse has to be better, both to get TE to 90%, to make TRoperational (greater than 80% F-M), and to make event merging more successful in ST. We planto pursue a hybrid strategy of both automated and manual acquisition of co-reference resolutionrules.
{ Trainable event rule generalization module.
Increasing the \reach" of extraction rules iscritical for next-generation extraction systems, particularly for ST. We are currently working ona trainable EventTag under the TIPSTER 3 eort. Develop a tightly integrated Annotation Toolset for ecient and consistent tagging of training textsfor all tasks.WALKTHROUGH (nyt960214-0509)SRA's system performed very well on nyt960214-0509.
Table 5 shows the scores for this article in the threerelevant tasks.
This text illustrates fairly well the strengths of SRA's system as well as some shortcomings.TE (94% F-M)With respect to TE's, below are the errors the system made:11Recall Precision F-MeasureTE 95 93 93.99TR 74 96 83.93ST 67 50 57.14Table 5: Scores for the Walkthrough Text It got some wrong aliases.
The system listed \Bloomberg" as an alias of \Bloomberg L.T".
It is analias of \Bloomberg Business News".
Similarly, the system did not recognize \News Corporation" asan alias of \News Corp." It failed to get the correct descriptor extent in two cases because of PhraseTag errors.
Instead of\one insurer in this consortium", the system reported \this consortium" as the descriptor.
Instead of\A Chinese rocket carrying an Intelsat satellite", the system reported \A Chinese rocket carrying anIntelsat satellite exploded as it" as the descriptor. It got a bad ENT CATEGORY for \Space Transportation Association": instead of ORG CO, it gotORG OTHER. It failed on one country normalization.
Instead of \French Guiana", it output \French Guyana". It output the wrong LOCAL TYPE for Xichang: AIRPORT instead or CITY/PROVINCE.And here is what the system did correctly: It recognized relatively long and complex descriptors.
We indicate the descriptors with brackets:\Intelsat is [a global supplier of international satellite communication services]"\Virnell Bruce, [spokeswoman for Lockheed Space and Strategic Missiles in Bethesda, Mary-land].
"\Eric Stallmer, [spokesman for the Space Transportation Association of Arlington, Virginia],which represents U.S. rocket makers who compete with the Chinese.
"\Bloomberg Information Television, [a unit of Bloomberg L.P., the parent of BloombergBusiness News], was in negotiations for carriage of its 24-hour news service on the satellitedestroyed today, it a company spokesman said.
" It recognized indenite NP's that are not associated with a name, but are specic:\today's failed launch of [a satellite built by Loral Corp. of New York for Intelsat]"\[a company spokesman] said"TR (84% F-M)As for TRs, here are the errors the system made: Some of the problems stem from the TEs.
\Space Transportation Association" got the wrongENT CATEGORY.
As a result, the scoring program matched it with the wrong TE in the keys.Moreover, two TRs which \Space Transportation Association" participates in were scored as partiallycorrect: the employee of TR (for Eric Stallmer) and location of TR (for Arlington).12 The system failed to recognize four long-distance relations.
One relation is implicit in the descriptor\a company spokesman," where \company" refers to \Bloomberg Information Television.
"\Bloomberg Information Television, a unit of Bloomberg L.P., the parent of Bloomberg Busi-ness News, was in negotiations for carriage of its 24-hour news service on the satellite de-stroyed today, a company spokesman said.
"Similarly, the second relation is implicit in the descriptor \company spokesman," where company refersto \News Corporation.
"\This failure will not aect News Corporation's launch plans for the direct-to-home satelliteservice" in Latin America, said company spokesman Howard J. Rubenstein in a statement.
"In the third case, the link requires the anaphoric resolution of a possessive pronoun across paragraphs:\its chief executive," where \its" refers to \International Technology Underwriters."
Currently, theanaphoric resolution module only resolves personal possessive pronouns.\International Technology Underwriters of Bethesda, Maryland, is one insurer in this con-sortium.
it The company is 80 percent owned by Paris insurer Axa SA and 20 percent byPrudential Reinsurance Holdings Inc. of Newark, New Jersey.Its chief executive, former space shuttle astronaut Rick Hauck, wouldn't comment on the sizeof International Technology's loss.
The company insures about 20 to 30 satellites a year.
"In the fourth case, the system missed the relation between \Intelsat" and \Washington."
Resolving\the company" to \Intelsat" would x this problem.\His comments came at the company's Washington headquarters" The system did not output a PRODUCT OF TR for \Long March 3B", because of an error in the TRtemplate generation.Most of the errors are easily xable, and the system got the rest of the relations correctly, with 84% asthe overall F-M for this text.ST (57% F-M)The system successfully recognized all the launch events mentioned and their respective payloads andvehicles in this text.
However, the system over-generated three more templates for two reasons.
First,EventTag did not recognize two generic discussions of a satellite launch in this text.
Consequently, itoutputs launch events for the following two sentences.\U.S.-made rockets are not yet powerful enough alone to send a satellite as heavy as the onelaunched today into orbit.
"\Communications satellites typically cost about $150 million to $300 million to build and launch.
"Second, the system did not recognize the launch event in (s2) below to be co-referent with the launchevent in (s1).
This merging was particularly dicult because of the use of the indenite noun phrase \asatellite" (instead of a denite \the satellite") in the second sentence.
(s1) A Chinese rocket carrying an Intelsat satellite exploded as it was being launched today,delivering a blow to a group including Rupert Murdoch's News Corp. and Tele-CommunicationsInc.
that planned to use the spacecraft to beam television signals to Latin America.13(s2) The China Great Wall Industry Corp. provided the Long March 3B rocket for today's failedlaunch of a satellite built by Loral Corp. of New York for Intelsat.In addition, the Time Module failes to interpret certain time descriptors correctly.
For instance, themodule converted \later this month" to the default document date.<TIME-9602140509-99> :=START: 14021996END: 14021996DESCRIPTOR: later this monthACKNOWLEDGEMENTSWe would like to thank John Maloney, Michael Niv, and Bob Schlosser for their help with various aspectsof the MUC-7 eort.REFERENCES[1] Aone, Chinatsu and Bennett, Scott W. Discourse Tagging Tool and Discourse-tagged Multilingual Cor-pora.
In Proceedings of International Workshop on Sharable Natural Language Resources (SNLR), 1994.
[2] Aone, Chinatsu and Bennett, Scott William.Evaluating Automated and Manual Acquisition of AnaphoraResolution Strategies.
In Proceedings of 33rd Annual Meeting of the ACL, 1995.
[3] Bennet, Scott W, Aone, Chinatsu, and Lovell, Craig.
Learning to Tag Multilingual Texts Through Ob-servation.
In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing(EMNLP-2)., 1997.
[4] Quinlan, J. Ross.
C4.5: Programs for Machine Learning.
Morgan Kaufmann Publishers, 1993.14
