A DP based Search Algorithm for Statistical Machine TranslationS.
Nieflen, S. Vogel, H. Ney, and C. T i l lmannLehrstuhl  fiir In format ik  VIRWTH Aachen - University of TechnologyD-52056 Aachen, GermanyEmaih n iessen?informat ik ,  rwth -aachen,  deAbst rac tWe introduce a novel search algorithm for statisti-cal machine translation based on dynamic program-ming (DP).
During the search process two statis-tical knowledge sources are combined: a translationmodel and a bigram language model.
This search al-gorithm expands hypotheses along the positions ofthe target string while guaranteeing progressive cov-erage of the words in the source string.
We presentexperimental results on the Verbmobil task.1 In t roduct ionIn this paper, we address the problem of finding themost probable target language representation f agiven source language string.
In our approach, weuse a DP based search algorithm which sequentiallyvisits the target string positions while progressivelyconsidering the source string words.The organization of the paper is as follows.
Af-ter reviewing the statistical approach to machinetranslation, we first describe the statistical know-ledge sources used during the search process.
Wethen present our DP based search algorithm in de-tail.
Finally, experimental results for a bilingual cor-pus are reported.1.1 Statistical Machine TranslationIn statistical machine translation, the goal of thesearch strategy can be formulated as follows: Weare given a source language ('French') string fl a =fl ?
.- f.t, which is to be translated into a target lan-guage ('English') string e~ = el...et with the un-known length I.
Every English string is consideredas a possible translation for the input string.
If weassign a probability Pr(e~\[f() to each pair of strings(e/, f~), then we have to choose the length Iopt and-/opt the English string e 1 that maximize Pr(e f If J) fora given French string f J .
According to Bayes deci-sion rule, Iopt and ~?"
can be found by(Iopt,J1 ?"')
=argmax{Pr(e~lf~)}I ,e{= argmax{Pr(e().Pr(f~Jlel)}.
(1)l ,e IPr(e~) is the English language model, whereasPr(flJ\[eZa) is the string translation model.The overall architecture of the statistical transla-tion approach issummarized in Fig.
1.
In this figure,we already anticipate the fact that we will transformthe source strings in a certain manner and that wewill countermand these transformations on the pro-duced output strings.
This aspect is explained inmore detail in Section 3.Source Language TextTransformation 1\[ Global Search: -1_ Pr(faIel)maximize Pr(el).
Pr(f~lel)over e III Transformation I1Target Language TextLexicon Model II ..o,o.o.,o?., \]Figure 1: Architecture of the translation approachbased on Bayes' decision rule.The task of statistical machine translation can besubdivided into two fields:1. the field of modelling, which introduces truc-tures into the probabilistic dependencies andprovides methods for estimating the parametersof the models from bilingual corpora;2. the field of decoding, i.e.
finding a search algo-rithm, which performs the argmax operation inEq.
(1) as efficient as possible.9601.2 Al ignment with Mixture Distr ibutionSeveral papers have discussed the first issue, espe-cially the problem of word alignments for bilingualcorpora (Brown et al, 1993), (Dagan et al, 1993),(Kay and RSscheisen, 1993), (Fung and Church,1994), (Vogel et al, 1996).In our search procedure, we use a mixture-basedalignment model that slightly differs from the modelintroduced as Model 2 in (Brown et al, 1993).
It isbased on a decomposition f the joint probability forf~ into a product of the probabilities for each worddPr(flJle~) = p( J\[I) " H p(fJl eta), (2)3=1where the lengths of the strings are regarded asrandom variables and modelled by the distributionp(JlI).
Now we assume a sort of pairwise interac-tion between the French word fj and each Englishword ei in el i.
These dependencies are captured inthe form of a mixture distribution:Ip(f31ezl) = ~'~p(ilj, J,I) " p(fjlei) ?
(3)i=1Inserting this into (2), we getJ IPr(Y(le~) = p(JII) YI ~~p(ilj, J, i) .
p(f~le,) (4)j= l  i=1with the following components: the sentence l ngthprobability p(JlI), the mixture alignment probabil-ity p(ilj, J, I) and the translation probability p(fle).So far, the model allows all English words in thetarget string to contribute to the translation of aFrench word.
This is expressed by the sum over iin Eq.
(4).
It is reasonable to assume that for eachsource string position j one position i in the targetstring dominates this sum.
This conforms with theexperience, that in most cases a clear word-to-wordcorrespgndence b tween a string and its translationexists.
As a consequence, we use the so-called max-imum approximation: At each point, only the bestchoice of i is considered for the alignment path:dPr(/( le~) =p( JII) ~I m~ p(ilj, J, I).p(fj\]e~).
(5)j= l  "e l ,  \]We can now formulate the criterion to be maximizedby a search algorithm:max \[p( JlI) max { Pr(e\[).
I elH m.ax ~(ilj, J,I ) .p(f31e~)l .j= l  ie\[1,1\](6)Because of the problem of data sparseness, we usea parametric model for the alignment probabilities.It assumes that the distance of the positions relativeto the diagonal of the (j, i) plane is the dominatingfactor:r(i _ j  I )p(ilj, J, I) = (7), E i ,= l  r(i' - j )As described in (Brown et al, 1993), the EM al-gorithm can be used to estimate the parameters ofthe model.1.3 Search in Stat ist ica l  Mach ineTranslationIn the last few years, there has been a number ofpapers considering the problem of finding an effi-cient search procedure (Wu, 1996), (Tillmann et al,1997a), (TiUmann et al, 1997b), (Wang and Waibel,1997).
All of these approaches u e a bigram languagemodel, because they are quite simple and easy-to-use and they have proven their prediction powerin stochastic language processing, especially speechrecognition.
Assuming abigram language model, wewould like to re-formulate Eq.
(6) in the followingway:Jmax\[p(J\]I)m~axljX~l max~(e i le i -1 ) ' l  e 1 .= iE\[1,I\]P(ilJ, J,I)'p(fjlei)\] }\] ?
(8)Any search algorithm tending to perform the max-imum operations in Eq.
(8) has to guarantee, thatthe predecessor word ei-1 can be determined at thetime when a certain word ei at position i in the tar-get string is under consideration.
Different solutionsto this problem have been studied.
(Tillmann et al, 1997b) and (Tillmann et al,1997a) propose asearch procedure based on dynamicprogramming, that examines the source string se-quentially.
Although it is very efficient in termsof translation speed, it suffers from the drawbackof being dependent on the so-called monotonicityconstraint: The alignment paths are assumed tobe monotone.
Hence, the word at position i - 1in the target sentence can be determined when thealgorithm produces ei.
This approximation corre-sponds to the assumption of the fundamental simi-laxity of the sentence structures in both languages.In (Tillmann et al, 1997b) text transformations ithe source language are used to adapt the word or-dering in the source strings to the target languagegrammar.
(Wang and Waibel, 1997) describe an algorithmbased on A*-search.
Here, hypotheses are extended961by adding a word to the end of the target stringwhile considering the source string words in any or-der.
The underlying translation model is Model 2from (Brown et al, 1993).
(Wu, 1996) formulates a DP search for stochasticbracketing transduction grammars.
The bigram lan-guage model is integrated into the algorithm at thepoint, where two partial parse trees are combined.2 DP  Search2.1  The  Inver ted  A l ignment  Mode lFor our search method, we chose an algorithm whichis based on dynamic programming.
Compared to anA'-based algorithm dynamic programming has thefundamental dvantage, that solutions of subprob-lems are stored and can then be re-used in laterstages of the search process.
However, for the op-timization criterion considered here dynamic pro-gramming is only suboptimal because the decompo-sition into independent subproblems is only approx-imately possible: to prevent he search time of asearch algorithm from increasing exponentially withthe string lengths and vocabulary sizes, local deci-sions have to be made at an earlier stage of the opti-mization process that might turn out to be subopti-mal in a later stage but cannot be altered then.
Asa consequence, the global optimum might be missedin some cases.The search algorithm we present here combinesthe advantages of dynamic programming with thesearch organization along the positions of the targetstring, which allows the integration of the bigram ina very natural way without restricting the alignmentpaths to the class of monotone alignments.The alignment model as described above is definedas a function that assigns exactly one target word toeach source word.
We introduce a new interpretationof the alignment model: Each position i in e / isassigned a position bi = j in fl J.
Fig.
2 illustratesthe possible transitions in this inverted model.At each position i of el, each word of the targetlanguage vocabulary can be inserted.
In addition,the fertility l must be chosen: A position i and theword ei at this position are considered to correspondto a sequence of words f~:+1-t in f \ ] .
In most cases,the optimal fertility is 1.
It is also possible, that aword ei has fertility 0, which means that there is nodirectly corresponding word in the source string.
Wecall this a skip, because the position i is skipped inthe alignment path.Using a bigram language model, Eq.
(9) specifiesthe modified search criterion for our algorithm.
Hereas above, we assume the maximum approximation tobe valid.?
~e~L , .. ~0.
NO: : _.
?
: _" : :c~.-... ~ ...-.~position in source stringFigure 2: Transitions in the inverted model.Imax\[p(dlI)ma:xH\[p(eiJei-1)"I el i=1' J max 1"I {P(ilJ'J'I)'P(:31ei)} j , t _  l= j - l+ l(9)For better legibility, we regard the second productin Eq.
(9) to be equal to 1, i f l  = 0.
It should bestressed that the pair (I,e{) optimizing Eq.
(9) isnot guaranteed to be also optimal in terms of theoriginal criterion (6).2.2  Bas ic  P rob lem:  Pos i t ion  Coverage.4.
closer look at Eq.
(9) reveals the most importantproblem of the search organization along the targetstring positions: It is not guaranteed, that all thewords in the source string are considered.
In otherwords we have to force the algorithm to cover allinput string positions.
Different strategies to solvethis problem are possible: For example, we can in-troduce a reward for covering a position, which hasnot yet been covered.
Or a penalty can be imposedfor each position without correspondence in the tar-get string.In preliminary experiments, we found that themost promising method to satisfy the position cov-erage constraint is the introduction of an additionalparameter into the recursion formula for DP.
In thefollowing, we will explain this method in detail.2.3  Recurs ion  Formula for DPIn the DP formalism, the search process is describedrecursively.
Assuming a total length I of the targetstring, Q1(c, i, j, e) is the probability of the best par-tial path ending in the coordinates i in el / and j inf J, if the last word ei is e and if c positions in thesource string have been covered.962This quantity is defined recursively.
Leaving aword ei without any assignment (skip) is the easiestcase:QS (c, i, j, e) = max {p(ele')Q1(c, i - 1, j, d)} .Note that it is not necessary to maximize over thepredecessor positions jr: This maximization is sub-sumed by the maximization over the positions on thenext level, as can easily be proved.In the original criterion (6), each position j in thesource string is aligned to exactly one target stringposition i.
Hence, if i is assigned to I subsequent po-sitions in f l  s, we want to verify that none of these po-sitions has already been covered: We define a controlfunction v which returns 1 if the above constraint issatisfied and 0 otherwise.
Then we can write:fH Qr\](c,i.j,e) = max {p(il3, J , I  ) ?
p(f3iei)} ??
l>0  " ~=j - - l+ lmax {p(eie')"e jmjax \ [Q , (c -  l,i - 1 , f  ,e') .v (c , l , f  ,j,e')\] )\] ,We now have to find the maximum:Q,(c, i, j, e) = max {QS(c, i, j, e), Qn(c, i, j, e)} .The decisions made during the dynamic program-ming process (choices of l, j '  and e ~) are stored forrecovering the whole translation hypothesis.The best translation hypothesis can be found byoptimizing the target string length I and requiringthe number of covered positions to be equal to thesource string length J:max {P(JlI) " maxQ1( J ' I ' j 'e)  } j,e (10)2.4 Accelerat ion TechniquesThe time comple.,dty of the translation method asdescribed above isO(i2ax " j3 .
iSl 2) ,where I~\] is the size of the target language vocab-ulary C. Some refinements of this algorithm havebeen implemented to increase the translation speed.1.
We can expect the progression of the sourcestring coverage to be roughly proportional tothe progression of the translation procedurealong the target string.
So it is legitimate todefine a minimal and maximal coverage for eachlevel i:Cmin(i)= \[ i J J  - r ,  Cmax(i)= \[ i~\] + r ,where r is a constant integer number.
In prelim-inary experiments we found that we could set rto 3 without any loss in translation accuracy.This reduces the time complexity by a factor J.2.
Optimizing the target string length as formu-lated in Eq.
(10) requires the dynamic program-ming procedure to start all over again for eachI.
If we assume the dependence of the align-ment probabilities p(ilj, J, I) on I to be negligi-ble, we can renormalize them by using an esti-mated target string length/~ and use p(ilj , J, I).Now we can produce one translation e~ at eachlevel i = I without restarting the whole process:max Vi(J ,  I, j, e) .
(11)3,eFor/~ we choose: /~ = \]( J )  = J - /~-  where p?and #j  denote the average lengths of the targetand source strings, respectively.This approximation is partly undone by whatwe call rescoring: For each translation hypoth-esis e / with length I, we compute the "true"score (~(I) by searching the best inverted align-ment given e / and fs  and evaluating the prob-abilities along this alignment.
Hence, we finallyfind the best translation via Eq.
(12):max{,(JII) (12)The time complexity for this additional step isnegligible, since there is no optimization overthe English words, which is the dominant factorin the overall time complexityO(Imax " j2 .
\[E.\[2) .3.
We introduced two thresholds:SL" If e' is the predecessor word of e and e isnot aligned to the source string ("skip"),then p(eie') must be higher than SL.ST" A word e can only be associated with asource language word f ,  if p(f\[e) is higherthan ST.This restricts the optimization over the targetlanguage vocabulary to a relatively small set ofcandidate words.
The resulting time complexityisO(Im~x.
J2- IE I ) .4.
When searching for the best partial path to agridpoint G = (c,i, j,e), we can sort the arcsleading to G in a specific manner that allows usto stop the computation whenever it becomesclear that no better partial path to G exists.The effect of this measure depends on the qual-ity of the used models; in preliminary experi-ments we observed a speed-up factor of about3.5.9633 Exper imentsThe search algorithm suggested in this paper wastested on the Verbmobil Corpus.
The results of pre-liminary tests on a small automatically generatedCorpus (Amengual et al, 1996) were quite promis-ing and encouraged us to apply our search algorithmto a more realistic task.The Verbmobil Corpus consists of spontaneouslyspoken dialogs in the domain of appointment sche-duling (Wahlster, 1993).
German source sentencesare translated into English.
In Table 1 the character-istics of the training and test sets are summarized.The vocabularies include category labels for dates,proper names, numbers, times, names of places andspellings.
The model parameters were trained on16 296 sentence pairs, where names etc.
had beenreplaced by the appropriate labels.Table 1: Training and test conditions of the Verb-mobil task.formed sample translations (i.e.
after labelling) was13.8.In preliminary evaluations, optimal values for thethresholds OL and OT had been determined and keptfixed during the experiments.As an automatic and easy-to-use measure of thetranslation performance, the Levenshtein distancebetween the produced translations and the sampletranslations was calculated.
The translation resultsare summarized in Table 2.Table 2: Word error rates on the Verbmobil Corpus:insertions (INS), deletions (DEL) and total rateof word errors (WER) before (BL) and after (AL)rule-based translation of the labels.before / after Error Rates (%)INS DEL WERBL 7.3 18.4 45.0AL 7.6 17.3 39.6Words in VocabularyGerman 4 498English 2 958Number of Sentencesin Training Corpus 16 296in Test Corpus 150Given the vocabulary sizes, it becomes quite ob-vious that the lexicon probabilities p(f\[e) can notbe trained sufficiently on only 16 296 sentence pairs.The fact that about 40% of the words in the lexiconare seen only once in training illustrates this.
To im-prove the lexicon probabilities, we interpolated themwith lexicon probabilities pM(fle) manually createdfrom a German-English dictionary:{'o ~ if (e, f) is in the dictionary pM(fle) _ otherwise 'where Ne is the number of German words listed astranslations of the English word e. The two lexicawere combined by linear interpolation with the in-terpolation parameter A.
For our first experiments,we set A to 0.5.The test corpus consisted of 150 sentences, forwhich sample translations exist.
The labels weretranslated separately: First, the test sentences werepreprocessed in order to replace words or groupsof words by the correct category label.
Then, oursearch algorithm translated the transformed sen-tences.
In the last step, a simple rule-based algo-rithm replaced the category labels by the transla-tions of the original words.We used a bigram language model for the Eng-lish language.
Its perplexity on the corpus of trans-(Tillmann et al, 1997a) report a word error rateof 51.8% on similar data.Although the Levenshtein distance has the greatadvantage to be automatically computable, we haveto keep in mind, that it depends fundamentally onthe choice of the sample translation.
For example,each of the expressions "thanks", "thank you" and"thank you very much" is a legitimate translationof the German "danke schSn", but when calculatingthe Levenshtein distance to a sample translation, atleast two of them will produce word errors.
Themore words the vocabulary contains, the more im-portant will be the problem of synonyms.This is why we also asked five experts to classifyindependently the produced translations into threecategories, being the same as in (Wang and Waibel,1997):Correct translations are grammatical nd conveythe same meaning as the input.Acceptable translations convey the same meaningbut with small grammatical mistakes or they conveymost but not the entire meaning of the input.Incorrect ranslations are ungrammatical or con-vey little meaningful information or the informationis different from the input.Examples for each category are given in Table3.
Table 4 shows the statistics of the translationperformance.
When different judgements existedfor one sentence, the majority vote was accepted.For the calculation of the subjective sentence rrorrate (SSER), translations from the second categorycounted as "half-correct".When evaluating the performance of a statisticalmachine translator, we would like to distinguish er-rors due to the weakness of the underlying models964Table 3: Examples of Correct (C), Acceptable (A), and Incorrect (I) translations on Verbmobil.
The sourcelanguage is German and the target language is English.CInput:Output:Input:Output:Ah neunter M/irz bin ich in KSln.I am in Cologne on the ninth of March.Habe ich mir notiert.I have noted that.AInput:Output:Input:Output:Samstag und Februar sind gut,  aber der siebzehnte w~ire besser.Saturday and February are quite but better the seventeenth.Ich kSnnte erst eigentlich jetzt wieder dann November vorschlagen.
Ab zweitenNovember.I could actually coming back November then.
Suggest beginning the second of November.Input:Output:Ja, also mit Dienstag und mittwochs und so h/itte ich Zeit, aber Montag kommen wirhier nicht weg aus Kiel.Yes, and including on Tuesday and Wednesday as well, I have time on Monday but wewill come to be away from Kiel.Input: Dann fahren wir da los.Output: We go out.Table 4: Subjective valuation of the translationperformance on Verbmobil: number of sentencesevaluated as Correct (C), Acceptable (A) or In-correct (I).
For the total percentage of non-correcttranslations (SSER), the "acceptable" translationsare counted as half-errors.I Total Correct Acceptable Incorrect SSER I150 61 45 44 44.3%from search errors, occuring whenever the searchalgorithm misses a translation hypothesis with ahigher score.
Unfortunately, we can never be surethat a search error does not occur, because we donot know whether or not there is another string withan even higher score than the produced output.Nevertheless, it is quite interesting to compare thescore of the algorithm's output and the score of thesample translation in such cases in which the out-put is not correct (it is classified as "acceptable" or"incorrect" ).The original value to be maximized by the searchalgorithm (see Eq.
(6)) is the score as defined by theunderlying models and described by Eq.
(13).JPr(e~).p(JII) H max ~(ilj , J, I).
p(fjlei)\] ?
(13)j=l ie\[1,1\]We calculated this score for the sample trans-lations as well as for the automatically generatedtranslations.
Table 5 shows the result of the com-parison.
In most cases, the incorrect outputs havehigher scores than the sample translations, whichleads to the conclusion that the improvement of themodels (stronger language model for the target lan-guage, better translation model and especially moretraining data) will have a strong impact on the qual-ity of the produced translations.
The other cases, i.e.
those in which the models prefer the sample trans-lations to the produced output, might be due to thedifference of the original search criterion (6) and thecriterion (9), which is the basis of our search algo-rithm.
The approximation made by the introductionof the parameters OT and OL is an additional reasonfor search errors.Table 5: Comparison: Score of Reference Transla-tion e and Translator Output e ~ for "acceptable"translations (A) and "incorrect" translations (I).For the total number of non-correct translations(T), the "acceptable" translations are counted ashalf-errors.A I T %Total number 45 44 66.5 100.0Score(e) >_ Score(C) 11 13 18.5 27.8Score(e) < Score(C) 34 31 48.0 72.2As far as we know, only two recent papers havedealt with decoding problem for machine translationsystems that use translation models based on hid-den alignments without a monotonicity constraint:(Berger et al, !994) and (Wang and Waibel, 1997).The former uses data sets that differ significantlyfrom the Verbmobil task and hence, the reportedresults cannot be compared to ours.
The latterpresents experiments carried out on a corpus corn-965parable to our test data in terms of vocabulary sizes,domain and number of test sentences.
The authorsreport a subjective sentence rror rate which is inthe same range as ours.
An exact comparison isonly possible if exactly the same training and test-ing data are used and if all the details of the searchalgorithms are considered.4 Conc lus ion  and  Future  WorkIn this paper, we have presented a new search al-gorithm for statistical machine translation.
Firstexperiments prove its applicability to realistic andcomplex tasks such as spontaneously spoken dialogs.Several improvements o our algorithm are plan-ned, the most important one being the implementa-tion of pruning methods (Ney et al, 1992).
Pruningmethods have already been used successfully in ma-chine translation (Tillmann et al, 1997a).
The firstquestion to be answered in this context is how tomake two different hypotheses H1 and/-/2 compara-ble: Even if they cover the same number of sourcestring words, they might cover different words, es-pecially words that are not equally difficult to trans-late, which corresponds to higher or lower transla-tion probability estimates.
To cope with this prob-lem, we will introduce a heuristic for the estimationof the cost of translating the remaining source words.This is similar to the heuristics in A'-search.
(Vogel et al, 1996) report better perplexity re-sults on the Verbmobil Corpus with their HMM-based alignment model in comparison to Model 2of (Brown et al, 1993).
For such a model, however,the new interpretation of the alignments becomesessential: We cannot adopt the estimates for thealignment probabilities p(ili', I).
Instead, we haveto re-calculate them as inverted alignments.
Thiswill provide estimates for the probabilities P(JlJ', J).The most important advantage of the HMM-basedalignment models for our approach is the fact, thatthey do not depend on the unknown target stringlength I.Acknowledgement.
This work was partly sup-ported by the German Federal Ministry of Educa-tion, Science, Research and Technology under theContract Number 01 IV 601 A (Verbmobil).ReferencesJ.
C. Amengual, J. M. Benedi, A. Castafio, A. Mar-zal, F. Prat, E. Vidal, J. M. Vilar, C. Delogu,A.
di Carlo, H. Ney, and S. Vogel.
1996.
Example-Based Understanding and Translation Systems(EuTrans): Final Report, Part I.
Deliverable ofESPRIT project No.
20268, October.A.L.
Berger, P.F.
Brown, J. Cocke, S.A. DellaPietra, V.J.
Della Pietra, J.R. Gillett, J.D.
Laf-ferty, R.L.
Mercer, H. Printz, and L. Ures.
1994.The Candide System for Machine Translation.
InProc.
ARPA Human Language Technology Work-shop, Plainsboro, N J, pages 152-157.
MorganKanfmann Publ., March.P.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra,and R.L.
Mercer.
1993.
Mathematics of Statisti-cal Machine Translation: Parameter Estimation.Computational Linguistics, 19(2):263-311.I.
Dagan, K. W. Church, and W. A. Gale.
1993.Robust Bilingual Word Alignment for MachineAided Translation.
In Proceedings of the Work-shop on Very Large Corpora, Columbus, Ohio,pages 1-8.P.
Fung and K.W.
Church.
1994.
K-vet: A new Ap-proach for Aligning Parallel Texts.
In Proceedingsof the 15th International Conference on Compu-tational Linguistics, Kyoto, Japan, pages 1096-1102.M.
Kay and M. RSscheisen.
1993.
Text-Trans-lation Alignment.
Computational Linguistics,19(1):121-142.H.
Ney, D. Mergel, A. Noll, and A. Paeseler.
1992.Data Driven Search Organization for ContinuousSpeech Recognition.
IEEE Transactions on Sig-nal Processing, 40(2):272-281, February.C.
Tillmann, S. Vogel, H. Ney, H. Sawaf, and A. Zu-biaga.
1997a.
Accelerated DP based Search forStatistical Translation.
In Proceedings of the 5thEuropean Conference on Speech Communicationand Technology, Rhodes, Greece, pages 2667-2670,September.C.
Tillmann, S. Vogel, H. Ney, and A. Zubia-ga. 1997b.
A DP-Based Search using MonotoneAlignments in Statistical Translation.
In Proceed-ings of the ACL/EACL '97, Madrid, Spain, pages289-296, July.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-Based Word Alignment in Statistical Translation.In Proceedings of the 16th International Confer-ence on Computational Linguistics, Copenhagen,Denmark, pages 836-841, August.W.
Wahlster.
1993.
Verbmobih Translation of Face-to-Face Dialogs.
In Proceedings of the MT Sum-mit IV, pages 127-135, Kobe, Japan.Ye-Yi Wang and A. Waibel.
1997.
Decoding Algo-rithm in Statistical Translation.
In Proceedings ofthe ACL/EACL '97, Madrid, Spain, pages 366-372, July.D.
Wu.
1996.
A Polynomial-Time Algorithm forStatistical Machine Translation.
In Proceedingsof the 34th Annual Conference of the Associationfor Computational Linguistics, Santa Cruz, CA,pages 152 - 158, June.966ZusammenfassungWir stellen einen neuartigen..Suchalgorithmus flitdie statistische maschinelle Ubersetzung vor, derauf der dynamischen Programmierung (DP) beruht.W~ihrend es Suchprozesses werden.zwei statistischeWissensquellen kombiniert: Ein Ubersetzungsmo-dell und ein Bigramm-Sprachmodell.
Dieser Such-algorithmus erweitert Hypothesen entlang den Posi-tionen des Zielsatzes, wobei garantiert wird, dab alleWSrter im Quellsatz berficksichtigt werden.
Es wer-den experimentelle Ergebnisse auf der Verbmobil-Aufgabe angegeben.RdsumdNous prdsentons un nouveau algorithme de recherchepour la traduction automatiquestatistique i estbasde sur la programmation dynamique (DP).
Pen-dant la recherche deux sources d'information statis-tiques sont combindes: Un module de traductionet un bigram language model.
Cet alorithme derecherche construit des hypotheses le long des po-sitions de la phrase en langue de cible tout engarantissant la considdration progressive des motsdans la phrase en langue de source.
Des rdsultatsexpdrimentaux sur la t~che Verbmobil sont prdsen-tds.967
