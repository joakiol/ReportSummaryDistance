Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 534?544,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsDiscriminative Word Alignment with a Function Word Reordering ModelHendra SetiawanUMIACSUniversity of Marylandhendra@umiacs.umd.eduChris DyerLanguage Technologies InstituteCarnegie Mellon Universitycdyer@cs.cmu.eduPhilip ResnikLinguistics and UMIACSUniversity of Marylandresnik@umd.eduAbstractWe address the modeling, parameter estima-tion and search challenges that arise from theintroduction of reordering models that capturenon-local reordering in alignment modeling.In particular, we introduce several reorderingmodels that utilize (pairs of) function wordsas contexts for alignment reordering.
To ad-dress the parameter estimation challenge, wepropose to estimate these reordering modelsfrom a relatively small amount of manually-aligned corpora.
To address the search chal-lenge, we devise an iterative local search al-gorithm that stochastically explores reorder-ing possibilities.
By capturing non-local re-ordering phenomena, our proposed alignmentmodel bears a closer resemblance to state-of-the-art translation model.
Empirical re-sults show significant improvements in align-ment quality as well as in translation perfor-mance over baselines in a large-scale Chinese-English translation task.1 IntroductionIn many Statistical Machine Translation (SMT) sys-tems, alignment represents an important piece of in-formation, from which translation rules are learnt.However, while translation models have evolvedfrom word-based to syntax-based modeling, the defacto alignment model remains word-based (Brownet al, 1993; Vogel et al, 1996).
This gap be-tween alignment modeling and translation modelingis clearly undesirable as it often generates tensionsthat would prevent the extraction of many usefultranslation rules (DeNero and Klein, 2007).
Recentwork, e.g.
by Blunsom et al (2009) and Haghihi etal.
(2009) just to name a few, show that alignmentmodels that bear closer resemblance to state-of-the-art translation model consistently yields not only abetter alignment quality but also an improved trans-lation quality.In this paper, we follow this recent effort to nar-row the gap between alignment model and trans-lation model to improve translation quality.
Moreconcretely, we focus on the reordering componentsince we observe that the treatment of reordering re-mains significantly different when comparing align-ment versus translation: the reordering componentin state-of-the-art translation models has focusedon long-distance reordering, but its counterpart inalignment models has remained focused on localreordering, typically modeling distortion based en-tirely on positional information.
This leaves mostalignment decisions to association-based scores.Why is employing stronger reordering modelsmore challenging in alignment than in translation?One answer can be attributed to the fact that align-ment points are unobserved in parallel text, thus soare their reorderings.
As such, introducing strongerreordering often further exacerbates the computa-tional complexity to do inference over the model.Some recent alignment models appeal to externallinguistic knowledge, mostly by using monolingualsyntactic parses (Cherry and Lin, 2006; Pauls et al,2010), which at the same time, provides an approx-imation of the bilingual syntactic divergences thatdrive the reordering.
To our knowledge, however,this approach has been used mainly to constrain re-ordering possibilities, or to add to the generalizationability of association-based scores, not to directlymodel reordering in the context of alignment.534In this paper, we introduce a new approach to im-proving the modeling of reordering in alignment.
In-stead of relying on monolingual parses, we condi-tion our reordering model on the behavior of func-tion words and the phrases that surround them.Function words are the ?syntactic glue?
of sen-tences, and in fact many syntacticians believe thatfunctional categories, as opposed to substantive cat-egories like noun and verb, are primarily responsi-ble for cross-language syntactic variation (Ouhalla,1991).
Our reordering model can be seen as offeringa reasonable approximation to more fully elaboratedbilingual syntactic modeling, and this approxima-tion is also highly practical, as it demands no exter-nal knowledge (other than a list of function words)and avoids the practical issues associated with theuse of monolingual parses, e.g.
whether the mono-lingual parser is robust enough to produce reliableoutput for every sentence in training data.At a glance, our reordering model enumeratesthe function words on both source and target sides,modeling their reordering relative to their neighbor-ing phrases, their neighboring function words, andthe sentence boundaries.
Because the frequency offunction words is high, we find that by predicting thereordering of function words accurately, the reorder-ing of the remaining words improves in accuracy aswell.
In total, we introduce six sub-models involvingfunction words, and these serve as features in a loglinear model.
We train model weights discrimina-tively using Minimum Error Rate Training (MERT)(Och, 2003), optimizing F-measure.The parameters of our sub-models are estimatedfrom manually-aligned corpora, leading the reorder-ing model more directly toward reproducing humanalignments, rather than maximizing the likelihoodof unaligned training data.
This use of manual datafor parameter estimation is a reasonable choice be-cause these models depend on a small, fixed numberof lexical items that occur frequently in language,hence only small training corpora are required.
Inaddition, the availability of manually-aligned cor-pora has been growing steadily.The remainder of the paper proceeds as follows.In Section 2, we provide empirical motivation forour approach.
In Section 3, we discuss six sub-models based on function word relationships andhow their parameters are estimated; these are com-?????????????oneoffewthathavedipl.
rels.withcountriesAustraliais??
?North Koreathe11 2 3 4 5 6 7 8 9 10 1123456789101112Figure 1: An aligned Chinese-English sentence pair.bined with additional features in Section 4 to pro-duce a single discriminative alignment model.
Sec-tion 5 describes a simple decoding algorithm to findthe most probable alignment under the combinedmodel, Section 6 describes the training of our dis-criminative model and Section 7 presents experi-mental results for the model using this algorithm.We wrap up in Sections 8 and 9 with a discussionof related work and a summary of our conclusions.2 Empirical MotivationFig.
1 shows an example of a Chinese-English sen-tence pair together with correct alignment points.Predicting the alignment for this particular Chinese-English sentence pair is challenging, since the sig-nificantly different syntactic structures of these twolanguages lead to non-monotone reordering.
For ex-ample, an accurate alignment model should accountfor the fact that prepositional phrases in Chinese ap-pear in a different order than in English, as illus-trated by the movement of the phrase ???
?/withNorth Korea?
from the beginning of the Chinesenoun phrase to the end of the corresponding English.The central question that concerns us here is howto define and infer regularities that can be usefulto predict alignment reorderings.
The approach wetake here is supported by empirical results from apilot study, conducted as an inquiry into the idea offocusing on function words to model alignment re-ordering, which we briefly describe.We took a Chinese-English manually-aligned cor-pus of approximately 21 thousand sentence pairs,535?????????
?oneoffewthathavedipl.
rels.withcountriesAustraliais?
?North Koreathe11 2 3 4 5 6 7 8 9 10 1123456789101112Figure 2: The all-monotone phrase pairs, indicated asrectangular areas in bold, that can be extracted from theFig.
1 example.and divided each sentence pair into all-monotonephrase pairs.
Visually, an all-monotone phrase paircorresponds to a maximal block in the alignmentmatrix for which internal alignment points appearin monotone order from the top-left corner to thebottom-right corner.
Fig.
2 illustrates seven suchpairs that can be extracted from the example inFig.
1.
In total, there are 154,517 such phrase pairsin our manually-aligned corpus.The alignment configuration internal to all-monotone phrase pair blocks is, obviously, mono-tonic, which is a configuration that is effectivelymodeled by traditional alignments models.
On theother hand, the reordering between two adjacentblocks is the focus of our efforts since existing mod-els are less effective at modeling non-monotonicalignment configurations.
To measure the functionwords?
potential to predict non-monotone reorder-ings, we examined the border words where two ad-jacent blocks meet.
In particular, we are interestedin how many adjacent blocks whose border wordsare function words.The results of this pilot study were quite encour-aging.
If we consider only the Chinese side of thephrase pairs, 88.35% adjacent blocks have functionwords as their boundary words.
If we consider onlythe English side, function words appear at the bor-ders of 93.91% adjacent blocks.
If we considerboth the Chinese and English sides, the percentageincreases to 95.53%.
Notice that in Fig.
2, func-tion words appear at the borders of all adjacent all-monotone phrase pairs, if both Chinese and Englishsides are considered.
Clearly with such high cov-erage, function words are central in predicting non-monotone reordering in alignment.3 Reordering with Function WordsThe reordering models we describe follow our previ-ous work using function word models for translation(Setiawan et al, 2007; Setiawan et al, 2009).
Thecore hypothesis in this work is that function wordsprovide robust clues to the reordering patterns of thephrases surrounding them.
To make this insight use-ful for alignment, we develop features that score thealignment configuration of the neighboring phrasesof a function word (which functions as an anchor)using two kinds of information: 1) the relative order-ing of the phrases with respect to the function wordanchor; and 2) the span of the phrases.
This sec-tion provides a high level overview of our reorderingmodel, which attempts to leverage this information.To facilitate subsequent discussions, we introducethe notion of monolingual function word phraseFWi, which consists of the tuple (Yi, Li, Ri), whereYi is the i-th function word and Li,Ri are its left andright neighboring phrases, respectively.
Note thatthis notion of ?phrase?
is defined only for reorder-ing purposes in our model, and does not necessar-ily correspond to a linguistic phrase.
We definesuch phrases on both sides to cover as many non-monotone reorderings as possible, as suggested bythe pilot study.
To denote the side, we append a sub-script: FWi,S = (Yi,S , Li,S , Ri,S) refers to a func-tion word phrase on the source side, and FWi,T =(Yi,T , Li,T , Ri,T ) to one on the target side.
In oursubsequent discussion, we will mainly use FWi,S ,and we will omit subscripts S or T if they are clearfrom context.The primary objective of our reordering modelis to predict the projection of monolingual func-tion word phrases from one language to theother, inferring bilingual function word phrase pairsFWi,S?T = (Yi,S?T , Li,S?T , Ri,S?T ), which en-code the two aforementioned pieces of informa-tion.1 To infer these phrases, we take a probabilis-1The subscript S ?
T denotes the projection direction fromsource to target.
The subscript for the other direction is T ?
S.536tic approach.
For instance, to estimate the spans ofLi,S?T , Ri,S?T , our reordering model assumes thatany span to the left of Yi,S is a possible Li,S andany span to the right of Yi,S is a possible Ri,S , de-ciding which is most probable via features, ratherthan committing to particular spans (e.g.
as definedby a monolingual text chunker or parser).
We onlyenforce one criterion on Li,S?T and Ri,S?T : theyhave to be the maximal alignment blocks satisfyingthe consistent heuristic (Och and Ney, 2004) that endor start with Yi,S?T on the source S side respec-tively.2To infer these phrases, we decompose Li,S?Tinto (o(Li,S?T ), d(FWi?1,S?T ), b(?s?
)); sim-ilarly, Ri,S?T into (o(Ri,S?T ),d(FWi+1,S?T ),b(?/s?)
)).
Taking the decomposition of Li,S?T asa case in point, here o(Li,S?T ) describes the re-ordering of the left neighbor Li,S?T with respectto the function word Yi,S?T , while d(FWi?1,S?T )and b(?s?))
probe the span of Li,S?T , i.e.
whetherit goes beyond the preceding function word phrasepairs FWi?1,S?T and up to the beginning-of-sentence marker ?s?
respectively.
The same defini-tion applies to the decomposition of Ri,S?T , whereFWi+1,S?T is the succeeding function word phrasepair and ?/s?
is the end-of-sentence marker.3.1 Six (Sub-)ModelsTo model o(Li,S?T ), o(Ri,S?T ), i.e.
the re-ordering of the neighboring phrases of a func-tion word, we employ the orientation model in-troduced by Setiawan et al (2007).
Formally,this model takes the form of probability distributionPori(o(Li,S?T ), o(Ri,S?T )|Yi,S?T ), which condi-tions the reordering on the lexical identity of thefunction word alignment (but independent of the lex-ical identity of its neighboring phrases).
In particu-lar, o maps the reordering into one of the followingfour orientation values (borrowed from Nagata et al(2006)) with respect to the function word: Mono-tone Adjacent (MA), Monotone Gap (MG), ReverseAdjacent (RA) and Reverse Gap (RG).
The Mono-tone/Reverse distinction indicates whether the pro-jected order follows the original order, while theAdjacent/Gap distinction indicates whether the pro-2This heuristic is commonly used in learning phrase pairsfrom parallel text.
The maximality ensures the uniqueness of Land R.jections of the function word and the neighboringphrase are adjacent or separated by an interveningphrase.To model d(FWi?1,S?T ), d(FWi+1,S?T ), i.e.whether Li,S?T and Ri,S?T extend beyond theneighboring function word phrase pairs, we uti-lize the pairwise dominance model of Setiawanet al (2009).
Taking d(FWi?1,S?T ) asa case in point, this model takes the formPdom(d(FWi?1,S?T )|Yi?1,S?T , Yi,S?T ), where dtakes one of the following four dominance val-ues: leftFirst, rightFirst, dontCare, or neither.We will detail the exact formulation of these val-ues in the next subsection.
However, to provideintuition, the value of either leftFirst or neitherfor d(FWi?1,S?T ) would suggest that the span ofLi,S?T doesn?t extend to Yi?1,S?T ; the further dis-tinction between leftFirst and neither concerns withwhether the span of Ri?1,S?T extends to FWi,S?T .To model b(?s?
), b(?/s?
), i.e.
whether the span ofLi,S?T and Ri,S?T extends up to sentence mark-ers, we introduce the borderwise dominance model.Formally, this model is similar to the pairwise domi-nance model, except that we use the sentence bound-aries as the anchors instead of the neighboringphrase pairs.
This model captures longer distancedependencies compared to the previous two mod-els; in the Chinese-English case, in particular, it isuseful to discourage word alignments from crossingclause or sentence boundaries.
The sentence bound-ary issue is especially important in machine trans-lation (MT) experimentation, since the Chinese sideof English-Chinese parallel text often includes longsentences that are composed of several independentclauses joined together; in such cases, words fromone clause should be discouraged from aligning towords from other clauses.
In Fig.
1, this model ispotentially useful to discourage words from cross-ing the copula ?
?/is?.We define each model for all (pairs of) functionword phrase pairs, forming features over a set ofword alignments (A) between source (S) and target537(T ) sentence pair, as follows:fori =N?i=1Pori(o(Li), o(Ri)|Yi) (1)fdom =N?i=2Pdom(d(FWi?1)|Yi?1, Yi) (2)fbdom =N?i=1Pbdom(b (?s?
)|?s?, Yi) ?Pbdom(b (?/s?
)|Yi, ?/s?)
(3)where N is the number of function words (of thesource side, in the S ?
T case).
As the bilingualfunction word phrase pairs are uni-directional, weemploy these three models in both directions, i.e.T ?
S as well as S ?
T .
As a result, there aresix reordering models based on function words.3.2 Prediction and Parameter EstimationGiven FWi?1,S?T (and all other FW?i?/i,S?T ),our reordering model has to decompose Li,S?T into(o(Li,S?T ), d(FWi?1,S?T ), b(?s?
)); and Ri,S?Tinto (o(Ri,S?T ),d(FWi+1,S?T ), b(?/s?)
)) duringprediction and parameter estimation.
In predictionmode (described in Section 5), it has to make the de-composition on the current state of alignment, whileduring parameter estimation, it has to make thesame decomposition on the manually-aligned cor-pora.
Since the process is identical, we proceed withthe discussion in the context of parameter estima-tion, where the decomposition is performed to col-lect counts to estimate the parameters of our models.Orientation model.
Using Li,S?T as a case inpoint and given (Yi,S?T =sll/tmm, Li,S?T =sl2l1/tm2m1 ,Ri,S?T =sl4l3/tm4m3)3, the value of o(Li,S?T ) in termsof Monotone/Reverse is:Monotone/Reverse ={M, m2 < m,R, m < m1.
(4)while its value in terms of Adjacent/Gap values is:Adjacent/Gap ={A, |m ?
m1| ?
|m ?
m2| = 1,G, otherwise.
(5)3We use subscripts to indicate the starting index, and super-scripts the ending index.By adjusting the indices, the computation ofo(Ri,S?T ) follows similarly to the procedure above.Suppose we want to estimate the probability ofLi,S?T =MA for a particular Yi.
Note that here, weare interested in the lexical identity of Yi, thus theindex i is irrelevant.
We first gather the counts of theorientation value for all Li,S?T of Yi in the corpus:c(o(Li,S?T ) ?
{MA, RA, MG, RG}, Yi).
ThenPori(MA|Yi) is estimated as follows:Pori(MA|Yi) =c(MA, Yi)c(Yi)(6)where c(Yi) is the frequency of Yi in the corpus.
Theestimation of other orientation values as well as theT ?
S version of the model, follows the same pro-cedure.Pairwise and Borderwise dominance models.Given Ri,S?T = sl2l1/tm2m1 and Li+1,S?T =sl4l3/tm4m3 , i.e.
the spans of the neighbors of apair of neighboring function word phrase pairs(Yi = sl5l5/tm5m5 , Yi+1 = sl6l6/tm6m6), the value ofd(FWi+1,S?T ) is:=??????????
?leftFirst, l2 ?
l6?l3 > l5rightFirst, l2 < l6?l3 ?
l5dontCare, l2 ?
l6?l3 ?
l5neither, l2 < l6?l3 > l5(7)Note that the neighbors of the sentence markers forthe borderwise models span the whole sentence, thusvalue of neither is impossible for these models.Suppose we want to estimate the probability of Yiand Yi+1 having a dontCare dominance value.
Notethat here we are interested in the lexical identity ofYi and Yi+1, thus the models are insensitive to the in-dices.
We first gather the counts of the Yi and Yi+1having the dontCare value c(dontCare, Yi, Yi+1);then Pdom(dontCare|Yi, Yi+1) is estimated as fol-lows:Pdom(dontCare|Yi, Yi+1) =c(dontCare, Yi, Yi+1)c(Yi, Yi+1)(8)where c(Yi, Yi+1) is the count of Yi appears afterYi+1 in the training corpus without any other func-tion word comes in between.5384 Alignment ModelTo use the function word alignment features de-scribed in the previous section to predict alignments,we use a linear model of the following form:A?
= arg maxA?A(S,T )?
?
f(A, S, T ) (9)where A(S, T ) is the set of all possible alignmentsof a source sentence S and target sentence T , andf(A, S, T ) is a vector of feature functions on A, S,and T , and ?
is a parameter vector.In addition to the six reordering models, ourmodel employs several association-based scores thatlook at alignments in isolation.
These features in-clude:1.
Normalized log-likelihood ratio (LLR).
Thisfeature represents an association score, derived fromstatistical testing statistics.
LLR (Dunning, 1993)has been widely used especially to measure lexicalassociation.
Since the values of LLR are unnormal-ized, we normalize them on a per-sentence basis, sothat the normalized LLRs of, say, a particular sourceword to the target words in a particular sentence sumup to one.2.
Translation table from IBM model 4.
Thisfeature represents another association score, derivedfrom a generative model, in particular the word-based IBM model 4.
The use of this feature iswidespread in recent alignment models, since it pro-vides a relatively accurate initial prediction.3.
Translation table from manually-alignedcorpora.
This feature represents a gold-standard as-sociation score, based on human annotation.
Whileattractive, this feature suffers from data sparse-ness issues since the lexical coverage of manually-aligned corpora, especially over content words, isvery low.
To overcome this issue, we design thisfeature to have two levels of granularity; as such, afine-grained one is applied for function words andthe coarse-grained one for content words.4.
Grow-diag-final alignments bonus.
This fea-ture encourages our alignment model to reuse align-ment points that are part of the alignments createdby the grow-diag-final heuristic, which we used asthe baseline of our machine translation experiments.5.
Fertility model from IBM model 4.
This fea-ture, which is another by-product of IBM model 4,measures the probability of a certain word aligningto zero, one, or two or more words.6.
Null-alignment probability.
This bino-mial feature models preference towards not aligningwords, i.e.
aligning to the NULL token.
The intu-ition is to penalize NULL alignments depending onword class, by assigning lower probability mass tounaligned content words than to unaligned functionwords.
In our experiment, we assign feature value10?3 for a function word aligning to NULL, and10?5 for a content word aligning to NULL.Note that with the exception of the alignmentbonus feature (4), all features are uni-directional,and therefore we employ these features in both di-rections just as was done for the reordering models.5 SearchTo find A?
using the model in Eq.
9, it is neces-sary to search 2|S|?|T | different alignment config-urations, and, because of the non-local dependen-cies in some of our features, it is not possible to usedynamic programming to perform this search effi-ciently.
We therefore employ an approximate searchfor the best alignment.
We use a local search pro-cedure which starts from some alignment (in ourcase, a symmetrized Model 4 alignment) and makelocal changes to it.
Rather than taking a pure hill-climbing approach which greedily moves to locallybetter configurations (Brown et al, 1993), we usea stochastic search procedure which can move intolower-scoring states with some probability, similarto the Monte Carlo techniques used to draw sam-ples from analytically intractable probability distri-butions.5.1 AlgorithmTo find A?, our search algorithm starts with an initialalignment A(1) and iteratively draws a new set bymaking a few small changes to the current set.
Foreach step i = [1, n], with alignment A(i), a set ofneighboring alignments N (A(i)) is induced by ap-plying small transformations (discussed below) tothe current alignment.
The next alignment A(i+1)539is sampled from the following distribution:p(A(i+1)|S, T, A(i)) = exp?
?
f(A(i+1), S, T )Z(A(i), S, T )where Z(A(i), S, T ) =?A?
?N (A(i))exp?
?
f(A?, S, T )In addition to the current ?active?
alignment configu-ration A(i), the algorithm keeps track of the highestscoring alignment observed so far, Amax.
After nsteps, the algorithm returns Amax as its approxima-tion of A?.
In the experiments reported below, weinitialized A(1) with the Model 4 alignments sym-metrized by using the grow-diag-final-and heuristic(Koehn et al, 2003).5.2 Alignment NeighborhoodsWe now turn to a discussion of how the alignmentneighborhoods used by our stochastic search algo-rithm are generated.
We define three local transfor-mation operations that apply to single columns ofthe alignment grid (which represent all of the align-ments to the lth source word), rows, or existing align-ment points (l, m).
Our three neighborhood gener-ating operators are ALIGN, ALIGNEXCLUSIVE, andSWAP.
The ALIGN operator applies to the lth col-umn of A and can either add an alignment point(l, m?)
or move an existing one (including to null,thus deleting it).
ALIGNEXCLUSIVE adds an align-ment point (l, m) and deletes all other points fromrow m. Finally, the SWAP operator swaps (l, m) and(l?, m?
), resulting in new alignment points (l, m?
)and (l?, m).
We increase the decoder?s mobilityby traversing the target side and applying the samesteps above for each target word.
Fig.
3 illustratesthe three operators.
By iterating over all columns land rows m, the full alignment space A(S, T ) canbe explored.4To further reduce the search space, an alignmentpoint (l, m) is only admitted into a neighborhood ifit is found in the high-recall alignment set R(S, T ),which we define to be the model 4 union alignments(bidirectional model 4 symmetrized via union) plusthe 5 best alignments according to the log-likelihoodratio.4Using only the ALIGN operator, it is possible to explorethe full alignment space; however, using all three operators in-creases mobility.
(a)(b)(c)l l' l l'mm'mm'mm'Figure 3: Illustrations for (a) ALIGN, (b) ALIGNEXCLU-SIVE, and (c) SWAP operators, as applied to align the dot-ted, smaller circle (l,m) to (l,m?).
The left hand side rep-resents A(i), while the right hand side represents a can-didate for A(i+1).
The solid circles represent the newalignment points added to A(i+1).6 Discriminative TrainingTo set the model parameters ?, we used the min-imum error rate training (MERT) algorithm (Och,2003) to maximize the F-measure of the 1-bestalignment of the model on a development set con-sisting of sentence pairs with manually generatedalignments.
The candidate set used by MERT to ap-proximate the model is simply the set of alignments{A(1), A(2), .
.
.
, A(n)} encountered in the stochasticsearch.While MERT does not scale to large numbers offeatures, the scarcity of manually aligned trainingdata also means that models with large numbers ofsparse features would be difficult to learn discrimi-natively, so this limitation is somewhat inherent inthe problem space.
Additionally, MERT has sev-eral advantages that make it particularly useful forour task.
First, we can optimize F-measure of thealignments directly, which has been shown to corre-late with translation quality in a downstream system(Fraser and Marcu, 2007b).
Second, we are opti-mizing the quality of the 1-best alignments under themodel.
Since translation pipelines typically use onlya single word alignment, this criterion is appropri-ate.
Finally, and very importantly for us, MERT re-quires only an approximation of the model?s hypoth-esis space to carry out optimization.
Since we areusing a stochastic search, this is crucial, since sub-540sequent evaluations of the same sentence pair (evenwith the same weights) may result in a different can-didate set.Although MERT is a non-probabilistic optimizer,we explore the alignment space stochastically.
Thisis necessary to make sure that the weights we usecorrespond to a probability distribution that is notoverly peaked (which would result in a greedy hill-climbing search) or flat (which would explore themodel space without information from the model).We found that normalizing the weights by the Eu-clidean norm resulted in a distribution that was well-balanced between the two extremes.7 ExperimentsWe evaluated our proposed alignment model intrin-sically on an alignment task and extrinsically on alarge-scale translation task, focusing on Chinese-English as the language pair.
Our training dataconsists of manually aligned corpora available fromLDC (LDC2006E93 and LDC2008E57) and un-aligned corpora, which include FBIS, ISI, HKNewsand Xinhua.
In total, the manually aligned corporaconsist of more than 21 thousand sentence pairs,while the unaligned corpora consist of more than710 thousand sentence pairs.
The manually-alignedcorpora are primarily used for training the reorder-ing models and for discriminative training purposes.For translation experiments, we used cdec (Dyeret al, 2010), a fast implementation of hierarchi-cal phrase-based translation models (Chiang, 2005),which represents a state-of-the-art translation sys-tem.We constructed the list of function words in En-glish manually and in Chinese from (Howard, 2002).Punctuation marks were added to the list, result-ing in 883 and 359 tokens in the Chinese and En-glish lists, respectively.
For the alignment experi-ments, we took the first 500 sentence pairs from thenewswire genre of the manually-aligned corpora andused the first 250 sentences as the development set,with the remaining 250 as the test set.
To ensureblind experimentation, we excluded these sentencepairs from the training of the features, including thereordering models.7.1 Alignment QualityWe used GIZA++, the implementation of the de-facto standard IBM alignment model, as our base-line alignment model.
In particular, we usedGIZA++ to align the concatenation of the develop-ment set, the test set, and the unaligned corpora, with5, 5, 3 and 3 iterations of model 1, HMM, model3, and model 4 respectively.
Since the IBM modelis asymmetric, we followed the standard practice ofrunning GIZA++ twice, once in each direction, andcombining the resulting outputs heuristically.
Wechose to use the grow-diag-final-and heuristic as itworked well for hierarchical phrase-based transla-tion in our early experiments.
We recorded the align-ment quality of the test set as our baseline perfor-mance.For our alignment model, we used the same set oftraining data.
To align the test set, we first tunedthe weights of the features in our discriminativealignment model using minimum error rate training(MERT) (Och, 2003) with F?=0.1 as the optimiza-tion criterion.
At each iteration, our aligner outputsk-best alignments under current set of weights, fromwhich MERT proceeds to compute the next set ofweights.
MERT terminates once the improvementover the previous iteration is lower than a predefinedvalue.
Once tuned, we ran our aligner on the test setand measured the quality of the resulting alignmentas the performance of our model.Model P R F0.5 F0.1gdfa 70.97 63.83 67.21 64.48association 73.70 76.85 75.24 76.52+ori 74.09 78.29 76.13 77.85+dom 75.06 78.98 76.97 78.57+bdom 75.41 80.53 77.89 79.99Table 1: Alignment quality results (F0.1) for our discrim-inative reordering models with various features (lines 2-5) versus the baseline IBM word-based Model 4 sym-metrized using the grow-diag-final-and heuristic.
Thebalanced F0.5 measure is reported for reference.
The bestscores are bolded.Table 1 reports the results of our experiments,which are conducted in an incremental fashion pri-marily to highlight the role of reordering model-ing.
The first line (gdfa) reports the baseline perfor-541mance.
In the first experiment (association), we em-ployed only the association-based features describedin Section 4.
As shown, we obtain a significant im-provement over baseline.
This result is consistentwith recent literature (Fraser and Marcu, 2007a) thatshows that a discriminatively trained model outper-forms baseline unsupervised models like GIZA++.In the second set of experiments, we added the re-ordering models into our discriminative model oneby one, starting with the orientation models, thenthe pairwise dominance model and finally the bor-derwise dominance model, reported in lines +ori,+dom and +bdom respectively.
As shown, each ad-ditional reordering model provides a significant ad-ditional improvement.
The best result is obtained byemploying all reordering models.
These results em-pirically confirm our hypothesis that we can improvealignment quality by employing reordering modelsthat capture non-local reordering phenomena.7.2 Translation QualityFor translation experiments, we used the productsfrom our intrinsic experiments to learn translationrules for the hierarchical phrase-based decoder, i.e.the features weights of the +bdom experiment toalign the MT training data using our discriminativemodel.
For our translation model, we used the stan-dard features based on the relative frequency counts,including a 5-gram language model feature trainedon the English portion of the whole training dataplus portions of the Gigaword v2 corpus.
Specif-ically, we tuned the weights of these features viaMERT on the NIST MT06 set and we report the re-sult on the NIST MT02, MT03, MT04 and MT05sets.MT02 MT03 MT04 MT05gdfa 25.61 32.05 31.80 29.34this work 26.56 33.79 32.61 30.47Table 2: The translation performance (BLEU) of hierar-chical phrase-based translation trained on training dataaligned by IBM model 4 symmetrized with the grow-diag-final-and heuristic, versus being trained on align-ments by our discriminative alignment model.
Boldedscores indicate that the improvement is statistically sig-nificant.Table 2 shows the result of our translation exper-iments.
In our alignment model, we employed thewhole set of reordering models, i.e.
the one reportedin the +bdom line in Table 1.
As shown, our dis-criminative alignment model produces a consistentand significant improvement over the baseline IBMmodel 4 (p < 0.01), ranging between 0.81 and 1.71BLEU points.8 Related WorkThe focus of our work is to strengthen the reorderingcomponent of alignment modeling.
Although the defacto standard, the IBM models do not generalizewell in practice: the IBM approach employs a seriesof reordering models based on the word?s position,but reordering depends on syntactic context ratherthan absolute position in the sentence.
Over theyears, there have been many proposals to improvethese reordering models, most notably Vogel et al(1996), which adds a first-order dependency.
Never-theless, the use of these distortion-based models re-mains widespread (Marcu and Wong, 2002; Moore,2004).Alignment modeling is challenging because itoften has to consider a prohibitively large align-ment space.
Efforts to constrain the space gen-erally comes from the use of Inversion Transduc-tion Grammar (ITG) (Wu, 1997).
Recent propos-als that use ITG constraints include (Haghighi etal., 2009; Blunsom et al, 2009) just to name a few.More recent models have begun to use linguistically-motivated constraints, often in combination withITG, primarily exploiting monolingual syntactic in-formation (Burkett et al, 2010; Pauls et al, 2010).Our reordering model is closely related to themodel proposed by Zhang and Gildea (2005; 2006;2007a), with respect to conditioning the reorderingpredictions on lexical items.
These related modelstreat their lexical items as latent variables to be es-timated from training data, while our model usesa fixed set of lexical items that correspond to theclass of function words.
With respect to the focuson function words, our reordering model is closelyrelated to the UALIGN system (Hermjakob, 2009).However, UALIGN uses deep syntactic analysis andhand-crafted heuristics in its model.5429 ConclusionsLanguages exhibit regularities of word order thatare preserved when projected to another language.We use the notion of function words to infer suchregularities, resulting in several reordering modelsthat are employed as features in a discriminativealignment model.
In particular, our models pre-dict the reordering of function words by lookingat their dependencies with respect to their neigh-boring phrases, their neighboring function words,and the sentence boundaries.
By capturing suchlong-distance dependencies, our proposed align-ment model contributes to the effort to unify align-ment and translation.
Our experiments demonstratethat our alignment approach achieves both its intrin-sic and extrinsic goals.AcknowledgementsThis research was supported in part by the GALEprogram of the Defense Advanced Research ProjectsAgency, Contract No.
HR0011-06-2-001.
Anyopinions, findings, conclusions or recommendationsexpressed in this paper are those of the authors anddo not necessarily reflect the view of the sponsors.ReferencesPhil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A gibbs sampler for phrasal synchronousgrammar induction.
In ACL, pages 782?790, Sun-tec, Singapore, August.
Association for ComputationalLinguistics.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311,June.David Burkett, John Blitzer, and Dan Klein.
2010.Joint parsing and alignment with weakly synchronizedgrammars.
In HLT-NAACL, pages 127?135, Los An-geles, California, June.
Association for ComputationalLinguistics.Colin Cherry and Dekang Lin.
2006.
Soft syntacticconstraints for word alignment through discriminativetraining.
In COLING/ACL, pages 105?112, Sydney,Australia, July.
Association for Computational Lin-guistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In ACL, pages263?270, Ann Arbor, Michigan, June.
Association forComputational Linguistics.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In ACL,pages 17?24, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1):61?74, March.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In ACL, Up-psala, Sweden.Alexander Fraser and Daniel Marcu.
2007a.
Gettingthe structure right for word alignment: LEAF.
InEMNLP-CoNLL, pages 51?60, Prague, Czech Repub-lic, June.
Association for Computational Linguistics.Alexander Fraser and Daniel Marcu.
2007b.
Measuringword alignment quality for statistical machine transla-tion.
Computational Linguistics, 33(3):293?303.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with superviseditg models.
In ACL, pages 923?931, Suntec, Singa-pore, August.
Association for Computational Linguis-tics.Ulf Hermjakob.
2009.
Improved word alignment withstatistics and linguistic heuristics.
In EMNLP, pages229?237, Singapore, August.
Association for Compu-tational Linguistics.Jiaying Howard.
2002.
A Student Handbook for ChineseFunction Words.
The Chinese University Press.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In HTL-NAACL,pages 127?133, Edmonton, Alberta, Canada, May.
As-sociation for Computational Linguistics.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In EMNLP, July 23.Robert C. Moore.
2004.
Improving ibm word alignmentmodel 1.
In ACL, pages 518?525, Barcelona, Spain,July.Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,and Kazuteru Ohashi.
2006.
A clustered global phrasereordering model for statistical machine translation.
InACL, pages 713?720, Sydney, Australia, July.
Associ-ation for Computational Linguistics.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL, pages 160?167.Jamal Ouhalla.
1991.
Functional Categories and Para-metric Variation.
Routledge.543Adam Pauls, Dan Klein, David Chiang, and KevinKnight.
2010.
Unsupervised syntactic alignment withinversion transduction grammars.
In HLT-NAACL,pages 118?126, Los Angeles, California, June.
Asso-ciation for Computational Linguistics.Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007.Ordering phrases with function words.
In ACL, pages712?719, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Hendra Setiawan, Min Yen Kan, Haizhou Li, and PhilipResnik.
2009.
Topological ordering of functionwords in hierarchical phrase-based translation.
InACL, pages 324?332, Suntec, Singapore, August.
As-sociation for Computational Linguistics.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In COLING, pages 836?841, Copenhagen.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404, Sep.Hao Zhang and Daniel Gildea.
2005.
Stochastic lexical-ized inversion transduction grammar for alignment.
InACL.
The Association for Computer Linguistics.Hao Zhang and Daniel Gildea.
2006.
Inducing wordalignments with bilexical synchronous trees.
In ACL.The Association for Computer Linguistics.544
