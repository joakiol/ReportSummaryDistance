First Joint Conference on Lexical and Computational Semantics (*SEM), pages 536?542,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSbdlrhmn: A Rule-based Human Interpretation System forSemantic Textual Similarity TaskSamir AbdelRahmansbdlrhmn@illinois.edu,s.abdelrahman@fci-cu.edu.egCatherine Blakeclblake@illinois.eduThe Graduate School of Library and Information ScienceUniversity Of Illinois at Urbana-ChampaignAbstractIn this paper, we describe the system architec-ture used in the Semantic Textual Similarity(STS) task 6 pilot challenge.
The goal of thischallenge is to accurately identify five levelsof semantic similarity between two sentences:equivalent, mostly equivalent, roughly equiva-lent, not equivalent but sharing the same topicand no equivalence.
Our participations weretwo systems.
The first system (rule-based)combines both semantic and syntax features toarrive at the overall similarity.
The proposedrules enable the system to adequately handledomain knowledge gaps that are inherentwhen working with knowledge resources.
Assuch one of its main goals, the system sug-gests a set of domain-free rules to help thehuman annotator in scoring semantic equiva-lence of two sentences.
The second system isour baseline in which we use the Cosine Simi-larity between the words in each sentencepair.1 IntroductionAccurately establishing sentence semantic similari-ty would provide one of the key ingredients forsolutions to many text-related applications, such asautomatic grading systems (Mohler and Mihalcea,2009), paraphrasing (Fernando and Stevenson,2008), text entailment (Corley et al, 2005) andsummarization (Erkan and Radev, 2004).
Currentapproaches for computing semantic similarity be-tween a pair of sentences focus on analyzing theirshared words (Salton, 1989), structures (Hu et al2011;Mandreoli et al 2002), semantics (Mihalceaet al 2006; Le el al.
2006; Hatzivassiloglou, 1999)or any of their combinations (Liu et al 2008; Foltzet al 1998).
The goal is to arrive at a score whichincreases proportionally with the relatedness be-tween the two sentences.
Yet, they are not con-cerned with scoring the interpretations of suchrelatedness (Zhang et al 2011; Jesus et al 2011;Wenyin et al 2010; Liu et al 2008).Semantic Textual Similarity (STS), SEMEVAL-12 Task 6 (Agirre et al 2012), measures the degreeof semantic equivalence between a pair of sentenc-es by comparing meaningful contents within a sen-tence.
The assigned scores range from 0 to 5 foreach sentence pair with the following interpreta-tions: (5) completely equivalent, (4) mostlyequivalent pair with missing unimportant infor-mation, (3) roughly equivalent with missing im-portant information, (2) not equivalent, but sharingsome details, (1) not equivalent but sharing thesame topic and (0) not equivalent and on differenttopics.
The goal of developing our rule-based sys-tem was to identify knowledge representationswhich have possibly all task human interpretations.Meanwhile, the system domain-free rules aim tohelp the human annotator in scoring semanticequivalence of sentence pair.The proposed rule-based solution exploits bothsentence syntax and semantics.
First, it uses Stan-ford parser (Klein and Manning, 2002) to exposethe sentence structure, part-of-speech (POS) wordtags, parse tree and Subject-Verb-Object (S-V-O)dependencies.
Second, Illinois Coreference Pack-age (Bengtson and Roth, 2008) is used to extractsentence named entities resolving possible men-536tions.
Third, WordNet (Miller, 1995) and AdaptedLesk Algorithm for word sense disambiguation(Banerjee and Pedersen, 2010) are used to computeeach sentence word semantic relatedness to theother sentence.
ReVerb (Etzioni et al 2011) aug-ments WordNet in case of uncovered words andhelps us to discriminate the topics of sentences.We use (Blake, 2007) thought to compare the sen-tence pair words with each other.
Finally, weevolve a rule-based module to present the humanheuristics when he interprets the relatedness of thesentence pair meaningful contents.Throughout our training and testing experi-ments, we used Task6 corpora (Agirre et al 2012)namely MSRpar, MSRvid, SMTeuroparl, OnWNand SMTnews; where:- MSRpar is 1500 pairs of sentences of MSR-Paraphrase, Microsoft Research Paraphrase Cor-pus; 750 for training and 750 for testing.- MSRvid is 1500 pairs of sentences of MSR-Video, Microsoft Research Video DescriptionCorpus; 750 for training and 750 for testing.- SMTeuroparl is 918 pairs of sentences ofWMT2008 development dataset (Europarl sec-tion); 459 for training and 459 for testing.-  OnWn is 750 pairs of sentences pairs of sen-tences where the first sentence comes from On-tonotes and the second sentence from a WordNetdefinition; it is only a testing corpus.-  SMTnews is 399 pairs of sentences of newsconversation sentence pairs from WMT; it is on-ly a testing corpus.The reminder of this paper is organized as fol-lows: Section 2 describes our two participations;Section 3 discusses their official results; Section 4draws our conclusion for both systems.2 The Proposed SystemsIn this section, we focus on the rule-based system,Sections 2.1, 2.2, 2.3 and 2.4, as our main taskcontribution.
Further, the section describes our se-cond run, Sections 2.5, to shed light on the role ofcosine similarity for solving the task problem.
Toestablish the task semantic textual similarity, weshow how the rule-based system exploits the sen-tence semantic, syntax and heuristics; also, we de-scribe how our base-line system uses the sentencesyntax only.2.1 DefinitionsWe say the two sentences are on different topics, ifall their verbs are mostly (> 50%) unrelated (Table1).
Otherwise, they are on the same topic.
For ex-ample, the two sentences ?A woman is putting onmakeup.
?, ?A band is singing.?
are on differenttopics as ?putting?, ?singing?
are not equivalent.However, the two sentences ?A baby is talking.
?,A boy is trying to say firetruck.?
are on the sametopics  as ?talking?
and ?trying to say?
are seman-tically equivalent.We define the sentence important information asits head nouns, named entities or main verbs;where the main verbs are all verbs except auxilia-ry, model and infinitive ones.
Hence, we say thattwo sentences miss important information if eitherloses at least one of these mentions from the other.Otherwise, they are candidates to be semanticallyequivalent.
For example, the sentence ?BesidesHampton and Newport News, the grant funds wa-ter testing in Yorktown, King George County, Nor-folk and Virginia Beach.?
misses ?Hampton andNewport News?
compared to the sentence ?Thegrant also funds beach testing in King GeorgeCounty, Norfolk and Virginia Beach.?
However,?on a table?
is unimportant information which ?Awoman is tapping her fingers.?
misses comparedto ?A woman is tapping her fingers on a table.
?Finally, we deploy a list of stop words and non-verbs as unimportant information.
However, if anyexists in both sentences, we match them with eachother; otherwise we ignore any occurrences.2.2  The Syntactic ModuleThis syntactic module is a preprocessing module inwhich the system calls Stanford parser, Version2.0.1, and the Illinois coreference package, Version1.3.2, to result in the sentence four type representa-tions: 1) part of speech (POS) tags, 2) Subject-Verb-Object (S-V-O), Subject-Verb (S-V) andVerb-Object (V-O) dependencies, 3) parse tree and4) coreference resolutions.
All sentences are lem-matized based on their POSs.
Also, verbs and CDsare utilized to determine topics/important infor-mation and numbers respectively.
All noun  andverb phrases are used to boost the sentence wordsemantic scores (Section 2.3).
We consider all oc-currences of S-V-O, S-V and V-O to distinguish537the topic compatibility between two comparablesentences (Section 2.3 and 2.4).The coreference package is used to match theequivalent discourse entities between two sentenc-es which improve the matching steps.
For example,in the pair of   ?Mrs Hillary Clinton explains herplan towards the Middle East countries?
and ?MrsClinton meets their ambassadors?, ?Mrs HillaryClinton?, ?her?
and ?Mrs Clinton?
refer to thesame entity where ?the Middle East countries?
and?their?
are equivalent.
Moreover, we consider thesecond sentence doesn?t lose ?Hillary?
as missingimportant information since the related mentionsare labeled equivalent.2.3 The Semantic Matching ModuleWordNet, Version 3.0, has approximately 5,947entries covering around 85% of training corporawords (Agirre et al 2012).
Most of the remaining15% words are abbreviations, named entities andincorrect POS tags.
We use WordNet shortest pathmeasure to compute the semantic similarity be-tween two words.
Also, we use Adapted Lesk algo-rithm to obtain the best WordNet word sense.
Thedisambiguation algorithm compares each pair ofwords through their contexts (windows) of wordscoupled with their all overlapping glosses of allWordNet relation types.The semantic matching module inputs are thesentence pair (S1, S2), their lemmatized words,parse trees, S-V-O/S-V/V-O dependencies and co-reference mentions (Section 2.2).
It matches syn-tactically the words with each other.
For anyuncovered WordNet word, the module calls Re-Verb (Section 2.4) and it assigns the returned valueto the word score.
All numbers, e.g.
million,300,45.6, are mathematically compared with eachother.
This module compares the noun phraseswith single words to handle the compound words,e.g.
?shot gun?
with ?shotgun?
or ?part-of-speech?
with ?part of speech?.
For those wordswhose scores are not equal to 1, it compares eachpair of words from the sentence pair within theirSubject-VP (subject with its verb phrase) contextsusing Adapted Lesk algorithm to find best sensefor each included word.
Then, it applies WordNetshortest path measure to score such words.
In ourdisambiguation algorithm implementation, wefound that the runtime requirement is directly pro-portional to the input sentence length.
So, weshortened the sentence length to Subject-VP whichincludes the underlying comparable words.Relatedness Score (S1, S2)unrelated  0 <= Ws <0.3weakly related 0.3 <= Ws <0.85strongly related Ws >= 0.85Table 1 ?
Mapping relatedness to wordnet similarityTable 1 describes the proposed system WordNetthresholds through our relatedness definitions.
Thethresholds were thoroughly selected depending onour analysis for the WordNet hierarchary andsemantic similarity measures (Pedersen et al,2004).
We obsereved that while most of the nearesttree sibilings and parent-child nodes scores havemore than 0.85 Wordnet semantic scores, most ofthe fartherest ones have scores less than 0.3.
Inbetween these extremes, there is a group ofscattered tree nodes which ranges from 0.3 to 0.85.The number of nodes per each mentioned group isrelated to the semantic simlarity measuretechnique.2.4 Semantics ?
Using ReVerbOur working hypothesis is that verbs that use thesame arguments are more likely to be similar.
Toestimate verb usage, the system uses frequenciesfrom the ReVerb (http://openie.cs.washington.edu/) online interface to count the number of timesa verb is used with two arguments.
For example,consider the sentence pair ?The man fires rifle?and ?The man cuts lemon?.
The number of sen-tences in ReVerb that contain the verb fires withthe argument rifle is 538 and the number of sen-tences for the verb cuts with the argument lemon is45, which tell us that you are more likely to findsentences that describe firing a rifle than cutting alemon on the web.
However, there a no ReVerbsentences for the verb fires with the argument lem-on or the verb cuts with the argument rifle.
Whichtells us that people generally don?t fire lemons orcut rifles.Reverb provides the system with informationabout the suitability of using argument in one sen-tence with verbs from another.
Specifically, fre-quencies from Reverb are retrieved for eachsubject-verb-object triple in each sentence, e.g.?S1-V1-O1?
and ?S2-V2-O2?.
The system thenretrieves ReVerb frequencies for the verb-object in538each sentence of ?V2-O1?
and ?V1-O2?.
If at leastone of all of these scores equals to 0, they are con-sidered to be weakly similar.ReVerb is also called for any sentence word thatWordNet doesn?t cover.
The system retrieves theReverb frequency for is-a relation using the wordmissing from Wordnet, as Argument1, and eachword from the other sentence as Argument2.
Thelargest Reverb retrieved score is taken.
Considerthe pair of ?A group of girls are exiting a taxi?
and?A video clip of Rihanna leaving a taxi.?.
Since?Rihanna?
is not a WordNet word, our ReVerbinterface hits the web for ?Rihanna is-a girl?, ?Ri-hanna is-a group?, ?Rihanna is-a taxi?
and ?Ri-hanna is-a existing?
and it returns ?Rihanna is-agirl?
as the best candidate with strength scoreequals 0.2.We explored several relatedness scores whichspecifically equal to 0, 0.2, 0.4, 0.6, 0.8 or 1 if thefrequencies are less than to 10, 50, 100, 500, 1000or 1000+ respectively.2.5 The Rule-Based ModuleRule-based module aims at defining human-likerules to interpret how the pair similar or dissimilarfrom each other.
Pair Similarity (P) is based on thestrong relatedness values (Table 1) and the Dissim-ilarity (D) is based on the other types of related-ness values.
As we believe that strong and notstrong are proportional to the pair similarity anddissimilarity respectivelyRule-based module input is sentence pair S1, S2word semantic scores, i.e.
Ws1s and Ws2s (Table1).
Then, it calculates: 1) their three types of aver-ages for S1 and S2 semantic scores, i.e.
all wordsemantic scores, weakly related only and unrelatedvalues; 2) P as the minimum percentage of strongWss in (S1 and S1); 3) D as, 100-P, the percentageof not strong Wss in S1 and S1This module outputs the semantic textual simi-larity semantic (STS) score which ranges from 0 to5.
Throughout this section, when we use ?unrelat-ed?, ?weak?
and strong terminologies, we use Ta-ble 1 Relatedness definitions.
Also, when we use?important?
term, we refer to our definition (Sec-tion 2.1)Human judgments for computing STS score ofthe sentence pair are based on word similaritiesand dissimilarities.
They consider that two sen-tences are similar if most (> 50%) of their wordsare strongly related, otherwise the sentences arecandidates to be dissimilar.
Since all Wss rangefrom 0 to 1, the average of strong scores is morethan the average of weak scores.
Likewise, the av-erage of weak scores is more than the average ofnon-related scores.Score(Sentence Ws1s, Sentence Ws2s)AllAvg = (Ws1s+ Ws1s)/2WeakAvg= the averaged weakly related scores ofWs1s and Ws1sUnRElAvg=the average of unrelated scores ofWs1s and Ws1sP = minimum (% Ws1s strong scores, % Ws2sstrong scores)D=100-PValue=0If 95 <= P <=100 then  Value = 5;If 80 <= P < 95 then Value = 4;If 50 <= P < 80  then Value = 3;If 20 <= P < 50  then Value = 2;If 0 <= P < 20 thenIf all verbs are strongly related then Value=1Else Value= 0.0001;If (Value in [4, 5]) thenIf all Ds for important words then Value=   3If (Value ==3) thenIf all Ds for not important words then Value= 2If (Value <> 5 AND Value <> 0) thenIf all Ds for weakly related wordsValue= Value+ AllAvgElse if at least half Ds for weakly related wordsValue= Value+ WeakAvgOtherwiseValue = Value + UnRelAvgReturn ValueWhen we call Score(Ws1s,Ws2s), we take careof the following two special cases where it goesdirectly to Value 3: 1) if missing some words leadsto missing the whole verb/noun phrases and 2) ifone sentence has all past tense verbs and the otherhas present verbs.When we design P inequalities, we make themhave relaxed boundaries conformed with humangrading values.
For example, we choose P between95 and 100 in Value (5); where 95 and 100 equalto grades 4.5 and 5 respectively.
Value (3) intervalare values between more than or equal 2.5 and lessthan 4.
Then, we utilize the important information539and verb constraints to direct classificationsthrough different groups.When we design range conditions between val-ues, we select D to present the distance betweenthe sentence pair.
As D weak values increase, thetwo sentences become closer.
As D unrelated val-ues increase, the two sentences become distant.We carefully analyzed the training corpora toassure that the above thresholds satisfy most of thetraining sentence pairs.
Each threshold output wasmanually checked and adjusted to satisfy around55% to 75% of the training corpora.Applying the above module, the pair of ?A manis playing football?
and ?The man plays football?STS score equals 5.00.
The pair of ?A man is sing-ing and playing?
and ?The man plays?
STS scoreequals 3.00 since the first one misses ?singing?.The pair of ?The cat is drinking milk.?
and ?Awhite cat is licking and drinking milk kept on aplate.?
STS scores equals to 3.4 since they haveP=0.66, ?white?
as unimportant information but?licking?, ?
kept?, ?plate?
as important infor-mation words.2.6 Our Baseline System DescriptionOur goal in the second run is to evaluate the relat-edness of the two sentences using only the wordsin the sentence.
Sentences are represented as a vec-tor (i.e.
based on the Vector Space Model) and thesimilarity between the two sentences S1 and S2 is(5* cosine similarity).
We take into account allsentence words such that they are lower-case andnon-stemmed.3 Results and Discussion3.1 Rule-based System AnalysisOur system was implemented in Python and usedthe Natural Language Toolkit (NLTK,www.nltk.org/), WordNet and lemmatization mod-ules.
Table 2 provides in the official results of oursystem Pearson-Correlation measure.D Para Vid Europ OnWn NewsTr 0.6011 0.7021 0.4528Te 0.5440 0.7335 0.3830 0.5860 0.2445Table 2.
Run1 Official Person-Correlation measureIn Table 2, the first row shows the proposed sys-tem results namely 0.6011, 0.7021 and 0.4528 forMSRpar, MSRvid and SMTeuropel training corpo-ra respectively.
The second row shows the test re-sults, namely 0.5440, 0.7335 and 0.3830, 0.5860and 0.2445 for MSRpar, MSRvid and SMTeuro-pel, On-Wn and SMTnews testing corpora respec-tively.In the Task-6 results (Agirre et al 2012), oursystem was ranked 21th out of 85 participants with0.6663 Pearson-Correlation ALL competition rank.We tested two WordNet measures, namely theshortest path and WUP, the path length to the rootnode from the least common subsumer (LCS) ofthe two concepts, measures on the training corpora.In contrast to the shortest path measure, WUPmeasure increased the P versus the D scores on thethree corpora.
This overestimated many trainingSTS scores and negatively affected the correlationwith the gold standard corpora.
Using WUP meas-ure, the correlations of MSRpar, MSRvid andSMTeuropel corpora were 0.5553, 0.3488 and0.4819 respectively.
We decided to use WordNetshortest path measure due to its better correlationresults.
When we used WUP measure on testingcorpora, the correlations were 0.5103, 0.4617,0.4810, 0.6422 and 0.4400 for MSRpar, MSRvidand SMTeuropel, On-Wn and SMTnews testingcorpora respectively.
We observed that when weused WUP measure on MSRvid corpora, the corre-lations were degraded.
This is because most ofMSRvid corpus pair sentences talking about hu-man genders which have high WUP scores whencomparing with each other.
Unfortunately, Word-Net shortest path measure underestimated SMT-news pair sentence similarities which affecteddramatically the related correlation measure.Hence, the choice of the suitable WordNet metricfor the whole corpora is still under our considera-tion.Thresholds and Semantic Pattern: Our currentefforts are directed towards statistical modeling ofthe system thresholds.
We intend also to use someweb semantic patterns or phrases, such as ReVerbpatterns, to boost the semantic scores of singlewords.3.2 Baseline System AnalysisIn Table 3, the first row shows the proposed sys-tem results namely 0.4688, 0.4175 and 0.5349 for540MSRpar, MSRvid and SMTeuropel training corpo-ra respectively.
The second row shows the pro-posed system results, namely 0.4617, 0.4489 and0.4719, 0.6353 and 0.4353 for MSRpar, MSRvidand SMTeuropel, On-Wn and SMTnews testingcorpora respectively.D Para Vid Europ OnWn NewsTr 0.4688 0.4175 0.5349Te 0.4617 0.4489 0.4719 0.6353 0.4353Table 3.
Run 2 Official Person-Correlation measureIn the Task-6 results (Agirre et al 2012), Run2was ranked 72th out of 85 participants with 0.4169Pearson-Correlation ALL competition rank.
Asanticipated, Run2 released fair results.
Its perfor-mance is penalized or awarded proportionally tothe number of exact matching pair words.
Accord-ingly, it may record considerable scores for pairswhich have highly percentage exact matchingwords.
For example, it provides competitive corre-lation scores compared to other participants on On-Wn and SMTnews testing corpora.
Though, thisdoesn?t imply that it is an ideal solution for STStask.
It usually indicates that many corpus pairsmay have some substantial exact matching words.4 ConclusionsIn this paper, we presented systems developed forSEMEVAL12- Task6.
The first run used both se-mantics and syntax.
The second run, our baseline,uses only the words in the initial two sentences anddefines similarity as the cosine similarity betweenthe two sentences.
The official task results suggestthat semantics and syntax (Run1) supersedes thewords alone (Run 2) with 0.2494 which indicatesthat the words alone are not sufficient to capturesemantic similarity.AcknowledgmentThis material is based upon work supported by theNational Science Foundation under Grant No.(1115774).
Any opinions, findings, and conclu-sions or recommendations expressed in this mate-rial are those of the author(s) and do notnecessarily reflect the views of the National Sci-ence Foundation.ReferencesCatherine Blake.
2007.
The Role of Sentence Structurein Recognizing Textual Entailment.
RTE Proceedingsof the ACL-PASCAL Workshop on Textual Entail-ment and Paraphrasing:101-106.Courtney Corley and Andras Csomai and Rada Mihal-cea.
2005.
Text Semantic Similarity, with Applica-tions.
Proceedings of the Conference on RecentAdvances in Natural Language Processing (RANLP),Borovetz, Bulgaria.Dan Klein and Christopher D. Manning.
2002.
FastExact Inference with a Factored Model for NaturalLanguage Parsing.
In Advances in Neural Infor-mation Processing Systems 15 (NIPS), Cambridge,MA: MIT Press:3-10.Dong-bin Hu and  Jun Ding.
2011.
Study on SimilarEngineering Decision Problem Identification BasedOn Combination of Improved Edit-Distance andSkeletal Dependency Tree with POS.
Systems Engi-neering Procedia 1: 406?413.Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-lez-Agirre.
2012.
SemEval-2012 Task 6: A Pilot onSemantic Textual Similarity.
In Proceedings of the6th International Workshop on Semantic Evaluation(SemEval 2012), in conjunction with the First JointConference on Lexical and Computational Semantics(*SEM 2012)Eric Bengtson and Dan Roth.
2008.
Understanding theValue of Features for Coreference Resolution.EMNLP:294-303.Federica Mandreoli and Riccardo Martoglia and PaoloTiberio.
2002 .
A Syntactic Approach for SearchingSimilarities within Sentences.
Proceeding of Interna-tional Conference on Information and KnowledgeManagement:656?637.George A. Miller.
1995.
WordNet: A Lexical Databasefor English.
Communications of the ACM, 38(11): 39-41.Gerard Salton.
1989.
Automatic Text Processing.
TheTransformation, Analysis, and Retrieval of Infor-mation by Computer.
Wokingham, Mass.Addison-Wesley.Gunes Erkan and Dragomir R. Radev.
2004.
LexRank:Graph-based Lexical Centrality as Salience in TextSummarization.
Journal of Artificial Intelligence Re-search 22:457-479.Junsheng Zhang, Yunchuan Sun, Huilin Wang, YanqingHe.
2011.
Calculating Statistical Similarity betweenSentences.
Journal of Convergence InformationTechnology, Volume 6, Number 2: 22-34.Liu Wenyin and Xiaojun Quan and Min Feng and BiteQiu.
2010.
A Short Text Modeling Method Combin-ing Semantic and Statistical Information.
InformationSciences 180: 4031?4041.541Michael Mohler and Rada Mihalcea.
2009.
Text-to-textSemantic Similarity for Automatic Short AnswerGrading.
Proceedings of the European Chapter of theAssociation for Computational Linguistics (EACL).Oliva Jesus and Serrano I. Jose and Mar?a D. del Cas-tillo and ?ngel Iglesias .2011.
SyMSS: A Syntax-based Measure for Short-Text Semantic Similarity.Data and Knowledge Engineering 70: 390?405.Oren Etzioni, Anthony Fader, Janara Christensen, Ste-phen Soderland, and Mausam.
2011.
Open Infor-mation Extraction: The Second Generation.Proceedings of the 22nd International Joint Confer-ence on Artificial Intelligence (IJCAI).Rada Mihalcea and Courtney Corley and Carlo Strap-parava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
Proceeding ofthe Twenty-First National Conference on ArtificialIntelligence and the Eighteenth Innovative Applica-tions of Artificial Intelligence Conference.Peter W. Foltz and Walter Kintsch and  Thomas K Lan-dauer.
1998.
The measurement of textual coherencewith latent semantic analysis.
Discourse ProcessesVol.
25, No.
2-3: 285-307.Samuel Fernando and Mark Stevenson.
2008.
A Seman-tic Similarity Approach to Paraphrase Detection.Computational Linguistics (CLUK) 11th Annual Re-search Colloquium, 2008.Satanjeev Banerjee and Ted Pedersen.
2010.
AnAdapted Lesk Algorithm for Word Sense Disambigu-ation Using WordNet.
CICLING:136-145.Ted Pedersen, Siddharth Patwardhan, and JasonMichelizzi.
2004.
WordNet::Similarity-measuring therelatedness of concepts.
In Proceedings of NAACL,2004.Vasileios Hatzivassiloglou , Judith L. Klavans , EleazarEskin.
1999.
Detecting text similarity over shortpassages: Exploring linguistic feature combinationsvia machine learning.
Proceeding of EmpiricalMethods in natural language processing and VeryLarge Corpora.Xiao-Ying Liu and Yi-Ming Zhou and Ruo-Shi Zheng.2008.
Measuring Semantic Similarity within Sentenc-es.
Proceedings of the Seventh International Confer-ence on Machine Learning and Cybernetics.Yuhua Li, David McLean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
Sentence Similaritybased on Semantic Nets and Corpus statistics.
2006.IEEE Transactions on Knowledgeand Data Engineer-ing Vol.
18, No.
8: 1138-1150.542
