Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 262?272,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsStoring the Web in Memory: Space Efficient Language Models withConstant Time RetrievalDavid GuthrieComputer Science DepartmentUniversity of SheffieldD.Guthrie@dcs.shef.ac.ukMark HeppleComputer Science DepartmentUniversity of SheffieldM.Hepple@dcs.shef.ac.ukAbstractWe present three novel methods of compactlystoring very large n-gram language models.These methods use substantially less spacethan all known approaches and allow n-gramprobabilities or counts to be retrieved in con-stant time, at speeds comparable to modernlanguage modeling toolkits.
Our basic ap-proach generates an explicit minimal perfecthash function, that maps all n-grams in amodel to distinct integers to enable storage ofassociated values.
Extensions of this approachexploit distributional characteristics of n-gramdata to reduce storage costs, including variablelength coding of values and the use of tieredstructures that partition the data for more effi-cient storage.
We apply our approach to stor-ing the full Google Web1T n-gram set and all1-to-5 grams of the Gigaword newswire cor-pus.
For the 1.5 billion n-grams of Gigaword,for example, we can store full count informa-tion at a cost of 1.66 bytes per n-gram (around30% of the cost when using the current state-of-the-art approach), or quantized counts for1.41 bytes per n-gram.
For applications thatare tolerant of a certain class of relatively in-nocuous errors (where unseen n-grams maybe accepted as rare n-grams), we can reducethe latter cost to below 1 byte per n-gram.1 IntroductionThe availability of very large text collections, suchas the Gigaword corpus of newswire (Graff, 2003),and the Google Web1T 1-5gram corpus (Brants andFranz, 2006), have made it possible to build mod-els incorporating counts of billions of n-grams.
Thestorage of these language models, however, presentsserious problems, given both their size and the needto provide rapid access.
A prevalent approach forlanguage model storage is the use of compact triestructures, but these structures do not scale well andrequire space proportional to both to the numberof n-grams and the vocabulary size.
Recent ad-vances (Talbot and Brants, 2008; Talbot and Os-borne, 2007b) involve the development of Bloom fil-ter based models, which allow a considerable reduc-tion in the space required to store a model, at the costof allowing some limited extent of false positiveswhen the model is queried with previously unseenn-grams.
The aim is to achieve sufficiently compactrepresentation that even very large language modelscan be stored totally within memory, avoiding thelatencies of disk access.
These Bloom filter basedmodels exploit the idea that it is not actually neces-sary to store the n-grams of the model, as long as,when queried with an n-gram, the model returns thecorrect count or probability for it.
These techniquesallow the storage of language models that no longerdepend on the size of the vocabulary, but only on thenumber of n-grams.In this paper we give three different models forthe efficient storage of language models.
The firststructure makes use of an explicit perfect hash func-tion that is minimal in that it maps n keys to in-tegers in the range 1 to n. We show that by us-ing a minimal perfect hash function and exploit-ing the distributional characteristics of the data weproduce n-gram models that use less space than allknow approaches with no reduction in speed.
Ourtwo further models achieve even more compact stor-age while maintaining constant time access by us-ing variable length coding to compress the n-gramsvalues and by using tiered hash structures to parti-262tion the data into subsets requiring different amountsof storage.
This combination of techniques allowsus, for example, to represent the full count informa-tion of the Google Web1T corpus (Brants and Franz,2006) (where count values range up to 95 billion) ata cost of just 2.47 bytes per n-gram (assuming 8-bit fingerprints, to exclude false positives) and just1.41 bytes per n-gram if we use 8-bit quantizationof counts.
These costs are 36% and 57% respec-tively of the space required by the Bloomier Filterapproach of Talbot and Brants (2008).
For the Gi-gaword dataset, we can store full count informationat a cost of only 1.66 bytes per n-gram.
We re-port empirical results showing that our approach al-lows a look-up rate which is comparable to existingmodern language modeling toolkits, and much fasterthan a competitor approach for space-efficient stor-age.
Finally, we propose the use of variable lengthfingerprinting for use in contexts which can toleratea higher rate of ?less damaging?
errors.
This moveallows, for example, the cost of storing a quantizedmodel to be reduced to 1 byte per n-gram or less.2 Related WorkA range of lossy methods have been proposed, toreduce the storage requirements of LMs by discard-ing information.
Methods include the use of entropypruning techniques (Stolcke, 1998) or clustering (Je-linek et al, 1990; Goodman and Gao, 2000) to re-duce the number of n-grams that must be stored.A key method is quantization (Whittaker and Raj,2001), which reduces the value information storedwith n-grams to a limited set of discrete alternatives.It works by grouping together the values (probabil-ities or counts) associated with n-grams into clus-ters, and replacing the value to be stored for eachn-gram with a code identifying its value?s cluster.For a scheme with n clusters, codes require log2nbits.
A common case is 8-bit quantization, allow-ing up to 256 distinct ?quantum?
values.
Differ-ent methods of dividing the range of values intoclusters have been used, e.g.
Whittaker and Raj(2001) used the Lloyd-Max algorithm, whilst Fed-erico and Bertoldi (2006) use the simpler Binningmethod to quantize probabilities, and show that theLMs so produced out-perform those produced us-ing the Lloyd-Max method on a phrase-based ma-chine translation task.
Binning partitions the rangeof values into regions that are uniformly populated,i.e.
producing clusters that contain the same num-ber of unique values.
Functionality to perform uni-form quantization of this kind is provided as part ofvarious LM toolkits, such as IRSTLM.
Some of theempirical storage results reported later in the paperrelate to LMs recording n-gram count values whichhave been quantized using this uniform binning ap-proach.
In the rest of this section, we turn to lookat some of the approaches used for storing languagemodels, irrespective of whether lossy methods arefirst applied to reduce the size of the model.2.1 Language model storage using TriestructuresA widely used approach for storing language mod-els employs the trie data structure (Fredkin, 1960),which compactly represents sequences in the formof a prefix tree, where each step down from theroot of the tree adds a new element to the sequencerepresented by the nodes seen so far.
Where twosequences share a prefix, that common prefix isjointly represented by a single node within the trie.For language modeling purposes, the steps throughthe trie correspond to words of the vocabulary, al-though these are in practice usually represented by24 or 32 bit integers (that have been uniquely as-signed to each word).
Nodes in the trie correspond-ing to complete n-grams can store other informa-tion, e.g.
a probability or count value.
Most mod-ern language modeling toolkits employ some ver-sion of a trie structure for storage, including SRILM(Stolcke, 2002), CMU toolkit (Clarkson and Rosen-feld, 1997), MITLM (Hsu and Glass, 2008), andIRSTLM (Federico and Cettolo, 2007) and imple-mentations exist which are very compact (Germannet al, 2009).
An advantage of this structure is that itallows the stored n-grams to be enumerated.
How-ever, although this approach achieves a compact ofrepresentation of sequences, its memory costs arestill such that very large language models requirevery large storage space, far more than the Bloomfilter based methods described shortly, and far morethan might be held in memory as a basis for morerapid access.
The memory costs of such modelshave been addressed using compression methods,see Harb et al (2009), but such extensions of the263approach present further obstacles to rapid access.2.2 Bloom Filter Based Language ModelsRecent randomized language models (Talbot andOsborne, 2007b; Talbot and Osborne, 2007a; Tal-bot and Brants, 2008; Talbot and Talbot, 2008; Tal-bot, 2009) make use of Bloom filter like structuresto map n-grams to their associated probabilities orcounts.
These methods store language models inrelatively little space by not actually keeping the n-gram key in the structure and by allowing a smallprobability of returning a false positive, i.e.
so thatfor an n-gram that is not in the model, there is asmall risk that the model will return some randomprobability instead of correctly reporting that the n-gram was not found.
These structures do not allowenumeration over the n-grams in the model, but formany applications this is not a requirement and theirspace advantages make them extremely attractive.Two major approaches have been used for storinglanguage models: Bloom Filters and Bloomier Fil-ters.
We give an overview of both in what follows.2.2.1 Bloom FiltersA Bloom filter (Bloom, 1970) is a compact datastructure for membership queries, i.e.
queries of theform ?Is this key in the Set??.
This is a weaker struc-ture than a dictionary or hash table which also asso-ciates a value with a key.
Bloom filters use well be-low the information theoretic lower bound of spacerequired to actually store the keys and can answerqueries in O(1) time.
Bloom filters achieve this featby allowing a small probability of returning a falsepositive.
A Bloom filter stores a set S of n elementsin a bit array B of size m. Initially B is set to con-tain all zeros.
To store an item x from S in B wecompute k random independent hash functions onx that each return a value in the range [0 .
.m?
1].These values serve as indices to the bit array B andthe bits at those positions are set to 1.
We do thisfor all elements in S, storing to the same bit array.Elements may hash to an index inB that has alreadybeen set to 1 and in this case we can think of theseelements as ?sharing?
this bit.
To test whether set Scontains a key w, we run our k hash functions on wand check if all those locations in B are set to 1.
Ifw ?
S then the bloom filter will always declare thatw belongs to S, but if x /?
S then the filter can onlysay with high probability that w is not in S. This er-ror rate depends on the number of k hash functionsand the ratio of m/n.
For instance with k = 3 hashfunctions and a bit array of size m = 20n, we canexpect to get a false positive rate of 0.0027.Talbot and Osborne (2007b) and Talbot and Os-borne (2007a) adapt Bloom filters to the requirementof storing values for n-grams by concatenating thekey (n-gram) and value to form a single item that isinserted into the filter.
Given a quantization schemeallowing values in the range [1 .
.
V ], a quantizedvalue v is stored by inserting into the filter all pair-ings of the n-gram with values from 1 up to v. To re-trieve the value for a given key, we serially probe thefilter for pairings of the key with each value from 1upwards, until the filter returns false.
The last valuefound paired with the key in the filter is the value re-turned.
Talbot and Osborne use a simple logarithmicquantization of counts that produce limited quan-tized value ranges, where most items will have val-ues that are low in the range, so that the serial look-up process will require quite a low number of stepson average.
For alternative quantization schemesthat involve greater value ranges (e.g.
the 256 valuesof a uniform 8-bit scheme) and/or distribute n-gramsmore evenly across the quantized values, the averagenumber of look-up steps required will be higher andhence the speed of access per n-gram accordinglylower.
In that case also, the requirement of insert-ing n-grams more than once in the filter (i.e.
withvalues from 1 up to the actual value v being stored)could substantially reduce the space efficiency of themethod, especially if low false positive rates are re-quired, e.g.
the case k = 3,m = 20n produces afalse positive rate of 0.0027, as noted above, but in asituation where 3 key-value items were being storedper n-gram on average, this error rate would in factrequire a storage cost of 60 bits per original n-gram.2.2.2 Bloomier FiltersMore recently, Talbot and Brants (2008) have pro-posed an approach to storing large language mod-els which is based on the Bloomier Filter techniqueof Chazelle et al (2004).
Bloomier Filters gener-alize the Bloom Filter to allow values for keys tobe stored in the filter.
To test whether a given keyis present in a populated Bloomier filter, we applyk hash functions to the key and use the results as264indices for retrieving the data stored at k locationswithin the filter, similarly to look-up in a Bloom fil-ter.
In this case, however, the data retrieved from thefilter consists of k bit vectors, which are combinedwith a fingerprint of the key, using bitwise XOR, toreturn the stored value.
The risk of false positivesis managed by making incorporating a fingerprint ofthe n-gram, and by making bit vectors longer thanthe minimum length required to store values.
Theseadditional error bits have a fairly predictable impacton error rates, i.e.
with e error bits, we anticipate theprobability of falsely construing an unseen n-gramas being stored in the filter to be 2?e.
The algo-rithm required to correctly populate the Bloomier fil-ter with stored data is complicated, and we shall notconsider its details here.
Nevertheless, when using vbits to represent values and e bits for error detection,this approach allows a language model to be storedat a cost of is 1.23 ?
(v + e) bits per n-gram.3 Single Minimal Perfect Hash RankingApproachWe first describe our basic structure we call SingleMinimal Perfect Hash Rank (S-MPHR) that is morecompact than that of Talbot and Brants (2008) whilestill keeping a constant look up time.
In the nexttwo sections we describe variations on this model tofurther reduce the space required while maintaininga constant look up time.
The S-MPHR structure canbe divided into 3 parts as shown in Figure 1: Stage1 is a minimal perfect hash function; Stage 2 is afingerprint and rank array; and Stage 3 is a uniquevalue array.
We discuss each stage in turn.FP5FP FP1FP3FP2FP FP4...key1key2key3key4key5keyN...Minimal Perfect Hash FunctionArray of K distinct probability values / frequency countsp1p2p3p4p5p6... pKrank(key5) rank(key) rank(key1) rank(key3) rank(key2) rank(key) rank(key4) ...Figure 1: The Single MPHR structure3.1 Minimal Perfect Hash FunctionThe first part of the structure is a minimal perfecthash function that maps every n-gram in the trainingdata to a distinct integer in the range 0 to N ?
1,whereN is the total number of n-grams to store.
Weuse these integers as indices into the array of Stage2 of our structure.We use the Hash, displace, and compress (CHD)(Belazzougui et al, 2009) algorithm to generate aminimal perfect hash function that requires 2.07 bitsper key and has O(1) access.
The algorithm worksas follows.
Given a set S that contains N = |S|keys (in our case n-grams) that we wish to map tointegers in the range 0 to N ?
1, so that every keymaps to a distinct integer (no collisions).The first step is to use a hash function g(x), tomap every key to a bucket B in the range 0 to r.(For this step we use a simple hash function like theone used for generating fingerprints in the pervioussection.
)Bi = {x ?
S|g(x) = i} 0 ?
i ?
rThe function g(x) is not perfect so several keys canmap to the same bucket.
Here we choose r ?
N ,so that the number of buckets is less than or equalto the number of keys (to achieve 2.07 bits per keywe use r = N5 , so that the average bucket size is 5).The buckets are then sorted into descending orderaccording to the number of keys in each bucket |Bi|.For the next step, a bit array, T , of size N is ini-tialized to contain all zeros T [0 .
.
.
N ?
1].
This bitarray is used during construction to keep track ofwhich integers in the range 0 to N ?
1 the minimalperfect hash has already mapped keys to.
Next wemust assume we have access to a family of randomand independent hash functions h1, h2, h3, .
.
.
thatcan be accessed using an integer index.
In practiceit sufficient to use functions that behave similarly tofully random independent hash functions and Belaz-zougui et al (2009) demonstrate how such functionscan be generated easily by combining two simplehash functions.Next is the ?displacement?
step.
For each bucket,in the sorted order from largest to smallest, theysearch for a random hash function that maps all ele-ments of the bucket to values in T that are currentlyset to 0.
Once this function has been found those265positions in T are set to 1.
So, for each bucket Bi,it is necessary to iteratively try hash functions, h`for ` = 1, 2, 3, .
.
.
to hash every element of Bi to adistinct index j in T that contains a zero.
{h`(x)|x ?
Bi} ?
{j|T [j] = 1} = ?where the size of {h`(x)|x ?
Bi} is equal to the sizeof Bi.
When such a hash function is found we needonly to store the index, `, of the successful functionin an array ?
and set T [j] = 1 for all positions j thath` hashed to.
Notice that the reason the largest buck-ets are handled first is because they have the most el-ements to displace and this is easier when the arrayT contains more empty positions (zeros).The final step in the algorithm is to compress the ?array (which has length equal to the number of buck-ets |B|), retaining O(1) access.
This compression isachieved using simple variable length encoding withan index array (Fredriksson and Nikitin, 2007).3.2 Fingerprint and Rank ArrayThe hash function used in Stage 1 is perfect, so itis guaranteed to return unique integers for seen n-grams, but our hash function will also return inte-ger values in the range 0 to N ?
1 for n-grams thathave not been seen before (were not used to build thehash function).
To reduce the probability of theseunseen n-grams giving false positives results fromour model we store a fingerprint of each n-gram inStage 2 of our structure that can be compared againstthe fingerprints of unseen n-grams when queried.If these fingerprints of the queried n-gram and thestored n-gram do not match then the model willcorrectly report that the n-gram has not been seenbefore.
The size of this fingerprint determines therate of false positives.
Assuming that the finger-print is generated by a random hash function, andthat the returned integer of an unseen key from theMPH function is also random, expected false posi-tive rate for the model is the same as the probabil-ity of two keys randomly hashing to the same value,12m , where m is the number of bits of the finger-print.
The fingerprint can be generated using anysuitably random hashing algorithm.
We use AustinAppleby?s Murmurhash21 implementation to finger-print each n-gram and then store the m highest or-der bits.
Stage 2 of the MPHR structure also stores1http://murmurhash.googlepages.com/a rank for every n-gram along with the fingerprint.This rank is an index into the array of Stage 3 ofour structure that holds the unique values associatedwith any n-gram.3.3 Unique Value ArrayWe describe our storage of the values associatedwith n-grams in our model assuming we are storingfrequency ?counts?
of n-grams, but it applies also tostoring quantized probabilities.
For every n-gram,we store the ?rank?
of the frequency count r(key),(r(key) ?
[0...R ?
1]) and use a separate array inStage 3 to store the frequency count value.
This issimilar to quantization in that it reduces the num-ber of bits required for storage, but unlike quanti-zation it does not require a loss of any information.This was motivated by the sparsity of n-gram fre-quency counts in corpora in the sense that if we takethe lowest n-gram frequency count and the high-est n-gram frequency count then most of the inte-gers in that range do not occur as a frequency countof any n-grams in the corpus.
For example in theGoogle Web1T data, there are 3.8 billion unique n-grams with frequency counts ranging from 40 to 95Billion yet these n-grams only have 770 thousanddistinct frequency counts (see Table 2).
Becausewe only store the frequency rank, to keep the pre-cise frequency information we need only dlog2Kebits per n-gram, where K is the number of distinctfrequency counts.
To keep all information in theGoogle Web1T data we need only dlog2 771058e =20 bits per n-gram.
Rather than the bits neededto store the maximum frequency count associatedwith an n-gram, dlog2 maxcounte, which for GoogleWeb1T would be dlog2 95119665584e = 37 bits pern-gram.unique maximum n-gram uniquen-grams frequency count counts1gm 1, 585, 620 71, 363, 822 16, 8962gm 55, 809, 822 9, 319, 466 20, 2373gm 250, 928, 598 829, 366 12, 4254gm 493, 134, 812 231, 973 6, 8385gm 646, 071, 143 86, 943 4, 201Total 1, 447, 529, 995 71, 363, 822 60, 487Table 1: n-gram frequency counts from Gigaword corpus266unique maximum n-gram uniquen-grams frequency count counts1gm 13, 588, 391 95, 119, 665, 584 238, 5922gm 314, 843, 401 8, 418, 225, 326 504, 0873gm 977, 069, 902 6, 793, 090, 938 408, 5284gm 1, 313, 818, 354 5, 988, 622, 797 273, 3455gm 1, 176, 470, 663 5, 434, 417, 282 200, 079Total 3, 795, 790, 711 95, 119, 665, 584 771, 058Table 2: n-gram frequency counts from Google Web1Tcorpus3.4 Storage RequirementsWe now consider the storage requirements of our S-MPHR approach, and how it compares against theBloomier filter method of Talbot and Brants (2008).To start with, we put aside the gains that can comefrom using the ranking method, and instead con-sider just the costs of using the CHD approach forstoring any language model.
We saw that the stor-age requirements of the Talbot and Brants (2008)Bloomier filter method are a function of the numberof n-grams n, the bits of data d to be stored per n-gram (with d = v + e: v bits for value storage, ande bits for error detection), and a multiplying factorof 1.23, giving an overall cost of 1.23d bits per n-gram.
The cost for our basic approach is also easilycomputed.
The explicit minimal PHF computed us-ing the CHD algorithm brings a cost of 2.07 bits pern-gram for the PHF itself, and so the comparableoverall cost to store a S-MPHR model is 2.07 + dbits per n-gram.
For small values of d, the Bloomierfilter approach has the smaller cost, but the ?break-even?
point occurs when d = 9.
When d is greaterthan 9 bits (as it usually will be), our approach winsout, being up to 18% more efficient.The benefits that come from using the rankingmethod (Stage 3), for compactly storing count val-ues, can only be evaluated in relation to the distribu-tional characteristics specific corpora, for which weshow results in Section 6.4 Compressed MPHR ApproachOur second approach, called Compressed MPHR,further reduces the size of the model whilst main-taining O(1) time to query the model.
Most com-pression techniques work by exploiting the redun-dancy in data.
Our fingerprints are unfortunatelyrandom sequences of bits, so trying to compressFP5FP FP1FP3FP2FP FP4...key1key2key3key4key5keyN...Minimal Perfect Hash FunctionArray of K distinct probability values / frequency countsp1p2p3p4p5p6... pKrank(key5) rank(key) rank(key1) rank(key3) rank(key2) rank(key) rank(key4) ...Fingerprint ArrayCompressed Rank ArrayFigure 2: Compressed MPHR structurethese is fruitless, but the ranks associated with n-grams contain much redundancy and so are likely tocompress well.
We therefore modify our original ar-chitecture to put the ranks and fingerprints into sep-arate arrays, of which the ranks array will be com-pressed, as shown in Figure 2.Much like the final stage of the CHD minimalperfect hash algorithm we employ a random accesscompression algorithm of Fredriksson and Nikitin(2007) to reduce the size required by the array ofranks.
This method allows compression while re-taining O(1) access to query the model.The first step in the compression is to encodethe ranks array using a dense variable length cod-ing.
This coding works by assigning binary codeswith different lengths to each number in the rank ar-ray, based on how frequent that number occurs.
Lets1, s2, s3, .
.
.
, sK be the ranks that occur in the rankarray sorted by there frequency.
Starting with mostfrequent number in the rank array (clearly 1 is themost common frequency count in the data unless ithas been pruned) s1 we assign it the bit code 0 andthen assign s2 the bit code 1, we then proceed by as-signing bit codes of two bits, so s3 is assigned 00, s4is assigned 01, etc.
until all two bit codes are usedup.
We then proceed to assign 3 bit codes and so on.All of the values from the rank array are coded inthis form and concatenated to form a large bit vectorretaining their original ordering.
The length in bitsfor the ith number is thus blog2 (i+ 2)c and so thenumber of bits required for the whole variable lengthcoded rank array is: b =?Ki=0 f(si)blog2 (i+ 2)c.Where f() gives the frequency that the rank occurs267andK is the total number of distinct ranks.
The codefor the ith number is the binary representation withlength blog2 (i+ 2)c of the number obtained usingthe formula:code = i+ 2?
2blog2 (i+2)cThis variable length coded array is not useful by it-self because we do not know where each number be-gins and ends, so we also store an index array holdthis information.
We create an additional bit arrayD of the same size b as the variable length coded ar-ray that simply contains ones in all positions that acode begins in the rank array and zeros in all otherpositions.
That is the ith rank in the variable lengthcoded array occurs at position select1(D, i), whereselect1 gives the position of the ith one in the ar-ray.
We do not actually store theD array, but insteadwe build a more space efficient structure to answerselect1 queries.
Due the distribution of n-gram fre-quencies, the D array is typically dense in contain-ing a large proportion of ones, so we build a rank9seldictionary structure (Vigna, 2008) to answer thesequeries in constant time.
We can use this structureto identify the ith code in our variable length en-coded rank array by querying for its starting posi-tion, select1(D, i), and compute its length using itsending position, select1(D, i+1)?1.
The code andits length can then be decoded to obtain the originalrank:rank = code + 2(length in bits) ?
25 Tiered MPHRIn this section we describe an alternative route to ex-tending our basic S-MPHR model to achieve betterspace efficiency, by using multiple hash stores.
Themethod exploits distributional characteristics of thedata, i.e.
that lower rank values (those assigned tovalues shared by very many n-grams) are sufficientfor representing the value information of a dispro-portionately large subset of the data.
For the GoogleWeb 1T data, for example, we find that the first 256ranks account for nearly 85% of distinct n-grams, soif we could store ranks for these n-grams using onlythe 8 bits they require, whilst allowing perhaps 20bits per n-gram for the remaining 15%, we wouldachieve an average of just under 10 bits per n-gramto store all the rank values.To achieve this gain, we might partition the n-gram data into subsets requiring different amountsof space for value storage, and put these subsets inseparate MPHRs, e.g.
for the example just men-tioned, with two MPHRs having 8 and 20 bit valuestorage respectively.
Partitioning to a larger numberh of MPHRs might further reduce this average cost.This simple approach has several problems.
Firstly,it potentially requires a series of look up steps (i.e.up to h) to retrieve the value for any n-gram, withall hashes needing to be addressed to determine theunseen status of an unseen n-gram.
Secondly, mul-tiple look ups will produce a compounding of errorrates, since we have up to h opportunities to falselyconstrue an unseen n-gram as seen, or to construea seen n-gram as being stored in the wrong MPHRand so return an incorrect count for it.FP5FP FP1FP3FP2FP FP4...key1key2key3key4key5keyN...Minimal Perfect Hash Function #1rank(key5) Redirect 1 Redirect 2 rank(key3) rank(key2) Redirect 1 Redirect 2 ...Minimal Perfect Hash Function  #2 Minimal Perfect Hash Function #3rank(key) rank(key) ... rank(key) rank(key) rank(key) ... rank(key)Figure 3: Tiered minimal perfect hash data structureWe will here explore an alternative approach thatwe call Tiered MPHR, which avoids this compound-ing of errors, and which limits the number of looksups to a maximum of 2, irrespective of how manyhashes are used.
This approach employs a singletop-level MPHR which has the full set of n-gramsfor its key-set, and stores a fingerprint for each.
Inaddition, space is allocated to store rank values, butwith some possible values being reserved to indicateredirection to other secondary hashes where valuescan be found.
Each secondary hash has a minimalperfect hash function that is computed only for then-grams whose values it stores.
Secondary hashesdo not need to record fingerprints, as fingerprint test-ing is done in the top-level hash.For example, we might have a configuration of268three hashes, with the top-level MPHR having 8-bitstorage, and with secondary hashes having 10 and 20bit storage respectively.
Two values of the 8-bit store(e.g.
0 and 1) are reserved to indicate redirectionto the specific secondary hashes, with the remainingvalues (2 .
.
255) representing ranks 1 to 254.
The10-bit secondary hash can store 1024 different val-ues, which would then represent ranks 255 to 1278,with all ranks above this being represented in the20-bit hash.
To look up the count for an n-gram,we begin with the top-level hash, where fingerprinttesting can immediately reject unseen n-grams.
Forsome seen n-grams, the required rank value is pro-vided directly by the top-level hash, but for othersa redirection value is returned, indicating preciselythe secondary hash in which the rank value will befound by simple look up (with no additional finger-print testing).
Figure 3 gives a generalized presenta-tion of the structure of tiered MPHRs.
Let us repre-sent a configuration for a tiered MPHR as a sequenceof bit values for their value stores, e.g.
(8,10,20)for the example above, or H = (b1, .
.
.
.bh) moregenerally (with b1 being the top-level MPHR).The overall memory cost of a particular config-uration depends on distributional characteristics ofthe data stored.
The top-level MPHR of config-uration (b1, .
.
.
.bh) stores all n-grams in its key-set, so its memory cost is calculated as before asN ?
(2.07 +m + b1) (m the fingerprint size).
Thetop-level MPHR must reserve h?
1 values for redi-rection, and so covers ranks [1 .
.
(2b1 ?h+1)].
Thesecond MPHR then covers the next 2b2 ranks, start-ing at (2b1 ?
h+2), and so on for further secondaryMPHRs.
This range of ranks determines the pro-portion ?i of the overall n-gram set that each sec-ondary MPHR bi stores, and so the memory cost ofeach secondary MPHR is N ??i?
(2.07+ bi).
Theoptimal T-MPHR configuration for a given data setis easily determined from distributional information(of the coverage of each rank), by a simple search.6 ResultsIn this section, we present some results comparingthe performance of our new storage methods to someof the existing methods, regarding the costs of stor-ing LMs, and regarding the data access speeds thatalternative systems allow.MethodGigaword Web1Tfull quantized full quantizedBloomier 6.00 3.08 7.53 3.08S-MPHR 3.76 2.76 4.26 2.76C-MPHR 2.19 2.09 3.40 2.09T-MPHR 2.16 1.91 2.97 1.91Table 3: Space usage in bytes/ngram using 12-bit finger-prints and storing all 1 to 5 gramsMethodGigaword Web1Tfull quantized full quantizedBloomier 5.38 2.46 6.91 2.46S-MPHR 3.26 2.26 3.76 2.26C-MPHR 1.69 1.59 2.90 1.59T-MPHR 1.66 1.41 2.47 1.41Table 4: Space usage in bytes/n-gram using 8-bit finger-prints and storing all 1 to 5 grams6.1 Comparison of memory costsTo test the effectiveness of our models we built mod-els storing n-grams and full frequency counts forboth the Gigaword and Google Web1T corpus stor-ing all 1,2,3,4 and 5 grams.
These corpora are verylarge, e.g.
the Google Web1T corpus is 24.6GBwhen gzip compressed and contains over 3.7 bil-lion n-grams, with frequency counts as large as 95billion, requiring at least 37 bits to be stored.
Us-ing the Bloomier algorithm of Talbot and Brants(2008) with 37 bit values and 12 bit fingerprintswould require 7.53 bytes/n-gram, so we would need26.63GB to store a model for the entire corpus.In comparison, our S-MPHR method requiresonly 4.26 bytes per n-gram to store full frequencycount information and stores the entire Web1T cor-pus in just 15.05GB or 57% of the space required bythe Bloomier method.
This saving is mostly due tothe ranking method allowing values to be stored at acost of only 20 bits per n-gram.
Applying the samerank array optimization to the Bloomier method sig-nificantly reduces its memory requirement, but S-MPHR still uses only 86% of the space that theBloomier approach requires.
Using T-MPHR in-stead, again with 12-bit fingerprints, we can storefull counts for the Web 1T corpus in 10.50GB,which is small enough to be held in memory onmany modern machines.
Using 8-bit fingerprints, T-269Method bytes/ngramSRILM Full, Compact 33.6IRSTLM, 8-bit Quantized 9.1Bloomier 12bit fp, 8bit Quantized 3.08S-MPHR 12bit fp, 8bit Quantized 2.76C-MPHR 12bit fp, 8bit Quantized 2.09T-MPHR 12bit fp, 8bit Quantized 1.91Table 5: Comparison between approaches for storing all1 to 5 grams of the Gigaword CorpusMPHR can store this data in just 8.74GB.Tables 3, 4 and 5 show results for all methods2 onboth corpora, for storing full counts, and for when8-bit binning quantization of counts is used.6.2 Access speed comparisonsThe three models we present in this paper performqueries in O(1) time and are thus asymptoticallyoptimal, but this does not guarantee they performwell in practice, therefore in this section we mea-sure query speed on a large set of n-grams and com-pare it to that of modern language modeling toolk-its.
We build a model of all unigrams and bigramsin the Gigaword corpus (see Table 1) using the C-MPHR method, SRILM (Stolcke, 2002), IRSTLM(Federico and Cettolo, 2007), and randLM3 (Talbotand Osborne, 2007a) toolkits.
RandLM is a mod-ern language modeling toolkit that uses Bloom filterbased structures to store large language models andhas been integrated so that it can be used as the lan-guage model storage for the Moses statistical ma-chine translation system (Koehn et al, 2007).
Weuse randLM with the BloomMap (Talbot and Tal-bot, 2008) storage structure option with 8 bit quan-tized values and an error rate equivalent to using 8bit fingerprints (as recommended in the Moses doc-umentation).
All methods are implemented in C++and are run on a machine with 2.80GHz Intel XeonE5462 processor and 64 GB of RAM.
In additionwe show a comparison to using a modern database,MySQL 5.0, to store the same data.
We measurethe speed of querying all models for the 55 mil-lion distinct bigrams that occur in the Gigaword,2All T-MPHR results are for optimal configurations: Gi-gaword full:(2,3,16), Gigaword quant:(1,8), Web1Tfull:(8,6,7,8,9,10,13,20), Web1T quant:(1,8).3http://sourceforge.net/projects/randlm/Test Time Speed(hr :min:sec) queries/secC-MPHR 00 : 01 : 50 507362IRSTLM 00 : 02 : 12 422802SRILM 00 : 01 : 29 627077randLM 00 : 27 : 28 33865MySQL 5 29 : 25 : 01 527Table 6: Look-up speed performance comparison for C-MPHR and several other LM storage methodsthese results are shown in Table 6.
Unsurprisinglyall methods perform significantly faster than using adatabase as they build models that reside completelyin memory.
The C-MPHR method tested here isslower than both S-MPHR and T-MPHR models dueto the extra operations required for access to the vari-able length encoded array yet still performs similarlyto SRILM and IRSTLM and is 14.99 times fasterthan using randLM.7 Variable Length FingerprintsTo conclude our presentation of new methods forspace-efficient language model storage, we suggestan additional possibility for reducing storage costs,which involves using different sizes of fingerprintfor different n-grams.
Recall that the only errors al-lowed by our approach are false-positives, i.e.
wherean unseen n-gram is falsely construed as being partof the model and a value returned for it.
The idea be-hind using different sizes of fingerprint is that, intu-itively, some possible errors seem worse than others,and in particular, it seems likely to be less damagingif we falsely construe an unseen n-gram as being aseen n-gram that has a low count or probability thanas being one with a high count or probability.False positives arise when our perfect hashingmethod maps an unseen n-gram to position wherethe stored n-gram fingerprint happens to coincidewith that computed for the unseen n-gram.
The riskof this occurring is a simple function of the sizeof fingerprints.
To achieve a scheme that admits ahigher risk of less damaging errors, but enforces alower risk of more damaging errors, we need onlystore shorter fingerprints for n-grams in our modelthat have low counts or probabilities, and longerfingerprints for n-grams with higher values.
This270idea could be implemented in different ways, e.g.by storing fingerprints of different lengths contigu-ously within a bit array, and constructing a ?selectionstructure?
of the kind described in Section 4 to allowus to locate a given fingerprint within the bit array.FP5FP FP1FP3FP2FP FP4...key1key2key3key4key5keyN...Minimal Perfect Hash Functionrank(key5) Redirect 1 Redirect 2 rank(key3) rank(key2) Redirect 1 Redirect 2 ...Minimal Perfect Hash Functionrank(key) rank(key) ... rank(key)Minimal Perfect Hash Functionfirst j bits offingerprintFP FP...FPlast m - jbits offingerprintrank(key) rank(key) ... rank(key)FP FP...FPFigure 4: Variable length fingerprint T-MPHR structureusing j bit fingerprints for the n-grams which are mostrare and m bit fingerprints for all others.We here instead consider an alternative imple-mentation, based on the use of tiered structures.
Re-call that for T-MPHR, the top-level MPHR has alln-grams of the model as keys, and stores a fin-gerprint for each, plus a value that may representan n-gram?s count or probability, or that may redi-rect to a second-level hash where that informationcan be found.
Redirection is done for items withhigher counts or probabilities, so we can achievelower error rates for precisely these items by stor-ing additional fingerprint information for them inthe second-level hash (see Figure 4).
For example,we might have a top-level hash with only 4-bit fin-gerprints, but have an additional 8-bits of fingerprintfor items also stored in a second-level hash, so thereis quite a high risk (close to 116 ) of returning a lowcount for an unseen n-gram, but a much lower riskof returning any higher count.
Table 7 applies thisidea to storing full and quantized counts of the Gi-gaword and Web 1T models, when fingerprints in thetop-level MPHR have sizes in the range 1 to 6 bits,with the fingerprint information for items stored insecondary hashes being ?topped up?
to 12 bits.
Thisapproach achieves storage costs of around 1 byte pern-gram or less for the quantized models.Bits inlowestfinger-printGiga-wordQuan-tizedWeb1TQuan-tizedGiga-wordAllWeb1TAll1 0.55 0.55 1.00 1.812 0.68 0.68 1.10 1.923 0.80 0.80 1.21 2.024 0.92 0.92 1.31 2.135 1.05 1.04 1.42 2.236 1.17 1.17 1.52 2.34Table 7: Bytes per fingerprint for T-MPHR model using 1to 6 bit fingerprints for rarest n-grams and 12 bit (in total)fingerprints for all other n-grams.
(All configurations areas in Footnote 2.
)8 ConclusionWe have presented novel methods of storing largelanguage models, consisting of billions of n-grams,that allow for quantized values or frequency countsto be accessed quickly and which require far lessspace than all known approaches.
We show that itis possible to store all 1 to 5 grams in the Gigawordcorpus, with full count information at a cost of just1.66 bytes per n-gram, or with quantized counts forjust 1.41 bytes per n-gram.
We have shown that ourmodels allow n-gram look-up at speeds comparableto modern language modeling toolkits (which havemuch greater storage costs), and at a rate approxi-mately 15 times faster than a competitor approachfor space-efficient storage.ReferencesDjamal Belazzougui, Fabiano Botelho, and Martin Diet-zfelbinger.
2009.
Hash, displace, and compress.
Al-gorithms - ESA 2009, pages 682?693.Burton H. Bloom.
1970.
Space/time trade-offs inhash coding with allowable errors.
Commun.
ACM,13(7):422?426.Thorsten Brants and Alex Franz.
2006.
Google Web1T 5-gram Corpus, version 1.
Linguistic Data Con-sortium, Philadelphia, Catalog Number LDC2006T13,September.Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, andAyellet Tal.
2004.
The bloomier filter: an efficientdata structure for static support lookup tables.
InSODA ?04, pages 30?39, Philadelphia, PA, USA.271Philip Clarkson and Ronald Rosenfeld.
1997.
Statis-tical language modeling using the CMU-cambridgetoolkit.
In Proceedings of ESCA Eurospeech 1997,pages 2707?2710.Marcello Federico and Nicola Bertoldi.
2006.
Howmany bits are needed to store probabilities for phrase-based translation?
In StatMT ?06: Proceedings of theWorkshop on Statistical Machine Translation, pages94?101, Morristown, NJ, USA.
Association for Com-putational Linguistics.Marcello Federico and Mauro Cettolo.
2007.
Efficienthandling of n-gram language models for statistical ma-chine translation.
In StatMT ?07: Proceedings of theSecond Workshop on Statistical Machine Translation,pages 88?95, Morristown, NJ, USA.
Association forComputational Linguistics.Edward Fredkin.
1960.
Trie memory.
Commun.
ACM,3(9):490?499.Kimmo Fredriksson and Fedor Nikitin.
2007.
Simplecompression code supporting random access and faststring matching.
In Proc.
of the 6th InternationalWorkshop on Efficient and Experimental Algorithms(WEA?07), pages 203?216.Ulrich Germann, Eric Joanis, and Samuel Larkin.
2009.Tightly packed tries: How to fit large models intomemory, and make them load fast, too.
Proceedings ofthe Workshop on Software Engineering, Testing, andQuality Assurance for Natural Language (SETQA-NLP 2009), pages 31?39.Joshua Goodman and Jianfeng Gao.
2000.
Languagemodel size reduction by pruning and clustering.
InProceedings of ICSLP?00, pages 110?113.David Graff.
2003.
English Gigaword.
Linguistic DataConsortium, catalog number LDC2003T05.Boulos Harb, Ciprian Chelba, Jeffrey Dean, and SanjayGhemawat.
2009.
Back-off language model compres-sion.
In Proceedings of Interspeech, pages 352?355.Bo-June Hsu and James Glass.
2008.
Iterative languagemodel estimation:efficient data structure & algorithms.In Proceedings of Interspeech, pages 504?511.F.
Jelinek, B. Merialdo, S. Roukos, and M. Strauss I.1990.
Self-organized language modeling for speechrecognition.
In Readings in Speech Recognition, pages450?506.
Morgan Kaufmann.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In ACL ?07:Proceedings of the 45th Annual Meeting of the ACL onInteractive Poster and Demonstration Sessions, pages177?180, Morristown, NJ, USA.
Association for Com-putational Linguistics.Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In Proceedings of DARPABroadcast News Transcription and UnderstandingWorkshop, pages 270?274.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,volume 2, pages 901?904, Denver.David Talbot and Thorsten Brants.
2008.
Randomizedlanguage models via perfect hash functions.
Proceed-ings of ACL-08 HLT, pages 505?513.David Talbot and Miles Osborne.
2007a.
Randomisedlanguage modelling for statistical machine translation.In Proceedings of ACL 07, pages 512?519, Prague,Czech Republic, June.David Talbot and Miles Osborne.
2007b.
Smoothedbloom filter language models: Tera-scale LMs on thecheap.
In Proceedings of EMNLP, pages 468?476.David Talbot and John M. Talbot.
2008.
Bloom maps.In 4th Workshop on Analytic Algorithmics and Com-binatorics 2008 (ANALCO?08), pages 203?212, SanFrancisco, California.David Talbot.
2009.
Succinct approximate counting ofskewed data.
In IJCAI?09: Proceedings of the 21stinternational jont conference on Artifical intelligence,pages 1243?1248, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.Sebastiano Vigna.
2008.
Broadword implementation ofrank/select queries.
In WEA?08: Proceedings of the7th international conference on Experimental algo-rithms, pages 154?168, Berlin, Heidelberg.
Springer-Verlag.Edward Whittaker and Bhinksha Raj.
2001.Quantization-based language model compres-sion.
Technical report, Mitsubishi Electric ResearchLaboratories, TR-2001-41.272
