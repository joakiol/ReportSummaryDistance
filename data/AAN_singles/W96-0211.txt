Automating Feature Set Selection for Case-Based Learning ofLinguistic KnowledgeClaire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853-7501E-mail: cardie@cs.cornell.eduAbstractThis paper addresses the issue of "algorithmvs.
representation" for case-based learning of lin-guistic knowledge.
We first present empiricalevidence that the success of case-based learningmethods for natural anguage processing tasks de-pends to a large degree on the feature set used todescribe the training instances.
Next, we presenta technique for automating feature set selectionfor case-based learning of linguistic knowledge.Given as input a baseline case representation, themethod modifies the representation i responseto a number of predefined linguistic biases byadding, deleting, and weighting features appro-priately.
We apply the linguistic bias approachto feature set selection to the problem of relativepronoun disambiguation and show that the case-based learning algorithm improves as relevant bi-asses are incorporated into the underlying instancerepresentation.
Finally, we argue that the linguis-tic bias approach to feature set selection offers newpossibilities for case-based learning of natural an-guage: it simplifies the process of instance repre-sentation design and, in theory, obviates the needfor separate instance representations for each lin-guistic knowledge acquisition task.
More impor-tantly, the approach offers a mechanism for explic-itly combining the frequency information availablefrom corpus-based techniques with linguistic biasinformation employed in traditional linguistic andknowledge-based approaches to natural languageprocessing.IntroductionStandard symbolic machine learning techniqueshave been successfully applied to a number oftasks in natural language processing (NLP).
Ex-amples include the use of decision trees for syntac-tic analysis (Magerman, 1995), coreference (Aoneand Bennett, 1995; McCarthy and Lehnert, 1995),and cue phrase identification (Litman, 1994); theuse of inductive logic programming for learningsemantic grammars and building prolog parsers113(Zelle and Mooney, 1994; Zelle and Mooney, 1993);the use of conceptual clustering algorithms for rel-ative pronoun resolution (Cardie, 1992a; Cardie1992b), and the use of case-based learning tech-niques for lexical tagging tasks (Cardie, 1993a;Daelemans et al, submitted).
In theory, both sta-tistical and machine learning techniques can sig-nificantly reduce the knowledge-engineering effortfor building large-scale NLP systems: they offeran automatic means for acquiring robust heuris-tics for a host of lexical and structural disam-biguation tasks.
It is well-known in the machinelearning community, however, that the successof a learning algorithm depends critically on therepresentation used to describe the training andtest instances (Almuallim and Dietterich, 1991,Langley and Sage, in press).
Unfortunately, thetask of designing an appropriate instance represen-tation - -  also known as feature set selection - -  canbe extraordinarily difficult, time-consuming, andknowledge-intensive (Quinlan, 1983).
This poses aproblem for current statistical and machine learn-ing approaches to natural anguage understandingwhere a new instance representation is typicallyrequired for each linguistic task tackled.This paper addresses the role of the un-derlying instance representation for one class ofsymbolic machine learning algorithm as appliedto natural language understanding tasks, thatof case-based learning (CBL).
In general, case-based learning algorithms (e.g., instance-basedlearning (Aha et al, 1991), case-based reason-ing (Riesbeck and Schank, 1989, Kolodner, 1993),memory-based reasoning (Stanfill and Waltz,1986) solve problems by first creating a case baseof previous problem-solving episodes.
Then, whena new problem is encountered, the "most simi-lar" case is retrieved from the case base and usedto solve the novel problem.
The retrieved casecan either be used directly or after one or moremodifications to adapt it to the current problem-solving situation.
Case-based learning algorithmshave been used in NLP for context-sensitive pars-ing (Simmons and Yu, 1992), for text categoriza-tion (Riloffand Lehnert, 1994); for lexical tag-ging tasks like part-of-speech tagging and seman-tic feature tagging (Daelemans et al, submitted,Cardie, 1994, Cardie, 1993a); for semantic inter-pretation (e.g., concept extraction (Cardie, 1994,Cardie, 1993a)); and for a number of low-levellanguage acquisition tasks, including stress ac-quisition (Daelemans et al, 1994) and grapheme-to-phoneme conversion (Bosch and Daelemans,1993).
In the sections below, we first presentempirical evidence that the success of case-basedlearning methods for natural anguage processingtasks depends to a large degree on the feature setused to describe the training instances.
Next, wepresent a technique for automating feature set se-lection for case-based learning of linguistic knowl-edge.
Given as input a baseline instance represen-tation comprised of both relevant and irrelevantattributes, the method modifies the representa-tion in response to any of a number of predefinedlinguistic biases.
More specifically, the techniqueuses linguistic biases to discard irrelevant featuresfrom the representation, to add new features tothe representation, and to weight features appro-priately.
We then apply the linguistic bias ap-proach to feature set selection in one natural an-guage learning task - -  the relative pronoun (RP)disambiguation task from Cardie (1992a, 1992b) .Experiments indicate that the case-based learningalgorithm improves on the relative pronoun taskas relevant biases are incorporated into the under-lying instance representation.
Furthermore, us-ing the modified instance representation, the case-based learning algorithm is able to outperform aset of hand-coded heuristics designed for the sametask.Finally, we argue that the linguistic bias ap-proach to feature set selection offers new possibil-ities for case-based learning of natural anguage:?
It provides a natural mechanism for combin-ing the frequency information available fromcorpus-based NLP techniques with linguisticbias information employed in traditional linguis-tic and knowledge-based approaches to languageprocessing.
The development of computationalmodels of language processing that combine fre-quencies and linguistic biases has been noted byPereira (Pereira, 1994) as an important area ofresearch in corpus-based NLP.?
The linguistic bias approach to feature set selec-tion simplifies and shortens the process of de-signing an appropriate instance representationfor individual natural anguage learning tasks.System developers can safely include featuresfor all available knowledge sources in the base-line instance representation - - the irrelevantones will be discarded automatically.114* By adopting the automated approach to fea-ture set selection for CBL of linguistic knowl-edge, the same underlying instance representa-tion can, in theory, be used across many linguis-tic knowledge acquisition tasks.
A separate in-stance representation need not be designed eachtime we want to apply the learning algorithm toa new problem in natural anguage understand-ing.The remainder of the paper is organized asfollows.
The section below describes the basiccase-based learning algorithm used throughout thepaper.
The following section determines the roleof the underlying instance representation i  case-based learning of natural anguage by comparingthe accuracy of the CBL algorithm on a numberof natural anguage learning tasks using differentinstance representations.
Next, we present he lin-guistic bias approach to feature set selection andapplies the technique to the relative pronoun dis-ambiguation task.
We conclude with a discussionof the general implications of the linguistic biasapproach to feature set selection for case-basedlearning of natural anguage.The  Bas ic  Case-Based  Learn ingA lgor i thmThroughout he paper, we employ a simple k-nearest neighbor case-based learning algorithm.In addition, we assume that the learning algorithmis embedded in a parser or larger NLP system and,hence, has access to all knowledge sources that areavailable to the NLP system.In case-based approaches to natural anguageunderstanding, the goal of the training phase is tocollect a set of cases that describe ambiguity res-olution episodes for a particular problem in textanalysis.
To do this, a small set of sentences ifirst selected randomly from an annotated train-ing corpus.
Next, the sentence analyzer processesthe training sentences and creates a training caseevery time an instance of the ambiguity occurs.To learn heuristics for prepositional phrase at-tachment, for example, the parser would create acase whenever it recognizes a prepositional phrase.Each case is a set of features, or attribute-valuepairs, that encode the context in which the ambi-guity was encountered.
In general, the context fea-tures represent the state of the parser at the pointof the ambiguity.
In addition, each case is anno-tated with one or more pieces of "class" informa-tion that describe how the ambiguity was resolvedin the current example.
We will refer to these assolution features.
For lexical tagging tasks, for ex-ample, the class information is the syntactic or se-mantic category associated with the current word;for structural attachment decisions, the class in-formation indicates the position of the preferredattachment point.
As cases are created, they arestored in a case base.After training, the system can use the casebase to resolve ambiguities in novel sentences.Whenever the sentence analyzer encounters anambiguity, it creates a problem case, automati-cally filling in its context portion based on thestate of the natural anguage system at the point ofthe ambiguity.
The structure of a problem case isidentical to that of a training case except hat thesolution part of the case is missing.
Next, the caseretrieval algorithm compares the problem case tothose stored in the case base, finds the most similartraining case, and then uses the class informationto resolve the current ambiguity.The experiments described below employ thefollowing case retrieval algorithm:1.
Compare the problem case, X, to each case, Y,in the case base and calculate for each pair:IglZ match(XN,, Ylv,)i=1where N is the set of features used to describeall instances, Ni is the ith feature in the orderedset, XN~ is the value of Ni in the problem case,YN~ is the value of Ni in the training case, andmatch(a, b)is a function that returns 1 if a andb are equal and 0 otherwise.2.
Return the k highest-scoring cases plus any ties.3.
Let the retrieved cases vote on the predictedclass (solution) value and use that value to re-solve the ambiguity for X.
We use a simplemajority vote and break ties randomly.The case retrieval algorithm is essentially a simplek-nearest neighbors algorithm, with minor modi-fications to handle symbolic features.The  Ro le  o f  Representat ion  inCase-Based  Learn ing  o f  L ingu is t i cKnowledgeThis section explores the role of the instance rep-resentation in case-based learning of natural an-guage.
In particular, it should be clear that thebasic case-based learning Mgorithm will performpoorly when cases contain many irrelevant at-tributes (Aha et al, 1991, Aha, 1989).
Unfortu-nately, deciding which features are important fora particular learning task is difficult, especiallywhen interactions among potentially relevant fea-tures are unpredictable.In previous work (Cardie, 1994), for example,we applied the above case-based learning algo-rithm to a number of problems in sentence analysis115both with and without mechanisms for feature setselection.
Table 1 summarizes our results for si-multaneous part-of-speech and semantic lass (i.e.,word sense) tagging.
1 Details regarding the exper-iments are included as part of Table 1.
It showsthat tagging accuracy increases ignificantly whenaccess to the available feature set is appropriatelylimited.
More specifically, each tagging decisionis initially described in the case representation interms of 33 features: 22 local context features en-code syntactic and semantic information for thewords within a five-word window centered on thecurrent word; 11 global context features encodeinformation for any major syntactic constituentsthat have been recognized (e.g., semantic lass andconcept activation information for the subject, di-rect object, verb).
The general idea behind therepresentation f context is to include any infor-mation available to the parser that might be usefulfor inferring the part of speech and semantic fea-tures of the current word.
Results for the CBL al-gorithm using all 33 features are shown in the col-umn labeled "w/o feature selection."
Intuitively,however, it seems that very different subsets ofthe feature set may be useful for part-of-speechprediction and semantic lass prediction.
Not sur-prisingly, the accuracy of the CBL algorithm in-creases when task-specific subsets of the originalfeature set are used instead of all of the availablefeatures (see the last column of Table 1).The task-specific subsets for the lexical tag-ging experiments of Table 1 were obtained auto-matically using the C4.5 decision tree algorithm(Quinlan, 1992) as described in Cardie(1993b).Very briefly, in addition to storing training cases inthe case base, we use them to train a decision treefor each of the selected lexical tasks.
Features thatappear in the pruned decision tree are assumed tobe relevant o the task; features that are missingfrom the tree are assumed to be unnecessary forthe task.
The feature sets proposed by C4.5 reducethe number of attributes used in the case retrievalalgorithm from 33 to an average of 14, 11, and 15features for part-of-speech, general semantic lass,and specific semantic lass tagging, respectively.
2In addition, this automated approach to featureselection outperforms feature sets chosen by hand(Cardie, 1993b): the automated approach locatesfeatures that human experts consider mildly rel-evant to the task at best, but that, in practice,provide statistically reliable cues for the prediction1 Word senses were represented in terms of a two-level domain-specific semantic feature hierarchy.2A more sophisticated variation of this approachhas been used by Daelemans et M. (1993) to provideweights on features rather than to eliminate features.It is able to improve on our semantic feature taggingresults by a few percentage points.Table 1: Results for Lexical Tagging Using Case-Based Learning Wi th  and Wi thout  FeatureSet Selection.
(All experiments draw training and test cases from a base set of 120 sentences from theMUC/TIPSTER Joint Ventures corpus (MUC-5, 1994).
A relatively small corpus was used because thedomain-specific semantic lass tags and the tags for another lexical tagging task (not described here) werenot available as part of any existing annotated corpus and had to be provided manually.
The results presentedare 10-fold cross validation averages using the same breakdown of training/test et cases for each experiment.The parser used to generate training and test cases was the CIRCUS system (Cardie and Lehnert, 1991;Lehnert, 1990).
The case retrieval algorithm was modified slightly to prefer cases among the top k = 10cases that match the current word.
A more detailed escription of the experiments and an analysis of theresults can be found in Cardie(1993a, 1994).
)LexicalTaggingTaskpart-of-speechgeneralsemantic lassspecificsemantic lassNumber Examples of Classof Information w/o feature selectionClasses (% correct)18 noun, gerund,noun modifier, adverb1442 company name,government, factoryCBL Algorithm91.1w/feature selection(% correct)95.0joint venture ntity, 67.1 80.6human, facility73.7 85.5local global!
II II p reced ing  II fo l low ing  I10i il.
itNINttkA~w ~ '~  ' , , ~ .
, , , , ,~  , , > .
z~dz~ozz~oz  ~ o~o~ ~oI 11 syntactic feature semantic featureFigure 1: Histogram of Relevant Context Features for Part-of-Speech Tagging.
(In the graph,prey and/ol  refer to the preceding and following lexical items; gen-sera and spec-sera refer to general andspecific semantic lass values; cn refers to concept/case-frame activation; morphol refers to the morphologyof the word to be tagged; s, do, v, and last-constit refer to the subject, direct object, verb, and last low-levelconstituent (i.e., noun phrase, verb, prepositional phrase), respectively.)116task.
Among the features deemed most importantfor part-of-speech tagging, for example, includedthe general semantic class of the two precedingwords, the general semantic lass of the followingword, and the semantic lass of the subject of thecurrent clause.
This is in addition to more ob-viously relevant features: e.g., the morphology ofthe current wordi the part of speech of the pre-ceding and following words.
A histogram of therelevant features for part-of-speech tagging acrossthe 10 folds of the cross-validation experiments areshown in Figure 1.Based on the experiments described in thissection, we can conclude that the overall accuracyof case-based learning of linguistic knowledge de-pends to a large degree on the feature set used inthe case representation.
Moreover, automatic ap-proaches to feature set selection can outperformfeature sets chosen manually by taking advantageof statistical relationships in the data that are dif-ficult for humans to predict and that may be id-iosyncrasies of the task and data set at hand.Us ing  L ingu is t i c  and  Cogn i t iveB iases  fo r  Feature  Set  Se lec t ionWe saw in the last section that the performancecase-based learning algorithms degrades when fea-tures irrelevant o the learning task are includedin the underlying instance representation.
As a re-sult, the basic CBL algorithm for lexical taggingtasks was augmented with a decision tree algo-rithm whose job it was to discard irrelevant fea-tures from the case representation.
This sectionpresents a new technique for feature set selectionfor case-based learning of natural anguage.
Thenew approach is potentially more powerful thanthe decision tree method in that it can improve abaseline case representation i  three ways ratherthan one:1.
It discards irrelevant features from the represen-tation.2.
It determines the relative importance of relevantfeatures.3.
It has a limited capability for adding new fea-tures when the existing ones are inadequate forthe learning task.Furthermore, the algorithm relies on an inductivebias that may be more appropriate to problemsin natural anguage understanding than the infor-mation gain metric used in the C4.5 decision treesystem: our linguistic bias approach to feature setselection automatically and explicitly encodes anyof a predefined set of linguistic biases and cogni-tive processing limitations into a baseline instancerepresentation.117Thus far, we have incorporated three such bi-ases into the feature set selection algorithm: (1) arecency bias, (2) a restricted memory bias, and (3)a subject accessibility bias.
Modifications to theinstance representation in response to these biaseseither directly or indirectly change the feature setused to describe all instances.
Direct changes tothe representation are made by adding or deletingfeatures; indirect changes modify a weight associ-ated with each feature.In the paragraphs below, we describe these bi-ases and show how they can be used to modify thecase representation for the task of relative pro-noun (RP) disambiguation.
The goal of the learn-ing algorithm for relative pronoun disambiguationis: (1) to determine whether the wh-word is be-ing used as a relative pronoun, and, if it is, (2)to determine which constituents comprise the an-tecedent.
In the sentence,I saw the boy who won the contest.for example, the CBL system must decide that"who" is a relative pronoun that refers to "theboy.
"The baseline instance representation for therelative pronoun task is similar to the one usedfor the lexical tagging tasks.
The main differenceis that additional global context features are in-cluded in the case representation - -  namely, theparser includes one attribute-value pair for everyconstituent in the clause that precedes the relativepronoun.
Figure 2 shows a portion of three relativepronoun disambiguation cases using the baselinecase representation.
Each constituent is describedin terms of its syntactic lass and its position inthe sentence as it was encountered by the CIR-CUS parser.
The value for each feature providesthe phrase's emantic lass.
The class informationassigned to each case describes the location of thecorrect antecedent.
Note that no attachment de-cisions have been made by the parser; these willbe made by learning algorithm as needed.
In ourcurrent implementation, the learning algorithm,rather than the parser, is also responsible for in-terpreting any conjunctions and appositives thatare part of the antecedent as shown in sentences$2 and $3 of Figure 2.The case representation for the RP task cre-ates a minor problem for the CBL algorithm: notwo instances are guaranteed to have the samefeatures.
Sentences that exhibit a direct object,for example, will have a "direct object" feature;sentences that have no direct object will containno "direct object" feature.
As a result, we re-quire that all instances are described in terms ofa normalized set of features.
To do this, the al-gorithm keeps track of every attribute that oc-curs in the training instances and augments theSI: \[The man\] \[from Oklahoma\] [,\] who ...features: (s human) (s-ppl location) (pvevl-syntactic-type comma) ...class: s(The antecedent is the subject.
)S2: \[I\] \[thank\] [Nike\] \[andl \[Reebok\] \[,\] who ...features: (s human) (v exists) (do name) (do-up1 name) (prevl-syntactic-type comma)...class: do -t- do-up1(The antecedent involves two constituents.
)83: \[I\] \[thank\] \[our sponsor\] [,\] \[GE\] \[,\] who ...features: (s human) (v exists) (do entity) (do-up1 name) (prevl-syntactic-type comma)...class: do-up1 V do(There are two semantically legal antecedents.
)Figure 2: Basel ine Ins tance  Representation.training and test instances to include every fea-ture of the normalized feature set, filling in a nilvalue if the feature does not apply for the particu-lar instance.
Unfortunately, this means that mostof the features in a normalized case will be oneof these "missing features."
To ensure that thecase retrieval algorithm focuses on features thatare present rather than missing from the problemcase, we also modify the original case retrieval al-gorithm to award full credit for matches on fea-tures present in the problem case and to allow par-tial credit for matches on missing features.
Thisis accomplished by associating with each featurea weight that indicates the importance of the fea-ture in determining case similarity and by using aweighted nearest-neighbor case retrieval algorithm:1.
Set the weight, wl, associated with each feature,f ,  in the normalized feature set3:w I = 0.2 if ff is missing from the(unnormalized) problem case,w/ = 1 otherwise.2.
Compare the problem case, P, to each trainingcase, T, in the case base and calculate, for eachpair:IglWN, * match(PN,, TNi)i=1where N is the normalized feature set, Ni isthe ith feature in N, PN~ is the value of Ni inthe problem case, TN~ is the value of Ni in thetraining case, and match(a, b) is a function thatreturns 1 if a and b are equal and 0 otherwise.3.
Return the case with the highest score as wellas all ties.4.
Let the retrieved cases vote on the value of theantecedent.
Again, we use a simple majorityvote and break ties randomly.3A number of other values for the missing featuresweight were tested as well.118Results using this 1-nearest neighbor CBL al-gorithm for relative pronoun disambiguation us-ing the baseline case representation are shown inTable 2.
For these experiments, we drew train-ing and test cases (241 instances) from MUC-3texts that describe Latin American terrorist events(Chinchor et al, 1993).
As above, all results are10-fold cross validation averages and the parserused to generate training and test cases was theCIRCUS system.
The performance of the CBL al-gorithm is compared to that of: (1) a default rulethat always chooses the most recent phrase as theantecedent, and (2) a set of hand-coded heuristicsdeveloped for the same task specifically for usein the terrorism domain.
Chi-square significancetests indicate: (1) that the hand-coded heuristicsperform better (at the 95% level) than the defaultrule and (2) that the CBL system is not signifi-cantly different from either the default rule or thehand-coded heuristics.Table 2: Relative Pronoun DisambiguationUsing CBL  Without Feature Set Selection.% correct)CBL Default i ~ -Algorithm Strategy Heuristicsw/o  featureset selection76.2 74.3 80.5In the sections below, we describe the recencybias, the restricted memory bias, and the subjectaccessibility bias in turn.
We show how each biascan be used to automatically modify the base-line case representation and measure the effectsof those modifications on the learning algorithm'sability to predict relative pronoun antecedents.Experiments will show that the changes in rep-resentation engender a 21.7% increase in accu-racy, raising the performance of the CBL algo-rithm from 69.2% correct to 84.2%.
In all ex-periments below, the same ten training and testset combinations as in the baseline xperiments ofTable 2 will be used.
This procedure nsures thatdifferences in performance are not attributable tothe random partitions chosen for the test set.Incorporat ing  the  Recency  B iasIn processing language, people consistently showa bias towards the use of the most recent infor-mation (e.g., Ffazier and Fodor (1978), Gibson(1990), Kimball: (1973), Nicol (1988)).
In par-ticular, Cuetos and Mitchell (1988), Frazier andFodor (1978), and others have investigated the im-portance of recency in finding the antecedents ofrelative pronouns.
They found that there is a pref-erence for choosing the most recent noun phrasein sentences of the form NP V NP OF-PP, withambiguous relative pronoun antecedents, e.g.
:The journalist interviewed the daughter of thecolonel who had had the accident.In addition, Gibson et al (1993) looked atphrases of the form: NP1 PREP NP2 OF NP3RELATIVE-CLAUSE,.
E.g.,...the lamps near the paintings of the house thatwas damaged in the flood....the lamps near the painting of the houses thatwas damaged in the flood....the lamp near the paintings of the houses thatwas damaged in the flood.He found that the most recent noun phrase (NP3)was initially preferred as the antecedent and thatrecognizing antecedents in the NP2 and NP1 po-sitions were significantly harder than recognizingthe most recent noun phrase as the antecedent.We translate this recency bias into representa-tional changes for the training and problem casesin two ways.
The first is a direct modification tothe attributes that comprise the case representa-tion, and the second modifies the weights to in-dicate a constituent's distance from the relativepronoun.In the first approach, we label the each con-stituent feature by its position relative to the rel-ative pronoun.
This establishes a right-to-left la-beling of constituents rather than the left-to-rightlabeling that the baseline representation incorpo-rates.
In Figure 3, for example, "in Congress" re-ceives the attribute ppl in the right-to-left labelingbecause it is a prepositional phrase one positionto the left of "who."
Similarly, "the hardliners"receives the attribute np2 because it is a nounphrase two positions to the left of "who."
Theright-to-left ordering yields a different feature setand, hence, a different case representation.
For ex-119ample, the right-to-left labeling assigns the sameantecedent value (i.e., ppP) to both of the followingsentences:?
"it was a message from the hardliners inCongress, who..."?
"it was from the hardliners in Congress, who..."The baseline (left-to-right) representation, on theother hand, labels the antecedents with distinctattributes - -  do-ppl and v-ppl, respectively.In the second approach to incorporating therecency bias, we increment the weight associatedwith each constituent as a function of its proximityto the relative pronoun (see Table 3).
The featureassociated with the constituent farthest from therelative pronoun receives a weight of one, and theweights are increased by one for each subsequentconstituent.
All features added to the case as aresult of feature normalization (not shown in Ta-ble 3) receive a weight of one.Table 3: Incorporat ing  the  Recency  Bias byMod i fy ing  the  Weight  Vector .Itwasthe hardlinersin Congresswho...Feature  Base-l ineweights 1v 1do 1do-ppl 1Re-cencyweightThe results of experiments hat use each of therecency representations separately and in a com-bined form are shown in Table 4.
To combine thetwo implementations of the recency bias, we firstrelabel the attributes of a case using the right-to-left labeling and then initialize the weight vectorusing the recency weighting procedure describedabove.
The table shows that the recency weightingrepresentation alone tends to degrade prediction ofrelative pronoun antecedents as compared to thebaseline CBL system.
Both the right-to-left label-ing and combined representations improve perfor-mance - -  they perform significantly better thanthe default heuristic, but do not yet exceed thelevel of the hand-coded heuristics.
The final rowof results will be described below.As shown in Table 4, the combined recencybias outperforms the right-to-left labeling despitethe fact that the recency weighting tends to lowerthe accuracy of relative pronoun antecedent pre-diction when used alone.
The right-to-left label-ing appears to provide a representation f the lo-cal context of the relative pronoun that is criticalfor finding antecedents.
The disappointing perfor-Sentence:basel ine representation:r ight - to - le f t  label ing:\[It\[ \[was\] [the hardliners\] [in Congress\] [,\] who ...(s entity) (v exists) (do human) (do-ppl entity) (prevl-syntactic-typeprep.phrase) ... (class do)(s entity) (v exists) (np2 human) (ppl entity) (prevl-syntactic-typeprep-phrase) ... (class np2)Figure 3: Incorporat ing  the Recency Bias Using a R ight - to -Le f t  Label ing.Table 4: Resu l ts  for the  Recency  Bias Representations.\[ Case Representation \[ % Cor rect  \[Baseline Representation 76.2(no feature selection)R-to-L Labeling 79.2Recency Weighting 75.8R-to-L + RecWt 80.0Hand-Coded Heuristics 80.5Default Heuristic 74.3Baseline Representation 69.2w/o built-in recency biasmance of the recency weighting representation, onthe other hand, may be caused by (1) its lack ofsuch a representation f local context, and (2) itsbias against antecedents hat are distant from therelative pronoun (e.g., "...to help especially thosepeople living in the Patagonia region of Argentina,who are being treated inhumanely...").
Nineteenof the 241 cases have antecedents hat include theoften distant subject of the preceding clause.Furthermore, the recency bias performs wellin spite of the fact that the baseline represen-tation already provides a built-in recency bias.The baseline represents the constituent that pre-cedes the relative pronoun up to three times inthe baseline representation - -  as a constituent fea-ture (e.g., "direct object") and via the "last con-stituent" global context features.
4 The last rowin Table 4 shows the performance of the baselinerepresentation when this built-in bias is removedby discarding the last-constituent features.Incorporating the Rest r i c ted  MemoryBiasPsychological studies have determined that peo-ple can remember at most seven plus or minustwo items at any one time (Miller, 1956).
More re-cently, Daneman and Carpenter (1983, 1980) showthat working memory capacity affects a subject'sability to find the referents of pronouns over vary-4This means that when the constituent immedi-ately preceding "who" in the problem case and a train-ing case match, that constituent accounts for a greaterpercentage of the similarity score than does any otherconstituent.120ing distances.
King and Just (1991) show thatdifferences in working memory capacity can causedifferences in the reading time and comprehensionof certain classes of relative clauses.
Moreover,it has been hypothesized that language learningin humans is successful precisely because limitson information processing capacities allow chil-dren to ignore much of the linguistic data they re-ceive (Newport, 1990).
Some computational lan-guage learning systems (e.g., Elman (1990)) actu-ally build a short term memory directly into thearchitecture of the system.Our baseline case representation does not nec-essarily make use of this restricted memory bias,however.
Each case is described in terms of thenormalized feature set, which contains an aver-age of 38.8 features.
Unfortunately, incorporat-ing the restricted memory limitations into the caserepresentation is problematic.
Previous restrictedmemory studies (e.g., short term memory stud-ies) do not state explicitly what the memory limitshould be - -  it varies from five to nine dependingon the cognitive task and depending on the sizeand type of the "chunks" that have to be remem-bered.
In addition, the restricted memory biasalone does not state which chunks, or features, tokeep and which to discard.To apply the restricted memory bias to thebaseline case representation, we let n represent thememory limit and, in each of five runs, set n to oneof five, six, seven, eight, or nine.
Then, for eachtest case, the system randomly chooses n featuresfrom the normalized feature set, sets the weightsassociated with those features to one, and sets theremaining weights to zero.
This effectively dis-Table 5: Results for the Restr icted Memory Bias Representation.
(%correct, *'s indicate significancewith respect o the original baseline result shown in boldface, * ~ p = 0.05)Memory LimitnoneBaseline776.278.374.276.275.875.0R-to-L + RecWt80.081.2"81.2"80.080.481.7*cards all but the n selected features from the caserepresentation.
Results for the restricted mem-ory bias representation are shown in Table 5.
Thefirst column of results shows the effect of mem-ory limitations on the baseline representation.
Igeneral, the restricted memory bias with randomfeature selection degrades the ability of the sys-tem to predict relative pronoun antecedents al-though none of the changes is statistically signifi-cant.
This is not surprising iven that the currentimplementation f the bias is likely to discard rel-evant features as well as irrelevant features.
Weexpect hat this bias will have a positive impacton performance only when it is combined with lin-guistic biases that provide feature relevancy infor-mation.
This is, in fact, the case: the final columnin Table 5 shows the effect of restricted memorylimitations on the combined recency representa-tion.
To incorporate the restricted memory biasand the combined recency bias into the baselinecase representation, we(1) apply the right-to-leftlabeling, (2) rank the features of the case accord-ing to the recency weighting, and (3) keep the nfeatures with the highest weights (where n is thememory limit).
Ties are broken randomly.We expected: the merged representation toperform rather well because the combined recencybias representation worked well on its own and be-cause the restricted memory (RM) bias essentiallydiscards features that are distant from the relativepronoun and rarely included in the antecedent.As shown in the last column of Table 5, four outof five RM/recency variations posted higher ac-curacies than the combined recency representa-tion.
In fact, three of the RM/recency represen-tations now outperform the original baseline rep-resentation (shown in boldface) at the 95% sig-nificance level.
(Until this point, the best rep-resentation had been the combined recency rep-resentation, which significantly outperformed thedefault heuristic, but not the baseline case repre-sentation.
)121I ncorporat ing  the  Sub jec t  Access ib i l i tyBiasA number of studies in psycholinguistics havenoted the special importance ofthe first item men-tioned in a sentence.
In particular, it has beenshown that the accessibility of the first discourseobject, which very often corresponds to the sub-ject of the sentence, remains high even at the endof a sentence (Gernsbacher tal., 1989).
This sub-ject accessibility bias is an example of a more gen-eral focus of attention bias.
In vision learningproblems, for example, the brightest object in viewmay be a highly accessible object for the learningagent; in aural tasks, very loud or high-pitchedsounds may be highly accessible.
We incorpo-rate the subject accessibility bias into the baselinerepresentation byincreasing the weight associatedwith the constituent attribute that represents hesubject of the clause preceding the relative pro-noun whenever that feature is part of the normal-ized feature set.Table 6: Results for the Subject AccessibilityBias Representation.
(% correct)Baseline 76.2Baseline, SubjWt=2 75.0Baseline, SubjWt=5 74.2Baseline, SubjWt=7 73.7Baseline, SubjWt=10 73.3Table 6 shows the effects of allowing matcheson the subject attribute to contribute two, five,seven, and ten times as much as they did in thebaseline representation.
The weights were chosenmore or less arbitrarily.
Results indicate that in-corporation of the subject accessibility bias neverimproves performance of the learning algorithm,although dips in performance are never statisti-cally significant.
At first it may seem surprisingthat this bias does not result in a better repre-sentation.
Like the recency bias, however, thebaseline representation already encodes the sub-ject accessibility bias by explicitly recognizing thesubject as a major constituent ofthe sentence (i.e.,"s") rather than by labeling it merely as a low-levelnoun phrase (i.e., "np").
It may be that this built-in encoding of the bias is adequate or that, like therestricted memory bias, additional modificationsto the baseline representation are required beforethe subject accessibility bias can have a positiveeffect on the learning algorithm's ability to findrelative pronoun antecedents.Table 7 shows the effects of merging the sub-ject accessibility bias with both recency biases andthe restricted memory bias (RM).
The results inthe first column (Baseline) are just the resultsfrom Table 6 - -  they indicate the performance ofthe baseline case representation with various levelsof the subject accessibility bias.
The second col-umn shows the effect of incorporating the subjectaccessibility bias into the combined recency biasrepresentation.
To create this merged represen-tation, we first establish the right-to-left labelingof features and then add together the weight vec-tors recommended by the recency weighting andsubject accessibility biases.
As was the case withthe baseline representation, i corporation of thesubject accessibility bias steadily decreases perfor-mance of the learning algorithm as the weight onthe subject constituent is increased.
None of thechanges is statistically significant.The remaining five columns of Table 7 showthe effects of incorporating all three linguistic bi-ases into the baseline case representation.
Tocreate this representation, we (1) relabel the at-tributes using the right-to-left labeling, (2) incor-porate the subject and recency weighting repre-sentations by adding the weight vectors proposedby each bias, (3) apply the restricted memory biasby keeping only the n features with the highestweights (where n is the memory limit) and choos-ing randomly in case of ties.
Results for these ex-periments indicate that some combinations of thelinguistic bias parameters work very well togetherand others do not.
In general, associating a weightof two with the subject constituent improves theaccuracy of the learning algorithm as compared tothe corresponding representation that omits thesubject accessibility bias.
(Compare the first andsecond rows of results).
In particular, three repre-sentations (shown in italics) now outperform thebest previous representation (which had the r-to-1 labeling, recency weighting, memory limit = 5and achieved 81.7% correct).
In addition, thebest-performing representation now outperformsthe hand-coded relative pronoun disambiguationrules (84.2% vs. 80.5%) at the 90% significancelevel.In summary, this section presented a linguisticbias approach to feature set selection and appliedit to the problem of finding the antecedent of the122relative pronoun "who."
Our experiments showedthat performance of the case-based learning algo-rithm steadily improved as each of the availablelinguistic biases was used to modify the baselinecase representation.
Although one would not ex-pect monotonic improvement to continue forever,it is clear that explicit incorporation of linguis-tic biases into the case representation can improvethe learning algorithm performance for the relativepronoun disambiguation task.
Table 8 summarizesthese results.
When all three biases are includedin the case representation, the learning algorithmperforms ignificantly better than the hand-codedrules (84.2% correct vs. 80.5% correct) at the 90%confidence level.D iscuss ion  and  Conc lus ionsIt should be emphasized that modifications to thebaseline case representation i  response to eachof the individual linguistic biases are performedautomatical ly  b the CBL system, subject o theconstraints provided in Table 9.
Upon invocationof the CBL algorithm, the user need only specify(1) the names of the biases to incorporate intothe case representation, and (2) any parametersrequired for those biases (e.g., the memory limitfor the restricted memory bias).In addition, the linguistic bias approach tofeature set selection relies on the following gen-eral procedure when incorporating more than onelinguistic bias into the baseline representation:1.
First, incorporate any bias that relabels at-tributes (e.g., r-to-1 labeling).2.
Then, incorporate biases that modify featureweights by adding the weight vectors proposedby each bias (e.g., recency weighting, subjectaccessibility bias).3.
Finally, incorporate biases that discard features(e.g., restricted memory bias), but give pref-erence to those features assigned the highestweights in Step 2.Thus far, we have implemented just three lin-guistic biases, all of which represent broadly ap-plicable cognitive processing limitations.
We ex-pect that additional biases will be needed to han-dle new natural language l arning tasks, but that,in general, arelatively small set of linguistic biasesshould be adequate for handling large number ofproblems in natural language learning.
Examplesof other useful inguistic biases to make availableinclude: minimal attachment, right association,lexical preference biases, and a syntactic structureidentity bias.One important problem that we have not ad-dressed is how to select automatically the com-bination of linguistic biases that will achieve theTable 7: Add i t iona l  Resu l ts  for  the  Sub ject  Accessibi l i ty Bias Representat ion .
(% correct, *'sindicate significance with respect o the original baseline result shown in boldface,  ?
~ p = 0.05, ** --, p =0.01; RM refers to the memory limit).SubjectWeightnone2SubjAccR-to-LRecWtRM=5Baseline SubjAccR-to-LRecWt76.2 80.075.0 79.674.2 78.373.7 77.573.3 76.781.7"84.2**79.679.6SubjAccR-to-LRecWtRM=680.482.5*SubjAcc SubjAccR-to-L R-to-LRecWt RecWtRM=7 RM=880.082.1 *78.377.981.2"81.2"80.476.7SubjAccR-to-LRecWtRM=981.2"80.879.677.910 79.6 79.2 78.3 80.4 79.6Table 8: Summary  of  L inguist ic  Bias Resul ts .Case Representat ion  % Cor rectBaseline w/o Built-in Recency Bias 69.2Default Heuristic: Choose Most Recent Phrase 74.3Baseline 76.2Baseline+ Recency Bias 80.0Hand-Coded Heuristics 80.5Baseline+ Recency Bias+ Restricted Memory Bias (limit=5) 81.7Baseline+ Recency Bias+ Restricted Memory Bias (limit=5)+ Subject Accessibility Bias (subj wt=2) 84.2TableBias Assumpt ionsRecency(r-to-1 labeling)Recency(recency weighting)Attribute names indicaterecencyAttributes in original caseare provided in inverserecency order9: Linguistic Bias Modif icat ions.ParametersFunction mapping originalattribute names to newattribute namesNoneRestricted Memory NoneFocus of Attention None(subject accessibility)memory limitWeight factor, attributeassociated with object offocus, e.g., the subject123best performance for a particular natural languagelearning task.
Our current approach assumes thatthe expert knowledge of computational linguistsis easier to apply at the level of linguistic bias se-lection than at the feature set selection level - -so at the very least, this expert knowledge can beused to seed the bias selection algorithm.
For therelative pronoun task, for example, we assumedthat all three linguistic biases were relevant andthen exhaustively enumerated all combinations ofthe biases, choosing the combination that per-formed best in cross-validation testing.
Becausethis method will get quickly out of hand as ad-ditional biases are included or parameters tested,future work should investigate l ss costly alterna-tives to linguistic bias selection.In addition, we have tested the linguistic biasapproach to feature selection on just one naturallanguage learning task.
We believe, however, thatit offers a generM approach for case-based learningof natural language.
In theory, it allows system de-velopers to use the same underlying case represen-tation for a variety of problems in NLP rather thandeveloping a new representation as each new taskis tackled.
The underlying case representationonly has to change when new knowledge sourcesbecome available to the NLP system in which theCBL system is embedded.
Hence, the baselinecase representation is parser-dependent (i.e., NLPsystem-dependent) ra her than task-dependent.In particular, we are currently applying thelinguistic bias CBL approach to the problem ofgeneral pronoun resolution.
While it appears thatour existing linguistic bias set will be of use, we be-lieve that the CBL system will benefit from addi-tional inguistic biases.
Centering constraints ( eeBrennan et al, 1987), for example, can be encodedas linguistic biases and applied to the pronoun res-olution task to increase system performance.Furthermore, we have focused on applying thelinguistic bias approach to feature set selectionfor case-based learning algorithms only.
In futurework, we plan to investigate the use of the ap-proach for feature selection in conjunction withother standard machine learning algorithms.
Herewe expect hat very different manipulations of thebaseline case representation will be needed to im-plement the linguistic biases presented in this pa-per.Finally, the viability of both the linguisticbias approach to feature set selection and the gen-eral CBL approach to natural anguage learningmust be tested using much larger corpora.
Exper-iments on case-based part-of-speech tagging by re-searchers at Tilburg University (Daelemans etal.,submitted), however, indicate that the CBL ap-proach to natural anguage learning will scale to124much larger data sets.In summary, this paper begins to address theissue of "algorithm vs. representation" for case-based learning of linguistic knowledge.
We haveshown empirically that the feature set used to de-scribe training and test instances plays an impor-tant role for a number of tasks in natural an-guage understanding.
In addition, we have pre-sented an automated approach to feature set se-lection for case-based learning of linguistic knowl-edge.
The approach takes a baseline case represen-tation and modifies it in response to one of threelinguistic biases by adding, deleting, and weight-ing features appropriately.
We applied the tech-nique to the task of relative pronoun disambigua-tion and found that the case-based learning al-gorithm improves as relevant biases are used tomodify the underlying case representation.
Fi-nally, we have argued that the linguistic bias ap-proach to feature set selection offers new possibil-ities for case-based learning of natural anguage.It simplifies the process of designing an appro-priate instance representation forindividual natu-ral language l arning tasks because system devel-opers can safely include in the baseline instancerepresentation features for all available knowledgesources.
In the long run, it may obviate the needfor separate instance representations for each lin-guistic knowledge acquisition task.
More impor-tantly, the linguistic bias CBL approach to naturallanguage learning offers a mechanism for explic-itly combining the frequency information availablefrom corpus-based techniques with linguistic biasinformation employed in traditional linguistic andknowledge-based approaches to natural anguageprocessing.Re ferences(Aha et al, 1991) D. Aha, D. Kibler, and M. Al-bert.
1991.
Instance-Based Learning Algo-rithms.
Machine Learning, 6(1):37-66.
(Aha, 1989) D. Aha.
1989.
Instance-Based Learn-ing Algorithms.
In Proceedings of the Sixth In-ternational Conference on Machine Learning,pages 387-391, Cornell University, Ithaca, NY.Morgan Kaufmann.
(Almuallim and Dietterich, 1991) H. Almual-lim and T. G. Dietterich.
1991.
Learning WithMany Irrelevant Features.
In Proceedings of theNinth National Conference on Artificial Intel-ligence, pages 547-552, Anaheim, CA.
AAAIPress / MIT Press.
(Aone and Bennett, 1995) Chinatsu Aoneand William Bennett.
1995.
Evaluating Au-tomated and Manual Acquisition of AnaphoraResolution Strategies.
In Proceedings of the 33rdAnnual Meeting of the A CL, pages 122-129.
As-sociation for Computational Linguistics.
(Bosch and Daelemans, 1993) A. van den Boschand W. Daelemans.
1993.
Data-oriented meth-ods for grapheme-to-phoneme conversion.
InProceedings of European Chapter of ACL, pages45-53, Utrecht.
Also available as ITK ResearchReport 42.
(Brennan et al, 1987) Susan E. Brennan, Mari-lyn Walker Friedman, and Carl J. Pollard.
1987.A Centering Approach to Pronouns.
In Proceed-ings of the 25th Annual Meeting of the ACL.Association for Computational Linguistics.
(Cardie and Lehnert, 1991) C. Cardie andW.
Lehnert.
1991.
A Cognitively Plausible Ap-proach to Understanding Complicated Syntax.In Proceedings of the Ninth National Conferenceon Artificial Intelligence, pages 117-124, Ana-heim, CA.
AAAI Press / MIT Press.
(Cardie, 1992a) C. Cardie.
1992a.
Corpus-BasedAcquisition of Relative Pronoun Disambigua-tion Heuristics.
In Proceedings of the 30th An-nual Meeting of the ACL, pages 216-223, Uni-versity of Delaware, Newark, DE.
Associationfor Computational Linguistics.
(Cardie, 1992b) C. Cardie.
1992b.
Learning toDisambiguate Relative Pronouns.
In Proceed-ings of the Tenth National Conference on Arti-ficial Intelligence, pages 38-43, San Jose, CA.AAAI Press / MIT Press.
(Cardie, 1993a)C. Cardie.
1993a.
A Case-Based Approach to Knowledge Acquisition forDomain-Specific Sentence Analysis.
In Proceed-ings of the Eleventh National Conference on Ar-tificial Intelligence, pages 798-803, Washington,DC.
AAAI Press / MIT Press.
(Cardie, 1993b) C. Cardie.
1993b.
Using De-cision Trees to Improve Case-Based Learning.In P. Utgoff, editor, Proceedings of the TenthInternational Conference on Machine Learn-ing, pages 25-32, University of Massachusetts,Amherst, MA.
Morgan Kaufmann.
(Cardie, 1994) C. Cardie.
1994.
Domain-SpecificKnowledge Acquisition for Conceptual SentenceAnalysis.
Ph.D. thesis, University of Mas-sachusetts, Amherst, MA.
Available as Univer-sity of Massachusetts, CMPSCI Technical Re-port 94-74.
(Chinchor et al, 1993) N. Chinchor,L.
Hirschman, and D. Lewis.
1993.
Evaluat-ing Message Understanding Systems: An Anal-ysis of the Third Message Undestanding Con-ference (MUC-3).
Computational Linguistics,19(3):409-449.
(Cuetos and Mitchell, 1988) F. Cuetos and D. C.Mitchell.
1988.
Cross-Linguistic Differences125in Parsing: Restrictions on the Use of theLate Closure Strategy in Spanish.
Cognition,30(1):73-105.
(Daelemans etal., 1994) W. Daelemans,G.
Durieux, and S. Gillis.
1994.
The Acquisitionof Stress: A Data-Oriented Approach.
Compu-tational Linguistics, 20(3):421-451.
(Daelemans etal., submitted)W. Daelemans, J. Zavrel, Berck P., and GillisS.
submitted.
Memory-Based Part of SpeechTagging.
Tilburg University.
(Daneman and Carpenter, 1980) M. Dane-man and P. A. Carpenter.
1980.
IndividualDifferences in Working Memory and Reading.Journal of Verbal Learning and Verbal Behav-ior, 19:450-466.
(Daneman and Carpenter, 1983)M. Daneman and P. A. Carpenter.
1983.
In-dividual Differences in Integrating InformationBetween and Within Sentences.
Journal of Ex-perimental Psychology: Learning, Memory, andCognition, 9:561-584.
(Elman, 1990) J. Elman.
1990.
Finding Structurein Time.
Cognitive Science, 14:179-211.
(Frazier and Fodor, 1978)L. Frazier and J. D.Fodor.
1978.
The Sausage Machine: A NewTwo-Stage Parsing Model.
Cognition, 6:291-325.
(Gernsbacher tal., 1989) M. A. Gernsbacher,D.
J. Hargreaves, and M. Beeman.
1989.
Build-ing and Accessing Clausal Representations: TheAdvantage of First Mention Versus the Advan-tage of Clause Recency.
Journal of Memory andLanguage, 28:735-755.
(Gibson et al, 1993) E. Gibson, N. Pearlmutter,E.
Canseco-Gonzalez, and G. Hickok.
1993.Cross-linguistic Attachment Preferences: Evi-dence from English and Spanish.
In Sixth An-nual CUNY Sentence Processing Conference,University of Massachusetts, Amherst, MA.Only abstract in the Sentence Processing Con-ference proceedings.
Full manuscript to appearin journal.
(Gibson, 1990) E. Gibson.
1990.
Recency Prefer-ences and Garden-Path Effects.
In Proceedingsof the Twelfth Annual Conference of the Cog-nitive Science Society, Massachusetts Instituteof Technology, Cambridge, MA.
Lawrence Erl-baum Associates.
(Kimball, 1973) J. Kimball.
1973.
Seven Prin-ciples of Surface Structure Parsing in NaturalLanguage.
Cognition, 2:15-47.
(King and Just, 1991)J.
King and M. A. Just.1991.
Individual Differences in Syntactic Pro-cessing: The Role of Working Memory.
Journalof Memory and Language, 30:580-602.
(Kolodner, 1993) J. Kolodner.
1993.
Case-BasedReasoning.
Morgan Kaufmann, San Mateo, CA.
(Langley and Sage, in press)P. Langley and S. Sage.
in press.
Scaling todomains with irrelevant features.
In R. Greiner,editor, Computational learning theory and natu-ral learning systems, volume 4.
The MIT Press,Cambridge, MA.
(Lehnert, 1990) W. Lehnert.
1990.
Sym-bolic/Subsymbolic Sentence Analysis: Exploit-ing the Best of Two Worlds.
In J. Barndenand J. Pollack, editors, Advances in Connec-tionist and Neural Computation Theory, pages135-164.
Ablex Publishers, Norwood, NJ.
(Litman, 1994) Diane J. Litman.
1994.
Classify-ing Cue Phrases in Text and Speech Using Ma-chine Learning.
In Proceedings of the TwelfthNational Conference on Artificial Intelligence,pages 806-813.
AAAI Press / MIT Press.
(Magerman, 1995) David M. Magerman.
1995.Statistical Decision-Tree Models for Parsing.
InProceedings of the 33rd Annual Meeting of theACL, pages 276-283.
Association for Computa-tional Linguistics.
(McCarthy and Lehnert, 1995)Joseph F. McCarthy and Wendy G. Lehnert.1995.
Using Decision Trees for Coreference Res-olution.
In C. Mellish, editor, Proceedings of theFourteenth International Conference on Artifi-cial Intelligence, pages 1050-1055.
(Miller, 1956) G. A. Miller.
1956.
The MagicalNumber Seven, Plus or Minus Two: Some Lim-its on our Capacity for Processing Information.Psychological Review, 63(1):81-97.
(MUC, 1994) 1994.
Proceedings of the Fifth Mes-sage Understanding Conference (MUC-5}.
Mor-gan Kaufmann, San Marco, CA.
(Newport, 1990) E. Newport.
1990.
MaturationalConstraints on Language Learning.
CognitiveScience, 14:11-28.
(Nicol, 1988) J. Nicol.
1988.
Coreference Pro-cessing During Sentence Comprehension.
Ph.D.thesis, Massachusetts Institute of Technology,Cambridge, MA.
(Pereira, 1994) F. Pereira.
1994.
Frequencies vsBiases: Machine learning problems in natu-ral language processing.
In Proceedings of theEleventh International Conference on MachineLearning, page 380, Rutgers University, NewBrunswick, NJ.
Morgan Kaufmann.
(Quinlan, 1983) J. R. Quinlan.
1983.
Learning Ef-ficient Classification Procedures and Their Ap-plication to Chess End Games.
In R. S. Michal-ski, J. G. Carbonell, and T. M. Mitchell, ed-itors, Machine Learning: An Artificial lntelli-126gence Approach.
Morgan Kaufmann, San Ma-teo, CA.
(Quinlan, 1992) J. R. Quinlan.
1992.
C4.5: Pro-grams for Machine Learning.
Morgan Kauf-mann, San Mateo, CA.
(Riesbeck and Schank, 1989) C. Riesbeck andR.
Schank.
1989.
Inside Case-Based Reason-ing.
Erlbaum, Northvale, NJ.
(Riloff and Lehnert, 1994) E. Riloff and W. Lehn-ert.
1994.
Information extraction as a basis forhigh-precision text classification.
ACM Trans-actions on Information Systems, 12(3):296-333.
(Simmons and Yu, 1992) Robert F. Simmons andYeong-Ho Yu.
1992.
The Acquisition and Useof Context-Dependent Grammars for English.Computational Linguistics, 18(4):391-418.
(Stanfill and Waltz, 1986) C. Stanfill andD.
Waltz.
1986.
Toward Memory-based Rea-soning.
Communications of the ACM, 29:1213-1228.
(Zelle and Mooney, 1993) J. Zelle and R. Mooney.1993.
Learning Semantic Grammars with Con-structive Inductive Logic Programming.
In Pro-ceedings of the Eleventh National Conference onArtificial Intelligence, pages 817-822, Washing-ton, DC.
AAAI Press / MIT Press.
(Zelle and Mooney, 1994) J. Zelle and R. Mooney.1994.
Inducing Deterministic Prolog Parsersfrom Treebanks: A Machine Learning Ap-proach.
In Proceedings of the Twelfth NationalConference on Artificial Intelligence, pages748-753, Seattle, WA.
AAAI Press / MIT Press.
