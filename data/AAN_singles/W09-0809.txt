Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69?77,Athens, Greece, 31 March, 2009. c?2009 Association for Computational LinguisticsSyntactic Reordering for English-ArabicPhrase-Based Machine TranslationJakob ElmingLanguagelensCopenhagen, Denmarkje@languagelens.comNizar HabashCenter for Computational Learning SystemsColumbia University, New York, USAhabash@ccls.columbia.eduAbstractWe investigate syntactic reordering withinan English to Arabic translation task.
Weextend a pre-translation syntactic reorder-ing approach developed on a close lan-guage pair (English-Danish) to the dis-tant language pair, English-Arabic.
Weachieve significant improvements in trans-lation quality over related approaches,measured by manual as well as automaticevaluations.
These results prove the viabil-ity of this approach for distant languages.1 IntroductionThe emergence of phrase-based statistical ma-chine translation (PSMT) (Koehn et al, 2003a)has been one of the major developments in statis-tical approaches to translation.
Allowing transla-tion of word sequences (phrases) instead of singlewords provides PSMT with a high degree of ro-bustness in word selection and in local-word re-ordering.
Recent developments have shown thatimprovements in PSMT quality are possible us-ing syntax.
One such development is the pre-translation reordering approach, which adjusts thesource sentence to resemble target-language wordorder prior to translation.
This is typically doneusing rules that are either manually created orautomatically learned from word-aligned parallelcorpora.One particular variety of this approach, pro-posed by Elming (2008), uses a large set oflinguistic features to automatically learn re-ordering rules.
The rules are applied non-deterministically; however, phrase-internal word-alignments are used to ensure that the intended re-ordering does not come undone because of phraseinternal reordering (Elming, 2008).
This approachwas shown to produce improved MT output onEnglish-Danish MT, a relatively closely-relatedand similarly-structured language pair.
In thispaper, we study whether this approach can beextended to distant language pairs, specificallyEnglish-to-Arabic.
We achieve significant im-provement in translation quality over related ap-proaches, measured by manual as well as auto-matic evaluations on this task.
This proves theviability of this approach on distant languages.We also examined the effect of the alignmentmethod on learning reordering rules.
Interestingly,our experiments produced better translation usingrules learned from automatic alignments than us-ing rules learned from manual alignments.In the next section, we discuss and contrast re-lated work.
Section 3 describes aspects of Englishand Arabic structure that are relevant to reorder-ing.
Section 4 describes the automatic inductionof reordering rules and its integration in PSMT.
Insection 5, we describe the SMT system used in theexperiments.
In section 6, we evaluate and discussthe results of our English-Arabic MT system.2 Related WorkMuch work has been done in syntactic reorder-ing for SMT, focusing on both source and target-language syntax.
In this paper, we adapt an ap-proach that utilizes source-syntax information asopposed to target-side syntax systems (Yamadaand Knight, 2001; Galley et al, 2004).
This isbecause we are translating from English to Arabicand we are discouraged by recent results indicat-ing Arabic parsing is not at a stage that makes itusable in MT (Habash et al, 2006).
While sev-eral recent authors using a pre-translation (source-side) reordering approach have achieved positiveresults, it has been difficult to integrate syntactic69information while retaining the strengths of thestatistical approach.
In some studies, reorderingdecisions are done ?deterministically?
by supply-ing the decoder with a canonical word order (Xiaand McCord, 2004; Collins et al, 2005; Wanget al, 2007; Habash, 2007).
These reorderingrules are either manually specified or automati-cally learned from alignments; and they are al-ways placed outside the actual PSMT system.
Bycontrast, other studies (Crego and Mari?o, 2007;Zhang et al, 2007; Li et al, 2007; Elming, 2008)are more in the spirit of PSMT, in that multi-ple reorderings are presented to the PSMT sys-tem as (possibly weighted) options that are al-lowed to contribute alongside other parameters.Specifically, we follow the pre-translation reorder-ing approach of Elming (2008).
This approachhas been proven to remedy shortcomings of otherpre-translation reordering approaches by reorder-ing the input word sequence, but scoring the out-put word sequence.Elming (2008) only examined the approachwithin English ?
Danish, a language pair that dis-plays little reordering.
By contrast, in this pa-per, we target the more demanding reordering taskof translating between two distant languages, En-glish and Arabic.
While much work has beendone on Arabic to English MT (Habash and Sa-dat, 2006; Lee, 2004) mostly focusing on ad-dressing the problems caused by the rich mor-phology of Arabic, we handle the less describedtranslation direction: English to Arabic.
Recently,there are some new publications on English toArabic MT.
Sarikaya and Deng (2007) use jointmorphological-lexical language models to re-rankthe output of English dialectal-Arabic MT, andBadr et al (2008) report results on the value ofthe morphological decomposition of Arabic dur-ing training and describe different techniques forre-composition of Arabic in the output.
We differfrom the previous efforts targeting Arabic in that(1) we do not address morphology issues throughsegmentation (more on this in section 3) and (2)we focus on utilizing syntactic knowledge to ad-dress the reordering challenges of this translationdirection.3 Arabic Syntactic IssuesArabic is a morphologically and syntacticallycomplex language with many differences from En-glish.
Arabic morphology has been well studiedin the context of MT.
Previous results all sug-gest that some degree of tokenization is helpfulwhen translating from Arabic (Habash and Sa-dat, 2006; Lee, 2004).
However, when trans-lating into a morphologically rich language, tar-get tokenization means that the translation processis broken into multiple steps (Badr et al, 2008).For our experiments, Arabic was not segmentedapart from simple punctuation tokenization.
Thislow level of segmentation was maintained in or-der to agree with the segmentation provided inthe manually aligned corpus we used to learn ourrules (section 6.1).
We found no simple means fortransferring the manual alignments to more seg-mented language.
We expect that better perfor-mance would be achieved by introducing moreArabic segmentation as reported by Badr et al(2008).1 As such, and unlike previous work inPSMT translating into Arabic, we focus here onsyntax.
We plan to investigate different tokeniza-tion schemes for syntactic preprocessing in futurework.
Next, we describe three prominent English-Arabic syntactic phenomena that have motivatedsome of our decisions in this paper.First is verb-subject order.
Arabic verb subjectsmay be: (a.)
pro-dropped (verb conjugated), (b.
)pre-verbal (SVO), or (c.) post-verbal (VSO).
Al-though the English SVO order is possible in Ara-bic, it is not always preferred, especially when thesubject is particularly long.
Unfortunately, this isthe harder case for PSMT to handle.
For smallsubject noun phrases (NP), PSMT might be ableto handle the reordering in the phrase table if theverb and subject were seen in training.
But this be-comes much less likely with very long NPs that ex-ceed the size of the phrases in a phrase table.
Theexample in Figure 1 illustrates this point.
Boldingand italics are used to mark the verb and subor-dinating conjunction that surround the subject NP(19 tokens) in English and what they map to inArabic, respectively.2Secondly, Arabic adjectival modifiers typicallyfollow their nouns with the exception of some su-perlative adjectives.
However, English adjectivalmodifiers can follow or precede their nouns de-pending on the size of the adjectival phrase: singleword adjectives precede but multi-word adjectivesphrases follow (or precede while hyphenated).
Forexample, a tall man translates as ?K??
?g.
P rjl1Our results are not comparable to their results, since theyreport on non-standard data sets.2All Arabic transliterations in this paper are provided inthe Habash-Soudi-Buckwalter scheme (Habash et al, 2007).70[NP-SBJ The general coordinator of the railroad project among the countries of the Gulf Coopera-tion Council , Hamid Khaja ,] [V announced] [SUB that ...][ ?k.
Ag Y?Ag ?j.J?m?'@??A?J?
@ ??m.?
??X?K.
YKYm?'@????
@ ??Q????
?A??
@????
@ NP-SBJ] [ 	???
@ V][.
.
.
?
@ SUB][V A?ln] [NP-SBJ Almnsq Al?Am lm?rw?
Alsk~ AlHdyd byn dwl mjls Alt?Awn Alxlyjy HAmdxAjh] [SUB An ...]Figure 1: An example of long distance reordering of English SVO order to Arabic VSO ordert7 ?
?
?
?
?
?
t6 ?
?
 ?
?
?
?t5 ?
 ?
?
?
?
?t4 ?
?
?
?
 ?
?t3 ?
?
?
?
?
 ?t2 ?
?
?
 ?
?
?t1  ?
?
?
?
?
?s1 s2 s3 s4 s5 s6 s7Figure 2: Abstract alignment matrix example ofreordering.Twyl ?man tall?
; however, the English phrase aman tall of stature translates with no reordering as??A??
@ ?K??
?g.
P rjl Twyl AlqAm~ ?man tall the-stature?.
So does the superlative the tallest mantranslating into ?g.
P ???
@ ATwl rjl ?tallest man.
?Finally, Arabic has one syntactic construction,called Idafa, for indicating possession and com-pounding, while English has three.
The Idafa con-struction typically consists of one or more indef-inite nouns followed by a definite noun.
For ex-ample, the English phrases the car keys, the car?skeys and the keys of the car all translate into theArabic?PAJ??
@ iJKA??
mfAtyH AlsyAr~ ?keys the-car.?
Only one of the three English constructionsdoes not require content word reordering.4 Reordering rules4.1 Definition of reorderingFollowing Elming (2008), we define reordering astwo word sequences, left sequence (LS) and rightsequence (RS), exchanging positions.
These twosequences are restricted by being parallel consecu-tive, maximal and adjacent.
The sequences are notrestricted in length, making both short and longdistance reordering possible.
Furthermore, theyneed not be phrases in the sense that they appearas an entry in the phrase table.Figure 2 illustrates reordering in a word align-ment matrix.
The matrix contains reorderings be-tween the light grey sequences (s32 and s64)3 and3Notation: syx means the consecutive source sequencethe dark grey sequences (s55 and s66).
On the otherhand, the sequences s33 and s54 are not consideredfor reordering, since neither one is maximal, ands54 is not consecutive on the target side.4.2 Learning rulesTable 1 contains an example of the features avail-able to the algorithm learning reordering rules.We include features for the candidate reorder-ing sequences (LS and RS) and for their possi-ble left (LC) and right (RC) contexts.
In addi-tion to words and parts-of-speech (POS), we pro-vide phrase structure (PS) sequences and subordi-nation information (SUBORD).
The PS sequenceis made up of the highest level nodes in the syntaxtree that cover the words of the current sequenceand only these.
Subordinate information can alsobe extracted from the syntax tree.
A subordinateclause is defined as inside an SBAR constituent;otherwise it is a main clause.
Our intuition is thatall these features will allow us to learn the bestrules possible to address the phenomena discussedin section 3 at the right level of generality.In order to minimize the amount of trainingdata, word and POS sequences are annotated astoo long (T/L) if they are longer than 4 words,and the same for phrase structure (PS) sequencesif they are longer than 3 units.
A feature vectoris only used if at least one of these three levels isnot T/L for both LS and RS, and T/L contexts arenot included in the set.
This does not constrainthe possible length of a reordering, since a PS se-quence of length 1 can cover an entire sentence.In the example in Table 1, LS and RS are singlewords, but they are not restricted in length.
Thespan of the contexts varies from a single neighbor-ing word to all the way to the sentence border.
Inthe example, LS and RS should be reordered, sinceadjectives appear as post-modifiers in Arabic.In order to learn rules from the annotated data,we use a rule-based classifier, Ripper (Cohen,covering word positions x to y.71Level LC LS RS RCWORD <s> he bought || he bought || bought new books today || today .
|| today .
< /s>POS <S> NN VBD || NN VBD || VBD JJ NNS NN || NN .
|| NN .
< /S>PS <S> NP VBD || NP VBD || VBD JJ NNS NP || NP .
|| NP .
< /S>SUBORD MAIN MAIN MAIN MAINTable 1: Example of features for rule-learning.
Possible contexts separated by ||.Figure 3: Example word lattice.1996).
The motivation for using Ripper is that itallows features to be sets of strings, which fits wellwith our representation of the context, and it pro-duces easily readable rules that allow better under-standing of the decisions being made.
In section6.3, extracted rules are exemplified and analyzed.The probabilities of the rules are estimated us-ing Maximum Likelihood Estimation based onthe information supplied by Ripper on the perfor-mance of the individual rules on the training data.These logarithmic probabilities are easily integrat-able in the log-linear PSMT model as an additionalparameter by simple addition.5 The PSMT systemOur baseline is the PSMT system used for the2006 NAACL SMT workshop (Koehn and Monz,2006) with phrase length 3 and a trigram languagemodel (Stolcke, 2002).
The decoder used for thebaseline system is Pharaoh (Koehn, 2004) withits distance-penalizing reordering model.
SincePharaoh does not support word lattice input, weuse our own decoder for the experiments.
Ex-cept for the reordering model, it uses the sameknowledge sources as Pharaoh, i.e.
a bidirectionalphrase translation model, a lexical weight model,phrase and word penalties, and a target languagemodel.
Its behavior is comparable to Pharaohwhen doing monotone decoding.The search algorithm of our decoder is similarto the RG graph decoder of (Zens et al, 2002).
Itexpects a word lattice as input.
Figure 3 showsthe word lattice for the example in table 2.
In theexample used here, we choose to focus on the re-ordering of adjective and noun.
For readability,we do not describe the possibility of reordering thesubject and verb.
This will also be the case in lateruse of the example.Since the input format defines all possible wordorders allowed by the rule set, a simple monotonesearch is sufficient.
Using a language model of or-der n, for each hypothesized target string endingin the same n-1-gram, we only have to extend thehighest scoring hypothesis.
None of the others canpossibly outperform this one later on.
This is be-cause the maximal context evaluating a phrase ex-tending this hypothesis, is the history (n-1-gram)of the first word of that phrase.
The decoder isnot able to look any further back at the precedingstring.5.1 The reordering approachSimilar to Elming (2008), the integration of therule-based reordering in our PSMT system is car-ried out in two separate stages:1.
Reordering the source sentence to assimilatethe word order of the target language.2.
Weighting of the target word order accordingto the rules.Stage (1) is done in a non-deterministic fashionby generating a word lattice as input.
This way, thesystem has both the original word order, and thereorderings predicted by the rule set.
The differentpaths of the word lattice are merely given as equalsuggestions to the decoder.
They are in no wayindividually weighted.Separating stage (2) from stage (1) is motivatedby the fact that reordering can have two distinctorigins.
They can occur because of stage (1), i.e.the lattice reordering of the original English wordorder (phrase external reordering), and they canoccur inside a single phrase (phrase internal re-ordering).
The focus of this approach lies in do-ing phrase-independent word reordering.
Rule-predicted reorderings should be promoted regard-less of whether they owe their existence to a syn-tactic rule or a phrase table entry.This is accomplished by letting the actual scor-ing of the reordering focus on the target string.72Source: he1 bought2 new3 books4 today5Rule: 3 4?
4 3Hypothesis Target string AlignmentH1 A?tr?
jdyd~ ktbA 1+2 3 4H2 A?tr?
ktbA jdyd~ 1+2 4 3Table 2: Example of the scoring approach duringdecoding at source word 4.The decoder is informed of where a rule has pre-dicted a reordering, how much it costs to do thereordering, and how much it costs to avoid it.
Thisis then checked for each hypothesized target stringvia a word alignment.The word alignment keeps track of whichsource position the word in each target positionoriginates from.
In order to access this informa-tion, each phrase table entry is annotated with itsinternal word alignment, which is available as anintermediate product from phrase table creation.If a phrase pair has multiple word alignments, themost frequent one is chosen.Table 2 exemplifies the scoring approach, againwith focus on the adjective-noun reordering.
Thesource sentence is ?he bought new books today?,and a rule has predicted that source word 3 and4 should change place.
Due to the pro-drop na-ture of Arabic, the first Arabic word is linked tothe two first English words (1+2).
When the de-coder has covered the first four input words, twoof the hypothesis target strings might be H1 andH2.
At this point, it becomes apparent that H2contains the desired reordering (namely what cor-responds to source word order ?4 3?
), and it getassigned the reordering cost.
H1 does not containthe rule-suggested reordering (instead, the wordsare in the original order ?3 4?
), and it gets the vi-olation cost.
Both these scorings are performedin a phrase-independent manner.
The decoder as-signs the reordering cost to H2 without knowingwhether the reordering is internal (due to a phrasetable entry) or external (due to a syntactic rule).Phrase internal reorderings at other points of thesentence, i.e.
points that are not covered by a rule,are not judged by the reordering model.
Our ruleextraction does not learn every possible reorder-ing between the two languages, but only the mostgeneral ones.
If no rule has an opinion at a certainpoint in a sentence, the decoder is free to choosethe phrase translation it prefers without reorderingcost.Separating the scoring from the source lan-guage reordering also has the advantage that theapproach in essence is compatible with otherapproaches such as a traditional PSMT system(Koehn et al, 2003b) or a hierarchical phrase sys-tem (Chiang, 2005).
We will, however, not exam-ine this possibility further in the present paper.6 Evaluation6.1 DataWe learn the reordering rules from the IBMArabic-English aligned corpus (IBMAC) (Itty-cheriah and Roukos, 2005).
Of its total 13.9K sen-tence pairs, we only use 8.8K sentences becausethe rest of the corpus uses different normalizationsfor numerals that make the two sets incompatible.6.6K of the sentences (179K English and 146KArabic words) are used to learn rule, while the restare used for development purposes.
In addition tothe manual alignment supplied with these data, wecreate an automatic word alignment for them usingGIZA++ (Och and Ney, 2003) and the grow-diag-final (GDF) symmetrization algorithm (Koehn etal., 2005).
This was done together with the dataused to train the MT system.
The English sideis parsed using a state-of-the-art statistical Englishparser (Charniak, 2000).
Two rule sets are learnedbased on the manual alignments (MAN) and theautomatic alignments (GDF).The MT system is trained on a corpus con-sisting of 126K sentences with 4.2M Englishand 3.3M Arabic words in simple tokeniza-tion scheme.
The domain is newswire (LDC-NEWS) taken from Arabic News (LDC2004T17),eTIRR (LDC2004E72), English translation ofArabic Treebank (LDC2005E46), and Ummah(LDC2004T18).
Although there are additionalcorpora available, we restricted ourselves to thisset to allow for a fast development cycle.
We planto extend the data size in the future.
The Ara-bic language model is trained on the 5.4M sen-tences (133M words) of newswire text in the 1994to 1996 part of the Arabic Gigaword corpus.
Werestricted ourselves to this part, since we are notable to run Pharaoh with a larger language model.4For test data, we used NIST MTEval test setsfrom 2004 (MT04) and 2005 (MT05)5.
Sincethese data sets are created for Arabic-English eval-uation with four English reference sentences for4All of the training data we use is available from the Lin-guistic Data Consortium (LDC): http://www.ldc.upenn.edu/.5 http://www.nist.gov/speech/tests/mt/73System Dev MT04 MT05Pharaoh Free 28.37 23.53 24.79Pharaoh DL4 29.52 24.72 25.88Pharaoh Monotone 27.93 23.55 24.72MAN NO weight 29.53 24.72 25.82SO weight 29.43 24.74 25.82TO weight 29.40 24.78 25.93GDF NO weight 29.87 25.11 26.04SO weight 29.84 25.06 26.01TO weight 29.95 25.17 26.09Table 3: Automatic evaluation scores for differentsystems using rules extracted from manual align-ments (MAN) and automatic alignments (GDF).The TO system using GDF rules is significantlybetter than the light grey cells at a 95% confidencelevel (Zhang et al, 2004).each Arabic sentence, we invert the sets by con-catenating all English sentences to one file.
Thismeans that the Arabic reference file contains fourduplicates of each sentence.
Each duplicate is thereference of a different English source sentence.Following this merger, MT04 consists of 5.4Ksentences with 193K English and 144K Arabicwords, and MT05 consists of 4.2K sentences with143K English and 114K Arabic words.
MT04 isa mix of domains containing speeches, editorialsand newswire texts.
On the other hand, MT05 isonly newswire.The NIST MTEval test set from 2002 (MT02)is split into a tuning set for optimizing decoder pa-rameter weights and a development set for ongo-ing experimentation.
The same merging procedureas for MT04 and MT05 is employed.
This resultsin a tune set of 1.0K sentences with 34K Englishand 26K Arabic words, and a development set of3.1K sentences with 102K English and 79K Ara-bic words.6.2 Results and discussionThe reordering approach is evaluated on the MT04and MT05 test sets.
Results are listed in table 3along with results on the development set.
We re-port on (a) Pharaoh with no restriction on reorder-ing (Pharaoh Free), (b) Pharaoh with distortionlimit 4 (Pharaoh DL4), (c) Pharaoh with monotonedecoding (Pharaoh Monotone), and (d) a systemprovided with a rule reordered word lattice but no(NO) weighting in the spirit of (Crego and Mari?o,2007), (e) the same system but with a source orderSystem MT04 MT05 Avr.
humanPharaoh Free 24.07 25.15 3.0 (2.80)Pharaoh DL4 25.42 26.51 ?NO scoring 25.68 26.29 2.5 (2.43)SO scoring 25.42 26.02 2.5 (2.64)TO scoring 25.98 26.49 2.0 (2.08)Table 4: Evaluation on the diff set.
Average hu-man ratings are medians with means in parenthe-sis, lower scores are better, 1 is the best score.
(SO) weighting in the spirit of (Zhang et al, 2007;Li et al, 2007), and finally (f) the same system butwith the target order (TO) weighting.In addition to evaluating the reordering ap-proaches, we also report on supplying them withdifferent reordering rule sets: a set that waslearned on manually aligned data (MAN), and aset learned on the same data but with automaticalignments (GDF).6.2.1 Overall ResultsPharaoh Monotone performs similarly to PharaohFree.
This shows that the question of improvedreordering is not about quantity, but rather qual-ity: what constraints are optimal to generate thebest word order.
The TO approach gets an increaseover Pharaoh Free of 1.3 and 1.6 %BLEU on thetest sets, and 0.2 and 0.5 %BLEU over PharaohDL4.Improvement is less noticeable over the otherpre-translation reordering approaches (NO andSO).
A possible explanation is that the rules do notapply very often, in combination with the fact thatthe approaches often behave alike.
The differencein SO and TO scoring only leads to a differencein translation in ?14% of the sentences.
This set,the diff set, is interesting, since it provides a focuson the difference between these approaches.
In ta-ble 4, we evaluate on this set.6.2.2 Diff SetOverall the TO approach seems to be a superiorreordering method.
To back this observation, 50sentences of MT04 are manually evaluated by anative speaker of Arabic.
Callison-Burch et al(2007) show that ranking sentences gives higherinter-annotator agreement than scoring adequacyand fluency.
We therefore employ this evaluationmethod, asking the evaluator to rank sentencesfrom four of the systems given the input sentence.Ties are allowed.
Table 4 shows the average rat-74Decoder choice NO SO TOMT04 Phrase internal 20.7 0.6 21.2Phrase external 30.1 43.0 33.1Reject 49.2 56.5 45.7MT05 Phrase internal 21.3 0.7 21.6Phrase external 29.5 42.9 31.8Reject 49.2 56.4 46.5Table 5: The reordering choices made based onthe three pre-translation reordering approaches forthe 20852 and 17195 reorderings proposed by therules for the MT04 and MT05 test sets.
Measuredin %.ings of the systems.
This shows the TO scoringto be significantly superior to the other methods(p < 0.01 using Wilcoxon signed-rank testing).6.2.3 MAN vs GDFAnother interesting observation is that reorderingrules learned from automatic alignments lead tosignificantly better translation than rules learnedfrom manual alignment.
Due to the much higherquality of the manual alignment, the oppositemight be expected.
However, this may be justa variant on the observation that alignment im-provements (measured against human references)seldom lead to MT improvements (Lopez andResnik, 2006).
The MAN alignments may in factbe better than GDF, but they are most certainlymore different in nature from real alignment thanthe GDF alignments are.
As such, the MAN align-ments are not as powerful as we would have likedthem to be.
In our data sets, the GDF rules, seemless specific, and they therefore apply more fre-quently than the MAN rules.
On average, this re-sults in more than 7 times as many possible re-ordering paths per sentence.
This means that theGDF rules supply the decoder with a larger searchspace, which in turn means more proposed trans-lation hypotheses.
This may play a big part in theeffect of the rule sets.6.2.4 Reordering ChoicesTable 5 shows the reordering choices made by theapproaches in decoding.
Most noticeable is thatthe SO approach is strongly biased against phraseinternal reorderings; TO uses more than 30 timesas many phrase internal reorderings as SO.
In ad-dition, TO is less likely to reject a rule proposedreordering.The 50 sentences from the manual evaluationare also manually analyzed with regards to re-ordering.
For each reordering in these sentences,the four systems are ranked according to how wellthe area affected by the reordering is translated.This indicates that the SO approach?s bias againstphrase internal reorderings may hurt performance.25% of the time, when SO chooses an external re-ordering, while the TO approach chooses an in-ternal reordering, the TO approach gets a bettertranslation.
Only in 7% of the cases is it the otherway around.Another discovery from the analysis is when TOchooses an internal reordering and NO rejects thereordering.
Here, TO leads to a better translation45% of the time, while NO never outperforms TO.In these cases, either approach has used a phraseto cover the area, but via rule-based motivation,TO has forced a less likely phrase with the correctword order through.
This clearly shows that lo-cal reordering is not handled sufficiently by phraseinternal reordering alone.
These need to be con-trolled too.6.3 Rule analysisThe rule learning resulted in 61 rules based onmanual alignments and 39 based on automaticalignments.
Of these, the majority handled theplacement of adjectives, while only a few handledthe placement of the verb.A few of the rules that were learned from themanual alignment are shown in table 6.
The firsttwo rules handle the placement of the finite verbin Arabic.
Rule 16 states that if a finite verbappears in front of a subordinate clause, then itshould be moved to sentence initial position witha probability of 68%.
Due to the restrictions ofsequence lengths, it can only swap across maxi-mally 4 words or a sequence of words that is de-scribable by maximally 3 syntactic phrases.
TheSBAR condition may help restrict the reorderingto finite verbs of the main clause.
This rule and itsprobability goes well with the description given insections 3, since VSO order is not obligatory.
Thesubject may be unexpressed, or it may appear infront of the verb.
This is even more obvious inrule 27, which has a probability of only 43%.Rules 11 and 1 deal with the inverse ordering ofadjectives and nouns.
The first is general but un-certain, the second is lexicalized and certain.
Thereason for the low probability of rule 11 is primar-ily that many proper names have been mis-taggedby the parser as either JJ or NN, and to a lesser75No LC LS RS RC Prob.16 WORD: <s> POS: FVF PS: SBAR 68%27 WORD: <s> PS: NP POS: FVF 43%11 POS: IN POS: JJ POS: NN 46%1 !
POS: JJ POS: JJ WORD: president 90%37 !
POS: NN POS: NN POS: NNS POS: IN 71%!
POS: JJTable 6: Example rules.
!
specifies negative conditions.extent that the rule should often not apply if theright context is also an NN.
Adding the latter re-striction narrows the scope of the rule but wouldhave increased the probability to 54%.Rule 1, on the other hand, has a high proba-bility of 90%.
It is only restricted by the con-dition that the left context should not be an ad-jective.
In these cases, the adjectives should of-ten be moved together, as is the case with ?thesouth african president??
??KQ?
@ H.
?Jm.?
'@ ?KQ?
@Alry?ys Aljnwb Afryqy where ?south african?
ismoved to the right of ?president?.Finally, rule 37 handles compound nouns.
Herea singular noun is moved to the right of a pluralnoun, if the right context is a preposition, and theleft context is neither an adjective nor a singularnoun.
This rule handles compound nouns, wherethe modifying function of the first noun often ishard to distinguish from that of an adjective.
Theleft context restrictions server the same purpose asthe left context in rule 1; these should often bemoved together with the singular noun.
The func-tion of the right context is harder to explain, butwithout this restriction, the rule would have beenmuch less successful; dropping from a probabilityof 71% to 51%.An overall comparison of the rules producedbased on the manual and automatic alignmentsshows no major difference in quality.
This is espe-cially interesting in light of the better translationusing the GDF rules.
It is also very interestingthat it seems possible to get as good rules from theGDF as from the MAN alignments.
This is a newresult compared to Elming (2008), where resultson manual alignments only are reported.7 Conclusion and Future PlansWe have explored the syntactic reordering ap-proach previously presented in (Elming, 2008)within a more distant language pair, English-Arabic.
A translation direction that is highlyunder-represented in MT research, compared tothe opposite direction.
We achieve significant im-provement in translation quality over related ap-proaches, measured by manual as well as auto-matic evaluations on this task.
Thus proving theviability of the approach on distant languages.We also examined the effect of the alignmentmethod on learning reordering rules.
Interestingly,our experiments produced better translation usingrules learned from automatic alignments than us-ing rules learned from manual alignments.
This isan aspect we want to explore further in the future.In future work, we would also like to addressthe morphological complexity of Arabic togetherwith syntax.
We plan to consider different seg-mentations for Arabic and study their interactionwith translation and syntactic reordering.An important aspect of the TO approach is thatit uses phrase internal alignments during transla-tion.
In the future, we wish to examine the effecttheir quality has on translation.
We are also inter-ested in examining the approach within a standardphrase-based decoder such as Moses (Koehn et al,2003b) or a hierarchical phrase system (Chiang,2005).The idea of training on reordered source lan-guage is often connected with pre-translation re-ordering.
The present approach does not em-ploy this strategy, since this is no trivial matterin a non-deterministic, weighted approach.
Zhanget al (2007) proposed an approach that buildson unfolding alignments.
This is not an opti-mal solution, since this may not reflect their rules.Training on both original and reordered data maystrengthen the approach, but it would not remedythe problems of the SO approach, since it wouldstill be ignorant of the internal reorderings of aphrase.
Nevertheless, it may strengthen the TOapproach even further.
We also wish to examinethis in future work.76ReferencesI.
Badr, R. Zbib, and J.
Glass.
2008.
Segmentation forEnglish-to-Arabic statistical machine translation.
InProceedings of ACL?08: HLT, Short Papers, Colum-bus, OH, USA.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(Meta-) evaluation of machinetranslation.
In Proceedings of ACL?07 Workshop onStatistical Machine Translation, Prague, Czech Re-public.E.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL?00, Seattle, WA,USA.D.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL?05, Ann Arbor, MI, USA.W.
Cohen.
1996.
Learning trees and rules with set-valued features.
In Proceedings of AAAI, Portland,OR, USA.M.
Collins, P. Koehn, and I. Kucerova.
2005.
Clauserestructuring for statistical machine translation.
InProceedings of ACL?05, Ann Arbor, MI, USA.J.
M. Crego and J.
B. Mari?o.
2007.
Syntax-enhancedn-gram-based smt.
In Proceedings of the MT Sum-mit, Copenhagen, Denmark.J.
Elming.
2008.
Syntactic reordering integrated withphrase-based smt.
In Proceedings of the ACL Work-shop on Syntax and Structure in Statistical Transla-tion (SSST-2), Columbus, OH, USA.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.2004.
What?s in a translation rule?
In Proceedingsof HLT/NAACL?04, Boston, MA, USA.N.
Habash and F. Sadat.
2006.
Arabic preprocessingschemes for statistical machine translation.
In Pro-ceedings of HLT-NAACL?06, New York, NY, USA.N.
Habash, B. Dorr, and C. Monz.
2006.
Challengesin Building an Arabic-English GHMT System withSMT Components.
In Proceedings of AMTA?06,Cambridge, MA, USA.N.
Habash, A. Soudi, and T. Buckwalter.
2007.On Arabic Transliteration.
In A. van den Boschand A. Soudi, editors, Arabic Computational Mor-phology: Knowledge-based and Empirical Methods.Springer.N.
Habash.
2007.
Syntactic preprocessing for statisti-cal machine translation.
In Proceedings of the MTSummit, Copenhagen, Denmark.A.
Ittycheriah and S. Roukos.
2005.
A maximumentropy word aligner for arabic-english machinetranslation.
In Proceedings of EMNLP, Vancouver,Canada.P.
Koehn and C. Monz.
2006.
Manual and auto-matic evaluation of machine translation between Eu-ropean languages.
In Proceedings of the Workshopon Statistical Machine Translation at NAACL?06,New York, NY, USA.P.
Koehn, F. J. Och, and D. Marcu.
2003a.
Statis-tical phrase-based translation.
In Proceedings ofNAACL?03, Edmonton, Canada.P.
Koehn, F. J. Och, and D. Marcu.
2003b.
Statis-tical Phrase-based Translation.
In Proceedings ofNAACL?03, Edmonton, Canada.P.
Koehn, A. Axelrod, A. Birch Mayne, C. Callison-Burch, M. Osborne, and D. Talbot.
2005.
Ed-inburgh system description for the 2005 IWSLTspeech translation evaluation.
In InternationalWorkshop on Spoken Language Translation 2005(IWSLT?05), Pittsburgh, PA, USA.P.
Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.In Proceedings of AMTA?04, Washington, DC, USA.Y.
Lee.
2004.
Morphological Analysis for Statisti-cal Machine Translation.
In Proceedings of HLT-NAACL?04, Boston, MA, USA.C.
Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan.2007.
A probabilistic approach to syntax-based re-ordering for statistical machine translation.
In Pro-ceedings of ACL?07, Prague, Czech Republic.A.
Lopez and P. Resnik.
2006.
Word-based alignment,phrase-based translation: what?s the link?
In Pro-ceedings of AMTA?06, Cambridge, MA, USA.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.R.
Sarikaya and Y. Deng.
2007.
Joint morphological-lexical language modeling for machine translation.In Proceedings of HLT-NAACL?07, Short Papers,Rochester, NY, USA.A.
Stolcke.
2002.
Srilm ?
an extensible language mod-eling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing, Den-ver, CO, USA.C.
Wang, M. Collins, and P. Koehn.
2007.
Chinesesyntactic reordering for statistical machine transla-tion.
In Proceedings of EMNLP-CoNLL, Prague,Czech Republic.F.
Xia and M. McCord.
2004.
Improving a statis-tical mt system with automatically learned rewritepatterns.
In Proceedings of COLING?04, Geneva,Switzerland.K.
Yamada and K. Knight.
2001.
A Syntax-BasedStatistical Translation Model.
In Proceedings ofACL?01, Toulouse, France.R.
Zens, F. J. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In M. Jarke, J. Koehler,and G. Lakemeyer, editors, KI - 2002: Advances inArtificial Intelligence.
25.
Annual German Confer-ence on AI.
Springer Verlag.Y.
Zhang, S. Vogel, and A. Waibel.
2004.
Interpret-ing bleu/nist scores: How much improvement do weneed to have a better system?
In Proceedings ofthe 4th International Conference on Language Re-sources and Evaluation (LREC?04), Lisbon, Portu-gal.Y.
Zhang, R. Zens, and H. Ney.
2007.
Improvedchunk-level reordering for statistical machine trans-lation.
In Proceedings of the IWSLT, Trento, Italy.77
