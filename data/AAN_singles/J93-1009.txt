Issues in the Choice of a Source forNatural Language GenerationDav id  D. McDonald*Brandeis UniversityThe most vexing question in natural anguage generation is 'what is the source'--what do speakers tart from when they begin to compose an utterance?
Theories ofgeneration i the literature differ markedly in their assumptions.
A few start with anunanalyzed body of numerical data (e.g.
Bourbeau et al 1990; Kukich 1988).
Most startwith the structured objects that are used by a particular reasoning system or simulatorand are cast in that system's representational formalism (e.g.
Hovy 1990; Meteer 1992;R6sner 1988).
A growing number of systems, largely focused on problems in machinetranslation or grammatical theory, take their input to be logical formulae based onlexical predicates (e.g.
Wedekind 1988; Shieber et al 1990).The lack of a consistent answer to the question of the generator's source has beenat the heart of the problem of how to make research on generation i telligible andengaging for the rest of the computational linguistics community, and has complicatedefforts to evaluate alternative treatments even for people in the field.
Nevertheless, asource cannot be imposed by fiat.
Differences in what information is assumed to beavailable, its relative decomposition when compared to the "packaging" available inthe words or syntactic onstructions of the language (linguistic resources), what amountand kinds of information are contained in the atomic units of the source, and whatsorts of compositions and other larger scale organizations are possible--all these havean impact on what architectures are plausible for generation and what efficienciesthey can achieve.
Advances in the field often come precisely through insights into therepresentation f the source.Language comprehension research does not have this problem--its source is a text.Differences in methodology govern where this text comes from (e.g., single sentencevs.
discourse, sample sentences vs. corpus study, written vs. spoken), but these asidethere is no question of what the comprehension process tarts with.Where comprehension "ends" is quite another matter.
If we go back to some of theearly comprehension systems, the end point of the process was an action, and therewas linguistic processing at every stage (Winograd 1972).
Some researchers, this authorincluded, take the end point to be an elaboration ofan already existing semantic modelwhereby some new individuals are added and new relations established betweenthem and other individuals (Martin and Riesbeck 1986; McDonald 1992a).
Today'sdominant paradigm, however, stemming perhaps from the predominance of researchon question-answering and following the lead of theoretical linguistics, is to take theend point to be a logical form: an expression that codifies the information i  the textat a fairly shallow level, e.g., a first-order formula with content words mapped topredicates with the same spelling, and with individuals represented by quantifiedvariables or constants.
* 14 Brantwood Road, Arlington, MA 02174-8004; mcdonald@cs.brandeis.edu(~) 1993 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 1It is somewhat puzzling that this question of where the comprehension processends has apparently never been debated in the literature.
Instead it seems largelytaken for granted that the parsing process ends with the assembly of an expressionin a suitable logic that captures the text's information content, perhaps with somefunctional annotations, and that a "reasoning" process then starts with that expressionand draws inferences in order to resolve anaphors and establish the speaker's intent.Problems arise when researchers project his default decomposition onto the pro-cess of producing language.
All too often the process is divided into a "reasoning"and a "generation" component (see, e.g., Shieber, this issue)--an unfortunate choiceof terminology because it reduces the scope of "generation" to triviality as we shallsee.
The primary motivation for the division is the desire for a bi-directional naturallanguage processing system--one where the representation f the linguistic resourcesis reversible for use in both the comprehension a d production of utterances.
But whilea reversible representation is indeed a proper goal for today's systems, the choice oflogical form as the "pivot point" is problematic, especially a first-order formula.A truly reversible linguistic mapping between intentional situation and utterancewill have the comprehension process end where the generation process begins.
Thusjust as the psycholinguistically correct source for generation is still very much a matterof research (as it is even when the source is a computational object in a well-designedAI system), so too is the end-point of comprehension, and by implication the divisionof that process into components and representational levels.
A declarative, reversible,form-meaning mapping does not ipso facto have to start/end at the level of logicalform, but can originate at a much deeper level with the class definitions of the objecttypes and relations of the speaker's conceptual model (McDonald 1993).Considered in isolation, the production of text from a logical form is, quite frankly,trivial.
It corresponds to the final "readout" phase of McDonald, Meteer, and Puste-jovsky (1987), since all that remains to be done is to linearize its elements in keepingwith the constraints of a surface grammar, carry out the trivial mapping to the pho-netic (orthographic) forms of the words implicit in the predicates, and add the requisitegrammatical function words and morphemes.
This capability has been an establishedpart of the state of the art for well over twenty years (see, e.g., Webber \[1971\], whichis also the first work on reversible grammars for generation this author is aware of).Over the years new architectures for this "tactical" part of generation (we prefer theterm surface realization) are introduced only because of new ideas in grammatical the-ory or in response to shifts in what is given in the immediately prior representationallevel.In current research that focuses just on surface realization, all of the substantialtasks of generation are invariably subordinated to the "reasoner" or "strategic om-ponent," which is treated as a black box whose operations are seldom discussed.Examples of these tasks include construing the speaker's ituation in realizable termsgiven the available vocabulary and syntactic resources (an especially important askwhen the source is raw data, e.g., precisely what points of the compass make the wind"easterly," \[Bourbeau etal.
1990\]); selecting the information to include in the utteranceand deciding whether it should be stated explicitly or left for inference; distributingthe information into sentences and giving it an organization that reflects the intendedrhetorical force, coherence, and necessary cohesion given the prior discourse; and find-ing a mapping of the information to linguistic resources that is collectively expressible(i.e., has a surface realization; see Meteer 1992).
How one chooses to approach thesetasks has substantial implications for the kinds of structures that a surface realizationprocess can sensibly be given as input and may not be taken for granted.
As a conse-quence, any generation architecture that is proposed without including an articulation192David D. McDonald Source for Natural Language Generationof the early stages of the process is issuing a large promissory note that it may not beable to redeem.Often the choice of a two-component process in comprehension (and by extensionin generation) is based on the judgment hat linguistic knowledge can and shouldbe restricted to its own component, the one responsible for the form and contentof grammatical rules, leaving to the other component all matters of general reason-ing ("reasoners should not have to truck with grammatical issues").
This assumptionhas been seriously questioned within the generation community in recent years.
Theconstraints imposed by the linguistic resources' limitations in what they are able toexpress and the delicacy of the conceptual and rhetorical choices state-of-the-art gen-erators are called on to make combine to force a strong interdependency between earlyand late aspects of the process to the point where many generation researchers todaydo not recognize any strong division into components, with different aspects of lin-guistic knowledge appearing at many levels of representation (see Hovy, McDonald,and Young 1989).All judgments about "components" are caught up in issues of modularity, infor-mation encapsulation, and the autonomy of syntax (see, e.g., Fodor 1983), issues thatcannot be settled without substantial empirical experiment and theoretical argument.That notwithstanding, it already seems evident that if one incorporates within thepurview of a "reasoner" such text planning activities as those listed earlier then it willbe very hard to sustain the argument hat knowledge of grammar can be restrictedto just surface realization.
Different aspects of this knowledge can still be relativelysegregated, however; in particular it seems likely that early generation decisions onlyrequire tacit knowledge of what lexemes and constructions the language provides,without yet requiring access to phonetic forms, the assembly of detailed sequentialstructures, or the imposition of grammatical relations.One of the more problematic aspects of taking the source for the generator to be alogical form is the very fact that it is represented as a single expression in a linear no-tation.
This may seem a small matter of notation, but the computational properties of alogical form as it is usually represented give it a very low notational efficiency in gen-eration (see Woods \[1986\] for a discussion of this notion).
These include the simple factthat expressions must be scanned and parsed before the information they contain canbe deployed, the lack of decompositional locality because of the use of scoping quan-tifiers and variables to represent individuals (Mellish 1985), and, indeed, as Shieber(this issue) points out, there is the question of the formula's intended structural re-alization, since the logical connectives that link a formula's terms underspecify theircorresponding syntactic onstructions because of the equivalence of other formulas un-der commutativity, associativity, and other truth-preserving logical transformations.The force of much of Shieber's argument in regard to logical-form equivalence r stson the constraint imposed by bi-directional processing.
If the choice of the informationthat an utterance is to express is made by a component with no knowledge of whatthe syntactic and lexical resources of the language are able to convey, then it is highlyunlikely that its representation f the information will match what the comprehensionprocess will arrive at as its representation of what the utterance meant--Shieber'snotion of "canonical logical form.
"Today's generators confront he problem regularly, as for example when a know-ledge-based system passes just the symbol 'red-porsche' to the generator and its de-signer wants the phrase "the red porsche," or "that car," or "the red one" produced as iscontextually appropriate.
Practical generators invariably interpose a special purposeinterface between the raw representation f the application they are speaking for and193Computational Linguistics Volume 19, Number 1their own general inguistic rules so as to compensate for the raw form's weaknessesor linguistically inappropriate organization--to 'match impedances' as it were.Seen as a transducer from meaning representations to surface forms, a generatoris driven by the terms and (formal) syntactic structure of its inputs.
In the ideal bi-directional system this mapping would be deterministic and reversible, but in practiceit is nondeterministic, with the generator adding information to a source represen-tation that severely underspecifies its target utterance.
Mismatch with the output ofcomprehension is inevitable since the parser in effect picks out a fully specified rep-resentation, reading into its form a correspondence with syntacticoqexical discrimi-nations that the knowledge-based system cannot appreciate.
In particular, the syntaxof today's ources' logics provide little useful guidance about the form of the surfaceutterance, or, alternatively, if the syntax is carefully attended to, it imposes a straight-jacket on the space of possible target utterances and limits the possibilities for fluentphrasing or adapting to the discourse context--a perennial problem with the 'directproduction' generators used with expert systems.While a (very) long-term solution to this problem waits on a fundamental redesignof meaning representations that would bring them into alignment with the require-ments of language, we can take steps in this direction now by improving the sourcenotation: Dispense with connected expressions in favor of dealing independently withthe terms that would have comprised it, 1We know that in any interesting system the logical form that specifies an utter-ance's meaning will be composed ynamically as the needs of the situation dictate,rather than being taken from a preconstructed repository, since if this were not thecase there would be no possibility for the creative use of language to accommodatenew situations.
Given this, one has to ask why the components of the representation fthe meaning would ever need to be assembled into an expression rather than entereddirectly into an early linguistic level of representation assoon as the need for them isappreciated.
What work in generation does a formula do qua formula that cannot bedone by its elements individually given a suitable representation?The extension of an abstract linguistic plan through the incremental addition ofelements is in fact a standard technique in generation.
2 A good example is Jeff Conk-lin's GENARO system (Conklin 1983; Arbib, Conklin, and Hill 1987), which producedparagraph-length descriptions of pictures of houses.
GENARO selected the informa-tion it would include using a procedure known as "iterative proposing," whereby itselected successive atomic units of information from its database (a KL-One network)in a sequence determined by their relative salience given the perspective of the picture.The units corresponded to individuals (e.g., houses, fences, colors), categorizations1 Since there would no longer be any logical connectives (the "glue" in the expressions) to be renderedin different but logically equivalent ways in a text, this technique also has the advantage that it reducesthe possibilities for mismatches between the way the speaker formulates information and acomprehension system will represent i s analysis of the corresponding text to just the more interestingcases of mismatches in the lexical semantics, e.g., "owns 40% of Ajax Corp." vs. "has a 40% stake inAjax Corp."2 Many of the ideas about bi-directional grammars and generation were developed by Shieber and DougAppelt at SRI, which makes it interesting to note here that in the original version of Appelt's KAMPgenerator, knowledge of the grammar was distributed throughout the system and acted locally in closecoordination with the system's planning decisions, making it rather like the approach being describedhere (Appelt 1982; p. 112).
Appelt later shifted to using Martin Kay's Functional Unification Grammar(Kay 1979) to increase modularity, perspicuity, and robustness to revisions in the plan, while at thesame time retaining the temporal interleaving of planning and linguistic realization, i.e., at no onemoment during the processing was there ever a full logical formula corresponding to the eventualutterance (Appelt 1985; p. 110).
The use of a FUG also of course directly facilitates bi-directionalapplications (Appelt 1989).194David D. McDonald Source for Natural Language Generationand properties of individuals, and the relations among them, each unit contributing areferent or content word(s) to the utterance.As each unit was selected, it was immediately incorporated into an abstract lin-guistic level of representation 3 i the position that best reflected its salience relative tothe units that were there already.
Thus the order in which units were selected had apotentially dramatic impact on the form of the final utterance.
Consider, for example,the NP "a white two-story house," embedded in the context "This is a picture of " atthe beginning of a description.
Following the rough heuristic that the most salientproperties of an object are positioned closest o the head when realized as adjectives,this NP is the result of GENARO selecting four semantic units in the following order:?
$housel - - the referent, and the source of "a __being newly introduced into the discourse)?
house($housel) "house"?
two-story-building($housel) "two story "?
color($housel, $white) "white "" given that the house isThe order of the units' selection follows their decreasing relative salience: thenumbers in this instance were 2.0, 1.0, .56, and .20 respectively.
Had the house orits appearance in the picture been different, say switching the relative salience of thetwo properties, then the order of selection and the resulting NP would reflect his: "atwo-story white house."
In different contexts, these units could have different realization,e.g., "\[it\] is two stories high.
"If we were to attempt to rationally reconstruct GENARO's selections as a standardlogical form, e.g.3(x) house(x) & two-story-building(x) & color(x,white)we would not only have to parse this linear notation and have to introduce somecanonical structural correspondences by which to direct its surface realization, but wewould have lost the salience information that gave GENARO its special sensitivity tothe particulars of the picture it was describing, markedly degrading its fluency.This example illustrates not only that semantic representations should explicitlyrecord information about salience, but also that the pivot point for bi-directional pro-cessing can be moved much deeper than is usually considered.
In GENARO and agoodly number of other generators we have rules for the selection of a set of minimalsemantic units and their organization into a text as just described.
On the parsingside we have the systems cited earlier, whose outputs are comparable units added toor embellishing an existing semantic model of essentially the same sort as this styleof generator starts from.
Given such architectures, the move to properly reversiblerules awaits only a declarative statement of the few .~'emaining parts of these systemswhere the mappin~ s have been formulated procedural ly--a project hat is already welladvanced (McDonald 1991, 1992b).Returning finally to the question of what processes hould be given the label"generation," we must be very careful to avoid reflexively identifying generation as3 Today this level would correspond toMeteer's "Text Structure" (1992).
At this level there is acommitment to constituency, lexical choices for heads, and the structural relations of head-argumentsand matrix-adjuncts.
The structure overall is unordered.195Computational Linguistics Volume 19, Number 1the obverse of parsing.
After all, the determination of where a "parser" leaves offand some non-text directed process of "general inferencing" takes over is very much aquestion of how individual systems are designed.
We also have evidence from state-of-the-art systems that an incommensurate mount of processing is presently being donein the two directions, and consequently any attempt to make components correspondis suspect.Existing comprehension systems as a rule extract considerably ess informationfrom a text than a generator must appreciate in generating one.
Examples include thereasons why a given word or syntactic onstruction is used rather than an alternative,what constitutes the style and rhetoric appropriate to a given genre and situation, orwhy information is clustered in one pattern of sentences rather than another.
Thereseems to be no reason in principle why comprehension systems couldn't notice suchthings, though of course their conclusions would have to be indeterminate since theydon't have access to all the information the speaker used.
More likely the present stateof affairs is simply reflective of the fact that the generation of quality text is a hardertask than its comprehension.My own answer to the question of 'how far back does generation go' is that it maybe considered to start at the first point where a speaker must appeal to her knowledgeof language as she begins the process of carrying out some action through the use oflanguage.
This classification is of course principally a mechanism for delimiting a fieldof research, but it does also suggest that the way we might best arrive at Shieber's "AI-complete" solution to the question of how semantic information should be representedis through a careful study of the needs of the generation process.ReferencesAppelt, Doug (1982).
"Planningnatural-language utterances to satisfymultiple goals."
SRI Technical Note 259,Menlo Park, CA.Appelt, Doug (1985).
Planning Englishsentences.
Cambridge University Press.Appelt, Doug (1989).
"Bidirectionalgrammars and the design of naturallanguage generation systems."
InTheoretical Issues in Natural LanguageProcessing, edited by Wilks, 199-205.Lawrence Erlbaum.Arbib, Michael; Conklin, Jeffery; and Hill,Jane (1987).
From Schema-Theory toLanguage.
Oxford University Press.Bourbeau, L.; Carcagno, D.; Goldberg, E.;Kittredge, R.; and Polgu6re, A.
(1990).
"Bilingual generation of weather forecastsin an operations environment."
InProceedings, 15th International Conference onComputational Linguistics (COLING-90).90-92.Conklin, E. Jeffery (1983).
"Data-drivenindelible planning of discourse generationusing salience."
Doctoral dissertation,University of Massachusetts, Amherst,MA.
Technical report 83-13.Hovy, Eduard (1990) "Unresolved issues inparagraph planning."
In Current Researchin Natural Language Generation, edited byDale, Mellish, and Zock.
Academic Press.Hovy, Eduard; McDonald, David; andYoung, Sheryl (1989).
"Current issues innatural language generation: Anoverview of the AAAI Workshop on TextPlanning and Realization."
A/Magazine,10(3), 27-29.Fodor, Jerry (1983).
The Modularity of Mind.The MIT Press.Kay, Martin (1979).
"Functional grammar.
"In Proceedings, 5th Annual Meeting of theBerkeley Linguistics Society.
University ofCalifornia, Berkeley, CA, 142-158.Kukich, Karen (1988).
"Fluency in NaturalLanguage Reports."
In Natural LanguageGeneration Systems, edited by McDonaldand Bole.
Springer-Verlag.Martin, Charles, and Riesbeck, Chris (1986).
"Uniform parsing and inference forlearning."
In Proceedings, AAAI-86.Philadelphia, PA. Morgan-Kaufmann.McDonald, David (1993).
"Reversible NLPby deriving the grammars from theknowledge base."
In Reversible Grammar inNatural Language Processing.
KluwerAcademic Publishers.McDonald, David (1992a).
"An efficientchart-based algorithm for partial-parsingof unrestricted texts."
In Proceedings, 3rdConference on Applied Natural LanguageProcessing (ACL).
Trento, Italy, 193-200.McDonald, David (1992b).
"Type-driven196David D. McDonald Source for Natural Language Generationsuppression of redundancy in thegeneration of inference-rich reports."
InAspects of Automated Natural LanguageGeneration, (Springer Verlag Lecture Notesin AI, Number 587), edited by Dale, Hovy,Rosner, and Stock, 73-88.
Springer-Verlag.McDonald, David; Meteer (Vaughan),Marie; and Pustejovsky, James (1987).
"Factors contributing to efficiency innatural language generation."
In NaturalLanguage Generation: Recent Advances inArtificial Intelligence, Psychology, andLinguistics, edited by Kempen, 159-181.Kluwer Academic Publishers.Meteer, Marie (1992).
Expressibility and theProblem of Efficient Text Planning.
PinterPublishers.Mellish, Chris (1985).
Computer Interpretationof Natural Language Descriptions.
JohnWiley.R6sner, Deitmar (1988).
"The generationsystem of the SEMSYN project: Towards atask-independent generator for German.
"In Advances in Natural Language Generation,edited by Zock and Sabah.
PinterPublishers.Shieber, Stuart; van Noord, Gertjan; Pereira,Fernando; and Moore, Robert (1990).
"Semantic-head-driven g eration.
"Computational Linguistics, 16(1), 30--42.Shieber, Stuart (1993).
"The problem oflogical-form equivalence."
ComputationalLinguistics, 19(1), 179-190.Webber, Bonnie (1971).
"The case forgeneration."
In Papers Presented attheSeminar in Mathematical Linguistics,Volume XIII, edited by Woods.
AikenComputer Laboratory, Department ofLinguistics, Harvard University,Cambridge, MA.Wedekind, J~irgen (1988).
"Generation asstructure driven derivation."
InProceedings, 13th International Conference onComputational Linguistics (COLING-88).Budapest, Hungary, 732-737.Winograd, Terry (1972).
UnderstandingNatural Language.
Academic Press.Woods, William (1986).
"Important issues inknowledge representation."
In Proceedings,IEEE.
74(10).197
