Transactions of the Association for Computational Linguistics, 1 (2013) 231?242.
Action Editor: Noah Smith.Submitted 11/2012; Revised 2/2013; Published 5/2013.
c?2013 Association for Computational Linguistics.Modeling Semantic Relations Expressed by PrepositionsVivek Srikumar and Dan RothUniversity of Illinois, Urbana-ChampaignUrbana, IL.
61801.
{vsrikum2, danr}@illinois.eduAbstractThis paper introduces the problem of predict-ing semantic relations expressed by preposi-tions and develops statistical learning modelsfor predicting the relations, their argumentsand the semantic types of the arguments.
Wedefine an inventory of 32 relations, build-ing on the word sense disambiguation taskfor prepositions and collapsing related sensesacross prepositions.
Given a preposition ina sentence, our computational task to jointlymodel the preposition relation and its argu-ments along with their semantic types, as away to support the relation prediction.
The an-notated data, however, only provides labels forthe relation label, and not the arguments andtypes.
We address this by presenting two mod-els for preposition relation labeling.
Our gen-eralization of latent structure SVM gives closeto 90% accuracy on relation labeling.
Further,by jointly predicting the relation, arguments,and their types along with preposition sense,we show that we can not only improve the re-lation accuracy, but also significantly improvesense prediction accuracy.1 IntroductionThis paper addresses the problem of predicting se-mantic relations conveyed by prepositions in text.Prepositions express many semantic relations be-tween their governor and object.
Predicting thesecan help advancing text understanding tasks likequestion answering and textual entailment.
Considerthe sentence:(1) The book of Prof. Alexander on primary schoolmethods is a valuable teaching resource.Here, the preposition on indicates that the bookand primary school methods are connected by therelation Topic and of indicates the Creator-Creation relation between Prof. Alexander andthe book.
Predicting these relations can help answerquestions about the subject of the book and also rec-ognize the entailment of sentences like Prof. Alexan-der has written about primary school methods.Being highly polysemous, the same prepositioncan indicate different kinds of relations, dependingon its governor and object.
Furthermore, severalprepositions can indicate the same semantic relation.For example, consider the sentence:(2) Poor care led to her death from pneumonia.The preposition from in this sentence expresses therelation Cause(death, pneumonia).
In a differ-ent context, it can denote other relations, as in thephrases copied from the film (Source) and recog-nized from the start (Temporal).
On the otherhand, the relation Cause can be expressed by sev-eral prepositions; for example, the following phrasesexpress a Cause relation: died of pneumonia andtired after the surgery.We characterize semantic relations expressed bytransitive prepositions and develop accurate modelsfor predicting the relations, identifying their argu-ments and recognizing the semantic types of the ar-guments.
Building on the word sense disambigua-tion task for prepositions, we collapse semanticallyrelated senses across prepositions to derive our re-lation inventory.
These relations act as predicatesin a predicate-argument representation, where thearguments are the governor and the object of the231preposition.
While ascertaining the arguments is alargely syntactic decision, we point out that syn-tactic parsers do not always make this predictioncorrectly.
However, as illustrated in the examplesabove, identifying the relation depends on the gov-ernor and object of the preposition.Given a sentence and a preposition, our goal isto model the predicate (i.e.
the preposition rela-tion) and its arguments (i.e.
the governor and ob-ject).
Very often, the relation label is not influencedby the surface form of the arguments but rather bytheir semantic types.
In sentence (2) above, wewant the predicate to be Cause when the object ofthe preposition is any illness.
We thus suggest tomodel the argument types along with the preposi-tion relations and arguments, using different notionsof types.
These three related aspects of the rela-tion prediction task are further explained in Section3 leading up to the problem definition.Though we wish to predict relations, argumentsand types, there is no corpus which annotates allthree.
The SemEval 2007 shared task of word sensedisambiguation for prepositions provides sense an-notations for prepositions.
We use this data to gen-erate training and test corpora for the relation la-bels.
In Section 4, we present two models for theprepositional relation identification problem.
Thefirst model considers all possible argument candi-dates from various sources along with all argumenttypes to predict the preposition relation label.
Thesecond model treats the arguments and types as la-tent variables during learning using a generalizationof the latent structural SVM of (Yu and Joachims,2009).
We show in Section 5 that this model notonly predicts the arguments and types, but also im-proves relation prediction performance.The primary contributions of this paper are:1.
We introduce a new inventory of prepositionrelations that covers the 34 prepositions thatformed the basis of the SemEval 2007 task ofpreposition sense disambiguation.2.
We model preposition relations, arguments andtheir types jointly and propose a learning algo-rithm that learns to predict all three using train-ing data that annotates only relation labels.3.
We show that jointly predicting relations withword sense not only improves the relation pre-dictor, but also gives a significant improvementin sense prediction.2 Prepositions & Predicate-ArgumentSemanticsSemantic role labeling (cf.
(Gildea and Jurafsky,2002; Palmer et al 2010; Punyakanok et al 2008)and others) is the task of converting text into apredicate-argument representation.
Given a triggerword or phrase in a sentence, this task solves tworelated prediction problems: (a) identifying the rela-tion label, and (b) identifying and labeling the argu-ments of the relation.This problem has been studied in the con-text of verb and nominal triggers using the Prop-Bank (Palmer et al 2005) and NomBank (Meyerset al 2004) annotations over the Penn Treebank,and also using the FrameNet lexicon (Fillmore etal., 2003), which allows arbitrary words to triggersemantic frames.This paper focuses on semantic relations ex-pressed by transitive prepositions1.
We can definethe two prediction tasks for prepositions as follows:identifying the relation label for a preposition, andpredicting the arguments of the relation.
Preposi-tions can mark arguments (both core and adjunct)for verbal and nominal predicates.
In addition, theycan also trigger relations that are not part of otherpredicates.
For example, in sentence (3) below, theprepositional phrase starting with to is an argumentof the verb visit, but the in triggers an independentrelation indicating the location of the aquarium.
(3) The children enjoyed the visit to the aquariumin Coney Island.FrameNet covers some prepositional relations, butallows only temporal, locative and directional sensesof prepositions to evoke frames, accounting for only3% of the targets in the SemEval 2007 shared taskof FrameNet parsing.
In fact, the state-of-the-artFrameNet parser of (Das et al 2010) does not con-sider any frame inducing prepositions.
(Baldwin et al 2009) highlights the importanceof studying prepositions for a complete linguistic1By transitive prepositions we refer to the standard usage ofprepositions that take an object.
In particular, we do not con-sider prepositional particles in our analysis.232analysis of sentences and surveys work in the NLPliterature that addresses the syntax and semanticsof prepositions.
One line of work (Ye and Bald-win, 2006) addressed the problem of prepositionsemantic role labeling by considering prepositionalphrases that act as arguments of verbs accordingto the PropBank annotation.
They built a systemthat predicts the labels of these prepositional phrasesalone.
However, by definition, this covered onlyverb-attached prepositions.
(Zapirain et al 2012)studied the impact of automatically learned selec-tional preferences for predicting arguments of verbsand showed that modeling prepositional phrases sep-arately improves the performance of argument pre-diction.Preposition semantics has also been studiedvia the Preposition Project (Litkowski and Har-graves, 2005) and the related SemEval 2007 sharedtask of word sense disambiguation of prepositions(Litkowski and Hargraves, 2007).
The Preposi-tion Project identifies preposition senses based ontheir definitions in the Oxford Dictionary of English.There are 332 different labels to be predicted with awide variance in the number of senses per preposi-tion ranging from 2 (during and as) to 25 (on).
Forexample, according to the preposition sense inven-tory, the preposition from in sentence (2) above willbe labeled with the sense from:12(9) to indicate acause.
(Dahlmeier et al 2009) added sense anno-tation to seven prepositions in four sections of thePenn Treebank with the goal of studying their inter-action with verb arguments.Using the SemEval data, (Tratz and Hovy, 2009)and (Hovy et al 2010) showed that the argumentsoffer an important cue to identify the sense of thepreposition and (Tratz, 2011) showed further im-provements by refining the sense inventory.
How-ever, though these works used a dependency parserto identify arguments, in order to overcome parsingerrors, they augment the parser?s predictions usingpart-of-speech based heuristics.We argue that, while disambiguating the senseof a preposition does indeed reveal nuances of itsmeaning, it leads to a proliferation of labels to bepredicted.
Most importantly, sense labels do nottransfer to other prepositions that express the samemeaning.
For example, both finish lunch beforenoon and finish lunch by noon express a Temporalrelation.
According to the Preposition Project, thesense label for the first preposition is before:1(1),and that for the second is by:17(4).
This both de-feats the purpose of identifying the relations to aidnatural language understanding and makes the pre-diction task harder than it should be: using the stan-dard word sense classification approach, we need totrain a separate classifier for each word because thelabels are defined per-preposition.
In other words,we cannot share features across the different prepo-sitions.
This motivates the need to combine suchsenses of prepositions into the same class label.In this direction, (O?Hara and Wiebe, 2009) de-scribes an inventory of preposition relations ob-tained using Penn Treebank function tags and frameelements from FrameNet.
(Srikumar and Roth,2011) merged preposition senses of seven preposi-tions into relation labels.
(Litkowski, 2012) alsosuggests collapsing the definitions of prepositionsinto a smaller set of semantic classes.
To aid bet-ter generalization and to reduce the label complex-ity, we follow this line of work to define a set of rela-tion labels which abstract word senses across prepo-sitions2.3 Preposition-triggered RelationsThis section describes the inventory of prepositionrelations introduced in this paper, and then identifiesthe components of the preposition relation extractionproblem.3.1 Preposition Relation InventoryWe build our relation inventory using the sense an-notation in the Preposition Project, focusing on the34 prepositions3 annotated for the SemEval-2007shared task of preposition sense disambiguation.As discussed in Section 2, we construct the in-ventory of preposition relations by collapsing se-mantically related preposition senses across differ-2Since the preposition sense data is annotated overFrameNet sentences, sense annotation can be used to extendFrameNet (Litkowski, 2012).
We believe that the abstract la-bels proposed in this paper can further help in this effort.3We consider the following prepositions: about, above,across, after, against, along, among, around, as, at, before, be-hind, beneath, beside, between, by, down, during, for, from, in,inside, into, like, of, off, on, onto, over, round, through, to, to-wards, and with.
This does not include multi-word prepositionssuch as because of and due to.233ent prepositions.
For each sense that is defined,the Preposition Project also specifies related prepo-sitions.
These definitions and related prepositionsprovide a starting point to identify senses that canbe merged across prepositions.
We followed thiswith a manual cleanup phase.
Some senses do notcleanly align with a single relation because the def-initions include idiomatic or figurative usage.
Forexample, the sense in:7(5) of the preposition in, ac-cording to the definition, includes both spatial andfigurative notions of the spatial sense (that is, bothin London and in a film).
In such cases, we sam-pled 20 examples from the SemEval 2007 trainingset and assigned the relation label based on majority.If sufficient examples could not be sampled, thesesenses were added to the label Other, which is nota semantically coherent category and represents the?overflow?
case.Overall, we have 32 labels, which are listed inTable 14.
A companion publication (available onthe authors?
website) provides detailed definitionsof each relation and the senses that were merged tocreate each label.
Since we define relations to begroups of preposition sense labels, each sense canbe uniquely mapped to a relation label.
Hence, wecan use the annotated sense data from SemEval 2007to obtain a corpus of relation-labeled sentences.To validate the labeling scheme, two native speak-ers of English annotated 200 sentences from theSemEval training corpus using only the definitionsof the labels as the annotation guidelines.
We mea-sured Cohen?s kappa coefficient (Cohen, 1960) be-tween the annotators to be 0.75 and also betweeneach annotator and the original corpus to be 0.76 and0.74 respectively.3.2 Preposition Relation ExtractionThe input to the prediction problem consists of apreposition in a sentence and the goal is to jointlymodel the following: (i) The relation expressed bythe preposition, and (ii) The arguments of the rela-tion, namely the governor and the object.We use sentence (2) in the introduction as our run-ning example the following discussion.
In our run-4Note that, even though we do not consider intransitiveprepositions, the definitions of some relations in Table 1 couldbe extended apply to prepositional particles such drive down(Direction) and run about (Manner).Relation Name ExampleActivity good at boxingAgent opened by AnnieAttribute walls of stoneBeneficiary fight for NapoleonCause died of cancerCo-Participants pick one among theseDestination leaving for LondonDirection drove towards the borderEndState driven to tearsExperiencer warm towards herInstrument cut with a knifeJourney travel by roadLocation living in LondonManner scream like an animalMediumOfCommunication new show on TVNumeric increase by 10%ObjectOfVerb murder of the boysOpponent/Contrast fight with himOther all othersParticipant/Accompanier steak with winePartWhole member of gangPhysicalSupport lean against the wallPossessor son of a friendProfessionalAspect works in publishingPurpose tools for making itRecipient unkind to herSeparation ousted from powerSource purchased from the shopSpecies city of PragueStartState recover from illnessTemporal arrived on MondayTopic books on ShakespeareTable 1: List of preposition relationsning example, the relation label is Cause.
We rep-resent the predicted relation label by r.Arguments The relation label crucially dependson correctly identifying the arguments of the prepo-sition, which are death and pneumonia in our run-ning example.
While a parser can identify the argu-ments of a preposition, simply relying on the parsermay impose an upper limit on the accuracy of rela-tion prediction.We build an oracle experiment to highlight thislimitation.
Table 2 shows the recall of the easy-firstdependency parser of (Goldberg and Elhadad, 2010)on Section 23 of the Penn Treebank for identifyingthe governor and object of prepositions.We define heuristics that generate a candidategovernors and objects for a preposition.
For the gov-234ernor, this set includes the previous verb or nounand for the object, it includes only the next noun.The row labeled Best(Parser, Heuristics) shows theperformance of an oracle predictor which selects thetrue governor/object if present among the parser?sprediction and the heuristics.
We see that, even forthe in-domain case, if we are able to re-rank the can-didates, we could achieve a big improvement in ar-gument identification.RecallGovernor ObjectParser 88.88 92.37Best(Parser, Heuristics) 92.50 93.06Table 2: Identifying governor and object of prepositionsin the Penn Treebank data.
Here, Best(Parser, Heuris-tics) reports the performance of an oracle that picks thetrue governor and object, if present among the candidatespresented by the parser and the heuristic.
This presentsan in-domain upper bound for governor and object detec-tion.
See text for further details.To overcome erroneous parser decisions, we en-tertain governor and object candidates proposedboth by the parser and the heuristics.
In the follow-ing discussion, we denote the chosen governor andobject by g and o respectively.Argument types While the primary purpose ofthis work is to model preposition relations and theirarguments, the relation prediction is strongly depen-dent on the semantic type of the arguments.
To il-lustrate this, consider the following incomplete sen-tence: The message was delivered at ?
?
?
.
Thispreposition can express both a Temporal or aLocation relation depending on the object (for ex-ample, noon vs. the doorstep).
(Agirre et al 2008) shows that modeling the se-mantic type of the arguments jointly with attachmentcan improve PP attachment accuracy.
In this work,we point out that argument types should be modeledjointly with both aspects of the problem of preposi-tion relation labeling.Types are an abstraction that capture commonproperties of groups of entities.
For example, Word-Net provides generalizations of words in the form oftheir hypernyms.
In our running example, we wishto generalize the relation label for death from pneu-monia to include cases such as suffering from flu.Figure 1 shows the hypernym hierarchy for the wordpneumonia.
In this case, synsets in the hypernymhierarchy, like pathological state or physical condi-tion, would also include ailments like flu.pneumonia=> respiratory disease=> disease=> illness=> ill health=> pathological state=> physical condition=> condition=> state=> attribute=> abstraction=> entityFigure 1: Hypernym hierarchy for the word pneumoniaWe define a semantic type to be a cluster of words.In addition to WordNet hypernyms, we also clusterverbs, nouns and adjectives using the dependency-based word similarity of (Lin, 1998) and treat clustermembership as types.
These are described in detailin Section 5.1.Relation prediction involves not only identifyingthe arguments, but also selecting the right semantictype for them, which together, help predicting therelation label.
Given an argument candidate and acollection of possible types (given by WordNet orthe similarity based clusters), we need to select oneof the types.
For example, in the WordNet case, weneed to pick one of the hypernyms in the hypernymhierarchy.
Thus, for the governor and object, wehave a set of type labels, comprised of one elementfor each type category.
We denote this by tg (gover-nor type) and to (object type) respectively.3.3 Problem definitionThe input to our prediction task is a preposition ina sentence.
Our goal is to jointly model the relationit expresses, the governor and the object of the rela-tion and the types of each argument (both WordNethypernyms and cluster membership).
We denote theinput by x, which consists not only of the prepo-sition but also a set of candidates for the governorand the object and, for each type category, the list oftypes for the governor and candidate.235The prediction, which we denote by y, consistsof the relation r, which can be one of the valid re-lation labels in Table 1 and the governor and object,denoted by g and o, each of which is one of text seg-ments proposed by the parser or the heuristics.
Ad-ditionally, y also consists of type predictions for thegovernor and object, denoted by tg and to respec-tively, each of which is a vector of labels, one foreach type category.
Table 3 summarizes the nota-tion described above.
We refer to the ith element ofvectors using subscripts and use the superscript ?
todenote gold labels.
Recall that we have gold labelsonly for the relation labels and not for arguments andtheir types.Symbol Meaningx Input (pre-processed sentence andpreposition)r relation label for the prepositiong, o governor and object of the relationtg, to vectors of type assignments forgovernor and object respectivelyy Full structure (r, g, o, tg, to)Table 3: Summary of notation4 Learning preposition relationsA key challenge in modeling preposition relations isthat our training data only annotates the relation la-bels and not the arguments and types.
In this section,we introduce two approaches for predicting preposi-tion relations using this data.4.1 Feature RepresentationWe use the notation ?
(x,y) to indicate the featurefunction for an input x and the full output y. Webuild ?
using the features of the components of y:1.
Arguments: For g and o, which represent anassignment to the governor and object, we de-note the features extracted from the argumentsas ?A(x, g) and ?A(x, o) respectively.2.
Types: Given a type assignment tgi to the ithtype category of the governor, we define fea-tures ?T (x, g, tgi ).
Similarly, we define features?T (x, o, toi ) for the types of the object.We combine the argument and their type features todefine the features for classifying the relation, whichwe denote by ?
(x, g, o, tg, to):?
=?a?
{g,o}(?A(x, a) +?i?T (x, a, tai ))(1)Section 5 describes the actual features used in ourexperiments.Observe that given the arguments and their types,the task of predicting relations is simply a multiclassclassification problem.
Thus, following the standardconvention for multiclass classification, the overallfeature representation for the relation and argumentprediction is defined by conjoining the relation rwith features for the corresponding arguments andtypes, ?.
This gives us the full feature representa-tion, ?
(x,y).4.2 Model 1: Predicting only relationsThe first model aims at predicting only the rela-tion labels and not the arguments and types.
Thisfalls into the standard multiclass classification set-ting, where we wish to predict one of 32 labels.
Todo so, we sum over all the possible assignments tothe rest of the structure and define features for theinputs as??
(x) =?g,o,tg ,to?
(x, g, o, tg, to) (2)Effectively, doing so uses all the governor and ob-ject candidates and all their semantic types to geta feature representation for the relation classifica-tion problem.
Once again, for a relation label r, theoverall feature representation is defined by conjoin-ing the relation r with the features for that relation?
?, which we write as ?R(x, r).
Note that this sum-mation is computationally inexpensive in our casebecause the sum decomposes according to equation(1).
With a learned weight vector w, the relationlabel is predicted asr = arg maxr?wT?R(x, r?)
(3)We use a structural SVM (Tsochantaridis et al2004) to train a weight vector w that predicts the re-lation label as above.
The training is parameterizedby C, which represents the tradeoff between gener-alization and the hinge loss.2364.3 Model 2: Learning from partialannotationsIn the second model, even though our annotationdoes not provide gold labels for arguments andtypes, our goal is to predict them.
At inference time,if we had a weight vector w, we could predict thefull structure using inference as follows:y = arg maxy?wT?
(x,y) (4)We propose an iterative learning algorithm to learnthis weight vector.In the following discussion, for a labeled example(x,y?
), we refer to the missing part of its structureas h(y?).
That is, h(y?)
is the assignment to thearguments of the relation and their types.
We use thenotation r(y) to denote the relation label specifiedby a structure y.Our learning algorithm is closely related to re-cently developed latent variable based frameworks(Yu and Joachims, 2009; Chang et al 2010a; Changet al 2010b), where the supervision provides onlypartial annotation.
We begin by defining two addi-tional inference procedures:1.
Latent Inference: Given a weight vector wand a partially labeled example (x,y?
), we can?complete?
the rest of the structure by infer-ring the highest scoring assignment to the miss-ing parts.
In the algorithm, we call this pro-cedure LatentInf(w,x,y?
), which solves thefollowing maximization problem:y?
= arg maxy wT?
(x,y), (5)s.t.
r(y) = r(y?).2.
Loss augmented inference: This is a variantof the the standard loss augmented inferencefor structural SVMs, which solves the follow-ing maximization problem for a given x andfully labeled y?
:arg maxywT?
(x,y) + ?(y,y?)
(6)Here, ?(y,y?)
denotes the loss function.
Inthe standard structural SVMs, the loss is overthe entire structure.
In the Latent StructuralSVM formulation of (Yu and Joachims, 2009),the loss is defined only over the part of thestructure with the gold label.
In this work, weuse the standard Hamming loss over the entirestructure, but scale the loss for the elements ofh(y) by a parameter ?
< 1.
This is a gen-eralization of the latent structural SVM, whichcorresponds to the setting ?
= 0.
The intu-ition behind having a non-zero ?
is that in ad-dition to penalizing the learning algorithm if itviolates the annotated part of the structure, wealso incorporate a small penalty for the rest ofthe structure.Using these two inference procedures, we definethe learning algorithm as Algorithm 1.
The weightvector is initialized using Model 1.
The algorithmthen finds the best arguments and types for all ex-amples in the training set (steps 3-5).
Doing sogives an estimate of the arguments and types foreach example, giving us ?fully labeled?
structureddata.
The algorithm then proceeds to use this data totrain a new weight vector using the standard struc-tural SVM with the loss augmented inference listedabove (step 6).
These two steps are repeated severaltimes.
Note that as with the summation in Model1, solving the inference problems described above iscomputationally inexpensive.Algorithm 1 Algorithm for learning Model 2Input: Examples D = {xi, r(y?i )}, where exam-ples are labeled only with the relation labels.1: Initialize weight vector w using Model 12: for t = 1, 2, ?
?
?
do3: for (xi,y?i ) ?
D do4: y?i ?
LatentInf(w,xi,y?i ) (Eq.
5)5: end for6: w ?
LearnSSVM({xi, y?i}) with the lossaugmented inference of Eq.
67: end for8: return wAlgorithm 1 is parameterized by C and ?.
Theparameter ?
controls the extent to which the hypoth-esized labels according to the previous iteration?sweight vector influence the learning.2374.4 Joint inference between preposition sensesand relationsBy defining preposition relations as disjoint sets ofpreposition senses, we effectively have a hierarchi-cal relationship between senses and relations.
Thissuggests that joint inference can be employed be-tween sense and relation predictions with a validityconstraint connecting the two.
The idea of employ-ing inference to combine independently trained pre-dictors to obtain a coherent output structure has beenused for various NLP tasks in recent years, startingwith the work of (Roth and Yih, 2004; Roth and Yih,2007).We use the features defined by (Hovy et al 2010),which we write as ?s(x, s) for a given input x andsense label s, and train a separate preposition sensemodel on the SemEval data with features ?s(x, s)using the structural SVM algorithm.
Thus, we havetwo weight vectors ?
the one for predicting preposi-tion relations described earlier, and the prepositionsense weight vector.
At prediction time, for a giveninput, we find the highest scoring joint assignment tothe relation, arguments and types and the sense, sub-ject to the constraint that the sense and the relationagree according to the definition of the relations.5 Experiments and ResultsThe primary research goal of our experiments is toevaluate the different models (Model 1, Model 2 andjoint relation-sense inference) for predicting prepo-sition relations.
In additional analysis experiments,we also show that the definition of preposition rela-tions indeed captures cross-preposition semantics bytaking advantage of shared features and also high-light the need for going beyond the syntactic parser.5.1 Types and FeaturesTypes As described in Section 3, we use WordNethypernyms as one of the type categories.
We use allhypernyms within four levels in the hypernym hier-archy for all senses.The second type category is defined by word-similarity driven clusters.
We briefly describe theclustering process here.
The thesaurus of (Lin,1998) specifies similar lexical items for a givenword along with a similarity score from 0 to 1.
Ittreats nouns, verbs and adjectives separately.
Weuse the score to cluster groups of similar words us-ing a greedy set-covering approach.
Specifically,we randomly select a word which is not yet in acluster as the center of a new cluster and add allwords whose score is greater than ?
to it.
We re-peat this process till all words are in some clus-ter.
A word can appear in more than one clusterbecause all words similar to the cluster center areadded to the cluster.
We repeat this process for?
?
{0.1, 0.125, 0.15, 0.175, 0.2, 0.25}.
By increas-ing the value of ?, the clusters become more selec-tive and hence smaller.
Table 4 shows example nounclusters created using ?
= 0.15.
For a given word,identifiers for clusters to which the word belongsserve as type label candidates for this type category5.Features Our argument features, denoted by ?Ain Section 4.1, are derived from the prepositionsense feature set of (Hovy et al 2010) and extractthe following from the argument: 1.
Word, part-of-speech, lemma and capitalization indicator, 2.
Con-flated part-of-speech (one of Noun, Verb, Adjective,Adverb, and Other), 3.
Indicator for existence inWordNet, 4.
WordNet synsets for the first and allsenses, 5.
WordNet lemma, lexicographer file namesand part, member and substance holonyms, 6.
Rogetthesaurus divisions for the word, 7.
The first and lasttwo and three letters, and 8.
Indicators for known af-fixes.
Our type features (?T ) are simply indicatorsfor the type label, conjoined with the type category.One advantage of abstracting word senses into re-lations is that we can share features across differentprepositions.
The base feature set (for both typesand arguments) defined above does not encode in-formation about the preposition to be classified.
Wedo so by conjoining the features with the preposi-tion.
In addition, since the relation labels are sharedacross all prepositions, we include the base featuresas a shared representation between prepositions.We consider two variants of our feature sets.We refer to the features described above as thetyped features.
In addition, we define thetyped+gen features by conjoining argument andtype features of typed with the name of the genera-tor that proposes the argument.
Recall that governorcandidates are proposed by the dependency parser,or by the heuristics described earlier.
Hence, for5The clusters can be downloaded from the authors?
website.238Jimmy Carter; Ronald Reagan; richard nixon; George Bush; Lyndon Johnson; Richard M. Nixon; Gerald Fordmetalwork; porcelain; handicraft; jade; bronzeware; carving; pottery; ceramic; earthenware; jewelry; stoneware; lacquerwaredegradation; erosion; pollution; logging; desertification; siltation; urbanization; felling; poaching; soil erosion; depletion;water pollution; deforestationexpert; Wall Street analyst; analyst; economist; telecommunications analyst; strategist; media analystfox news channel; NBC News; MSNBC; Fox News; CNBC; CNNfn; C-SpanTuesdays; Wednesdays; weekday; Mondays; Fridays; Thursdays; sundays; SaturdaysTable 4: Examples of noun clusters generated using the set-covering approach for ?
= 0.15a governor, the typed+gen features would conjointhe corresponding typed features with one of parser,previous-verb, previous-noun, previous-adjective, orprevious-word.5.2 Experimental setup and dataAll our experiments are based on the Sem-Eval 2007 data for preposition sense disambigua-tion (Litkowski and Hargraves, 2007) comprisingword sense annotation over 16176 training and8058 examples of prepositions labeled with theirsenses.
We pre-processed sentences with part-of-speech tags using the Illinois POS tagger and de-pendency graphs using the parser of (Goldberg andElhadad, 2010)6.
For the experiments described be-low, we used the relation-annotated training set totrain the models and evaluate accuracy of predictionon the test set.We chose the structural SVM parameter C usingfive-fold cross-validation on a 1000 random exam-ples chosen from the training set.
For Model 2, wepicked ?
= 0.1 using a validation set consisting ofa separate set of 1000 training examples.
We ranAlgorithm 1 for 20 rounds.Predicting the most frequent relation for a prepo-sition gives an accuracy of 21.18%.
Even thoughthe performance of the most-frequent relation labelis poor, it does not represent the problem?s difficultyand is not a good baseline.
To compare, for prepo-sition senses, using features from the neighboringwords, (Ye and Baldwin, 2007) obtained an accuracyof 69.3%, and with features designed for the prepo-sition sense task, (Hovy et al 2010) get up to 84.8%accuracy for the task.
Our re-implementation of thelatter system using a different set of pre-processingtools gets an accuracy of 83.53%.For preposition relations, our baseline system for6We used the Curator (Clarke et al 2012) for all pre-processing.relation labeling uses the typed feature set, but with-out any type information.
This produces an accuracyof 88.01% with Model 1 and 88.64% with Model 2.We report statistical significance of results using ourimplementation of Dan Bikel?s stratified-shufflingbased statistical significance tester7.5.3 Main results: Relation predictionOur main result, presented in Table 5, compares thebaseline model (without types) against other sys-tems, using both models described in Section 4.First, we see that adding type information (typed)improves performance over the baseline.
Expand-ing the feature space (typed+gen) gives further im-provements.
Finally, jointly predicting the relationswith preposition senses gives another improvement.Setting AccuracyModel 1 Model 2No types 88.01 88.64typed 88.77 89.14typed+gen 89.90?
89.43?Joint typed+gen & sense 89.99?
90.26?
?Table 5: Main results: Accuracy of relation labeling.Results in bold are statistically significant (p < 0.01)improvements over the system that is unaware of types.Superscripts ?
and ?
indicate significant improvementsover typed and typed+gen respectively at p < 0.01.
ForModel 2, the improvement of typed over the model with-out types is significant at p < 0.05.Our objective is not predicting preposition sense.However, we observe that with Model 2, jointly pre-dicting the sense and relations improves not only theperformance of relation identification, but via jointinference between relations and senses also leads toa large improvement in sense prediction accuracy.Table 6 shows the accuracy for sense prediction.
We7http://www.cis.upenn.edu/?dbikel/software.html239see that while Model 1 does not lead to a significantimprovement in the accuracy, Model 2 gives an ab-solute improvement of over 1%.Setting Sense accuracyHovy (re-implementation) 83.53Joint + Model 1 83.78Joint + Model 2 84.78?Table 6: Sense prediction performance.
Joint inferencewith Model 1, while improving relation performance,does not help sense accuracy in comparison to our re-implementation of the Hovy sense disambiguation sys-tem.
However, with Model 2, the improvement is statis-tically significant at p < 0.01.5.4 Ablation experimentsFeature sharing across prepositions In our firstanalysis experiment, we seek to highlight the utilityof sharing features between different prepositions.To do so, we compare the performance of a sys-tem trained without shared features against the type-independent system, which uses shared features.
Todiscount the influence of other factors, we use Model1 in the typed setting without any types.
Table 7reports the accuracy of relation prediction for thesetwo feature sets.
We observed a similar improve-ment in performance even when type features areadded or the setting is changed to typed+gen or withModel 2.Setting AccuracyIndependent 87.17+ Shared 88.01Table 7: Comparing the effect of feature sharing acrossprepositions.
We see that having a shared representationthat goes across prepositions improves accuracy of rela-tion prediction (p < 0.01).Different argument candidate generators Oursecond ablation study looks at the effect of the var-ious argument candidate generators.
Recall that inaddition to the dependency governor and object, ourmodels also use the previous word, the previousnoun, adjective and verb as governor candidates andthe next noun as object candidate.
We refer to thecandidates generated by the parser as Parser onlyand the others as Heuristics only.
Table 8 comparesthe performance of these two argument candidategenerators against the full set using Model 1 in boththe typed and typed+gen settings.We see that the heuristics give a better accu-racy than the parser based system.
This is becausethe heuristics often contain the governor/object pre-dicted by the dependency.
This is not always thecase, though, because using all generators gives aslightly better performing system (not statisticallysignificant).
In the overall system, we retain the de-pendency parser as one of the generators in order tocapture long-range governor/object candidates thatmay not be in the set selected by the heuristics.Feature setsGenerator typed typed+genParser only 87.12 87.12Heuristics only 87.63 88.84All 88.01 89.12Table 8: The performance of different argument candi-date generators.
We see that considering a larger set ofcandidate generators gives a big accuracy improvement.6 DiscussionThere are two key differences between Model 1 and2.
First, the former predicts only the relation label,while the latter predicts the entire structure.
Table 9shows example predictions of Model 2 for relationlabel and WordNet argument types.
These examplesshow how the argument types can be thought of asan explanation for the choice of relation label.Input Relation Hypernymsgovernor objectdied of pneumonia Cause experience diseasesuffered from flu Cause experience diseaserecovered from flu StartState change diseaseTable 9: Example predictions according to Model 2.
Thehypernyms column shows a representative of the synsetchosen for the WordNet types.
We see that in the com-bination of experience and disease suggests the relationCause while the change and disease indicate the rela-tion StartState.The main difference between the two models isin the treatment of the unlabeled (or latent) parts ofthe structure (namely, the arguments and the types)during training and inference.
During training, for240each example, Model 1 aggregates features from allgovernors and objects even if they are possibly ir-relevant, which may lead to a much bigger model interms of the number of active weights.
On the otherhand, for Model 2, Algorithm 1 uses the single high-est scoring prediction of the latent variables, accord-ing to the current parameters, to refine the parame-ters.
Indeed, in our experiments, we observed thatthe number of non-zero weights in the weight vec-tor of Model 2 is much smaller than that of Model1.
For instance, in the typed setting, the weight vec-tor for Model 1 had 2.57 million elements while thatfor Model 2 had only 1.0 million weights.
Similarly,for the typed+gen setting, Model 1 had 5.41 millionnon-zero elements in the weight vector while Model2 had only 2.21 million non-zero elements.The learning algorithm itself is a generalizationof the latent structural SVM of (Yu and Joachims,2009).
By setting ?
to zero, we get the latent struc-ture SVM.
However, we found via cross-validationthat this is not the best setting of the parameter.
Atheoretical understanding of the sparsity of weightslearned by the algorithm and a study of its conver-gence properties is an avenue of future research.7 ConclusionWe addressed the problem of modeling semantic re-lations expressed by prepositions.
We approachedthis task by defining a set of preposition relationsthat combine preposition senses across prepositions.Doing so allowed us to leverage existing annotatedpreposition sense data to induce a corpus for prepo-sition labels.
We modeled preposition relations interms of its arguments, namely the governor and ob-ject of the preposition, and the semantic types ofthe arguments.
Using a generalization of the latentstructural SVM, we trained a relation, argument andtype predictor using only annotated relation labels.This allowed us to get an accuracy of 89.43% on re-lation prediction.
By employing joint inference witha preposition sense predictor, we further improvedthe relation accuracy to 90.23%.AcknowledgmentsThe authors wish to thank Martha Palmer, Nathan Schneider,the anonymous reviewers and the editor for their valuable feed-back.
The authors gratefully acknowledge the support of theDefense Advanced Research Projects Agency (DARPA) Ma-chine Reading Program under Air Force Research Laboratory(AFRL) prime contract no.
FA8750-09-C-0181.
This materialis also based on research sponsored by DARPA under agree-ment number FA8750-13-2-0008.
The U.S. Government is au-thorized to reproduce and distribute reprints for Governmentalpurposes notwithstanding any copyright notation thereon.
Theviews and conclusions contained herein are those of the authorsand should not be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed or implied, ofDARPA, AFRL or the U.S. Government.ReferencesE.
Agirre, T. Baldwin, and D. Martinez.
2008.
Improv-ing parsing and PP attachment performance with senseinformation.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 317?325, Columbus, USA.T.
Baldwin, V. Kordoni, and A. Villavicencio.
2009.Prepositions in applications: A survey and introduc-tion to the special issue.
Computational Linguistics,35(2):119?149.M.
Chang, D. Goldwasser, D. Roth, and V. Srikumar.2010a.
Discriminative learning over constrained latentrepresentations.
In Proceedings of the Annual Meet-ing of the North American Association of Computa-tional Linguistics (NAACL), pages 429?437, Los An-geles, USA.M.
Chang, V. Srikumar, D. Goldwasser, and D. Roth.2010b.
Structured output learning with indirect super-vision.
In Proceedings of the International Conferenceon Machine Learning (ICML), pages 199?206, Haifa,Israel.J.
Clarke, V. Srikumar, M. Sammons, and D. Roth.
2012.An NLP Curator (or: How I Learned to Stop Wor-rying and Love NLP Pipelines).
In Proceedings ofthe International Conference on Language Resourcesand Evaluation (LREC), pages 3276?3283, Istanbul,Turkey.J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Measurement,20:37?46.D.
Dahlmeier, H. T. Ng, and T. Schultz.
2009.
Jointlearning of preposition senses and semantic roles ofprepositional phrases.
In Proceedings of the Confer-ence on Empirical Methods for Natural Language Pro-cessing (EMNLP), pages 450?458, Singapore.D.
Das, N. Schneider, D. Chen, and N. Smith.
2010.Probabilistic frame-semantic parsing.
In Proceedingsof Human Language Technologies: The 2010 Annual241Conference of the North American Chapter of the As-sociation for Computational Linguistics, pages 948?956, Los Angeles, USA.C.
Fillmore, C. Johnson, and M. Petruck.
2003.
Back-ground to FrameNet.
International Journal of Lexi-cography, 16(3):235?250.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.Y.
Goldberg and M. Elhadad.
2010.
An efficient algo-rithm for easy-first non-directional dependency pars-ing.
In Proceedings of Human Language Technolo-gies: The 2010 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 742?750, Los Angeles, USA.D.
Hovy, S. Tratz, and E. Hovy.
2010.
What?s in a prepo-sition?
dimensions of sense disambiguation for an in-teresting word class.
In Coling 2010: Posters, pages454?462, Beijing, China.D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 768?774, Montreal, Canada.K.
Litkowski and O. Hargraves.
2005.
The PrepositionProject.
In ACL-SIGSEM Workshop on the LinguisticDimensions of Prepositions and their Use in Computa-tional Linguistics Formalisms and Applications, pages171?179, Colchester, UK.K.
Litkowski and O. Hargraves.
2007.
SemEval-2007Task 06: Word-Sense Disambiguation of Prepositions.In Proceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007), pages 24?29, Prague, Czech Republic.K.
Litkowski.
2012.
Proposed Next Steps for The Prepo-sition Project.
Technical Report 12-01, CL Research.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
The Nom-Bank project: An interim report.
In HLT-NAACL 2004Workshop: Frontiers in Corpus Annotation, pages 24?31, Boston, USA.T.
O?Hara and J. Wiebe.
2009.
Exploiting semantic roleresources for preposition disambiguation.
Computa-tional Linguistics, 35(2):151?184.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of SemanticRoles.
Computational Linguistics, 31(1):71?106.M.
Palmer, D. Gildea, and N. Xue.
2010.
Semantic RoleLabeling, volume 3.
Morgan & Claypool Publishers.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics, 34(2).D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.In Proceedings of the Annual Conference on Compu-tational Natural Language Learning (CoNLL), pages1?8, Boston, USA.D.
Roth and W. Yih.
2007.
Global inference for en-tity and relation identification via a linear program-ming formulation.
Introduction to Statistical Rela-tional Learning.V.
Srikumar and D. Roth.
2011.
A joint model forextended semantic role labeling.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), Edinburgh, Scotland.S.
Tratz and D. Hovy.
2009.
Disambiguation of prepo-sition sense using linguistically motivated features.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,Companion Volume: Student Research Workshop andDoctoral Consortium, pages 96?100, Boulder, USA.S.
Tratz.
2011.
Semantically-enriched Parsing for Natu-ral Language Understanding.
Ph.D. thesis, Universityof Southern California.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interde-pendent and structured output spaces.
In Proceedingsof the International Conference on Machine Learning(ICML), pages 104?111, Banff, Canada.P.
Ye and T. Baldwin.
2006.
Semantic role labelingof prepositional phrases.
ACM Transactions on AsianLanguage Information Processing (TALIP), 5(3):228?244.P.
Ye and T. Baldwin.
2007.
MELB-YB: Prepositionsense disambiguation using rich semantic features.In Proceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007), pages 241?244, Prague, Czech Republic.C.
Yu and T. Joachims.
2009.
Learning structural SVMswith latent variables.
In Proceedings of the Inter-national Conference on Machine Learning (ICML),pages 1?8, Montreal, Canada.B.
Zapirain, E. Agirre, L. Ma`rquez, and M. Surdeanu.2012.
Selectional preferences for semantic role classi-fication.
Computational Linguistics, pages 1?33.242
