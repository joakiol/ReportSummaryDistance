Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 22?31,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsEffective Use of Function Words for Rule Generalizationin Forest-Based TranslationXianchao Wu?
Takuya Matsuzaki?
Jun?ichi Tsujii???
?Department of Computer Science, The University of Tokyo?School of Computer Science, University of Manchester?National Centre for Text Mining (NaCTeM){wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jpAbstractIn the present paper, we propose the ef-fective usage of function words to generategeneralized translation rules for forest-basedtranslation.
Given aligned forest-string pairs,we extract composed tree-to-string translationrules that account for multiple interpretationsof both aligned and unaligned target func-tion words.
In order to constrain the ex-haustive attachments of function words, welimit to bind them to the nearby syntacticchunks yielded by a target dependency parser.Therefore, the proposed approach can notonly capture source-tree-to-target-chunk cor-respondences but can also use forest structuresthat compactly encode an exponential num-ber of parse trees to properly generate targetfunction words during decoding.
Extensiveexperiments involving large-scale English-to-Japanese translation revealed a significant im-provement of 1.8 points in BLEU score, ascompared with a strong forest-to-string base-line system.1 IntroductionRule generalization remains a key challenge forcurrent syntax-based statistical machine translation(SMT) systems.
On the one hand, there is a ten-dency to integrate richer syntactic information intoa translation rule in order to better express the trans-lation phenomena.
Thus, flat phrases (Koehn et al,2003), hierarchical phrases (Chiang, 2005), and syn-tactic tree fragments (Galley et al, 2006; Mi andHuang, 2008; Wu et al, 2010) are gradually used inSMT.
On the other hand, the use of syntactic phrasescontinues due to the requirement for phrase cover-age in most syntax-based systems.
For example,Mi et al (2008) achieved a 3.1-point improvementin BLEU score (Papineni et al, 2002) by includingbilingual syntactic phrases in their forest-based sys-tem.
Compared with flat phrases, syntactic rules aregood at capturing global reordering, which has beenreported to be essential for translating between lan-guages with substantial structural differences, suchas English and Japanese, which is a subject-object-verb language (Xu et al, 2009).Forest-based translation frameworks, which makeuse of packed parse forests on the source and/or tar-get language side(s), are an increasingly promisingapproach to syntax-based SMT, being both algorith-mically appealing (Mi et al, 2008) and empiricallysuccessful (Mi and Huang, 2008; Liu et al, 2009).However, forest-based translation systems, and, ingeneral, most linguistically syntax-based SMT sys-tems (Galley et al, 2004; Galley et al, 2006; Liuet al, 2006; Zhang et al, 2007; Mi et al, 2008;Liu et al, 2009; Chiang, 2010), are built upon wordaligned parallel sentences and thus share a criticaldependence on word alignments.
For example, evena single spurious word alignment can invalidate alarge number of otherwise extractable rules, and un-aligned words can result in an exponentially largeset of extractable rules for the interpretation of theseunaligned words (Galley et al, 2006).What makes word alignment so fragile?
In or-der to investigate this problem, we manually ana-lyzed the alignments of the first 100 parallel sen-tences in our English-Japanese training data (to beshown in Table 2).
The alignments were generatedby running GIZA++ (Och and Ney, 2003) and thegrow-diag-final-and symmetrizing strategy (Koehnet al, 2007) on the training set.
Of the 1,324 wordalignment pairs, there were 309 error pairs, among22which there were 237 target function words, whichaccount for 76.7% of the error pairs1.
This indicatesthat the alignments of the function words are moreeasily to be mistaken than content words.
More-over, we found that most Japanese function wordstend to align to a few English words such as ?of?and ?the?, which may appear anywhere in an Englishsentence.
Following these problematic alignments,we are forced to make use of relatively large Englishtree fragments to construct translation rules that tendto be ill-formed and less generalized.This is the motivation of the present approach ofre-aligning the target function words to source treefragments, so that the influence of incorrect align-ments is reduced and the function words can be gen-erated by tree fragments on the fly.
However, thecurrent dominant research only uses 1-best trees forsyntactic realignment (Galley et al, 2006; May andKnight, 2007; Wang et al, 2010), which adverselyaffects the rule set quality due to parsing errors.Therefore, we realign target function words to apacked forest that compactly encodes exponentiallymany parses.
Given aligned forest-string pairs, weextract composed tree-to-string translation rules thataccount for multiple interpretations of both alignedand unaligned target function words.
In order to con-strain the exhaustive attachments of function words,we further limit the function words to bind to theirsurrounding chunks yielded by a dependency parser.Using the composed rules of the present study ina baseline forest-to-string translation system resultsin a 1.8-point improvement in the BLEU score forlarge-scale English-to-Japanese translation.2 Backgrounds2.1 Japanese function wordsIn the present paper, we limit our discussionon Japanese particles and auxiliary verbs (Martin,1975).
Particles are suffixes or tokens in Japanesegrammar that immediately follow modified con-tent words or sentences.
There are eight types ofJapanese function words, which are classified de-pending on what function they serve: case markers,parallel markers, sentence ending particles, interjec-1These numbers are language/corpus-dependent and are notnecessarily to be taken as a general reflection of the overall qual-ity of the word alignments for arbitrary language pairs.tory particles, adverbial particles, binding particles,conjunctive particles, and phrasal particles.Japanese grammar also uses auxiliary verbs togive further semantic or syntactic information aboutthe preceding main or full verb.
Alike English, theextra meaning provided by a Japanese auxiliary verbalters the basic meaning of the main verb so that themain verb has one or more of the following func-tions: passive voice, progressive aspect, perfect as-pect, modality, dummy, or emphasis.2.2 HPSG forestsFollowing our precious work (Wu et al, 2010), weuse head-drive phrase structure grammar (HPSG)forests generated by Enju2 (Miyao and Tsujii, 2008),which is a state-of-the-art HPSG parser for English.HPSG (Pollard and Sag, 1994; Sag et al, 2003) is alexicalist grammar framework.
In HPSG, linguisticentities such as words and phrases are representedby a data structure called a sign.
A sign gives afactored representation of the syntactic features ofa word/phrase, as well as a representation of theirsemantic content.
Phrases and words represented bysigns are collected into larger phrases by the appli-cations of schemata.
The semantic representation ofthe new phrase is calculated at the same time.
Assuch, an HPSG parse forest can be considered tobe a forest of signs.
Making use of these signs in-stead of part-of-speech (POS)/phrasal tags in PCFGresults in a fine-grained rule set integrated with deepsyntactic information.For example, an aligned HPSG forest3-string pairis shown in Figure 1.
For simplicity, we only drawthe identifiers for the signs of the nodes in the HPSGforest.
Note that the identifiers that start with ?c?
de-note non-terminal nodes (e.g., c0, c1), and the iden-tifiers that start with ?t?
denote terminal nodes (e.g.,t3, t1).
In a complete HPSG forest given in (Wu etal., 2010), the terminal signs include features suchas the POS tag, the tense, the auxiliary, the voice ofa verb, etc..
The non-terminal signs include featuressuch as the phrasal category, the name of the schema2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html3The forest includes three parse trees rooted at c0, c1, andc2.
In the 1-best tree, ?by?
modifies the passive verb ?verified?.Yet in the 2- and 3-best tree, ?by?
modifies ?this result was ver-ified?.
Furthermore, ?verified?
is an adjective in the 2-best treeand a passive verb in the 3-best tree.23jikken niyotte kono kekka ga sa re ta kensyouRealign target function words??
0????
1??
2??
3?
4 ?
6?
7?
8??
5thisresultwasverifiedbytheexperimentst3 t1 t4 t8 t10 t7 t0 t6 t5 t2 t9c9 c10 c16 c22 c4 c21 c12 c18 c19 c14 c15c23c8c13c5 c17c3c6c2c7c11c0c20c11-best tree 2-best tree 3-best treeexperimentsbythisresultverifiedc1??
0????
1??
2??
3?
4?
6?
7?
8??
5C1 C2 C3 C4thisresultwasverifiedbytheexperimentst3 t1 t4 t8 t10 t7 t0 t6 t5 t2 t9c9 c16 c22 c45-7 | 5-8 c125-7 | 5-8  c18 c19 c14 c15c2 c0c215-7 | 5-8c23 c8c13c5 c17c3c6c7c11 c20c103 | 3-4Figure 1: Illustration of an aligned HPSG forest-string pair for English-to-Japanese translation.
The chunk-leveldependency tree for the Japanese sentence is shown as well.applied in the node, etc..3 Composed Rule ExtractionIn this section, we first describe an algorithm thatattaches function words to a packed forest guidedby target chunk information.
That is, given a triple?FS , T, A?, namely an aligned (A) source forest(FS) to target sentence (T ) pair, we 1) tailor thealignment A by removing the alignments for tar-get function words, 2) seek attachable nodes in thesource forest FS for each function word, and 3) con-struct a derivation forest by topologically travers-ing FS .
Then, we identify minimal and composedrules from the derivation forest and estimate theprobabilities of rules and scores of derivations us-ing the expectation-maximization (EM) (Dempsteret al, 1977) algorithm.3.1 DefinitionsIn the proposed algorithm, we make use of the fol-lowing definitions, which are similar to those de-scribed in (Galley et al, 2004; Mi and Huang, 2008):?
s(?
): the span of a (source) node v or a (target)chunk C, which is an index set of the words that24v or C covers;?
t(v): the corresponding span of v, which is anindex set of aligned words on another side;?
c(v): the complement span of v, which is theunion of corresponding spans of nodes v?
thatshare an identical parse tree with v but are nei-ther antecedents nor descendants of v;?
PA: the frontier set of FS , which containsnodes that are consistent with an alignment A(gray nodes in Figure 1), i.e., t(v) ?= ?
andclosure(t(v)) ?
c(v) = ?.The function closure covers the gap(s) that mayappear in the interval parameter.
For example,closure(t(c3)) = closure({0-1, 4-7}) = {0-7}.Examples of the applications of these functions canbe found in Table 1.
Following (Galley et al,2006), we distinguish between minimal and com-posed rules.
The composed rules are generated bycombining a sequence of minimal rules.3.2 Free attachment of target function words3.2.1 MotivationWe explain the motivation for the present researchusing an example that was extracted from our train-ing data, as shown in Figure 1.
In the alignment ofthis example, three lines (in dot lines) are used toalign was and the with ga (subject particle), and waswith ta (past tense auxiliary verb).
Under this align-ment, we are forced to extract rules with relativelylarge tree fragments.
For example, by applying theGHKM algorithm (Galley et al, 2004), a rule rootedat c0 will take c7, t4, c4, c19, t2, and c15 as theleaves.
The final tree fragment, with a height of 7,contains 13 nodes.
In order to ensure that this ruleis used during decoding, we must generate subtreeswith a height of 7 for c0.
Suppose that the input for-est is binarized and that |E| is the average numberof hyperedges of each node, then we must generateO(|E|26?1) subtrees4 for c0 in the worst case.
Thus,4For one (binarized) hyperedge e of a node, suppose thereare x subtrees in the left tail node and y subtrees in the right tailnode.
Then the number of subtrees guided by e is (x + 1) ?(y+1).
Thus, the recursive formula is Nh = |E|(Nh?1 +1)2,where h is the height of the hypergraph and Nh is the numberof subtrees.
When h = 1, we let Nh = 0.the existence of these rules prevents the generaliza-tion ability of the final rule set that is extracted.In order to address this problem, we tailor thealignment by ignoring these three alignment pairs indot lines.
For example, by ignoring the ambiguousalignments on the Japanese function words, we en-large the frontier set to include from 12 to 19 of the24 non-terminal nodes.
Consequently, the numberof extractable minimal rules increases from 12 (withthree reordering rules rooted at c0, c1, and c2) to19 (with five reordering rules rooted at c0, c1, c2,c5, and c17).
With more nodes included in the fron-tier set, we can extract more minimal and composedmonotonic/reordering rules and avoid extracting theless generalized rules with extremely large tree frag-ments.3.2.2 Why chunking?In the proposed algorithm, we use a target chunkset to constrain the attachment explosion problembecause we use a packed parse forest instead of a 1-best tree, as in the case of (Galley et al, 2006).
Mul-tiple interpretations of unaligned function words foran aligned tree-string pair result in a derivation for-est.
Now, we have a packed parse forest in whicheach tree corresponds to a derivation forest.
Thus,pruning free attachments of function words is prac-tically important in order to extract composed rulesfrom this ?
(derivation) forest of (parse) forest?.In the English-to-Japanese translation test case ofthe present study, the target chunk set is yieldedby a state-of-the-art Japanese dependency parser,Cabocha v0.535 (Kudo and Matsumoto, 2002).
Theoutput of Cabocha is a list of chunks.
A chunk con-tains roughly one content word (usually the head)and affixed function words, such as case markers(e.g., ga) and verbal morphemes (e.g., sa re ta,which indicate past tense and passive voice).
Forexample, the Japanese sentence in Figure 1 is sepa-rated into four chunks, and the dependencies amongthese chunks are identified by arrows.
These arrowspoint out the head chunk that the current chunk mod-ifies.
Moreover, we also hope to gain a fine-grainedalignment among these syntactic chunks and sourcetree fragments.
Thereby, during decoding, we arebinding the generation of function words with thegeneration of target chunks.5http://chasen.org/?taku/software/cabocha/25Algorithm 1 Aligning function words to the forestInput: HPSG forest FS , target sentence T , word alignmentA = {(i, j)}, target function word set {fw} appeared inT , and target chunk set {C}Output: a derivation forest DF1: A?
?
A \ {(i, s(fw))} ?
fw ?
{fw}2: for each node v ?
PA?
in topological order do3: Tv ?
?
?
store the corresponding spans of v4: for each function word fw ?
{fw} do5: if fw ?
C and t(v)?
(C) ?= ?
and fw are not attachedto descendants of v then6: append t(v) ?
{s(fw)} to Tv7: end if8: end for9: for each corresponding span t(v) ?
Tv do10: R ?
IDENTIFYMINRULES(v, t(v), T ) ?
rangeover the hyperedges of v, and discount the factionalcount of each rule r ?
R by 1/|Tv|11: create a node n in DF for each rule r ?
R12: create a shared parent node ?
when |R| > 113: end for14: end for3.2.3 The algorithmAlgorithm 1 outlines the proposed approach toconstructing a derivation forest to include multipleinterpretations of target function words.
The deriva-tion forest is a hypergraph as previously used in(Galley et al, 2006), to maintain the constraint thatone unaligned target word be attached to some nodev exactly once in one derivation tree.
Starting froma triple ?FS , T, A?, we first tailor the alignment Ato A?
by removing the alignments for target functionwords.
Then, we traverse the nodes v ?
PA?
in topo-logical order.
During the traversal, a function wordfw will be attached to v if 1) t(v) overlaps with thespan of the chunk to which fw belongs, and 2) fwhas not been attached to the descendants of v.We identify translation rules that take v as the rootof their tree fragments.
Each tree fragment is a fron-tier tree that takes a node in the frontier set PA?of FS as the root node and non-lexicalized frontiernodes or lexicalized non-frontier nodes as the leaves.Also, a minimal frontier tree used in a minimal ruleis limited to be a frontier tree such that all nodesother than the root and leaves are non-frontier nodes.We use Algorithm 1 described in (Mi and Huang,2008) to collect minimal frontier trees rooted at v inFS .
That is, we range over each hyperedges headedat v and continue to expand downward until the cur-A ?
(A?
)node s(?)
t(?)
c(?)
consistentc0 0-6 0-8(0-3,5-7) ?
1c1 0-6 0-8(0-3,5-7) ?
1c2 0-6 0-8(0-3,5-7) ?
1c3 3-6 0-1,4-7(0-1, 5-7) 2,8 0c4 3 5-7 0,8(0-3) 1c5* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)c6* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)c7 0-1 2-3 0-1,4-8(0-1,5-7) 1c8* 2-3 4-8(5-7) 0-4(0-3) 0(1)c9 0 2 0-1,3-8(0-1,3,5-7) 1c10 1 3 0-2,4-8(0-2,5-7) 1c11 2-6 0-1,4-8(0-1,5-7) 2-3 0c12 3 5-7 0,8(0-3) 1c13* 5-6 0,4(0) 1-8(1-3,5-7) 0(1)c14 5 4(?)
0-8(0-3,5-7) 0c15 6 0 1-8(1-3,5-7) 1c16 2 4,8(?)
0-7(0-3,5-7) 0c17* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)c18 4 1 0,2-8(0,2-3,5-7) 1c19 4 1 0,2-8(0,2-3,5-7) 1c20* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)c21 3 5-7 0,8(0-3) 1c22 2 4,8(?)
0-7(0-3,5-7) 0c23* 2-3 4-8(5-7) 0-4(0-3) 0(1)Table 1: Change of node attributes after alignment modi-fication from A to A?
of the example in Figure 1.
Nodeswith * superscripts are consistent with A?
but not consis-tent with A.rent set of hyperedges forms a minimal frontier tree.In the derivation forest, we use ?
nodes to man-age minimal/composed rules that share the samenode and the same corresponding span.
Figure 2shows some minimal rule and ?
nodes derived fromthe example in Figure 1.Even though we bind function words to theirnearby chunks, these function words may still be at-tached to relative large tree fragments, so that richersyntactic information can be used to predict thefunction words.
For example, in Figure 2, the treefragments rooted at node c0?80 can predict ga and/orta.
The syntactic foundation behind is that, whetherto use ga as a subject particle or to use wo as an ob-ject particle depends on both the left-hand-side nounphrase (kekka) and the right-hand-side verb (kensyousa re ta).
This type of node v?
(such as c0?80 ) shouldsatisfy the following two heuristic conditions:?
v?
is included in the frontier set PA?
of FS , and?
t(v?)
covers the function word, or v?
is the rootnode ofFS if the function word is the beginningor ending word in the target sentence T .Starting from this derivation forest with minimal26c103-4t13: resultkekka ga* c103t13: resultkekkac92t32: thekonoc72-3c103 c92x0 x1x0x1c72-4c103-4 c92x0 x1x0  x1  *c62-7c85-7 c72-3x0 ga x1x0x1* c62-7c85-7 c72-4x0 x1x0x1  *c00-8c16c45-7 c50-1c3c72-4 c11 x2 x0 x1 tax0x1x2+*c00-8c16c45-8 c50-1c3c72-4 c11 x2 x0 x1x0x1x2+*c00-8c16c45-7 c50-1c3c72-3 c11 x2 x0 ga x1 tax0x1x2* + c00-8c16c45-8 c50-1c3c72-3 c11 x2 x0 ga x1x0x1x2*+t4{}:was t4{}:was t4{}:was t4{}:wasFigure 2: Illustration of a (partial) derivation forest.
Gray nodes include some unaligned target function word(s).Nodes annotated by ?*?
include ga, and nodes annotated by ?+?
include ta.rules as nodes, we can further combine two or moreminimal rules to form composed rules nodes and canappend these nodes to the derivation forest.3.3 Estimating rule probabilitiesWe use the EM algorithm to jointly estimate 1)the translation probabilities and fractional counts ofrules and 2) the scores of derivations in the deriva-tion forests.
As reported in (May and Knight, 2007),EM, as has been used in (Galley et al, 2006) to es-timate rule probabilities in derivation forests, is aniterative procedure and prefers shorter derivationscontaining large rules over longer derivations con-taining small rules.
In order to overcome this biasproblem, we discount the fractional count of a ruleby the product of the probabilities of parse hyper-edges that are included in the tree fragment of therule.4 Experiments4.1 SetupWe implemented the forest-to-string decoder de-scribed in (Mi et al, 2008) that makes use of forest-based translation rules (Mi and Huang, 2008) asthe baseline system for translating English HPSGforests into Japanese sentences.
We analyzed theperformance of the proposed translation rule sets byTrain Dev.
Test# sentence pairs 994K 2K 2K# En 1-best trees 987,401 1,982 1,984# En forests 984,731 1,979 1,983# En words 24.7M 50.3K 49.9K# Jp words 28.2M 57.4K 57.1K# Jp function words 8.0M 16.1K 16.1KTable 2: Statistics of the JST corpus.
Here, En = Englishand Jp = Japanese.using the same decoder.The JST Japanese-English paper abstract corpus6(Utiyama and Isahara, 2007), which consists of onemillion parallel sentences, was used for training,tuning, and testing.
Table 2 shows the statistics ofthis corpus.
Note that Japanese function words oc-cupy more than a quarter of the Japanese words.Making use of Enju 2.3.1, we generated 987,4011-best trees and 984,731 parse forests for the En-glish sentences in the training set, with successfulparse rates of 99.3% and 99.1%, respectively.
Us-ing the pruning criteria expressed in (Mi and Huang,2008), we continue to prune a parse forest by set-ting pe to be 8, 5, and 2, until there are no more thane10 = 22, 026 trees in a forest.
After pruning, thereare an average of 82.3 trees in a parse forest.6http://www.jst.go.jp27C3-T M&H-F Min-F C3-Ffree fw Y N Y Yalignment A?
A A?
A?English side tree forest forest forest# rule 86.30 96.52 144.91 228.59# reorder rule 58.50 91.36 92.98 162.71# tree types 21.62 93.55 72.98 120.08# nodes/tree 14.2 42.1 26.3 18.6extract time 30.2 52.2 58.6 130.7EM time 9.4 - 11.2 29.0# rules in dev.
0.77 1.22 1.37 2.18# rules in test 0.77 1.23 1.37 2.15DT(sec./sent.)
2.8 15.7 22.4 35.4BLEU (%) 26.15 27.07 27.93 28.89Table 3: Statistics and translation results for four types oftree-to-string rules.
With the exception of ?# nodes/tree?,the numbers in the table are in millions and the time is inhours.
Here, fw denotes function word, and DT denotesthe decoding time, and the BLEU scores were computedon the test set.We performed GIZA++ (Och and Ney, 2003)and the grow-diag-final-and symmetrizing strategy(Koehn et al, 2007) on the training set to obtainalignments.
The SRI Language Modeling Toolkit(Stolcke, 2002) was employed to train a five-gramJapanese LM on the training set.
We evaluated thetranslation quality using the BLEU-4 metric (Pap-ineni et al, 2002).Joshua v1.3 (Li et al, 2009), which is afreely available decoder for hierarchical phrase-based SMT (Chiang, 2005), is used as an externalbaseline system for comparison.
We extracted 4.5Mtranslation rules from the training set for the 4K En-glish sentences in the development and test sets.
Weused the default configuration of Joshua, with the ex-ception of the maximum number of items/rules, andthe value of k (of the k-best outputs) is set to be 200.4.2 ResultsTable 3 lists the statistics of the following translationrule sets:?
C3-T: a composed rule set extracted from thederivation forests of 1-best HPSG trees thatwere constructed using the approach describedin (Galley et al, 2006).
The maximum numberof internal nodes is set to be three when gen-erating a composed rule.
We free attach targetfunction words to derivation forests;05101520252 12 22 32 42 52 62 72 82 92# of rules(M)# of tree nodes in ruleM&H-FMin-FC3-TC3-FFigure 3: Distributions of the number of tree nodes in thetranslation rule sets.
Note that the curves of Min-F andC3-F are duplicated when the number of tree nodes beinglarger than 9.?
M&H-F: a minimal rule set extracted fromHPSG forests using the extracting algorithm of(Mi and Huang, 2008).
Here, we make use ofthe original alignments.
We use the two heuris-tic conditions described in Section 3.2.3 to at-tach unaligned words to some node(s) in theforest;?
Min-F: a minimal rule set extracted from thederivation forests of HPSG forests that wereconstructed using Algorithm 1 (Section 3).?
C3-F: a composed rule set extracted from thederivation forests of HPSG forests.
Similar toC3-T, the maximum number of internal nodesduring combination is three.We investigate the generalization ability of theserule sets through the following aspects:1. the number of rules, the number of reorderingrules, and the distributions of the number oftree nodes (Figure 3), i.e., more rules with rel-atively small tree fragments are preferred;2. the number of rules that are applicable to thedevelopment and test sets (Table 3); and3.
the final translation accuracies.Table 3 and Figure 3 reflect that the generalizationabilities of these four rule sets increase in the or-der of C3-T < M&H-F < Min-F < C3-F.
The ad-vantage of using a packed forest for re-alignment isverified by comparing the statistics of the rules and28010203040500.00.51.01.52.02.5C3-T M&H-F Min-F C3-FDecoding time(sec./sent.
)# of rules(M)# rules (M)DTFigure 4: Comparison of decoding time and the numberof rules used for translating the test set.the final BLEU scores of C3-T with Min-F and C3-F.
Using the composed rule set C3-F in our forest-based decoder, we achieved an optimal BLEU scoreof 28.89 (%).
Taking M&H-F as the baseline trans-lation rule set, we achieved a significant improve-ment (p < 0.01) of 1.81 points.In terms of decoding time, even though we usedAlgorithm 3 described in (Huang and Chiang, 2005),which lazily generated the N-best translation can-didates, the decoding time tended to be increasedbecause more rules were available during cube-pruning.
Figure 4 shows a comparison of decodingtime (seconds per sentence) and the number of rulesused for translating the test set.
Easy to observe that,decoding time increases in a nearly linear way fol-lowing the increase of the number of rules used dur-ing decoding.Finally, compared with Joshua, which achieveda BLEU score of 24.79 (%) on the test set witha decoding speed of 8.8 seconds per sentence, ourforest-based decoder achieved a significantly better(p < 0.01) BLEU score by using either of the fourtypes of translation rules.5 Related ResearchGalley et al (2006) first used derivation forests ofaligned tree-string pairs to express multiple inter-pretations of unaligned target words.
The EM al-gorithm was used to jointly estimate 1) the trans-lation probabilities and fractional counts of rulesand 2) the scores of derivations in the derivationforests.
By dealing with the ambiguous word align-ment instead of unaligned target words, syntax-based re-alignment models were proposed by (Mayand Knight, 2007; Wang et al, 2010) for tree-basedtranslations.Free attachment of the unaligned target wordproblem was ignored in (Mi and Huang, 2008),which was the first study on extracting tree-to-stringrules from aligned forest-string pairs.
This inspiredthe idea to re-align a packed forest and a target sen-tence.
Specially, we observed that most incorrect orambiguous word alignments are caused by functionwords rather than content words.
Thus, we focus onthe realignment of target function words to sourcetree fragments and use a dependency parser to limitthe attachments of unaligned target words.6 ConclusionWe have proposed an effective use of target functionwords for extracting generalized transducer rules forforest-based translation.
We extend the unalignedword approach described in (Galley et al, 2006)from the 1-best tree to the packed parse forest.
Asimple yet effective modification is that, during ruleextraction, we account for multiple interpretationsof both aligned and unaligned target function words.That is, we chose to loose the ambiguous alignmentsfor all of the target function words.
The consider-ation behind is in order to generate target functionwords in a robust manner.
In order to avoid gener-ating too large a derivation forest for a packed for-est, we further used chunk-level information yieldedby a target dependency parser.
Extensive experi-ments on large-scale English-to-Japanese translationresulted in a significant improvement in BLEU scoreof 1.8 points (p < 0.01), as compared with ourimplementation of a strong forest-to-string baselinesystem (Mi et al, 2008; Mi and Huang, 2008).The present work only re-aligns target functionwords to source tree fragments.
It will be valuableto investigate the feasibility to re-align all the tar-get words to source tree fragments.
Also, it is in-teresting to automatically learn a word set for re-aligning7.
Given source parse forests and a targetword set for re-aligning beforehand, we argue ourapproach is generic and applicable to any languagepairs.
Finally, we intend to extend the proposedapproach to tree-to-tree translation frameworks by7This idea comes from one reviewer, we express our thank-fulness here.29re-aligning subtree pairs (Liu et al, 2009; Chiang,2010) and consistency-to-dependency frameworksby re-aligning consistency-tree-to-dependency-treepairs (Mi and Liu, 2010) in order to tackle the rule-sparseness problem.AcknowledgmentsThe present study was supported in part by a Grant-in-Aid for Specially Promoted Research (MEXT,Japan), by the Japanese/Chinese Machine Transla-tion Project through Special Coordination Funds forPromoting Science and Technology (MEXT, Japan),and by Microsoft Research Asia Machine Transla-tion Theme.Wu (wu.xianchao@lab.ntt.co.jp) hasmoved to NTT Communication Science Laborato-ries and Tsujii (junichi.tsujii@live.com)has moved to Microsoft Research Asia.ReferencesDavid Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL, pages 263?270, Ann Arbor, MI.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1443?1452, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the emalgorithm.
Journal of the Royal Statistical Society,39:1?38.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT-NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING-ACL, pages 961?968, Sydney.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of IWPT.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the Human Language Technology and NorthAmerican Association for Computational LinguisticsConference (HLT/NAACL), Edomonton, Canada, May27-June 1.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the ACL 2007 Demo and Poster Sessions, pages177?180.Taku Kudo and Yuji Matsumoto.
2002.
Japanese depen-dency analysis using cascaded chunking.
In Proceed-ings of CoNLL-2002, pages 63?69.
Taipei, Taiwan.Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenN.
G. Thornton, Jonathan Weese, and Omar F. Zaidan.2009.
Demonstration of joshua: An open sourcetoolkit for parsing-based machine translation.
In Pro-ceedings of the ACL-IJCNLP 2009 Software Demon-strations, pages 25?28, August.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment templates for statistical machinetransaltion.
In Proceedings of COLING-ACL, pages609?616, Sydney, Australia.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Pro-ceedings of ACL-IJCNLP, pages 558?566, August.Samuel E. Martin.
1975.
A Reference Grammar ofJapanese.
New Haven, Conn.: Yale University Press.Jonathan May and Kevin Knight.
2007.
Syntactic re-alignment models for machine translation.
In Pro-ceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 360?368, Prague, Czech Republic,June.
Association for Computational Linguistics.Haitao Mi and Liang Huang.
2008.
Forest-based transla-tion rule extraction.
In Proceedings of EMNLP, pages206?214, October.Haitao Mi and Qun Liu.
2010.
Constituency to depen-dency translation with forests.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1433?1442, Uppsala, Swe-den, July.
Association for Computational Linguistics.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08:HLT,pages 192?199, Columbus, Ohio.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature forestmodels for probabilistic hpsg parsing.
ComputationalLingustics, 34(1):35?80.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,pages 311?318.30Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.Ivan A.
Sag, Thomas Wasow, and Emily M. Bender.2003.
Syntactic Theory: A Formal Introduction.Number 152 in CSLI Lecture Notes.
CSLI Publica-tions.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proceedings of InternationalConference on Spoken Language Processing, pages901?904.Masao Utiyama and Hitoshi Isahara.
2007.
A japanese-english patent parallel corpus.
In Proceedings of MTSummit XI, pages 475?482, Copenhagen.Wei Wang, Jonathan May, Kevin Knight, and DanielMarcu.
2010.
Re-structuring, re-labeling, and re-aligning for syntax-based machine translation.
Com-putational Linguistics, 36(2):247?277.Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.2010.
Fine-grained tree-to-string translation rule ex-traction.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 325?334, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improvesmt for subject-object-verb languages.
In Proceedingsof HLT-NAACL, pages 245?253.Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li,and Chew Lim Tan.
2007.
A tree-to-tree alignment-based model for statistical machine translation.
InProceedings of MT Summit XI, pages 535?542, Copen-hagen, Denmark, September.31
