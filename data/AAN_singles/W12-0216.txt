Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 117?125,Avignon, France, April 23 - 24 2012. c?2012 Association for Computational LinguisticsLexStat: Automatic Detection of Cognates in Multilingual WordlistsJohann-Mattis ListInstitute for Romance Languages and LiteratureHeinrich Heine University D?sseldorf, Germanylistm@phil.uni-duesseldorf.deAbstractIn this paper, a new method for automaticcognate detection in multilingual wordlistswill be presented.
The main idea behind themethod is to combine different approaches tosequence comparison in historical linguisticsand evolutionary biology into a new frame-work which closely models the most impor-tant aspects of the comparative method.
Themethod is implemented as a Python programand provides a convenient tool which is pub-licly available, easily applicable, and openfor further testing and improvement.
Testingthe method on a large gold standard of IPA-encoded wordlists showed that its results arehighly consistent and outperform previousmethods.1 IntroductionDuring the last two decades there has been an in-creasing interest in automatic approaches to his-torical linguistics, which is reflected in the largeamount of literature on phylogenetic reconstruc-tion (e.g.
Ringe et al, 2002; Gray and Atkin-son, 2003; Brown et al, 2008), statistical aspectsof genetic relationship (e.g.
Baxter and ManasterRamer, 2000; Kessler, 2001; Mortarino, 2009),and phonetic alignment (e.g.
Kondrak, 2002;Proki?
et al, 2009; List, forthcoming).While the supporters of these new automaticmethods would certainly agree that their greatestadvantage lies in the increase of repeatability andobjectivity, it is interesting to note that the mostcrucial part of the analysis, namely the identifica-tion of cognates in lexicostatistical datasets, is stillalmost exclusively carried out manually.
That thismay be problematic was recently shown in a com-parison of two large lexicostatistical datasets pro-duced by different scholarly teams where differ-ences in item translation and cognate judgmentsled to topological differences of 30% and more(Geisler and List, forthcoming).
Unfortunately,automatic approaches to cognate detection stilllack the precision of trained linguists?
judgments.Furthermore, most of the methods that have beenproposed so far only deal with bilingual as op-posed to multilingual wordlists.The LexStat method, which will be presentedin the following, is a convenient tool which notonly closely renders the most important aspects ofmanual approaches but also yields transparent de-cisions that can be directly compared with the re-sults achieved by the traditional methods.2 Identification of Cognates2.1 The Comparative MethodIn historical linguistics, cognacy is traditionallydetermined within the framework of the compar-ative method (Trask, 2000, 64-67).
The finalgoal of this method is the reconstruction of proto-languages, yet the basis of the reconstruction it-self rests on the identification of cognate words ormorphemes within genetically related languages.Within the comparative method, cognates in agiven set of language varieties are identified byapplying a recursive procedure.
First an initial listof putative cognate sets is created by comparingsemantically and phonetically similar words fromthe languages to be investigated.
In most of the lit-erature dealing with the comparative method, thequestion of which words are most suitable for theinitial compilation of cognate lists is not explic-itly addressed, yet it seems obvious that the com-paranda should belong to the basic vocabulary ofthe languages.
Based on this cognate list, an ini-117tial list of putative sound correspondences (corre-spondence list) is created.
Sound correspondencesare determined by aligning the cognate words andsearching for sound pairs which repeatedly oc-cur in similar positions of the presumed cognatewords.
After these initial steps have been made,the cognate list and the correspondence list aremodified by1.
adding and deleting cognate sets from thecognate list depending on whether or not theyare consistent with the correspondence list,and2.
adding and deleting sound correspondencesfrom the correspondence list, depending onwhether or not they find support in the cog-nate list.These steps are repeated until the results seem sat-isfying enough such that no further modifications,neither of the cognate list, nor of the correspon-dence list, seem to be necessary.The specific strength of the comparativemethodlies in the similarity measure which is applied forthe identification of cognates: Sequence similar-ity is determined on the basis of systematic soundcorrespondences (Trask, 2000, 336) as opposed tosimilarity based on surface resemblances of pho-netic segments.
Thus, comparing English token[t?
?k?n] and German Zeichen [?a??
?n] ?sign?,the words do not really sound similar, yet theircognacy is assumed by the comparative method,since their phonetic segments can be shown to cor-respond regularly within other cognates of bothlanguages.1 Lass (1997, 130) calls this notionof similarity genotypic as opposed to a pheno-typic notion of similarity, yet the most crucial as-pect of correspondence-based similarity is that it islanguage-specific: Genotypic similarity is neverdefined in general terms but always with respectto the language systems which are being com-pared.
Correspondence relations can thereforeonly be established for individual languages, theycan never be taken as general statements.
Thismay seem to be a weakness, yet it turns out thatthe genotypic similarity notion is one of the mostcrucial strengths of the comparative method: Not1Compare, for example, English weak [wi?k] vs. Ger-man weich [va??]
?soft?
for the correspondence of [k] with[?
], and English tongue [t??]
vs. German Zunge [????]?tongue?
for the correspondence of [t] with [?
].only does it allow us to dive deeper in the his-tory of languages in cases where phonetic changehas corrupted the former identity of cognates tosuch an extent that no sufficient surface similarityis left, it also makes it easier to distinguish bor-rowed from commonly inherited items, since theformer usually come along with a greater degreeof phenotypic similarity.2.2 Automatic ApproachesIn contrast to the language-specific notion of simi-larity that serves as the basis for cognate detectionwithin the framework of the comparative method,most automatic methods seek to determine cog-nacy on the basis of surface similarity by calcu-lating the phonetic distance or similarity betweenphonetic sequences (words, morphemes).The most popular distance measures are basedon the paradigm of sequence alignment.
In align-ment analyses two ormore sequences are arrangedin a matrix in such a way that all correspond-ing segments appear in the same column, whileempty cells of the matrix, resulting from non-corresponding segments, are filled with gap sym-bols (Gusfield, 1997, 216).
Table 1 gives an ex-ample for the alignment of German Tochter [t?x-t?r] ?daughter?
and English daughter [d?
?t?r]:Here, all corresponding segments are inserted inthe same columns, while the velar fricative [x] ofthe German sequence which does not have a cor-responding segment in the English word is repre-sented by a gap symbol.German t ?
x t ?
rEnglish d ??
- t ?
rTable 1: Alignment AnalysisIn order to retrieve a distance or a similar-ity score from such an alignment analysis, thematched residue pairs, i.e.
the segments whichappear in the same column of the alignment, arecompared and given a specific score depending ontheir similarity.
How the phonetic segments arescored depends on the respective scoring functionwhich is the core of all alignment analyses.
Thus,the scoring function underlying the edit distanceonly distinguishes identical from non-identicalsegments, while the scoring function used in theALINE algorithm of Kondrak (2002) assigns in-dividual similarity scores for the matching of pho-netic segments based on phonetic features.118Using alignment analyses, cognacy can be de-termined by converting the distance or similarityscores to normalized distance scores and assum-ing cognacy for distances beyond a certain thresh-old.
The normalized edit distance (NED) of twosequencesA andB is usually calculated by divid-ing the edit distance by the length of the small-est sequence.
The normalized distance score ofalgorithms which yield similarities (such as theALINE algorithm) can be calculated by the for-mula of Downey et al (2008):(1) 1?
2SABSA + SB,where SA and SB are the similarity scores of thesequences aligned with themselves, and SAB isthe similarity score of the alignment of both se-quences.
For the alignment given in Table 1, thenormalized edit distance is 0.6, and the ALINEdistance is 0.25.A certain drawback of most of the commonalignment methods is that their scoring functiondefines segment similarity on the basis of phe-notypic criteria.
The similarity of phonetic seg-ments is determined on the basis of their phoneticfeatures and not on the basis of the probabilitythat their segments occur in a correspondence re-lation in genetically related languages.
An alter-native way to calculate phonetic similarity whichcomes closer to a genotypic notion of similarityis to compare phonetic sequences with respect totheir sound classes.
The concept of sound classesgoes back to Dolgopolsky (1964).
The originalidea was ?to divide sounds into such groups, thatchanges within the boundary of the groups aremore probable than transitions from one groupinto another?
(Burlak and Starostin, 2005, 272)2.In his original study, Dolgopolsky proposed tenfundamental sound classes, based on an empiricalanalysis of sound-correspondence frequencies in asample of 400 languages.
Cognacy between twowords is determined by comparing the first twoconsonants of both words.
If the sound classes areidentical, the words are judged to be cognate.
Oth-erwise no cognacy is assumed.
Thus, given thewords German Tochter [t?xt?r] ?daughter?
andEnglish daughter [d?
?t?r], the sound class rep-resentation of both sequences will be TKTR and2My translation, original text: ?[...]
????????
???????????
?????
?, ???
?????????
?
????????
??????
????????????
?, ???
????????
??
?????
??????
?
??????
?.TTR, respectively.
Since the first two consonantsof both words do not match regarding their soundclasses, the words are judged to be non-cognate.0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Distances4002000200400Sequence PairsSCA cog.SCA non-cog.NED cog.NED non-cog.1Figure 1: SCA Distance vs. NEDIn recent studies, sound classes have also beenused as an internal representation format forpairwise and multiple alignment analyses.
Themethod for sound-class alignment (SCA, cf.
List,forthcoming) combines the idea of sound classeswith traditional alignment algorithms.
In contrastto the original proposal by Dolgopolsky, SCAemploys an extended sound-class model whichalso represents tones and vowels along with a re-fined scoring scheme that defines specific transi-tion probabilities between sound classes.
The ben-efits of the SCA distance compared to NED canbe demonstrated by comparing the distance scoresthe methods yield for the comparison of the samedata.
Figure 1 contrasts the scores of NED withSCA distance for the alignment of 658 cognateand 658 non-cognate word pairs between Englishand German (see Sup.
Mat.
A).
As can be seenfrom the figure, the scores for NED do not showa very sharp distinction between cognate and non-cognate words.
Even with a ?perfect?
thresholdof 0.8 that minimizes the number of false positiveand false negative decisions there are still 13% ofincorrect decisions.
The SCA scores, on the otherhand, show a sharper distinction between scoresfor cognates and non-cognates.
With a thresholdof 0.5 the percentage of incorrect decisions de-creases to 8%.There are only three recent approaches knownto the author which explicitly deal with the task ofcognate detection in multilingual wordlists.
Allmethods take multilingual, semantically aligned119wordlists as input data.
Bergsma and Kondrak(2007) first calculate the longest common sub-sequence ratio between all word pairs in the in-put data and then use an integer linear program-ming approach to cluster the words into cognatesets.
Unfortunately, their method is only testedon a dataset containing alphabetic transcriptions;hence, no direct comparison with the methodproposed in this paper is possible.
Turchin etal.
(2010) use the above-mentioned sound-classmodel and the cognate-identification criterion byDolgopolsky (1964) to identify cognates in lexi-costatistical datasets.
Their method is also imple-mented within LexStat, and the results of a directcomparisonwill be reported in section 4.3.
Steineret al (2011) propose an iterative approach whichstarts by clustering words into tentative cognatesets based on their alignment scores.
These pre-liminary results are then refined by filtering wordsaccording to similar meanings, computing multi-ple alignments, and determining recurrent soundcorrespondences.
The authors test their methodon two large datasets.
Since no gold standard fortheir test set is available, they only report interme-diate results, and their method cannot be directlycompared to the one proposed in this paper.3 LexStatLexStat combines the most important aspects ofthe comparative method with recent approachesto sequence comparison in historical linguisticsand evolutionary biology.
The method employsautomatically extracted language-specific scor-ing schemes for computing distance scores frompairwise alignments of the input data.
Theselanguage-specific scoring schemes come close tothe notion of sound correspondences in traditionalhistorical linguistics.The method is implemented as a part of theLingPy library, a Python library for automatictasks in quantitative historical linguistics.3 It caneither be used in Python scripts or directly becalled from the Python prompt.The input data are analyzed within a four-stepapproach: (1) sequence conversion, (2) scoring-scheme creation, (3) distance calculation, and (4)sequence clustering.
In stage (1), the input se-quences are converted to sound classes and their3Online available under http://lingulist.de/lingpy/.sonority profiles are determined.
In stage (2), apermutation method is used to create language-specific scoring schemes for all language pairs.In stage (3) the pairwise distances between allword pairs, based on the language-specific scor-ing schemes, are computed.
In stage (4), the se-quences are clustered into cognate sets whose av-erage distance is beyond a certain threshold.3.1 Input and Output FormatThe method takes multilingual, semanticallyaligned wordlists in IPA transcription as input.The input format is a CSV-representation of theway multilingual wordlists are represented in theSTARLING software package for lexicostatisticalanalyses.4 Thus, the input data are specified in asimple tab-delimited text file with the names of thelanguages in the first row, an ID for the semanticslots (basic vocabulary items in traditional lexico-statistic terminology) in the first column, and thelanguage entries in the columns corresponding tothe language names.
The language entries shouldbe given either in plain IPA encoding.
Addition-ally, the file can contain headwords (items) for se-mantic slots corresponding to the IDs.
Synonyms,i.e.
multiple entries in one language for a givenmeaning are listed in separate rows and given thesame ID.
Table 2 gives an example for the possiblestructure of an input file.ID Items German English Swedish1 hand hant h?nd hand2 woman fra?
w?m?n kvina3 know k?n?n n??
?
?na3 know v?s?n - ve?taTable 2: LexStat Input FormatThe output format is the same as the input for-mat except that each language column is accom-panied by a column indicating the cognate judg-ments made by LexStat.
Cognate judgments aredisplayed by assigning a cognate ID to each entry.If entries in the output file share the same cognateID, they are judged to be cognate by the method.3.2 Sequence ConversionIn the stage of sequence conversion, all input se-quences are converted to sound classes, and their4Online available under http://starling.rinet.ru/program.php; a closer description of thesoftware is given in Burlak and Starostin (2005, 270-275)120respective sonority profiles are calculated.
Lex-Stat uses the SCA sound-class model by default,yet other sound class models are also available.The idea of sonority profiles was developedin List (forthcoming).
It accounts for the well-known fact that certain types of sound changes aremore likely to occur in specific prosodic contexts.Based on the sonority hierarchy of Geisler (1992,30), the sound segments of phonetic sequencesare assigned to different prosodic environments,depending on their prosodic context.
The cur-rent version of SCA distinguishes seven differentprosodic environments.5 The information regard-ing sound classes and prosodic context are com-bined, and each input sequence is further repre-sented as a sequence of tuples, consisting of thesound class and the prosodic environment of therespective phonetic segment.
During the calcula-tion, only those segments which are identical re-garding their sound class as well as their prosodiccontext are treated as identical.3.3 Scoring-Scheme CreationIn order to create language specific scoringschemes, a permutation method is used (Kessler,2001).
The method compares the attested distri-bution of residue pairs in phonetic alignment anal-yses of a given dataset to the expected distribution.The attested distribution of residue pairs is de-rived from global and local alignment analyses ofall word pairs whose distance is beyond a cer-tain threshold.
The threshold is used to reflect thefact that within the comparative method, recurrentsound correspondences are only established withrespect to presumed cognate words, whereas non-cognate words or borrowings are ignored.
Tak-ing only the best-scoring word pairs for the cal-culation of the attested frequency distribution in-creases the accuracy of the approach and helps toavoid false positive matches contributing to thecreation of the scoring scheme.
Alignment analy-ses are carried out with help of the SCA method.While the attested distribution is derived fromalignments of semantically aligned words, the ex-pected distribution is calculated by aligning wordpairs without regard to semantic criteria.
Thisis achieved by repeatedly shuffling the wordlists5The different environments are: # (word-initial, cons.
),V (word-initial, vow.
), C (ascending sonority, cons.
), v (max-imum sonority, vow.
), c (descending sonority, cons.
), $(word-final, cons.
), and > (word-final, vow.
).and aligning them with help of the same methodswhich were used for the calculation of the attesteddistributions.
In the default settings, the numberof repetitions is set to 1000, yet many tests showedthat even the number of 100 repetitions is suffi-cient to yield satisfying results that do not varysignificantly.Once the attested and the expected distributionsfor the segments of all language pairs are cal-culated, a language-specific score sx,y for eachresidue pair x and y in the dataset is created us-ing the formula(2) sx,y =1r1 + r2(r1 log2(a2x,ye2x,y)+ r2dx,y),where ax,y is the attested frequency of the segmentpair, ex,y is the expected frequency, r1 and r2 arescaling factors, and dx,y is the similarity score ofthe original scoring function which was used toretrieve the attested and the expected distributions.Formula (2) combines different approachesfrom the literature on sequence comparison in his-torical linguistics and biology.
The idea of squar-ing the frequencies of attested and expected fre-quencies was adopted from Kessler (2001, 150),reflecting ?the general intuition among linguiststhat the evidence of phoneme recurrence growsfaster than linearly?.
Using the binary loga-rithm of the division of attested and expected fre-quencies of occurrence is common in evolution-ary biology to retrieve similarity scores (?log-odds scores?)
which are apt for the computationof alignment analyses (Henikoff and Henikoff,1992).
The incorporation of the alignment scoresof the original language-independent scoring-scheme copes with possible problems resultingfrom small wordlists: If the dataset is too small toallow the identification of recurrent sound corre-spondences, the language-independent alignmentscores prevent the method from treating gener-ally probable and generally improbable matchingsalike.
The ratio of language-specific to language-independent alignment scores is determined by thescaling factors r1 and r2.As an example of the computation of language-specific scoring schemes, Table 3 shows attestedand expected frequencies along with the resultingsimilarity scores for the matching of word-initialand word-final sound classes in the KSL testset(see Sup.
Mat.
B and C).
The word-initial andword-final classes T = [t, d], C = [?
], S = [?, s, z]121English German Att.
Exp.
Score#[t,d] #[t,d] 3.0 1.24 6.3#[t,d] #[?]
3.0 0.38 6.0#[t,d] #[?,s,z] 1.0 1.99 -1.5#[?,?]
#[t,d] 7.0 0.72 6.3#[?,?]
#[?]
0.0 0.25 -1.5#[?,?]
#[s,z] 0.0 1.33 0.5[t,d]$ [t,d]$ 21.0 8.86 6.3[t,d]$ [?
]$ 3.0 1.62 3.9[t,d]$ [?,s]$ 6.0 5.30 1.5[?,?
]$ [t,d]$ 4.0 1.14 4.8[?,?
]$ [?
]$ 0.0 0.20 -1.5[?,?
]$ [?,s]$ 0.0 0.80 0.5Table 3: Attested vs. Expected Frequenciesin German are contrasted with the word-initial andword-final sound classes T = [t, d] and D = [?, ?
]in English.
As can be seen from the table, the scor-ing scheme correctly reflects the complex soundcorrespondences between English and German re-sulting from the High German Consonant Shift(Trask, 2000, 300-302), which is reflected in suchcognate pairs as English town [ta?n] vs. Ger-man Zaun [?aun] ?fence?, English thorn [???n]vs.
German Dorn [d?rn] ?thorn?, English dale[de?l] vs. German Tal ?valley?
[ta?l], and Englishhot [h?t] vs. German hei?
[ha?s] ?hot?.
The spe-cific benefit of representing the phonetic segmentsas tuples consisting of their respective sound classalong with their prosodic context also becomesevident: The correspondence of English [t] withGerman [s] is only attested in word-final position,correctly reflecting the complex change of former[t] to [s] in non-initial position in German.
If itwere not for the specific representation of the pho-netic segments by both their sound class and theirprosodic context, the evidence would be blurred.3.4 Distance CalculationOnce the language-specific scoring scheme iscomputed, the distances between all word pairsare calculated.
Here, LexStat uses the ?end-spacefree variant?
(Gusfield, 1997, 228) of the tradi-tional algorithm for pairwise sequence alignmentswhich does not penalize gaps introduced in the be-ginning and the end of the sequences.
This mod-ification is useful when words contain prefixes orsuffixes which might distort the calculation.
Thealignment analysis requires no further parameterssuch as gap penalties, since they have already beencalculated in the previous step.
The similarityscores for pairwise alignments are converted todistance scores following the approach of Downeyet al (2008) which was described in section 2.2.Word Pair SCA LexStatGerman Schlange [?la??
]English Snake [sne?k] 0.44 0.67German Wald [valt]English wood [w?d] 0.40 0.64German Staub [?taup]English dust [d?st] 0.43 0.78Table 4: SCA Distance vs. LexStat DistanceThe benefits of the language-specific distancescores become obvious when comparing themwith general ones.
Table 4 gives some exam-ples for non-cognate word pairs taken from theKSL testset (see Sup.
Mat.
B and C).
While theSCA distances for these pairs are all considerablylow, as it is suggested by the surface similarity ofthe words, the language-specific distances are allmuch higher, resulting from the fact that no fur-ther evidence for the matching of specific residuepairs can be found in the data.3.5 Sequence ClusteringIn the last step of the LexStat algorithm all se-quences occurring in the same semantic slot areclustered into cognate sets using a flat cluster vari-ant of the UPGMA algorithm (Sokal and Mich-ener, 1958) which was written by the author.
Incontrast to traditional UPGMA clustering, this al-gorithm terminates when a user-defined thresholdof average pairwise distances is reached.Ger.
Eng.
Dan.
Swe.
Dut.
Nor.Ger.
[frau] 0.00 0.95 0.81 0.70 0.34 1.00Eng.
[w?m?n] 0.95 0.00 0.78 0.90 0.80 0.80Dan.
[kven?]
0.81 0.78 0.00 0.17 0.96 0.13Swe.
[kvin?a] 0.70 0.90 0.17 0.00 0.86 0.10Dut.
[vr?u?]
0.34 0.80 0.96 0.86 0.00 0.89Nor.
[k?in?]
1.00 0.80 0.13 0.10 0.89 0.00Clusters 1 2 3 3 1 3Table 5: Pairwise Distance MatrixTable 5 shows pairwise distances of German,English, Danish, Swedish, Dutch, and Norwegianentries for the itemWOMAN taken from the GERdataset (see Sup.
Mat.
B) along with the resulting122cluster decisions of the algorithm when setting thethreshold to 0.6.4 Evaluation4.1 Gold StandardIn order to test the method, a gold standard wascompiled by the author.
The gold standard con-sists of 9 multilingual wordlists conforming to theinput format required by LexStat (see Supplemen-tary Material B).
The data was collected from dif-ferent publicly available sources.
Hence, the se-lection of language entries as well as the man-ually conducted cognate judgments were carriedout independently of the author.
Since not all theoriginal sources provided phonetic transcriptionsof the language entries, the respective alphabeticentries were converted to IPA transcription by theauthor.
The datasets differ regarding the treatmentof borrowings.
In some datasets they are explic-itly marked as such and treated as non-cognates, inother datasets no explicit distinction between bor-rowing and cognacy is drawn.
Information on thestructure and the sources of the datasets is givenin Table 6.File Family Lng.
Itm.
Entr.
SourceGER Germanic 7 110 814 Starostin (2008)ROM Romance 5 110 589 Starostin (2008)SLV Slavic 4 110 454 Starostin (2008)PIE Indo-Eur.
18 110 2057 Starostin (2008)OUG Uralic 21 110 2055 Starostin (2008)BAI Bai 9 110 1028 Wang (2006)SIN Sinitic 9 180 1614 H?u (2004)KSL varia 8 200 1600 Kessler (2001)JAP Japonic 10 200 1986 Shir?
(1973)Table 6: The Gold Standard4.2 Evaluation MeasuresBergsma andKondrak (2007) test their method forautomatic cognate detection by calculating the setprecision (PRE), the set recall (REC), and the setF-score (FS): The set precision p is the proportionof cognate sets calculated by the method whichalso occurs in the gold standard.
The set recall r isthe proportion of cognate sets in the gold standardwhich are also calculated by the method, and theset F-score f is calculated by the formula(3) f = 2 prp+ r.A certain drawback of these scores is that theyonly check for completely identical decisions re-garding the clustering of words into cognate setswhile neglecting similar tendencies.
The similar-ity of decisions can be evaluated by calculating theproportion of identical decisions (PID)when com-paring the test results with those of the gold stan-dard.
Given all pairwise decisions regarding thecognacy of word pairs inherent in the gold stan-dard and in the testset, the differences can be dis-played using a contingency table, as shown in Ta-ble 7.Cognate Non-CognateGold Standard Gold StandardCognateTestset true positives false positivesNon-CognateTestset false negatives true negativesTable 7: Comparing Gold Standard and TestsetThe PID score can then simply be calculated bydividing the sum of true positives and true nega-tives by the total number of decisions.
In an analo-gous way the proportion of identical positive deci-sions (PIPD) and the proportion of identical nega-tive decisions (PIND) can be calculated by divid-ing the number of true positives by the sum of truepositives and false negatives, and by dividing thenumber of false positives by the sum of false pos-itives and true negatives, respectively.4.3 ResultsBased on the new method for automatic cognatedetection, the 9 testsets were analyzed by Lex-Stat, using a gap penalty of -2 for the alignmentanalysis, a threshold of 0.7 for the creation ofthe attested distribution, and 1:1 as the ratio oflanguage-specific to language-independent simi-larity scores.
The threshold for the clustering ofsequences into cognate sets was set to 0.6.
In orderto compare the output of LexStat with other meth-ods, three additional analyses of the datasets werecarried out: The first two analyses were based onthe calculation of SCA and NED distances of alllanguage entries.
Based on these scores all wordswere clustered into cognate sets using the flat clus-ter variant of UPGMA with a threshold of 0.4 forSCA distances and a threshold of 0.7 for NED,since these both turned out to yield the best resultsfor these approaches.
The third analysis was basedon the above-mentioned approach by Turchin etal.
(2010).
Since in this approach all decisions re-123garding cognacy are either positive or negative, nospecific cluster algorithm had to be applied.Score LexStat SCA NED TurchinPID 0.85 0.82 0.76 0.74PIPD 0.78 0.75 0.66 0.56PIND 0.93 0.90 0.86 0.94PRE 0.59 0.51 0.39 0.39REC 0.68 0.57 0.47 0.55FS 0.63 0.55 0.42 0.46Table 8: Performance of the MethodsThe results of the tests are summarized in Ta-ble 8.
As can be seen from the table, LexStatoutperforms the other methods in almost all re-spects, the only exception being the proportion ofidentical negative decisions (PIND).
Since non-identical negative decisions point to false posi-tives, this shows that ?
for the given settings ofLexStat ?
the method of Turchin et al (2010) per-forms best at avoiding false positive cognate judg-ments, but it fails to detect many cognates cor-rectly identified by LexStat.6 Figure 2 gives theseparate PID scores for all datasets, showing thatLexStat?s good performance is prevalent through-out all datasets.
The fact that all methods per-form badly on the PIE dataset may point to prob-lems resulting from the size of the wordlists: ifthe dataset is too small and the genetic distance ofthe languages too large, one may simply lack theevidence to prove cognacy without doubt.SLV KSL GER BAI SIN PIE ROM JAP OUG0.60.70.80.91.0LexStatSCANEDTurchin1Figure 2: PID Scores of the Methods6LexStat can easily be adjusted to avoid false positivesby lowering the threshold for sequence clustering.
Using athreshold of 0.5 will yield a PIND score of 0.96, yet the PIDscore will lower down to 0.82.The LexStat method was designed to distin-guish systematic from non-systematic similarities.The method should therefore produce less falsepositive cognate judgments resulting from chanceresemblances and borrowings than the other meth-ods.
In the KSL dataset borrowings are markedalong with their sources.
Out of a total of 5600word pairs, 72 exhibit a loan relation, and 83 arephonetically similar (with an NED score less then0.6) but unrelated.
Table 9 lists the number and thepercentage of false positives resulting from unde-tected borrowings or chance resemblances for thedifferent methods (see also Sup.
Mat.
D).
WhileLexStat outperforms the other methods regardingthe detection of chance resemblances, it is notparticularly good at handling borrowings.
Lex-Stat cannot per se deal with borrowings, but onlywith language-specific as opposed to language-independent similarities.
In order to handle bor-rowings, other methods (such as, e.g., the one byNelson-Sathi et al, 2011) have to be applied.LexStat SCA NED TurchinBorr.
36 / 50% 44 / 61% 35 / 49% 38 / 53%Chance R. 14 / 17% 35 / 42% 74 / 89% 26 / 31%Table 9: Borrowings and Chance Resemblances5 ConclusionIn this paper, a new method for automatic cognatedetection in multilingual wordlists has been pre-sented.
The method differs from other approachesin so far as it employs language-specific scoringschemes which are derived with the help of im-proved methods for automatic alignment analy-ses.
The test of the method on a large dataset ofwordlists taken from different language familiesshows that it is consistent regardless of the lan-guages being analyzed and outperforms previousapproaches.In contrast to the black box character of manyautomatic analyses which only yield total scoresfor the comparison of wordlists, the method yieldstransparent decisions which can be directly com-pared with the traditional results of the compar-ative method.
Apart from the basic ideas of theprocedure, which surely are in need of enhance-ment through reevaluation and modification, themost striking limit of the method lies in the data:If the wordlists are too short, certain cases of cog-nacy are simply impossible to be detected.124ReferencesWilliam H. Baxter and Alexis Manaster Ramer.
2000.Beyond lumping and splitting.
Probabilistic issuesin historical linguistics.
In Colin Renfrew, AprilMcMahon, and Larry Trask, editors, Time depth inhistorical linguistics, pages 167?188.
McDonald In-stitute for Archaeological Research, Cambridge.Shane Bergsma and Grzegorz Kondrak.
2007.
Mul-tilingual cognate identification using integer lin-ear programming.
In RANLP Workshop on Acqui-sition and Management of Multilingual Lexicons,Borovets, Bulgaria.Cecil H. Brown, Eric W. Holman, S?ren Wich-mann, Viveka Velupillai, and Michael Cysouw.2008.
Automated classification of the world?slanguages.
Sprachtypologie und Universalien-forschung, 61(4):285?308.Svetlana A. Burlak and Sergej A. Starostin.2005.
Sravnitel?no-istori?eskoe jazykoznanie[Comparative-historical linguistics].
Akademia,Moscow.Aron B. Dolgopolsky.
1964.
Gipoteza drevne-j?ego rodstva jazykovych semej Severnoj Evrazii sverojatnostej to?ky zrenija [A probabilistic hypoth-esis concerning the oldest relationships among thelanguage families of Northern Eurasia].
VoprosyJazykoznanija, 2:53?63.Sean S. Downey, Brian Hallmark, Murray P. Cox, Pe-ter Norquest, and Stephen Lansing.
2008.
Com-putational feature-sensitive reconstruction of lan-guage relationships: Developing the ALINE dis-tance for comparative historical linguistic recon-struction.
Journal of Quantitative Linguistics,15(4):340?369.Hans Geisler and Johann-Mattis List.
forthcoming.Beautiful trees on unstable ground.
Notes on the dataproblem in lexicostatistics.
In Heinrich Hettrich, ed-itor,Die Ausbreitung des Indogermanischen.
Thesenaus Sprachwissenschaft, Arch?ologie und Genetik.Reichert, Wiesbaden.Hans Geisler.
1992.
Akzent und Lautwandel in derRomania.
Narr, T?bingen.Russell D. Gray and Quentin D. Atkinson.
2003.Language-tree divergence times support the Ana-tolian theory of Indo-European origin.
Nature,426(6965):435?439.Dan Gusfield.
1997.
Algorithms on strings, treesand sequences.
Cambridge University Press, Cam-bridge.Steven Henikoff and Jorja G. Henikoff.
1992.
Aminoacid substitution matrices from protein blocks.PNAS, 89(22):10915?10919.J?ng H?u, editor.
2004.
Xi?nd?i H?ny?
f?ngy?ny?nk?
[Phonological database of Chinese dialects].Sh?ngh?i Ji?oy?, Shanghai.Brett Kessler.
2001.
The significance of word lists.Statistical tests for investigating historical connec-tions between languages.
CSLI Publications, Stan-ford.Grzegorz Kondrak.
2002.
Algorithms for languagereconstruction.
Dissertation, University of Toronto,Toronto.Roger Lass.
1997.
Historical linguistics and languagechange.
Cambridge University Press, Cambridge.Johann-Mattis List.
forthcoming.
SCA: Phoneticalignment based on sound classes.
In MarijaSlavkovik and Dan Lassiter, editors, New direc-tions in logic, language, and computation.
Springer,Berlin and Heidelberg.Cinzia Mortarino.
2009.
An improved statistical testfor historical linguistics.
Statistical Methods andApplications, 18(2):193?204.Shijulal Nelson-Sathi, Johann-Mattis List, HansGeisler, Heiner Fangerau, Russell D. Gray, WilliamMartin, and Tal Dagan.
2011.
Networks uncoverhidden lexical borrowing in Indo-European lan-guage evolution.
Proceedings of the Royal SocietyB, 278(1713):1794?1803.Jelena Proki?, Martijn Wieling, and John Nerbonne.2009.
Multiple sequence alignments in linguis-tics.
In Proceedings of the EACL 2009 Workshop onLanguage Technology and Resources for CulturalHeritage, Social Sciences, Humanities, and Educa-tion, pages 18?25, Stroudsburg, PA. Association forComputational Linguistics.Donald Ringe, Tandy Warnow, and Ann Taylor.
2002.Indo-european and computational cladistics.
Trans-actions of the Philological Society, 100(1):59?129.Hattori Shir?.
1973.
Japanese dialects.
In Henry M.Hoenigswald and Robert H. Langacre, editors, Di-achronic, areal and typological linguistics, pages368?400.
Mouton, The Hague and Paris.Robert.
R. Sokal and Charles.
D. Michener.
1958.A statistical method for evaluating systematic rela-tionships.
University of Kansas Scientific Bulletin,28:1409?1438.George Starostin.
2008.
Tower of Babel.
An etymo-logical database project.
Online ressource.
URL:http://starling.rinet.ru.Lydia Steiner, Peter F. Stadler, and Michael Cysouw.2011.
A pipeline for computational historicallinguistics.
Language Dynamics and Change,1(1):89?127.Robert L. Trask, editor.
2000.
The dictionary of his-torical and comparative linguistics.
Edinburgh Uni-versity Press, Edinburgh.Peter Turchin, Ilja Peiros, and Murray Gell-Mann.2010.
Analyzing genetic connections between lan-guages by matching consonant classes.
Journal ofLanguage Relationship, 3:117?126.Feng Wang.
2006.
Comparison of languages incontact.
Institute of Linguistics Academia Sinica,Taipei.125
