A Figure of Merit for the Evaluation of Web-Corpus RandomnessMassimiliano CiaramitaInstitute of Cognitive Science and TechnologyNational Research CouncilRoma, Italym.ciaramita@istc.cnr.itMarco BaroniSSLMITUniversita` di BolognaForl?`, Italybaroni@sslmit.unibo.itAbstractIn this paper, we present an automated,quantitative, knowledge-poor method toevaluate the randomness of a collectionof documents (corpus), with respect to anumber of biased partitions.
The methodis based on the comparison of the wordfrequency distribution of the target corpusto word frequency distributions from cor-pora built in deliberately biased ways.
Weapply the method to the task of building acorpus via queries to Google.
Our resultsindicate that this approach can be used,reliably, to discriminate biased and unbi-ased document collections and to choosethe most appropriate query terms.1 IntroductionThe Web is a very rich source of linguistic data,and in the last few years it has been used in-tensively by linguists and language technologistsfor many tasks (Kilgarriff and Grefenstette, 2003).Among other uses, the Web allows fast and in-expensive construction of ?general purpose?
cor-pora, i.e., corpora that are not meant to repre-sent a specific sub-language, but a language as awhole.
There are several recent studies on theextent to which Web-derived corpora are com-parable, in terms of variety of topics and styles,to traditional ?balanced?
corpora (Fletcher, 2004;Sharoff, 2006).
Our contribution, in this paper, isto present an automated, quantitative method toevaluate the ?variety?
or ?randomness?
(with re-spect to a number of non-random partitions) ofa Web corpus.
The more random/less-biased to-wards specific partitions a corpus is, the more itshould be suitable as a general purpose corpus.We are not proposing a method to evaluatewhether a sample of Web pages is a random sam-ple of the Web, although this is a related issue(Bharat and Broder, 1998; Henzinger et al, 2000).Instead, we propose a method, based on simpledistributional properties, to evaluate if a sampleof Web pages in a certain language is reasonablyvaried in terms of the topics (and, perhaps, tex-tual types) it contains.
This is independent fromwhether they are actually proportionally represent-ing what is out there on the Web or not.
For exam-ple, although computer-related technical languageis probably much more common on the Web than,say, the language of literary criticism, one mightprefer a biased retrieval method that fetches docu-ments representing these and other sub-languagesin comparable amounts, to an unbiased methodthat leads to a corpus composed mostly of com-puter jargon.
This is a new area of investigation ?with traditional corpora, one knows a priori theircomposition.
As the Web plays an increasinglycentral role as data source in NLP, we believe thatmethods to efficiently characterize the nature ofautomatically retrieved data are becoming of cen-tral importance to the discipline.In the empirical evaluation of the method, wefocus on general purpose corpora built issuing au-tomated queries to a search engine and retrievingthe corresponding pages, which has been shown tobe an easy and effective way to build Web-basedcorpora (Ghani et al, 2001; Ueyama and Baroni,2005; Sharoff, 2006).
It is natural to ask whichkinds of query terms, henceforth seeds, are moreappropriate to build a corpus comparable, in termsof variety, to traditional balanced corpora such asthe British National Corpus, henceforth BNC (As-ton and Burnard, 1998).
We test our procedureto assess Web-corpus randomness on corpora built217using seeds chosen following different strategies.However, the method per se can also be used to as-sess the randomness of corpora built in other ways;e.g., by crawling the Web.Our method is based on the comparison of theword frequency distribution of the target corpusto word frequency distributions constructed usingqueries to a search engine for deliberately biasedseeds.
As such, it is nearly resource-free, as itonly requires lists of words belonging to specificdomains that can be used as biased seeds.
In ourexperiments we used Google as the search engineof choice, but different search engines could beused as well, or other ways to obtain collectionsof biased documents, e.g., via a directory of pre-categorized Web-pages.2 Relevant workOur work is related to the recent literature onbuilding linguistic corpora from the Web using au-tomated queries to search engines (Ghani et al,2001; Fletcher, 2004; Ueyama and Baroni, 2005;Sharoff, 2006).
Different criteria are used to se-lect the seeds.
Ghani and colleagues iterativelybootstrapped queries to AltaVista from retrieveddocuments in the target language and in other lan-guages.
They seeded the bootstrap procedure withmanually selected documents, or with small setsof words provided by native speakers of the lan-guage.
They showed that the procedure producesa corpus that contains, mostly, pages in the rele-vant language, but they did not evaluate the resultsin terms of quality or variety.
Fletcher (2004) con-structed a corpus of English by querying AltaVistafor the 10 top frequency words from the BNC.He then conducted a qualitative analysis of fre-quent n-grams in the Web corpus and in the BNC,highlighting the differences between the two cor-pora.
Sharoff (2006) built corpora of English, Rus-sian and German via queries to Google seededwith manually cleaned lists of words that are fre-quent in a reference corpus in the relevant lan-guage, excluding function words, while Ueyamaand Baroni (2005) built corpora of Japanese usingseed words from a basic Japanese vocabulary list.Both Sharoff and Ueyama and Baroni evaluatedthe results through a manual classification of theretrieved pages and by qualitative analysis of thewords that are most typical of the Web corpora.We are also interested in evaluating the effectthat different seed selection (or, more in general,corpus building) strategies have on the nature ofthe resulting Web corpus.
However, rather thanperforming a qualitative investigation, we developa quantitative measure that could be used to evalu-ate and compare a large number of different corpusbuilding methods, as it does not require manual in-tervention.
Moreover, our emphasis is not on thecorpus building methodology, nor on classifyingthe retrieved pages, but on assessing whether theyappear to be reasonably unbiased with respect to arange of topics or other criteria.3 Measuring distributional properties ofbiased and unbiased collectionsOur goal is to create a ?balanced?
corpus of Webpages in a given language; e.g., the portion com-posed of all Spanish Web pages.
As we observedin the introduction, obtaining a sample of unbi-ased documents is not the same as obtaining anunbiased sample of documents.
Thus, we will notmotivate our method in terms of whether it favorsunbiased samples from the Web, but in terms ofwhether the documents that are sampled appear tobe balanced with respect to a set of deliberatelybiased samples.
We leave it to further research toinvestigate how the choice of the biased samplingmethod affects the performance of our procedureand its relations to uniform sampling.3.1 Corpora as unigram distributionsA compact way of representing a collection ofdocuments is by means of frequency lists, whereeach word is associated with the number of timesit occurs in the collection.
This representation de-fines a simple ?language model?, a stochastic ap-proximation to the language of the collection; i.e.,a ?0th order?
word model or a ?unigram?
model.Language models of varying complexity can bedefined.
As the model?s complexity increases, itsapproximation to the target language improves ?cf.
the classic example of Shannon (1948) on theentropy of English.
In this paper we focus on un-igram models, as a natural starting point, howeverthe approach extends naturally to more complexlanguage models.3.2 Corpus similarity measureWe start by making the assumption that similarcollections will determine similar language mod-els, hence that the similarity of collections of doc-uments is closely related to the similarity of the218derived unigram distributions.
The similarity oftwo unigram distributions P and Q is estimated asthe relative entropy, or Kullback Leibler distance,or KL (Cover and Thomas, 1991) D(P ||Q):D(P ||Q) =?x?WP (x) log P (x)Q(x) (1)KL is a measure of the cost, in terms of aver-age number of additional bits needed to describethe random variable, of assuming that the distribu-tion is Q when instead the true distribution is P .Since D(P ||Q) ?
0, with equality only if P = Q,unigram distributions generated by similar collec-tions should have low relative entropy.
To guaran-tee that KL is always finite we make the assump-tion that the random variables are defined over thesame finite alphabet W , the set of all word typesoccurring in the observed data.
To avoid furtherinfinite cases a smoothing value ?
is added whenestimating probabilities; i.e.,P (x) = cP (x) + ?|W |?
+?x?W cP (x)(2)where cP (x) is the frequency of x in distributionP, and |W | is the number of word types in W .3.3 A scoring function for sampled unigramdistributionsWhat properties distinguish unigram distributionsdrawn from the whole of a document collectionsuch as the BNC or the Web (or, rather, from thespace of the Web we are interested in samplingfrom) from distributions drawn from biased sub-sets of it?
This is an important question because,if identified, such properties might help discrimi-nating between sampling methods which producemore random collections of documents from morebiased ones.
We suggest the following hypothesis.Unigrams sampled from the full set of documentshave distances from biased samples which tendto be lower than the distances of biased samplesto other samples based on different biases.
Sam-ples from the whole corpus, or Web, should pro-duce lower KL distances because they draw wordsacross the whole vocabulary, while biased sampleshave mostly access to a single specialized vocab-ulary.
If this hypothesis is true then, on average,the distance between the unbiased sample and allother samples should be lower than the distancebetween a biased sample and all other samples.21mba21b2abmmlccc1ahgACBFigure 1.
Distances (continuous lines with arrows) be-tween points representing unigram distributions, sam-pled from biased partitions A and B and from the fullcollection of documents C = A ?B.Figure 1 depicts a geometric interpretation ofthe intuition behind this hypothesis.
Suppose thatthe two squares A and B represent two parti-tions of the space of documents C. Additionally,m pairs of unigram distributions, represented aspoints, are produced by sampling documents uni-formly at random from these partitions; e.g.
a1and b1.
The mean Euclidean distance between(ai, bi) pairs is a value between 0 and h, the lengthof the diagonal of the rectangle which is the unionof A and B.
Instead of drawing pairs we can drawtriples of points, one point from A, one from B,and another point from C = A ?
B. Approxi-mately half of the points drawn from C will lie inthe A square, while the other half will lie in the Bsquare.
The distance of the points drawn from Cfrom the points drawn from B will be between 0and g, for approximately half of the points (thoselaying in the B region), while the distance is be-tween 0 and h for the other half of the points (thoseinA).
Therefore, ifm is large enough, the averagedistance between C and B (or A) must be smallerthan the average distance between A and B, be-cause h > g.To summarize, then, we suggest the hypothe-sis that samples from the full distribution havea smaller mean distance than all other samples.More precisely, let Ui,k be the kth of N unigramdistributions sampled with method yi, yi ?
Y ,where Y is the set of sampling categories.
Ad-ditionally, for clarity, we will always denote withy1 the predicted unbiased sample, while yj , j =2..|Y |, denote the biased samples.
Let M be amatrix of measurements, M ?
IR|Y |?|Y |, suchthat Mi.j =PNk=1 D(Ui,k,Uj,k)N , where D(., .)
is therelative entropy.
In other words, the matrix con-tains the average distances between pairs of sam-219Mode Domain Genre1 BNC BNC BNC2 W S education W miscellaneous3 S W leisure W pop lore4 W arts W nonacad soc sci5 W belief thought W nonacad hum art.. .. ..C-4 S spont conv C1 S sportsliveC-3 S spont conv C2 S consultationC-2 S spont conv DE W fict dramaC-1 S spont conv UN S lect commerceC no cat no catTable 1.
Rankings based on ?, as the mean distancebetween samples from the BNC partitions plus samplesfrom the whole corpus (BNC).
C is the total number ofcategories.
W stands for Written, S for Spoken.
C1, C2,DE, UN are demographic classes for the spontaneousconversations, no cat is the BNC undefined category.ples (biased or unbiased).
Each row Mi ?
IR|Y |contains the average distances between yi and allother ys, including yi.
A score ?i is assigned toeach yi which is equal to the mean of the vectorMi (excluding Mi,j , j = i, which is always equalto 0):?i =1|Y | ?
1|Y |?j=1,j 6=iMi,j (3)We propose this function as a figure of merit1for assigning a score to sampling methods.
Thesmaller the ?
value the closer the sampling methodis to a uniform sampling method, with respect tothe pre-defined set of biased sampling categories.3.4 Randomness of BNC samplesLater we will show how this hypothesis is consis-tent with empirical evidence gathered from Webdata.
Here we illustrate a proof-of-concept exper-iment conducted on the BNC.
In the BNC docu-ments come classified along different dimensionsthus providing a controlled environment to test ourhypothesis.
We adopt here David Lee?s revisedclassification (Lee, 2001) and we partition the doc-uments in terms of ?mode?
(spoken/written), ?do-main?
(19 labels; e.g., imaginative, leisure, etc.
)and ?genre?
(71 labels; e.g., interview, advertise-ment, email, etc.).
For each of the three mainpartitions we sampled with replacement (from adistribution determined by relative frequency inthe relevant set) 1,000 words from the BNC andfrom each of the labels belonging to the specific1A function which measures the quality of the samplingmethod with the convention that smaller values are better aswith merit functions in statistics.partitions.2 Then we measured the distance be-tween each label in a partition, plus the samplefrom the whole BNC.
We repeated this experiment100 times, built a matrix of average distances, andranked each label yi, within each partition type,using ?i.
Table 1 summarizes the results (only par-tial results are shown for domain and genre).
In allthree experiments the unbiased sample ?BNC?
isranked higher than all other categories.
At the topof the rankings we also find other less narrowlytopic/genre-dependent categories such as ?W?
formode, or ?W miscellaneous?
and ?W pop lore?for genre.
Thus the hypothesis seems supported bythese experiments.
Unbiased sampled unigramstend to be closer, on average, to biased samples.4 Evaluating the randomness ofGoogle-derived corporaWhen downloading documents from the Web via asearch engine (or sample them in other ways), onecannot choose to sample randomly, nor select doc-uments belonging to a certain category.
One cantry to control the typology of documents returnedby using specific query terms.
At this point a mea-sure such as the one we proposed can be used tochoose the least biased retrieved collection amonga set of retrieved collections.4.1 Biased and unbiased query categoriesTo construct a ?balanced?
corpus via a searchengine one reasonable strategy is to use appro-priately balanced query terms, e.g., using ran-dom terms extracted from an available balancedcorpus (Sharoff, 2006).
We will evaluate sev-eral such strategies by comparing the derivedcollections with those obtained with openly bi-ased/specialized Web corpora.
In order to buildspecialized domain corpora, we use biased queryterms from the appropriate domain following theapproach of Baroni and Bernardini (2004).
Wecompiled several lists of words that define likelybiased and unbiased categories.
We extracted theless biased terms from the balanced 1M-wordsBrown corpus of American English (Kuc?era andFrancis, 1967), from the 100M-words BNC, andfrom a list of English ?basic?
terms.
From theseresources we defined the following categories ofquery terms:2We filtered out words in a stop list containing 1,430types, which were either labeled with one of the BNC func-tion word tags (such as ?article?
or ?coordinating conjunc-tion?
), or occurred more than 50,000 times.2201.
Brown.hf: the top 200 most frequent wordsfrom the Brown corpus;2.
Brown.mf: 200 random terms with fre-quency between 100 and 50 inclusive fromBrown;3.
Brown.af: 200 random terms with minimumfrequency 10 from Brown;4.
BNC.mf: 200 random terms with frequencybetween 506 and 104 inclusive from BNC;5.
BNC.af: 200 random terms from BNC;6.
BNC.demog: 200 random terms with fre-quency between 1000 and 50 inclusive fromthe BNC spontaneous conversation sections;7.
3esl: 200 random terms from an ESL ?corevocabulary?
list.3Some of these lists implement plausible strate-gies to get an unbiased sample from the searchengine: high frequency words and basic vocab-ulary words should not be linked to any specificdomain; while medium frequency words, such asthe words in the Brown.mf/af and BNC.mf lists,should be spread across a variety of domains andstyles.
The BNC.af list is sampled randomly fromthe whole BNC and, because of the Zipfian prop-erties of word types, coupled with the large sizeof the BNC, it is mostly characterized by very lowfrequency words.
In this case, we might expectdata sparseness problems.
Finally, we expect thespoken demographic sample to be a ?mildly bi-ased?
set, as it samples only words used in spokenconversational English.In order to build biased queries, hopefully lead-ing to the retrieval of topically related documents,we defined a set of specialized categories us-ing the WordNet (Fellbaum, 1998) ?domain?
lists(Magnini and Cavaglia, 2000).
We selected 200words at random from each of the following do-mains: administration, commerce, computer sci-ence, fashion, gastronomy, geography, law, mili-tary, music, sociology.
These domains were cho-sen since they look ?general?
enough that theyshould be very well-represented on the Web, butnot so general as to be virtually unbiased (cf.
theWordNet domain person).
We selected words onlyamong those that did not belong to more than3http://wordlist.sourceforge.net/12dicts-readme.htmlone WordNet domain, and we avoided multi-wordterms.It is important to realize that a balanced corpusis not necessary to produce unbiased seeds, nor atopic-annotated lexical resource for biased seeds.Here we focus on these sources to test plausiblecandidate seeds.
However, biased seeds can be ob-tained following the method of Baroni and Bernar-dini (2004) for building specialized corpora, whileunbiased seeds could be selected, for example,from word lists extracted from all corpora ob-tained using the biased seeds.4.2 Experimental settingFrom each source list we randomly select 20 pairsof words without replacement.
Each pair is usedas a query to Google, asking for pages in En-glish only.
Pairs are used instead of single wordsto maximize our chances to find documents thatcontain running text (Sharoff, 2006).
For eachquery, we retrieve a maximum of 20 documents.The whole procedure is repeated 20 times with alllists, so that we can compute the mean distancesto fill the distance matrices.
Our unit of analysisis the corpus of all the non-duplicated documentsretrieved with a set of 20 paired word queries.The documents retrieved from the Web undergopost-processing, including filtering by minimumand maximum size, removal of HTML code and?boilerplate?
(navigational information and simi-lar) and heuristic filtering of documents that donot contain connected text.
A corpus can con-tain maximally 400 documents (20 queries times20 documents retrieved per query), although typi-cally the documents retrieved are less, because ofduplicates, or because some query pairs are foundin less than 20 documents.
Table 2 summarizesthe average size in terms of word types, tokensand number of documents of the resulting cor-pora.
Queries for the unbiased seeds tend to re-trieve more documents except for the BNC.af set,which, as expected, found considerably less datathan the other unbiased sets.
Most of the differ-ences are not statistically significant and, as the ta-ble shows, the difference in number of documentsis often counterbalanced by the fact that special-ized queries tend to retrieve longer documents.4.3 Distance matrices and bootstrap errorestimationAfter collecting the data each sample was repre-sented as a frequency list as we did before with221Search category Types Tokens DocsBrown.hf 39.3 477.2 277.2Brown.mf 32.8 385.3 261.1Brown.af 35.9 441.5 262.5BNC.mf 45.6 614.7 253.6BNC.af 23.0 241.7 59.7BNC.demog 32.6 367.1 232.23esl 47.1 653.2 261.9Admin 39.8 545.1 220.5Commerce 38.9 464.5 184.7Comp sci 25.8 311.5 185.3Fashion 44.5 533.7 166.2Gastronomy 36.5 421.7 159.0Geography 42.7 498.0 167;6Law 49.2 745.4 211.4Military 47.1 667.8 223.0Music 45.5 558.7 201.3Sociology 56.0 959.5 258.8Table 2.
Average number of types, tokens and docu-ments of corpora constructed with Google queries (typeand token sizes in thousands).the BNC partitions (cf.
section 3.4).
Unigram dis-tributions resulting from different search strate-gies were compared by building a matrix of meandistances between pairs of unigram distributions.Rows and columns of the matrices are indexed bythe query category, the first category correspondsto one unbiased query, while the remaining in-dexes correspond to the biased query categories;i.e., M ?
IR11?11, Mi,j =P20k=1 D(Ui,k,Uj,k)20 ,where Us,k is the kth unigram distribution pro-duced with query category ys.These Web-corpora can be seen as a dataset Dof n = 20 data-points each consisting of a seriesof unigram word distributions, one for each searchcategory.
If all n data-points are used once to buildthe distance matrix we obtain one such matrix foreach unbiased category and rank each search strat-egy yi using ?i, as before (cf.
section 3.3).
Insteadof using all n data-points once, we createB ?boot-strap?
datasets (Duda et al, 2001) by randomly se-lecting n data-points fromD with replacement (weused a value of B=10).
The B bootstrap datasetsare treated as independent sets and used to produceB individual matricesMb from which we computethe score ?i,b, i.e., the mean distance of a categoryyi with respect to all other query categories in thatspecific bootstrap dataset.
The bootstrap estimateof ?i, called ?
?i is the mean of the B estimates onthe individual datasets:?
?i =1BB?b=1?i,b (4)Bootstrap estimation can be used to compute thestandard error of ?i:?boot[?i] =????1BB?b=1[?
?i ?
?i,b]2 (5)Instead of building one matrix of average dis-tances over N trials, we could build N matri-ces and compute the variance from there ratherthan with bootstrap methods.
However this sec-ond methodology produces noisier results.
Thereason for this is that our hypothesis rests on theassumption that the estimated average distance isreliable.
Otherwise, the distance of two arbitrarybiased distributions can very well be smaller thanthe distance of one unbiased and a biased one, pro-ducing noisier measurements.As we did before for the BNC data, wesmoothed the word counts by adding a count of 1to all words in the overall dictionary.
This dictio-nary is approximated with the set of all words oc-curring in the unigrams involved in a given exper-iment, overall on average approximately 1.8 mil-lion types (notice that numbers and other specialtokens are boosting up this total).
Words with anoverall frequency greater than 50,000 are treatedas stop words and excluded from consideration(188 types).5 ResultsTable 3 summarizes the results of the experimentswith Google.
Each column represents one experi-ment involving a specific ?
supposedly ?
unbiasedcategory.
The category with the best (lowest) ?score is highlighted in bold.
The unbiased sampleis always ranked higher than all biased samples.The results show that the best results are achievedwith Brown corpus seeds.
The bootstrapped er-ror estimate shows that the unbiased Brown sam-ples are significantly more random than the biasedsamples and, orthogonally, of the BNC and 3eslsamples.
In particular medium frequency termsseem to produce the best results, although the dif-ference among the three Brown categories are notsignificant.
Thus, while more testing is needed,our data provide some support for the choice ofmedium frequency words as best seeds.Terms extracted from the BNC are less effec-tive than terms from the Brown corpus.
One pos-sible explanation is that the Web is likely to con-tain much larger portions of American than BritishEnglish, and thus the BNC queries are overall222?
scores with bootstrap error estimatesCategory Brown.mf Brown.af Brown.hf BNC.mf BNC.demog BNC.all 3eslUnbiased .1248/.0015 .1307/.0019 .1314/.0010 .1569/.0025 .1616/.0026 .1635/.0026 .1668/.0030Commerce .1500/.0074 .1500/.0074 .1500/.0073 .1708/.0088 .1756/.0090 .1771/.0091 .1829/.0093Geography .1702/.0084 .1702/.0084 .1707/.0083 .1925/.0089 .1977/.0091 .1994/.0092 .2059/.0094Fashion .1732/.0060 .1732/.0060 .1733/.0059 .1949/.0069 .2002/.0070 .2019/.0071 .2087/.0073Admin .1738/.0034 .1738/.0034 .1738/.0033 .2023/.0037 .2079/.0038 .2096/.0038 .2163/.0039Comp sci .1749/.0037 .1749/.0037 .1746/.0038 .1858/.0041 .1912/.0042 .1929/.0042 .1995/.0043Military .1899/.0070 .1899/.0070 .1901/.0067 .2233/.0079 .2291/.0081 .2311/.0082 .2384/.0084Music .1959/.0067 .1959/.0067 .1962/.0067 .2196/.0077 .2255/.0078 .2274/.0079 .2347/.0081Gastronomy .1973/.0122 .1973/.0122 .1981/.0120 .2116/.0133 .2116/.0133 .2193/.0138 .2266/.0142Law .1997/.0060 .1997/.0060 .1990/.0061 .2373/.0067 .2435/.0068 .2193/.0138 .2533/.0070Sociology .2393/.0063 .2393/.0063 .2389/.0062 .2885/.0069 .2956/.0070 .2980/.0071 .3071/.0073Table 3.
Mean scores based on ?
with bootstrap standard error (B=10).
In bold the lowest (best) score in eachcolumn, always the unbiased category.more biased than the Brown queries.
Alterna-tively, this might be due to the smaller, more con-trolled nature of the Brown corpus, where evenmedium- and low-frequency words tend to be rel-atively common terms.
The internal ranking of theBNC categories, although not statistically signifi-cant, seems also to suggest that medium frequencywords (BNC.mf) are better than low frequencywords.
In this case, the all/low frequency set(BNC.af) tends to contain very infrequent words;thus, the poor performance is likely due to datasparseness issues, as also indicated by the rela-tively smaller quantity of data retrieved (Table 2above).
We take the comparatively lower rankof BNC.demog to constitute further support forthe validity of our method, given that the corre-sponding set, being entirely composed of wordsfrom spoken English, should be more biased thanother unbiased sets.
This latter finding is partic-ularly encouraging because the way in which thisset is biased, i.e., in terms of mode of communica-tion, is completely different from the topic-basedbias of the WordNet sets.
Finally, the queriesextracted from the 3esl set are the most biased.This unexpected result might relate to the factthat, on a quick inspection, many words in thisset, far from being what we would intuitively con-sider ?core?
vocabulary, are rather cultivated, of-ten technical terms (aesthetics, octopi, misjudg-ment, hydroplane), and thus they might show aregister-based bias that we do not find in listsextracted from balanced corpora.
We randomlyselected 100 documents from the corpora con-structed with the ?best?
unbiased set (Brown.mf)and 100 documents from this set, and we classi-fied them in terms of genre, topic and other cat-egories (in random order, so that the source ofthe rated documents was not known).
This pre-liminary analysis did not highlight dramatic dif-ferences between the two corpora, except for thefact that 6 over 100 documents in the 3esl sub-corpus pertained to the rather narrow domain ofaviation and space travel, while no comparablynarrow topic had such a large share of the distri-bution in the Brown.mf sub-corpus.
More researchis needed into the qualitative differences that cor-relate with our figure of merit.
Finally, althoughdifferent query sets retrieve different amounts ofdocuments, and lead to the construction of corporaof different lengths, there is no sign that these dif-ferences are affecting our figure of merit in a sys-tematic way; e.g., some of the larger collections,in terms of number of documents and token size,are both at the top (most unbiased samples) and atthe bottom of the ranks (law, sociology).On Web data we observed the same effect wesaw with the BNC data, where we could directlysample from the whole collection and from its bi-ased partitions.
This provides support for the hy-pothesis that our measure can be used to evaluatehow unbiased a corpus is, and that issuing unbi-ased/biased queries to a search engine is a viable,nearly knowledge-free way to create unbiased cor-pora, and biased corpora to compare them against.6 ConclusionAs research based on the Web as corpus becomesmore prominent within computational and corpus-based linguistics, many fundamental issues haveto be tackled in a systematic way.
Among these,the problem of assessing the quality and natureof automatically created corpora, where we donot know a priori the composition of the cor-pus.
In this paper, we considered an approach toautomated corpus construction, via search enginequeries for combinations of a set of seed words.223We proposed an automated, quantitative, nearlyknowledge-free way to evaluate how biased a cor-pus constructed in this way is.
Our method isbased on the idea that the more a collection is un-biased the closer its distribution of words will be,on average, to reference distributions derived frombiased partitions (we showed that this is indeed thecase using a fully available balanced collection;i.e., the BNC), and on the idea that biased collec-tions of Web documents can be created by issu-ing biased queries to a search engine.
The resultsof our experiments with Google support our hy-pothesis, and suggest that seeds to build unbiasedcorpora should be selected among mid-frequencywords rather than high or low frequency words.We realize that our study opens many ques-tions.
The most crucial issue is probably what itmeans for a corpus to be unbiased.
As we alreadystressed, we do not necessarily want our corpusto be an unbiased sample of what is out there onthe Net ?
we want it to be composed of content-rich pages, and reasonably balanced in terms oftopics and genres, despite the fact that the Webitself is unlikely to be ?balanced?.
For our pur-poses, we implicitly define balance in terms of theset of biased corpora that we compare the targetcorpus against.
Assuming that our measure is ap-propriate, what it tells us is that a certain corpus ismore/less biased than another corpus with respectto the biased corpora they are compared against.
Itremains to be seen how well the results generalizeacross different typologies of biased corpora.The method is not limited to the evaluation ofcorpora built via search engine queries; e.g., itwould be interesting to compare the latter to cor-pora built by Web crawling.
The method couldbe also applied to the analysis of corpora in gen-eral (Web-derived or not), both for the purpose ofevaluating biased-ness, and as a general purposecorpus comparison technique (Kilgarriff, 2001).AcknowledgmentsWe would like to thank Ioannis Kontoyiannis,Adam Kilgarriff and Silvia Bernardini for usefulcomments on this work.ReferencesG.
Aston and L. Burnard.
1998.
The BNC Handbook:Exploring the British National Corpus with SARA.Edinburgh University Press, Edinburgh.M.
Baroni and S. Bernardini.
2004.
BootCaT: Boot-strapping Corpora and Terms from the Web.
In Pro-ceedings of LREC 2004, pages 1313?1316.K.
Bharat and A. Broder.
1998.
A Technique for Mea-suring the Relative Size and Overlap of the PublicWeb Search Engines.
In Proceedings of WWW7,pages 379?388.T.M.
Cover and J.A.
Thomas.
1991.
Elements of In-formation Theory.
Wiley, New York.R.O.
Duda, P.E.
Hart, and D.G.
Stork.
2001.
PatternClassification 2nd ed.
Wiley Interscience, Wiley In-terscience.C.
Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge.B.
Fletcher.
2004.
Making the Web more Useful asa Source for Linguistic Corpora.
In U. Conor andT.
Upton, editors, Corpus Linguistics in North Amer-ica 2002.
Rodopi, Amsterdam.R.
Ghani, R. Jones, and D. Mladenic.
2001.
Usingthe Web to Create Minority Language Corpora.
InProceedings of the 10th International Conference onInformation and Knowledge Management.M.
Henzinger, A. Heydon, and M. Najork.
2000.
OnNear-Uniform URL Sampling.
In Proceedings ofWWW9.A.
Kilgarriff and G. Grefenstette.
2003.
Introductionto the Special Issue on the Web as Corpus.
Compu-tational Linguistics, 29:333?347.A.
Kilgarriff.
2001.
Comparing Corpora.
Interna-tional Journal of Corpus Linguistics, 6:1?37.H.
Kuc?era and W. Francis.
1967.
Computational Anal-ysis of Present-Day American English.
Brown Uni-versity Press, Providence, RI.D.
Lee.
2001.
Genres, Registers, Text, Types, Do-mains and Styles: Clarifying the Concepts and Nav-igating a Path through the BNC Jungle.
LanguageLearning & Technology, 5(3):37?72.B.
Magnini and G. Cavaglia.
2000.
Integrating SubjectField Codes into WordNet.
In Proceedings of LREC2000, Athens, pages 1413?1418.C.E.
Shannon.
1948.
A Mathematical Theory of Com-munication.
Bell System Technical Journal, 27:379?423 and 623?656.S.
Sharoff.
2006.
Creating General-Purpose CorporaUsing Automated Search Engine Queries.
In M. Ba-roni and S. Bernardini, editors, WaCky!
Working pa-pers on the Web as Corpus.
Gedit, Bologna.M.
Ueyama and M. Baroni.
2005.
Automated Con-struction and Evaluation of a Japanese Web-BasedReference Corpus.
In Proceedings of Corpus Lin-guistics 2005.224
