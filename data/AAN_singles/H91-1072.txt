In tegrat ing  Syntax  and Semant ics  intoSpoken Language Unders tand ing  1Lynette Hirschman, Stephanie Seneff,David Goodine, and Michael PhillipsSpoken Language Systems GroupLaboratory  for Computer  Sc ienceMassachuset ts  Ins t i tu te  of  Techno logyCambr idge ,  MA 02139ABSTRACTThis paper describes everal experiments combining naturallanguage and acoustic onstraints o improve overall performanceof the MIT VOYAGER spoken language system.
This system cou-ples the SUMMIT speech recognition system with the TINA lan-guage understanding system to answer spoken queries about nav-igational assistance in the Cambridge, MA, area.
The overallgoal of our research is to combine acoustic, syntactic and seman-tic knowledge sources.
Our first experiment showed improvementby combining acoustic score and parse probability normalized fornumber of terminals.
Results were further improved by the useof an explicit rejection criterion based on normalized parse prob-abilities.
The use of the combined parse/acoustic score, togetherwith the rejection criterion, gave an improvement in overall scoreof more than 33% on both training and test data, where score isdefined as percent correct minus percent incorrect.
Experimentson a fully integrated system which uses the parser to predict pos-sible next words to the recognizer are now underway.BACKGROUNDThe experiments that we report on in this paper repre-sent some initial steps in combining speech knowledge withsyntactic and semantic knowledge.
These experiments havebeen performed using the MIT VOYAGER system \[10\], whichprovides navigational ssistance for finding directions and lo-cations of various objects (e.g., hotels, restaurants, banks)in a geographic region (Cambridge, MA).
VOYAGER, acceptsspoken queries from untrained users and produces answersin the form of a map, written answers, and spoken output.The system has a vocabulary of some 320 words and handlesquestions, indirect questions, and various forms of interac-tive dialogue, including anaphoric reference and clarificationdialogue.In this research, we have taken an incremental pproachto combining speech and language.
First, we have exploredhow use of combined knowledge sources can influence theshape of the search space, by changing the overall scores as-1This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research.sociated with competing hypotheses.
In addition, the use ofcombined knowledge sources can change computational effi-ciency by applying language constraints to predict possiblenext words, thus achieving significant pruning of the recog-nition search space.
In this paper, we report primarily onexperiments hat change the shape of the search space; thiswork has been done on our loosely-coupled system using theN-best interface \[11\].
However, we also report briefly on thestatus of our experiments on the tight coupling of speechrecognition and language understanding.LANGUAGE CONSTRAINTSDURING RECOGNITIONIn order to obtain adequate recognition results for con-tinuous speech recognition, it is imperative to provide somesort of language constraints.
The usual approach is to adoptsimple but efficient word-pair or bigram "language models,"which specify the set of words that can follow a given word.Such models have the advantage ofbeing automatically deriv-able from training data and computationally efficient.
How-ever, they lose any non-local anguage constraints and, ofcourse, provide no linguistically relevant structural descrip-tion.
Furthermore, it is difficult, even when steps are takento generalize words to their semantic ategory (i.e.
"Boston"all city names), to assure sufficient coverage in an inde-pendent est set.
Given adequate training data, the simpleword-pair or bigram language model will overgenerate, sinceit fails to take larger context into account.
Thus the system isallowed to recognize many sentences that are ungrammaticalor incoherent or inappropriate in the overall context.The obvious solution is to bring linguistic knowledge tobear.
One way is to take the best acoustic candidate anduse a flexible, semantically-based phrase-spotting system toassign a meaning to the sequence of words \[8\].
This providesa robust interface which can ignore many recognition errorsand abandons the notion of a linguistically well-formed over-all sentence.
It almost always produces ome interpretation.However, since it adds no real linguistic constraints, it mayproduce many false positives (misinterpretation of the input ).366A second possiblity which has been explored at some sites \[1\]is to have the recognizer produce a word lattice, with (acous-tic) transition probabilities between words.
The languagesystem can then search this lattice for the best candidate.Another approach, which is the baseline for these exper-iments, uses an N-best interface between the recognizer andthe language understanding system.
In this interface, therecognizer produces entence hypotheses in decreasing orderof acoustic score.
The role of the language understandingsystem is to filter these hypotheses, choosing the first onethat can be fully processed.
Finally, the approach that weexplore here combines a score provided by the parser (e.g,on the basis of a probability assignment), with the acousticscore, to provide a "best" answer.
We will report here on theresults of several experiments combining parse probabilitiesand acoustic score.SYSTEM ARCHITECTUREThe VOYAGER system consists of the TINA natural an-guage understanding system and the s u M M IT speech recogni-tion system.
These components will only be described brieflyhere, as they are more fully documented in \[7,9,10\].TINA combines a general English syntax at the top levelwith a semantic grammar framework at lower levels, to pro-vide an interleaved syntax/semantics analysis that minimizesperplexity.
As a result, most sentences in TINA have only oneparse.
TINA uses a best-first heuristic search in parsing, stor-ing alternate candidate parse paths while it pursues the mostpromising (most probable) path.
In addition, the grammaris trainable from instances of parse trees, as described in thenext section.The SUMMIT system transforms a speech waveform intoa segment lattice.
Features are extracted for each segmentand used to determine a set of acoustic scores for phone can-didates.
A lexicon provides word pronunciations which areexpanded through phonological rules into a network of al-ternate pronunciations for each word.
The control strategyto align the segmental acoustic-phonetic network with thelexical word-pronunciation network uses an N-best interfacewhich produces the top N candidate word sequences in de-creasing order of total path score.
It makes use of an A*search algorithm \[3,4\] with an initial Viterbi search serving asthe mechanism for establishing a tight upper-bound estimateof the score for the unseen portion of each active hypothesis.Language constraints include both a local word-pair con-straint and a more global inguistic onstraint based on parsabil-ity.
The word-pair constraint is precompiled into the wordnetwork, and limits the set of words that can follow any givenword, without regard to sentence context.
Allowable wordpairs were determined automatically by generating a largenumber of random sentences from the grammar \[7\].
The lin-guistic constraints are incorporated either as a filter on full-sentence hypotheses as they come off the top of the stack,or with a tighter coupling in which active partial theoriesdynamically prune the set of allowable next-word candidatesduring the search.TRAIN ING PARSEPROBABIL IT IESThis section describes our procedure for training the prob-abilities in the grammar automatically from a set of parsedtraining sentences.
From each training sentence is derived aset of context-free rules needed to parse that sentence.
Theentire pool of rules is used to train the grammar probabilities,with each rule occurring one or more times in the trainingdata.
By training on a set of some 3500 sentences within theVOYAGER domain, we were able to reduce the perplexity onan independent test set by a factor of three \[10\].Our approach to training a grammar differs from that ofmany current schemes, mainly in that we have intentionallytried to set up a framework that easily produces a probabilityestimate for the next word given the preceding word sequence.We feel that a next-word probability is much more appropri-ate than a rule-production probability for incorporating intoa tightly coupled system, since it leads to a simple definitionof the total score for the next word as the weighted sum ofthe language model probability and the acoustic probability.While rule-production probabilities can in fact be generatedfrom the probabilities we provide, they will not, in general,agree with the probabilities as determined by a proceduresuch as the inside/outside algorithm \[6,2\].In our approach, the grammar is partitioned into rulesets, according to the left-hand side (LHS) category in thecontext free rule set.
Within each partition, the categoriesthat show up on the right-hand side (RHS) of all the rulessharing the unique LHS category for the partition are used toform a bigram language model for the categories particularto that partition.
Thus the language model statistics are en-coded as a set of two-dimensional t bles of category-categorytransition probabilities, one table for each partition.
A di-rect consequence of this bigram model within each partitionis that new sibling "chains" may form, producing, in manycases, combinations that were never explicitly mentioned inthe original rule set.
The parser is driven only by the setof local node-node transitions for a given LHS, so that anynew chains take on the same status as sibling sets (RHS)that appeared explicitly in the original grammar.
While thisproperty can at times lead to inadvertent rules that are in-appropriate, it often yiekls productive new rules and allowsfor faster generalization of the grammar.
Given a particularparse tree, the probability for the next word is the product ofthe node-node transition probabilities linking the next wordto the previous word.
The overall next-word probability fora given initial word sequence is then the sum over all parsetrees spanning the entire word sequence.A specific example should help to elucidate the training367process.
Imagine that a training set provides a set of fiverules as shown in Table 1.
The training algorithm producesa transition etworks as shown in Figure 1, with probabilitiesestablished by counting and normalizing pair frequencies, aswould be done at the word level in a traditional bigram lan-guage model.
Rule production probabilities can be regener-ated from these pair transition probabilities, giving the resultshown in the column "Derived Probability" in the table, tobe compared with "Original Probability.
"The derived probabilities are not the same as what onewould get by simply counting rule frequencies.
The prob-abilities are correct up to the point of the category NOUN;that is, there is a 2/5 probability of getting the rule group(1,4) and a 3/5 probability of getting the group (2,3,4).
How-ever, the transitions out of NOUN are conditional on the rulegroup.
That is, rules that start with (ART NOUN) have a50/50 chance of being followed by an ADJUNCT, whereas theremaining rules have a 1/3 chance.
The method of ignor-ing everything except the preceding node has the effect ofsmoothing these two groups, giving all of them an equalchance (2/5) of finding an ADJUNCT next.
This is a formof deleted-interpolation \[5\] and it helps to get around sparsedata problems, although it is making an independence as-sumption: whether or not a noun is followed by an adjunctis assumed to be independent of the preceding context of thenoun.
In this example, no new rules were introduced.
Ifwe had, however, no training for Rule 4, it would still get anonzero probability, because all of its sibling pairs are avail-able from other rules.
That is to say, not only does thismethod smooth probabilities among rules, but it also createsnew rules that had not been explicitly seen in the trainingset.The grammar itself includes yntactic and semantic on-straints that may cause a particular next-sibling tofail.
Thereis also a trace mechanism that restores a moved category toits deep-structure position.
Both of these mechanisms dis-rupt the probabilities in ways that are ignored by the train-ing method.
While it is possible to renormalize on the flyby checking all next-siblings against he constraints and ac-cumulating the total probability of those that pass, we didnot in fact do this, in the interest of computation.
We doplan to incorporate this normalizing step in a future exper-iment, to assess whether it offers a significant improvementin the probability estimates.
We do currently make somecorrections for the gap mechanism.
Rather than using the apriori statistics on the likelihood of a node whose child is atrace, we simply assume that node occurred with probability1.0.
While neither of these is absolutely correct, the latter isgenerally much closer to the truth than the former.The TINA grammar has been trained on more than 3500sentences.
The parse score is computed as the sum of the logprobabilities of the node-node transitions in the parse tree.The probability of a given terminal is taken to be 1/K, whereK is the number of lexical items having the same lexical class.TRAINING RULES OriginalProbabilityi: NP = ART NOUN I/52: NP = ART ADJ NOUN 2/52: NP = ART ADJ NOUN (repeat)3: NP = ART ADJ NOUN ADJUNCT i/54: NP = ART NOUN ADJUNCT i/5DerivedProbability61259/256/254/25Table 1: Deriving Probabilities from Training Rules.1.0<_L)Figure 1: Probabilistic Network for NPIt would not be difficult o incorporate more sophisticated s-timates of lexical items, for example, using unigram probabil-ities within a given lexical class, but we did not do that here,to avoid sparse data problems.
The parse scores reportedin the following section are the log probabilities, .normalizedfor the number of terminals, to compensate for decreasingprobabilities in longer sentences.EXPERIMENTAL  RESULTSWe have used the N-best interface with TINA as the filterfor our baseline in measuring performance improvements de-rived from combining parse and acoustic information.
To aidus in assessing the impact of various changes, we have useda composite score for system performance, computed as per-cent of correct answers minus the percent of wrong answers 2.Here we define "correct answer" very strictly, namely pro-ducing tbe call to the VOYAGER back-end that would havebeen produced by a "clean" transcription of the sentence, re-moving false starts and filled pauses?
The advantage of thisstrict method is that the procedure can be fully automatedand requires no human judgements.
It does allow certain"meaning preserving" alterations, e.g, insertion or deletion2This is the metric urrently in use for overall performance evaluationin the DARPA Spoken Language System program.3Note that this metric differs from that used to determine correct-hess in the results reported in the DARPA June 1990 meeting \[10\].
Forthose results, correctness was judged in terms of producing the sameaction, as judged by an expert.
Under the new, stricter criterion, if thetranscribed sentence produces no function call (action), the recognizedsentence annot possibly be correct - even if it has produced a reason-able interpretation f the input.
We estimate that approximately 5%of the sentences that are incorrect here would have been judged correctunder the earlier criterion.
This accounts for a difference in about 19points in score, bringing the two results into approximate agreement.368of "the" as in "the Royal East" vs. "Royal East".
Such a cri-terion seems reasonable, given that correctness for a spokenlanguage system should measure understanding, rather thanword accuracy.The N-best  InterfaceIn the N-best interface, the grammar functions only asthe filter, and N is used as a rejection criterion.
As Nincreases, the number of correct answers increases, but thenumber of incorrect answers also increases.
Overall systemperformance rises rapidly between N = 1 and N = 6, peaksat N = 25 and then drops off gradually, as the system findsincorrect answers at a faster rate than correct answers (seeFigure 2).
The optimal N is 25, with a score of 18.3 (36.1%correct and 17.8% incorrect).
This figure is used as the base-line, against which performance improvements are calculated.I-zU_I uUJ4030201000.
- - .
.
.
?
.
.
.
.
.
.
- .
, o o. e e o o.
?r c, 9 ?
B o ~ c, o-r ~ ds"Pm_  r .
f .
jr. r "  m"  ~"  r "  db"~dr"I " Score20 40 60 8"0 I O0Figure 2: Performance as a Function of N. Score = Correct -IncorrectAdding Parse Probabi l i t iesIf we now combine a parse score with the acoustic score,we get much better results.
We can see how this works bylooking at the example in Table 2.
Here we see that thecorrect answer is eventually found in the N-best output (theeleventh sentence).
However, it is preceded by other sen-tences that parse and produce possible (but wrong)functioncalls to VOYAGER.
The N-best output produces its candi-dates in order of acoustic score.
We see that the correctsentence has a worse acoustic score (-1336 vs. -1521) butits parse score is substantially better (-14.1 vs. -18.0).
Ingeneral, we note that the normalized parse score is a gooddiscriminator of right vs. wrong sentences: the mean for cor-rect answers is -2.92 with a standard deviation of 0.75, whilefor incorrect answers it is -4.31 with a standard deviation of1.78.
If we compute the most obvious thing, which is a linearcombination of the normalized parse score and acoustic score,it is possible, by proper choice of weight, to get the correctanswer to have the best combined score.
This is illustratedin Table 3, which shows the relative combined scores at twoRank Acoustics Pat'se #Wds Sentence1.
-1336 X it i get to kendall sq2.
-1387 -18.0 6 could i get to kendall sq3.
-1432 -18.0 6 would i get to kendall sq4.
-1455 X it i'd get to kendall sq5.
-1460 X it i do the kendall sq6.
-1472 X at do i get to kendall sq7.
-1506 X could i'd get to kendall sq8.
-1509 X i'd i get to kendall sq9.
-1511 X could i do the kendall sq10.
-1516 X it i get at kendall sq11.
-1521 -14.1 7 how do i get to kendall sqTable 2: N-best Output With Acoustic and Parse Scores2.
could i get to kendall sqAcoustic Score Parse/#Wds = Norm Parse-1387 -18.0/6 -3.0Total Score @ W = 100 : -1387 + 100" -3.0 = -1687Total Score @ W = 200 : -1387 + 200* -3.0 = -198711. how do i get to kendall sqAcoustic Score Parse/#Wds = Norm Parse-1521 -14.1/77 = -2.0Total Score @ W = 100:-1521 + 100" -2.0 = -1721Total Score @ W = 200:-1521 + 200* -2.0 = -1971Table 3: Combining the Parse and Acoustic Scoresdifferent weights; at weight W = 100, the wrong answer stillhas a higher combined score.
However, as we increase theweight of the parse score (e.g., to W = 200), the correctparse receives a higher combined score.
We can determinean optimal parse score weight for the training data by look-ing at overall score (percent correct minus percent incorrect)as a function of parse score weight.
The combination thatproduced the optimal overall score for the VOYAGER trainingdata was Acoustics + 600 * Normalized-Parse, as shown inFigure 3.
In order to determine the effect of size of N on thisnumber, we also ran experiments varying size of N. It turnsout that although optimal N using only the acoustic scoreis N = 25, optimal N for the combined parse and acousticscore is 35, but it is fairly stable between N = 25 to N = 100.Using the combined acoustic plus weighted parse score, someof the original errors are corrected: the percent correct (at N= 25) goes up from 36.1% for the N-best case to 38.7% forthe combined score, while the incorrect percent goes downfrom 17.8% to 15.1%.
At N = 25, we get an overall score of23.6%, compared to 18.3% for N-best alone (an increase of30%).36960~- 40 ZI.LIU|F~--,oco~rect ~t l "  score0 1000 2000Parse  WeightFigure 3: Performance as a Function of Weighting Factor50-o  - ?
-e  -0  -e  - ?
-e  -P  -e  -e .
-D  .~  1 ,,- - -  IncorrectL Score ,10 " - - ~  ~ _ _0 - ?
- ?
- ?
- ?
- - -- '0  -8  -6  -4  -2  0Thresho ldF igure  4: Performance as a Function of the Thresholdz 30wU20A Rejection Thresho ldFinally, if we make use of the normalized parse score toformulate an explicit rejection criterion, we find that we canimprove our results still further.
Figure 4 shows how per-cent correct and percent incorrect vary with the choice ofthreshold.
Using an empirically determined threshold of -4.0, the performance at N = 25 shows 37.5% correct (losingsome correct answers that fall below the threshold), 10.9%incorrect (a substantial reduction from 15.1% without theuse of a rejection threshold), and an overall score of 26.6%(up from 23.6% for use of combined parse and acoustic scorewithout a rejection criterion).
We also experimented with arejection criterion based on acoustic score (e.g., difference be-tween best score and current score) but did not find it usefulin this domain; however this did turn out to be useful in theATIS domain \[12\].A comparison ofthe following four different configurationsA: N = iB: N-Best ?
N = 25C: Weighted Parse + Acoustics Q N = 25D: Weighted Parse + Acoustics,Parse Rejection Threshold = -4.0 @ N = 25CORRECT INCORRECT NO ANSWER SCOREh 18.0 5.3 76.7 12.3B 36 .1  17 .8  46 .1  18 .3C 38.7 15.1 46.1 23.6D 37.5 10.9 51.6 26.6100806o~.
4o20Table 4: Scores for Training Data\ [~  No  AnswerIncor rectCorrectA B C DA: N=IB: N-Best  @ N = 25C: Comblned Parse + Acoust l c  @ N = 25D: Comblned Score, Threshold = -4  @ N = 25Figure 5: Overall Performance Under Four Conditionsat N = 25 is shown in Figure 5, with results in Table 4.The Test ResultsThe overall score was optimized by running on a set of568 training sentences (the development test set).
Once wedetermined optimum parameters for parse score weight (W-- 600), rejection threshold (T = -4), and value of N, wethen ran the test data (497 sentences) using these parame-ters.
The resulting increases in score are shown in Figure6 for both training and test data.
Overall, the test resultsare quite comparable to the training results.
The use of acombined parse plus acoustic score resulted in an increasefrom 21.5 to 28.0 in overall score (30%).
The use of a rejec-tion threshold together with the combined score resulted ina small additional increase to 28.8, more than 33% over theN-best results for N = 25.FUTURE D IRECT IONSAll of this research as been done as a first step towardscoupling the recognizer and the language understanding sys-tem more closely.
Our initial results show more than 33%improvement in score by using parse information i  addition370to acoustic score.
Having demonstrated that it is beneficial tochange the shape of the search space using this knowledge, weare now pursuing experiments with a tightly coupled systemto explore ways of increasing search e~ciency.
We currentlyhave a tightly coupled version of the system running thatproduces the identical output but uses TINA to predict al-lowable next words for the recognizer, given a string of wordshypothesized by the recognizer.
This approach as the po-tential to reduce the search space for the recognizer, sinceit will explore only word strings that can be interpreted byTINA.
This reduction in search space is done, of course, atthe price of considerable computation (namely parsing thecurrent hypotheses).
We plan to  investigate the trade-offsinvolved between the greater pruning provided by tight cou-pling vs. the greater computation required.
However, ourinitial results are quite promising: the tightly coupled systemproduces its answer in under a minute, running unoptimizedon a Sun SPARC-2 workstation.
The next step in tight cou-pling will be to incorporate the parse probabilities into theoverall A* (or other) search strategy.
By tuning the algo-rithm and off-loading some of the acoustic search to specialpurpose signal processing boards, we believe that the tightlycoupled mode will provide improved performance over theloosely-coupled N-best interface.30 'LIJCE8 20_J13ELxJ> 10 o' - Test: ScoreA: N=IB: N-Best  e N = 25 (opt imum)C: Combined Score @ N = 35 (opt imum)D: Combined w. Thresho ld  @ N = 25 (opt imum)Figure 6: Performance Results Incorporating Parse ProbabilitiesOur results to date provide strong evidence that we canuse additional knowledge from syntactic and semantic prob-abilities to improve overall system performance.
It also indi-cates that explicit rejection criteria play an important partin improving system performance.
In particular, the parsescore threshold provides a good rejection criterion based onsyntactic and semantic information.
Once we develop reli-able rejection criteria, we can begin to experiment with re-covery strategies from rejection.
For example, given a sen-tence that fails the rejection criterion, it might be possibleto interact with the user, saying e.g., "I thought you said'..?
; did I understand you correctly?"
This would allow theuser to confirm a correctly understood sentence and to cor-rect a misunderstood sentence.
This is surely preferable toproviding misleading information on the basis of an incor-rectly understood sentence.
The notion of rejection criteriashould also be helpful in identifying new words and sentenceswhich contain these words.
We plan to explore how to usehuman-machine interaction and combined syntax, semanticand acoustic knowledge to make further improvements in per-formance and usability of the spoken language interface.REFERENCES\[1\] Boisen, S., Y.-L. Chow, A. Hass, R. Ingria, S. Roukos, andD.
Stallard, "The BBN Spoken Language System," Proc.DARPA Speech and Natural Language Workshop, Philadel-phia, PA, February 1989.\[2\] Chitrao, M., and R. Grishman, "Statistical Parsing of Mes-sages," Proc.
DARPA Speech and Natural Language Work-shop, Hidden Valley, PA, June 1990.\[3\] Hart, P., N.J. Nilsson, and B. Raphael, "A Formal Basis forthe Heuristic Determination fMinimum Cost Paths," IEEETransactions ofSystems, Science and Cybernetics, Vol.
SSC-4, No.
2, pp.
100-107, 1968.\[4\] Jelinek, F., "Continuous Speech Recognition by StatisticalMethods," IEEE Proc., Vol.
64, No.
4, pp.
532-556, 1976.\[5\] Jelinek, F., and I~.L.
Mercer,, "Interpolated Estimationof Markov Source Parameters from Sparse Data," In E.S.Gelsema and L.N.
Kanal, Pattern Recognition in Practice,pp.
381-397, North-Holland, Amsterdam, 1980.\[6\] Jelinek, F., "Language Modelling for Speech Recognition Us-ing Context-free Grammars," Proc.
3rd Symposium on Ad-vanced Man-Machine Interaction through Spoken Language,Tokyo, Japan, December 1989.\[7\] Seneff, S, "TINA: A Probabilistic Syntactic Parser for SpeechUnderstanding Systems," Proc.
DARPA Speech and NaturalLanguage Workshop, Philadelphia, PA, February 1989.\[8\] Ward, W.,"The CMU Air Travel Information Service: Un-derstanding Spontaneous Speech," Proc.
DARPA Speech andNatural Language Workshop, Hidden Valley, PA, June 1990.\[9\] Zue, V., J.
Glass, M. Phillips, and S. Seneff, "The MIT SUM-MIT Speech Recognition System, a Progress Report," Proc.DARPA Speech and Natural Language Workshop, Philadel-phia, PA, February 1989.\[10\] Zue, V., J.
Glass, D. Goodine, H. Leung, M. Phillips, J. Po-lifroni, and S. Seneff, "The Voyager Speech UnderstandingSystem: Preliminary Development and Evaluation,"Proc.ICASSP-90, Albuquerque, NM, April 1990.\[11\] Zue, V., J.
Glass, D. Goodine, H. Leung, M. Phillips, J.Polifroni and S. Seneff, "Integration of Speech Recognitionand Natural Language Processing in the MIT VOYAGER Sys-tem," Proc.ICASSP-91, Toronto, Ontario, May 1991.\[12\] Zue, V., J.
Glass, D. Goddeau, D. Goodine, L. Hirschman, H.Leung, M. Phillips, J. Polifroni and S. Seneff, "Developmentand Preliminary Evaluation of the MIT ATIS System," theseproceedings, 1991.371
