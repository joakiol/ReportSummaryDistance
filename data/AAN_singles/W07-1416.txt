Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 95?100,Prague, June 2007. c?2007 Association for Computational LinguisticsTextual Entailment Using Univariate Density Model and MaximizingDiscriminant FunctionScott SettembreSNePS Research Group, Department of Computer Science and EngineeringUniversity at BuffaloBuffalo, NY 14214, USAss424@cse.buffalo.eduAbstractThe primary focuses of this entry this yearwas firstly, to develop a framework to al-low multiple researchers from our group toeasily contribute metrics measuring textualentailment, and secondly, to provide abaseline which we could use in our tools toevaluate and compare new metrics.
A de-velopment environment tool was created toquickly allow for testing of various metricsand to easily randomize the developmentand test sets.
For each test, this RTE toolcalculated two sets of results by applyingthe metrics to both a univariate Gaussiandensity and by maximizing a linear dis-criminant function.
The metrics used forthe submission were a lexical similaritymetric and a lexical similarity metric usingsynonym and antonym replacement.
Thetwo submissions for RTE 2007 scored anaccuracy of 61.00% and 62.62%.1 IntroductionThe task of textual entailment for the PASCALTextual Entailment Challenge for 2007 was to cre-ate a system to determine if a given pair of sen-tences, called the Text-Hypothesis (T-H) pair, hadthe property of having the Text sentence entail theHypothesis sentence.
Each Text-Hypothesis pair isalso assigned the type of entailment that should beapplied to the pair when evaluating its entailment.There are four types of entailment, each of whichmay or may not need different techniques to de-termine entailment, and for the purposes of theRTE tool developed, are considered separate prob-lems.2 RTE Development Environment ToolOur research group decided to begin focusing onthe Recognizing Textual Entailment challenge thisyear in February and to continue our participationfor years to come.
It was decided to create adevelopment environment from which ourresearchers could attempt different techniques ofexamining a Text-Hypothesis pair and yet almetrics resulting from those techniques could beused in calculating the final results.
The RTE toolalso randomly generates training and testing setsfrom the 800 Text-Hypothesis pairs provided fordevelopment by the competition to avoid over-fitting the data during the training stage.Figure 1.
Screenshot of the RTE Development Environment.95The RTE Tool can generate a metric by callinga .NET object, COM object, web page, commandline, or an internal function.
These metrics arecached to speed testing, though a specific metrictype can be cleared manually should the object orfunction generating the metric be changed.In the image of the RTE tool above, we can seea typical results screen.
We have a misclassifiedsample highlighted and all the relevant data for thatsample displayed on the bottom.
Each category isrepresented with a folder and displays the accuracyresults of the last classification.
In this way, wecan easily compare and contrast different metricsand their effectiveness on the samples in a simpleand intuitive way.2.1 Defining MetricsEach metric developed is required to produce acontinuous variable that can measure a feature ofthe T-H pair.
The metric value is required to benormalized between 0 and 1 inclusive so that wecan use the same metrics for future expansionwhen possibly dealing with nearest-neighbor clas-sification techniques and not be subject to scalingissues.
This is also valuable if we intend to de-velop vague predicates [Brachman and Levesque,2004] to use in Boolean rules, another potentialclassification implementation.There is also currently a constraint that themetric value ?0?
means the least entailment(according to that particular metric) and the value?1?
means the most entailment.
This helped createan easy way to maximize our linear discriminantfunction (which will be described below).
Thisconstraint is unnecessary when classifying usingthe univariate density model.2.2 Classification MethodsThe tool classifies a T-H test pair using one of twoclassification methods.
The first method uses themetrics of the training set to generate the parame-ters for eight Gaussian distributions, or two distri-butions for each type of textual entailment.
Eachdistribution describes a probability density function(PDF) for a particular type of entailment.
For ex-ample, there is one PDF for the entailment type of?Question Answering?
(QA) whose entailment is?YES?, and there is one PDF for the entailmenttype of QA whose entailment is ?NO?.
This uni-variate normal model was chosen to simplify thecalculations over the multivariate model weplanned to use.
Since the submissions would onlyconsider one metric for each run, instead of   usingall the metrics we have defined, the univariatemodel was appropriate.The second method of classification uses themetrics from the training set to develop a lineardecision boundary to maximize the accuracy out-come in the test set.
Once this boundary, orthreshold, is determined for each of the four typesof entailment, a simple comparison of the metricfrom a T-H pair can be classified based on whatside of the boundary it is on.
This linear discrimi-nant function had a further constraint that requiredthe metric values be described in a certain way tosimplify the classification function.
This require-ment will be lifted for our next submission in orderto deal with solution landscapes that may not ad-here to our Gaussian distribution model.3 Metric Set Used for SubmissionThree different metrics were developed for use inour RTE tool this year.
We decided to concentrateon producing simple measurements to create abaseline for which to judge the development ofnew metrics as well as to judge the performance offuture training or classification methods.Due to time constraints, we chose to employsimple metrics, which have been used before, inorder to meet our primary goals.
Despite the sim-plicity and the lack of semantic interpretation ofthe metrics, these metrics coupled with our patternclassification strategy yielded competitive results.3.1 Lexical Similarity Ratio MetricOur first metric is a simple lexical similarity ratiobetween the words in the Text and Hypothesis sen-tences in a T-H pair.
The formula counts numberof matches between the occurrences of a word inthe Hypothesis and the words in the Text.
Thesum is then normalized by dividing it by the num-ber of words in the Hypothesis itself.
For baselinepurposes, every word was considered and onlypunctuation was removed.
This technique was alsoused by other teams in previous challenge submis-sions [Jijkoun and Rijke, 2005].3.2 Average Matched Word DisplacementOur second metric was not used in the final results,but will be described for completeness.
This met-ric was the average of the distances in the Text be-96tween matched words from the Hypothesis normal-ized by dividing that average by the maximum pos-sible distance.
In other words, if two words in theHypothesis were found in the Text, the distancebetween them in the Text would be averaged withall the other combinations of matched word pairdistances and then normalized by dividing themaximum possible distance value for that particu-lar sentence.
Preliminary results showed a lessthan significant correlation and so were not used inthis submission.3.3 Synonym and Antonym ReplacementThe third metric is nearly identical to the lexicalsimilarity metric defined above except that if aword in the Hypothesis sentence is not matched,then all its synonyms and antonyms are alsosearched for in the Text sentence.
Any synonymmatches raise the score and any antonym matcheslower the score by a fixed amount, and in this casearbitrarily selected as ?1 (before normalization).
AMicrosoft Word 2003 COM object was used tosearch for the synonyms and antonyms from Mi-crosoft Word?s lexical database.4 Classification used for SubmissionTwo different types of classification methods wereused to classify entailment for a Text-Hypothesispair.
Both types are described below.We chose to initially keep our classificationmodels simple and easy to visualize so that bothour experienced and inexperienced research groupmembers could participate.
The ?No Free LunchTheorem?
[Duda, Hart, and Stork, 2001] showsthat there is no inherent benefit to any specificclassifier1 , and since the more important task ofgenerating the metrics 2  crosses academic disci-plines in our research group, we found communi-cating in terms of a Gaussian distribution was eas-ily understood.1 For ?good generalization performance, there are nocontext-independent or usage-independent reasons tofavor one learning or classification method over an-other.
?2 Since we are creating the metrics, we are attempting todistribute the values in a Gaussian curve.
This becomesa ?context?
which we can favor a classifier that willclassify the data better, such as the univariate normalmodel.
Our goal is to create a better metric and notnecessarily to find a better classifier.4.1 Univariate Normal ModelThe continuous univariate normal model, or Gaus-sian density, allows us to calculate p(x), or theprobability that feature x will appear in a dataset.The data points in the given dataset is assumed tobe distributed in a Gaussian distribution, some-times referred to as a bell curve.
Of course if thedata points in that data set turn out to be distributedin a non-Gaussian curve (i.e.
exponential curve oreven linear) or multimodal curve (more than onepeak), then we may not be able to draw any con-clusions.
For the purposes of our metrics, we areassuming a Gaussian distribution, and encouragethe developer of the metric function to attempt tofit the metric results into Gaussian curve.The two parameters of interest are the mean ?and the variance ?2, of the data points.
With thesetwo parameters, we are essentially able to calculatethe probability density function (PDF) for the cate-gory.
After calculating these parameters from thedevelopment data set, we can apply the followingformula to generate the probability, p(x), of a sam-ple, where x is the metric value we wish to classify.??????????
?= ??????
????
?xxp221exp21)(During the training step, the mean of a categoryis calculated.
The following formula does this cal-culation, where n is the number of samples, and xiis a particular metric of the ith sample:nxni i?== 1?Also during the training step, the variance of acategory is also calculated, with this formula:( )nnxxni inii?
?= =?= 12122/?For each type of entailment, there are two classi-fiers: one classifier for ?YES?
and one classifierfor ?NO?, representing the two categories.
Duringthe training step, the mean and variance parame-ters are calculated directly from the metrics that97come from the development data.
During the test-ing step, the specified metric is calculated for theT-H pair, and using the univariate normal formula,we can calculate the probability that the calculatedmetric is in the ?YES?
category or the ?NO?
cate-gory.
Then which ever result is larger, that cate-gory is chosen as the answer.To understand the limitations of this method, wehave a quick example.
Here is a parameter list ofeach category as well as the decisions that weremade from them:(IE,NO) = { ?
= 0.6867668 , ?
= 0.1824087}(IE,YES) = { ?
= 0.6874263 , ?
= 0.1622136}(IR,NO) = { ?
= 0.3649016 , ?
= 0.1984567}(IR,YES) = { ?
= 0.5888839 , ?
= 0.2035728}(QA,NO) = { ?
= 0.4470804 , ?
= 0.1821738}(QA,YES) = { ?
= 0.7330091 , ?
= 0.1873602}(SUM,NO) = { ?
= 0.4470848 , ?
= 0.2625011}(SUM,YES) = { ?
= 0.657442 , ?
= 0.250246}Overall correct entailments made: 492 out of 800.Overall probability of success : 0.615IE (200) [  %47.5  with 95 correct]Predicted YES (0) [  %NaN  with 0 correct]Predicted NO (200) [  %47.5  with 95 correct]IR (200) [  %66.5  with 133 correct]Predicted YES (76) [  %63.16  with 48 correct]Predicted NO (124) [  %68.55  with 85 correct]QA (200) [  %73.5  with 147 correct]Predicted YES (95) [  %77.89  with 74 correct]Predicted NO (105) [  %69.52  with 73 correct]SUM (200) [  %58.5  with 117 correct]Predicted YES (133) [  %60.9  with 81 correct]Predicted NO (67) [  %53.73  with 36 correct]As we can see, the two categories (IE,NO) and(IE,YES) are very similar in mean, ?.
This essen-tially translates to two Gaussian curves peaking atthe same point, which would cause an overlap thatwould favor the curve with the larger variance dur-ing the calculation of p(x).
If we look at the resultsusing these parameters, we can see that in the ?IE?type of entailment all decisions were made in favorof that category.
This does not mean that there isan error, just that the distribution of this metric istoo similar and so probably is not a good metric touse in deciding the classification for that category.Whereas in entailment type ?QA?, we find that thismetric does indeed divide the categories into twocurves that are quite separated, and so yields agood accuracy.4.2 Maximizing the Discriminant FunctionThis is the easiest way the RTE tool calculateswhether a T-H pair is in a specific category.
If ametric is less-than a specific threshold, then the T-H pair is classified as ?NO?, and if it is above thethreshold, then the pair is classified as ?YES?.Each type of entailment has its own discriminantfunction and therefore, there are only four classifi-ers or in this case, technically defined as four di-chotomizers.Each threshold is calculated using a brute forceiterative technique.
After the metric is calculatedfor each sample, the RTE tool simply incrementsthe threshold a certain fixed amount (arbitrarilyselected as 0.001 each each iteration) and recordsthe accuracy over the entire development data setfor that iteration.
As the process concludes afterone thousand iterations (that is, moving the thresh-old from 0 to 1 in .001 increments), the thresholdwith the maximum accuracy is selected as thethreshold for that classifier.
This, however, placesa constraint on the way the metric needs to be de-fined, as described above in section 2.1.5 ResultsThere are four result sets representing each of themetrics used paired with each of the classificationstrategies used.
The first table below shows theactual results, broken down into each type of en-tailment, using the released annotated test set.
Thesecond table shows our results by randomly split-ting the development dataset 80%/20% into a train-ing set (80%) and a testing set (20%).
From theresults listed in the second table, it was decidedwhich metric/classification pair would be used inour final submission.Although we cannot truly compare results fromthis competition to last years RTE 2 competition,we found that our results seemed quite competitive.
[Bar-Haim, Dagan, et al 2006]  We do recognizethat some of our metrics have already been em-ployed by other teams [Jijkoun and Rijke, 2005]and that our results may be different because of thethesaurus corpus we employed and the classifica-tion strategy we used.5.1 Actual ResultsThe actual results are based on training the RTEtool we developed on the released annotated de-velopment dataset and then applying the trainedclassifiers on the test dataset.
In this table, eachcolumn represents a training metric used with a98classification method.
For the two metrics used,?LS?
represents Lexical Similarity, while ?LR?represents Lexical Similarity with Synonym andAntonym Replacement (or Lexical Replacementfor short).
For the two types of classification used,?UN?
represents the Univariate Normal model,while ?DM?
represents Linear DiscriminantMaximization.LS+UN LR+UN LS+DM LR+DMOverall 0.615 0.626 0.61 0.629IE 0.475 0.510 0.495 0.505IR 0.665 0.630 0.635 0.640QA 0.735 0.750 0.750 0.750SUM 0.585 0.615 0.560 0.620As the reader can see, our final submissions?scores were not the maximal ones from the table.Our first submission we submitted scored 61% andour second submission scored 62.62%.
For ourfirst submission, the Lexical Similarity metric wasused in conjunction with the Linear DiscriminantMaximization model for classification.
For oursecond submission, our Lexical Replacement met-ric was used in combination with the UnivariateNormal model of classification.
These two combi-nations were chosen, however, from the trainingresults below.5.2 Training resultsUsing these results, it was decided to pick themaximal overall accuracy using both metrics.
Itwas assumed that the same correlations found inthe development dataset would be found in thetesting dataset.
Though this did not ring true inactuality, the final results using either method werequite close.LS+UN LR+UN LS+DM LR+DMOverall 0.669 0.675 0.717 0.644IE 0.425 0.575 0.625 0.600IR 0.688 0.667 0.688 0.646QA 0.811 0.784 0.811 0.784SUM 0.771 0.686 0.775 0.5436 Conclusions and Future EnhancementsThe lexical similarity metric and its variants obvi-ously have some correlation to whether a Text-Hypothesis pair has entailment or not.
Though wewere surprised by the results (from our runs ex-ceeding results from other teams?
runs from previ-ous years) and at how well they worked initially,further investigation found the accuracy of certaintypes of entailment, especially Information Extrac-tion (IE), lacking and perhaps making some met-rics almost irrelevant as a viable metric.By focusing our efforts this year on developinga tool to test various methods of classification andmetrics, we created an excellent way to developour ideas and distribute our research efforts amongresearchers.
The RTE Development Environmentwill help us coordinate our efforts and allow smallgains in any individual metric to contribute to theoverall classification in a proportionately signifi-cant way.For future enhancements, we intend to apply themultivariate model to process a metric vector indetermining classification instead of just consider-ing one metric at a time (as we did in the univariatemodel).
In addition, we intend to extend our met-rics to consider semantic interpretations and com-parisons between the Text-Hypothesis pair.We feel that our overall success was illuminat-ing to the larger task at hand and we are lookingforward to applying our decision making frame-work to next year?s submission.
Judging by ourresults, the simplicity of our approach will quitepossibly yield a competitive entailment strategyeven in comparison to more syntactic or semanticdecompositions of the sentence pairs at this time.Our primary success, over the three week periodin which we addressed this problem, was the de-velopment of a tool and a process by which mem-bers of our research group can interact.
The pool-ing of expertise from our linguistics, computer sci-ence, and cognitive science disciplines and con-structing our future plan of action culminated inthe development of this tool, benchmarks for ourgroup, and constraints in which we can operateefficiently and address this problem with moredepth in the future.7 AcknowledgementsWe would like to thank Dr. Stuart Shapiro and Dr.William Rapaport of the SNePS Research Group,University at Buffalo, for their encouragement andguidance in this year and in the years to come.99Special thanks to Dr. Sargur Srihari of CEDAR,?Center of Excellence for Document Analysis andRecognition?, University at Buffalo, for providinginsight into various classification techniques.
Fi-nally, we congratulate our members of the SNePSResearch Group for their contributions over theshort amount of time we had to address this chal-lenge this year.ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini and IdanSzpektor.
2006.
The Second PASCAL RecognisingTextual Entailment Challenge.
In Proceedings of theSecond PASCAL Challenges Workshop on Recognis-ing Textual Entailment.Ronald J. Brachman and Hector J. Levesque.
2004.Knowledge Representation and Reasoning.
MorganKaufmann Publishers, San Francisco, CA.Richard O. Duda, Peter E. Hart, David G. Stork.
PatternClassification.
Wily, New York, second edition,2001.Valentin Jijkoun and Maarten de Rijke.
RecognizingTextual Entailment Using Lexical Similarity.
Pro-ceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment, 2005.100
