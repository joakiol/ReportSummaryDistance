Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 874?878,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDecipherment with a Million Random RestartsTaylor Berg-Kirkpatrick Dan KleinComputer Science DivisionUniversity of California, Berkeley{tberg,klein}@cs.berkeley.eduAbstractThis paper investigates the utility and effect ofrunning numerous random restarts when us-ing EM to attack decipherment problems.
Wefind that simple decipherment models are ableto crack homophonic substitution ciphers withhigh accuracy if a large number of randomrestarts are used but almost completely failwith only a few random restarts.
For partic-ularly difficult homophonic ciphers, we findthat big gains in accuracy are to be had by run-ning upwards of 100K random restarts, whichwe accomplish efficiently using a GPU-basedparallel implementation.
We run a series ofexperiments using millions of random restartsin order to investigate other empirical proper-ties of decipherment problems, including thefamously uncracked Zodiac 340.1 IntroductionWhat can a million restarts do for decipherment?EM frequently gets stuck in local optima, so runningbetween ten and a hundred random restarts is com-mon practice (Knight et al 2006; Ravi and Knight,2011; Berg-Kirkpatrick and Klein, 2011).
But, howimportant are random restarts and how many randomrestarts does it take to saturate gains in accuracy?We find that the answer depends on the cipher.
Welook at both Zodiac 408, a famous homophonic sub-stitution cipher, and a more difficult homophonic ci-pher constructed to match properties of the famouslyunsolved Zodiac 340.
Gains in accuracy saturate af-ter only a hundred random restarts for Zodiac 408,but for the constructed cipher we see large gainsin accuracy even as we scale the number of ran-dom restarts up into the hundred thousands.
In bothcases the difference between few and many randomrestarts is the difference between almost completefailure and successful decipherment.We also find that millions of random restarts canbe helpful for performing exploratory analysis.
Welook at some empirical properties of deciphermentproblems, visualizing the distribution of local op-tima encountered by EM both in a successful deci-pherment of a homophonic cipher and in an unsuc-cessful attempt to decipher Zodiac 340.
Finally, weattack a series of ciphers generated to match proper-ties of Zodiac 340 and use the results to argue thatZodiac 340 is likely not a homophonic cipher underthe commonly assumed linearization order.2 Decipherment ModelVarious types of ciphers have been tackled by theNLP community with great success (Knight et al2006; Snyder et al 2010; Ravi and Knight, 2011).Many of these approaches learn an encryption keyby maximizing the score of the decrypted messageunder a language model.
We focus on homophonicsubstitution ciphers, where the encryption key is a1-to-many mapping from a plaintext alphabet to acipher alphabet.
We use a simple method introducedby Knight et al(2006): the EM algorithm (Demp-ster et al 1977) is used to learn the emission pa-rameters of an HMM that has a character trigramlanguage model as a backbone and the ciphertextas the observed sequence of emissions.
This meansthat we learn a multinomial over cipher symbols foreach plaintext character, but do not learn transition874parameters, which are fixed by the language model.We predict the deciphered text using posterior de-coding in the learned HMM.2.1 ImplementationRunning multiple random restarts means runningEM to convergence multiple times, which can becomputationally intensive; luckily, restarts can berun in parallel.
This kind of parallelism is a goodfit for the Same Instruction Multiple Thread (SIMT)hardware paradigm implemented by modern GPUs.We implemented EM with parallel random restartsusing the CUDA API (Nickolls et al 2008).
With aGPU workstation,1 we can complete a million ran-dom restarts roughly a thousand times more quicklythan we can complete the same computation with aserial implementation on a CPU.3 ExperimentsWe ran experiments on several homophonic sub-stitution ciphers: some produced by the infamousZodiac killer and others that were automaticallygenerated to be similar to the Zodiac ciphers.
Ineach of these experiments, we ran numerous randomrestarts; and in all cases we chose the random restartthat attained the highest model score in order to pro-duce the final decode.3.1 Experimental SetupThe specifics of how random restarts are producedis usually considered a detail; however, in this workit is important to describe the process precisely.
Inorder to generate random restarts, we sampled emis-sion parameters by drawing uniformly at randomfrom the interval [0, 1] and then normalizing.
Thecorresponding distribution on the multinomial emis-sion parameters is mildly concentrated at the centerof the simplex.2For each random restart, we ran EM for 200 itera-1We used a single workstation with three NVIDIA GTX 580GPUs.
These are consumer graphics cards introduced in 2011.2We also ran experiments where emission parameters weredrawn from Dirichlet distributions with various concentrationparameter settings.
We noticed little effect so long as the distri-bution did not favor the corners of the simplex.
If the distribu-tion did favor the corners of the simplex, decipherment resultsdeteriorated sharply.0.10.20.30.40.50.60.70.80.911  10  100  1000  10000  100000  1e+06-1530-1520-1510-1500-1490-1480-1470-1460AccuracyLog likelihoodNumber of random restartsLog likelihoodAccuracyFigure 1: Zodiac 408 cipher.
Accuracy by best model score andbest model score vs. number of random restarts.
Bootstrappedfrom 1M random restarts.tions.3 We found that smoothing EM was importantfor good performance.
We added a smoothing con-stant of 0.1 to the expected emission counts beforeeach M-step.
We tuned this value on a small heldout set of automatically generated ciphers.In all experiments we used a trigram characterlanguage model that was linearly interpolated fromcharacter unigram, bigram, and trigram counts ex-tracted from both the Google N-gram dataset (Brantsand Franz, 2006) and a small corpus (about 2Kwords) of plaintext messages authored by the Zodiackiller.43.2 An Easy Cipher: Zodiac 408Zodiac 408 is a homophonic cipher that is 408 char-acters long and contains 54 different cipher sym-bols.
Produced by the Zodiac killer, this cipher wassolved, manually, by two amateur code-breakers aweek after its release to the public in 1969.
Ravi andKnight (2011) were the first to crack Zodiac 408 us-ing completely automatic methods.In our first experiment, we compare a decode ofZodiac 408 using one random restart to a decode us-ing 100 random restarts.
Random restarts have high3While this does not guarantee convergence, in practice 200iterations seems to be sufficient for the problems we looked at.4The interpolation between n-gram orders is uniform, andthe interpolation between corpora favors the Zodiac corpus withweight 0.9.875variance, so when we present the accuracy corre-sponding to a given number of restarts we present anaverage over many bootstrap samples, drawn froma set of one million random restarts.
If we attackZodiac 408 with a single random restart, on aver-age we achieve an accuracy of 18%.
If we insteaduse 100 random restarts we achieve a much betteraverage accuracy of 90%.
The accuracies for vari-ous numbers of random restarts are plotted in Fig-ure 1.
Based on these results, we expect accuracyto increase by about 72% when using 100 randomrestarts instead of a single random restart; however,using more than 100 random restarts for this partic-ular cipher does not appear to be useful.Also in Figure 1, we plot a related graph, this timeshowing the effect that random restarts have on theachieved model score.
By construction, the (maxi-mum) model score must increase as we increase thenumber of random restarts.
We see that it quicklysaturates in the same way that accuracy did.This raises the question: have we actuallyachieved the globally optimal model score or havewe only saturated the usefulness of random restarts?We can?t prove that we have achieved the global op-timum,5 but we can at least check that we have sur-passed the model score achieved by EM when it isinitialized with the gold encryption key.
On Zodiac408, if we initialize with the gold key, EM findsa local optimum with a model score of ?1467.4.The best model score over 1M random restarts is?1466.5, which means we have surpassed the goldinitialization.The accuracy after gold initialization was 92%,while the accuracy of the best local optimum wasonly 89%.
This suggests that the global optimummay not be worth finding if we haven?t alreadyfound it.
From Figure 1, it appears that large in-creases in likelihood are correlated with increasesin accuracy, but small improvements to high like-lihoods (e.g.
the best local optimum versus the goldinitialization) may not to be.5ILP solvers can be used to globally optimize objectivescorresponding to short 1-to-1 substitution ciphers (Ravi andKnight, 2008) (though these objectives are slightly differentfrom the likelihood objectives faced by EM), but we find thatILP encodings for even the shortest homophonic ciphers cannotbe optimized in any reasonable amount of time.0.30.350.40.450.50.550.60.650.70.750.81  10  100  1000  10000  100000  1e+06-1310-1305-1300-1295-1290-1285-1280AccuracyLog likelihoodNumber of random restartsLog likelihoodAccuracyFigure 2: Synth 340 cipher.
Accuracy by best model score andbest model score vs. number of random restarts.
Bootstrappedfrom 1M random restarts.3.3 A Hard Cipher: Synth 340What do these graphs look like for a harder cipher?Zodiac 340 is the second cipher released by the Zo-diac killer, and it remains unsolved to this day.
How-ever, it is unknown whether Zodiac 340 is actually ahomophonic cipher.
If it were a homophonic cipherwe would certainly expect it to be harder than Zo-diac 408 because Zodiac 340 is shorter (only 340characters long) and at the same time has more ci-pher symbols: 63.
For our next experiment we gen-erate a cipher, which we call Synth 340, to matchproperties of Zodiac 340; later we will generate mul-tiple such ciphers.We sample a random consecutive sequence of 340characters from our small Zodiac corpus and usethis as our message (and, of course, remove this se-quence from our language model training data).
Wethen generate an encryption key by assigning eachof 63 cipher symbols to a single plain text charac-ter so that the number of cipher symbols mapped toeach plaintext character is proportional to the fre-quency of that character in the message (this bal-ancing makes the cipher more difficult).
Finally, wegenerate the actual ciphertext by randomly samplinga cipher token for each plain text token uniformly atrandom from the cipher symbols allowed for that to-ken under our generated key.In Figure 2, we display the same type of plot, thistime for Synth 340.
For this cipher, there is an abso-8760100002000030000400005000060000-1340 -1330 -1320 -1310 -1300 -1290 -1280 -1270FrequencyLog likelihood42%ntyouldli 59%veautital74%veautifulFigure 3: Synth 340 cipher.
Histogram of the likelihoods of thelocal optima encountered by EM across 1M random restarts.Several peaks are labeled with their average accuracy and asnippet of a decode.
The gold snippet is ?beautiful.
?lute gain in accuracy of about 9% between 100 ran-dom restarts and 100K random restarts.
A similarlylarge gain is seen for model score as we scale up thenumber of restarts.
This means that, even after tensof thousands of random restarts, EM is still findingnew local optima with better likelihoods.
It also ap-pears that, even for a short cipher like Synth 340,likelihood and accuracy are reasonably coupled.We can visualize the distribution of local optimaencountered by EM across 1M random restarts byplotting a histogram.
Figure 3 shows, for each rangeof likelihood, the number of random restarts thatled to a local optimum with a model score in thatrange.
It is quickly visible that a few model scoresare substantially more likely than all the rest.
Thiskind of sparsity might be expected if there werea small number of local optima that EM was ex-tremely likely to find.
We can check whether thepeaks of this histogram each correspond to a singlelocal optimum or whether each is composed of mul-tiple local optima that happen to have the same like-lihood.
For the histogram bucket corresponding to aparticular peak, we compute the average relative dif-ference between each multinomial parameter and itsmean.
The average relative difference for the highestpeak in Figure 3 is 0.8%, and for the second highestpeak is 0.3%.
These values are much smaller than02000400060008000100001200014000-1330 -1320 -1310 -1300 -1290 -1280FrequencyLog likelihoodFigure 4: Zodiac 340 cipher.
Histogram of the likelihoods of thelocal optima encountered by EM across 1M random restarts.the average relative difference between the means ofthese two peaks, 40%, indicating that the peaks docorrespond to single local optima or collections ofextremely similar local optima.There are several very small peaks that have thehighest model scores (the peak with the highestmodel score has a frequency of 90 which is toosmall to be visible in Figure 3).
The fact that thesemodel scores are both high and rare is the reason wecontinue to see improvements to both accuracy andmodel score as we run numerous random restarts.The two tallest peaks and the peak with highestmodel score are labeled with their average accuracyand a small snippet of a decode in Figure 3.
Thegold snippet is the word ?beautiful.
?3.4 An Unsolved Cipher: Zodiac 340In a final experiment, we look at the Zodiac 340cipher.
As mentioned, this cipher has never beencracked and may not be a homphonic cipher or evena valid cipher of any kind.
The reading order ofthe cipher, which consists of a grid of symbols, isunknown.
We make two arguments supporting theclaim that Zodiac 340 is not a homophonic cipherwith row-major reading order: the first is statistical,based on the success rate of attempts to crack similarsynthetic ciphers; the second is qualitative, compar-ing distributions of local optimum likelihoods.If Zodiac 340 is a homophonic cipher should we877expect to crack it?
In order to answer this questionwe generate 100 more ciphers in the same way wegenerated Synth 340.
We use 10K random restarts toattack each cipher, and compute accuracies by bestmodel score.
The average accuracy across these 100ciphers was 75% and the minimum accuracy was36%.
All but two of the ciphers were decipheredwith more than 51% accuracy, which is usually suf-ficient for a human to identify a decode as partiallycorrect.We attempted to crack Zodiac 340 using a row-major reading order and 1M random restarts, but thedecode with best model score was nonsensical.
Thisoutcome would be unlikely if Zodiac 340 were likeour synthetic ciphers, so Zodiac 340 is probably nota homophonic cipher with a row-major order.
Ofcourse, it could be a homophonic cipher with a dif-ferent reading order.
It could also be the case thata large number of salt tokens were inserted, or thatsome other assumption is incorrect.In Figure 4, we show the histogram of modelscores for the attempt to crack Zodiac 340.
We notethat this histogram is strikingly different from thehistogram for Synth 340.
Zodiac 340?s histogram isnot as sparse, and the range of model scores is muchsmaller.
The sparsity of Synth 340?s histogram (butnot Zodiac 340?s histogram) is typical of histogramscorresponding to our set of 100 generated ciphers.4 ConclusionRandom restarts, often considered a footnote of ex-perimental design, can indeed be useful on scalesbeyond that generally used in past work.
In particu-lar, we found that the initializations that lead to thelocal optima with highest likelihoods are sometimesvery rare, but finding them can be worthwhile; forthe problems we looked at, local optima with highlikelihoods also achieved high accuracies.
While thepresent experiments are on a very specific unsuper-vised learning problem, it is certainly reasonable tothink that large-scale random restarts have potentialmore broadly.In addition to improving search, large-scalerestarts can also provide a novel perspective whenperforming exploratory analysis, here letting us ar-gue in support for the hypothesis that Zodiac 340 isnot a row-major homophonic cipher.ReferencesTaylor Berg-Kirkpatrick and Dan Klein.
2011.
Sim-ple effective decipherment via combinatorial optimiza-tion.
In Proceedings of the 2011 Conference on Em-pirical Methods in Natural Language Processing.Thorsten Brants and Alex Franz.
2006.
Web 1t 5-gramversion 1.
Linguistic Data Consortium, Catalog Num-ber LDC2009T25.Arthur Dempster, Nan Laird, and Donald Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society.Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-mada.
2006.
Unsupervised analysis for deciphermentproblems.
In Proceedings of the 2006 Annual Meetingof the Association for Computational Linguistics.John Nickolls, Ian Buck, Michael Garland, and KevinSkadron.
2008.
Scalable parallel programming withCUDA.
Queue.Sujith Ravi and Kevin Knight.
2008.
Attacking deci-pherment problems optimally with low-order n-grammodels.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing.Sujith Ravi and Kevin Knight.
2011.
Bayesian inferencefor Zodiac and other homophonic ciphers.
In Proceed-ings of the 2011 Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies.Benjamin Snyder, Regina Barzilay, and Kevin Knight.2010.
A statistical model for lost language decipher-ment.
In Proceedings of the 2010 Annual Meeting ofthe Association for Computational Linguistics.878
