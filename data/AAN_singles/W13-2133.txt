Proceedings of the 14th European Workshop on Natural Language Generation, pages 208?209,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsContent Selection Challenge - University of Aberdeen EntryRoman KutlakChris MellishKees van DeemterDepartment of Computing ScienceUniversity of AberdeenAberdeen AB24 3UE, UKr.kutlak, c.mellish, k.vdeemter @abdn.ac.uk1 IntroductionBouayad-Agha et al(2012) issued a content de-termination challenge in which researchers wereasked to create systems that can automaticallyselect content suitable for a first paragraph in aWikipedia article from an RDF knowledge baseof information about people.
This article is a de-scription of the system built at the University ofAberdeen.Our working assumption is that the target textshould contain information that is commonlyknown about the target person.
The Wikipedia?smanual of style mentions that ?The lead [section]serves as an introduction to the article and a sum-mary of its most important aspects1.?
What is mostimportant about a person is likely to be often men-tioned in biographies and hence it is more likely tobe commonly known.Our system was motivated by the notion ofcommon ground, especially the way it was ac-counted for by (Clark and Marshall, 1981).
Clarkand Marshall (1981) introduce two categories ofcommon ground: personal common ground sharedby a small group of individuals and communalcommon ground shared by a community of peo-ple.
We are most interested in the concept of com-munal common ground, which arises from the ex-posure to the same information within a commu-nity.
For example, if there is a statue in front ofyour work place, you expect your colleagues toalso know about this statue and so the informationthat there is a statue in front of you workplace be-comes a part of the community knowledge (wherethe community are people who work at the sameplace).Our hypothesis is that if we take a corpus ofdocuments produced by some large community(e.g., English speakers), we should be able to ap-1http://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Lead_sectionproximate the community?s knowledge of certainfacts by counting how frequently they are men-tioned in the corpus.
For example, if a corpus con-tains 1000 articles about Sir Isaac Newton and 999of the examined documents mention the propertyof him being a physicist and only 50 documentsmention that he held the position as the wardenof the Royal Mint in 1696 we should expect morepeople to know that he was a physicist.We implemented the heuristic for approximat-ing communal common ground and tested it inan experiment with human participants to measurewhether there is a correlation between the heuris-tic?s predictions and actual knowledge of people(Kutlak et al 2012).
In our implementation, weused the Internet as a corpus of documents and weused the Google search engine for counting thenumber of documents containing the properties.Although the number of hits is only an estimateof the actual number of documents containing aparticular term, the heuristic achieved a Spearmancorrelation of 0.639 with p < 0.001 between theknowledge of people and the numbers of hits re-turned by Google.Although there are some issues with the use of aproprietary search engine such as Google (for ex-ample, the search engine can perform stemming;see Kilgarriff (2007) for a discussion) search en-gines have been successfully used previously (Tur-ney, 2001; Goudbeek and Krahmer, 2012).2 AlgorithmThe submitted system employs the heuristic out-lined in in the previous section.
The input is a col-lection of files containing information about peo-ple and a collection of human readable strings foreach of the files.
The data were taken from Free-base - a community created repository of informa-tion about people, places and other things.
Eachfile is a small knowledge base containing a set ofRDF triples describing the entity.208The data is encoded in machine-readable form(e.g., the fact that Newton was an astronomeris encoded as ns:m.03s9v ns:type.object.typens:astronomy.astronomer .)
so in order to findcollocations in a human written text, each RDFtriple has to be ?lexicalised.?
This is done by map-ping the RDF values to human produced stringsprovided by Freebase.
After substituting the lexi-calisations and removing some unnecessary infor-mation the algorithm adds the name of the target,which results in text such as Isaac Newton typeAstronomer.The algorithm reads one file at a time and cre-ates a human readable string for each of the prop-erties in the file.
In the second step, the system re-moves disambiguations (text in brackets) and fil-ters out properties that have the same string rep-resentation (duplicates).
Additionally, propertieswith certain attributes are filtered out to reduce thenumber of queries2.In the third step, the system uses Google cus-tom search API (a programming interface to thesearch engine) to estimate the score of each prop-erty.
Properties that contain the name of the entityare penalised.
This is done to reduce the impor-tance of properties such as the target?s parents orrelatives.
For example, if the algorithm was rank-ing properties of Sir Isaac Newton and a propertycontained the string Newton, the score assigned tothat property was multiplied by 0.75.
The prop-erties were then ordered by the number of corre-sponding hits in descending order.In the last step the algorithm selects the topranked properties.
The number of properties toselect was calculated by the following equation5 ?
log(|properties|).
This equation was chosenby intuition so that a larger proportion of proper-ties was selected for entities with a small numberof properties than for entities with a large numberof properties.
The set of properties in the aboveequation is the set obtained after the filtering.To prevent the system from selecting too manyproperties with the same attribute and to intro-duce variation, the system selected only five prop-erties with the same attribute (e.g., five films, fivebooks).2For example, the knowledge base describing Anton?
?nDvor?a?k contains 5670 properties of which 5154 have the at-tribute music.artist.track.3 Concluding RemarksThe implemented system uses a simple document-based collocation heuristic to decide what prop-erties to select.
This makes it prone to favour-ing properties that contain common words or thename of the described entity.
The advantage isthat the system is relatively simple and versatile.The ?common ground?
heuristic could be com-bined with another heuristic that assigns negativescore to properties that contain common words ora heuristic that estimates how interesting the prop-erty is.Finally, we do not expect the system to performbetter than machine learning based approachessuch as that of Duboue and McKeown (2003) butit will certainly be interesting to see how far onecan get with a simple heuristic.ReferencesNadjet Bouayad-Agha, Gerard Casamayor, Leo Wan-ner, and Chris Mellish.
2012.
Content selectionfrom semantic web data.
In Proceedings of INLG2012, pages 146?149, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Herbert H. Clark and Catherine Marshall.
1981.
Def-inite reference and mutual knowledge.
In A. K.Joshi, B. L. Webber, and I.
A.
Sag, editors, El-ements of discourse understanding, pages 10?63.Cambridge University Press, New York.Pablo A. Duboue and Kathleen R. McKeown.
2003.Statistical acquisition of content selection rules fornatural language generation.
In Proceedings ofthe 2003 EMNLP, pages 121?128, Morristown, NJ,USA.
Association for Computational Linguistics.Martijn Goudbeek and Emiel Krahmer.
2012.
Align-ment in interactive reference production: Con-tent planning, modifier ordering, and referentialoverspecification.
Topics in Cognitive Science,4(2):269?289.Adam Kilgarriff.
2007.
Googleology is bad science.Comput.
Linguist., 33:147?151, March.Roman Kutlak, Kees van Deemter, and Chris Mellish.2012.
Corpus-based metrics for assessing commu-nal common ground.
Proceedings of the 34th An-nual Meeting of the Cognitive Science Society.P.
Turney.
2001.
Mining the web for synonyms: PMI-IR versus LSA on TOEFL.
In Proceedings of thetwelfth european conference on machine learning(ecml-2001).209
