Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 58?65Manchester, UK.
August 2008Evaluation of Automatically Reformulated Questions in Question SeriesRichard Shaw, Ben Solway, Robert Gaizauskas and Mark A. GreenwoodDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 PortobelloSheffield S1 4DP UK{aca04rcs, aca04bs}@shef.ac.uk{r.gaizauskas, m.greenwood}@dcs.shef.ac.ukAbstractHaving gold standards allows us to evalu-ate new methods and approaches against acommon benchmark.
In this paper we de-scribe a set of gold standard question re-formulations and associated reformulationguidelines that we have created to supportresearch into automatic interpretation ofquestions in TREC question series, wherequestions may refer anaphorically to thetarget of the series or to answers to pre-vious questions.
We also assess variousstring comparison metrics for their utilityas evaluation measures of the proximity ofan automated system?s reformulations tothe gold standard.
Finally we show howwe have used this approach to assess thequestion processing capability of our ownQA system and to pinpoint areas for im-provement.1 IntroductionThe development of computational systems whichcan answer natural language questions using largetext collections as knowledge sources is widelyseen as both intellectually challenging and prac-tically useful.
To stimulate research and devel-opment in this area the US National Institute ofStandards and Technology (NIST) has organized ashared task evaluation as one track at the annualTExt Retrieval Conference (TREC) since 19991.These evaluations began by considering factoid-type questions only (e.g.
How many calories arec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1http://trec.nist.gov/there in a Big Mac?)
each of which was asked inisolation to any of the others.
However, in an effortto move the challenge towards a long term visionof interactive, dialogue-based question answer-ing to support information analysts (Burger et al,2002), the track introduced the notion of questiontargets and related question series in TREC2004(Voorhees, 2005), and this approach to questionpresentation has remained central in each of thesubsequent TRECs.
In this simulated task, ques-tions are grouped into series where each series hasa target of a definition associated with it (see Fig-ure 1).
Each question in the series asks for someinformation about the target and there is a final?other?
question which is to be interpreted as ?Pro-vide any other interesting details about the targetthat has not already been asked for explicitly?.
Inthis way ?each series is a (limited) abstraction ofan information dialogue in which the user is tryingto define the target.
The target and earlier ques-tions in a series provide the context for the currentquestion.?
(Voorhees, 2005).One consequence of putting questions into se-ries in this way is that questions may not makemuch sense when removed from the context theirseries provides.
For example, the question Whenwas he born?
cannot be sensibly interpreted with-out knowledge of the antecedent of he providedby the context (target or prior questions).
Inter-preting questions in question series, therefore, be-comes a critical component within a QA systems.Many QA systems have an initial document re-trieval stage that takes the question and derives aquery from it which is then passed to a search en-gine whose task is to retrieve candidate answeringbearing documents for processing by the rest ofthe system.
Clearly a question such as When washe born?
is unlikely to retrieve documents rele-58Target 136: ShiiteQ136.1 Who was the first Imam of the Shiitesect of Islam?Q136.2 Where is his tomb?Q136.3 What was this persons relationship tothe Prophet Mohammad?Q136.4 Who was the third Imam of ShiiteMuslims?Q136.5 When did he die?Figure 1: An Example Question Seriesvant to answering a question about Kafka?s dateof birth if passed directly to a search engine.
Thisproblem can be addressed in a naive way by sim-ply appending the target to every question.
How-ever, this has several disadvantages: (1) in somecases co-reference in a question series is to theanswer of a previous question and not to the tar-get, so blindly substituting the target is not ap-propriate; (2) some approaches to query formula-tion and to answer extraction from retrieved docu-ments may require syntactically well-formed ques-tions and may be able to take advantage of the extrainformation, such as syntactic dependencies, pro-vided in a fully de-referenced, syntactically correctquestion.Thus, it is helpful in general if systems can auto-matically interpret a question in context so as to re-solve co-references appropriately, and indeed mostTREC QA systems do this to at least a limited ex-tent as part of their question pre-processing.
Ide-ally one would like a system to be able to reformu-late a question as a human would if they were to re-express the question so as to make it independentof the context of the preceding portion of the ques-tion series.
To support the development of suchsystems it would useful if there were a collectionof ?gold standard?
reformulated questions againstwhich systems?
outputs could be compared.
How-ever, to the best of our knowledge no such resourceexists.In this paper we describe the creation of such acorpus of manually reformulated questions, mea-sures we have investigated for comparing systemgenerated reformulations against the gold stan-dard, and experiments we have carried out com-paring our TREC system?s automatic question re-formulator against the gold standard and insightswe have obtained therefrom.2 The Gold Standard CorpusOur aim was to take the questions in a TRECquestion series and re-express them as questionsthat would naturally be asked by a human ask-ing them as a single, stand-alone question outsidethe context of the question series.
Our intuitionwas that most adult native speakers would agreeon a small number of variant forms these refor-mulated questions would take.
We explored thisintuition by having two persons iteratively refor-mulate some questions independently, compare re-sults and evolve a small set of guidelines for theprocess.2.1 Creating the Gold StandardTen question sets were randomly selected fromsets available at http://trec.nist.gov/data/qa/t2007_qadata.html.
These werereformulated separately by two people and resultscompared.
From this an initial set of guidelineswas drawn up.
Using these guidelines another 10question sets from the TREC 2007 QA set were in-dependently reformulated and then the guidelinesrefined.At this point the reformulators?
outputs weresufficiently close to each other and the guidelinessufficiently stable that, given limited resources, itwas decided reformulation could proceed singly.Using the guidelines, therefore, a further 48 ques-tion sets from 2007 were reformulated, wherethis time each question set was only reformulatedby a single person.
Each question set containedbetween 5 and 7 individual questions thereforearound 406 questions were reformulated, creatingone or more gold standard forms for each question.In total there are approximately 448 individual re-formulations, with a maximum number of 3 refor-mulations for any single question and a mean of1.103 reformulations per question.2.2 GuidelinesUsing the above method we derived a set of simpleguidelines which anyone should be able to followto create a set of reformulated questions.Context independence and readability: Thereformulation of questions should be understand-able outside of the question series context.
The re-formulation should be written as a native speakerwould naturally express it; this means, for exam-ple, that stop words are included.Example: ?How many people were killed 199159eruption of Mount Pinatubo??
vs ?How manypeople were killed in the 1991 eruption of MountPinatubo?.
The latter is preferred as it more read-able due to the inclusion of stop words ?in the?.Reformulate questions so as to maximisesearch results:Example: ?Who was William Shakespeare?
?vs ?Who was Shakespeare??.
William should beadded to the phrase as it adds extra informationwhich could allow more results to be found.Target matches a sub-string of the question:If the target string matches a sub-string of the ques-tion the target string should substitute the entiretyof the substring.
Stop-words should not be usedwhen determining if strings and target match butshould usually be substituted along with the rest ofthe target.Example: Target: ?Sony Pictures Entertainment(SPE)?
; Question: ?What U.S. company did Sonypurchase to form SPE??
; Gold Standard: ?WhatU.S.
company did Sony purchase to form Sony Pic-tures Entertainment (SPE)?
?Rephrasing: A Question should not be unnec-essarily rephrased.Example: Target: ?Nissan Corp?
; Question:?What was Nissan formerly known as??
; ?Whatwas Nissan Corp. formerly known as??
is pre-ferred over the other possible reformulation ?Nis-san Corp. was formerly known as what?
?.Previous Questions and Answers: Questionswhich include a reference to a previous ques-tion should be reformulated to include a PREVI-OUS ANSWER variable.
Another reformulationshould also be provided should a system know itneeds the answer to the previous question but hasnot found one.
This should be a reformulation ofthe previous question within the current question.Example: Target: ?Harriet Miers withdrawsnomination to Supreme Court?
; Question: ?Whatcriterion did this person cite in nominatingMiers??
; Gold Standard 1: ?What criterion didPREVIOUS ANSWER cite in nominating HarrietMiers??
; Gold Standard 2: ?What criterion didthis person who nominated Harriet Miers for thepost cite in nominating Harriet Miers?
?Targets that contain brackets: Brackets in tar-get should be dealt with in the following way.
Thefull target should be substituted into the questionin the correct place as one of the Gold Standards.The target without the bracketed word and with itshould also be included in the Gold Standard.Example: Target: ?Church of Jesus Christof Latter-day Saints (Mormons)?
; Question:?Whofounded the Church of Jesus Christ of Latter-daySaints??
; Gold Standard 1: ?Who founded theChurch of Jesus Christ of Latter-day Saints (Mor-mons)??
; Gold Standard 2: ?Who founded theChurch of Jesus Christ of Latter-day Saints??
;Gold Standard 3 ?Who founded the Mormons?
?Stemming and Synonyms: Words should notbe stemmed and synonyms should not be used un-less they are found in the target or the current ques-tion series.
If they are found then both should beused in the Gold Standard.Example: Target: ?Chunnel?
; Question:?Howlong is the Chunnel??
; Gold Standard: ?How longis the Chunnel??
; Incorrect reformulation: ?Howlong is the Channel Tunnel?
?As the term ?Channel Tunnel?
is not referencedin this section or hard-coded into the QA engine itcannot be substituted for ?Chunnel?, even thoughdoing so may increase the probability of findingthe correct answer.It: The word it should be interpreted as referringto either the answer of the previous question of thatset or if no answer available to the target itself.Example:Target: ?1980 Mount St. Helens erup-tion?
; Question: ?How many people died whenit erupted??
; Gold Standard: ?How many peopledied when Mt.
St. Helens?
erupted in 1980?
?Pronouns (1): If the pronouns he or she areused within a question and the TARGET is of type?Person?
then substitute the TARGET string for thepronoun.
If however the PREVIOUS ANSWERis of type ?Person?
then it should be substituted in-stead as in this case the natural interpretation of thepronoun is to the answer of the previous question.Example: Target: ?Jay-Z?
; Question: ?Whenwas he born??
; Gold Standard: ?When was Jay-Zborn?
?Pronouns (2): If the pronouns his/hers/theirare used within a question and the TARGET is oftype ?Person?
then substitute the TARGET stringfor the pronoun appending the string ??s?
to theend of the substitution.
If however the PREVI-OUS ANSWER is of type ?Person?
then it shouldbe substituted as the natural interpretation of thepronoun is to the answer of the previous question.Example: Target: ?Jasper Fforde?
; Question:?What year was his first book written??
; GoldStandard: ?What year was Jasper Fforde?s firstbook written?
?603 Evaluation against the Gold StandardTo assess how close a system?s reformulation of aquestion in a questions series is to the gold stan-dard requires a measure of proximity.
Whatevermetric we adopt should have the property that re-formulations that are closer to our gold standard re-formulations get a higher score.
The closest possi-ble score is achieved by getting an identical stringto that of the gold standard.
Following conven-tional practice we will adopt a metric that gives usa value between 0 and 1, where 1 is highest (i.e.
ascore of 1 is achieved when the pre-processed re-formulation and the gold standard are identical).Another requirement for the metric is that theordering of the words in the reformulation is notas important as the content of the reformulation.We assume this because one key use for reformu-lated questions in the retrieval of candidate answerbearing documents and the presence of key contentterms in a reformulation can help to find answerswhen it is used as a query, regardless of their orderOrdering does still need to be taken into accountby the metric but it should alter the score less thanthe content words in the reformulation.Related to this point, is that we would like refor-mulations that simply append the target onto theend of the original question to score more highlyon average than the original questions on theirown, since this is a default strategy followed bymany systems that clearly helps in many cases.These requirement can help to guide metric selec-tion.3.1 Choosing a metricThere are many different systems which attemptto measure string similarity.
We considered a va-riety of tools like ROUGE (Lin, 2004) and ME-TEOR (Lavie and Agarwal, 2007) but decided theywere unsuitable for this task.
ROUGE and ME-TEOR were developed to compare larger stretchesof text ?
they are usually used to compare para-graphs rather than sentences.
We decided develop-ing our own metric would be simpler than trying toadapt one of these existing tools.To explore candidate similarity measures wecreated a program which would take as input a listof reformulations to be assessed and a list of goldstandard reformulations and compare them to eachother using a selection of different string compar-ison metrics.
To find out which of these metricsbest scored reformulations in the way which weexpected, we created a set of test reformulations tocompare against the gold standard reformulations.Three test data sets were created: one wherethe reformulation was simply the original ques-tion, one where the reformulation included the tar-get appended to the end, and one where the refor-mualation was identical to the gold standard.
Theidea here was that the without target question setshould score less than the with target question setand the identical target question set should have ascore of 1 (the highest possible score).We then had to choose a set of metrics to test andchose to use metrics from the SimMetrics libraryas it is an open source extensible library of stringsimilarity and distance metrics 2.3.2 Assessing MetricsAfter running the three input files against the met-rics we could see that certain metrics gave a scorewhich matched our requirements more closely thanothers.Table 1 shows the metrics used and the meanscores across the data set for the different questionsets.
A description of each of these metrics can befound in the SimMetrics library.From these results we can see that certain met-rics are not appropriate.
SmithWaterman, Jaro andJaroWinkler all do the opposite to what we requirethem to do in that they score a reformulation with-out the target higher than one with the target.
Thiscould be due to over-emphasis on word ordering.These metrics can therefore be discounted.Levenshtein, NeedlemanWunch and QGrams-Distance can also be discounted as the differencebetween With target and Without target is not largeenough.
It would be difficult to measure improve-ments in the system if the difference is this small.MongeElkan can also be discounted as overall itsscores are too large and for this reason it would bedifficult to measure improvements using it.Of the five remaining metrics ?
DiceSimilar-ity, JaccardSimilarity, BlockDistance, Euclidean-Distance and CosineSimilarity ?
we decided thatwe should discount EuclideanDistance as it had thesmallest gap between with target and without tar-get.
We now look at the other four metrics in moredetail3:2http://www.dcs.shef.ac.uk/?sam/simmetrics.html3Refer to Manning and Schu?tze (2001) for more details onthese algorithms.61Metric Without Target With Target IdenticalJaccardSim.
0.798 0.911 1.0DiceSim.
0.872 0.948 1.0CosineSim.
0.878 0.949 1.0BlockDistance 0.869 0.941 1.0EuclideanDistance 0.902 0.950 1.0MongeElkan 0.922 0.993 1.0Levenshtein 0.811 0.795 1.0NeedlemanWunch 0.830 0.839 1.0SmithWaterman 0.915 0.859 1.0QGramsDistance 0.856 0.908 1.0JaroWinkler 0.855 0.831 0.993Jaro 0.644 0.589 0.984Table 1: Mean scores across the data set for each of the different question sets.3.2.1 Block DistanceBlock Distance metric is variously named blockdistance, L1 distance or city block distance.
It is avector-based approach, where q and r are definedin n-dimensional vector space.
The L1or blockdistance is calculated from summing the edge dis-tances.L1(q, r) =?y| q(y) ?
r(y)|This can be described in two dimensions withdiscrete-valued vectors.
When we can picture theset of points within a grid, the distance value issimply the number of edges between points thatmust be traversed to get from q to r within the grid.This is the same problem as getting from cornera to b in a rectilinear street map, hence the name?city-block metric?.3.2.2 Dice SimilarityThis is based on Dice coefficient which is a termbased similarity measure (0-1) whereby the simi-larity measure is defined as twice the number ofterms common to compared entities divided by thetotal number of terms in both.
A coefficient resultof 1 indicates identical vectors while a 0 indicatesorthogonal vectors.Dice Coefficient =2 ?
|S1?
S2||S1| + |S2|3.2.3 Jaccard SimilarityThis is a token based vector space similaritymeasure like the cosine distance.
Jaccard Sim-ilarity uses word sets from the comparison in-stances to evaluate similarity.
The Jaccard mea-sure penalizes a small number of shared entries(as a portion of all non-zero entries) more thanthe Dice coefficient.
Each instance is representedas a Jaccard vector similarity function.
The Jac-card similarity between two vectors X and Y is(X ?
Y )/(|X||Y | ?
(X ?
Y )) where (X ?
Y ) is theinner product of X and Y , and |X| = (X ?
X)1/2,i.e.
the Euclidean norm of X.
This can more easilybe described as (|X ?
Y |)/(|X ?
Y |)3.2.4 Cosine similarityThis is a common vector based similarity mea-sure similar to the Dice Coefficient.
The inputstring is transformed into vector space so that theEuclidean cosine rule can be used to determinesimilarity.
The cosine similarity is often pairedwith other approaches to limit the dimensionalityof the problem.
For instance with simple strings alist of stopwords is used to reduce the dimension-ality of the comparison.
In theory this problem hasas many dimensions as terms exist.cos(q, r) =?yq(y)r(y)?
?yq(y)2?yr(y)23.3 Using bigrams and trigramsAll four of these measures appear to value the con-tent of the strings higher than ordering which iswhat we want our metric to do.
However the scoresare quite large, and as a result we considered refin-ing the metrics to give scores that are not as closeto 1.
To do this we decided to try and increase theimportance of ordering by also taking into accountshared bigrams and trigrams.
As we do not wantordering to be too important in our metric we intro-duced a weighting mechanism into the program to62Metric Without Target With Target ?GapDice 0.872 0.948 +0.076Cosine 0.878 0.949 +0.071Jaccard 0.798 0.911 +0.113Block 0.869 0.941 +0.072Table 2: Results for Unigram weightingMetric Without Target With Target ?GapDice 0.783 0.814 -3.6Cosine 0.789 0.816 -3.5Jaccard 0.698 0.748 -5.5Block 0.782 0.811 -3.5Table 3: U:1, B:1, T:0allow us to used a weighted combination of sharedunigrams, bigrams and trigrams.The results for just unigram weighting is shownin Table 2.We began by testing the metrics by introduc-ing just bigrams to give us an idea of what effectthey would have.
A weight ratio of U:1, B:1, T:0was used (where U:unigram, B:bigram, T:trigram).The results are shown in Table 3.The ?
Gap column is the increase in the differ-ence between Without Target and With Target fromthe first test run which used only unigrams.The introduction of bigrams decreases the gapbetween Without Target and With Target.
It alsolowers the scores which is good as it is then eas-ier to distinguish between perfect reformulationsand reformulations which are close but not perfect.This means that the introduction of bigrams is al-ways going to decrease a system?s ability to dis-tinguish between Without Target and With Target.We had to now find the lowest decrease in this gapwhilst still lowering the score of the with target re-sult.From the results of the bigrams we expected thatthe introduction of trigrams would further decreasethe gap (U : 1, B : 1, T : 1).
The results provedMetric Without Target With Target ?GapDice 0.725 0.735 -6.4Cosine 0.730 0.735 -6.3Jaccard 0.639 0.663 -9.0Block 0.724 0.733 -6.1Table 4: U:1, B:1, T:1Metric Without Target With Target ?GapDice 0.754 0.770 -4.8Cosine 0.759 0.771 -4.9Jaccard 0.664 0.694 -7.4Block 0.753 0.767 -4.6Table 5: U:1, B:2Metric Without Target With Target ?GapDice 0.813 0.859 -2.4Cosine 0.819 0.860 -2.4Jaccard 0.731 0.802 -3.7Block 0.811 0.854 -2.2Table 6: U:2, B:1this and are shown in Table 4.The introduction of trigrams has caused the gapsto significantly drop.
It has also lowered the scorestoo much.
From this evidence we decided trigramsare not appropriate to use to refine these metrics.We now had to try and find the best weightingof unigram to bigram that would lower the WithTarget score from 1.0 whilst still keeping the gapbetween Without Target and With Target high.We would expect that further increasing the bi-gram weighting would further decrease the gapand the With Target score.
The results in Table 5show this to be the case.
However this has de-creased the gap too much.
The next step was tolook at decreasing the weighting of the bigrams.Table 6 shows that the gap has decreased slightlybut the With Target score has decreased by around10% on average.
The Jaccard score for this run isparticularly good as it has a good gap and is nottoo close to 1.0.
The Without Target is also quitelow which is what we want.U : 2, B : 1 is currently the best weightingfound with the best metric being Jaccard.
Fur-ther work in this area could be directed at furthermodifying these weightings using machine learn-ing techniques to refine the weightings using linearregression.4 Our system against the MetricOur current pre-processing system takes a questionand its target and looks to replace pronouns like?he?, ?she?
and certain definite nominals with thetarget and also to replace parts of the target withthe full target (Gaizauskas et al, 2005).
Givenour choice of metric we would hope that this strat-63Figure 2: Graph of Jaccard score distributionegy gets a better score than just adding the targeton the end, as the ordering of the words is alsotaken into account by our pre-processing as it triesto achieve natural reformulations like those of ourgold standard.
We would therefore expect that itachieves at least the same score as adding the targeton the end, which is its default strategy when noco-reference can be determined, though of courseincorrect coreference resolutions will have a neg-ative effect.
One of the aims of creating the goldstandard and a comparison metric was to quicklyidentify whether strategies such as ours are work-ing and if not where not.A subset of the gold standard was preprocessedby our system then compared against the resultsof doing no reformulation and of reformulating bysimply appending the target.Tables 7 and 8 shows how our system did incomparison.
Diff shows the difference betweenWithTarget and Our System.
Table 7 is results forweighting U : 1, B : 0, T : 0, Table 8 is results forU : 2, B : 1, T : 0.Our system does do better than just adding thetarget on the end, and this difference is exaggerated(Table 8) when bigrams are taken into account, asexpected since this weighting increases the met-ric?s sensitivity to recognising our system?s abilityto put the target in the correct place.Mean scores across a data set tell part of thestory, but to gain more insight we need to exam-ine the distribution of scores and then, in order toimprove the system, we need to look at questionswhich have a low score and work out what hasgone wrong.
Figure 2 shows the distribution ofJaccard scores across the test data set.
Looking atthe scores from the data set using the U:2,B:1,T:0weighting we find that the minimum Jaccard scorewas 0.44 and was for the following example:Metric ScoreDice 0.574Cosine 0.578Jaccard 0.441Block 0.574Table 9: Finding Bad ReformulationsTarget: ?Hindenburg disaster?
; Question:?How many of them were killed?
; Our System:?How many of Hindenburg disaster were killed?
;Gold Standard: ?How many people were killedduring the Hindenburg disaster?.The results of comparing our system with thegold standard for this question for all four metricsare shown in Table 9.The problem here is that our system has wronglyreplaced the term ?them?
with the target when infact its antecedent was in the previous questionin the series How many people were on board?.Once again the low score has helped us to quicklyidentify a problem: the system is only interpret-ing pronouns as references to the target, which isclearly insufficient.
Furthermore should the pre-processing system be altered to address a problemlike this the gold system and scoring software canbe used for regression testing to ensure no previ-ously correct reformulations have been lost.Another example of a poor scoring reformula-tion is:Target: ?Hindenburg disaster?
; Question:?What type of craft was the Hindenburg?
; OurSystem: ?What type of craft was the Hindenburgdisaster?
; Gold Standard: ?What type of craft wasthe Hindenburg?.For this example Jaccard gave our system refor-mulation a score of 0.61.
The problem here is oursystem blindly expanded a substring of the targetappearing in the question to the full target withoutrecognizing that in this case the substring is not anabbreviated reference to the target (an event) but toan entity that figured in the event.5 Conclusions and Future WorkIn this paper we have presented a Gold Standardfor question reformulation and an associated set ofguidelines which can be used to reformulate otherquestions in a similar fashion.
We then evaluatedmetrics which can be used to assess the effective-ness of the reformulations and validated the wholeapproach by showing how it could be used to help64Metric Without Target With Target Our System DiffDice 0.776 0.901 0.931 +3.1Cosine 0.786 0.904 0.936 +3.1Jaccard 0.657 0.834 0.890 +5.5Block 0.772 0.888 0.920 +4.2Table 7: How our system compared, U:1,B:0,T:0Metric Without Target With Target Our System DiffDice 0.702 0.819 0.889 +8.7Cosine 0.742 0.822 0.893 +9.2Jaccard 0.616 0.738 0.839 +12.3Block 0.732 0.812 0.884 +9.1Table 8: How our system compared, U:2,B:1,T:0improve the question pre-processing component ofa QA system.Further work will aim to expand the Gold Stan-dard to at least 1000 questions, refining the guide-lines as required.
The eventual goal is to incor-porate the approach into an evaluation tool suchthat a developer would have a convenient wayof evaluating any question reformulation strategyagainst a large gold standard.
Of course one alsoneeds to develop methods for observing and mea-suring the effect of question reformulation withinquestion pre-processing upon the performance ofdownstream components in the QA system, suchas document retrieval.ReferencesBurger, J., C. Cardie, V. Chaudhri, R. Gaizauskas,S.
Harabagiu, D. Israel, C. Jacquemin, C-Y.Lin, S. Maiorano, G. Miller, D. Moldovan,B.
Ogden, J. Prager, E. Riloff, A. Singhal,R.
Shrihari, T. Strzalkowski, E. Voorhees, andR.
Weischedel.
2002.
Issues, tasks and pro-gram structures to roadmap research in question& answering (q&a).
Technical report.
www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper v2.doc.Gaizauskas, Robert, Mark A. Greenwood, Mark Hep-ple, Henk Harkemaa, Horacio Saggion, and AtheeshSanka.
2005.
The University of Sheffield?s TREC2005 Q&A Experiments.
In Proceedings of the 14thText REtrieval Conference.Lavie, Alon and Abhaya Agarwal.
2007.
METEOR:An automatic metric for MT evaluation with highlevels of correlation with human judgments.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 228?231, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Lin, Chin-Yew.
2004.
Rouge: A package forautomatic evaluation of summaries.
In Marie-Francine Moens, Stan Szpakowicz, editor, Text Sum-marization Branches Out: Proceedings of the ACL-04 Workshop, pages 74?81, Barcelona, Spain, July.Association for Computational Linguistics.Manning, Christopher D. and Hinrich Schu?tze.
2001.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.Voorhees, E. 2005.
Overview of the TREC 2004question answering track.
In Proceedings of theThirteenth Text Retrieval Conference (TREC 2004).NIST Special Publication 500-261.65
