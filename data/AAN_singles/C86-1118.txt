TOPIC Essentials*Udo Hahn / Ulrich ReimerUniversitaet KonstanzInformationswissenschaftPostfaeh 5560D-7750 Konstanz, F.R.G.AbstractAn overview of TOPIC is provided, a knowledge-basedtext information system for the analysis of German-language texts.
TOPIC supplies text condensates(summaries) on variable degrees of generality andmakes available facts acquired from the texts.
Thepresentation focuses on the major methodologicalprinciples underlying the design of TOPIC: a framerepresentation model that incorporates various integ-rity constraints, text parsing with focus on textcohesion and text coherence properties of expositorytexts, a lexlcally distributed semantic text grammarin the format of word experts, a model of partialtext parsing, and text graphs as appropriate repre-sentation structures for text condensates.I.
IntroductionThis paper provides an overview of TOPIC, a textunderstanding and text condensation system whichanalyzes German-language texts: complete magazinearticles in tbe domain of information technologyproducts.
TOPIC performs the following functions:Text summarization (abstracting)TOPIC produces a graph representation of the mostrelevant topics dealt with in a text.
This summaryis derived from text representation structures andits level of generality varies from quite genericdescriptions (similar to a system of index terms)to rather detailed information concerning facts,newly acquired concepts and their properties.
Dueto the flexibility inherent to this cascadedapproach to text summarization (cf.
KUHLEN 84) werefer to it as text condensation.
This is opposedto invariant forms of text summarization based onsummary schemata (DeJONG 79, TAIT 82) or struc-tural features of the text representations (TAYLOR74, LEHNERT 81), and dynamic abstracting proce-dures which depend on a priori specifications ofappropriate parameters (FUM et el.
82) or rulesets for importance evaluation (FUM et el.
85)prior to text analysis.
* Extraction of facts / acquisition of new conceptsKnowledge extraction resulting from text analysisnot only leads TOPIC to the assignment of specificproperties to concepts already known to the sys-tem, but also comprises the acquisition of newconcepts and corresponding properties.Linking thematic descriptions with text passagesTOPIC's analytic devices are by no means exhaus-tive to capture all the knowledge encoded in atext.
Thus, the text representation structuresprovided might be incomplete, llowever, the themat-* The development of the TOPIC system is supported byBMFT/GID under contract 1020016 0.
We want to thankD.
Soergel for his contributions to this paper.ic descriptions generated are linked to the corre-sponding text passages so that querying a textknowledge base may end up in the retrieval ofrelevant fragments of the original text (cf.similar approaches in LOEF 80, HOBBS et el.
82).To perform these functions, the design of TOPIC isbased on the following methodological principles:* a method for making strategic decisions to controlthe depth of text understanding according to thefunctional level of system performance desired* a knowledge representation model whose expressivepower primarily comes from various integrity con-straints which control\[ the validity of the knowl-edge representation structures during text analy-sis* a parsing model adapted to the specific construc-tive requirements of expository prose (local textcohesion and global text coherence phenomena)* a text condensation model based on empirical well-formedness conditions on texts (text grammaticalmacro rules) and criteria derived from the knowl-edge representation model (complex operations)2.
Methodological Principles of Text Analysis Under-lying the TOPIC Text Condensation SystemPartial Text ParsingThe current version of TOPIC acts as a shallow under-stander of the original text (cf.
the approach to"integrated parsing" in SCHANK et al 80).
It concen-trates on the thematic foci of texts and significantfacts related to them and thus establishes an indica-tive level of text understanding.
Partial parsing isrealized by restricting the text analysis to taxo-nomic knowledge representation structures and byproviding only those limited amounts of linguisticspecifications which are needed for a text parserwitb respect to a taxonomic representation level.Primarily, the concepts which are available in theknowledge base correspond to nouns or nominal groupsand their attributes (adjectives, numerical values).A Frame Representation Model that IncorporatesVarious Integrity ConstraintsThe world knowledge underlying text analysis isrepresented by means of a frame representation model\[REIMER/HAIIN 85\].
The large degree of schematizationinherent to frame representations provides knowledgeof the immediate semantic context of a concept (lexi-cal cohesion).
Additionally supplied integrity con-straints formally restrict the execution of variousoperations (e.g.
property assignment to supportknowledge extraction from a text) in order to keepthe knowledge base valid.Text Parsing with Focus on Text Cohesion and TextCoherence PatternsText linguists seriously argue that texts constitutean object to be modeled differently from sentences inisolation.
This is due to the occurrence of phenomenawhich establish textuallty above the sentence level.A common distinction is made between local text497cohesion for immediate connectivity among adjacenttext  items (due to anaphora, lexical cohesion,co-ordination, etc.
; see HALLIDAY/HASAN 76) and thethematic organization in terms of text coherencewhich primarily concerns the global structuring oftexts according to pragmatic well-formedness con-straints.
Instances of global text structuringthrough text coherence phenomena are given by regularpatterns of thematic progression in a text \[DANES74\], or by various additional functional coherencerelations, such as contrast, generalization, explana-tion, compatibility \[REICHMAN 78, HOBBS 83\].
Dis-regarding textual cohesion and coherence structureswil l  inevitably result in invalid (text cohesion) andunderstructured (text coherence) text knowledgecomparable to mere sentence level accumulations ofknowledge structures which completely lack indicatorsof text structure.
Therefore, there should be noquestion that specially tuned text grammars areneeded.
Unfortunately, the overwhelming majority ofgrammar/parser specifications currently available isunable to provide broad coverage of textual phenomenaon the level of text cohesion and coherence, so thatthe format of text grammars and corresponding parsingdevices is still far from being settled.A Lexically Distributed Semantic Text GrammarSince major linguistic processes provide textualcohesion by immediate reference to conceptual struc-tures of the world knowledge, and since many of thetext coherence relations can be attributed to thesesemantic sources, a semantic approach to text parsinghas been adopted which primarily incorporates theconceptual constraints inherent to the domain ofdiscourse as well as structural properties of thetext class considered (for an opposite view of textparsing, primarily based on syntactic considerations,ef.
POLANYI/SCHA 84).
Thus, the result of a textparse are knowledge structures in terms of the framerepresentation model, i.e.
valid extensions of thesemantic representation of the applieational domainin terms of text-specific knowledge.Text parsing, although crucially depending on seman-tic knowledge~ demands that additional knowledgesources (focus indications, parsing memory, etc.)
beaccessible without delay.
This can best be achievedby highly modularized grammatical processes (actors)which take over/give up control and communicate witheach other and with the knowledge sources mentionedabove.
Since the semantic foundation of text under-standing is most evidently reflected by the interac-tion of the senses of the various lexical items thatmake up a text, these modular elements themselvesprovide the most natural point of departure topropose a lexical distribution of grammaticalknowledge \[HAHN 86\] when deciding on the linguisticorganization of a semantic text grammar (ALTERMAN 85argues in a similar vein).Text Graphs as Representation Structures for TextCondensatesKnowledge representation structures built up duringtext parsing are submitted to a condensation processwhich transforms them into a condensate repre--sentation on different levels of thematic specializa-tion or explicitness.
The structure resulting fromcorresponding complex operations is a text graph (itsvisualized form resembles an idea first introduced bySTRONG 74).
It is a hyper graph which is composed of* leaf nodes each of which contains a semantic netthat indicates the topic description of a themati-cally coherent text passage498* the text passages that correspond to these topicdescriptions* the higher-order nodes which comprise generalizedtopic descriptionsFrom this condensate representation of a text accesscan also be provided to the factual knowledge ac-quired during text analysis.
TOPIC does not includenatural language text generation devices since theretrieval interface to TOPIC, TOPOGRAPHIC \[HAMMWOEH-NER/THIEL 84\], is exclusively based on an interac-tive-graphical access mode.3.
An Outline of the Text ModelDespite the apparent diversity of linguistic phenome-na occurring in expository texts, a large degree ofthe corresponding variety can be attributed to twobasic processes (cf.
HALLIDAY/HASAN 76): variousforms of anaphora (and cataphora), and processesincorporating lexical cohesion.
Both serve as basictext cohesion preserving mechanisms on the locallevel of textuality.
Their repeated applicationyields global text coherence patterns which eitherfollow the principle of constant theme, linearthematization of rhemes, or derived themes (see DANES74).
In Fig 1 we give a fairly abstracted presenta-tion of the~e coherence patterns which should beconsidered together with the linguistic examplesprovided in Fig 2 and their graphical reinterpreta-tion in Fig 3.
The notions of frames, slots, and slotentries occurring in Fig 1 correspond to concepts ofthe world knowledge, their property domains, andassociated properties, which may be frames again.I Constant Themeframe~< slot 2 >~< slot n >II Linear Thematization of Rhemesframeframej\]> ~ = frmE~ 1 } >III Derived ThemesFramesu pfr~l~subl .
.
.
framest~k .
.
.
framesul~ n~-~< slOtll > k_~< slotkl > ~---< s\]otml ><< slotl2 > <~< slotk2 > k--~< s lo~ >k--~< slOtlp > %---< slO~q > k-.~< slOtmr >Fig I: Basic Patterns of Thematic ProgressionThe interpretation of coherence patterns as given inFig_l refers to two kinds of knowledge structures:* concept specialization corresponds to the phenomenaof anaphora* aggregation of slots to frames corresponds to thephenomena of lexical cohesionThis tight coupling of text licking processes andrepresentation structures of the underlying worldknowledge strongly supports the hypothesis that textunderstanding is basically a semantic process which,as a consequence, requires a semantic text parser.A linguistic illustration of the coherence patternsintroduced above is given by the following textpassages.
For convenience, the examples in this paperare in English, although the TOPIC system deals withGerman textual input only.I Constant ThemeThe PC2000 is equipped with a 8086 cpu asopposed to the 8088 of the previous model.
Thestandard amount of dynamic RAM is 256K bytes.One of the two RS-232C ports also serves as ahigher-speed RS-422 port.II Linear Thematization of RhemesA floppy disk drive by StorComp is availablewhich holds around 710K bytes.
Also availableby StorComp is a hard disk drive which provides20M bytes of mass storage.Ill Derived ThemesCompared to the FS-190 by DP-Products whichcomes with Concurrent CP/M the PC2000 runs UNIXiiust like the new UNPC by PCP Inc.Fig 2: Linguistic Examples of the Basic Patterns ofThematic ProgressionFig 3 shows an interpretation of the text passages ofFig 2 in terms of thematic progression patterns.I Constant Theme (PC2000)pc2~~,~< cpu >8086~< main memory >RAM~-~< size >256K bytesRS-232CRS-422II Linear Thematization of Rhemes (disk drivesfrom StorComp)PC2~< mass storage >~ floppy disk drive~< size >~ 71~ bytes~ n~nufacturer >StorCc~p~< product >~hard  disk drivek~ size >20M bytesIll Derived Themes (personal computers)persollal co.puterFS-19S PC2OO~ \[~qPC~-~< manufacturer > ~--~< operating system1 > ~-+< n~lufactt~er >DP-Products UNIX FCP Inc.~-~ < operating system > ~'~< operating system >Concurrent CP/M \[RClXFig 3: Interpretation of the Text Passages of Fig 2in Terms of Thematic Progression Patterns4.
The Process of Text ParsingTOPIC is a knowledge-based system with focus on se-mantic parsing.
Accordingly, incoming text is direct-ly mapped onto the frame representation structures ofthe system's predefined world knowledge withoutconsidering in-depth intermediate linguistic descrip-tions.
Basically, these mappings perfolT~L continuousactivations of frames and slots in order to provideoperational indicators for text summarization.Together with slot filling procedures they build upthe thematic structure of the text under analysis inthe system's world knowledge base.
To account forlinguistic phenomena these concept activation andproperty assignment processes are controlled by a setof decision procedures which test for certain struc-tural patterns in the world knowledge and the text tooccur.
Consequently, TOPIC's text parser consists oftwo main components: the world knowledge whichprovides the means of correctly associating conceptswith each other (see sec.4.1) and the decision proce-dures (word experts) which util ize this foreknowledgeto relate the concepts that are actually referred toby lexical items in a text, thus determining thepatterns of thematic progression (see see.4.2).4.1 Representation of World Knowledge by a FrameRepresentation ModelKnowledge of the underlying domain of discourse isprovided through a frame representation model\[REIMER/HAHN 85\] which supports relationally con-nected frames.
A frame can be considered as providinghighly stereotyped and pre-structured pieces of knowl-edge about the corresponding concept of the world.
Itdescribes the semantic context of a concept by asso-ciating slots to it which either refer to semanti-cally closely related frames or which simply describebasic properties.
A slot may roughly he considered asa property domain while actual properties of a frameare represented by entries in these slots (Fig 4).
Anentry may only be assigned to a slot if it isdeclared as being a permitted entry (see below).PC2~ frm~m< c~xl > < slOt I >8ZS6 { permitted entrY\].l, ''' )< nk~in Z~:~\[IO\]~ > slotentrYllRAM-I< size >256K bytes slotentrYlr<port>RS~232C, ~422< mass storaqe > < slot n hnrd disk (Irivc-i ( permitted entrYnl ' ... }size > slotentrYnl2~M bytesflopl~ ?
disk drive-isize > slotentrYns71ZK bytesFig 4: Examples of Frames, Slots and Slot EntriesTwo kinds of frames are distinguished.
A prototypeframe acts as a representative of a concept classconsisting of instance frames which all have the sameslots but differ from the prototype in that they arefurther characterized by slot entries.
Thus, instanceframes stand for individual concepts of a domain ofdiscourse.
This point may be illustrated by a micro-processor frame which represents as a prototype theset of all microprocessor instances (Fig 5).Prototype frame (concept class):micropr~es ~r< wor~ leng~ >I 4 bit, S bit, 16 bit, 32 bit \]( \ ]~nu~churer  >Associated instance frames (individual concepts):zs@ ~szo0S bit 16 bit< reanufact~er > < \]~nu~urer >zil~j 5bto~laFig 5: A Prototype and Associated Instance FramesFrames are connected with each other by semanticrelations (cf.
Fig 6).
Concept specialization betweenprototypes (is-a r~lation) is of fundamental impor~tance to anaphora resolution.
Concept specializationbetween a prototype and its instances (instance-of)499requires the instances to have the same slots as theprototype with the same set of permitted entries,resp.
This property supports learning of new con-cepts from the text (i.e.
incorporating new data inthe knowledge base).
When a new concept occurs in thetext and it is possible to determine its conceptelass the structural description of the new conceptis taken from the prototype that stands for theconcept class.
Indicators of what concept class a newconcept belongs to are e.g.
given by composite nouns,which are particularly characteristic of Germanlanguage (8-Bit-Cpu, Sirius-Computer), attributions(serial interface, monochromatic display), orspecific noun phrases (laser printer LA-9).The semantic relation part-of isaggregation which expresses asemantic closeness.personal e~puter< cpu>< mass storage >instance-of part-ofPC-XZX<cpu>8086< main memory >RAM-I<port>< mass storage >memory< size >- -RAMis-a ~ < sizeI < CyCle tir~e ) instance-of RAM-Ipart-of ( size >256K bytescycle time >Fig 6: Semantic Relations among Framesa special kind ofparticularly tight8~86< word length >16 bit< manufacturer >IntelWhile the learning of new concepts is supported bythe distinction of prototypes and instances, theacquisition of new facts from the text is possible byutil izing knowledge about the permitted entries of aslot.
Two cases can be distinguished which correspondto two slot types.
Non-terminal slots are slots whosename is identical to the name of a frame in the knowl-edge base.
Permitted entries for them are definedimplicitly and are given by all those frames whichare specializations of the frame whose name equalsthe slot name (el.
the slot "operating system" inFig_7).
On the other hand, entries of the complemen-tary class of terminal slots must he specifiedexplicitly (cf.
the slot "word length" in F ig7) .o~rating systemo~tar / \< operating system ) single-user multi-user{ single--user system, system systemmulti-user system,CPIM ... UNIX VMS ...micrc~processor< word length >\[ 4bit, 8bit, 16bit, 32bit \]Fig 7: Permitted Entries for (Non-)Termlnal SlotsFurther devices for controlling slot filling aregiven by the construct of singleton slots which mayhold at most one entry (e.g.
the slots "epu" and"size" in Fig__4).
Singleton slots are of use whenseveral permitted entries for a slot occur atadjacent text positions.
0nly if that slot is asingleton slot, the filling is constrained to one of500those candidates; linguistic knowledge has to accountfor the selection of the appropriate one.
Moreover,such a situation is interpreted as an indication ofcomparison (see F ig2 / l  and the parsing effectsoccurring with respect to "epu" and the candidateentries "8086" and "8088" in Fig_10).Control of slot filling is also supported by aninferential construct called cross reference filling.When two frames, frame-i and frame-2 (F ig8) ,  referto each other in such a way that each has a non-ter-minal slot for which the other frame is a permittedentry, then assigning frame-I to the appropriate slotof frame-2 automatically results in assigning frame-2to the appropriate slot of frame-l. Now, if thesecond slot assignment is not permitted and thereforeblocked, the primary assignment is blocked, too.
Thefollowing sentence gives an example (Fig 8): "Com-pared to the FS-190 by DP-Products the PC2000 runsUNIX".
The concept "PC2000" is a permitted entry ofthe product slot of the manufacturer "DP-Products'.Its assignment would trigger the assignment of"DP-Products" in the manufacturer slot of "PC2000"which is a singleton slot and already occupied.Therefore no slot filling at all is performed.frame-i ~ frame-2< slot-2 > < slot-i >{ frame-2 .... \] { frame-i .... \]DP-Produets PC2Z~O< products > < manufacturer >{ PC2~ ... \] { PCP Inc., Dp-Products, ... }PeP Inc.Fig 8: Cross Reference FillingThe structural features of the frame representationmodel are extended by activation weights attached toframes and slots.
They serve the purpose of indicat-ing the frequency of reference to the correspondingconcepts in a text and are of significant importancefor the summarization procedures.Currently, TOPIC's frame knowledge base comprisesabout 120 frames, an average of 6 slots per frame.4.2 A Generalized Word Expert Model of LexicallyDistributed Text ParsingCharacterizations of what texts are about are carriedpredominantly in domain-specific keywords as desig-nators of contents (of.
SMETACEK/KOENIGOVA 77 for thetask domain of abstracting) - in linguistic terminol-ogy: nominals or nominal groups.
Accordingly,TOPIC's parsing system is essentially based on a nounphrase grammar adapted to the requirements of textphenomena.
Its shallow text parsing performance canbe attributed to the exhaustive recognition of allrelevant keywords and the semantic and thematicrelationships holding among them.
This is sufficientfor the provision of indicative text condensates.Accordingly, word experts \[SMALL/RIEGER 82\] have beendesigned which reflect the specific role of nominalsin the process of making up connected text.
Thecurrent section illustrates this idea through adiscussion of a word expert for lexieal cohesion (fora more detailed account ef.
}~HN 86).
Together withvarious forms of anaphora (not considered here,although we refer to the effects of a correspondingexpert in F ig l0  by NA) it provides for a continuouscohesion stream and a corresponding thematic develop-ment in (expository) texts.
Exceptions to this basicrule are due to special linguistic markers in termsof quantifiers, connectors, etc.
As a consequence,supplementary word experts have to be provided whichreflect the influence these markers have on the basictext cohesion and text coherence processes: expertsapplying to quantifiers and comparative expressionstypically block simple text cohesion processes (foran example cf.
Fig_10), experts for conjunctionstrigger them, and experts referring to negationparticles provide appropriately modified assignmentsof properties to frames.This kind of selective parsing is based on strategicconsiderations which, however, do not affect thelinguistic generality of the approach at all.
On thecontrary, due to the high degree of modularizationinherent to word expert specifications a word expertgrammar can easily be extended to incrementally covermore and more linguistic phenomena.
Moreover, thepartial specifications of grammatical knowledge inthe format of word experts lead to a highly robustparsing system, while full-fledged text grammarsaccounting for the whole range of propositional andpragmatic implications of a comprehensive understand-ing of texts are simply not available (not even insublanguage domains).
In other words, current textanalysis systems must cope with linguistic descrip-tions that will reveal specification lags in thecourse of a text analysis if ~realistic texts" \[RIES-BECK 82\] are being processed.
Therefore, the textparser carries the burden of recovering even in casesof severe nnder-speciflcation of lexical, grammati-cal, and pragmatic knowledge.
Unlike question-answering systems, this problem cannot beside-stepped by asking a user to rephrase unparsableinput, since the input to text understanding systemsis entirely fixed.
Distributing knowledge overvarious interacting knowledge sources allows easyrecovery mechanisms since the agents which areexecutable take over the initiative while thoselacking of appropriate information simply shut down.Summing up, each of the word expert specificationssupplied (those for nominals, quantifiers, conjunc-tions, etc.)
is not bound to a particular lexicalitem and its idiosyncrasies, but reflects function-ally regular linguistic processes (anaphora, lexicalcohesion, coordination, etc.).
Accordingly, a rela-tively small number of general grammatical descrip-tions encapsulated in highly modularized communitiesof agents form the declarative base of lexicallydistributed text parsing.By word experts (consider the word expert prototypeprovided below) we refer to a declarative organiza-tion of linguistic knowledge in terms of a decisionnet whose root is assigned the name of a lexicalclass or a specific word, Appropriate occurrences oflexical items in the text prompt the execution ofcorresponding word experts.
Non-terminal nodes of aword expert's decision net are constructed of booleanexpressions of query predicates or messages while itsterminal nodes are composed of readings.
With respectto non-terminal nodes word experts- query the frame knowledge base,e.g, testing for se-mantic relations (e.g.
is-a, instance-of) to hold,for the existence and activation weight of conceptsin the knowledge base, or for integrity criteriathat restrict the assignment of slot entries- investigate the current state of text analysis~e.g.
the types of operations already performed inthe knowledge base (activation, slot entry assign-ment, creation of new concepts~ etc.
)- consider the immediate textual environment, e.g.testing co-occurrences of lexical items underqualified conditions~ e.g.
within sentence or nounphrase boundaries- have message sending facilities to force directcommunication among the running experts for block-ing, canceling, or re--starting companion expertsAccording to the path actually taken in the decisionnet of a word expert, readings are worked out whicheither demand various actions to be performed on theknowledge base in order to keep it valid in terms oftext cohesion (incrementing/decrementlng activationweights of concepts, assignment of slot entries,creation of new frames as specializations of alreadyexisting ones, etc.
), or which indicate functionalcoherence relations (e.g.
contrast, classificatoryrelations) and demand overlaying the knowledge baseby the corresponding textual macro structure.Apparently, the basic constructs of the word expertmodel (query predicates, messages, and readings) donot refer to any particular domain of discourse.
Thisguarantees a high degree of transportability of acorresponding word expert grammar.The word expert collection currently comprises about15 word expert prototypes, i.e.
word experts forlexical classes, like frames, quantifiers, negationparticles, etc.
Word expert modules encapsulatingknowledge common to different word experts amount to20 items.
The word expert system is implemented in Cand running under UNIX~ Grammatical knowledge isrepresented using a high-level word expert specifica-tion language, and it is inserted and modified usingan interactive graphical\[ word expert editor.These principles will be illustrated by consideringan informal specification of a word expert (a moreformal treatment gives I~HN/REIMER 85) which accountsfor lexical cohesion that is due to relations betweena concept and its corresponding properties.Fig I0 shows a sample parse of text I (F ig2)  whichgives an impression of the way text parsing is real-ized by word experts that incorporate the linguisticphenomena just mentioned.With respect to text summarization (cf.
HAHN/REIMER84) it is an important point to determine the properextension of the world knowledge actually consideredin a text as well as its conceptual foci.
This isachieved by incrementing activation weightsassociated to frames and slots whenever they arereferred to in the text (this default activationprocess is denoted DA in F ig l0 ) .
In order to guaran-tee valid activation values their assignment must beindependent from linguistic interferences.
As anexample for a process that causes illegal activationvalues consider the case of nominal anaphora whichholds for \[17\] in Fig I0 (the associated word expertNA is not considered here, cf.
HAHN 86).Recognizing lexleal cohesion phenomena contributes toassociating concepts with each other in terms ofaggregation.
The word expert for lexlcal cohesion, anextremely simplified version of which is given inFig 9, tests if a frame refers to a slot or to anactual or permitted entry of a frame preceding in thetext.
In the case of a slot or of an actual entry theactivation weight of the slot (entry) is incremented;in the case of a permitted entry the appropriate slotfill ing is performed~ thus acquiring new knowledgefrom the text.
Examples of lexlcal cohesion processes501are given by positions \[02/07\], \[07.1/24\], \[24/26\],\[26.2/32\], and \[32.1/38\] in F ig l0 .i ........................... Im: itm~ of the m~ledge base <kb i t~  ~ r s  |in t I~ i~ .
\ ] ia te  l~ft ~tex~ of ( f r~>J aE~ (kl~ item> de~tes all active f r~\] .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
b it~n> I .
.
.
.
.
.
.
.
.
.
.
.
* incT~nt  weight of Blot ( f r~> in fr ,~ <kh its,#.
*I <Erm~,> ?le~te.
an acts1 slot valm~ / T \[* i lmr~t  weight of slot value ( f r~> ~ Ffor slot <slot> in f r~ (kb its).I ........................... \[ of slot ~slot> in f r~ (~ i t~>* assign <f r~> to slot ~slot> of f r~ <~) $t~n~, ?
I~ r~d i~ 3, *Fig 9: Word Expert for Lexical Cohesion (= LC)Fig I0 shows a sample parse with respect to the textI gTven in Fig 2.
It includes all actions taken withrespect to nominal anaphora resolution (NA) andlexical cohesion recognition (LC).\[02\] PC2~Z DA: 'PC20~O': O---> 1\[ST\] 8~6 DA: '8086': 0---> i\[07.1\] LC: PC2000 < cpu: 8~6 >\[09/10\] opposed to < start blocking of ?C >\[13\] 8~88 DA: '8~8S'.. 0---> 1\[13.1\] LC: PC-ZI ?
q~u : 8088 >\[17\] r0odel DAz 'model': 0---> 1\[17.1\] NA: 'raodel': 1 ---> 0, 'PC-~I': 1 ---> 2< stop blocking of LC >\[24\] RAM DA: 'RAM': 0 ---> 1\[24.1\] LCs PC2~ ?
main n ~  : ~ >\[26\] 256K bytes\[26.1\] IJC: RAM-OI < size : 256K bytes >\[26.2\] LCI PC2~@~ ?
main raemory : RAM~OI ?\[32\] P~-232C DAz '\[~S-232C'."
0 --> 1\[32.1\] I~ PC2~ < port t RS-232C >\[38\] RS-422 DAz 'RS-422'I O---> 1\[38.1\] LC: PC2~ ?
port : RS-232C, RS-422 >Fig i0: Sample Parse of Text Fragment I in Fig__2Applying the Experts LC and NASome comments seem worthy:i) \[13.1\]: The frame "8088" is not considered as anentry of the slot <cpu> of "PC2OOO" since it al-ready has been assigned an entry and it is a sin-gleton slot (cf.
sec.4.1).
Instead, a new instanceof a personal computer is created ('PC-01") towhich "8088" is assigned as a slot entry2) \[24.1\]: "RAM" does not refer to "PC-01" as mightbe expected from the specification of LC because acomparative expression (\[09/10\]) occurs in thetext.
This blocks the execution of the LC expertwith respect to the noun phrase occurringimmediately after that expression.3) \[26.1/26.2\]: The instance created ('RAM-OI')describes the main memory of the "PC2000".
There-fore it is assigned as an entry to "PC2000" andreadjusts the previous assignment of "RAM'.502Our constructive approach to text cohesion andcoherence provides a great amount of flexibility,since the identification of variable patterns ofthematic organization of topics is solely based ongeneralized, dynamically re-combinable cohesiondevices yielding fairly regular coherence patterns.This is in contrast to the analytic approach of storygrammars \[RUMELHART 75\] which depend completely onpre-defined global text structures and thus can onlyaccount for fairly idealized texts in static domains.5.
Text CondensationDuring the process of text parsing, activation pat-terns and patterns of property assignment (slotfilling) are continuously evolving in the knowledgebase, which consequently exhibits an increasingdegree of connectivity between the frames involved(text cohesion).
If the analysis of a whole textwould proceed this way, we would finally get anamorphous mass of activation and slot filling data inthe knowledge base without any structural organiza-tion, although the original text does not lackappropriate organizational indicators.
In order toavoid this deficiency, it is essential in textparsing to recognize topic shifts and breaks in textsto delimit the extension of topics exactly and torelate different topics properly.
For this purposeevery paragraph boundary triggers a condensationprocess which determines the topic of the latest para-graph (in the sublanguage domain we are working intopic shifts occur predominantly at paragraph bound-aries).
If its topic description matches with thetopic description of the preceding paragraph(s), bothdescriptions are merged; thus they form a text pas-sage of a coherent thematic characterization, calleda text constituent.
If the topic descriptions do notmatch a new text constituent is created.
After thetopic of a paragraph has been determined, the activa-tion weights in the world knowledge are reset, exceptof a residual activation of the frame(s) in focus.This way the thematic characterization of a paragraphcan be exactly determined without any interferencewith knowledge structures that result from parsingpreceding paragraphs.The next section presents the main ideas underlyingthe process of determining the thematic charac-terization of a text passage.
Sec.5.2 concludes bygiving a very concise discussion of the concept of atext graph which is the representational device fortext condensates in the TOPIC system.5.1 Determination of Text ConstituentsThe condensation process (for details cf.
HAHN/REIMER84) completely depends on the knowledge structuresgenerated in the course of text analysis.
As outlinedabove, this text knowledge consists of frame struc-tures which have been extended by an activationcounter associated to eacb concept (frame, slot, orslot entry) to indicate the frequency of reference toa concept in the text under analysis.
These activa-tion weights as well as distribution patterns of slotfilling among frames together with connectivitypatterns of frames via semantic relations provide themajor estimation parameters for computing text con-stituents (connectivity approaches to text summariza-tion are also described in TAYLOR 74, LEHNERT 81).These indicators are evaluated in a first condensa-tion step where the significantly salient conceptsare determined.
We distinguish between dominantframes, dominant slots and dominant clusters offrames, the latter being represented by the commonsuperordinate frame (for a detailed discussion seeH~{N/REIMER 84).
The determination of dominant con-cepts can be viewed as a complex query operation on aframe knowledge base.
In a subsequent step thedominant concepts are related to each other withrespect to concept specialization as well as theframe/slot relationship.
The topic of a text passageis thus represented by a semantic net al of whoseelements are given by the dominant concepts (cf.
thenodes of the text graph in Fig II).5.2 The Text GraphThe text graph (Fig ii) is a hierarchical hyper graphwhose leaf nodes are the text constituents (as givenabove) and whose higher-order nodes represent general-izations of their topics.
Similar to the distinctionof micro and macro propositions \[CORREIRA 80\] itsnodes are associated by different kinds of relation-ships which are based on the frame representationmodel (is-a, instance-of, is-slot, identity) or whichare constituted by the coherence relations (e.g.contrast).. ?
.
.
.
.
?
? "
: . "
,"  "~ ~  . "
?
.
. "
.
L~nufaotu~er  \]J/I\]c~xM\ ]entP~q--232C RS4228086 8088contrast%~ ?PC200~ ----\]~kanu facturer / \DI~Produets I~C~ Inc.IIcexlp~lentStorC~oI{productd isk O/'ivef lOppy disk ha;d diskdr ive driveidentity: .
.
.
.
is-a: .
.
.
.
.
i ns tance-o f : - - - - -FS-190 I~72~ZS U~C~ rat Jng system \I)hrfK Conc~irrent ~/Mis-slot:Fig Ii: Text Graph for Text Fragments I-Iii (F ig2)6.
ConclusionsA comprehensive description of the text condensationsystem TOPIC has been provided which serves for theconceptual analysis of textual input of a knowledge-based full-text information system.
The followingissues are most characteristic of it:- a frame representation model which incorporatesvarious integrity constraints- a text grammar with focus on text cohesion and textcoherence properties of expository texts- a lexically distributed semantic text grammar inthe format of word experts- partial text parsing based on a noun phrase wordexpert parser and a taxonomic knowledge repre-sentation- text graphs as representation structures of textcondensates which provide different layers ofinformational specifityReferencesAlterman, R~: A Dictionary Based on Concept Coher~ence.
In: Art.
Intell.
25.
1985, ppo153-186.Correlra, A.: Computing Story Trees.
In: Amer.
J. Com-put ing .
6.
1980, pp.135-149.Danes, F.: Functional Sentence Perspective and theOrganization of the Text.
In: Danes (ed): Papers onFunctional Sentence Perspective.
Academia, 1974,pp.i06-128.DeJong, G.: Skimming Stories in Real Time: an Exper-iment in Integrated Understanding.
Yale Univ, 1979.Fu_mm, D. et al: Forward and Backward Reasoning in Au-tomatic Abstracting.
In: Proc.
COLING 82, pp.83-88.Fun~, D. et al: Evaluating Importance: a Step towardsText Summarization.
In: Pron.
IJCAI-85, pp.840-844.Hahn, U.: On Lexieally Distributed Text Parsing: AComputational Model for the Analysis of Textualityon the Level of Text Cohesion and Text Coherence.In: Kiefer (ed): Linking in Text.
Reidel, 1986.
}lahn, U.; U. Reimer: Computing Text Constituency: AnAlgorithmic Approach to the Generation of TextGraphs.
In: Rijsbergea (ed): Research and Develop-ment in Information Retrieval.
Cambridge U?P.,1984, pp.343-368.Hahn, U.; U. Reimer: The TOPIC Project: Text-OrientedProcedures for Information Management and Condensa-tion of Expository Texts.
Final Report Univ.
Kon-stanz, 1985 (TOPIC-17/85)llalliday, M.; R. Hasan: Cohesion in English.
Longman,1976.Hammwoehner, R.; U. Thiel: TOPOGRAPHIC: eine gra-phisch-interaktive Retrievalschnittstelle.
In:Proc.
MICROGRAPHICS.
GI, 1984, pp.155-169.Hobbs, J.: Why is Discourse Coherent?
In: Neubauer(ed): Coherence in Natural-Language Texts.
Buske,1983, pp.29-70.Hobbs, J. et al: Natural Language Access to Struc-tured Text.
In: Proc.
COLING 82, pp.127-132.Kuhlen, R.: A Knowledge-Based Text Analysis Systemfor the Graphically Supported Production of Cas--caded Text Condensa tes.
Univ.
Konstanz, 1984(TOPIC-9/84)Lehnert, W.: Plot Units and Narrative Summarization.In: Cognitive Science 5.
1981, pp.293~331.Loef, S.: The POLYTEXT/ARBIT Demonstration System.Umea/Sweden: Foersvarets Forskningsanstalt, FOA 4rapport, C 40121-M7, 1980.P_o_olanyl, L.; R. Scha: A Syntactic Approach to Dis-course Semantics.
In: Proc.
COLING 84, pp.413-419.Reichman, R.: Conversational Coherency.
In: CognitiveScience 2.
1978, pp.283-327.Re\]met, U.; U.. Hahn: On Formal Semantic Properties ofa Frame Data Model.
In: Computers and ArtificialIntelligence 4.
1985, pp.335-351.Riesbeck, C?
: Realistic Language Comprehension.
In:Lehnert / Ringle (eds): Strategies for NaturalLanguage Processing.
Erlbaum, 1982, pp.37-54.Rnmelhart, D?
: Notes on a Schema for Stories?
In:Bobrow /Co I l ins  (eds): Representation and Under-standing.
Academic P., 1975, pp.211-236.Schank, R. et al: An Integrated Understander.
In:Amer.
J. Comput.
Ling.
6.
1980, pp.13-30.Small, S.; C. Rieger: Parsing and Comprehending withWord Experts (a Theory and its Realization)?
In:Lehnert / Ringle (eds): Strategies for NaturalLanguage Processing.
Erlbaum, 1982, pp.89-147oSmetacek, V.; M. Koenigova: Vnimani odborneho textu:experiment.
In: Ceskoslovenska Informatika 19.1977, pp.40-46.Strong,S: An Algorithm for Generating Structural Sur-rogates of English Text.
In:JASIS 25.1974, pp.10-24Tait, J.: Automatic Snmmarising of English Texts.Univ.
of Cambridge, 1982 (= Technical Report 47)Taylor, S.: Automatic Abstracting by Applying Graphi-cal Techniques to Semantic Networks.
Evanston/Iii.
: Northwestern Univ., 1974.503
