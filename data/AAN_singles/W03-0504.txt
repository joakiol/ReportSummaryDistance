Summarization of Noisy Documents: A Pilot StudyHongyan JingIBM T.J. Watson Research CenterYorktown Heights, NYhjing@us.ibm.comDaniel Lopresti19 Elm StreetHopewell, NJdpl@dlopresti.comChilin Shih150 McMane AvenueBerkeley Heights, NJcls@prosodies.orgAbstractWe investigate the problem of summarizingtext documents that contain errors as a result ofoptical character recognition.
Each stage in theprocess is tested, the error effects analyzed, andpossible solutions suggested.
Our experimentalresults show that current approaches, which aredeveloped to deal with clean text, suffer signif-icant degradation even with slight increases inthe noise level of a document.
We conclude byproposing possible ways of improving the per-formance of noisy document summarization.1 IntroductionPrevious work in text summarization has focused pre-dominately on clean, well-formatted documents, i.e.,documents that contain relatively few spelling and gram-matical errors, such as news articles or published tech-nical material.
In this paper, we present a pilot study ofnoisy document summarization, motivated primarily bythe impact of various kinds of physical degradation thatpages may endure before they are scanned and processedusing optical character recognition (OCR) software.As more and more documents are now scanned inby OCR, an understanding of the impact of OCRon summarization is crucial and timely.
The Mil-lion Book Project is one of the projects that usesOCR technology for digitizing books.
Pioneered byresearchers at Carnegie Mellon University, it aims todigitize a million books by 2005, by scanning thebooks and indexing their full text with OCR technology(http://www.archive.org/texts/millionbooks.php).Understandably, summarizing documents that containmany errors is an extremely difficult task.
In our study,we focus on analyzing how the quality of summariesis affected by the level of noise in the input document,and how each stage in summarization is impacted by thenoise.
Based on our analysis, we suggest possible ways ofimproving the performance of automatic summarizationsystems for noisy documents.
We hope to use what wehave learned from this initial investigation to shed lighton future directions.What we ascertain from studying the problem of noisydocument summarization can be useful in a number ofother applications as well.
Noisy documents constitutea significant percentage of documents we encounter ineveryday life.
The output from OCR and speech recogni-tion (ASR) systems typically contain various degrees oferrors, and even purely electronic media, such as email,are not error-free.
To summarize such documents, weneed to develop techniques to deal with noise, in addi-tion to working on the core algorithms.
Whether we cansuccessfully handle noise will greatly influence the finalquality of summaries of such documents.Some researchers have studied problems relating to in-formation extraction from noisy sources.
To date, thiswork has focused predominately on errors that arise dur-ing speech recognition, and on problems somewhat dif-ferent from summarization.
For example, Gotoh andRenals propose a finite state modeling approach to ex-tract sentence boundary information from text and audiosources, using both n-gram and pause duration informa-tion (Gotoh and Renals, 2000).
They found that precisionand recall of over 70% could be achieved by combiningboth kinds of features.
Palmer and Ostendorf describe anapproach for improving named entity extraction by ex-plicitly modeling speech recognition errors through theuse of statistics annotated with confidence scores (Palmerand Ostendorf, 2001).
Hori and Furui summarize broad-cast news speech by extracting words from automatictranscripts using a word significance measure, a confi-dence score, linguistic likelihood, and a word concatena-tion probability (Hori and Furui, 2001).There has been much less work, however, in the caseof noise induced by optical character recognition.
Earlypapers by Taghva, et al show that moderate error rateshave little impact on the effectiveness of traditional infor-mation retrieval measures (Taghva et al, 1996a; Taghvaet al, 1996b), but this conclusion does not seem to applyto the task of summarization.
Miller, et al study the per-formance of named entity extraction under a variety ofscenarios involving both ASR and OCR output (Miller etal., 2000), although speech is their primary interest.
Theyfound that by training their system on both clean andnoisy input material, performance degraded linearly as afunction of word error rates.
They also note in their pa-per: ?To our knowledge, no other information extractiontechnology has been applied to OCR material?
(pg.
322).An intriguing alternative to text-based summarizationis Chen and Bloomberg?s approach to creating summarieswithout the need for optical character recognition (Chenand Bloomberg, 1998).
Instead, they extract indicativesummary sentences using purely image-based techniquesand common document layout conventions.
While thisis effective when the final summary is to be viewed on-screen by the user, the issue of optical character recog-nition must ultimately be faced in most applications ofinterest (e.g., keyword-driven information retrieval).For the work we present in this paper, we performed asmall pilot study in which we selected a set of documentsand created noisy versions of them.
These were generatedboth by scanning real pages via OCR and by using a fil-ter we have developed that injects various levels of noiseinto an original source document.
The clean and noisydocuments were then piped through a summarization sys-tem.
We tested different modules that are often includedin such systems, including sentence boundary detection,part-of-speech tagging, syntactic parsing, extraction, andediting of extracted sentences.
The experimental resultsshow that these modules suffer significant degradation asthe noise level in the document increases.
We discuss theerrors made at each stage and how they affect the qualityof final summaries.In Section 2, we describe our experiment, includingthe data creation process and various tests we performed.In Section 3, we analyze the results of the experiment andcorrelate the quality of summaries with noise levels in theinput document and the errors made at different stages ofthe summarization process.
We then discuss some of thechallenges in summarizing noisy documents and suggestpossible methods for improving the performance of noisydocument summarization.
We conclude with future work.2 The Experiment2.1 Data creationWe selected a small set of four documents to study inour experiment.
Three of four documents were from theTREC corpus and one was from a Telecommunicationscorpus we collected ourselves (Jing, 2001).
All are pro-fessionally written news articles, each containing from200 to 800 words (the shortest document was 9 sentencesand the longest was 38 sentences).For each document, we created 10 noisy versions.
Thefirst five corresponded to real pages that had been printed,possibly subjected to a degradation, scanned at 300 dpiusing a UMAX Astra 1200S scanner, and then OCR?edwith Caere OmniPage Limited Edition.
These included:clean The page as printed.fax A faxed version of the page.dark An excessively dark (but legible) photocopy.light An excessively light (but legible) photocopy.skew The clean page skewed on the scanner glass.Note that because the faxed and photocopied documentswere processed by running them through automatic pagefeeders, these pages can also exhibit noticeable skew.The remaining five sample documents in each case wereelectronic copies of the original that had had syntheticnoise (single-character deletions, insertions, and substi-tutions) randomly injected at predetermined rates: 5%,10%, 15%, 20%, and 25%.In general, we want to study both real and syntheticnoise.
The arguments in favor of the former are quite ob-vious.
The arguments in favor of the latter is that it iseasier to control synthetic noise effects, and often theyhave exactly the same impact on the overall process asreal noise.
Even though the errors may be artificial, theimpact on later processes is probably the same.
For ex-ample, changing ?nuclear?
to ?nZclear?
does not reflecta common OCR error.
But it does have the same effect?
changing a word in the dictionary to a word that is nolonger recognized.
If the impact is identical and it is eas-ier to control, then it is beneficial to use synthetic noisein addition to real noise.A summary was created for each document by humanexperts.
For the three documents from the TREC cor-pus, the summaries were generated by taking a majorityopinion.
Each document was given to five people whowere asked to select 20% of the original sentences as thesummary.
Sentences selected by three or more of the fivehuman subjects were included in the summary of the doc-ument.
For the document from the Telecommunicationscorpus, an abstract of the document was provided by astaff writer from the news service.
These human-createdsummaries are useful in evaluating the quality of the au-tomatic summaries.2.2 Summarization stagesWe are interested in testing how each stage of a summa-rization system is affected by noise, and how this in turnaffects the quality of the summaries.
Many summariza-tion approaches exist, and it would be difficult to studythe effects of noise on all of them.
However, the follow-ing stages are common to many summarization systems:?
Step 1: Tokenization.
The main task here is to breakthe text into sentences.
Tokens in the input text arealso identified.?
Step 2: Preprocessing.
This typically involves part-of-speech tagging and syntactic parsing.
This step isoptional; some systems do not perform tagging andparsing at all.
Topic segmentation is deployed bysome summarization systems, but not many.?
Step 3: Extraction.
This is the main step in summa-rization, in which the automatic summarizer selectskey sentences (sometimes paragraphs or phrases) toinclude in the summary.?
Step 4: Editing.
Some systems post-edit the ex-tracted sentences to make them more coherent andconcise.For each stage, we selected one or two systems thatperform the task and tested their performance on bothclean and noisy documents.?
For tokenization, we tested two tokenizers: one is arule-based system that decides sentence boundariesbased on heuristic rules encoded in the program, andthe other one is a trainable tokenizer that uses a deci-sion tree approach for detecting sentence boundariesand has been trained on a large amount of data.?
For part-of-speech tagging and syntactic parsing, wetested the English Slot Grammar (ESG) parser (Mc-Cord, 1990).
The outputs from both tokenizers weretested on ESG.
The ESG parser requires as input di-vided sentences and returns a parse tree for each in-put sentence, including a part-of-speech tag for eachword in the sentence.
The reason we chose a fullparser such as ESG rather than a part-of-speech tag-ger and a phrase chunking system is that the sum-mary editing system in Step 4 uses the output fromESG.
Although many sentence extraction systemsdo not use full syntactic information, it is not rarefor summarization systems that do use parsing out-put to use a full parser, whether it is ESG or a sta-tistical parser such as Collin?s, since such summa-rization systems often perform operations that needdeep understanding of the original text.?
For extraction, we used a program that relies on lexi-cal cohesion, frequency, sentence positions, and cuephrases to identify key sentences (Jing, 2001).
Thelength parameter of the summaries was set to 20%of the number of sentences in the original document.The output from the rule-based tokenizer was usedin this step.
This particular extraction system doesnot use tagging and parsing.?
In the last step, we tested a cut-and-paste system thatedits extracted sentences by simulating the revisionoperations often performed by professional abstrac-tors (Jing, 2001).
The outputs from all the three pre-vious steps were used by the cut-and-paste system.All of the summaries produced in this experiment weregeneric, single-document summaries.3 Results and AnalysisIn this section, we present results at each stage of sum-marization, analyzing the errors made and their effectson the quality of summaries.3.1 OCR performanceWe begin by examining the overall performance ofthe OCR process.
Using standard edit distance tech-niques (Esakov et al, 1994), we can compare the out-put of OCR to the ground-truth to classify and quantifythe errors that have arisen.
We then compute, on a per-character and per-word basis, a figure for average preci-sion (percentage of characters or words recognized thatare correct) and recall (percentage of characters or wordsin the input document that are correctly recognized).
Asindicated in Table 1, OCR performance varies widely de-pending on the type of degradation.
Precision values aregenerally higher than recall because, in certain cases, theOCR system failed to produce output for a portion of thepage in question.
Since we are particularly interested inpunctuation due to its importance in delimiting sentenceboundaries, we tabulate a separate set of precision andrecall values for such characters.
Note that these are uni-formly lower than the other values in the table.
Recall, inparticular, is a serious issue; many punctuation marks aremissed in the OCR output.Table 1: OCR performance relative to ground-truth (av-erage precision and recall).Per-CharacterAll Symbols Punctuation Per-WordPrec.
Rec.
Prec.
Rec.
Prec.
Rec.OCR.clean 0.990 0.882 0.869 0.506 0.963 0.874OCR.light 0.897 0.829 0.556 0.668 0.731 0.679OCR.dark 0.934 0.739 0.607 0.539 0.776 0.608OCR.fax 0.969 0.939 0.781 0.561 0.888 0.879OCR.skew 0.991 0.879 0.961 0.496 0.963 0.8693.2 Sentence boundary errorsSince most summarization systems rely on sentence ex-traction, it is important to identify sentence boundariescorrectly.
For clean text, the reported accuracy of sen-tence boundary detection is usually above 95% (Palmerand Hearst, 1997; Reyner and Ratnaparkhi, 1997; Riley,1989).
However, detecting sentence boundaries in noisydocuments is a serious challenge since punctuation andcapitalization, which are important features in sentenceboundary detection, are unreliable in noisy documents.As we have just noted, punctuation errors arise frequentlyin the OCR output of degraded page images.We tested two tokenizers: one is a rule-based systemand the other is a decision tree system.
The experimen-tal results show that for the clean text, the two systemsperform almost equally well.
Manual checking of the re-sults indicates that both tokenizers made very few errors.There should be 90 sentence boundaries in total.
The de-cision tree tokenizer correctly identified 88 of the sen-tence boundaries and missed two (precision: 100%; re-call: 98%).
The rule-based tokenizer correctly identified89 of the boundaries and missed one (precision: 100%;recall: 99%).
Neither system made any false positive er-rors (i.e., they did not break sentences at non-sentenceboundaries).For the noisy documents, however, both tokenizersmade significant numbers of errors.
The types of er-rors they made, moreover, were quite different.
Whilethe rule-based system made many false negative errors,the decision tree system made many false positive errors.Therefore, the rule-based system identified far fewer sen-tence boundaries than the truth, while the decision treesystem identified far more than the truth.Table 2: Sentence boundary detection results: total num-ber of sentences detected and average words per sentencefor two tokenizers.
Tokenizer 1 is decision tree based,and tokenizer 2 is rule based.Tokenizer 1 Tokenizer 2Sent Words/sent Sent Words/sentOriginal 88 23 89 22Snoise.05 95 20 70 27Snoise.10 97 20 69 28Snoise.15 105 19 65 30Snoise.20 109 17 60 31Snoise.25 121 15 51 35OCR.clean 77 23 82 21OCR.light 119 15 64 28OCR.dark 70 21 46 33OCR.fax 78 26 75 27OCR.skew 77 23 82 21Table 2 shows the number of sentences identified byeach tokenizer for different versions of the documents.As we can see from the table, the noisier the documents,the more errors the tokenizers made.
This relationshipwas demonstrated clearly by the results for the docu-ments with synthetic noise.
As the noise rate increases,the number of boundaries identified by the decision treetokenizer gradually increases, and the number of bound-aries identified by the rule-based tokenizer gradually de-creases.
Both numbers diverge from truth, but they err inopposite directions.The two tokenizers behaved less consistently on theOCR?ed documents.
For OCR.light, OCR.dark, andOCR.fax, the decision tree tokenizer produced more sen-tence boundaries than the rule-based tokenizer.
But forOCR.clean and OCR.skew, the decision tree tokenizerproduced fewer sentence boundaries.
This may be re-lated to the noise level in the document.
OCR.clean andOCR.skew contain fewer errors than the other noisy ver-sions (recall Table 1).
This indicates that the decisiontree tokenizer tends to identify fewer sentence boundariesthan the rule-based tokenizer for clean text or documentswith very low levels of noise, but more sentence bound-aries when the documents have a relatively high level ofnoise.Errors made at this stage are extremely detrimental,since they will propagate to all of the other modules ina summarization system.
When a sentence boundary isincorrectly marked, the part-of-speech tagging and thesyntactic parsing are likely to fail.
Sentence extractionmay become problematic; for example, one of the docu-ments in our test set contains 24 sentences, but for one ofits noisy versions (OCR.dark), the rule-based tokenizermissed most sentence boundaries and divided the docu-ment into only three sentences, making extraction at thesentence level difficult at best.Since sentence boundary detection is important tosummarization, the development of robust techniquesthat can handle noisy documents is worthwhile.
We willreturn to this point in Section 4.3.3 Parsing errorsSome summarization systems use a part-of-speech taggeror a syntactic parser in their preprocessing steps.We computed the percentage of sentences that ESGfailed to return a complete parse tree, and used that valueas one way of measuring the performance of the parser onthe noisy documents.
If the parser cannot return a com-plete parse tree, then it definitely fails to analyze the sen-tence; but even when a complete parse tree is returned,the parse can be wrong.
As we can see from Table 3, asignificant percentage of noisy sentences were not parsed.Even for the documents with synthetic noise at a 5% rate,around 60% of the sentences cannot be handled by theparser.
This indicates that a full parser such as ESG isvery sensitive to noise.Even when ESG produces a complete parse tree for anoisy sentence, the result is incorrect most of times.
Forinstance, the sentence ?Internet sites found that almost 90percent collected personal information from youngsters?was transformed to ?uInternet sites fo6ndha alQmostK0pecent coll / 9ed pe??
after adding synthetic noise at a25% rate.
For this noisy sentence, the parser returneda complete parse tree that marked the word ?sites?
asthe main verb of the sentence, and tagged all the otherwords in the sentence as nouns.1 Although a completeparse tree is returned in this case, it is incorrect.
Thisexplains the phenomenon that the parser returned a higherpercentage of complete parse trees for documents withsynthetic noise at the 25% rate than for documents withlower levels of noise.Table 3: Percentage of sentences with incomplete parsetrees.
Sentence boundaries were first detected using Tok-enizer 1 and Tokenizer 2.Tokenizer 1 Tokenizer 2Original 10% 5%Snoise.05 59% 58%Snoise.10 69% 71%Snoise.15 66% 81%Snoise.20 64% 66%Snoise.25 58% 76%OCR.clean 2% 3%OCR.light 46% 53%OCR.dark 37% 43%OCR.fax 37% 30%OCR.skew 5% 6%The above results indicate that syntactic parsers arevery vulnerable to noise in a document.
Even low lev-els of noise lead to a significant drop in performance.3.4 Extract quality versus noise levelIn the next step, we studied how the sentence extrac-tion module in a summarization system is affected bynoise in the input document.
The sentence extractor weused (Jing, 2001) relies on lexical links between words,word frequency, cue phrases, and sentence positions toidentify key sentences.
The performance of the system isaffected by noise in multiple dimensions: lexical links areless reliable in a noisy condition; cue phrases are likelyto be missed due to noisy spelling; and word frequencyis less accurate due to different noisy occurrences of thesame word.Evaluation of noisy document summaries is an inter-esting problem.
Both intrinsic evaluation and extrinsicevaluation need to deal with noise effect on the quality1One reason might be that the tagger is likely to tag un-known words as nouns, and all the noisy words are consideredunknown words.of final summaries.
For intrinsic evaluation, it is de-batable whether clean human summaries or noisy doc-ument summaries (or both) should be used for compari-son.
There are two issues related to ?noisy?
human sum-maries: one, whether such summaries are obtainable, andtwo, whether such summaries should be used in evalua-tion.
We note that it is already difficult for a human torecover the information in the noisy documents when thesynthetic noise rate reached 10%.
Therefore, noisy hu-man summaries will not be available for documents withrelatively high level of noise.
Secondly, even though theoriginal documents are noisy, it is desirable for the finalsummaries to be fluent and clean.
Therefore, if our ul-timate goal is to produce a fluent and clean summary, itbenefits to compare the automatic summaries with suchsummaries rather than noisy summaries.We compared the noisy automatic summaries with theclean human summaries by using three measures: uni-gram overlap between the automatic summary and thehuman-created summary, bigram overlap, and the simplecosine.
These results are shown in Table 4.
The unigramoverlap is computed as the number of unique words oc-curring both in the extract and the ideal summary for thedocument, divided by the total number of unique wordsin the extract.
Bigram overlap is computed similarly, re-placing words with bigrams.
The simple cosine is com-puted as the cosine of two document vectors, the weightof each element in the vector being 1/?N , where N isthe total number of elements in the vector.Not surprisingly, summaries of noisier documents gen-erally have a lower overlap with human-created sum-maries.
However, this can be caused by either the noisein the document or poor performance of the sentence ex-traction system.
To separate these effects and measure theperformance of sentence extraction alone, we also com-puted the unigram overlap, bigram overlap, and cosinebetween each noisy document and its corresponding orig-inal text.
These numbers are included in Table 4 in paren-theses; they are an indication of the average noise levelin a document.
For instance, the table shows that 97%of words that occurred in OCR.clean documents also ap-peared in the original text, while only 62% of words thatoccurred in OCR.light appeared in the original.
This in-dicates that OCR.clean is less noisy than OCR.light.3.5 Abstract generation for noisy documentsTo generate more concise and coherent summaries, asummarization system may edit extracted sentences.
Tostudy how this step in summarization is affected by noise,we tested a cut-and-paste system that edits extracted sen-tences by simulating revision operations often used byhuman abstractors, including the operations of removingphrases from an extracted sentence, and combining a re-duced sentence with other sentences (Jing, 2001).
ThisTable 4: Unigram overlap, bigram overlap, and simplecosine between extracts and human-created summaries(the numbers in parentheses are the corresponding valuesbetween the documents and the original text).Unigram Bigram CosineOriginal 0.85 (1.00) 0.75 (1.00) 0.51 (1.00)Snoise.05 0.55 (0.61) 0.38 (0.50) 0.34 (0.65)Snoise.10 0.41 (0.41) 0.22 (0.27) 0.25 (0.47)Snoise.15 0.25 (0.26) 0.10 (0.13) 0.20 (0.31)Snoise.20 0.17 (0.19) 0.04 (0.07) 0.14 (0.23)Snoise.25 0.18 (0.14) 0.04 (0.04) 0.09 (0.16)OCR.clean 0.86 (0.97) 0.78 (0.96) 0.50 (0.93)OCR.light 0.62 (0.63) 0.47 (0.55) 0.36 (0.65)OCR.dark 0.81 (0.70) 0.73 (0.65) 0.38 (0.66)OCR.fax 0.77 (0.84) 0.67 (0.79) 0.48 (0.86)OCR.skew 0.84 (0.97) 0.74 (0.96) 0.48 (0.93)cut-and-paste stage relies on the results from sentence ex-traction in the previous step, the output from ESG, and aco-reference resolution system.For the clean text, the cut-and-paste system performedsentence reduction on 59% of the sentences that were ex-tracted in the sentence extraction step, and sentence com-bination on 17% of the extracted sentences.
For the noisytext, however, the system applied very few revision oper-ations to the extracted (noisy) sentences.
Since the cut-and-paste system relies on the output from ESG and co-reference resolution, which failed on most of the noisytext, it is not surprising that it did not perform well underthese circumstances.Editing sentences requires a deeper understanding ofthe document and, as the last step in the summarizationpipeline, relies on results from all of the previous steps.Hence, it is affected most severely by noise in the inputdocument.4 Challenges in Noisy DocumentSummarizationIn the previous section, we have presented and analyzederrors at each stage of summarization when applied tonoisy documents.
The results show that the methodswe tested at every step are fragile, susceptible to fail-ures and errors even with slight increases in the noiselevel of a document.
Clearly, much work needs to bedone to achieve acceptable performance in noisy docu-ment summarization.
We need to develop summarizationalgorithms that do not suffer significant degradation whenused on noisy documents.
We also need to develop robustnatural language processing techniques.
For example, itwill be useful to develop a sentence boundary detectionsystem that can identify sentence breaks in noisy docu-ments more reliably.
One way to achieve this might be toretrain an existing system on tokenized noisy documentsso that it will learn features that are indicative of sentencebreaks in noisy documents.
However, this is only appli-cable if the noise level in the documents is low.
For doc-ument with high level of noise, such approach will not beeffective.In the remainder of this section, we discuss several is-sues in noisy document summarization, identifying theproblems and proposing possible solutions.
We regardthis as a first step towards a more comprehensive studyon the topic of noisy document summarization.4.1 Choosing an appropriate granularityIt is important to choose an appropriate unit level to rep-resent the summaries.
For clean text, sentence extractionis a feasible goal since we can reliably identify sentenceboundaries.
For documents with very low levels of noise,sentence extraction is still possible since we can probablyimprove our programs to handle such documents.
How-ever, for documents with relatively high noise rates, webelieve it is better to forgo sentence extraction and insteadfavor extraction of keywords or noun phrases, or gener-ation of headline-style summaries.
In our experiment,when the synthetic noise rate reached 10% (which is rep-resentative of what can happen when real-world docu-ments are degraded), it was already difficult for a humanto recover the information intended to be conveyed fromthe noisy documents.Keywords, noun phrases, or headline-style summariesare informative indications of the main topic of a doc-ument.
For documents with high noise rates, extractingkeywords or noun phrases is a more realistic and attain-able goal than sentence extraction.
Still, it may be de-sirable to correct the noise in the extracted keywords orphrases, either before or after summarization.
There hasbeen past work on correcting spelling mistakes and errorsin OCR output; these techniques could be useful for thispurpose.4.2 Using other information sourcesIn addition to text, target documents contain other typesof useful information that could be employed in creatingsummaries.
As noted previously, Chen and Bloomberg?simage-based summarization technique avoids many ofthe problems we have been discussing by exploiting doc-ument layout features.
A possible approach to summariz-ing noisy documents, then, might be to use their methodto create an image summary and then apply OCR af-terwards to the resulting page.
We note, though, that itseems unlikely this would lead to an improvement of theoverall OCR results, a problem which may almost cer-tainly must be faced at some point in the process.4.3 Assessing error rates without ground-truthThe quality of summarization is directly tied to the levelof noise in a document.
In this context, it would be use-ful to develop methods for assessing document noise lev-els without having access to the ground-truth.
Intuitively,OCR may create errors that cause the output text to devi-ate from ?normal?
text.
Therefore, one way of evaluatingOCR output, in the absence of the original ground-truth,is to compare its features against features obtained from alarge corpus of correct text.
Letter trigrams (Church andGale, 1991) are commonly used to correct spelling andOCR errors (Angell et al, 1983; Kuckich, 1992; Zamoraet al, 1981), and can be applied to evaluate OCR output.We computed trigram tables (including symbols andpunctuation marks) for 10 days of AP news articles andevaluated the documents used in our experiment.
The tri-grams were computed on letters and Good-Turing estima-tion is used for smoothing.
The values in the table are av-erage trigram scores for each document set.
As expected,OCR errors create rare or previously unseen trigrams thatlead to higher trigram scores in noisy documents.
As in-dicated in Table 5, the ground-truth (original) documentshave the lowest average trigram score.
These scores pro-vide a relative ranking that reflects the controlled noiselevels (Snoise.05 through Snoise.25), as well as cer-tain of the real OCR data (OCR.clean, OCR.dark, andOCR.light).Table 5: Average trigram scores.Trigram scoreOriginal 2.30Snoise.05 2.75Snoise.10 3.13Snoise.15 3.50Snoise.20 3.81Snoise.25 4.14OCR.clean 2.60OCR.light 3.11OCR.dark 2.98OCR.fax 2.55OCR.skew 2.40Different texts have very different baseline trigramscores.
The ranges of scores for clean and noisy textoverlap.
This is because some documents contain moreinstances of frequent words than others (such as ?the?
),which bring down the average scores.
This issue makesit impractical to use trigram scores in isolation to judgeOCR output.It may be possible to identify some problems if wescan larger units and incorporate contextual information.For example, a window of three characters is too smallto judge whether the symbol @ is used properly: a@bseems to be a potential OCR error, but is acceptable whenit appears in an email address such as lsa@bbb.com.
In-creasing the unit size will create sparse data problems,however, which is already an issue for trigrams.In the future, we plan to experiment with improvedmethods for identifying problematic regions in OCRtext, including using language models and incorporat-ing grammatical patterns.
Many linguistic properties canbe identified when letter sequences are encoded in broadclasses.
For example, long consonant strings are rare inEnglish text, while long number strings are legal.
Theseproperties can be captured when characters are mappedinto carefully selected classes such as symbols, numbers,upper- and lower-case letters, consonants, and vowels.Such mappings effectively reduce complexity, allowingus to sample longer strings to scan for abnormal patternswithout running into severe sparse data problems.Our intention is to establish a robust index that mea-sures whether a given section of text is ?summarizable.
?This problem is related to the general question of assess-ing OCR output without ground-truth, but we shift thescope of the problem to ask whether the text is summa-rizable, rather than how many errors it may contain.We also note that documents often contain logicalcomponents that go beyond basic text.
Pages may includephotographs and figures, program code, lists, indices, etc.Tables, for example, can be detected, parsed, and refor-mulated so that it becomes possible to describe their over-all structure and even allow users to query them (Hu etal., 2000).
Developing appropriate ways of summarizingsuch material is another topic of interest.5 Conclusions and Future WorkIn this paper, we have discussed some of the challengesin summarizing noisy documents.
In particular, we brokedown the summarization process into four steps: sen-tence boundary detection, preprocessing (part-of-speechtagging and syntactic parsing), extraction, and editing.We tested each step on noisy documents and analyzedthe errors that arose.
We also studied how the quality ofsummarization is affected by the noise level and the er-rors made at each stage of processing.To improve the performance of noisy document sum-marization, we suggest extracting keywords or phrasesrather than full sentences, especially when summarizingdocuments with high levels of noise.
We also propose us-ing other sources of information, such as document lay-out cues, in combination with text when summarizingnoisy documents.
In certain cases, it will be importantto be able to assess the noise level in a document; wehave begun exploring this question as well.
Our plansfor the future include developing robust techniques to ad-dress the issues we have outlined in this paper.Lastly, we regard presentation and user interaction as acrucial component in real-world summarization systems.Given that noisy documents, and hence their summaries,may contain errors, it is important to find the best waysof displaying such information so that the user may pro-ceed with confidence, knowing that the summary is trulyrepresentative of the document(s) in question.ReferencesR.
Angell, G. Freund, and P. Willet.
1983.
Automaticspelling correction using a trigram similarity measure.Information Processing and Management, 19(4):255?261.F.
R. Chen and D. S. Bloomberg.
1998.
Summarizationof imaged documents without OCR.
Computer Visionand Image Understanding, 70(3):307?320.K.
Church and W. Gale.
1991.
Probability scoring forspelling correction.
Statistics and Computing, 1:93?103.Jeffrey Esakov, Daniel P. Lopresti, and Jonathan S. Sand-berg.
1994.
Classification and distribution of opticalcharacter recognition errors.
In Proceedings of Doc-ument Recognition I (IS&T/SPIE Electronic Imaging),volume 2181, pages 204?216, San Jose, CA, February.Y.
Gotoh and S. Renals.
2000.
Sentence boundary detec-tion in broadcast speech transcripts.
In Proceedomgsof ISCA Tutorial and Research Workshop ASR-2000,Paris, France.C.
Hori and S. Furui.
2001.
Advances in automaticspeech summarization.
In Proceedings of the 7thEuropean Conference on Speech Communication andTechnology, pages 1771?1774, Aalborg, Denmark.Jianying Hu, Ramanujan Kashi, Daniel Lopresti, andGordon Wilfong.
2000.
A system for understandingand reformulating tables.
In Proceedings of the FourthIAPR International Workshop on Document AnalysisSystems, pages 361?372, Rio de Janeiro, Brazil, De-cember.Hongyan Jing.
2001.
Cut-and-paste Text Summariza-tion.
Ph.D. thesis, Department of Computer Science,Columbia University, New York, NY.K.
Kuckich.
1992.
Techniques for automatically cor-recting words in text.
ACM Computing Surveys,24(4):377?439.M.
McCord, 1990.
English Slot Grammar.
IBM.D.
Miller, S. Boisen, R. Schwartz, R. Stone, andR.
Weischedel.
2000.
Named entity extraction fromnoisy input: Speech and OCR.
In Proceedings of the6th Applied Natural Language Processing Conference,pages 316?324, Seattle, WA.D.
Palmer and M. Hearst.
1997.
Adaptive multilin-gual sentence boundary disambiguation.
Computa-tional Linguistics, 23(2):241?267, June.D.
D. Palmer and M. Ostendorf.
2001.
Improving infor-mation extraction by modeling errors in speech recog-nizer output.
In J. Allan, editor, Proceedings of theFirst International Conference on Human LanguageTechnology Research.J.
C. Reyner and A. Ratnaparkhi.
1997.
A maximum en-tropy approach to identifying sentence boundaries.
InProceedings of the Fifth Conference on Applied Natu-ral Language Processing, Washington D.C.M.
Riley.
1989.
Some applications of tree-based mod-elling to speech and language.
In Proceedings ofthe DARPA Speech and Natural Language Workshop,pages 339?352, Cape Cod, MA.Kazem Taghva, Julie Borsack, and Allen Condit.
1996a.Effects of OCR errors on ranking and feedback usingthe vector space model.
Information Processing andManagement, 32(3):317?327.Kazem Taghva, Julie Borsack, and Allen Condit.
1996b.Evaluation of model-based retrieval effectiveness withOCR text.
ACM Transactions on Information Systems,14:64?93, January.E.
Zamora, J. Pollock, and A. Zamora.
1981.
The use oftrigram analysis for spelling error detection.
Informa-tion Processing and Management, 17(6):305?316.
