Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948?957,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsContextualizing Semantic RepresentationsUsing Syntactically Enriched Vector ModelsStefan Thater and Hagen F?rstenau and Manfred PinkalDepartment of Computational LinguisticsSaarland University{stth, hagenf, pinkal}@coli.uni-saarland.deAbstractWe present a syntactically enriched vec-tor model that supports the computationof contextualized semantic representationsin a quasi compositional fashion.
It em-ploys a systematic combination of first- andsecond-order context vectors.
We applyour model to two different tasks and showthat (i) it substantially outperforms previ-ous work on a paraphrase ranking task, and(ii) achieves promising results on a word-sense similarity task; to our knowledge, it isthe first time that an unsupervised methodhas been applied to this task.1 IntroductionIn the logical paradigm of natural-language seman-tics originating from Montague (1973), semanticstructure, composition and entailment have beenmodelled to an impressive degree of detail andformal consistency.
These approaches, however,lack coverage and robustness, and their impacton realistic natural-language applications is lim-ited: The logical framework suffers from over-specificity, and is inappropriate to model the per-vasive vagueness, ambivalence, and uncertaintyof natural-language semantics.
Also, the hand-crafting of resources covering the huge amountsof content which are required for deep semanticprocessing is highly inefficient and expensive.Co-occurrence-based semantic vector models of-fer an attractive alternative.
In the standard ap-proach, word meaning is represented by featurevectors, with large sets of context words as dimen-sions, and their co-occurrence frequencies as val-ues.
Semantic similarity information can be ac-quired using unsupervised methods at virtually nocost, and the information gained is soft and gradual.Many NLP tasks have been modelled successfullyusing vector-based models.
Examples include in-formation retrieval (Manning et al, 2008), word-sense discrimination (Sch?tze, 1998) and disam-biguation (McCarthy and Carroll, 2003), to namebut a few.Standard vector-space models have serious lim-itations, however: While semantic information istypically encoded in phrases and sentences, distri-butional semantics, in sharp contrast to logic-basedsemantics, does not offer any natural concept ofcompositionality that would allow the semanticsof a complex expression to be computed from themeaning of its parts.
A different, but related prob-lem is caused by word-sense ambiguity and con-textual variation of usage.
Frequency counts ofcontext words for a given target word provide in-variant representations averaging over all differentusages of the target word.
There is no obvious wayto distinguish the different senses of e.g.
acquirein different contexts, such as acquire knowledge oracquire shares.Several approaches for word-sense disambigua-tion in the framework of distributional semanticshave been proposed in the literature (Sch?tze, 1998;McCarthy and Carroll, 2003).
In contrast to theseapproaches, we present a method to model the mu-tual contextualization of words in a phrase in a com-positional way, guided by syntactic structure.
Tosome extent, our method resembles the approachesproposed by Mitchell and Lapata (2008) and Erkand Pad?
(2008).
We go one step further, however,in that we employ syntactically enriched vectormodels as the basic meaning representations, as-suming a vector space spanned by combinationsof dependency relations and words (Lin, 1998).This allows us to model the semantic interactionbetween the meaning of a head word and its de-pendent at the micro-level of relation-specific co-occurrence frequencies.
It turns out that the benefitto precision is considerable.Using syntactically enriched vector modelsraises problems of different kinds: First, the use948of syntax increases dimensionality and thus maycause data sparseness (Pad?
and Lapata, 2007).Second, the vectors of two syntactically relatedwords, e.g., a target verb acquire and its direct ob-ject knowledge, typically have different syntacticenvironments, which implies that their vector repre-sentations encode complementary information andthere is no direct way of combining the informationencoded in the respective vectors.To solve these problems, we build upon pre-vious work (Thater et al, 2009) and propose touse syntactic second-order vector representations.Second-order vector representations in a bag-of-words setting were first used by Sch?tze (1998);in a syntactic setting, they also feature in Dligachand Palmer (2008).
For the problem at hand, theuse of second-order vectors alleviates the sparse-ness problem, and enables the definition of vectorspace transformations that make the distributionalinformation attached to words in different syntacticpositions compatible.
Thus, it allows vectors fora predicate and its arguments to be combined in acompositional way.We conduct two experiments to assess the suit-ability of our method.
Our first experiment is car-ried out on the SemEval 2007 lexical substitutiontask dataset (McCarthy and Navigli, 2007).
It willshow that our method significantly outperformsother unsupervised methods that have been pro-posed in the literature to rank words with respectto their semantic similarity in a given linguisticcontext.
In a second experiment, we apply ourmodel to the ?word sense similarity task?
recentlyproposed by Erk and McCarthy (2009), which isa refined variant of a word-sense disambiguationtask.
The results show a substantial positive effect.Plan of the paper.
We will first review relatedwork in Section 2, before presenting our model inSection 3.
In Sections 4 and 5 we evaluate ourmodel on the two different tasks.
Section 6 con-cludes.2 Related WorkSeveral approaches to contextualize vector repre-sentations of word meaning have been proposed.One common approach is to represent the mean-ing of a word a in context b simply as the sum, orcentroid of a and b (Landauer and Dumais, 1997).Kintsch (2001) considers a variant of this simplemodel.
By using vector representations of a predi-cate p and an argument a, Kintsch identifies wordsthat are similar to p and a, and takes the centroidof these words?
vectors to be the representation ofthe complex expression p(a).Mitchell and Lapata (2008), henceforth M&L,propose a general framework in which meaning rep-resentations for complex expressions are computedcompositionally by combining the vector represen-tations of the individual words of the complex ex-pression.
They focus on the assessment of differentoperations combining the vectors of the subexpres-sions.
An important finding is that component-wisemultiplication outperforms the more common addi-tion method.
Although their composition methodis guided by syntactic structure, the actual instanti-ations of M&L?s framework are insensitive to syn-tactic relations and word-order, assigning identicalrepresentation to dog bites man and man bites dog(see Erk and Pad?
(2008) for a discussion).
Also,they use syntax-free bag-of-words-based vectors asbasic representations of word meaning.Erk and Pad?
(2008), henceforth E&P, representthe meaning of a word w through a collection ofvectors instead of a single vector: They assumeselectional preferences and inverse selectional pref-erences to be constitutive parts of the meaning inaddition to the meaning proper.
The interpretationof a word p in context a is a combination of p?smeaning with the (inverse) selectional preferenceof a.
Thus, a verb meaning does not combine di-rectly with the meaning of its object noun, as onthe M&L account, but with the centroid of the vec-tors of the verbs to which the noun can stand in anobject relation.
Clearly, their approach is sensitiveto syntactic structure.
Their evaluation shows thattheir model outperforms the one proposed by M&Lon a lexical substitution task (see Section 4).
Thebasic vectors, however, are constructed in a wordspace similar to the one of the M&L approach.In Thater et al (2009), henceforth TDP, we tookup the basic idea from E&P of exploiting selec-tional preference information for contextualization.Instead of using collections of different vectors,we incorporated syntactic information by assuminga richer internal structure of the vector represen-tations.
In a small case study, moderate improve-ments over E&P on a lexical substitution task couldbe shown.
In the present paper, we formulate ageneral model of syntactically informed contextu-alization and show how to apply it to a number aof representative lexical substitution tasks.
Eval-uation shows significant improvements over TDP949acquireVBpurchaseVBgainVBshareNNknowlegeNNobj, 5 obj, 3 obj, 6 obj, 7skillNNbuy-backNNconj, 2 nn, 1Figure 1: Co-occurrence graph of a small samplecorpus of dependency trees.and E&P.3 The modelIn this section, we present our method of contex-tualizing semantic vector representations.
We firstgive an overview of the main ideas, which is fol-lowed by a technical description of first-order andsecond-order vectors (Section 3.2) and the contex-tualization operation (Section 3.3).3.1 OverviewOur model employs vector representations forwords and expressions containing syntax-specificfirst and second order co-occurrences information.The basis for the construction of both kinds ofvector representations are co-occurrence graphs.Figure 1 shows the co-occurrence graph of a smallsample corpus of dependency trees: Words arerepresented as nodes in the graph, possible depen-dency relations between them are drawn as labelededges, with weights corresponding to the observedfrequencies.
From this graph, we can directly readoff the first-order vector for every word w: the vec-tor?s dimensions correspond to pairs (r,w?)
of agrammatical relation and a neighboring word, andare assigned the frequency count of (w,r,w?
).The noun knowledge, for instance, would be rep-resented by the following vector:?5(OBJ?1,gain),2(CONJ?1,skill),3(OBJ?1,acquire), .
.
.
?This vector talks about the possible dependencyheads of knowledge and thus can be seen as the(inverse) selectional preference of knowledge (seeErk and Pad?
(2008)).As soon as we want to compute a meaning rep-resentation for a phrase like acquire knowledgefrom the verb acquire together with its direct ob-ject knowledge, we are facing the problem thatverbs have different syntactic neighbors than nouns,hence their first-order vectors are not easily com-parable.
To solve this problem we additionallyintroduce another kind of vectors capturing infor-mations about all words that can be reached withtwo steps in the co-occurrence graph.
Such a pathis characterized by two dependency relations andtwo words, i.e., a quadruple (r,w?,r?,w??
), whoseweight is the product of the weights of the twoedges used in the path.
To avoid overly sparse vec-tors we generalize over the ?middle word?
w?
andbuild our second-order vectors on the dimensionscorresponding to triples (r,r?,w??)
of two depen-dency relations and one word at the end of the two-step path.
For instance, the second-order vector foracquire is?15(OBJ,OBJ?1,gain),6(OBJ,CONJ?1,skill),6(OBJ,OBJ?1,buy-back),42(OBJ,OBJ?1,purchase), .
.
.
?In this simple example, the values are the prod-ucts of the edge weights on each of the paths.
Themethod of computation is detailed in Section 3.2.Note that second order vectors in particular con-tain paths of the form (r,r?1,w?
), relating a verbw to other verbs w?
which are possible substitutioncandidates.With first- and second-order vectors we cannow model the interaction of semantic informa-tion within complex expressions.
Given a pairof words in a particular grammatical relation likeacquire knowledge, we contextualize the second-order vector of acquire with the first-order vec-tor of knowledge.
We let the first-order vectorwith its selectional preference information act as akind of weighting filter on the second-order vector,and thus refine the meaning representation of theverb.
The actual operation we will use is point-wise multiplication, which turned out to be thebest-performing one for our purpose.
Interestingly,Mitchell and Lapata (2008) came to the same resultin a different setting.In our example, we obtain a new second-ordervector for acquire in the context of knowledge:?75(OBJ,OBJ?1,gain),12(OBJ,CONJ?1,skill),0(OBJ,OBJ?1,buy-back),0(OBJ,OBJ?1,purchase), .
.
.
?Note that all dimensions that are not ?licensed?
bythe argument knowledge are filtered out as they aremultiplied with 0.
Also, contextualisation of ac-quire with the argument share instead of knowledge950would have led to a very different vector, whichreflects the fact that the two argument nouns inducedifferent readings of the inherently ambiguous ac-quire.3.2 First and second-order vectorsAssuming a set W of words and a set R of depen-dency relation labels, we consider a Euclidean vec-tor space V1 spanned by the set of orthonormalbasis vectors {~er,w?
| r ?
R,w?
?W}, i.e., a vectorspace whose dimensions correspond to pairs of a re-lation and a word.
Recall that any vector of V1 canbe represented as a finite sum of the form ?ai~er,w?with appropriate scalar factors ai.
In this vectorspace we define the first-order vector [w] of a wordw as follows:[w] = ?r?Rw??W?(w,r,w?)
?~er,w?where ?
is a function that assigns the dependencytriple (w,r,w?)
a corresponding weight.
In the sim-plest case, ?
would denote the frequency in a cor-pus of dependency trees of w occurring togetherwith w?
in relation r. In the experiments reported be-low, we use pointwise mutual information (Churchand Hanks, 1990) instead as it proved superior toraw frequency counts:pmi(w,r,w?)
= logp(w,w?
| r)p(w | r)p(w?
| r)We further consider a similarly defined vec-tor space V2, spanned by an orthonormal basis{~er,r?,w?
| r,r?
?
R,w?
?W}.
Its dimensions there-fore correspond to triples of two relations and aword.
Evidently this is a higher dimensional spacethan V1, which therefore can be embedded intoV2 by the ?lifting maps?
Lr : V1 ??
V2 defined byLr(~er?,w?)
:=~er,r?,w?
(and by linear extension there-fore on all vectors of V1).
Using these lifting mapswe define the second-order vector [[w]] of a word was[[w]] = ?r?Rw??W?(w,r,w?)
?Lr([w?
])Substituting the definitions of Lr and [w?
], thisyields[[w]] = ?r,r??Rw???W(?w??W?(w,r,w?)?(w?,r?,w??))~er,r?,w?
?which shows the generalization over w?
in form ofthe inner sum.For example, if w is a verb, r = OBJ and r?
=OBJ?1 (i.e., the inverse object relation), then thecoefficients of ~er,r?,w??
in [[w]] would characterizethe distribution of verbs w??
which share objectswith w.3.3 CompositionBoth first and second-order vectors are defined forlexical expressions only.
In order to represent themeaning of complex expressions we need to com-bine the vectors for grammatically related wordsin a given sentence.
Given two words w and w?
inrelation r we contextualize the second-order vectorof w with the r-lifted first-order vector of w?:[[wr:w? ]]
= [[w]]?Lr([w?
])Here ?
may denote any operator on V2.
The ob-jective is to incorporate (inverse) selectional pref-erence information from the context (r,w?)
in sucha way as to identify the correct word sense of w.This suggests that the dimensions of [[w]] shouldbe filtered so that only those compatible with thecontext remain.
A more flexible approach thansimple filtering, however, is to re-weight those di-mensions with context information.
This can beexpressed by pointwise vector multiplication (interms of the given basis of V2).
We therefore take?
to be pointwise multiplication.To contextualize (the vector of) a word w withmultiple words w1, .
.
.
,wn and corresponding rela-tions r1, .
.
.
,rn, we compute the sum of the resultsof the pairwise contextualizations of the target vec-tor with the vectors of the respective dependents:[[wr1:w1,...,rn:wn ]] =n?k=1[[wrk:wk ]]4 Experiments: Ranking ParaphrasesIn this section, we evaluate our model on a para-phrase ranking task.
We consider sentences withan occurrence of some target word w and a list ofparaphrase candidates w1, .
.
.
,wk such that each ofthe wi is a paraphrase of w for some sense of w.The task is to decide for each of the paraphrasecandidates wi how appropriate it is as a paraphraseof w in the given context.
For instance, buy, pur-chase and obtain are all paraphrases of acquire, inthe sense that they can be substituted for acquire insome contexts, but purchase and buy are not para-phrases of acquire in the first sentence of Table 1.951Sentence ParaphrasesTeacher education students will acquire the knowl-edge and skills required to [.
.
.
]gain 4; amass 1; receive 1; obtain 1Ontario Inc. will [.
.
. ]
acquire the remaining IXOSshares [.
.
.
]buy 3; purchase 1; gain 1; get 1; procure 2; obtain 1Table 1: Two examples from the lexical substitution task data set4.1 ResourcesWe use a vector model based on dependency treesobtained from parsing the English Gigaword corpus(LDC2003T05).
The corpus consists of news fromseveral newswire services, and contains over fourmillion documents.
We parse the corpus using theStanford parser1 (de Marneffe et al, 2006) and anon-lexicalized parser model, and extract over 1.4billion dependency triples for about 3.9 millionwords (lemmas) from the parsed corpus.To evaluate the performance of our model, weuse various subsets of the SemEval 2007 lexicalsubstitution task (McCarthy and Navigli, 2007)dataset.
The complete dataset contains 10 instancesfor each of 200 target words?nouns, verbs, adjec-tives and adverbs?in different sentential contexts.Systems that participated in the task had to generateparaphrases for every instance, and were evaluatedagainst a gold standard containing up to 10 possibleparaphrases for each of the individual instances.There are two natural subtasks in generatingparaphrases: identifying paraphrase candidates andranking them according to the context.
We followE&P and evaluate it only on the second subtask:we extract paraphrase candidates from the goldstandard by pooling all annotated gold-standardparaphrases for all instances of a verb in all con-texts, and use our model to rank these paraphrasecandidates in specific contexts.
Table 1 shows twoinstances of the target verb acquire together withits paraphrases in the gold standard as an example.The paraphrases are attached with weights, whichcorrespond to the number of times they have beengiven by different annotators.4.2 Evaluation metricsTo evaluate the performance of our method we usegeneralized average precision (Kishida, 2005), a1We use version 1.6 of the parser.
We modify the depen-dency trees by ?folding?
prepositions into the edge labels tomake the relation between a head word and the head noun ofa prepositional phrase explicit.variant of average precision.Average precision (Buckley and Voorhees, 2000)is a measure commonly used to evaluate systemsthat return ranked lists of results.
Generalized aver-age precision (GAP) additionally rewards the cor-rect order of positive cases w.r.t.
their gold standardweight.
We define average precision first:AP =?ni=1xi piRpi =?ik=1xkiwhere xi is a binary variable indicating whetherthe ith item as ranked by the model is in the goldstandard or not, R is the size of the gold standard,and n is the number of paraphrase candidates tobe ranked.
If we take xi to be the gold standardweight of the ith item or zero if it is not in thegold standard, we can define generalized averageprecision as follows:GAP =?ni=1 I(xi) pi?Ri=1 I(yi)yiwhere I(xi) = 1 if xi is larger than zero, zero oth-erwise, and yi is the average weight of the idealranked list y1, .
.
.
,yi of gold standard paraphrases.As a second scoring method, we use precisionout of ten (P10).
The measure is less discriminativethan GAP.
We use it because we want to compareour model with E&P.
P10 measures the percentageof gold-standard paraphrases in the top-ten list ofparaphrases as ranked by the system, and can bedefined as follows (McCarthy and Navigli, 2007):P10 =?s?M?G f (s)?s?G f (s),where M is the list of 10 paraphrase candidates top-ranked by the model, G is the corresponding anno-tated gold-standard data, and f (s) is the weight ofthe individual paraphrases.4.3 Experiment 1: Verb paraphrasesIn our first experiment, we consider verb para-phrases using the same controlled subset of the952lexical substitution task data that had been used byTDP in an earlier study.
We compare our modelto various baselines and the models of TDP andE&P, and show that our new model substantiallyoutperforms previous work.Dataset.
The dataset is identical to the one usedby TDP and has been constructed in the same wayas the dataset used by E&P: it contains those gold-standard instances of verbs that have?accordingto the analyses produced by the MiniPar parser(Lin, 1993)?an overtly realized subject and object.Gold-standard paraphrases that do not occur in theparsed British National Corpus are removed.2 Intotal, the dataset contains 162 instances for 34 dif-ferent verbs.
On average, target verbs have 20.5substitution candidates; for individual instances ofa target verb, an average of 3.9 of the substitutioncandidates are annotated as correct paraphrases.Below, we will refer to this dataset as ?LST/SO.
?Experimental procedure.
To compute the vec-tor space, we consider only a subset of the completeset of dependency triples extracted from the parsedGigaword corpus.
We experimented with variousstrategies, and found that models which considerall dependency triples exceeding certain pmi- andfrequency thresholds perform best.Since the dataset is rather small, we use a four-fold cross-validation method for parameter tuning:We divide the dataset into four subsets, test vari-ous parameter settings on one subset and use theparameters that perform best (in terms of GAP) toevaluate the model on the three other subsets.
Weconsider the following parameters: pmi-thresholdsfor the dependency triples used in the computa-tion of the first- and second-order vectors, andfrequency thresholds.
The parameters differ onlyslightly between the four subsets, and the generaltendency is that good results are obtained if a lowpmi-threshold (?
2) is applied to filter dependencytriples used in the computation of the second-ordervectors, and a relatively high pmi-threshold (?
4)to filter dependency triples in the computation ofthe first-order vectors.
Good performing frequencythresholds are 10 or 15.
The threshold values forcontext vectors are slightly different: a mediumpmi-threshold between 2 and 4 and a low frequencythreshold of 3.To rank paraphrases in context, we compute con-textualized vectors for the verb in the input sen-2Both TDP and E&P use the British National Corpus.tence, i.e., a second order vector for the verb thatis contextually constrained by the first order vec-tors of all its arguments, and compare them to theunconstrained (second-order) vectors of each para-phrase candidate, using cosine similarity.3 For thefirst sentence in Table 1, for example, we compute[[acquireSUBJ:student,OBJ:knowledge]] and compare it to[[gain]], [[amass]], [[buy]], [[purchase]] and so on.Baselines.
We evaluate our model against a ran-dom baseline and two variants of our model: Onevariant (?2nd order uncontexualized?)
simply usescontextually unconstrained second-order vectorsto rank paraphrase candidates.
Comparing the fullmodel to this variant will show how effective ourmethod of contextualizing vectors is.
The sec-ond variant (?1st order contextualized?)
representsverbs in context by their first order vectors thatspecify how often the verb co-occurs with its argu-ments in the parsed Gigaword corpus.
We compareour model to this baseline to demonstrate the bene-fit of (contextualized) second-order vectors.
As forthe full model, we use pmi values rather than rawfrequency counts as co-occurrence statistics.Results.
For the LST/SO dataset, the generalizedaverage precision, averaged over all instances in thedataset, is 45.94%, and the average P10 is 73.11%.Table 2 compares our model to the random base-line, the two variants of our model, and previouswork.
As can be seen, our model improves about8% in terms of GAP and almost 7% in terms ofP10 upon the two variants of our model, which inturn perform 10% above the random baseline.
Weconclude that both the use of second-order vectors,as well as the method used to contextualize them,are very effective for the task under consideration.The table also compares our model to the modelof TDP and two different instantiations of E&P?smodel.
The results for these three models are citedfrom Thater et al (2009).
We can observe thatour model improves about 9% in terms of GAPand about 7% in terms of P10 upon previous work.Note that the results for the E&P models are based3Note that the context information is the same for bothwords.
With our choice of pointwise multiplication for thecomposition operator ?
we have (~v1?~w) ?~v2 =~v1 ?
(~v2?~w).Therefore the choice of which word is contextualized does notstrongly influence their cosine similarity, and contextualizingboth should not add any useful information.
On the contrarywe found that it even lowers performance.
Although thiscould be repaired by appropriately modifying the operator ?,for this experiment we stick with the easier solution of onlycontextualizing one of the words.953Model GAP P10Random baseline 26.03 54.25E&P (add, object) 29.93 66.20E&P (min, subject & object) 32.22 64.86TDP 36.54 63.321st order contextualized 36.09 59.352nd order uncontextualized 37.65 66.32Full model 45.94 73.11Table 2: Results of Experiment 1on a reimplementation of E&P?s original model?the P10-scores reported by Erk and Pad?
(2009)range between 60.2 and 62.3, over a slightly lowerrandom baseline.According to a paired t-test the differences arestatistically significant at p < 0.01.Performance on the complete dataset.
To findout how our model performs on less controlleddatasets, we extracted all instances from the lexicalsubstitution task dataset with a verb target, exclud-ing only instances which could not be parsed bythe Stanford parser, or in which the target was mis-tagged as a non-verb by the parser.
The resultingdataset contains 496 instances.
As for the LST/SOdataset, we ignore all gold-standard paraphrasesthat do not occur in the parsed (Gigaword) corpus.If we use the best-performing parameters fromthe first experiment, we obtain a GAP score of45.17% and a P10-score of 75.43%, compared torandom baselines of 27.42% (GAP) and 58.83%(P10).
The performance on this larger dataset isthus almost the same compared to our results forthe more controlled dataset.
We take this as evi-dence that our model is quite robust w.r.t.
differentrealizations of a verb?s subcategorization frame.4.4 Experiment 2: Non-verb paraphrasesWe now apply our model to parts of speech (POS)other than verbs.
The main difference betweenverbs on the one hand, and nouns, adjectives, andadverbs on the other hand, is that verbs typicallycome with a rich context?subject, object, and soon?while non-verbs often have either no depen-dents at all or only closed class dependents such asdeterminers which provide only limited contextualinformations, if any at all.
While we can apply thesame method as before also to non-verbs, we mightexpect it to work less well due to limited contextualPOS Instances M1 M2 BaselineNoun 535 46.38 42.54 30.01Adj 508 39.41 43.21 28.32Adv 284 48.19 51.43 37.25Table 3: GAP-scores for non-verb paraphrases us-ing two different methods.information.We therefore propose an alternative method torank non-verb paraphrases: We take the second-order vector of the target?s head and contextuallyconstrain it by the first order vector of the target.For instance, if we want to rank the paraphrasecandidates hint and star for the noun lead in thesentence(1) Meet for coffee early, swap leads and get per-mission to contact if possible.we compute [[swapOBJ:lead]] and compare it to thelifted first-order vectors of all paraphrase candi-dates, LOBJ([hint]) and LOBJ([star]), using cosinesimilarity.To evaluate the performance of the two methods,we extract all instances from the lexical substitutiontask dataset with a nominal, adjectival, or adverbialtarget, excluding instances with incorrect parse orno parse at all.
As before, we ignore gold-standardparaphrases that do not occur in the parsed Giga-word corpus.The results are shown in Table 3, where ?M1?refers to the method we used before on verbs, and?M2?
refers to the alternative method describedabove.
As one can see, M1 achieves better resultsthan M2 if applied to nouns, while M2 is betterthan M1 if applied to adjectives and adverbs.
Thesecond result is unsurprising, as adjectives and ad-verbs often have no dependents at all.We can observe that the performance of ourmodel is similarly strong on non-verbs.
GAP scoreson nouns (using M1) and adverbs are even higherthan those on verbs.
We take these results to showthat our model can be successfully applied to allopen word classes.5 Experiment: Ranking Word SensesIn this section, we apply our model to a differentword sense ranking task: Given a word w in context,the task is to decide to what extent the different954WordNet (Fellbaum, 1998) senses of w apply tothis occurrence of w.Dataset.
We use the dataset provided by Erk andMcCarthy (2009).
The dataset contains ordinaljudgments of the applicability of WordNet senseson a 5 point scale, ranging from completely differ-ent to identical for eight different lemmas in 50different sentential contexts.
In this experiment,we concentrate on the three verbs in the dataset:ask, add and win.Experimental procedure.
Similar to Pennac-chiotti et al (2008), we represent different wordsenses by the words in the corresponding synsets.For each word sense, we compute the centroid ofthe second-order vectors of its synset members.Since synsets tend to be small (they even may con-tain only the target word itself), we additionallyadd the centroid of the sense?s hypernyms, scaleddown by the factor 10 (chosen as a rough heuristicwithout any attempt at optimization).We apply the same method as in Section 4.3:For each instance in the dataset, we compute thesecond-order vector of the target verb, contextuallyconstrain it by the first-order vectors of the verb?sarguments, and compare the resulting vector tothe vectors that represent the different WordNetsenses of the verb.
The WordNet senses are thenranked according to the cosine similarity betweentheir sense vector and the contextually constrainedtarget verb vector.To compare the predicted ranking to the gold-standard ranking, we use Spearman?s ?
, a standardmethod to compare ranked lists to each other.
Wecompute ?
between the similarity scores averagedover all three annotators and our model?s predic-tions.
Based on agreement between human judges,Erk and McCarthy (2009) estimate an upper bound?
of 0.544 for the dataset.Results.
Table 4 shows the results of our exper-iment.
The first column shows the correlation ofour model?s predictions with the human judgmentsfrom the gold-standard, averaged over all instances.All correlations are significant (p< 0.001) as testedby approximate randomization (Noreen, 1989).The second column shows the results of afrequency-informed baseline, which predicts theranking based on the order of the senses in Word-Net.
This (weakly supervised) baseline outper-forms our unsupervised model for two of the threeverbs.
As a final step, we explored the effect ofWord Present paper WN-Freq Combinedask 0.344 0.369 0.431add 0.256 0.164 0.270win 0.236 0.343 0.381average 0.279 0.291 0.361Table 4: Correlation of model predictions and hu-man judgmentscombining our rankings with those of the frequencybaseline, by simply computing the average ranksof those two models.
The results are shown in thethird column.
Performance is significantly higherthan for both the original model and the frequency-informed baseline.
This shows that our model cap-tures an additional kind of information, and thuscan be used to improve the frequency-based model.6 ConclusionWe have presented a novel method for adaptingthe vector representations of words according totheir context.
In contrast to earlier approaches, ourmodel incorporates detailed syntactic information.We solved the problems of data sparseness andincompatibility of dimensions which are inherent inthis approach by modeling contextualization as aninterplay between first- and second-order vectors.Evaluating on the SemEval 2007 lexical substitu-tion task dataset, our model performs substantiallybetter than all earlier approaches, exceeding thestate of the art by around 9% in terms of general-ized average precision and around 7% in terms ofprecision out of ten.
Also, our system is the first un-supervised method that has been applied to Erk andMcCarthy?s (2009) graded word sense assignmenttask, showing a substantial positive correlation withthe gold standard.
We further showed that a weaklysupervised heuristic, making use of WordNet senseranks, can be significantly improved by incorporat-ing information from our system.We studied the effect that context has on targetwords in a series of experiments, which vary thetarget word and keep the context constant.
A natu-ral objective for further research is the influence ofvarying contexts on the meaning of target expres-sions.
This extension might also shed light on thestatus of the modelled semantic process, which wehave been referring to in this paper as ?contextu-alization?.
This process can be considered one of955mutual disambiguation, which is basically the viewof E&P.
Alternatively, one can conceptualize it assemantic composition: in particular, the head of aphrase incorporates semantic information from itsdependents, and the final result may to some extentreflect the meaning of the whole phrase.Another direction for further study will be thegeneralization of our model to larger syntactic con-texts, including more than only the direct neighborsin the dependency graph, ultimately incorporatingcontext information from the whole sentence in arecursive fashion.Acknowledgments.
We would like to thank Ed-uard Hovy and Georgiana Dinu for inspiring discus-sions and helpful comments.
This work was sup-ported by the Cluster of Excellence ?MultimodalComputing and Interaction?, funded by the Ger-man Excellence Initiative, and the project SALSA,funded by DFG (German Science Foundation).ReferencesChris Buckley and Ellen M. Voorhees.
2000.
Evaluat-ing evaluation measure stability.
In Proceedings ofthe 23rd Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 33?40, Athens, Greece.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation, mutual information and lexicography.Computational Linguistics, 16(1):22?29.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the fifth international conference onLanguage Resources and Evaluation (LREC 2006),pages 449?454, Genoa, Italy.Dmitriy Dligach and Martha Palmer.
2008.
Novel se-mantic features for verb sense disambiguation.
InProceedings of ACL-08: HLT, Short Papers, pages29?32, Columbus, OH, USA.Katrin Erk and Diana McCarthy.
2009.
Graded wordsense assignment.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 440?449, Singapore.Katrin Erk and Sebastian Pad?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Honolulu,HI, USA.Katrin Erk and Sebastian Pad?.
2009.
Paraphrase as-sessment in structured vector space: Exploring pa-rameters and datasets.
In Proc.
of the Workshopon Geometrical Models of Natural Language Seman-tics, Athens, Greece.Christiane Fellbaum, editor.
1998.
Wordnet: An Elec-tronic Lexical Database.
Bradford Book.Walter Kintsch.
2001.
Predication.
Cognitive Science,25:173?202.Kazuaki Kishida.
2005.
Property of average precisionand its generalization: An examination of evaluationindicator for information retrieval experiments.
NIITechnical Report.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104(2):211?240.Dekang Lin.
1993.
Principle-based parsing withoutovergeneration.
In Proceedings of the 31st AnnualMeeting of the Association for Computational Lin-guistics, pages 112?120, Columbus, OH, USA.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics, Volume 2, pages 768?774.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Diana McCarthy and John Carroll.
2003.
Disam-biguating nouns, verbs, and adjectives using auto-matically acquired selectional preferences.
Compu-tational Linguistics, 29(4):639?654.Diana McCarthy and Roberto Navigli.
2007.
SemEval-2007 Task 10: English Lexical Substitution Task.
InProc.
of SemEval, Prague, Czech Republic.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof ACL-08: HLT, pages 236?244, Columbus, OH,USA.Richard Montague.
1973.
The proper treatment ofquantification in ordinary English.
In Jaakko Hin-tikka, Julius Moravcsik, and Patrick Suppes, editors,Approaches to Natural Language, pages 221?242.Dordrecht.Eric W. Noreen.
1989.
Computer-intensive Methodsfor Testing Hypotheses: An Introduction.
John Wi-ley and Sons Inc.Sebastian Pad?
and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Marco Pennacchiotti, Diego De Cao, Roberto Basili,Danilo Croce, and Michael Roth.
2008.
Automaticinduction of framenet lexical units.
In Proceedingsof the 2008 Conference on Empirical Methods inNatural Language Processing, pages 457?465, Hon-olulu, HI, USA.956Hinrich Sch?tze.
1998.
Automatic word sense discrim-ination.
Computational Linguistics, 24(1):97?124.Stefan Thater, Georgiana Dinu, and Manfred Pinkal.2009.
Ranking paraphrases in context.
In Proceed-ings of the 2009 Workshop on Applied Textual Infer-ence, pages 44?47, Singapore.957
