Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 31?41,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsStatistical Machine Translation Features with Multitask Tensor NetworksHendra Setiawan, Zhongqiang Huang, Jacob Devlin?
?, Thomas Lamar,Rabih Zbib, Richard Schwartz and John MakhoulRaytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA?Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA{hsetiawa,zhuang,tlamar,rzbib,schwartz,makhoul}@bbn.comjdevlin@microsoft.comAbstractWe present a three-pronged approach toimproving Statistical Machine Translation(SMT), building on recent success in theapplication of neural networks to SMT.First, we propose new features based onneural networks to model various non-local translation phenomena.
Second, weaugment the architecture of the neural net-work with tensor layers that capture im-portant higher-order interaction among thenetwork units.
Third, we apply multitasklearning to estimate the neural networkparameters jointly.
Each of our proposedmethods results in significant improve-ments that are complementary.
The over-all improvement is +2.7 and +1.8 BLEUpoints for Arabic-English and Chinese-English translation over a state-of-the-artsystem that already includes neural net-work features.1 IntroductionRecent advances in applying Neural Networks toStatistical Machine Translation (SMT) have gen-erally taken one of two approaches.
They ei-ther develop neural network-based features thatare used to score hypotheses generated from tra-ditional translation grammars (Sundermeyer et al,2014; Devlin et al, 2014; Auli et al, 2013; Leet al, 2012; Schwenk, 2012), or they implementthe whole translation process as a single neu-ral network (Bahdanau et al, 2014; Sutskever etal., 2014).
The latter approach, sometimes re-ferred to as Neural Machine Translation, attemptsto overhaul SMT, while the former capitalizes onthe strength of the current SMT paradigm andleverages the modeling power of neural networksto improve the scoring of hypotheses generated?
* Research conducted when the author was at BBN.by phrase-based or hierarchical translation rules.This paper adopts the former approach, as n-bestscores from state-of-the-art SMT systems oftensuggest that these systems can still be significantlyimproved with better features.We build on (Devlin et al, 2014) who proposeda simple yet powerful feedforward neural networkmodel that estimates the translation probabilityconditioned on the target history and a large win-dow of source word context.
We take advantageof neural networks?
ability to handle sparsity, andto infer useful abstract representations automati-cally.
At the same time, we address the challengeof learning the large set of neural network param-eters.
In particular,?
We develop new Neural Network Featuresto model non-local translation phenomenarelated to word reordering.
Large fully-lexicalized contexts are used to model thesephenomena effectively, making the use ofneural networks essential.
All of the featuresare useful individually, and their combinationresults in significant improvements (Section2).?
We use a Tensor Neural Network Architecture(Yu et al, 2012) to automatically learn com-plex pairwise interactions between the net-work nodes.
The introduction of the tensorhidden layer results in more powerful fea-tures with lower model perplexity and signif-icantly improved MT performance for all ofneural network features (Section 3).?
We apply Multitask Learning (MTL) (Caru-ana, 1997) to jointly train related neural net-work features by sharing parameters.
Thisallows parameters learned for one feature tobenefit the learning of the other features.
Thisresults in better trained models and achievesadditional MT improvements (Section 4).We apply the resulting Multitask Tensor Net-works to the new features and to existing ones,31obtaining strong experimental results over thestrongest previous results of (Devlin et al, 2014).We obtain improvements of +2.5 BLEU pointsfor Arabic-English and +1.8 BLEU points forChinese-English on the DARPA BOLT Web Fo-rum condition.
We also obtain improvements of+2.7 BLEU point for Arabic-English and +1.9BLEU points for Chinese-English on the NISTOpen12 test sets over the best previously pub-lished results in (Devlin et al, 2014).
Both thetensor architecture and multitask learning are gen-eral techniques that are likely to benefit other neu-ral network features.2 New Non-Local SMT FeaturesExisting SMT features typically focus on local in-formation in the source sentence, in the target hy-pothesis, or both.
For example, the n-gram lan-guage model (LM) predicts the next target wordby using previously generated target words as con-text (local on target), while the lexical translationmodel (LTM) predicts the translation of a sourceword by taking into account surrounding sourcewords as context (local on source).In this work, we focus on non-local transla-tion phenomena that result from non-monotone re-ordering, where local context becomes non-localon the other side.
We propose a new set of power-ful MT features that are motivated by this simpleidea.
To facilitate the discussion, we categorize thefeatures into hypothesis-enumerating features thatestimates a probability for each generated targetword (e.g., n-gram language model), and source-enumerating features that estimates a probabilityfor each source word (e.g., lexical translation).More concretely, we introduce a) Joint Modelwith Offset Source Context (JMO), a hypothesisenumerating feature that predicts the next targetword the source context affiliated to the previoustarget words; and b) Translation Context Model(TCM), a source-enumerating feature that predictsthe context of the translation of a source wordrather than the translation itself.
These two mod-els extend pre-existing features: the Joint (lan-guage and translation) Model (JM) of (Devlin etal., 2014) and the LTM respectively respectively.We use a large lexicalized context for there fea-tures, making the choice of implementing them asneural networks essential.
We also present neural-network implementations of pre-existing source-enumerating features: lexical translation, orien-tation and fertility models.
We obtain additionalgains from using tensor networks and multitasklearning in the modeling and training of all the fea-tures.2.1 Hypothesis-Enumerating FeaturesAs mentioned, hypothesis-enumerating featuresscore each word in the hypothesis, typically byconditioning it on a context of n-1 previous tar-get words as in the n-gram language model.
Onerecent such model, the joint model of Devlin et al(2014) achieves large improvements to the state-of-the-art SMT by using a large context windowof 11 source words and 3 target words.
The JointModel with Offset Source Context (JMO) is anextension of the JM that uses the source wordsaffiliated with the n-gram target history as con-text.
The source contexts of JM and JMO over-lap highly when the translation is monotone, butare complementary when the translation requiresword reordering.2.1.1 Joint Model with Offset Source ContextFormally, JMO estimates the probability of the tar-get hypothesis E conditioned on the source sen-tence F and a target-to-source affiliation A:P (E|F,A) ?|E|?i=1P (ei|ei?n+1i?1, Cai?k= fai?k+mai?k?m)where eiis the word being predicted; ei?n+1i?1is thestring of n?
1 previously generated words; Cai?kto the source context of m source words aroundfai?k, the source word affiliated with ei?k.
Werefer to k as the offset parameter.
We use the def-inition of word affiliation introduced in Devlin etal.
(2014).
When no source context is used, themodel is equivalent to an n-gram language model,while an offset parameter of k = 0 reduces themodel to the JM of Devlin et al (2014).When k > 0, the JMO captures non-local con-text in the prediction of the next target word.
Morespecifically, ei?kand ei, which are local on thetarget side, are affiliated to fai?kand faiwhichmay be distant from each other on the source sidedue to non-monotone translation, even for k = 1.The offset model captures reordering constraintsby encouraging the predicted target word eito fitwell with the previous affiliated source word fai?kand its surrounding words.
We implement a sep-arate feature for each value of k, and later train32them jointly via multitask learning.
As our ex-periments in Section 5.2.1 confirm, the history-affiliated source context results in stronger SMTimprovement than just increasing the number ofsurrounding words in JM.Fig.
1 illustrates the difference between JMOand JM.
Assuming n = 3 and m = 1, then JMestimates P (e5|e4, e3, Ca5= {f6, f7, f8}).
Onthe other hand, for k = 1 , JMOk=1estimatesP (e5|e4, e3, Ca4= {f8, f9, f10}).f9f5.
.
.e5 e6e4 e7e3.
.
.
.
.
.C7 = Ca5.
.
.?
??
?f6 f7 f8Figure 1: Example to illustrate features.
f95is thesource segment, e73is the corresponding transla-tion and lines refer to the alignment.
We showhypothesis-enumerating features that look at f7and source-enumerating features that look at e5.We surround the source words affiliated with e5and its n-gram history with a bracket, and sur-round the source words affiliated with the historyof e5with squares.2.2 Source-Enumerating FeaturesSource-Enumerating Features iterate over wordsin the source sentence, including unaligned words,and assign it a score depending on what as-pect of translation they are modeling.
A source-enumerating feature can be formulated as follows:P (E|F,A) ?|F |?j=1P (Yj|Cj= fj+mj?m)where Cajis the source context (similar to thehypothesis-enumerating features above) and Yjis the label being predicted by the feature.
Wefirst describe pre-existing source-enumerating fea-tures: the lexical translation model, the orientationmodel and the fertility model, and then discuss anew feature: Translation Context Model (TCM),which is an extension of the lexical translationmodel.2.2.1 Pre-existing FeaturesLexical Translation model (LTM) estimates theprobability of translating a source word fjto a tar-get word l(fj) = ebjgiven a source context Cj,bj?
B is the source-to-target word affiliation asdefined in (Devlin et al, 2014).
When fjis trans-lated to more than one word, we arbitrarily keepthe left-most one.
The target word vocabulary Vis extended with a NULL token to accommodateunaligned source words.Orientation model (ORI) describes the proba-bility of orientation of the translation of phrasessurrounding a source word fjrelative to its owntranslation.
We follow (Setiawan et al, 2013)in modeling the orientation of the left and rightphrases of fjwith maximal orientation span (thelongest neighboring phrase consistent with align-ment), which we denote by Ljand Rjrespec-tively.
Thus, o(fj) = ?oLj(fj), oRj(fj)?, whereoLjand oRjrefer to the orientation of Ljand Rjrespectively.
For unaligned fj, we set o(fj) =oLj(Rj), the orientation of Rjwith respect to Lj.Fertility model (FM) models the probability thata source word fjgenerates ?
(fj) words in thehypothesis.
Our implemented model only dis-tinguishes between aligned and unaligned sourcewords (i.e., ?
(fj) ?
{0, 1}).
The generalization ofthe model to account for multiple values of ?
(fi)is straightforward.2.2.2 Translation Context ModelAs with JMO in Section 2.1.1, we aim to cap-ture translation phenomena that appear local onthe target hypothesis but non-local on the sourceside.
Here, we do so by extending the LTMfeature to predict not only the translated wordebj, but also its surrounding context.
For-mally, we model P (l(fj)|Cj), where l(fj) =ebj?d, ?
?
?
, ebj, ?
?
?
ebj+dis the hypothesis wordwindow around ebj.
In practice, we decomposeTCM further into+d?d?=?dP (ebj+d?|Cj) and imple-mented each as a separate neural network-basedfeature.
Note that TCM is equivalent to the LTMwhen d = 0.
Because of word reordering, a givenhypothesis word in l(fj) might not be affiliatedwith fjor even to the words in Cj.
TCM can modelnon-local information in this way.2.2.3 Combined ModelSince the feature label is undefined for unalignedsource words, we make the model hierarchical,based on whether the source word is aligned or33not, and thus arrive at the following formulation:P (l(fj)) ?
P (ori(fj)) ?
P (?
(fj)) =??????????
?P (?p(fj) = 0) ?
P (oLj(Rj))P (?p(fj) ?
1) ?+d?d?=?dP (ebj+d?
)?P (oLj(fj), oRj(fj))We dropped the common context (Cj) for readabil-ity.We reuse Fig.
1 to illustrate the source-enumerating features.
Assuming d = 1, the scoresassociated with f7are P (?
(f7) ?
1|C7) for theFM; P (e4|C7) ?P (e5|C7) ?P (e6)|C7) for the TCM;and P (o(f7) = ?oL7(f7) = RA, oR7(f7) = RA?
)for the ORI(RA refers to Reverse Adjacent).
L7and R7(i.e.
f6and f98respectively), the longestneighboring phrase of f7, are translated in reverseorder and adjacent to e5.3 Tensor Neural NetworksThe second part of this work improves SMT byimproving the neural network architecture.
NeuralNetworks derive their strength from their ability tolearn a high-level representation of the input auto-matically from data.
This high-level representa-tion is typically constructed layer by layer througha weighted sum linear operation and a non-linearactivation function.
With sufficient training data,neural networks often achieve state-of-the-art per-formance on many tasks.
This stands in sharp con-trast to other algorithms that require tedious man-ual feature engineering.
For the features presentedin this paper, the context words are fed to the net-work network with minimal engineering.We further strengthen the network?s ability tolearn rich interactions between its units by intro-ducing tensors in the hidden layers.
The multi-plicative property of the tensor bares a close re-semblance to collocation of context words whichare useful in many natural language processingtasks.In conventional feedforward neural networks,the output of hidden layer l is produced by mul-tiplying the output vector from the previous layerwith a weight matrix (Wl) and then applying theactivation function ?
to the product.
Tensor Neu-ral Networks generalize this formulation by usinga tensor Ulof order 3 for the weights.
The outputof node k in layer l is computed as follows:hl[k] = ?(hl?1?
Ul[k] ?
hTl?1)where Ul[k], the k-th slice of Ul, is a square ma-trix.In our implementation, we follow (Yu et al,2012; Hutchinson et al, 2013) and use a low-rankapproximation of Ul[k] = Ql[k] ?
Rl[k]T, whereQl[k], Rl[k] ?
Rn?r.
The output of node k be-comes:hl[k] = ?
(hl?1?Ql[k] ?Rl[k]T?
hTl?1)In our experiments, we choose r = 1, and alsoapply the non-linear activation function ?
distribu-tively.
We arrive at the following three equationsfor computing the hidden layer outputs (0 < l <L):vl= ?
(hl?1?Ql)v?l= ?
(hl?1?Rl)hl= vl?
v?lwhere hl?1is double-projected to vland v?l,and the two projections are merged using theHadamard element-wise product operator ?.This formulation allows us to use the same in-frastructure of the conventional neural networksby projecting the previous layer to two differentspaces of the same dimensions, then multiply-ing them element-wise.
The only component thatis different from conventional feedforward neuralnetworks is the multiplicative function, which istrivially differentiable with respect to the learnableparameters.
Figure 3(b) illustrates the tensor ar-chitecture for two hidden layers.The tensor network can learn collocation fea-tures more easily.
For example, it can learn a col-location feature that is activated only if hl?1[i] col-locates with hl?1[j] by setting Ul[k][i][j] to somepositive number.
This results in SMT improve-ments as we describe in Section 5.4 Multitask LearningThe third part of this paper addresses the challengeof effectively learning a large number of neuralnetwork parameters without overfitting.
The chal-lenge is even larger for tensor network since theypractically doubles the number of parameters.
Inthis section, we propose to apply Multitask Learn-ing (MTL) to partially address this issue.
We im-plement MTL as parameter sharing among the net-works.
This effectively reduces the number of pa-rameters, and more importantly, it takes advan-tage of parameters learned for one feature to better34Inputh1h2Inputh1?v1 v?1h2?v2 v?2Inputh1?v1 v?1h12?v12 v?12hM2?vM2 v?M2?
?
??
?
?W1W2Q1 R1R2Q2R1Q1Q12 R12 QM2 RM2(a) (b) (c)OutputOutput Task 1Task MFigure 2: The network architecture for (a) a conventional feedforward neural network, (b) tensor hiddenlayers, and (c) multitask learning with M features that share the embedding and first hidden layers(t = 1).learn the parameters of the other features.
Anotherway of looking at this is that MTL facilitates reg-ularization through learning the other tasks.MTL is suitable for SMT features as they modeldifferent but closely related aspects of the sametranslation process.
MTL has long been used bythe wider machine learning community (Caruana,1997) and more recently for natural language pro-cessing (Collobert and Weston, 2008; Collobertet al, 2011).
The application of MTL to ma-chine translation, however, has been much less re-stricted, which is rather surprising since SMT fea-tures arise from the same translation task and arenaturally related.We apply MTL for the features described inSection 2.
We design all the features to also sharethe same neural network architecture (in this case,the tensor architecture described in Section 3) andthe same input, thus resulting in two large neuralnetworks: one for the hypothesis-enumerating fea-tures and another for the source-enumerating ones.This simplifies the implementation of MTL.
Us-ing this setup, it is possible to vary the numberof shared hidden layers t from 0 (only sharing theembedding layer) to L ?
1 (sharing all the layersexcept the output).
Note that in principle MTL isapplicable to other set of networks that have differ-ent architecture or even different input set.
WithMTL, the training procedure is the same as that ofstandard neural networks.We use the back propagation algorithm, and useas the loss function the product of likelihood ofeach feature1:1In this and in the other parts of the paper, we add thenormalization regularization term described in (Devlin et al,2014) to the loss function to avoid computing the normaliza-tion constant at model query/decoding time.Loss =?iM?jlog (P (Yj(Xi)))where Xiis the training sample and Yjis one oftheM models trained.
We use the sum of log like-lihoods since we assume that the features are inde-pendent.Fig.
3(c) illustrates MTL between M modelssharing the input embedding layer and the firsthidden layer (t = 1) compared to the separately-trained conventional feedforward neural networkand tensor neural network.5 ExperimentsWe demonstrate the impact of our work with ex-tensive MT experiments on Arabic-English andChinese-English translation for the DARPA BOLTWeb Forum and the NIST OpenMT12 conditions.5.1 Baseline MT SystemWe run our experiments using a state-of-the-artstring-to-dependency hierarchical decoder (Shenet al, 2010).
The baseline we use includes a setof powerful features as follow:?
Forward and backward rule probabilities?
Contextual lexical smoothing (Devlin, 2009)?
5-gram Kneser-Ney LM?
Dependency LM (Shen et al, 2010)?
Length distribution (Shen et al, 2010)?
Trait features (Devlin and Matsoukas, 2012)?
Factored source syntax (Huang et al, 2013)?
Discriminative sparse feature, totaling 50kfeatures (Chiang et al, 2009)?
Neural Network Joint Model (NNJM) andNeural Network Lexical Translation Model35(NNLTM) (Devlin et al, 2014)As shown, our baseline system already includesneural network-based features.
NNJM, NNLTMand use two hidden layers with 500 units and useembedding of size 200 for each input.We use the MADA-ARZ tokenizer (Habash etal., 2013) for Arabic word tokenization.
For Chi-nese tokenization, we use a simple longest-match-first lexicon-based approach.
We align the trainingdata using GIZA++ (Och and Ney, 2003).
For tun-ing the weights of MT features including the newfeatures, we use iterative k-best optimization withan ExpectedBLEU objective function (Rosti et al,2010), and decode the test sets after 5 tuning iter-ation.
We report the lower-cased BLEU and TERscores.5.2 BOLT Discussion ForumThe bulk of our experiments is on the BOLT WebDiscussion Forum domain, which uses data col-lected by the LDC.
The parallel training data con-sists of all of the high-quality NIST training cor-pora, plus an additional 3 million words of trans-lated forum data.
The tuning and test sets consistof roughly 5000 segments each, with 2 indepen-dent references for Arabic and 3 for Chinese.5.2.1 Effects of New FeaturesWe first look at the effects of the proposed featurescompared to the baseline system.
Table 1 summa-rizes the primary results of the Arabic-English andChinese-English experiments for the BOLT condi-tion.
We show the experimental results related tohypothesis-enumerating features (HypEn) in rowsS2-S5, those related to source-enumerating fea-tures (SrcEn) in rows S6-S9, and the combinationof the two in row S10.
For all the features, we setthe source context length to m = 5 (11-word win-dow).
For JM and JMO, we set the target contextlength to n = 4.
For the offset parameter k ofJMO, we use values 1 to 3.
For TCM, we modelone word around the translation (d = 1).
Largervalues of d did not result in further gains.
Thebaseline is comparable to the best results of (De-vlin et al, 2014).In rows S3to S5, we incrementally add a modelwith different offset source context, from k = 1to k = 3.
For AR-EN, adding JMOs with differ-ent offset source context consistently yields pos-itive effects in BLEU score, while in ZH-EN, ityields positive effects in TER score.
Utilizing alloffset source contexts ?+JMOk?3?
(row S5) yieldsaround 0.9 BLEU point improvement in AR-ENand around 0.3 BLEU in ZH-EN compared tothe baseline.
The JMO consistently provides bet-ter improvement compared to a larger JM con-text (row S2), validating our hypothesis that usingoffset source context captures important non-localcontext.Rows S6to S9present the improvements thatresult from implementing pre-existing source-enumerating SMT features as neural networks,and highlight the contribution of our translationcontext model (TCM).
This set of experiments isorthogonal to the HypEn experiments (rows S2-S5).
Each pre-existing model has a modest pos-itive cumulative effect on both BLEU and TER.We see this result as further confirming the cur-rent trend of casting existing SMT features as neu-ral network since our baseline already containssuch features.
The next row present the resultsof adding the translation context model, with oneword surrounding the translation (d = 1).
Asshown, TCM yields a positive effect of around0.5 BLEU and TER improvements in AR-EN andaround 0.2 BLEU and TER improvements in ZH-EN.Separately, the set of source-enumerating fea-tures and the set of target-enumerating featuresproduce around 1.1 to 1.2 points BLEU gain inAR-EN and 0.3 to 0.5 points BLEU gain in ZH-EN.
The combination of the two sets produces acomplementary gain in addition to the gains of theindividual models as Row (S10) shows.
The com-bined gain improves to 1.5 BLEU points in AR-EN and 0.7 BLEU points in ZH-EN.SystemAR-EN ZH-ENBL TER BL TERS1: Baseline 43.2 45.0 30.2 58.3S2: S1+JMLC843.5 45.0 30.2 58.5S3: S1+JMOk=143.9 44.7 30.8 57.8S4: S3+JMOk=243.9 44.7 30.7 57.8S5: S4+JMOk=344.4 44.5 30.5 57.5S6: S1+LTM 43.5 44.7 30.3 58.0S7: S6+ORI 43.7 44.6 30.4 57.8S8: S7+FERT 43.8 44.7 30.5 57.8S9: S8+TCM 44.3 44.2 30.7 57.5S10: S9+JMOk?344.7 44.1 30.9 57.3Table 1: MT results of various model combinationin BLEU and in TER.365.2.2 Effects of Tensor Network andMultitask LearningWe first analyze the impact of tensor architectureand MTL intrinsically by reporting the models?average log-likelihood on the validation sets (asubset of the test set) in Table 2.
As mentioned, wegroup the models to HypEn (JM and JMOk?3) andSrcEn (LTM, ORI,FERT and TCM) as we performMTL on these two groups.
Likelihood of thesetwo groups in the previous subsection are in col-umn ?NN?
(for Neural Network), which serves asa baseline.
The application of the tensor architec-ture improves their likelihood as shown in column?Tensor?
for both languages and models.Feat.Independent MTLNN Tensor t = 0 t = 1L = 2 L = 3ARHypEn -8.85 -8.54 -8.35 -SrcEn -8.47 -8.32 -8.10 -8.09ZHHypEn -11.48 -11.06 -10.87 -SrcEn -10.77 -10.66 -10.54 -10.49Table 2: Sum of the average log-likelihood of themodels in HypEn and SrcEn.
t = 0 refers to MTLthat shares only the embedding layer, while t = 1shares the first hidden layer as well.
L refers to thenetwork?s depth.
Higher value is better.The likelihoods of the MTL-related experi-ments are in columns with ?MTL?
header.
Wepresent two set of results.
In the first set (col-umn ?MTL,t=0,L=2?
), we run MTL for featuresfrom column ?Tensor?
by sharing the embeddinglayer only (t = 0).
This allows us to isolatethe impact of MTL in the presence of Tensors.Column ?MTL,t=1,l=3?
corresponds to the exper-iment that produces the best intrinsic result, whereeach model uses Tensors with three hidden lay-ers (500x500x500, l = 3) and the models sharethe embedding and the first hidden layers (t = 1).MTL consistently gives further intrinsic gain com-pared to tensors.
More sharing provides an extragain for SrcEn as shown in the last column.
Notethat we only experiment with different l and t forSrcEn and not for HypEn because the models inHypEn have different input sets.
In our experi-ments, further sharing and more hidden layers re-sulted in no further gain.
In total, we see a consis-tent positive effect in intrinsic evaluation from thetensor networks and multitask learning.Moving on to MT evaluation, we summarize theexperiments showing the impact of Tensors andMTL in Table 3.
For MTL, we use L = 3, t = 2since it gives the best intrinsic score.
Employingtensors instead of regular neural networks gives asignificant and consistent positive impact for allmodels and language pairs.
For the system withthe baseline features, we use the tensor architec-ture for both the joint model and the lexical trans-lation model of Devlin et al resulting in an im-provement of around 0.7 BLEU points, and show-ing the wide applicability of the tensor architec-ture.
On top of this improved baseline, we also ob-serve an improvement of the same scale for othermodels (column ?Tensor?
), except for HypEn fea-tures in AR-EN experiment.
Moving to MTL ex-periments, we see improvements, especially fromSrcEn features.
MTL gives around 0.5 BLEUpoint improvement for AR-EN and around 0.4BLEU point for ZH-EN.
When we employ bothHypEn and SrcEn together, MTL gives around 0.4BLEU point in AR-EN and 0.2 BLEU point inZH-EN.
In total, our work results in an improve-ment of 2.5 BLEU point for AR-EN and 1.8 forBLEU point in ZH-EN on top of the best results in(Devlin et al, 2014).5.3 NIST OpenMT12Our NIST system is compatible with theOpenMT12 constrained track, which consists of10M words of high-quality parallel training forArabic, and 25M words for Chinese.
The n-gramLM is trained on 5B words of data from the En-glish GigaWord.
For test, we use the ?Arabic-To-English Original Progress Test?
(1378 segments)and ?Chinese-to-English Original Progress Test +OpenMT12 Current Test?
(2190 segments), whichconsists of a mix of newswire and web data.All test segments have 4 references.
Our tuningset contains 5000 segments, and is a mix of theMT02-05 eval set as well as additional held-outparallel data from the training corpora.We report the experiments for the NIST con-dition in Table 4.
In particular, we investigatethe impact of deploying our new features (column?Feat?)
and demonstrate the effects of the ten-sor architecture (column ?Tensor?)
and multitasklearning (column ?MTL?).
As shown the resultsare inline with the BOLT condition where we ob-serve additive improvements from adding our newfeatures, applying tensor network and multitasklearning.
On Arabic-English, we see a gain of 2.737Feature setAR-EN ZH-ENNN Tensor MTL NN Tensor MTLR1: Baseline Features 43.2 43.9 - 30.2 30.8 -R2: R1+ HypEn 44.4 44.4 44.5 30.5 31.5 31.3R3: R1+ SrcEn 44.3 44.9 45.5 30.7 31.5 31.9R4: R1+ HypEn + SrcEn 44.7 45.3 45.7 30.9 31.8 32.0Table 3: Experimental results to investigate the effects of the new features, DTN and MTL.
The toppart shows the BOLT results, while the bottom part shows the NIST results.
The best results for eachconditions and each language-pair are in bold.
The baselines are in italics.
.Base.
Feat Tensor MTLAR-EN 53.7 55.4 55.9 56.4mixed-case 51.8 53.1 53.7 54.1ZH-EN 36.6 37.8 38.2 38.5mixed-case 34.4 35.5 35.9 36.1Table 4: Experimental results for the NIST condi-tion.
Mixed-case scores are also reported.
Base-lines are in italics.BLEU point and on Chinese-English, we see a 1.9BLEU point gain.
We also report the mixed-casedBLEU scores for comparison with previous bestpublished results, i.e.
Devlin et al (2014) report52.8 BLEU for Arabic-English and 34.7 BLEU forChinese-English.
Thus, our results are around 1.3-1.4 BLEU point better.
Note that they use addi-tional rescoring features but we do not.6 Related WorkOur work is most closely related to Devlin et al(2014).
They use a simple feedforward neuralnetwork to model two important MT features: Ajoint language and translation model, and a lex-ical translation model.
They show very largeimprovements on Arabic-English and Chinese-English web forum and newswire baselines.
Weimprove on their work in 3 aspects.
First, wemodel more features using neural networks, in-cluding two novel ones: a joint model with off-set source context and a translation context model.Second, we enhance the neural network architec-ture by using tensor layers, which allows us tomodel richer interactions.
Lastly, we improve theperformance of the individual features by trainingthem using multitask learning.
In the remainderof this section, we describe previous work relat-ing to the three aspect of our work, namely MTmodeling, neural network architecture and modellearning.The features we propose in this paper addressthe major aspects of SMT modeling that haveinformed much of the research since the origi-nal IBM models (Brown et al, 1993): lexicaltranslation, reordering, word fertility, and lan-guage models.
Of particular relevance to our workare approaches that incorporate context-sensitivityinto the models (Carpuat and Wu, 2007), formu-late reordering as orientation prediction task (Till-man, 2004) and that use neural network languagemodels (Bengio et al, 2003; Schwenk, 2010;Schwenk, 2012), and incorporate source-side con-text into them (Devlin et al, 2014; Auli et al,2013; Le et al, 2012; Schwenk, 2012).Approaches to incorporating source context intoa neural network model differ mainly in how theyrepresent the source sentence and in how long isthe history they keep.
In terms of representa-tion of the source sentence, we follow (Devlin etal., 2014) in using a window around the affiliatedsource word.
To name some other approaches,Auli et al (2013) uses latent semantic analysis andsource sentence embeddings learned from the re-current neural network; Sundermeyer et al (2014)take the representation from a bidirectional LSTMrecurrent neural network; and Kalchbrenner andBlunsom (2013) employ a convolutional sentencemodel.
For target context, recent work has triedto look beyond the classical n-gram history.
(Auliet al, 2013; Sundermeyer et al, 2014) consideran unbounded history, at the expense of makingtheir model only applicable for N-best rescoring.Another recent line of research (Bahdanau et al,2014; Sutskever et al, 2014) departs more rad-ically from conventional feature-based SMT andimplements the MT system as a single neural net-work.
These models use a representation of thewhole input sentence.We use a feedforward neural network in thiswork.
Besides feedforward and recurrent net-38works, other network architectures that have beenapplied to SMT include convolutional networks(Kalchbrenner et al, 2014) and recursive networks(Socher et al, 2011).
The simplicity of feedfor-ward networks works to our advantage.
Morespecifically, due to the absence of a feedback loop,the feedforward architecture allows us to treatindividual decisions independently, which makesparallelization of the training easy and the query-ing the network at decoding time straightforward.The use of tensors in the hidden layers strengthensthe neural network model, allowing us to modelmore complex feature interactions like colloca-tion, which has been long recognized as impor-tant information for many NLP tasks (e.g.
wordsense disambiguation (Lee and Ng, 2002)).
Thetensor formulation we use is similar to that of(Yu et al, 2012; Hutchinson et al, 2013).
Ten-sor Neural Networks have a wide application inother field, but have only been recently applied inNLP (Socher et al, 2013; Pei et al, 2014).
Toour knowledge, our work is the first to use tensornetworks in SMT.Our approach to multitask learning is related towork that is often labeled joint training or transferlearning.
To name a few of these works, Finkeland Manning (2009) successfully train name en-tity recognizers and syntactic parsers jointly, andSingh et al (2013) train models for coreferenceresolution, named entity recognition and relationextraction jointly.
Both efforts are motivated bythe minimization of cascading errors.
Our workis most closely related to Collobert and Weston(2008; Collobert et al (2011), who apply multi-task learning to train neural networks for multi-ple NLP models: part-of-speech tagging, semanticrole labeling, named-entity recognition and lan-guage model variations.7 ConclusionThis paper argues that a relatively simple feedfor-ward neural network can still provides significantimprovement to Statistical Machine Translation(SMT).
We support this argument by presenting amulti-pronged approach that addresses modeling,architectural and learning aspects of pre-existingneural network-based SMT features.
More con-cretely, we paper present a new set of neuralnetwork-based SMT features to capture importanttranslation phenomena, extend feedforward neu-ral network with tensor layers, and apply multi-task learning to integrate the SMT features moretightly.
Empirically, all our proposals successfullyproduce an improvement over state-of-the-art ma-chine translation system for Arabic-to-English andChinese-to-English and for both BOLT web fo-rum and NIST conditions.
Building on the suc-cess of this paper, we plan to develop other neural-network-based features, and to also relax the lim-iteation of current rule extraction heuristics bygenerating translations word-by-word.AcknowledgementThis work was supported by DARPA/I2O ContractNo.
HR0011-12-C-0014 under the BOLT Pro-gram.
The views, opinions, and/or findings con-tained in this article are those of the author andshould not be interpreted as representing the of-ficial views or policies, either expressed or im-plied, of the Defense Advanced Research ProjectsAgency or the Department of Defense.ReferencesMichael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint language and translationmodeling with recurrent neural networks.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1044?1054, Seattle, Washington, USA, October.
Associa-tion for Computational Linguistics.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
Technical Report1409.0473, arXiv.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Comput.
Linguist., 19(2):263?311, June.Marine Carpuat and Dekai Wu.
2007.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 61?72, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Rich Caruana.
1997.
Multitask learning.
MachineLearning, 28(1):41?75.39David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In HLT-NAACL, pages 218?226.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th International Conference onMachine Learning, ICML ?08, pages 160?167, NewYork, NY, USA.
ACM.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 12:2493?2537,November.Jacob Devlin and Spyros Matsoukas.
2012.
Trait-based hypothesis selection for machine translation.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 528?532, Stroudsburg, PA,USA.
Association for Computational Linguistics.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1370?1380, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Jacob Devlin.
2009.
Lexical features for statisticalmachine translation.
Master?s thesis, University ofMaryland.Jenny Rose Finkel and Christopher D. Manning.
2009.Joint parsing and named entity recognition.
In Pro-ceedings of Human Language Technologies: The2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 326?334, Boulder, Colorado, June.Association for Computational Linguistics.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-kander, and Nadi Tomeh.
2013.
Morphologicalanalysis and disambiguation for dialectal arabic.
InHLT-NAACL, pages 426?432.Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.2013.
Factored soft source syntactic constraints forhierarchical machine translation.
In EMNLP, pages556?566.Brian Hutchinson, Li Deng, and Dong Yu.
2013.
Ten-sor deep stacking networks.
IEEE Trans.
PatternAnal.
Mach.
Intell., 35(8):1944?1957, August.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1700?1709, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages655?665, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proceedings of the 2012 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, NAACL HLT ?12, pages 39?48, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Yoong Keok Lee and Hwee Tou Ng.
2002.
An em-pirical evaluation of knowledge sources and learn-ing algorithms for word sense disambiguation.
InProceedings of the ACL-02 Conference on Empiri-cal Methods in Natural Language Processing - Vol-ume 10, EMNLP ?02, pages 41?48, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Wenzhe Pei, Tao Ge, and Baobao Chang.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 293?303, Bal-timore, Maryland, June.
Association for Computa-tional Linguistics.Antti Rosti, Bing Zhang, Spyros Matsoukas, andRich Schwartz.
2010.
BBN system descrip-tion for WMT10 system combination task.
InWMT/MetricsMATR, pages 321?326.Holger Schwenk.
2010.
Continuous-space languagemodels for statistical machine translation.
PragueBull.
Math.
Linguistics, 93:137?146.Holger Schwenk.
2012.
Continuous space translationmodels for phrase-based statistical machine transla-tion.
In COLING (Posters), pages 1071?1080.Hendra Setiawan, Bowen Zhou, Bing Xiang, and Li-bin Shen.
2013.
Two-neighbor orientation modelwith cross-boundary global contexts.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 1264?1274, Sofia, Bulgaria, August.Association for Computational Linguistics.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2010.String-to-dependency statistical machine transla-tion.
Computational Linguistics, 36(4):649?671,December.40Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-ing Zheng, and Andrew McCallum.
2013.
Jointinference of entities, relations, and coreference.
InProceedings of the 2013 Workshop on AutomatedKnowledge Base Construction, AKBC ?13, pages 1?6, New York, NY, USA.
ACM.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and translation model adaptationusing comparable corpora.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?08, pages 857?866,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Richard Socher, Cliff C. Lin, Andrew Y. Ng, andChristopher D. Manning.
2011.
Parsing NaturalScenes and Natural Language with Recursive NeuralNetworks.
In Proceedings of the 26th InternationalConference on Machine Learning (ICML).Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
InC.J.C.
Burges, L. Bottou, M. Welling, Z. Ghahra-mani, and K.Q.
Weinberger, editors, Advances inNeural Information Processing Systems 26, pages926?934.
Curran Associates, Inc.Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,and Hermann Ney.
2014.
Translation modelingwith bidirectional recurrent neural networks.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 14?25, Doha, Qatar, October.
Association forComputational Linguistics.Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le.2014.
Sequence to sequence learning with neuralnetworks.
In Z. Ghahramani, M. Welling, C. Cortes,N.D.
Lawrence, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems27, pages 3104?3112.
Curran Associates, Inc.Christoph Tillman.
2004.
A unigram orienta-tion model for statistical machine translation.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Short Papers, pages 101?104, Boston, Massachusetts, USA, May 2 - May 7.Association for Computational Linguistics.Dong Yu, Li Deng, and Frank Seide.
2012.
Large vo-cabulary speech recognition using deep tensor neu-ral networks.
In INTERSPEECH.
ISCA.41
