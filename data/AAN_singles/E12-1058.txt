Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 570?579,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsThe Impact of Spelling Errors on Patent SearchBenno Stein and Dennis Hoppe and Tim GollubBauhaus-Universit?t Weimar99421 Weimar, Germany<first name>.<last name>@uni-weimar.deAbstractThe search in patent databases is a riskybusiness compared to the search in otherdomains.
A single document that is relevantbut overlooked during a patent search canturn into an expensive proposition.
Whilerecent research engages in specialized mod-els and algorithms to improve the effective-ness of patent retrieval, we bring anotheraspect into focus: the detection and ex-ploitation of patent inconsistencies.
In par-ticular, we analyze spelling errors in the as-signee field of patents granted by the UnitedStates Patent & Trademark Office.
We in-troduce technology in order to improve re-trieval effectiveness despite the presence oftypographical ambiguities.
In this regard,we (1) quantify spelling errors in terms ofedit distance and phonological dissimilarityand (2) render error detection as a learn-ing problem that combines word dissimi-larities with patent meta-features.
For thetask of finding all patents of a company,our approach improves recall from 96.7%(when using a state-of-the-art patent searchengine) to 99.5%, while precision is com-promised by only 3.7%.1 IntroductionPatent search forms the heart of most retrievaltasks in the intellectual property domain?cf.
Ta-ble 1, which provides an overview of various usergroups along with their typical (?)
and related (?)tasks.
The due diligence task, for example, isconcerned with legal issues that arise while inves-tigating another company.
Part of an investiga-tion is a patent portfolio comparison between oneor more competitors (Lupu et al 2011).
Withinall tasks recall is preferred over precision, a factwhich distinguishes patent search from generalweb search.
This retrieval constraint has produceda variety of sophisticated approaches tailored tothe patent domain: citation analysis (Magdy andJones, 2010), the learning of section-specific re-trieval models (Lopez and Romary, 2010), and au-tomated query generation (Xue and Croft, 2009).Each approach improves retrieval performance,but what keeps them from attaining maximum ef-fectiveness in terms of recall are the inconsisten-cies found in patents: incomplete citation sets, in-correctly assigned classification codes, and, notleast, spelling errors.Our paper deals with spelling errors in an oblig-atory and important field of each patent, namely,the patent assignee name.
Bibliographic fields arewidely used among professional patent searchersin order to constrain keyword-based search ses-sions (Joho et al 2010).
The assignee name isparticularly helpful for patentability searches andportfolio analyses since it determines the com-pany holding the patent.
Patent experts addressthese search tasks by formulating queries contain-ing the company name in question, in the hope offinding all patents owned by that company.
A for-mal and more precise description of this relevantsearch task is as follows: Given a query q whichspecifies a company, and a set D of patents, de-termine the set Dq ?
D comprised of all patentsheld by the respective company.For this purpose, all assignee names in thepatents in D should be analyzed.
Let A denotethe set of all assignee names in D, and let a ?
qdenote the fact that an assignee name a ?
A refersto company q.
Then in the portfolio search task,all patents filed under a are relevant.
The retrievalof Dq can thus be rendered as a query expansion570Table 1: User groups and patent-search-related retrieval tasks in the patent domain (Hunt et al 2007).User groupAnalyst Attorney Manager Inventor Investor ResearcherPatentability ?
?
?
?State of the art ?
?Patent search task Infringement ?Opposition ?
?Due diligence ?
?Portfolio ?
?
?
?task, where q is expanded by the disjunction ofassignee names Aq with Aq = {a ?
A | a ?
q}.While the trivial expansion of q by the entireset A ensures maximum recall but entails an un-acceptable precision, the expansion of q by theempty set yields a reasonable baseline.
The latterapproach is implemented in patent search enginessuch as PatBase1 or FreePatentsOnline,2 whichreturn all patents where the company name q oc-curs as a substring of the assignee name a. Thisbaseline is simple but reasonable; due to trade-mark law, a company name q must be a uniqueidentifier (i.e.
a key), and an assignee name a thatcontains q can be considered as relevant.
It shouldbe noted in this regard that |q| < |a| holds formost elements in Aq, since the assignee namesoften contain company suffixes such as ?Ltd?or ?Inc?.Our hypothesis is that due to misspelled as-signee names a substantial fraction of relevantpatents cannot be found by the baseline ap-proach.
In this regard, the types of spelling er-rors in assignee names given in Table 2 shouldbe considered.Table 2: Types of spelling errors with increasingproblem complexity according to Stein and Curatolo(2006).
The first row refers to lexical errors, whereasthe last two rows refer to phonological errors.
For eachtype, an example is given, where a misspelled com-pany name is followed by the correctly spelled variant.Spelling error type ExamplePermutations or dropped letters ?
Whirpool Corporation?
Whirlpool CorporationMisremembering spelling details ?
Whetherford International?
Weatherford InternationalSpelling out the pronunciation ?
Emulecks Corporation?
Emulex CorporationIn order to raise the recall for portfolio searchwithout significantly impairing precision, an ap-1www.patbase.com2www.freepatentsonline.comproach more sophisticated than the standard re-trieval approach, which is the expansion of q bythe empty set, is needed.
Such an approach muststrive for an expansion of q by a subset of Aq,whereby this subset should be as large as possible.1.1 ContributionsThe paper provides a new solution to the problemoutlined.
This solution employs machine learn-ing on orthographic features, as well as on patentmeta features, to reliably detect spelling errors.
Itconsists of two steps: (1) the computation of A+q ,the set of assignee names that are in a certain editdistance neighborhood to q; and (2) the filtering ofA+q , yielding the set A?q , which contains those as-signee names from A+q that are classified as mis-spellings of q.
The power of our approach can beseen from Table 3, which also shows a key resultof our research; a retrieval system that exploitsour classifier will miss only 0.5% of the relevantpatents, while retrieval precision is compromisedby only 3.7%.Another contribution relates to a new, manu-ally-labeled corpus comprising spelling errors inthe assignee field of patents (cf.
Section 3).
Inthis regard, we consider the over 2 million patentsgranted by the USPTO between 2001 and 2010.Last, we analyze indications of deliberately in-serted spelling errors (cf.
Section 4).Table 3: Mean average Precision, Recall, and F -Measure (?
= 2) for different expansion sets for q ina portfolio search task, which is conducted on our testcorpus (cf.
Section 3).Expansion set for q Precision Recall F2?
(baseline) 0.993 0.967 0.968A?q (machine learning) 0.956 0.995 0.980A (trivial) 0.001 1.0 0.005A+q (edit distance) 0.274 1.0 0.6725711.2 Causes for Inconsistencies in PatentsWe identify the following six factors for inconsis-tencies in the bibliographic fields of patents, inparticular for assignee names: (1) Misspellingsare introduced due to the lack of knowledge, thelack of attention, and due to spelling disabili-ties.
Intellevate Inc. (2006) reports that 98%of a sample of patents taken from the USPTOdatabase contain errors, most which are spellingerrors.
(2) Spelling errors are only removed by theUSPTO upon request (U.S. Patent & TrademarkOffice, 2010).
(3) Spelling variations of inventornames are permitted by the USPTO.
The Manualof Patent Examining Procedure (MPEP) states inparagraph 605.04(b) that ?if the applicant?s fullname is ?John Paul Doe,?
either ?John P. Doe?
or?J.
Paul Doe?
is acceptable.?
Thus, it is valid to in-troduce many different variations: with and with-out initials, with and without a middle name, orwith and without suffixes.
This convention ap-plies to assignee names, too.
(4) Companies of-ten have branches in different countries, whereeach branch has its own company suffix, e.g.,?Limited?
(United States), ?GmbH?
(Germany),or ?Kabushiki Kaisha?
(Japan).
Moreover, theusage of punctuation varies along company suf-fix abbreviations: ?L.L.C.?
in contrast to ?LLC?,for example.
(5) Indexing errors emerge fromOCR processing patent applications, because sim-ilar looking letters such as ?e?
versus ?c?
or ?l?versus ?I?
are likely to be misinterpreted.
(6) Withthe advent of electronic patent application filing,the number of patent reexamination steps was re-duced.
As a consequence, the chance of unde-tected spelling errors increases (Adams, 2010).All of the mentioned factors add to a highly in-consistent USPTO corpus.2 Related WorkInformation within a corpus can only be retrievedeffectively if the data is both accurate and unique(M?ller and Freytag, 2003).
In order to yield datathat is accurate and unique, approaches to datacleansing can be utilized to identify and removeinconsistencies.
M?ller and Freytag (2003) clas-sify inconsistencies, where duplicates of entitiesin a corpus are part of a semantic anomaly.
Theseduplicates exist in a database if two or more dif-ferent tuples refer to the same entity.
With respectto the bibliographic fields of patents, the assigneenames ?Howlett-Packard?
and ?Hewett-Packard?are distinct but refer to the same company.
Thesekinds of near-duplicates impede the identificationof duplicates (Naumann and Herschel, 2010).Near-duplicate Detection The problem ofidentifying near-duplicates is also known asrecord linkage, or name matching; it is sub-ject of active research (Elmagarmid et al 2007).With respect to text documents, slightly modi-fied passages in these documents can be identi-fied using fingerprints (Potthast and Stein, 2008).On the other hand, for data fields which con-tain natural language such as the assignee namefield, string similarity metrics (Cohen et al2003) as well as spelling correction technol-ogy are exploited (Damerau, 1964; Monge andElkan, 1997).
String similarity metrics com-pute a numeric value to capture the similarityof two strings.
Spelling correction algorithms,by contrast, capture the likelihood for a givenword being a misspelling of another word.
Inour analysis, the similarity metric SoftTfIdf isapplied, which performs best in name matchingtasks (Cohen et al 2003), as well as the completerange of spelling correction algorithms shown inFigure 1: Soundex, which relies on similarityhashing (Knuth, 1997), the Levenshtein distance,which gives the minimum number of edits neededto transform a word into another word (Leven-shtein, 1966), and SmartSpell, a phonetic pro-duction approach that computes the likelihoodof a misspelling (Stein and Curatolo, 2006).
Inorder to combine the strength of multiple met-rics within a near-duplicate detection task, sev-eral authors resort to machine learning (Bilenkoand Mooney, 2002; Cohen et al 2003).
Christen(2006) concludes that it is important to exploit allkinds of knowledge about the type of data in ques-tion, and that inconsistencies are domain-specific.Hence, an effective near-duplicate detection ap-proach should employ domain-specific heuristicsand algorithms (M?ller and Freytag, 2003).
Fol-lowing this argumentation, we augment variousword similarity assessments with patent-specificmeta-features.Patent Search Commercial patent search en-gines, such as PatBase and FreePatentsOnline,handle near-duplicates in assignee names as fol-lows.
For queries which contain a company namefollowed by a wildcard operator, PatBase suggests572Single wordspellingcorrectionNear similarityhashingEditingPhonetic productionapproachEdit-distance-basedTrigram-basedRule-basedCollision-basedNeighborhood-basedHeuristic searchHidden MarkovmodelsFigure 1: Classification of spelling correction methodsaccording to Stein and Curatolo (2006).a set of additional companies (near-duplicates),which can be considered alongside the companyname in question.
These suggestions are solelyretrieved based on a trailing wildcard query.
Eachadditional company name can then be marked in-dividually by a user to expand the original query.In case the entire set of suggestions is consid-ered, this strategy conforms to the expansion ofa query by the empty set, which equals a rea-sonable baseline approach.
This query expansionstrategy, however, has the following drawbacks:(1) The strategy captures only inconsistencies thatsucceed the given company name in the origi-nal query.
Thus, near-duplicates which containspelling errors in the company name itself are notfound.
Even if PatBase would support left trailingwildcards, then only the full combination of wild-card expressions would cover all possible cases ofmisspellings.
(2) Given an acronym of a companysuch as IBM, it is infeasible to expand the ab-breviation to ?International Business Machines?without considering domain knowledge.Query Expansion Methods for Patent SearchTo date, various studies have investigated queryexpansion techniques in the patent domain thatfocus on prior-art search and invalidity search(Magdy and Jones, 2011).
Since we are dealingwith queries that comprise only a company name,existing methods cannot be applied.
Instead, thenear-duplicate task in question is more related to atext reuse detection task discussed by Hagen andStein (2011); given a document, passages whichalso appear identical or slightly modified in otherdocuments, have to be retrieved by using standardkeyword-based search engines.
Their approach isguided by the user-over-ranking hypothesis intro-duced by Stein and Hagen (2011).
It states that?the best retrieval performance can be achievedwith queries returning about as many results ascan be considered at user site.?
If we make useof their terminology, then we can distinguish thequery expansion sets (cf.
Table 3) into two cate-gories: (1) The trivial as well as the edit distanceexpansion sets are underspecific, i.e., users cannotcope with the large amount of irrelevant patentsreturned; the precision is close to zero.
(2) Thebaseline approach, by contrast, is overspecific;it returns too few documents, i.e., the achievedrecall is not optimal.
As a consequence, thesequery expansion sets are not suitable for portfoliosearch.
Our approach, on the other hand, excelsin both precision and recall.Query Spelling Correction Queries which aresubmitted to standard web search engines differfrom queries which are posed to patent search en-gines with respect to both length and languagediversity.
Hence, research in the field of websearch is concerned with suggesting reasonablealternatives to misspelled queries rather than cor-recting single words (Li et al 2011).
Since stan-dard spelling correction dictionaries (e.g.
ASpell)are not able to capture the rich language used inweb queries, large-scale knowledge sources suchas Wikipedia (Li et al 2011), query logs (Chenet al 2007), and large n-gram corpora (Brants etal., 2007) are employed.
It should be noted thatthe set of correctly written assignee names is un-known for the USPTO patent corpus.Moreover, spelling errors are modeled on thebasis of language models (Li et al 2011).
Okuno(2011) proposes a generative model to encounterspelling errors, where the original query is ex-panded based on alternatives produced by a smalledit distance to the original query.
This strategycorrelates to the trivial query expansion set (cf.Section 1).
Unlike using a small edit distance, weallow a reasonable high edit distance to maximizethe recall.Trademark Search The trademark search isabout identifying registered trademarks which aresimilar to a new trademark application.
Sim-ilarities between trademarks are assessed basedon figurative and verbal criteria.
In the formercase, the focus is on image-based retrieval tech-niques.
Trademarks are considered verbally simi-lar for a variety of reasons, such as pronunciation,spelling, and conceptual closeness, e.g., swappingletters or using numbers for words.
The verbalsimilarity of trademarks, on the other hand, canbe determined by using techniques comparableto near-duplicate detection: phonological parsing,573fuzzy search, and edit distance computation (Falland Giraud-Carrier, 2005).3 Detection of Spelling ErrorsThis section presents our machine learning ap-proach to expand a company query q; the classi-fier c delivers the set A?q = {a ?
A | c(q, a) = 1},an approximation of the ideal set of relevant as-signee names Aq.
As a classification technol-ogy a support vector machine with linear kernelis used, which receives each pair (q, a) as a six-dimensional feature vector.
For training and testpurposes we identified misspellings for 100 dif-ferent company names.
A detailed description ofthe constructed test corpus and a report on theclassifiers performance is given in the remainderof this section.3.1 Feature SetThe feature set comprises six features, three ofthem being orthographic similarity metrics, whichare computed for every pair (q, a).
Each metriccompares a given company name q with the first|q| words of the assignee name a:1.
SoftTfIdf.
The SoftTfIdf metric is consid-ered, since the metric is suitable for the com-parison of names (Cohen et al 2003).
Themetric incorporates the Jaro-Winkler met-ric (Winkler, 1999) with a distance thresholdof 0.9.
The frequency values for the similar-ity computation are trained on A.2.
Soundex.
The Soundex spelling correctionalgorithm captures phonetic errors.
Since thealgorithm computes hash values for both qand a, the feature is 1 if these hash valuesare equal, 0 otherwise.3.
Levenshtein distance.
The Levenshtein dis-tance for (q, a) is normalized by the charac-ter length of q.To obtain further evidence for a misspellingin an assignee name, meta information about thepatents in D, to which the assignee name refersto, is exploited.
In this regard, the following threefeatures are derived:1.
Assignee Name Frequency.
The numberof patents filed under an assignee name a:FFreq (a) = Freq(a,D).
We assume that theprobability of a misspelling to occur multi-ple times is low, and thus an assignee namewith a misspelled company name has a lowfrequency.2.
IPC Overlap.
The IPC codes of a patentspecify the technological areas it appliesto.
We assume that patents filed under thesame company name are likely to share thesame set of IPC codes, regardless whetherthe company name is misspelled or not.Hence, if we determine the IPC codes ofpatents which contain q in the assigneename, IPC(q), and the IPC codes of patentsfiled under assignee name a, IPC(a), thenthe intersection size of the two sets serves asan indicator for a misspelled company namein a:FIPC (q, a) =IPC(q) ?
IPC(a)IPC(q) ?
IPC(a)3.
Company Suffix Match.
The suffix matchrelies on the company suffixes Suffixes(q)that occur in the assignee names of A con-taining q.
Similar to the IPC overlap fea-ture, we argue that if the company suffixof a exists in the set Suffixes(q), a mis-spelling in a is likely: FSuffixes(q, a) = 1iff Suffixes(a) ?
Suffixes(q).3.2 Webis Patent Retrieval Assignee CorpusA key contribution of our work is a new cor-pus called Webis Patent Retrieval Assignee Cor-pus 2012 (Webis-PRA-12).
We compiled the cor-pus in order to assess the impact of misspelledcompanies on patent retrieval and the effective-ness of our classifier to detect them.3 The corpusis built on the basis of 2 132 825 patents D grantedby the USPTO between 2001 and 2010; the patentcorpus is provided publicly by the USPTO inXML format.
Each patent contains bibliographicfields as well as textual information such as theabstract and the claims section.
Since we are in-terested in the assignee name a associated witheach patent d ?
D, we parse each patent and ex-tract the assignee name.
This yields the set A of202 846 different assignee names.
Each assigneename refers to a set of patents, which size variesfrom 1 to 37 202 (the number of patents filedunder ?International Business Machines Corpo-ration?).
It should be noted that for a portfolio3The Webis-PRA-12 corpus is freely available viawww.webis.de/research/corpora574Table 4: Statistics of spelling errors for the 100 companies in the Webis-PRA-12 corpus.
Considered are thenumber of words and the number of letters in the company names, as well as the number of different companysuffixes that are used together with a company name (denoted as variants of q)Total Num.
of words in q Num.
of letters in q Num.
of variants of q1 2 3-4 2-10 11-15 16-35 1-5 6-15 16-96Number of companies in Q 100 36 53 11 30 35 35 45 32 23Avg.
num.
of misspellings in A 3.79 2.13 3.75 9.36 1.16 2.94 6.88 0.91 3.81 9.39search task the number of patents which refer toan assignee name matters for the computation ofprecision and recall.
If we, however, isolate thetask of detecting misspelled company names, thenit is also reasonable to weight each assignee nameequally and independently from the number ofpatents it refers to.
Both scenarios are addressedin the experiments.Given A, the corpus construction task is to mapeach assignee name a ?
A to the company nameq it refers to.
This gives for each company nameq the set of relevant assignee names Aq.
For ourcorpus, we do not construct Aq for all companynames but take a selection of 100 company namesfrom the 2011 Fortune 500 ranking as our set ofcompany names Q.
Since the Fortune 500 rank-ing contains only large companies, the test cor-pus may appear to be biased towards these com-panies.
However, rather than the company size thestructural properties of a company name are de-terminative; our sample includes short, medium,and long company names, as well as companynames with few, medium, and many differentcompany suffixes.
Table 4 shows the distributionof company names in Q along these criteria in thefirst row.For each company name q ?
Q, we ap-ply a semi-automated procedure to derive theset of relevant assignee names Aq .
In a firststep, all assignee names in A which do not re-fer to the company name q are filtered auto-matically.
From a preliminary evaluation weconcluded that the Levenshtein distance d(q, a)with a relative threshold of |q|/2 is a reasonablechoice for this filtering step.
The resulting setsA+q = {a ?
A | d(q, a) ?
|q|/2) contain, in totalover Q, 14 189 assignee names.
These assigneenames are annotated by human assessors within asecond step to derive the final set Aq for each q ?Q.
Altogether we identify 1 538 assignee namesthat refer to the 100 companies in Q.
With respectto our classification task, the assignee names ineach Aq are positive examples; the remaining as-signee names A+q \ Aq form the set of negativeexamples (12 651 in total).During the manual assessment, names of as-signees which include the correct company nameq were distinguished from misspelled ones.
Thelatter holds true for 379 of the 1 538 assigneenames.
These names are not retrievable by thebaseline system, and thus form the main target forour classifier.
The second row of Table 4 reportson the distribution of the 379 misspelled assigneenames.
As expectable, the longer the companyname, the more spelling errors occur.
Compa-nies which file patents under many different as-signee names are likelier to have patents with mis-spellings in the company name.3.3 Classifier PerformanceFor the evaluation with the Webis-PRA-12 cor-pus, we train a support vector machine,4 whichconsiders the six outlined features, and compareit to the other expansion techniques.
For the train-ing phase, we use 2/3 of the positive examplesto form a balanced training set of 1 025 posi-tive and 1 025 negative examples.
After 10-foldcross validation, the achieved classification accu-racy is 95.97%.For a comparison of the expansion techniqueson the test set, which contains the examples notconsidered in the training phase, two tasks aredistinguished: finding near duplicates in assigneenames (cf.
Table 5, Columns 3?5), and finding allpatents of a company (cf.
Table 5, Columns 6?8).The latter refers to the actual task of portfo-lio search.
It can be observed that the perfor-mance improvements on both tasks are pretty sim-ilar.
The baseline expansion ?
yields a recallof 0.83 in the first task.
The difference of 0.17to a perfect recall can be addressed by consid-ering query expansion techniques.
If the triv-ial expansion A is applied to the task the max-imum recall can be achieved, which, however,4We use the implementation of the WEKA toolkit with defaultparameters.575Table 5: The search results (macro-averaged) for two retrieval tasks and various expansion techniques.
BesidesPrecision and Recall, the F-Measure with ?
= 2 is stated.Misspelling detection Task: assignee names Task: patentsP R F2 P R F2Baseline (?)
.975 .829 .838 .993 .967 .968Trivial (A) .000 1.0 .001 .001 1.0 .005Edit distance (A+q ) .274 1.0 .499 .412 1.0 .672SVM (Levenshtein) .752 .981 .853 .851 .991 .911SVM (SoftTfIdf) .702 .980 .796 .826 .993 .886SVM (Soundex) .433 .931 .624 .629 .984 .759SVM (orthographic features) .856 .975 .922 .942 .990 .967SVM (A?q , all features) .884 .975 .938 .956 .995 .980is bought with precision close to zero.
Usingthe edit distance expansion A+q yields a precisionof 0.274 while keeping the recall at maximum.
Fi-nally, the machine learning expansion A?q leadsto a dramatic improvement (cf.
Table 5, bottomlines), whereas the exploitation of patent meta-features significantly outperforms the exclusiveuse of orthography-related features; the increasein recall which is achieved by A?q is statisticallysignificant (matched pair t-test) for both tasks (as-signee names task: t = ?7.6856, df = 99,p = 0.00; patents task: t = ?2.1113, df = 99,p = 0.037).
Note that when being applied as asingle feature none of the spelling metrics (Lev-enshtein, SoftTfIdf, Soundex) is able to achievea recall close to 1 without significantly impairingthe precision.4 Distribution of Spelling ErrorsEncouraged by the promising retrieval resultsachieved on the Webis-PRA-12 corpus, we ex-tend the analysis of spelling errors in patents tothe entire USPTO corpus of granted patents be-tween 2001 and 2010.
The analysis focuses onthe following two research questions:1.
Are spelling errors an increasing issue inpatents?
According to Adams (2010), theamount of spelling errors should have beenincreased in the last years due to the elec-tronic patent filing process (cf.
Section 1.2).We address this hypothesis by analyzing thedistribution of spelling errors in companynames that occur in patents granted between2001 and 2010.2.
Are misspellings introduced deliberately inpatents?
We address this question by analyz-ing the patents with respect to the eight tech-nological areas based on the InternationalPatent Classification scheme IPC: A (Hu-man necessities), B (Performing operations;transporting), C (Chemistry; metallurgy),D (Textiles; paper), E (Fixed constructions),F (Mechanical engineering; lighting; heat-ing; weapons; blasting), G (Physics), andH (Electricity).
If spelling errors are in-troduced accidentally, then we expect themto be uniformly distributed across all ar-eas.
A biased distribution, on the otherhand, indicates that errors might be in-serted deliberately.In the following, we compile a second corpuson the basis of the entire set A of assignee names.In order to yield a uniform distribution of the com-panies across years, technological areas and coun-tries, a set of 120 assignee names is extracted foreach dimension.
After the removal of duplicates,we revised these assignee names manually in or-der to check (and correct) their spelling.
Finally,trailing business suffixes are removed, which re-sults in a set of 3 110 company names.
For eachcompany name q, we generate the set A?q as de-scribed in Section 3.The results of our analysis are shown in Table 6.Table 6(a) refers to the first research question andshows that the amount of misspellings in compa-nies decreased over the years from 6.67% in 2001to 4.74% in 2010 (cf.
Row 3).
These results let usreject the hypothesis of Adams (2010).
Neverthe-less, the analysis provides evidence that spellingerrors are still an issue.
For example, the companyidentified with most spelling errors are ?Konin-klijke Philips Electronics?
with 45 misspellingsin 2008, and ?Centre National de la RechercheScientifique?
with 28 misspellings in 2009.
Theresults are consistent with our findings with re-576Table 6: Distribution of spelling errors for 3 110 company identifiers in the USPTO patents.
The mean of spellingerrors per company identifier and the standard deviation ?
refer to companies with misspellings.
The last row ineach table shows the number of patents that are additionally found if the original query q is expanded by A?q .
(a) Distribution of spelling errors between the years 2001 and 2010.Year2001 2002 2003 2004 2005 2006 2007 2008 2009 2010MeasureNumber of companies 1 028 1 066 1 115 1 151 1 219 1 261 1 274 1 210 1 224 1 268Number of companies with misspellings 67 63 53 65 65 60 65 64 53 60Companies with misspellings (%) 6.52 5.91 4.75 5.65 5.33 4.76 5.1 5.29 4.33 4.73Mean 2.78 2.35 2.23 2.28 2.18 2.48 2.23 3.0 2.64 2.8Standard deviation ?
4.62 3.3 3.63 3.13 2.8 3.55 2.87 6.37 4.71 4.6Maximum misspellings per company 24 12 16 12 10 18 12 45 28 22Additional number of patents 7.1 7.21 7.43 7.68 7.91 8.48 7.83 8.84 8.92 8.92(b) Distribution of spelling errors based on the IPC scheme.IPC codeA B C D E F G HMeasureNumber of companies 954 1 231 811 277 412 771 1 232 949Number of companies with misspellings 59 70 51 7 10 33 83 63Companies with misspellings (%) 6.18 5.69 6.29 2.53 2.43 4.28 6.74 6.64Mean 3.0 2.49 3.57 1.86 2.8 1.88 3.29 4.05Standard deviation ?
5.28 3.65 7.03 1.99 4.22 2.31 5.72 7.13Maximum misspellings per company 32 14 40 3 12 6 24 35Additional number of patents 9.25 9.67 11.12 4.71 4.6 4.79 8.92 12.84spect to the Fortune 500 sample (cf.
Table 4),where company names that are longer and pre-sumably more difficult to write contain morespelling errors.In contrast to the uniform distribution of mis-spellings over the years, the situation with re-gard to the technological areas is different (cf.
Ta-ble 6(b)).
Most companies are associated withthe IPC sections G and B, which both refer totechnical domains (cf.
Table 6(b), Row 1).
Thepercentage of misspellings in these sections in-creased compared to the spelling errors groupedby year.
A significant difference can be seen forthe sections D and E. Here, the number of as-signed companies drops below 450 and the per-centage of misspellings decreases significantlyfrom about 6% to 2.5%.
These findings mightsupport the hypothesis that spelling errors are in-serted deliberately in technical domains.5 ConclusionsWhile researchers in the patent domain concen-trate on retrieval models and algorithms to im-prove the search performance, the original aspectof our paper is that it points to a different (and or-thogonal) research avenue: the analysis of patentinconsistencies.
With the analysis of spelling er-rors in assignee names we made a first yet consid-erable contribution in this respect; searches withassignee constraints become a more sensible op-eration.
We showed how a special treatment ofspelling errors can significantly raise the effec-tiveness of patent search.
The identification ofthis untapped potential, but also the utilization ofmachine learning to combine patent features withtypography, form our main contributions.Our current research broadens the applicationof a patent spelling analysis.
In order to iden-tify errors that are introduced deliberately weinvestigate different types of misspellings (editdistance versus phonological).
Finally, we con-sider the analysis of acquisition histories of com-panies as promising research direction: sinceacquired companies often own granted patents,these patents should be considered while search-ing for the company in question in order to furtherincrease the recall.AcknowledgementsThis work is supported in part by the German Sci-ence Foundation under grants STE1019/2-1 andFU205/22-1.577ReferencesStephen Adams.
2010.
The Text, the Full Text andnothing but the Text: Part 1 ?
Standards for creatingTextual Information in Patent Documents and Gen-eral Search Implications.
World Patent Information,32(1):22?29, March.Mikhail Bilenko and Raymond J. Mooney.
2002.Learning to Combine Trained Distance Metricsfor Duplicate Detection in Databases.
TechnicalReport AI 02-296, Artificial Intelligence Labora-tory, University of Austin, Texas, USA, Austin,TX, February.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large LanguageModels in Machine Translation.
In EMNLP-CoNLL?07: Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learn-ing, pages 858?867.
ACL, June.Qing Chen, Mu Li, and Ming Zhou.
2007.
Improv-ing Query Spelling Correction Using Web SearchResults.
In EMNLP-CoNLL ?07: Proceedings ofthe 2007 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning, pages 181?189.
ACL,June.Peter Christen.
2006.
A Comparison of PersonalName Matching: Techniques and Practical Is-sues.
In ICDM ?06: Workshops Proceedings ofthe sixth IEEE International Conference on DataMining, pages 290?294.
IEEE Computer Society,December.William W. Cohen, Pradeep Ravikumar, and StephenE.
Fienberg.
2003.
A Comparison of StringDistance Metrics for Name-Matching Tasks.
InSubbarao Kambhampati and Craig A. Knoblock,editors, IIWeb ?03: Proceedings of the IJCAIworkshop on Information Integration on the Web,pages 73?78, August.Fred J. Damerau.
1964.
A Technique for ComputerDetection and Correction of Spelling Errors.
Com-munications of the ACM, 7(3):171?176.Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, andVassilios S. Verykios.
2007.
Duplicate Record De-tection: A Survey.
IEEE Trans.
Knowl.
Data Eng.,19(1):1?16.Caspas J.
Fall and Christophe Giraud-Carrier.
2005.Searching Trademark Databases for Verbal Similar-ities.
World Patent Information, 27(2):135?143.Matthias Hagen and Benno Stein.
2011.
CandidateDocument Retrieval for Web-Scale Text Reuse De-tection.
In 18th International Symposium on StringProcessing and Information Retrieval (SPIRE 11),volume 7024 of Lecture Notes in Computer Science,pages 356?367.
Springer.David Hunt, Long Nguyen, and Matthew Rodgers, ed-itors.
2007.
Patent Searching: Tools & Techniques.Wiley.Intellevate Inc. 2006.
Patent Quality, a blog en-try.
http://www.patenthawk.com/blog/2006/01/patent_quality.html, January.Hideo Joho, Leif A. Azzopardi, and Wim Vander-bauwhede.
2010.
A Survey of Patent Users: AnAnalysis of Tasks, Behavior, Search Functionalityand System Requirements.
In IIix ?10: Proceed-ing of the third symposium on Information Inter-action in Context, pages 13?24, New York, NY,USA.
ACM.Donald E. Knuth.
1997.
The Art of Computer Pro-gramming, Volume I: Fundamental Algorithms, 3rdEdition.
Addison-Wesley.Vladimir I. Levenshtein.
1966.
Binary codes capa-ble of correcting deletions, insertions and reversals.Soviet Physics Doklady, 10(8):707?710.
Originalin Doklady Akademii Nauk SSSR 163(4): 845-848.Yanen Li, Huizhong Duan, and ChengXiang Zhai.2011.
CloudSpeller: Spelling Correction for SearchQueries by Using a Unified Hidden Markov Modelwith Web-scale Resources.
In Spelling Alterationfor Web Search Workshop, pages 10?14, July.Patrice Lopez and Laurent Romary.
2010.
Experi-ments with Citation Mining and Key-Term Extrac-tion for Prior Art Search.
In Martin Braschler,Donna Harman, and Emanuele Pianta, editors,CLEF 2010 LABs and Workshops, Notebook Pa-pers, September.Mihai Lupu, Katja Mayer, John Tait, and Anthony J.Trippe, editors.
2011.
Current Challenges in PatentInformation Retrieval, volume 29 of The Informa-tion Retrieval Series.
Springer.Walid Magdy and Gareth J. F. Jones.
2010.
Ap-plying the KISS Principle for the CLEF-IP 2010Prior Art Candidate Patent Search Task.
In MartinBraschler, Donna Harman, and Emanuele Pianta,editors, CLEF 2010 LABs and Workshops, Note-book Papers, September.Walid Magdy and Gareth J.F.
Jones.
2011.
A Studyon Query Expansion Methods for Patent Retrieval.In PAIR ?11: Proceedings of the 4th workshop onPatent information retrieval, AAAI Workshop onPlan, Activity, and Intent Recognition, pages 19?24, New York, NY, USA.
ACM.Alvaro E. Monge and Charles Elkan.
1997.
An Ef-ficient Domain-Independent Algorithm for Detect-578ing Approximately Duplicate Database Records.In DMKD ?09: Proceedings of the 2nd workshopon Research Issues on Data Mining and Knowl-edge Discovery, pages 23?29, New York, NY,USA.
ACM.Heiko M?ller and Johann-C. Freytag.
2003.
Prob-lems, Methods and Challenges in ComprehensiveData Cleansing.
Technical Report HUB-IB-164,Humboldt-Universit?t zu Berlin, Institut f?r Infor-matik, Germany.Felix Naumann and Melanie Herschel.
2010.
An In-troduction to Duplicate Detection.
Synthesis Lec-tures on Data Management.
Morgan & ClaypoolPublishers.Yoh Okuno.
2011.
Spell Generation based on EditDistance.
In Spelling Alteration for Web SearchWorkshop, pages 25?26, July.Martin Potthast and Benno Stein.
2008.
New Is-sues in Near-duplicate Detection.
In ChristinePreisach, Hans Burkhardt, Lars Schmidt-Thieme,and Reinhold Decker, editors, Data Analysis, Ma-chine Learning and Applications.
Selected papersfrom the 31th Annual Conference of the GermanClassification Society (GfKl 07), Studies in Classi-fication, Data Analysis, and Knowledge Organiza-tion, pages 601?609, Berlin Heidelberg New York.Springer.Benno Stein and Daniel Curatolo.
2006.
PhoneticSpelling and Heuristic Search.
In Gerhard Brewka,Silvia Coradeschi, Anna Perini, and Paolo Traverso,editors, 17th European Conference on Artificial In-telligence (ECAI 06), pages 829?830, Amsterdam,Berlin, August.
IOS Press.Benno Stein and Matthias Hagen.
2011.
Introducingthe User-over-Ranking Hypothesis.
In Advances inInformation Retrieval.
33rd European Conferenceon IR Resarch (ECIR 11), volume 6611 of LectureNotes in Computer Science, pages 503?509, BerlinHeidelberg New York, April.
Springer.U.S.
Patent & Trademark Office.
2010.
Manual ofPatent Examining Procedure (MPEP), Eighth Edi-tion, July.William W. Winkler.
1999.
The State of Record Link-age and Current Research Problems.
Technical re-port, Statistical Research Division, U.S. Bureau ofthe Census.Xiaobing Xue and Bruce W. Croft.
2009.
AutomaticQuery Generation for Patent Search.
In CIKM?09: Proceeding of the eighteenth ACM conferenceon Information and Knowledge Management, pages2037?2040, New York, NY, USA.
ACM.579
