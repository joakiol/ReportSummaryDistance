Information Extraction from Voicemail TranscriptsMartin JanscheDepartment of LinguisticsThe Ohio State UniversityColumbus, OH 43210, USAjansche.1@osu.eduSteven P. AbneyAT&T Labs ?
Research180 Park AvenueFlorham Park, NJ 07932, USAabney@research.att.comAbstractVoicemail is not like email.
Even such ba-sic information as the name of the caller/sender or a phone number for returningcalls is not represented explicitly and mustbe obtained from message transcripts orother sources.
We discuss techniques fordoing this and the challenges these taskspresent.1 IntroductionWhen you?re away from the phone and someonetakes a message for you, at the very least you?d ex-pect to be told who called and whether they left anumber for you to call back.
If the same call ispicked up by a voicemail system, even such basic in-formation like the name of the caller and their phonenumber may not be directly available, forcing one tolisten to the entire message1 in the worst case.
Bycontrast, information about the sender of an emailmessage has always been explicitly represented inthe message headers, starting with early standard-ization attempts (Bhushan et al, 1973) and includingthe two decade old current standard (Crocker, 1982).Applications that aim to present voicemail messagesthrough an email-like interface ?
take as an examplethe idea of a ?uniform inbox?
presentation of email,voicemail, and other kinds of messages2 ?
must dealwith the problem of how to obtain information anal-ogous to what would be contained in email headers.1The average message length in the corpus described belowis 36 seconds.2Similar issues arise with FAX messages, for example.Here we will discuss one way of addressing thisproblem, treating it exclusively as the task of extract-ing relevant information from voicemail transcripts.In practice, e.g.
in the context of a sophisticatedvoicemail front-end (Hirschberg et al, 2001) that istightly integrated with an organization-wide voice-mail system and private branch exchange (PBX), ad-ditional sources of information may be available: thevoicemail system or the PBX might provide infor-mation about the originating station of a call, andspeaker identification can be used to match a caller?svoice against models of known callers (Rosenberget al, 2001).
Restricting our attention to voicemailtranscripts means that our focus and goals are sim-ilar to those of Huang et al (2001), but the featuresand techniques we use are very different.While the present task may seem broadly similarto named entity extraction from broadcast news (Go-toh and Renals, 2000), it is quite distinct from thelatter: first, we are only interested in a small subsetof the named entities; and second, the structure ofthe voicemail transcripts in our corpus is very dif-ferent from broadcast news and certain aspects ofthis structure can be exploited for extracting callernames.Huang et al (2001) discuss three approaches:hand-crafted rules; grammatical inference of subse-quential transducers; and log-linear classifiers withbigram and trigram features used as taggers (Ratna-parkhi, 1996).
While the latter are reported to yieldthe best overall performance, the hand-crafted rulesresulted in higher recall.
Our phone number extrac-tor is based on a two-phase procedure that employs asmall hand-crafted component to propose candidatephrases, followed by a classifier that retains the de-sirable candidates.
This allows for more or less inde-Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
320-327.Proceedings of the Conference on Empirical Methods in Naturalpendent optimization of recall and precision, some-what similar to the PNrule classifier learner (Agar-wal and Joshi, 2001; Joshi et al, 2001).
We shall seethat hand-crafted rules achieve very good recall, justas Huang et al (2001) had observed, and the prun-ing phase successfully eliminates most undesirablecandidates without affecting recall too much.
Over-all performance of our method is better than if weemploy a log-linear model with trigram features.The success of the method proposed here is alsodue to the use of a rich set of features for candi-date classification.
For example, the majority ofphone numbers in voicemail messages has eitherfour, seven, or ten digits, whereas nine digits wouldindicate a social security number.
In our two-phaseapproach it is straightforward for the second-phaseclassifier to take the length of a candidate phonenumber into account.
On the other hand, standardnamed entity taggers that use trigram features do notexploit this information, and doing so would entailsignificant changes to the underlying models and pa-rameter estimation procedures.The rest of this paper is organized as follows.
Abrief overview of the data we used in ?2 is followedby a discussion of methods for extracting two kindsof caller information in ?3.
Methods for extractingtelephone numbers are discussed in ?4, and ?5 sum-marizes and concludes.2 Voicemail CorpusDevelopment and evaluation was done using a pro-prietary corpus of almost 10,000 voicemail mes-sages that had been manually transcribed andmarked up for content.
Some more details aboutthis corpus can be found in (Bacchiani, 2001).
Therelevant content labeling is perhaps best illustratedwith an (anonymized) excerpt form a typical mes-sage transcript:?greeting?
hi Jane ?/greeting?
?caller?
thisis Pat Caller ?/caller?
I just wanted to Iknow you?ve probably seen this or maybeyou already know about it .
.
.
so if youcould give me a call at ?telno?
one twothree four five ?/telno?
when you get themessage I?d like to chat about it hopethings are well with you ?closing?
talk toyou soon ?/closing?This transcript is representative of a large class ofmessages that start out with a short greeting fol-lowed by a phrase that identifies the caller eitherby name as above or by other means (?hi, it?s me?
).A phone number may be mentioned as part of thecaller?s self-identification, or is often mentionednear the end of the message.
It may seem natu-ral and obvious that voicemail messages should bestructured in this way, and this prototypical struc-ture can therefore be exploited for purposes of lo-cating caller information or deciding whether a digitstring constitutes a phone number.
The next sectionsdiscuss this in more detail.The corpus was partitioned into two subsets, with8120 messages used for development and 1869 forevaluation.
Approximately 5% of all messages areempty.
Empty messages were not discarded fromthe evaluation set since they constitute realistic sam-ples that the information extraction component hasto cope with.
The development set contains 7686non-empty messages.3 Caller InformationOf the non-empty messages in the development set,7065 (92%) transcripts contain a marked-up callerphrase.
Of those, 6731 messages mention a name inthe caller phrase.
Extracting caller information canbe broken down into two slightly different tasks: wemight want to reproduce the existing caller annota-tion as closely as possible, producing caller phraseslike ?this is Pat Caller?
or ?it?s me?
; or we might onlybe interested in caller names such as ?Pat Caller?
inour above example.
We make use of the fact thatfor the overwhelming majority of cases, the caller?sself-identification occurs somewhere near the begin-ning of the message.3.1 Caller PhrasesMost caller phrases tend to start one or two wordsinto the message.
This is because they are typi-cally preceded by a one-word (?hi?)
or two-word(?hi Jane?)
greeting.
Figure 1 shows the empiri-cal distribution of the beginning of the caller phraseacross the 7065 applicable transcripts in the devel-opment data.
As can be seen, more than 97% ofall caller phrases start somewhere between one andseven words from the beginning of the message,though in one extreme case the start of the callerphrase occurred 135 words into the message.01020304050607080901000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16observed max.
: 135entropy: 1.48 bitsdensitycumulativeFigure 1: Empirical probability of a caller phrasestarting x words into the messageThese observations strongly suggest that when ex-tracting caller phrases, positional cues should betaken into account.
This is good news, especiallysince intrinsic features of the caller phrase may notbe as reliable: a caller phrase is likely to containnames that are problematic for an automatic speechrecognizer.
While this is less of a problem whenevaluating on manual transcriptions, the experiencereported in (Huang et al, 2001) suggests that therelatively high error rate of speech recognizers maynegatively affect performance of caller name ex-traction on automatically generated transcripts.
Wetherefore avoid using anything but a small numberof greetings and commonly occurring words like?hi?, ?this?, ?is?
etc.
and a small number of commonfirst names for extracting caller phrases and use po-sitional information in addition to word-based fea-tures.We locate caller phrases by first identifying theirstart position in the message and then predictingthe length of the phrase.
The empirical distribu-tion of caller phrase lengths in the development datais shown in Figure 2.
Most caller phrases are be-tween two and four words long (?it?s Pat?, ?this isPat Caller?)
and there are moderately good lexicalindicators that signal the end of a caller phrase (?I?,?could?, ?please?, etc.).
Again, we avoid the use ofnames as features and rely on a small set of fea-tures based on common words, in addition to phraselength, for predicting the length of the caller phrase.01020304050607080901000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16observed max.
: 47entropy: 3.11 bitsdensitycumulativeFigure 2: Empirical probability of a caller phrasebeing x words longWe have thus identified two classes of featuresthat allow us to predict the start of the caller phraserelative to the beginning of the message, as well asthe end of the caller phrase relative to its start.
Sincewe are dealing with discrete word indices in bothcases, we treat this as a classification task, ratherthan a regression task.
A large number of classifierlearners can be used to automatically infer classifiersfor the two subtasks at hand.
We chose a decisiontree learner for convenience and note that this choicedoes not affect the overall results nearly as much asmodifying our feature inventory.Since a direct comparison to the log-linear namedentity tagger described in (Huang et al, 2001) (werefer to this approach as HZP log-linear below) isnot possible due to the use of different corpora andannotation standards, we applied a similar namedentity tagger based on a log-linear model with tri-gram features to our data (we refer to this approachas Col log-linear as the tagger was provided byMichael Collins).
Table 1 summarizes precision (P),recall (R), and F-measure (F) for three approachesevaluated on manual transcriptions: row HZP log-linear repeats the results of the best model from(Huang et al, 2001); row Col log-linear containsthe results we obtained using a similar named entitytagger on our own data; and row JA classifiers showsthe performance of the classifier method proposed inthis section.Like Huang et al (2001), we count a proposedcaller phrase as correct if and only if it matchesthe annotation of the evaluation data perfectly.
Thenumbers could be made to look better by using con-tainment as the evaluation criterion, i.e., we wouldcount a proposed phrase as correct if it contained anactual phrase plus perhaps some additional material.While this may be more useful in practice (see be-low), it is not the objective that was maximized dur-ing training, and so we prefer the stricter criterionfor evaluation on previously annotated transcripts.P R FHZP log-linear .89 .80 .84Col log-linear .83 .78 .81JA classifiers .73 .68 .71Table 1: Performance of caller phrase extraction(manual transcriptions)While the results for the approach proposedhere appear clearly worse than those reported byHuang et al (2001), we hasten to point out that thisis most likely not due to any difference in the cor-pora that were used.
This is corroborated by the factthat we were able to obtain performance much closerto that of the best, finely tuned log-linear model from(Huang et al, 2001) by using a generic named entitytagger that was not adapted in any way to the par-ticular task at hand.
The log-linear taggers employn-gram features based on family names and otherparticular aspects of the development data that donot necessarily generalize to other settings, wherethe family names of the callers may be different ormay not be transcribed properly.
In fact, it seemsrather likely that the log-linear models and the fea-tures they employ over-fit the training data.This becomes clearer when one evaluates on un-seen transcripts produced by an automatic speechrecognizer (ASR),3 as summarized in Table 2.
RowsHZP strict and HZP containment repeat the figuresfor the best model from (Huang et al, 2001) whenevaluated on automatic transcriptions.
The differ-ence is that HZP strict uses the strict evaluation cri-terion described above, whereas HZP containmentuses the weaker criterion of containment, i.e., anextracted phrase counts as correct if it contains ex-actly one whole actual phrase.
Row JA containmentsummarizes the performance of our approach when3An automatic transcription is the single best word hypoth-esis of the ASR for a given voicemail message.evaluated on 101 unseen automatically transcribedmessages.
Since we did not have any labeled au-tomatic transcriptions available to compare with thepredicted caller phrase labels using the strict crite-rion, we only report results based on the weakercriterion of containment.
In fact, we count callerphrases as correct as long as they contain the fullname of the caller, since this is the common denom-inator in the otherwise somewhat heterogeneous la-beling of our training corpus; more on this issue inthe next section.P R FHZP strict .24 .16 .19HZP containment .73 .41 .52JA containment .74 .66 .70Table 2: Performance of caller phrase extraction (au-tomatic transcriptions)The difference between the approach in (Huang etal., 2001) and ours may be partly due to the perfor-mance of the ASR components: Huang et al (2001)report a word error rate of ?about 35%?, whereaswe used a recognizer (Bacchiani, 2001) with a worderror rate of only 23%.
Still, the reduced perfor-mance of the HZP model on ASR transcripts com-pared with manual transcripts is points toward over-fitting, or reliance on features that do not generalizeto ASR transcripts.
Our main approach, on the otherhand, uses classifiers that are extremely knowledge-poor in comparison with the many features of thelog-linear models for the various named entity tag-gers, employing no more than a few dozen categori-cal features.3.2 Caller NamesExtracting an entire caller phrase like ?this is PatCaller?
may not be all that relevant in practice: theprefix ?this is?
does not provide much useful infor-mation, so simply extracting the name of the callershould suffice.
This is more or less a problem withthe annotation standard used for marking up voice-mail transcripts.
We decided to test the effects ofchanging that standard post hoc.
This was relativelyeasy to do, since proper names are capitalized inthe message transcripts.
We heuristically identifycaller names as the leftmost longest contiguous sub-sequence of capitalized words inside a marked-upcaller phrase.
This leaves us with 6731 messageswith caller names in our development data.4As we did for caller phrases, we briefly examinethe distributions of the start position of caller names(see Figure 3) as well as their lengths (see Figure 4).Comparing the entropies of the empirical distribu-tions with the corresponding ones for caller phrasessuggests that we might be dealing with a simplerextraction task here.
The entropy of the empiricalname length distribution is not much more than onebit, since predicting the length of a caller name ismostly a question of deciding whether a first nameor full name was mentioned.01020304050607080901000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16observed max.
: 138entropy: 2.20 bitsdensitycumulativeFigure 3: Empirical probability of a caller namestarting x words into the message01020304050607080901000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16observed max.
: 8entropy: 1.17 bitsdensitycumulativeFigure 4: Empirical probability of a caller name be-ing x words long4The vast majority of messages that do not mention a nameas part of their caller phrase employ the caller phrase ?it?s me?,which would be easy to detect and treat separately.The performance comparison in Table 3 showsthat we are in fact dealing with a simpler task.
No-tice however that our method has not changed at all.We still use one classifier to predict the beginningof the caller name and a second classifier to predictits length, with the same small set of lexical featuresthat do not include any names other than a handfulof common first names.P R Fphrase .730 .684 .706name .860 .871 .865Table 3: Caller phrase vs. name extraction (manualtranscriptions)4 Phone NumbersThe development data contain 5303 marked-upphone numbers, for an average of almost 0.7 phonenumbers per non-empty message.
These phonenumbers fall into the following categories based ontheir realization:?
4472 (84%) consist exclusively of spoken num-bers?
679 (13%) consist of spoken numbers and thewords ?area?, ?code?, and ?extension??
152 (3%) have additional material, due to cor-rections, hesitations, fragments, and question-able markupNote that phone numbers in the North AmericanNumbering Plan are either ten or seven digits long,depending on whether the Numbering Plan Areacode is included or not.
Two other frequent lengthsfor phone numbers in the development data are four(for internal lines) and, to a lesser extent, eleven(when the long distance dialing prefix is included,as in ?one eight hundred .
.
.
?
).This allows us to formulate the following baselineapproach: find all maximal substrings consisting ofspoken digits (?zero?
through ?nine?)
and keep thoseof length four, seven, and ten.
Simple as it mayseem, this approach (which we call digits below)performs surprisingly well.
Its precision is morethan 78%, partly because in our corpus there do notoccur many seven or ten digit numbers that are notphone numbers.Named entity taggers based on conditional mod-els with trigram features are not particularly suitedfor this task.
The reason is that trigrams do not pro-vide enough history to allow the tagger to judge thelength of a proposed phone number: it inserts begin-ning and end tags without being able to tell how farapart they are.
Data sparseness is another problem,since we are dealing with 1000 distinct trigrams overdigits alone, so a different event model that replacesall spoken digits with the same representative tokenmight be better suited, also because it avoids over-fitting issues like accidentally learning area codesand other number patterns that are frequent in thedevelopment data.However, there is a more serious problem.
Evenif the distance between the start and end tags that anamed entity tagger predicts could be taken into ac-count, this would not help with all spoken renditionsof phone numbers.
For example, ?327-1025?
couldbe read aloud using only six words (?three two seventen twenty five?
), and might be incorrectly rejectedbecause it appears to be of a length that is not verycommon for phone numbers.We therefore approach the phone number extrac-tion task differently, using a two-phase procedure.In the first phase we use a hand-crafted grammar topropose candidate phone numbers.
This avoids allof the problems mentioned so far, yet the complex-ity of the task remains manageable because of therather simple structure of most phone numbers inour development data noted above.
The advantageis that it allows us to simultaneously convert spo-ken digits and numbers to a numeric representation,whose length can then be used as an important fea-ture for deciding whether to keep or throw away acandidate.
Note that such a conversion process isdesirable in any case, since a text-based applicationwould presumably want to present digit strings like?327-1025?
to a user, rather than ?three two seventen twenty five?.
This conversion step is not entirelytrivial, though: for example, one might transcribethe spoken words ?three hundred fourteen ninetynine?
as either ?300-1499?
or ?314.99?
depending onwhether they are preceded by ?call me back at?
vs. ?Ican sell it to you for?, for example.
But since we areonly interested in finding phone numbers, the extrac-tion component can treat all candidates it proposesas if they were phone numbers.Adjustments of the hand-crafted grammar wereonly made in order to increase recall on the devel-opment data.
The grammar should locate as manyactual phone numbers in the development corpus aspossible, but was free to also propose spurious can-didates that did not correspond to marked-up phonenumbers.
While it has recently been argued thatsuch separate optimization of recall and precision isgenerally desirable for certain learning tasks (Agar-wal and Joshi, 2001; Joshi et al, 2001), the mainadvantage in connection with hand-crafted compo-nents is simplified development.
Since we notedabove that 97% of all phone numbers in our devel-opment data are expressed fairly straightforwardlyin terms of digits, numbers, and a few other wordsparticular to the phone number domain, we mightexpect to achieve recall figures close to 97% withoutdoing anything special to deal with the remaining3% of difficult cases.
It was very easy to achieve thisrecall figure on the development data, while the ratioof proposed phone numbers to actual phone numberswas about 3.2 at worst.5A second phase is now charged with the task ofweeding through the set of candidates proposed dur-ing the first phase, retaining those that correspond toactual phone numbers.
This is a simple binary clas-sification task, and again many different techniquescan be applied.
As a baseline we use a classifierthat accepts any candidate of length four or more(now measured in terms of numeric digits, ratherthan spoken words), and rejects candidates of lengththree and less.
Without this simple step (which werefer to as prune below), the precision of our hand-crafted extraction grammar is only around 30%, butby pruning away candidate phone numbers shorterthan four digits precision almost doubles while re-call is unaffected.We again used a decision tree learner to automat-ically infer a classifier for the second phase.
Thefeatures we made available to the learner were thelength of the phone number in numeric digits, its5It would of course be trivial to achieve 100% recall by ex-tracting all possible substrings of a transcript.
The fact that ourgrammar extracts only about three times as many phrases asneeded is evidence that it falls within the reasonable subset ofpossible extraction procedures.distance from the end of the message, and a smallnumber of lexical cues in the surrounding context ofa candidate number (?call?, ?number?, etc.).
This ap-proach (which we call classify below) increases theprecision of the combined two steps to acceptablelevels without hurting recall too much.A comparison of performance results is presentedin Table 4.
Rows HZP rules and HZP log-linear re-fer to the rule-based baseline and the best log-linearmodel of (Huang et al, 2001) and the figures aresimply taken from that paper; row Col log-linearrefers to the same named entity tagger we used in theprevious section and is included for comparison withthe HZP models; row JA digits refers to the simplebaseline where we extract strings of spoken digits ofplausible lengths.
Our main results appear in the re-maining rows.
The performance of our hand-craftedextraction grammar (in row JA extract) was aboutwhat we had seen on the development data before,with recall being as high as one could reasonably ex-pect.
As mentioned above, using a simple pruningstep in the second phase (see JA extract + prune)results in a doubling of precision and leaves recallessentially unaffected (a single fragmentary phonenumber was wrongly excluded).
Finally, if we usea decision tree classifier in the second phase, wecan achieve extremely high precision with a minimalimpact on recall.
Our two-phase procedure outper-forms all other methods we considered.P R FHZP rules .81 .83 .82HZP log-linear .90 .83 .86Col log-linear .88 .93 .91JA digits .78 .70 .74JA extract .30 .96 .45JA extract + prune .59 .96 .73JA extract + classify .94 .94 .94Table 4: Performance of phone number extraction(manual transcriptions)We evaluated the performance of our best modelson the same 101 unseen ASR transcripts used abovein the evaluation of the caller phrase extraction.
Theresults are summarized in Table 5, which also re-peats the best results from (Huang et al, 2001), us-ing the same terminology as earlier: rows HZP strictand HZP containment refer to the best model from(Huang et al, 2001) ?
corresponding to row HZPlog-linear in Table 4 ?
when evaluated using thestrict criterion and containment, respectively; androw JA containment refers to our own best model?
corresponding to row JA extract + classify in Ta-ble 4.P R FHZP strict .56 .52 .54HZP containment .85 .79 .82JA containment .95 .94 .95Table 5: Performance of phone number extraction(automatic transcriptions)It is not very plausible that the differences be-tween the approaches in Table 5 would be due toa difference in the performance of the ASR compo-nents that generated the message transcripts.
Frominspecting our own data it is clear that ASR mistakesinside phone numbers are virtually absent, and wewould expect the same to hold even of an automaticrecognizer with an overall much higher word errorrate.
Also, for most phone numbers the labeling isuncontroversial, so we expect the corpora used byHuang et al (2001) and ourselves to be extremelysimilar in terms of mark-up of phone numbers.
Sothe observed performance difference is most likelydue to the difference in extraction methods.5 Conclusion and OutlookThe novel contributions of this paper can be summa-rized as follows:?
We demonstrated empirically that positionalcues can be an important source of informationfor locating caller names and phrases.?
We showed that good performance on the taskof extracting caller information can be achievedusing a very small inventory of lexical and po-sitional features.?
We argued that for extracting telephone num-bers it is extremely useful to take the lengthof their numeric representation into account.Our grammar-based extractor translates spokennumbers into such a numeric representation.?
Our two-phase approach allows us to efficientlydevelop a simple extraction grammar for whichthe only requirement is high recall.
This placesless of a burden on the grammar developersthan having to write an accurate set of rules likethe baseline of (Huang et al, 2001).?
The combined performance of our simple ex-traction grammar and the second-phase clas-sifier exceeded the performance of all othermethods, including the current state of the art(Huang et al, 2001).Our results point towards approaches that use asmall inventory of features that have been tailoredto specific tasks.
Generic methods like the namedentity tagger used by Huang et al (2001) may notbe the best tools for particular tasks; in fact, we donot expect the bigram and trigram features used bysuch taggers to be sufficient for accurately extract-ing phone numbers.
We also believe that using allavailable lexical information for extracting caller in-formation can easily lead to over-fitting, which canpartly be avoid by not relying on names being tran-scribed correctly by an ASR component.In practice, determining the identity of a callermight have to take many diverse sources of infor-mation into account.
The self-identification of acaller and the phone numbers mentioned in the samemessage are not uncorrelated, since there is usuallyonly a small number of ways to reach any particularcaller.
In an application we might therefore try to usea combination of speaker identification (Rosenberget al, 2001), caller name extraction, and recognizedphone numbers to establish the identity of the caller.An investigation of how to combine these sources ofinformation is left for future research.AcknowledgementsWe would like to thank Michiel Bacchiani, MichaelCollins, Julia Hirschberg, and the SCANMail groupat AT&T Labs.
Special thanks to Michiel Bacchianifor help with ASR transcripts and to Michael Collinsfor letting us use his named entity tagger.ReferencesRamesh C. Agarwal and Mahesh V. Joshi.
2001.
PNrule:A new classification framework in data mining (A casestudy in network intrusion detection).
In First SIAMInternational Conference on Data Mining, Chicago,IL.Michiel Bacchiani.
2001.
Automatic transcription ofvoicemail at AT&T.
In International Conference onAcoustics, Speech, and Signal Processing, Salt LakeCity, UT.Abhay Bhushan, Ken Pogran, Ray Tomlinson, and JimWhite.
1973.
Standardizing network mail headers.Internet RFC 561.David H. Crocker.
1982.
Standard for the format ofARPA internet text messages.
Internet RFC 822,STD 11.Yoshihiko Gotoh and Steve Renals.
2000.
Informa-tion extraction from broadcast news.
PhilosophicalTransactions of the Royal Society of London, SeriesA, 358:1295?1310.Julia Hirschberg, Michiel Bacchiani, Don Hindle, PhilIsenhour, Aaron Rosenberg, Litza Stark, Larry Stead,Steve Whittaker, and Gary Zamchick.
2001.
SCAN-Mail: Browsing and searching speech data by content.In 7th European Conference on Speech Communica-tion and Technology, Aalborg, Denmark.Jing Huang, Geoffrey Zweig, and Mukund Padmanab-han.
2001.
Information extraction from voicemail.
In39th Annual Meeting of the Association for Computa-tional Linguistics, Toulouse, France.Mahesh V. Joshi, Ramesh C. Agarwal, and Vipin Ku-mar.
2001.
Mining needles in a haystack: Classify-ing rare classes via two-phase rule induction.
In ACMSIGMOD International Conference on Management ofData, Santa Barbara, CA.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Empirical Methods inNatural Language Processing, Philadelphia, PA.Aaron Rosenberg, Julia Hirschberg, Michiel Bacchiani,S.
Parthasarathy, Philip Isenhour, and Larry Stead.2001.
Caller identification for the SCANMail voice-mail browser.
In 7th European Conference on SpeechCommunication and Technology, Aalborg, Denmark.
