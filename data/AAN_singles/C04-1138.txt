Multilingual and cross-lingual news topic trackingBruno Pouliquen, Ralf Steinberger, Camelia Ignat, Emilia K?sper & Irina TemnikovaJoint Research Centre, European CommissionT.P.
267, Via E. Fermi 121020 Ispra (VA), Italyhttp://www.jrc.it/langtechFirstname.Lastname@jrc.itAbstractWe are presenting a working system for automatednews analysis that ingests an average total of 7600news articles per day in five languages.
For eachlanguage, the system detects the major news storiesof the day using a group-average unsupervised ag-glomerative clustering process.
It also tracks, foreach cluster, related groups of articles publishedover the previous seven days, using a cosine ofweighted terms.
The system furthermore tracks re-lated news across languages, in all language pairsinvolved.
The cross-lingual news cluster similarityis based on a linear combination of three types ofinput: (a) cognates, (b) automatically detected ref-erences to geographical place names and (c) the re-sults of a mapping process onto a multilingual clas-sification system.
A manual evaluation showed thatthe system produces good results.1 IntroductionMost large organisations, companies and politi-cal parties have a department analysing the newson a daily basis.
Motivations differ, but often theseorganisations want to know how they and theirleading members are represented in the news, orthey need to know whether there has been anyevent they ought to know about.
Examples of ex-isting news gathering and analysis systems are In-formedia1 and the Europe Media Monitor (Best etal.
2002).
DARPA has taken an interest in the do-main and launched, in 1996, the Topic Detectionand Tracking task2 (TDT) under the TIDES pro-gram.
It distinguishes three major tasks: (a) seg-mentation of a continuous information flow (e.g.spoken news) into individual news items, (b) de-tection of breaking news, i.e.
of a new subject thathas not previously been discussed, and (c) topictracking, i.e.
the identification of related news overtime.
Our task is the analysis of a multilingual col-lection of written news articles, which means thatsegmentation (task a) is of no relevance.
Neitherdo we present here work on the detection of new1 http://www.informedia.cs.cmu.edu/2 http://www.nist.gov/speech/tests/tdt/topics (task b).
Instead, we focus on the topictracking task (c), and especially on the novel as-pect of cross-lingual tracking.The aim of our work is to provide an automati-cally generated overview over the major news ofeach day (midnight to midnight) in the languagesEnglish, German, French, Spanish and Italian.
Thecorpus consists of news items gathered from alarge number of internet news sites world-wide,and of various subscription news wires (Best et al2002).
The texts are thus from hundreds of differ-ent sources (feeds) which often discuss the sameevents.
Newspapers often publish the news theyreceive from press agencies with no or fewamendments.
The corpus of news articles thus con-tains not only summaries of the same events writ-ten by different journalists, but also many dupli-cates and near duplicates of the same original textwhich need to be eliminated from the collection.In order to identify the major news, we identifyclusters of similar news items, i.e.
news items thatdeal with the same subject.
All subjects that triggera large number of news articles from various feedsare of interest.
The related news thus do not neces-sarily have to discuss events, i.e.
things that happenat a particular time and place (e.g.
the 11/03 Ma-drid bombing), but they can also be a thread of dis-cussions on the same subject, such as the campaignfor the US presidential elections.In section 2, we summarise other work on topictracking, on cross-lingual news linking and on fea-ture extraction methods.
Section 3 describes themultilingual news corpus and the text feature ex-traction used for the document representation.
Insection 4, we present the process and evaluation ofmajor news identification.
Section 5 is dedicated tothe multi-monolingual topic tracking process andits evaluation.
Section 6 describes the cross-linguallinking of related clusters of major news, plusevaluation results.
Section 7 points to future work.2 Related workAllan et al (1998) identify new events and thentrack the topic like in an information filtering taskby querying new documents against the profile ofthe newly detected topic.
Topics are represented asa vector of stemmed words and their TF.IDF val-ues, only considering nouns, verbs, adjectives andnumbers.
In their experiments, using between 10and 20 features produced optimal results.
Schultz(1999) took the alternative approach of clusteringtexts with a single-linkage unsupervised agglom-erative clustering method, using cosine similarityand TF.IDF for term weighting.
He concludes that?a successful clustering algorithm must incorporatea representation for a cluster itself as group aver-age clustering does?.
We followed Schultz?
advice.Unlike Schultz, however, we use the log-likelihoodtest for term weighting as this measure seems to bebetter when dealing with varying text sizes (Kil-garriff 1996).
We do not consider parts-of-speech,lemmatisation or stemming, as we do not have ac-cess to linguistic resources for all the languages weneed to work with, but we use an extensive list ofstop words.Approaches to cross-lingual topic tracking arerather limited.
Possible solutions for this task are toeither translate documents or words from one lan-guage into the other, or to map the documents inboth languages onto some multilingual referencesystem such as a thesaurus.
Wactlar (1999) usedbilingual dictionaries to translate Serbo-Croatianwords and phrases into English and using the trans-lations as a query on the English texts to find simi-lar texts.
In TDT-3, only four systems tried to es-tablish links between documents written in differ-ent languages.
All of them tried to link English andChinese-Mandarin news articles by using MachineTranslation (e.g.
Leek et al 1999).
Using a ma-chine translation tool before carrying out the topictracking resulted in a 50% performance loss, com-pared to monolingual topic tracking.Friburger & Maurel (2002) showed that the iden-tification and usage of proper names, and espe-cially of geographical references, significantly im-proves document similarity calculation and cluster-ing.
Hyland et al (1999) clustered news and de-tected topics exploiting the unique combinations ofvarious named entities to link related documents.However, according to Friburger & Maurel (2002),the usage of named entities alone is not sufficient.Our own approach to cross-lingual topic track-ing, presented in section 6, is therefore based onthree kinds of information.
Two of them exploitthe co-occurrence of named entities in related newsstories: (a) cognates (i.e.
words that are the sameacross languages, including names) and (b) geo-graphical references.
The third component, (c) aprocess mapping texts onto a multilingual classifi-cation scheme, provides an additional, more con-tent-oriented similarity measure.
Pouliquen et al(2003) showed that mapping texts onto a multilin-gual classification system can be very successfulfor the task of identifying document translations.This approach should thus also be an appropriatemeasure to identify similar documents in otherlanguages, such as news discussing the same topic.3 Feature extraction for document represen-tationThe similarity measure for monolingual newsitem clustering, discussed in section 4, is a cosineof weighted terms (see 3.1) enriched with informa-tion about references to geographical place names(see 3.2).
Related news are tracked over time bycalculating the cosine of their cluster representa-tions, while setting certain thresholds (section 5).The cross-lingual linking of related clusters, as de-scribed in section 6, additionally uses the results ofa mapping process onto a multilingual classifica-tion scheme (see 3.3).The news corpus consists of a daily average of3350 English news items, 2100 German, 870 Ital-ian, 800 French and 530 Spanish articles, comingfrom over three hundred different internet sources.3.1 Keyword identificationFor monolingual applications, we representdocuments by a weighted list of their terms.
Forthe weighting, we use the log-likelihood test,which is said to perform better than the alternativesTF.IDF or chi-square when comparing documentsof different sizes (Kilgarriff 1996).
The referencecorpus was produced with documents of the sametype, i.e.
news articles.
It is planned to update thereference word frequency list daily or weekly so asto take account of the temporary news bias towardsspecific subjects (e.g.
the Iraq war).
We set the p-value to 0.01 in order to limit the size of the vectorto the most important words.
Furthermore, we usea large list of stop words that includes not onlyfunction words, but also many other words that arenot useful to represent the contents of a document.We do not consider part-of-speech information anddo not carry out stemming or lemmatisation, inorder to increase the speed of the process and to beable to include new languages quickly even if wedo not have linguistic resources for them.
Cluster-ing results do not seem to suffer from this lack oflinguistic normalisation, but when we extend thesystem to more highly inflected languages, we willhave to see whether lemmatisation will be neces-sary.
The result of the keyword identification proc-ess is thus a representation of each incoming newsarticle in a vector space.3.2 Geographical Place Name RecognitionFor place name recognition, we use a system thathas been developed by Pouliquen et al (2004).Compared to other named entity recognition sys-tems, this tool has the advantage that it recognisesexonyms (foreign language equivalences, e.g.
Ven-ice vs. Venezia) and that it disambiguates betweenplaces with the same name (e.g.
Paris in France vs.the other 13 places called Paris in the world).However, instead of using the city and regionnames as they are mentioned in the article, eachplace name simply adds to the country score ofeach article.
The idea behind this is that the placenames themselves are already contained in the listof keywords.
By adding the country score sepa-rately, we heighten the impact of the geographicalinformation on the clustering process.The country scores are calculated as follows: foreach geographical place name identified for agiven country, we add one to the country counter.We then normalise this value using the log-likelihood value, using the average country counterin a large number of other news articles as a refer-ence base.
As with keywords, we plan to updatethe country counter reference frequency list on adaily or weekly basis.
The resulting normalisedcountry score has the same format as the keywordlist so that it can simply be added to the documentvector space representation.3.3 Mapping documents onto a multilingualclassification schemeFor the semantic mapping of news articles, weuse an existing system developed by Pouliquen etal.
(2003), which maps documents onto a multilin-gual thesaurus called Eurovoc.
Eurovoc is a wide-coverage classification scheme with approximately6000 hierarchically organised classes.
Each of theclasses has exactly one translation in the currently22 languages for which it exists.
The system car-ries out category-ranking classification using Ma-chine Learning methods.
In an inductive process, itbuilds a profile-based classifier by observing themanual classification on a training set of docu-ments with only positive examples.
The outcomeof the mapping process is a ranked list of the 100most pertinent Eurovoc classes.
Due to the multi-lingual nature of Eurovoc, this representation isindependent of the text language so that it is verysuitable for cross-lingual document similarity cal-culation, as was shown by Pouliquen et al (2003).4 Clustering of news articlesIn this process, larger groups of similar articlesare grouped into clusters.
Unlike in document clas-sification, clustering is a bottom-up, unsupervisedprocess, because the document classes are notknown beforehand.4.1 Building a dendrogramIn the process, we build a hierarchical clusteringtree (dendrogram), using an agglomerative algo-rithm (Jain et al 1999).
In a first step, (1) we cal-culate the similarity between each document pairin the collection (i.e.
one full day of news in onelanguage), applying the cosine formula to thedocument vector pairs.
The vector for each singledocument consists of its keywords and their log-likelihood values, enhanced with the country pro-file as described in sections 3.1 and 3.2.
(2) Whentwo or more documents have a cosine similarity of90% or more, we eliminate all but one of them aswe assume that they are duplicates or near-duplicates, i.e.
they are exact copies or slightlyamended versions of the same news wire.
(3) Wethen combine the two most similar documents intoa cluster, for which we calculate a new representa-tion by merging the two vectors into one.
For thenode combining the two documents, we also havean intra-cluster similarity value showing the degreeto which the two documents are similar.
For therest of the clustering process, this node will betreated like a single document, with the exceptionthat it will have twice the weight of a single docu-ment when being merged with another documentor cluster of documents.
We iteratively repeat steps(1) and (3) so as to include more and more docu-ments into the binary dendrogram until all docu-ments are included.
The resulting dendrogram willhave clusters of articles that are similar, and a listof keywords and their weight for each cluster.
Thedegree of similarity for each cluster is shown by itsintra-cluster similarity value.4.2 Cluster extraction to identify main eventsIn a next step, we search the dendrogram for themajor news clusters of the day, by identifying allsub-clusters of documents that fulfil the followingconditions: (a) the intra-cluster similarity (clustercohesiveness) is above the threshold of 50%; (b)the number X of articles in the cluster is at least0.6% of the total number of articles of that lan-guage per day; (c) the number Y of different feedsis at least half the minimum number of articles percluster (Y = X/2).The threshold of 50% in (a) was chosen becauseit guarantees that most related articles are includedin the cluster, while unrelated ones are mostly ex-cluded (see section 4.3).
The minimum number ofarticles per cluster in (b) was chosen to limit thenumber of major news clusters per day.
We re-quested a minimum number of different newsfeeds (c) so as to be sure that the news items are ofgeneral interest and that we are not dealing withsome newspaper-specific or local issues.With the current settings, the system produces anaverage of 9 English major news clusters per day,11 Italian, 16 German, 20 French and 21 Spanish.The varying numbers indicate that the settingsshould probably be changed so as to produce asimilar number of major news clusters per day inthe various languages.
Most likely, the minimumnumber of feeds should have an upper maximumvalue for languages like English with thousands ofnews articles per day.For each cluster, we have the following informa-tion: number of articles, number of sources (feeds),intra-cluster similarity measure and keywords.
Us-ing our group-average approach we also have thecentroid of the cluster (i.e.
the vector of featuresthat represents the cluster).
For each cluster, wecompute the article that is most similar to the cen-troid (short: the centroid article).
We use the titleof this centroid article as the title for the clusterand we present this article to the users as a firstdocument to read about the contents of the wholecluster.The collection of clusters is mainly presented tothe users as a flat and independent list of clusters.However, as we realised that some of the clustersare more related than others (e.g.
with the recentinterest in Iraq, there are often various clusterscovering different aspects of the political situationof the country), we position clusters with an inter-cluster similarity of over 30% closer to each otherwhen presenting them to the users.4.3 Evaluation of the monolingual clusteringThe evaluation of clustering results is rathertricky.
According to Joachims (2003), clusteringresults can be evaluated using a variety of differentways: (a) let the market decide (select the winner);(b) ask end users; (c) measure the ?tightness?
or?purity?
of clusters; (d) use human-identified clus-ters to evaluate system-generated ones.
The lastsolution (d) is out of our reach because it is veryresource-consuming; several evaluators would beneeded for cross-checking the human judgement.The ?market?
(a) and user groups (b) will use andevaluate our system in the near future, but we needto evaluate the system prior to showing it to a largenumber of customers.
We therefore focus onmethod (c) by letting a person judge how consis-tently the articles of each cluster treat the samestory.We evaluated the major clusters of English newsarticles (using the 50% intra-cluster similaritythreshold) produced for the seven-day period start-ing 9 March 2004.
During this period, 71 clusterscontaining 1072 news articles were produced.
Theevaluator was asked to decide, for each cluster andon a four-grade scale, to what extent the clusteredarticles were related to the centroid article.
Com-paring the clustered articles to the centroid articlewas chosen over evaluating the homogeneity of thecluster because it is both easier and closer to thereal-life situation of the users: users will enter thecluster via the centroid article and will judge theother articles according to whether or not they con-tain the information they expect.
The evaluationscale distinguishes the following ratings:(0) wrong link, e.g.
Madrid football results vs.Madrid elections; this is a hypothetical exam-ple as no such link was found.
(1) loosely connected story, e.g.
Welsh documen-tary on drinking vs. alcohol policy in Britain;(2) interlinked news stories, e.g.
11/03 Madridbombing vs. elections of the Spanish PrimeMinister Zapatero vs. Spanish decision to pulltroops out of Iraq;(3) same news story.In the evaluation, 91.5% of the articles wererated as good (3), 7.7% were rated as interlinked(2) and 0.8% were rated as loosely connected.
Nowrong links were found.
47 of the 71 clusters onlycontained good articles (3).
Loosely connected ar-ticles (1) were distributed evenly.
No more thantwo  articles of this rating were found in a singlecluster.
They never amounted to more than 17% ofall articles in a cluster (2 out of 12 articles).An evaluation of the clusters produced on oneday?s data with 30% and 40% intra-cluster similar-ity thresholds showed that the performance de-creased drastically.
In 30%-clusters, we found sev-eral wrong links (category 0), while no such wronglinks were found in the 50%-clusters.
The totalnumber of wrong (0) or loosely connected (1) arti-cles went up from one (in the 50%-cluster for thatday) to 37.
Furthermore, the worst clusters con-tained over 50% of such unrelated articles.
The40%-clusters were of a slightly better quality, butthey still were clearly less good than the 50%-clusters: The percentage of wrong (0) and looselyconnected (1) articles only went up from 0.8% (inthe 50%-clusters) to 4%, but some of the 40%-clusters still had more bad (category 0 or 1) thangood (category 2 or 3) articles.
These numbersconfirm that our choice of the 50% intra-clustersimilarity threshold is most useful.We have not produced a quantitative evaluationof the miss rate of the clustering process (i.e.
thenumber of related articles not included in the clus-ter, showing the recall).
However, a full-textsearch of the relevant proper names in the rest ofthe news collection showed that the clusteringprocess missed very few related articles.
In anycase, from our users?
point of view, it is muchmore important to know the major news stories ofa specific day than being able to access all articleson the subject.Statistical evaluation showed no correlation be-tween cluster size and accuracy.
However, cate-gory (2) results were more frequently found inclusters pertaining to news stories that go on for along time, such as the US presidential elections.These stories get wide coverage without being?breaking news?, and many of the articles involvedare commentaries.
Some of the category (2) resultswere also found in stories around the Madridbombing and its consequences: some articles dis-cussed the bombing itself on 11 March (number ofdead, investigation, mourning); others discussedthe fact that, in the 14 March elections, the Spanishpeople elected the socialists as they felt that formerPrime Minister Aznar?s politics were partially re-sponsible for this tragedy; yet other articles dis-cussed the post-election consequences such as thedecision of the new Socialist government to pullout the Spanish troops from Iraq, etc.
Many of thearticles touched upon several of these issues.
Arti-cles were rated as good (3) if they had at least onecore topic in common with the centroid article.5 Monolingual linking of news over timeEstablishing automatic links between the majorclusters of news published in one language in thelast 24 hours and the news published in previousdays can help users in their analysis of events.
Es-tablishing historical links between related newsstories is the third of the TDT tasks (see the intro-duction in section 1).We track topics by calculating the cosine simi-larity between all major news clusters of one daywith all major news clusters of the previous days,currently up to a maximum distance of seven days.The input for the similarity calculation is the clus-ter vector produced by the monolingual clusteringprocess (see section 4.2).
The output for each pair-wise similarity calculation is a similarity value be-tween 0 and 1.
Whether we decide that two clustersare related or not depends on the similarity thresh-old we set.
We found that related clusters over timehave an extremely high similarity, often around90%, which shows that the vocabulary used innews stories over time changes very little.
For test-ing purposes, we set the threshold very low, at15%, so that we could determine a useful thresholdduring the evaluation process.5.1 Evaluation of historical linkingWe evaluated the historical links for the 136English clusters of major news produced for thetwo-week period starting on 9 March 2004, look-ing at the seven-day window preceding the day forwhich each major news cluster was identified.
Thetotal number of historical links found for this pe-riod is 228, i.e.
on average 1.68 historical links permajor news cluster.
However, for 42 of the 136major news clusters, the system did not find anyrelated news clusters with a similarity of 15% ormore.We made a binary distinction between ?closelyrelated articles?
(+) and ?unrelated, or not so re-lated articles?
(?
).The evaluation results at varyingcosine similarity thresholds, displayed in Table 1,show that there is no threshold which includes allgood clusters and excludes all bad ones.
Setting thethreshold at 40% would mean that 173 (135+24+14) of the 203 good clusters (86%) would be foundwhile three bad ones would also be shown to theuser.
Setting the threshold at the more inclusivelevel of 20% would mean that 199 of the 203 goodclusters (98%) would be found, but the number ofunrelated ones would increase to 17.Similarity + Related  ?
Unrelated15 ?
19% 4 820 ?
39% 26 1440 ?
59% 14 260 ?
79% 24 080 ?
100% 135 1Total 203 25Table 1: Evaluation, for varying similarity thresh-olds, of the automatically detected links betweenmajor news of the day and the major news pub-lished in the seven days before.
The distinctionwas binary: Related (+) or Not (so) related (?
).6 Cross-lingual linking of news clustersNews analysts and employees in press roomsand public relations departments often want to seehow the same news is discussed in different coun-tries.
To allow easy access to related news in otherlanguages, we establish cross-lingual links betweenthe clusters of major news stories.
As major newsin one country sometimes is only minor news inanother, we calculate a second, alternative group ofnews clusters for each language and each day, con-taining a larger number of smaller clusters.
To getthis alternative group of clusters, we set the intra-cluster similarity to 25% and require that the newsof the cluster come from at least two differentnews sources.
These conditions are much weakerthan the requirements described in section 4.2.
Foreach major news cluster (50% intra-cluster similar-ity) per day and per language, we thus try to findrelated news in the other languages among any ofthe smaller clusters produced with the 25% intra-cluster similarity requirement.We use three types of input for the calculation ofcross-lingual cluster similarity: (a) the vector ofkeywords, as described in section 3.1, not en-hanced with geographical information, (b) thecountry score vector, as described in section 3.2,and (c) the vector of Eurovoc descriptors, as de-scribed in section 3.3.
The impact of the threecomponents is currently set to 20%, 30% and 50%respectively.
Using the Eurovoc vector alonewould give very high similarity values for, say,news about elections in France and in the UnitedStates.
By adding the country score, a considerableweight in the cross-lingual similarity calculation isgiven to the countries that are mentioned in eachnews cluster.
The overlap between the keywordvectors of documents in two different languageswill, of course, be extremely little, but it increaseswith the number of named entities that the docu-ments have in common.
According to Gey (2000),30% of content-bearing words in journalistic textare proper names.The system ignores individual articles, but calcu-lates the similarity between whole clusters of thedifferent languages.
The country score and theEurovoc descriptor vector are thus assigned to thecluster as a whole, treating all articles of each clus-ter like one big bag of words.6.1 Evaluation of cross-lingual cluster linksThe evaluation for the cross-lingual linking wascarried out on the same corpus as the evaluation ofthe historical links, i.e.
taking the 136 English ma-jor news clusters as a starting point.
Cross-lingualcluster links were evaluated for two languages,English to French and English to Italian.
Theevaluation was again binary, i.e.
clusters were ei-ther judged as being ?closely related?
(+) or ?unre-lated, or not so related?
(?).
For 31 English clus-ters, no French cluster was found.
Similarly, for 32English clusters, no Italian cluster was found.
Thismeans that for almost 25% of the English-speakingmajor news stories (31/136), there was no equiva-lent news cluster in the other languages.For the remaining English clusters, a total of 131French and 133 Italian clusters were detected bythe system, i.e.
on average more than one for eachEnglish cluster.
However, when several relatednews clusters were found, only the one with thehighest score was considered in the evaluation.Table 2 not only shows that the English-Italianlinks are less reliable than the English-French ones(the Italian document representation is inferior tothe French one because we spent less effort on op-timising the Italian keyword assignment), but alsothat the quality of cross-lingual links is generallylower than the historical links presented in sec-tion 5.1.
If we set the threshold for identifying re-lated news across languages to 30%, the systemcatches 74 of the 75 good French clusters (99%)and 67 of the 69 Italian clusters (97%).
However,the system then also proposes 13 bad French and12 bad Italian clusters to the users.
Setting thethreshold higher would decrease the number ofwrong hits.
However, we decided to use thethreshold of 30% because we consider it importantfor users to be able to find related news in otherlanguages.
Furthermore, unrelated clusters are usu-ally very easy to detect just by looking at the titleof the cluster.Similarity FR +  FR ?
IT + IT ?15 ?
19% 0 7 0 120 ?
29% 1 6 2 1130 ?
39% 5 6 7 840 ?
49% 16 4 13 550 ?
59% 19 1 18 660 ?
100% 34 1 29 1Total 75 25 69 32Table 2: Evaluation, for varying similarity thresh-olds, of the automatically detected cross-linguallinks between English major news and French (FR)or Italian (IT) news of the same day.
The distinc-tion was binary: Related (+) or Not (so) related (?
).7 Conclusion and future workWe have shown that our system can rather accu-rately identify clusters of major news per day infive languages and that it can link these clusters torelated news over time (topic tracking).
The mostinteresting and novel feature of the system is, how-ever, that it can also identify related news acrosslanguages, without translating articles or using bi-lingual dictionaries.
This cross-lingual cluster simi-larity is achieved by a combination of three featuresets, which currently have an impact of 50%, 30%and 20%, respectively: the main feature set is themapping onto the multilingual classificationscheme Eurovoc; the others are the countries re-ferred to in the articles (direct mention of the coun-try, or of a smaller place name of that country) andthe cognates (same strings used in the articlesacross languages, i.e.
mainly named entities).
Theevaluation has shown that the results are good, butthat the cross-lingual linking performs less wellthan the monolingual historical linking of relatednews clusters.
Users felt that the system performswell enough for it to go online soon, for usage by alarge user community of several thousand people.Improvements to the system will nevertheless besought.Future work will include testing different set-tings concerning the relative impact of the threecomponents, as well as detecting and using morenamed entities such as absolute and relative dateexpressions, proper names, etc.
A further aim is toextend the system to another six languages.The usage of cognate similarity could be im-proved.
Currently it will not work with Greek, forinstance, except for a few proper names.
We wouldtherefore like to experiment with multi-lingualstemming methods to exploit the existence of simi-lar words across languages such as English ele-phant, French ?l?phant, Spanish and Italian ele-fante and German Elefant.Several customer groups requested an advancednews analysis that distinguishes between articlesabout concrete events and articles commentingabout these events.
We will explore this issue, butit is very likely that this distinction will require asyntactic analysis of the news and cannot be madewith our bag-of-words approach.Finally, we intend to work on breaking news de-tection, i.e.
detecting new events, as opposed todetecting major news.
This work will requireworking on smaller time windows than the current24-hour window.8 AcknowledgementsWe would like to thank the Web Technology groupof the Joint Research Centre for their collaborationand for giving us access to their valuable multilin-gual news collection.
Our special thanks goes toClive Best, Erik van der Goot, Ken Blackler andTeofilo Garcia.
We would also like to thank ourformer colleague Johan Hagman for introducing usto the methods and usefulness of cluster analysis.ReferencesAllan James, Ron Papka & Victor Lavrenko(1998).
On-line New Event Detection and Track-ing.
Proceedings of the 21st Annual InternationalACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 37-45.
Melbourne, AustraliaBest Clive, Erik van der Goot, Monica de Paola,Teofilo Garcia & David Horby (2002).
EuropeMedia Monitor ?
EMM.
JRC Technical Note No.I.02.88.
Ispra, Italy.Friburger N. & D. Maurel (2002).
Textual Similar-ity Based on Proper Names.
Proceedings of theworkshop Mathematical/Formal Methods in In-formation Retrieval (MFIR?2002) at the 25thACM SIGIR Conference, pp.
155-167.
Tampere,Finland.Gey Frederic (2000).
Research to Improve Cross-Language Retrieval ?
Position Paper for CLEF.In C. Peters (ed.
): Cross-Language InformationRetrieval and Evaluation, Workshop of Cross-Language Evaluation Forum (CLEF?2000), Lis-bon, Portugal.
Lecture Notes in Computer Sci-ence 2069, Springer.Hyland R., C. Clifton & R. Holland (1999).
Geo-NODE: Visualizing News in Geospatial Context.In Afca99.Jain A., M. Murty & P. Flynn (1999).
Data cluster-ing: a review.
Pages 264Joachims Thorsten (2003).
Representing and Ac-cessing Digital Information.
Available at http://www.cs.cornell.edu/Courses/cs630/2003fa/lectures/tclust.pdfKilgarriff A.
(1996) Which words are particularlycharacteristic of a text?
A survey of statisticalapproaches.
Proceedings of the AISB Workshopon Language Engineering for Document Analy-sis and Recognition.
Sussex, 04/1996, pp.
33-40.Leek Tim, Hubert Jin, Sreenivasa Sista & RichardSchwartz (1999).
The BBN Crosslingual TopicDetection and Tracking System.
In 1999 TDTEvaluation System Summary Papers.http://www.nist.gov/speech/tests/tdt/tdt99/papersPouliquen Bruno, Ralf Steinberger & Camelia Ig-nat (2003).
Automatic identification of documenttranslations in large multilingual document col-lections.
Proceedings of the International Con-ference Recent Advances in Natural LanguageProcessing (RANLP'2003), pp.
401-408.
Borov-ets, Bulgaria, 10 - 12 September 2003.Pouliquen Bruno, Ralf Steinberger, Camelia Ignat& Tom de Groeve (2004).
Geographical Infor-mation Recognition and Visualisation in TextsWritten in Various Languages.
Proceedings ofthe 2004 ACM Symposium on Applied Comput-ing, Session on Information Access and Retrieval(Nicosia, Cyprus), Volume 2 of 2, pages 1051-1058.
New York.Schultz J. Michael & Mark Liberman (1999).Topic detection and Tracking using idf-weightedCosine Coefficient.
DARPA Broadcast NewsWorkshop Proceedings.Wactlar H.D.
(1999).
New Directions in Video In-formation Extraction and Summarization.
InProceedings of the 10th DELOS Workshop, Sa-norini, Greece, 24-25 June 1999.
