Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 89?96Manchester, August 2008Are Morpho-Syntactic Features More Predictive forthe Resolution of Noun Phrase Coordination Ambiguitythan Lexico-Semantic Similarity Scores?Ekaterina Buyko and Udo HahnJena UniversityLanguage & Information Engineering (JULIE) LabFu?rstengraben 30, 07743 Jena, Germanyekaterina.buyko|udo.hahn@uni-jena.deAbstractCoordinations in noun phrases often posethe problem that elliptified parts have tobe reconstructed for proper semantic inter-pretation.
Unfortunately, the detection ofcoordinated heads and identification of el-liptified elements notoriously lead to am-biguous reconstruction alternatives.
Whilelinguistic intuition suggests that semanticcriteria might play an important, if not su-perior, role in disambiguating resolutionalternatives, our experiments on the re-annotated WSJ part of the Penn Treebankindicate that solely morpho-syntactic crite-ria are more predictive than solely lexico-semantic ones.
We also found that thecombination of both criteria does not yieldany substantial improvement.1 IntroductionLooking at noun phrases such as?cat and dog owner?
?novels and travel books?their proper coordination reading (and asymmetricdistribution of coordinated heads) as?cat owner?
AND ?dog owner??novels?
AND ?travel books?seems to be licensed by the striking semantic sim-ilarity between ?cat?
and ?dog?, and ?novels?
and?books?, respectively.
If this were a general rule,then automatic procedures for the resolution of co-ordination ambiguities had to rely on the a prioriprovision of potentially large amounts of semanticc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.background knowledge to make this similarity ex-plicit.
Furthermore, any changes in languages ordomains where such resources were missing (or,were incomplete) would severely hamper coordi-nation analysis.Indeed, previous research has gathered lot ofevidence that conjoined elements tend to be se-mantically similar.
The important role of seman-tic similarity criteria for properly sorting out con-juncts was first tested by Resnik (1999).
He in-troduced an information-content-based similaritymeasure that uses WORDNET (Fellbaum, 1998) asa lexico-semantic resource and came up with theclaim that semantic similarity is helpful to achievehigher coverage in coordination resolution for co-ordinated noun phrases of the form ?noun1 andnoun2 noun3?
than similarity measures based onmorphological information only.In a similar vein, Hogan (2007b) inspectedWORDNET similarity and relatedness measuresand investigated their role in conjunct identifi-cation.
Her data reveals that several measuresof semantic word similarity can indeed detectconjunct similarity.
For the majority of thesesimilarity measures, the differences between themean similarity of coordinated elements and non-coordinated ones were statistically significant.However, it also became evident that these wereonly slight differences, and not all coordinatedheads were semantically related as evidenced, e.g.,by ?work?/?harmony?
in ?hard work and harmony?.The significance tests did also not reveal particu-larly useful measures for conjunct identification.Rus et al (2002) in an earlier study presented analternative heuristics-based approach to conjunctidentification for coordinations of the form ?noun1and noun2 noun3?.
They exploit, e.g., look-upsin WORDNET for a compound noun as a con-89cept, and for the sibling relation between nouns inthe coordination and report bracketing precision of87.4% on 525 candidate coordinations.
Althoughthe authors demonstrated that WORDNET was re-ally helpful in coordination resolution, the eval-uation was only conducted on compound nounsextracted from WORDNET?s noun hierachy and,furthermore, the senses of nouns were manuallytagged in advance for the experiments.Despite this preference for semantic criteria, onemight still raise the question how far non-semanticcriteria might guide the resolution of noun phrasecoordination ambiguities, e.g., by means of the dis-tribution of resolution alternatives in a large corpusor plain lexical or morpho-syntactic criteria.
Thisidea has already been explored before by variousresearchers from different methodological anglesincluding distribution-based statistical approaches(e.g., Chantree et al (2005), Nakov and Hearst(2005)), similarity-based approaches incorporat-ing orthographical, morpho-syntactic, and syntac-tic similarity criteria (e.g., Agarwal and Boggess(1992), Okumura and Muraki (1994)), as well asa combination of distribution information and syn-tactic criteria (Hogan, 2007a).Statistical approaches enumerate all candidateconjuncts and calculate the respective likelihoodaccording to a distribution estimated on a cor-pus.
For the coordination ?movie and televisionindustry?
the distributional similarity of ?movie?and ?industry?
and the collocation frequencies ofthe pairs [?movie?
- ?industry?]
and [?television?
-?industry?]
would be compared against each other.However, for such an approach only an F-measureunder 50% was reported (Chantree et al, 2005).Unsupervised Web-distribution-based algorithms(Nakov and Hearst, 2005) achieved 80% on thedisambiguation of coordinations of the fixed form?noun1 and noun2 noun3?.
Hogan (2007a) pre-sented a method for the disambiguation of nounphrase coordination by modelling two sources ofinformation, viz.
distribution-based similarity be-tween conjuncts and the dependency between con-junct heads.
This method was incorporated inBikel?s parsing model (Bikel, 2004) and achievedan increase in NP coordination dependency F-score from 69.9% to 73.8%.Similarity-based approaches consider those el-ements of a coordination as conjuncts which aremost ?similar?
under syntactic, morphological, oreven semantic aspects.
Agarwal and Boggess(1992) include in their NP coordination analysissyntactic and some semantic information aboutcandidate conjuncts and achieve an accuracy boostup to 82%.
Okumura and Muraki (1994) estimatethe similarity of candidate conjuncts by means ofa similarity function which incorporates syntactic,orthographical, and semantic information aboutthe conjuncts.
The model provides about 75% ac-curacy.The resolution of coordination ambiguity canalso be tried at parsing time.
Charniak and John-son (2005), e.g., supply a discriminative rerankerthat uses e.g., features to capture syntactic paral-lelism across conjuncts.
The reranker achieves anF-score of 91%.Recently, discriminative learning-based ap-proaches were proposed, which exploit only lex-ical, morpho-syntactic features and the symmetryof conjuncts.
Shimbo and Hara (2007) incorpo-rate morpho-syntactic and symmetry features ina discriminative learning model and end up with57% F-measure on the GENIA corpus (Ohta et al,2002).
Buyko et al (2007) employ ConditionalRandom Fields (Lafferty et al, 2001) and success-fully tested this technique in the biomedical do-main for the identification and resolution of ellipti-fied conjuncts.
They evaluate on the GENIA corpusand report an F-score of 93% for the reconstruc-tion of the elliptical conjuncts employing lexicaland morpho-syntactic criteria only.
At least twoquestions remain ?
whether the latter approachcan achieve similar results in the newswire lan-guage domain (and is thus portable), and whetherthe incorporation of additional semantic criteria inthis approach might boost the resolution rate, ornot (and is thus possibly more parsimonious).
Thelatter question is the main problem we deal with inthis paper.2 Data Sets for the Experiments2.1 Coordination Annotation in the PENNTREEBANKFor our experiments, we used the WSJ part of thePENN TREEBANK (Marcus et al, 1993).
Some re-searchers (e.g., Hogan (2007a)) had recently foundseveral inconsistencies in its annotation of thebracketing of coordinations in NPs.
These bugswere shown to pose problems for training and test-ing of coordination resolution and parsing tools.Fortunately, a re-annotated version has been pro-vided by Vadas and Curran (2007), with a focus90on the internal structure of NPs.
They added addi-tional bracketing annotation for each noun phrasein the WSJ section of the PENN TREEBANK as-suming a right-bracketing structure in NPs.
In ad-dition, they introduced tags, e.g., ?NML?
for ex-plicitly marking any left-branching constituents asin(NP (NML (JJ industrial) (CC and) (NN food))(NNS goods))where ?industrial?
and ?food?
are conjuncts.
In theexample(NP (DT some) (NN food) (CC and) (NN house-hold) (NNS goods))the structure of the noun phrase is already cor-rect and should not be annotated further, since?household goods?
is already right-most and is co-ordinated with ?food?.
Still, in the original PENNTREEBANK annotation, we find annotations ofnoun phrases such as(NP (NN royalty) (CC and) (NP (NN rock)(NNS stars)))that remain unchanged after the re-annotation pro-cess.2.2 Coordination CorpusWe, first, extracted a set of 3,333 non-nested NPcoordinations involving noun compounds and oneconjunction, with a maximal number of nine nouns(no prepositional phrases were considered).
Wefocused on two patterns in the re-annotated WSJportion:(1) Noun phrases containing at least two nouns anda conjunction as sister nodes as in(NP (NML (NN movie) (CC and) (NN book))(NNS pirates))or in(NP (DT some) (NN food) (CC and) (NN house-hold) (NNS goods))(2) Noun phrases containing at least two nounphrases and a conjunction as sister nodes (asthey remained unchanged from the original PENNTREEBANK version).
Thereby, the second nounphrase contains at least two nouns as sister nodesas in(NP (NP (NNP France)) (CC and) (NP (NNPHong) (NNP Kong)))We removed from this original set NPs whichcould not be reduced to the following pattern:11These are typically coordinations of the form ?
(W )N1 andN2?, e.g., ?government sources and lobbyists?, where W is asequence of i tokens (i ?
0).
646 coordinations of this typeoccurred in the WSJ portion of the PTB.
(W ) N1 and (W ) N2 N3,where (W ) is a sequence of i tokens with i ?
0 asin ?street lampsN1 and ficusN2 treesN3?.The remaining major data set (A) then contained2,687 NP coordinations.
A second data set (B)was formed, which is a proper subset of A andcontained only those coordination structures thatmatch the following pattern:(X) N1 and (W ) N2 N3,where (X) is defined as a sequence of i tokens(i ?
0) with all part-of-speech (POS) tags exceptnouns and (W ) defined as above; e.g., ?a happycatN1 and dogN2 ownerN3?.
Test set B contains,in our opinion, a selection of less ?hard?
coordi-nations from the set A, and includes 1,560 items.All these patterns focus on three forms of con-junctions, namely ?and?, ?or?, and ?but not?, whichconnect two conjuncts (the extension of whichvaries in our data from one up to maximally eighttokens as in ?London?s ?Big Bang?
1986 deregu-lation and Toronto?s ?Little Bang?
the same year?.The remainders from the conjunctions and thetwo conjuncts in a coordinated NP are calledshared elements (e.g., ?owner?
and ?a happy?
inthe above example).
It is evident that the correctrecognition of conjunct boundaries allows for theproper identification of the shared elements.Set A contains 1,455 coordinations where N1and N3 are coordinated (e.g, ?food and householdgoods?)
and 1,232 coordinations where N1 and N2are coordinated (e.g., ?cotton and acetate fibers?
).Set B consists of 643 coordinations where N1 andN3 are coordinated and 917 coordinations whereN1 and N2 are coordinated.The extracted data sets were converted into anIO representation of tokens labeled as ?C?
for con-junct, ?CC?
for conjunction, and ?S?
for the sharedelement(s).
The noun phrase ?cotton and acetatefibers?, e.g., is represented as a sequence ?C CCC S?, while ?food and household goods?
is repre-sented as a sequence ?C CC C C?.3 MethodsWe here compare three different approaches to theresolution of noun phrase coordination ambiguity,viz.
ones relying solely on morpho-syntactic infor-mation, solely on lexico-semantic information, anda cumulative combination of both.
As far as se-mantic information is concerned we make use ofvarious WORDNET similarity measures.913.1 BaselinesWe used three baselines for resolving noun phrasecoordination ambiguities ?
one incorporatingonly lexico-semantic information, the WordNetSimilarity baseline, and two alternative ones in-corporating only morpho-syntactic and syntacticparse information, the Number Agreement and theBikel Parser baseline, respectively.3.1.1 WORDNET Similarity (WN) BaselineOur lexico-semantic baseline comes withWORDNET semantic similarity scores of puta-tively coordinated nouns.
For our experiments,we used the implementation of WORDNET simi-larity and relatedness measures provided by TedPedersen.2 The following similarity measureswere considered: two measures based on pathlenghts between concepts (path and lch (Leacocket al, 1998)), three measures based on informa-tion content, i.e., corpus-based measures of thespecificity of a concept (res (Resnik, 1999), lin(Lin, 1998), and jcn (Jiang and Conrath, 1997)).Furthermore, we used two relatedness measures,namely, lesk (Banerjee and Pedersen, 2003) andvector (Patwardhan et al, 2003), which score thesimilarity of the glosses of both concepts.
Weapplied these similarity measures to any pair ofputatively coordinated nouns in the noun phrasesfrom our data sets, A and B.
To determine poten-tial conjuncts we calculate two similarity scoresrelative to the structures discussed in Section 2.2:s1 = sim(N1,N2) and s2 = sim(N1,N3)Our final score is the maximum over both scoreswhich is then the semantic indicator for the mostplausible resolution of the coordination.3.1.2 Number Agreement (NA) BaselineWe compared here the number agreementbetween selected nouns (see Resnik (1999)).Accordingly, N1 and N2 are coordinated, ifnumber(N1) = number(N2) AND number(N1) 6=number(N3), while N1 and N3 are coordinated, ifnumber(N1) = number(N3) AND number(N1) 6=number(N2).3.1.3 Post-Processing HeuristicsIn the WN and NA baselines, after the detectionof coordinated elements we used simple heuris-tics to tag the remaining part of the noun phrase.If N1 and N2 were hypothesized to be coordi-nated, then all tokens preceding N1 were tagged as2http://www.d.umn.edu/?tpederse/shared elements, N3 was tagged as shared elementas well, while all tokens between the conjunctionand N2 were tagged as conjuncts.
For example,in ?a happy dogN1 and catN2 ownerN3?
we identify?dog?
and ?cat?
as coordinated elements and tag ?ahappy?
and ?owner?
as shared elements.
The finalresolution looks like ?S S C CC C S?.
If N1 and N3were hypothesized to be coordinated, then all otherelements except conjunctions were tagged as partsof conjuncts, as well.3.1.4 Bikel Parser (BP) BaselineWe used the well-known Bikel Parser (Bikel,2004) in its original version and the one used byCollins (2003).
We trained both of them onlywith NPs extracted from the re-annotated versionof WSJ (see Section 2) and converted the bracket-ing output of the parsers to the IO representationfor NP coordinations for further evaluations.3.2 Chunking of Conjuncts with CRFsThe approach to conjunct identification presentedby Buyko et al (2007) employs Conditional Ran-dom Fields (CRF) (Lafferty et al, 2001),3 whichassign a label to each token of coordinated NPsaccording to its function in the coordination: ?C?for conjuncts, ?CC?
for conjunctions, and ?S?
forshared elements.
Since non-nested conjuncts canbe assumed to be in a sequential order, sequen-tial learning approaches (instead of single positionclassification approaches) seem appropriate here.Buyko et al (2007) report an F-measure of 93%on conjunct identification in the GENIA corpus.They use a feature set including lexical (words),and morpho-syntactic features (POS tags, morpho-syntactic similarity of putative conjuncts), but ex-clude any semantic criteria.
The morpho-syntacticsimilarity features were generated from a rule-based approach to conjunct identification using themaximal symmetry of conjuncts as constituted bytheir respective POS annotation.We here intend to apply this approach for resolv-ing coordination ambiguities involving noun com-pounds in the newswire language such as ?presi-dent and chief executive?.
This restricts the spec-trum of considered coordinations in noun phrasesto more complicated cases than those consideredby Buyko et al (2007).
We will thus test the vari-ous resolution models under harder test conditions,3They employ the linear-chain CRF implementation fromthe MALLET toolsuite available at http://mallet.cs.umass.edu/index.php/Main_Page92Feature Class Descriptiondefault feature prior probability distribution over thepossible argument labelslexical wordmorpho-syntacticthe token?s POS tag; output labels of themorpho-syntactic similarity (?C?,?CC?and ?S?)
(see Buyko et al (2007)); out-put labels of the number agreementbaseline (?C?, ?CC?
and ?S?
)semantic WN output labels of the WORDNET similar-ity baseline (?C?, ?CC?
and ?S?
)contextual conjunctions of all features of neighbor-ing tokens (two tokens to the left andone token to the right)Table 1: Feature Classes Used for Conjunct Iden-tificationsince Buyko et al consider, e.g., adjective coordi-nations in noun phrases such as ?positive and neg-ative IL-2 regulator?
that are predominant in thebiomedical language domain.We also propose in this work an extension of thefeature space in terms of lexico-semantic features(see Table 1), information that originates from sim-ilarity computations on WORDNET data.
Further-more, we do not use orthographical features of theoriginal approach as they are well suited only forthe biomedical language domain.4 Results and Error AnalysisTo evaluate the different approaches to conjunctidentification, we used recall and precision scoressince they are well suited for the evaluation of seg-mentation tasks.
Two types of decisions were eval-uated ?
the assignment of ?C?
labels denoting con-juncts in terms of the F-measure, and (given thetagged conjuncts) the accuracy of the complete co-ordination resolution.
A coordination is resolvedproperly only, if all tokens of both conjuncts arecorrectly identified.We carried out a ten-fold cross-validation of allML-based methods (Bikel parser (Bikel, 2004) andCRF-based conjunct identification (Buyko et al,2007)).
For the evaluation of the NA and WN base-lines, we tested their performance on the completedata sets, A and B (see Section 2).As Table 2 depicts the NA baseline achieved anaccuracy of 28.4% on A (36.6% on B), the Bikelparser reached 77.2% on A (73.4% in B), whilethe WN baseline got in its best run (vector mea-sure) an accuracy of 41.7% on A (49.6% on B).These results already reveal that parsing almostdramatically outperforms the coordination resolu-tion based on the NA similarity by up to 35.5%points.
The results of the WN baseline indicatethat the best similarity measure for conjunct iden-tification is the vector similarity (Patwardhan etal., 2003) that scores the similarity between theglosses of the concepts.Our error analysis of the WN baseline on thetest set A reveals that its low accuracy has var-ious reasons.
First, about 37% of the coordina-tions could not be resolved due to the absence of atleast one noun involved in the coordination fromthe WORDNET.
These coordinations usually in-clude named entities such as person and organi-zation names (e.g., ?brothers Francis and GilbertGros?).
These coverage gaps have clearly a nega-tive effect on the resolution results for all WORD-NET similarity measures.To find out errors which are specific for theconsidered similarity measures, we have chosenthe res measure and inspected the analysis resultson all noun phrases where nouns are covered byWORDNET.
The remaining set of coordinationscontains 1,740 noun phrases.
1,022 coordinations(59%) of this set were completely resolved by theWN baseline, while 1,117 coordinations (64% ofthe remaining part, 41.5% of the test set A) couldbe at least partly resolved.
Obviously, the coordi-nated heads are properly detected by the res mea-sure but our heuristics for tagging the remainingmodifiers (see Subsection 3.1.3) fail to provide thecorrect conjunct boundaries.623 coordinations (36%) were mis-classified bythe res measure.
A closer look at this data re-veals two types of errors.
The first and minortype is the misleading selection of putatively co-ordinated heads N1, N2, and N3.
We presupposein the WN baseline that the heads appear right-most in the noun phrase, although that is not al-ways the case as illustrated by the phrase ?North-ern California earthquake and Hurricane Hugo?.The res measure detected correctly a higher sim-ilarity between ?earthquake?
(N1) and ?hurricane?
(N2), but ?Hugo?
(N3) is a modifier of ?hurricane?.Although the res measure works fine, the coordi-nation cannot be properly resolved due to syntac-tic reasons.
In some cases, N2 is wrongly selectedas in ?life and health insurance operation?
wherethe WN baseline selects ?insurance?
as right-mostnoun (except the last noun ?operation?)
and not?health?.44?turbineN2 ?
is, however, correctly selected in ?steam tur-bine and gas turbine plants?.93Set A Set BRecall/Precision/F-Score Accuracy Recall/Precision/F-Score AccuracyNA 32.7 / 75.9 / 45.7 28.4 41.6 / 83.9 / 55.6 36.6Bikel 85.6 / 85.4 / 85.5 77.2 83.8 / 83.6 / 83.7 73.4Bikel (Collins) 85.9 / 85.7 / 85.8 77.5 83.6 83.4 / 83.5 72.9WN jcn 45.6 / 69.3 / 55.0 36.2 54.7 / 72.1 / 62.2 41.2WN lch 48.7 / 70.8 / 57.7 39.2 57.8 / 74.1 / 65.0 44.4WN lesk 49.2 / 66.2 / 56.5 38.1 59.3 / 70.3 / 64.3 43.3WN lin 44.7 / 69.9 / 54.6 35.5 53.5 / 72.6 / 61.6 40.5WN res 45.9 / 71.8 / 56.0 37.4 55.7 / 75.8 / 64.2 43.8WN path 48.7 / 70.8 / 57.7 39.2 57.8 / 74.1 / 65.0 44.4WN vector 51.2 / 68.9 / 58.8 41.7 62.8 / 74.5 / 68.1 49.6CRF (default), contextual 75.2 / 72.4 / 73.8 60.3 77.9 / 75.1 / 76.5 63.1+ Lexical, morpho-syntactic 87.1 / 87.2 / 87.1 77.9 88.1/ 88.0 / 88.0 78.8+ WN (lesk) 87.2 / 87.2 / 87.2 78.0 88.2/ 88.2 / 88.2 79.1CRF (default), contextual + only WN (lesk) 79.3 / 78.4 / 78.9 64.8 81.2 / 80.6 / 80.9 66.9CRF (default), contextual + only morpho-syntactic86.2 / 86.3 / 86.3 76.6 87.6 / 87.6 / 87.6 78.1Table 2: F-measure of Conjunct Identification and Accuracy of Coordination Resolution on the WSJSection of the PENN TREEBANK CorpusThe second type of error comes as erroneousclassifications of the res measure such as in ?hos-pitals and blood banks?
where ?hospitals?
and?blood?
have a higher similarity than ?hospitals?and ?banks?
although they are, in fact, coordi-nated here.
?hotels and large restaurant chains?,?records and music publishing?, ?chemicals andtextiles company?
are other examples for the obser-vation that the coordinated elements have a lowersimilarity as non-coordinated ones.We also carried out a ten-fold cross-validation ofthe CRF-based approach for the conjunct identifi-cation.
First of all, the CRF-based approach (withand without WN similarity) achieved the highestaccuracy score ?
up to 78.0% on set A, and 79.1%on B ?
compared with all other approaches wescrutinized on.
We also tested the performance ofthe original semantics-free approach and the ad-ditional effects of the WORDNET similarity mea-sures (see Table 2).
Although the integration of se-mantic information leads to a mild gain comparedwith the original approach (up to 0.3% points, withthe lesk measure), the results indicate that no sub-stantial benefit can be traced to semantic features.We ran several tests with solely morpho-syntactic features (as enumerated in Table 1) andsolely WN features, too.
They reveal that solelymorpho-syntactic features are up to 11.8% pointsmore predictive than WN features.
The best re-sults were still achieved using the gloss-orientedlesk measure (see Table 3).The inspection of the errors types from the var-ious runs is not fully conclusive though.
Afteradding WN features to both sets, we detected someimprovements for conjunct tagging with high WNsimilarity.
Some conjunct boundaries could be cor-rected as in ?record and movie producer?
where,in the first run, ?producer?
was tagged as a con-junct and was corrected as being shared by inte-grating WN features.
But we also detected a de-grading tagging behavior of conjuncts with WNfeatures where the WN similarity was not helpfulat all as in ?chairman and chief designer?
where?chairman?
and ?chief?
under the influence of WNfeatures were judged to be conjuncts.
We foundout that the addition of WN features positively in-fluences the classification of coordinations whereN1 and N2 are coordinated, while it increased er-rors in the classification of coordinations where N1and N3 are coordinated.In addition, we calculated intersections betweenthe set A error data (unique) of the res WN base-line (1400 phrases) and the error data of the CRFapproach without WN features (391), and the er-ror data of the CRF approach with WN features(385), respectively.
These error data sets con-tain noun phrases where coordinated heads couldnot be properly detected.
The set of the res WNbaseline and the set of the CRF approach with-out WN features have an intersection of 230 in-stances, where 138 instances could not be foundin the WORDNET.
That means that for about 161instances (59%) in the mis-classified data of theCRF approach the additional WN features wouldnot be helpful.
The intersection remains similar(226 instances) between the set of the res WN base-94Default, Context, WN Recall Accu-Lexical, Morpho- Sim Precision / racySyntactic Feats.
F-Score?
88.1/ 88.0 / 88.0 78.8?
jcn 87.9/ 87.8 / 87.9 78.3?
lch 87.9 / 87.9 / 87.9 78.5?
lesk 88.2/ 88.2 / 88.2 79.1?
lin 88.0 / 88.0 / 88.0 78.8?
res 88.2 / 88.2 / 88.2 79.0?
path 87.9 / 87.9 / 87.9 78.5?
vector 87.8 / 87.7 / 87.7 78.2Table 3: Conjunct Identification ?
Cross-validation on the WSJ section of the PENN TREE-BANK Corpus on Test Set Bline and the set of the CRF approach enriched withWN features.
The intersection between the errrorsets of the both CRF approaches includes 352 in-stances.
The integration of the WN features wasnot helful for almost the complete error data fromthe original CRF approach.
We have previouslyshown that the res WN baseline features correlatewith the correct label sequence for only 1,117 co-ordinations (41.5%) of the complete evaluation setA and the features thus do not seem to be effectivein our approach.Furthermore, we evaluated the results of theCRF approach only for the correct detection ofcoordinated heads (see above for the res measureand intersection counts) and disregarded the mod-ifier classification.
The results ?
85.3% on set Aand 85.4% on set B ?
reveal that the classificationof modifiers is a major source of classification er-rors.
In both configurations the problematic nounphrases are the ones with (e.g., adjectival) modi-fiers.
The boundaries of conjuncts are not properlyrecognized in such noun phrases, as for example in?American comic book and television series?
wherethe correct label sequence is ?S C C CC C S?, since?American?
is the shared modifier of ?book?
and?television?, while ?comic?
just modifies ?book?.As most adjectives appearing at the beginningof the noun phrase as in ?medical products andservices company?
tend to be used as shared modi-fiers of coordinations in our data, this, erroneously,leads to false taggings, e.g., ?personal?
in ?per-sonal computer and software design?
as a sharedelement.
To cope adequately with modifiers weneed to integrate more appropriate features suchas collocation frequencies of modifiers and coor-dinated heads.
The detection of a higher collo-cation frequency of ?personal computer?
in com-parison to ?personal software?
(e.g., using the pro-cedures proposed by Wermter and Hahn (2004))would help tagging the conjunct boundaries.5 Conclusions and Future WorkWe investigated the problem of noun phrase co-ordination resolution as a segmentation problemamong conjuncts involved in the coordination.While resolving coordination ellipsis is often con-sidered as a semantically constrained problem, wewanted to assess a less ?costly?
solution strategy,namely relying on ?cheaper?
to get syntactic crite-ria as much as possible, though not sacrificing theaccurary of resolutions.We, first looked at morpho-syntactic criteriaonly and lexico-semantic criteria only, and then atthe combination of both approaches.
The evalu-ation results from a variety of experiments revealthat the major part of ambiguous coordinationscan be resolved using solely morpho-syntactic fea-tures.
Surprising as it might be, the semantic in-formation as derived from the WORDNET sim-ilarity measures does not yield any further sub-stantial improvement for our approach.
This issomehow counter-intuitive, but our findings, un-like those from earlier studies which emphasizedthe role of semantic criteria, are based on exten-sive corpus data ?
the PENN TREEBANK.Results from our error analysis will guide futurework to further boost results.
Particular empha-sis will be laid on the integration of named en-tity recognizers, collocation frequencies and dis-tributional similarity data as also advocated byChantree et al (2005).The presented sequential labeling-based ap-proach to coordination resolution was here ap-plied to the resolution of a special type of ambigu-ous noun phrases.
In general, this approach caneasily be applied to the resolution of other typesof coordinative structures in noun phrases as al-ready presented in Buyko et al (2007).
As far asother phrasal types (e.g., verbal phrases) are con-cerned, long-distance coordinations play a muchmore prominent role.
The token-based labeling ap-proach may be thus substituted by a chunk-basedapproach operating on sentences.AcknowledgementsThis research was partly funded by the GermanMinistry of Education and Research within theSTEMNET project (01DS001A-C) and by the ECwithin the BOOTSTREP project (FP6-028099).95ReferencesAgarwal, R. and L. Boggess.
1992.
A simple butuseful approach to conjunct identification.
In Pro-ceedings of the 30th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 15?21.Newark, DE, USA, 28 June - 2 July 1992.Banerjee, S. and T. Pedersen.
2003.
Extended glossoverlaps as a measure of semantic relatedness.
In IJ-CAI?03 ?
Proceedings of the 18th International JointConference on Artificial Intelligence, pages 805?810.
Acapulco, Mexico, August 9-15, 2003.Bikel, D. 2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4):479?511.Buyko, E., K. Tomanek, and U. Hahn.
2007.
Reso-lution of coordination ellipses in biological namedentities using Conditional Random Fields.
In PAC-LING 2007 - Proceedings of the 10th Conference ofthe Pacific Association for Computational Linguis-tics, pages 163?171.
Melbourne, Australia, Septem-ber 19-21, 2007.Chantree, F. A. Kilgarriff, A. de Roeck, and A. Willis.2005.
Disambiguating coordinations using word dis-tribution information.
In RANLP 2005 ?
Proceed-ings of the Intl.
Conference on ?Recent Advancesin Natural Language Processing?, pages 144?151.Borovets, Bulgaria, 21-23 September, 2005.Charniak, E. and M. Johnson.
2005.
Coarse-to-finen-best parsing and MaxEnt discriminative reranking.In ACL?05 ?
Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics,pages 173?180.
Ann Arbor, MI, 25-30 June 2005.Collins, M. 2003.
Head-driven statistical models fornatural language parsing.
Computational Linguis-tics, 29(4):589?637.Fellbaum, C., editor.
1998.
WORDNET: An ElectronicLexical Database.
MIT Press.Hogan, D. 2007a.
Coordinate noun phrase disam-biguation in a generative parsing model.
In ACL?07?
Proceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 680?687.
Prague, Czech Republic, June 28-29, 2007.Hogan, D. 2007b.
Empirical measurements of lexi-cal similarity in noun phrase conjuncts.
In Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics.
Demo and Poster Ses-sions, pages 149?152.
Prague, Czech Republic, June28-29, 2007.Jiang, J. and D. Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In ROCLING-X ?
Proceedings of the 1997 Inter-national Conference on Research in ComputationalLinguistics.
Taipei, Taiwan, August 22-24, 1997.Lafferty, J., A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML-2001?
Proceedings of the 18th International Conferenceon Machine Learning, pages 282?289.
WilliamsCollege, MA, USA, June 28 - July 1, 2001.Leacock, C., M. Chodorow, and G. Miller.
1998.Using corpus statistics and WORDNET relationsfor sense identification.
Computational Linguistics,24(1):147?165.Lin, D. 1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the 15th InternationalConference on Machine Learning, pages 296?304.Madison, WI, USA, July 24-27, 1998.Marcus, M., B. Santorini, and M.-A.
Marcinkiewicz.1993.
Building a large annotated corpus of English:The PENN TREEBANK.
Computational Linguistics,19(2):313?330.Nakov, P. and M. Hearst.
2005.
Using the Web as animplicit training set: Application to structural ambi-guity resolution.
In HLT-EMNLP?05 ?
Proceedingsof the 5th Human Language Technology Conferenceand 2005 Conference on Empirical Methods in Nat-ural Language Processing, pages 835?842.
Vancou-ver, B.C., Canada, October 6-8, 2005.Ohta, T., Y. Tateisi, and J.-D. Kim.
2002.
The GE-NIA corpus: An annotated research abstract corpusin molecular biology domain.
In HLT 2002 ?
Pro-ceedings of the 2nd International Conference on Hu-man Language Technology Research, pages 82?86.San Diego, CA, USA, March 24-27, 2002.Okumura, A. and K. Muraki.
1994.
Symmetric patternmatching analysis for English coordinate structures.In ANLP 1994 ?
Proceedings of the 4th Conferenceon Applied Natural Language Processing, pages 41?46.
Stuttgart, Germany, 13-15 October 1994.Patwardhan, S., S. Banerjee, and T. Pedersen.
2003.Using measures of semantic relatedness for wordsense disambiguation.
In CICLing 2003 ?
Proceed-ings 4th Intl.
Conference on Computational Linguis-tics and Intelligent Text Processing, pages 241?257.Mexico City, Mexico, February 16-22, 2003.Resnik, P. 1999.
Semantic similarity in a taxonomy:An information-based measure and its application toproblems of ambiguity in natural language.
Journalof Artificial Intelligence Research, 11:95?130.Rus, V., D. Moldovan, and O. Bolohan.
2002.
Bracket-ing compound nouns for logic form derivation.
InFLAIRS 2002 ?
Proceedings of the 15th Interna-tional Florida Artificial Intelligence Research Soci-ety Conference, pages 198?202.
Pensacola Beach,FL, USA, May 14-16, 2002.Shimbo, M. and K. Hara.
2007.
A discriminative learn-ing model for coordinate conjunctions.
In EMNLP-CoNLL 2007 ?
Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 610?619.
Prague,Czech Republic,June 28-29, 2007.Vadas, D. and J. Curran.
2007.
Adding noun phrasestructure to the PENN TREEBANK.
In ACL?07 ?
Pro-ceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics, pages 240?247.Prague, Czech Republic, June 28-29, 2007.Wermter, J. and U. Hahn.
2004.
Collocation extractionbased on modifiability statistics.
In COLING 2004 ?Proceedings of the 20th International Conference onComputational Linguistics, pages 980?986.
Geneva,Switzerland, August 23-27, 2004.96
