Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 167?170, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsXLING: Matching Query Sentences to a Parallel Corpus usingTopic Models for Word Sense DisambiguationLiling Tan and Francis BondDivision of Linguistics and Multilingual Studies,Nanyang Technological University14 Nanyang Drive, Singapore 637332alvations@gmail.com, bond@ieee.orgAbstractThis paper describes the XLING system partici-pation in SemEval-2013 Crosslingual WordSense Disambiguation task.
The XLING systemintroduces a novel approach to skip the sensedisambiguation step by matching query sentenc-es to sentences in a parallel corpus using topicmodels; it returns the word alignments as thetranslation for the target polysemous words.Although, the topic-model base matching under-performed, the matching approach showed po-tential in the simple cosine-based surface simi-larity matching.1 IntroductionThis paper describes the XLING system, an un-supervised Cross-Lingual Word Sense Disam-biguation (CLWSD) system based on matchingquery sentence to parallel corpus using topicmodels.
CLWSD is the task of disambiguating aword given a context by providing the most ap-propriate translation in different languages(Lefever and Hoste, 2013).2 BackgroundTopic models assume that latent topics exist intexts and each semantic topic can be representedwith a multinomial distribution of words andeach document can be classified into differentsemantic topics (Hofmann, 1999).
Blei et al(2003b) introduced a Bayesian version of topicmodeling using Dirichlet hyper-parameters, La-tent Dirichlet Allocation (LDA).
Using LDA, aset of topics can be generated to classify docu-ments within a corpus.
Each topic will contain alist of all the words in the vocabulary of the cor-pus where each word is assigned a probability ofoccurring given a particular topic.3 ApproachWe hypothesized that sentences with differentsenses of a polysemous word will be classifiedinto different topics during the LDA process.
Bymatching the query sentence to the training sen-tences by LDA induced topics, the most appro-priate translation for the polysemous word in thequery sentence should be equivalent to transla-tion of word in the matched training sentence(s)from a parallel corpus.
By pursuing this ap-proach, we escape the traditional mode of dis-ambiguating a sense using a sense inventory.4 System DescriptionThe XLING_TnT system attempts the matchingsubtask in three steps (1) Topicalize: match-ing the query sentence to the training sentencesby the most probable topic.
(2) Rank: thematching sentences were ranked according tothe cosine similarity between the query andmatching sentences.
(3) Translate: providesthe translation of the polysemous word in thematched sentence(s) from the parallel corpus.4.1 PreprocessingThe Europarl version 7 corpus bitexts (English-German, English-Spanish, English-French, Eng-lish-Italian and English-Dutch) were aligned atword-level with GIZA++ (Och and Ney, 2003).The translation tables from the word-alignmentswere used to provide the translation of the poly-semous word in the Translate step.The English sentences from the bitexts werelemmatized using a dictionary-based lemmatiz-167er: xlemma1.
After the lemmatization, Englishstopwords2 were removed from the sentences.The lemmatized and stop filtered sentences wereused as document inputs to train the LDA topicmodel in the Topicalize step.Previously, topic models had been incorpo-rated as global context features into a modifiednaive Bayes network with traditional WSD fea-tures (Cai et al2007).
We try a novel approachof integrating local context (N-grams) by usingpseudo-word sentences as input for topic induc-tion.
Here we neither lemmatize or remove stopswords.
For example:Original Europarl sentence: ?Education andcultural policies are important tools for creatingthese values?Lemmatized and stopped: ?education culturalpolicy be important tool create these values?Ngram pseudo-word: ?education_and_culturaland_cultural_policies cultural_policies_areare_important_tools important_tools_fortools_for_creating for_creating_these creat-ing_these_values?4.2 Topicalize and MatchThe Topicalize step of the system first (i)induced a list of topics and trained a topic modelfor each polysemous word using LDA, then (ii)allocated the topic with the highest probabilityto each training sentence.Finally, at evaluation, (iii) the query sentenceswere assigned the most probable topic inferredusing the trained topic models.
Then the trainingsentences allocated with the same topic wereconsidered as matching sentences for the nextRank step.4.2.1 Topic InductionTopic models were trained using Europarl sen-tences that contain the target polysemous words;one model per target word.
The topic modelswere induced using LDA by setting the numberof topics (#topics) as 50, and the alpha and beta1  http://code.google.com/p/xlemma/2  Using the Page and Article Analyzer stopwords fromhttp://www.ranks.nl/resources/stopwords.htmlhyper-parameters were symmetrically set at1.0/#topics.
Blei et al(2003) had shown that theperplexity plateaus when #topics ?
50; higherperplexity means more computing time neededto train the model.4.2.2 Topic AllocationEach sentence was allocated the most probabletopic induced by LDA.
An induced topic con-tained a ranked list of tuples where the 2nd ele-ment in each tuple is a word that associated withthe topic, the 1st element is the probability thatthe associated word will occur given the topic.The probabilities are generatively output usingVariational Bayes algorithm as described inHoffman et al(2010).
For example:[(0.0208, 'sport'), (0.0172, 'however'),(0.0170, 'quite'), (0.0166, 'maritime'),(0.0133, 'field'), (0.0133, 'air-transport'),(0.0130, 'appear'), (0.0117, 'arrangement'),(0.0117, 'pertain'), (0.0111, 'supervision')]4.2.3 Topic InferenceWith the trained LDA model, we inferred themost probable topic of the query sentence.
Thenwe extracted the top-10 sentences from the train-ing corpus that shared the same top ranking top-ic.The topic induction, allocation and inferencewere done separately on the lemmatized andstopped sentences and on the pseudo-word sen-tence, resulting in two sets of matching sentenc-es.
Only the sentences that were in both sets ofmatches are considered for the Rank step.4.3 RankMatched sentences from the Topicalize stepwere converted into term vectors.
The vectorswere reweighted using tf-idf and ranked accord-ing to the cosine similarity with the query sen-tences.
The top five sentences were piped intothe Translate step.4.4 TranslateFrom the matching sentences, the Translatestep simply checks the GIZA++ word alignmenttable and outputs the translation(s) of the targetpolysemous word.
Each matching sentence,168could output more than 1 translation dependingon the target word alignment.
As a simple wayof filtering stop-words from target Europeanlanguages, translations with less than 4 charac-ters were removed.
This effectively distills misa-ligned non-content words, such as articles, pro-nouns, prepositions, etc.
To simplify the lemma-tization of Spanish and French plural noun suf-fixes, the ?-es?
and ?-s?
are stemmed from thetranslation outputs.The XLING_TnT system outputs one transla-tion for each query sentence for the best resultevaluation.
It output the top 5 translations for theout-of-five evaluation.4.5 FallbackFor the out-of-five evaluation, if the query re-turned less than 5 answers, the first fallback3appended the lemma of the Most Frequent Sense(according to Wordnet) of the target polysemousword in their respective language from the OpenMultilingual Wordnet.4 If the first fallback wasinsufficient, the second fallback appended themost frequent translation of the target polyse-mous word to the queries?
responses.4.6 BaselineWe also constructed a baseline for matching sen-tences by cosine similarity between the lemmasof the query sentence and the lemmas of eachEnglish sentence in the training corpus.5 Thebaseline system is named XLING_SnT (Similarand Translate).
The cosine similarity is calculat-ed from the division of the vector product of thequery and training sentence (i.e.
numerator) bythe root product of the vector?s magnitudesquared.5 ResultsTables 1 and 2 present the results for the XLINGsystem for best and out-of-five evaluation.
Oursystem did worse than the task?s baseline, i.e.the Most Frequent Translation (MFT) of the tar-get word for all languages.
Moreover the topic3    Code sample for the fallback can be found athttp://goo.gl/PbdK74    http://www.casta-net.jp/~kuribayashi/multi/5  Code-snippet for the baseline can be found athttp://pythonfiddle.com/surface-cosine-similaritymodel based matching did worse than the cosinesimilarity matching baseline.
The results showthat matching on topics did not help.
However,Li et al(2010) and Anaya-Sanchez et al(2007)had shown that pure topic model based unsuper-vised system for WSD should perform a littlebetter than Most Frequent Sense baseline incoarse-grain English WSD.
Hence it was neces-sary to perform error analysis and tweaking toimprove the XLING system.BEST German Spanish French Italian DutchSnT8.13(10.36)19.59(24.31)17.33(11.57)12.74(11.27)9.89(9.56)TnT5.28(5.82)18.60(24.31)16.48(11.63)10.70(7.54)7.40(8.54)MFT17.43(15.30)23.23(27.48)25.74(20.19)20.21(19.88)20.66(24.15)Table 1: Precision and (Mood) for the best evaluationOOF German Spanish French Italian DutchSnT23.71(30.57)44.83(50.04)38.44(32.45)32.38(29.17)27.11(27.31)TnT19.13(23.54)39.52(44.96)35.3(28.02)33.28(29.61)23.27(22.98)MFT38.86(44.35)53.07(57.35)51.36(47.42)42.63(41.69)43.59(41.97)Table 2: Precision and (Mood) for the oof evaluation6 Error Analysis and ModificationsStatistically, we could improve the robustness ofthe topic models in the Topicalize step by(i) tweaking the Dirichlet hyper-parameters toalpha = 50/#topics, beta = 0.01 as suggested byWang et al(2009).BEST OOFPrecision Mood Precision MoodGerman 6.50 6.71 20.98 25.18Spanish 14.77 19.43 40.22 45.67French 10.79 7.95 31.26 23.37Italian 13.10 10.95 36.56 31.94Dutch 7.42 7.47 21.66 20.42Table 3: Evaluations on Hyper-parameter tweaksAlthough the hyperparameters tweaks improvesthe scores for German and Dutch evaluations itbrings the overall precision and mood precisionof the other three languages down.
Since thedocuments from each language are parallel, this169suggests that there is some language-dependencyfor LDA?s hyperparameters.By going through the individual queries andresponses, several issues in the translatestep need to be resolved to achieve higher preci-sion; (i) German-English and Dutch-Englishword alignments containing compound wordsneed to be segmented (e.g.
kraftomnibusverkehr?kraft omnibus verkehr) and realigned such thatthe target word coach only aligns to omnibus,(ii) lemmatization of Italian, German and Dutchis crucial is getting the gold answers of the task(e.g.
XLING answers omnibussen while the goldanswers allowed omnibus).
The use of targetlanguage lemmatizers, such as TreeTagger(Schmid, 1995) would have benefited the sys-tem.7 DiscussionThe main advantage of statistical language inde-pendent approaches is the ability to scale thesystem in any possible language.
However lan-guage dependent processing remains crucial inbuilding an accurate system, especially lemmati-zation in WSD tasks (e.g.
kraftomnibusverkehr).We also hypothesize that more context wouldhave improved the results of using topics: dis-ambiguating senses solely from sentential con-text is artificially hard.8 ConclusionOur system has approached the CLWSD task inan unconventional way of matching query sen-tences to parallel corpus using topic models.Given no improvement from hyper-parametertweaks, it reiterates Boyd-Graber, Blei andZhu?s (2007) assertion that while topic modelscapture polysemous use of words, they do notcarry explicit notion of senses that is necessaryfor WSD.
Thus our approach to match querysentences by topics did not perform beyond theMFT baseline in the CLWSD evaluation.However, the surface cosine baseline, with-out any incorporation of any sense knowledge,had surprisingly achieved performance closer toMFT It provides a pilot platform for future workto approach the CLWSD as a vector-based doc-ument retrieval task on parallel corpora andproviding the translation from the word align-ments.Referencesenry Anaya-   anche , Aurora  ons-Porrata, andRafael Berlanga-Llavori.
2007.
Tkb-uo: Usingsense clustering for wsd.
In Proceedings of theFourth International Workshop on Semantic Eval-uations (SemEval-2007), pp.
322?325.Jordan Boyd-Graber, David M. Blei, and XiaojinZhu.
2007.
A Topic Model for Word Sense Dis-ambiguation.
In Proc.
of Empirical Methods inNatural Language Processing( EMNLP).David M. Blei, Andrew Y. Ng, and Michael L. Jor-dan.
2003.
Latent Dirichlet alcation.
Journal ofMachine Learning Research, 3:993?1022.Jun-Fu Cai, Wee-Sun Lee and Yee-Whye Teh.
2007.Improving word sense disambiguation using topicfeatures.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pp.
1015?1023.Christiane Fellbaum.
(ed.)
(1998) WordNet: An Elec-tronic Lexical Database, MIT PressThomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of SIGIR '99, Berkeley,CA, USA.Matthew Hoffman, David Blei and Francis Bach.2010.
Online Learning for Latent Dirichlet Alloca-tion.
In Proceedings of NIPS 2010.Els Lefever and V?ronique Hoste.
2013.
SemEval-2013 Task 10: Cross-Lingual Word Sense Disam-biguation, In Proceedings SemEval 2013, in con-junction with *SEM 2013, Atlanta, USA.Linlin Li, Benjamin Roth and Caroline Sporleder.Topic Models for Word Sense Disambiguation andToken-based Idiom Detection.
In Proc.
of The48th Annual Meeting of the Association for Com-putational Linguistics (ACL), 2010.
Uppsala,Sweden.Franz Josef Och, Hermann Ney.
2003.
A SystematicComparison of Various Statistical AlignmentModels.
Computational Linguistics 29:1. pp.
19-51.Helmut Schmid.
1995.
Improvements in Part-of-Speech Tagging with an Application to German.Proceedings of the ACL SIGDAT-Workshop.
Dub-lin, Ireland.Yi Wang, Hongjie Bai, Matt Stanton, Wen-YenChen, Edward Y. Chang.
2009.
Plda: Parallel la-tent dirichlet alcation for large-scale applica-tions.
In Proc.
of 5th International Conference onAlgorithmic Aspects in Information and Manage-ment.170
