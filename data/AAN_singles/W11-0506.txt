Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 41?48,Portland, Oregon, June 23, 2011. c?2011 Association for Computational LinguisticsAbstractive Summarization of Line Graphs from Popular MediaCharles F. Greenbacker Peng WuSandra Carberry Kathleen F. McCoy Stephanie Elzer*Department of Computer and Information SciencesUniversity of Delaware, Newark, Delaware, USA[charlieg|pwu|carberry|mccoy]@cis.udel.edu*Department of Computer ScienceMillersville University, Millersville, Pennsylvania, USAelzer@cs.millersville.eduAbstractInformation graphics (bar charts, line graphs,etc.)
in popular media generally have a dis-course goal that contributes to achieving thecommunicative intent of a multimodal docu-ment.
This paper presents our work on ab-stractive summarization of line graphs.
Ourmethodology involves hypothesizing the in-tended message of a line graph and using itas the core of a summary of the graphic.
Thiscore is then augmented with salient proposi-tions that elaborate on the intended message.1 IntroductionSummarization research has focused primarily onsummarizing textual documents, and until recently,other kinds of communicative vehicles have beenlargely ignored.
As noted by Clark (1996), languageis more than just words ?
it is any signal that isintended to convey a message.
Information graph-ics (non-pictorial graphics such as bar charts, linegraphs, etc.)
in popular media such as Newsweek,Businessweek, or newspapers, generally have a com-municative goal or intended message.
For exam-ple, the graphic in Figure 1 is intended to conveya changing trend in sea levels ?
relatively flat from1900 to 1930 and then rising from 1930 to 2003.Thus, using Clark?s view of language, informationgraphics are a means of communication.Research has shown that the content of informa-tion graphics in popular media is usually not re-peated in the text of the accompanying article (Car-berry et al, 2006).
The captions of such graphicsare also often uninformative or convey little of thegraphic?s high-level message (Elzer et al, 2005).This contrasts with scientific documents in whichgraphics are often used to visualize data, with ex-plicit references to the graphic being used to explaintheir content (e.g., ?As shown in Fig.
A...?).
Infor-mation graphics in popular media contribute to theoverall communicative goal of a multimodal docu-ment and should not be ignored.Our work is concerned with the summarizationof information graphics from popular media.
Suchsummaries have several major applications: 1) theycan be integrated with the summary of a multimodaldocument?s text, thereby producing a richer sum-mary of the overall document?s content; 2) they canbe stored in a digital library along with the graphicitself and used to retrieve appropriate graphics in re-sponse to user queries; and 3) for individuals withsight impairments, they can be used along with ascreen reader to convey not only the text of a docu-ment, but also the content of the document?s graph-ics.
In this paper we present our work on summariz-ing line graphs.
This builds on our previous effortsinto summarizing bar charts (Demir et al, 2008;Elzer et al, 2011); however, line graphs have dif-ferent messages and communicative signals than barcharts and their continuous nature requires differentprocessing.
In addition, a very different set of visualfeatures must be taken into account in deciding theimportance of including a proposition in a summary.2 MethodologyMost summarization research has focused on ex-tractive techniques by which segments of text areextracted and put together to form the summary.41?102468 1900?10?20?50?60?70?80?90?03?30?402000108.91.979 inches overthe past century.
Annual difference from Seattle?sIn the seattle area, forexample, the PacificOcean hasrisen nearlythey are rising about 0.04?0.09 of an inch each year.Sea levelsfluctuate around the globe, but oceanographers believeOcean levels rising1899 sea level, in inches:Figure 1: From ?Worry flows from Arctic ice to tropicalwaters?
in USA Today, May 31, 2006.However, the Holy Grail of summarization work isabstractive summarization in which the document?scontent is understood and the important concepts areintegrated into a coherent summary.
For informa-tion graphics, extractive summarization might meantreating the text in the graphic (e.g., the caption) as ifit were document text.
One could imagine perhapsexpanding this view to include selecting particulardata points or segments and constructing sentencesthat convey them.
Abstractive summarization, onthe other hand, requires that the high-level contentof the graphic be identified and conveyed in the sum-mary.
The goal of our work is abstractive summa-rization.
The main issues are identifying the knowl-edge conveyed by a graphic, selecting the conceptsthat should be conveyed in a summary, and integrat-ing them into coherent natural language sentences.As noted in the Introduction, information graphicsin popular media generally have a high-level mes-sage that they are intended to convey.
This mes-sage constitutes the primary communicative or dis-course goal (Grosz and Sidner, 1986) of the graphicand captures its main contribution to the overall dis-course goal of the entire document.
However, thegraphic also includes salient features that are impor-tant components of the graphic?s content.
For exam-ple, the graphic in Figure 1 is very jagged with sharpfluctuations, indicating that short-term changes havebeen inconsistent.
Since the graphic?s intended mes-sage represents its primary discourse goal, we con-tend that this message should form the core or fo-cus of the graphic?s summary.
The salient featuresshould be used to augment the summary of the graphand elaborate on its intended message.
Thus, ourmethodology consists of the following steps: 1) hy-pothesize the graphic?s primary discourse or com-municative goal (i.e., its intended message), 2) iden-tify additional propositions that are salient in thegraphic, and 3) construct a natural language sum-mary that integrates the intended message and theadditional salient propositions into a coherent text.Section 3 presents our methodology for hypothe-sizing a line graph?s intended message or discoursegoal.
It starts with an XML representation of thegraphic that specifies the x-y coordinates of the sam-pled pixels along the data series in the line graph, theaxes with tick marks and labels, the caption, etc.
;constructing the XML representation is the respon-sibility of a Visual Extraction Module similar to theone for bar charts described by Chester and Elzer(2005).
Section 4 presents our work on identifyingthe additional propositions that elaborate on the in-tended message and should be included in the sum-mary.
Section 5 discusses future work on realizingthe propositions in a natural language summary, andSection 6 reviews related work in multimodal andabstractive summarization.3 Identifying a Line Graph?s MessageResearch has shown that human subjects have astrong tendency to use line graphs to portray trendrelationships, as well as a strong tendency to de-scribe line graphs in terms of trends (Zacks andTversky, 1999).
We analyzed a corpus of sim-ple line graphs collected from various popular me-dia including USA Today, Businessweek, and The(Wilmington) News Journal, and identified a set of10 high-level message categories that capture thekinds of messages that are conveyed by a simpleline graph.
Table 1 defines four of them.
The com-plete list can be found in (Wu et al, 2010b).
Eachof these messages requires recognizing the visualtrend(s) in the depicted data.
We use a support vec-tor machine (SVM) to first segment the line graphinto a sequence of visually-distinguishable trends;this sequence is then input into a Bayesian net-work that reasons with evidence from the graphic42Intention Category DescriptionRT: Rising-trend There is a rising trend from <param1> to <param2>.CT: Change-trend There is a <direction2> trend from <param2> to <param3> that is signifi-cantly different from the <direction1> trend from <param1> to <param2>.CTR:Change-trend-returnThere is a <direction1> trend from <param3> to <param4> that is differentfrom the <direction2> trend between <param2> and <param3> and reflectsa return to the kind of <direction1> trend from <param1> to <param2>.BJ: Big-jump There was a very significant sudden jump in value between <param1> and<param2> which may or may not be sustained.Table 1: Four categories of High Level Messages for Line Graphsin order to recognize the graphic?s intended mes-sage.
The next two subsections outline thesesteps.
(Our corpus of line graphs can be found atwww.cis.udel.edu/?carberry/Graphs/viewallgraphs.php)3.1 Segmenting a Line GraphA line graph can consist of many short, jaggedline segments, although a viewer of the graphic ab-stracts from it a sequence of visually-distinguishabletrends.
For example, the line graph in Figure 1 con-sists of two trends: a relatively stable trend from1900 to 1930 and a longer, increasing trend from1930 to 2003.
Our Graph Segmentation Module(GSM) takes a top-down approach (Keogh et al,2001) to generalize the line graph into sequences ofrising, falling, and stable segments, where a segmentis a series of connected data points.
The GSM startswith the entire line graph as a single segment anduses a learned model to recursively decide whethereach segment should be split into two subsegments;if the decision is to split, the division is made at thepoint being the greatest distance from a straight linebetween the two end points of the original segment.This process is repeated on each subsegment untilno further splits are identified.
The GSM returns asequence of straight lines representing a linear re-gression of the points in each subsegment, whereeach straight line is presumed to capture a visually-distinguishable trend in the original graphic.We used Sequential Minimal Optimization (Platt,1999) in training an SVM to make segment split-ting decisions.
We chose to use an SVM because itworks well with high-dimensional data and a rela-tively small training set, and lessens the chance ofoverfitting by using the maximum margin separat-ing hyperplane which minimizes the worst-case gen-eralization errors (Tan et al, 2005).
18 attributes,falling into two categories, were used in buildingthe data model (Wu et al, 2010a).
The first cat-egory captures statistical tests computed from thesampled data points in the XML representation ofthe graphic; these tests estimate how different thesegment is from a linear regression (i.e., a straightline).
The second category of attributes capturesglobal features of the graphic.
For example, onesuch attribute relates the segment size to the size ofthe entire graphic, based on the hypothesis that seg-ments comprising more of the total graph may bestronger candidates for splitting than segments thatcomprise only a small portion of the graph.Our Graph Segmentation Module was trainedon a set of 649 instances that required a split/no-split decision.
Using leave-one-out cross validation,in which one instance is used for testing and theother 648 instances are used for training, our modelachieved an overall accuracy rate of 88.29%.3.2 A Bayesian Recognition SystemOnce the line graph has been converted intoa sequence of visually-distinguishable trends, aBayesian network is built that captures the possibleintended messages for the graphic and the evidencefor or against each message.
We adopted a Bayesiannetwork because it weighs different pieces of evi-dence and assigns a probability to each candidateintended message.
The next subsections briefly out-line the Bayesian network and its evaluation; detailscan be found in (Wu et al, 2010b).Structure of the Bayesian Network Figure 2shows a portion of the Bayesian network constructedfor Figure 1.
The top-level node in our Bayesian net-work represents all of the high-level message cat-43Intended Message... ...... ......CT?Suggestion?1CT IntentionRT IntentionEvidenceOtherPointsAnnotatedHave SuggestionEvidencePortion of GraphicEvidence EndpointsAnnotatedEvidence EvidenceSplittingPointsAnnotatedAdjective in CaptionEvidenceVerb in CaptionEvidenceFigure 2: A portion of the Bayesian networkegories.
Each of these possible non-parameterizedmessage categories is repeated as a child of thetop-level node; this is purely for ease of repre-sentation.
Up to this point, the Bayesian net-work is a static structure with conditional proba-bility tables capturing the a priori probability ofeach category of intended message.
When givena line graph to analyze, an extension of this net-work is built dynamically according to the partic-ulars of the graph itself.
Candidate (concrete) in-tended messages, having actual instantiated param-eters, appear beneath the high-level message cat-egory nodes.
These candidates are introduced bya Suggestion Generation Module; it dynamicallyconstructs all possible intended messages with con-crete parameters using the visually-distinguishabletrends (rising, falling, or stable) identified by theGraph Segmentation Module.
For example, for eachvisually-distinguishable trend, a Rising, Falling, orStable trend message is suggested; similary, for eachsequence of two visually-distinguishable trends, aChange-trend message is suggested.
For the graphicin Figure 1, six candidate messages will be gener-ated, including RT(1930, 2003), CT(1900, stable,1930, rise, 2003) and BJ(1930, 2003) (see Table 1).Entering Evidence into the Bayesian NetworkJust as listeners use evidence to identify the intendedmeaning of a speaker?s utterance, so also must aviewer use evidence to recognize a graphic?s in-tended message.
The evidence for or against eachof the candidate intended messages must be enteredinto the Bayesian network.
We identified three kindsof evidence that are used in line graphs: attention-getting devices explicitly added by the graphic de-signer (e.g., the annotation of a point with its value),aspects of a graphic that are perceptually-salient(e.g., the slope of a segment), and clues that sug-gest the general message category (e.g., a verb [ornoun derived from a verb such as rebound] in thecaption which might indicate a Change-trend mes-sage).
The first two kinds of evidence are attachedto the Bayesian network as children of each candi-date message node, such as the child nodes of ?CT-Suggestion-1?
in Figure 2.
The third kind of evi-dence is attached to the top level node as child nodesnamed ?Verb in Caption Evidence?
and ?Adjectivein Caption Evidence?
in Figure 2.Bayesian Network Inference We evaluated theperformance of our system for recognizing a linegraph?s intended message on a corpus of 215 linegraphs using leave-one-out cross validation in whichone graph is held out as a test graph and the con-ditional probability tables for the Bayesian networkare computed from the other 214 graphs.
Our sys-tem recognized the correct intended message withthe correct parameters for 157 line graphs, resultingin a 73.36% overall accuracy rate.4 Identifying Elaborative PropositionsOnce the intended message has been determined,the next step is to identify additional importantinformational propositions1 conveyed by the linegraph which should be included in the summary.To accomplish this, we collected data to determinewhat kinds of propositions in what situations weredeemed most important by human subjects, and de-veloped rules designed to make similar assessmentsbased on the graphic?s intended message and visualfeatures present in the graphic.4.1 Collecting Data from Human SubjectsParticipants in our study were given 23 different linegraphs.
With each graph, the subjects were provided1We define a ?proposition?
as a logical representation de-scribing a relationship between one or more concepts, while a?sentence?
is a surface form realizing one or more propositions.44Figure 3: From ?This Cable Outfit Is Getting Tuned In?in Businessweek magazine, Oct 4, 1999.with an initial sentence describing the overall in-tended message of the graphic.
The subjects wereasked to add additional sentences so that the com-pleted summary captured the most important infor-mation conveyed by the graphic.
The graphs werepresented to the subjects in different orders, and thesubjects completed as many graphs as they wantedduring the one hour study session.
The set coveredthe eight most prevalent of our intended messagecategories and a variety of visual features.
Roughlyhalf of the graphs were real-world examples fromthe corpus used to train the Bayesian network inSection 3.2, (e.g., Figure 3), with the others createdspecifically to fill a gap in the coverage of intendedmessages and visual features.We collected a total of 998 summaries written by69 human subjects for the 23 different line graphs.The number of summaries we received per graphranged from 37 to 50.
Most of the summaries werebetween one and four sentences long, in addition tothe initial sentence (capturing the graphic?s intendedmessage) that was provided for each graph.
A rep-resentative sample summary collected for the linegraph shown in Figure 3 is as follows, with the initialsentence provided to the study participants in italics:This line graph shows a big jump in Blon-der Tongue Laboratories stock price inAugust ?99.
The graph has many peaksand valleys between March 26th 1999 toAugust ?99 but maintains an average stockprice of around 6 dollars.
However, in Au-gust ?99 the stock price jumps sharply toaround 10 dollars before dropping quicklyto around 9 dollars by September 21st.4.2 Extracting & Weighting PropositionsThe data collected during the study was analyzed bya human annotator who manually coded the propo-sitions that appeared in each individual summary inorder to determine, for each graphic, which proposi-tions were used and how often.
For example, the setof propositions coded in the sample summary fromSection 4.1 were:?
volatile(26Mar99, Aug99)?
average val(26Mar99, Aug99, $6)?
jump 1(Aug99, $10)?
steep(jump 1)?
decrease 1(Aug99, $10, 21Sep99, $9)?
steep(decrease 1)From this information, we formulated a set ofrules governing the use of each proposition accord-ing to the intended message category and variousvisual features.
Our intuition was that by findingand exploiting a correlation between the intendedmessage category and/or certain visual features andthe propositions appearing most often in the human-written summaries, our system could use these in-dicators to determine which propositions are mostsalient in new graphs.
Our rules assign a weightto each proposition in the situation captured by therule; these weights are based on the relative fre-quency of the proposition being used in summariesreflecting similar situations in our corpus study.
Therules are organized into three types:1.
Message Category-only (M):IF M = m THEN select P with weight w12.
Visual Feature-only (V):IF V = v THEN select P with weight w23.
Message Category + Visual Feature:IF M = m and V = vTHEN select P with weight w2We constructed type 1 (Message Category-only)rules when a plurality of human-written summaries45in our corpus for all line graphs belonging to agiven message category contain the proposition.
Aweight was assigned according to the frequency withwhich the proposition was included.
This weighting,shown in Equation 1, is based on the proportion ofsummaries for each line graph in the corpus havingintended message m and containing proposition P.w1 =n?i=1PiSi(1)In this equation, n is the number of line graphs inthis intended message category, Si is the total num-ber of summaries for a particular line graph with thisintended message category, and Pi is the number ofthese summaries that contain the proposition.Intuitively, a proposition appearing in all sum-maries for all graphs in a given message categorywill have a weight of 1.0, while a proposition whichnever appears will have a weight of zero.
How-ever, a proposition appearing in all summaries forhalf of the graphs in a category, and rarely for theother half of the graphs in that category, will have amuch lower weight than one which appears in halfof the summaries for all the graphs in that category,even though the overall frequencies could be equalfor both.
In this case, the message category is aninsufficient signal, and it is likely that the formerproposition is more highly correlated to some par-ticular visual feature than to the message category.Weights for type 2 and type 3 rules (VisualFeature-only and Message Category + Visual Fea-ture) are slightly more complicated in that they in-volve a measure of degree for the associated visualfeature rather than simply its presence.
The defini-tion of this measure varies depending on the natureof the visual feature (e.g., steepness of a trend line,volatility), but all such measures range from zero toone.
Additionally, since the impact of a visual fea-ture is a matter of degree, the weighting cannot relyon a simple proportion of summaries containing theproposition as in type 1 rules.
Instead, it is neces-sary to find the covariance between the magnitude ofthe visual feature (|v|) and how frequently the corre-sponding proposition is used (PS ) in the corpus sum-maries for the n graphs having this visual feature, asshown in Equation 2.Cov(|v|,PS) =[(?ni=1 |vi|n?ni=1PiSin)?
?ni=1 |vi|PiSin] (2)Then for a particular graphic whose magnitude forthis feature is |v|, we compute the weight w2 for theproposition P as shown in Equation 3.w2 = |v| ?
Cov(|v|,PS) (3)This way, the stronger a certain visual feature is in agiven line graph, the higher the weight for the asso-ciated proposition.Type 3 rules (Message Category + Visual Fea-ture) differ only from type 2 rules in that they arerestricted to a particular intended message category,rather than any line graph having the visual featurein question.
For example, a proposition compar-ing the slope of two trends may be appropriate fora graph in the Change-trend message category, butdoes not make sense for a line graph with only a sin-gle trend (e.g., Rising-trend).Once all propositions have been extracted andranked, these weights are passed along to a graph-based content selection framework (Demir et al,2010) that iteratively selects for inclusion in the ini-tial summary those propositions which provide thebest coverage of the highest-ranked information.4.3 Sample Rule ApplicationFigures 1 and 4 consist of two different line graphswith the same intended message category: Change-trend.
Figure 1 shows a stable trend in annual sealevel difference from 1900 to 1930, followed by arising trend through 2003, while Figure 4 shows arising trend in Durango sales from 1997 to 1999,followed by a falling trend through 2006.
Proposi-tions associated with type 1 rules will have the sameweights for both graphs, but propositions related tovisual features may have different weights.
For ex-ample, the graph in Figure 1 is far more volatile thanthe graph in Figure 4.
Thus, the type 2 rule associ-ated with volatility will have a very high weight forthe graph in Figure 1 and will almost certainly be in-cluded in the initial summary of that line graph (e.g.,462006200520042003200219971998199920012000200,000 150,0001999: 189,84070,6062006:50,000100,000Declining Durango sales0Figure 4: From ?Chrysler: Plant had $800 million im-pact?
in The (Wilmington) News Journal, Feb 15, 2007.?The values vary a lot...?, ?The trend is unstable...?
),possibly displacing a type 1 proposition that wouldstill appear in the summary for the graph in Figure 4.5 Future WorkOnce the propositions that should be included in thesummary have been selected, they must be coher-ently organized and realized as natural language sen-tences.
We anticipate using the FUF/SURGE sur-face realizer (Elhadad and Robin, 1996); our col-lected corpus of line graph summaries provides alarge set of real-world expressions to draw fromwhen crafting the surface realization forms our sys-tem will produce for the final-output summaries.Our summarization methodology must also be eval-uated.
In particular, we must evaluate the rules foridentifying the additional informational propositionsthat are used to elaborate the overall intended mes-sage, and the quality of the summaries both in termsof content and coherence.6 Related WorkImage summarization has focused on constructing asmaller image that contains the important content ofa larger image (Shi et al, 2009), selecting a set ofrepresentative images that summarize a collectionof images (Baratis et al, 2008), or constructing anew diagram that summarizes one or more diagrams(Futrelle, 1999).
However, all of these efforts pro-duce an image as the end product, not a textual sum-mary of the content of the image(s).Ferres et al (2007) developed a system for con-veying graphs to blind users, but it generates thesame basic information for each instance of a graphtype (e.g., line graphs) regardless of the individualgraph?s specific characteristics.
Efforts toward sum-marizing multimodal documents containing graph-ics have included na?
?ve approaches relying on cap-tions and direct references to the image in the text(Bhatia et al, 2009), while content-based imageanalysis and NLP techniques are being combined formultimodal document indexing and retrieval in themedical domain (Ne?ve?ol et al, 2009).Jing and McKeown (1999) approached abstrac-tive summarization as a text-to-text generation task,modifying sentences from the original document viaediting and rewriting.
There have been some at-tempts to do abstractive summarization from seman-tic models, but most of it has focused on text docu-ments (Rau et al, 1989; Reimer and Hahn, 1988),though Alexandersson (2003) used abstraction andsemantic modeling for speech-to-speech translationand multilingual summary generation.7 DiscussionInformation graphics play an important communica-tive role in popular media and cannot be ignored.We have presented our methodology for construct-ing a summary of a line graph.
Our method is ab-stractive, in that we identify the important high-levelknowledge conveyed by a graphic and capture it inpropositions to be realized in novel, coherent natu-ral language sentences.
The resulting summary canbe integrated with a summary of the document?s textto produce a rich summary of the entire multimodaldocument.
In addition, the graphic?s summary canbe used along with a screen reader to provide sight-impaired users with full access to the knowledgeconveyed by multimodal documents.AcknowledgmentsThis work was supported in part by the National In-stitute on Disability and Rehabilitation Research un-der Grant No.
H133G080047.ReferencesJan Alexandersson.
2003.
Hybrid Discourse Modelingand Summarization for a Speech-to-Speech Transla-tion System.
Ph.D. thesis, Saarland University.Evdoxios Baratis, Euripides Petrakis, and Evangelos Mil-ios.
2008.
Automatic web site summarization by im-age content: A case study with logo and trademark47images.
IEEE Transactions on Knowledge and DataEngineering, 20(9):1195?1204.Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.2009.
Generating synopses for document-elementsearch.
In Proceeding of the 18th ACM Conferenceon Information and Knowledge Management, CIKM?09, pages 2003?2006, Hong Kong, November.
ACM.Sandra Carberry, Stephanie Elzer, and Seniz Demir.2006.
Information graphics: an untapped resource fordigital libraries.
In Proc.
of the 29th Annual Int?l ACMSIGIR Conf.
on Research & Development in Informa-tion Retrieval, SIGIR ?06, pages 581?588, Seattle, Au-gust.
ACM.Daniel Chester and Stephanie Elzer.
2005.
Getting com-puters to see information graphics so users do not haveto.
In Proceedings of the 15th International Sympo-sium on Methodologies for Intelligent Systems (LNAI3488), ISMIS 2005, pages 660?668, Saratoga Springs,NY, June.
Springer-Verlag.Herbert Clark.
1996.
Using Language.
Cambridge Uni-versity Press.Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.2008.
Generating textual summaries of bar charts.In Proceedings of the 5th International Natural Lan-guage Generation Conference, INLG 2008, pages 7?15, Salt Fork, Ohio, June.
ACL.Seniz Demir, Sandra Carberry, and Kathleen F. Mc-Coy.
2010.
A discourse-aware graph-based content-selection framework.
In Proceedings of the 6th In-ternational Natural Language Generation Conference,INLG 2010, pages 17?26, Trim, Ireland, July.
ACL.Michael Elhadad and Jacques Robin.
1996.
An overviewof SURGE: a re-usable comprehensive syntactic re-alization component.
In Proceedings of the 8th In-ternational Natural Language Generation Workshop(Posters & Demos), Sussex, UK, June.
ACL.Stephanie Elzer, Sandra Carberry, Daniel Chester, SenizDemir, Nancy Green, Ingrid Zukerman, and KeithTrnka.
2005.
Exploring and exploiting the limitedutility of captions in recognizing intention in infor-mation graphics.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics, pages 223?230, Ann Arbor, June.
ACL.Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.2011.
The automated understanding of simple barcharts.
Artificial Intelligence, 175:526?555, February.Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, LouisBoucher, Antoine Chretien, and Martin Lachance.2007.
Improving accessibility to statistical graphs: theiGraph-Lite system.
In Proc.
of the 9th Int?l ACMSIGACCESS Conf.
on Computers & Accessibility, AS-SETS ?07, pages 67?74, Tempe, October.
ACM.Robert P. Futrelle.
1999.
Summarization of diagrams indocuments.
In I. Mani and M. Maybury, editors, Ad-vances in Automatic Text Summarization.
MIT Press.Barbara Grosz and Candace Sidner.
1986.
Attention,Intentions, and the Structure of Discourse.
Computa-tional Linguistics, 12(3):175?204.Hongyan Jing and Kathleen R. McKeown.
1999.
Thedecomposition of human-written summary sentences.In Proc.
of the 22nd Annual Int?l ACM SIGIR Conf.on Research & Development in Information Retrieval,SIGIR ?99, pages 129?136, Berkeley, August.
ACM.Eamonn J. Keogh, Selina Chu, David Hart, andMichael J. Pazzani.
2001.
An online algorithmfor segmenting time series.
In Proceedings of the2001 IEEE International Conference on Data Mining,ICDM ?01, pages 289?296, Washington, DC.
IEEE.Aure?lie Ne?ve?ol, Thomas M. Deserno, Ste?fan J. Darmoni,Mark Oliver Gu?ld, and Alan R. Aronson.
2009.
Nat-ural language processing versus content-based imageanalysis for medical document retrieval.
Journal of theAmerican Society for Information Science and Tech-nology, 60(1):123?134.John C. Platt.
1999.
Fast training of support vectormachines using sequential minimal optimization.
InB.
Scho?lkopf, C. J. C. Burges, and A. J. Smola, editors,Advances in kernel methods: support vector learning,pages 185?208.
MIT Press, Cambridge, MA, USA.Lisa F. Rau, Paul S. Jacobs, and Uri Zernik.
1989.
In-formation extraction and text summarization using lin-guistic knowledge acquisition.
Information Process-ing & Management, 25(4):419 ?
428.Ulrich Reimer and Udo Hahn.
1988.
Text condensationas knowledge base abstraction.
In Proceedings of the4th Conference on Artificial Intelligence Applications,CAIA ?88, pages 338?344, San Diego, March.
IEEE.Liang Shi, Jinqiao Wang, Lei Xu, Hanqing Lu, andChangsheng Xu.
2009.
Context saliency based im-age summarization.
In Proceedings of the 2009 IEEEinternational conference on Multimedia and Expo,ICME ?09, pages 270?273, New York.
IEEE.Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.2005.
Introduction to Data Mining.
Addison Wesley.Peng Wu, Sandra Carberry, and Stephanie Elzer.
2010a.Segmenting line graphs into trends.
In Proceedings ofthe 2010 International Conference on Artificial Intel-ligence, ICAI ?10, pages 697?703, Las Vegas, July.Peng Wu, Sandra Carberry, Stephanie Elzer, and DanielChester.
2010b.
Recognizing the intended messageof line graphs.
In Proc.
of the 6th Int?l Conf.
on Dia-grammatic Representation & Inference, Diagrams ?10,pages 220?234, Portland.
Springer-Verlag.Jeff Zacks and Barbara Tversky.
1999.
Bars and lines:A study of graphic communication.
Memory & Cog-nition, 27:1073?1079.48
