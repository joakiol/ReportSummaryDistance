Coling 2010: Poster Volume, pages 766?774,Beijing, August 2010A Review Selection Approach for Accurate Feature Rating EstimationChong Long?
Jie Zhang?
Xiaoyan Zhu???
State Key Laboratory on Intelligent Technology and Systems,Tsinghua National Laboratory for Information Science and Technology,Department of Computer Science, Tsinghua University?School of Computer Engineering, Nanyang Technological University?
{Corresponding Author: zxy-dcs@tsinghua.edu.cn}AbstractIn this paper, we propose a review se-lection approach towards accurate esti-mation of feature ratings for services onparticipatory websites where users writetextual reviews for these services.
Ourapproach selects reviews that compre-hensively talk about a feature of a serviceby using information distance of the re-views on the feature.
The rating estima-tion of the feature for these selected re-views using machine learning techniquesprovides more accurate results than thatfor other reviews.
The average of theseestimated feature ratings also better rep-resents an accurate overall rating for thefeature of the service, which providesuseful feedback for other users to choosetheir satisfactory services.1 IntroductionMost of participatory websites such as Amazon(amazon.com) do not collect from users feature1ratings for services, simply because it may costusers too much effort to provide detailed featureratings.
Even for a very few websites that do col-lect feature ratings such as a popular travel web-site TripAdvisor (tripadvisor.com), a big portion(approximately 43%) of users may still not pro-vide them.
However, feature ratings are usefulfor users to make informed consumption deci-sions especially in the case where users may beinterested more in some particular features of theservices.
Machine learning techniques have beenproposed for sentiment classification (Pang et al,2002; Mullen and Collier, 2004) based on anno-tated samples from experts, but they have limited1A feature broadly means an attribute or a function of aservice.performance especially when estimating ratingsof a multi-point scale (Pang and Lee, 2005).In this paper, we propose a novel review se-lection approach for accurate feature rating es-timation.
More specifically, our approach se-lects reviews written by the users who compre-hensively talk about a certain feature of a ser-vice - that are comprehensive on this feature, us-ing information distance of reviews on the fea-ture based on Kolmogorov complexity (Li andVita?nyi, 1997).
This feature is obviously impor-tant to the users.
People tend to be more knowl-edgable in the aspects they consider important.These users therefore represent a subset of ex-perts.
Statistical analysis reveals that these ex-pert users are more likely to agree on a commonrating for the feature of the service.
The ratingestimation of the feature for these selected re-views based on annotated samples from expertsusing machine learning techniques is thus able toprovide more accurate results than that for otherreviews.
This statistical evidence also allows usto use the average of the estimated feature rat-ings to better represent an overall opinion of ex-perts for the feature of the service, which willbe particularly useful for assisting other users tocorrectly make their consumption decisions.We verify our approach and arguments basedon real data collected from the TripAdvisor web-site.
First, our approach is shown to be able toeffectively select reviews that comprehensivelytalk about features of a service.
We then adoptthe machine learning method proposed in (Pangand Lee, 2005) and the Bayesian Network clas-sifier (Russell and Norvig, 2002) for feature rat-ing estimation.
Our experimental results showthat the accuracy of estimating feature ratingsfor these selected reviews is higher than thatfor other reviews, for both the machine learning766methods.
And, the average of these estimatedratings is testified to closely represent the over-all feature rating of the service.
Our approach istherefore verified to be a successful step towardsaccurate feature rating estimation.2 Related WorkOur work aims at estimating feature ratings of aservice based on its textual reviews.
It is relatedto sentiment classification.
The task of sentimentclassification is to determine the semantic orien-tations of words, sentences or documents.
(Panget al, 2002) is the earliest work of automaticsentiment classification at document level, usingseveral machine learning approaches with com-mon textual features to classify movie reviews.Mullen and Collier (Mullen and Collier, 2004)integrated PMI values, Osgood semantic factorsand some syntactic relations into the features ofSVM.
Pang and Lee (Pang and Lee, 2004) pro-posed another machine learning method basedon subjectivity detection and minimum-cut ingraph.
However, these approaches focus only onbinary classification of reviews.In 2005, Pang and Lee extended their ear-lier work in (Pang and Lee, 2004) to determinea reviewer?s evaluation with respect to multi-scales (Pang and Lee, 2005).
The rating esti-mation is viewed as multi-class sentiment cate-gorization on documents.
They used SVM re-gression as the multi-class classifier, and also ap-plied a meta-algorithm based on a metric label-ing formulation of the problem, which alters agiven n-ary classifier?s output in an explicit at-tempt to ensure that similar items receive sim-ilar labels.
They collected movie reviews froma website named IMDB and tested the perfor-mance of their classifier under both four-classand five-class categorization.
The five-class sen-timent classification is adopted in the evaluationof our method (see Section 5).
The performanceof their approach is limited.
One important rea-son is that their method considers every reviewwhen estimating a feature rating of a movie.However, some reviews do not contain much ofthe users?
opinions about a certain feature sim-ply because the users do not care much or arenot knowledgable about the feature.
In our work,we study the characteristics of reviews?
featureratings.
We investigate which reviews are moreuseful for us to estimate feature ratings.
Fromsome observations stated in the next section, wewill see that reviews written by different usersreflect their own preferred features of a service.3 Accurate Feature Rating EstimationParticipatory websites allow users to write tex-tual reviews to discuss features of services thatthey have consumed.
These reviews usually con-tain words that strongly express the users?
opin-ions about the corresponding features.
Thesewords contain important information for estimat-ing a numerical rating for the feature.
The es-timated ratings can be used for assisting otherusers when they need to choose which servicesto consume.
Machine learning techniques areoften used for training a learner based on an-notated samples from experts and estimating arating for a feature discussed in a review.
How-ever, for a review that does not mention a fea-ture or discusses it only in a limited sense, theestimation accuracy is expected to be very low.Besides, the opinion expressed by the user whowrites this kind of review is not representativebecause this user obviously does not care muchabout the feature.
We believe that if we carefullyselect reviews for estimating feature ratings, theaccuracy will be increased and the estimated rat-ings will be more representative.We then statistically analyze real data col-lected from the TripAdvisor website.
The resultsreveal that users who comprehensively discussa feature of a service in their reviews are morelikely to agree on a common rating for this fea-ture of the service.
This phenomenon can alsobe intuitively explained as follows.
For the userswho comprehensively discuss about a feature,the feature is obviously more important to them.People tend to be more knowledgable in the as-pects they consider important.
These users there-fore represent a subset of experts.
Experts likelyprovide more objective and representative feed-back about the feature, and therefore the ratingsfrom them for the feature contain less noise and767are more similar.Based on the above discussion that expertstend to have similar opinions on a feature of aservice, a learner trained by a machine learningtechnique based on annotated samples from ex-perts should then be able to more accurately esti-mate the feature ratings from reviews written byother experts.
Since the opinions of experts con-verge, the average of the estimated feature rat-ings also better represents an overall rating forthe feature of the service.We propose a review selection approach us-ing information distance of reviews on the fea-ture based on Kolmogorov complexity, to selectreviews that comprehensively discuss a featureof a service.
We rank the reviews based on thecomprehensiveness on the feature.
The top re-views will be selected for the estimation of fea-ture ratings.
Also, the average of these estimatedfeature ratings will be used for representing theoverall rating for the feature.
Next, we will firstdescribe in detail how our approach selects com-prehensive reviews on a given feature.4 Our Review Selection ApproachOur review selection approach selects reviewsthat comprehensively talk about a feature.
Ac-cording to this definition, a review?s comprehen-siveness depends on the amount of informationdiscussed on a feature.
We use Kolmogorovcomplexity and information distance to measurethe amount of information.
Kolmogorov com-plexity was introduced almost half a century agoby R. Solomonoff, A.N.
Kolmogorov and G.Chaitin, see (Li and Vita?nyi, 1997).
It is nowwidely accepted as an information theory for in-dividual objects parallel to that of Shannon?s in-formation theory which is defined on an ensem-ble of objects.4.1 TheoryFix a universal Turing machine U .
The Kol-mogorov complexity (Li and Vita?nyi, 1997) of abinary string x condition to another binary stringy, KU (x|y), is the length of the shortest (prefix-free) program for U that outputs x with input y.It can be shown that for different universal Tur-ing machine U ?, for all x, yKU (x|y) = KU ?
(x|y) + C,where the constant C depends only on U ?.
ThusKU (x|y) can be simply written as K(x|y).
Theywrite K(x|?
), where ?
is the empty string, asK(x).
It has also been defined in (Bennett etal., 1998) that the energy to convert between xand y to be the smallest number of bits needed toconvert from x to y and vice versa.
That is, withrespect to a universal Turing machine U , the costof conversion between x and y is:E(x, y)=min{|p|: U(x, p)=y, U(y, p)=x}(1)It is clear that E(x, y) ?
K(x|y) + K(y|x).From this observation, the following theorem hasbeen proved in (Bennett et al, 1998):Theorem 1 E(x, y) = max{K(x|y),K(y|x)}.Thus, the max distance was defined in (Ben-nett et al, 1998):Dmax(x, y) = max{K(x|y),K(y|x)}.
(2)This distance is shown to satisfy the basicdistance requirements such as positivity, sym-metricity, triangle inequality and is admissible.Here for an object x, we can measure its in-formation by Kolmogorov complexity K(x); fortwo objects x and y, their shared information canbe measured by information distance D(x, y).In (Long et al, 2008), the authors generalizethe theory of information distance to more thantwo objects.
Similar to Equation 1, given stringsx1, .
.
.
, xn, they define the minimum amount ofthermodynamic energy needed to convert fromany xi to any xj as:Em(x1, .., xn)=min{|p|:U(xi, p, j)=xj for all i,j}Then it is proved in (Long et al, 2008) that:Theorem 2 Modulo to an O(logn) additive fac-tor,miniK(x1 .
.
.
xn|xi) ?
Em(x1, .
.
.
, xn)Given n objects, the left-hand side of Equa-tion 3 may be interpreted as the most compre-hensive object that contains the most informationabout all of the others.7684.2 Review Selection MethodOur review selection method is based on the in-formation distance discussed in the previous sec-tion.
However, our problem is that neither theKolmogorov complexity K(?, ?)
nor Dmax(?, ?
)is computable.
Therefore, we find a way to ?ap-proximate?
these two measures.
The most use-ful information in a review article is the Englishwords that are related to the features.
If we canextract all of these related words from the reviewarticles, the size of the word set can be regardedas a rough estimation of information content (orKolmogorov complexity) of the review articles.In Section 5 we will see that this gives very goodpractical results.4.2.1 OutlineOur method is outlined in the following.
First,for each type of product or service (such as a ho-tel), a small set of core feature words (such asprice and room) is generated through statistics.Then, these feature words are used to generatethe expanded words.
Third, a parser is used tofind the dependent words associated with the oc-currence of the core feature words and expandedwords in a review.
For each review-feature pair,the union of the core feature words, expandedwords and dependent words in the review definesthe related word set of the review on the feature.Lastly, information distance is used to select themost comprehensive reviews on a feature.4.2.2 Word ExtractionFeature words are the most direct and frequentwords describing a feature, for example, price,room or service of a hotel.
Given a feature, thecore feature words are the very few most com-mon English words that are used to refer to thatfeature.
For example, both ?value?
and ?price?are used to refer to the same feature of a ho-tel.
In (Hu and Liu, 2004), the authors indicatethat when customers comment on product fea-tures, the words they use converge.
If we re-move the feature words with frequency lowerthan 1% of the total frequency of all featurewords, the remaining words, which are just corefeature words, can still cover more than 90%occurrences.
So firstly we extract those wordsthrough statistics; then some of those with thesame meaning (such as ?value?
and ?price?)
aregrouped into one feature.
They are just ?core fea-ture words?.Apart from core feature words, many otherless-frequently used words that are connectedto the feature also contribute to the informationcontent of the feature.
For example, ?price?
isan important feature of a hotel, but the word?price?
is usually dropped from a sentence.
In-stead, words such as ?$?, ?dollars?, ?USD?, and?CAD?
are used.
We use information distanced(., .)
based on Google to expand words (Cili-brasi and Vita?nyi, 2007).
Let ?
be a feature andA be the set of its core feature words.
The dis-tance between a word w and the feature ?
is thendefined to bed(w,?)
= minv?Ad(w, v)A distance threshold is then used to determinewhich words should be in the set of expandedwords for a given feature.If a core feature word or an expanded word isfound in a sentence, the words which have gram-matical dependent relationship with it are calledthe dependent words (de Marneffe et al, 2006).For example, in sentence ?It has a small, butbeautiful room?, the words ?small?
and ?beauti-ful?
are both dependent words of the core featureword ?room?.
All these words also contribute tothe reviews and are important to determine thereviewer?s attitude towards a feature.The Stanford Parser (de Marneffe et al, 2006)is used to parse each review.
For review i andfeature j, the core feature words and expandedwords in the review are first computed.
Then theparsing result is examined to find all the depen-dent words for the core feature words and ex-panded words, all of which are called ?relatedwords?.4.2.3 Computing Information DistanceIf there are m reviews x1, x2, .
.
.
, xm, n fea-tures u1, u2, .
.
.
, un, and the related word set Siis defined to be the union of all the related wordsthat occur in the review xi.
From the left-handside of Equation 3, the most comprehensive xi769on feature uk is such thati = argminiK(S1 .
.
.
Sn|Si, uk).
(3)Let Si and Sj be two sets of words,K(SiSj |uk) = K(Si ?
Sj |uk),K(Si|Sj , uk) = K(Si \ Sj |uk),andK(Si|uk)=?wK(w|uk)?
?w(K(w, uk)?K(uk))where w ?
Si and w is in xi?s related word set onfeature uk.
For each word w in a set S, the Kol-mogorov complexity can be estimated throughcoding theorem (Li and Vita?nyi, 1997):K(w, uk)=?
logP (w, uk), K(uk)=?
logP (uk)where P (w, uk) can be estimated by df(w, uk),which is the document frequency of word w andfeature uk co-exist on the whole corpus.
Sim-ilarly, P (uk) can be estimated by feature uk?sdocument frequency on the corpus.
In the nextsection, Equation 3 will be used to select reviewsthat comprehensively talk about a feature.5 Experimental VerificationIn this section, we present a set of experimen-tal results to support our work.
Our experimentsare carried out using real data collected from thetravel website TripAdvisor.
This website indexeshotels from cities across the world.
It collectsfeedback from travelers.
Feedback of each trav-eler consists of a textual review written by thetraveler and numerical ratings (from 1, lowest,to 5, highest) for different features of hotels (e.g.,value, service, rooms).Table 1: Summary of the Data SetLocation# Hotels# Feedback# Feedback withfeature ratingBoston 57 3949 2096Sydney 47 1370 879Vegas 40 5588 3144We crawled this website to collect travelers?feedback for hotels in three cities: Boston, Syd-ney and Las Vegas.
Note that during this crawl-ing process, we carefully removed informationabout travelers and hotels to protect their privacy.For users?
feedback, we recorded only the tex-tual reviews and the numerical ratings on fourfeatures: Value(V), Rooms(R), Service(S) andCleanliness(C).
These features are rated by a sig-nificant number of users.
Table 1 summarizesour data set.
For each one of the cities, this tablecontains information about the number of hotels,the total amount of feedback and the amount offeedback with feature ratings.
In general, eachhotel has sufficient amount of feedback with fea-ture ratings for us to evaluate our work.Table 2: Comprehensive Reviews on Each Fea-ture (Boston)Top # V R S C1 Y Y Y Y2 Y Y Y Y3 N Y Y N4 Y Y Y N5 Y Y Y Y6 Y Y N Y... ... ... ... ...5.1 Evaluation of Review SelectionWe first evaluate the performance of our re-view selection approach using manually anno-tated data.
More specifically, in our data set,for one city, 40 reviews (120 reviews in total)are selected for manual annotation.
The annota-tor looks over each review and decides whetherthe review is comprehensive on a given feature.Comprehensive reviews on the feature are anno-tated as ?Y?, and the reviews that are not com-prehensive on this feature are annotated as ?N?.For the review set of each city, the number of re-views annotated as comprehensive is equal to orless than 20% of the total number of the selectedreviews for this city (eight in this experiment).Note that it is possible that one review can becomprehensive on more than one features.We then use our review selection approach770discussed in Section 4 to rank the reviews for ho-tels in each city, according to their comprehen-siveness on each feature.
For example, the mostcomprehensive review on the feature ?Value?,which has the minimal information distance tothis feature (see Equation 3), is ranked No.1.
Ta-ble 2 shows the annotated reviews for Boston ho-tels that are ranked on top six on each feature.
Itcan be obviously seen from the table that mostof these top reviews are labeled as comprehen-sive reviews on respective features.
Our com-prehensive review selection approach generallyperforms well.Table 3: Performance of Comprehensive ReviewSelectionCity Feature Precision Recall F-ScoreBoston V 0.833 0.714 0.769R 1.000 0.875 0.933S 0.857 1.000 0.923C 0.833 1.000 0.909Sydney V 0.667 1.000 0.800R 0.600 0.857 0.706S 0.667 0.857 0.750C 0.750 1.000 0.857Vegas V 0.778 1.000 0.875R 0.727 1.000 0.842S 0.714 0.714 0.714C 0.667 0.800 0.727To clearly present the performance of ourcomprehensive review selection approach, weuse the measures of precision, recall and f-score.The measure f-score is a single value that canrepresent the result of our evaluation.
It is theharmonic mean of precision and recall.
Supposethere are n reviews in total.
Let pjk (1 ?
k ?
n)be the review ranked the kth comprehensive onfeature j. Definezjk ={1 if pjk is labelled comprehensive on j;0 otherwise.The precision P , recall R, and f-score F of top kcomprehensive reviews on feature j are formal-ized as followsPjk =?kl=1 zjlk ,Rjk =?kl=1 zjl?Nl=1 zjl,Fjk =2PjkRjkPjk +RjkFor each ranked review set on feature j, themaximum Fjk and its associated Pjk and Rjk arelisted in Table 3.
From this table, it can be seenthat for the best f-scores, the precision and recallvalues are mostly larger than 70%, that is, a greatpart of reviews that are labeled as comprehensivereceive top rankings from our comprehensive re-view selection approach.
Our approach is thuscarefully verified to be able to accurately selectcomprehensive reviews on any given feature.5.2 Statistical AnalysisA group of users who comprehensively discussa certain feature are more likely to agree on acommon rating for that feature.
In this experi-ment, we use our review selection approach toverify this argument.Table 4: Deviation of Feature RatingsCity Feature 20% 50% AllV 0.884 (0.0003) 1.030 1.136Boston R 0.940 (0.2248) 1.037 1.013S 1.026 (0.0443) 1.130 1.144C 0.798 (0.0093) 0.892 0.949V 0.862 (0.0266) 1.009 1.054Sydney R 0.788 (0.0497) 0.932 0.945S 0.941 (0.0766) 1.162 1.116C 0.651 (0.0037) 0.905 0.907V 0.845 (0.0002) 1.236 1.291Vegas R 1.105 (0.2111) 1.148 1.175S 1.112 (0.0574) 1.286 1.269C 0.936 (0.0264) 1.096 1.158More specifically, for each city, hotels that re-ceive no less than 10 reviews with feature ratingsare selected.
We use our comprehensive reviewselection approach to select top 20% and 50%comprehensive reviews on each feature for ho-tels in each city.
We calculate the standard devi-ation of their feature ratings, as well as that of allfeature ratings, for each hotel in a city.
We thenaverage these standard deviations over the hotelsin the same city.
The average values are listedin Table 4.
The feature ratings of comprehensivereviews on the feature have smaller average stan-771dard deviations.
Standard T-test is used to mea-sure the significance of the results between top20% comprehensive reviews and all reviews, cityby city and feature by feature.
Their p-values areshown in the braces, and they are significant atthe standard 0.05 significance threshold.
It canbe seen from the table that although for someitems there does not seem to be a significant dif-ference, the results are significant for the entiredata set.Therefore, when these travelers write reviewsthat are comprehensive on one feature, their rat-ings for this feature tend to converge.
This evi-dence indicates that the estimation of ratings forthe feature from these comprehensive reviewscan provide better results, which will be con-firmed in Section 5.3.
These estimated featureratings can also be averaged to represent a spe-cific opinion of these travelers on the feature,which will be verified in Section 5.4.5.3 Feature Rating EstimationIn this section, we carry out experiments to tes-tify that the estimation of feature ratings for com-prehensive reviews using our review selectionapproach provides better performance than thatfor all reviews.
We adopt the approach of Pangand Lee (Pang and Lee, 2005) described in Sec-tion 2 for feature rating estimation.
In short, theyapplied a meta-algorithm, based on a metric la-beling formulation of the problem to alter a givenn-ary SVM?s output in an explicit attempt.
Wealso adopt a Baysian Network classifier for fea-ture rating estimation.Similar to the method of Pang and Lee, webuild up a feature rating classification system toestimate reviews?
feature ratings.
However, themethod of Pang and Lee focuses only on sin-gle rating classification for a review and assumesthat every word of the review can contribute tothis single rating.
While it comes to feature rat-ing classification, the system has to decide whichterms or phrases in the review are talking aboutthis feature.
We train a Naive Bayes classifierto retrieve all the sentences related to a feature.Then all the core feature words, expanded wordsand dependent words are extracted to train aSVM classifier and the Bayesian Network clas-sifier for five-class classification (1 to 5).
Theeight-fold cross-validation is used to train andtest the performance of feature rating estimationon all the reviews and the top 20% comprehen-sive reviews, respectively.0.40.50.60.70.80.911.11.2Value Rooms Service Clean AverageAverageDifferenceComprehensive ReviewsAll ReviewsFigure 1: Average Error of Feature Rating Esti-mation for the Adopted Method of Pang and Lee0.40.50.60.70.80.911.11.2Value Rooms Service Clean AverageAverageDifferenceComprehensive ReviewsAll ReviewsFigure 2: Average Error of Feature Rating Esti-mation for the Bayesian Network classifierWe formalize a performance measure as fol-lows.
Suppose there are n reviews in total.
For atest review i (1 ?
i ?
n), its real feature rating(given by the review writer) is fi, and its predi-cated feature rating (predicted by our classifica-tion system) is gi.
Both fi and gi are integersbetween 1 and 5.
The performance of the classi-fication on all n reviews can be measured by theaverage of the absolute difference (d) betweeneach fi and gi pair,d =?ni=1 |fi ?
gi|n .
(4)The lower d is, the better performance the clas-sifier can provide.Figures 1 and 2 show the results for the perfor-mance of feature rating estimation on all reviewsversus that on selected comprehensive reviews,772for the adopted approach of Pang and Lee andthe Baysian Network classifier respectively.
Itcan be seen that the average difference betweenreal feature ratings and estimated feature ratingson each feature when using selected comprehen-sive reviews is significantly lower than that whenusing all reviews, for both the approaches.
Onaverage, the performance of feature rating esti-mation is improved by more than 12.5% usingour review selection approach.
And, our reviewselection approach is generally applicable to dif-ferent classifiers.5.4 Estimating Overall Feature RatingSupported by the statistical evidence verified inSection 5.2 that the users who write compre-hensive reviews on one feature will more likelyagree on a common rating for this feature, wecan then use an average of the feature ratings fortop 20% comprehensive reviews to reflect a gen-eral opinion of knowledgable/expert users.
Inthis section, we show directly the performanceof estimating an overall feature rating for a ho-tel using ratings for the selected comprehensivereviews, and compare it with that for all reviews.Table 5: Performance of Estimating Overall Fea-ture Rating for Comprehensive ReviewsCity V R S C AVGBoston 0.637 0.426 0.570 0.660 0.573Sydney 0.273 0.729 0.567 0.680 0.562Vegas 0.485 0.502 0.277 0.613 0.469Average 0.465 0.552 0.471 0.651 0.535Table 6: Performance of Estimating Overall Fea-ture Rating for All reviewsCity V R S C AVGBoston 0.809 0.791 0.681 0.642 0.731Sydney 0.433 0.886 0.588 0.593 0.625Vegas 0.652 0.733 0.502 0.942 0.707Average 0.631 0.803 0.590 0.726 0.688Suppose there are m hotels.
For each hotelj, we first select the top 20% comprehensive re-views on each feature using our review selectionapproach.
We average the real ratings of one fea-ture provide by travelers for these reviews, de-noted as f?j .
We then estimate the feature rat-ings for these comprehensive reviews using theadopted machine learning method of Pang andLee.
The average of these estimated ratings isdenoted as g?j .
Similar to Equation 4, the av-erage difference between all f?j and g?j pairs oneach feature for hotels in each city are calculatedand listed in Table 5.
From this table, we cansee that the average difference between the es-timated average feature rating and real averagefeature rating is only about 0.53.
Our reviewselection approach produces fairly good perfor-mance for estimating an overall feature rating fora hotel.
We then also calculate the average dif-ference for all reviews.
The results are listed inTable 6.
We can see that the average differenceis larger (about 0.69) in this case.
The perfor-mance of estimating an overall feature rating isincreased by nearly 23.2% through our reviewselection approach.6 ConclusionIn this paper, we presented a novel review selec-tion approach to improve the accuracy of featurerating estimation.
We select reviews that com-prehensively talk about a feature of one service,using information distance of reviews on the fea-ture based on Kolmogorov complexity.
As eval-uated using real data, the rating estimation forthe feature from these reviews provides more ac-curate results than that for other reviews, inde-pendent of which classifiers are used.
The aver-age of these estimated feature ratings also betterrepresents an accurate overall rating for the fea-ture of the service.In future work, we will further improve the ac-curacy of estimating a general rating for a featureof a service based on the selected comprehensivereviews on this feature using our review selec-tion approach.
Comprehensive reviews may con-tribute differently to the estimation of an overallfeature rating.
In our next step, a more sophisti-cated model will be developed to assign differentweights to these different reviews.773ReferencesBennett, C.H., P Gacs, M Li, P.M.B.
Vita?nyi, andW.H.
Zurek.
1998.
Information distance.
IEEETransactions on Information Theory, 44(4):1407?1423, July.Cilibrasi, Rudi L. and Paul M.B.
Vita?nyi.
2007.The google similarity distance.
IEEE Transactionson Knowledge and Data Engineering, 19(3):370?383, March.de Marneffe, Marie Catherine, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.In The fifth international conference on LanguageResources and Evaluation (LREC), May.Hu, Minqing and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In 10th ACM Inter-national Conference on Knowledge Discovery andData Mining, pages 168?177.Li, M. and P. Vita?nyi.
1997.
An Introductionto Kolmogorov Complexity and its Applications.Springer-Verlag.Long, Chong, Xiaoyan Zhu, Ming Li, and Bin Ma.2008.
Information shared by many objects.
InACM 17th Conference on Information and Knowl-edge Management.Mullen, Tony and Nigel Collier.
2004.
Sentimentanalysis using support vector machines with di-verse information sources.
In Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 412?418, July.Pang, Bo and Lillian Lee.
2004.
A sentimental edu-cation: Sentiment analysis using subjectivity sum-marization based on minimum cuts.
In AnnualMeeting of the Association of Computational Lin-guistics (ACL), pages 271?278, July.Pang, Bo and Lillian Lee.
2005.
Seeing stars: Ex-ploiting class relationships for sentiment catego-rization with respect to rating scales.
In AnnualMeeting of the Association of Computational Lin-guistics (ACL), pages 115?124, June.Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?
sentimentclassification using machine learning techniques.In Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 79?86,July.Russell, S. and P. Norvig.
2002.
Artificial Intel-ligence: A Modern Approach.
Second Edition,Prentice Hall, Englewood Cliffs, New Jersey.774
