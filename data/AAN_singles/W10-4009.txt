Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 52?60,Beijing, August 2010Ontology driven content extraction using interlingual annotation oftexts in the OMNIA projectAchille Falaise, David Rouquet, Didier Schwab, Herve?
Blanchon, Christian BoitetLIG-GETALP, University of Grenoble{Firstname}.
{Lastname}@imag.frAbstractOMNIA is an on-going project that aimsto retrieve images accompanied withmultilingual texts.
In this paper, we pro-pose a generic method (language and do-main independent) to extract conceptualinformation from such texts and sponta-neous user requests.
First, texts are la-belled with interlingual annotation, thena generic extractor taking a domain on-tology as a parameter extract relevantconceptual information.
Implementationis also presented with a first experimentand preliminary results.1 IntroductionThe OMNIA project (Luca Marchesotti et al,2010) aims to retrieve images that are describedwith multilingual free companion texts (cap-tions, comments, etc.)
in large Web datasets.Images are first classified with formal descrip-tors in a lightweight ontology using automatictextual and visual analysis.
Then, users may ex-press spontaneous queries in their mother tongueto retrieve images.
In order to build both formaldescriptors and queries for the ontology, a con-tent extraction in multilingual texts is required.Multilingual content extraction does not im-ply translation.
It has been shown in (Daoud,2006) that annotating words or chunks with in-terlingual lexemes is a valid approach to initiatea content extraction.
We thus skip syntacticalanalysis, an expensive and low quality process,and get language-independent data early in ourflow, allowing further treatments to be language-independent.
We use the lightweight ontologyfor image classifications as the formal knowl-edge representation tha determines relevant in-formation to extract.
This ontology is consideredas a domain parameter for the content extractor.We are testing this method on a database pro-vided for the image retrieval challenge CLEF09by the Belgium press agency Belga.
Thedatabase contains 500K images with free com-panion texts of about 50 words (about 25Mwords in total).
The texts in the database are inEnglish only, and we ?simulate?
multilinguismwith partially post-edited machine translation.The rest of the paper is organized as fol-low.
We first depict our general architecture de-ployed for CLIA and then detail the various pro-cesses involved : interlingual annotation, con-ceptual vector based disambiguation and ontol-ogy driven content extraction.
We concludewith the first results of experimentations on theCLEF09 data.2 General architecture2.1 General processIn our scenario, there are two types of tex-tual data to deal with : companion texts in thedatabase (captions), but also user requests.
Thetwo are processed in a very similar way.The general architecture is depicted in figure1.
The main components, that will be describedin detail, may be summarized as follows:?
Texts (both companions and requests) arefirst lemmatised with a language-dependentpiece of software.
Ambiguities are pre-served in a Q-graph structure presented insection 3.1.2.52ConceptsQ-Graph Conceptextraction Lemmatisation DisambCompaniontextsNL RequestsNL-UWdictionnary UW-ConceptMap OntologyInterlingualannotationFigure 1: General architecture of CLIA in the OMNIA project?
Then, the lemmatised texts are annotatedwith interlingual (ideally unambiguous)lexemes, namely Universal Words (UW)presented in section 3.1.1.
This adds a lotof ambiguities to the structure, as an ac-tual lemma may refer to several semanti-cally different lexemes.?
The possible meanings for lemmas are thenweighted in the Q-graph through a disam-biguation process.?
Finally, relevant conceptual information isextracted using an alignment between a do-main ontology and the interlingual lexemes.The conceptual information in the output mayadopt different shapes, such as a weighted con-ceptual vector, statements in the A-Box of theontology or annotations in the original text, etc.In the case of OMNIA, conceptual informa-tion extracted from companion texts is storedin a database, while conceptual information ex-tracted from users requests are transformed intoformal requests for the database (such as SQL,SPARQL, etc.
).2.2 ImplementationThe general process is implemented following aService Oriented Architecture (SOA).
Each partof the process corresponds to a service.This allowed us to reuse part of existing re-sources developed on heterogeneous platformsusing web interfaces (in the best case REST in-terfaces (Fielding, 2000), but frequently onlyHTML form-based interfaces).
A service su-pervisor has been built to deal with such anheterogeneity and address normalization issues(e.g.
line-breaks, encoding, identification, cook-ies, page forwarding, etc.
).This architecture is able to process multipletasks concurrently, allowing to deal with usersrequests in real time while processing compan-ion texts in the background.3 Interlingual annotationWe present in this section the preliminary treat-ments of multilingual texts (image companiontexts or user requests) that are required forour content extraction process (Rouquet andNguyen, 2009a).In order to allow a content extraction in multi-lingual texts, we propose to represent texts withthe internal formalism of the Q-Systems andto annotate chunks with UNL interlingual lex-emes (UW) .
Roughly, we are making an inter-lingual lemmatisation, containing more informa-tion than simple tagging, that is not currentlyproposed by any lemmatisation software.3.1 Resources and data structures3.1.1 The Universal Network LanguageUNL (Boitet et al, 2009; Uchida Hiroshi etal., 2009) is a pivot language that represents themeaning of a sentence with a semantic abstractstructure (an hyper-graph) of an equivalent En-glish sentence.The vocabulary of UNL consists in a set ofUniversal Words (UW).
An UW consists of:1. a headword, if possible derived from En-glish, that can be a word, initials, an expres-sion or even an entire sentence.
It is a labelfor the concepts it represents in its originallanguage ;2. a list of restrictions that aims to preciselyspecify the concept the UW refers to.
Re-strictions are semantic relations with other53UW.
The most used is the ?icl?
relation thatpoints to a more general UW.Examples :?
book(icl>do, agt>human, obj>thing)and book(icl>thing).Here, the sense of the headword is focusedby the attributes.?
ikebana(icl>flower arrangement).Here, the headword comes from Japanese.?
go down.Here, the headword does not need any re-finement.Ideally, an UW refers unambiguously to a con-cept, shared among several languages.
However,UW are designed to represent acceptions in alanguage ; we therefore find distinct UW thatrefer to the same concept as for ?affection?
and?disease?.We are mainly using the 207k UW built by theU++ Consortium (Jesus Carden?osa et al, 2009)from the synsets of the Princeton WordNet, thatare linked to natural languages via bilingual dic-tionaries.
The storage of these dictionaries canbe supported by a suitable platform like PIVAX(Nguyen et al, 2007) or a dedicated database.The gain of a pivot language is illustrated in fig-ure 2.
If we want to add a new language in themultilingual system, we just need to create thelinks with the pivot but not with all the other lan-guages.3.1.2 The Q-SystemsWe can think of inserting the UW annotationswith tags (e.g.
XML) directly along the sourcetext as in table 1.
However, this naive approach isnot adequate to represent the segmentation am-biguities that can occur in the text interpretation(in the example of table 1, we list the differentpossible meanings for ?in?, but cannot represent?waiting?, ?room?
and ?waiting room?
as threepossible lexical units).In order to allow the representation of segmen-tation and other ambiguities, that can occur ina text interpretation, we propose to use the Q-Systems.
They represent texts in an adequateInterlingualUW volumeFrenchvolumeEnglishvolumeChinesevolumeFigure 2: Multilingual architecture with a pivotin a waiting room<tag uw=?in(icl-sup-how),in(icl-sup-adj),in(icl-sup-linear unit,equ-sup-inch)?>in</tag><tag uw=?unk?>a</tag> <taguw=?waiting room(icl-sup-room,equ-sup-lounge)?>waitingroom</tag>Table 1: Naive annotation of a text fragmentgraph structure decorated with bracketed expres-sions (trees) and, moreover, allow processing onthis structure via graph rewriting rules (a set ofsuch rewriting rules is a so called Q-System).An example of the Q-System formalism isgiven in figure 3 of section 3.2.3.
It presentssuccessively : the textual input representing a Q-graph, a rewriting rule and a graphical view ofthe Q-graph obtained after the application of therule (and others).The Q-Systems were proposed by AlainColmeraurer at Montreal University (Colmer-auer, 1970).
For our goal, they have three mainadvantages :?
they provide the formalized internal struc-ture for linguistic portability that we men-tioned in the introduction (Hajlaoui andBoitet, 2007) ;?
they unify text processing with powerfulgraph rewriting systems ;54?
they allow the creation or the edition ofa process by non-programmers (e.g.
lin-guists) using SLLP (Specialized Languagefor Linguistic Programming).We are actually using a reimplementation ofthe Q-Systems made in 2007 by Hong-ThaiNguyen during his PhD in the LIG-GETALPteam (Nguyen, 2009).3.2 Framework of the annotation process3.2.1 OverviewThe annotation process is composed by thefollowing steps :1. splitting the text in fragments if too long ;2. lemmatisation with a specialized software ;3. transcription to the Q-Systems format ;4. creation of local bilingual dictionaries(source language - UW) for each fragmentwith PIVAX ;5. execution of those dictionaries on the frag-ments ;3.2.2 LemmatisationAs we want to use dictionaries where entriesare lemmas, the first step is to lemmatise the in-put text (i.e.
to annotate occurrences with possi-ble lemmas).
This step is very important becauseit although gives the possible segmentations ofthe text in lexical units.
It brings two kinds ofambiguities into play : on one hand, an occur-rence can be interpreted as different lemmas, onthe other, there can be several possible segmen-tations (eventually overlapping) to determine thelexical units.For content extraction or information retrievalpurpose, it is better to preserve an ambiguity thanto badly resolve it.
Therefore we expect from alemmatiser to keep all ambiguities and to repre-sent them in a confusion network (a simple tag-ger is not suitable).
Several lemmatiser can beused to cover different languages.
For each ofthem, we propose to use a dedicated ANTLRgrammar (Terence Parr et al, 2009) in order tosoundly transform the output in a Q-graph.To process the Belga corpus, we developed alemmatiser that produce natively Q-graphs.
Itis based on the morphologic dictionary DELA1available under LGPL licence.3.2.3 Local dictionaries as Q-SystemsHaving the input text annotated with lemmas,with the Q-System formalism, we want to use thegraph rewriting possibilities to annotate it withUW.
To do so, we use PIVAX export features toproduce rules that rewrite a lemma in an UW (seefigure 3).
Each rule correspond to an entry inthe bilingual dictionary.
To obtain a tractable Q-Systems (sets of rules), we built local dictionar-ies that contain the entries for fragments of thetext (about 250 words in the first experiment).Figure 3: Creation and execution of a Q-SystemConsidering the significant quantity of ambi-guities generated by this approach (up to a dozenUW for a single word), we need to include adisambiguation process.
This process, based onconceptual vectors, is presented in the next sec-tion.4 Conceptual vector baseddisambiguationVectors have been used in NLP for over 40 years.For information retrieval, the standard vectormodel (SVM) was invented by Salton (Salton,1991) during the late 60?s, while for meaningrepresentation, latent semantic analysis (LSA)1http://infolingu.univ-mlv.fr/DonneesLinguistiques/Dictionnaires/telechargement.html55was developed during the late 80?s (Deerwesteret al, 1990).
These approaches are inspiredby distributional semantics (Harris et al, 1989)which hypothesises that a word meaning can bedefined by its co-text.
For example, the mean-ing of ?milk?
could be described by {?cow?, ?cat?,?white?, ?cheese?, ?mammal?, .
.
.
}.
Hence, distribu-tional vector elements correspond directly (forSVM) or indirectly (for LSA) to lexical itemsfrom utterances.The conceptual vector model is different as itis inspired by componential linguistics (Hjelm-lev, 1968) which holds that the meaning of wordscan be described with semantic components.These can be considered as atoms of meaning(known as primitives (Wierzbicka, 1996)), oralso only as constituents of the meaning (knownas semes, features (Greimas, 1984), concepts,ideas).
For example, the meaning of ?milk?could be described by {LIQUID, DAIRY PRODUCT, WHITE,FOOD, .
.
.}.
Conceptual vectors model a formal-ism for the projection of this notion in a vectorialspace.
Hence, conceptual vector elements corre-spond to concepts indirectly, as we will see later.For textual purposes2, conceptual vectors canbe associated to all levels of a text (word, phrase,sentence, paragraph, whole texts, etc.).
As theyrepresent ideas, they correspond to the notion ofsemantic field3 at the lexical level, and to theoverall thematic aspects at the level of the entiretext.Conceptual vectors can also be applied tolexical meanings.
They have been studied inword sense disambiguation (WSD) using iso-topic properties in a text, i.e.
redundancy of ideas(Greimas, 1984).
The basic idea is to maximisethe overlap of shared ideas between senses oflexical items.
This can be done by computing theangular distance between two conceptual vectors(Schwab and Lafourcade, 2007).In our case, conceptual vectors are used forautomatic disambiguation of texts.
Using thismethod, we calculate confidence score for eachUW hypothesis appearing in the Q-Graph.2Conceptual vectors can be associated with any content,not only text: images, videos, multimedia, Web pages, etc.3The semantic field is the set of ideas conveyed by aterm.5 Ontology driven content extractionThe content extraction has to be leaded by a?knowledge base?
containing the informationswe want to retrieve.5.1 Previous works in content extractionThis approach has its roots in machine trans-lation projects such as C-Star II (1993-1999)(Blanchon and Boitet, 2000) and Nespole!
(2000-2002) (Metze et al, 2002), for on the flytranslation of oral speech acts in the domain oftourism.
In these projects, semantic transfer wasachieved through an IF (Inter-exchange Format),that is a semantic pivot dedicated to the domain.This IF allows to store information extractedfrom texts but is although used to lead the con-tent extraction process by giving a formal repre-sentation of the relevant informations to extract,according to the domain.The Nespole!
IF consists of 123 conceptsfrom the tourism domain, associated with sev-eral arguments and associable with speech actsmarkers.
The extraction process is based on pat-terns.
As an example, the statement ?I wish asingle room from September 10th to 15th?
maybe represented as follows:{ c:give-information+disposition+room( disposition=(desire, who=i),room-spec=( identifiability=no,single_room ),time=( start-time=(md=10),end-time(md=15, month=9)))}5.2 Ontologies as parameter for the domainIn the project OMNIA, the knowledge base hasthe form of a lightweight ontology for imageclassification 4.
This ontology contains 732 con-cepts in the following domains : animals, pol-itics, religion, army, sports, monuments, trans-ports, games, entertainment, emotions, etc.
Tous, using an ontology has the following advan-tages :?
Ontologies give an axiomatic descriptionof a domain, based on formal logics (usu-4http://kaiko.getalp.org/kaiko/ontology/OMNIA/OMNIA current.owl56ally description logics (Baader et al, 2003))with an explicit semantic.
Thus, the knowl-edge stored in them can be used soundly bysoftware agents;?
Ontological structures are close to the or-ganisation of ideas as semantic networks inhuman mind (Aitchenson, 2003) and are la-beled with strings derived from natural lan-guages.
Thus humans can use them (brows-ing or contributing) in a pretty natural way;?
Finally, with the advent of the SemanticWeb and normative initiatives such as theW3C5, ontologies come with a lot of sharedtools for editing, querying, merging, etc.As the content extractor might only processUW annotations, it is necessary that the knowl-edge base is whether expressed using UW orlinked to UW.
The ontology is here consideredas a domain parameter of content extractionand can be changed to improve preformanceson specific data collections.
Therefore, givenany OWL ontology6, we must be able to link itwith a volume of UW considering the followingconstraints :Creating manually such correspondencesis costly due to the size of resources so anautomatic process is requiered.Ontologies and lexicons evolve over the timeso an alignment must be adaptable to incremen-tal evolutions of resources.The correspondences must be easily manip-ulated by users so they can manually improvethe quality of automatically created alignmentswith post-edition.Constructing and maintaining an alignmentbetween an ontology and an UW lexicon is achallenging task (Rouquet and Nguyen, 2009b).Basically, any lexical resource can be repre-sented in an ontology language as a graph.
Wepropose to use an OWL version of the UW vol-ume available on Kaiko website 7.
It allows us5http://www.w3.org/6http://www.w3.org/2004/OWL/7http://kaiko.getalp.orgto benefit of classical ontology matching tech-niques and tools (Euzenat and Shvaiko, 2007)to represent, compute and manipulate the align-ment.
We implemented two string based match-ing techniques on top of the alignment API (Eu-zenat, 2004).
Specific disambiguation methodsare in development to improve the alignmentprecision.
Some of them are based on conceptualvectors presented in section 4, others will adaptstructural ontology matching techniques.
Thisapproach to match an ontology with a lexical re-source is detailled in (Rouquet et al, 2010).5.3 The generic extractorIn the case of the OMNIA project, the systemoutput format is constraint by the goal of an in-tegration with visual analysis results, in a largermultimodal system.
The visual analysis systemsare also based on concept extraction, but does notneed an ontology to organise concepts.
There-fore, our results has to remain autonaumous,which means without references to the ontologyused to extract concepts.
So, we use a simpleconcept vector as output, with intensity weights;practically, a simple data-value pairs sequenceformatted in XML.Concept extraction is achieved through a 3steps process, has shown in figure 4.1.
Concept matching: each UW in the Q-Graph, that matches a concept according tothe UW-concept map, is labelled with thisconcept.2.
Confidence calculation: each concept la-bel is given a confidence score, in accor-dance with the score of the UW carrying theconcept, obtained after disambiguation, andpondered according to the number of UWsin the Q-Graph.
It is planed to take into ac-count a few linguistics hints here, such asnegations, and intensity adverbs.3.
Score propagation: because we need au-tonomous results, we have to perform allontology-based calculation before releasingthem.
The confidence scores are propagatedin the ontology concept hierarchy: for each57labelled concept, its score is added to thesuper-concept, and so on.The ontology and the derivated UW-conceptmap are considered as parameters for the treat-ments, and may be replaced in accordance withthe domain, and the relevance of the conceptsand their hierarchy, according to the task.ConceptnstsQ-sG-QreonConrtnrtarhQsreons xiQrs enLmsohtxhoxQLQreonDbNhQxConstxrRqubConstxrUQxWnro-oLdFigure 4: Detail of concept extraction.6 ExperimentsFor a first experiment, we used a small dataset,containing:?
a sub-corpus of 1046 English companiontexts from CLEF09 corpus (press picturesand captions of about 50 words),?
a 159 concepts ontology, designed for pic-ture and emotions depiction,?
a UW-concept map comprising 3099 UW.It appeared that, with this parameters, con-cepts where extracted for only 25% of the texts.This preliminary result stressed the importanceof recall for such short texts.
However, therewere many ways to improve recall in the system:?
improve the ontology, in order to bettercover the press domain;?
significantly increase the quantity of UWlinked to concepts (only 3099 obtained forthis experiment), by considering synonymsduring the linking process;?
using UW restrictions during conceptmatching for UW that are not directlylinked to a concept, as these restrictions area rich source of refined semantic informa-tion.A second experiment with an improved on-tology, including 732 concepts, and the use ofUW restrictions, showed very promising results.Concepts were retrieved from 77% of texts.
Theremaining texts were very short (less than 10words, sometime just date or name).For example, we extracted the following con-cepts from the picture and companion text repro-duced in figure 5.CoCncepetnntnstQepe-CGraCahexiC eLexmDbNeRquUWedNyeMWOUmeDelqmymDNyeqgexmDbNeImUdNOUWye?OODuerMddUNWeNWeDeRDNyNW?mqqueNWe-D?ODO?deD??WdMme?dlNyD?tQe?lyUu?metnnt??OODuerMddUNWeNdeOq?UO?elMmdMNW?ey?UeOU??qluUWyeqgeRUDlqWdeqgeuDddeOUdymM?NqWeDWOeRN?eOqe?de?Udyeyqe?NOUey?uegmque?eNWdlU?qmdhey?e-mNyNd???mWuUWye?DNuUOeNWeDeccplD?eOqddNUmeuDOUelM????dye?qMmde?UgqmUeDedlU?D?erqMdUeqge?quuqWdeOU?DyUeqWexmDb?xmDbNe?M?MmUe?NWNdyUmerDuDOe?MddUgerDuuDONe???Oey?e-mNyNd?D?U?yNqWde??DdU?dd??eeeee?CeIr?
?C?xeCoCaeCoCaFigure 5: Picture document and companion textexample.CONCEPT WEIGHTBUILDING 0.098HOSPITAL 0.005HOUSE 0.043MINISTER 0.016OTHER BUILDING 0.005PEOPLE 0.142PERSON 0.038POLITICS 0.032PRESIDENT 0.016RESIDENTIAL BUILDING 0.043WOMAN 0.005As this results were more consistent, we couldhave a preliminary survey about precision, on a30 texts sample.
While disambiguation imple-mentation is still at an early stage, weights werenot yet taken into account.
A concept match canbe considered correct following two criterons :1.
Visual relevance considers a concept ascorrect if carried by an element of the pic-ture; for instance, the match of concept58?SPORT?
is regarded as correct for a pic-ture containing a minister of sports, even ifnot actually performing any sport.2.
Textual relevance considers a concept ascorrect if carried by a word of the text,as parts of texts may involve concepts thatare not actually present in the picture, suchas contextual information, previous events,etc.124 concepts were found in 23 texts (7 texts hadno concept match):1.
99 concepts were correct according to thevisual relevance,2.
110 were correct according to the textualrelevance,3.
14 were totally incorrect.We thus have an overall precision score of 0.798according to the visual relevance and 0.895 ac-cording to the textual relevance.
Most of the er-rors where caused by ambiguity problems, andmay be addressed with disambiguation processthat are not fully implemented yet.7 Conclusion and perspectivesWe exposed a generic system designed to extractcontent (in the form of concepts) from multi-lingual texts.
Our content extraction process isgeneric regarding to two aspects :?
it is language independent, as it process aninterlingual representation of the texts?
the content to be extracted can be specifiedusing a domain ontology as a parameterThis is an ongoing work, and disambiguationthrough conceptual vectors is expected to im-prove accuracy, giving significant weights to thehypothetical meanings of words.In the long run, we will focus on integrationwith visual content extractors, speed optimiza-tion to achieve a real-time demonstrator and de-tailled evaluation of the method.ReferencesAitchenson, J.
2003.
Words in the Mind.
An Intro-duction to the Mental Lexicon.
Blackwell Publish-ers.Baader, De Franz, Diego Calvanese, DeborahMcGuinness, Peter Patel-Schneider, and DanieleNardi.
2003.
The Description Logic Handbook.Cambridge University Press.Blanchon, H. and C. Boitet.
2000.
Speech translationfor french within the C-STAR II consortium andfuture perspectives.
In Proc.
ICSLP 2000, pages412?417, Beijing, China.Boitet, Christian, Igor Boguslavskij, and JesusCarden?osa.
2009.
An evaluation of UNL usabil-ity for high quality multilingualization and projec-tions for a future UNL++ language.
In Computa-tional Linguistics and Intelligent Text Processing,pages 361?373.Colmerauer, A.
1970.
Les syste`mes-q ou un for-malisme pour analyser et synthe?tiser des phrasessur ordinateur.
de?partement d?informatique del?Universite?
de Montre?al, publication interne, 43,September.Daoud, Daoud.
2006.
Il faut et on peut constru-ire des syste`mes de commerce e?lectronique a` inter-face en langue naturelle restreints (et multilingues)en utilisant des me?thodes oriente?es vers les sous-langages et le contenu.
Ph.D. thesis, UJF, Septem-ber.Deerwester, Scott C., Susan T. Dumais, Thomas K.Landauer, George W. Furnas, and Richard A.Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society of In-formation Science, 41(6).Euzenat, Je?ro?me and Pavel Shvaiko.
2007.
Ontologymatching.
Springer, Heidelberg (DE).Euzenat, Je?ro?me.
2004.
An API for ontology align-ment.
In Proceedings of the 3rd InternationalSemantic Web Conference, pages 698?7112, Hi-roshima, Japan.Fielding, Roy T. 2000.
Architectural styles and thedesign of network-based software architectures.Ph.D.
thesis, University of California.Greimas, Algirdas Julien.
1984.
Structural Seman-tics: An Attempt at a Method.
University of Ne-braska Press.Hajlaoui, Najeh and Christian Boitet.
2007.
Portagelinguistique d?applications de gestion de contenu.In TOTh07, Annecy.59Harris, Zellig S., Michael Gottfried, Thomas Ryck-man, Paul Mattick Jr., Anne Daladier, T.N.
Har-ris, and S. Harris.
1989.
The form of Informationin Science, Analysis of Immunology Sublanguage,volume 104 of Boston Studies in the Philosophy ofScience.
Kluwer Academic Publisher, Dordrecht.Hjelmlev, Louis.
1968.
Prole?gole`me a` une the?oriedu langage.
e?ditions de minuit.Jesus Carden?osa et al 2009.
The U++ con-sortium (accessed on september 2009).http://www.unl.fi.upm.es/consorcio/index.php,September.Luca Marchesotti et al 2010.
The Omnia project(accessed on may 2010).
http://www.omnia-project.org, May.Max Silberztein.
2009.
NooJ linguisticsoftware (accessed on september 2009).http://www.nooj4nlp.net/pages/nooj.html,September.Metze, F., J. McDonough, H. Soltau, A. Waibel,A.
Lavie, S. Burger, C. Langley, L. Levin,T.
Schultz, F. Pianesi, R. Cattoni, G. Lazzari,N.
Mana, and E. Pianta.
2002.
The Nespole!speech-to-speech translation system.
In Proceed-ings of HLT-2002 Human Language TechnologyConference, San Diego, USA, march.Nguyen, H.T., C. Boitet, and G. Se?rasset.
2007.
PI-VAX, an online contributive lexical data base forheterogeneous MT systems using a lexical pivot.In SNLP, Bangkok, Thailand.Nguyen, Hong-Thai.
2009.
EMEU w,a simple interface to test the Q-Systems (accessed on september 2009).http://sway.imag.fr/unldeco/SystemsQ.po?localhost=/home/nguyenht/SYS-Q/MONITEUR/, Septem-ber.Rouquet, David and Hong-Thai Nguyen.
2009a.Interlingual annotation of texts in the OMNIAproject.
Poznan, Poland.Rouquet, David and Hong-Thai Nguyen.
2009b.Multilingu?
?sation d?une ontologie par des core-spondances avec un lexique pivot.
In TOTh09, An-necy, France, May.Rouquet, David, Cassia Trojahn, Didier Scwab, andGilles Se?rasset.
2010.
Building correspondencesbetween ontologies and lexical resources.
In to bepublished.Salton, Gerard.
1991.
The Smart document re-trieval project.
In Proc.
of the 14th Annual Int?lACM/SIGIR Conf.
on Research and Developmentin Information Retrieval, Chicago.Schwab, Didier and Mathieu Lafourcade.
2007.
Lex-ical functions for ants based semantic analysis.
InICAI?07- The 2007 International Conference onArtificial Intelligence, Las Vegas, Nevada, USA,juin.Terence Parr et al 2009.
ANTLR parsergenerator (accessed on september 2009).http://www.antlr.org/, September.Uchida Hiroshi et al 2009.
The UNDLfoundation (accessed on september 2009).http://www.undl.org/, September.Wierzbicka, Anna.
1996.
Semantics: Primes andUniversals.
Oxford University Press.60
