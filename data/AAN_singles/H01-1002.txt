Adapting an Example-Based Translation System toChineseYing Zhang, Ralf D. Brown, and Robert E. FrederkingLanguage Technologies Institute, Carnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213-3890 USAfjoy,ralf,refg@cs.cmu.eduABSTRACTWe describe an Example-Based Machine Translation (EBMT) sys-tem and the adaptations and enhancementsmade to create a Chinese-English translation system from the Hong Kong legal code and var-ious other bilingual resources available from the Linguistic DataConsortium (LDC).1.
BACKGROUNDWe describe an Example-Based Machine Translation (EBMT)system and the adaptations and enhancements made to create aChinese-English translation system from the Hong Kong legal codeand various other bilingual resources available from the LinguisticData Consortium (LDC).The EBMT software [1, 3] used for the experiments describedhere is a shallow system which can function using nothing morethan sentence-alignedplaintext and a bilingual dictionary; and givensufficient parallel text, the dictionary can be extracted statisticallyfrom the corpus [2].
To perform a translation, the program looks upall matching phrases in the source-language half of the parallel cor-pus and performs a word-level alignment on the entries containingmatches to determine a (usually partial) translation.
Portions of theinput for which there are no matches in the corpus do not generatea translation.Because the EBMT system does not generate translations for100% of the text it is given as input, a bilingual dictionary andphrasal glossary are used to fill any gaps.
Selection of a ?best?translation is guided by a trigram model of the target language [6].Supporting Chinese required a number of changes to the programand training procedures; those changes are discussed in the nextsection.2.
ENHANCEMENTSThe first change required of the translation software was sup-port for the two-byte encoding used for the Chinese text (GB-2312,?GB?
for short).
Further, the EBMT (as well as dictionary and glos-sary) approaches are word-based, but Chinese is ordinarily writ-ten without breaks between words.
Thus, Chinese input must be.segmented into individual words.
The initial baseline system usedthe segmenter made available by the LDC.
This segmenter uses aword-frequency list to make segmentation decisions, but althoughthe list provided by the LDC is large, it did not completely coverthe vocabulary of the EBMT training corpus (described below).
Asa result, many sentences had incorrect segmentations or includedlong sequences which were not segmented at all or were brokeninto single characters.
Almost every Chinese character has at leastone meaning, and its meaning may be entirely different from themeaning of the word containing it.
The mis-segmenting of Chinesewords due to the inadequate dictionary makes it very hard to builda statistical dictionary and properly index the EBMT corpus.To improve the performance of the Chinese segmenter, we aug-mented its word list by finding sequences of characters in the train-ing corpus that belong together, based on their frequency and highmutual information.
We developed a form of term extraction tofind English phrases which should be treated as atomic units fortranslation, thus increasing the average length of ?words?
in bothsource and target languages.
Finally, we also created an augmentedbilingual dictionary for use in word-level alignment for EBMT byapplying statistical dictionary extraction techniques to the trainingcorpus.As the improved segmenter and the term finder may be produc-ing excessively long phrases or phrases which are impossible tomatch in the other language, we repeat the procedure of segment-ing/bracketing/dictionary-building several times.
On each succes-sive iteration, the segmenter and bracketer are limited to words andphrases for which the statistical dictionary from the previous itera-tion contains translations.
Through this iteration, we increased thesize of the statistical dictionary from each step and guaranteed thatall Chinese words generated by the segmenter have translations inthe dictionary.
This helps ensure that the EBMT engine can per-form word-level alignments.3.
EXPERIMENTAL DESIGNThe primary purpose of this experiment was to determine theeffect of each enhancement by operating with various subsets ofthe enhancements.
Since it rapidly becomes impractical to test allpossible combinations, we opted for the following test conditions:1. baseline: parallel corpus segmented with the LDC segmenterand LDC dictionary/glossary2.
baseline plus improved segmenter3.
baseline plus improved segmenter and term finder4.
baseline plus improved segmenter and statistical dictionary5.
baseline plus improved segmenter, term finder, and statisticaldictionaryFor training, we had available two parallel Chinese-English cor-pora distributed by the LDC: the complete Hong Kong legal code(after cleaning: 47.86 megabytes, 5.5 million English words, 9 mil-lion Chinese characters) where 85% of the content (by sentence) isunique, and a collection of Hong Kong news articles (after clean-ing: 24.58 megabytes, 2.67 million English words, 4.5 million Chi-nese characters).
In addition, LDC distributes a bilingual dictio-nary/phrasebook, which we also used.To determine the effects of varying amounts of training data onoverall performance, we divided the bilingual training corpus intoten nearly equal slices.
Each test condition was then run ten times,each time increasing the number of slices used for training the sys-tem.
After each training pass, the test sentences were translated andthe system?s performance evaluated automatically; selected pointswere then manually evaluated for translation quality.The automatic performance evaluation measured coverage of theinput and average phrase length.
Coverage is the percentage of theinput text for which a translation is produced by a particular trans-lation method (since the EBMT engine does not generally producehypotheses that cover every word of input), while average phraselength is a crude indication of translation quality ?
the longer thephrase that is translated, the more context is incorporated and theless likely it is that the wrong sense will be used in the translation orthat (for EBMT) the alignment will be incorrect.
Since the dictio-nary and glossary remain constant for a given test condition, onlythe EBMT coverage will be presented.Manual grading of the output was performed using a web-basedsystem with which the graders could assign one of three scores(?Good?, ?OK?, ?Bad?)
in each of two dimensions: grammaticalcorrectness and meaning preservation.
This type of quality scoringis commonly used in assessing translation quality, and is used byother TIDES participants.
Fifty-two test sentences were translatedfor each of four points from the automated evaluation and these setsof four alternatives presented to the graders.
The four points chosenwere the baseline system with 100% of the training corpus, the fullsystem with 20% and 100% training, and the full system trained ona corpus of Hong Kong news text (cross-domain); only four pointswere selected due to the difficulty and expense of obtaining largenumbers of manual quality judgements.To assess the performance of the system in a different domain,as well as the effect of the trigram language model on the selec-tion of translated fragments for the final translation, we obtainedmanual judgements for 44 sentences on an additional four test con-ditions, each trained with the entire available parallel text and testedon Hong Kong news text rather than legal sentences.
These pointswere the cross-domain case (trained on the legal corpus) and threedifferent language models for within-domain training: an Englishlanguage model derived from the legal corpus, one derived fromthe news corpus, and a pre-existing model generated from two gi-gabytes of newswire and broadcast news transcriptions.4.
RESULTSWe discovered that there is a certain amount of synergy betweensome of the improvements, particularly the term finder and statis-tical dictionary extraction.
Applying the term finder modifies theparallel corpus in such a way that it becomes more difficult forthe EBMT engine to find matches which it can align, while addingdictionary entries derived from the modified corpus eliminates thateffect.
As a result, we will not present the performance results forTest Condition 3 (improved segmenter plus term finder); further,the data for Test Conditions 2 (improved segmenter only) and 4(improved segmenter plus statistical dictionary) may not accuratelyreflect the contribution of those two components to the full systemTranslating Legal CodeSystem Baseline Full Full X-DomTraining 100% 20% 100% 100%Syntactic 42.31% 54.81% 61.06% 39.42%Semantic 43.75% 61.54% 64.42% 34.62%Translating Hong Kong NewsTraining News News News LegalLangModel Legal News Prior LegalSyntactic 45.67% 44.71% 47.60% 34.62%Semantic 50.00% 50.96% 51.92% 47.12%Figure 1: Judgements ?
Acceptable Translationsused for Test Condition 5.Figure 2 shows the proportion of the words in the test sentencesfor which the EBMT engine was able to produce a translation,while Figure 3 shows the average number of source-languagewordsper translated fragment.
These curves do not increase monotoni-cally because, for performance reasons, the EBMT engine does notattempt to align every occurrence of a phrase, only theN (currently12) most-recently added ones; as a result, adding more text to thecorpus can cause EBMT to ignore matches that successfully alignin favor of newer occurrences which it is unable to align.Examining Figure 3, it is clear that the fifth slice (from 40 to50%) is much more like the test data than other slices, resulting inlonger matches.
In general, the closer training and test text are toeach other, the longer the phrases they have in common.Figure 1 summarizes the results of human quality assessments.The ?Good?
and ?OK?
judgements were combined into ?Accept-able?
and the the percentage of ?Acceptable?
judgements was aver-aged across sentences and graders.
As hoped and expected, the im-provements do in fact result not only in better coverage by EBMT,but also in better quality assessments by the human graders.
Fur-ther, the results on Hong Kong news text show that the choice oflanguage model does have a definite effect on quality.
These resultsalso confirm the adage that there is no such thing as too much train-ing text for language modeling, since the model generated from theEBMT corpus was unable to match the performance of the pre-existing model generated from two orders of magnitude more text.5.
CONCLUSIONS AND FUTURE WORKAs seen in Figure 2, the enhancements described here cumula-tively provide a 12% absolute improvement in coverage for EBMTtranslations without requiring any additional knowledge resources.Further, the enhanced coverage does, in fact, result in improvedtranslations, as verified by human judgements.
We can also con-clude that when we combine words into larger chunks on both sidesof the corpus, the possibility of finding larger matches between thesource language and the target language increases, which leads tothe improvement of the translation quality for EBMT.We will do further research on the interaction between the im-proved segmenter, term finder and statistical dictionary builder, uti-lizing the information provided by the statistical dictionary as feed-back for the segmenter and term finder to modify their results.
Weare also investigating the effects of splitting the EBMT training intomultiple sets of topic-specific sentences, automatically separatedusing clustering techniques.The relatively low slope of the coverage curve also indicates thatthe training corpus is sufficiently large.
Our prior experience withSpanish (using the UN Multilingual Corpus [5]) and French (using00.050.10.150.20.250.30.350.40.450.50.550.60.650.70.750.80 10 20 30 40 50 60 70 80 90 100Coverageof EBMTPercentage of corpus used for training (%)Improved Segmenter, term finder, statDictImproved Segmenter, statDictImproved SegmenterBaseline systemTrained on News tested On LegalcodeFigure 2: EBMT Coverage with Varying Trainingthe Hansard corpus [7]) was that the curve flattens out at betweentwo and three million words of training text, which appears also tobe the case for Chinese (each training slice contains approximatelyone million words of total text).We have not yet taken full advantage of the features of the EBMTsoftware.
In particular, it supports equivalence classes that permitgeneralization of the training text into templates for improved cov-erage.
We intend to test automatic creation of equivalence classesfrom the training corpus [4] in conjunction with the other improve-ments reported herein.6.
ACKNOWLEDGEMENTSWe would like to thank Alon Lavie and Lori Levin for their com-ments on drafts of this paper.7.
REFERENCES[1] R. D. Brown.
Example-Based Machine Translation in thePANGLOSS System.
In Proceedings of the SixteenthInternational Conference on Computational Linguistics, pages169?174, Copenhagen, Denmark, 1996.http://www.cs.cmu.edu/?ralf/papers.html.
[2] R. D. Brown.
Automated Dictionary Extraction for?Knowledge-Free?
Example-Based Translation.
InProceedings of the Seventh International Conference onTheoretical and Methodological Issues in MachineTranslation (TMI-97), pages 111?118, Santa Fe, New Mexico,July 1997.http://www.cs.cmu.edu/?ralf/papers.html.
[3] R. D. Brown.
Adding Linguistic Knowledge to a LexicalExample-Based Translation System.
In Proceedings of theEighth International Conference on Theoretical andMethodological Issues in Machine Translation (TMI-99),pages 22?32, Chester, England, August 1999.http://www.cs.cmu.edu/?ralf/papers.html.
[4] R. D. Brown.
Automated Generalization of TranslationExamples.
In Proceedings of the Eighteenth InternationalConference on Computational Linguistics (COLING-2000),pages 125?131, 2000.
[5] D. Graff and R. Finch.
Multilingual Text Resources at theLinguistic Data Consortium.
In Proceedings of the 1994 ARPAHuman Language Technology Workshop.
Morgan Kaufmann,1994.00.511.522.533.544.555.566.577.50 10 20 30 40 50 60 70 80 90 100Averagephraselength(words)Percentage of corpus used for training (%)Improved Segmenter, term finder, statDictImproved Segmenter, statDictImproved SegmenterBaseline systemTrained on News tested On LegalcodeFigure 3: Average EBMT Match Lengths[6] C. Hogan and R. E. Frederking.
An Evaluation of theMulti-engine MT Architecture.
In Machine Translation andthe Information Soup: Proceedings of the Third Conference ofthe Association for Machine Translation in the Americas(AMTA ?98), volume 1529 of Lecture Notes in ArtificialIntelligence, pages 113?123.
Springer-Verlag, Berlin, October1998.
[7] Linguistic Data Consortium.
Hansard Corpus of ParallelEnglish and French.
Linguistic Data Consortium, December1997.
http://www.ldc.upenn.edu/.
