Incremental Construction of MinimalAcyclic Finite-State AutomataJan Daciuk*Technical University of GdafiskBruce W. Watson  ~University of PretoriaS toyan  Mihov  tBulgarian Academy of SciencesR ichard  E. Watson~In this paper, we describe a new method for constructing minimal, deterministic, acyclic finite-state automata from a set of strings.
Traditional methods consist of two phases: the first to constructa trie, the second one to minimize it.
Our approach is to construct a minimal automaton in asingle phase by adding new strings one by one and minimizing the resulting automaton on-the-fly.
We present ageneral algorithm as well as a specialization that relies upon the lexicographicalordering of the input strings.
Our method is fast and significantly lowers memory requirementsin comparison to other methods.1.
IntroductionFinite-state automata are used in a variety of applications, including aspects of naturallanguage processing (NLP).
They may store sets of words, with or without annotationssuch as the corresponding pronunciation, base form, or morphological categories.
Themain reasons for using finite-state automata in the NLP domain are that their repre-sentation of the set of words is compact, and that looking up a string in a dictionaryrepresented by a finite-state automaton is very fast--proportional to the length of thestring.
Of particular interest o the NLP community are deterministic, acyclic, finite-state automata, which we call dictionaries.Dictionaries can be constructed in various ways--see Watson (1993a, 1995) for ataxonomy of (general) finite-state automata construction algorithms.
A word is simplya finite sequence of symbols over some alphabet and we do not associate it witha meaning in this paper.
A necessary and sufficient condition for any deterministicautomaton to be acyclic is that it recognizes a finite set of words.
The algorithmsdescribed here construct automata from such finite sets.The Myhill-Nerode theorem (see Hopcroft and Ullman \[1979\]) states that amongthe many deterministic automata that accept a given language, there is a unique au-tomaton (excluding isomorphisms) that has a minimal number of states.
This is calledthe minimal deterministic automaton of the language.The generalized algorithm presented in this paper has been independently devel-oped by Jan Daciuk of the Technical University of Gdafisk, and by Richard Watson?
Department of Applied Informatics, Technical University of Gdafisk, U1.
G. Narutowicza 11/12,PL80-952 Gdafisk, Poland.
E-mail: jandac@pg.gda.plLinguistic Modelling Laboratory, LPDP--Bulgarian Academy ofSciences, Bulgaria.
E-mail:stoyan@lml.bas.bg:~ Department of Computer Science, University of Pretoria, Pretoria 0002, South Africa.
E-mail:watson@cs.up.ac.za?
E-mail: watson@OpenFIRE.org(~) 2000 Association for Computational LinguisticsComputational Linguistics Volume 26, Number 1and Bruce Watson (then of the IST Technologies Research Group) at Ribbit SoftwareSystems Inc.
The specialized (to sorted input data) algorithm was independently de-veloped by Jan Daciuk and by Stoyan Mihov of the Bulgarian Academy of Sciences.Jan Daciuk has made his C++ implementations of the algorithms freely availablefor research purposes at www.pg.gda.pl/~jandac/fsa.html.
1 Stoyan Mihov has imple-mented the (sorted input) algorithm in a Java package for minimal acyclic finite-stateautomata.
This package forms the foundation of the Grammatical Web Server for Bul-garian (at origin2000.bas.bg) and implements operations on acyclic finite automata,such as union, intersection, and difference, as well as constructions for perfect hash-ing.
Commercial C++ and Java implementations are available via www.OpenFIRE.org.The commercial implementations include several additional features uch as a methodto remove words from the dictionary (while maintaining minimality).
The algorithmshave been used for constructing dictionaries and transducers for spell-checking, mor-phological analysis, two-level morphology, restoration of diacritics, perfect hashing,and document indexing.
The algorithms have also proven useful in numerous prob-lems outside the field of NLP, such as DNA sequence matching and computer virusrecognition.An earlier version of this paper, authored by Daciuk, Watson, and Watson, ap-peared at the International Workshop on Finite-state Methods in Natural LanguageProcessing in 1998--see Daciuk, Watson, and Watson (1998).2.
Mathematical PreliminariesWe define a deterministic finite-state automaton to be a 5-tuple M = (Q, ~, 6, q0, F),where Q is a finite set of states, q0 E Q is the start state, F C Q is a set of final states,is a finite set of symbols called the alphabet, and 6 is a partial mapping 6: Q x G ~ Qdenoting transitions.
When 6(q,a) is undefined, we write ~(q,a) = _L.
We can extendthe 6 mapping to partial mapping 6*: Q x ~* ~ Q as follows (where a E Y,, x E ~*):= q6*(q, ax) = {6*(6(q,a),x) ifotherwise6(q,a) ~ J_Let DAFSA be the set of all deterministic finite-state automata in which the transitionfunction 6 is acyclic--there is no string w and state q such that 6" (q, w) = q.We define ?
(M) to be the language accepted by automaton M:?
(M) = {xE I 6*(q0,x)The size of the automaton, IMI, is equal to the number of states, IQ\[.
~(G*) is the setof all languages over G. Define the function 2: Q ~ 7~(G *) to map a state q to theset of all strings on a path from q to any final state in M. More precisely,Z (q) = {x 16"(q,x) c F}?
(q) is called the right language of q.
Note that ?
(M) =?
(q0).
The right language of1 The algorithms inDaciuk's implementation differ slightly from those presented here, as he usesautomata with final transitions, not final states.
Such automata h ve fewer states and fewer transitionsthan traditional ones.4Daciuk, Mihov, Watson, and Watson Incremental Construction of FSAsa state can also be defined recursively:(q)= {a?
(6 (q ,a ) ) \ [ac~A6(q ,a )~ _L } U {{~ } i fqEF  otherwiseOne may ask whether such a recursive definition has a unique solution.
Most texts onlanguage theory, for example Moll, Arbib, and Kfoury (1988), show that the solutionis indeed unique--it is the least fixed-point of the equation.We also define a property of an automaton specifying that all states can be reachedfrom the start state:Reachable(M) = Vq~Q 3xc ~, (6* ( qo, x) = q)The property of being a minimal automaton is traditionally defined as follows (seeWatson \[1993b, 1995\]):Min(M) = VM, EDAFSA(~(M ) = ?
(M') ~ IMI ~ IM'I)We will, however, use an alternative definition of minimality, which is shown to beequivalent:Minimal(M) = (Vq,q, cQ(q ~ q' ~?
(q) #?
(q'))) A Reachable(M)A general treatment ofautomata minimization can be found in Watson (1995).
A formalproof of the correctness of the following algorithm can be found in Mihov (1998).3.
Construct ion from Sorted DataA trie is a dictionary with a tree-structured transition graph in which the start stateis the root and all leaves are final states.
2 An example of a dictionary in a form of atrie is given in Figure 1.
We can see that many subtrees in the transition graph areisomorphic.
The equivalent minimal dictionary (Figure 2) is the one in which onlyone copy of each isomorphic subtree is kept.
This means that, pointers (edges) toall isomorphic subtrees are replaced by pointers (edges) to their unique representa-tive.The traditional method of obtaining a minimal dictionary is to first create a (notnecessarily minimal) dictionary for the language and then minimize it using any oneof a number of algorithms (again, see Watson \[1993b, 1995\] for numerous examples ofsuch algorithms).
The first stage is usually done by building a trie, for which there arefast and well-understood algorithms.
Dictionary minimization algorithms are quite ef-ficient in terms of the size of their input dictionary--for some algorithms, the memoryand time requirements are both linear in the number of states.
Unfortunately, even suchgood performance is not sufficient in practice, where the intermediate dictionary (thetrie) can be much larger than the available physical memory.
(Some effort towardsdecreasing the memory requirement has been made; see Revuz \[1991\].)
This paperpresents a way to reduce these intermediate memory requirements and decrease the2 There may also be nonleaf, inother words interior, states that are final.Computational Linguistics Volume 26, Number 1Figure 1A trie whose language is the French regular endings of verbs of the first group.Figure 2The unique minimal dictionary whose language is the French regular endings of verbs of thefirst group.total construction time by constructing the minimal dictionary incrementally (word byword, maintaining an invariant of minimality), thus avoiding ever having the entiretrie in memory.Daciuk, Mihov, Watson, and Watson Incremental Construction of FSAsThe central part of most automata minimization algorithms is a classificationof states.
The states of the input dictionary are partitioned such that the equiva-lence classes correspond to the states of the equivalent minimal automaton.
Assum-ing the input dictionary has only reachable states (that is, Reachable is true), we candeduce (by our alternative definition of minimality) that each state in the minimaldictionary must have a unique right language.
Since this is a necessary and suffi-cient condition for minimality, we can use equality of right languages as the equiv-alence relation for our classes.
Using our definition of right languages, it is easilyshown that equality of right languages is an equivalence relation (it is reflexive,symmetric, and transitive).
We will denote two states, p and q, belonging to thesame equivalence class by p = q (note that = here is different from its use for log-ical equivalence of predicates).
In the literature, this relation is sometimes writtenas E.To aid in understanding, letus traverse the trie (see Figure 1) with the postordermethod and see how the partitioning can be performed.
For each state we encounter,we must check whether there is an equivalent state in the part of the dictionary thathas already been analyzed.
If so, we replace the current state with the equivalentstate.
If not, we put the state into a register, so that we can find it easily.
It followsthat the register has the following property: it contains only states that are pairwiseinequivalent.
We start with the (lexicographically) first leaf, moving backward throughthe trie toward the start state.
All states up to the first forward-branching state (statewith more than one outgoing transition) must belong to different classes and we im-mediately place them in the register, since there will be no need to replace them byother states.
Considering the other branches, and starting from their leaves, we need toknow whether or not a given state belongs to the same class as a previously registeredstate.
For a given state p (not in the register), we try to find a state q in the registerthat would have the same right language.
To do this, we do not need to compare thelanguages themselves---comparing setsof strings is computationally expensive.
Wecan use our recursive definition of the right language.
State p belongs to the sameclass as q if and only if:.2.3.4.they are either both final or both nonfinal; andthey have the same number of outgoing transitions; andcorresponding outgoing transitions have the same labels; andcorresponding outgoing transitions lead to states that have the sameright languages.Because the postorder method ensures that all states reachable from the states al-ready visited are unique representatives of their classes (i.e., their right languagesare unique in the visited part of the automaton), we can rewrite the last conditionas :4'.
corresponding transitions lead to the same states.If all the conditions are satisfied, the state p is replaced by q.
Replacing p simply in-volves deleting it while redirecting all of its incoming transitions to q.
Note that allComputational Linguistics Volume 26, Number 1leaf states belong to the same equivalence class.
If some of the conditions are not sat-isfied, p must be a representative of a new class and therefore must be put into theregister.To build the dictionary one word at a time, we need to merge the process ofadding new words to the dictionary with the minimization process.
There are twocrucial questions that must be answered.
First, which states (or equivalence classes)are subject o change when new words are added?
Second, is there a way to add newwords to the dictionary such that we minimize the number of states that may need tobe changed uring the addition of a word?
Looking at Figures 1 and 2, we can repro-duce the same postorder traversal of states when the input data is lexicographicallysorted.
(Note that in order to do this, the alphabet G must be ordered, as is the casewith ASCII and Unicode).
To process a state, we need to know its right language.
Ac-cording to the method presented above, we must have the whole subtree whose rootis that state.
The subtree represents endings of subsequent (ordered) words.
Furtherinvestigation reveals that when we add words in this order, only the states that needto be traversed to accept he previous word added to the dictionary may change whena new word is added.
The rest of the dictionary remains unchanged, because a newword eitherbegins with a symbol different from the first symbols of all wordsalready in the automaton; the beginning symbol of the new word islexicographically placed after those symbols; orit shares ome (or even all) initial symbols of the word previously addedto the dictionary; the algorithm then creates a forward branch, as thesymbol on the label of the transition must be later in the alphabet thansymbols on all other transitions leaving that state.When the previous word is a prefix of the new word, the only state that is to bemodified is the last state belonging to the previous word.
The new word may shareits ending with other words already in the dictionary, which means that we need tocreate links to some parts of the dictionary.
Those parts, however, are not modified.This discovery leads us to Algorithm 1, shown below.Algorithm 1.Register := ~;do there is another word --*Word := next word in lexicographic order;CommonPrefix := common_prefix(Word);LastS tate := 6*(q0, CommonPrefix ) ;CurrentSuffix := Word\[length(CommonPrefix)+ l. .
.
length(Word)l;if has_children(LastState) --,replace ~r_register(Last S tate)fi;add_suffix(LastState, CurrentSuffix)od;replace_or_register(qo)8Daciuk, Mihov, Watson, and Watson Incremental Construction of FSAsfunc common_prefix(Word)return the longest prefix w of Word such that ~* (q0, w) ~ 3_cnuffunc replace_or_register(State) --~Child := last_child(State);if has_children(Child)replace_or_register(Child)fi;if 3qEQ( q E Register A q = Child) --,last_child(State) :-- q: (q E Register A q = Child);delete(Child)elseRegister := Register U {Child}ficnufThe main loop of the algorithm reads subsequent words and establishes whichpart of the word is already in the automaton (the CommonPrefix), and which is not(the CurrentSuffix).
An  important step is determining what the last state (here calledLastState) in the path of the common prefix is.
If LastState already has children, itmeans that not all states in the path of the previously added word are in the path ofthe common prefix.
In that case, by calling the function replace_or_register, we can letthe minimization process work on those states in the path of the previously addedword that are not in the common prefix path.
Then we can add to the LastState a chainof states that would recognize the CurrentSuffix.The function common_prefix finds the longest prefix (of the word to be added)that is a prefix of a word already in the automaton.
The prefix can be empty (since= q).The function add_suffix creates a branch extending out of the dictionary, whichrepresents he suffix of the word being added (the maximal suffix of the word whichis not a prefix of any other word already in the dictionary).
The last state of this branchis marked as final.The function last_child returns a reference to the state reached by the lexicographi-cally last transition that is outgoing from the argument s ate.
Since the input data is lex-icographically sorted, last_child returns the outgoing transition (from the state) most re-cently added (during the addition of the previous word).
The function replace_or_registereffectively works on the last child of the argument state.
It is called with the argu-ment that is the last state in the common prefix path (or the initial state in the lastcall).
We need the argument state to modify its transition in those instances in whichthe child is to be replaced with another (equivalent) state.
Firstly, the function callsitself recursively until it reaches the end of the path of the previously added word.Note that when it encounters a state with more than one child, it takes the last one,as it belongs to the previously added word.
As the length of words is limited, so isthe depth of recursion.
Then, returning from each recursive call, it checks whether astate equivalent to the current state can be found in the register.
If this is true, thenthe state is replaced with the equivalent state found in the register.
If not, the state isregistered as a representative of a new class.
Note that the function replace-or_registerprocesses only the states belonging to the path of the previously added word (a part,or possibly all, of those created with the previous call to add_suffix), and that thoseComputational Linguistics Volume 26, Number 1states are never reprocessed.
Finally, has_children returns true if, and only if, there areoutgoing transitions from the state.During the construction, the automaton states are either in the register or on thepath for the last added word.
All the states in the register are states in the resultingminimal automaton.
Hence the temporary automaton built during the constructionhas fewer states than the resulting automaton plus the length of the longest word.Memory is needed for the minimized ictionary that is under construction, the callstack, and for the register of states.
The memory for the dictionary is proportionalto the number of states and the total number of transitions.
The memory for theregister of states is proportional to the number of states and can be freed once con-struction is complete.
By choosing an appropriate implementation method, one canachieve a memory complexity O(n) for a given alphabet, where n is the numberof states of the minimized automaton.
This is an important advantage of our algo-rithm.For each letter from the input list, the algorithm must either make a step in thefunction common_prefix or add a state in the procedure add_suyqx.
Both operations canbe performed in constant time.
Each new state that has been added in the procedureadd~ufix has to be processed exactly once in the procedure replace_or_register.
The num-ber of states that have to be replaced or registered is clearly smaller than the numberof letters in the input list.
3 The processing of one state in the procedure consists ofone register search and possibly one register insertion.
The time complexity of thesearch is ?
(log n),where n is the number of states in the (minimized) dictionary.
Thetime complexity of adding a state to the register is also O(log n).
In practice, however,by using a hash table to represent the register (and equivalence r lation), the averagetime complexity of those operations can be made almost constant.
Hence the timecomplexity of the whole algorithm is 0(I log n), where l is the total number of lettersin the input list.4.
Construct ion from Unsorted DataSometimes it is difficult or even impossible to sort the input data before constructinga dictionary.
For example, there may be insufficient time or storage space to sort thedata or the data may originate in another program or physical source.
An incrementaldictionary-building algorithm would still be very useful in those situations, althoughunsorted ata makes it more difficult o merge the trie-building and the minimizationprocesses.
We could leave the two processes disjoint, although this would lead tothe traditional method of constructing a trie and minimizing it afterwards.
A bettersolution is to minimize verything on-the-fly, possibly changing the equivalence classesof some states each time a word is added.
Before actually constructing a new statein the dictionary, we first determine if it would be included in the equivalence classof a preexisting state.
Similarly, we may need to change the equivalence classes ofpreviously constructed states ince their right languages may have changed.
This leadsto an incremental construction algorithm.
Naturally, we would want to create the statesfor a new word in an order that would minimize the creation of new equivalenceclasses.As in the algorithm for sorted data, when a new word w is added, we searchfor the prefix of w already in the dictionary.
This time, however, we cannot assume3 The exact number of the states that are processed in the procedure replace-or_register is equal to thenumber of states in the trie for the input language.10Daciuk, Mihov, Watson, and Watson Incremental Construction of FSAsa \bFigure 3The result of blindly adding the word bae to a minimized ictionary (appearing on the left)containing abd and bad.
The rightmost dictionary inadvertently contains abe as well.
The lowerdictionary is correct--state 3 had to be cloned.that the states traversed by this common prefix will not be changed by the additionof the word.
If there are any preexisting states traversed by the common prefix thatare already targets of more than one in-transition (known as confluence states), thenblindly appending another transition to the last state in this path (as we would in thesorted algorithm) would accidentally add more words than desired (see Figure 3 foran example of this).To avoid generation of such spurious words, all states in the common prefix pathfrom the first confluence state must be cloned.
Cloning is the process of creating a newstate that has outgoing transitions on the same labels and to the same destination statesas a given state.
If we compare the minimal dictionary (Figure 1) to an equivalent trie(Figure 2), we notice that a confluence state can be seen as a root of several original,isomorphic subtrees merged into one (as described in the previous section).
One ofthe isomorphic subtrees now needs to be modified (leaving it no longer isomorphic),so it must first be separated from the others by cloning of its root.
The isomorphicsubtrees hanging off these roots are unchanged, so the original root and its clone havethe same outgoing transitions (that is, transitions on the same labels and to the samedestination states).In Algorithm 1, the confluence states were never traversed uring the search forthe common prefix.
The common prefix was not only the longest common prefix of theword to be added and all the words already in the automaton, it was also the longestcommon prefix of the word to be added and the last (i.e., the previous) word added tothe automaton.
As it was the function replace_or_register hat created confluence states,and that function was never called on states belonging to the path of the last wordadded to the automaton, those states could never be found in the common prefixpath.Once the entire common prefix is traversed, the rest of the word must be appended.If there are no confluence states in the common prefix, then the method of adding therest of the word does not differ from the method used in the algorithm for sorteddata.
However, we need to withdraw (from the register) the last state in the commonprefix path in order not to create cycles.
This is in contrast o the situation in thealgorithm for sorted data where that state is not yet registered.
Also, CurrentSuffixcould be matched with a path in the automaton containing states from the commonprefix path (including the last state of the prefix).11Computational Linguistics Volume 26, Number 1bCa d e a ~Figure 4Consider an automaton (shown in solid lines on the left-hand figure) accepting abcde andfghde.
Suppose we want to add fgh@de.
As the common prefix path (shown in thicker lines)contains a confluence state, we clone state 5 to obtain state 9, add the suffix to state 9, andminimize it.
When we also consider the dashed lines in the left-hand figure, we see that state8 became a new confluence state earlier in the common prefix path.
The right-hand figureshows what could happen if we did not rescan the common prefix path for confluence states.State 10 is a clone of state 4.When there is a confluence state, then we need to clone some states.
We start withthe last state in the common prefix path, append the rest of the word to that clone andminimize it.
Note that in this algorithm, we do not wait for the next word to come, sowe can minimize (replace or register the states of) CurrentSuffix state by state as theyare created.
Adding and minimizing the rest of the word may create new confluencestates earlier in the common prefix path, so we need to rescan the common prefix pathin order not to create cycles, as illustrated in Figure 4.
Then we proceed with cloningand minimizing the states on the path from the state immediately preceding the laststate to the current first confluence state.Another, less complicated but also less economical, method can be used to avoidthe problem of creating cycles in the presence of confluence states.
In that solution, weproceed from the state immediately preceding the confluence state towards the end ofthe common prefix path, cloning the states on the way.
But first, the state immediatelypreceding the first confluence state should be removed from the register.
At the endof the common prefix path, we add the suffix.
Then, we call replace_or_register with thepredecessor f the state immediately preceding the first confluence state.
The followingshould be noted about this solution:memory requirements are higher, as we keep more than one isomorphicstate at a time,the function replace_or_register must remain recursive (as in the sortedversion), andthe argument to replace_or_register must be a string, not a symbol, inorder to pass subsequent symbols to children.When the process of traversing the common prefix (up to a confluence state) andadding the suffix is complete, further modifications follow.
We must recalculate theequivalence class of each state on the path of the new word.
If any equivalence classchanges, we must also recalculate the equivalence classes of all of the parents of allof the states in the changed class.
Interestingly, this process could actually make thenew dictionary smaller.
For example, if we add the word abe to the dictionary at thebottom of Figure 3 while maintaining minimality, we obtain the dictionary shown in12Daciuk, Mihov, Watson, and Watson Incremental Construction of FSAsthe right of Figure 3, which is one state smaller.
The result ing algor i thm is shown inA lgor i thm 2.A lgor i thm 2.Register := 0;do there is another word  --*Word := next word;CommonPrefix := common_prefix(Word);CurrentSuffix := Word\[length(CommonPrefix) + 1 ... length(Word)I;if CurrentSuffix = c A 6" (qo, CommonPrefix) E F --*continuefi;FirstState := first_state( CommonPrefix);if FirstState = 0 --*LastState := 6* (q0, CommonPrefix)elseLastState := clone( 6* ( qo, CommonPrefix ) )fi;add_suffix(LastState, CurrentSuffix);if FirstState ~ ~ --,FirstState := first~state(CommonPrefix);Currentlndex := (length(x): 6* (q0, x) = FirstState);for i from length(CommonPrefix) - 1 downto  Currentlndex --+CurrentState := clone( 6* ( qo, CommonPrefix\[1. .
.
i\]));6( CurrentState, CommonPrefixli\]) := LastState;replace_or_register( CurrentState);LastState := CurrentStaterofelseCurrentlndex := length( CommonPrefix)fi;Changed := true;do ChangedCurrentlndex := Currentlndex - 1;CurrentState := 6* (q0, Word\[1... Currentlndex\]);OldState := LastState;if Currentlndex > 0 --*Register := Register - {LastState}fi;replace_or_register( CurrentState);Changed := OldState ~ LastStateodif ~Changed A Currentlndex > 0 --~Register := Register U {CurrentState}fiodfunc replace Jar_register(State, Symbol)Child := 6(State, Symbol);if 3q E Q(q c Register A q = Child)13Computational Linguistics Volume 26, Number 1cnufdelete(Child);last_child(State) := q: (q E Register A q -- Child)elseRegister := Register u{Child}fiThe main loop reads the words, finds the common prefix, and tries to find thefirst confluence state in the common prefix path.
Then the remaining part of the word(CurrentSuf-fi'x) is added.If a confluence state is found (i.e., FirstState points to a state in the automaton), allstates from the first confluence state to the end of the common prefix path are cloned,and then considered for replacement or registering.
Note that the inner loop (with i asthe control variable) begins with the penultimate state in the common prefix, becausethe last state has already been cloned and the function replace~r_register acts on a childof its argument state.Addition of a new suffix to the last state in the common prefix changes the rightlanguages of all states that precede that state in the common prefix path.
The last partof the main loop deals with that situation.
If the change resulted in such modificationof the right language of a state that an equivalent state can be found somewhere lsein the automaton, then the state is replaced with the equivalent one and the changepropagates towards the initial state.
If the replacement of a given state cannot takeplace, then (according to our recursive definition of the right language) there is noneed to replace any preceding state.Several changes to the functions used in the sorted algorithm are necessary tohandle the general case of unsorted ata.
The replace~r_register procedure needs to bemodified slightly.
Since new words are added in arbitrary order, one can no longerassume that the last child (lexicographically) of the state (the one that has been addedmost recently) is the child whose equivalence class may have changed.
However, weknow the label on the transition leading to the altered child, so we use it to access thatstate.
Also, we do not need to call the function recursively.
We assume that add~uffixreplaces or registers the states in the CurrentSuffix n the correct order; later we processone path of states in the automaton, starting from those most distant from the initialstate, proceeding towards the initial state q0.
So in every situation in which we callreplace_or_register, all children of the state Child are already unique representatives oftheir equivalence classes.Also, in the sorted algorithm, add_suffix is never passed ~ as an argument, whereasthis may occur in the unsorted version of the algorithm.
The effect is that the LastStateshould be marked as final since the common prefix is, in fact, the entire word.
In thesorted algorithm, the chain of states created by add_suffix was left for further treatmentuntil new words are added (or until the end of processing).
Here, the automaton iscompletely minimized on-the-fly after adding a new word, and the function add~suffixcan call replace_or_register fo each state it creates (starting from the end of the suffix).Finally, the new function first_state simply traverses the dictionary using the givenword prefix and returns the first confluence state it encounters.
If no such state exists,first_state returns 0.As in the sorted case, the main loop of the unsorted algorithm executes m times,where m is the number of words accepted by the dictionary.
The inner loops are exe-cuted at most Iwl times for each word.
Putting a state into the register takes O(logn),although it may be constant when using a hash table.
The same estimation is valid14Daciuk, Mihov, Watson, and Watson Incremental Construction of FSAsfor a removal from the register.
In this case, the time complexity of the algorithmremains the same, but the constant changes.
Similarly, hashing can be used to pro-vide an efficient method of determining the state equivalence classes.
For sorted data,only a single path through the dictionary could possibly be changed each time anew word is added.
For unsorted ata, however, the changes frequently fan out andpercolate all the way back to the start state, so processing each word takes moretime.4.1 Extending the AlgorithmsThese new algorithms can also be used to construct transducers.
The alphabet of the(transducing) automaton would be G1 x G2, where G1 and ~2 are the alphabet ofthe levels.
Alternatively, elements of G~ can be associated with the final states of thedictionary and only output once a valid word from G~ is recognized.5.
Related WorkAn algorithm described by Revuz \[1991\] also constructs a dictionary from sorted datawhile performing a partial minimization on-the-fly.
Data is sorted in reverse orderand that property is used to compress the endings of words within the dictionary asit is being built.
This is called a pseudominimization a d must be supplemented bya true minimization phase afterwards.
The minimization phase still involves findingan equivalence relation over all of the states of the pseudominimal dictionary.
It ispossible to use unsorted data but it produces a much bigger dictionary in the firststage of processing.
However, the time complexity of the minimization can be reducedsomewhat by using knowledge of the pseudominimization process.
Although thispseudominimization technique is more economic in its use of memory than traditionaltechniques, we are still left with a subminimal dictionary that can be a factor of 8 timeslarger than the equivalent minimal dictionary (Revuz \[1991, page 33\], reporting on theDELAF dictionary).Recently, a semi-incremental lgorithm was described by Watson (1998) at theWorkshop on Implementing Automata.
That algorithm requires the words to be sortedin any order of decreasing length (this sorting process can be done in linear time),and takes advantage of automata properties imilar to those presented in this paper.In addition, the algorithm requires a final minimization phase after all words havebeen added.
For this reason, it is only semi-incremental and does not maintain fullminimality while adding words--although it usually maintains the automata closeenough to minimality for practical applications.6.
ConclusionsWe have presented two new methods for incrementally constructing a minimal, deter-ministic, acyclic finite-state automaton from a finite set of words (possibly with corre-sponding annotations).
Their main advantage is their minimal intermediate memoryrequirements.
4 The total construction time of these minimal dictionaries is dramati-cally reduced from previous algorithms.
The algorithm constructing a dictionary fromsorted data can be used in parallel with other algorithms that traverse or utilize thedictionary, since parts of the dictionary that are already constructed are no longersubject o future change.4 It is minimal in asymptotic erms; naturally compact data structures can also be used.15Computational Linguistics Volume 26, Number 1AcknowledgmentsJan Daciuk would like to express hisgratitude to the Swiss Federal ScholarshipCommission for providing a scholarshipthat made possible the work described here.Jan would also like to thank friends fromISSCO, Geneva, for their comments andsuggestions on early versions of thealgorithms given in this paper.Bruce Watson and Richard Watsonwould like to thank Ribbit SoftwareSystems Inc. for its continued support inthese fields of applicable research.All authors would like to thank theanonymous reviewers and Nanette Saes fortheir valuable comments and suggestionsthat led to significant improvements in thepaper.ReferencesDaciuk, Jan, Bruce W. Watson, andRichard E. Watson.
1998.
Incrementalconstruction of minimal acyclic finite stateautomata nd transducers.
In Proceedingsof the International Workshop on Finite StateMethods in Natural Language Processing,pages 48-56, Ankara, Turkey, 30 June-1July.Hopcroft, John E. and Jeffrey D. Ullman.1979.
Introduction to Automata Theory,Languages, and Computation.Addison-Wesley, Reading, MA.Mihov, Stoyan.
1998.
Direct building ofminimal automaton for given list.
InAnnuaire de l'Universitd e Sofia "St. KI.Ohridski', volume 91, book 1, pages 38-40.Facult4 de Mathematique etInformatique,Sofia, Bulgaria, livre 1 edition, February.Available at http://lml.bas.bg/,-~stoyan/publications.html.Moll, Robert N., Michael A. Arbib, and A. J.Kfoury.
1988.
Introduction to FormalLanguage Theory.
Springer Verlag, NewYork, NY.Revuz, Dominique.
1991.
Dictionnaires etlexiques: mdthodes talgorithmes.
Ph.D.thesis, Institut Blaise Pascal, Paris, France.LITP 91.44.Watson, Bruce W. 1993a.
A taxonomy offinite automata construction algorithms.Computing Science Note 93/43,Eindhoven University of Technology, TheNetherlands.
Available atwww.OpenFIRE.org.Watson, Bruce W. 1993b.
A taxonomy offinite automata minimization algorithms.Computing Science Note 93/44,Eindhoven University of Technology, TheNetherlands.
Available atwww.OpenFIRE.org.Watson, Bruce W. 1995.
Taxonomies andToolkits of Regular Language Algorithms.Ph.D.
thesis, Eindhoven University ofTechnology, the Netherlands.
Available atwww.OpenFIRE.org.Watson, Bruce W. 1998.
A fast newsemi-incremental algorithm forconstruction of minimal acyclic DFAs.
InProceedings ofthe Third InternationalWorkshop on Implementing Automata, pages121-32, Rouen, France, 17-19 September.16
