Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 88?97,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsRecession Segmentation: Simpler Online Word Segmentation UsingLimited Resources?Constantine Lignos, Charles YangDept.
of Computer and Information Science, Dept.
of LinguisticsUniversity of Pennsylvanialignos@cis.upenn.edu, charles.yang@ling.upenn.eduAbstractIn this paper we present a cognitively plau-sible approach to word segmentation thatsegments in an online fashion using onlylocal information and a lexicon of pre-viously segmented words.
Unlike popu-lar statistical optimization techniques, thelearner uses structural information of theinput syllables rather than distributionalcues to segment words.
We develop amemory model for the learner that like achild learner does not recall previously hy-pothesized words perfectly.
The learner at-tains an F-score of 86.69% in ideal condi-tions and 85.05% when word recall is un-reliable and stress in the input is reduced.These results demonstrate the power that asimple learner can have when paired withappropriate structural constraints on its hy-potheses.1 IntroductionThe problem of word segmentation presents animportant challenge in language acquisition.
Thechild learner must segment a continuous stream ofsounds into words without knowing what the in-dividual words are until the stream has been seg-mented.
Computational models present an op-portunity to test the potentially innate constraints,structures, and algorithms that a child may be us-ing to guide her acquisition.
In this work we de-velop a segmentation model from the constraintssuggested by Yang (2004) and evaluate it in ideal-ized conditions and conditions that better approx-imate the environment of a child learner.
We seekto determine how these limitations in the learner?sinput and memory affect the learner?s performanceand to demonstrate that the presented learner is ro-bust even under non-ideal conditions.
?Portions of this work were adapted from an earliermanuscript, Word Segmentation: Quick But Not Dirty.2 Related WorkMost recent work in word segmentation of child-directed speech has operated within statistical op-timization frameworks, particularly Bayesian ap-proaches (Goldwater et al, 2009; Johnson andGoldwater, 2009).
These models have establishedthe state-of-the-art for the task of selecting appro-priate word boundaries from a stream of unstruc-tured phonemes.
But while these models deliverexcellent performance, it is not clear how they in-form the process of acquisition.Trying to find cognitive insight from these typesof models is difficult because of the inherent mis-match in the quality and types of hypotheses theymaintain during learning.
Children are incremen-tal learners (Brown, 1973), and learners relyingon statistical optimization are generally not.
Achild?s competence grows gradually as she hearsand produces more and more utterances, goingthrough predictable changes to her working gram-mar (Marcus et al, 1992) that statistical optimiza-tion techniques typically do not go through and donot intend to replicate.Statistical models provide excellent informationabout the features, distributional cues, and priorsthat can be used in learning, but provide little in-formation about how a child learner can use thisinformation and how her knowledge of languagedevelops as the learning process evolves.
Previ-ous simulations in word segmentation using thesame type of distributional information as manystatistical optimization-based learners but withoutan optimization model suggest that statistics aloneare not sufficient for learning to succeed in a com-putationally efficient online manner; further con-straints on the search space are needed (Yang,2004).Previous computational models have demandedtremendous memory and computational capacityfrom human learners.
For example, the algorithm88of Brent & Cartwright (1996) produces a set ofpossible lexicons that describe the learning cor-pus, each of which is evaluated as the learner it-erates until no further improvement is possible.
Itis unlikely that an algorithm of this type is some-thing a human learner is capable of using given therequirement to remember at the very least a longhistory of recent utterances encountered and con-stantly reanalyze them to find a optimal segmenta-tion.
Work in this tradition makes no claims, how-ever, that these methods are actually the ones usedby human learners.On the other hand, previous computationalmodels often underestimate the human learner?sknowledge of linguistic representations.
Most ofthese models are ?synthetic?
in the sense of Brent(1999): the raw material for segmentation is astream of segments, which are then successivelygrouped into larger units and eventually, conjec-tured words.
This assumption may make the childlearner?s job unnecessarily hard; since syllablesare hierarchical structures consisting of segments,treating the linguistic data as unstructured segmentsequences makes the problem harder than it actu-ally is.
For a given utterance, there are fewer sylla-bles than segments, and hence fewer segmentationpossibilities.Modeling the corpus using hierarchical gram-mars that can model the input at varying levels(word collocations, words, syllables, onsets, etc.
)provide the learner the most flexibility, allowingthe learner to build structure from the individualphonemes and apply distributions at each level ofabstraction (Johnson and Goldwater, 2009).
Whilethis results in state-of-the-art performance for seg-mentation performed at the phoneme level, thisapproach requires significant computational re-sources as each additional level of representationincreases the complexity of learning.
In addition,it is not clear that some of the intermediate levelsin such an approach, such as word level colloca-tions which are not syntactic constituents, wouldhave any linguistic or psychological reality to ahuman learner.A number of psychologically-motivated mod-els of word segmentation rely on the use of syl-labic transitional probabilities (TPs), basing theuse of TPs on experimental work in artificial lan-guage learning (Saffran et al, 1996a; Saffran etal., 1996b) and in corpus studies (Swingley, 2005).The identification of the syllable as the basic unitof segmentation is supported research in experi-mental psychology using infants as young as 4-days-old (Bijeljac-Babic et al, 1993), but whensyllable transitional probabilities are evaluated inonline learning procedures that only use local in-formation (Yang, 2004), the results are surpris-ingly poor, even under the assumption that thelearner has already syllabified the input perfectly.Precision is 41.6%, and recall is 23.3%, whichwe will show is worse than a simple baseline ofassuming every syllable is a word.
The below-baseline performance is unsurprising given that inorder for this type of model to posit a word bound-ary, a transitional probability between syllablesmust be lower than its neighbors.
This conditioncannot be met if the input is a sequence of mono-syllabic words for which a boundary must be pos-tulated for every syllable; it is impossible to treatevery boundary as a local minimum.While the pseudo-words used in infant stud-ies measuring the ability to use transitional prob-ability information are uniformly three-syllableslong, much of child-directed English consists ofsequences of monosyllabic words.
Corpus statis-tics reveal that on average a monosyllabic wordis followed by another monosyllabic word 85%of time (Yang, 2004), and thus learners that useonly local transitional probabilities without anyglobal optimization are unlikely to succeed.
Thisproblem does not affect online approaches thatuse global information, such as computing themaximum likelihood of the corpus incrementally(Venkataraman, 2001).
Since these approaches donot require each boundary be a local minimum,they are able to correctly handle a sequence ofmonosyllable words.We believe that the computational modeling ofpsychological processes, with special attention toconcrete mechanisms and quantitative evaluations,can play an important role in identifying the con-straints and structures relevant to children?s acqui-sition of language.
Rather than using a prior whichguides the learner to a desired distribution, we ex-amine learning with respect to a model in whichthe hypothesis space is constrained by structuralrequirements.In this paper we take a different approach thanstatistical optimization approaches by exploringhow well a learner can perform while processinga corpus in an online fashion with only local in-formation and a lexicon of previously segmented89words.
We present a simple, efficient approachto word segmentation that uses structural informa-tion rather than distributional cues in the input tosegment words.
We seek to demonstrate that evenin the face of impoverished input and limited re-sources, a simple learner can succeed when it op-erates with the appropriate constraints.3 Constraining the Learning SpaceModern machine learning research (Gold, 1967;Valiant, 1984; Vapnik, 2000) suggests that con-straints on the learning space and the learningalgorithm are essential for realistically efficientlearning.
If a domain-neutral learning model failson a specific task where children succeed, it islikely that children are equipped with knowledgeand constraints specific to the task at hand.
Itis important to identify such constraints to see towhat extent they complement, or even replace, do-main neutral learning mechanisms.A particularly useful constraint for word seg-mentation, introduced to the problem of wordsegmentation by Yang (2004) but previously dis-cussed by Halle and Vergnaud (1987), is as fol-lows:Unique Stress Constraint (USC): A word canbear at most one primary stress.A simple example of how adult learners mightuse the USC is upon hearing novel names orwords.
Taking Star Wars characters as an exam-ple, it is clear that chewbacca is one word butdarthvader cannot be as the latter bears two pri-mary stresses.The USC could give the learner many isolatedwords for free.
If the learner hears an utterancethat contains exactly one primary stress, it is likelyit is a single word.
Moreover, the segmenta-tion for a multiple word utterance can be equallystraightforward under USC.
Consider a sequenceW1S1S2S3W2, where W stands for a weak sylla-ble and S stands for a strong syllable.
A learnerequipped with USC will immediately know thatthe sequence consists of three words: specifically,W1S1, S2, and S2W2.The USC can also constrain the use of otherlearning techniques.
For example, the syllableconsequence S1W1W2W3S2 cannot be segmentedby USC alone, but it may still provide cues thatfacilitate the application of other segmentationstrategies.
For instance, the learner knows that thesequence consists of at least two words, as indi-cated by two strong syllables.
Moreover, it alsoknows that in the window between S1 and S2 theremust be one or more word boundaries.Yang (2004) evaluates the effectiveness of theUSC in conjunction with a simple approach to us-ing transitional probabilities.
The performance ofthe approach presented there improves dramati-cally if the learner is equipped with the assump-tion that each word can have only one primarystress.
If the learner knows this, then it maylimit the search for local minima to only the win-dow between two syllables that both bear primarystress, e.g., between the two a?s in the sequencelanguageacquisition.
This assumption is plau-sible given that 7.5-month-old infants are sensi-tive to strong/weak prosodic distinctions (Jusczyk,1999).
Yang?s stress-delimited algorithm achievesthe precision of 73.5% and recall of 71.2%, a sig-nificant improvement over using TPs alone, butstill below the baseline presented in our results.The improvement of the transitionalprobability-based approach when providedwith a simple linguistic constraint suggeststhat structural constraints can be powerful innarrowing the hypothesis space so that evensparse, local information can prove useful andsimple segmentation strategies can become moreeffective.It should be noted that the classification of everysyllable as ?weak?
or ?strong?
is a significant sim-plification.
Stress is better organized into hierar-chical patterns constructed on top of syllables thatvary in relative prominence based on the domainof each level of the hierarchy, and generally lan-guages avoid adjacent strong syllables (Libermanand Prince, 1977).
We later discuss a manipula-tion of the corpus used by Yang (2004) to addressthis concern.Additionally, there are significant challengesin reconstructing stress from an acoustic signal(Van Kuijk and Boves, 1999).
For a child learnerto use the algorithm presented here, she wouldneed to have mechanisms for detecting stress inthe speech signal and categorizing the gradientstress in utterances into a discrete level for eachsyllable.
These mechanisms are not addressed inthis work; our focus is on an algorithm that cansucceed given discrete stress information for eachsyllable.
Given the evidence that infants can dis-tinguish weak and strong syllables and use that in-90formation to detect word boundaries (Jusczyk etal., 1999), we believe that it is reasonable to as-sume that identifying syllabic stress is a task aninfant learner can perform at the developmentalstage of word segmentation.4 A Simple Algorithm for WordSegmentationWe now present a simple algebraic approach toword segmentation based on the constraints sug-gested by Yang (2004).
The learner we present isalgebraic in that it has a lexicon which stores pre-viously segmented words and identifies the inputas a combination of words already in the lexiconand novel words.
No transitional probabilities orany distributional data are calculated from the in-put.
The learner operates in an online fashion, seg-menting each utterance in a primarily left-to-rightfashion and updating its lexicon as it segments.The USC is used in two ways by the learner.First, if the current syllable has primary stress andthe next syllable also has primary stress, a wordboundary is placed between the current and nextsyllable.
Second, whenever the algorithm is facedwith the choice of accepting a novel word into thelexicon and outputting it as a word, the learner?abstains?
from doing so if the word violates USC,that is if it contains more than one primary stress.Since not all words are stressed, if a word containsno primary stresses it is considered an acceptableword; only a word with more that one primarystress is prohibited.
If a sequence of syllables hasmore than one primary stress and cannot be seg-mented further, the learner does not include thatsequence in its segmentation of the utterance anddoes not add it to the lexicon as it cannot be a validword.The algorithm is as follows, with each step ex-plained in further detail in the following para-graphs.For each utterance in the corpus, do the following:1.
As each syllable is encountered, use InitialSubtraction and USC Segmentation to seg-ment words from the beginning of the utter-ance if possible.2.
If unsegmented syllables still remain, applyFinal Subtraction, segmenting words itera-tively from the end of the utterance if pos-sible.3.
If unsegmented syllables still remain, if thosesyllables constitute a valid word under theUSC, segment them as a single word and addthem to the lexicon.
Otherwise, abstain, anddo not include these syllables in the segmen-tation of the sentence and do not add them tothe lexicon.Initial Subtraction.
If the syllables of the utter-ance from the last segmentation (or the start of theutterance) up to this point matches a word in thelexicon but adding one more syllable would resultin it not being a known word, segment off the rec-ognized word and increase its frequency.
This iter-atively segments the longest prefix word from theutterance.USC Segmentation.
If the current and next syl-lables have primary stress, place a word bound-ary after the current syllable, treating all syllablesfrom the last segmentation point up to and and in-cluding the current syllable as a potential word.
Ifthese syllables form a valid word under the USC,segment them as a word and add them to the lex-icon.
Otherwise, abstain, not including these syl-lables in the segmentation of the sentence and notadding them to the lexicon.Final Subtraction.
After initial subtraction andUSC Segmentation have been maximally appliedto the utterance, the learner is often left with asequence of syllables that is not prefixed by anyknown word and does not have any adjacent pri-mary stresses.
In this situation the learner worksfrom right to left on the remaining utterance, iter-atively removing words from the end of the utter-ance if possible.
Similar to the approach used inInitial Subtraction, the longest word that is a suf-fix word of the remaining syllables is segmentedoff, and this is repeated until the entire utterance issegmented or syllables remain that are not suffixedby any known word.The ability to abstain is a significant differencebetween this learner and most recent work on thistask.
Because the learner has a structural descrip-tion for a word, the USC, it is able to reject anyhypothesized words that do not meet the descrip-tion.
This improves the learner?s precision andrecall because it reduces the number of incorrectpredictions the learner makes.
The USC also al-lows the learner keep impossible words out of itslexicon.910 20 40 60 80 1000.00.20.40.60.81.0CountProbability ofRecallFigure 1: The selected probabilistic memory func-tion for ?
= 0.05.
The dashed line at 0.05 rep-resents the threshold above which a word is morelikely than not to be recalled, occurring at a countof approximately 14.5 A Probabilistic LexiconTo simulate the imperfect memory of a childlearner, we use a simple exponential function togenerate the probability with which a word is re-trieved from the lexicon:pr(word) = 1.0?
e?
?c(word)pr(word) is the probability of a word being re-trieved, ?
is a constant, and c(word) is the numberof times the word has been identified in segmen-tations thus far.
This type of memory function isa simplified representation of models of humans?memory recall capabilities (Anderson et al, 1998;Gillund and Shiffrin, 1984).
This memory func-tion for the value of ?
= 0.05, the value used inour experiments, is given in Figure 1.
We latershow that the choice of ?
has little impact on thelearner?s segmentation performance, and thus themore or less arbitrary selection of a value for ?
isof little consequence.When the algorithm attempts to subtract wordsfrom the beginning or end of an utterance, it maymiss words in the lexicon due to this probabilis-tic retrieval.
The learner only has one opportu-nity to recall a word in a given utterance.
For ex-ample, in the utterance P.EH1.N S.AH0.L (pencil),if the learner has P.EH1.N and P.EH1.N S.AH0.Lin its lexicon but P.EH1.N is more frequent, itmay fail to recall P.EH1.N S.AH0.L when exam-ining the second syllable but succeed in recog-nizing P.EH1.N in the first.
Thus it will breakoff P.EH1.N instead of P.EH1.N S.AH0.L.
Thismeans the learner may fail to reliably break offthe longest words, instead breaking off the longestword that is successfully recalled.While probabilistic memory means that thelearner will fail to recognize words it has seen be-fore, potentially decreasing recall, it also providesthe learner the benefit of probabilistically failingto repeat previous mistakes if they occur rarely.Probabilistic word recall results in a ?richget richer?
phenomenon as the learner segments;words that are used more often in segmentationsare more likely to be reused in later segmentations.While recent work from Bayesian approaches hasused a Dirichlet Process to generate these distri-butions (Goldwater et al, 2006), in this learner thereuse of frequent items in learning is a result ofthe memory model rather than an explicit processof reusing old outcomes or generating new ones.This growth is an inherent property of the cogni-tive model of memory used here rather than an ex-ternally imposed computational technique.6 EvaluationOur computational model is designed to processchild-directed speech.
The corpus we use to eval-uate it is the same corpus used by Yang (2004).Adult utterances were extracted from the Brown(1973) data in the CHILDES corpus (MacWhin-ney, 2000), consisting of three children?s data:Adam, Eve, and Sarah.
We obtained the pho-netic transcriptions of words from the CarnegieMellon Pronouncing Dictionary (CMUdict) Ver-sion 0.6 (Weide, 1998), using the first pronunci-ation of each word.
In CMUdict, lexical stressinformation is preserved by numbers: 0 for un-stressed, 1 for primary stress, 2 for secondarystress.
For instance, cat is represented as K.AE1.T,catalog is K.AE1.T.AH0.L.AO0.G, and catapult isK.AE1.T.AH0.P.AH2.L.T.
We treat primary stressas ?strong?
and secondary or unstressed syllablesas ?weak.
?For each word, the phonetic segments weregrouped into syllables.
This process is straightfor-ward by the use of the principle ?Maximize On-set,?
which maximizes the length of the onset aslong as it is valid consonant cluster of English, i.e.,92it conforms to the phonotactic constraints of En-glish.
For example, Einstein is AY1.N.S.T.AY0.Nas segments and parsed into AY1.N S.T.AY0.N assyllables: this is because /st/ is the longest validonset for the second syllable containing AY0 while/nst/ is longer but violates English phonotac-tics.
While we performed syllabification as a pre-processing step outside of learning, a child learnerwould presumably learn the required phonotac-tics as a part of learning to segment words.
9-month old infants are believed to have learnedsome phonotactic constraints of their native lan-guage (Mattys and Jusczyk, 2001), and learningthese constraints can be done with only minimalexposure (Onishi et al, 2002).Finally, spaces and punctuation betweenwords were removed, but the boundaries be-tween utterances?as indicated by line breaks inCHILDES?are retained.
Altogether, there are226,178 words, consisting of 263,660 syllables.The learning material is a list of unsegmentedsyllable sequences grouped into utterances, andthe learner?s task is to find word boundaries thatgroup substrings of syllables together, building alexicon of words as it segments.We evaluated the learner?s performance to ad-dress these questions:?
How does probabilistic memory affectlearner performance??
How much does degrading stress informationrelied on by USC segmentation reduce per-formance??
What is the interaction between the proba-bilistic lexicon and non-idealized stress infor-mation?To evaluate the learner, we tested configurationsthat used a probabilistic lexicon and ones with per-fect memory in two scenarios: Dictionary Stress,and Reduced Stress.
We create the Reduced Stresscondition in order to simulate that stress is of-ten reduced in casual speech, and that language-specific stress rules may cause reductions or shiftsin stress that prevent two strong syllables from oc-curring in sequence.
The difference between thescenarios is defined as follows:Dictionary Stress.
The stress information isgiven to the learner as it was looked up in CMU-dict.
For example, the first utterance from theAdam corpus would be B.IH1.G D.R.AH1.M (bigdrum), an utterance with two stressed monosyl-lables (SS).
In most languages, however, condi-tions where two stressed syllables are in sequenceare handled by reducing the stress of one syllable.This is simulated in the reduced stress condition.Reduced Stress.
The stress information ob-tained from CMUdict is post-processed in the con-text of each utterance.
For any two adjacent pri-mary stressed syllables, the first syllable is re-duced from a strong syllable to a weak one.
This isapplied iteratively from left to right, so for any se-quence of n adjacent primary-stress syllables, onlythe nth syllable retains primary stress; all othersare reduced.
This removes the most valuable clueas to where utterances can be segmented, as USCsegmentation no longer applies.
This simulates thestress retraction effect found in real speech, whichtries to avoid adjacent primary stresses.Learners that use probabilistic memory were al-lowed to iterate over the input two times with ac-cess to the lexicon developed over previous iter-ations but no access to previous segmentations.This simulates a child hearing many of the samewords and utterances again, and reduces the effectof the small corpus size used on the learning pro-cess.
Because the probabilistic memory reducesthe algorithm?s ability to build a lexicon, perfor-mance in a single iteration is lower than perfectmemory conditions.
In all other conditions, thelearner is allowed only a single pass over the cor-pus.The precision and recall metrics are calculatedfor the segmentation that the learner outputs andthe lexicon itself.
For an utterance, each wordin the learner?s segmentation that also appears inthe gold standard segmentation is counted as cor-rect, and each word in the learner?s segmentationnot present in the gold standard segmentation isa false alarm.
F-score is computed using equallybalanced precision and recall (F0).
The correctwords, false words, and number of words in thegold standard are summed over the output in eachiteration to produce performance measures for thatiteration.Precision, recall, and F-score are similarly com-puted for the lexicon; every word in the learner?slexicon present in the gold standard is counted ascorrect, and every word in the learner?s lexicon notpresent in the gold standard is a false alarm.
Thesecomputations are performed over word types inthe lexicon, thus all words in the lexicon are of93equal weight in computing performance regard-less of their frequency.
In the probabilistic mem-ory conditions, however, the memory function de-fines the probability of each word being recalled(and thus being considered a part of the lexicon)at evaluation time.In addition to evaluating the learner, we also im-plemented three baseline approaches to comparethe learner against.
The Utterance baseline seg-menter assumes every utterance is a single word.The Monosyllabic baseline segmenter assumes ev-ery syllable is a single word.
The USC segmenterinserts word boundaries between all adjacent syl-lables with primary stress in the corpus.6.1 ResultsThe performance of the learner and baseline seg-menters is given in Table 1.
While the Utterancesegmenter provides expectedly poor performance,the Monosyllabic segmenter sets a relatively highbaseline for the task.
Because of the impoverishedmorphology of English and the short words thattend to be used in child-directed speech, assumingeach syllable is a word proves to be an excellentheuristic.
It is unlikely that this heuristic will per-form as well in other languages.
Because the USCsegmenter only creates segmentation points wherethere are words of adjacent primary stress, it isprone to attaching unstressed monosyllabic func-tion words to content words, causing very low lex-icon precision (13.56%).With both perfect memory and dictionary stressinformation, the learner attains an F-score of86.69%, with precision (83.78%) lower than re-call (89.81%).
First, we consider the effects ofprobabilistic memory on the learner.
In the Dictio-nary Stress condition, using probabilistic memorydecreases Fo by 1.15%, a relatively small impactgiven that with the setting of ?
= 0.05 the learnermust use a word approximately 14 times before itcan retrieve it with 50% reliability and 45 timesbefore it can retrieve it with 90% reliability.
In thefirst iteration over the data set, 17.87% of lexiconlookups for words that have been hypothesized be-fore fail.
The impact on F0 is caused by a drop inrecall, as would be expected for a such a memorymodel.To examine the effect of the ?
parameter forprobabilistic memory on learner performance, weplot the utterance and lexicon F0 after the learneriterates over the corpus once in the Probabilistic0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Memory ParameterF?Scorel l l l l l l l l l ll UtteranceLexiconFigure 2: Learner utterance and lexicon F-scoresafter two iterations when ?
is varied in the Proba-bilistic Memory, Dictionary Stress conditionPerfectMemory,DictionaryStressPerfectMemory,ReducedStressUSC Seg.
114,333 0Initial Sub.
65,800 164,989Final Sub.
5,690 14,813Total 185,823 179,802Table 2: Number of segmentations performed byeach operation: USC Segmentation, Initial Sub-traction, and Final Subtraction.Memory, Dictionary Stress condition.
As Figure 2shows, the choice of ?
has little effect on the ut-terance F0 through most of a broad range from0.01 to 0.9.
Because the setting of ?
determinesthe number of times a word must be hypothesizedbefore it can reliably be recalled, it expectedlyhas a significant effect on lexicon F0.
The selec-tion of ?
= 0.05 for our experiments is thus un-likely to have had any significant bearing on theutterance segmentation performance, although forlower values of ?
precision is favored while forlarger values recall is favored.
Larger values of?
imply the learner is able to recall items afterfewer exposures.
While a larger value of ?
wouldhave yielded higher performance in lexicon per-formance, it also assumes much more about thelearner?s memory capabilities.The Reduced Stress condition also has only a94Utterances LexiconSegmenter Precision Recall F0 Precision Recall F0Utterance 18.61% 4.67% 7.47% 3.57% 30.35% 6.39%Monosyllabic 73.29% 85.44% 78.90% 55.41% 43.88% 48.97%USC 81.06% 61.52% 69.95% 13.56% 66.97% 22.55%Perfect Memory, Dictionary Stress 83.78% 89.81% 86.69% 67.72% 58.60% 62.83%Perfect Memory, Reduced Stress 82.32% 85.81% 84.03% 39.18% 50.08% 43.97%Prob.
Memory, Dictionary Stress 84.05% 87.07% 85.54% 72.34% 30.01% 42.42%Prob.
Memory, Reduced Stress 84.85% 85.24% 85.05% 41.13% 22.91% 29.43%Table 1: Baseline and Learner Performance.
Performance is reported after two iterations over the corpusfor probabilistic memory learners and after a single iteration for all other learners.small impact on utterance segmentation perfor-mance.
This suggests that the USC?s primaryvalue to the learner is in constraining the contentsof the lexicon and identifying words in isolation asgood candidates for the lexicon.
In the ReducedStress condition where the USC is not directly re-sponsible for any segmentations as there are noadjacent primary-stressed syllables, the learner re-lies much more heavily on subtractive techniques.Table 2 gives the number of segmentations per-formed using each segmentation operation.
Thetotal number of segmentations is very similar be-tween the Dictionary and Reduced Stress condi-tions, but because USC Segmentation is not effec-tive on Reduced Stress input, Initial and Final Sub-traction are used much more heavily.7 DiscussionThe design of the segmenter presented here sug-gests that both the quality of memory and thestructural purity of the input would be critical fac-tors in the learner?s success.
Our results suggest,however, that using probabilistic memory and aless idealized version of stress in natural languagehave little impact on the performance of the pre-sented learner.
They do cause the learner to learnmuch more slowly, causing the learner to need tobe presented with more material and resulting inworse performance in the lexicon evaluation.
Butthis slower learning is unlikely to be a concern fora child learner who would be exposed to muchlarger amounts of data than the corpora here pro-vide.Cognitive literature suggests that limited mem-ory during learning may be essential to a learner inits early stages (Elman, 1993).
But we do not seeany notable improvement as a result of the prob-abilistic memory model used in our experiments,although the learner does do better in the ReducedStress condition with Probabilistic Memory thanPerfect Memory.
This should not be interpretedas a negative result as we only analyze a singlelearner and memory model.
Adding decay to themodel such that among words of equal frequencythose that have not been used in segmentation re-cently are less likely to be remembered may besufficient to create the desired effect.The success of this learner suggests that thetype of ?bootstrapping?
approaches can succeedin word segmentation.
The learner presented usesUSC to identify utterances that are likely to belone words, seeding the lexicon with initial infor-mation.
Even if these first items in the lexicon areof relatively low purity, often combining functionwords and content words into one, the learner isable to expand its lexicon by using these hypothe-sized words to segment new input.
As the learnersegments more, these hypotheses become more re-liable, allowing the learner to build a lexicon ofgood quality.The subtraction approaches presented in thiswork provide a basic algorithm for to handlingsegmentation of incoming data in an online fash-ion.
The subtractive heuristics used here are ofcourse not guaranteed to result in a perfect seg-mentation even with a perfect lexicon; they arepresented to show how a simple model of pro-cessing incoming data can be paired with struc-tural constraints on the hypothesis space to learnword segmentation in a computationally efficientand cognitively plausible online fashion.8 ConclusionsThe learner?s strong performance using minimalcomputational resources and unreliable memorysuggest that simple learners can succeed in un-95supervised tasks as long as they take advantageof domain-specific knowledge to constrain the hy-pothesis space.
Our results show that, even in ad-versarial conditions, structural constraints remainpowerful tools for simple learning algorithms indifficult tasks.Future work in this area should focus on learn-ers that can take advantage of the benefits of aprobabilistic lexicon and memory models suitedto them.
Also, a more complex model of the typeof stress variation present in natural speech wouldhelp better determine a learner that uses USC?sability to handle realistic variation in the input.Our model of stress reduction is a worst-case sce-nario for USC segmentation but is unlikely to bean accurate model of real speech.
Future workshould adopt a more naturalistic model to deter-mine whether the robustness found in our resultsholds true in more realistic stress permutations.AcknowledgementsWe thank Kyle Gorman, Josef Fruehwald, andDan Swingley for their helpful discussions regard-ing this work.
We are grateful to and Mitch Mar-cus and Jana Beck for their feedback on earlierversions of this paper.ReferencesJ.R.
Anderson, D. Bothell, C. Lebiere, and M. Matessa.1998.
An integrated theory of list memory.
Journalof Memory and Language, 38(4):341?380.R.
Bijeljac-Babic, J. Bertoncini, and J. Mehler.
1993.How do 4-day-old infants categorize multisyllabicutterances?
Developmental Psychology, 29:711?711.M.R.
Brent and T.A.
Cartwright.
1996.
Distributionalregularity and phonotactic constraints are useful forsegmentation.
Cognition, 61(1-2):93?125.M.R.
Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word discov-ery.
Machine Learning, 34(1):71?105.R.
Brown.
1973.
A First Language: The EarlyStages.
Harvard Univ.
Press, Cambridge, Mas-sachusetts 02138.J.L.
Elman.
1993.
Learning and development in neuralnetworks: The importance of starting small.
Cogni-tion, 48(1):71?99.G.
Gillund and R.M.
Shiffrin.
1984.
A retrieval modelfor both recognition and recall.
Psychological Re-view, 91(1):1?67.E.M.
Gold.
1967.
Language identification in the limit.Information and control, 10(5):447?474.S.
Goldwater, T. Griffiths, and M. Johnson.
2006.
In-terpolating between types and tokens by estimatingpower-law generators.
Advances in Neural Informa-tion Processing Systems, 18:459.S.
Goldwater, T.L.
Griffiths, and M. Johnson.
2009.A Bayesian framework for word segmentation: Ex-ploring the effects of context.
Cognition.M.
Halle and J.R. Vergnaud.
1987.
An essay on stress.MIT Press.M.
Johnson and S. Goldwater.
2009.
Improving non-parameteric Bayesian inference: experiments on un-supervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 317?325.
Association forComputational Linguistics.P.W.
Jusczyk, D.M.
Houston, and M. Newsome.
1999.The Beginnings of Word Segmentation in English-Learning Infants.
Cognitive Psychology, 39(3-4):159?207.P.W.
Jusczyk.
1999.
How infants begin to extractwords from speech.
Trends in Cognitive Sciences,3(9):323?328.M.
Liberman and A.
Prince.
1977.
On stress and lin-guistic rhythm.
Linguistic Inquiry, 8(2):249?336.B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum Associates.G.F.
Marcus, S. Pinker, M. Ullman, M. Hollander, T.J.Rosen, F. Xu, and H. Clahsen.
1992.
Overregular-ization in language acquisition.
Monographs of theSociety for Research in Child Development, 57(4).S.L.
Mattys and P.W.
Jusczyk.
2001.
Phonotactic cuesfor segmentation of fluent speech by infants.
Cogni-tion, 78(2):91?121.K.H.
Onishi, K.E.
Chambers, and C. Fisher.
2002.Learning phonotactic constraints from brief auditoryexperience.
Cognition, 83(1):B13?B23.J.R.
Saffran, R.N.
Aslin, and E.L. Newport.
1996a.Statistical Learning by 8-month-old Infants.
Sci-ence, 274(5294):1926.J.R.
Saffran, E.L. Newport, and R.N.
Aslin.
1996b.Word Segmentation: The Role of DistributionalCues.
Journal of Memory and Language,35(4):606?621.D.
Swingley.
2005.
Statistical clustering and the con-tents of the infant vocabulary.
Cognitive Psychol-ogy, 50(1):86?132.LG Valiant.
1984.
A theory of the learnable.
Commu-nications of the ACM, 27(11):1142.96D.
Van Kuijk and L. Boves.
1999.
Acoustic char-acteristics of lexical stress in continuous telephonespeech.
Speech Communication, 27(2):95?111.V.N.
Vapnik.
2000.
The nature of statistical learningtheory.
Springer.A.
Venkataraman.
2001.
A statistical model for worddiscovery in transcribed speech.
ComputationalLinguistics, 27(3):351?372.R.L.
Weide.
1998.
The Carnegie Mellon PronouncingDictionary [cmudict.
0.6].C.D.
Yang.
2004.
Universal Grammar, statistics orboth?
Trends in Cognitive Sciences, 8(10):451?456.97
