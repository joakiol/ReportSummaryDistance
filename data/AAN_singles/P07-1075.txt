Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 592?599,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Multi-resolution Framework for Information Extraction from Free TextMstislav Maslennikov and Tat-Seng ChuaDepartment of Computer ScienceNational University of Singapore{maslenni,chuats}@comp.nus.edu.sgAbstractExtraction of relations between entities isan important part of Information Extractionon free text.
Previous methods are mostlybased on statistical correlation and depend-ency relations between entities.
This paperre-examines the problem at the multi-resolution layers of phrase, clause and sen-tence using dependency and discourse rela-tions.
Our multi-resolution frameworkARE (Anchor and Relation) uses clausalrelations in 2 ways: 1) to filter noisy de-pendency paths; and 2) to increase reliabil-ity of dependency path extraction.
The re-sulting system outperforms the previousapproaches by 3%, 7%, 4% on MUC4,MUC6 and ACE RDC domains respec-tively.1 IntroductionInformation Extraction (IE) is the task of identify-ing information in texts and converting it into apredefined format.
The possible types of informa-tion include entities, relations or events.
In thispaper, we follow the IE tasks as defined by theconferences MUC4, MUC6 and ACE RDC: slot-based extraction, template filling and relation ex-traction, respectively.Previous approaches to IE relied on co-occurrence (Xiao et al, 2004) and dependency(Zhang et al, 2006) relations between entities.These relations enable us to make reliable extrac-tion of correct entities/relations at the level of asingle clause.
However, Maslennikov et al (2006)reported that the increase of relation path lengthwill lead to considerable decrease in performance.In most cases, this decrease in performance occursbecause entities may belong to different clauses.Since clauses in a sentence are connected byclausal relations (Halliday and Hasan, 1976), it isthus important to perform discourse analysis of asentence.Discourse analysis may contribute to IE in sev-eral ways.
First, Taboada and Mann (2005) re-ported that discourse analysis helps to decomposelong sentences into clauses.
Therefore, it helps todistinguish relevant clauses from non-relevantones.
Second, Miltsakaki (2003) stated that entitiesin subordinate clauses are less salient.
Third, theknowledge of textual structure helps to interpretthe meaning of entities in a text (Grosz and Sidner1986).
As an example, consider the sentences?ABC Co. appointed a new chairman.
Addition-ally, the current CEO was retired?.
The word ?ad-ditionally?
connects the event in the second sen-tence to the entity ?ABC Co.?
in the first sentence.Fourth, Moens and De Busser (2002) reported thatdiscourse segments tend to be in a fixed order forstructured texts such as court decisions or news.Hence, analysis of discourse order may reduce thevariability of possible relations between entities.To model these factors, we propose a multi-resolution framework ARE that integrates bothdiscourse and dependency relations at 2 levels.ARE aims to filter noisy dependency relationsfrom training and support their evaluation withdiscourse relations between entities.
Additionally,we encode semantic roles of entities in order toutilize semantic relations.
Evaluations on MUC4,MUC6 and ACE RDC 2003 corpora demonstratesthat our approach outperforms the state-of-art sys-tems mainly due to modeling of discourse rela-tions.The contribution of this paper is in applying dis-course relations to supplement dependency rela-tions in a multi-resolution framework for IE.
The592framework enables us to connect entities in differ-ent clauses and thus improve the performance onlong-distance dependency paths.Section 2 describes related work, while Section3 presents our proposed framework, including theextraction of anchor cues and various types of rela-tions, integration of extracted relations, and com-plexity classification.
Section 4 describes our ex-perimental results, with the analysis of results inSection 5.
Section 6 concludes the paper.2 Related workRecent work in IE focuses on relation-based, se-mantic parsing-based and discourse-based ap-proaches.
Several recent research efforts werebased on modeling relations between entities.
Cu-lotta and Sorensen (2004) extracted relationshipsusing dependency-based kernel trees in SupportVector Machines (SVM).
They achieved an F1-measure of 63% in relation detection.
The authorsreported that the primary source of mistakes comesfrom the heterogeneous nature of non-relation in-stances.
One possible direction to tackle this prob-lem is to carry out further relationship classifica-tion.
Maslennikov et al (2006) classified relationpath between candidate entities into simple, aver-age and hard cases.
This classification is based onthe length of connecting path in dependency parsetree.
They reported that dependency relations arenot reliable for the hard cases, which, in our opin-ion, need the extraction of discourse relations tosupplement dependency relation paths.Surdeanu et al (2003) applied semantic parsingto capture the predicate-argument sentence struc-ture.
They suggested that semantic parsing is use-ful to capture verb arguments, which may be con-nected by long-distance dependency paths.
How-ever, current semantic parsers such as the ASSERTare not able to recognize support verb construc-tions such as ?X conducted an attack on Y?
underthe verb frame ?attack?
(Pradhan et al 2004).Hence, many useful predicate-argument structureswill be missed.
Moreover, semantic parsing be-longs to the intra-clausal level of sentence analysis,which, as in the dependency case, will need thesupport of discourse analysis to bridge inter-clausalrelations.Webber et al (2002) reported that discoursestructure helps to extract anaphoric relations.
How-ever, their set of grammatical rules is heuristic.
Ourtask needs construction of an automated approachto be portable across several domains.
Cimiano etal.
(2005) employed a discourse-based analysis forIE.
However, their approach requires a predefineddomain-dependent ontology in the format of ex-tended logical description grammar as described byCimiano and Reely (2003).
Moreover, they useddiscourse relations between events, whereas in ourapproach, discourse relations connect entities.3 Motivation for using discourse relationsOur method is based on Rhetorical Structure The-ory (RST) by Taboada and Mann (2005).
RSTsplits the texts into 2 parts: a) nuclei, the most im-portant parts of texts; and b) satellites, the secon-dary parts.
We can often remove satellites withoutlosing the meaning of text.
Both nuclei and satel-lites are connected with discourse relations in ahierarchical structure.
In our work, we use 16classes of discourse relations between clauses: At-tribution, Background, Cause, Comparison, Condi-tion, Contrast, Elaboration, Enablement, Evalua-tion, Explanation, Joint, Manner-Means, Topic-Comment, Summary, Temporal, Topic-Change.The additional 3 relations impose a tree structure:textual-organization, span and same-unit.
All thediscourse relation classes are potentially useful,since they encode some knowledge about textualstructure.
Therefore, we decide to include all ofthem in the learning process to learn patterns withbest possible performance.We consider two main rationales for utilizingdiscourse relations to IE.
First, discourse relationshelp to narrow down the search space to the levelof a single clause.
For example, the sentence?
[<Soc-A1>Trudeau</>'s <Soc-A2>son</> toldeveryone], [their prime minister was his father],[who took him to a secret base in the arctic] [andlet him peek through a window].?
contains 4clauses and 7 anchor cues (key phrases) for thetype Social, which leads to 21 possible variants.Splitting this sentence into clauses reduces thecombinations to 4 possible variants.
Additionally,this reduction eliminates the long and noisy de-pendency paths.Second, discourse analysis enables us to connectentities in different clauses with clausal relations.As an example, we consider a sentence ?It?s a darkcomedy about a boy named <AT-A1>Marshal</>played by Amourie Kats who discovers all kinds of593on and scary things going on in <AT-A2>a seem-ingly quiet little town</>?.
In this example, weneed to extract the relation ?At?
between the enti-ties ?Marshal?
and ?a seemingly quiet little town?.The discourse structure of this sentence is given in.Figure 1Figure 1.
Example of discourse parsingThe discourse path ?Marshal <-elaboration- _<-span- _ -elaboration-> _ -elaboration-> town?is relatively short and captures the necessary rela-tions.
At the same time, prediction based on de-pendency path ?Marshal <?obj- _ <-i- _ <-fc- _<-pnmod- _ <-pred- _ <-i- _ <-null- _ -null-> _ -rel-> _ -i-> _ -mod-> _ -pcomp-n-> town?
is un-reliable, since the relation path is long.
Thus, it isimportant to rely on discourse analysis in this ex-ample.
In addition, we need to evaluate both thescore and reliability of prediction by relation pathof each type.4 Anchors and RelationsIn this section, we define the key components thatwe use in ARE: anchors, relation types and generalarchitecture of our system.
Some of these compo-nents are also presented in detail in our previouswork (Maslennikov et al, 2006).4.1 AnchorsThe first task in IE is to identify candidate phrases(which we call anchor or anchor cue) of a pre-defined type  (anchor  type) to fill a desired slot inan  IE  template.
The  example  anchor  for  the  phrase?Marshal?
is shown in Figure 2.Given a training set of sentences,we extract the anchor cues ACj =[A1, ?, ANanch] of type Cj usingthe procedures described inMaslennikov et al (2006).
Thelinguistic features of these an-chors for the anchor types of Per-petrator, Action, Victim and Target for the MUC4domain are given in Table 1.AnchortypesFeaturePerpetrator_Cue(A)Action_Cue(D)Victim_Cue(A)Target_Cue(A)Lexical(Head noun)terrorists,individuals,Soldiersattacked,murder,MassacreMayor,general,priestsbridge,house,MinistryPart-of-Speech Noun Verb Noun NounNamed Enti-tiesSoldiers(PERSON)- Jesuit priests(PERSON)WTC(OBJECT)Synonyms Synset 130, 166 Synset 22 Synset 68 Synset 71Concept Class ID 2, 3 ID 9  ID 22, 43 ID 61, 48Co-referencedentityHe -> terrorist,soldier- They ->peasants-Clausal type NucleusSatelliteNucleus,SatelliteNucleus,SatelliteNucleus,SatelliteArgument type Arg0 , Arg1RootTarget, -,ArgM-MNRArg0 ,  Arg1 Arg1 , ArgM-MNRTable 1.
Linguistic features for anchor extractionGiven an input phrase P from a test sentence, weneed to classify if the phrase belongs to anchor cuetype Cj.
We calculate the entity score as:Entity_Score(P) =?
?
i * Feature_Scorei(P,Cj) (1)where Feature_Score(P,Cj) is a score function fora particular linguistic feature representation of typeCj, and ?
i is the corresponding weight for that rep-resentation in the overall entity score.
The weightsare learned automatically using Expectation Maxi-mization (Dempster et al, 1977).
The Fea-ture_Scorei(P,Cj) is estimated from the training setas the number of slots containing the correct fea-ture representation type versus all the slots:Feature_Scorei(P,Cj) = #(positive slots) / #(all slots) (2)We classify the phrase P as belonging to an anchortype Cj when its Entity_score(P) is above an em-pirically determined threshold ?.
We refer to thisanchor as Aj.
We allow a phrase to belong to mul-tiple anchor types and hence the anchors alone arenot enough for filling templates.4.2 RelationsTo resolve the correct filling of phrase P of type Ciin a desired slot in the template, we need to con-sider the relations between multiple candidatephrases of related slots.
To do so, we consider sev-eral types of relations between anchors: discourse,dependency and semantic relations.
These relationscapture the interactions between anchors and aretherefore useful for tackling the paraphrasing andalignment problems (Maslennikov et al, 2006).Given 2 anchors Ai and Aj of anchor types Ci andCj, we consider a relation Pathl = [Ai, Rel1,?,Reln, Aj] between them, such that there are no an-chors between Ai and Aj.
Additionally, we assumethat the relations between anchors are representedin the form of a tree Tl, where l = {s, c, d} refers toSatellitewho discovers all kinds of on andscary things going on in a seem-ingly quiet little town.NucleusIt's a darkcomedyabout a boySatellitenamed Mar-shalNucleusplayed byAmourie KatsNucleus Satellitespan elaborationspan elaboration elaboration spanFigure 2.
Exam-ple of anchorAnchor AiMarshalpos_NNPlist_personWordCand_AtArg1Minipar_objArg2Spade_Satellite594discourse, dependency and semantic relation typesrespectively.
We describe the nodes and edges ofTl separately for each type, because their represen-tations are different:1) The nodes of discourse tree Tc consist of clauses[Clause1, ?, ClauseNcl]; and their relation edgesare obtained from the Spade system described inSoricut and Marcu (2003).
This system performsRST-based parsing at the sentence level.
The re-ported accuracy of Spade is 49% on the RST-DTcorpus.
To obtain a clausal path, we map eachanchor Ai to its clause in Spade.
If anchors Aiand Aj belong to the same clause, we assignthem the relation same-clause.es.2) The nodes of dependency tree Td consist ofwords in sentences; and their relation edges areobtained from Minipar by Lin (1997).
Lin(1997) reported a parsing performance of Preci-sion = 88.5% and Recall = 78.6% on the SU-SANNE corpus.3) The nodes of semantic tree Ts consist of argu-ments [Arg0, ?, ArgNarg] and targets [Target1,?, TargetNtarg].
Both arguments and targets areobtained from the ASSERT parser developed byPradhan (2004).
The reported performance ofASSERT is F1=83.8% on the identification andclassification task for all arguments, evaluatedusing PropBank and AQUAINT as the trainingand testing corpora, respectively.
Since the rela-tion edges have a form Targetk -> Argl, the rela-tion path in semantic frame contains only a sin-gle relation.
Therefore, we encode semantic rela-tions as part of the anchor features.In later parts of this paper, we consider only dis-course and dependency relation paths Pathl, wherel={c, d}.Figure 3.
Architecture of the system4.3 Architecture of ARE systemIn order to perform IE, it is important to extractcandidate entities (anchors) of appropriate anchortypes, evaluate the relationships between them,further evaluate all possible candidate templates,and output the final template.
For the case of rela-tion extraction task, the final templates are thesame as an extracted binary relation.
The overallarchitecture of ARE is given in Figure 3.The focus of this paper is in applying discourserelations for binary relationship evaluation.5 Overall approachIn this section, we describe our relation-based ap-proach to IE.
We start with the evaluation of rela-tion paths (single relation ranking, relation pathranking) to assess the suitability of their anchors asentities to template slots.
Here we want to evaluategiven a single relation or relation path, whether thetwo anchors are correct in filling the appropriateslots in a template.
This is followed by the integra-tion of relation paths and evaluation of templates.5.1 Evaluation of relation pathIn the first stage, we evaluate from training datathe relevance of relation path Pathl = [Ai, Rel1,?,Reln, Aj] between candidate anchors Ai and Aj oftypes Ci and Cj.
We divide this task into 2 steps.The first step ranks each single relation Relk ?Pathl; while the second step combines the evalua-tions of Relk to rank the whole relation path Pathl.Single relation rankingLet Seti and Setj be the set of linguistic features ofanchors Ai and Aj respectively.
To evaluate Relk,we consider 2 characteristics: (1) the direction ofrelation Relk as encoded in the tree structure; and(2) the linguistic features, Seti and Setj, of anchorsAi and Aj.
We need to construct multiple singlerelation classifiers, one for each anchor pair oftypes Ci and Cj, to evaluate the relevance of Relkwith respect to these 2 anchor typPreprocessing Corpus(a) Construction of classifiers.
The training datato each classifier consists of anchor pairs of typesCi and Cj extracted from the training corpus.
Weuse these anchor pairs to construct each classifierin four stages.
First, we compose the set of possi-ble patterns in the form P+ = { Pm = <Si ?Rel->Sj> | Si ?
Seti , Sj ?
Setj }.
The construction of PmAnchorevaluationTemplatesAnchor NEsTemplateevaluationSentencesBinary relationshipevaluationCandidatetemplates595conforms to the 2 characteristics given above.Figure 4 illustrates several discourse and depend-ency patterns of P+ constructed from a sample sen-tence.Figure 4.
Examples of discourse and dependency patternsSecond, we identify the candidate anchor A,whose type matches slot C in a template.
Third, wefind the correct patterns for the following 2 cases:1) Ai, Aj are of correct anchor types; and 2) Ai is anaction anchor, while Aj is a correct anchor.
Anyother patterns are considered as incorrect.
We notethat the discourse and dependency paths betweenanchors Ai and Aj are either correct or wrong si-multaneously.Fourth, we evaluate the relevance of each pat-tern Pm ?
P+.
Given the training set, let PairSetmbe the set of anchor pairs extracted by Pm; andPairSet+(Ci, Cj) be the set of correct anchor pairsof types Ci, Cj.
We evaluate both precision andrecall of Pm as|||||),(||)(mjimm PairSetCCPairsSetPairSetPrecisionP|=+?
(3)||),(|||),(||)(jijimm CCPairsSetCCPairsSetPairSetPecallR ++ |= ?
(4)These values are stored and used in the trainingmodel for use during testing.
(b) Evaluation of relation.
Here we want toevaluate whether relation InputRel belongs to apath between anchors InputAi and InputAj.
Weemploy the constructed classifier for the anchortypes InputCi and InputCj in 2 stages.
First, wefind a subset P(0) = { Pm = <Si ?InputRel-> Sj> ?P+  | Si ?
InputSeti, Sj ?
InputSetj } of applicablepatterns.
Second, we utilize P(0) to find the patternPm(0) with maximal precision:Precision(Pm(0)) = argmaxPm?P(0) Precision (Pm) (5)A problem arises if Pm(0) is evaluated only on asmall amount of training instances.
For example,we noticed that patterns that cover 1 or 2 instancesmay lead to Precision=1, whereas on the testingcorpus their accuracy becomes less than 50%.Therefore, it is important to additionally considerthe recall parameter of Pm(0).Relation path rankingIn this section, we want to evaluate relation pathconnecting template slots Ci and Cj.
We do thisindependently for each relation of type discourseand dependency.
Let Recallk and Precisionk be therecall  and precision values of Relk in Path = [Ai,Rel1,?, Reln, Aj], both obtained from the previousstep.
First, we calculate the average recall of theinvolved relations:W = (1/LengthPath) * ?Relk?Path Recallk (6)W gives the average recall of the involved rela-tions and can be used as a measure of reliability ofthe relation Path.
Next, we compute a combinedscore of average Precisionk weighted by Recallk:Score = 1/(W*LengthPath)*?Relk?Path Recallk*Precisionk (7)We use all Precisionk values in the path here, be-cause omitting a single relation may turn a correctpath into the wrong one, or vice versa.
The com-bined score value is used as a ranking of the rela-tion path.
Experiments show that we need to givepriority to scores with higher reliability W. Hencewe use (W, Score) to evaluate each Path.5.2 Integration of different relation pathtypesThe purpose of this stage is to integrate the evalua-tions for different types of relation paths.
The inputto this stage consists of evaluated relation pathsPathC and PathD for discourse and dependencyrelations respectively.
Let (Wl, Scorel) be anevaluation for Pathl, l ?
[c, d].
We first define anintegral path PathI between Ai and Aj as: 1) PathIis enabled if at least one of Pathl, l ?
[c, d], is en-abled; and 2) PathI is correct if at least one ofPathl is correct.
To evaluate PathI, we consider theaverage recall Wl of each Pathl, because Wl esti-elaborationobjAnchor Ajtownpos_NNCand_AtArg2Minipar_pcompnArgM-LocSpade_SatelliteAnchor AiMarshalpos_NNPlist_personWordCand_AtArg1Minipar_objArg2Spade Satellitepcomp-nfcspanDiscourse pathDependency pathielaborationInput sentenceMarshal?
named <At-A1> </> played by Amourie Kats who discovers all kindsof on and scary things going on in <At-A2>Dependency patternsMinipar_obj <?i- ArgM-LocMinipar_obj <?obj- ArgM-LocMinipar_obj ?pcompn-> Minipar_pcompnMinipar_obj ?mod-> Minipar_pcompn?a seemingly quiet little town</> ...elaborationpnmodpred inullnullreli modDiscourse patternslist_personWord <?elaboration- pos_NNlist_personWord ?elaboration-> townlist_personWord <?span- townlist_personWord <?elaboration- town?596mates the reliability of Scorel.
We define aweighted average for Pathl as:WI = WC + WD (8)ScoreI = 1/WI * ?
l  Wl*Scorel (9)Next, we want to determine the threshold scoreScoreIO above which ScoreI is acceptable.
Thisscore may be found by analyzing the integral pathson the training corpus.
Let SI = { PathI } be the setof integral paths between anchors Ai and Aj on thetraining set.
Among the paths in SI, we need to de-fine a set function SI(X) = { PathI | ScoreI(PathI)?
X } and find the optimal threshold for X.
We findthe optimal threshold based on F1-measure, be-cause precision and recall are equally important inIE.
Let SI(X)+ ?
SI(X) and S(X)+ ?
S(X) be sets ofcorrect path extractions.
Let FI(X) be F1-measureof SI(X):||)(||||)(||)(XSXSXPIII+=  (10)||)(||||)(||)( ++=XSXSXR II(11))()()(*)(*2)(XRXPXRXPXFIIIII +=(12)Based on the computed values FI(X) for each X onthe training data, we determine the optimal thresh-old as Score  = argmax  F  (X)IO X I , which corre-sponds to the maximal expected F1-measure ofanchor pair Ai and Aj.5.3 Evaluation of templatesAt this stage, we have a set of accepted integralrelation paths between any anchor pair Ai and Aj.The next task is to merge appropriate set of an-chors into candidate templates.
Here we follow themethodology of Maslennikov et al (2006).
Foreach sentence, we compose a set of candidate tem-plates T using the extracted relation paths betweeneach Ai and Aj.
To evaluate each template Ti?T,we combine the integral scores from relation pathsbetween its anchors Ai and Aj into the overall Rela-tion_ScoreT:MAAScoreTScoreelationR KjijiIiT?
?
?= ,1 ),()(_  (13)where K is the number of extracted slots, M is thenumber of extracted relation paths between an-chors Ai and Aj, and ScoreI(Ai, Aj) is obtainedfrom Equation (9).Next, we calculate the extracted entity scorebased on the scores of all the anchors in Ti:?
?
?= Kk kiT KAScoreEntityTScoreEntity 1 /)(_)(_  (14)where Entity_Score(Ai) is taken from Equation (1).Finally, we obtain the combined evaluation for atemplate:ScoreT(Ti) = (1- ?)
* Entity_ScoreT (Ti) +?
* Relation_ScoreT (Ti) (15)where ?
is a predefined constant.In order to decide whether the template Tishould be accepted or rejected, we need to deter-mine a threshold ScoreTO from the training data.
Ifanchors of a candidate template match slots in acorrect template, we consider the candidate tem-plate as correct.
Let TrainT = { Ti }  be the set ofcandidate templates extracted from the trainingdata, TrainT+ ?
TrainT be the subset of correctcandidate templates, and TotalT+ be the total set ofcorrect templates in the training data.
Also, letTrainT(X) = { Ti | ScoreT(Ti) ?
X, Ti ?
TrainT } bethe set of candidate templates with score above Xand TrainT+(X) ?
TrainT(X) be the subset of cor-rect candidate templates.
We define the measuresof precision, recall and F1 as follows:||)(||||)(||)(XTrainTXTrainTXPT+=  (16) ||||||)(||)( ++=TotalTXTrainTXRT (17))()()()(*2)(XRXPXRXPXFTTTTT +=(18)Since the performance in IE is measured in F1-measure, an appropriate threshold to be used forthe most prominent candidate templates is:ScoreTO = argmaxX FT (X) (19)The value ScoreTO is used as a training model.During testing, we accept a candidate template In-putTi if ScoreT(InputTi) > Sco Ore .
TAs an additional remark, we note that domainsMUC4, MUC6 and ACE RDC 2003 are signifi-cantly different in the evaluation methodology forthe candidate templates.
While the performance ofthe MUC4 domain is measured for each slot indi-vidually; the MUC6 task measures the perform-ance on the extracted templates; and the ACE RDC2003 task evaluates performance on the matchingrelations.
To overcome these differences, we con-struct candidate templates for all the domains andmeasure the required type of performance for eachdomain.
Our candidate templates for the ACERDC 2003 task consist of only 2 slots, which cor-respond to entities of the correct relations.5976 Experimental resultsWe carry out our experiments on 3 domains:MUC4 (Terrorism), MUC6 (Management Succes-sion), and ACE-Relation Detection and Characteri-zation (2003).
The MUC4 corpus contains 1,300documents as training set and 200 documents(TST3 and TST4) as official testing set.
We used amodified version of the MUC6 corpus describedby Soderland (1999).
This version includes 599documents as training set and 100 documents astesting set.
Following the methodology of Zhang etal.
(2006), we use only the English portion of ACERDC 2003 training data.
We used 97 documentsfor testing and the remaining 155 documents fortraining.
Our task is to extract 5 major relationtypes and 24 subtypes.Case (%) P R F1GRID 52% 62% 57%Riloff?05 46% 51% 48%ARE (2006) 58% 61% 60%ARE 65% 61% 63%Table 2.
Results on MUC4To compare the results on the terrorism domainin MUC4, we choose the recent state-of-art sys-tems GRID by Xiao et al (2004), Riloff et al(2005) and ARE (2006) by Maslennikov et al(2006) which does not utilize discourse and seman-tic relations.
The comparative results are given inTable 2.
It shows that our enhanced ARE results in3% improvement in F1 measure over ARE (2006)that does not use clausal relations.
The improve-ment was due to the use of discourse relations onlong paths, such as ?X distributed leaflets claimingresponsibility for murder of Y?.
At the same time,for many instances, it would be useful to store theextracted anchors for another round of learning.For example, the extracted features of discoursepattern ?murder ?same_clause-> HUM_PERSON?may boost the score for patterns that correspond torelation path ?X <-span- _ -Elaboration->  mur-der?.
In this way, high-precision patterns will sup-port the refinement of patterns with average recalland low precision.
This observation is similar tothat described in Ciravegna?s work on (LP)2(Ciravegna 2001).Case (%) P R F1Chieu et al?02 75% 49% 59%ARE (2006) 73% 58% 65%ARE 73% 70% 72%Table 3.
Results on MUC6Next, we present the performance of our systemon MUC6 corpus (Management Succession) asshown in Table 3.
The improvement of 7% in F1 ismainly due to the filtering of irrelevant depend-ency relations.
Additionally, we noticed that 22%of testing sentences contain 2 answer templates,and entities in many of such templates are inter-twined.
One example is the sentence ?Mr.
Bronc-zek who is 39 years old succeeds Kenneth Newell55 who was named to the new post of senior vicepresident?, which refers to 2 positions.
We there-fore we need to extract 2 templates ?PersonIn:Bronczek, PersonOut: Newell?
and ?PersonIn:Newell, Post: senior vice president?.
The discourseanalysis is useful to extract the second template,while rejecting another long-distance template?PersonIn: Bronczek, PersonOut: Newell, Post:seniour vice president?.
Another remark is that itis important to assign 2 anchors of?Cand_PersonIn?
and ?Cand_PersonOut?
for thephrase ?Kenneth Newell?.The characteristic of the ACE corpus is that itcontains a large amount of variations, while only2% of possible dependency paths are correct.
Sincemany of the relations occur only at the level of sin-gle clause (for example, most instances of relationAt), the discourse analysis is used to eliminatelong-distance dependency paths.
It allows us tosignificantly decrease the dimensionality of theproblem.
We noticed that 38% of relation paths inACE contain a single relation, 28% contain 2 rela-tions and 34% contain ?
3 relations.
For the caseof  ?
3 relations, the analysis of dependency pathsalone is not sufficient to eliminate the unreliablepaths.
Our results for general types and specificsubtypes are presented in Tables 6 and 7, respec-tively.Case (%) P R F1Zhang et al?06 77% 65% 70%ARE 79% 66% 73%Table 4.
Results on ACE RDC?03, general typesBased on our results in Table 4, discourse anddependency relations support each other in differ-ent situations.
We also notice that multiple in-stances require modeling of entities in the path.Thus, in our future work we need to enrich thesearch space for relation patterns.
This observationcorresponds to that reported in Zhang et al (2006).Discourse parsing is very important to reducethe amount of variations for specific types on ACE598RDC?03, as there are 48 possible anchor types.Case (%) P R F1Zhang et al?06 64% 51% 57%ARE 67% 54% 61%Table 5.
Results on ACE RDC?03, specific typesThe relatively small improvement of results inTable 5 may be attributed to the following reasons:1) it is important to model the commonality rela-tions, as was done by Zhou et al (2006); and 2)our relation paths do not encode entities.
This isdifferent from Zhang et al (2006), who were usingentities in their subtrees.Overall, the results indicate that the use of dis-course relations leads to improvement over thestate-of-art systems.7 ConclusionWe presented a framework that permits the inte-gration of discourse relations with dependency re-lations.
Different from previous works, we tried touse the information about sentence structure basedon discourse analysis.
Consequently, our systemimproves the performance in comparison with thestate-of-art IE systems.
Another advantage of ourapproach is in using domain-independent parsersand features.
Therefore, ARE may be easily port-able into new domains.Currently, we explored only 2 types of relationpaths: dependency and discourse.
For future re-search, we plan to integrate more relations in ourmulti-resolution framework.ReferencesP.
Cimiano and U. Reyle.
2003.
Ontology-based semanticconstruction, underspecification and disambiguation.
InProc of the Prospects and Advances in the Syntax-Semantic Interface Workshop.P.
Cimiano, U. Reyle and J. Saric.
2005.
Ontology-drivendiscourse analysis for information extraction.
Data &Knowledge Engineering, 55(1):59-83.H.L.
Chieu and H.T.
Ng.
2002.
A Maximum Entropy Ap-proach to Information Extraction from Semi-Structuredand Free Text.
In Proc of AAAI-2002.F.
Ciravegna.
2001.
Adaptive Information Extraction fromText by Rule Induction and Generalization.
In Proc ofIJCAI-2001.A.
Culotta and J. Sorensen J.
2004.
Dependency tree ker-nels for relation extraction.
In Proc of ACL-2004.A.
Dempster, N. Laird, and D. Rubin.
1977.
Maximumlikelihood from incomplete data via the EM algorithm.Journal of the Royal Statistical Society B, 39(1):1?38.B.
Grosz and C. Sidner.
1986.
Attention, Intentions andthe Structure of Discourse.
Computational Linguistics,12(3):175-204.M.
Halliday and R. Hasan.
1976.
Cohesion in English.Longman, London.D.
Lin.
1997.
Dependency-based Evaluation of Minipar.
InWorkshop on the Evaluation of Parsing systems.M.
Maslennikov, H.K.
Goh and T.S.
Chua.
2006.
ARE:Instance Splitting Strategies for Dependency Relation-based Information Extraction.
In Proc of ACL-2006.E.
Miltsakaki.
2003.
The Syntax-Discourse Interface: Ef-fects of the Main-Subordinate Distinction on AttentionStructure.
PhD thesis.M.F.
Moens and R. De Busser.
2002.
First steps in buildinga model for the retrieval of court decisions.
InternationalJournal of Human-Computer Studies, 57(5):429-446.S.
Pradhan, W. Ward, K. Hacioglu, J. Martin and D. Juraf-sky.
2004.
Shallow Semantic Parsing using SupportVector Machines.
In Proc of HLT/NAACL-2004.E.
Riloff, J. Wiebe, and W. Phillips.
2005.
Exploiting Sub-jectivity Classification to Improve Information Extrac-tion.
In Proc of AAAI-2005.S.
Soderland.
1999.
Learning Information Extraction Rulesfor Semi-Structured and Free Text.
Machine Learning,34:233-272.R.
Soricut and D. Marcu.
2003.
Sentence Level DiscourseParsing using Syntactic and Lexical Information.
InProc of HLT/NAACL.M.
Surdeanu, S. Harabagiu, J. Williams, P. Aarseth.
2003.Using Predicate Arguments Structures for InformationExtraction.
In Proc of ACL-2003.M.
Taboada and W. Mann.
2005.
Applications of Rhetori-cal Structure Theory.
Discourse studies, 8(4).B.
Webber, M. Stone, A. Joshi and A. Knott.
2002.Anaphora and Discourse Structure.
Computational Lin-guistics, 29(4).J.
Xiao, T.S.
Chua and H. Cui.
2004.
Cascading Use ofSoft and Hard Matching Pattern Rules for Weakly Su-pervised Information Extraction.
In Proc of COLING-2004.M.
Zhang, J. Zhang, J. Su and G. Zhou.
2006.
A Compos-ite Kernel to Extract Relations between Entities withboth Flat and Structured Features.
In Proc of ACL-2006.G.
Zhou, J. Su and M. Zhang.
2006.
Modeling Commonal-ity among Related Classes in Relation Extraction.
InProc of ACL-2006.599
