Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 388?392,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsGrammar Error CorrectionUsing Pseudo-Error Sentences and Domain AdaptationKenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and Hitoshi NishikawaNTT Cyber Space Laboratories, NTT Corporation1-1 Hikari-no-oka, Yokosuka, 239-0847, Japan{imamura.kenji, saito.kunikosadamitsu.kugatsu, nishikawa.hitoshi}@lab.ntt.co.jpAbstractThis paper presents grammar error correctionfor Japanese particles that uses discrimina-tive sequence conversion, which corrects erro-neous particles by substitution, insertion, anddeletion.
The error correction task is hinderedby the difficulty of collecting large error cor-pora.
We tackle this problem by using pseudo-error sentences generated automatically.
Fur-thermore, we apply domain adaptation, thepseudo-error sentences are from the sourcedomain, and the real-error sentences are fromthe target domain.
Experiments show that sta-ble improvement is achieved by using domainadaptation.1 IntroductionCase marks of a sentence are represented by postpo-sitional particles in Japanese.
Incorrect usage of theparticles causes serious communication errors be-cause the cases become unclear.
For example, inthe following sentence, it is unclear what must bedeleted.mail o todoi tara sakujo onegai-shi-masumail ACC.
arrive when delete please?When ?
has arrived an e-mail, please delete it.
?If the accusative particle o is replaced by a nomi-native one ga, it becomes clear that the writer wantsto delete the e-mail (?When the e-mail has arrived,please delete it.?).
Such particle errors frequentlyoccur in sentences written by non-native Japanesespeakers.This paper presents a method that can automat-ically correct Japanese particle errors.
This taskcorresponds to preposition/article error correction inEnglish.
For English error correction, many stud-ies employ classifiers, which select the appropriateprepositions/articles, by restricting the error typesto articles and frequent prepositions (Gamon, 2010;Han et al, 2010; Rozovskaya and Roth, 2011).On the contrary, Mizumoto et al (2011) proposedtranslator-based error correction.
This approach canhandle all error types by converting the learner?ssentences into the correct ones.
Although the targetof this paper is particle error, we employ a similarapproach based on sequence conversion (Imamuraet al, 2011) since this offers excellent scalability.The conversion approach requires pairs of thelearner?s and the correct sentences.
However, col-lecting a sufficient number of pairs is expensive.
Toavoid this problem, we use additional corpus con-sisting of pseudo-error sentences automatically gen-erated from correct sentences that mimic the real-errors (Rozovskaya and Roth, 2010b).
Furthermore,we apply a domain adaptation technique that re-gards the pseudo-errors and the real-errors as thesource and the target domain, respectively, so thatthe pseudo-errors better match the real-errors.2 Error Correction by DiscriminativeSequence ConversionWe start by describing discriminative sequence con-version.
Our error correction method converts thelearner?s word sequences into the correct sequences.Our method is similar to phrase-based statistical ma-chine translation (PBSMT), but there are three dif-ferences; 1) it adopts the conditional random fields,2) it allows insertion and deletion, and 3) binary andreal features are combined.
Unlike the classification388Incorrect Particle Correct Particle Note?
no/POSS.
INS?
o/ACC.
INSga/NOM.
o/ACC.
SUBo/ACC.
ni/DAT.
SUBo/ACC.
ga/NOM.
SUBwa/TOP.
o/ACC.
SUBno/POSS.
?
DEL: :Table 1: Example of Phrase Table (partial)approach, the conversion approach can correct mul-tiple errors of all types in a sentence.2.1 Basic ProcedureWe apply the morpheme conversion approach thatconverts the results of a speech recognizer into wordsequences for language analyzer processing (Ima-mura et al, 2011).
It corrects particle errors in theinput sentences as follows.?
First, all modification candidates are obtained byreferring to a phrase table.
This table, called theconfusion set (Rozovskaya and Roth, 2010a) inthe error correction task, stores pairs of incorrectand correct particles (Table 1).
The candidates arepacked into a lattice structure, called the phraselattice (Figure 1).
To deal with unchanged words,it also copies the input words and inserts them intothe phrase lattice.?
Next, the best phrase sequence in the phrase lat-tice is identified based on the conditional randomfields (CRFs (Lafferty et al, 2001)).
The Viterbialgorithm is applied to the decoding because errorcorrection does not change the word order.?
While training, word alignment is carried out bydynamic programming matching.
From the align-ment results, the phrase table is constructed by ac-quiring particle errors, and the CRF models aretrained using the alignment results as superviseddata.2.2 Insertion / DeletionSince an insertion can be regarded as replacing anempty word with an actual word, and deletion is thereplacement of an actual word with an empty one,we treat these operations as substitution without dis-tinction while learning/applying the CRF models.mailnounInput Words oACC.
todoiverb taraPART ?Phrase Lattice mail o todoi taracopy INS copySUB copy copy<s>Incorrect ParticlenounnoPOSS.ACC.gaNOM.niDAT.verb PARToACC.Figure 1: Example of Phrase LatticeHowever, insertion is a high cost operation be-cause it may occur at any location and can causelattice size to explode.
To avoid this problem, wepermit insertion only immediately after nouns.2.3 FeaturesIn this paper, we use mapping features and link fea-tures.
The former measure the correspondence be-tween input and output words (similar to the trans-lation models of PBSMT).
The latter measure thefluency of the output word sequence (similar to lan-guage models).The mapping features are all binary.
The focusingphrase and its two surrounding words of the inputare regarded as the window.
The mapping featuresare defined as the pairs of the output phrase and 1-,2-, and 3-grams in the window.The link features are important for the error cor-rection task because the system has to judge outputcorrectness.
Fortunately, CRF, which is a kind ofdiscriminative model, can handle features that de-pend on each other; we mix two types of featuresas follows and optimize their weights in the CRFframework.?
N -gram features: N -grams of the output words,from 1 to 3, are used as binary features.
Theseare obtained from a training corpus (paired sen-tences).
Since the feature weights are optimizedconsidering the entire feature space, fine-tuningcan be achieved.
The accuracy becomes almostperfect on the training corpus.?
Language model probability: This is a logarith-mic value (real value) of the n-gram probabilityof the output word sequence.
One feature weightis assigned.
The n-gram language model can be389constructed from a large sentence set because itdoes not need the learner?s sentences.Incorporating binary and real features yields arough approximation of generative models in semi-supervised CRFs (Suzuki and Isozaki, 2008).
It canappropriately correct new sentences while maintain-ing high accuracy on the training corpus.3 Pseudo-error Sentences and DomainAdaptationThe error corrector described in Section 2 requirespaired sentences.
However, it is expensive to col-lect them.
We resolve this problem by using pseudo-error sentences and domain adaptation.3.1 Pseudo-Error GenerationCorrect sentences, which are halves of the pairedsentences, can be easily acquired from corpora suchas newspaper articles.
Pseudo-errors are generatedfrom them by the substitution, insertion, and dele-tion functions according to the desired error pat-terns.We utilize the method of Rozovskaya and Roth(2010b).
Namely, when particles appear in the cor-rect sentence, they are replaced by incorrect ones ina probabilistic manner by applying the phrase table(which stores the error patterns) in the opposite di-rection.
The error generation probabilities are rel-ative frequencies on the training corpus.
The mod-els are learnt using both the training corpus and thepseudo-error sentences.3.2 Adaptation by Feature AugmentationAlthough the error generation probabilities are com-puted from the real-error corpus, the error distribu-tion that results may be inappropriate.
To better fitthe pseudo-errors to the real-errors, we apply a do-main adaptation technique.
Namely, we regard thepseudo-error corpus as the source domain and thereal-error corpus as the target domain, and modelsare learnt that fit the target domain.In this paper, we use Daume (2007)?s feature aug-mentation method for the domain adaptation, whicheliminates the need to change the learning algo-rithm.
This method regards the models for thesource domain as the prior distribution and learnsthe models for the target domain.Common Source TargetFeature SpaceDs Ds 0Source DataDt 0 DtTarget DataFigure 2: Feature AugmentationWe briefly review feature augmentation.
The fea-ture space is segmented into three parts: common,source, and target.
The features extracted from thesource domain data are deployed to the commonand the source spaces, and those from the target do-main data are deployed to the common and the targetspaces.
Namely, the feature space is tripled (Figure2).The parameter estimation is carried out in theusual way on the above feature space.
Consequently,the weights of the common features are emphasizedif the features are consistent between the source andthe target.
With regard to domain dependent fea-tures, the weights in the source or the target spaceare emphasized.Error correction uses only the features in the com-mon and target spaces.
The error distribution ap-proaches that of the real-errors because the weightsof features are optimized to the target domain.
In ad-dition, it becomes robust against new sentences be-cause the common features acquired from the sourcedomain can be used even when they do not appear inthe target domain.4 Experiments4.1 Experimental SettingsReal-error Corpus: We collected learner?s sen-tences written by Chinese native speakers.
The sen-tences were created from English Linux manualsand figures, and Japanese native speakers revisedthem.
From these sentences, only particle errorswere retained; the other errors were corrected.
Asa result, we obtained 2,770 paired sentences.
Thenumber of incorrect particles was 1,087 (8.0%) of13,534.
Note that most particles did not need to berevised.
The number of pair types of incorrect parti-cles and their correct ones was 132.Language Model: It was constructed fromJapanese Wikipedia articles about computers and3900.50.60.70.80.91Precision RateTRGSRCALLAUG0.30.40 0.05 0.1 0.15 0.2 0.25Precision RateRecall RateFigure 3: Recall/Precision Curve (Error Generation Mag-nification is 1.0)Japanese Linux manuals, 527,151 sentences in total.SRILM (Stolcke et al, 2011) was used to train atrigram model.Pseudo-error Corpus: The pseudo-errors weregenerated using 10,000 sentences randomly selectedfrom the corpus for the language model.
The mag-nification of the error generation probabilities waschanged from 0.0 (i.e., no errors) to 2.0 (the relativefrequency in the real-error corpus was taken as 1.0).Evaluation Metrics: Five-fold cross-validationon the real-error corpus was used.
We used two met-rics: 1) Precision and recall rates of the error correc-tion by the systems, and 2) Relative improvement,the number of differences between improved and de-graded particles in the output sentences (no changeswere ignored).
This is a practical metric because itdenotes the number of particles that human rewritersdo not need to revise after the system correction.4.2 ResultsFigure 3 plots the precision/recall curves for the fol-lowing four combinations of training corpora andmethod.?
TRG: The models were trained using only thereal-error corpus (baseline).?
SRC: Trained using only the pseudo-error corpus.?
ALL: Trained using the real-error and pseudo-error corpora by simply adding them.?
AUG:The proposed method.
The feature augmentationwas realized by regarding the pseudo-errors as the-500+50+1000.0 0.5 1.0 1.5 2.0Relative Improvement-150-100Relative ImprovementError Generation Probability(Magnification)TRGSRCALLAUGFigure 4: Relative Improvement among Error GenerationProbabilitiessource domain and the real-errors as the target do-main.The SRC case, which uses only the pseudo-errorsentences, did not match the precision of TRG.
TheALL case matched the precision of TRG at highrecall rates.
AUG, the proposed method, achievedhigher precision than TRG at high recall rates.
Atthe recall rate of 18%, the precision rate of AUGwas55.4%; in contrast, that of TRG was 50.5%.
Fea-ture augmentation effectively leverages the pseudo-errors for error correction.Figure 4 shows the relative improvement of eachmethod according to the error generation probabil-ities.
In this experiment, ALL achieved higher im-provement than TRG at error generation probabili-ties ranging from 0.0 to 0.6.
Although the improve-ments were high, we have to control the error gen-eration probability because the improvements in theSRC case fell as the magnification was raised.
Onthe other hand, AUG achieved stable improvementregardless of the error generation probability.
Wecan conclude that domain adaptation to the pseudo-error sentences is the preferred approach.5 ConclusionsThis paper presented an error correction method ofJapanese particles that uses pseudo-error generation.We applied domain adaptation in which the pseudo-errors are regarded as the source domain and thereal-errors as the target domain.
In our experiments,domain adaptation achieved stable improvement insystem performance regardless of the error genera-tion probability.391ReferencesHal Daume, III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics (ACL 2007),pages 256?263, Prague, Czech Republic.Michael Gamon.
2010.
Using mostly native data tocorrect errors in learners?
writing.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL-HLT 2010), pages163?171, Los Angeles, California.Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-Young Ha.
2010.
Using an error-annotated learnercorpus to develop an ESL/EFL error correction sys-tem.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), Valletta, Malta.Kenji Imamura, Tomoko Izumi, Kugatsu Sadamitsu, Ku-niko Saito, Satoshi Kobashikawa, and Hirokazu Masa-taki.
2011.
Morpheme conversion for connectingspeech recognizer and language analyzers in unseg-mented languages.
In Proceedings of Interspeech2011, pages 1405?1408, Florence, Italy.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
InProceedings of the 18th International Conferenceon Machine Learning (ICML-2001), pages 282?289,Williamstown, Massachusetts.Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata,and Yuji Matsumoto.
2011.
Mining revision log oflanguage learning SNS for automated Japanese errorcorrection of second language learners.
In Proceed-ings of 5th International Joint Conference on NaturalLanguage Processing (IJCNLP 2011), pages 147?155,Chiang Mai, Thailand.Alla Rozovskaya and Dan Roth.
2010a.
Generatingconfusion sets for context-sensitive error correction.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2010), pages 961?970, Cambridge, Massachusetts.Alla Rozovskaya and Dan Roth.
2010b.
Trainingparadigms for correcting errors in grammar and usage.In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics (NAACL-HLT2010), pages 154?162, Los Angeles, California.Alla Rozovskaya and Dan Roth.
2011.
Algorithm se-lection and model adaptation for ESL correction tasks.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Techologies (ACL-HLT 2011), pages 924?933,Portland, Oregon.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
SRILM at sixteen: Update andoutlook.
In Proceedings of IEEE Automatic SpeechRecognition and Understanding Workshop (ASRU2011), Waikoloa, Hawaii.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-wordscale unlabeled data.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies (ACL-08:HLT), pages 665?673, Columbus, Ohio.392
