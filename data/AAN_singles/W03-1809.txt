A Statistical Approach to the Semantics of Verb-ParticlesColin BannardSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKc.j.bannard@ed.ac.ukTimothy BaldwinCSLIStanford University210 Panama StreetStanford CA 94305, USAtbaldwin@csli.stanford.eduAlex LascaridesSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKalex@inf.ed.ac.ukAbstractThis paper describes a distributional ap-proach to the semantics of verb-particleconstructions (e.g.
put up, make off ).
Wereport first on a framework for implement-ing and evaluating such models.
We then goon to report on the implementation of sometechniques for using statistical models ac-quired from corpus data to infer the mean-ing of verb-particle constructions.1 IntroductionThe semantic representation of multiword expres-sions (MWEs) has recently become the target of re-newed attention, notably in the area of hand-writtengrammar development (Sag et al, 2002; Villavicen-cio and Copestake, 2002).
Such items cause con-siderable problems for any semantically-groundedNLP application (including applications where se-mantic information is implicit, such as informationretrieval) because their meaning is often not sim-ply a function of the meaning of the constituentparts.
However, corpus-based or empirical NLP hasshown limited interest in the problem.
While therehas been some work on statistical approaches tothe semantics of compositional compound nominals(e.g.
Lauer (1995), Barker and Szpakowicz (1998),Rosario and Hearst (2001)), the more idiosyncraticitems have been largely ignored beyond attempts atidentification (Melamed, 1997; Lin, 1999; Schone andJurafsky, 2001).
And yet the identification of non-compositional phrases, while valuable in itself, wouldby no means be the end of the matter.
The uniquechallenge posed by MWEs for empirical NLP is pre-cisely that they do not fall cleanly into the binaryclasses of compositional and non-compositional ex-pressions, but populate a continuum between the twoextremes.Part of the reason for the lack of interest by com-putational linguists in the semantics of MWEs is thatthere is no established gold standard data from whichto construct or evaluate models.
Evaluation to date hastended to be fairly ad hoc.
Another key problem is thelack of any firm empirical foundations for the notionof compositionality.
Given this background, this pa-per has two aims.
The first is to put the treatment ofnon-compositionality in corpus-based NLP on a firmempirical footing.
As such it describes the develop-ment of a resource for implementing and evaluatingstatistical models of MWE meaning, based on non-expert human judgements.
The second is to demon-strate the usefulness of such approaches by imple-menting and evaluating a handful of approaches.The remainder of this paper is structured as follows.We outline the linguistic foundations of this researchin Section 2 before describing the process of resourcebuilding in Section 3.
Section 4 summarises previ-ous work on the subject and Section 5 details our pro-posed models of compositionality.
Section 6 lays outthe evaluation of those models over the gold standarddata, and we conclude the paper in Section 7.2 Verb Particle ConstructionsWe selected the English verb-particle construction asour test case MWE in this paper.
Verb-particle con-structions (hereafter referred to as VPCs) consist of ahead verb and one or more obligatory particles, in theform of intransitive prepositions (e.g.
hand in), ad-jectives (e.g.
cut short) or verbs (e.g.
let go) .
Here,we focus exclusively on prepositional particles due totheir high productivity and variable compositionality.Examples of prepositional VPCs are put up, finish up,gun down and make out as used in the following sen-tences:(1) Peter put the picture up(2) Susan finished up her paper(3) Philip gunned down the intruder(4) Barbara and Simon made outVPCs cause significant problems for NLP sys-tems.
Semantically, they often cannot be understoodthrough the simple composition of their independentparts.
Compare, for example, sentences (1) and (4).In (1), the meaning seems to be that Peter put the pic-ture somewhere and that as a consequence the picturewas up.
That is, the verb and the particle make in-dependent contributions to the sentence.
A (partial)Parsons-style semantic analysis of this might be asfollows:put(e1, x, y) ?
peter(x) ?
picture(y) ?
up(e1, y)Sentence (4), on the other hand requires a rather dif-ferent analysis.
Neither Barbara nor Simon can besaid to have made or to be out.
The semantic anal-ysis we would want then might be something like thefollowing:make out(e1, e2) ?
and(e2, x, y) ?
barbara(x) ?
simon(y)How are we to identify whether the first or the secondkind of semantic representation is appropriate for anygiven item?
If we look at the other two sentences wecan see that the problem is even more complicated.In (2) it is the case that the paper is finished, but itwould be hard to claim that anything or anyone is up.Only the verb then seems to be contributing its sim-plex meaning, and the semantic analysis is (roughly):finish(e1, x, y) ?
susan(x) ?
paper(y)In (3), by contrast, it is the particle that contributes itssimplex meaning and not the verb.
As a consequenceof Philip?s action the intruder is down, but since thereis no simplex verb to gun, we would not say that any-one gunned or was gunned The semantic analysis isconsequently as follows:gun down(e1, x, y) ?
philip(x) ?
intruder(y) ?
down(e1, y)In the linguistic literature, the semantics of VPCsis frequently viewed in rather more complicated termsthan we are suggesting here, with particles often seenas making significant construction-specific contribu-tions in terms of aspect (e.g.
Brinton (1985)).
How-ever no such existing linguistic account is completelyrobust, and for practical NLP purposes we are forcedto adopt a rather straightforward definition of compo-sitionality as meaning that the overall semantics of theMWE can be composed from the simplex semanticsof its parts, as described (explicitly or implicitly) in afinite lexicon.3 Building the ResourceRather than attempting to model compositionality byanchoring word semantics to a given lexicon, our ap-proach in this work is to defer to an empirical refer-ence based on human judgements.
We define MWEcompositionality to be an entailment relationship be-tween the whole and its various parts, and solicit en-tailment judgements based on a handful of examplesentences.Entailment is conventionally defined for logicalpropositions, where a proposition P entails a proposi-tion Q iff there is no conceivable state of affairs thatcould make P true and Q false.
This can be gener-alised to refer to the relationship between two verbsV1 and V2 that holds when the sentence Someone V1sentails the sentence Someone V2s (see, e.g., the treat-ment of verbs in the WordNet hierarchy (Miller et al,1990)).
According to this generalisation we wouldthen say that the verb run entails the verb move be-cause the sentence He runs entails the sentence Hemoves.
The same idea can be generalised to the rela-tionship between simplex verbs (e.g.
walk) and VPCs(e.g.
walk off ).
For example, sentence (1) can be saidto entail that Peter put the picture somewhere and sowe can say that put up entails put.
The same might besaid of finish up and finish in (2).
However, (3) and(4) produce a rather different result.
(4) does not en-tail that Simon and Barbara made something, and (3)cannot entail that Philip gunned the intruder becausethere is no simplex verb to gun.
This is a very usefulway of testing whether the simplex verb contributes tothe meaning of the construction.We can approach the relationship between VPCsand particles in this same way.
For (1), while it isnot true that Peter was up, it is true that The picturewas up.
We can therefore say that the VPC entails theparticle here.
For (2), it is not true that either Susanor the paper were up, and the VPC therefore does notentail the particle.
In the case of (3), while it is nottrue that Philip was down it is true that The intruderwas down, and the VPC therefore entails the particle.Finally, for (4), it is not true that Barbara and Simonwere out, and the VPC therefore does not entail theparticle.We make the assumption that these relationshipsbetween the component words of the VPC and thewhole are intuitive to non-experts, and aim to usetheir ?entailment?
judgements accordingly.
Thisuse of entailment in exploring the semantics ofverb and preposition combinations was first pro-posed by Hawkins (2000), and applied to VPCs byLohse et al (in preparation).3.1 Experimental MaterialsIn an attempt to normalise the annotators?
entailmentjudgements, we decided upon an experimental setupwhere the subject is, for each VPC type, presentedwith a fixed selection of sentential contexts for thatVPC.
So as to avoid introducing any bias into theexperiment through artificially-generated sentences,we chose to extract the sentences from naturally-occurring text, namely the written component of theBritish National Corpus (BNC, Burnard (2000)).Extraction of the VPCs was based on the methodof Baldwin and Villavicencio (2002).
First, we used aPOS tagger and chunker (both built using fnTBL 1.0(Ngai and Florian, 2001)) to (re)tag the BNC.
This al-lowed us to extract VPC tokens through use of: (a)the particle POS in the POS tagged output, for eachinstance of which we simply then look for the right-most verb within a fixed window to the left of theparticle, and (b) the particle chunk tag in the chunkeroutput, where we similarly locate the rightmost verbassociated with each particle chunk occurrence.
Fi-nally, we ran a stochastic chunk-based grammar overthe chunker output to extend extraction coverage toinclude mistagged particles and also more reliably de-termine the valence of the VPC.
The token output ofthese three methods was amalgamated by weightedvoting.The above method extracted 461 distinct VPC typesoccurring at least 50 times, attested in a total of110,199 sentences.
After partitioning the sentencedata by type, we randomly selected 5 sentences foreach VPC type.
We then randomly selected 40 VPCtypes (with 5 sentences each) to use in the entailmentexperiment.
That is, all results described in this paperare over 40 VPC types.3.2 Participants28 participants took part in our initial experiment.They were all native speakers of English, recruitedby advertisements posted to newsgroups and mailinglists.3.3 Experimental MethodEach participant was presented with 40 sets of 5 sen-tences, where each of the five sentences contained aparticular VPC.
The VPC in question was indicated atthe top of the screen, and they were asked two ques-tions: (1) whether the VPC implies the verb, and (2)whether the VPC implies the particle.
If the VPCwas round up, e.g., the subject would be asked ?Doesround up imply round??
and ?Does round up im-ply up?
?, respectively.
They were given the option ofthree responses: ?Yes?, ?No?
or ?Don?t Know?.
Oncethey had indicated their answer and pressed next, theyadvanced to the next VPC and set of 5 sentences.
Theywere unable to move on until a choice had been indi-cated.As with any corpus-based approach to lexical se-mantics, our study of VPCs is hampered by poly-semy, e.g.
carry outTRANSin the execute and transportout (from a location) senses.1 Rather than interveneto customise example sentences to a prescribed sense,we accepted whatever composition of senses randomsampling produced.
Participants were advised that ifthey felt more that one meaning was present in the setof five sentences, they should base their decision onthe sense that had the greatest number of occurrencesin the set.1The effects of polysemy were compounded by not having anyreliable method for determining valence.
We consider that sim-ply partitioning VPC items into intransitive and transitive usageswould reduce polysemy significantly.VPC Component word Yes No Don?t Knowget 19 5 2get down down 14 10 2move 14 12 0move offoff 19 7 0throw 20 6 0throw outout 15 10 1pay 11 12 3pay offoff 16 8 2lift 25 1 0lift outout 26 0 0roll 13 9 4roll back back 14 12 0dig 21 5 0dig upup 18 7 1lie 24 2 0lie down down 25 1 0wear 6 19 1wear onon 3 22 1fall 23 3 0fall offoff 25 1 0move 22 4 0move outout 26 0 0hand 15 9 2hand outout 19 7 0seek 13 13 0seek outout 15 11 0sell 14 12 0sell offoff 16 9 1trail 8 18 0trail offoff 10 16 0stay 20 5 1stay upup 21 5 0go 18 7 1go down down 22 3 1hang 22 4 0hang outout 25 1 0get 20 6 0get back back 19 6 1throw 15 9 2throw in in 13 12 1put 8 17 1put offoff 5 19 2shake 12 14 0shake offoff 15 11 0step 25 1 0step offoff 26 0 0give 12 12 2give offoff 21 5 0carry 7 17 2carry awayaway 6 18 2throw 18 7 1throw back back 21 4 1pull 13 10 3pull offoff 13 6 7carry 0 25 1carry outout 0 25 1brighten 9 16 1brighten upup 16 10 0map 9 17 0map outout 10 16 0slow 11 14 1slow down down 19 7 0sort 6 19 1sort outout 11 15 0bite 15 10 1bite offoff 16 8 2add 12 14 0add upup 19 6 1mark 13 13 0mark outout 14 12 0lay 11 14 1lay outout 10 14 2catch 6 20 0catch upup 7 18 1run 12 13 1run upup 13 10 3stick 20 6 0stick outout 15 11 0play 10 15 1play down down 6 20 0Table 1: Participant entailment judgementsOverall Verbs only Particles onlyAgreement .677 .703 .650Kappa (?)
.376 .372 .352% Yes .575 .655 .495% No .393 .319 .467% Don?t Know .032 .026 .038Table 2: Summary of judgements for all VPCsThe experiment was conducted remotely over theWeb, using the experimental software package Web-Exp (Corley et al, 2000).
Experimental sessionslasted approximately 20 minutes and were self-paced.The order in which the forty sets of sentences werepresented was randomised by the software.3.4 Annotator agreementWe performed a pairwise analysis of the agreementbetween our 28 participants.
The overall mean agree-ment was .655, with a kappa (?)
score (Carletta, 1996)of .329.
An initial analysis showed that two partic-ipants strongly disagreed with the other, achieving amean pairwise ?
score of less than .1.
We decidedtherefore to remove these from the set before pro-ceeding.
The overall results for the remaining 26 par-ticipants can be seen in Table 2.
The ?
score overthese 26 participants (.376) is classed as fair (0.2?0.4) and approaching moderate (0.4?0.6) according toAltman (1991).As mentioned above, a major problem with lexi-cal semantic studies is that items tend to occur withmore than one meaning.
In order to test the effects ofpolysemy in the example sentences on inter-annotatoragreement, we analysed the agreement obtained overthose VPCs which have only one meaning accord-ing to WordNet (Miller et al, 1990).
There was atotal of 14 such items, giving 28 entailment judge-ments (one for the verb and one for the particle ineach item).
For these items, mean agreement and the?
score were .700 and .387, respectively.
These areonly very slightly higher than the overall scores, sug-gesting, although by no means proving, that polysemywas not a significant confounding factor.The results for each VPC type can be seen in Ta-ble 1, broken down into the verb and particle entail-ment judgements and based on the 26 participants.
Wetook two approaches to deriving a single judgementfor each test.
First, we took the majority judgementto be the correct one (majority).
Second, we identi-fied the participant who achieved the highest overall ?score with the other participants, and took their judge-ments to be correct (centroid annotator).
Both setsof results will be referred to in evaluating our models.It is interesting to look at the way in which the re-sults for component entailment are distributed acrossthe VPCs.
According to the majority view, there are21 fully-compositional items, 10 items where neitherthe verb nor the particle is entailed, 9 items where onlythe particle is entailed, and 0 items where the verbalone is entailed.
According to the judgements of thecentroid annotator, there are 10 fully-compositionalitems, 12 items where neither the verb nor the parti-cle is entailed, 15 where only the verb is entailed, and3 where only the particle is entailed.
It is surprisingto notice that the majority view holds there to be noitems in which the verb alone is contributing mean-ing.
It could be the case that items where only theverb contributes meaning are rare, or that they are notrepresented in our dataset.
Another possible, and toour minds more likely, conclusion is that the contribu-tion of the head verb strongly affects the way in whichparticipants view the whole item.
Thus if a verb isconsidered to be contributing simplex semantics, theparticipant is likely to assume that the VPC is com-pletely compositional, and conversely if a verb is con-sidered to not be contributing simplex semantics, theparticipant is more likely to assume the VPC to benon-compositional.4 Related WorkWe devote this section to a description of statisticalNLP work on the non-compositionality of MWEs.Perhaps the singularly most influential work onMWE non-compositionality is that of Lin (1999).
Wedescribe Lin?s method in some detail here as it formsthe basis of one of the methods tested in this re-search.
Lin?s method is based on the premise that non-compositional items have markedly different distribu-tional characteristics to expressions derived throughsynonym substitution over the original word compo-sition.
Lin took his multiword items from a colloca-tion database (Lin, 1998b).
For each collocation, hesubstituted each of the component words with a wordwith a similar meaning.
The list of similar meaningswas obtained by taking the 10 most similar words ac-cording to a corpus-derived thesaurus, the construc-tion of which is described in Lin (1998a).
The mutualinformation value was then found for each item pro-duced by this substitution by taking a collocation toconsist of three events: the type of dependency rela-tionship, the head lexical item, and the modifier.
Aphrase ?
was then said to be non-compositional iffthere exists no phrase ?
where: (a) ?
can be pro-duced by substitution of the components of ?
as de-scribed above, and (b) there is an overlap betweenthe 95% confidence interval of the mutual informa-tion values of ?
and ?.
These judgements were eval-uated by comparison with a dictionary of idioms.
Ifan item was in the dictionary then it was said to benon-compositional.
Scores of 15.7% for precision and13.7% for recall are reported.There are, to our minds, significant problems withthe underlying assumptions of Lin?s method.
Thetheoretical basis of the technique is that composi-tional items should have a similar distribution to itemsformed by replacing components words with seman-tically similar ones.
The idea presumably is that if anitem is the result of the free combination of words, ora fully productive lexical rule, then word-substitutedvariants should be distributed similarly.
This seems areasonable basis for modelling productivity but notcompositionality, as Lin claims.
There are many ex-amples in natural language of phrases that are not atall productive but are still compositional (e.g.
fryingpan); we term the process by which these expressionsarise institutionalisation .
Similar work to Lin?s hasbeen done in the area of collocation extraction (e.g.Pearce (2002)), to pick up on this alternate concept ofinstitutionalisation.Schone and Jurafsky (2001) employed Latent Se-mantic Analysis (LSA, Deerwester et al (1990)) in aneffort to improve on existing techniques for extract-ing MWEs from corpora.
One property they try andpick up on in doing so is non-compositionality.
Theymeasure the cosine between the vector representationfor the candidate MWE and a weighted vector sumof its component words, suggesting that a small co-sine would indicate compositionality.
They evaluatethis by comparing the extracted items with those listedin existing dictionaries, and report that it offers noimprovement in extracting MWEs over existing tech-niques.
The assumption that non-compositionality isrequisite for the presence of a MWE in a dictionary,while interesting, is not well-founded, and hence itdoes not seem to us that the poor results reflect a fail-ure of the LSA approach in measuring compositional-ity.Bannard (2002) used a combination of hand-builtthesauri and corpus statistics to explore the compo-sitionality of VPCs.
The task was to predict whetherthe verb and/or the particle were contributing meaningto a given item, using statistical analysis of a set ofVPCs extracted from the Wall Street Journal sectionof the Penn Treebank (Marcus et al, 1993).
Two tech-niques were used.
The first of these loosely followedLin in measuring the extent to which the componentverb or particle of any VPC could be replaced withitems of a similar semantic class to form a corpus-attested VPC; WordNet (Miller et al, 1990) was usedas the source for verb substitution candidates, and ahand-build semantic taxonomy for particles.
The sec-ond technique explored the semantic similarity of aVPC to its component verb by comparing their subcat-egorisation preferences, assuming that semantic sim-ilarity between a VPC and its component verb indi-cates compositionality.
Poor results were put downto data-sparseness, and the lexical resources not beingwell suited to the task.
We use a larger corpus and anautomatically-derived thesaurus for the research de-scribed in this paper, with the hope of overcomingthese problems.McCarthy et al (2003) carry out research close inspirit to that described here, in taking VPC tokensautomatically extracted from the BNC and using anautomatically acquired thesaurus to classify their rel-ative compositionality.
One significant divergencefrom our research is that they consider composition-ality to be an indivisible property of the overall VPC,and not the individual parts.
Gold-standard data wasgenerated by asking human annotators to describe thecompositionality of a given VPC according to a 11-point scale, based upon which the VPCs were rankedin order of compositionality.
Similarly to this re-search, McCarthy et al in part used the similaritymeasure of Lin (1998a) to model compositionality,e.g., in taking the top N similar words to each VPCand looking at overlap with the top N similar words tothe head verb.
They also examine the use of statisticaltests such as mutual information in modelling com-positionality, and find the similarity-based methods tocorrelate more highly with the human judgements.Baldwin et al (2003) use LSA as a technique foranalysing the compositionality (or decomposabil-ity) of a given MWE.
LSA is suggested to bea construction-inspecific test for compositionality,which is illustrated by testing its effectivity over bothEnglish noun-noun compounds and VPCs.
Baldwinet al used LSA to calculate the distributional similar-ity between an MWE and its head word, and demon-strate a correlation between similarity and composi-tionality (modelled in terms of endocentricity) by wayof items with higher similarity being more composi-tional.
They do not go as far as to classify MWEs asbeing compositional or non-compositional, however.5 Building a classifierHaving created our gold-standard data, we imple-mented some statistical techniques for automatic anal-ysis.
In this, we use the VPC tokens with sententialcontexts extracted from the BNC as reported in Sec-tion 3, i.e.
a superset of the data used to annotate theVPCs.
We mapped the gold-standard data onto fourbinary (yes/no) classification tasks over VPC items:TASK 1: The item is completely compositional.TASK 2: The item includes at least one item that iscompositional.TASK 3: The verb in the item contributes its simplexmeaning.TASK 4: The particle in the item contributes its sim-plex meaning.Note the partial conditional chaining between thesetests, e.g.
an item for which the verb and particle con-tribute their simplex meaning (i.e.
positive exemplarsfor TASKS 3 and 4) is completely compositional (i.e.a positive exemplar for TASK 1).The following sections describe four methods formodelling VPC compositionality, each of which istested over the 4 individual compositionality classi-fication tasks.
The results for each method are givenin Table 4, in which the baseline for each task is thescore obtained when we assign the most frequent labelto all items.
Each method is evaluated in terms of pre-cision (Prec), Recall (Rec) and F-score (?
= 1, FB1),and all values which exceed the baseline are indicatedin boldface.5.1 Method 1We decided to gain a sense of the start-of-the-art onthe task by reimplementing the technique describedin Lin (1999) over VPCs.
In our implementation wereplaced Lin?s collocations with our VPCs, treatingthe relationship between a verb and a particle as akind of grammatical relation.
In addition to the binarycompositional/non-compositional judgement that Linoffers (which seems to be equivalent to TASK 1), wetested the method over the other three tasks.
Ac-knowledging, as we must, that items can be partiallycompositional (i.e.
have one component item con-tributing a conventional meaning), it would seem tobe the case, according to the assumptions made bythe technique, that the substitutability of each itemwill give us some insight into its semantic contribu-tion.
The thesaurus used by Lin has been generouslymade available online.
However this is not adequatefor our purposes since it includes only verbs, nounsand adjectives/adverbs.
We therefore replicated theapproach described in Lin (1998a) to build the the-saurus, using BNC data and including prepositions.5.2 Method 2Method 2 is very similar to Method 1, except thatinstead of using a thesaurus based on Lin?s method,we took a knowledge-free approach to obtaining syn-onyms.
Our technique is very similar to the approachtaken to building a ?context space?
by Schu?tze (1998).We measured the frequency of co-occurrence of ourtarget words (the 20,000 most frequent words, includ-ing all of our VPCs2 and all of their component verbsand prepositions) with a set of 1000 ?content-bearing?words (we used the 51st to the 1050th most frequentwords, the 50 most frequent being taken to have ex-tremely low infomation content).
A target word wassaid to co-occur with a content word if that contentword occurred within a window of 5 words to eitherside of it.
These co-occurrence figures were stored asfeature vectors.
In order to overcome data sparseness,we used techniques borrowed from Latent Seman-tic Indexing (LSI, Deerwester et al (1990)).
LSI isan information retrieval technique based on SingularValue Decomposition (SVD), and works by project-ing a term-document matrix onto a lower-dimensionalsubspace, in which relationships might more easily be2Concatenated into a single-word itemMajority Centroid 60% Agreement1.29 4.09 0.48All (p =.255) (p=.043) (p=.488)2.19 0.01 5.56Monosemous (p=.137) (p=.924) (p =.018)Table 3: Logistic regression for Method 4observed between terms which are related but do notco-occur.
We used this technique to reduce the featurespace for our target words from 1000 to 100, allow-ing relations to be discovered between target wordseven if there is not direct match between their contextwords.
We used the various tools in the GTP softwarepackage, created at the University of Tennessee3 tobuild these matrices from the co-occurrence data, andto perform SVD analysis.We calculated the similarity between two terms byfinding the cosine of the angle between their vec-tors.
We performed a pairwise comparison betweenall verbs and all particles.
For each term we thensorted all of the other items of the same part-of-speechin descending order of similarity, which gave us thethesaurus for use in substitution.
As with the Linmethod, we performed substitutions by taking the 10most similar items for the head verb and particle ofeach VPC.5.3 Method 3We noted in Section 4 that a significant problemwith the substitution approach is that it is sensitive toinstitutionalisation rather than non-compositionality.Method 3 attempts to adapt substitution to more ac-curately reflect non-compositionality by removing theassumption that an item formed by substitution shouldhave the same distributional characteristics as theoriginal item.
Rather than basing the composition-ality judgement on the relative mutual informationscores of the original items and the items resultingfrom substitution, we instead base it on the corpus-based semantic similarity between the original ex-pression and word-substituted derivative expressions.The same method of substitution is used, with eachcomponent being replaced by each of its 10 nearestneighbours according to the knowledge-free similar-ity measure described above.
We judge a VPC itemto be compositional if an expression formed by sub-stitution occurs among the nearest 100 verb-particleitems to the original, and failing this, we judge it to benon-compositional.
We experimented with a numberof cut-off points for identifying semantically similaritems, and found that a value of 100 gave the best re-sults.5.4 Method 4While Method 3 softens the reliance upon productiv-ity as a test for compositionality, it still confuses insti-3http://www.cs.utk.edu/?lsi/soft.htmlTASK 1 TASK 2(mean agreement = .693) (mean agreement = .750)Majority Centroid annotator Majority Centroid annotatorPrec Rec FB1 Prec Rec FB1 Prec Rec FB1 Prec Rec FB1Baseline .525 1.000 .680 .250 1.000 .400 .750 1.000 .860 .700 1.000 .820Method 1 .577 .714 .638 .269 .700 .389 .731 .633 .678 .731 .679 .704Method 2 .575 .714 .638 .308 .800 .447 .769 .667 .717 .769 .714 .739Method 3 .558 .905 .690 .235 .800 .360 .765 .866 .810 .735 .892 .810Method 4 .514 .857 .642 .200 .700 .280 .771 .900 .830 .714 .893 .794TASK 3 TASK 4(mean agreement = .729) (mean agreement = .688)Majority Centroid annotator Majority Centroid annotatorPrec Rec FB1 Prec Rec FB1 Prec Rec FB1 Prec Rec FB1Baseline .525 1.000 .690 .625 1.000 .770 .750 1.000 .857 .670 1.000 .800Method 1 .474 .429 .450 .632 .480 .546 .818 .300 .442 .454 .385 .417Method 2 .608 .666 .639 .782 .720 .749 .818 .300 .442 .454 .385 .417Method 3 .531 .810 .641 .625 .800 .717 .769 .333 .480 .308 .308 .308Method 4 .666 .286 .400 .666 .240 .353 .758 .833 .793 .303 .769 .435Table 4: Results for the four methods over the different compositionality classification taskstutionalisation with non-compositionality somewhatin its reliance upon substitution.
We now suggest an-other technique which we claim is based on sounderprinciples.
The underlying intuition is that identify-ing the degree of semantic similarity between a VPCand its component verb and/or particle will indicatewhether that component part contributes independentsemantics.
This is similar to the assumption madein Schone and Jurafsky (2001), except that we makea distinction between the contribution of the differentcomponent parts.
We again used the knowledge-freesemantic similarity measure.
We performed a pair-wise comparison of all VPCs with all verbs and allparticles, obtaining cosine similarity scores for eachpair.In order to measure the usefulness of this score,we performed a logistic regression of the similarityscores and the human judgements as to whether thegiven verb or particle is entailed by the VPC.
We didthis for the majority human judgements, and also thecentroid annotator scores.
We also did the same us-ing the majority scores but rejecting those items onwhich there was less than 60% agreement.
In ad-dition to performing a regression for all items (All),we also performed a regression for only those itemswhich have only one meaning according to WordNet(Monosemous).
The results for all of these are shownin Table 3.
The figures shown are chi-squared scores,with their associated significance values.
We observedsignificant correlations for a number of the regres-sions (notably all items vs. the centroid annotator, andmonosemous items vs. 60% agreement).
While theresults are far from stable, such variation is perhaps tobe expected on a test like this since the nature of con-text space models means that rogue items sometimesget extremely high similarity scores, and we are per-forming the regression over only 40 VPCs (80 VPC-component pairs).In order to build a classifier for making composi-tionality decisions, we again used a neighbour-basedapproach with a cut-off.
We said that a verb wascontributing meaning to a VPC if it occurred in the20 most similar items to the VPC.
For particles, wesaid that the item was contributing meaning if it wasamong the 10 nearest neighbours.
We tried out a rangeof different cut-offs for each item and found that thesegave the best results.6 ResultsThe results in Table 4 show that on all tasks (for themajority-view based data and three out of four for thecentroid data), at least one of the four statistical meth-ods offers an improvement in precision over the base-line, and that there is an improvement in F-score forTASK 1 on both sets of data.
There are swings in therelative scores obtained over the majority as comparedto centroid annotator data for a given task.
In termsof relative performance, the semantic similarity basedapproach of Methods 3 and 4 outperform the distribu-tion based approach of Methods 1 and 2 in terms ofF-score, on 6 of the 8 sets of results reported.In order to get a reliable sense for how good thesescores are, we compare them with the level of agree-ment across human judges.
We calculated pairwiseagreement across all participants on the four classifi-cation tasks, resulting in the figures given in Table 4.These agreement scores give us an upper bound forclassification accuracy on each task, from which it ispossible to benchmark the classification accuracy ofthe classifiers on that same task.
On TASK 1, three ofthe four classifiers achieved a classification accuracyof .575.
On TASK 2, the highest-performing classi-fier (Method 4), achieved a classification accuracy of.725.
On TASK 3, Method 2 achieved the highest clas-sification accuracy at .600, and on TASK 4, Method 4achieved a classification accuracy of .675.
We can seethen that the best classifiers perform only marginallybelow the upper bound on at least two of the tasks.While these results may appear at first glance to beless than conclusive, we must bear in mind that we areworking with limited amounts of data and relativelysimplistic models of a cognitively intensive task.
Weinterpret them as very positive indicators of the via-bility of using empirical methods to analyse VPC se-mantics.7 ConclusionThis paper has described the implementation and eval-uation of four corpus-based approaches to the seman-tics of verb-particle constructions.
We created a set ofgold-standard data, based on non-expert judgementsacquired via a web-based experiment.
We then imple-mented four different techniques and showed that theyoffer a significant improvement over a naive approach.AcknowledgementsWe would like to thank Ann Copestake, Maria Lapata, Diana Mc-Carthy, Aline Villavicencio, Tom Wasow, Dominic Widdows andthe three anonymous reviewers for their valuable input on thisresearch.
Timothy Baldwin is supported by the National ScienceFoundation under Grant No.
BCS-0094638 and also the ResearchCollaboration between NTT Communication Science Laborato-ries, Nippon Telegraph and Telephone Corporation and CSLI,Stanford University.
Colin Bannard is supported by ESRC GrantPTA-030-2002-01740ReferencesDouglas G. Altman.
1991.
Practical Statistics for Medical Re-search.
Chapman and Hall.Timothy Baldwin and Aline Villavicencio.
2002.
Extracting theunextractable: A case study on verb-particles.
In Proc.
ofthe 6th Conference on Natural Language Learning (CoNLL-2002), Taipei, Taiwan.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and DominicWiddows.
2003.
An empirical model of multiword expres-sion decomposability.
In Proc.
of the ACL-2003 Workshop onMultiword Expressions: Analysis, Acquisition and Treatment.
(this volume).Colin Bannard.
2002.
Statistical techniques for automaticallyinferring the semantics of verb-particle constructions.
LinGOWorking Paper No.
2002-06.Ken Barker and Stan Szpakowicz.
1998.
Semi-automatic recog-nition of noun modifier relationships.
In Proc.
of the 36th An-nual Meeting of the ACL and 17th International Conference onComputational Linguistics (COLING/ACL-98), pages 96?102,Montreal, Canada.Laurel Brinton.
1985.
Verb particles in English: Aspect or ak-tionsart.
Studia Linguistica, 39:157?68.Lou Burnard.
2000.
User Reference Guide for the British Na-tional Corpus.
Technical report, Oxford University Comput-ing Services.Jean Carletta.
1996.
Assessing agreement on classification tasks:the kappa statistic.
Computational Linguistics, 22(2):249?254.Martin Corley, Frank Keller, and Christoph Scheepers.
2000.Conducting psychological experiments over the world wideweb.
Unpublished manuscript, University of Edinburgh andSaarland University.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.
Indexingby latent semantic analysis.
Journal of the American Societyof Information Science, 41(6).John A. Hawkins.
2000.
The relative order of preposition phrasesin English: Going beyond manner ?
place ?
time.
LanguageVariation and Change, 11:231?266.Mark Lauer.
1995.
Designing Statistical Language Learners:Experiments on Noun Compounds.
Ph.D. thesis, MacquarieUniversity.Dekang Lin.
1998a.
Automatic retrieval and clustering of similarwords.
In Proc.
of the 36th Annual Meeting of the ACL and17th International Conference on Computational Linguistics(COLING/ACL-98), Montreal, Canada.Dekang Lin.
1998b.
Extracting collocations from text corpora.In First Workshop on Computational Terminology.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proc.
of the 37th Annual Meetingof the ACL, pages 317?24, College Park, USA.Barbara Lohse, John A. Hawkins, and Tom Wasow.
in prepara-tion.
Domain minimization in English verb-particle construc-tions.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: the Penn treebank.
Computational Linguistics,19(2):313?30.Diana McCarthy, Bill Keller, and John Carroll.
2003.
Detectinga continuum of compositionality in phrasal verbs.
In Proc.
ofthe ACL-2003 Workshop on Multiword Expressions: Analysis,Acquisition and Treatment.
(this volume).I.
Dan Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In Proceedings of2nd Conference on Empirical Methods in Natural LanguageProcessing.G.A.
Miller, R. Beckwith, C. Fellbaum, D Gross, and K.J.
Miller.1990.
Introduction to WordNet: an on-line lexical database.International Journal of Lexicography, 3(4):235?44.Grace Ngai and Radu Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proc.
of the 2nd Annual Meeting ofthe North American Chapter of Association for ComputationalLinguistics (NAACL2001), pages 40?7, Pittsburgh, USA.Darren Pearce.
2002.
A comparative evaluation of collocationextraction techniques.
In Proc.
of the 3rd International Con-ference on Language Resources and Evaluation (LREC 2002),Las Palmas, Canary Islands.Barbara Rosario and Marti Hearst.
2001.
Classifying the seman-tic relations in noun compounds via a domain-specific lexicalhierarchy.
In Proc.
of the 6th Conference on Empirical Meth-ods in Natural Language Processing (EMNLP 2001), Pitts-burgh, USA.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann Copestake,and Dan Flickinger.
2002.
Multiword expressions: A pain inthe neck for NLP.
In Proc.
of the 3rd International Conferenceon Intelligent Text Processing and Computational Linguistics(CICLing-2002), pages 1?15, Mexico City, Mexico.Patrick Schone and Dan Jurafsky.
2001.
Is knowledge-free in-duction of multiword unit dictionary headwords a solved prob-lem?
In Proc.
of the 6th Conference on Empirical Methods inNatural Language Processing (EMNLP 2001), pages 100?108.Hinrich Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.Aline Villavicencio and Ann Copestake.
2002.
Phrasal verbs andthe LinGO-ERG.
LinGO Working Paper No.
2002-01.
