Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1753?1764,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsSuccess with Style: Using Writing Style to Predict the Success of NovelsVikas Ganjigunte Ashok Song Feng Yejin ChoiDepartment of Computer ScienceStony Brook UniversityStony Brook, NY 11794-4400vganjiguntea, songfeng, ychoi@cs.stonybrook.eduAbstractPredicting the success of literary works is acurious question among publishers and aspir-ing writers alike.
We examine the quantitativeconnection, if any, between writing style andsuccessful literature.
Based on novels overseveral different genres, we probe the predic-tive power of statistical stylometry in discrim-inating successful literary works, and identifycharacteristic stylistic elements that are moreprominent in successful writings.
Our studyreports for the first time that statistical stylom-etry can be surprisingly effective in discrim-inating highly successful literature from lesssuccessful counterpart, achieving accuracy upto 84%.
Closer analyses lead to several newinsights into characteristics of the writing stylein successful literature, including findings thatare contrary to the conventional wisdom withrespect to good writing style and readability.1 IntroductionPredicting the success of novels is a curious ques-tion among publishers, professional book reviewers,aspiring and even expert writers alike.
There are po-tentially many influencing factors, some of whichconcern the intrinsic content and quality of the book,such as interestingness, novelty, style of writing, andengaging storyline, but external factors such as so-cial context and even luck can play a role.
As a re-sult, recognizing successful literary work is a hardtask even for experts working in the publication in-dustries.
Indeed, even some of the best sellers andaward winners can go through several rejections be-fore they are picked up by a publisher.1Perhaps due to its obvious complexity of the prob-lem, there has been little previous work that attemptsto build statistical models that predict the success ofliterary works based on their intrinsic content andquality.
Some previous studies do touch on the no-tion of stylistic aspects in successful literature, e.g.,extensive studies in Literature discuss literary stylesof significant authors (e.g., Ellega?rd (1962), Mc-Gann (1998)), while others consider content char-acteristics such as plots, characteristics of charac-ters, action, emotion, genre, cast, of the best-sellingnovels and blockbuster movies (e.g., Harvey (1953),Hall (2012), Yun (2011)).All these studies however, are qualitative in na-ture, as they rely on the knowledge and insights ofhuman experts on literature.
To our knowledge, noprior work has undertaken a systematic quantitativeinvestigation on the overarching characterization ofthe writing style in successful literature.
In consid-eration of widely different styles of authorship (e.g.,Escalante et al(2011), Peng et al(2003), Argamonet al(2003)), it is not even readily clear whetherthere might be common stylistic elements that helpdiscriminating highly successful ones from less suc-cessful counterpart.In this work, we present the first study that in-vestigates this unstudied and unexpected connectionbetween stylistic elements and the literary success.The key findings of our research reveal that thereexists distinct linguistic patterns shared among suc-1E.g., Paul Harding?s ?Tinkers?
that won 2010 Pulitzer Prizefor Fiction and J. K. Rowling?s ?Harry Potter and the Philoso-pher?s Stone?
that sold over 450 million copies.1753cessful literature, at least within the same genre,making it possible to build a model with surprisinglyhigh accuracy (up to 84%) in predicting the successof a novel.
This result is surprising for two reasons.First, we tackle the hard task of predicting the suc-cess of novels written by previously unseen authors,avoiding incidental learning of authorship signature,since previous research demonstrated that one canachieve very high accuracy in authorship attribution(as high as 96% in some experimental setup) (e.g.,Raghavan et al(2010), Feng et al(2012)).
Sec-ond, we aim to discriminate highly successful nov-els from less successful, but nonetheless publishedbooks written by professional writers, which are un-doubtedly of higher quality than average writings.It is important to note that the task we tackle hereis much harder than discriminating highly success-ful works from those that have not even passed thescrutinizing eyes of publishers.In order to quantify the success of literary works,and to obtain corresponding gold standard labels,one needs to first define ?success?.
For practi-cal convenience, we largely rely on the downloadcounts available at Project Gutenberg as a surrogateto quantify the success of novels.
For a small num-ber of novels however, we also consider award re-cipients (e.g., Pulitzer, Nobel), and Amazon?s salesrecords to define a novel?s success.
We also ex-tend our empirical study to movie scripts, where wequantify the success of movies based on the aver-age review scores at imdb.com.
We leave analysisbased on other measures of literary success as futureresearch.In this study, we do not attempt to separate outsuccess based on literary quality (award winners)from success based on popularity (commercial hit,often in spite of bad literary quality), mainly becauseit is not practically easy to determine whether thehigh download counts are due to only one reason orthe other.
We expect that in many cases, the twodifferent aspects of success are likely to coincide,however.
In the case of the corpus obtained fromProject Gutenberg, where most of our experimentsare conducted, we expect that the download countsare more indicative of success based on the literaryquality (which then may have resulted in popularity)rather than popularity without quality.We examine several genres in fiction and movieGENRE #BOOKS ??
?+Adventure 409 17 100Detective / Mystery 374 25 90Fiction 1148 7 125Historical Fiction 374 25 115Love Stories 342 16 85Poetry 580 9 70Science Fiction 902 30 100Short Stories 1117 9 224Table 1: # of books available per genre at Gutenberg withdownload thresholds used to define more successful (?
?+) and less successful (?
??)
classes.scripts, e.g., adventure stories, mystery, fiction, his-torical fiction, sci-fi, short stories, as well as poetry,and present systematic analyses based on lexical andsyntactic features which have been known to be ef-fective in a variety of NLP tasks ranging from au-thorship attribution (e.g., Raghavan et al(2010)),genre detection (e.g., Rayson et al(2001), Douglasand Broussard (2000)), gender identification (e.g.,Sarawgi et al(2011)) and native language detection(e.g., Wong and Dras (2011)).Our empirical results demonstrate that (1) statis-tical stylometry can be surprisingly effective in dis-criminating successful literature, achieving accuracyup to 84%, (2) some elements of successful stylesare genre-dependent while others are more univer-sal.
In addition, this research results in (3) find-ings that are somewhat contrary to the conventionalwisdom with respect to the connection between suc-cessful writing styles and readability, (4) interestingcorrelations between sentiment / connotation and theliterary success, and finally, (5) comparative insightsbetween fiction and nonfiction with respect to thesuccessful writing style.2 Dataset ConstructionFor our experiments, we procure novels from projectGutenberg2.
Project Gutenberg houses over 40, 000books available for free download in electronic for-mat and provides a catalog containing brief descrip-tions (title, author, genre, language, download count,etc.)
of these books.
We experiment with genres inTable 1, which have sufficient number of books al-lowing us to construct reasonably sized datasets.We use the download counts in Gutenberg-catalog2http://www.gutenberg.org/1754Figure 1: Differences in POS tag distribution between more successful and less successful books across differentgenres.
Negative (positive) value indicates higher percentage in less (more) successful class.as a surrogate to measure the degree of success ofnovels.
For each genre, we determine a lower bound(?+) and an upper bound (??)
of download counts asshown in Table 1 to categorize the available booksas more successful and less successful respectively.These thresholds are set to obtain at least 50 booksfor each class, and for each genre.
To balance thedata, for each genre, we construct a dataset of 100novels (50 per class).We make sure that no single author has more than2 books in the resulting dataset, and in the major-ity of the cases, only one book has been taken fromeach author.3 Furthermore, we make sure that thebooks from the same author do not show up in bothtraining and test data.
These constraints make surethat we learn general linguistic patterns of success-ful novels, rather than a particular writing style of afew successful authors.3 MethodologyIn what follows, we describe five different aspects oflinguistic styles we measure quantitatively.
The firstthree correspond to the features that have been fre-quently utilized in previous studies in related tasks,3The complete list of novels used for each genre in ourdataset is available at http://www.cs.stonybrook.edu/?ychoi/successwithstyle/e.g., genre detection (e.g., Kessler et al(1997))and authorship attribution (e.g., Stamatatos (2009)),while the last two are newly explored in this work.I.
Lexical Choices: unigrams and bigrams.II.
Distribution of Word Categories: Many pre-vious studies have shown that the distribution ofpart-of-speech (POS) tags alone can reveal surpris-ing insights on genre and authorship (e.g., Koppeland Schler (2003)), hence we examine their distri-butions with respect to the success of literary works.III.
Distribution of Grammar Rules: Recentstudies reported that features based on CFG rules arehelpful in authorship attribution (e.g., Raghavan etal.
(2010), Feng et al(2012)).
We experiment withfour different encodings of production rules:?
?
: lexicalized production rules (all productionrules, including those with terminals)?
?G: lexicalized production rules prependedwith the grandparent node.?
?
: unlexicalized production rules (all produc-tion rules except those with terminals).?
?G: unlexicalized production rules prependedwith the grandparent node.1755FEATUREGENREAvgAvg w/oHistoryAdven Myster Fiction Histor Love Poetr Sci-fi ShortPOS 74.0 63.9 72.0 47.0 65.9 63.0 63.0 67.0 64.5 66.9Unigram 84.0 73.0 75.0 60.0 82.0 71.0 61.0 57.0 70.3 71.8Bigram 81.0 73.0 75.0 51.0 72.0 70.0 59.0 57.0 67.2 69.5?
73.0 71.0 75.0 54.0 78.0 74.0 71.0 77.0 71.6 74.1?G 75.0 74.0 75.0 58.0 81.0 72.0 76.0 77.0 73.5 75.7?
72.0 70.0 65.0 53.0 70.0 66.0 64.0 71 66.3 68.2?G 72.0 69.0 74.0 55.0 75.0 69.0 67.0 73.0 69.2 71.2?+Unigram 79.0 73.0 73.0 59.0 80.0 73.0 71.0 73.0 72.6 74.5?G+Unigram 80.0 74.0 74.0 56.0 82.0 72.0 73.0 72.0 72.8 75.2?+Unigram 82.0 72.0 73.0 56.0 81.0 69.0 62.0 59.0 69.2 71.1?G+Unigram 80.0 73.0 74.0 58.0 82.0 70.0 65.0 58.0 70 71.7PHR 74.0 65.0 65.0 56.0 64.0 62.0 69.0 71.0 65.7 67.1PHR+CLS 75.0 69.0 64.0 61.0 59.0 62.0 69.0 67.0 65.7 66.4PHR+Unigram 80.0 74.0 71.0 56.0 79.0 73.0 67.0 66.0 70.7 72.8PHR+CLS+Unigram 80.0 75.0 71.0 56.0 79.0 73.0 66.0 66.0 70.7 72.8Table 2: Classification results in accuracy (%).IV.
Distribution of Constituents: PCFG gram-mar rules are overly specific to draw a big pictureon the distribution of large, recursive syntactic units.We hypothesize that the distribution of constituentscan serve this purpose, and that it will reveal inter-esting and more interpretable insights into writingstyles in highly successful literature.
Despite its rel-ative simplicity, we are not aware of previous workthat looks at the distribution of constituents directly.In particular, we are interested in examining the dis-tribution of phrasal and/or clausal tags as follows:(i) Phrasal tag percent (PHR) - percentage distribu-tion of phrasal tags.4 (ii) Clausal tag percent (CLS)- percentage distribution of clausal tags.V.
Distribution of Sentiment and Connotation:Finally, we examine whether the distribution of sen-timent and connotation words, and their polarity, hasany correlation with respect to the success of literaryworks.
We are not aware of any previous work thatlooks into this connection.4 Prediction PerformanceWe use LibLinear SVM (Fan et al 2008) withL2 tuned over training data, and all performance isbased on 5-fold cross validation.
We take 1000 sen-tences from the beginning of each book.
POS fea-tures are encoded as unit normalized frequency andall other features are encoded as tf-idf.54The percentage of any phrasal tag is the count of occurrenceof that tag over the sum of counts of all phrasal tags.5POS tags are obtained using the Stanford POS tagger(Toutanova and Manning, 2000), and parse trees are based onthe Stanford parser (Klein and Manning, 2003).Prediction Results Table 2 shows the classifica-tion results.
The best performance reaches as highas 84% in accuracy.
In fact, in all genres exceptfor history, the best performance is at least 74%,if not higher.
Another notable observation is thateven in the poetry genre, which is not prose, the ac-curacy gets as high as 74%.
This level of perfor-mance is not entirely anticipated, given that (1) thetest data consists of books written only by previouslyunseen authors, and (2) each author has widely dif-ferent writing style, and (3) we do not have trainingdata at scale, and (4) we aim to tackle the hard taskof discriminating highly successful ones from lesssuccessful, but nonetheless successful ones, as all ofthem were, after all, good enough to be published.6Prediction with Varying Thresholds of Down-load Counts Before we proceed to comprehensiveanalysis of writing style that are prominent in moresuccessful literature (?5), in Table 3, we present howthe prediction accuracy varies as we adjust the defi-nition of more-successful and less-successful litera-ture, by gradually increasing (decreasing) the thresh-old ??
(?+).
As we reduce the gap between ??
and?+, the performance decreases, which shows that in-deed there are notable statistical differences in lin-guistic patterns between novels with high and lowdownload counts, and the stylistic difference mono-tonically increases (thereby higher prediction accu-racy) as we increase the gap between two classes.6In our pilot study, we also experimented with the binaryclassification task of discriminating highly successful ones fromthose that are not even published (unpublished online novels),and it was a much easier task as expected.1756??
?+ ACCURACY17 100 84.025 90 78.435 80 77.645 70 76.455 60 73.5Table 3: Accuracy (%) with varying thresholds of down-load counts for ADVENTURE with unigram features.This is particularly interesting as the size of trainingdata set is actually monotonically decreasing (mak-ing it harder to achieve high accuracy) while we in-crease the separation between ??
and ?+.5 Analysis of Successful Writing Styles5.1 Insights Based on Lexical ChoicesIt is apparent from Table 2 that unigram featuresyield curiously high performance in many genres.We therefore examine discriminative unigrams forADVENTURE, shown in Table 4.
Interestingly, lesssuccessful books rely on verbs that are explicitly de-scriptive of actions and emotions (e.g., ?wanted?,?took?, ?promised?, ?cried?, ?cheered?, etc.
), whilemore successful books favor verbs that describethought-processing (e.g., ?recognized?, ?remem-bered?
), and verbs that serve the purpose of quotesand reports (e.g,.
?say?).
Also, more successfulbooks use discourse connectives and prepositionsmore frequently, while less successful books relymore on topical words that could be almost cliche?,e.g., ?love?, typical locations, and involve more ex-treme (e.g., ?breathless?)
and negative words (e.g.,?risk?
).5.2 Distribution of Sentiment & ConnotationWe also determine the distribution of sentiment andconnotation words separately for each class (Table5) to check if there exists a connection with respectto successful writing styles.7 We first compare dis-tribution of sentiment and connotation for the entirewords.
As can be seen in Table 5 ?
Top, there arenot notable differences.
However, when we comparedistribution only with respect to discriminative uni-grams only (i.e., features with non-zero weights), as7We use MPQA subjectivity lexicon (Wilson et al 2005)and connotation lexicon (Feng et al 2013) for determining sen-timent and connotation of words respectively.Less SuccessfulCATEGORY UNIGRAMSNegativenever, risk, worse, slaves, hard,murdered, bruised, heavy, prison,Body Parts face, arm, body, skinsLocationroom, beach, bay, hills,avenue, boat, doorEmotional / want, went, took, promise,Action Verbs cry, shout, jump, glare, urgeExtreme Wordsnever, very, breathless, sacredslightest, absolutely, perfectlyLove Related desires, affairsMore SuccessfulCATEGORY UNIGRAMSNegation notReport / Quote said, words, saysSelf Reference I, me , myConnectivesand, which, though, that,as, after, but, where, what,whom, since, wheneverPrepositions up, into, out, after, in, withinThinking Verbs recognized, rememberedTable 4: Discriminative unigrams for ADVENTURE.shown in Table 5 ?
Bottom, we find substantial dif-ferences in all genres.
In particular, discriminativeunigrams that characterize less successful novels in-volve significantly more sentiment-laden words.5.3 Distribution of Word CategoriesSummarized analysis of POS distribution across allgenres is reported in Table 6.
It can be seen thatprepositions, nouns, pronouns, determiners and ad-jectives are predictive of highly successful bookswhereas less successful books are characterized byhigher percentage of verbs, adverbs, and foreignwords.
Per genre distributions of POS tags are vi-sualized in Figure 1.
Interestingly, some POS tagsshow almost universal patterns (e.g., prepositions(IN), NNP, WP, VB), while others are more genre-specific.In Relation to Journalism Style The work ofDouglas and Broussard (2000) reveals that informa-tive writing (journalism) involves increased use ofnouns, prepositions, determiners and coordinatingconjunctions whereas imaginative writing (novels)involves more use of verbs and adverbs, as has beenalso confirmed by Rayson et al(2001).
Compar-ing their findings with Table 6, we find that highly1757Adven Myster Fiction Histor Love Poetr Sci-fi Short- + - + - + - + - + - + - + - ++ve S 4.7 4.9 4.8 4.6 5.6 4.9 5.0 5.1 5.5 5.1 6.3 5.7 4.1 3.7 4.7 4.8-ve S 4.0 4.0 4.0 4.0 4.3 4.2 4.2 4.2 4.1 4.2 4.3 4.3 2.9 2.9 3.8 4.0Tot S 8.7 8.9 8.9 8.7 9.9 9.0 9.2 9.3 9.6 9.3 10.6 9.9 7.0 6.7 8.5 8.9+ve C 22.3 22.5 22.3 22.5 23.7 23.0 23.0 23.2 23.34 23.3 23.8 22.9 21.2 20.6 22.6 22.7-ve C 19.4 19.6 19.8 19.8 20.3 19.5 19.2 19.4 20.2 20.4 17.7 17.4 16.6 16.7 18.3 18.9Total C 41.7 42.1 42.1 42.3 44.0 42.5 42.3 42.6 43.5 43.7 41.5 40.3 37.9 37.3 41.0 41.6+ve S 3.5 1.8 4.1 2.0 3.7 1.4 3.0 1.0 3.4 1.3 3.9 2.0 7.3 5.9 5.1 2.7-ve S 5.5 3.4 6.3 3.6 5.5 2.9 4.7 1.9 5.1 2.6 5.8 3.3 9.0 8.0 7.3 4.8Total S 9.1 5.2 10.4 5.6 9.2 4.3 7.7 3.0 8.5 3.9 9.7 5.2 16.3 13.9 12.4 7.5+ve C 12.9 8.9 14.3 9.8 12.9 8.5 11.5 6.2 12.0 7.7 14.0 9.6 19.6 19.2 16.5 11.9-ve C 14.1 9.8 15.2 10.9 13.7 9.9 12.4 7.0 12.9 8.5 14.3 10.3 20.0 19.7 17.0 13.3Total C 27.0 18.7 29.5 20.7 26.6 18.4 23.9 13.2 24.87 16.1 28.3 19.8 39.7 38.9 33.5 25.2Table 5: Top: Distribution of sentiment (connotation) among entire unigrams.
Bottom: distribution of sentiment(connotation) among discriminative unigrams.
?S?
and ?C?
stand for sentiment and connotation respectively.More SuccessfulCATEGORY SUB-CATEGORY DIFFPrepositions General 0.00592Determiners General 0.00226NounsPlural 0.00189Proper (Singular) 0.00016Coord.
conj.
General 0.00118Numbers General 0.00102PronounsPosesseive 0.00081General WH 0.00042Possessive WH 5.4E-05AdjectivesGeneral 0.00102Superlative 0.00011Less SuccessfulCATEGORY SUB-CATEGORY DIFFAdverbsGeneral -0.00272General WH -0.00028VerbsBase -0.00239Non-3rd sing.
present -0.00084Past tense -0.00041Past participle -0.000393rd person sing.
present -0.00036Modal -0.00091Foreign General -0.00067Symbols General -0.00018Interjections General -0.00016Table 6: Top discriminative POS tags.successful books tend to bear closer resemblance toinformative articles.5.4 Distribution of ConstituentsIt can be seen in Table 2 that deep syntactic fea-tures expressed in terms of different encodings ofproduction rules consistently yield good perfor-mance across almost all genres.
Production rulesare overly specific to gain more generalized, in-terpretable, high-level insights however (Feng etal., 2012).
Therefore, similarly as word categories(POS), we consider the categories of nonterminalnodes of the parse trees, in particular, phrasal andclausal tags, as they represent the gist of constituentstructure that goes beyond shallow syntactic infor-mation represented by POS.Table 8 shows how the distribution of phrasal andclausal tags differ for successful books when com-puted over all genres.
Positive (negative) DIFF val-ues indicate that the corresponding tags are favoredin more successful (less successful) books whencounted across all genres.
We also report the num-ber of genres (#Genres) in which the individual dif-ference is positive / negative.In terms of phrasal tags, we find that more suc-cessful books are composed of higher percentage ofPP, NP and wh-noun phrases (WHNP), whereas lesssuccessful books are composed of higher percentageof VP, adverb phrases (ADVP), interjections (INTJ)and fragments (FRAG).
Notice that this observationis inline with our earlier findings with respect to thedistribution of POS.In regard to clausal tags, more successful booksinvolve more clausal tags that are necessary for com-plex sentence structure and inverted sentence struc-ture (SBAR, SBARQ and SQ) whereas less success-ful books rely more on simple sentence structure (S).Figure 2 shows the visualization of the distributionof these phrasal and clausal tags.It is also worth to mention that phrasal and clausal1758Figure 2: Difference between phrasal and clausal tag percentage distributions of more successful and less successfulbooks across different genres.
Specifically, we plot D?
?D+, where D+ is the phrasal tag distribution (in %) of moresuccessful books and D?
is the phrasal tag distribution (in %) of less successful books.READABILITY INDICES More Succ.
Less Succ.FOG index 9.88 9.80Flesch index 87.48 87.64Table 7: Readability: Lower FOG and higher Flesch in-dicate higher readability (numbers in Boldface).tags alone can yield classification performance thatare generally better than that of POS tags, in spite ofthe very small feature set (26 tags in total).
In fact,constituent tags deliver the best performance in caseof historical fiction genre (Table 2).Connection to Readability Pitler and Nenkova(2008) provide comprehensive insights into assess-ment of readability.
In their work, among the mostdiscriminating features characterizing text with bet-ter readability is increased use of verb phrases (VP).Interestingly, contrary to the conventional wisdom ?that readability is of desirable quality of good writ-ings ?
our findings in Table 2 suggest that the in-creased use of VP correlates strongly with the writ-ing style of the opposite spectrum of highly success-ful novels.As a secondary way of probing the connection be-tween readability and the writing style of successfulliterature, we also compute two different readabil-ity measures that have been used widely in priorliterature (e.g., Sierra et al(1992), Blumenstock(2008), Ali et al(2010)): (i) Flesch reading easescore (Flesch, 1948), (ii) Gunning FOG index (Gun-ning, 1968).
The overall weighted average readabil-ity scores are reported in Table 7.
Again, we find thatless successful novels have higher readability com-pared to more successful ones.The work of Sawyer et al(2008) provides yetanother interesting contrasting point, where the au-thors found that award winning academic papers inmarketing journals correlate strongly with increasedreadability, characterized by higher percentage ofsimple sentences.
We conjecture that this oppositetrend is likely to be due to difference between fic-tion and nonfiction, leaving further investigation asfuture research.In sum, our analysis reveals an intriguing andunexpected observation on the connection betweenreadability and the literary success ?
that they cor-relate into the opposite directions.
Surely our find-ings only demonstrate correlation, not to be con-1759Phrasal + ?
DIFF #+Gen/#?GenADJP 0.030 0.031 -6E-4 5/3ADJP 0.030 0.031 -6E-4 5/3ADVP 0.052 0.054 -0.002 2/6CONJP 3E-4 3E-4 2E-5 5/3FRAG 0.008 0.008 -1E-4 2/6LST 2E-4 1E-4 5E-5 6/2NAC 9E-6 6E-6 3E-6 5/3NP 0.459 0.453 0.005 6/2NX 1E-4 1E-4 -4E-7 3/5PP 0.122 0.117 0.005 7/1PRN 0.005 0.004 2E-4 4/4PRT 0.010 0.010 -5E-4 3/5QP 0.001 0.001 7E-5 6/2RRC 8E-5 8E-5 6E-6 6/2UCP 8E-4 7E-4 1E-4 8/0VP 0.292 0.300 -0.008 1/7WHADJP 2E-4 2E-4 -5E-5 1/7WHAVP 0 0 0 -WHNP 0.013 0.012 0.001 8/0WHPP 0.001 9E-4 1E-4 6/2X 0.001 0.001 -4E-5 4/4Clausal + ?
DIFF +Gen/#?GenSBAR 0.166 0.164 0.002 4/4SQ 0.020 0.018 0.002 7/1SBARQ 0.014 0.013 0.001 7/1SINV 0.018 0.018 -6E-5 5/3S 0.781 0.785 -0.004 3/5Table 8: Overall Phrasal / Clausal Tag Distribution andanalysis.
All values are rounded to [3-5] decimal places.fused as causation, between readability and literarysuccess.
We conjecture that the conceptual complex-ity of highly successful literary work might requiresyntactic complexity that goes against readability.6 Literature beyond Project GutenbergOne might wonder how the prediction algorithmstrained on the dataset based on Project Gutenbergmight perform on books not included at Guten-berg.
This section attempts to address such a ques-tion.
Due to the limited availability of electronicallyavailable books that are free of charge however, wecould not procure more than a handful of books.86.1 Highly Successful BooksFirst, we apply the classifiers trained on the ProjectGutenberg dataset (all genres merged) on a few ex-tremely successful novels (Pulitzer prize, NationalAward recipients, etc).
Table 9 shows the results of8We report our prediction results on all books beyondProject Gutenberg of which we managed to get electroniccopies, i.e., the results in Table 9 are not cherry-picked.MORE SUCCESSFULBOOK (Q) PDKL UPDKL Su S??
?Don Quixote?
0.139 0.152 + +?
Miguel De Cervantes?Other Voices, Other Rooms?
0.014 0.010 + +?
Truman Capote?The Fixer?
0.013 0.015 + +?
Bernard Malamud?Robinson Crusoe?
0.042 0.051 + +?
Daniel Defoe?The old man and the sea?
0.065 0.060 + +?
Ernest Hemingway?A Tale of Two Cities?
0.027 0.030 + +?
Charles Dickens?Independence Day?
0.031 0.026 + +?
Richard Ford?Rabbit At Rest?
0.047 0.048 + +?
John Updike?American Pastoral?
0.039 0.043 + +?
Philip Roth?Dr Jackel and Mr. Hyde?
0.036 0.037 + +?
Robert StevensonLESS SUCCESSFUL?The lost symbol?
0.046 0.042 - -?
Dan Brown?The magic barrel?
0.0288 0.0284 + -?
Bernard Malamud?Two Soldiers?
0.130 0.117 - +?
William Faulkner?My life as a man?
0.046 0.052 - +?
Philip RothTable 9: Prediction on books beyond Gutenberg.
Shadedentries indicate incorrect predictions.two classification options: (1) KL-divergence based,and (2) unigram-feature based.Although KL-divergence based prediction wasnot part of the classifiers that we explored in the pre-vious sections, we include it here mainly to providebetter insights as to which well-known books sharecloser structural similarity to either more or less suc-cessful writing style.
As a probability model, we usethe distributions of phrasal tags, as those can give usinsights on deep syntactic structure while suppress-ing potential noises due to topical variances.
Table 9shows symmetrised KL-divergence between each ofthe previously unseen novels and the collection ofbooks from Gutenberg corresponding to more suc-cessful (less successful) labels.
For prediction, thelabel with smaller KL is chosen.Based only on the distribution of 26 phrasal tags,the KL divergence classifier is able to make correct1760predictions on 7 out of 10 books, a surprisingly highperformance based on mere 26 features.
Of course,considering only the distribution of phrasal tags issignificantly less informed than considering numer-ous other features that have shown substantially bet-ter performance, e.g., unigrams and CFG rewriterules.
Therefore, we also present the SVM classi-fier trained on unigram features.
It turns out uni-gram features are powerful enough to make correctpredictions for all ten books in Table 9.Hemingway and Minimalism It is good to thinkabout where and why KL-divergence-based ap-proach fails.
In fact, when we included Heming-way?s The Old Man and the Sea into the test set, wewere expecting some level of confusions when rely-ing only on high-level syntactic structure, as Hem-ingway?s signature style is minimalism, with 70%of his sentences corresponding to simple sentences.Not surprisingly, more adequately informed clas-sifiers, e.g., SVM with unigram features, are stillable to recognize Hemingway?s writings as those ofhighly successful ones.6.2 Less Successful BooksIn order to obtain less successful books, we considerthe Amazon seller?s rank included in the product de-tails of a book.
The less successful books consideredin Table 9 had an Amazon seller?s rank beyond 200k(higher rank indicating less commercial success) ex-cept Dan Brown?s The lost symbol, which we in-cluded mainly because of negative critiques it hadattracted from media despite its commercial success.As shown in Table 9, all three classifiers make (ar-guably) correct predictions on Dan Brown?s book.9This result also supports our earlier assumption onthe nature of novels available at Project Gutenberg?
that they would be more representative of liter-ary success than general popularity (with or withoutliterary quality).7 Predicting Success of Movie ScriptsWe have seen successful results in the novel domain,but can stylometry-based prediction work on verydifferent domains, such as screenplays?
Unlike nov-els, movie scripts are mostly in dialogues, which9Most notable pattern based on phrasal tag analysis is a sig-nificantly increased use of fragments (FRAG), which associatesstrongly with less successful books in our dataset.FEATURE Adven Fanta Roman ThrilPOS 62.0 58.0 61.7 56.0Unigram 62.0 81.3 70.0 80.0Bigrams 73.3 84.7 80.8 76.0?
66.0 81.3 70.0 76.0?G 62.0 69.3 86.7 60.0?
62.0 81.3 78.3 76.0?G 69.3 77.3 77.5 68.0?+Uni 62.0 85.3 70.0 76.0?G+Uni 54.7 81.3 70.0 76.0?+Uni 58.0 89.3 70.0 76.0?G+Uni 58.0 84.7 70.0 76.0PHR 46.0 42.7 65.8 80.0PHR+CLR 76.7 31.3 65.8 80.0PHR+Uni 62.0 81.3 70.0 80.0PHR+CLR+Uni 62.0 81.3 70.0 80.0Table 10: Classification results on movie dialogue data(rating ?
8 vs rating ?
5.5).are likely to be more informal.
Also, what to keepin mind is that much of the success of movies de-pends on factors beyond the quality of writing of thescripts, such as the quality of acting, the popularityof actors, budgets, artistic taste of directors and pro-ducers, editing and so forth.We use the Movie Script Dataset introduced inDanescu-Niculescu-Mizil and Lee (2011).
It in-cludes the dialogue scripts of 617 movies.
The aver-age rating of all movies is 6.87.
We consider movieswith IMDb rating ?
8 as ?more successful?, theones with IMDb rating ?
5.5 as ?less successful?.We combine all the dialogues of each movie andfilter out the movies with less than 200 sentences.There are 11 genres (?ADVENTURE?, ?FANTASY?,?ROMANCE?, ?THRILLER?, ?ACTION?, ?COMEDY?,?CRIME?, ?DRAMA?, ?HORROR?, ?MYSTERY?, ?SCI-FI?)
with 15 movies or more per class, we take 15movies per class and perform classification taskswith the same experiment setting as Table 2.Table 10, we show some of the example genreswith relatively successful outcome, reaching as highas 89.3% accuracy in FANTASY genre.
We wouldlike to note however that in many other genres, theprediction did not work as well as it did for the noveldomain.
We suspect that there are at least two rea-sons for this: it must be partly due to very limiteddata size ?
only 15 instances per class with the rat-ing threshold we selected for defining the success of1761movies.
The second reason is due to many other ex-ternal factors that can also influence the success ofmovies, as discussed earlier.8 Related WorkPredicting success of novels and movies: To thebest of our knowledge, our work is the first that pro-vides quantitative insights into the unstudied con-nection between the writing style and the success ofliterary works.
There have been some previous workthat aims to gain insights into the secret recipe ofsuccessful books, but most were qualitative, basedonly on a dozen of books, focusing mainly on thehigh-level content of the books, such as the per-sonalities of protagonists, antagonists, the nature ofplots (e.g., Harvey (1953), Yun (2011)).
In con-trast, our work examines a considerably larger col-lection of books (800 in total) over eight differentsub-genres, providing insights into lexical, syntac-tic, and discourse patterns that characterize the writ-ing styles commonly shared among the successfulliterature.
Another relevant work has been on a dif-ferent domain of movies (Yun, 2011), however, theprediction is based only on external, non-textual in-formation such as the reputation of actors and direc-tors, and the power of distribution systems etc, with-out analyzing the actual content of the movie scripts.Text quality and readability: Louis (2012) ex-plored various features that measure the quality oftext, which has some high-level connections to ourwork.
Combining the insights from Louis (2012)with our results, we find that the characteristics oftext quality explored in Louis (2012), readability oftext in particular, do not correspond to the prominentwriting style of highly successful literature.
Therehave been a number of other work that focused onpredicting and measuring readability (e.g., Kate etal.
(2010), Pitler and Nenkova (2008), Schwarm andOstendorf (2005), Heilman and Eskenazi (2006) andCollins-Thompson et al(2004)) employing variouslinguistic features.There is an important difference however, in re-gard to the nature of the selected text for analysis:most studies in readability focus on differentiatinggood writings from noticeably bad writings, ofteninvolving machine generated text or those writtenby ESL students.
In contrast, our work essentiallydeals with differentiating good writings from evenbetter writings.
After all, all the books that we an-alyzed are written by expert writers who passed thescrutinizing eyes of publishers, hence it is reason-able to expect that the writing quality of even lesssuccessful books is respectful.Predicting success among academic papers: Inthe domain of academic papers, which belongs tothe broad genre of non-fiction, the work of Sawyeret al(2008) investigated the stylistic characteris-tics of award winning papers in marketing journals,and found that the readability plays an importantrole.
Combined with our study which focuses on fic-tion and creative writing, it suggests that the recipefor successful publications can be very different de-pending on whether it belongs to fiction or nonfic-tion.
The work of Bergsma et al(2012) is alsosomewhat relevant to ours in that their work in-cluded differentiating the writing styles of workshoppapers from major conference papers, where the lat-ter would be generally considered to be more suc-cessful.9 ConclusionWe presented the first quantitative study that learnsto predict the success of literary works based on theirwriting styles.
Our empirical results demonstratedthat statistical stylometry can be surprisingly effec-tive in discriminating successful literature, achiev-ing accuracy up to 84% in the novel domain and89% in the movie domain.
Furthermore, our studyresulted in several insights including: lexical andsyntactic elements of successful styles, the connec-tion between successful writing style and readabil-ity, the connection between sentiment / connotationand the literary success, and last but not least, com-parative insights between successful writing stylesof fiction and nonfiction.Acknowledgments This research was supportedin part by the Stony Brook University Office ofthe Vice President for Research, and in part by giftfrom Google.
We thank anonymous reviewers, SteveGreenspan, and Mike Collins for helpful commentsand suggestions, Alex Berg for the title, and ArunNampally for helping with the preliminary work.1762ReferencesOmar Ali, Ilias N Flaounas, Tijl De Bie, Nick Mosdell,Justin Lewis, and Nello Cristianini.
2010.
Automat-ing news content analysis: An application to genderbias and readability.
Journal of Machine LearningResearch-Proceedings Track, 11:36?43.Shlomo Argamon, Moshe Koppel, Jonathan Fine, andAnat Rachel Shimoni.
2003.
Gender, genre, and writ-ing style in formal written texts.
TEXT-THE HAGUETHEN AMSTERDAM THEN BERLIN-, 23(3):321?346.Shane Bergsma, Matt Post, and David Yarowsky.
2012.Stylometric analysis of scientific articles.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 327?337.Association for Computational Linguistics.Joshua E Blumenstock.
2008.
Automatically assessingthe quality of wikipedia articles.Kevyn Collins-Thompson, James P. Callan, and James P.Callan.
2004.
A language modeling approach to pre-dicting reading difficulty.
In HLT-NAACL, pages 193?200.Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011.Chameleons in imagined conversations: A new ap-proach to understanding coordination of linguisticstyle in dialogs.
In Proceedings of the Workshop onCognitive Modeling and Computational Linguistics,ACL 2011.Dan Douglas and Kathleen M Broussard.
2000.
Long-man grammar of spoken and written english.
TESOLQuarterly, 34(4):787?788.Alvar Ellega?rd.
1962.
A Statistical method for determin-ing authorship: the Junius Letters, 1769-1772, vol-ume 13.
Go?teborg: Acta Universitatis Gothoburgen-sis.Hugo J Escalante, Thamar Solorio, and M Montes-yGo?mez.
2011.
Local histograms of character n-grams for authorship attribution.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,volume 1, pages 288?298.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
Liblinear: A libraryfor large linear classification.
The Journal of MachineLearning Research, 9:1871?1874.Song Feng, Ritwik Banerjee, and Yejin Choi.
2012.Characterizing stylistic elements in syntactic struc-ture.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 1522?1533.
Association for Computational Lin-guistics.Song Feng, Jun Sak Kang, Polina Kuznetsova, and YejinChoi.
2013.
Connotation lexicon: A dash of senti-ment beneath the surface meaning.
In Proceedings ofthe 51th Annual Meeting of the Association for Com-putational Linguistics (Volume 2: Short Papers), Sofia,Bulgaria, Angust.
Association for Computational Lin-guistics.Rudolph Flesch.
1948.
A new readability yardstick.Journal of applied psychology, 32(3):221.Robert Gunning.
1968.
The technique of clear writing.McGraw-Hill New York.James W Hall.
2012.
Hit Lit: Cracking the Code ofthe Twentieth Century?s Biggest Bestsellers.
RandomHouse Digital, Inc.John Harvey.
1953.
The content characteristics of best-selling novels.
Public Opinion Quarterly, 17(1):91?114.Michael Heilman and Maxine Eskenazi.
2006.
Languagelearning: Challenges for intelligent tutoring systems.In Proceedings of the workshop of intelligent tutor-ing systems for ill-defined tutoring systems.
Eight in-ternational conference on intelligent tutoring systems,pages 20?28.Rohit J Kate, Xiaoqiang Luo, Siddharth Patwardhan,Martin Franz, Radu Florian, Raymond J Mooney,Salim Roukos, and Chris Welty.
2010.
Learning topredict readability using diverse linguistic features.
InProceedings of the 23rd International Conference onComputational Linguistics, pages 546?554.
Associa-tion for Computational Linguistics.Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.1997.
Automatic detection of text genre.
In Proceed-ings of the 35th Annual Meeting of the Association forComputational Linguistics and Eighth Conference ofthe European Chapter of the Association for Computa-tional Linguistics, pages 32?38.
Association for Com-putational Linguistics.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Associ-ation for Computational Linguistics.Moshe Koppel and Jonathan Schler.
2003.
Exploit-ing stylistic idiosyncrasies for authorship attribution.In Proceedings of IJCAI?03 Workshop on Computa-tional Approaches to Style Analysis and Synthesis, vol-ume 69, page 72.
Citeseer.Annie Louis.
2012.
Automatic metrics for genre-specifictext quality.
Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies: Student Research Workshop, page 54.Jerome McGann.
1998.
The Poetics of Sensibility: ARevolution in Literary Style.
Oxford University Press.1763Fuchun Peng, Dale Schuurmans, Shaojun Wang, andVlado Keselj.
2003.
Language independent author-ship attribution using character level language mod-els.
In Proceedings of the tenth conference on Eu-ropean chapter of the Association for ComputationalLinguistics-Volume 1, pages 267?274.
Association forComputational Linguistics.Emily Pitler and Ani Nenkova.
2008.
Revisiting read-ability: a unified framework for predicting text qual-ity.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 186?195, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Sindhu Raghavan, Adriana Kovashka, and RaymondMooney.
2010.
Authorship attribution using proba-bilistic context-free grammars.
In Proceedings of theACL 2010 Conference Short Papers, pages 38?42.
As-sociation for Computational Linguistics.Paul Rayson, Andrew Wilson, and Geoffrey Leech.2001.
Grammatical word class variation within thebritish national corpus sampler.
Language and Com-puters, 36(1):295?306.Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.2011.
Gender attribution: tracing stylometric evi-dence beyond topic and genre.
In Proceedings of theFifteenth Conference on Computational Natural Lan-guage Learning, pages 78?86.
Association for Com-putational Linguistics.Alan G Sawyer, Juliano Laran, and Jun Xu.
2008.The readability of marketing journals: Are award-winning articles better written?
Journal of Marketing,72(1):108?117.Sarah E. Schwarm and Mari Ostendorf.
2005.
Read-ing level assessment using support vector machinesand statistical language models.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics, ACL ?05, pages 523?530, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Arlene E Sierra, Mark A Bisesi, Terry L Rosenbaum, andE James Potchen.
1992.
Readability of the radiologicreport.
Investigative radiology, 27(3):236?239.Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
Journal of the Ameri-can Society for information Science and Technology,60(3):538?556.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In In EMNLP/VLC2000, pages 63?70.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing, pages 347?354.Association for Computational Linguistics.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploitingparse structures for native language identification.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 1600?1610.Association for Computational Linguistics.Chang-Joo Yun.
2011.
Performance evaluation of intel-ligent prediction models on the popularity of motionpictures.
In Interaction Sciences (ICIS), 2011 4th In-ternational Conference on, pages 118?123.
IEEE.1764
