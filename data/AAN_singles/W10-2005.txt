Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 36?44,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsThe role of memory in superiority violation gradienceMarisa Ferrara BostonCornell UniversityIthaca, NY, USAmfb74@cornell.eduAbstractThis paper examines how grammatical andmemory constraints explain gradience insuperiority violation acceptability.
A com-putational model encoding both categoriesof constraints is compared to experimentalevidence.
By formalizing memory capac-ity as beam-search in the parser, the modelpredicts gradience evident in human data.To predict attachment behavior, the parsermust be sensitive to the types of nominalintervenors that occur between a wh-fillerand its head.
The results suggest memoryis more informative for modeling violationgradience patterns than grammatical con-straints.1 IntroductionSentences that include twowh-words, as in Exam-ple (1), are often considered difficult by Englishspeakers.
(1) *Diego asked what1 who2 read?This superiority effect holds when a second wh-word, who in this example, acts as a barrier to at-tachment of the first wh-word and its verb (Chom-sky, 1973).The difficulty is ameliorated when the wh-words are switched to which-N, or which-Noun,form as in Examples (2) and (3) (Karttunen, 1977;Pesetsky, 1987).
This is confirmed by experimen-tal evidence (Arnon et al, To appear; Hofmeister,2007).
(2) ?Diego asked which book who read?
(3) ?Diego asked what which girl read?Memory is often implicated as the source of thisgradience, though it is unclear which aspects ofmemory best model experimental results.
Thiscomputational model encodes grammatical andmemory-based constraints proposed in the liter-ature to account for the phenomenon.
The re-sults demonstrate that as memory resources are in-creased, the parser can model the human pattern ifit is sensitive to the types of nominal intervenors.This supports memory-based accounts of superi-ority violation (SUV) gradience.2 Explanations for SUV gradienceThis section details grammatical and reductionistexplanations for SUV gradience, motivating theencoding of various constraints in the computa-tional model.2.1 Grammatical explanationsGrammatical accounts of gradience rely on intrin-sic discourse differences between phrases that al-low for SUVs and those that do not.
In this work,which-N phrases are examples of the former, andso-called bare wh-phrases (including who andwhat) the latter1.
Rizzi (1990) incorporates ideasfrom Pesetsky?s D-Linking, or discourse-linking,hypothesis (1987) into a grammatical account ofSUV gradience, Relativized Minimality.
He ar-gues that referential phrases like which-N referto a pre-established set in the discourse and arenot subject to the same constraints on attachmentas non-referential phrases, like what.
Which bookdelimits a set of possible discourse entities, books,and is more restrictive than what, which could in-stead delimit sets of books, cats, or abstract en-tities.
The Relativized Minimality hypothesis ac-counts for SUV gradience on the basis of this cate-gorical separation on wh-phrases in the discourse.1Both bare phrases and which-N phrases could have theappropriate discourse conditions to allow for superiority vio-lations, and vice versa.
However, to relate the theory?s pre-dictions to the experiment modeled here, I use a categoricalsplit between which-N and bare wh-phrases.362.2 Reductionist explanationsMany grammatical accounts, particularly thosethat are grounded in cognitive factors, incorporatesome element of processing or memory in their ex-planations (Phillips, Submitted).
Reductionist ac-counts are different; their proponents do not be-lieve that superiority requires a grammatical ex-planation.
Rather, SUVs that appear ungrammat-ical, such as Example (1), are the result of severeprocessing difficulty alone.These accounts attribute processing difficulty tomemory: severe memory resource limitations ac-count for ungrammatical sentences in SUVs, andincreased memory resources allow for more ac-ceptable sentences.
This is the central idea be-hind Hofmeister?s Memory Facilitation Hypothe-sis (2007):Memory Facilitation HypothesisLinguistic elements that encode more informa-tion (lexical, semantic, syntactic, etc.)
facili-tate their own subsequent retrieval from memory(Hofmeister, 2007, p.4)2.This memory explanation is central to activation-based memory hypotheses previously proposedin the psycholinguistic literature, such as CC-READER (Just and Carpenter, 1992), ACT-R(Lewis and Vasishth, 2005), and 4CAPS (Just andVarma, 2007).
This work considers activation,and manipulates memory resources by varying thenumber of analyses the parser considers at eachparse step.Table 1 lists memory factors that may contributeto SUV gradience.
They are sensitive to the mem-ory resources available during syntactic parsing,but account for memory differently.
Below I de-scribe these variations.2.2.1 Distance and the DLTDistance, as measured by the number of wordsbetween, for example, a wh-word and its verb,has been argued to affect sentence comprehen-sion (Wanner and Maratsos, 1978; Rambow andJoshi, 1994; Gibson, 1998).
Experimental evi-dence supports this claim, but there exist a num-ber of anomalous results that resist explanation interms of distance alone (Gibson, 1998; Hawkins,1999; Gibson, 2000).
For example, it is not thecase that processing difficulty increases solely as2Recent work by Hofmeister and colleagues attributes theadvantage to a decrease in memory interference rather thanretrieval facilitation (Submitted), but the spirit of the workremains the same.a function of the number of words in a sentence.However, it is possible that SUV gradience couldbe affected by this simple metric.The Dependency Locality Theory (DLT) (Gib-son, 2000) is a more linguistically-informed mea-sure of distance.
The DLT argues that an accuratemodel of sentence processing difficulty is sensitiveto the number and discourse-status (given or new)of nominal intervenors that occur across a particu-lar distance.
The DLT?s sensitivity to discourse-newness integrates aspects of D-linking: whichbook, for example, requires that books already be apart of the discourse, though what does not (Gun-del et al, 1993; Warren and Gibson, 2002).
TheDLT has been demonstrated to model difficulty inways that simple distance alone can not (Grodnerand Gibson, 2005).This study also considers a stronger version ofthe DLT, Intervenors.
Intervenors considers boththe number and part-of-speech (POS) of nominalintervenors between a wh-word and its head.
Thisfeature is sensitive to nuanced differences betweennominal intervenors, providing a more accuratemodel of the Memory Facilitation Hypothesis.2.2.2 Stack memoryDistance can also be measured in terms of theparser?s internal resources.
The computationalmodel described here incorporates a stack mem-ory.
Although stacks are not accurate models ofhuman memory (McElree, 2000), this architec-tural property may provide insight into how mem-ory affects SUV gradience.2.2.3 Activation and interferenceSentence processing difficulty has been attributedto the amount of time it takes to retrieve a wordfrom memory.
Lewis & Vasishth (2005) find sup-port for this argument by applying equations froma general cognitive model, ACT-R (Adaptive Con-trol of Thought-Rational) (Anderson, 2005), to asentence processsing model.
Their calculation ofretrieval time, henceforth retrieval, is sensitive to aword?s activation and its similarity-based interfer-ence with other words in memory (Gordon et al,2002; Van Dyke and McElree, 2006).
Activation,Interference, and the conjunction of the two in theform of Retrieval, are considered in this work.The grammatical and memory-based accountsdescribed above offer several explanations forSUV gradience.
They can be represented along acontinuum, where the type of information consid-37Hypothesis Sensitive toDistance String distance between words.DLT Number of nominal intervenors.Intervenors POS of nominal intervenors.Stack Memory Elements currently in parser memory.Baseline Activation Amount structure is activated in memory.Interference Amount of competition from similar words in memory.Retrieval Retrieval time of word from memory.Table 1: Memory-based sentence processing theories.ered in memory varies from the simple (Distance)to complex (Retrieval), as in (4).
(4) Distance < DLT < Intervenors < Stack Memory< Activation < Interference < RetrievalDespite this representation, in this work I con-sider each as an independent theory.
Together,they form the hypothesis set in the model, se-lected because they represent the major explana-tions posited for gradience in SUVs and relatedphenomena, like islands.The computational model not only formalizesthe memory accounts, but also provides a frame-work for memory-based factors that require acomputational model, such as retrieval.
The re-sults determine memory factors that best accountfor SUV gradience patterns.3 MethodologyThe test set for SUV gradience is the experimentalresults from Arnon et al (To appear).
The experi-ment tests gradience across four conditions, shownin Examples (5)-(8).
(5) Pat wondered what who read.
(bare.bare)(6) Pat wondered what which student read.
(bare.which)(7) Pat wondered which book who read.
(which.bare)(8) Pat wondered which book which student read.
(which.which)The conditions substitute the wh-type of both wh-fillers and wh-intervenors in the island context.In Example (5) both the filler and intervenor arebare (the bare.bare condition), whereas in Exam-ple (8), both the filler and intervenor are which-Ns(which.which).
Examples (6) and (7) provide theother possible configurations.Arnon and colleagues find which.which to bethe fastest condition.
Figure 1 depicts these re-sults.
The other conditions are more difficult,Figure 1: Reading time is fastest in thewhich.which condition (Arnon et al, To appear,p.5).at varying levels: the which.bare condition isless difficult than the bare.which condition, andboth are less difficult than the bare.bare condi-tion.
These results roughly pattern with accept-ability judgments discussed in syntactic literature(Pesetsky, 1987).Corpora for superiority processing results donot exist.
Further, few studies on SUVs incorpo-rate the same structures, techniques, and experi-mental conditions.
Although Arnon et al consid-ered 20 lexical variations, the unlexicalized parsercan not distinguish these variations.
Therefore, theparser is only evaluated on these four sentences;however, they are taken to represent classes ofstructures that generalize to all SUV gradience inEnglish.3.1 The parsing modelThe computational model is based on Nivre?s(2004) dependency parsing algorithm.
The al-gorithm builds directed, word-to-word analysesof test input following the Dependency Gram-mar syntactic formalism (Tesnie`re, 1959; Hays,1964).
Figure 2 depicts the full dependency anal-ysis of the which.which condition from Example38Figure 2: A dependency analysis of thewhich.which condition.
(8), where heads point to their dependents via arcs.The Nivre parser assembles dependency struc-ture incrementally by passing through parser statesthat aggregate four data structures, shown in Table2.
The stack ?
holds parsed words that require fur-ther analysis, and the list ?
holds words yet to beparsed.
h and d encode the current list of depen-dency relations.?
A stack of already-parsed unreduced words.?
An ordered input list of words.h A function from dependent words to heads.d A function from dependent words to arc types.Table 2: Parser configuration.The parser transitions from state to state via fourpossible actions.
Shift and Reducemanipulate?.
LeftArc and RightArc build dependenciesbetween ?1 (the element at the top of the stack)and ?1 (the next input word); LeftArc makes?1 the dependent, and RightArc makes ?1 thehead.The parser determines actions by consulting aprobability model derived from the Brown Corpus(Francis and Kucera, 1979).
The corpus is con-verted to dependencies via the Pennconverter tool(Johansson and Nugues, 2007).
The parser is thensimulated on these dependencies, providing a cor-pus of parser states and subsequent actions thatform the basis of the training data.
Because theparser is POS-based, this corpus is manipulated intwo ways to sensitize it to the differences in theexperimental conditions.
First, the corpus is givenfiner-grained POS tags for each of the wh-words,described in Table 3.Secondly, which-N dependencies are encodedas DPs (determiner phrases) and are headed bythe wh-phrase (Abney, 1987).
This ensures theparser differentiates a wh-word retrieval from asimple noun retrieval, which is necessary for sev-eral of the memory-based constraints.
Other nounphrases are headed by their nouns.
The corpus isOriginal POS Wh ExampleWP WP-WHAT whatWP WP-WHO whoWDT WDT-WHICH which bookWDT WDT-WHAT what bookIN IN-WHETHER whetherWRB WRB how/why/whenTable 3: POS for wh fillers and intervenors.Figure 3: The relevant attachment is betweenwhich and read.not switched to a fully DP analysis to preserve asmany of the original relationships as possible.I extend the Nivre algorithm to allow for beamsearch within the parser state space.
This allowsthe parser to consider different degrees of paral-lelism k, and manipulate the amount of memoryallotted to incremental parse states.
This manipu-lation serves as a model of variation in an individ-ual?s memory as a sentence is parsed.3.2 EvaluationTo determine how well the accounts model the ex-perimental data, I consider the likelihood of theparser resolving the island-violating dependencybetween wh-fillers and their verbs in the Arnon etal.
data.
In terms of the dependency parser, the testdetermines whether the parser creates a LeftArcattachment in a state where which or what is ?1and read is ?1 .
The dependency structure associ-ated with this parser state is depicted in Figure 3for the which.which condition.This evaluation is categorical rather than statis-tical: SUV-processing is based on the decision toform an attachment in a superiority-violating con-text, given four experimental sentences.
While fu-ture work will incorporate more experiments forrobust statistical analysis, this work focuses on asmall subset that generalizes to the greater phe-nomenon.3.3 Encoding constraintsThe parser determines actions on the basis of prob-abilistic models, or features.
In this work, I en-39code each of the grammatical and memory-basedexplanations as its own feature.
I normalize theweights from the LIBLINEAR (Lin et al, 2008)SVM classification tool to determine probabilitiesfor each parser action (LeftArc, RightArc,Shift, Reduce) .
The features are sensitive tospecific aspects of the current parser state, allow-ing an examination of whether the features sug-gest the superiority violating LeftArc action inthe context depicted in Figure 3.
The prediction isthat attachment will be easiest in the which.whichcondition and impossible in the other conditionswhen memory resources are limited (k=1), as inTable 4.Condition b.b b.w w.b w.wAttachment N N N YTable 4: LeftArc attachments given Arnon et al(To appear) results.
Y = Yes, N=No.Table 5 depicts the full list of grammatical andmemory-based features considered in this study,which are detailed below.3.3.1 Grammatical constraintIn Relativized Minimality, referential nounphrases override superiority violations, whereasnon-referential noun phrases do not.
This con-straint is included as a probabilistic feature ofthe parser, RELMIN, specified in Table 5.
Thecondition holds if a non-referential NP (what) isin ?1 (RELMIN=Yes).
But the violation conditiondoes not hold (RELMIN=No) if a non-referentialNP (which) is in ?1 .
The feature categoricallyseparates which-N and bare wh-phrases to capturethe Relativized Minimality predictions for theseexperimental sentences.
The probabilistic featurealso adds a grammatical gradience component tothe model, which is not proposed by the originalhypothesis.3.3.2 Memory constraintsThe parser encodes each of the memory accountsprovided in Table 1 as probabilistic features.
DIS-TANCE, the simplest feature, determines parser ac-tions on the basis of how far apart ?1 and ?1 arein the string.DLT and INTERVENORS require parser sen-sitivity to the nominal intervenors between ?1and ?1 according to Gibson?s DLT specification(2000).
Table 6 provides a list of the nominal inter-venors considered.
Gibson?s hierarchy is extendedto include nominal wh-words to more accuratelymodel the experimental conditions.Intervenor POS ExampleNN bookNNS booksPRP theyNNP PatNNPS AmericansWP-WHAT whatWP-WHO whoWDT-WHICH which bookWDT-WHAT what bookTable 6: POS for nominal intervenors.The sequence of STACKNEXT features are sen-sitive to the parser?s memory, in the form of thePOS of elements at varying depths of the stack.These features are found to have high overall ac-curacy in the Nivre parser (Nivre, 2004) and in hu-man sentence processing modeling (Boston et al,2008).ACTIVATION, INTERFERENCE, and RE-TRIEVAL predictions are based on the sequenceof Lewis & Vasisth (2005) calculations providedin Equations 1-4.
These equations require somenotion of duration, which is calculated as a func-tion of parser actions and word retrieval times.Table 7 describes this calculation, motivated bythe production rule time in Lewis & Vasisth?sACT-R model.Transition TimeLEFT 50 ms + 50 ms + Retrieval TimeRIGHT 50 ms + 50 ms + Retrieval TimeSHIFT 50msREDUCE 0msTable 7: How time is determined in the parser.Because only words at the top of the stack canbe retrieved, the following will be described for?1 .
Retrieval time for ?1 is based on its activationA, calculated as in Equation 1.Ai = Bi +?jW jSji (1)Total activation is the sum of two quantities, theword?s baseline activation Bi and similarity-basedinterference for that word, calculated in the sec-ond addend of the equation.
The baseline activa-tion, provided in Equation 2, increases with more40Feature Feature Type IncludesGrammarRELMIN Yes/No ?1 wh?word :: intervenorswh?word(?1 ...?1 )MemoryDISTANCE String Position ?1 ?
?1DLT Count intervenorsnom(?1 ...?1 )INTERVENORS POS intervenorsnom(?1 ...?1 )STACK1NEXT POS ?1 :: ?1STACK2NEXT POS ?1 :: ?2 :: ?1STACK3NEXT POS ?1 :: ?2 :: ?3 :: ?1ACTIVATION Value baselineActivation(?1 )INTERFERENCE Value interference(?1 )RETRIEVAL Time (ms.) retrievalTime(?1 )Table 5: Feature specification.
:: indicates concatenation.recent retrievals at time tj .
This implementationfollows standard ACT-R practice in setting the de-cay rate d to 0.5 (Lewis and Vasishth, 2005; An-derson, 2005).Bi = ln??n?j=1tj?d??
(2)?1 ?s activation can decrease if competitors, orother words with similar grammatical categories,have already been parsed.
In Equation (1), W j de-notes weights associated with the retrieval cues jthat are shared with these competitors, and Sjisymbolizes the strengths of association betweencues j and the retrieved item i (?1 ).
For thismodel, weights are set to 1 because there is onlyone retrieval cue j in operation: the POS.
Thestrength of association Sji is computed as in Equa-tion 3.Sji = Smax ?
ln(fanj ) (3)The fan, fanj , is the number of words that havethe same grammatical category as cue j, the POS.The maximum degree of association between sim-ilar items in memory is Smax which is set to 1.5following Lewis & Vasishth.To get the retrieval time, in milliseconds, of ?1 ,the activation value calculated in Equation 1 is in-serted in Equation 4.
The implementation followsLewis & Vasishth in setting F to 0.14.T i = Fe?Ai (4)The time T i is the quantity the parser is sensi-tive to in determining attachments based on theRETRIEVAL feature.
Because it is possible thatSUVs are better modeled by only part of the re-trieval equation, such as baseline activation or in-terference, the implementation also considers AC-TIVATION and INTERFERENCE features.
The fea-tures are sensitive to the quantities in the addendsin Equation 1, Bi and?jW jSji respectively.4 ResultsThe results focus on whether the parser choosesa LeftArc attachment when it is in the config-uration depicted in Figure 3 given the grammati-cal and memory constraints listed in Table 5.
Ta-ble 8 depicts the outcome, where Y signifies aLeftArc attachment is preferred and N that it isnot.Only one feature correctly patterns with the ex-perimental evidence: INTERVENORS.
It allows aLeftArc in the which.which condition, and dis-allows the arc in other conditions.
The INTER-VENORS feature also patterns with the experimen-tal evidence as more memory is added.
Table 9 de-picts the LeftArc attachment for increasing lev-els of k with this feature.
At k=1, the parser onlychooses the attachment for the which.which con-dition.
At k=2, the parser chooses the attachmentfor both which.which and which.bare.
At k=3, itchooses the attachment for all conditions.
Thismimics the decreases in difficulty evident in Fig-ure 1, and provides support for reductionist theo-ries: if memory is restricted (k=1), only the easi-est attachment is allowed.
As memory increases,more attachments are possible.INTERVENORS is sensitive to the nominal in-41Condition b.b b.w w.b w.wExperiment N N N YGrammarRELMIN=YES N N N NRELMIN=NO N N N NMemoryDISTANCE N N N NDLT N N N NINTERVENORS N N N YSTACK1NEXT N N N NSTACK2NEXT Y N Y NSTACK3NEXT Y Y Y YACTIVATION N N N NINTERFERENCE Y N N NRETRIEVAL Y N N NTable 8: LeftArc attachments for the experi-mental data.Condition b.b b.w w.b w.wINTERVENORS K=1 N N N YINTERVENORS K=2 N N Y YINTERVENORS K=3 Y Y Y YTable 9: INTERVENORS allows more attachmentsas k increases.tervenors between which and read.
RETRIEVAL,INTERFERENCE, and particularly DLT, shouldalso be sensitive to these intervenors.
Despite theirsimilarity, none of these features are able to modelthe attachment behavior in the experimental data.The STACK3NEXT feature differs from theother features in that it allows the LeftArc at-tachment to occur in any of the conditions.
Al-though this does not match the interpretation ofthe experimental results followed in this paper, itleaves open the possibility that the feature couldmodel the data according to a different measure ofparsing difficulty, such as surprisal (Hale, 2001).The RELMIN constraint is not able to model theexperimental results for gradience.5 DiscussionThe results demonstrate that modeling the exper-imental data for SUV gradience requires a parserthat can vary memory resources as well as be sen-sitive to the types of the nominal intervenors cur-rently in memory.
The gradience is modeled byincreasing memory resources, in the form of in-creases in the beam-width.
This demonstrates theusefulness of varying both the types and amountsof memory resources available in a computationalmodel of human sentence processing.The positive results from the INTERVENORSfeature confirms the discourse accessibility hierar-chy encoded in the DLT (Gundel et al, 1993; War-ren and Gibson, 2002), but only when wh-wordsare included as nominal intervenors.
The resultsalso suggest that it is the type, and not just thenumber of intervenors as suggested by the DLT,that is important.Further, the INTERVENORS feature does notpattern with the DLT hypothesis.
DLT assumesthat increasing the number of nominal inter-venors causes sentence processing difficulty (Gib-son, 2000; Warren and Gibson, 2002).
Here, thenumber of intervenors is increased, but sentenceprocessing is relatively easier.
This effect is ex-plained by the intrinsic difference between theDLT and INTERVENORS features: INTERVENORSprovides more information to the parser, in theform of the POS of all intervenors.
This indicatesthat certain intervenors help, rather than hinder,the retrieval process.The negative results demonstrate that other rep-resentations of memory do not model SUV gra-dience.
If we consider this along the continuumfrom (4), those features that take into account lessinformation than INTERVENORS (DISTANCE andDLT) are too restrictive.
Of those features thatare more complex than INTERVENORS, many aretoo permissive, or permit the wrong attachments.This pattern is also visible in the STACKNEXTfeatures: STACK1NEXT is too restrictive, whileSTACK3NEXT too permissive.
STACK2NEXT un-fortunately permits the wrong attachments.
Thispattern in the continuum indicates that an interme-diate amount of memory information is requiredto adequately model these results.INTERFERENCE, which also considers competi-tors in the intervening string, would seem likelyto pattern with the INTERVENORS results.
Infact, similarity-based interference and retrievalhave previously been argued to account for thesegradience patterns (Hofmeister et al, Submitted).However, the only words considered as competi-tors with which for both features in this modelare other wh-words.
For the which.which con-dition, for example, INTERFERENCE would onlyconsider the second which a competitor.
IN-TERVENORS, on the other hand, considers book,42which, and student as possible intervenors.
Thissuggests that the INTERFERENCE measure in re-trieval would be more accurate if it consideredmore competitors, a consideration for future work.Hofmeister (2007) suggests that it is not a sin-gle memory factor, but a number of factors, thatcontribute to SUV gradience.
Some features, suchas INTERFERENCE or DLT, may be more accuratewhen they are considered in addition to other fea-tures.
It is also likely that probabilistic models thatinclude many features will be more robust thansingle-feature models, particularly when tested onsimilar phenomena, like islands.
I leave these pos-sibilities to future work.Although the variable beam-width INTER-VENORS feature patterns well with the Arnon etal.
results, it does not capture the reading time dif-ference between the bare.bare and the bare.whichconditions; both are unavailable at k=2 and avail-able at k=3.
Although this may indicate a prob-lem with the feature itself, it is also possible that amore gradient evaluation technique is needed.
Assuggested in Section 4, determining accuracy onthe basis of attachment alone may be insufficientto correctly model the full experimental evidencein terms of reading times.
This is an empiricalquestion that can be tested with this computationalmodel.
In future work, I consider the role of parserdifficulty, via linking hypotheses such as surprisal,in modeling the experimental data.The interpretation of Relativized Minimalityused here as a grammatical constraint could notderive the experimental results.
LeftArc is notpreferred when the parser is in a SUV context(RELMIN=Yes)?an expected result as attachmentsshould not occur in SUV contexts.
However, thewhich.which, which.bare, and the bare.which con-ditions are not violations because they includenon-referential NPs.
Even with the RELMIN=NOfeature, the parser does not select LeftArc at-tachments, suggesting grammatical gradience isnot useful in modeling the SUV gradience results.This model does not attempt to capture exper-imental evidence that SUVs and similar phenom-ena, like islands, are better modeled by grammati-cal constraints (Phillips, 2006; Sprouse et al, Sub-mitted).
Not only does this work only focus on onekind of grammatical constraint for SUV gradience,but the results reported here do not reveal whetherthe intervention effect itself is better modeled bygrammatical or reductionist factors.
Rather, theresults demonstrate that the gradience in the inter-vention effect is better modeled by memory thanby the gradient grammatical feature.
Future workwith this computational model will allow for anexamination of those memory factors and gram-matical factors most useful in exploring the sourceof the intervention effect itself.6 ConclusionThis study considers grammatical and memory-based explanations for SUV gradience in a hu-man sentence processing model.
The results sug-gest that gradience is best modeled by a parserthat can vary memory resources while being sen-sitive to the types of nominal intervenors that havebeen parsed.
Grammatical and other memory con-straints do not determine correct attachments inthe SUV environment.
The results argue for a the-ory of language that accounts for SUV gradiencein terms of specific memory factors.AcknowledgmentsI am grateful to Sam Epstein, John T. Hale, PhilipHofmeister, Rick Lewis, Colin Phillips, studentsof the University of Michigan Rational Behaviorand Minimalist Inquiry course, and two anony-mous reviewers for helpful comments and sugges-tions on this work.ReferencesS.
Abney.
1987.
The English noun phrase in its sen-tential aspect.
Ph.D. thesis, MIT, Cambridge, MA.J.
R. Anderson.
2005.
Human symbol manipulationwithin an integrated cognitive architecture.
Cogni-tive Science, 29:313?341.I.
Arnon, N. Snider, P. Hofmeister, T. F. Jaeger, andI.
Sag.
To appear.
Cross-linguistic variation ina processing account: The case of multiple wh-questions.
In Proceedings of Berkeley LinguisticsSociety, volume 32.M.
F. Boston, J. T. Hale, R. Kliegl, and S. Vasishth.2008.
Surprising parser actions and reading diffi-culty.
In Proceedings of ACL-08: HLT Short Papers,pages 5?8.N.
Chomsky.
1973.
Conditions on transformations.In Stephen Anderson and Paul Kiparsky, editors, AFestschrift for Morris Halle, pages 232?286.
Holt,Reinhart and Winston, New York.W.
N. Francis and H. Kucera.
1979.
Brown corpusmanual.
Technical report, Department of Linguis-tics, Brown University, Providence, RI.43E.
Gibson.
1998.
Linguistic complexity: locality ofsyntactic dependencies.
Cognition, 68:1?76.E.
Gibson.
2000.
Dependency locality theory: Adistance-based theory of linguistic complexity.
InA.
Marantz, Y. Miyashita, and W. O?Neil, editors,Image, language, brain: Papers from the First MindArticulation Symposium.
MIT Press, Cambridge,MA.P.
C. Gordon, R. Hendrick, and W. H. Levine.
2002.Memory-load interference in syntactic processing.Psychological Science, 13(5):425?430.D.
J. Grodner and E. A. F. Gibson.
2005.
Conse-quences of the serial nature of linguistic input forsentential complexity.
Cognitive Science, 29:261?91.J.
K. Gundel, N. Hedberg, and R. Zacharski.
1993.Cognitive status and the form of referring expres-sions in discourse.
Language, 69:274?307.J.
T. Hale.
2001.
A probabilistic earley parser as apsycholinguistic model.
In Proceedings of NAACL2001, pages 1?8.J.
A. Hawkins.
1999.
Processing complexity and filler-gap dependencies across grammars.
Language,75(2):244?285.D.
G. Hays.
1964.
Dependency Theory: A formalismand some observations.
Language, 40:511?525.P.
Hofmeister, I. Arnon, T. F. Jaeger, I.
A.
Sag, andN.
Snider.
Submitted.
The source ambiguity prob-lem: distinguishing the effects of grammar and pro-cessing on acceptability judgments.
Language andCognitive Processes.P.
Hofmeister.
2007.
Retrievability and gradience infiller-gap dependencies.
In Proceedings of the 43rdRegional Meeting of the Chicago Linguistics Soci-ety, Chicago.
University of Chicago Press.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.In Proceedings of NODALIDA 2007, Tartu, Estonia.M.
A.
Just and P.A.
Carpenter.
1992.
A capacity theoryof comprehension: Individual differences in work-ing memory.
Psychological Review, 98:122?149.M.
A.
Just and S. Varma.
2007.
The organizationof thinking: What functional brain imaging revealsabout the neuroarchitecture of complex cognition.Cognitive, Affective, and Behavioral Neuroscience,7(3):153?191.L.
Karttunen.
1977.
Syntax and semantics of ques-tions.
Linguistics and Philosophy, 1:3?44.R.
Lewis and S. Vasishth.
2005.
An activation-basedmodel of sentence processing as skilled memory re-trieval.
Cognitive Science, 29:1?45.C.-J.
Lin, R. C. Weng, and S. S. Keerthi.
2008.
Trustregion newton method for large-scale regularized lo-gistic regression.
Journal of Machine Learning Re-search, 9.B.
McElree.
2000.
Sentence comprehension is me-diated by content-addressable memory structures.Journal of Psycholinguistic Research, 29(2):111?123.J.
Nivre.
2004.
Incrementality in deterministic depen-dency parsing.
In Proceedings of the Workshop onIncremental Parsing (ACL), pages 50?57.D.
Pesetsky.
1987.
Wh-in-situ: movement and unse-lective binding.
In Eric Reuland and A. ter Meulen,editors, The representation of (In)Definiteness,pages 98?129.
MIT Press, Cambridge, MA.C.
Phillips.
2006.
The real-time status of island phe-nomena.
Language, 82:795?823.C.
Phillips.
Submitted.
Some arguments and non-arguments for reductionist accounts of syntacticphenomena.
Language and Cognitive Processes.O.
Rambow and A. K. Joshi.
1994.
A processingmodel for free word-order languages.
In CharlesClifton, Jr., Lyn Frazier, and Keith Rayner, editors,Perspectives on sentence processing, pages 267?301.
Erlbaum, Hillsdale, NJ.L.
Rizzi.
1990.
Relativized Minimality.
MIT Press.J.
Sprouse, M. Wagers, and C. Phillips.
Sub-mitted.
A test of the relation between work-ing memory capacity and syntactic island effects.http://ling.auf.net/lingBuzz/001042.L.
Tesnie`re.
1959.
E?le?ments de syntaxe structurale.Editions Klincksiek.J.
A.
Van Dyke and B. McElree.
2006.
Retrieval in-terference in sentence comprehension.
Journal ofMemory and Language, 55:157?166.E.
Wanner and M. Maratsos.
1978.
An ATN approachin comprehension.
In Morris Halle, Joan Bresnan,and George Miller, editors, Linguistic theory andpsychological reality, pages 119?161.
MIT Press,Cambridge, MA.T.
Warren and Edward Gibson.
2002.
The influence ofreferential processing on sentence complexity.
Cog-nition, 85:79?112.44
