First Joint Workshop on Statistical Parsing of Morphologically Rich Languagesand Syntactic Analysis of Non-Canonical Languages, pages 39?53 Dublin, Ireland, August 23-29 2014.Experiments with Easy-first nonprojective constituent parsingYannick VersleyDepartment of Computational LinguisticsUniversity of Heidelbergversley@cl.uni-heidelberg.deAbstractLess-configurational languages such as German often show not just morphological variation butalso free word order and nonprojectivity.
German is not exceptional in this regard, as othermorphologically-rich languages such as Czech, Tamil or Greek, offer similar challenges thatmake context-free constituent parsing less attractive.Advocates of dependency parsing have long pointed out that the free(r) word order and non-projective phenomena are handled in a more straightforward way by dependency parsing.
How-ever, certain other phenomena in language, such as gapping, ellipses or verbless sentences, aredifficult to handle in a dependency formalism.In this paper, we show that parsing of discontinuous constituents can be achieved using easy-firstparsing with online reordering, an approach that previously has only been used for dependencies,and that the approach yields very fast parsing with reasonably accurate results that are close tothe state of the art, surpassing existing results that use treebank grammars.
We also investigatethe question whether phenomena where dependency representations may be problematic ?
inparticular, verbless clauses ?
can be handled by this model.1 IntroductionAutomatic syntactic parsing has been fruitfully incorporated into sytems for information extraction(Miyao et al., 2008), question answering, machine translation (Huang and Chiang, 2007), among others,but we also see syntactic structures being used to communicate facts about language use in the digitalhumanities or in investigations of the language of language learners.
In all of these applications, we seefruitful use both of constituent trees, and of dependency trees.Depending on the application, different criteria may become important: on one hand, the ability toproduce structures that are (intuitively) compatible with semantic composition, or where arguments andadjuncts are related to their predicate in the tree, which commonly requires dealing with nonprojectivity.Such a formalism should also deal with a wide range of constructions including verbless clauses.
Finally,parsing speed is somewhat important for many application cases, and a parser that changes the tokeniza-tion of the input or inserts additional ?null?
tokens runs afoul many of the fundamental assumptions inpipelines for semantic processing or information extraction.If we look at the current three largest treebanks for German, namely the Hamburg Dependency Tree-bank (Foth et al., 2014) with 101 000 sentences, the Tu?Ba-D/Z treebank (Telljohann et al., 2009) with85 000 sentences or the Tiger treebank (Brants et al., 2002) with about 50 000 sentences, we see find acontinuum of the nonprojective single-parent dependencies of the HDT on one side and projective phrasestructures of Tu?Ba-D/Z, with Tiger straddling in the middle with a scheme that is neither projective norlimited to dependencies, and which represents, we?ll argue, both the best and the worst of both worlds.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/39Because of its expressivity, the Negra/Tiger scheme has also been used for other languages such asSwedish Volk and Samuelsson (2004) as well as Georgian/Russian/Ukrainian (Kapanadze, 2012), and asEarly New High German (Pauly et al., 2012).The Tiger scheme is arguably more expressive than either of the alternatives since it can captureboth elliptic clauses (which are difficult to represent in normal dependency schemes) and nonprojectiveconstructions (which have to be added as a second annotation layer in purely projective treebanks suchas Tu?Ba-D/Z).
It also makes it the most difficult to provide good automatic tool support, in terms ofeffective parsing components or of annotation tools, since parsing of discontinuous constituents has onlyrecently become practical.The straightforward approach of Kallmeyer and Maier (2013) to use a treebank-derived linear context-free rewriting system suffers from near-exponential observed time consumption in practice.
Approachesthat use context-free grammar approximation such as the ones of Schmid (2006), Cai et al.
(2011) orvan Cranenburgh and Bod (2013), still have cubic time complexity; especially in the latter case, it is notclear whether techniques that allow fast PCFG parsing such as those of Bodenstab et al.
(2011) would besuitable for the subsequent steps with increased grammar complexity.In this paper, we present a novel application of the easy-first parsing principle of Goldberg and Elhalad(2010) to discontinuous constituent parsing, which performs fast enough for interactive use (about 40sentences per second) while giving an acceptable accuracy that is within the range normally seen withunmodified treebank grammars.In the remainder of the paper, we will include a short discussion of the interrelation between con-stituency and dependency relations of syntax, as well as relevant prior work in section 2, and discuss theconstruction of the parser in section 3.
Section 4 and following contain a discussion of quantitative re-sults on the Tiger corpus, whereas the penultimate section contains a more detailed analysis of the parserbehaviour on constructions that are problematic for either dependency parsers or projective constituentparsing.2 Constituency and Dependency: Good friends?Constituency and dependency structures are two formalisms that are frequently used for theory-neutraldescription of syntactic structures.
In constituent structures, usually influenced by some version of X-bar theory (see Kornai and Pullum, 1990 for a discussion; most notably, phrases are supposed to beprojections of a head), whereas in dependency structures it is usually assumed that each word has exactlyone governor (except one or more words that are attached to a virtual root node).The common subset of both can be described (in the words of Hockenmaier, 2007) as ?Heads, argu-ments, modifiers, conjuncts?, which includes the grammatical function labels that are added in depen-dency structures, and to varying extent in phrase structure treebanks.
Nivre (2011) goes further and askswhether we need constituents at all, since pure dependency parsing recovers arguments and adjunctswhile being generally faster (and, at least for results published on Czech and French which Nivre cites,more accurate).
Versley and Zinsmeister (2006) similarly argue that even ?deep?
dependency relations(including nonlocal ones) can be recovered from single-parent dependencies if subsequent disambigua-tion steps identify the scope of conjunctions, argument sharing in coordination, passive identification,and lexicalized control phenomena.
However, verbless clauses as they may occur in coordination posea problem to the idea that every phrase is headed by a preterminal, or the equivalent assumption independency grammar that every argument has a governing head word.In constituent treebanks, the solution to this problem is rather simple: deviate from the descriptive-Xbar schema outlined earlier on and introduce headless projections for these clauses.
Dependency tree-banks lack this additional degree of freedom, and the choice is usually to either attach the respectivenodes somewhere else (Bo?hmova et al., 2001; Foth, 2006) or introduce empty nodes that are the gover-nors of the orphaned subtrees (Bosco and Lombardo, 2006; Vincze et al., 2010; Dipper et al., 2013).In dependency parsing, good solutions for nonprojective edges have been found, including pseudopro-jective parsing (Nivre and Nilsson, 2005), approximate weighted constraint solving (Koo et al., 2010), aswell as deterministic online reordering (Nivre, 2009), which also has been applied to easy-first decoding40strategies (Tratz and Hovy, 2011).
Seeker et al.
(2012) additionally employs an attach-inner opera-tion which allows non-projective insertion into a structure that has already been built.
Despite thesevery reasonable solutions, the treatment of elliptic phrases, whether it is done using the somewhere-elseapproach or by introducing empty nodes (see Seeker et al., 2012 and references therein) yields unin-formative structures for subsequent processing components or even makes it necessary to re-engineersubsequent processing stages for dealing with the newly introduced empty nodes, or (equally impracti-cal) require the refactoring of annotated corpus resources to accommodate a new tokenization whenevera null element is introduced or changed.In constituency parsing, the problem of discontinuous constituents in parsing has, at least in German,first been met with a proposals of raising degrees of complexity (among others, van Noord, 1991; Plaehn,2000) and then silently been ignored both in the building of parsers and in their evaluation: researchersfrom Dubey and Keller (2003) to the present day cite bracketing scores based on structures that wouldmake the reconstruction of ?Heads, arguments, modifiers, and conjuncts?
?
usually ?
rather difficult.Only relatively recently has the problem of discontinuous constituent parsing been tackled head-on.Kallmeyer and Maier (2013) propose an approach that extracts a treebank LCFRS grammar, which isthen used for probabilistic parsing, albeit with near-exponential time consumption.
Maier et al.
(2012)present an approach to make parsing in this approach more efficient by flattening coherent structures ina sentence to one single sentence node and thus eliminating scrambling as a source of discontinuities,together with other transformations, which allows a time complexity of O(n6) and parsing times of about2 minutes for a 40-word sentence.
van Cranenburgh and Bod (2013) use a more practical approach thatfirst creates phrase candidates from the n-best list of a projective constituent parser, and uses these toconstruct LCFRS items that do not necessarily correspond to grammar rules seen in the training set, butwhich are then matched against a collection of tree fragments extracted from the training set.There exists some work on transforming dependency structures into constituents that may help in therecovery of discontinuous constituents: Hall and Nivre (2008) propose to encode information about nodelabels in the dependency labels, whereas Carreras et al.
(2008) show that an ILP-based combination offinding dependencies and adding phrase projections and adjunctions to a dependency backbone workswell for constructing structures matching those of the Penn Treebank.
Seddah (2010) found that similarspinal structures can be used for the French Treebank.3 Incremental parsingIn general, statistical parsing follows one of several general approaches: one is the approach of item-based decoding, which is centered around the creation of a parse forest that implicitly stores a very largenumber of possible trees, followed by either dynamic programming in the case of projective parsing (e.g.
(Collins, 2003)) or techniques that provide an approximate or exact solution to the intractable problemin the case of nonprojective parsing with second-order factors (Koo et al., 2010).
The second large groupof approaches is based on incremental structure building, including the approaches of Magerman (1995)or Sagae and Lavie (2006) in the case of constituent parsing, or of Nivre (2003) and following in the caseof dependency parsing, with approaches such as Stolcke (1995) or Huang and Sagae (2010) occupying amiddle ground.While the idea of head lexicalization has played a large role in projective constituent parsing, there arerather few approaches that attempt to bridge the gap between dependency and constituency representa-tions in a way that could be exploited for the efficient building of discontinuous constituent structures.Among these, both the approaches of Hall and Nivre (2008) and of Carreras et al.
(2008) could bedescribed in terms of a spinal transform: each terminal in the input string is assigned a set of governingnodes that form its spine; parsing then consists of assigning a dependency structure among the terminalnodes and of assigning spines and the relation to each other.In the remainder of this section, we describe two approaches that we used to perform nonprojectiveconstituent parsing in expected linear time: one is relatively close to the approach of Hall and Nivre(2008), but instead of assigning nodes to the first terminal of their yield, uses a strategy more like thespinal tree adjoining grammr of Carreras et al.
(2008).
The other is an application of the principle41Zum einenon the one handPPadd?ist erhe isSadd?au?ergewo?hnlichextraordinarilyADJDpopula?rpopular[i:i+1] APADJD[i] APFigure 1: Example for an intermediate state in EaFi, with the preferred action candidates for each positionof easy-first parsing, which has been used for unlabeled dependency parsing by Goldberg and Elhalad(2010), and for non-projective labeled dependency parsing by Tratz and Hovy (2011), towards discon-tinuous constituency parsing.
Because computed feature vectors can be memorized and only have to berecomputed in a small window around the last parser action, this latter approach, just as a left-to-righttransition-based parser, has an expected time consumption that is linear in the number of words to beparsed.3.1 ADG: Constituency-to-Dependency ReductionOur baseline is an approach close in spirit to Hall and Nivre (2008): The tree with node labels is turnedinto a dependency graph that encodes, on the governor edge of each terminal, a combination of (i) thenode labels on the spine of this node, and (ii) the level at which this node attaches to its parent?s spine.We change two parameters of Hall and Nivre?s approach: on one hand, we do not use the first terminalin the yield of a node as its representative but the head according to the head table that we also use toassign the head in the easy-first parser.
The reason for this is a practical one: using the head, we geta distribution of 531 different spine/level combinations when we use the head, whereas we would getabout 1525 categories when we use the first terminal.To ensure efficient parsing, this list is further pared down to 100 entries, with the remaining entriesbeing replaced by an UNK placeholder.
In decoding, terminals with these entries are assigned the mostfrequent combination of spine and parent category for the POS tags of the node and its governor, and thetopmost spine node with a matching category (or simply the topmost one) would be chosen.The decoding algorithm and parameter settings for MaltParser were then determined using the Malt-Optimizer software (Ballesteros and Nivre, 2012).
The settings selected use the stack-projective algo-rithm with head+path marking strategy for pseudoprojective parsing.1Hall and Nivre?s approach is more complex than the approach presented here, and involves interleavingof identifying dependency edges (using the nonprojective Covington parsing scheme) and the stepwisedetermination of the topmost edge label, then the path of edge labels, and finally the path of constituentlabels and its attachment.
However, we find that this approach of dependency reduction constitutes avery reasonable intelligent baseline, and is able to perform at a similar speed than our approach.3.2 EaFi: Easy-first Constituency ParsingThe main approach that we will present here constitutes an adaptation of the Easy-First approach tononprojective constituent parsing.
The parser keeps track of a sequence of nodes, beginning with theterminals that are output by the preprocessing consisting of morphological analyzer and lemmatization,and at each point applies one of several actions:?
Reduce-Unary: one node is grouped under a unary node of a given category, with the restriction thatthe corresponding unary rule must have been observed in the treebank.
(Additionally, we collapseany two nodes with the same category embedding each other, which sometimes occurs in the Tigertreebank when several empty-headed phrases are assumed to embed each other in coordination).1MaltParser is able to do direct nonprojective parsing using the reordering approaches of Nivre (2009) and Nivre et al.
(2009), however the pseudoprojective approach was selected in MaltOptimizer?s parameter selection.42Basic featuresetUnigram: n ?
ni?2 .
.
.
ni+3 CnPn CnWn CnMn CnLnLeft/Right Children: n ?
ni,Lni,Rni+1,Lni+1,R CnPnBigram: m,n ?
ni?1ni .
.
.
ni+2ni+3 WmWn WmCn CmWn WmWnTrigram: r,m,n ?
ni?1nini+1 .
.
.
ni+1ni+2ni+3Medium featuresetBigram+Child: m,n, r ?
{nini+1ni+1,L;nini+1ni,R;ni+1ni+2ni+1,R;nini?1ni,L} CmCnCr CmCnPrDistance: ?
?
{dist(ni, ni+1),gap(ni, ni+1)} ?Cm ?Cn ?Pm ?Pnm,n = ni, ni+1 ?Fm ?FnLarge featuresetGap bigram: m,n ?
ni?1ni+2, nini+2 WmWn WmCn CmWn WmWnBigram+2child: m,n ?
ni?1ni .
.
.
ni+2ni+3;C,D ?
L,R CmCnCmCCnD CmCnCmCPnDCmCnPmCCnDBigram-Dist m,n ?
ni?1ni .
.
.
ni+2ni+3 ??
= dist(m,n) ?PmPnTable 1: Features used: W=word, C=phrase label, M=head morph, P=head pos?
Reduce-Binary: two nodes are grouped under one node, with the head of the new node beingdetermined by a head table.?
Add-Left/Add-Right: one node is added as a child to a node on its left/to its right.?
Swap: if two nodes are in surface order (i.e., the head of the first node being left of the head of thesecond node in the normal word ordering), they can be swapped.In addition, we experimented with a retag action which allows the parser to change the tag of a wordthat has been mistagged.
While this has a positive effect on the parser?s accuracy for verb phrases, it alsoresults in a slight deterioration of other phrases, resulting in a very slight decline in performance.To decide among different parsing actions, the parser uses a linear classifier with a pre-defined featureset (POS, word form, morphological tag and lemma in a two-token window around the two nodes thatare being considered, the category and part-of-speech tag of the leftmost and rightmost dependent ofthe nodes that are being considered; bigrams of words, categories, and one of each, in a window ofone around the two nodes being considered, and trigrams consisting of two category and one category,part-of-speech tag, or word form within said window).Weights are learned by performing online learning with early stopping, similar to the strategy em-ployed by Collins and Roark (2004).
We use the Adaptive Gradients method (Duchi et al., 2011) forweight updates and averaging of the weight vector (in a fashion identical to the averaged perceptron).We found that 5-10 epochs of training on Tiger were sufficient to get a mostly usable model, and used15 epochs of training for the results reported in the later section.
Considering that Goldberg and Elhalad(2010) use a learning strategy that performs multiple perceptron updates until the constraint violation isfixed, we also tried this strategy but did not achieve convergence.3.3 Reordering Oracles for ConstituentsThe basic idea for reordering oracles in deterministic dependency parsing has been presented by Nivre(2009).
In the following, we present a straightforward adapation of the idea to constituent trees.Given a set of terminals T = {w1, .
.
.
,wn} that is totally ordered by a relation <, an unordered treegraph is a directed graph (NT ?T,?)
with nonterminal (NT) and terminal nodes (T), where the transitivehull ??
of the parent relation ?
is acyclic, no node has a parent from T , and exactly one node, vroot, hasno parent.An node ordering ?
is consistent with ?
whenever, for any node u and an descendant u?
??
u, and anode v with an descendant v?
??
v, u ?
v entails u?
?
v?.43A tree cut of a tree is a sequence v1, .
.
.
, vn that contains exactly one node from each path vroot, .
.
.wifrom the root to a terminal.
Nivre?s insight, applied to constituent structures, is that sorting the terminalsin a ?-compatible order ?
will allow us to use normal projective parsing techniques to find a sequence ofreductions that parses this tree, since any needed reduction would reduce one ?-ordered cut to another?-ordered cut.
In the following, two orderings ?, ??
are considered equivalent iff they only differ on pairsof nodes u, v where one is the ancestor of the other.Two subtrees under nodes u and v with yields yield(u) = {u?
?
T ?u?
??
u} and yield(v) = {v?
?T ?v?
??
v} are separated by a surface ordering < whenever any two terminals u?
of u and v?
of v fulfillu?
< v?.
Note that two nodes without gaps (i.e.
block-degree one) either embed each other (in whichcase u ??
v or v ??
u is the case) or they are separated by <.
In a slight abuse of notation, we extend< from a total order of the terminals to a partial order of the nonterminals by writing u < v whenever uand v are separated.
For projective trees, this extension of < specifies exactly one total relation (moduloequivalence), and which is also ?-compatible.For trees that are non-projective, we can have the situation where two nodes u and v are overlappingin that u has descendants u?, u??
and v has a descendant v?
with u?
< v?
< u??.
Then we cannot extend <to an ordering of nodes that is ?-compatible.
However, we can always find an ordering that respects <locally such that, for two children u?
and u??
of u, u?
< u??
entails u?
?
u??.
Nivre proposes the sequenceassigned by an in-order traversal of the dependency tree.
In our case, any function h ?
NT ?
(NT ?
T )that assigns a ?head?
child to each node will do the same, with an extension h?
(w) = w for all terminalsand h?
(v) = h?
(h(v)) otherwise, through u ?
v ??
h?
(u) ?
h?
(v).2A transition sequence for parsing a tree is then a sequence consisting of reductions (leading froma cut .
.
.
vi, u?, .
.
.
u?
?, vj , .
.
.
with a contiguous subsequence of the children of u to the sequence.
.
.
vi, u, vj , .
.
.
that contains u instead) and swaps (leading from a cut .
.
.
vi, vj .
.
.
that has h?
(vi) andh?
(vj) ordered with respect to < but not with respect to ?
to a cut .
.
.
vj , vi .
.
.
that is orders h?
(vi) andh?
(vj) with respect to ?
but not <).Nivre (2009) defines an oracle for shift-reduce parsing that is swap-eager in that it always allowsswapping.
In Nivre?s case, the oracle is deterministic and always performs the swapping before anyreduction.Nivre et al.
(2009) note that the swap-eager oracle performs too many swaps because it swaps groupsof words that are later reduced.
They propose a swap-lazy algorithm that does not swap two nodes ifone of them is adjacent to another node that is within the same maximal projective subtree.The perspective of parsing as a series of swap and reduce actions allows us to specify a strategy thatperforms less reductions in some cases: Consider that we need to reorder the <-contiguous sequence ofterminals to the ?-contiguous sequence that is needed for reducing the tree to its final form.
The numberof swaps performed, if we assume that we always swap adjacent constituents, is exactly equal to thenumber of terminal pairs vi, vj that are <-ordered but not ?-ordered.
Any reduction of the number ofswaps relative to this baseline will come from a group of nodes with heads vi1, .
.
.
vik that are reduced totheir parent vi before being swapped with a node vj .We can take advantage of this fact by using any node with blockdegree one as a barrier: no node thatis a descendant of this node can be swapped with a node that is not a descendant before the reductionthat results in the barrier node has been carried out.
Because any projective subtree has all nodes asbarrier nodes, any pair of nodes whose swapping is delayed by the swap-lazy approach will be kept fromswapping by a barrier.
Conversely, nodes with a block-degree of one can also occur higher-up in the tree(e.g.
as clause or sentence nodes), in which case they can act as a barrier even when their subtrees arenot projective.4 Quantitative EvaluationIn order to evaluate our approach, we used the Tiger treebank, with the split used in the SPMRL?2013shared task (about 40 000 training sentences and 5 000 development and test sentences each; see also2Note that the concrete choice of h is quite arbitrary: we could take the actual head child, but also the first or last child of anode.44` ?
30 ` ?
40F1 F1 LA EX NP PP VPEaFi: Preprocessing (large, barrier, noretag)gold 77.95 76.64 92.17 41.71 75.0 82.8 56.6marmot 75.51 73.97 91.08 38.48 72.7 81.3 48.3pred 74.71 73.18 90.81 37.67 72.1 80.6 48.9ADG, marmot preprocessingmarmot 73.42 72.24 90.95 33.77 68.0 77.4 52.1EaFi: Train projective, evaluate on real datagold 76.86 75.50 92.13 38.38 74.4 81.7 48.2marmot 74.43 72.98 91.20 36.52 72.1 79.8 42.6pred 73.75 72.32 90.72 35.55 71.8 79.2 42.4EaFi: Train projective, evaluate on projectivegold 79.95 78.59 93.40 44.20 76.1 83.0 68.7marmot 77.00 75.64 92.38 40.79 73.8 81.1 59.1pred 76.25 74.94 91.87 39.60 73.5 80.5 58.4Table 2: Results on SPMRL?13-dev (German, Tiger treebank) with varying preprocessing` ?
30 ` ?
40F1 F1 LA EX NP PP VPEaFi: Feature set (barrier, noretag)basic 70.26 68.60 89.03 34.03 69.1 77.1 40.1medium 73.31 71.75 90.13 36.22 70.5 79.9 45.8large 74.71 73.18 90.81 37.67 72.1 80.6 48.9EaFi: Reordering (large, noretag)eager 73.33 71.66 90.33 37.43 71.7 80.6 47.8lazy 74.85 73.37 90.85 38.08 72.2 80.8 49.0barrier 74.71 73.18 90.81 37.67 72.1 80.6 48.9EaFi: Tag correction (large, barrier)noretag 74.71 73.18 90.81 37.67 72.1 80.6 48.9retag 74.62 73.16 90.83 37.51 71.5 80.3 49.4Table 3: Results on SPMRL?13-dev (German, Tiger treebank) with pred preprocessing45Seddah et al., 2013 for a more extensive description), with the state-of-the-art preprocessing resultsfor part-of-speech and morphological tags3 which were produced by Bjo?rkelund et al.
(2013) using theMarMoT tagger (Mu?ller et al., 2013), in addition to the gold-standard preprocessing (gold) and automaticpredictions (pred) that are part of the official dataset of the SPMRL shared task.We applied two transformations to the data, which are automatically reversed in the parser output:one is adding NPs into PPs, which is also done by Seeker et al.
(2012), and the other is that we makeparenthetical material subordinate to its embedding clause, as Maier et al.
(2012) also advocate.Evaluation was performed using the evaluator from the DISCODOP package of van Cranenburgh andBod (2013), excluding punctuation and the ROOT label added by disco-dop from the evaluation.
Train-ing was run for 15 epochs.
Parsing the 5000 development sentences took about 90-120 seconds forEAFI, which corresponds to 40-55 sentences per second (on a Core i7 2GHz) and is slightly faster thanMaltParser using the ADG-derived model and a LibLinear classifier.In the results in table 2, we see the results for the dependency-to-constiuents approach, as well as forthe easy-first parsing with different reordering heuristics.
As in Nivre et al.
(2009), we notice that thelazy strategy that keeps projective constituents together yields better results than the eager strategy whichallows moving right away.
The overall results ?
around 76.6% f-score on gold tags and 73.1% f-score onpredicted tags in sentences of 40 words and below ?
indicate the promise of this approach, even thoughthey are significantly below the results of van Cranenburgh and Bod (2013) who achieve more than 78%f-measure using predicted tags on a different split of the Tiger treebank.
Van Cranenburgh?s approach isabout 15-20 times slower than ours, using 10 seconds for a 40-word sentence.For informative purposes, we also included results for projective parsing in table 2, using a conversionthat first attaches punctuation and then projectivizes the tree by detaching non-head children.4 Compar-ing the nonprojective parser and a variant that was trained on the projectivized version of the dataset,we see that the projective parser is about 1-2 percent worse than the nonprojective one, correspondingto our intuition that the reordering part improves the parsing on average.
We also see that the projectiveevaluation yields an estimate of parser performance that is substantially more optimistic than evaluatingon the original treebank.4.1 Comparison with Related workTables 4 and 5 show previous results for discontinous constituent parsing on the Tiger and Negra tree-banks.
The current best results on the Tiger treebank have been achieved by van Cranenburgh and Bod(2013), whose approach yields 78.8% Parseval F1 measure on the Tiger treebank in the split by Hall andNivre (2008), and 76.8% on the Negra treebank, in both cases with above 40% of exact matches amongthe sentences of up to 40 words.
Kallmeyer and Maier (2013) only report results on shorter sentences inNegra for their approach using a modified treebank LCFRS.
They achieve 75.6% on sentences of up to30 words.A recent approach that attempts to speed up discontinuous constituent parsing is the one by Angelovand Ljunglo?f (2014), whose parser takes about 100 seconds for a length-40 sentence, which can bereduced to 10 seconds for a length-40 sentence with an approximate search strategy.
For sentencesbetween 5 and 60 tokens, their approach reaches an F1 score of 69.3%, which however deterioratesquickly when approximate search is used, to 61.9% F1 in the latter case.It is quite evident that pushing for more speed in these formalisms forcibly leads to a deteriorationin the quality of the results.
As such, we think that the speed/quality tradeoff achieved in our system isquite useful.5 Qualitative AnalysisIn the following, we will provide a categorization of the phenomena concerning verbless clauses onone hand, and discontinuous constituents on the other.
Table 6 contains a breakdown on these types of3Data from http://www.cis.lmu.de/?muellets/marmot/marmot_spmrl.tar.bz2, version with file datesof June 13th 2014.
See http://code.google.com/p/cistern/wiki/marmotSPMRL4The SPMRL shared task dataset is idiosyncratic in that it deprojectivizes before attaching punctuation, which leads to aresult that is rather dissimilar to the original treebank.46` ?
30 ` ?
40F1 EX F1 EXHall and Nivre (2008), golda ?
?
79.93 37.78Hall and Nivre (2008), preda ?
?
75.33 32.63van Cranenburgh and Bod (2013), preda ?
?
78.8 40.8This work, golda 76.47 40.61 74.23 37.32Maier (2010), LCFRS goldc 73.43 29.87 ?
?Maier (2010), CFG goldc 75.57 31.80 ?
?This work, goldb 77.95 43.81 76.64 41.71This work, gold, eval w/ ROOTbc 81.13 43.81 79.80 41.71a) Hall&Nivre split b) SPMRL split c) includes the ROOT node in the evaluationTable 4: Previous results on the Tiger treebank` ?
30 ` ?
40F1 EX F1 EXMaier (2010), LCFRS goldc 71.52 31.65 ?
?Maier (2010), CFG goldc 74.04 33.43 ?
?van Craenburgh (2012), LCFRS, gold ?
?
67.26 27.90van Craenburgh (2012), Disco-DOP, gold ?
?
72.33 33.16Maier et al.
(2012) 74.5 ?
?
?Kallmaier and Maier (2013), LCFRS, gold 75.75 ?
?
?van Cranenburgh and Bod (2013), gold ?
?
76.8 40.5c) includes the ROOT node in the evaluationTable 5: Previous results on the NeGra treebankphenomena according to whether they are:?
correctly parsed (+): when the incredients for the construction are present in the parse and they arecombined in a suitable fashion.?
missed (o): when the ingredients for the construction are present, but combined in another way ?for example, an extraposition where the extraposed item is misattached?
broken (-): when the ingredients for the construction are not present and the parse has a completelydifferent structure.Many of the same categories are discussed by Seeker and Kuhn (2012), who only discuss examples,and by Maier et al.
(2014), who published a list of sentence numbers for each phenomenon that is,however, disjoint with the development portion considered here.
Although the distinctions between?missed?
and ?broken?
analyses are somewhat subjective, we think that it is still informative in the sensethat it helps to compare the relative difficulty of the problems involved.5.1 Types of Verbless ClausesIn their conversion Seeker and Kuhn (2012) found 3 035 sentences that contain at least one empty nodein the Tiger treebank, or about one every 16 sentences.
While this phenomenon may be more frequent inspontaneously-produced text such as it may occur in user-generated content, it is still quite frequent.Seeker et al.
only distinguish among edge labels, followed by a guess on the clause type that they needin order to place the inserted null element.In this work, we will concentrate on verbless VP and S nodes, with rougly three categories:The first consists of verbless copula clauses that mostly occur at top level,5 and where the mostobvious way to build a complete clause would be to add a be copula to the clause.5sentences 40499, 41442, 41468, 41676, 41682, 41736, 41743, 42566, 42606, 4273847ADG/marm large/pred+ o - + o -copula clauses 3 4 3 2 5 3gapping/ellipsis 0 4 6 0 4 6parentheticals 1 6 4 0 4 6extraposition 1 7 2 0 7 3scrambling 3 2 4 3 1 5topicalization 3 6 1 4 4 2+) construction parsed ok, o) construction missed, -) broken parseTable 6: Qualitative analysis: Counts for ok/missed/broken examplesA second group consists of clauses with gapping/ellipsis which occur in a coordinated structure, butdo not have a verb of their own.6 Such cases can occur with a final constituent in clause coordination aswell as with a non-final constituent in verb-last clauses:(1) a.
DietheAnstaltinstitutionsollshall[Anfangstart19981998nochstill12001200Bescha?ftigte]employeesundand[einoneJahryearspa?terlater600600za?hlen].count.
?The institution will count 1200 employees at the start of 1998 and one year later, 600?.b.
[DietheZahlnumberderofUrlaubsreisenholiday tripsiminInlandinteriorfielfelllautaccording toScho?rcherScho?rcherumbyzweitwoProzent]percentundand[damithencenichtnotmehranymoresoasstarkstrongwieasimin theVorjahreszeitraum]previous year period.
?The national number of holiday trips fell by two percent according to Schorcher, and hencenot as strongly anymore as in the corresponding period from last year?.Finally, we have parentheticals, which are rather rather similar to the examples listed under verblesscopula clauses, except that they occur as parenthetical material in a larger clause rather than by them-selves.75.2 Types of Non-projectivity PhenomenaFor the purpose of this paper, we will make a three-way distinction in the phenomena that create discon-tinuities, according to the following questions:?
If we serialize the sorted (sub)tree, would the result yield a grammatical sequence?
Or, to ask arelated question, would anything be missing if we kept only the continuous block of the head??
If we flatten the tree by introducing a common ordering domain for multiple heads (which wouldbe the result of tree flattening as proposed by Uszkoreit, 1987 or of a common argument list asadvocated by Hinrichs and Nakazawa, 1989; flattening the sentence is also the solution used in theGerman LFG grammar of Forst, 2007), would we have gotten rid of the problem?Making these distinctions gives us three rather large categories that we can use to classify nonprojec-tivity phenomena:Extraposition8 is phenomenon where the sorted subtree would (usually) be grammatical, and wherethe continuous part only would (usually) be acceptable:6sentences 40698, 40788, 40836, 41003, 41174, 41218, 41356, 41399, 41544, 416657sentences 40698, 40749, 40861, 40894, 40899, 40924, 41219, 41267, 41437, 414438Sentences 40506, 40507, 40517, 40528, 40567, 40583, 40589, 40594, 40622, 4067248(2) a.
EleElehathasmirme[einaBuch]bookgeschenktgiven[u?beraboutdietheSavanne].savannah.
?Ele gave me a book about the savannah?.b.
IchIhabehave[Ele]EleeinaBuchbookgeschenktgiven[undandSusi].Susi.
?I gave a book to Ele and Susi?.c.
HeuteTodayistis[dertheStaubsauger]vacuum cleanergekommen,come,[denwhichDuyoubestelltorderedhast].have.
?Today, the vacuum cleaner that you ordered came.
?Note that extraposition can be arbitrarily deep, as NPs can embed each other recursively:(3) a. HeuteTodayistis(dietheRechnunginvoicefu?rfor[dentheStaubsauger])vacuum cleanerjgekommen,come,([denwhichjDuyoubestelltorderedhast]).have.
?Today, the invoice for the vacuum cleaner you ordered came?b.
IchIhabehave(dietheRechnunginvoiceifu?rfor[dentheStaubsauger])vacuum cleanergefunden,found,(diewhichiDuyouvermi?tmissedhast).have.
?I found the invoice for the vacuum cleaner which you were missing?Scrambling9 is the effect that occurs when two verbs that both have arguments are in the same clause,and share the ordering domain in that clause, have crossing argument dependencies:(4) .
.
.
da?.
.
.
that(demtheKunden)customer[dentheKu?hlschrank]fridgebisheruntil nownochyetniemandnobody[zutoreparieren]repairzutoversuchentry(versprochen)promisedhat.has.
?that no one has promised the customer to try repairing the fridge.
?The example above is due to Becker et al.
(1992), who claim that there is no bound on the distance overwhich each element can scramble, nor a bound on the number of unbounded dependencies that can occurin one sentence.
Becker et al.
further claim that no LCFRS can faithfully represent a sequence of mverbs which are each preceded by one argument, where the arguments can be permuted freely.10Finally, Topicalization11 or more generally V2-order phenomena are those where a part of the verbclause (either an argument, or an argument of a phrase within the verb clause, or part of the verb clauseitself) is moved into clause-initial position.
(5) a.
EinaBuchbooku?beraboutdietheSavannesavannahhathasEleElemirmegeschenkt.given.
?Ele gave me a book about the savannah.?b.
U?beraboutdietheSavannesavannahhathasmirmeEleEleeinaBuchbookgeschenkt.given.
?Ele gave me a book about the savannah.?c.
EinaBuchbookgeschenktgivenhathasEleElemir.me.
?Give me a book, Ele did.
?9Sentences 40524, 40567, 40572, 40588, 40594, 40595, 40601, 40885, 40966, 4102510The practical consequence is that any LCFRS extracted from a treebank will either underspecify the dependencies in sucha construction ?
this is the flattening solution ?
or yield rules with growing block-degree.
van Cranenburgh (2012) shows thatthe sentences with up to 25 words in Negra can be parsed with an LCFRS that leads to O(n9) time complexity when a suitablebinarization is used, where the original treebank grammar would mean a parsing complexity of O(n19).
Maier et al.
(2012)point out that the observed time complexity of the arbitrary-block-degree parser used by Maier (2010) and Kallmeyer and Maier(2013) is due to necessary bookkeeping, and that their variant with fixed block degree yields a polynomial time complexity.11Sentences 40513, 40521, 40528, 40544, 40546, 40548, 40551, 40572, 40580, 40585495.3 Analysis of Parser behaviourUsing the movement actions, the parser is able to correctly attach topicalized nodes in simple sentences,and to sort out in most cases which nodes belong to the VP and which ones to the S node.
In the presenceof complex sentence structure, the very local view on the sentence that the parser has quickly becomesa hindrance.
Extraposed material is attached correctly in the case of relative clauses, whereas infinitivalconstructions (which can plausibly attach to the verb) are often missed, and clauses that are extraposedmodifiers of adverbs or adjectives are mostly missed.
As with early treebank-based parsers, the presenceof multiple verbs (as in coherent constructions) can mislead the parser into assuming a more complexstructure than is actually present.In general, verbless copula clauses, asyndetic coordination, and gapping/ellipsis, which are difficultfor dependency parsing, are also especially prone to confuse the very local view of the easy-first parser,which is a rather anticlimactic, yet commonsensical conclusion.In summary, simple material is often handled surprisingly well, whereas sentences with a complextopological structure ?
i.e., coordination, clauses embedded in a nominal phrase, or correlations, arerather challenging for easy-first parsing.
Parsing algorithms with more context such as Sartorio et al.
(2013) or an application of beam search might help in some of these cases.6 Summary and Future WorkIn this article, we presented a deterministic parser that uses an easy-first strategy to perform non-projective constituent parsing in expected linear time, with results that perform in a similar range asresults for discontinuous treebank grammars, and provides a means to provide rather fast parsing incases where discontinuous structure is required.
We introduced the barrier formulation as an alternativeto the lazy reordering of Nivre et al.
(2009), which shows similar performance but which may reveal acloser connection to formalisms with restricted discontinuities.While all experiments and the phenomen-oriented analysis have been performed on German data, thereordering oracle approach does not make any language-specific assumptions and constitutes a generaltechnique for deterministic parsing of discontinuous constituent trees.Acknowledgements The author would like to thank the three anonymous reviewers for their valuablecomments, and Thomas Mu?ller for providing the Marmot-tagged version of the SPMRL dataset.ReferencesAngelov, Krasimir and Peter Ljunglo?f.
2014.
Fast statistical parsing with multiple context-free grammars.In Proceedings of EACL 2014.Ballesteros, Miguel and Joakim Nivre.
2012.
MaltOptimizer: A system for MaltParser optimization.In Proceedings of the Eigth International Conference on Language Resources and Evaluation (LREC2012).Becker, Tilman, Owen Rambow, and Michael Niv.
1992.
The derivational generative power, or, scram-bling is beyond LCFRS.
Technical report, University of Pennsylvania.
A version of this paper waspresented at MOL3, Austin, Texas, November 1992.Bjo?rkelund, Anders, O?zlem C?etinoglu, Richard Farkas, Thomas Mu?ller, and Wolfgang Seeker.
2013.
(re)ranking meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task.
In Proc.SPMRL 2013.Bodenstab, Nathan, Aaron Dunlop, Keith Hall, and Brian Roark.
2011.
Adaptive beam-width predictionfor efficient CYK parsing.
In Proceedings of ACL/HLT 2011.Bo?hmova, A., Jan Hajic?, Eva Hajic?ova?, and B. Hladka?.
2001.
The Prague dependency treebank: Three-level annotaion scenario.
In Treebanks: Building and using syntactically annotated corpora, KluwerAcademic Publishers, pages 103?127.Bosco, Cristina and Vincenzo Lombardo.
2006.
Comparing linguistic information in treebank annota-tions.
In LREC 2006.50Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Proc.
TLT 2002.Cai, Shu, David Chiang, and Yoav Goldberg.
2011.
Language-independent parsing with empty elements.In Proceedings of ACL 2011.Carreras, Xavier, Michael Collins, and Terry Koo.
2008.
TAG, dynamic programming, and the perceptronfor efficient, feature-rich parsing.
In Proceedings of CoNLL.Collins, Michael.
2003.
Head-Driven statistical models for Natural Language parsing.
ComputationalLinguistics 29(4):589?637.Collins, Michael and Brian Roark.
2004.
Incremental parsing with the perceptron algorithm.
In ACL-04.Dipper, Stefanie, Anke Lu?deling, and Marc Resnicek.
2013.
NoSta-D: A corpus of german non-standardvarieties.
In Marcos Zampieri and Sascha Diwersy, editors, Non-standard DataSources in Corpus-based Research, Shaker Verlag.Dubey, Amit and Frank Keller.
2003.
Probabilistic parsing for German using sister-head dependencies.In ACL?2003.Duchi, John, Elad Hazan, and Yoram Singer.
2011.
Adaptive subgradient methods for online learningand stochastic optimization.
Journal of Machine Learning Research 12:2121?2159.Forst, Martin.
2007.
Filling statistics with linguistics - property design for the disambiguation of GermanLFG parses.
In ACL 2007 workshop on deep linguistic processing.Foth, Kilian.
2006.
Eine umfassende Dependenzgrammatik des Deutschen.
Technical report, Fachbe-reich Informatik, Universita?t Hamburg.Foth, Kilian, Arne Ko?hn, Niels Beuck, and Wolfgang Menzel.
2014.
Because size does matter: TheHamburg dependency treebank.
In Proceedings of the Language Resources and Evaluation Confer-ence (LREC 2014).Goldberg, Yoav and Michael Elhalad.
2010.
An efficient algorithm for easy-first non-directional depen-dency parsing.
In Proceedings of NAACL-2010.Hall, Johann and Joakim Nivre.
2008.
Parsing discontinuous phrase structure with grammatical func-tions.
In Proceedings of the 6th International Conference on Natural Language Processing (GoTAL2008).Hinrichs, Erhard and Tsuneko Nakazawa.
1989.
Flipped out: AUX in German.
In Papers from the 25thAnnual Regional Meeting of the Chicago Linguistic Society.Hockenmaier, Julia.
2007.
CCG grammar extraction from treebanks: translation algorithms and applica-tions.
Presentation from the Treebank Workshop, 2007, Rochester NY.Huang, Liang and David Chiang.
2007.
Forest rescoring: Faster decoding with integrated languagemodels.
In Proceedings of ACL 2007.Huang, Liang and Kenji Sagae.
2010.
Dynamic programming for linear-time incremental parsing.
InProceedings of ACL 2010.Kallmeyer, Laura and Wolfgang Maier.
2013.
Data-driven parsing using probabilistic linear context-freerewriting systems.
Computational Linguistics 39:87?119.Kapanadze, Oleg.
2012.
Building parallel treebanks for the lesser-resourced languages.
Technical report,Tbilisi State University.Koo, Terry, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective head automata.
In Proceedings of EMNLP 2010.Kornai, Andras and Geoff Pullum.
1990.
The X-bar theory of phrase structure.
Language 66:24?50.Magerman, David M. 1995.
Statistical decision-tree models for parsing.
In ACL?1995.Maier, Wolfgang.
2010.
Direct parsing of discontinuous constituents in German.
In Proceedings of theNAACL-HLT First Workshop on Statistical Parsing of Morphologically Rich Languages.51Maier, Wolfgang, Miriam Kaeshammer, Peter Baumann, and Sandra Ku?bler.
2014.
Discosuite ?
a parsertest suite for German discontinuous structures.
In Proceedings of LREC 2014.Maier, Wolfgang, Miriam Kaeshammer, and Laura Kallmeyer.
2012.
PLCFRS parsing revisited: Re-stricting the fan-out to two.
In Proceedings of the 11th International Workshop on Tree AdjoiningGrammar and Related Formalisms (TAG+11).Miyao, Yusuke, Rune S?tre, Kenji Sagae, Takuya Matsuzaki, and Jun?ichi Tsujii.
2008.
Task-orientedevaluation of syntactic parsers and their representations.
In ACL 2008.Mu?ller, Thomas, Helmut Schmid, and Hinrich Schu?tze.
2013.
Efficient higher-order CRFs for morpho-logical tagging.
In Proceedings fo EMNLP 2013.Nivre, Joakim.
2003.
An efficient algorithm for projective dependency parsing.
In 8th InternationalWorkshop on Parsing Technologies.Nivre, Joakim.
2009.
Non-projective dependency parsing in expected linear time.
In Proc.
Joint ACL-AFNLP 2009.Nivre, Joakim.
2011.
Bare-bones dependency parsing - a case for occam?s razor?
In Nodalida 2011.Nivre, Joakim, Marco Kuhlmann, and Johan Hall.
2009.
An improved oracle for dependency parsingwith online reordering.
In Proceedings of the 11th International Conference on Parsing Technologies(IWPT).Nivre, Joakim and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings of ACL2005.Pauly, Dennis, Ulyana Senyuk, and Ulrike Demske.
2012.
Strukturelle Mehrdeutigkeit infru?hneuhochdeutschen Texten.
Journal for Language Technology and Computational Linguistics27(2):65?82.Plaehn, Oliver.
2000.
Computing the most probable parse for a discontinuous phrase structure grammar.In Proceedings of the 6th International Workshop on Parsing Technologies.Sagae, Kenji and Alon Lavie.
2006.
A best-first probabilistic shift-reduce parser.
In Proceedings of theHuman Language Technology Conference of the NAACL (NAACL/HLT 2006).Sartorio, Francesco, Giorgio Satta, and Joakim Nivre.
2013.
A transition-based dependency parser usinga dynamic parsing strategy.
In Proceedings of ACL 2013.Schmid, Helmut.
2006.
Trace prediction and recovery with unlexicalized PCFGs and slash features.
InProceedings of COLING-ACL 2006.Seddah, Djame?.
2010.
Exploring the spinal-tig model for parsing French.
In Proceedings of LREC 2010.Seddah, Djame?, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho D. Choi, Richa?rd Farkas, JenniferFoster, Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash,Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepio?rkowski, Ryan Roth, WolfgangSeeker, Yannick Versley, Veronika Vincze, Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemontede la Clergerie.
2013.
Overview of the SPMRL 2013 Shared Task: A Cross-Framework Evaluationof Parsing Morphologically Rich Languages.
In Proceedings of the Fourth Workshop on StatisticalParsing of Morphologically-Rich Languages.
Seattle, WA, pages 146?182.Seeker, Wolfgang, Richa?rd Farkas, Bernd Bohnet, Helmut Schmid, and Jonas Kuhn.
2012.
Data-drivendependency parsing with empty heads.
In Proceedings of Coling 2012.Seeker, Wolfgang and Jonas Kuhn.
2012.
Making ellipses explicit in dependency conversion for a ger-man treebank.
In Proceedings of the Eight International Conference on Language Resources andEvaluation (LREC?12).Stolcke, Andreas.
1995.
An efficient probabilistic context-free parsing algorithm that computes prefixprobabilities.
Computational Linguistics 21(2):165?201.52Telljohann, Heike, Erhard W. Hinrichs, Sandra Ku?bler, Heike Zinsmeister, and Kathrin Beck.
2009.Stylebook for the Tu?bingen Treebank of Written German (Tu?Ba-D/Z).
Technical report, Seminar fu?rSprachwissenschaft, Universita?t Tu?bingen.Tratz, Stephen and Eduard Hovy.
2011.
A fast, accurate, non-projective, semantically-enriched parser.
InProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP2011).Uszkoreit, Hans.
1987.
Word order and constituent structure in German.
Number 8 in CSLI LectureNotes.
Center for the Study of Language and Information.van Cranenburgh, Andreas.
2012.
Efficient parsing with linear context-free rewriting systems.
In EACL2012.van Cranenburgh, Andreas and Rens Bod.
2013.
Discontinuous parsing with an efficient and accurateDOP model.
In Proceedings of the International Conference on Parsing Technologies (IWPT 2013).van Noord, Geertjan.
1991.
Head corner parsing for discontinuous constituency.
In Proceedings of ACL1991.Versley, Yannick and Heike Zinsmeister.
2006.
From dependency parsing to deep(er) semantics.
InProceedings of the Fifth International Workshop on Treebanks and Linguistic Theories (TLT 2006).Vincze, Veronika, Do?ra Szauter, Attila Alma?si adn Gyo?rgy Mo?ra, Zolta?n Alexin, and Ja?nos Csirik.
2010.Hungarian dependency treebank.
In Proceedings of the Seventh Conference on International LanguageResources and Evaluation (LREC 2010).Volk, Martin and Yvonne Samuelsson.
2004.
Bootstrapping parallel treebanks.
In Proceedings of the5th International Workshop on Linguistically Interpreted Corpora (LINC) at Coling 2004.53
