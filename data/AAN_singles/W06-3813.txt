Workshop on TextGraphs, at HLT-NAACL 2006, pages 81?88,New York City, June 2006. c?2006 Association for Computational LinguisticsMatching Syntactic-Semantic Graphs for Semantic Relation AssignmentVivi Nastase1 and Stan Szpakowicz1,21 School of Information Technology and Engineering,University of Ottawa, Ottawa, Canada2 Institute of Computer Science,Polish Academy of Sciences, Warsaw, Poland{vnastase,szpak}@site.uottawa.caAbstractWe present a graph-matching algorithmfor semantic relation assignment.
The al-gorithm is part of an interactive text analy-sis system.
The system automatically ex-tracts pairs of syntactic units from a textand assigns a semantic relation to eachpair.
This is an incremental learning algo-rithm, in which previously processed pairsand user feedback guide the process.
Af-ter each assignment, the system adds to itsdatabase a syntactic-semantic graph cen-tered on the main element of each pair ofunits.
A graph consists of the main unitand all syntactic units with which it is syn-tactically connected.
An edge contains in-formation both about syntax and about se-mantic relations for use in further process-ing.
Syntactic-semantic graph matching isused to produce a list of candidate assign-ments for 63.75% of the pairs analysed,and in 57% of situations the correct rela-tions is one of the system?s suggestions;in 19.6% of situations it suggests only thecorrect relation.1 IntroductionWhen analysing texts, it is essential to see how el-ements of meaning are interconnected.
This is anold idea.
The first chronicled endeavour to con-nect text elements and organise connections betweenthem goes back to the 5th century B.C.
and the workof Panini1.
He was a grammarian who analysed San-skrit (Misra, 1966).
The idea resurfaced forcefullyat several points in the more recent history of lin-guistic research (Tesnie`re, 1959; Gruber, 1965; Fill-more, 1968).
Now it has the attention of many re-searchers in natural language processing, as shownby recent research in semantic parsing and semantic1The sources date his work variously between the 5th and7th century.role labelling (Baker et al, 1998; Kipper et al, 2000;Carreras and Marquez, 2004; Carreras and Marquez,2005; Atserias et al, 2001; Shi and Mihalcea, 2005).Graph-like structures are a natural way of or-ganising one?s impressions of a text seen fromthe perspective of connections between its simplerconstituents of varying granularity, from sectionsthrough paragraphs, sentences, clauses, phrases,words to morphemes.In this work we pursue a well-known and of-ten tacitly assumed line of thinking: connections atthe syntactic level reflect connections at the seman-tic level (in other words, syntax carries meaning).Anecdotal support for this stance comes from thefact that the grammatical notion of case is the basisfor semantic relations (Misra, 1966; Gruber, 1965;Fillmore, 1968).
Tesnie`re (1959), who proposes agrouping of verb arguments into actants and circum-stances, gives a set of rules to connect specific typesof actants ?
for example, agent or instrument ?
tosuch grammatical elements as subject, direct object,indirect object.
This idea was expanded to includenouns and their modifiers through verb nominaliza-tions (Chomsky, 1970; Quirk et al, 1985).We work with sentences, clauses, phrases andwords, using syntactic structures generated by aparser.
Our system incrementally processes a text,and extracts pairs of text units: two clauses, a verband each of its arguments, a noun and each of itsmodifiers.
For each pair of units, the system builds asyntactic graph surrounding the main element (mainclause, head verb, head noun).
It then tries to findamong the previously processed instances anothermain element with a matching syntactic graph.
Ifsuch a graph is found, then the system maps pre-viously assigned semantic relations onto the currentsyntactic graph.
We have a list of 47 relations thatmanifest themselves in compound clauses, inside asimple clause or in noun phrases.
The list, a syn-thesis of a number of relation lists cited in the lit-erature, has been designed to be general, domain-independent (Barker et al, 1997a).Section 2 overviews research in semantic relationanalysis.
Section 3 describes the text we used in ex-81periments, and the semantic relation list.
Section 4looks in detail at the graph-matching heuristic.
Sec-tion 5 describes the experimental setting and showshow often the heuristic was used when processingthe input text.
We show in detail our findings aboutsyntactic levels (how often graph matching helpedassign a relation between two clauses, a verb and itsarguments, or a noun and its modifier) and about theaccuracy of the suggestion.
Discussion and conclu-sions appear in Section 6.2 Related WorkSome methods of semantic relation analysis rely onpredefined templates filled with information fromprocessed texts (Baker et al, 1998).
In other meth-ods, lexical resources are specifically tailored tomeet the requirements of the domain (Rosario andHearst, 2001) or the system (Gomez, 1998).
Suchsystems extract information from some types of syn-tactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002; Hull and Gomez, 1996);noun phrases in (Hull and Gomez, 1996; Rosario etal., 2002)).
Lists of semantic relations are designedto capture salient domain information.In the Rapid Knowledge Formation Project (RKF)a support system was developed for domain experts.It helps them build complex knowledge bases bycombining components: events, entities and modi-fiers (Clark and Porter, 1997).
The system?s inter-face facilitates the expert?s task of creating and ma-nipulating structures which represent domain con-cepts, and assigning them relations from a relationdictionary.In current work on semantic relation analysis, thefocus is on semantic roles ?
relations between verbsand their arguments.
Most approaches rely on Verb-Net (Kipper et al, 2000) and FrameNet (Baker etal., 1998) to provide associations between verbs andsemantic roles, that are then mapped onto the cur-rent instance, as shown by the systems competing insemantic role labelling competitions (Carreras andMarquez, 2004; Carreras and Marquez, 2005) andalso (Gildea and Jurafsky, 2002; Pradhan et al,2005; Shi and Mihalcea, 2005).These systems share two ideas which make themdifferent from the approach presented here: they allanalyse verb-argument relations, and they all usemachine learning or probabilistic approaches (Prad-han et al, 2005) to assign a label to a new instance.Labelling every instance relies on the same previ-ously encoded knowledge (see (Carreras and Mar-quez, 2004; Carreras and Marquez, 2005) for anoverview of the systems in the semantic role la-belling competitions from 2004 and 2005).
Pradhanet al (2005) combine the outputs of multiple parsersto extract reliable syntactic information, which istranslated into features for a machine learning ex-periment in assigning semantic roles.Our system analyses incrementally pairs of unitscoming from three syntactic levels ?
clause (CL),intra-clause (or verb-argument, IC), noun-phrase(NP).
There are no training and testing data sets.
In-stead of using previously built resources, the systemrelies on previously processed examples to find themost appropriate relation for a current pair.
Becausethe system does not rely on previously processedor annotated data, it is flexible.
It allows the userto customize the process for a specific domain bychoosing the syntactic units of interest and her ownlist of relations that best fit the domain.It is also interesting to assess, using the currentsystem configuration, the effect of syntactic infor-mation and incremental learning on semantic analy-sis.
This is described in section 5.Because of these differences in the type of dataused, and in the learning approach, the results weobtain cannot be compared to previous approaches.In order to show that the system does learn, we showthat the number of examples for which it providesthe correct answer increases with the number of ex-amples previously analysed.3 Input data and semantic relations3.1 Input dataWe work with a semi-technical text on meteorolog-ical phenomena (Larrick, 1961), meant for primaryschool students.
The text gradually introduces con-cepts related to precipitation, and explains them.
Itsnature makes it appropriate for the semantic analy-sis task in an incremental approach.
The system willmimic the way in which a human reader accumu-lates knowledge and uses what was written before toprocess ideas introduced later in the text.The text contains 513 sentences, with an averagelength of 9.13 words.
There are 4686 word tokensand 969 types.
The difference between the num-ber of types (2850) and tokens (573) in the extractedpairs (which contain only open-class words) showsthat the same concepts recur, as expected in a didac-tic text.The syntactic structures of the input data areproduced by a parser with good coverage and de-tailed syntactic information, DIPETT (Delisle andSzpakowicz, 1995).
The parser, written in Prolog,implements a classic constituency English grammarfrom Quirk et al (1985).
Pairs of syntactic unitsconnected by grammatical relations are extractedfrom the parse trees.
A dependency parser would82produce a similar output, but DIPETT also providesverb subcategorization information (such as, for ex-ample, subject-verb-object or subject-verb-object-indirect object), which we use to select the (best)matching syntactic structures.To find pairs, we use simple structural informa-tion.
If a unit is directly embedded in another unit,we assume a subordinate relation between them; ifthe two units are coordinate, we assume a coordinaterelation.
These assumptions are safe if the parse iscorrect.
A modifier is subordinate to its head noun,an argument to its head verb, and a clause perhaps tothe main clause in the sentence.If we conclude that two units should interact, weseek an appropriate semantic relation to describe thisinteraction.
The system uses three heuristics to findone or more semantic relation candidates for the cur-rent pair.1.
Word match ?
the system will propose the se-mantic relation(s) that have previously been as-signed to a pair containing the same lemmas.2.
Syntactic graph match ?
we elaborate thisheuristic in Section 4.3.
Marker ?
the system uses a manually built dic-tionary of markers (prepositions, coordinators,subordinators) associated with the semantic re-lations they indicate.
The dictionary contains325 markers, and a total of 662 marker-relationassociations.If neither of the three heuristics yield results, thesystem will present an empty list, and expect the userto input the appropriate relation.
When at least onerelation is proposed, the user can accept a uniquerelation, choose among several options, or supplya new one.
The system records which action tookplace, as well as the heuristic that generated the op-tions presented to the user.
The pair is also analysedto determine the syntactic level from which it came,to allow for a more detailed analysis of the behaviourof the system.3.2 Semantic relationsThe list of semantic relations with which we workis based on extensive literature study (Barker et al,1997a).
Three lists of relations for three syntacticlevels ?
inter-clause, intra-clause (case) and noun-modifier relations ?
were next combined based onsyntactic and semantic phenomena.
The resultinglist is the one used in the experiments we presentin this paper.
The relations are grouped by generalsimilarity into 6 relation classes (H denotes the headof a base NP, M denotes the modifier).1.
CAUSAL groups relations enabling or oppos-ing an occurrence.
Examples:cause - H causes M: u virus;effect - H is the effect (was caused by) M:exam anxiety;purpose - H is for M: concert hall;2.
CONJUNCTIVE includes relations that describethe conjunction or disjunction of occurrences(events/act/actions/states/activities), entities orattributes:conjunction - both H and M occur or exist(and nothing more can be said about thatfrom the point of view of causality ortemporality): running and swimming (aregood for you);disjunction - either one or both H and M occuror exist: painting or drawing;3.
PARTICIPANT groups relations between an oc-currence and its participants or circumstances.Examples:agent - M performs H: student protest;object - M is acted upon by H: metal separa-tor;beneficiary - M benefits from H: student dis-count;4.
SPATIAL groups relations that place an occur-rence at an absolute or relative point in space.Examples:direction - H is directed towards M: outgoingmail;location - H is the location of M: home town;location at - H is located at M: desert storm;5.
TEMPORAL groups relations that place an oc-currence at an absolute or relative point in time.Examples:frequency - H occurs every time M occurs:weekly game;time at - H occurs when M occurs: morningcoffee;time through - H existed while M existed: 2-hour trip;6.
QUALITY groups the remaining relations be-tween a verb or noun and its arguments.
Exam-ples:manner - H occurs as indicated by M: stylishwriting;83material - H is made of M: brick house;measure - M is a measure of H: heavy rock;There is no consensus in the literature on a list ofsemantic relations that would work in all situations.This is, no doubt, because a general list of relationssuch as the one we use would not be appropriate forthe semantic analysis of texts in a specific domain,such as for example medical texts.
All the relationsin the list we use were necessary, and sufficient, forthe analysis of the input text.4 Syntactic-semantic graph-matchingOur system begins operation with a minimum ofmanually encoded knowledge, and accumulates in-formation as it processes the text.
This design ideawas adopted from TANKA (Barker et al, 1997b).The only manually encoded knowledge is a dictio-nary of markers (subordinators, coordinators, prepo-sitions).
This resource does not affect the syntactic-semantic graph-matching heuristic.Because the system gradually accumulatesknowledge as it goes through the input text, it uses aform of memory-based learning to make predictionsabout the semantic relation that fits the current pair.The type of knowledge that it accumulates consistsof previously analysed pairs, together with thesemantic relation assigned, and a syntactic-semanticgraph centered on each word in a sentence whichappears as the main element in a processed pair.To process a pair P not encountered previously,the system builds a graph centered on the main ele-ment (often the head) of P .
This idea was inspiredby Delisle et al (1993), who used a list of argu-ments surrounding the main verb together with theverb?s subcategorization information and previouslyprocessed examples to analyse semantic roles (caserelations).
In recent approaches, syntactic informa-tion is translated into features which, together withinformation from FrameNet, WordNet or VerbNet,will be used with ML tools to make predictions foreach example in the test set (Carreras and Marquez,2004; Carreras and Marquez, 2005).Our system builds a (simple) graph surroundinga head word (which may be a verb ?
representingthe predicate of a sentence, or representing a clause?
or noun), and matches it with previously analysedexamples.A graph G(w) centered on word w consists ofthe following: a node for w; a set of nodes foreach of the words wi in the sentence with which wis connected by a grammatical relation (includingsituations when w is a modifier/argument); edgesthat connect w with each wi, tagged with gram-matical relation GR (such as subject, object, com-plement) and connective information Con (preposi-tions, coordinators, subordinators, or nil).
The nodesalso contain part-of-speech information for the cor-responding word, and other information from theparser (such as subcategorization structure for theverb, if it is available).Graph matching starts with the central node, andcontinues with edge matching.
If G(w) is the graphcentered on word w whose pairs are currently beingprocessed, the system selects from the collection ofpreviously stored graphs, a set of graphs {G(wi)},which satisfy the following conditions:?
The central nodes match.
The matching isguided by a set of contraints.
We choose thegraphs centered on the nodes that satisfy themost constraints, presented here in the order oftheir importance:?
w and wi must have the same part ofspeech.?
w and wi have the same syntactic proper-ties.
If w and wi are verbs, they must havethe same subcategorization structure.?
w and wi are the same lemma.
We empha-size that a graph centered on a differentlemma, but with the same subcategoriza-tion structure is preferred to a graph withthe same lemma, but a different subcate-gorization structure.?
The edge representing the word pair to whichwe want to assign a semantic relation has amatch in G(wi).
From all graphs that com-ply with this constraint, the ones that have thelowest distance ?
corresponding to the high-est matching score ?
are chosen.
The graphsare matched edge by edge.
Two edges matchif the grammatical relation and the connectivesmatch.
Figure 1 shows the formula that com-putes the distance between two graphs.
Wenote that edge matching uses only edge infor-mation ?
grammatical and connective informa-tion.
Using the node information as is (lemmasand their part of speech) is too restrictive.
Weare looking into using word similarity as a so-lution of node matching.If no matching graph has been found, the systemsearches for a simpler match, in which the currentword pair is matched against previously processedpairs, using the same formula as for edge distance,and preferring the pairs that have the same modifier.This algorithm will retrieve the set of graphs{G(wi)}, which give the same score when matched84Definition of a graph centered on w:G(w) = {wi, edge(w,wi) or edge(wi, w)|wiappears in sent.
S, and is connected to w}edge(w,wi) = {GRi, Coni} ; GRi ?
{subject, object, complement, ...}Coni ?
{at, in, on,with, for, ...}Distance metric between two graphs:dist(G(w1), G(w2)) =N?k=1d(edge1k , edge2k); edgeik ?
G(wi), N is the number of edges in G(wi)d(edge1k , edge2k)=d({GR1k , Con1k}, {GR2k , Con2k})=d1(GR1k, GR2k) + d1(Con1k, Con2k)d1(x, y) ={ 0 : x = y1 : x 6= yFigure 1: Distance between two graphswith the current graph.
The set of possible semanticrelations presented to the user consists of the seman-tic relation on the edge of each G(wi) that matchesthe edge (of the current graph) corresponding to theword pair which we are analysing.To the sentence:When you breathe out on a cold day, you make acloud.corresponds the following syntactic graph:(compl,nil)(subord,when)out (v, sv)(compl,nil)(v,svo)breathe makecolddayyou you(subj,nil)(subj,nil)(compl,on)cloudWhen we focus on the graph centered on a spe-cific word, such as breathe, we look only at the nodecorresponding to the word breathe, and the nodesadjacent to it.To process a pair P = (wH , wM ), the system firstbuilds G(wH), and then searches through previouslystored graphs for those which have the same centerwH , or have the same part of speech as wH assignedto its central node.
For each graph found, we com-pute a distance that gives a measure of the matchbetween the two graphs.
The best match will havethe smallest distance.For example, for the sentence:Weathermen watch the clouds day and night.the system builds the following network centeredon the predicate watch2:cloudweathermanwatch(v, svo)(subj,nil)(compl,nil)day and night(compl,nil)The system locates among previously stored net-works those centered around verbs3.
For the sen-tence above, the system uses the following graph,2The nil value on the edges means that no preposition orother connective explicitly links the two words or the corre-sponding syntactic structures.3If more detailed information is available, the system willchoose only networks associated with verbs that have the samesubcategorisation pattern (svo, svoi and so on).85built from the immediately preceding sentence in thetext:Air pilots know that clouds can bring rain, hail,sleet and snow.
(v, svo)(subj,nil)knowair pilots bring(compl,nil)AGENTOBJECTAccording to the metric, the networks match andthe pairs (watch, weatherman) and (know, air pi-lots) match, so the semantic relation for the pair(know, air pilots) is proposed as a possible relationfor pair (watch, weatherman) .5 ExperimentsThe system processes the 513 sentences interac-tively.
It begins by running the DIPETT parser.Next, it extracts syntactic units (clauses, phrases,words) and pairs them up according to the informa-tion in the parse tree.
Each unit is represented byits head word.
Next, the system checks if the samepair of word lemmas has already been processed, topropose the same relation(s) to the user as options.If not, the system builds a graph centered on thehead word, and proceeds with the matching on pre-viously encountered instances, as described in sec-tion 4.
When a set of candidates has been found, thesystem goes through a dialogue with the user.The system generated 2020 pairs from the 513sentences.
The experiment was run in 5 interactivesessions of approximately 3 hours each.
The totalnet processing time was 6 hours, 42 minutes and 52seconds4 .
While it would have been instructive torun the system several times with different users, itwas not feasible.
The experiment was run once, withtwo cooperating users.
They were instructed on theset of semantic relations, and told how the systemworks.
They discussed the semantic relation assign-ment and, once agreed, compared the system?s sug-gestion with their decision.DIPETT did not produce a complete parse for allsentences.
When a complete parse (correct or incor-rect) was not possible, DIPETT produced fragmen-tary parses.
The semantic analyser extracted unitseven from tree fragments, although sometimes thefragments were too small to accommodate any pairs.Of the 513 input sentences, 441 had a parse tree thatallowed the system to extract pairs.4The time difference accounts for system processing times,and user interaction for other steps of the analysis.# of analyzed examples 1475level statistics CL IC NP64 978 433user actions accept choose supply459 393 623avg.
# of suggestions 2.81graph-matching usage 933level/action statistics CL IC NPaccept 183 (19.61%) 9 141 33choose 349 (37.41%) 23 314 12supply 401 (42.98%) 27 316 58Table 1: Summary of semantic analysisOf 2020 pairs generated, the users discarded 545in the dialogue step.
An example of an erroneouspair comes from the sentence:Tiny clouds drift across like feathers on parade.The semantic analyser produced the pair(drift,parade), because of a parsing error: pa-rade was parsed as a complement of drift, insteadof a post-modifier for feathers.
The correct pairing(feather,parade) is missing, because it cannot beinferred from the parse tree.Table 1 gives a summary of the processing statis-tics.
We observe that graph-matching was used toprocess a clear majority of the total pairs extracted ?63.25% (933/1475) , leaving the remaining 36.75%for the other two heuristics and for cases where nosuggestion could be made.
In 57.02% of the situa-tions when graph-matching was used, the system?ssuggestion contained the correct answer (user?s ac-tion was either accept or choose), and in 19.61% ofthe situations a single correct semantic relation wasproposed (user action was accept).When the system presents multiple suggestions tothe user, including the correct one, the average num-ber of suggestions is 3.75.
The small number ofsuggestions shows that the system does not simplyadd to the list relations that it has previously encoun-tered, but it learns from past experience and graph-matching helps it make good selections.
Figure 2plots the difference between the number of exam-ples for which the system gives the correct answer(possibly among other suggestions) and the numberof examples when the user must supply the correctrelation, from the first example processed until theend of the experiment.
We observe a steady increasein the number of correctly processed examples.Our system does not differentiate between syntac-tic levels, but based on the structures of the syntac-tic units in each pair we can decide which syntacticlevel it pertains to.
For a more in-depth analysis, wehave separated the results for each syntactic level,86-200204060801001201400  100  200  300  400  500  600  700  800  900  1000Differenceinnumber of examplesExamples processedFigure 2: Difference between the number of situa-tions in which the user accepts or chooses from thesystem?s suggestions, and when it must supply thecorrect relationand present them for comparison in Figure 3.We observe that the intra-clause level ?
verbsand their arguments ?
makes the best use of graph-matching, with the curve showing the cumulativenumber of situations in which the system makes cor-rect predictions becoming steeper as more text isprocessed.
At the same time, the curve that plots thecumulative number of cases in which the user has tosupply a correct answer begins to level off.
As ex-pected, at the noun-phrase level where the syntacticstructures are very simple, often consisting of onlythe noun and its modifier (without even a connec-tive), the graph-matching algorithm does not help asmuch.
At the inter-clause level the heuristic helps,as shown by the marginally higher curve for cumula-tive accept/choose user actions, compared to supplyactions.6 ConclusionsWe have shown through the results gathered from aninteractive and incremental text processing systemthat syntactic-semantic graph-matching can be usedwith good results for semantic analysis of texts.
Thegraph-matching heuristic clearly dominates otherheuristics used, and it learns to make better predic-tions as more examples accumulate.Graph-matching is most useful for assigning se-mantic relations between verbs and their arguments,but it also gives good results for inter-clause rela-tions.
At the noun-phrase level, we could only tacklenoun-modifier pairs that exhibit a modicum of syn-tactic structure ?
a connective.
For base NPs thereis practically nothing that syntactic information canbring to the semantic analysis process.The graph-matching process could be improvedby bringing into play freely available lexical re-1.
All syntactic levels01002003004005006000  100  200  300  400  500  600  700  800  900  1000Cummulativenumber of examplesExamples processedaccept+choosesupply2.
Clause level (CL)051015202530350  10  20  30  40  50  60Cummulativenumber of examplesExamples processedaccept+choosesupply3.
Intra-clause level (IC)0501001502002503003504004505000  100  200  300  400  500  600  700  800Cummulativenumber of examplesExamples processedaccept+choosesupply4.
Noun phrase level(NP)01020304050600  20  40  60  80  100  120Cummulativenumber of examplesExamples processedaccept+choosesupplyFigure 3: Graph-matching for different syntacticlevels87sources.
For now, the actual words in the graphnodes are not used at all.
We could use WordNetto compute word similarities, to select closer match-ing graphs.
VerbNet or FrameNet information couldhelp choose graphs centered on verbs with simi-lar syntactic behaviour, as captured by Levin?s verbgroups (Levin, 1993) which are the basis of VerbNet.ReferencesJordi Atserias, L.
Padro?, and German Rigau.
2001.
Integratingmultiple knowledge sources for robust semantic parsing.
InProceedings of RANLP - 2001, Tsigov Czark, Bulgaria.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998.The Berkeley FrameNet project.
In COLING-ACL, pages86?90, Montreal, Canada.Ken Barker, Terry Copeck, Sylvain Delisle, and Stan Szpakow-icz.
1997a.
Systematic construction of a versatile case sys-tem.
Journal of Natural Language Engineering, 3(4):279?315.Ken Barker, Sylvain Delisle, and Stan Szpakowicz.
1997b.Test-driving TANKA: Evaluating a semi-automatic systemof text analysis for knowledge acquisition.
In Proceedingsof CAI 1997, pages 60?71, Vancouver, BC, Canada.Xavier Carreras and Lluis Marquez, editors.
2004.
Introductionto the CoNLL-2004 Shared Task: Semantic Role Labelling.Boston, MA, USA.Xavier Carreras and Lluis Marquez, editors.
2005.
Introductionto the CoNLL-2005 Shared Task: Semantic Role Labelling.Ann Arbour, MI, USA.Noam Chomsky.
1970.
Remarks on nominalizations.
In Rod-erick Jacobs and Peter Rosenbaum, editors, Readings in En-glish Transformational Grammar, pages 184?221.
Ginn andCo., Waltham, MA, USA.Peter Clark and Bruce Porter.
1997.
Building concept reprezen-tations from reusable components.
In AAAI, pages 367?376,Providence, Rhode Island.Sylvain Delisle and Stan Szpakowicz.
1995.
Realistic pars-ing: Practical solutions of difficult problems.
In PACLING,Brisbane, Queensland, Australia.Sylvain Delisle, Terry Copeck, Stan Szpakowicz, and KenBarker.
1993.
Pattern matching for case analysis: A com-putational definition of closeness.
In ICCI, pages 310?315,Sudbury, ON, Canada.Charles Fillmore and Beryl T. Atkins.
1998.
FrameNet andlexicographic relevance.
In Proceedings of the 1st Interna-tional Conference on Language Resources and Evaluation,Granada, Spain.Charles Fillmore.
1968.
The case for case.
In Emmond Bachand Robert T. Harms, editors, Universals in Linguistic The-ory, pages 1?88.
Holt, Rinehart and Winston.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic labelingof semantic roles.
Computational Linguistics, 28(3):245?288.Fernando Gomez.
1998.
A representation of complex eventsand processes for the acquisition of knowledge from text.Kowledge-Based Systems, 10(4):237?251.Jeffrey Gruber.
1965.
Studies in Lexical Relations.
Ph.D.thesis, MIT, Cambridge, MA.
Reprinted in Jeffrey Gru-ber.
1976.
Lexical Structures in Syntax and Semantics.
PartI.
North-Holland Publishing Company, Amsterdam.Richard D. Hull and Fernando Gomez.
1996.
Semantic inter-pretation of nominalizations.
In 13th National Conferenceon Artificial Intelligence, pages 1062?1068, Portland, Ore-gon, USA.Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000.Class-based construction of a verb lexicon.
In AAAI/IAAI,pages 691?696.Nancy Larrick.
1961.
Junior Science Book of Rain, Hail, Sleetand Snow.
Garrard Publishing Company, Champaign, Illi-nois.Beth Levin.
1993.
English Verb Classes and Alternations.
Uni-versity of Chicago Press.Vidya Niwas Misra.
1966.
The Descriptive Technique ofPanini.
Mouton, The Hague.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Mar-tin, and Daniel Jurafsky.
2005.
Semantic role labelling us-ing different syntactic views.
In Proceedings of ACL 2005,pages 581?588, Ann Arbour, MI, USA.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and JanSvartvik.
1985.
A Comprehensive Grammar of the EnglishLanguage.
Longman, London and New York.Barbara Rosario and Marti Hearst.
2001.
Classifying the se-mantic relations in noun-compounds via a domain specifichierarchy.
In EMNLP, pages 82?90, Pittsburg, PA, USA.Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002.The descent of hierarchy and selection in relational seman-tics.
In ACL, Philadelphia, PA, USA.Lei Shi and Rada Mihalcea.
2005.
Putting pieces together:Combining framenet, verbnet and wordnet for robust seman-tic parsing.
In Proceedings of CICLing 2005, pages 100?111, Mexico City, Mexico.Lucien Tesnie`re.
1959.
E?le?ments de syntaxe structurale.
C.Klincksieck, Paris.88
