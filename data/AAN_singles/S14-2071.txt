Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 411?415,Dublin, Ireland, August 23-24, 2014.LyS: Porting a Twitter Sentiment Analysis Approachfrom Spanish to EnglishDavid Vilares, Miguel Hermo, Miguel A. Alonso, Carlos Go?mez-Rodr?
?guez, Yerai DovalGrupo LyS, Departamento de Computacio?n, Facultade de Informa?ticaUniversidade da Corun?a, Campus de A Corun?a15071 A Corun?a, Spain{david.vilares, miguel.hermo, miguel.alonso, carlos.gomez, yerai.doval}@udc.esAbstractThis paper proposes an approach to solvemessage- and phrase-level polarity classi-fication in Twitter, derived from an exist-ing system designed for Spanish.
As afirst step, an ad-hoc preprocessing is per-formed.
We then identify lexical, psycho-logical and semantic features in order tocapture different dimensions of the humanlanguage which are helpful to detect sen-timent.
These features are used to feed asupervised classifier after applying an in-formation gain filter, to discriminate irrel-evant features.
The system is evaluated onthe SemEval 2014 task 9: Sentiment Anal-ysis in Twitter.
Our approach worked com-petitively both in message- and phrase-level tasks.
The results confirm the robust-ness of the approach, which performedwell on different domains involving shortinformal texts.1 IntroductionMillions of opinions, conversations or just triviaare published each day in Twitter by users of dif-ferent cultures, countries and ages.
This providesan effective way to poll how people praise, com-plain or discuss about virtually any topic.
Compre-hending and analysing all this information has be-come a new challenge for organisations and com-panies, which aim to find out a way to make quickand more effective decisions for their business.
Inparticular, identifying the perception of the publicwith respect to an event, a service or an entity aresome of their main goals in a short term.
In thisrespect, sentiment analysis, and more specificallypolarity classification, is playing an important roleThis work is licensed under a Creative Commons Attribu-tion 4.0 International Licence.
Page numbers and proceed-ings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/in order to automatically analyse subjective infor-mation in texts.This paper describes our participation at Sem-Eval 2014 task 9: Sentiment Analysis in Twit-ter.
Specifically, two subtasks were presented:(A) contextual polarity disambiguation and (B)message polarity classification.
The first sub-task consists on determining the polarity of wordsor phrases extracted from short informal texts,the scope of extracts being provided by the Se-mEval organisation.
Subtask B focusses on clas-sifying the content of the whole message.
Inboth cases, three possible sentiments are consid-ered: positive, negative and neutral (which in-volves mixed and non-opinionated instances).
Al-though the training set only contains tweets, thetest set also includes short informal texts fromother domains, in order to measure cross-domainportability.
You can test the model for subtask Bat miopia.grupolys.org.2 SemEval 2014-Task 9: SentimentAnalysis in TwitterOur contribution is a reduced version of a Span-ish sentiment classification system (Vilares et al.,2013a; Vilares et al., 2013b) that participated inTASS 2013 (Villena-Roma?n et al., 2014), achiev-ing the 5th place on the global sentiment classifi-cation task and the 1st place on topic classificationon tweets.
In this section we describe how we haveported to English this system originally designedfor Spanish.
Tasks A and B are addressed fromthe same perspective, which is described below.2.1 PreprocessingWe implement a naive preprocessing algorithmwhich seeks to normalise some of the most com-mon ungrammatical elements.
It is intended forTwitter, but many of the issues addressed wouldalso be valid in other domains:411?
Replacement of frequent abbreviations Thelist of the most frequent ones was extractedfrom the training set, taking the Penn Tree-bank (Marcus et al., 1993) as our dictionary.A term is considered ungrammatical if it doesnot appear in our dictionary.
We then carryout a manual review to distinguish betweenunknown words and abbreviations, providinga correction in the latter case.
For example,?c?mon?
becomes ?come on?
and ?Sat?
is re-placed by ?Saturday?.?
Emoticon normalisation: We employ theemoticon collection published in (Agarwal etal., 2011).
Each emoticon is replaced withone of these five labels: strong positive (ESP),positive (EP), neutral (ENEU), negative (EN)or strong negative (ESN).?
Laughs : Multiple forms used in social mediato reflect laughs (e.g.
?hhahahha?, ?HHEHE-HEH?)
are preprocessed in a homogeneousway to obtain a pattern of the form ?hxhx?where x ?
{a, e, i, o, u}.?
URL normalisation: External links are re-placed by the string ?url?.?
Hashtags (?#?)
and usernames (?@?
): If thehashtag appears at the end or beginning ofthe tweet, we remove the hashtag.
Basedon other participant approaches at SemEval2013 (Nakov et al., 2013), we realized maybethis is not the best option, although we be-lieve hashtags will not be useful in most ofcases, since they refer to very specific events.Otherwise, only the ?#?
is removed, hypothe-sising the hashtag is used to emphasise a term(e.g.
?Matthew #Mcconaughey has won theOscar?
).2.2 Feature ExtractionOur approach only takes into account informationextracted from the text, without considering anykind of meta-data.
Extracted features combinelexical, psychological and semantic knowledge inorder to build a linguistic model able to analysetweets, but also other kinds of messages.
Thesefeatures can be divided into two types: corpus-extracted features and lexicon-extracted features.All of them take the total number of occurrencesof the respective feature as the weighting factor tothen feed the supervised classifier.2.2.1 Corpus-extracted featuresGiven a corpus, we use it to extract the followingset of features:?
Word forms: A model based on this type offeatures is our baseline.
Each single word isconsidered as a feature in order to feed thesupervised classifier.
This often becomes asimple and acceptable start point which ob-tains a decent performance.?
Part-of-speech (PoS) information: somecoarse-grained PoS-tags such as adjective oradverb are usually good indicators of subjec-tive texts while some fine-grained PoS tagssuch as third person personal pronoun pro-vide evidence of non-opinionated messages(Pak and Paroubek, 2010).2.2.2 Lexicon-extracted featuresWe also consider information obtained from exter-nal lexicons in order to capture linguistic informa-tion that can not be extracted from a training cor-pus by means of bag-of-words and PoS-tag mod-els.
We rely on two manually-build lexicons:?
Pennebaker et al.
(2001) psychometric dictio-naries.
Linguistic Inquiry and Word Count1(LIWC) is a software which includes a seman-tic dictionary to measure how people use dif-ferent kinds of words over a wide number oftexts.
It categorises terms into psychometricproperties, which correspond to different di-mensions of the human language.
The dictio-nary relates terms with psychological prop-erties (e.g.
anger or anxiety), but also withtopics (e.g.
family, friends, religion) or evenmorphological features (e.g.
future time, pasttime or exclamations).?
Hu and Liu (2004) opinion lexicon.
It is a col-lection of positive and negative words.
Manyof the occurrences are misspelled, since theyoften come from web environments.2.2.3 Syntactic featuresWe also parsed the tweets using MaltParser (Nivreet al., 2007) in order to obtain dependency tripletsof the form (wi, arcij, wj), where wiis the headword wj, the dependent one and arcijthe exist-ing syntactic relation between them.
We tried toincorporate generalised dependency triplets (Joshi1http://www.liwc.net/412and Penstein-Rose?, 2009), following an enrichedperspective presented in Vilares et al.
(2014).
Ageneralisation consists on backing off the wordsto more abstracted terms.
For example, a valid de-pendency triplet for the phrase ?awesome villain?is (villain, modifier, awesome), which could begeneralised into (anger, modifier, assent) by meansof psychometric properties.
However, experimen-tal results over the development corpus using thesefeatures decreased performance with respect toour best model, probably due to the small size ofthe training corpus, since dependency triplets tendto suffer from sparsity, so a larger training corpusis needed to exploit them in a proper way (Vilareset al., 2014).2.3 Feature SelectionFor a machine learning approach, sparsity couldbe an issue.
In particular, due to the size of the cor-pus, many of the terms extracted from the trainingset only appear a few times in it.
This makes itimpossible to properly learn the polarity of manytokens.
Thus, we carry out a filtering step beforefeeding our classifier.
In particular, we rely onthe information gain (IG) method to then rank themost relevant features.
Information gain measuresthe relevance of an attribute with respect to a class.It takes values between 0 and 1, where a highervalue implies a higher relevance.
Table 1 showsthe top five relevant features based on their infor-mation gain for our best model.
The top featuresfor task A were very similar.
Our official runs onlyconsider features with an IG greater than zero.IG Feature Category0.140 positive emotion Pennebaker et al.
(2001)0.137 #positive-words Hu and Liu (2004)0.126 affect Pennebaker et al.
(2001)0.089 #negative-words Hu and Liu (2004)0.083 negative emotion Pennebaker et al.
(2001)Table 1: Most relevant features for task B.
?#?
mustbe read this table as ?the number of?and not as ahashtag.2.4 ClassifierWe have trained our runs with a SVMLibLINEARclassifier (Fan et al., 2008) taking the implementa-tion provided in WEKA (Hall et al., 2009).
Theselection was motivated by the acceptable resultsthat some of the participants in SemEval 2013, e.g.Becker et al.
(2013), obtained using this imple-mentation.
We configured the multi-class supportvector machine by Crammer and Singer (2002) asthe SVMtype.
Since the corpus was unbalanced,we tuned the weights for the classes using the de-velopment corpus: 1 for the positive class, 2 fornegative and 0.5 for neutral.
The rest of parame-ters were set to default values.3 Experimental ResultsThe SemEval 2014 organisation provides a stan-dard training corpus for both tasks A an B.
For taskA, each tweet is marked with a list of the wordsand phrases to analyse, and for each one its senti-ment label is provided.
In addition, a developmentcorpus was released for tuning the system parame-ters.
The training and the development corpus canbe used jointly (constrained runs) to train mod-els that are then evaluated over the test corpus.2Some participants used external annotated corpora(unconstrained runs) to build their models.
Withrespect to the test corpus, it contains texts fromtweets but also from LiveJournal texts, which weare abbreviating as LJ, and SMS messages.Table 2 contains the statistics of the corpora weused.
Sharing data is a violation of Twitter?s termsof service, so we had to download them.
Unfortu-nately, some of the tweets were no longer availablefor several reasons, e.g., user or a tweet does notexist anymore or the privacy settings of a user havechanged.
As a result, the size of our training anddevelopment corpora may be different from thoseof other participant?s corpora.Task Set Positive Negative NeutralTrain 4,917 2,591 385A Dev 555 365 45Test 6,354 3,771 556Train 3,063 1,202 3,935B Dev 493 290 633Test 3,506 1,541 3,940Table 2: SemEval 2014 corpus statistics.3.1 Evaluation MetricsF-measure is the official score to measure how sys-tems behave on each class.
In order to rank partic-ipants, the SemEval 2014 organisation proposedthe averaged F-measure of positive and negativetweets.2We followed this angle.4133.2 Performance on SetsTables 3 and 4 show performance on the test setof different combinations of the proposed features.Table 5 shows the performance of our run on taskA.
The results over the corresponding sets for taskB are illustrated in Table 6.
They are significantlower than in task A.
This suggests that when amessage involves more than one of two tokens, alexical approach is not enough.
Improving perfor-mance should involve taking into account contextand linguistic phenomena that appear in sentencesto build a model based on the composition of lin-guistic information.Model LJ SMS Twitter Twitter Twitter2013 2014 SarcasmWPLT 82.21 82.32 84.82 81.69 71.19(no IG)WPL 83.55 81.04 84.85 80.64 68.79WPLT* 83.96 81.46 85.63 79.93 71.98WP 78.53 80.97 80.34 73.35 74.18P 75.70 78.74 73.58 65.75 71.82W 61.58 65.45 64.56 59.16 62.93L 66.04 64.11 62.96 53.81 61.26T 47.07 51.37 71.82 43.64 49.37Table 3: Performance on the test set for task A.The model marked with a * was our official run.
Wstands for features obtained from a bag-of-wordsapproach, L from Hu and Liu (2004), P from Pen-nebaker et al.
(2001) and T for fine-grained PoS-tags.
They can be combined, e.g., a model namedWP use both words and psychometric properties.Model LJ SMS Twitter Twitter Twitter2013 2014 SarcasmWPLT* 69.79 60.45 66.92 64.92 42.40WPL 70.19 61.41 66.71 64.51 45.72WP 66.84 60.22 65.29 63.90 45.90WPLT 66.38 57.01 61.96 62.84 43.71(no IG)W 65.12 56.00 62.87 62.64 48.75P 63.42 54.80 60.05 57.66 54.20T 45.99 35.85 46.53 45.99 48.58L 57.53 45.14 48.80 44.48 49.14Table 4: Performance on the test set for task B.4 ConclusionsThis papers describes the participation of the LySResearch Group (http://www.grupolys.org) at the SemEval 2014 task 9: Sentiment Anal-ysis in Twitter, with a system that attained com-petitive performance both in message and phrase-Test set Positive Negative NeutralDEV 86.30 81.60 4.30TWITTER 2013 88.70 81.90 17.60(full)TWITTER 2013 88.81 82.57 20.75(progress subset)LJ 84.34 83.56 13.84SMS 80.31 82.56 7.10TWITTER 2014 89.02 70.82 4.44TWITTER SARCASM 85.71 57.63 28.57Table 5: Performance on different sets for ourmodel on task A.
The model evaluated on the de-velopment set was only built using the training set.Test set Positive Negative NeutralDEV 69.80 60.40 66.70TWITTER 2013 72.50 64.30 72.30(full)TWITTER 2013 71.92 61.92 71.22(progress subset)LJ 71.94 67.65 66.23SMS 63.83 57.06 73.76TWITTER 2014 74.26 55.58 66.76TWITTER SARCASM 55.17 29.63 51.61Table 6: Performance on different sets for ourmodel on task B.Test set Task A Task BLiveJournal 2014 4 / 27 13 / 50SMS 2013 12 / 27 19 / 50Twitter 2013 9 / 27 10 / 50Twitter 2014 11 / 27 18 / 50Twitter 2014 Sarcasm 10 / 27 33 / 50Table 7: Position of our submission on each cor-pus and task, according to results provided by theorganization on April 22, 2014.level tasks, as can be observed in Table 7.
Thissystem is a reduced version of a sentiment classifi-cation model for Spanish texts that performed wellin the TASS 2013 (Villena et al., 2013).
The offi-cial results show how our approach works com-petitively both on tasks A and B without needinglarge and automatically-built resources.
The ap-proach is based on a bag-of-words that includesword-forms and PoS-tags.
We also extract psy-chometric and sentiment information from exter-nal lexicons.
In order to reduce sparsity problems,we firstly apply an information gain filter to selectonly the most relevant features.
Experiments onthe development set showed a significant improve-ment on the same model with respect to skippingit on subtask B.414AcknowledgementsResearch reported in this paper has been partiallyfunded by Ministerio de Econom?
?a y Competitivi-dad and FEDER (Grant TIN2010-18552-C03-02)and by Xunta de Galicia (Grant CN2012/008).ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau.
2011.
Sentimentanalysis of Twitter data.
In Proceedings of the Work-shop on Languages in SocialMedia, LSM ?11, pages30?38, Stroudsburg, PA, USA.
ACL.Lee Becker, George Erhart, David Skiba, and Valen-tine Matula.
2013.
AVAYA: Sentiment Analysis onTwitter with Self-Training and Polarity Lexicon Ex-pansion.
Atlanta, Georgia, USA, page 333.Koby Crammer and Yoram Singer.
2002.
On the algo-rithmic implementation of multiclass kernel-basedvector machines.
The Journal of Machine LearningResearch, 2:265?292.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
The Journalof Machine Learning Research, 9:1871?1874.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explorations, 11(1):10?18, November.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.ACM.Manesh Joshi and Carolyn Penstein-Rose?.
2009.
Gen-eralizing dependency features for opinion mining.In Proceedings of the ACL-IJCNLP 2009 Confer-ence Short Papers, ACLShort ?09, pages 313?316,Suntec, Singapore.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 Task 2: Sentiment Analysis inTwitter.
pages 312?320, June.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.A.
Pak and P. Paroubek.
2010.
Twitter as a Corpus forSentiment Analysis and Opinion Mining.
In Pro-ceedings of the Seventh conference on InternationalLanguage Resources and Evaluation (LREC?10),pages 1320?1326, Valletta, Malta, May.
EuropeanLanguage Resources Association (ELRA).J.W.
Pennebaker, M.E.
Francis, and R.J. Booth.
2001.Linguistic inquiry and word count: LIWC 2001.Mahway: Lawrence Erlbaum Associates, 71.David Vilares, Miguel A. Alonso, and Carlos Go?mez-Rodr??guez.
2013a.
LyS at TASS 2013: AnalysingSpanish tweets by means of dependency pars-ing, semantic-oriented lexicons and psychometricword-properties.
In Alberto D?
?az Esteban, In?akiAlegr?
?a Loinaz, and Julio Villena Roma?n, editors,XXIX Congreso de la Sociedad Espan?ola de Proce-samiento de Lenguaje Natural (SEPLN 2013).
TASS2013 - Workshop on Sentiment Analysis at SEPLN2013, pages 179?186, Madrid, Spain, September.David Vilares, Miguel A. Alonso, and Carlos Go?mez-Rodr??guez.
2013b.
Supervised polarity classifica-tion of Spanish tweets based on linguistic knowl-edge.
In DocEng?13.
Proceedings of the 13th ACMSymposium on Document Engineering, pages 169?172, Florence, Italy, September.
ACM.David Vilares, Miguel A. Alonso, and Carlos Go?mez-Rodr??guez.
2014.
On the usefulness of lexicaland syntactic processing in polarity classification ofTwitter messages.
Journal of the Association for In-formation Science Science and Technology, to ap-pear.Julio Villena-Roma?n, Janine Garc?
?a-Morera, CristinaMoreno-Garc?
?a, Sara Lana-Serrano, and Jose?
CarlosGonza?lez-Cristo?bal.
2014.
TASS 2013 ?
a sec-ond step in reputation analysis in Spanish.
Proce-samiento del Lenguaje Natural, 52:37?44, March.415
