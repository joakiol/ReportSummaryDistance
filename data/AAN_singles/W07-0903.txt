Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 17?24,Prague, 28 June 2007. c?2007 Association for Computational LinguisticsRetrieving lost information from textual databases:rediscovering expeditions from an animal specimen databaseMarieke van ErpDept.
of Language and Information SciencesTilburg University, P.O.
Box 90153NL-5000 LE Tilburg, The NetherlandsM.G.J.vanErp@uvt.nlAbstractImporting large amounts of data intodatabases does not always go without theloss of important information.
In this work,methods are presented that aim to rediscoverthis information by inferring it from the in-formation that is available in the database.From and animal specimen database, theinformation to which expedition an ani-mal that was found belongs is rediscovered.While the work is in an early stage, the ob-tained results are promising, and prove thatit is possible to rediscover expedition infor-mation from the database.1 IntroductionDatabases made up of textual material tend to con-tain a wealth of information that remains unexploredwith simple keyword-based search.
Maintainers ofthe databases are often not aware of the possibilitiesoffered by text mining methods to discover hiddeninformation to enrich the basic data.
In this workseveral machine learning methods are explored toinvestigate whether ?hidden information?
can be ex-tracted from an animal specimen database belongingto the Dutch National Museum for Natural History,Naturalis1.
The database is a combination of infor-mation about objects in the museum collection fromhandwritten data sources in the museum, such asjournal-like entries that are kept by biologists whilecollecting animal or plant specimens on expedition1http://www.naturalis.nland tables that link the journal entries to the mu-seum register.
What is not preserved in the transitionfrom the written sources to the database is the nameof the expedition druing which an animal specimenwas found.By expedition, the following event is implied: agroup of biologists went on expedition together ina country during a certain time period.
Entries inthe database that belong to this expedition can becollected by one or a subset of the participating bi-ologists.
For researchers at the natural history mu-seum it would be helpful to have access to expedi-tion information in their database, as for biodiver-sity research they sometimes need overviews of ex-peditions.
It may also help further enrichment ofthe database and cleansing, because if the expedi-tion information is available, missing information incertain fields, such as the country where a specimenwas found, may be inferred from the information onother specimens found during the same expedition.Currently, if one wants to retrieve all objects fromthe database that belong to an expedition, one wouldhave to create a database query that contains the ex-act data boundaries of the expeditions and the namesof all collectors involved.
Either one of these bits ofinformation is not enough, as the same group of bi-ologists may have participated in an expedition morethan once, and the database may also contain expe-ditions that overlap in time.
In this paper a seriesof experiments is described to find a way to inferexpedition information from the information avail-able in the database.
To this end, three approachesare compared: supervised machine learning, unsu-pervised machine learning, and rule-based methods.17The obtained results vary, but prove that it is pos-sible to extract the expedition information from thedata at hand.2 Related WorkThe field of data mining, which is concerned withthe extraction of implicit, previously unknown andpotentially useful information from data (Frawley etal., 1992), is a branch of research that has becomequite important recently as every day the world isflooded with larger amounts of information that areimpossible to analyse manually.
Data mining can,for instance, help banks identify suspicious trans-actions among the millions of transactions that areexecuted daily (Fayyad and Uthurusamy, 1996), orautomatically classify protein sequences in genomedatabases (Mewes et al, 1999), or aid a companyin creating better customer profiles to present cus-tomers with personalised ads and notifications (Lin-den et al, 2003).
Knowledge discovery approachesoften rely on machine learning techniques as theseare particularly well suited to process large amountsof data to find similarities or dissimilarities betweeninstances (Mitchell, 1997).Traditionally, governments and companies havebeen interested in gaining more insight into theirdata by applying data mining techniques.
Only re-cently , digitisation of data in the cultural heritagedomain has taken off, which means that there hasnot been much work done on knowledge discoveryin this domain.
Databases in this domain are oftencreated and maintained manually and are thus of-ten significantly smaller than automatically gener-ated databases from, for example, customers?
pur-chase information in a large company.This means it is not clear whether data miningtechniques, aimed at analysing enormous amountsof data, will work for the data at hand.
This is in-vestigated here.
Manual data typically also containsmore spelling variations/errors and other inconsis-tencies than automatically generated databases, dueto different persons entering data into the database.Therefore, before one can start the actual process ofknowledge discovery, it is very important to care-fully select, clean and model the data one wantsto use in order to avoid using data that is toosparse (Chapman, 2003).
This applies in particularto databases that contain large amounts of textual in-formation, which are quite prevalent in the culturalheritage domain.
Examples of textual databases canbe found freely on the internet, such as the databasesof the Global Biodiversity Information Facility2, theUniversity of St. Andrews Photographic Collec-tion3, and the Internet Movie Database4.3 DataThe data that has been used in this experiment is ananimal specimen database from the Dutch NationalMuseum for Natural History.
The database currentlycontains 16,870 entries that each represent an objectstored in the museum?s reptiles and amphibians col-lection.
The entries provide a variety of informationabout the objects in 37 columns, such as the scien-tific name of the object, how the specimen is kept(in alcohol, stuffed, pinned) and under which regis-tration number, where it was found, by whom andunder which circumstances, the name of the personwho determined the species of the animal and thename of the person who first described the species.Most fields are rather compact; they only contain anumeric value or a textual value consisting of one orseveral words.
The database also contains fields ofwhich the entries consist of longer stretches of text,such as the ?special remarks?
field, describing any-thing about the object that did not fit in the otherdatabase fields and ?biotope?, describing the bioticand abiotic components of the habitat from whichthe object was collected.
Dutch is the most frequentlanguage in the database, followed by English.
Alsosome Portuguese and German entries occur.
Taxo-nomic values, i.e., the scientific names of the animalspecimens, are in a restricted type of Latin.
A snip-pet of the database can be found in Figure 1.3.1 Data ConstructionIn order to be able the measure the performance ofthe approaches used in the experiments, the databasewas annotated manually with expedition informa-tion.
Adding this information was possible becausethere was access to the original field books fromwhich the database is made up.
Annotating 81662http://www.gbif.org/3http://special.st-andrews.ac.uk/saspecial/4http://www.imdb.com/18Collector Coll.
Date Coll.
# Class Genus Species Country ExpeditionButtikofer, J.
30-07-1881 424 Reptilia Lamprolepis lineatus 132 buttikoferliberia1881Buttikofer, J.
& Sala 09-10-1881 504 Amphibia Bufo regularis 132 buttikoferliberia1881M.
Dachsel 02-05-1971 971-MSH186 Reptilia Blanus mettetali 156 mshbrazil71Hoogmoed, M.S.
04-05-1971 1971-MSH187 Reptilia Quendenfeldtia trachyblepharus 156 mshbrazil71Hoogmoed, M.S.
09-05-1971 1971-MSH202 Reptilia Lacerta hispanica 156 mshbrazil71C.
Schuil 14-03-1972 1972-MSH35 Amphibia Ptychadaena sp.
92 mshghana72P.
Lavelle -03-1972 1972-MSH40 Reptilia Crotaphopeltis hotamboeia 92 mshghana72Hoogmoed, M.S.
23-03-1972 1972-MSH55 Amphibia Phrynobatrachus plicatus 92 mshghana72Figure 1: Snippet of the animal specimen databaseentries with this information took one person about2 days.
There were 8704 entries to which no ex-pedition is assigned, either because these specimenswere not collected during an expedition or because itwas not possible to determine the expedition.
Theseentries were excluded from the experiments .
Ex-peditions which contained 10 or fewer entries werealso excluded because these would make the dataset too sparse.
A total of 7831 database entrieswere used in this work, divided into 60 expeditions.Although the ?smallest?
expeditions were excludedfrom the experiments, the sizes of the expeditionsstill vary greatly: between 2170 and 11 items (?
=310.04).
This is mainly due to the fact that newitems are still added to the database continuously, ina rather random order, hence some expeditions aremore completely represented than others.The database contains several fields that containinformation that will probably not be that useful forthis work.
Information that was excluded was thespecimen?s sex, the number of specimens (in caseswhere one database entry refers to several speci-mens, for instance kept together in a jar), how theanimal is preserved, and fields that contain informa-tion not on the specimen itself or how it was foundbut on the database (e.g., when the database entrywas added and by whom).
Values from the ?alti-tude?
and ?coordinates?
fields were also not includedin the experiments as this is information is too of-ten missing in the database to be of any use (altitudeinformation is missing in 85% of the entries and co-ordinates in 96%).Some information in the database is repetitive;there is for instance a field called ?country?
contain-ing the name of the country in which a specimen wasfound, but there is also a field called ?country-id?
inwhich the same information is encoded as a numer-ical value.
The latter is more often filled than the?country?
field, which also contains values in differ-ent languages, and thus it makes more sense to onlyinclude values from the ?country-id?
field.
A smallconversion is applied to rule out that an algorithmwill interpret the intervals between the different val-ues as a measure of geographical proximity betweenthe values, as the country values are chosen alpha-betically and do not encode geographical location.In some cases it seemed useful to have an al-gorithm employ interval relations between num-bers.
The fields ?registration number?
and ?collec-tion number?
were used as such.
These fields some-times contain some alphabetical values: certain col-lectors, for instance, included their initials in theirseries of collection registration numbers.
Thesewere converted to a numeric code to obtain com-pletely numeric values with preservation of the col-lector information.
This also goes for the fields inthe database that contain information on dates, i.e.,the ?date of determination?, the ?date the specimencame into the museum?
and the ?collection date?fields.
The collection date is the most importantdate here as this directly links to an expedition.
Theother dates might provide indirect information, forinstance if the collection date is missing (which isthe case in 14%).
To aid clustering, the dates werenormalised to a number, possibly the algorithm ben-efits from the fact that a small numerical intervalmeans that the dates are close together.Person names from the ?author?, ?collector?, ?de-terminer?, and ?donator?
fields were normalised toa ?first name - last name?
format.
From valuesfrom the taxonomic fields (?class?, ?order?, ?fam-ily?, ?genus?, ?species?, and ?sub species?
), and?town/village?
and ?province/state?
fields, as well asfrom the person name fields, capitals, umlauts, ac-cents and any other non-alphanumerical characterswere removed.It proved that certain database fields were not suit-able for inclusion in the experiments.
This goes for19the free text fields ?biotope?, ?location?
and ?specialremarks?.
Treating these values as they are will re-sult in data that is too sparse, as their values are ex-tremely varied.
Preliminary experiments to see ifit was possible to select only certain parts of thesefields did not yield any satisfying results and wastherefore abandoned.This resulted in feature vectors containing 18 fea-tures, plus the manually assigned expedition class.4 MethodologyThe majority of the experiments that were carriedout in an attempt to infer the expedition informa-tion from the database involved machine learning.Therefore in this section three algorithms for super-vised learning are described, followed by a cluster-ing algorithm for unsupervised learning.
This sec-tion is concluded with a description of the evaluationmetrics for clusters used by the different approaches.AlgorithmsThe first algorithm that was used is the k-NearestNeighbour algorithm (k-NN) (Aha et al, 1991;Cover and Hart, 1967; DeVijver and Kittler, 1982).This algorithm is an example of a lazy learner: itdoes not model the training data it is given, but sim-ply stores each instance of the training data in mem-ory.
During classification it compares the item itneeds to classify to each item in its memory andassigns the majority class of the closest k (in theseexperiments k=1) instances to the new item.
Todetermine which instances are closest, a variety ofdistance metrics can be applied.
In this experi-ment the standard settings in the TiMBL implemen-tation (Daelemans et al, 2004), developed at theILK research group at Tilburg University, were used.The standard distance metric in the TiMLB imple-mentation of k-NN is the Overlap metric, given inEquations1 and 2.
?
(X,Y) is the distance betweeninstances X and Y, represented by n features, where?
is the distance between the features.?
(X,Y ) =n?i=1?
(xi, yi) (1)where:?
(xi, yi) =????
?abs if numeric, else0 ifxi = yi1 ifxi 6= yi(2)The second algorithm that was used is the C4.5decision tree algorithm (Quinlan, 1986).
In thelearning phase, it creates a decision tree in a re-cursive top-down process in which the database ispartitioned according to the feature that separatesthe classes best; each node in the tree representsone partition.
Deeper nodes represent more class-homogeneous partitions.
During classification, C4.5traverses the tree in a deterministic top-down passuntil it meets a class-homogeneous end node, or anon-ending node when a feature-value test is notrepresented in the tree.Naive Bayes is the third algorithm that was usedin the experiments.
It computes the probabilityof a certain expedition, given the observed train-ing data according to the formula given in Equa-tion 3.
In this formula ?NB is the target expeditionvalue, chosen from the maximally probably hypoth-esis (argmax?jV P (?j), i.e., the expedition with the high-est probability) given the product of the probabilitiesof the features (Qi P (ai|?j)).
?NB =argmax?jV P (?j)?iP (ai|?j) (3)For both the C4.5 algorithm and Naive Bayes theWEKA machine learning environment (Witten andFrank, 2005), that was developed at the Universityof Waikato, New Zealand, was used.A quite different machine learning approach thatwas applied to try to identify expeditions in the rep-tiles and amphibians database is clustering.
Clus-tering methods are unsupervised, i.e., they do notrequire annotated data, and in some cases not eventhe number of expeditions that are in the data.
Itemsin the data set are grouped according to similarity.A maximum dissimilarity between the group mem-bers may be specified to steer the algorithm, butother than that it runs on its own.
For an exten-sive overview of clustering methods see Jain et al,(1999).
For this work, the options in choosing animplementation of a clustering algorithm were lim-ited because many data mining tools are designed20only for numerical data, therefore the WEKA ma-chine learning environment was also used for theclustering experiments.
As clustering is computa-tionally expensive, it was only possible to run ex-periments with WEKA?s implementation of the Ex-pectation Maximisation (EM) algorithm (Dempsteret al, 1977).
Preliminary experiments with other al-gorithms indicated execution times in the order ofmonths.
The EM algorithm iteratively tries to con-verge to a maximum likelihood by first computingan expectation of the likelihood of a certain cluster-ing, then maximising this likelihood by computingthe maximum likelihood estimates of the features.Termination of the algorithm occurs when the pre-defined number of iterations has been carried out,or when the overall likelihood (the measure of how?good?
a clustering is) does not increase significantlywith each iteration.Cluster EvaluationSince the data is annotated with expedition in-formation it was possible to use external qualitymeasures (Steinbach et al, 2000).
Three differ-ent evaluation measures were used: accuracy, en-tropy (Shannon, 1948), and the F-measure (van Ri-jsbergen, 1979).The evaluation of results for the supervised learn-ing algorithms was calculated in a straightforwardway: because the classifier knows which expedi-tions there are and which entries belong to whichexpedition, it checks the expeditions it assigned tothe database entries to the manually assigned expe-ditions and reports the overlap as accuracy.It gets a little bit more complicated with entropy.Entropy is a measure of informativity, i.e., the min-imum number of bits of information needed to en-code the classification of each instance.
If the ex-pedition clusters are uniform, i.e., all items in thecluster are very similar, the entropy will be low.
Themain problem with using entropy for evaluation ofclusters is that the best score (an entropy of 0) isreached when every cluster contains exactly on in-stance.
Entropy is calculated as follows: first, themain class distribution, i.e., per cluster the probabil-ity that a member of that cluster belongs to a certaincluster, is computed.
Using that distribution the en-tropy of each cluster is calculated via the formula inEquation 4.
For a set of clusters the total entropyis then computed via the formula in Equation 5, inwhich m is the total number of clusters, sy the sizeof cluster y and n the total number of instances.Ey = ?
?xPxylog(Pxy) (4)Etotal =m?y?1sy ?
Eyn(5)The F-measure is the harmonic mean of precisionand recall, and is commonly used in information re-trieval.
In information retrieval recall is the propor-tion of relevant documents retrieved out of the totalset of relevant documents.
When applied to clus-tering a ?relevant document?
is an instance that isassigned correctly to a certain expedition, the set ofall relevant documents is the set of all instances be-loning to that expedition.
Precision is the number ofrelevant documents retrieved from the total numberof documents.
So when applied to cluster evalua-tion this means the number of instances of an expe-dition that were retrieved from the total number ofinstances (Larsen and Aone, 1999).
This boils downto Equations 6 and 7 in which x stands for expedi-tion, y for cluster, nxy for the number of instancesbelonging to expedition x that were assigned to clus-ter y, and nx is the number of items in expedition x.Recall(x, y) =nxynx(6)Precision(x, y) =nxyny(7)The F-measure for a cluster y with respect to ex-pedition x is then computed via Equation 8.
TheF-measure of the entire set of clusters is computedthrough the function in Equation 9, which takes theweighted average of the maximum F-measure perexpedition.F (x, y) =2 ?
Recall(x, y) ?
Precision(x, y)Precision(x, y) +Recall(x, y)(8)F =?xnxnmax{F (x, y)} (9)215 Experiments and ResultsFirst, two baselines were set to illustrate the situationif no machine learning or other techniques would beapplied to the database.
if one were to randomlyassign one of the 60 expeditions t the entries thiswould go well in 1.7% of the cases.
If all entrieswere labelled as belonging to the largest expeditionthis would yield an accuracy of 28%.
In all ma-chine learning experiments 10-fold cross validationwas used for testing performance.A series of supervised machine learning experi-ments was carried out first to investigate whether itis possible to extract the expeditions during whichthe animal specimens were found at all.
Threelearning algorithms were applied to the completedata set, which yielded accuracies between 88%and 98%.
Feature selection experiments with theC4.5 decision tree algorithm indicated that fea-tures ?town/village?, ?collection number?, ?registra-tion number?, ?collector?
and ?collection date?
wereconsidered most informative for this task, hence theexperiments were repeated with a data set contain-ing only those features.
The results of both seriesof experiments are to be found in Table 1.
Forthe C4.5 and Naive Bayes experiments the accu-racy deteriorates significantly when using only theselected features (?
= 0.05, computing using McNe-mar?s test (McNemar, 1962)), but it stays stable forthe k-NN classifier.
This indicates that not all datais needed to infer the expeditions, but that it mat-ters greatly which approach is taken.
However, asneither of the algorithm benefits from it, feature se-lection was not further explored.Algorithm All feat.
Sel.
feat.k-NN 95.9% 95.9%C.4.5 98.3% 94.4%NaiveBayes 88.1% 73.5%Table 1: Accuracy of supervised machine learningexperiments using all features and selected featuresIn these experiments all database entries were an-notated with expedition information, which in a realsetting is of course not the case.
Through runninga series of experiments with significantly smalleramounts of training data it was found that by us-ing only as little as 5% of the training data (amount-ing to 392 instances) already an accuracy of 85% isreached.
Annotating this amount of data with expe-dition information would take on person less than anhour.
By only using 45% of the training data an ac-curacy of 97% is reached5.
In Figure 2 the completelearning curve of the k-NN classifier is shown.Annotating this amount of data with expedition in-formation would take one person less than an hour.By only using 45% of the training data an accuracyof 97% is reached5.
In Figure 2 the complete learningcurve of the k -NN classifier is shown.Fig.
2: Accuracy of k-NN per percentage of trainingdataIdeally, one does not want to annotate data at all,therefore the use of clustering algorithms was explored.For this, the EM algorithm from the WEKA MachineLearning environment was used.
The results, as shownin Table 2, are not quite satisfying, but still well abovethe set baselines.
As can be seen in Table 2, the clus-tering algorithms do not come up with anywhere nearas many clusters as needed and unfortunately WEKAdoes not present the user with many options to remedythis.
An intermediate experiment between completelysupervised and unsupervised was attempted, i.e., pre-specifying a number of clusters for the algorithm todefine, but this was computationally too expensive tocarry out.Algorithm # Clusters AccuracyEM 7 46.0%Table 2: Results of clustering experimentsSince clustering algorithms do not achieve an accu-racy that is satisfying enough to use in a real settingand supervised learning requires annotated data, alsoa traditional, and quite different approach was tried,namely finding expeditions via rules.
Via a coupleof simple rules the dataset was split into possible ex-peditions using only information on collection dates,collector information and country information.1.
Sort dates in ascending order, start a new ex-pedition when the distance between two dates isgreater than the average distance of the collectiondates2.
First sort collector information in ascending or-der, then sort collection dates in ascending order,start a new expedition when the distance betweentwo dates is greater than the average distance be-tween dates or when a new collector is encoun-tered5 The slightly higher achieved accuracy in the learning curveexperiments is due to the fact the learning curve was notcomputed via cross-validation3.
First sort by country information, then by collec-tor, and finally by collection dates, start a newexpedition when country or collector change, orwhen the distance between two dates is greaterthan the average distance between datesSurprisingly, only grouping collection dates alreadyyields an F-measure of .83, this includes 1299 en-tries that contain no information on the collectiondate, leaving this data out would increase precisionon the entries whose date is not missing to an F-measure of .94.
In Table 3 results of the rule-basedexperiments are shown.
It is expected that when thedatabase is further populated, the date-rule will workless well as there will be expeditions that overlap, thedate+collector-rule should remedy this, although itdoes not work very well yet as spelling variations inthe collector names are not taken into account.Rules # clusters Fmeasure entropy1 78 .83 .162 199 .75 .153 216 .73 .11Table 3: Results of rule-based experiments6 Conclusions and Future WorkIn this work we have presented various approachesto rediscover expedition information from an animalspecimen database.
As expected, the supervised learn-ing algorithms performed best, but the disadvantage inusing such an approach is the requirement to provideannotated data; however, a series of experiments togain more insight into the quantities of data necessaryfor a supervised approach to perform well indicate thatonly a small set of annotated data is required to ob-tain very reasonable accuracies.
If no training data isavailable, a rule-based approach is a more realistic al-ternative.
Although it must be kept in mind that rulesneed to be created manually for every new data set.For this data set relatively simple rules already provedto be quite effective, but for other data sets derivingrules can be much more complicated and thus moreexpensive.
This particular set of rules is also expectedto behave differently when the database is extendedwith entries from overlapping expeditions.For the experiments presented in this work, onlyentries from the database of which the expeditionthey belonged to was known were used, which con-stitutes only half of the database entries.
Researchersat the natural history museum estimate that about30% of the database entries do not belong to an ex-pedition, while the other 20% not included here be-long to unknown expeditions.
The decision to excludethe expedition-less entries was made as these entrieswould imbalance the data and impair evaluation as itwould not be possible to check predictions against a?real value?.
If all database entries would belong to aknown expedition the performance of the approachesdescribed in this paper show that satisfactory resultsshould be achieved over the complete data set.
Toprove this hypothesis one would need to test the ap-proaches on other data sets which can be completely5Figure 2: Accuracy of k-NN per percentage of train-ing dataIdeally, one does not want to annotate data at all,therefore the use of a clustering algorithm was ex-plored.
For this, the EM algorithm from the WEKAmachine learning environment was used.
The re-sult, as shown in Table 2, is not quite satisfying, butstill well above the set baselines.
As can be seen inTable 2, th clustering algorithm does not com upwith anywhere near as many clusters as needed andunfortunately WEKA does not present the user withmany options to remedy this.
An intermediate ex-periment between completely supervised and unsu-pervised machine learning was attempted, i.e., pre-specifying a number of clusters for the algorithm todefine, ut this was comp tatio ally too expensiveto carry out.Algorithm # Clusters AccuracyEM 7 46.0%Table 2: Result of clustering experimentSince the clus ering algorithm does not achieve anaccuracy that is satisfying enough to use in a realsetting and sup rvised lear ing requires annotateddata, also a traditional, and quite different approach5The slightly higher achieved accuracy in the learning curveexperiments is due to the fact that the learning curve was notcomputed via cross-validation22was tried: namely finding expeditions via rules.
Viaa couple of simple rules the data set was split intopossible expeditions using only information on col-lection dates, collector information and country in-formation.1.
Sort dates in ascending order, start a new expe-dition when the distance between two sequen-tial dates is greater than the average distance ofthe collection dates2.
First, sort collector information in ascendingorder, then sort collection dates in ascendingorder, start a new expeditions when the distancebetween two dates is greater than the averagedistance between dates or when a new collectoris encountered3.
First, sort by country information, then by col-lector, and finally by collection date, start a newexpedition when country or collectors change,or when the distance between two dates isgreater than the average distance between datesSurprisingly, only grouping collection dates al-ready yields an F-measure of .83.
This includes1299 entries that contain no information on the col-lection date, leaving those out would increase preci-sion on the entries whose collection date is not miss-ing to an F-measure of .94.
In Table 3 results ofthe rule-based experiments are shown.
It is expectedthat when the database is further populated the date-rule will work less well as there will be more expe-ditions that overlap.
The date+collector-rule shouldremedy this, although it does not work very well yetas spelling variations in the collector names are nottaken into account at the moment.Rules # Clusters F-measure Entropy1 78 .83 .162 199 .75 .153 216 .73 .11Table 3: Results of the rule-based experiments6 Conclusions and Future WorkIn this work various approaches were presented torediscover expedition information from an animalspecimen database.
As expected, the supervisedlearning algorithms performed best, but the disad-vantage in using such an approach is the require-ment to provide annotated data.
However, a seriesof experiments to gain more insight into the quanti-ties of data necessary for a supervised approach toperform well, indicate that only a small set of an-notated data is required in this case to obtain veryreasonable results.
If no training data is available,a rule-based approach is a realistic alternative.
Al-though it must be kept in mind that rules need to becreated manually for every new data set.
For thisdata set relatively simple rules already proved to bequite effective, but for other data sets deriving rulescan be much more complicated and thus more ex-pensive.
This particular set of rules is also expectedto behave differently when the database is extendedwith more entries from overlapping expeditions.For the experiments presented in this work, onlyentries from the database of which the expeditionthey belonged to was known were used, whichconstitutes only half of the database entries.
Re-searchers at Naturalis estimate that about 30% ofthe database entries do not belong to an expedi-tion, while the other 20% not included here belongto unknown expeditions.
The decision to excludethe expedition-less entries was made as these en-tries would imbalance the data and impair evalua-tion as it would not be possible to check predictionsagainst a ?real value?.
If all database entries wouldbelong to a known expedition the performance ofthe approaches described in this paper that satisfac-tory results could be achieved over the complete dataset.
To prove this hypothesis one would need totest the approaches on other data sets which can becompletely annotated.
Performing such tests mightprovide more insight into how well the approacheswould deal with a data set where all entries havean associated expedition.
The natural history mu-seum has several other similar (but smaller) datasets, which might be suitable for this task, and whichwill be tested as part of future work for evaluatingthe approaches described here.
It may also be inter-esting to investigate what can be inferred from theother fields defined in other data sets.A less satisfying aspect of the research describedin this paper is that many of the intended experi-ments with unsupervised machine learning were too23computationally expensive to be executed.
Potentialworkarounds to the limitation of certain implemen-tations of clustering algorithms, in that they onlywork on numeric data, are sought in converting thetextual data to numeric values and in the investi-gations into implementations of algorithms that candeal with textual data.A particular peculiarity of textual data, fromwhich the rule-based approach suffers, is the factthat the same name or meaning can be conveyed inseveral ways.
Spelling variations and errors werefor instance not normalised.
Hence the approachestreated ?Hoogmoed?
and ?M S Hoogmoed?
as twodifferent values whereas they may very well refer tothe same entity.From this work it can be concluded that the ex-pedition information can definitely be reconstructedfrom the animal specimen database that was usedhere, but for it to be used in a real world applica-tion it still needs to be tested and fine-tuned on otherdata sets and extended to be able to deal with entriesthat are not associated with any expedition.AcknowledgmentsThe research reported in this paper was funded byNWO, the Netherlands Organisation of ScientificResearch as part of the CATCH programme.
Theauthor would like to thank the anonymous reviewers,and Antal van den Bosch and Caroline Sporleder fortheir helpful suggestions and comments.ReferencesDavid W. Aha, Dennis Kibler, and Mark K. Albert.
1991.Instance-based learning algorithms.
Machine Learn-ing, 6:37?66.Arthur D. Chapman.
2003.
Notes on Environmen-tal Data Quality-b.
Data Cleaning Tools.
Internal re-port, Centro de Refere?ncia em Informac?a?o Ambiental(CRIA).T.
M. Cover and P. E. Hart.
1967.
Nearest neighborpattern classification.
Institute of Electrical and Elec-tronics Engineers Transactions on Information The-ory, 13:21?27.Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, andAntal Van den Bosch.
2004.
Timbl: Tilburg memorybased learner, version 5.1, reference guide.
TechnicalReport 04-02, ILK/Tilburg University.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data using theem algorithm.
Journal of the Royal Statistical Society.Series B (Methodology), 39(1):1?38.P.
A. DeVijver and J. Kittler.
1982.
Pattern recognition.A statistical approach.
Prentice-Hall, London.U.
Fayyad and R. Uthurusamy.
1996.
Data mining andknowledge discovery in databases.
Communicationsof the ACM, 39(11):24?26.William J. Frawley, Gregory Piatetsky-Shapiro, andChristopher J. Matheus.
1992.
Knowledge discoveryin databases: An overview.
AI Magazine, 13:57?70.A.
K. Jain, M. N. Murty, and P. J. Flynn.
1999.Data clustering: A review.
ACM Computing Surveys,31(3):264?323, September.Bjorner Larsen and Chinatsu Aone.
1999.
Fast and effec-tive text mining using linear-time document clustering.In Proceedings of KDD-99, San Diego, CA.G.
Linden, B. Smith, and J. York.
2003.
Amazon.comrecommendations: item-to-item collaborative filtering.IEEE Internet Computing, 7(1):76?80, Jan/Feb.Q.
McNemar.
1962.
Psychological Statistics.
Wiley,New York.H.
W. Mewes, K. Heumann, A. Kaps, K. Mayer, F. Pfeif-fer, S. Stocker, and D. Frishman.
1999.
Mips: adatabase for genomes and protein sequences.
NucleicAcids Research, 27(1):44?48.Tom M. Mitchell.
1997.
Machine Learning.
McGraw-Hill.J.
R. Quinlan.
1986.
Induction of decision trees.
Ma-chine Learning, 1:81?106.Claude E. Shannon.
1948.
A mathematical theoryof communication.
Bell System Technical Journal,27:379?423, July.Michael Steinbach, George Karypis, and Vipin Kumar.2000.
A comparison of document clustering tech-niques.
Technical report, Department of ComputerScience, University of Minnesota.Cornelis Joost van Rijsbergen.
1979.
Information Re-trieval.
Buttersworth.Ian H.Witten and Eibe Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
MorganKaufmann, San Francisco, 2nd edition.24
