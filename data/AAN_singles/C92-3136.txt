ARGUING ABOUT PLANNING ALTERNATIVESALEX QUILICIDepar tment  of E lec t r i ca l  Eng ineer ing2540 Dole Street ,  Ho lmes  Hal l  455Univers i ty  of Hawai i  at MaaoaHonolu lu,  HI ,  96822AbstractIn discourse processing, two major problems are un-derstanding the underlying connections between suc-ce~ive dialog utterances and deciding on the contentof a coherent dialog response.
Thin paper presents acomputational model of these tasks for a restrictedclass of argumentative dialogs.
In these dialogs, eachresponse presents a belief that justifies or contradictsanother belief presented or inferred earlier in the di-alog.
Understanding a response involves relating astated belief to these earlier beliefs, and producing aresponse involves electing a belief to justify and de-ciding upon the set of beliefs to provide as its justifi-cation.
Our approach is knowledge baaed, using gen-eral, common-sense justification rules to recognizehow a belief in being justified and to form new justifi-cations for beliefs.
This approach provides the abilityto recognize and respond to never before seen beliefjustifications, a necessary capability for any systemthat participates indialogs involving disagreements.1 In t roduct ionIn discourse processing, two major problems are un-derstanding the underlying connections between suc-cessive dialog responses and deciding on the contentof a coherent dialog response.
This paper presentsan initial model that accomplinhes these tasks for oneclass of argumentative dialogs.
In this class, each di-alog respouse presents a belief that justifies or con-tradicts a belief provided earlier in the dialog.The following dialog fragment is an example:(1) TIDY: The members of the AI lab should cleanit themselves.
(2) ScguPPY: But that interferes with doing re-search.
(3) TIDY: There's no other way to keep it clean.
(4) SCRUFf'Y: We can pay a janitor to keep itclean.
(5) TIDY: We need money to pay a janitor.
(6) SCRUFFY: We can transfer the money fromthe salary fund.
(7) TIDY: But doing that interferes with payingthe lab members.
(8) SCRUVFY: It's more desirable to have a cleanlab than to pay the lab members.Each response states one or more plan-oriented be-liefs, usually as part of a short chain of reeanningjustifying or contradicting a belief provided earlierin the dialog.In (1), TIDY begins by stating a belief: the labmembers hould execute the plan of cleaning the lab.In (2), SCRUFFY responds with a belief that thelab members executing this plan interferes with theirdoing research.
This belief justifies SCRUFFY~s un-stated belief that the lab members hould not exe-cute the plan of cleaning the lab, which contradictsTIvY's stated belief in (1).
SCRUFPY's underlyingreasoning is that the lab members houldn't cleanthe lab because it interferes with their executing themore desirable plan of doing research.In (3), TIDY presents belief that there's no al-ternative plan for keeping the lab clean.
This beliefjustifies TIDY's belief in (1).
TIDY's underlying rea-soning is that the lab members hould clean the labbecause it's the best plan for the goal of keeping thelab clean, and it's the best plan because it's the onlyplan that achieves the goal.Finally, in (4), Scs.uFta'y states a belief that pay-ing a janitor achieves the goal of keeping the labclean.
This contradicts TIDY's stated belief in (3).It also justifies a belief that the lab members clean-ing the lab isn't the best plan for keeping the labclean, which contradict~ one of the beliefs inferredfrom (3).
SCRUFFY's reasoning is that paying a jan-itor is a more desirable plan that achieves thin goal.The remaining responses follow the same pattern.Understanding responses like these involves relatinga stated belief to beliefs appearing earlier in the di-alog.
That requires inferring the participant's un-derlying reasoning chain and the beliefs it justifies.Producing these responses involves electing a beliefto justify and deciding upon the set of beliefs to pro-vide as its justification.
That requires constructingan appropriate reasoning chain that justifies holdingany unshared beliefs.Our focus in this paper is on an initial methodfor representing, recognising, and producing the be-lief justifications underlying dialog responses thatprovide coherent defenses of why beliefs are held.ACRES DE COLING-92, NANTES, 23-28 AOl~'r 1992 9 0 6 PROC.
OF COLING-92, NANTES, AUG. 23-28.
1992The behavior modeled it limited in several signifi-cant ways.
FirJt, we do not try to recognite whenan trguer's response contradicts one of his earlier e-sponses, such as the contradiction between (2) and(8), nor do we try to avoid producing such responses.Second, we do not try to recngnise or make useof high-level arguing strategies, uch as reductio edab*urdum.
Third, we restrict ourselves to a smallclam of beliefs involving planning.
Finally, we startwith representatin~ of beliefs and ignore the linguis-tic issues involved in turning responses into be\]ida.Clearly, all these limitations must eventually be exi-dressed in order to produce a more realistic model ofdebate.
Our belief, however, it that an initial modelof the process of rccognising and producing beliefjustifications i a useful and necessary first step.2 Our ApproachOur approach to these tasks rests on a simple as-sumption: Dialog participants jusLif~ beliefs with in-stantialions of general, common-sense justificationra/es.
For plan-oriented beliefs, a justification rulecorresponds to a planning heuristic that's basedsolely on structural features of plans in general, noton characteristics of specifc plans themselves.The first few responses in this dialog illustrate sev-eral justification rules.
In (2), SCRUI~F'?
uses therule: O~e re.on wh~ a plan shouldn'~ be ezecuted isthat it conflicts with assenting a more desirable plan.Similarly, in (3), TXDY chains together a pair of theserules: One reason why a plan should be ezecuted isthat it's the be,t plan/or achieving a goal, and Onereason why a plan il the be,t plan for a goal is thatif'# the onl~ plan that achieves the goal.Given our assumption, understanding a responseit equivalent to recogniting which justification ruleswere chained together and instantiated to form it,determining which belief to address in a response itequivalent to determining which beliefs in a chain ofinstantiated justification rules axe not shared, andproducing a justification is equivalent o selectingand instantiating justification rules with beliefs frommemory.We make this assumption for two reasons.
First,dialog participants hould be able to understand andrespond to never before seen belief justifications.That suggests applying general knowledge, such asour jtmtification rules, to analyse and produce spe-cific juJtifications, as that knowledge is likely to beshared by different participants, even if they hold dif-ferent beliefs about specific courses of action.
Andsecond, dialog parlieipants should abo be able io usethe same knowledge for different foJks.
That sug-gests that arguments about planning should use theMsne knowledge as planning itsel?
The justifies-tion rules for plan-oriented belief1 describe knowl-edge that a planner would aim find nsdul in welectlngor constructing new plans.Our approach diffem in two ways fzom previonsmodeh of participating in dialogs.
First, the*?
mod-els emphe~ised plan recognition: the task of recog-nising and inferring the underlying plans and goalJof a dialog paxtlcipant \[4, 10, 17, 18, 2\].
They viewutternnces as providing steps in plans (typically bydescribing oals or actions) and tie them togetherby inferring an underlying plan.
But in an argumentnot only must the participant's plans and goals be in-ferred, but alto their underlying belie/s about thoseplans and goals.
Our approach suggests a model thatinfers these beliefs as a natural consequence of tryingto understand connections between successive diMogutterances.
In contrast, existing approaches to in-ferring participant beliefs take a stated belief andtry to reason about possible justifications for it \[12,9\].
Previous models have also tended to view provid-ing a dialog response solely as a part of the questionanswering process.
In contrast, our approach sug-gests that responses arise as a natural consequenceof trying to integrate newly-encountered beliefs withcurrent beliefs in memory, and trying to understandany contradictions that result.3 Justif ication RulesThe argumentative dialogs we've examined have twotypes of plan-oriented beliefs: facts61 and evalus-flee \[1\].
Factual beliefs are objective judgementsabout planning relationships, uch as whether a planhas a particular effect or enablement.
They repre-sent the planning knowledge held by moat previousplan-understanding and plan-constructing systems.Evaluative beliefs, on the other hand, are subjec-tive judgements about planning relationJhipe, suchas whether or not a plan should be executed.
Al-though these beliefs have generally been ignored byprevious systems, they are crucial to participating inarguments involving plan-oriented beliefs.Our assumption is there exists a small set of jus-tification rules for each planning relationship.
Eachrule is represented as an abstract configuration ofplanning relationships that, when instantiated, pro-vides a reason for holding a particular belief.
Forexample, the rule that a plan shouldn't be executedif it conflicts with a preferred plan is represented as:IF interforso(occtur(P) .occtn'(P')) tlIDfavoxa(occu~r(P'),occ~(p))THEN ought (not (occu.~ OF)))That is, a plan shouldn't be executed if (1) it inter-fereB with another plan, and (2) that plan is preferredto it.
Figure 1 lists our current justification rules forACRES DE COLING-92.
NANTES.
23-28 AO~r 1992 9 0 7 I~ROC.
OV COLING-92.
NANTES.
AUG. 23-28.
1992\]~tee~'ot~ why execuginl~ plan X/n desirable:X iJ the be~t plt~ for g g0al.Executing X h aa enablemeat for n goal._Re.
a spas why execntinl~ plan X.IS undesirable:X conflicts with a more desirable plan.X has an uadefirable ffect.X h~ an undefirable nablemeat.Remtoua why plan X iJ the best plan for n ~oa\]:X hi the only plaza that achiev~ the goal.No plan more desirable than X achieves the goal.Re~oas why plan X is not the best plan for a goal:X hat an unachievable ensblement.X's execution is undesirable.Some more desirable plan schieve~ the goal.Rettsons why plan X is more desirable than plan Y:X heat a desirable ffect that Y doesn't have.X doesn't have an undesirable effect that Y h~.X doesn't have an undesirable enablement that Y has.Y conflicts with a more desirable plan and X doesn't.X i* an enablement of a mote desirable plan than Y.X has an effect more des~nble than Y.Re~ons why achieving oal G is undesirable:The only plan for achieving G in undesirable.Achieving G has an undesirable effect S.Reasons why achieving oal G i~ desirable:Achieving G in an enablement for another goal.Not achieving G has an undesirable effect S.Figure 1: Justification rules.evaluative beliefs (~ee \[13\] tbr representational de-tails and criteria for dedding what is a reasonablejustification rule).
These rul?~ were abstracted fromexamining a variety of different plan-oriented argu-mentative dialogs.The power of these justification rules comes fromtheir generality: A single rule can be instantiated indifferent ways to provide justifications for differentbeliefs.
In (2), SCRUFFY USes the above rule to jus-tify a belief that the lab members houldn't cleanthe lab themselves.
In (7), TIDY uses the same ruleto justify a belief that the lab members shouldn'ttransfer money front the salary fnnd.
Here, TIDY'sjustification is that tranderring the money interfereswith the more desirable plan of paying researchers.4 Recognizing JustificationsThe proee~ of understanding a dialog response ismodeled as a forwar&chaining search for a chain ofinstantiated justification rules that (1) contains theuser~s tated belief, and (2) jastifies an earlier dialogbelief or its negation.We briefly illustrate this proce~ by showing howSCRUt'FY understands TIDe's response in (3).
Theinput belief is that the lab members denning the labis the only plan that achieves the goal of keeping thelab clean.
This belief matches an antecedent in apair of justification rules, so the process begins byinetantiating these rules, resulting in pair of possiblejustification chains that contain TIDY's stated belief:(1) the lab members cleaning is the beef plast for ~ep-lag the lab clean becalst it's the only pianist keepingthe lab clean, and (2) the lab shonldntl ~ kept c/cartbecause the only plan for that goal is the wades~bleplan of having the lab members cleaning iLNeither justification directly relates to the dialog,so the next step is to determine which one to pursuefurther, and whether either can be el iminated fromfurther consideration.
Here, the second justif icationcontains a belief that the lab members  cleaning thelab is undesirable, which contradicts TIDY's statedbelief in (1).
Applying the heuristic "D/aeard anypotential justification containing beliefs that contra-dict the speaker's earlier beliefs" leaves only the firstjustification to pursue further.
It 's consequent in theantecedent of a single justification rule, and instan-tinting tiffs rule leads to this justification chain: thelab members should clean the lab because their elear~.lag the lab is the best plan for the goal of keeping thelab clear* because it's the only plan for keeping tlAe labclean.
The justified belief is TIDY's belief in (1), sothe process stops.In general, the understanding proceu it more com-plex, since justification rules may not be completelyinstantiated by a single antecedent, and may there-fore need to be further iastantiated from beliefs inthe dialog context and memory.
There ahm may bemany possible chains to pursue even e~ter heuristi-cally discarding some of them, requiring the tree ofother heuristics to determine which path to follow,such as "Pursue the reasoning chain whidt eoltainsthe most beliefs found in the dialog eontea~.5 Selecting A Belief To JustifyAfter recognizing a participant's reasoning chain, it 'snecessary to select a belief to justify as a response.This task involves determining which beliefs are notshared, and selecting the negation of one of  tho~beliefs to justify.An intuitive notion of agreement is that  a beliefis shared if it it's found in memory or can be justi-fied, and it's not shared if its negation it found inmemory or can be justified.
But this notion is com-putationally expensive, since it could conceivably in.volvo trying to justify all the beliefs in the lmrtie-ipant'a reasoning chain, as well as their negatinas.As ml alternative, our model determines whether abelief is shared by searching memory for the beliefand its negation and, if that fails, applying a smallAcrl~s DE COLING-gZ NarcH~s, 23-28 Ao(rf 1992 9 0 8 PROC.
OF COLING-92.
NANTES.
AUG. 23-28, 1992set of agreement heuristics.
One such heuristic is"Assume a belief is sassed if a justil~ling geaera//za-lion is found in tattooer.
So, for exanlpie, if thebelief "keep everything clean" is found in memory,the belief *keep the AI lab clean ~ is considered to heshared.
If no agreement heuristic applies, the beliefis simply marked as Uunknown".After determining whether each belief in the par-ticipant's reasoning chain is shared, the model firstsearches for an existing justification for an unsharedbelief's negation.
If that fails, it then tries to createa new justification for an unshared belief's negation.And if that fails, it tries to create a new justifica-tion for the negation of one of the unknown beliefs.This way existing justifications are presented beforean attempt is made to construct new ones.
If none ofthese steps succeed, the assumption is that the rea-Boning chain is shared, and an attempt is made toform a new justification for the belief it contradicts.Thus, the belief our model addresses in a responsearises from trying to discover whether or not it agreeswith another participant's reasoning.6 Forming Justi f icationsTo form a new justification for a belief, our modelperforms a backward chaining search fo~" a chain ofjustification rules that justify the given belief andthat can be iustantiated with beliefs from memory.We briefly illustrate this process by showing howSCRU~'Fy forms the response in (2).
The belief tojustify is that it's not desirable to have the lab mem-bers clean the lab.
The first step is to instantiate thejustification rules that have this belief as their conse-quent.
That results in several possible justifications:(1) there's an undesirable nablement of cleaning thelab, (2) there's an undesirable effecf of cleaning thelab, or (3) the lab members cleaning the lab conflictswith a more desirable action.The next step is to try to fully iastantiate one ofthese rules.
Applying the heuristic "Pursue the mostinstantiafed justification rule" suggests working onthe last rule.
Here, SCRUFFY instantiates it with abelief from memory that research is more desirablethan cleaning.
Once a rule is instantiated, it's neces-sary to verify that the beliefs it contains are shared.Here, that involves verifying that cleaning conflictswith research.
It does, so the instantiated rule canbe presented an the response.In general, the process is more complex than out-lined here, since not all of the belief in an iustantiatedjustification rule may be shared, and there may beseveral ways to instantiate a particular ule.
Thoserules containing unknown beliefs require further jus-tification, while those rules containing unshared be~l ids can be discarded.7 BackgroundThe closest related system is ABDUL/ILANA \[8\],which debated the responsibility mad cause for hlstot-ical events.
It focused on the complementary prob-lem of recogniling and providing episodic justifi?~tions, rather than justifications b~ed on the rel~.tionships between different plans.There are several models for recognising the r?-lationship between argument propositions.
Cobea's\[5\] taken each new belief and checks it for a justifi-cation relationship with a subset of the previnusly-stated belief~ determined through the use of dipsing structure and clue words.
That modeltureen the existence of an evidence oracle capableof determining whether a justification relationshipholds between may pair of beliefs.
Our model ira.plements this oracle for a particular clam of plan-oriented belief justifications.
OpFkt \[3\] recogniset bo.lief justifications in editorials almut economic plan-ning through the use of argument units, a knowbedge structure that can be viewed as complex cowfigurations of justification rules.
The approachesare complementary, just as scripts \[7\] and plans \[6,18\] are both useful methods for recognising the camnections between events in a narrative.Several systems have concentrated on producingbelief justifications.
Our own earlier work \[14, 15,16\] used a primitive form of j~tstification rules forfactual beliefs as a template for producing corre~tive responses for user misconceptions.
Our currentmodel extends this work to use these rules in bothunderstanding and responding, and provides addi-tional rules for evaluative beliefs.ROMPER \[11\] providas justifications for belidkabout an object's class or attributes.
But it profidesthese justifications purely by template matching, notby constructing more general reasoning chains.8 Current  StatusWe've completely implemented the model di~umedin this paper.
The program is written in Quintu~Prolog and runs on an l IP /APOLLO workstation.Its input is a representation for a stated participantbelief, and its output is a representation for m, up.propriate response.
It currently includes 30 justitka~tips rules and over 400 beliefs about various plans.We've used the program to participate in short ar-.gumentative dialogs in two disparate domains: day-to~day planning in the A!
lab, and removing andrecovering files in UNIX.
We're currently using it toexperiment with different heuristics for controllingthe search process involved in rer.ognisbtg and c~u.-strutting these reasoning ch~in~.Our xxmdcl he~ eevt.L'~l /~ey Ib~dt~tion~ ~e e~e c,~_dyAcrEs DE COLING-92, NAN1q~, 23-28 hofrr 1992 9 0 9 P toc t  1: COLING.9"~ ~qt, l,~'n!s, Aut;.
23-28, 1992now starting to addrem.
First, it views plans asatomic units and comiders only a small set of "all ornothing" plan-oriented beliefs.
This means it can'tproduce or understand justifications involving atel~in a plan, conditional planning relationships, or be-liefs not directly involving plans.
Second, our modelcan understand only those responses that jnstify anearlier belief.
It can't, for example, understand a re-sponse that contradicts an inferred justification foran earlier belief.
These more complex relationshipscan be represented using juetificntinn rules, but ourmodel must be extended to recognise them.
Third,our model is reactive rather than initiatory: it pro-duces respon~ only when there's n perceived in-.agreement.
It needs to be extended to know why itsin an argument, and to be aware of the underlyinggoals of the other argument participants.9 Conclus ionsPrevious dialog models have focused primarily onrecognising a participant's plans and goals.
But toparticipate in an argument i 's also necessary to rec-ognize when participants are providing beliefs abouttheir planl and goals and how they're justifying thesebeliefs.
It's also necessary to be able to determinewhich beliefs require further justification and to for-mulate justifications for these beliefs.
This papersuggests a knowledge-based approach for these tasks.Our approach has several attractive features.Firs L it builds It model of many relevant but un-stated participant beliefs as a side-effect of tryingto relate their utterance to the dialog.
Second, itdecides which belief to address in n response as anatural consequence of trying to understand why itdisagrees with another participant's belief.
Third, itunderstands belief jnstifieations using the same gen-eral, common-sense planning knowledge that it usesto formulate them.
Finally, it suggests how never be-fore seen belief justiflcatinns can be understood, solong as they were formed from general justificationrules known to the participants.
That ability is cru-cial for participating in dialogs whose participantshold differing beliefs.References\[1\] R. Abelson.
Differences Between Beliefs andKnowledge Systems.
Cognitive Science 3, 1979.\[2\] J.F.
Allen and C.R.
Perranlt, Analysing Inten-tion in Utterances.
Artificial Intelligence 15, 1980.\[3\] S.J.
Alvarado.
Understanding Editorial Tezt:A Computer Model of A~umsnt Comprehension.Kluwer, Boston, MA, 1990.\[4\] S. Carberry.
Modeling the Uxr ' -  Plans andGoals.
Computational Li~q~isties, 14(3), 1988.\[5\] R. Cohen.
Analysing the Structure of Argu-mentstive Discourse.
Compwfational Linlmistiosj13(1), 1987.\[6\] M.G.
Dyer.
ln,-depCt U~derrlanding: A Corn-pater Model of Narrative Comprehension.
MITPress, Cambridge, MAj 1983.\[7\] M.G.
Dyer, R.E.
Cullingford, and S.5.
Alvare~lo.Script~.
In Encyclopedia of Artificial InteSigeuce,John Wiley, NY, NY, 1987.\[8\] M. Flowers, R. McGuire, and L. Birnbanm.
Ad-versary Arguments and the Logic of Personal At-tacks.
In Strategies/or Natural Language Procen-ing.
Lawrence Erlbaum, Hill~iale, NJ, 1982.\[9\] R. Kaas.
Building n User Model Implicitly from aCooperative Advisory Dialog.
User Modeling andUser-Adapted Inieraclion~ 1(3), 1991.\[19\] D.J.
Litman and J.F.
Allen.
A Plan RecognitionModel for Subdialoguea in Conversatinns.
C0gn/-live Science, 11, 1987.\[11\] K. McCoy.
Reasoning on n Highlighted UserModel to Respond to Misconceptions.
Comp~la-tional Linguistics, 14(3), 1988.\[12\] M. Pollack.
Inferring Domain Plans iBQuestion-Answering.
PhD Thesis, University ofPennsylvania, Philadelphia, PA, 1986.\[13\] A. Qnilici.
The Csrre~ion Machine: A Com-puter Model Of Rscogniu'ng and Producing BeliefJnstifications in Argumentative Dialogs.
PhD The-sis, University of California, LA, CA, 1991.
{14\] A. Quilici.
Participating In Plan-Oriented Di-alogs.
In Proceedings o/tAe l~h Conference of titsCognitive Science 5ocietj, Boston, MA, 1990.\[15\] A. Qnilici.
The Correction Machine: Formulat-ing Explanations for User Misconceptions.
In Pro-ceedings of ~he 1989 International Joint Confer-ence on Artificial Intelligence, Detroit, MI, 1989.\[16\] A. Quilici, M.G.
Dyer~ and M. FlowerL Recog-nising and Responding to Plan-oriented Miscon-ceptions.
Computational Linguistics, 14(3), 1988.\[17\] C. L. Sidner.
Plan Parsing For Intended Re-sponse Recognition in Discoume.
CompugatiomalIntelligence, 1(1), 1985.\[18\] IL Wileneky.
Planning and Understanding.
Ad-dison Wesley, Reading, MA, 1983.Ac'r~ DE COLING-92.
NANTES, 23-28 AOfrr 1992 9 1 0 PROC.
OF COLING-92, NANI'ES.
AUG. 23-28.
1992
