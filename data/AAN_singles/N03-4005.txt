A Spoken Dialogue Interface to a Geologist?s Field AssistantJohn Dowding and James HieronymusMail Stop T-27A-2NASA Ames Research CenterMoffett Field, CA 94035-1000{jdowding,jimh}@riacs.eduAbstractWe will demonstrate a spoken dialogue inter-face to a Geologist?s Field Assistant that isbeing developed as part of NASA?s MobileAgents project.
The assistant consists of arobot and an agent system which helps an as-tronaut wearing a planetary space suit whileconducting a geological exploration.
The pri-mary technical challanges relating to spokendialogue systems that arise in this project arespeech recognition in noise, open-microphone,and recording voice annotations.
This systemis capable of discriminating between speech in-tended for the system and for other purposes.1 IntroductionThe Geologist?s Field Assistant is one component of Mo-bile Agents, a NASA project studying technologies, tech-niques, and work practices for sophisticated human-agentand human-robot cooperation in space and planetary ex-ploration environments, such as the surface of Mars.
Theevolution, development and evaluation of this project oc-curs in a series of increasingly complex field tests in Marsanalog environments (deserts and artic sites) on Earth.The Spoken Dialogue component assists an astronautwearing a space suit while conducting a geological ex-ploration, by tagging samples by spoken discriptions,commanding the taking of pictures, recording descriptivevoice annotations, and tracking the associations betweenthese samples, images, and annotations.
The assistantwill also help track the astronaut?s location and progressthough the survey, and help track their body exertion level(heart and respiration rate).The Spoken Dialogue interface is one of several of Mo-bile Agents, each with different goals and evaluation met-rics.
Other components include: Brahms work-practicemodelling and simulation system (NASA Ames); MEXmobile wireless networking (Ames); robots (JohnsonSpace Center (JSC) and Georgia Tech); Spacesuits (JSC)Start tracking my g p s coordinates.Start logging my bio sensors every fifteen seconds.Where is my my current location?Call this location Asparagus.Create a new sample bag and label it sample bag three.Take a voice notePlease begin recording voice note now:This sample originated in a dry creek bed.
[pause]Would you like to continue recording the voice note?noVoice note has been created.Associate that voice note with sample bag three.Play the voice note associated with sample bag three.Table 1: Example utterancesand Biomedical sensors (Stanford University); SatelliteInternet services (Goddard Space Flight Center); and Ge-ologists (US Geological Survey).The primary technical challanges relating to spokendialogue systems that arise in this project are open-microphone speech recognition and understanding whichdecides which agent receives, and responds to a particularutterance and space suit noise.2 Example DialogueThe language capabilities developed so far are largely di-rect commanding with the user controlling task initiative.A sample of user commands is given in Table 1.
A systemresponse is always given, but is usually omitted below forthe sake of brevity.
When given, the system response ap-pears in italics.3 ArchitectureThis spoken dialogue system shares a common architec-ture with several prior systems: CommandTalk (Stent etal., 1999), PSA (Rayner et al, 2000), WITAS (Lemon etal., 2001), and the Intelligent Procedure Assistant (Aistet al, 2002).
The architecure has been well described inEdmonton, May-June 2003Demonstrations , pp.
9-10Proceedings of HLT-NAACL 2003prior work.
The critical feature of the architecture rel-evant to this work is the use of a grammar-based lan-guage model for speech recognition that is automaticallyderived from the same Unification Grammar that is usedfor parsing and interpretation.4 Data CollectionThe Mobile Agents project conducted two field tests in2002: a one week dress rehearsal at JSC in the Mars yardin May, and a two week field test in the Arizona desertin September, split between two sites of geological inter-est, one near the Petrified Forest National Park, and theother on the ejecta field at Meteor Crater.
We collectedapproxmimately 5,000 recorded sound files from 8 sub-jects during the September tests, some from space-suitsubjects, and the rest in shirt-sleeve walk-throughs (stilla high wind condition).
We transcribed 1059 wave files.All conditions were performed open-mic and all soundsthat were picked up by the microphone were recorded, sonot all of these files contained within-domain utterancesintended.
Of the transcribed sound files, 208 containedno speech (mostly wind noise) and 243 contained out-of-domain speech that was intended for other hearers.
Thatleft 608 within-domain utterances that were split 80%-20% into test and training utterances.5 Technical ChallangesThe Geologist?s Field Assitant requires the ability tomake voice notes that can be stored and transmitted.We implemented this by adding a recording mode tothe speech recognizer agent, and temporarily increas-ing the speech end-pointing interval.
This allows us torecord multi-sentence voice notes without treating inter-sentence pauses as end-of-voice-note markers.
Enteringrecording mode is triggered by specific speach acts likeTake a voice note or Annotate sample bag one.When considering recognition accuracy in the open-mic condition, we consider additional metrics beyondword-error rate (WER).
Since the recognizer can fail tofind a hypothesis for an utterance, we compute the false-rejection rate (FREJ) for within-domain utterances andadjusted word-error (AWER) counting only the word er-rors on the non-rejected utterances.
We also considermisrecognitions of out-of-domain utterance as within-domain, and compute the false-accept rate (FACC).
Table2 gives the performance results for the grammar-basedlanguage model that was used in the September test.
Thismodel gives reasonable performance on within-domainutterances, but falsely accepts 25.5% of out-of-domainutterances.
After the September test, we used the trainingdata we had collected to build a Probabilistic Context-Free Grammar using the compute-grammar-probstool that comes with Nuance (Nuance, 2002).
Using only485 utterances of training data, there was improvementin both the AWER and FACC rates, resulting in a lan-guage model where both FREF and FACC were under10%.
There was also a substantial improvement in recog-nition speed, as measured in multiples of CPU real-time.Version WER FREJ AWER FACC xCPUrt(%) (%) (%) (%) (%)Baseline CFG Language ModelTraining 12.56 4.54 7.72 ?
58.9Test 9.5 3.25 7.5 25.5 57.5Probabilistic CFG Language ModelTraining 9.97 5.57 4.6 ?
19.4Test 8.99 7.32 3.7 9.09 19.0Table 2: Comparing Baseline and Probabilistic CFG6 ConclusionsWe will demonstrate a dialogue system that has an im-proved ability to discriminate between speech that is in-tended for different purposes, treating some as data ob-jects to be saved, and others identified as being out-of-domain.
With probabilities on the rules the system has anacceptably low false accept rate and is fast and accurate.ReferencesG.
Aist, J. Dowding, B.A.
Hockey, and J. Hieronymus.2002.
An intelligent procedure assistant for astro-naut training and support.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (demo track), Philadelphia, PA.O.
Lemon, A. Bracy, A. Gruenstein, and S. Peters.
2001.Multimodal dialogues with intelligent agents in dy-namic environments: the WITAS conversational in-terface.
In Proceedings of 2nd Meeting of the NorthAmerican Association for Computational Linguistics,Pittsburgh, PA.Nuance, 2002. http://www.nuance.com.
As of 15November 2002.M.
Rayner, B.A.
Hockey, and F. James.
2000.
A com-pact architecture for dialogue management based onscripts and meta-outputs.
In Proceedings of the 6th Ap-plied Natural Language Processing Conference, Seat-tle, WA.A.
Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore.1999.
The CommandTalk spoken dialogue system.
InProceedings of the Thirty-Seventh Annual Meeting ofthe Association for Computational Linguistics, pages183?190.
