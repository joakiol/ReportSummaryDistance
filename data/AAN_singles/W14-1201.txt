Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 1?10,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsOne Step Closer to Automatic Evaluationof Text Simplification SystemsSanja?Stajner1and Ruslan Mitkov1and Horacio Saggion21Research Group in Computational Linguistics, University of Wolverhampton, UK2TALN Research Group, Universitat Pompeu Fabra, SpainS.Stajner@wlv.ac.uk, R.Mitkov@wlv.ac.uk, horacio.saggion@upf.eduAbstractThis study explores the possibility of re-placing the costly and time-consuminghuman evaluation of the grammaticalityand meaning preservation of the outputof text simplification (TS) systems withsome automatic measures.
The focus is onsix widely used machine translation (MT)evaluation metrics and their correlationwith human judgements of grammatical-ity and meaning preservation in text snip-pets.
As the results show a significant cor-relation between them, we go further andtry to classify simplified sentences into:(1) those which are acceptable; (2) thosewhich need minimal post-editing; and (3)those which should be discarded.
The pre-liminary results, reported in this paper, arepromising.1 IntroductionLexically and syntactically complex sentences canbe difficult to understand for non-native speak-ers (Petersen and Ostendorf, 2007; Alu?
?sio etal., 2008b), and for people with language impair-ments, e.g.
people diagnosed with aphasia (Car-roll et al., 1999; Devlin, 1999), autism spectrumdisorder (?Stajner et al., 2012; Martos et al., 2012),dyslexia (Rello, 2012), congenital deafness (Inuiet al., 2003), and intellectual disability (Feng,2009).
At the same time, long and complex sen-tences are also a stumbling block for many NLPtasks and applications such as parsing, machinetranslation, information retrieval, and summarisa-tion (Chandrasekar et al., 1996).
This justifies theneed for Text Simplification (TS) systems whichwould convert such sentences into their simplerand easier-to-read variants, while at the same timepreserving the original meaning.So far, TS systems have been developed for En-glish (Siddharthan, 2006; Zhu et al., 2010; Wood-send and Lapata, 2011a; Coster and Kauchak,2011; Wubben et al., 2012), Spanish (Saggion etal., 2011), and Portuguese (Alu?
?sio et al., 2008a),with recent attempts at Basque (Aranzabe et al.,2012), Swedish (Rybing et al., 2010), Dutch(Ruiter et al., 2010), and Italian (Barlacchi andTonelli, 2013).Usually, TS systems are either evaluated for: (1)the quality of the generated output, or (2) the effec-tiveness/usefulness of such simplification on read-ing speed and comprehension of the target popula-tion.
For the purpose of this study we focused onlyon the former.
The quality of the output generatedby TS systems is commonly evaluated by usinga combination of readability metrics (measuringthe degree of simplification) and human assess-ment (measuring the grammaticality and meaningpreservation).
Despite the noticeable similaritybetween evaluation of the fluency and adequacy ofa machine translation (MT) output, and evaluationof grammaticality and meaning preservation of aTS system output, there have been no works ex-ploring whether any of the MT evaluation metricsare well correlated with the latter, and could thusreplace the time-consuming human assessment.The contributions of the present work are thefollowing:?
It is the first study to explore the possibility ofreplacing human assessment of the quality ofTS system output with automatic evaluation.?
It is the first study to investigate the correla-tion of human assessment of TS system out-put with MT evaluation metrics.?
It proposes a decision-making procedure forthe classification of simplified sentences into:(1) those which are acceptable; (2) thosewhich need further post-editing; and (3) thosewhich should be discarded.12 Related WorkThe output of the TS system proposed by Sid-dharthan (2006) was rated for grammaticality andmeaning preservation by three human evaluators.Similarly, Drndarevic et al.
(2013) evaluated thegrammaticality and the meaning preservation ofautomatically simplified Spanish sentences on aLikert scale with the help of twenty-five humanannotators.
Additionally, the authors used sevenreadability metrics to assess the degree of simplifi-cation.
Woodsend and Lapata (2011b), and Glava?sand?Stajner (2013) used human annotators?
rat-ings for evaluating simplification, meaning preser-vation, and grammaticality, while additionally ap-plying several readability metrics for evaluatingcomplexity reduction in entire texts.Another set of studies approached TS as an MTtask translating from ?original?
to ?simplified?language, e.g.
(Specia, 2010; Woodsend and Lap-ata, 2011a; Zhu et al., 2010).
In this case, the qual-ity of the output generated by the system was eval-uated using several standard MT evaluation met-rics: BLEU (Papineni et al., 2002), NIST (Dod-dington, 2002), and TERp (Snover et al., 2009).3 MethodologyAll experiments were conducted on a freely avail-able sentence-level dataset1, fully described in(Glava?s and?Stajner, 2013), and the two datasetswe derived from it.
The original dataset and theinstructions for the human assessment are given inthe next two subsections.
Section 3.3 explains howwe derived two additional datasets from the origi-nal one, and to what end.
Section 3.4 describes theautomatic MT evaluation metrics used as featuresin correlation and classification experiments; Sec-tion 3.5 presents the main goals of the study; andSection 3.6 describes the conducted experiments.3.1 Original datasetThe dataset contains 280 pairs of original sen-tences and their corresponding simplified versionsannotated by humans for grammaticality, meaningpreservation, and simplicity of the simplified ver-sion.
We used all sentence pairs, focusing only onfour out of eight available features: (1) the originaltext, (2) the simplified text, (3) the grammaticalityscore, and (4) the score for meaning preservation.21http://takelab.fer.hr/data/evsimplify/2The other four features contain the pairID, groupID, themethod with which the simplification was obtained, and theCategory weighted ?
Pearson MAEGrammaticality 0.68 0.77 0.18Meaning 0.53 0.67 0.37Simplicity 0.54 0.60 0.28Table 1: IAA from (Glava?s and?Stajner, 2013)The simplified versions of original sentenceswere obtained by using four different simplifi-cation methods: baseline, sentence-wise, event-wise, and pronominal anaphora.
The baseline re-tains only the main clause of a sentence, and dis-cards all subordinate clauses, based on the out-put of the Stanford constituency parser (Klein andManning, 2003).
Sentence-wise simplificationeliminates all those tokens in the original sentencethat do not belong to any of the extracted factualevent mentions, while the event-wise simplifica-tion transforms each factual event mention into aseparate sentence of the output.
The last simplifi-cation scheme (pronominal anaphora) additionallyemploys pronominal anaphora resolution on top ofthe event-wise simplification scheme.33.2 Human AssessmentHuman assessors were asked to score the givensentence pairs (or text snippets in the case of splitsentences) on a 1?3 scale based on three crite-ria: Grammaticality (1 ?
ungrammatical, 2 ?
mi-nor problems with grammaticality, 3 ?
grammati-cal), Meaning (1 ?
meaning is seriously changedor most of the relevant information lost, 2 ?
someof the relevant information is lost but the meaningof the remaining information is unchanged, 3 ?
allrelevant information is kept without any change inmeaning), and Simplicity (1 ?
a lot of irrelevant in-formation is retained, 2 ?
some of irrelevant infor-mation is retained, 3 ?
all irrelevant information iseliminated).
The inter-annotator agreement (IAA)was calculated using weighted Kappa (weighted?
), Pearson?s correlation (Pearson), and mean av-erage error (MAE), and the obtained results arepresented in Table 1.
A few examples of assignedscores are given in Table 2, where G, M, and Sdenote human scores for grammaticality, meaningpreservation and simplicity respectively.score for simplicity, which are not relevant here.3For more detailed explanation of simplification schemesand the dataset see (Glava?s and?Stajner, 2013).2Ex.
Original Simplified G M S SM(a) ?It is understood the dead girl hadbeen living at her family home, ina neighbouring housing estate, andwas visiting her older sister at thetime of the shooting.?
?The dead girl had been living ather family home, in a neighbouringhousing estate and was visiting herolder sister.
?3 3 3 S(b) ?On Facebook, more than 10,000people signed up to a page an-nouncing an opposition rally forSaturday.?
?On Facebook, more than 10,000people signed to a page announcingan opposition rally for Saturday.
?2 3 3 S(c) ?Joel Elliott, also 22, of NorthRoad, Brighton, was charged onMay 3 with murder.
He appeared atLewes Crown Court on May 8 butdid not enter a plea.?
?Joel Elliott was charged on May 3with murder.
He appeared at LewesCrown Court on May 8.?3 2 3 S(d) ?For years the former Bosnia Serbarmy commander Ratko Mladic hadevaded capture and was one of theworld?s most wanted men, but histime on the run finally ended lastyear when he was arrested nearBelgrade.?
?For years the former Bosnia Serbarmy commander Ratko Mladic hadevaded but his time the run endedlast year he was arrested near Bel-grade.
?1 2 3 S(e) ?Police have examined the scene ata house at William Court in Bel-laghy, near Magherafelt for clues tothe incident which has stunned thecommunity.?
?Police have examined the sceneat William Court near Magherafelt.The incident has stunned the com-munity.
?3 1 3 P(f) ?Rastan, 25 km (15 miles) northof Homs city, has slipped in andout of government control severaltimes since the uprising against As-sad erupted in March 2011.?
?Rastan has slipped governmentcontrol several times.
The uprisingerupted in March 2011.?2 1 3 P(g) ?But opposition parties and inter-national observers said the vote wasmarred by vote-rigging, includingalleged ballot-box stuffing and falsevoter rolls.?
?But opposition parties and inter-national observers said .
?1 1 3 B(h) ?Foreign Affairs Secretary Albertdel Rosario was seeking a diplo-matic solution with Chinese Ambas-sador Ma Keqing, the TV networksaid.?
?Foreign Affairs Secretary Albertdel Rosario was seeking a diplo-matic solution with Chinese Ambas-sador Ma Keqing, the TV networksaid.
?3 3 1 B(h) ?
On Wednesday, two video jour-nalists working for the state-ownedRIA Novosti news agency werebriefly detained outside the ElectionCommission building where Putinwas handing in his application torun.?
?On Wednesday two video journal-ists were briefly detained outsidethe Election Commission building.Two video journalists worked forthe state-owned RIA Novosti newsagency.
Putin was handing in hisapplication.
?3 2 2 ETable 2: Human evaluation examples (G, M, and S correspond to the human scores for grammaticality,meaning preservation and simplicity, and SM denotes the simplification method used: B ?
baseline, S ?sentence-wise, E ?
event-wise, and P ?
pronominal anaphora)33.3 Derived DatasetsThe original dataset (Original) contains separatescores for grammaticality (G), meaning preserva-tion (M), and simplicity (S), each of them on a 1?3scale.
From this dataset we derived two additionalones: Total3 and Total2.The Total3 dataset contains three marks (OK ?use as it is, PE ?
post-editing required, and Dis?
discard) derived from G and M in the Originaldataset.
Those simplified sentences which scored?3?
for both meaning preservation (M) and gram-maticality (G) are placed in the OK class as theydo not need any kind of post-editing.
A closerlook at the remaining sentences suggests that anysimplified sentence which got a score ?2?
or ?3?for meaning preservation (M) could be easily post-edited, i.e.
it requires minimal changes which areobvious from its comparison to the correspondingoriginal.
For instance, in the sentence (b) in Ta-ble 2 the only change that needs to be made isadding the word ?up?
after ?signed?.
Those sen-tences which scored ?2?
for meaning need slightlymore, albeit simple modification.
The simplifiedtext snippet (c) in Table 2 would need ?but didnot enter a plea?
added at the end of the lastsentence.
The next sentence (d) in the same ta-ble needs a few more changes, but still very mi-nor ones: adding the word ?capture?
after ?hadevaded?, adding the preposition ?on?
before ?therun?, and adding ?when?
after ?last year?.
There-fore, we grouped all those sentences into one class?
PE (sentences which require a minimal post-editing effort).
Those sentences which scored ?1?for meaning need to either be left in their originalform or simplified from scratch.
We thus classifythem as Dis.
This newly created dataset (Total3)allows us to investigate whether we could auto-matically classify simplified sentences into thosethree categories, taking into account both gram-maticality and meaning preservation at the sametime.The Total2 dataset contains only two marks (?0?and ?1?)
which correspond to the sentences whichshould be discarded (?0?)
and those which shouldbe retained (?1?
), where ?0?
corresponds to Dis inTotal3, and ?1?
corresponds to the union of OK andPE in Total3.
The derivation procedure for bothdatasets is presented in Table 3.
We wanted to in-vestigate whether the classification task would besimpler (better performed) if there were only twoclasses instead of three.
In the case that such clas-sification could be performed with satisfactory ac-curacy, all sentences classified as ?0?
would be leftin their original form or simplified with some dif-ferent simplification strategy, while those classi-fied as ?1?
would be sent for a quick human post-editing procedure.OriginalTotal3 Total2G M3 3 OK 12 3 PE 11 3 PE 13 2 PE 12 2 PE 11 2 PE 13 1 Dis 02 1 Dis 01 1 Dis 0Table 3: DatasetsHere it is important to mention that we decidednot to use human scores for simplicity (S) for sev-eral reasons.
First, simplicity was defined as theamount of irrelevant information which was elim-inated.
Therefore, we cannot expect that any ofthe six MT evaluation metrics would have a sig-nificant correlation with this score (except maybeTERp and, in particular, one of its parts ?
?numberof deletions?.
However, none of the two demon-strated any significant correlation with the sim-plicity score, and those results are thus not re-ported in this paper).
Second, the output sentenceswith a low simplicity score are not as detrimentalfor the TS system as those with a low grammat-icality or meaning preservation score.
The sen-tences with a low simplicity score would simplynot help the target user read faster or understandbetter, but would not do any harm either.
Alter-natively, if the target ?user?
is an MT or infor-mation extraction (IE) system, or a parser for ex-ample, such sentences would not lower the perfor-mance of the system; they would just not improveit.
Low scores for G and M, however, would leadto a worse performance for such NLP systems,longer reading time, and a worse or erroneous un-derstanding of the text.
Third, the simplicity ofthe output (or complexity reduction performed bya TS system) could be evaluated separately, in afully automatic manner ?
using some readabilitymeasures or average sentence length as features(as in (Drndarevi?c et al., 2013; Glava?s and?Stajner,42013) for example).3.4 Features: MT Evaluation MetricsIn all experiments, we focused on six commonlyused MT evaluation metrics.
These are cosinesimilarity (using the bag-of-words representation),METEOR (Denkowski and Lavie, 2011), TERp(Snover et al., 2009), TINE (Rios et al., 2011), andtwo components of TINE: T-BLEU (which differsfrom the standard BLEU (Papineni et al., 2002) byusing 3-grams, 2-grams, and 1-grams when thereare no 4-grams found, where the ?original?
BLEUwould give score ?0?)
and SRL (which is the com-ponent of TINE based on semantic role labelingusing SENNA4).
Although these two componentscontribute equally to TINE (thus being linearlycorrelated with TINE), we wanted to investigatewhich one of them contributes more to the cor-relation of TINE with human judgements.
Giventheir different natures, we expect T-BLEU to con-tribute more to the correlation of TINE with hu-man judgements of grammaticality, and SRL tocontribute more to the correlation of TINE withhuman judgements of meaning preservation.As we do not have the reference for the simpli-fied sentence, all metrics are applied in a slightlydifferent way than in MT.
Instead of evaluating thetranslation hypothesis (output of the automatic TSsystem in our case) with the corresponding ref-erence translation (which would be a ?gold stan-dard?
simplified sentence), we apply the metricsto the output of the automatic TS system com-paring it with the corresponding original sentence.Given that the simplified sentences in the useddataset are usually shorter than the original ones(due to the elimination of irrelevant content whichwas the main focus of the TS system proposed byGlava?s and?Stajner (2013)), we expect low scoresof T-BLEU and METEOR which apply a brevitypenalty.
However, our dataset does not contain anykind of lexical simplification, but rather copies allrelevant information from the original sentence5.Therefore, we expect the exact matches of wordforms and semantic role labels (which are compo-nents of the MT evaluation metrics) to have a goodcorrelation to human judgements of grammatical-ity and meaning preservation.4http://ml.nec-labs.com/senna/5The exceptions being changes of gerundive forms intopast tense, and anaphoric pronoun resolution in some simpli-fication schemes.
See Section 3.1 and (Glava?s and?Stajner,2013) for more details.3.5 GoalAfter we obtained the six automatic metrics (co-sine, METEOR, TERp, TINE, T-BLEU, andSRL), we performed two sets of experiments, try-ing to answer two main questions:1.
Are the chosen MT evaluation metrics cor-related with the human judgements of gram-maticality and meaning preservation of theTS system output?2.
Could we automatically classify the simpli-fied sentences into those which are: (1) cor-rect, (2) require a minimal post-editing, (3)incorrect and need to be discarded?A positive answer to the first question wouldmean that there is a possibility of finding an au-tomatic metric (or a combination of several au-tomatic metrics) which could successfully replacethe time consuming human evaluation.
The searchfor that ?ideal?
combination of automatic metricscould be performed by using various classificationalgorithms and carefully designed features.
If wemanage to classify simplified sentences into thethree aforementioned categories with a satisfyingaccuracy, the benefits would be two-fold.
Firstly,such a classification system could be used for anautomatic evaluation of TS systems and an easycomparison of their performances.
Secondly, itcould be used inside a TS system to mark thosesentences of low quality which need to be checkedfurther, or those sentences whose original mean-ing changed significantly.
The latter could then beleft in their original form or simplified using somedifferent technique.3.6 ExperimentsThe six experiments conducted in this study arepresented in Table 4.
The first two experimentshad the aim of answering the first question (Sec-tion 3.5) as to whether the chosen MT metrics cor-relate with the human judgements of grammatical-ity (G) and meaning preservation (M) of the TSsystem output.
The results were obtained in termsof Pearson?s, Kendall?s and Spearman?s correla-tion coefficients.
The third and the fourth exper-iments (Table 4) could be seen as the intermediateexperiments exploring the possibility of automaticclassification of simplified sentences according totheir grammaticality, and meaning preservation.The main experiment was the fifth experiment, try-ing to answer the second question (Section 3.5)5Exp.
Description1.
Correlation of the six automatic MT metrics with the human scores for Grammaticality2.
Correlation of the six automatic MT metrics with the human scores for Meaning preservation3.
Classification of the simplified sentences into 3 classes (?1?
?
Bad, ?2?
?
Medium, and ?3?
?
Good) according totheir Grammaticality4.
Classification of the simplified sentences into 3 classes (?1?
?
Bad, ?2?
?
Medium, and ?3?
?
Good) according totheir Meaning preservation5.
Classification of the simplified sentences into 3 classes (OK, PE, Dis) according to their Total3 score6.
Classification of the simplified sentences into 2 classes (?1?
?
Retain, ?0?
?
Discard) according to their Total2 scoreTable 4: Experimentsas to whether we could automatically classify thesimplified sentences into those which are: (1) cor-rect (OK), (2) require minimal post-editing (PE),and (3) incorrect and need to be discarded (Dis).The last experiment (Table 4) was conducted withthe aim of exploring whether the classification ofsimplified sentences into only two classes ?
Retain(for further post-editing) and Discard ?
would leadto better results than the classification into threeclasses (OK, PE, and Dis) in the fifth experiment.All classification experiments were performedin Weka workbench (Witten and Frank, 2005; Hallet al., 2009), using seven classification algorithmsin a 10-fold cross-validation setup:?
NB ?
NaiveBayes (John and Langley, 1995),?
SMO ?
Weka implementation of SupportVector Machines (Keerthi et al., 2001) withnormalisation (n) or with standardisation (s),?
Logistic (le Cessie and van Houwelingen,1992),?
Lazy.IBk ?
K-nearest neighbours (Aha andKibler, 1991),?
JRip ?
a propositional rule learner (Cohen,1995),?
J48 ?
Weka implementation of C4.5 (Quin-lan, 1993).As a baseline we use the classifier which assignsthe most frequent (majority) class to all instances.4 Results and DiscussionThe results of the first two experiments (correla-tion experiments in Table 4) are presented in Sec-tion 4.1, while the results of the other four exper-iments (classification experiments in Table 4) canbe found in Section 4.2.
When interpreting the re-sults of all experiments, it is important to keep inmind that human agreements for meaning preser-vation (M) and grammaticality (G) were accept-able but far from perfect (Section 3.2), and thusit would be unrealistic to expect the correlationbetween the MT evaluation metrics and humanjudgements or the agreement of the classificationsystem with human assessments to be higher thanthe reported IAA agreement.4.1 Correlation of Automatic Metrics withHuman JudgementsThe correlations of automatic metrics with hu-man judgements of grammaticality and meaningpreservation are given in Tables 5 and 6 respec-tively.
Statistically significant correlations (at a0.01 level of significance) are presented in bold.Metric Pearson Kendall Spearmancosine 0.097 0.092 0.115METEOR 0.176 0.141 0.178T-BLEU 0.226 0.185 0.234SRL 0.097 0.076 0.095TINE 0.175 0.145 0.181TERp -0.208 -0.158 -0.198Table 5: Correlation between automatic evaluationmetrics and human scores for grammaticalityMetric Pearson Kendall Spearmancosine 0.293 0.262 0.334METEOR 0.386 0.322 0.405T-BLEU 0.442 0.382 0.475SRL 0.348 0.285 0.356TINE 0.427 0.385 0.447TERp -0.414 -0.336 -0.416Table 6: Correlation between automatic evaluationmetrics and human scores for meaning preserva-tionIt can be noted that human perception of gram-maticality is positively correlated with three auto-6AlgorithmGrammaticality Meaning Total3 Total2P R F P R F P R F P R FNB 0.53 0.46 0.48 0.54 0.54 0.54 0.54 0.53 0.53 0.74 0.69 0.71SMO(n) 0.39 0.63 0.48 0.52 0.49 0.45 0.43 0.53 0.44 0.55 0.74 0.63SMO(s) 0.39 0.63 0.48 0.57 0.56 0.55 0.57 0.55 0.51 0.60 0.73 0.63Logistic 0.45 0.61 0.49 0.57 0.57 0.56 0.61 0.60 0.59 0.75 0.77 0.74Lazy.IBk 0.57 0.58 0.57 0.50 0.50 0.50 0.54 0.54 0.54 0.73 0.73 0.73JRip 0.41 0.59 0.48 0.53 0.50 0.48 0.57 0.56 0.55 0.72 0.75 0.73J48 0.45 0.61 0.49 0.48 0.47 0.47 0.59 0.57 0.54 0.68 0.71 0.69baseline 0.39 0.63 0.48 0.17 0.41 0.24 0.21 0.46 0.29 0.55 0.74 0.63Table 7: Classification results (the best performances are shown in bold; baseline uses the majority class)ActualGrammaticality MeaningGood Med.
Bad Good Med.
BadGood 127 21 23 50 31 7Med.
29 19 10 24 73 16Bad 24 9 10 9 31 31Table 8: Confusion matrices for the best classifications according to Grammaticality (Lazy.IBk) andMeaning (Logistic).
The number of ?severe?
classification mistakes (classifying Good as Bad or viceversa) are presented in bold.matic measures ?
METEOR, T-BLEU, and TINE,while it is negatively correlated with TERp (TERpmeasures the number of edits necessary to performon the simplified sentence to transform it into itsoriginal one, i.e.
the higher the value of TERp,the less similar the original and its correspondingsimplified sentence are.
The other five MT metricsmeasure the similarity between the original and itscorresponding simplified version, i.e.
the highertheir value is, the more similar are the sentencesare).
All the MT metrics appear to be even bet-ter correlated with the human scores for meaningpreservation (Table 6), demonstrating six positiveand one (TERp) negative statistically significantcorrelation with M. The correlation is the highestfor T-BLEU, TINE, and TERp, though closely fol-lowed by all others.4.2 Sentence ClassificationThe results of the four classification experiments(Section 3.6) are given in Table 7.At first glance, the performance of the classifi-cation algorithms seems similar for the first twotasks (classification of the simplified sentencesaccording to their Grammaticality and Meaningpreservation).
However, one needs to take into ac-count that the baseline for the first task was muchmuch higher than for the second task (Table 7).Furthermore, it can be noted that for the first task,recall was significantly higher than precision formost classification algorithms (all except NB andLogistic), while for the second task they were verysimilar in all cases.
More importantly, a closerlook at the confusion matrices reveals that most ofthe incorrectly classified sentences were assignedto the nearest class (Medium into Bad or Good;Bad into Medium; and Good into Medium6) in thesecond task, while it was not the case in the firsttask (Table 8).Classification performed on the Total3 datasetoutperformed both previous classifications ?
thatbased on Grammaticality and that based on Mean-ing ?
on four different algorithms (NB, Logis-tic, JRip, and J48).
Classification conducted onTotal3 using Logistic outperformed all results ofclassifications on either Grammaticality or Mean-ing separately (Table 7).
It reached a 0.61, 0.60,and 0.59 score for the weighted precision (P), re-call (R), and F-measure (F), respectively, thus out-performing the baseline significantly.
More im-portantly, classification on the Total3 dataset ledto significantly fewer mis-classifications betweenGood and Bad (Table 9) than the classificationbased on Grammaticality, and slightly less than6Bad, Medium, and Good correspond to marks ?1?, ?2?,and ?3?
given by human evaluators.7ActualTotal3OK PE Dis.OK 41 32 4PE 17 85 12Dis.
6 31 28Table 9: Confusion matrix for the best classifica-tion according to Total3 (Logistic).
The number of?severe?
classification mistakes (classifying Goodas Bad or vice versa) are presented in bold.ActualTotal2Retain DiscardRetain 21 50Discard 12 189Table 10: Confusion matrix for the best classifi-cation according to Total2 (Logistic).
The num-ber of ?severe?
classification mistakes (classifyingRetain as Discard or vice versa) are presented inbold.the classification based only on Meaning (Table 8).Therefore, it seems that simplified sentences arebetter classified into three classes giving a uniquescore for both grammaticality and preservation ofmeaning together.The binary classification experiments based onthe Total2 led to results which significantly out-performed the baseline in terms of precision andF-measure (Table 7).
However, they resulted ina great number of sentences which should be re-tained (Retain) being classified into those whichshould be discarded (Discard) and vice versa (Ta-ble 10).
Therefore, it seems that it would be betterto opt for classification into three classes (Total3)than for classification into two classes (Total2).Additionally, we used CfsSubsetEval attributeselection algorithm (Hall and Smith, 1998) in or-der to identify the ?best?
subset of features.
The?best?
subsets of features for each of the four clas-sification tasks returned by the algorithm are listedin Table 11.
However, the classification perfor-mances achieved (P, R, and F) when using onlythe ?best?
features did not differ significantly fromthose when using all initially selected features, andthus are not presented in this paper.5 LimitationsThe used dataset does not contain any kind oflexical simplification (Glava?s and?Stajner, 2013).Classification ?Best?
featuresMeaning {TERp, T-BLEU, SRL, TINE}Grammaticality {TERp, T-BLEU}New3 {TERp, T-BLEU, SRL, TINE}New2 {TERp, T-BLEU, SRL}Table 11: The ?best?
features (CfsSubsetEval)Therefore, one should consider the limitation ofthis TS system which performs only syntactic sim-plification and content reduction.
On the otherhand, the dataset used contains a significant con-tent reduction in most of the sentences.
If the sameexperiments were conducted on a dataset whichperforms only syntactic simplification, we wouldexpect much higher correlation of MT evaluationmetrics to human judgements, due to the lesser im-pact of the brevity penalty in that case.If we were to apply the same MT evaluationmetrics to a TS system which additionally per-forms some kind of lexical simplification (eithera simple lexical substitution or paraphrasing), thecorrelation results for T-BLEU and cosine similar-ity would be lower (due to the lower number ofexact matches), but not for METEOR, TERp andSRL (and thus TINE as well).
As a similar prob-lem is also present in the evaluation of MT sys-tems where the obtained output could differ fromthe reference translation (while still being equallygood), METEOR, TERp, and SRL in TINE ad-ditionally use inexact matching.
The first two usethe stem, synonym, and paraphrase matches, whileSRL uses ontologies and thesaurus.6 Conclusions and Future WorkWhile the results reported are preliminary andtheir universality needs to be validated on differentTS datasets, the experiments and results presentedcan be regarded as a promising step towards an au-tomatic assessment of grammaticality and mean-ing preservation for the output of TS systems.
Inaddition and to the best of our knowledge, thereare no such datasets publicly available other thanthe one used.
Nevertheless, we hope that these re-sults would initiate an interesting discussion in theTS community and start a new direction of studiestowards automatic evaluation of text simplificationsystems.8AcknowledgementsThe research described in this paper was par-tially funded by the European Commission un-der the Seventh (FP7-2007-2013) Framework Pro-gramme for Research and Technological Develop-ment (FP7-ICT-2011.5.5 FIRST 287607).ReferencesD.
Aha and D. Kibler.
1991.
Instance-based learningalgorithms.
Machine Learning, 6:37?66.S.
M.
Alu?
?sio, L. Specia, T. A. S. Pardo, E. G. Maziero,H.
M. Caseli, and R. P. M. Fortes.
2008a.
A cor-pus analysis of simple account texts and the pro-posal of simplification strategies: first steps towardstext simplification systems.
In Proceedings of the26th annual ACM international conference on De-sign of communication, SIGDOC ?08, pages 15?22,New York, NY, USA.
ACM.S.
M.
Alu?
?sio, L. Specia, T. A.S. Pardo, E. G. Maziero,and R. P.M. Fortes.
2008b.
Towards brazilian por-tuguese automatic text simplification systems.
InProceedings of the eighth ACM symposium on Doc-ument engineering, DocEng ?08, pages 240?248,New York, NY, USA.
ACM.M.
J. Aranzabe, A.
D?
?az De Ilarraza, and I. Gonz?alez.2012.
First Approach to Automatic Text Simplifica-tion in Basque.
In Proceedings of the first NaturalLanguage Processing for Improving Textual Acces-sibility Workshop (NLP4ITA).G.
Barlacchi and S. Tonelli.
2013.
ERNESTA: A sen-tence simplification tool for childrens stories in ital-ian.
In Computational Linguistics and IntelligentText Processing.J.
Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-vlin, and J. Tait.
1999.
Simplifying text forlanguage-impaired readers.
In Proceedings of the9th Conference of the European Chapter of the ACL(EACL?99), pages 269?270.R.
Chandrasekar, Christine Doran, and B. Srinivas.1996.
Motivations and methods for text simplifi-cation.
In In Proceedings of the Sixteenth Inter-national Conference on Computational Linguistics(COLING ?96, pages 1041?1044.W.
Cohen.
1995.
Fast Effective Rule Induction.
InProceedings of the Twelfth International Conferenceon Machine Learning, pages 115?123.W.
Coster and D. Kauchak.
2011.
Learning to Sim-plify Sentences Using Wikipedia.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics, pages 1?9.M.
Denkowski and A. Lavie.
2011.
Meteor 1.3: Au-tomatic Metric for Reliable Optimization and Evalu-ation of Machine Translation Systems.
In Proceed-ings of the EMNLP Workshop on Statistical MachineTranslation.S.
Devlin.
1999.
Simplifying natural language text foraphasic readers.
Ph.D. thesis, University of Sunder-land, UK.G.
Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram coocurrencestatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, pages 138?145.
Morgan Kaufmann Pub-lishers Inc.B.
Drndarevi?c, S.?Stajner, S. Bott, S. Bautista, andH.
Saggion.
2013.
Automatic Text Simplicationin Spanish: A Comparative Evaluation of Com-plementing Components.
In Proceedings of the12th International Conference on Intelligent TextProcessing and Computational Linguistics.
LectureNotes in Computer Science.
Samos, Greece, 24-30March, 2013., pages 488?500.L.
Feng.
2009.
Automatic readability assessment forpeople with intellectual disabilities.
In SIGACCESSAccess.
Comput., number 93, pages 84?91.
ACM,New York, NY, USA, jan.G.
Glava?s and S.?Stajner.
2013.
Event-Centered Sim-plication of News Stories.
In Proceedings of theStudent Workshop held in conjunction with RANLP2013, Hissar, Bulgaria, pages 71?78.M.
A.
Hall and L. A. Smith.
1998.
Practical featuresubset selection for machine learning.
In C. Mc-Donald, editor, Computer Science ?98 Proceedingsof the 21st Australasian Computer Science Confer-ence ACSC?98, pages 181?191.
Berlin: Springer.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I. H. Witten.
2009.
The weka data min-ing software: an update.
SIGKDD Explor.
Newsl.,11:10?18, November.K.
Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.2003.
Text simplification for reading assistance: aproject note.
In Proceedings of the second inter-national workshop on Paraphrasing - Volume 16,PARAPHRASE ?03, pages 9?16, Stroudsburg, PA,USA.
Association for Computational Linguistics.G.
H. John and P. Langley.
1995.
Estimating Contin-uous Distributions in Bayesian Classifiers.
In Pro-ceedings of the Eleventh Conference on Uncertaintyin Artificial Intelligence, pages 338?345.S.
S. Keerthi, S. K. Shevade, C. Bhattacharyya, andK.
R. K. Murthy.
2001.
Improvements to Platt?sSMO Algorithm for SVM Classifier Design.
NeuralComputation, 13(3):637?649.9D.
Klein and C.D.
Manning.
2003.
Accurate unlex-icalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics, volume 1, pages 423?430.
Association forComputational Linguistics.S.
le Cessie and J.C. van Houwelingen.
1992.
RidgeEstimators in Logistic Regression.
Applied Statis-tics, 41(1):191?201.J.
Martos, S. Freire, A. Gonz?alez, D. Gil, and M. Se-bastian.
2012.
D2.1: Functional requirements spec-ifications and user preference survey.
Technical re-port, FIRST technical report.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of ACL.S.
E. Petersen and M. Ostendorf.
2007.
Text Sim-plification for Language Learners: A Corpus Anal-ysis.
In Proceedings of Workshop on Speech andLanguage Technology for Education.R.
Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann Publishers, San Ma-teo, CA.L.
Rello.
2012.
Dyswebxia: a model to improve ac-cessibility of the textual web for dyslexic users.
InSIGACCESS Access.
Comput., number 102, pages41?44.
ACM, New York, NY, USA, January.M.
Rios, W. Aziz, and L. Specia.
2011.
TINE: A met-ric to assess MT adequacy.
In Proceedings of theSixth Workshop on Statistical Machine Translation(WMT-2011), Edinburgh, UK.M.
B. Ruiter, T. C. M. Rietveld, Cucchiarini C., Krah-mer E. J., and H. Strik.
2010.
Human LanguageTechnology and communicative disabilities: Re-quirements and possibilities for the future.
In Pro-ceedings of the the seventh international conferenceon Language Resources and Evaluation (LREC).J.
Rybing, C. Smithr, and A. Silvervarg.
2010.
To-wards a Rule Based System for Automatic Simpli-fication of Texts.
In The Third Swedish LanguageTechnology Conference.H.
Saggion, E. G?omez Mart?
?nez, E. Etayo, A. An-ula, and L. Bourg.
2011.
Text Simplification inSimplext: Making Text More Accessible.
Revistade la Sociedad Espa?nola para el Procesamiento delLenguaje Natural, 47:341?342.A.
Siddharthan.
2006.
Syntactic simplification andtext cohesion.
Research on Language & Computa-tion, 4(1):77?109.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.2009.
Fluency, Adequacy, or HTER?
Exploring Dif-ferent Human Judgments with a Tunable MT Metric.In Proceedings of the Fourth Workshop on Statisti-cal Machine Translation at the 12th Meeting of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL-2009), Athens, Greece.L.
Specia.
2010.
Translating from complex to simpli-fied sentences.
In Proceedings of the 9th interna-tional conference on Computational Processing ofthe Portuguese Language, pages 30?39, Berlin, Hei-delberg.S.
?Stajner, R. Evans, C. Orasan, and R. Mitkov.
2012.What Can Readability Measures Really Tell UsAbout Text Complexity?
In Proceedings of theLREC?12 Workshop: Natural Language Processingfor Improving Textual Accessibility (NLP4ITA), Is-tanbul, Turkey.I.
H. Witten and E. Frank.
2005.
Data mining: practi-cal machine learning tools and techniques.
MorganKaufmann Publishers.K.
Woodsend and M. Lapata.
2011a.
Learning to Sim-plify Sentences with Quasi-Synchronous Grammarand Integer Programming.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).K.
Woodsend and M. Lapata.
2011b.
WikiSimple:Automatic Simplification of Wikipedia Articles.
InProceedings of the 25th AAI Coference on ArtificialIntelligence.S.
Wubben, A. van den Bosch, and E. Krahmer.
2012.Sentence simplification by monolingual machinetranslation.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers - Volume 1, ACL ?12, pages 1015?1024, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Z.
Zhu, D. Berndard, and I. Gurevych.
2010.
A Mono-lingual Tree-based Translation Model for SentenceSimplification.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics(Coling 2010), pages 1353?1361.10
