INFORMATION EXTRACTION RESEARCH ANDAPPL ICAT IONS:  CURRENT PROGRESS ANDFUTURE D IRECT IONSAndrew Keh ler ,  Je r ry  R .
Hobbs ,  Doug las  Appe l t ,J ohn  Bear ,  Mat thew Caywood,  Dav id  I s rae l ,Megumi  Kameyama,  Dav id  Mar t in ,  and  C la i re  Monte leon iSRI International*1 In t roduct ionAnalysts face a daunting task: they must accuratelyanalyze, categorize, and assimilate a large body ofinformation from a variety of sources and for a va-riety of domains of interest.
The complexity of thetask necessitates a variety of information access andextraction tools which technology up to this pointhas not been able to provide.
SRI's TIPSTER PhaseIII project has focused on two major obstacles to thedevelopment of such tools: inadequate degrees of ac-curacy and portability.
We begin by providing anoverview of SRI's information extraction (IE) sys-tem, FASTUS, and then describe our efforts in thesetwo areas in turn.
We then conclude with somethoughts concerning future directions.2 Overv iew o f  FASTUSFASTUS processes natural language and producesrepresentations of the information relevant to a par-ticular application, typically in the form of databasetemplates.
As an example, we consider the taskspecified for the Sixth Message Understanding Con-ference (MUC-6), which was, roughly speaking, toidentify information i business news that describesexecutives moving in and out of high-level positionswithin companies (Appelt et al, 1995).
When FAS-TUS encounters a passage such as example (1),(1) John Smith, 47, was named president of ABCCorp.
He replaces Mike Jones.it should extract he information that Mike Jones is'out' and John Smith is 'in' at the position of presi-dent of company ABC Corp.FASTUS consists of three major components.
Thefirst is the pattern recognition module, which consistsof a series of finite state transducers that recognizeArtificial Intelligence Center, 333 Ravenswood Av-enue, Menlo Park, CA 94025, kehler@ai.sri, cornpatterns in the text and create templates represent-ing event and entity descriptions.
Pattern recogni-tion relies on a second component, he coreferencemodule, which identifies the referents of a varietyof types of referential expressions (e.g., pronouns,definite noun phrases).
Finally, the merger unifiestemplates created from different phrases in the textthat describe the same events.We illustrate by walking through an analysis ofpassage (1).
The input is initially processed by us-ing the finite state transducers to recognize relevantpatterns and annotate the text accordingly.
First,one or more preprocessing phases recognize low-levelpatterns uch as person names, organization names,and parts of speech.\[John Smith\]pERS--NAME \[47\]gUM \[was\]AUX\[named\]v \[president\]g \[of JR \[ABCCorp\]ORG-NAMEThe parsing phase identifies very local syntactic on-stituents, such as noun groups and verb groups; noattachment ofambiguous modifiers is attempted.\[John Smith\]pEIRS-NAMIZ \[47\]NUM \[was named\]vG\[president\]NG \[of\]p \[ABC Corp\]oRG--NAMEThe combiner phase pieces together slightly largerconstituents when it can be done reliably.\[John Smith, 47\]PERS--NG \[was named\]vc \[presi-dent of ABC Corp.\]POS--NGFinally, the domain phase applies domain-dependentpatterns to the sentence to identify clause-levelstates and events.
In this case, the entire sentencewill match such a pattern.\[John Smith, 47, was named president of ABCCorp\]DOMAIN--EV NTRecognizing a pattern in the domain phase typi-cally causes one or more template objects to be cre-ated.
In light of the MUC-6 task specification, we61defined transition templates that track movementsin and out of positions at companies; a person'sleaving a job is represented by the start state of atransition, whereas a person's taking a job is rep-resented by the end state.
Therefore, the person,company, and position in the first sentence of (1)are represented in an end state since Smith is takingthe described position.
To facilitate certain types ofinferencing during the merging phase, we also positthat someone, at this time unknown, is most likelyleaving the position, this being represented in thetransition's tart state as shown in Figure 1.in Figure 1 to produce the template shown in Fig-ure 3, which will lead to the correct output.STARTENDPERSON MIKE JONES \]POSITION PRESIDENTORGANIZATION ABC CORP.PERSON JOHN SMITHPOSITION PRESIDENTORGANIZATION ABC CORP.Figure 3: A Successful MergeSTARTENDPERSONPOSITION PRESIDENTORGANIZATION ABC CORP.\] POSITION PRESIDENTORCANIZATION ABC CORP.Figure h Template Generated from John Smith wasnamed president of ABC Corp.The second sentence in the passage, He replacesMike Jones, is then analyzed by the pattern match-ing phases, the details of which we omit.
Duringthis analysis, the coreference module identifies JohnSmith as the referent of "he".
Having recognized adomain-level pattern, all that is known is that thereis a start state involving the person Mike Jones andan end state involving the person John Smith, rep-resented by the template shown in Figure 2.\[ PERSON MIKE JONES \]START / POSITION - - -L ORGANIZATIONEND POSITIONORGANIZATIONFigure 2: Template Generated from He replacesMike Jones.As they stand, of course, these two templates donot appropriately summarize the information in thetext; there is a discourse-level relationship betweenthe two that must be captured.
This is the job of themerging component.
When a new template is cre-ated, the merger attempts to unify it with templatesthat precede it.
In this case, the template shown inFigure 2 should be unified with the template shown3 Focus  on  AccuracyThe first major obstacle to the broad deploymentof IE technology we address is the inadequate l velof accuracy of existing systems.
We have sought topush the accuracy of each of the three major modulesof FASTUS in our T IPSTER effort.3.1 A Lattice-Based System for PatternRecognit ionOne of the main reasons for the success of FASTUS isthat it bypasses much of the complex linguistic pro-cessing characteristic of previous ystems.
Process-ing decisions are made using local rather than globalevidence, minimizing the risk that correct analysesget lost in a sea of incorrect ones.
For instance, ateach phase in the pattern recognition component,only the analysis deemed to be the best is passed tothe next phase.
Unfortunately, while this strategyhas proved advantageous in general, in many casesit leads to premature processing decisions based ontoo little information.For instance, for the following example,(2) The committee heads announced the appoint-ment of John Smith as CEO.the parser phase of FASTUS will correctly mark "thecommittee heads" as a noun group.
This decision ismade because the noun usage of "head" is more com-mon than the verb usage in this domain, and becauseof a "greedy" preference for longer constituents.
Us-ing the same heuristics for example (3),(3) The committee heads Viacom's CEO recruit-ment efforts.the system will generate the same analysis for "thecommittee heads".
Since "heads" is actually used asa main verb in this example (with "the committee"as its subject), the parser's incorrect choice will re-sult in there being no domain-phase analysis for thesentence.62The trick, then, is to try to improve the scope (andthus, the accuracy) of the current mechanisms, with-out adopting the inadequacies of previous frame-works that FASTUS was designed to improve upon.To do this, we implemented a lattice-based version ofFASTUS.
In keeping with the finite state paradigm,the pattern recognition phases perform transduc-tions over compact lattice representations of the in-put, passing such representations between phases.With a lattice representation, there are as manyanalyses for a string as there are paths through it,yet processing remains efficient.
The processing ofexamples (2) and (3) will result in a lattice withboth possible analyses for "the committee heads".In example (2), the successful match will result frommatching a path in which "the committee heads" isanalyzed as a noun group, whereas in (3), the suc-cessful domain-phase match will result from a pathin which "the committee" is analyzed as a noungroup and "heads" is analyzed as a verb group.Although compactly represented, the numerousanalyses that can result from lattice-based process-ing still require some methods for pruning and pathselection.
To date, we have implemented and eval-uated a variety of strategies.
Thus far, the resultsof these experiments, as measured by F-score on theMUC-6 task, have been somewhat mixed.
A typi-cal experiment will yield about one point of gain inF-score; as expected, recall generally climbs with asmaller sacrifice in precision.
We plan to do furtherexperimentation in the future.3.2 Improvements to CorefereneeResolutionWe have implemented various high-precision andlargely domain-independent i cremental extensionsto the coreference resolution module.Delayed Resolution in the Lattice SystemThe implementation of the lattice-based systemopened up the possibility of addressing several coref-erence issues that could not be cleanly addressedwithin the nonlattice system.
The first is a catch-22 which results from a need to perform coreferenceresolution both before and after the domain phaselevel of analysis.
(Recall that coreference resolutioncomes before the domain phase.)
We illustrate withexample (4).
(4) Analysts have been expecting IBM to announcesome changes.
In fact, today they named JohnSmith as president.Let us assume, plausibly enough, that the domainphase contains a pattern of the following sort, whichwill match the second sentence if the referent of"they" is a company (in this case IBM).Event := Company named Person as PositionCoreference resolution must necessarily apply be-fore the domain phase, since the pattern interpreterneeds to know whether the denotation of the sub-ject (the referent of "they") is a company.
Unfortu-nately, in this particular case the coreference moduleis likely to choose "analysts" as the referent, since itoccupies the subject position of the preceding clause,which usually indicates a higher degree of saliencethan the object position that "IBM" occupies.
In-tuitively, however, just the fact that the system hasthe aforementioned pattern suggests that one wouldexpect a company to be situated at that point inthe context of the clause.
Thus, there is reason towant coreference to apply after it has access to thatpattern, that is, after domain-phase processing.A similar problem occurs with respect o intrasen-tential coreference constraints.
Consider the sen-tence(5) John Smith removed him from the CEO post.Intrasentential constraints, dictated by the syntac-tic structure of the sentence, tell us that "him" can-not refer to John Smith.
However, only the domainphase has a notion of sentence-level syntax, so thesystem has no way of knowing of the applicabilityof this constraint given that the coreference moduleoperates before this phase.The lattice-based system provides a way to in-corporate and preserve ambiguities through the do-main phase, and thus offers an opportunity to ad-dress these problems.
Instead of selecting only themost preferred referent for a referential expression,the coreference module takes the set of alternativesand writes arcs for each onto the lattice in placeof the referential phrase, including relative levels ofpreference.
This lattice then serves as input to thedomain phase, as before, at which point the aboveconstraints can be enforced.
In the case of exam-ple (4), for instance, the path in which "they" isrewritten as "analysts" will not result in a success-ful match, whereas the path in which it is rewrittenas "IBM" will.
Alternatively, if both potential ref-erents were company names, then the one that thecoreference module considers to be most preferredwill be selected.Contributions of the coreference and lattice com-ponents were independently measured on an earlierbaseline system.
We observed that both componentsincrease the recall and precision independently, with63the recall (error reduction of 8% to 10%) much moreaffected than the precision (error reduction of 1% to2%).
The coreference-lattice combination leads toan even greater increase in the recall (13% error re-duction) but less impact on the precision (less than1% error reduction).
After this evaluation, we real-ized that the logic of the coreference-lattice ntegra-tion was incomplete, because of certain destructiveoperations that should not be maintained in a strat-egy in which alternatives are preserved.
We expectan even greater performance impact when integra-tion is completed.Extens ions  to Coverage  We also implementedextensions to the coverage of the coreference module.First, we implemented a module for resolving im-plicit arguments for certain relational nouns.
Re-lational nouns are those whose denotations are de-termined in association to a possessor entity.
Inthe business-political domain, position nouns suchas CEO and vice president are relational, associatedwith (sometimes implicit) organizations at whichthese positions exist.
As a first step, we added amechanism for resolving implicit organizations forposition expressions similar to those for pronoun res-olution.
This addition increased the IE recall by al-most a point (0.85%), a nontrivial gain for a changeof relatively limited scope.We also added a resolution routine for definitetemporal expressions.
Indexicals such as "today","next week", "last Monday", and "10 years ago" areresolved with respect to the document date.
Par-tial temporal expressions such as "Friday" and "the23rd" are resolved with respect o the combinationof the closest verb tense and the salient date in theglobal or local context.
The globally salient date isthe document date, whereas the locally salient dateis the most recent date mentioned in the text.
Theperformance of the date resolution routine was eval-uated with eight training articles containing a totalof 53 definite date expressions.
Among the currentlyintended coverage of 43 expressions, 37 were cor-rectly resolved.
We can interpret it as having 69.8%recall (37/53) and 86.0% precision (37/43).F ragment  Analys is  After we observed the ef-fectiveness of implicit argument resolution as de-scribed above, we added a domain-specific treatmentof what we call fragment analysis.
FASTUS oftenfinds fragments of domain patterns in texts becauseof insufficient domain coverage--an i evitable limi-tation, given the ability for natural language to ex-press the same content in many different, often un-predictable surface realizations.
Consider the follow-ing example.
(6) John Doe, who is known for his "my way or thehighway" management s yle, but who nonethe-less receives rave reviews from industry insiders,even his enemies, was named president of IBM.In this case, FASTUS is likely to match the fragment"was named president of IBM," outputting a tran-sition with a position and organization.
Unfortu-nately, given the intervening material between thisfragment and the subject, it will also most likely failto link the transition to the incoming person, JohnDoe.
The fragment analysis code corrects this by in-specting each transition created for a sentence, and,assuming that a substantial but incomplete templateis found, attempts to locate candidates from thesurrounding discourse context o replace the emptyslots.
The overall effect, specifically of making par-tial domain event templates more complete, is sim-ilar to that of the merging phase.
The difference isthat while merging combines two or more partiallyfilled domain events, missing argument resolutionfills empty slots of each domain event with recentlymentioned entities even if they are not associatedwith extracted events.
We compared the effects offragment analysis and merging on the overall scoreusing the 100 message MUC-6 training set.
The re-sult is shown in Table 1; fragment analysis aloneperformed better than merging alone, with the twotogether performing the best.An Analys is  of  WordNet  Sanda Harabagiu, aformer post-doctoral fellow at SRI, performed ananalysis of how WordNet might be used to improvecoreference r solution, particularly by exploiting hy-pernym and synonym information.
Using the MUC-6 coreference training messages as her corpus, shefound that 60% of the coreferenee xamples fallinto categories in which WordNet is of no poten-tial use: Cases of identity between strings (e.g., "acompany...the company") comprised 42.3% of theexamples, and cases in which coreference is indi-cated by syntactic onfiguration (e.g., appositives, asin "John Smith, president of Acme Widgets") com-prised 18.27% of the examples.Reference involving a synonym relation made up8.33% of the examples.
Of these, 3.1% were syn-onyms in WordNet, such as "bill" and "measure".However, 5.23% were not in WordNet.
Some of thesecases one could imagine being in such a knowledgesource, such as "business" and "company"; it justso happens that they are not.
On the other hand,there are also more difficult cases, such as "IBM"and "wounded computer giant", for which no knowl-edge base is likely to contain a relation.Reference involving a hypernym relation made up64Merging Fragment AnalysisOff OffOff OnOn !
OffOn !
OnPrecision \[ Recall \[ F-score71 42 52.6068 48 56.1065 52 57.6764 55 59.17Table 1: Contributions of Fragment Analysis and Merging11.0% of the cases.
Of these, 3.7% were in Word-Net, such as "quarter" and "period", and "chair-man" and "officer".
The other 7.3% were not inWordNet.
Again, there were cases which one couldimagine being there, such as "automaker" and "com-pany".
Others, however, such as "Clinton officials"and "Clinton camp", are not likely to be found inany such knowledge base.The remaining cases were often more difficult;many involving metonymy.3.3 Learning Merging StrategiesEarly in the project, we performed an analysis of theerrors FASTUS made on a subset of the MUC-6 devel-opment corpus.
The majority of the errors indictedmerging at least in part, suggesting that mergingimprovements had a potential for high payoff.The existing FASTUS merging algorithm is quitesimple - it attempts to merge newly created tem-plates with previous ones, starting with the mostrecent.
Templates are merged when they are unifi-able in accordance with any prespecified constraints.Despite its simplicity, the algorithm has proven tobe fairly successful.
Nonetheless, it is quite possi-ble that other merging strategies could yield betterresults.There are two ways in which one might attemptto identify such strategies.
First, one could performdata analyses to identify good merging principles,handcode them, and test the results.
Alternatively,one could attempt to have merging strategies beacquired by the system automatically, using sometraining mechanism.
We attempted both of these,which we discuss in turn.Data  Analyses  and  Exper imentat ion  Thefirst action we took was to perform an extensiveanalysis of merging results.
We developed etailedmechanisms for tracing merging behavior and dis-tributed transcripts among several project partici-pants.
In analyzing these, we identified a variety ofconstraints which appeared to be extremely reliable,in particular, characteristics of templates that werealmost always correlated with incorrect merges.One by one, these constraints were implementedand tested.
In each case, end-to-end performanceon the scenario template task either remained thesame or decreased slightly.
In no case did we get anontrivial increase in performance.This was rather puzzling and frustrating, andhighlighted some of the problems with handcodingsystem improvements.
For one, the processes of dataanalysis, system coding, and testing are labor inten-sive.
One cannot try all possible alternative sets ofconstraints one might consider, so one can never besure that other, unattempted constraints would nothave fared better.
Second, it could be that we werebeing misguided by the relatively small data setsthat we were analyzing by hand.
Thus, we beganconsidering other paradigms for identifying bettermerging strategies.There were also other, longer-term considerationsfor moving away from handcoding merging improve-ments.
For one, the optimal merging strategy ishighly dependent on the quality of the input it re-ceives, which is constantly evolving in any realis-tic development setting, thus requiring continual re-experimentation.
Thus, changes that improve per-formance at one point in system development couldpotentially decrease performance at another time, orvice versa.
Second, a general goal of IE research isto have systems that can be trained for new applica-tions long after the system developers are involved,which precludes experimentation by hand.These considerations motivate research to deter-mine if merging strategies can be learned automat-ically.
There are several different ypes of learning,including supervised, unsupervised, and an area inbetween which one might call indirectly supervised.We have performed experiments using all three typesof technique, which we describe below.
1x The work reported on here, also discussed in Kehler(1998), concerns learning merging strategies in supportof the scenario template task of MUC-6 as described inSection 2.
While we are unaware of any other reportedresearch on this task, other work has addressed otherMUC-style tasks.
For instance, Kehler (1997) describesa probabilistic approach to entity-level merging that out-performs everal baseline metrics.
Also, researchers atBBN (Ralph Weischedel, TIPSTER 18-month meeting)65Superv ised  Methods  In our first set of experi-ments, we took the approach most commonly pur-sued in the computational linguistics literature,namely supervised learning.
Supervised methods re-quire a set of training data that the learning algo-r ithm can consult in constructing its model.
For ourinitial experiments, we ran the 100 MUC-6 train-ing messages through FASTUS and wrote out featuresignatures for the 534 merges that the system per-formed.
The feature signatures were created by ask-ing a set of 50 questions about the context in whichthe proposed merge is taking place, referencing thecontent of the two templates and/or the distancebetween the phrases from which each template wascreated.
Some example questions are:?
SUBSUMED?
: true if the contents of one tem-plate completely subsume the contents of theother.
* UNNAMED-REFERENCES?
:  true if eithertransition has a slot filled with an object lack-ing a proper name, e.g., "an employee" in theperson slot.
While these objects can mergewith other (perhaps named) entities of the sametype, in general they should not.
* LESS-THAN-700-CHARS?
: true if the phrasesfrom which the templates are created are lessthan 700 characters apart in the text.After the feature signatures were written, we exam-ined the texts and manually encoded a key for each.We attempted two approaches to classifyingmerges using this corpus as training data.
Thefirst was to grow a classification tree in the style ofBreiman et al (1984).
At each node, the algorithmasks each question and selects the one resulting inthe purest split of the data.
Entropy was used as themeasure of node purity.
In the second set of exper-iments, we used the approach to maximum entropymodeling described by Berger et al (1996).
The twopossible values for each of the same 50 questions (i.e.,yes or no) were paired with each of the two possi-ble outcomes for merging (i.e., correct merge or not)to create a set of feature functions, or features forshort, which were used in turn to define constraintson a probabilistic model.
We used the learned max-imum entropy model as a classifier by consideringany merge with a probability strictly greater than0.5 to be correct, and otherwise incorrect.report on learned merging strategies achieving ood per-formance on the less complex template ntity and tem-plate relation tasks in MUC-7, although no comparisonwith a similar hand-coded system was provided.Out of the available set of questions, each ap-proach selects only those that are most informativefor the classifier being developed.
In the case of thedecision tree, questions are selected based on howwell they split the data.
In the case of maximumentropy, the algorithm approximates the gain in themodel's predictiveness that would result from im-posing the constraints corresponding to each of theexisting inactive features, and selects the one withthe highest anticipated payoff.
One potential advan-tage of maximum entropy is that it does not splitdata like a decision tree does, which may prove im-portant as training sets will necessarily be limited intheir size.In our preliminary evaluations, we used two-thirdsof our annotated corpus as a training set (356 exam-ples), and the remaining one-third as a test set (178examples).
We ran experiments using three differentsuch divisions, using each example twice in a train-ing set and once in a test set.
In each case the maxi-mum entropy classifier chose features correspondingto either 6 or 7 of the available questions, whereasthe decision tree classifier asked anywhere from 7 to14 questions to get to the deepest leaf node.
In eachcase there was considerable, but not total, overlap inthe questions utilized.
Adding the errors from thethree evaluations together, the decision tree made34 errors (out of a possible 534), in which 13 correctmerges were classified as incorrect and 21 incorrectmerges were classified as correct.
The maximum en-tropy classifier made a total of 31 errors, in which14 correct merges were classified as incorrect and17 incorrect merges were classified as correct.
Thisis compared to a total of 139 errors out of the 534merges that the current merger made according tothe annotations.These results may appear to be positive, as itwould seem that both methods found some reliableinformation on which to make classifications.
How-ever, our goal here was to improve end-to-end per-formance on the scenario template task, and thus wewanted to know how much of an impact these im-proved merging strategies have on that performance.Therefore, we replaced the existing FASTUS merg-ing algorithm with two more discriminating mergers,each directed by one of our learned classifiers.
Thefirst version consulted the decision tree and mergedonly when the example was classified as correct.
Thesecond version did the same using the maximum en-tropy classifier.
For these experiments, the two mod-els were trained using the entire set of 534 examples.As we were still experimenting at this point, wewere not ready to perform an evaluation using ourset of blind test messages.
As an information gath-66ering experiment, we applied FASTUS using the newmergers to the corpus of messages that producedthe training data.
We would of course expect theseexperiments to yield better results than when ap-plied to unseen messages.
Nonetheless, the resultswere humbling - both experiments failed to improvethe performance of the overall system, and in factdegraded it slightly.
Generally, a point of precisionwas gained at the expense of a point or two of recall.Clearly, there is a rift between what one mightconsider to be good performance at discriminatingcorrect and incorrect merges based on human judg-ments, and the effect these decisions have on over-all performance.
Because the baseline FASTUS algo-rithm merges too liberally, using the classifiers causemany of the incorrect merges that were previouslyperformed to be blocked, at the expense of blockinga smaller number of correct merges.
Thus, it is possi-ble that the correct merges the system performs helpits end-to-end performance much more than incor-rect merges hurt it.
For instance, it may be that cor-rect merges often result in well-populated templatesthat have a marked impact on performance, whereasincorrect merges may often add only one incorrectslot to an otherwise correct template, or even resultin templates that do not pass the threshold for ex-tractability at all.
In fact, in certain circumstancesincorrect merges can actually help performance, iftwo incorrect emplates that would produce incor-rect end results are unified to become one.In any case, it should be clear that improved per-formance on an isolated subcomponent of an IE sys-tem, as measured against human annotations forthat subcomponent, does not necessarily translateto improved end-to-end system performance.
Addthis to the cost of creating this annotated ata -which will continually become obsolete as the up-stream FASTUS modules undergo development - andit becomes clear that we need to look to other meth-ods for learning merging mechanisms.Unsuperv ised  Methods  Naturally, the main al-ternatives to supervised methods are unsupervisedmethods.
We consider eplacing our merging algo-rithm with one that performs an unsupervised clus-tering of the templates and merges the templates ineach cluster.
Of course, we will not know a priorihow many clusters there are, that is, how many tem-plates we should be left with when we are finished.
Amethod that does not require such knowledge is Hi-erarchical Agglomerative Clustering (HAC) (Dudaand Hart, 1973; Everitt, 1980, inter alia).The HAC algorithm is conceptually straightfor-ward.
Given a set of examples, the algorithm beginsby assigning each to its own cluster.
A predeter-mined similarity metric is then applied to each pair-wise combination of clusters, and the most similarpair combined.
The process is iterated until no pairof clusters have a similarity that exceeds a presetthreshold.Our application of clustering is somewhat differentfrom many problems to which clustering has beenapplied.
For one, our clusters will always have onlyone member, since templates are merged upon clus-tering.
Issues with how to compute similarity be-tween two nonsingleton sets O f data points are there-fore avoided.
Furthermore, our notion of similarity isnonstandard.
Usually, similar examples are distinct,but have properties that are "close" to each other insome space.
Here, similarityis meant o measure thelikelihood that the two templates are incomplete de-scriptions of the same complex of eventualities (i.e.,the same transition), although the templates them-selves may look very different.We performed some informal experiments inwhich we intuited a similarity metric, assigningweights to a subset of the questions that we haddefined for the supervised learning experiments.
Forinstance, templates that were created from phrasesclose to each other in the text and that overlapped incontent received high similarity, whereas those thatwere far apart and did not overlap received low sim-ilarity.
Instead of merging incrementally as in thesupervised learning experiments, pattern matchingwas first applied to the entire text, and the resultingtemplates were clustered and merged until no pairof templates passed a preset similarity threshold.Running the system over the MUC-6 developmentset yielded results similar to our experiments usingthe supervised mergers.
We did not find this to beparticularly surprising; for instance, the mediocreresults could be attributable to the similarity metricsnot being very good.We did not push this approach any further, be-cause it is still lacking with respect to one of ourgoals for pursuing learning strategies.
While it ad-dresses the problem of requiring annotated trainingdata, it does not address the fact that the optimalmerging strategy is inherently dependent on its in-put.
If we encode a similarity metric for clusteringand keep it fixed, we are left with only a single degreeof freedom - the  similarity threshold at which to haltthe clustering process.
While this may yield someleverage (for instance, good input to the merger maycall for a high threshold, whereas bad input may callfor a lower threshold), it will certainly be too inflex-ible in the general case.In sum, several factors could influence the likeli-67hood of a potential merge within a particular appli-cation, and it therefore seems that something tied tothe application eeds to guide the learning process.Indirectly Supervised Methods  When devel-oping an IE system, one typically encodes (or isgiven) a moderate-size set of end-to-end develop-ment keys for a set of sample messages.
These keysneed to be encoded only once.
We did not use thesekeys for supervised learning because of the difficul-ties in aligning the inaccurate and incomplete inter-mediate templates produced by the system with the(normalized) end results.
However, we can use thekeys to evaluate the end results of the system, andattempt o tune a merging strategy based on theseevaluations.
After all, it is improved end-to-end per-formance that we are seeking in the first place.Thus, we consider a form of what we are calling in-directly supervised learning.
We use the HAC mech-anism described in the previous ection, but attemptto learn the similarity metric instead of stating itexplicitly.
The search through the space of possi-ble similarity metrics will be driven by end-to-endperformance on a set of training messages.We start by defining a space of similarity met-rics.
In a preliminary experiment, we used 7 of thequestions that were used in the supervised experi-ments, coupled with their negations, for a total of14 questions.
These questions are assigned weights,either positive or negative, that get incorporatedinto a similarity metric when the question is trueof a potential merge.
Let Ai be the weights assignedto corresponding questions qi, and let the functionfq, (tl,t2) be 1 if the question qi is true of the tem-plates tl and t2, and 0 if not.
Then the similarityS(tx,t2) is given byeE i  fqi (tl ,t2)*)~iS( t l , t2 )  --= e~\]ifqi(tx,t2).A i q- 1This function, which is adapted from the form ofthe probability model used in the maximum entropyframework, provides a similarity measure in termsof a probability.We used an annealing strategy to tune the weightsAi.
The algorithm begins by processing the 100-message MUC-6 development set, usually with arandomly selected initial configuration that estab-lishes a baseline F-score.
The algorithm then iter-ates, selecting some of the questions at random (per-haps just one, perhaps all of them) and permutingtheir weights by a random amount, either positive ornegative.
The system is then rerun over the trainingset and the F-score measured.
Any permutation re-sulting in an F-score that is strictly greater than thecurrent baseline is adopted as the new baseline.
Tostay out of local maxima, a permutation leading to adecrease in performance may also be adopted.
Thisis the annealing part - such negative permutationsare accepted with a probability that is proportionalto a steadily decreasing measure of 'temperature',and inversely proportional to the magnitude of thedecrement in performance.
Thus, permutations thatdecrease performance slightly in early stages of thesearch are likely to be adopted, whereas permuta-tions that decrease performance either significantlyor in later stages of the search are not.The results of one of several experiments areshown in Figure 4.
The search began with an initialsimilarity metric achieving an F-score of 58.83, andcontinued for 300 iterations.
A low F-score of 57.70was achieved early, in iteration 10.
The best metricsconsidered yielded an F-score of 59.80.Obviously, and somewhat surprisingly, this graphis practically flat.
On one hand, it is unfortunatethat there aren't higher high points: The learnerwas not able to leverage the available features to ac-quire a much better merging strategy than the oneit started with.
Perhaps even more surprising, how-ever, is that there were also not lower low points -only iteration 10 achieved a score lower than 58.
Be-cause the learner was not given any bias with respectto the permutations it attempted, some of those itconsidered were intuitively poor (e.g., boosting theweight for phrases that are very far apart, loweringthe weight for sparsely filled templates with no over-lap).
Thus, one might have expected certain of theseto devastate performance, but none did.
It seemsthat as long as a certain amount of merging is per-formed, it matters less which templates are actuallymerged, and in what order.Conclus ions and Future Direct ions In sum,the learned mechanisms were neither significantlybetter nor worse than a hand-coded merging strat-egy.
The inability to outperform the existing strat-egy could be attributed to several facts.
We sus-pect that a major problem is the lack of accessi-ble, reliable, and informative indicators for merg-ing decisions.
Unlike lower-level problems in naturallanguage processing (NLP) in which local informa-tion appears to bear highly on the outcome, includ-ing, for instance, part-of-speech tagging (Church,1988; Brill, 1992, inter alia) and sense disambigua-tion (Yarowsky, 1994; Yarowsky, 1995, inter alia),none of the questions we have formulated appear tobe particularly indicative of what effect a potentialmerge will have on system performance.
This sug-gests that more research is needed to identify ways686462605856i i i i i50  1oo 15o 200  250Figure 4: Results of a Learning Experiment300to access the necessary knowledge from independentsources uch as existing knowledge bases, or by min-ing it from online text corpora using unsupervisedor indirectly supervised learning techniques.Furthermore, these experiments may be cause forconcern about the nature of the scoring metric andprocedure used in MUC-6.
All of the merging strate-gies attempted, both hand-coded and automaticallylearned, performed similarly.
This (rather unex-pected) result would suggest hat the scoring mech-anisms be given a closer look, which we do in thefollowing section.3.4 Analys is  of  the  Scor ing  SystemAs we have indicated, the lack of more significantprogress in some of the foregoing efforts had us puz-zled.
Intuitively positive system changes were notshowing much effect in terms of end-to-end perfor-mance, nor were certain intuitively negative changes.Of course, judgments of what constitute positive andnegative changes are only as good as the scoringmechanism which is providing the feedback.
As partof a related project at SRI, we began to find somemore concrete vidence that at times this feedbackhas been misguiding our efforts.
Incremental refine-ments in the system's output, ones that should yieldsuperior results, nevertheless receive a lower scorefrom the scoring mechanism.The following text (WSJ article 870112-0001) pro-vides an example illustrating this point:(7) The board also named a three-man executivecommittee to perform the chief executive's role.The three members are Victor Steele, head ofthe company's beverage division; Brian Bal-dock, head of the leisure and health division;and Shaun Dowling, who runs industrial oper-ations.Further executive resignations or dismissals arewidely expected.
The positions of Olivier Roux,head of financial planning, and Thomas Ward,a U.S. attorney who is a close aide to Mr.Saunders, are "open to question," one Guinnesssource said.FASTUS does poorly on this example, for under-standable reasons.
It did not produce any succes-sion events for the first paragraph, because doing sowould require resolving a variety of difficult linguis-tic issues lying beyond the depth of processing atwhich FASTUS operates.
On the other hand, for rea-sons that won't be described in detail, the systemgenerated a succession event from the second para-graph involving the position "head of financial plan-ning", with four IN-AND-OUT templates involvingRoux, Saunders, and two other people mentioned inthe article.While not much could be done for the first para-graph, we modified FASTUS SO that it would not pro-duce a template from the second paragraph.
Thechange to the system performance on this messagehas to be positive: while we do not generate anyadditional correct information from the change, weeliminated four predications about an irrelevant po-sition, three of which would be false even if one con-sidered the position to be relevant.
Other outputfor this text was not affected, so we would expectto observe the same recall (correct output was notchanged), but notably higher precision from havingeliminated the incorrect succession event, four incor-69rect IN-AND-OUTs, and two irrelevant PERSONtemplates.In reality, this change resulted in a slight rise inprecision (from 57 to 59) and a dramatic reduc-tion in recall (from 50 to 33), causing the F-scoreon this message to plummet from 53.30 to 42.67.The reward for eliminating four irrelevant predica-tions was a 20% drop in the score.
This result is, tosay the least, counterintuitive, and suggests eriousproblems in the ability of the scoring mechanism toprovide adequate feedback.We have several speculations regarding the causesof this behavior, but final conclusions await a morecomprehensive study.
It should be obvious in anycase, however, that further progress in IE is cruciallydependent on these issues being resolved.
While thisis true regardless of the approach one takes to systemdevelopment, it is especially so if we want to movetoward systems with rules and procedures that arelearned automatically.
Successful earning dependson the assumption that learned improvements arereflected in the evaluation function; if this is not thecase then learning is all but hopeless.
Thus, futureresearch in IE must be coupled with research intoevaluation strategies.3.5 The  Z ip f  Ef fect  on InformationExtraction ApplicationsA fundamental question with respect o IE applica-tions is the nature of the Zipf curve relating patterndevelopment to improved coverage.
In a given appli-cation, there is usually a small set of patterns whichwill have broad applicability - that is, they are likelyto match on many examples in any given set of un-seen data.
For instance, a MUC-6 pattern designedto match the sentence(8) John Smith was appointed CEO of IBM.will almost certainly match many other similar ex-amples also.
At the other end of the spectrum,there are many 'one-of-a-kind' examples in any giventraining corpus for which the corresponding patternis unlikely to match many other examples.
For in-stance, a pattern developed to handle the sentence(9) John Smith and his associate, Roger Jones, theformer of which will soon be on board at IBMand the latter of which will be heading to Ap-ple, are in line to be CEO and chairman, re-spectively.is unlikely to match other examples in anyreasonably-sized corpus of unseen data.
The bigquestion, then, is at what point in development dothe great majority of examples fall into the secondclass; at this point performance gains on trainingdata do not transfer to gains on test data.
It couldvery well be the case that after developing patternsto handle the examples in a moderately-sized train-ing set - say 100 messages, as in the MUC-6 trainingcorpora -  one has reached the point of diminishingreturns.In support of a project related to T IPSTER,  theOffice of Research and Development provided uswith an additional set (90 messages) of data withkeys annotated in accordance with the MUC-6 taskspecification.
This gave us an opportunity to seewhether new improvements inspired by this datawould transfer to the test data.
The changes we im-plemented were all relatively minor.
They included:?
Fixing a few problems in name recognition?
Adding a parser phase pattern?
Adding domain phase patterns for a fewmetaphorical expressions?
Eliminating a filter for irrelevant exts?
Fixing other minor bugsThese modifications caused our score on the newtraining data to increase from 46.4 to 52.1, whichis not a surprising result.
Given that the fixes weredirected narrowly at specific examples in this set,we did not expect to see much of an improvementin either of the other data sets.
Our suspicions wereconfirmed by results on the basic training data; ourscore on this set went from 58.6 to 59.6.
Quite sur-prisingly, however, our score on the blind test setrose significantly, from 51.7 to 57.1 - an increase ofover 10%.Thus, necessarily adding a proviso about the ade-quacy of the evaluation metrics per the last section,we have a negative data point for the hypothesis that100 training messages place us beyond the point ofdiminishing returns.
The second set of messages ap-parently had considerable overlap with the test datain areas that did not overlap with the original train-ing set.4 Focus  on Por tab i l i tyA second major obstacle to the broad utilization ofIE technology is the time and expertise needed todevelop new systems.
Users need to be able to de-velop extraction systems for new information eedsrapidly and without the assistance of a system de-veloper.
We have been developing infrastructure,consisting of patterns, ontologies, and tools, whichbrings us closer to these capabilities.704.1 Open Domain SystemThe majority of previously pursued IE tasks, in-cluding those in the MUC evaluations, have beencentered on extracting information from a narrowlydefined domain.
Alternatively, one might imaginedeveloping a system capable of extracting informa-tion about a significantly broader set of events thatmight potentially be of interest to an analyst.
Wecall such a system an open domain application.We are currently completing our implementationof an open domain system for business news.
Thesystem is built upon an infrastructure consisting of abroad set of patterns and ontologies.
These patternsand ontologies will serve as a basis for the analystto produce special-purpose IE systems (which wecall FASTLETS) for specific information eeds.
SuchFASTLETS could be used not only for database gen-eration, but also to improve systems for documentand subdocument retrieval and for task-driven sum-marization, among other applications.The patterns and ontologies were developed froman in-depth analysis of the 150 most common verbsand nominalizations within a corpus of Wall StreetJournal texts.
A frequency analysis was performedto identify these verbs and nominalizations, and alist was generated of all the sentences in the corpuscontaining each.
A chart was then constructed foreach group, listing each verb and its role fillers (sub-ject, object, prepositional objects).
This gave rise tothe patterns required to cover the examples, and theelements and organization of an ontology emerged.A few example patterns are shown below.Person analyzes { Industry I Commodity \]Financial-Instrument ){ Company I Person } controls Company{ Company \] Country } exports Goods toCountryCoperorg invests Money in { Financial-Instrument I Market\] Country I Company }The italicized elements indicate concepts in the de-veloped ontologies; for instance, Coperorg is a cat-egory subsuming several other concepts includingPerson, Company, and Organization.Open domain patterns are integrated with thecompile-time transformation component of FASTUS.This component is capable of taking a single patternand specifying the different ways in which it can beexpressed in English.
Thus, the first pattern in .thelist above will not only match sentence (10),(10) John Smith analyzed the automobile industry.but it will also match examples uch as (11) and(12).
(11) The automobile industry has been analyzed byJohn Smith.
(12) John Smith's analysis of the automobile indus-try...The output of the open domain pattern set is acase-frame style template, marking roles and modi-fiers such as agent, patient, location, time, and pur-pose.Open Domain and Rule Acquisition As wehave indicated, one of the ways in which the opendomain infrastructure can be used is as the basisfor allowing end users to construct heir own pat-terns tailored to their own information eeds.
Thedevelopment process will be much like what expertdevelopers do to build systems, except that therewill be a richer set of tools for doing so.
For in-stance, in our MUC-6 effort, we first outlined theevents of interest, and then scanned training textsto determine the verbs and nominalizations that en-coded those events.
We then categorized them intoclasses of verbs with the same case frames, and wrotesubject-verb-object patterns for each of the classes.We are currently developing an interface that willallow end users to accomplish this.
Analysts will se-lect the open domain patterns that are relevant otheir needs, and constrain their arguments in appro-priate fashions.
The system will support testing onexisting corpora and provide assistance for furtherrule adaptation.
The interface is being implementedin Java.4.2 An Application: Using IE to ImproveDocument RetrievalAs we have mentioned previously, one of the possi-ble uses for FASTLETS is to improve the quality ofdocument retrieval (DR) results.
We discuss someof our past and current work, as well as future plans.Completed Experiments In work predatingT IPSTER Phase III, a topic was chosen from theTREC-5 corpus which overlapped significantly withthe MUC-6 management succession topic.
SRI'sMUC-6 system was used to reorder the retrieval re-sults from the UMASS Inquery ad-hoc query system,based on the results of finite state pattern matching.This experiment produced a positive result, which,while far from being definitive, suggested that fur-ther investigation should be performed.
Of course,the scenario that was being tested is not realistic, assuch highly developed \[E systems will not generally71exist for most information eeds.
A more reasonablescenario would be one in which a rapidly developedFASTLET is used to perform such a task.During T IPSTER Phase III, SRI teamed withGE R&D to participate in the TREC-6 evaluation.FASTLETS were developed for 23 of the 47 topics inthe TREC-6 routing task, in which up to 4 hoursper topic was spent reading a small set of relevanttexts and writing a small number of grammar ulesand lexical attributes.
Each FASTLET was then runovernight on some additional training data, and an-other 1 to 2 hours was spent (on average) making anynecessary adjustments, for a total of an average of 4to 6 hours per topic.
The majority of these FASTLETgrammars were developed by a Stanford undergrad-uate, who has the characteristics one might expectthe end user of such a system to have: he is smartand computer literate, but knows essentially nothingabout NLP, linguistics, IE, and DR.In the GE/SRI  joint TREC-6 entry, the routingquery version of GE's DR system was used to pro-vide the top 2000 ranked documents for each topic.The FASTLETS were then used to rerank the list andproduce the top 1000.
The results were encourag-ing, albeit again not definitive.
In abstract erms,the GE/SRI  system improved on the results of theGE system alone for 16 topics, degraded them for5 topics, and received the same results on 2 topics.Of the 16 topics in which the results were improved,in 2 cases the improvement was very significant, in6 cases the improvement was significant, and in 8cases the improvement was small and insignificant.For one topic, ours was the best performing system.Of the 5 cases in which the results were degraded,in 3 cases the decline was significant and in 2 casesit was very significant.These results are encouraging in that they indicatethat the FASTLET approach to improving DR maybe feasible, considering that in at least some casesNLP techniques improved the results of an alreadycompetitive routing query system.An Ongo ing  Study  The results of the forego-ing experiments are especially encouraging consid-ering that they were achieved using a highly subop-timal overall architecture.
The DR and IE systemswere treated as black boxes: the DR system rankeddocuments using standard DR types of evidence(word frequency analysis), and then the FASTLETSreranked the documents based on pattern matchingevidence, without considering (or even having accessto) the DR evidence.
All the FASTLETS had accessto was the output ordering.
In actuality, it is likelythat both types of evidence are useful for relevancedetermination, and that the relative usefulness ofeach varies on a per-topic basis.
What is needed isan architecture in which the DR and IE evidenceis considered together, with a principled mechanismfor selecting the most informative features for docu-ment relevance on a per-topic basis.We are currently pursuing such an architecture,which, in addition to certain modifications to FAS-TUS, requires a research-level DR capability.
Wehave implemented a variety of word collection andfrequency analysis mechanisms which leverage theconsiderable tokenization and morphological anal-ysis capabilities of FASTUS.
We have also imple-mented several earning algorithms capable of incor-porating and weighing heterogeneous types of evi-dence.In order to speed up FASTUS processing on largedata collections, we implemented a "trigger word"compiler for FASTUS grammars.
The mechanismreads in a pattern set and generates a list of wordsrequired to match them.
Any sentence that doesnot contain a word on the list can be ignored afterearly stages of processing.
2 Initial experiments haveindicated a speed up of more than a factor of three.Mechanisms for generating relevance featuresfrom FASTLET results are currently being imple-mented, in preparation for the learning experiments.We will report on the results of these experiments ina future forum.5 Conc lus ions  and  Future  D i rec t ionsWe have summarized SRI's developments in address-ing two major obstacles to the broad deployment ofIE technology: accuracy and portability.
The TIP-STER program has witnessed significant progress inboth areas, and has perhaps witnessed even greaterprogress in our understanding of IE technology.We believe that the current state of IE technol-ogy suggests two main directions for future work;directions which look to opposite directions of theresearch-to-applications spectrum.
The first direc-tion is to leverage the progress we have made toembed IE technology within applications in whichit can be useful.
Candidate applications includedocument retrieval, task-based summarization, task-based machine translation, cross-document and mul-timedia fusion, and trend analysis.
Current progress2Although it should be noted that every sentenceneeds to be processed up through the combiner phaseif coreference is to work optimally, since referents forreferential expressions can occur in otherwise irrelevantsentences.
The degree to which ignoring this fact af-fects performance is an empirical question, which will bestudied in future work.72prepares us well for such investigations, the criticalquestion being whether current levels of accuracy aresufficient for success.Our work has also suggested that if we are toachieve revolutionary (rather than merely evolution-ary) improvements in the state-of-the-art, we alsoneed to step back and focus on fundamental re-search.
Current approaches are good at identifyingthe information that natural language "wears on itssleeve"; the remainder will require new and richertechniques.
Basic research is necessary to guide thedevelopment ofsuch mechanisms, and must be cou-pled with an investigation i to evaluation mecha-nisms.6 AcknowledgmentsWe thank Sanda Harabagiu and Jeffrey Petit fortheir contributions to this work.
This researchwas supported by the Defense Advanced ResearchProjects Agency and the Office of Research and De-velopment under the TIPSTER Phase III program.We also thank the Office of Research and Develop-ment for providing an additional set of annotatedtraining data.Everitt, B.
1980.
Clustering Analysis.
John Wileyand Sons, New York.Kehler, Andrew.
1997.
Probabilistic oreference ininformation extraction.
In Proceedings of the Sec-ond Conference on Empirical Methods in Natu-ral Language Processing (EMNLP-97), pages 163-173, Providence, RI, August.Kehler, Andrew.
1998.
Learning embedded is-course mechanisms for information extraction.
InProceedings of the AAAI Spring Symposium onApplying Machine Learning to Discourse Process-ing.
AAAI, Stanford, CA.Yarowsky, David.
1994.
Decision lists for lexical am-biguity resolution: Application to accent restora-tion in Spanish and French.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics (A CL-9~), pages 89-95, LasCruces, June.Yarowsky, David.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics (ACL-95), pages 189-196, Cambridge, MA, June.Re ferencesAppelt, Douglas E., Jerry R. Hobbs, John Bear,David Israel, Megumi Kameyama, Andy Kehler,David Martin, Karen Myers, and Mabry Tyson.1995.
SRI International FASTUS system MUC-6 test results and analysis.
In Proceedings of theSixth Message Understanding Conference (MUC-6), Columbia, Maryland.Berger, Adam, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Compu-tational Linguistics, 22(1):39-71.Breiman, L., J. H. Friedman, R. A. Olshen, and C. J.Stone.
1984.
Classification and Regression Trees.Wadsworth, Belmont, CA.Brill, Eric.
1992.
A simple rule-based part of speechtagger.
In Proceedings of the Third Conferenceon Applied Natural Language Processing, Trento,Italy.Church, Kenneth W. 1988.
A stochastic parts pro-gram and noun phrase parser for unrestricted text.In Proceedings of the Second Conference on Ap-plied Natural Language Processing, pages 136-143,Austin.Duda, Richard O. and Peter E. Hart.
1973.
Pat-tern Classification and Scene Analysis.
John Wi-ley and Sons, New York.73
