Proceedings of the 12th Conference of the European Chapter of the ACL, pages 621?629,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsUsing lexical and relational similarity to classify semantic relationsDiarmuid O?
Se?aghdhaComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FDUnited Kingdomdo242@cl.cam.ac.ukAnn CopestakeComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FDUnited Kingdomaac10@cl.cam.ac.ukAbstractMany methods are available for comput-ing semantic similarity between individ-ual words, but certain NLP tasks requirethe comparison of word pairs.
This pa-per presents a kernel-based framework forapplication to relational reasoning tasks ofthis kind.
The model presented here com-bines information about two distinct typesof word pair similarity: lexical similarityand relational similarity.
We present anefficient and flexible technique for imple-menting relational similarity and show theeffectiveness of combining lexical and re-lational models by demonstrating state-of-the-art results on a compound noun inter-pretation task.1 IntroductionThe problem of modelling semantic similarity be-tween words has long attracted the interest of re-searchers in Natural Language Processing and hasbeen shown to be important for numerous applica-tions.
For some tasks, however, it is more appro-priate to consider the problem of modelling sim-ilarity between pairs of words.
This is the casewhen dealing with tasks involving relational oranalogical reasoning.
In such tasks, the chal-lenge is to compare pairs of words on the basis ofthe semantic relation(s) holding between the mem-bers of each pair.
For example, the noun pairs(steel,knife) and (paper,cup) are similar becausein both cases the relation N2 is made of N1 fre-quently holds between their members.
Analogi-cal tasks are distinct from (but not unrelated to)other kinds of ?relation extraction?
tasks whereeach data item is tied to a specific sentence con-text (e.g., Girju et al (2007)).One such relational reasoning task is the prob-lem of compound noun interpretation, whichhas received a great deal of attention in recentyears (Girju et al, 2005; Turney, 2006; But-nariu and Veale, 2008).
In English (and otherlanguages), the process of producing new lexicalitems through compounding is very frequent andvery productive.
Furthermore, the noun-noun re-lation expressed by a given compound is not ex-plicit in its surface form: a steel knife may be aknife made from steel but a kitchen knife is mostlikely to be a knife used in a kitchen, not a knifemade from a kitchen.
The assumption made bysimilarity-based interpretation methods is that thelikely meaning of a novel compound can be pre-dicted by comparing it to previously seen com-pounds whose meanings are known.
This is anatural framework for computational techniques;there is also empirical evidence for similarity-based interpretation in human compound process-ing (Ryder, 1994; Devereux and Costello, 2007).This paper presents an approach to relationalreasoning based on combining information abouttwo kinds of similarity between word pairs: lex-ical similarity and relational similarity.
The as-sumptions underlying these two models of similar-ity are sketched in Section 2.
In Section 3 we de-scribe how these models can be implemented forstatistical machine learning with kernel methods.We present a new flexible and efficient kernel-based framework for classification with relationalsimilarity.
In Sections 4 and 5 we apply ourmethods to a compound interpretation task anddemonstrate that combining models of lexical andrelational similarity can give state-of-the-art re-sults on a compound noun interpretation task, sur-passing the performance attained by either modeltaken alone.
We then discuss previous researchon relational similarity, and show that some previ-ously proposed models can be implemented in ourframework as special cases.
Given the good per-formance achieved for compound interpretation, itseems likely that the methods presented in this pa-621per can also be applied successfully to other rela-tional reasoning tasks; we suggest some directionsfor future research in Section 7.2 Two models of word pair similarityWhile there is a long tradition of NLP researchon methods for calculating semantic similarity be-tween words, calculating similarity between pairs(or n-tuples) of words is a less well-understoodproblem.
In fact, the problem has rarely beenstated explicitly, though it is implicitly addressedby most work on compound noun interpretationand semantic relation extraction.
This section de-scribes two complementary approaches for usingdistributional information extracted from corporato calculate noun pair similarity.The first model of pair similarity is based onstandard methods for computing semantic similar-ity between individual words.
According to thislexical similarity model, word pairs (w1, w2) and(w3, w4) are judged similar if w1 is similar to w3and w2 is similar to w4.
Given a measure wsimof word-word similarity, a measure of pair simi-larity psim can be derived as a linear combinationof pairwise lexical similarities:psim((w1, w2), (w3, w4)) = (1)?
[wsim(w1, w3)] + ?
[wsim(w2, w4)]A great number of methods for lexical semanticsimilarity have been proposed in the NLP liter-ature.
The most common paradigm for corpus-based methods, and the one adopted here, is basedon the distributional hypothesis: that two wordsare semantically similar if they have similar pat-terns of co-occurrence with other words in someset of contexts.
Curran (2004) gives a comprehen-sive overview of distributional methods.The second model of pair similarity rests on theassumption that when the members of a word pairare mentioned in the same context, that contextis likely to yield information about the relationsholding between the words?
referents.
For exam-ple, the members of the pair (bear, forest) maytend to co-occur in contexts containing patternssuch as w1 lives in the w2 and in the w2,.
.
.
a w1,suggesting that a LOCATED IN or LIVES IN re-lation frequently holds between bears and forests.If the contexts in which fish and reef co-occur aresimilar to those found for bear and forest, this isevidence that the same semantic relation tends tohold between the members of each pair.
A re-lational distributional hypothesis therefore statesthat two word pairs are semantically similar if theirmembers appear together in similar contexts.The distinction between lexical and relationalsimilarity for word pair comparison is recognisedby Turney (2006) (he calls the former attributionalsimilarity), though the methods he presents focuson relational similarity.
O?
Se?aghdha and Copes-take?s (2007) classification of information sourcesfor noun compound interpretation also includes adescription of lexical and relational similarity.
Ap-proaches to compound noun interpretation havetended to use either lexical or relational similarity,though rarely both (see Section 6 below).3 Kernel methods for pair similarity3.1 Kernel methodsThe kernel framework for machine learning is anatural choice for similarity-based classification(Shawe-Taylor and Cristianini, 2004).
The cen-tral concept in this framework is the kernel func-tion, which can be viewed as a measure of simi-larity between data items.
Valid kernels must sat-isfy the mathematical condition of positive semi-definiteness; this is equivalent to requiring that thekernel function equate to an inner product in somevector space.
The kernel can be expressed in termsof a mapping function ?
from the input space X toa feature space F :k(xi,xj) = ??
(xi), ?
(xj)?F (2)where ?
?, ?
?F is the inner product associated withF .
X and F need not have the same dimension-ality or be of the same type.
F is by definition aninner product space, but the elements of X neednot even be vectorial, so long as a suitable map-ping function ?
can be found.
Furthermore, it isoften possible to calculate kernel values withoutexplicitly representing the elements of F ; this al-lows the use of implicit feature spaces with a veryhigh or even infinite dimensionality.Kernel functions have received significant at-tention in recent years, most notably due to thesuccessful application of Support VectorMachines(Cortes and Vapnik, 1995) to many problems.
TheSVM algorithm learns a decision boundary be-tween two data classes that maximises the mini-mum distance or margin from the training pointsin each class to the boundary.
The geometry of thespace in which this boundary is set depends on the622kernel function used to compare data items.
Bytailoring the choice of kernel to the task at hand,the user can use prior knowledge and intuition toimprove classification performance.One useful property of kernels is that any sumor linear combination of kernel functions is itselfa valid kernel.
Theoretical analyses (Cristianiniet al, 2001; Joachims et al, 2001) and empiri-cal investigations (e.g., Gliozzo et al (2005)) haveshown that combining kernels in this way can havea beneficial effect when the component kernelscapture different ?views?
of the data while indi-vidually attaining similar levels of discriminativeperformance.
In the experiments described below,we make use of this insight to integrate lexical andrelational information for semantic classificationof compound nouns.3.2 Lexical kernelsO?
Se?aghdha and Copestake (2008) demonstratehow standard techniques for distributional similar-ity can be implemented in a kernel framework.
Inparticular, kernels for comparing probability dis-tributions can be derived from standard probabilis-tic distance measures through simple transforma-tions.
These distributional kernels are suited to adata representation where each word w is identi-fied with the a vector of conditional probabilities(P (c1|w), .
.
.
, P (c|C||w)) that defines a distribu-tion over other terms c co-occurring with w. Forexample, the following positive semi-definite ker-nel between words can be derived from the well-known Jensen-Shannon divergence:kjsd(w1, w2) =?
?c[P (c|w1) log2(P (c|w1)P (c|w1) + P (c|w2))+ P (c|w2) log2(P (c|w2)P (c|w1) + P (c|w2))] (3)A straightforward method of extending this modelto word pairs is to represent each pair (w1, w2) asthe concatenation of the co-occurrence probabilityvectors forw1 andw2.
Taking kjsd as a measure ofword similarity and introducing parameters ?
and?
to scale the contributions of w1 and w2 respec-tively, we retrieve the lexical model of pair similar-ity defined above in (1).
Without prior knowledgeof the relative importance of each pair constituent,it is natural to set both scaling parameters to 0.5,and this is done in the experiments below.3.3 String embedding functionsThe necessary starting point for our implementa-tion of relational similarity is a means of compar-ing contexts.
Contexts can be represented in a va-riety of ways, from unordered bags of words torich syntactic structures.
The context representa-tion adopted here is based on strings, which pre-serve useful information about the order of wordsin the context yet can be processed and comparedquite efficiently.
String kernels are a family of ker-nels that compare strings s, t by mapping theminto feature vectors ?String(s), ?String(t) whosenon-zero elements index the subsequences con-tained in each string.A string is defined as a finite sequence s =(s1, .
.
.
, sl) of symbols belonging to an alphabet?.
?l is the set of all strings of length l, and ??
isset of all strings or the language.
A subsequenceu of s is defined by a sequence of indices i =(i1, .
.
.
, i|u|) such that 1 ?
i1 < ?
?
?
< i|u| ?
|s|,where |s| is the length of s. len(i) = i|u| ?
i1 + 1is the length of the subsequence in s. An embed-ding ?String : ??
?
R|?|lis a function that mapsa string s onto a vector of positive ?counts?
thatcorrespond to subsequences contained in s.One example of an embedding function is agap-weighted embedding, defined as?gapl(s) = [?i:s[i]=u?len(i)]u?
?l (4)?
is a decay parameter between 0 and 1; thesmaller its value, the more the influence of a dis-continuous subsequence is reduced.
When l = 1this corresponds to a ?bag-of-words?
embedding.Gap-weighted string kernels implicitly computethe similarity between two strings s, t as an innerproduct ??
(s), ?(t)?.
Lodhi et al (2002) presentan efficient dynamic programming algorithm thatevaluates this kernel in O(l|s||t|) time without ex-plicitly representing the feature vectors ?
(s), ?
(t).An alternative embedding is that used by Turney(2008) in his PairClass system (see Section 6).
Forthe PairClass embedding ?PC , an n-word context[0?1 words]N1|2 [0?3 words]N1|2 [0?1 words]containing target words N1, N2 is mapped ontothe 2n?2 patterns produced by substituting zeroor more of the context words with a wildcard ?.Unlike the patterns used by the gap-weighted em-bedding these are not truly discontinuous, as eachwildcard must match exactly one word.6233.4 Kernels on setsString kernels afford a way of comparing individ-ual contexts.
In order to compute the relationalsimilarity of two pairs, however, we do not want toassociate each pair with a single context but ratherwith the set of contexts in which they appear to-gether.
In this section, we use string embeddingsto define kernels on sets of strings.One natural way of defining a kernel over setsis to take the average of the pairwise basic kernelvalues between members of the two sets A and B.Let k0 be a kernel on a set X , and let A,B ?
Xbe sets of cardinality |A| and |B| respectively.
Theaveraged kernel is defined askave(A,B) =1|A||B|?a?A?b?Bk0(a, b) (5)This kernel was introduced by Ga?rtner etal.
(2002) in the context of multiple instance learn-ing.
It was first used for computing relational sim-ilarity by O?
Se?aghdha and Copestake (2007).
Theefficiency of the kernel computation is dominatedby the |A| ?
|B| basic kernel calculations.
Wheneach basic kernel calculation k0(a, b) has signifi-cant complexity, as is the case with string kernels,calculating kave can be slow.A second perspective views each set as corre-sponding to a probability distribution, and takesthe members of that set as observed samples fromthat distribution.
In this way a kernel on distribu-tions can be cast as a kernel on sets.
In the case ofsets whose members are strings, a string embed-ding ?String can be used to estimate a probabilitydistribution over subsequences for each set by tak-ing the normalised sum of the feature mappings ofits members:?Set(A) =1Z?s?A?String(s) (6)where Z is a normalisation factor.
Differentchoices of ?String yield different relational simi-larity models.
In this paper we primarily use thegap-weighted embedding ?gapl ; we also discussthe PairClass embedding ?PC for comparison.Once the embedding ?Set has been calculated,any suitable inner product can be applied to theresulting vectors, e.g.
the linear kernel (dot prod-uct) or the Jensen-Shannon kernel defined in (3).In the latter case, which we term kjsd below, thenatural choice for normalisation is the sum of theentries in?s?A ?String(s), ensuring that ?Set(A)has unit L1 norm and defines a probability dis-tribution.
Furthermore, scaling ?Set(A) by 1|A| ,applying L2 vector normalisation and applyingthe linear kernel retrieves the averaged set kernelkave(A,B) as a special case of the distributionalframework for sets of strings.Instead of requiring |A||B| basic kernel evalua-tions for each pair of sets, distributional set kernelsonly require the embedding ?Set(A) to be com-puted once for each set and then a single vectorinner product for each pair of sets.
This is gen-erally far more efficient than the kernel averagingmethod.
The significant drawback is that repre-senting the feature vector for each set demandsa large amount of memory; for the gap-weightedembedding with subsequence length l, each vec-tor potentially contains up to |A|(|smax|l)entries,where smax is the longest string in A.
In practice,however, the vector length will be lower due tosubsequences occurring more than once and manystrings being shorter than smax.One way to reduce the memory load is to re-duce the lengths of the strings used, either by re-taining just the part of each string expected to beinformative or by discarding all strings longer thanan acceptable maximum.
The PairClass embed-ding function implicitly restricts the contexts con-sidered by only applying to strings where no morethan three words occur between the targets, and byignoring all non-intervening words except singleones adjacent to the targets.
A further techniqueis to trade off time efficiency for space efficiencyby computing the set kernel matrix in a blockwisefashion.
To do this, the input data is divided intoblocks of roughly equal size ?
the size that is rele-vant here is the sum of the cardinalities of the setsin a given block.
Larger block sizes b thereforeallow faster computation, but they require morememory.
In the experiments described below, bwas set to 5,000 for embeddings of length l = 1and l = 2, and to 3,000 for l = 3.4 Experimental setup for compoundnoun interpretation4.1 DatasetThe dataset used in our experiments is O?
Se?aghdhaand Copestake?s (2007) set of 1,443 compoundnouns extracted from the British National Corpus(BNC).1 Each compound is annotated with one of1The data are available from http://www.cl.cam.ac.uk/?do242/resources.html.624six semantic relations: BE,HAVE, IN, AGENT, IN-STRUMENT and ABOUT.
For example, air disas-ter is labelled IN (a disaster in the air) and freighttrain is labelled INSTRUMENT (a train that car-ries freight).
The best previous classification resulton this dataset was reported by O?
Se?aghdha andCopestake (2008), who achieved 61.0% accuracyand 58.8% F-score with a purely lexical model ofcompound similarity.4.2 General MethodologyAll experiments were run using the LIBSVM Sup-port Vector Machine library.2 The one-versus-allmethod was used to decompose the multiclass taskinto six binary classification tasks.
Performancewas evaluated using five-fold cross-validation.
Foreach fold the SVM cost parameter was optimisedin the range (2?6, 2?4, .
.
.
, 212) through cross-validation on the training set.All kernel matrices were precomputed on near-identical machines with 2.4 Ghz 64-bit processorsand 8Gb of memory.
The kernel matrix compu-tation is trivial to parallelise, as each cell is inde-pendent.
Spreading the computational load acrossmultiple processors is a simple way to reduce thereal time cost of the procedure.4.3 Lexical featuresOur implementation of the lexical similaritymodel uses the same feature set as O?
Se?aghdhaand Copestake (2008).
Two corpora were usedto extract co-occurrence information: the writ-ten component of the BNC (Burnard, 1995) andthe Google Web 1T 5-Gram Corpus (Brants andFranz, 2006).
For each noun appearing as a com-pound constituent in the dataset, we estimate a co-occurrence distribution based on the nouns in co-ordinative constructions.
Conjunctions are identi-fied in the BNC by first parsing the corpus withRASP (Briscoe et al, 2006) and extracting in-stances of the conj grammatical relation.
As the5-Gram corpus does not contain full sentences itcannot be parsed, so regular expressions were usedto extract coordinations.
In each corpus, the set ofco-occurring terms is restricted to the 10,000 mostfrequent conjuncts in that corpus so that each con-stituent distribution is represented with a 10,000-dimensional vector.
The probability vector for thecompound is created by appending the two con-stituent vectors, each scaled by 0.5 to weight both2http://www.csie.ntu.edu.tw/?cjlin/libsvmconstituents equally and ensure that the new vec-tor sums to 1.
To perform classification with thesefeatures we use the Jensen-Shannon kernel (3).34.4 Relational featuresTo extract data for computing relational similarity,we searched a large corpus for sentences in whichboth constituents of a compound co-occur.
Thecorpora used here are the written BNC, contain-ing 90 million words of British English balancedacross genre and text type, and the English Giga-word Corpus, 2nd Edition (Graff et al, 2005), con-taining 2.3 billion words of newswire text.
Extrac-tion from the Gigaword Corpus was performed atthe paragraph level as the corpus is not annotatedfor sentence boundaries, and a dictionary of pluralforms and American English variants was used toexpand the coverage of the corpus trawl.The extracted contexts were split into sentences,tagged and lemmatised with RASP.
Duplicate sen-tences were discarded, as were sentences in whichthe compound head and modifier were more than10 words apart.
Punctuation and tokens containingnon-alphanumeric characters were removed.
Thecompound modifier and head were replaced withplaceholder tokens M:n and H:n in each sentenceto ensure that the classifier would learn from re-lational information only and not from lexical in-formation about the constituents.
Finally, all to-kens more than five words to the left of the left-most constituent or more than five words to theright of the rightmost constituent were discarded;this has the effect of speeding up the kernel com-putations and should also focus the classifier onthe most informative parts of the context sen-tences.
Examples of the context strings extractedfor the modifier-head pair (history,book) are the:a1957:m pulitizer:n prize-winning:j H:n describe:vevent:n in:i american:j M:n when:c elect:v of-ficial:n take:v principle:v and he:p read:v con-stantly:r usually:r H:n about:i american:j M:nor:c biography:n.This extraction procedure resulted in a corpusof 1,472,798 strings.
There was significant varia-tion in the number of context strings extracted foreach compound: 288 compounds were associatedwith 1,000 or more sentences, while 191 were as-3O?
Se?aghdha and Copestake (2008) achieve their singlebest result with a different kernel (the Jensen-Shannon RBFkernel), but the kernel used here (the Jensen-Shannon lin-ear kernel) generally achieves equivalent performance andpresents one fewer parameter to optimise.625kjsd kaveLength Acc F Acc F1 47.9 45.8 43.6 40.42 51.7 49.5 49.7 48.33 50.7 48.4 50.1 48.6?12 51.5 49.6 48.3 46.8?23 52.1 49.9 50.9 49.5?123 51.3 49.0 50.5 49.1?PC 44.9 43.3 40.9 40.0Table 1: Results for combinations of embeddingfunctions and set kernelssociated with 10 or fewer and no sentences werefound for 45 constituent pairs.
The largest contextsets were predominantly associated with politicalor economic topics (e.g., government official, oilprice), reflecting the journalistic sources of the Gi-gaword sentences.Our implementation of relational similarity ap-plies the two set kernels kave and kjsd defined inSection 3.4 to these context sets.
For each kernelwe tested gap-weighted embedding functions withsubsequence length values l in the range 1, 2, 3,as well as summed kernels for all combinationsof values in this range.
The decay parameter ?for the subsequence feature embedding was set to0.5 throughout, in line with previous recommen-dations (e.g., Cancedda et al (2003)).
To inves-tigate the effects of varying set sizes, we ran ex-periments with context sets of maximal cardinalityq ?
{50, 250, 1000}.
These sets were randomlysampled for each compound; for compounds asso-ciated with fewer strings than the maximal cardi-nality, all associated strings were used.
For q = 50we average results over five runs in order to re-duce sampling variation.
We also report someresults with the PairClass embedding ?PC .
Therestricted representative power of this embeddingbrings greater efficiency and we were able to useq = 5, 000; for all but 22 compounds, this allowedthe use of all contexts for which the ?PC embed-ding was defined.5 ResultsTable 1 presents results for classification with re-lational set kernels, using q = 1, 000 for the gap-weighted embedding.
In general, there is little dif-ference between the performance of kjsd and kavewith ?gapl ; the only statistically significant differ-ences (at p < 0.05, using paired t-tests) are be-tween the kernels kl=1 with subsequence lengthl = 1 and the summed kernels k?12 = kl=1+kl=2.The best performance of 52.1% accuracy, 49.9%F-score is obtained with the Jensen-Shannon ker-nel kjsd computed on the summed feature embed-dings of length 2 and 3.
This is significantly lowerthan the performance achieved by O?
Se?aghdhaand Copestake (2008) with their lexical similar-ity model, but it is well above the majority classbaseline (21.3%).
Results for the PairClass em-bedding are much lower than for the gap-weightedembedding; the superiority of ?gapl is statisticallysignificant in all cases except l = 1.Results for combinations of lexical co-occurrence kernels and (gap-weighted) relationalset kernels are given in Table 2.
With the excep-tion of some combinations of the length-1 setkernel, these results are clearly better than thebest results obtained with either the lexical orthe relational model taken alone.
The best resultis obtained by the combining the lexical kernelcomputed on BNC conjunction features with thesummed Jensen-Shannon set kernel k?23 ; thiscombination achieves 63.1% accuracy and 61.6%F-score, a statistically significant improvement (atthe p < 0.01 level) over the lexical kernel aloneand the best result yet reported for this dataset.Also, the benefit of combining set kernels ofdifferent subsequence lengths l is evident; of the12 combinations presented Table 2 that includesummed set kernels, nine lead to statisticallysignificant improvements over the correspondinglexical kernels taken alone (the remaining threeare also close to significance).Our experiments also show that the distribu-tional implementation of set kernels (6) is muchmore efficient than the averaging implementation(5).
The time behaviour of the two methodswith increasing set cardinality q and subsequencelength l is illustrated in Figure 1.
At the largesttested values of q and l (1,000 and 3, respectively),the averaging method takes over 33 days of CPUtime, while the distributional method takes justover one day.
In theory, kave scales quadraticallyas q increases; this was not observed because formany constituent pairs there are not enough con-text strings available to keep adding as q growslarge, but the dependence is certainly superlinear.The time taken by kjsd is theoretically linear in q,but again scales less dramatically in practice.
Onthe other hand kave is linear in l, while kjsd scalesexponentially.
This exponential dependence may626kjsd kaveBNC 5-Gram BNC 5-GramLength Acc F Acc F Acc F Acc F1 60.6 58.6 60.3 58.1 59.5 57.6 59.1 56.52 61.9* 60.4* 62.6 60.8 62.0 60.5* 61.3 59.13 62.5* 60.8* 61.7 59.9 62.8* 61.2** 62.3** 60.8**?12 62.6* 61.0** 62.3* 60.6* 62.0* 60.3* 61.5 59.2?23 63.1** 61.6** 62.3* 60.5* 62.2* 60.7* 62.0 60.3?123 62.9** 61.3** 62.6 60.8* 61.9* 60.4* 62.4* 60.6*No Set 59.9 57.8 60.2 58.1 59.9 57.8 60.2 58.1Table 2: Results for set kernel and lexical kernel combination.
*/** indicate significant improvement atthe 0.05/0.01 level over the corresponding lexical kernel alone, estimated by paired t-tests.50 250 1000100102104106108qtime/skavekjsd(a) l = 150 250 1000100102104106108qtime/skavekjsd(b) l = 250 250 1000100102104106108qtime/skavekjsd(c) l = 3Figure 1: Timing results (in seconds, log-scaled) for averaged and Jensen-Shannon set kernelsseem worrying, but in practice only short subse-quence lengths are used with string kernels.
Insituations where set sizes are small but long sub-sequence features are desired, the averaging ap-proach may be more appropriate.
However, itseems likely that many applications will be sim-ilar to the task considered here, where short sub-sequences are sufficient and it is desirable to useas much data as possible to represent each set.We note that calculating the PairClass embedding,which counts far fewer patterns, took just 1h21m.For optimal efficiency, it seems best to use a gap-weighted embedding with small set cardinality;averaged across five runs kjsd with q = 50 andl = ?123 took 26m to calculate and still achieved47.6% Accuracy, 45.1% F-score.6 Related workTurney et al (2003) suggest combining various in-formation sources for solving SAT analogy prob-lems.
However, previous work on compound in-terpretation has generally used either lexical simi-larity or relational similarity but not both in com-bination.
Previously proposed lexical models in-clude the WordNet-based methods of Kim andBaldwin (2005) and Girju et al (2005), and thedistributional model of O?
Se?aghdha and Copes-take (2008).
The idea of using relational similar-ity to understand compounds goes back at least asfar as Lebowitz?
(1988) RESEARCHER system,which processed patent abstracts in an incrementalfashion and associated an unseen compound withthe relation expressed in a context where the con-stituents previously occurred.Turney (2006) describes a method (Latent Rela-tional Analysis) that extracts subsequence patternsfor noun pairs from a large corpus, using queryexpansion to increase the recall of the search andfeature selection and dimensionality reduction toreduce the complexity of the feature space.
LRAperforms well on analogical tasks including com-pound interpretation, but has very substantial re-source requirements.
Turney (2008) has recentlyproposed a simpler SVM-based algorithm for ana-logical classification called PairClass.
While itdoes not adopt a set-based or distributional modelof relational similarity, we have noted above thatPairClass implicitly uses a feature representationsimilar to the one presented above as (6) by ex-tracting subsequence patterns from observed co-occurrences of word pair members.
Indeed, Pair-Class can be viewed as a special case of our frame-627work; the differences from the model we haveused consist in the use of a different embeddingfunction ?PC and a more restricted notion of con-text, a frequency cutoff to eliminate less commonsubsequences and the Gaussian kernel to comparevectors.
While we cannot compare methods di-rectly as we do not possess the large corpus of5 ?
1010 words used by Turney, we have testedthe impact of each of these modifications on ourmodel.4 None improve performance with our setkernels, but the only statistically significant effectis that of changing the embedding model as re-ported in section Section 5.
Implementing the fullPairClass algorithm on our corpus yields 46.2%accuracy, 44.9% F-score, which is again signifi-cantly worse than all results for the gap-weightedmodel with l > 1.In NLP, there has not been widespread use ofset representations for data items, and hence setclassification techniques have received little at-tention.
Notable exceptions include Rosario andHearst (2005) and Bunescu and Mooney (2007),who tackle relation classification and extractiontasks by considering the set of contexts in whichthe members of a candidate relation argument pairco-occur.
While this gives a set representation foreach pair, both sets of authors apply classifica-tion methods at the level of individual set mem-bers rather than directly comparing sets.
Thereis also a close connection between the multino-mial probability model we have proposed and thepervasive bag of words (or bag of n-grams) repre-sentation.
Distributional kernels based on a gap-weighted feature embedding extend these modelsby using bags of discontinuous n-grams and down-weighting gappy subsequences.A number of set kernels other than those dis-cussed here have been proposed in the machinelearning literature, though none of these propos-als have explicitly addressed the problem of com-paring sets of strings or other structured objects,and many are suitable only for comparing sets ofsmall cardinality.
Kondor and Jebara (2003) take adistributional approach similar to ours, fitting mul-tivariate normal distributions to the feature spacemappings of setsA andB and comparing the map-pings with the Bhattacharrya vector inner product.The model described above in (6) implicitly fitsmultinomial distributions in the feature space F ;4Turney (p.c.)
reports that the full PairClass modelachieves 50.0% accuracy, 49.3% F-score.this seems more intuitive for string kernel embed-dings that map strings onto vectors of positive-valued ?counts?.
Experiments with Kondor andJebara?s Bhattacharrya kernel indicate that it canin fact come close to the performances reportedin Section 5 but has significantly greater compu-tational requirements due to the need to performcostly matrix manipulations.7 Conclusion and future directionsIn this paper we have presented a combined modelof lexical and relational similarity for relationalreasoning tasks.
We have developed an efficientand flexible kernel-based framework for compar-ing sets of contexts using the feature embeddingassociated with a string kernel.5 By choosing aparticular embedding function and a particular in-ner product on subsequence vectors, the previ-ously proposed set-averaging and PairClass algo-rithms for relational similarity can be retrieved asspecial cases.
Applying our methods to the taskof compound noun interpretation, we have shownthat combining lexical and relational similarity is avery effective approach that surpasses either simi-larity model taken individually.Turney (2008) argues that many NLP tasks canbe formulated in terms of analogical reasoning,and he applies his PairClass algorithm to a numberof problems including SAT verbal analogy tests,synonym/antonym classification and distinctionbetween semantically similar and semantically as-sociated words.
Our future research plans includeinvestigating the application of our combined sim-ilarity model to analogical tasks other than com-pound noun interpretation.
A second promisingdirection is to investigate relational models for un-supervised semantic analysis of noun compounds.The range of semantic relations that can be ex-pressed by compounds is the subject of some con-troversy (Ryder, 1994), and unsupervised learningmethods offer a data-driven means of discoveringrelational classes.AcknowledgementsWe are grateful to Peter Turney, Andreas Vla-chos and the anonymous EACL reviewers for theirhelpful comments.
This work was supported inpart by EPSRC grant EP/C010035/1.5The treatment presented here has used a string represen-tation of context, but the method could be extended to otherstructural representations for which substructure embeddingsexist, such as syntactic trees (Collins and Duffy, 2001).628ReferencesThorsten Brants and Alex Franz, 2006.
Web 1T 5-gramCorpus Version 1.1.
Linguistic Data Consortium.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the ACL-06 Interactive PresentationSessions.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the Web usingminimal supervision.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics (ACL-07).Lou Burnard, 1995.
Users?
Guide for the British Na-tional Corpus.
British National Corpus Consortium.Cristina Butnariu and Tony Veale.
2008.
A concept-centered approach to noun-compound interpretation.In Proceedings of the 22nd International Conferenceon Computational Linguistics (COLING-08).Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean-Michel Renders.
2003.
Word-sequence ker-nels.
Journal of Machine Learning Research,3:1059?1082.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proceedings of the15th Conference on Neural Information ProcessingSystems (NIPS-01).Corinna Cortes and Vladimir Vapnik.
1995.
Supportvector networks.
Machine Learning, 20(3):273?297.Nello Cristianini, Jaz Kandola, Andre Elisseeff, andJohn Shawe-Taylor.
2001.
On kernel target algn-ment.
Technical Report NC-TR-01-087, Neuro-COLT.James Curran.
2004.
From Distributional to Seman-tic Similarity.
Ph.D. thesis, School of Informatics,University of Edinburgh.Barry Devereux and Fintan Costello.
2007.
Learningto interpret novel noun-noun compounds: Evidencefrom a category learning experiment.
In Proceed-ings of the ACL-07 Workshop on Cognitive Aspectsof Computational Language Acquisition.Thomas Ga?rtner, Peter A. Flach, Adam Kowalczyk,and Alex J. Smola.
2002.
Multi-instance kernels.In Proceedings of the 19th International Conferenceon Machine Learning (ICML-02).Roxana Girju, Dan Moldovan, Marta Tatu, and DanielAntohe.
2005.
On the semantics of nouncompounds.
Computer Speech and Language,19(4):479?496.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 Task 04: Classification of seman-tic relations between nominals.
In Proceedings ofthe 4th International Workshop on Semantic Evalu-ations (SemEval-07).Alfio Gliozzo, Claudio Giuliano, and Carlo Strappar-ava.
2005.
Domain kernels for word sense disam-biguation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL-05).David Graff, Junbo Kong, Ke Chen, and KazuakiMaeda, 2005.
English Gigaword Corpus, 2nd Edi-tion.
Linguistic Data Consortium.Thorsten Joachims, Nello Cristianini, and John Shawe-Taylor.
2001.
Composite kernels for hypertext cate-gorisation.
In Proceedings of the 18th InternationalConference on Machine Learning (ICML-01).Su Nam Kim and Timothy Baldwin.
2005.
Automaticinterpretation of noun compounds using WordNetsimilarity.
In Proceedings of the 2nd InternationalJoint Conference on Natural Language Processing(IJCNLP-05).Risi Kondor and Tony Jebara.
2003.
A kernel betweensets of vectors.
In Proceedings of the 20th Interna-tional Conference on Machine Learning (ICML-03).Michael Lebowitz.
1988.
The use of memory intext processing.
Communications of the ACM,31(12):1483?1502.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Christopher J. C. H. Watkins.2002.
Text classification using string kernels.
Jour-nal of Machine Learning Research, 2:419?444.Diarmuid O?
Se?aghdha and Ann Copestake.
2007.
Co-occurrence contexts for noun compound interpreta-tion.
In Proceedings of the ACL-07 Workshop on ABroader Perspective on Multiword Expressions.Diarmuid O?
Se?aghdha and Ann Copestake.
2008.
Se-mantic classification with distributional kernels.
InProceedings of the 22nd International Conferenceon Computational Linguistics (COLING-08).Barbara Rosario and Marti A. Hearst.
2005.
Multi-way relation classification: Application to protein-protein interactions.
In Proceedings of the 2005Human Language Technology Conference and Con-ference on Empirical Methods in Natural LanguageProcessing (HLT-EMNLP-05).Mary Ellen Ryder.
1994.
Ordered Chaos: The Inter-pretation of English Noun-Noun Compounds.
Uni-versity of California Press, Berkeley, CA.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press, Cambridge.Peter D. Turney, Michael L. Littman, Jeffrey Bigham,and Victor Shnayder.
2003.
Combining indepen-dent modules to solve multiple-choice synonym andanalogy problems.
In Proceedings of the 2003 Inter-national Conference on Recent Advances in NaturalLanguage Processing (RANLP-03).Peter D. Turney.
2006.
Similarity of semantic rela-tions.
Computational Linguistics, 32(3):379?416.Peter D. Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics (COLING-08).629
