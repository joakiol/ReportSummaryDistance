Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 102?109,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsUsing Deep Belief Nets for Chinese Named Entity CategorizationYu Chen1, You Ouyang2, Wenjie Li2, Dequan Zheng1, Tiejun Zhao11School of Computer Science and Technology, Harbin Institute of Technology, China{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn2Department of Computing, The Hong Kong Polytechnic University, Hong Kong{csyouyang, cswjli}@comp.polyu.edu.hkAbstractIdentifying named entities is essential inunderstanding plain texts.
Moreover, thecategories of the named entities are indicativeof their roles in the texts.
In this paper, wepropose a novel approach, Deep Belief Nets(DBN), for the Chinese entity mentioncategorization problem.
DBN has very strongrepresentation power and it is able toelaborately self-train for discoveringcomplicated feature combinations.
Theexperiments conducted on the AutomaticContext Extraction (ACE) 2004 data setdemonstrate the effectiveness of DBN.
Itoutperforms the state-of-the-art learningmodels such as SVM or BP neural network.1 IntroductionNamed entities (NE) are defined as the names ofexisting objects, such as persons, organizationsand etc.
Identifying NEs in plain texts providesstructured information for semantic analysis.Hence the named entity recognition (NER) taskis a fundamental task for a wide variety ofnatural language processing applications, such asquestion answering, information retrieval and etc.In a text, an entity may either be referred to by acommon noun, a noun phrase, or a pronoun.Each reference of the entity is called a mention.NER indeed requires the systems to identifythese entity mentions from plain texts.
The taskcan be decomposed into two sub-tasks, i.e., theidentification of the entities in the text and theclassification of the entities into a set of pre-defined categories.
In the study of this paper, wefocus on the second sub-task and assume that theboundaries of all the entity mentions to becategorized are already correctly identified.In early times, NER systems are mainly basedon handcrafted rule-based approaches.
Althoughrule-based approaches achieved reasonably goodresults, they have some obvious flaws.
First, theyrequire exhausted handcraft work to construct aproper and complete rule set, which partiallyexpressing the meaning of entity.
Moreover,once the interest of task is transferred to adifferent domain or language, rules have to berevised or even rewritten.
The discovered rulesare indeed heavily dependent on the taskinterests and the particular corpus.
Finally, themanually-formatted rules are usually incompleteand their qualities are not guaranteed.Recently, more attentions are switched to theapplications of machine learning models withstatistic information.
In this camp, entitycategorization is typically cast as a multi-classclassification process, where the named entitiesare represented by feature vectors.
Usually, thevectors are abstracted by some lexical andsyntactic features instead of semantic feature.Many learning models, such as Support VectorMachine (SVM) and Neural Network (NN), arethen used to classify the entities by their featurevectors.Entity categorization in Chinese attracted lessattention when compared to English or otherwestern languages.
This is mainly because theunique characteristics of Chinese.
One of themost common problems is the lack of boundaryinformation in Chinese texts.
For this problem,character-based methods are reported to be apossible substitution of word-based methods.
Asto character-based methods, it is important tostudy the implicit combination of characters.In our study, we explore the use of DeepBelief Net (DBN) in character-based entitycategorization.
DBN is a neural network modelwhich is developed under the deep learningarchitecture.
It is claimed to be able toautomatically learn a deep hierarchy of the inputfeatures with increasing levels of abstraction forthe complex problem.
In our problem, DBN isused to automatically discover the complicatedcomposite effects of the characters to the NEcategories from the input data.
With DBN, weneed not to manually construct the charactercombination features for expressing the semanticrelationship among characters in entities.Moreover, the deep structure of DBN enables thepossibility of discovering very sophisticated102combinations of the characters, which may evenbe hard to discover by human.The rest of this paper is organized as follow.Section 2 reviews the related work on nameentity categorization.
Section 3 introduces themethodology of the proposed approach.
Section4 provides the experimental results.
Finally,section 5 concludes the whole paper.2 Related workOver the past decades, NER has evolved fromsimple rule-based approaches to adapted self-training machine learning approaches.As early rule-based approaches, MacDonald(1993) utilized local context, which implicateinternal and external evidence, to aid oncategorization.
Wacholder (1997) employed anaggregation of classification method to captureinternal rules.
Both used hand-written rules andknowledge bases.
Later, Collins (1999) adoptedthe AdaBoost algorithm to find a weightedcombination of simple classifiers.
They reportedthat the combination of simple classifiers canyield some powerful systems with much betterperformances.
As a matter of fact, these methodsall need manual studies on the construction of therule set or the simple classifiers.Machine learning models attract moreattentions recently.
Usually, they trainclassification models based on context features.Various lexical and syntactic features areconsidered, such as N-grams, Part-Of-Speech(POS), and etc.
Zhou and Su (2002) integratedfour different kinds of features, which conveydifferent semantic information, for aclassification model based on the HiddenMarkov Model (HMM).
Koen (2006) built aclassifier with the Conditional Random Field(CRF) model to classify noun phrases in a textwith the WordNet SynSet.
Isozaki and Kazawa(2002) studied the use of SVM instead.There were fewer studies in Chinese entitycategorization.
Guo and Jiang (2005) appliedRobust Risk Minimization to classify the namedentities.
The features include seven traditionallexical features and two external-NE-hints basedfeatures.
An important result they reported is thatcharacter-based features can be as good as word-based features since they avoid the Chinese wordsegmentation errors.
In (Jing et al, 2003), it wasfurther reported that pure character-based modelscan even outperform word-based models withcharacter combination features.Deep Belief Net is introduced in (Hinton et al,2006).
According to their definition, DBN is adeep neural network that consists of one or moreRestricted Boltzmann Machine (RBM) layersand a Back Propagation (BP) layer.
This multi-layer structure leads to a strong representationpower of DBN.
Moreover, DBN is quite efficientby using RBM to implement the middle layers,since RBM can be learned very quickly by theContrastive Divergence (CD) approach.Therefore, we believe that DBN is very suitablefor the character-level Chinese entity mentioncategorization approach.
It can be used to solvethe multi-class categorization problem with justsimple binary features as the input.3 Deep Belief Network for ChineseEntity Categorization3.1 Problem FormalizationAn Entity mention categorization is a process ofclassifying the entity mentions into differentcategories.
In this paper, we assume that theentity mentions are already correctly detectedfrom the texts.
Moreover, an entity mentionshould belong to one and only one predefinedcategory.
Formally, the categorization functionof the name entities is( ( ))if V e C?
(1)whereie  is an entity mention from all themention set E, ( )iV e  is the binary featurevector ofie , C={C1, C2, ?, CM} is the pre-defined categories.
Now the question is to find aclassification function : Df R C?
which mapsthe feature vector V(ei) of an entity mention to itscategory.
Generally, this classification functionis learned from training data consisting of entitymentions with labeled categories.
The learnedfunction is then used to predict the category ofnew entity mentions by their feature vectors.3.2 Character-based FeaturesAs mentioned in the introduction, we intend touse character-level features for the purpose ofavoiding the impact of the Chinese wordsegmentation errors.
Denote the characterdictionary as D={d1, d2, ?, dN}.
To an e, it?sfeature vector is V(e)={ v1, v2, ?, vN }.
Each unitvi can be valued as Equation 2.???????
?01ededviii(2)103For example, there is an entity mention ???
?Clinton?.
So its feature vector is a vectorwith the same length as the character dictionary,in which all the dimensions are 0 except the threedimensions standing for ?, ?, and ?.
Therepresentation is clearly illustrated in Figure 1below.
Since our objective is to test theeffectiveness of DBN for this task.
Therefore, wedo not involve any other feature.Fig.
1.
Generating the character-level featuresCharacters compose the named entity andexpress its meaning.
As a matter of fact, thecomposite effect of the characters to themention category is quite complicated.
Forexample, ??
?Mr.
Li?
and ??
?Laos?
bothhave character ?, but ??
?Mr.
Li?
indicatesa person but ??
?Laos?
indicates a country.These are totally different NEs.
Anotherexample is ?????
?Capital of Paraguay?and ???
?Asuncion?.
They are two entitymentions point to the same entity despite thatthe two entities do not have any commoncharacters.
In such case, independent characterfeatures are not sufficient to determine thecategories of the entity mentions.
So we shouldalso introduce some features which are able torepresent the combinational effects of thecharacters.
However, such kind of features isvery hard to discover.
Meanwhile, a completeset of combinations is nearly impossible to befound manually due to the exponential numberof all the possible combinations.
As in ourstudy, we adopt DBN to automatically find thecharacter combinations.3.3 Deep Belief NetsDeep Belief Network (DBN) is a complicatedmodel which combines a set of simple modelsthat are sequentially connected (Ackley, 1985).This deep architecture can be viewed as multiplelayers.
In DBN, upper layers are supposed torepresent more ?abstract?
concepts that explainthe input data whereas lower layers extract ?low-level features?
from the data.
DBN often consistsof many layers, including multiple RestrictedBoltzmann Machine (RBM) layers and a BackPropagation (BP) layer.Fig.
2.
The structure of a DBN.As illustrated in Figure 2, when DBN receivesa feature vector, the feature vector is processedfrom the bottom to the top through several RBMlayers in order to get the weights in each RBMlayer, maintaining as many features as possiblewhen they are transferred to the next layer.
RBMdeals with feature vectors only and omits the la-bel information.
It is unsupervised.
In addition,each RBM layer learns its parameters indepen-dently.
This makes the parameters optimal forthe relevant RBM layer but not optimal for thewhole model.
To solve this problem, there is asupervised BP layer on top of the model whichfine-tunes the whole model in the learningprocess and generates the output in the inferenceprocess.
After the processing of all these layers,the final feature vector consists of some sophisti-cated features, which reflect the structured in-formation among the original features.
With thisnew feature vector, the classification perfor-mance is better than directly using the originalfeature vector.None of the RBM is capable of guaranteeingthat all the information conveyed to the output isaccurate or important enough.
However thelearned information produced by preceding RBMlayer will be continuously refined through thenext RBM layer to weaken the wrong or insigni-ficant information in the input.
Each layer candetect feature in the relevant spaces.
Multiplelayers help to detect more features in differentspaces.
Lower layers could support object detec-tion by spotting low-level features indicative ofobject parts.
Conversely, information about ob-jects in the higher layers could resolve lower-level ambiguities.
The units in the final layershare more information from the data.
This in-creases the representation power of the wholemodel.
It is certain that more layers mean morecomputation time.104DBN has some attractive features which makeit very suitable for our problem.1) The unsupervised process can detect thestructures in the input and automatically ob-tain better feature vectors for classification.2) The supervised BP layer can modify thewhole network by back-propagation to im-prove both the feature vectors and the classi-fication results.3) The generative model makes it easy to in-terpret the distributed representations in thedeep hidden layers.4) This is a fast learning algorithm that canfind a fairly good set of parameters quicklyand can ensure the efficiency of DBN.3.3.1 Restricted Boltzmann Machine (RBM)In this section, we will introduce RBM, which isthe core component of DBN.
RBM is BoltzmannMachine with no connection within the samelayer.
An RBM is constructed with one visiblelayer and one hidden layer.
Each visible unit inthe visible layer V  is an observed variable ivwhile each hidden unit in the hidden layer H  isa hidden variablejh.
Its joint distribution is( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ?
??
?
?
(3)In RBM, the parameters that need to be esti-mated are ( , , )W b c?
?
and 2( , ) {0,1}v h ?
.To learn RBM, the optimum parameters areobtained by maximizing the above probability onthe training data (Hinton, 1999).
However, theprobability is indeed very difficult in practicalcalculation.
A traditional way is to find the gra-dient between the initial parameters and the re-spect parameters.
By modifying the previous pa-rameters with the gradient, the expected parame-ters can gradually approximate the target para-meters as0( 1) ( ) ( )WP vW W W ??
?
??
??
?
?
(4)where ?
is a parameter controlling the leaningrate.
It determines the speed of W converging tothe target.Traditionally, the Markov chain Monte Carlomethod (MCMC) is used to calculate this kind ofgradient.0 0log ( , )p v h h v h vw ?
??
?
??
(5)where log ( , )p v h  is the log probability of thedata.0 0h vdenotes the multiplication of the av-erage over the data states and its relevant samplein hidden unit.h v?
?denotes the multiplicationof the average over the model states in visibleunit and its relevant sample in hidden unit.However, MCMC requires estimating an ex-ponential number of terms.
Therefore, it typicallytakes a long time to converge toh v?
?.
Hinton(2002) introduced an alternative algorithm, i.e.,the contrastive divergence (CD) algorithm, as asubstitution.
It is reported that CD can train themodel much more efficiently than MCMC.
Toestimate the distribution ( )p x , CD considers aseries of distributions { ( )np x } which indicate thedistributions in n steps.
It approximates the gapof two different Kullback-Leiler divergences(Kullback, 1987) as0( || ) ( || )n nCD KL p p KL p p?
??
?
(6)Maximizing the log probability of the data isexactly the same as minimizing the Kullback?Leibler divergence between the distribution ofthe data0p  and the equilibrium distribution p?defined by the model.
In each step, the gap isapproximately minimized so that we can obtainthe final distribution which has the smallestKullback-Leiler divergence with the fantasy dis-tribution.After n steps, the gradient can be estimatedand used in Equation 4 to adjust the weights ofRBM.
In our experiments, we set n to be 1.
Itmeans that in each step of gradient calculation,the estimate of the gradient is used to adjust theweight of RBM.
In this case, the estimate of thegradient is just the gap between the products ofthe visual layer and the hidden layer, i.e.,0 0 1 1log ( , )p v h h v h vW?
?
??
(7)Figure 3 below illustrates the process of learningRBM with CD-based gradient estimation.105Fig.
3.
Learning RBM with CD-based gradientestimation3.3.2 Back-propagation (BP)The RBM layers provide an unsupervised analy-sis on the structures of data set.
They automati-cally detect sophisticated feature vectors.
Thelast layer in DBN is the BP layer.
It takes theoutput from the last RBM layer and applies it inthe final supervised learning process.
In DBN,not only is the supervised BP layer used to gen-erate the final categories, but it is also used tofine-tune the whole network.
Specifically speak-ing, when the parameters in BP layer arechanged during its iterating process, the changesare passed to the other RBM layers in a top-to-bottom sequence.The BP algorithm has a feed-forward step anda back-propagation step.
In the feed-forward step,the input values are propagated to obtain the out-put values.
In the back-propagation step, the out-put values are compared to the real category la-bels and used them to modify the parameters ofthe model.
We consider the weightijwwhichindicates the edge pointing from the i-th node inone RBM layer to the j-th node in its upper layer.The computation in feed-forward isi ijo w ,whereio  is the stored output for the unit i. Inthe back-propagation step, we compute the errorE in the upper layers and also the gradient withrespect to this error, i.e.,i ijE o w?
?.
Then theweightijwwill be adjusted by the gradient des-cent.ij i i ji ijEw o oo w?
?
???
?
?
?
??
(8)where ??
is used to control the length of themoving step.3.3.3 DBN-based Entity Mention Categori-zationFor each entity mention, it is represented by thecharacter feature vector as introduced in section3.2 and then fed to DBN.
The training procedurecan be divided into two phases.
The first phase isthe parameter estimation process of the RBMs onall the inputted feature vectors.
When a featurevector is fed to DBN, the first RBM layer isadjusted automatically according to this vector.After the first RBM layer is ready, its outputbecomes the input of the second RBM layer.
Theweights of the second RBM layer are alsoadjusted.
The similar procedure is carried out onall the RBM layers.
Then DBN will operates inthe second phase, the back-propagationalgorithm.
The labeled categories of the entitymention are used to tune the parameters of theBP layer.
Moreover, the changes of the BP layerare also fed back to the RBM layers.
Theprocedure will iterate until the terminatingcondition is met.
It can be a fixed number ofiterations or a pre-given precision threshold.Once the weights of all the layers in DBN areobtained, the estimated model could be used toprediction.Fig.
4.
The mention categorization processof DBNFigure 4 illustrates the classification process ofDBN.
In prediction, for an entity mention e, wefirst calculate its feature vector V(e) and used asthe input of DBN.
V(e) is passed through all thelayers to get the outputs for all RBM layers andlast back-propagation layer.
In the ith RBM layer,the dimensions in the input vector Vinput_i(e) arecombined to yield the dimensions of the nextfeature vector Voutput_i(e) as input of the next layer.After the feature vector V(e) goes through all theRBM layers, it is indeed transformed to anotherfeature vector V?
(e) which consists ofcomplicated combinations of the originalcharacter features and contains rich structuredinformation between the characters.
This featurevector is then fed into the BP layer to get thefinal category c(e).4 Experiments4.1 Experiment SetupIn our experiment, we use the ACE 2004 corpusto evaluate our approach.
The objective of thisstudy is that the correctly detected Chinese entitymentions categorization using DBN from the textand figure out the suitability of DBN on this task.Moreover, an entity mention should belong toone and only one category.106According to the guideline of the ACE04 task,there are five categories for consideration in total,i.e., Person, Organization, Geo-political entity,Location, and Facility.
Moreover, each entitymention is expressed in two forms, i.e., the headand the extent.
For example, ???????
?President Clinton of USA?
is the extent of anentity mention and ???
?Clinton?
is thecorresponding head.
The two phrases both pointto a named entity whose name is Clinton and heis the president of USA.
Here we make the?breakdown?
strategy mentioned in Li et al(2007) that only the entity head is considered togenerate the feature vector, considering that theinformation from the entity head refines thename entity.
Although the entity extent includesmore information, it also brings many noiseswhich may make the learning process muchmore difficult.In our experiments, we test the machinelearning models under a 4-flod cross-validation.All entity mentions are divided into four partsrandomly where three parts are used for trainingand one for test.
In total, 7746 mentions are usedfor training and 2482 mentions are used fortesting at each round.
Precision is chosen as theevaluation criterion, calculated by the proportionof the number of correctly categorized instancesand the number of total instances.
Since all theinstances should be classified, the recall value isequal to the precision value.4.2 Evaluation on Named Entity categoriza-tionFirst of all, we provide some statistics of the dataset.
The distribution of entity mentions in eachcategory is given in table 1.
The size of thecharacter dictionary in the corpus is 1185, sodoes the dimension of each feature vector.Type QuantityPerson 4197Organization 1783Geo-political entity 287Location 3263Facility 399Table 1.
Number of entity mentions in eachcategoryIn the first experiment, we compare theperformance of DBN with some popularclassification algorithms, including SupportVector Machine (labeled by SVM) and atraditional BP neutral network (labeled by NN(BP)).
To implement the models, we use theLibSVM toolkit1 for SVM and the neural neutralnetwork toolbox in Matlab2 for BP.
The DBN inthis experiment includes two RBM layers andone BP layer.
Results of the first experiment aregiven in Table 2.Learning Model PrecisionDBN 91.45%SVM 90.29%NN(BP) 87.23%Table 2.
Performances of the systems withdifferent classification modelsIn this experiment, the DBN has three RBMlayers and one BP layer.
And the numbers ofunits in each RBM layer are 900, 600 and 300respectively.
NN (BP) has the same structure asDBN.
As for SVM, we choose the linear kernelwith the penalty parameter C=1 and set the otherparameters as default after comparing differentkernels and parameters.In the results, DBN achieved betterperformance than both SVM and BP neuralnetwork.
This clearly proved the advantages ofDBN.
The deep architecture of DBN yieldsstronger representation power which makes itable to detect more complicated and efficientfeatures, thus better performance is achieved.In the second experiment, we intend toexamine the performance of DBN with differentnumber of RBM layers, from one RBM layerplus one BP layer to three RBM layers plus oneBP layer.
The amount of the units in the firstRBM layer is set 900 and the amount in thesecond RBM layer is 600, if the second layerexists.
As for the third RBM layers, the amountof units is set to 300.Construction of Neural Network PrecisionThree RBMs and One BP 91.45%Two RBMs and One BP 91.42%One RBM and one BP 91.05%Table 3.
Performance of DBNs with differentnumber s of RBM layersResults in Table 3 show that the performancetends to be better when more RBM layers areincorporated.
More RBM layers do enhance therepresentation power of DBN.
However, it isalso noted that the improvement is not significantfrom two layers to three layers.
The reason may1 available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/2 available athttp://www.mathworks.com/access/helpdesk/help/toolbox/nnet/backprop.html107be that two-RBM DBN already has enoughrepresentation power for modeling this data setand thus one more RBM layer bringsinsignificant improvement.
It is also mentionedin Hinton (2006) that more than three RBMlayers are indeed not necessary.
Anotherimportant result in Table 3 is that the DBN withOne RBM and one BP performs much better thanthe neutral network with only BP in Table 1.This clearly showed the effectiveness of featurecombination by the RBM layer again.As to the amount of units in each RBM layer,it is manually fixed in upper experiments.
Thisnumber certainly affects the representationpower of an RBM layer, consequently therepresentation power of the whole DBN.
In thisset of experiment, we intend to study theeffectiveness of the unit size to the performanceof DBN.
A series of DBNs with only one RBMlayer and different unit numbers for this RBMlayer is evaluated.
The results are provided inTable 4 below.Construction of Neural Network Precisionone RBM(300 units) + one BP 90.61%one RBM(600 units) + one BP 90.69%one RBM(900 units) + one BP 91.05%one RBM(1200 units) + one BP 90.98%one RBM(1500 units) + one BP 90.61%one RBM(1800 units) + one BP 90.57%Table 4.
Performance of One-RBM DBNswith different number of unitsBased on the results, we can see that theperformance is quite stable with different unitnumbers.
But the numbers that are closer to theoriginal feature size seem to be some better.
Thiscould suggest that we should not decrease orincrease the dimension of the vector feature toomuch when casting the vector transformation byRBM layers.Finally, we show the results of the individualcategories.
For each category, the Precision-Recall-F values are provided in table 5, in whichthe F-measure is calculated by2*Precision*Recall-measure= Precision+RecallF(9)Type P R FPerson 91.26% 96.26% 93.70%Organization 89.86% 89.04% 89.45%Location 77.58% 59.21% 76.17%Geo-politicalentity93.60% 91.89% 92.74%Facility 77.43% 63.72% 69.91%Table 5.
Performances of the system on eachcategory5 ConclusionsIn this paper we presented our recent work onapplying a novel machine learning model, theDeep Belief Nets, on Chinese entity mentioncategorization.
It is demonstrated that DBN isvery suitable for character-level mentioncategorization approaches due to its strongrepresentation power and the ability ondiscovering complicated feature combinations.We conducted a series of experiments to provethe benefits of DBN.
Experimental resultsclearly showed the advantages of DBN that itobtained better performance than existingapproaches such as SVM and traditional BPneutral network.ReferencesDavid Ackley, Geoffrey Hinton, and TerrenceSejnowski.
1985.
A learning algorithm forBoltzmann machines.
Cognitive Science.
9.David MacDonald.
1993.
Internal and externalevidence in the identification and semanticcategorization of proper names.
CorpusProcessing for Lexical Acquisition, MIT Press, 61-76.Geoffrey Hinton.
1999.
Products of experts.
InProceedings of the Ninth International.Conference on Artificial Neural Networks(ICANN).
Vol.
1, 1?6.Geoffrey Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
NeuralComputation, 14, 1771?1800.Geoffrey Hinton, Simon Osindero, and Yee-WheyTeh.
2006.
A fast learning algorithm for deepbelief nets.
Neural Computation.
18, 1527?1554 .GuoDong Zhou and Jian Su.
2002.
Named entityrecognition using an hmm-based chunk tagger.
Inproceedings of ACL.
473-480.Hideki Isozaki and Hideto Kazawa.
2002.
Efficientsupport vector classifiers for named entityrecognition.
In proceedings of IJCNLP.
1-7.Honglei Guo, Jianmin Jiang, Guang Hu and TongZhang.
2005.
Chinese named entity recognitionbased on multilevel linguistics features.
Inpr ceedings of IJCNLP.
90-99.Jing, Hongyan, Radu Florian, Xiaoqiang Luo, TongZhang and Abraham Ittycheriah.
2003.
How to geta Chinese name (entity): Segmentation andcombination issues.
In proceedings of EMNLP.200-207.Koen Deschacht and Marie-Francine Moens.
2006,Efficient Hierarchical Entity Classifier UsingConditional Random Field.
In Proceedings of the1082nd Workshop on Ontology Learning andPopulation.
33-40.Michael Collins and Yoram Singer.
1999.Unsupervised models for named entityclassification.
In Proceedings of EMNLP'99.Nina Wacholder, Yael Ravin and Misook Choi.
1997.Disambiguation of Proper Names in Text.
InProceedings of the Fifth Conference on AppliedNatural Language Processing.Solomon Kullback.
1987.
Letter to the Editor: TheKullback-Leibler distance.
The AmericanStatistician 41 (4): 340?341.Wenjie Li and Donglei Qian.
2007.
Detecting,Categorizing and Clustering Entity Mentions inChinese Text, in Proceedings of the 30th AnnualInternational ACM SIGIR Conference (SIGIR?07).647-654.Yoshua Bengio and Yann LeCun.
2007.
Scalinglearning algorithms towards ai.
Large-Scale Ker-nel Machines.
MIT Press.109
