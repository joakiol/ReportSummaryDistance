A fast  par t ia l  parse  of  natura l  language sentencesus ing  a connect ion is t  methodCaroline LyonDivision of Computer ScienceUniversity of HertfordshireHatfield ALl0 9AB, UKcomrcml@herts, ac.
ukBob DickersonDivision of Computer ScienceUniversity of HertfordshireHatfield ALl0 9AB, UKcomqrgd@hert s. ac.
ukAbstractThe pattern matching capabilities ofneural networks can be used to loc-ate syntactic onstituents of natural an-guage.
This paper describes a fully auto-mated hybrid system, using neural netsoperating within a grammatic frame-work.
It addresses the representationof language for connectionist processing,and describes methods of constrainingthe problem size.
The function of thenetwork is briefly explained, and resultsare given.1 IntroductionThe pattern matching capabilities of neural net-works can be used to detect syntactic onstituentsof natural anguage.
This approach bears compar-ison with probabilistic systems, but has the ad-vantage that negative as well as positive inform-ation can be modelled.
Also, most computationis done in advance, when the nets are trained,so the run time computational load is low.
Inthis work neural networks are used as part of afully automated system that finds a partial parseof declarative sentences.
The connectionist pro-cessors operate within a grammatic framework,and are supported by pre-processors that filter thedata and reduce the problem to a computation-ally tractable size.
A prototype can be accessedvia the Internet, on which users can try their owntext (details from the authors).
It will take a sen-tence, locate the subject and then find the head ofthe subject.
Typically 10 sentences take about 2seconds, 50 sentences about 4 seconds, to processon a Sparcl0 workstation.
Using the prototype ontechnical manuals the subject and its head can bedetected in over 90% of cases (See Section 7).The well known complexity of parsing is ad-dressed by decomposing the problem, and thenlocating one syntactic onstituent at a time.
Thesentence is first decomposed into the broad syn-tactic categoriespre-subject - subject - predicateby locating the subject.
Then these constituentscan be processed further.
The underlying prin-ciple employed at each step is to take a sentence,or part of a sentence, and generate strings withthe boundary markers of the syntactic onstituentin question placed in all possible positions.
Thena neural net selects the string with the correctplacement.This paper gives an overview of how naturallanguage is converted to a representation that theneural nets can handle, and how the problem isreduced to a manageable size.
It then outlinesthe neural net selection process.
A comprehensiveaccount is given in (Lyon, 1994); descriptions ofthe neural net process are also in (Lyon, 1993;Lyon and Frank, 1992).
This is a hybrid sys-tem.
The core process is data driven, as the para-meters of the neural networks are derived fromtraining text.
The neural net is trained in super-vised mode on examples that have been manuallymarked "correct" and "incorrect".
It will then beable to classify unseen examples.
However, the ini-tial processing stages, in which the problem size isconstrained, operate within a skeletal grammaticframework.
Computational tractability is furtheraddressed by reducing data through the applica-tion of prohibitive rules as local constraints.
Thepruning process is remarkably effective.2 The corpus of sentences fromtechnical manualsThis work has principally been developed on textof technical manuals from Perkins Engines Ltd.,which have been translated by a semi-automaticprocess (Pyre, 1993).
Now, a partial parse cansupport such a process.
For instance, frequentlyoccurring modal verbs such as "must" are not dis-21520181614121086420Number ofoccurrences1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Number of words before the subject Number of words in the subject8072645648403224168Figure I: The frequency of constituent length for pre-subject and subject in 351 sentencestinguished by number in English, but they are inmany other languages.
It is necessary to locatethe subject, then identify the head and determ-ine its number in order to translate the main verbcorrectly in sentences like (1) below.If a cooler is fitted to the geaxl~ox, \[ the pipe\[ connections \] of the cooler \] must be regu-laxly checked for corrosion.
(1)This parser has been trained to find the syntacticsubject head that agrees in number with the mainverb.
The manuals are written using the PACE(Perkins Approved Clear English) guidelines, withthe aim of producing clear, unambiguous texts.AlI declarative sentences have been extracted forprocessing: about half were imperatives.
Thislevel of classification can be done automaticallyin future.
Table 1 and Figure 1 show some of thecharacteristics of the corpus.Number of sentences 351Average length 17.98 wordsNo.
of subordinate clausesIn pre-subject 65In subject 19In predicate 136Co-ordinated clauses 50Punctuation marks are counted as words, formulae as1 word.Table 1: Corpus statistics3 Language representation (I)In order to reconcile computational feasibility toempirical realism an appropriate form of languagerepresentation is critical.
The first step in con-straining the problem size is to partition an unlim-ited vocabulary into a restricted number of part-of-speech tags.
Different stages of processing placedifferent requirements on the classification system,so customised tagsets have been developed.
Forthe first processing stage we need to place the sub-ject markers, and, as a further task, disambiguatetags.
It was not found necessary to use numberinformation at this stage.
For example, considerthe sentence:Still waters run deep.
(2)The word "waters" could be a 3rd person, singu-lar, present verb or a plural noun.
However, inorder to disambiguate the tag and place the sub-ject markers it is only necessary to know that it isa noun or else a verb.
The sentence parsed at thefirst level returns:\[ Still waters \] run deep.
(2.1)The tagset used at this stage, mode 1, has 21classes, not distinguished for number.
However,the head of the subject is then found and num-ber agreement with the verb can be assessed.
Atthis stage the tagset, mode 2, includes number in-formation and has 28 classes.
Devising optimaltagsets for given tasks is a field in which furtherwork is planned.
We need larger tagsets to cap-ture more linguistic information, but smaller onesto constrain the computational load.
Informationtheoretic tools can be used to find the entropy ofdifferent ag sequence languages, and support de-cisions on representation.A functional approach is taken to tagging:words are allocated to classes depending on theirsyntactic role.
For instance, superlative adjectivescan act as nouns, so they are initially given the 2tags: noun or adjective.
This approach can be ex-tended by taking adjacent words which act jointlyas single lexical items as a unit.
Thus the pair216"most <adjective>" is taken as a single superlat-ive adjective.Text is automatically tagged using the firstmodules of the CLAWS program (1985 version), inwhich words are allocated one or more tags from134 classes (Garside, 1987).
These 134 tags arethen mapped onto the small customised tagsets.Tag disambiguation is part of the parsing task,handled by the neural net and its pre-processor.This version of CLAWS has a dictionary of about6,300 words only.
Other words are tagged usingsuffix information, or else defaults are invoked.The correct tag is almost always included in theset alocated, but more tags than necessary are of-ten proposed.
A larger dictionary in later versionswill address this problem.Represent ing syntact ic boundary markersIn the same way that tags are allocated to words,or to punctuation marks, they can represent theboundaries of syntactic onstituents, uch as nounphrases and verb phrases.
Boundary markers canbe considered invisible tags, or hypertags, whichhave probabilistic relationships with adjacent tagsin the same way that words do.
(Atwell, 1987)and (Church, 1989) have used this approach.
Ifembedded syntactic onstituents are sought in asingle pass, this can lead to computational over-load (Pocock and Atwell, 1994).
Our approachuses a similar concept, but differs in that embed-ded syntactic constituents are detected one at atime in separate steps.
There are only 2 hyper-tags - the opening and closing brackets markingthe possible location(s) of the syntactic onstitu-ent in question.
Using this representation a hier-archical language structure is converted to a stringof tags represented by a linear vector.4 Constraining the generation ofcandidate stringsThis system generates sets of tag strings for eachsentence, with the hypertags placed in all possiblepositions.
Thus, for the subject detection task:Then the performance of the pump must bemonitored.
(3)will generate strings of tags including:\[ Then \] the performance of the pump mustbe monitored.
(3.1)\[ Then the \] performance of the pump mustbe monitored.
(3.2)Then \[ the performance of the \] pump mustbe monitored.
(3.n)Then \[ the performance of the pump \] mustbe monitored.
(3.n + 1)Hypertags are always inserted in pairs, so thatclosure is enforced.
There were arbitrary limits ofa maximum of 10 words in the pre-subject and 10words within the subject for the initial work de-scribed here.
These are now extended to 15 wordsin the pre-subject, 12 in the subject - see Section7.
There must be at least one word beyond theend of the subject and before the end-of-sentencemark.
Therefore, using the initial restrictions, ina sentence of 22 words or more (counting punc-tuation marks as words) there could be 100 al-ternative placements.
However, some words willhave more than one possible tag.
For instance,in sentence (1) above 5 words have 2 alternativetags, which will generate 25 possible strings be-fore the hypertags are inserted.
Since there are22 words (including punctuation ) the total num-ber of strings would be 25 * 100 -- 3200.
It is notfeasible to detect one string out of this number: ifthe classifier marked all strings incorrect he per-centage wrongly classified would only be 0.03%,yet it would be quite useless.
In order to find thecorrect string most of the outside candidates mustbe dropped,The skeletal grammatic frameworkA minimal grammar, set out in (Lyon, 1994) inEBNF form, is composed of 9 rules.
For instance,the subject must contain a noun-type word.
Ap-plying this particular ule to sentence (3) abovewould eliminate candidate strings (3.1) and (3.2).We also have the 2 arbitrary limits on length ofpre-subject and subject.
There is a small set of4 extensions to the grammar, or semi-local con-straints.
For instance, if a relative pronoun oc-curs, then a verb must follow in that constituent.On the technical manuals the constraints of thegrammatic framework put up to 6% of declarativesentences outside our system, most commonly be-cause the pre-subject is too long.
A small numberare excluded because the system cannot handle aco-ordinated head.
With the length of pre-subjectextended to 15 words, and subject o 12 words, anaverage of 2% are excluded (7 out of 351).Prohib i t ion tablesThe grammatic framework alone does not reducethe number of candidate strings sufficiently forthe subject detection stage.
This problem is ad-dressed further by a method suggested by Bar-ton et al (Barton, Berwick and Ristad, 1987)that local constraints can rein in the generationof an intractable number of possibilities.
In oursystem the local constraints are prohibited tagpairs and triples.
These are adjacent ags whichare not allowed, such as "determiner - verb" or217INPUT SENTENCEgenerate input 1 prune ~mode l tags ~ find subject _ j  i ne a, e wo l~enten~ ~generate inp~( fin:e2:~dn::::~J:gtmode 2tags ~ KEY with numgerO processI I data store))- - )  neural networkHEAD OF SUBJECT SUBJECTFOUND FOUNDFigure 2: Overview of the syntactic pattern recognition process"start of subject - verb".
If during the genera-tion of a candidate string a prohibited tuple is en-countered, then the process is aborted.
There areabout 100 prohibited pairs and 120 triples.
By us-ing these methods the number of candidate stringsis drastically reduced.
For the technical manu-als an average of 4 strings, seldom more than 15strings, are left.
Around 25% of sentences are leftwith a single string.
These filters or "rules" differfundamentally from generative rules that produceallowable strings in a language.
In those casesonly productions that are explicitly admitted areallowed.
Here, in contrast, anything that is notexpressly prohibited is allowed.
At this stage thedata is ready to present o the neural net.
Figure2 gives an overview of the whole process.5 Language representation (II)Different network architectures have been invest-igated, but they all share the same input and out-put representation.
The output from the net is avector whose 2 elements, or nodes, represent "cor-rect" and "incorrect", "yes" and "no" - see Figure3.
The input to the net is derived from the candid-ate strings, the sequences of tags and hypertags.These must be converted to binary vectors.
Eachelement of the vector will represent a feature thatis flagged 0 or 1, absent or present.Though the form in which the vector is writtenmay give an illusion of representing order, no se-quential order is maintained.
A method of repres-enting a sequence must be chosen.
The sequentialorder of the input is captured here, partially, bytaking adjacent ags, pairs and triples, as the fea-ture elements.
The individual tags are convertedto a bipos and tripos representation.
Using thismethod each tag is in 3 tripos and 2 bipos ele-ments.
This highly redundant code will aid theprocessing of sparse data typical of natural lan-guage.For most of the work described here the sen-tence was dynamically truncated 2 words beyondthe hypertag marking the close of the subject.This process has now been improved by going fur-ther along the sentence.6 The function of the netThe net that gave best results was a simple singlelayer net (Figure 3), derived from Wyard andNightingale's Hodyne net (Wyard and Nightin-gale, 1990).
This is conventionally a "single layer"net, since there is one layer of processing nodes.Multi-layer networks, which can process linearlyinseparable data, were also investigated, but arenot necessary for this particular processing task.The linear separability of data is related to itsorder, and this system uses higher order pairsand triples as input.
The question of appropriatenetwork architecture is examined in (Pao, 1989;Widrow and Lehr, 1992; Lyon, 1994).218yes(noun,verb,noun) (adj,noun) ( ' \ [ '  det).
.
.
.
.
.
.
.
.
.
.
.
output nodes.
weighted links~ input nodes('\[' prep)'\[' represents he start of the subject.
The node ( '\[' determiner) would occuroften in both correct and incorrect strings.
The node ('\[' preposition) would notoccur in a correct string, so it is not connected tothe "yes" output node.2~ represents summing function.Figure 3: The single layer net: showing the feed forward processThe training processThe net is presented with training strings whosedesired classification has been manually marked.The weights on the connections between input andoutput nodes are adjusted until a required levelof performance is reached.
Then the weights arefixed and the trained net is ready to classify un-seen sentences.
The prototype accessible via theInternet has been trained on sentences from thetechnical manuals, slightly augmented.Initially the weighted links are disabled.
Whena string is presented to the network in trainingmode, it activates a set of input nodes.
If aninput node is not already linked to the outputnode representing the desired response, it will beconnected and the weight on the connection willbe initialised to 1.0.
Most input nodes are con-nected to both outputs, since most tuples occurin both grammatical and ungrammatical strings.However, some will only be connected to one out-put - see Figure 3.The input layer potentially has a node for eachpossible tuple.
With 28 tags, 2 hypertags and astart symbol the upper bound on the number ofinput nodes is 313 + 312.
In practice the max-imum activated is currently about 1000.
In test-ing mode, if a previously unseen tuple appears itmakes zero contribution to the result.
The activ-ations at the input layer are fed forward throughthe weighted connections to the output nodes;where they are summed.
The highest outputmarks the winning node.
If the desired node wins,then no action is taken.
If the desired node doesnot win, then the weight on connections to the de-sired node are incremented, while the weights onconnections to the unwanted node are decremen-ted.This algorithm differs from some commonlyused methods.
In feed forward networks trainedin supervised mode to perform a classification taskdifferent penalty measures can be used to trigger aweight update.
Back propagation and some singlelayer training methods typically minimise a met-ric based on the least squared error (LSE) betweendesired and actual activation of the output nodes.The reason why a differentiable error measure ofthis sort is necessary for multi-layer nets is welldocumented, for example see (Rumelhart and Mc-Clelland, 1986).
However, for single layer nets wecan choose to update weights directly: the error atan output node can trigger weight updates on theconnections that feed it.
Solutions with LSE arenot necessarily the same as minimising the num-ber of misclassifications, and for certain types ofdata this second method of direct training may beappropriate.
Now, in the natural anguage domainit is desirable to get information from infrequentas well as common events.
Rare events, ratherthan being noise, can make a useful contributionto a classification task.
We need a method thatcaptures information from infrequent events, andadopt a direct measure of misclassification.
This219may be better suited to data with a "Zipfian" dis-tribution (Shannon, 1951).The update factor is chosen to meet several re-quirements.
It should always be positive, andasymptotic to maximum and minimum bounds.The factor should be greatest in the central re-gion, least as it moves away in either direction.We are currently still using the original Hodynefunction because it works well in practice.
Theupdate factor is given in the following formula.
If5-- +1 for strengthening weights and ~ = -1 forweakening them, thenwn~,o = l + l + (~ , Wold) 4 wo~dRecall that weights are initialised to 1.0.
Aftertraining we find that the weight range is boundedby10 -3 < w < 5.0Total time for training is measured in seconds.The number of iterative cycles that are necessarydepends on the threshold chosen for the trainednet to cross, and on details of the vector epresent-ation.
The demonstration prototype takes about15 seconds.
With the most recent improved rep-resentation about 1000 strings can be trained in 1second, to 97%.
The results from using these netsare given in Table 3.
It was found that triplesalone gave as good results as pairs and triples to-gether.
And though the nets easily train to 99%correct, the lower threshold gives slightly bettergeneralisation and thus gives better esults on thetest data.The  tes t ing  processWhen the trained net is run on unseen data theweights on the links are fixed.
Any link that isstill disabled is activated and initialised to 0, sothat tuples which have not occurred in the train-ing corpus make no contribution to the classific-ation task.
Sentences are put through the pre-processer one at a time and the candidate stringswhich are generated are then presented to the net-work.
The output is now interpreted ifferently.The difference between the "yes" and "no" activ-ation levels is recorded for each string, and thisscore is considered a measure of grammaticality,P.
The string with the highest I" score is taken asthe correct one.For the results given below, the networks weretrained on part of the corpus and tested on an-other part of the corpus.
For the prototype inwhich users can process their own text, the netwas trained on the whole corpus, slightly augmen-ted.7 ResultsThere are several measures of correctness that canbe taken when results are evaluated.
The mostlenient is whether or not the subject and headmarkers are placed correctly - the type of measureused in the IBM/Lancaster work (Black, Garsideand Leech, 1993).
Since we are working towards ahierarchical language structure, we may want thewords within constituents correctly tagged, readyfor the next stage of processing.
"correct- A" alsorequires that the words within the subject are cor-rectly tagged.
The results in Tables 2 and 3 givean indication of performance l vels.8 Using negative informationWhen parses are postulated for a sentence negat-ive as well as positive examples are likely to occur.Now, in natural anguage negative correlations arean important source of information: the occur-rence of some words or groups of words inhibitothers from following.
We wish to exploit theseconstraints.
(Brill et al , 1990) recognised this,and introduced the idea of distituents.
These areelements of a sentence that should be separated,as opposed to elements of const i tuents that clingtogether.
Brill addresses the problem of findinga valid metric for distituency by using a gener-alized mutual information statistic.
Distituencyis marked by a mutual information minima.
Hismethod is supported by a small 4 rule grammar.However, this approach does not fully capturethe sense in which inhibitory factors play a neg-ative and not just a neutral role.
We want to dis-tinguish between items that are unlikely to occurever, and those that have just not happened toturn up in the training data.
For example, in sen-tence (3) above strings 3.1, 3.2 and 3.n can neverbe correct.
These should be distinguished frompossibly correct parses that are not in the train-ing data.
In order that "improbabilities" can bemodelled by inhibitory connections (Niles and Sil-verman, 1990) show how a Hidden Markov Modelcan be implemented by a neural network.The theoretical ground for incorporating negat-ive examples in a language learning process ori-ginates with Gold's work (Gold, 1967; Angluin,1980).
He examined the process of learning thegrammar of a formal anguage from examples.
Heshowed that, for languages at least as high in theChomsky hierarchy as CFGs, inference from pos-itive data alone is strictly less powerful than in-ference from both positive and negative data to-gether.
To illustrate this informally consider acase of inference from a number of examples: asthey are presented to the inference machine, pos-220no.
of no.
of % sents with % sents % sents withtraining sents, test sents, subject correct subject and headfound measure A found220 42 100 100 95198 63 97 97 90204 58 95 95 93276 50 94 94Table 2: Performance on text from Perkins manuals after 6% sentences have been excludedno.
of no.
of % sents with % sents % sents withtraining seats, test sents, subject correct subject and headfound measure A found t309 42 100 97.6 97.6288 63 98.4 96.8 96.8292 59 98.3 98.3 96.6284 67 94.0 94.0 94.0Table 3: Performance on text from Perkins manuals, using improved representation a d larger trainingset, after 2% sentences have been excludedsible grammars are postulated.
However, withpositive data alone a problem of over generaliz-ation arises: the postulated grammar may be asuperset of the real grammar, and sentences thatare outside the real grammar could be accepted.If both positive and negative data is used, counterexamples will reduce the postulated grammar sothat it is nearer the real grammar.
Gold developedhis theory for formal languages: it is argued thatsimilar considerations apply here.
A grammarmay be inferred from positive examples alone forcertain subsets of regular languages (Garcia andVidal, 1990), or an inference process may degen-erate into a look up procedure if every possiblepositive example is stored.
In these cases negat-ive information is not required, but they are notplausible models for unbounded natural anguage.In our method the required parse is found by infer-ring the grammar from both positive and negativeinformation, which is effectively modelled by theneural net.
~-hture work will investigate the effectof training the networks on the positive examplesalone.
With our current size corpus there is notenough data.Re la t ionsh ip  between the neura l  net  andproh ib i t ion  tab leThe relationship between the neural net and therules in the prohibition table should be seen inthe following way.
Any single rule prohibitinga tuple of adjacent tags could be omitted andthe neural network would handle it by linkingthe node representing that tuple to "no" only.However, for some processing steps we need to re-duce the number of candidate tag strings presen-ted to the neural network to manageable propor-tions (see Section 4).
The data must be pre-processed by filtering through the prohibition ruleconstraints.
If the number of candidate strings iswithin desirable bounds, such as for the head de-tection task, no rules are used.
Our system is datadriven as far as possible: the rules are invoked ifthey are needed to make the problem computa-tionally tractable.9 Conc lus ionOur working prototype indicates that the methodsdescribed here are worth developing, and that con-nectionist methods can be used to generalise fromthe training corpus to unseen text.
Since datacan be represented as higher-order tuples, singlelayer networks can be used.
The traditional prob-lems of training times do not arise.
We have alsoused multi-layer nets on this data: they have noadvantages, and perform slightly less well (Lyon,1994).The supporting role of the grammatic frame-work and the prohibition filters should not be un-derestimated.
Whenever the scope of the systemis extended it has been found necessary to enhancethese elements.The most laborious part of this work is prepar-ing the training data.
Each time the representa-tion is modified a new set of strings is generatedthat need marking up.
An autodidactic heck isnow included which speeds up this task.
We runmarked up training data through an early version221of the network trained on the same data, so theresults should be almost all correct.
If an "incor-rect" parse occurs we can then check whether thatsentence was properly marked up.Some of the features of the system describedhere could be used in a stochastic process.However, connectionist methods have low compu-tational loads at runtime.
Moreover, they can util-ise more of the implicit information i the trainingdata by modelling negative relationships.
This isa powerful concept that can be exploited in the ef-fort to squeeze out every available piece of usefulinformation for natural language processing.Future work is planned to extend this very lim-ited partial parser, and decompose sentences fur-ther into their hierarchical constituent parts.
Inorder to do this a number of subsidiary tasks willbe addressed.
The system is being improved byidentifying roups of words that act as single lex-ical items.
The decomposition f the problem canbe investigated further: for instance, should thetag disambiguation task precede the placement ofthe subject boundary markers i.n a separate step?More detailed investigation oflanguage represent-ation issues will be undertaken.
And the criticalissues of investigating the most appropriate net-work architectures will be carried on.Trans.
on Pattern Analysis and Machine Intel-ligence, 12.R Garside.
1987.
The CLAWS word-tagging sys-tem.
In R Garside, G Leech, and G Sampson,editors, The Computational Analysis of English:a corpus based approach.
Longman.E M Gold.
1967.
Language identification i thelimit.
Information and Control, 10.CCLyon.
1994.
The representation ofnatural lan-guage to enable neural networks to detect syn-tactic features.
PhD thesis.Re ferencesD Angluin.
1980.
Inductive inference of formallanguages from positive data.
Information andControl, 45. pE Atwell.
1987.
Constituent-likelihood grammar.In 1~ Garside, G Leech, and G Sampson, edit-ors, The Computational Analysis of English: acorpus-based approach.
Longman.G E Barton, R C Berwick, and E S Ristad.1987.
Computational Complexity and NaturalLanguage.
MIT Press.E Black, R Garside, and G Leech.
1993.
Statistic-ally driven computer grammars of English: theIBM/Lancaster approach.
R0dopi.
BE Brill, D Magerman, M Marcus and B Santorini.1990.
Deducing linguistic structure from thestatistics of large corpora.
In DARPA Speechand Natural Language Workshop.
PK W Church, Bell Laboratories.
1989.
Astochastic parts program and noun phraseparser for unrestricted text.
In IEEE confer-ence record of ICASSP.P Garcia and E Vidal.
1990.
Inference of k-testable languages in the strict sense and applic-ation to syntactic pattern recognition.
IEEELyon I993.
Using neural networks to infergrammatical structures in natural language.
InProe.
of IEE Colloquium on Grammatical In-~erence.C Lyon and R Frank 1992.
Detecting structuresin natural language using a neural net withrules.
In Proc.
of International Conference onArtificial Neural Networks (ICANN).L Niles and H Silverman.
1990.
Combining Hid-den Markov Models and Neural Network Clas-sifters.
In IEEE conference r cord of ICASSP.Yoh-Han Pao.
1989.
Adaptive Pattern Recogni-tion and Neural Networks.
Addison Wesley.R Pocock and E Atwell.
1994.
Treebank trainedprobabilistic parsing of lattices.
School of Com-puter Studies, Leeds University.
In The Speech-Oriented Probabilistic Parser Project: FinalReport to MoD.Pym.
1993.
Perkins Engines and Publications.In Proceedings of Technology and Language inEurope 2000.
DGXIII-E of the European Com-mission.D Rumelhart and J McClelland.
1986.
ParallelDistributed Processing MIT.C E Shannon 1951.
Prediction and Entropyof Printed English.
In Bell System TechnicalJournal.Widrow and M Lehr.
1992.
30 years of adaptiveneural networks.
In Neural networks: theoret-ical foundations and analysis edited by CLau.IEEE press.Wyard and C Nightingale.
1990.
A Single LayerHigher Order Neural Net and its Application toContext Free Grammar Recognition In Con-nection Science, 4.222
