A Robust and Efficient Parser for Non-Canonical InputsPhilippe BlacheCNRS & Universit?
de Provence29, Avenue Robert Schuman13621 Aix-en-Provence, Francepb@lpl.univ-aix.frAbstractWe present in this paper a parser relyingon a constraint-based formalism calledProperty Grammar.
We show how con-straints constitute an efficient solution inparsing non canonical material such asspoken language transcription or e-mails.This technique, provided that it is imple-mented with some control mechanisms,is very efficient.
Some results are pre-sented, from the French parsing evalua-tion campaign EASy.1 IntroductionParsing spoken languages and non canonical in-puts remains a challenge for NLP systems.
Manydifferent solutions have been experimented, de-pending on the kind of material to be parsed orthe kind of application: in some cases, superficialinformation such as bracketing is enoughwhereas in other situations, the system needsmore details.
The question of robustness, andmore generally the parsing strategy, is addresseddifferently according to these parameters.
Classi-cally, three families of solutions are proposed:- Reducing the complexity of the output- Controlling the parsing strategy- Training and adapting the system to thetype of inputIn the first case, the idea consists in buildingstructures with little information, even under-specified (which means the possibility of build-ing partial structures).
We find in this family thedifferent shallow parsing techniques (see for ex-ample [Hindle83], [Abney96]).
Unsurprisingly,the use of statistical methods is very frequent andefficient in this kind of application (see [TjongKim Sang00] for some results of a comparisonbetween different shallow parsers).
Generally,such parsers (being them symbolic or not) aredeterministic and build non recursive units.
Insome cases, they can also determine relationsbetween units.The second family contains many different tech-niques.
The goal is to control a given parsingstrategy by means of different mechanisms.Among them, we can underline three proposals:- Implementing recovering mechanisms,triggering specific treatments in case oferror (cf.
[Boulier05])- Controlling the parsing process bymeans of probabilistic information (cf.
[Johnson98])- Controlling deep parsers by means ofshallow parsing techniques (cf.
[Crys-mann02], [UszKoreit02], [Marimon02])The last kind of control mechanism consists inadapting the system to the material to be parsed.This can be done in different ways:- Adding specific information in order toreduce the search space of the parsingprocess.
This kind of information canappear under the form of ad hoc rules orinformation depending on the kind ofdata to be treated.- Adapting the resources (lexicon, gram-mars) to the linguistic materialThese different strategies offer several advan-tages and some of them can be used together.Their interest is that the related questions of ro-bustness and efficiency are both taken into ac-count.
However, they do not constitute a generic19solution in the sense that something has to bemodified either in the goal, in the formalism or inthe process.
In other words, they constitute anadditional mechanism to be plugged into a givenframework.We propose in this paper a parsing techniquerelying on a constraint-based framework beingboth efficient and robust without need to modifythe underlying formalism or the process.
Thenotion of constraints is used in many differentways in NLP systems.
They can be a very basicfiltering process as proposed by ConstraintGrammars (see [Karlsson90]) or can be part toan actual theory as with HPSG (see [Sag03]), theOptimality Theory (see [Prince03]) or ConstraintDependency Grammars (cf.
[Maruyama90]).
Ourapproach is very different: all information is rep-resented by means of constraints; they do notstipulate requirements on the syntactic structure(as in the above cited approaches) but representdirectly syntactic knowledge.
In this approach,robustness is intrinsic to the formalism in thesense that what is built is not a structure of theinput (for example under the form of a tree) but adescription of its properties.
The parsing mecha-nism can then be seen as a satisfaction processinstead of a derivational one.
Moreover, it be-comes possible, whatever the form of the input,to give its characterization.
The technique relieson constraint relaxation and is controlled bymeans of a simple left-corner strategy.
One of itsinterests is that, on top of its efficiency, the sameresources and the same parsing technique is usedwhatever the input.After a presentation of the formalism and theparsing scheme, we describe an evaluation of thesystem for the treatment of spoken language.This evaluation has been done for French duringthe evaluation campaign Easy.2 Property Grammars: a constraint-based formalismWe present in this section the formalism of Prop-erty Grammars (see [B?s99] for preliminaryideas, and [Blache00], [Blache05] for a presenta-tion).
The main characteristics of PropertyGrammars (noted hereafter PG), is that all infor-mation is represented by means of constraints.Moreover, grammaticality does not constitute thecore question but become a side effect of a moregeneral notion called characterization: an input isnot associated to a syntactic structure, but de-scribed with its syntactic properties.PG makes it possible to represent syntactic in-formation in a decentralized way and at differentlevels.
Instead of using sub-trees as with classicalgenerative approaches, PG specifies directly con-straints on features, categories or set of catego-ries, independently of the structure to which theyare supposed to belong.
This characteristic isfundamental in dealing with partial, underspeci-fied or non canonical data.
It is then possible tostipulate relations between two objects, inde-pendently from their position in the input or intoa structure.
The description of the syntactic prop-erties of an input can then be done very pre-cisely, including the case of non canonical or nongrammatical input.
We give in the remaining ofthe section a brief overview of GP characteristicsAll syntactic information is represented in PG bymeans of constraints (also called properties).They stipulate different kinds of relation betweencategories such as linear precedence, imperativeco-occurrence, dependency, repetition, etc.
Thereis a limited number of types of properties.
In thetechnique described here, we use the followingones:- Linear precedence: Det < N (a determinerprecedes the noun)- Dependency: AP ?
N (an adjectival phrasedepends on the noun)- Requirement: V[inf] ?
to (an infinitivecomes with to)- Exclusion: seems ?
ThatClause[subj] (theverb seems cannot have That clause subjects)- Uniqueness : UniqNP{Det}(the determiner isunique in a NP)- Obligation : ObligNP{N, Pro}(a pronoun or anoun is mandatory in a NP)This list can be completed according to the needsor the language to be parsed.
In this formalism, acategory, whatever its level is described with aset of properties, all of them being at the samelevel and none having to be verified before an-other.Parsing a sentence in PG consists in verifying foreach category the set of corresponding propertiesin the grammar.
More precisely, the idea consistsin verifying for each constituent subset its rele-vant constraints (i.e.
the one applying to the ele-20ments of the subset).
Some of these propertiescan be satisfied, some other can be violated.
Theresult of this evaluation, for a category, is a set ofproperties together with their evaluation.
We callsuch set the characterization of the category.Such an approach makes it possible to describeany kind of input.Such flexibility has however a cost: parsing inPG is exponential (cf.
[VanRullen05]).
Thiscomplexity comes from several sources.
First,this approach offers the possibility to consider allcategories, independently from its correspondingposition in the input, as possible constituent foranother category.
This makes it possible for ex-ample to take into account long distance or nonprojective dependencies between two units.Moreover, parsing non canonical utterances re-lies on the possibility of building characteriza-tions with satisfied and violated constraints.
Interms of implementation, a property being a con-straint, this means the necessity to propose aconstraint relaxation technique.
Constraint re-laxation and discontinuity are the main complex-ity factors of the PG parsing problem.
The tech-nique describe in the next section propose to con-trol these aspects.3 Parsing in PGBefore a description of the controlled parsingtechnique proposed here, we first present thegeneral parsing schemata in PG.
The processconsists in building the list of all possible sets ofcategories that are potentially constituents of asyntactic unit (also called constructions).
A char-acterization is built for each of this set.
Insofar asconstructions can be discontinuous, it is neces-sary to build all possible combinations of catego-ries, in other words, the subsets set of the catego-ries corresponding to the input to be parsed,starting from the lexical categories.
We call as-signment such a subset.
All assignments havethen, theoretically, to be evaluated with respectto the grammar.
This means, for each assign-ment, traversing the constraint system and evalu-ating all relevant constraints (i.e.
constraints in-volving categories belonging to the assignment).For some assignments, no property is relevantand the corresponding characterization is theempty set: we say in this case that the assignmentin non productive.
In other cases, the characteri-zation is formed with all the evaluated properties,whatever their status (satisfied or not).
At thefirst stage, all constructions contain only lexicalcategories, as in the following example:Construction Assignment CharacterizationAP  {Adv, Adj} {Adv < Adj; Adv ?
Adj;...}NP {Det, N} {Det < N; Det ?
N; N ?Pro; ...}An assignment with a productive characteriza-tion entails the instantiation of the constructionas a new category; added to the set of categories.In the previous examples, AP and NP are thenadded to the initial set of lexical categories.
Anew set of assignments is then built, includingthese new categories as possible constituents,making it possible to identify new constructions.This general mechanism can be summarized asfollows:Initialization?
word at a position i:create the set ci of its possiblecategoriesK ?
{ci | 1<i<number of words}S ?
set of subsets of KRepeat?
Si ?
Sif Si  is a productive assignmentadd ki the characterizationlabel to KS ?
set of subsets of KUntil new characterization are builtThis parsing process underlines the complexitycoming from the number of assignments to betaken into account: this set has to be rebuilt ateach step (i.e.
when a new construction isadded).As explained above, each assignment has to beevaluated.
This process comes to build a charac-terization formed by the set of its relevant prop-erties.
A property p is relevant for an assignmentA when A contains categories involved in theevaluation of p. In the case of unary propertiesconstraining a category c, the relevance is di-rectly known.
In the case of n-ary properties, thesituation is different for positive or negativeproperties.
The former (e.g.
cooccurrence con-straints) concern two realized categories.
In thiscase, c1 and c2 being these categories, we have{c1, c2} ?
A.
In the case of negative properties(e.g.
cooccurrence restriction), we need to haveeither c1 ?A or c2 ?A.When a property is relevant for a given A, itssatisfiability is evaluated, according to the prop-21erty semantics, each property being associated toa solver.
The general process is described as fol-lows:Let G the set of properties in the gram-mar, let A an assignment?
pi ?
G, if pi is relevantEvaluate the satisfiability of pifor AAdd pi and its evaluation to thecharacterization C of ACheck whether C is productiveIn this process, for all assignments, all propertieshave to be checked to verify their relevance andeventually their satisfiability.The last aspect of this general process concernsthe evaluation of the productivity of the charac-terization or an assignment.
A productive as-signment makes it possible to instantiate the cor-responding category and to consider it as real-ized.
A characterization is obviously productivewhen all properties are satisfied.
But it is alsopossible to consider an assignment as productivewhen it contains violated properties.
It is thenpossible to build categories, or more generallyconstructions, even for non canonical forms.
Inthis case, the characterization is not entirely posi-tive.
This process has to be controlled.
The basiccontrol consists in deciding a threshold of vio-lated constraints.
It is also possible to be moreprecise and propose a hierarchization of the con-straint system: some types of constraints or someconstraints can play a more important role thanothers (cf.
[Blache05b]).A controlled version of this parsing schema, im-plemented in the experimentation described inthe next section, takes advantage of the generalframework, in particular in terms of robustnessimplemented as constraint relaxation.
The proc-ess is however controlled for the construction ofthe assignment.This control process relies on a left-corner strat-egy, adapted to the PG parsing schema.
Thisstrategy consists in identifying whether a cate-gory can start a new phrase.
It makes it possibleto drastically reduce the number of assignmentsand then control ambiguity.
Moreover, the leftcorner suggests a construction label.
The set ofproperties taken into consideration when build-ing the characterization is then reduced to the setof properties corresponding to the label.
Thesetwo controls, plus a disambiguation of the lexicallevel by means of an adapted POS tagger, renderthe parsing process very efficient.The left corner process relies on a precedencetable, calculated for each category according tothe precedence properties in the grammar.
Thistable is built automatically in verifying for eachcategory whether, according to a given construc-tion, it can precede all the other categories.
Theprocess consists in verifying that the category isnot a left member of a precedence property of theconstruction.
If so, the category is said to be apossible left corner of the construction.
Theprecedence table contains then for each categorythe label of the construction for which it can beleft corner.During the process, when a category is a poten-tial left corner of a construction C, we verify thatthe C is not the last construction opened by a leftcorner.
If so, a new left corner is identified, andC is added to the set of possible constituents (us-able by other assignments).
Moreover, the char-acterization of the assignment beginning with ciis built in verifying the subset of properties de-scribing C.The generation of the assignments can also becontrolled by means of a co-constituency table.This table consists for each category, in indicat-ing all the categories with which it belongs to apositive property.
This table is easily built with asimple traversal of the constraint system.
Addinga new category ci  to an assignment A is possibleonly when ci appears as a co-constituent of acategory belonging to A.S initial set of lexical categoriesIdentification all the left cornersFor all C, construction opened by a leftcorner ci with G?
the set ofproperties describing CBuild assignments beginning by ciBuild characterizations verifying G?The parsing mechanism described here takes ad-vantage of the robustness of PG.
All kind of in-put, whatever its form, can be parsed because ifthe possibility of relaxing constraints.
Moreover,the control technique makes it possible to reducethe complexity of the process without modifyingits philosophy.4 Evaluation22We experimented this approach during theFrench evaluation campaign EASy (cf.[Paroubek05]).
The test consisted in parsing sev-eral files containing various kinds of material:literature, newspaper, technical texts, questions,e-mails and spoken language.
The total size ofthis corpus is one million words.
Part of this cor-pus was annotated with morpho-syntactic (POStags) and syntactic annotations.
The last one pro-vides bracketing as well as syntactic relationsbetween units.
The annotated part of the corpusrepresents 60,000 words and constitutes the goldstandard.The campaign consisted for the participants toparse the entire corpus (without knowing whatpart of the corpus constituted the reference).
Theresults of the campaign are not yet available con-cerning the evaluation of the relations.
The fig-ures presented in this section concern constituentbracketing.
The task consisted in identifyingminimal non recursive constituents described byannotation guidelines given to the participants.The different categories to be built are: GA (ad-jective group: adjective or passed participle), GN(nominal group: determiner, noun adjective andits modifiers), GP (prepositional group), GR (ad-verb), NV (verbal nucleus: verb, clitics) and PV(verbal propositional group).Our system parses the entire corpus (1 millionwords) in 4 minutes on a PC.
It presents then avery good efficiency.We have grouped the different corpora into threedifferent categories: written texts (includingnewspapers, technical texts and literature), spo-ken language (orthographic transcription ofspontaneous speech) and e-mails.
The results arethe following:Precision Recall F-mesureWritten texts 77.78 82.96 79.84Spoken lan-guage 75.13 78.89 76.37E-Mails 71.86 79.06 74.42These figures show then very stable results inprecision and recall, with only little loss of effi-ciency for non-canonical material.
When study-ing more closely the results, some elements ofexplanation can be given.
The e-mail corpus is tobe analyzed separately: many POS tagging er-rors, due to the specificity of this kind of inputexplain the difference.
Our POS-tagger was nottuned for this kind of lexical material.The interpretation of the difference between writ-ten and oral corpora can have some linguisticbasis.
The following figures give quantitativeindications on the categories built by the parser.The first remark is that the repartition betweenthe different categories is the same.
The onlymain difference concerns the higher number ofnucleus VP in the case of written texts.
Thisseems to support the classical idea that spokenlanguage seems to use more nominal construc-tions than the written one.Constituents for Written Corpora78471869310706460517599101202000400060008000100001200014000160001800020000GA GN GP GR NV PVConstituantsConstituents for Oral Corpora54601672696084992130201001020004000600080001000012000140001600018000GA GN GP GR NV PVConstituantsThe problem is that our parser encounters somedifficulties in the identification of the NP bor-ders.
It very often also includes some materialbelonging in the grammar given during the cam-paign to AP or VP.
The higher proportion of NPsin spoken corpora is an element of explanationfor the difference in the results.235 ConclusionThe first results obtained during the evaluationcampaign described in this paper are very inter-esting.
They illustrate the relevance of usingsymbolic approaches for parsing non-canonicalmaterial.
The technique described here makes itpossible to use the same method and the sameresources whatever the kind of input and offersthe possibility to do chunking as well as deepanalysis.
Moreover, such techniques, providedthat they are implemented with some controlmechanisms, can be very efficient: our parsertreat more than 4,000 words per second.
It con-stitutes then an efficient tool capable of dealingwith large amount of data.
On top of this effi-ciency, the parser has good results in terms ofbracketing, whatever the kind of material parsed.This second characteristics also shows that thesystem can be used in real life applications.In terms of theoretical results, such experimenta-tion shows the interest of using constraints.
First,they makes it possible to represent very fine-level information and offers a variety of controlmechanisms, relying for example on the possibil-ity of weighting them.
Moreover, constraint re-laxation techniques offer the possibility of build-ing categories violating part of syntactic descrip-tion of the grammar.
They are then particularlywell adapted to the treatment of non canonicaltexts.
The formalism of Property Grammars be-ing a fully constraint-based approach, it consti-tutes an efficient solution for the description ofany kind of inputs.Reference[Abney 96] Abney S. (1996) ?Partial Parsing via Fi-nite-State Calculus?, in proceedings of ESSLLI'96Robust Parsing Workshop[B?s99] B?s G. (1999) ?La phrase verbale noyau enfran?ais?, in Recherches sur le fran?ais parl?, 15,Universit?
de Provence.
[Blache00] Blache P. (2000) ?Constraints, LinguisticTheories and Natural Language Processing?, inNatural Language Processing, D. Christodoulakis(ed), LNAI 1835, Springer-Verlag[Blache05a] Blache P. (2005) ?Property Grammars: AFully Constraint-Based Theory?, in ConstraintSolving and Language Processing, H. Christiansen& al.
(eds), LNAI 3438, Springer[Boullier 05] Boullier P. & B. Sagot (2005) ?Efficientand robust LFG parsing: SxLfg?, in Proceedings ofIWPT '05.
[Crysmann02] Crysmann B.
A. Frank, B. Kiefer, S.M?ller, G. Neumann, J. Piskorski, U. Sch?fer, M.Siegel, H. Uszkoreit, F. Xu, M. Becker & H.Krieger (2002) ?An Integrated Architecture forShallow and Deep Processing?, in proceedings ofACL-02.
[Frank03] Frank A., M. Becker, B. Crysmann, B.Kiefer & U. Sch?fer (2003) ?Integrated Shallowand Deep Parsing: TopP meets HPSG?, in proceed-ings of ACL-03.
[Hindle83] Hindle D. (1983) User manual for Fid-ditch, a deterministic parser, Technical memoran-dum 7590-142, Naval Research Laboratory.
[Johnson98] Johnson M. (1998) ?PCFG Models ofLinguistic Tree Representations'?, in Computa-tional Linguistics, 24:4.
[Karlsson90] Karlsson F. (1990) ?Constraint grammaras a framework for parsing running texts?, in pro-ceedings of ACL-90.
[Marimon02] Marimon M. (2002) ?Integrating Shal-low Linguistic Processing into a Unification-BasedSpanish Grammar?, in proceedings of COLING-02.
[Maruyama90] Maruyama H. (1990) ?Structural Dis-ambiguation with Constraint Propagation'?, in pro-ceedings of ACL'90.
[Paroubek05] Paroubek P., L. Pouillot, I. Robba & A.Vilnat (2005) ?EASy : campagne d?
?valuation  desanalyseurs syntaxiques?, in proceedings of theworkshop EASy, TALN-2005.
[Prince93] Prince A.
& Smolensky P. (1993) ?Opti-mality Theory: Constraint Interaction in GenerativeGrammars?, Technical Report RUCCS TR-2, Rut-gers Center for Cognitive Science.
[Tjong Kim Sang00] Tjong Kim Sang E. & SBuchholz (2000) ?Introduction do the CoNLL-2000 Shared Task: Chunking?, in proceedings ofCoNLL-2000.
[Uszkoreit02] Uszkoreit H. (2002) ?New Chances forDeep Linguistic Processing?, in proceedings ofCOLING-02.
[VanRullen05] Van Rullen T. (2005), Vers une ana-lyse syntaxique ?
granularit?
variable, PhD Thesis,Universit?
de Provence.24
