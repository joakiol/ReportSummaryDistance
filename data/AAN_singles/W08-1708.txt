Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 57?64Manchester, August 2008Towards Domain-Independent Deep Linguistic Processing:Ensuring Portability and Re-Usability of Lexicalised GrammarsKostadin Cholakov?, Valia Kordoni?
?, Yi Zhang???
Department of Computational Linguistics, Saarland University, Germany?
LT-Lab, DFKI GmbH, Germany{kostadin,kordoni,yzhang}@coli.uni-sb.deAbstractIn this paper we illustrate and underlinethe importance of making detailed linguis-tic information a central part of the pro-cess of automatic acquisition of large-scalelexicons as a means for enhancing robust-ness and at the same time ensuring main-tainability and re-usability of deep lexi-calised grammars.
Using the error miningtechniques proposed in (van Noord, 2004)we show very convincingly that the mainhindrance to portability of deep lexicalisedgrammars to domains other than the onesoriginally developed in, as well as to ro-bustness of systems using such grammarsis low lexical coverage.
To this effect,we develop linguistically-driven methodsthat use detailed morphosyntactic informa-tion to automatically enhance the perfor-mance of deep lexicalised grammars main-taining at the same time their usually al-ready achieved high linguistic quality.1 IntroductionWe focus on enhancing robustness and ensur-ing maintainability and re-usability for a large-scale deep grammar of German (GG; (Crysmann,2003)), developed in the framework of Head-driven Phrase Structure Grammar (HPSG).
Specif-ically, we show that the incorporation of detailedlinguistic information into the process of auto-matic extension of the lexicon of such a languageresource enhances its performance and provideslinguistically sound and more informative predic-tions which bring a bigger benefit for the grammarwhen employed in practical real-life applications.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.In recent years, various techniques and re-sources have been developed in order to improverobustness of deep grammars for real-life applica-tions in various domains.
Nevertheless, low cover-age of such grammars remains the main hindranceto their employment in open domain natural lan-guage processing.
(Baldwin et al, 2004), as wellas (van Noord, 2004) and (Zhang and Kordoni,2006) have clearly shown that the majority of pars-ing failures with large-scale deep grammars arecaused by missing or wrong entries in the lexiconsaccompanying grammars like the aforementionedones.
Based on these findings, it has become clearthat it is crucial to explore and develop efficientmethods for automated (Deep) Lexical Acquisition(henceforward (D)LA), the process of automati-cally recovering missing entries in the lexicons ofdeep grammars.Recently, various high-quality DLA approacheshave been proposed.
(Baldwin, 2005), as wellas (Zhang and Kordoni, 2006), (van de Cruys,2006) and (Nicholson et al, 2008) describe effi-cient methods towards the task of lexicon acqui-sition for large-scale deep grammars for English,Dutch and German.
They treat DLA as a classi-fication task and make use of various robust andefficient machine learning techniques to performthe acquisition process.However, it is our claim that to achieve bet-ter and more practically useful results, apart fromgood learning algorithms, we also need to incorpo-rate into the learning process fine-grained linguis-tic information which deep grammars inherentlyinclude and provide for.
As we clearly show inthe following, it is not sufficient to only developand use good and complicated classification algo-rithms.
We must look at the detailed linguistic in-formation that is already included and provided forby the grammar itself and try to capture and makeas much use of it as possible, for this is the infor-mation we aim at learning when performing DLA.57In this way, the learning process is facilitated andat the same time it is as much as possible ensuredthat its outcome be linguistically more informativeand, thus, practically more useful.We use the GG deep grammar for the work wepresent in this paper because German is a languagewith rich morphology and free word order, whichexhibits a range of interesting linguistic phenom-ena, a fair number of which are already analysed inthe GG.
Thus, the grammar is a valuable linguisticresource since it provides linguistically sound anddetailed analyses of these phenomena.
Apart fromthe interesting syntactic structures, though, the lex-ical entries in the lexicon of the aforementionedgrammar also exhibit a rich and complicated struc-ture and contain various important linguistic con-straints.
Based on our claim above, in this pa-per we show how the information these constraintsprovide can be captured and used in linguistically-motivated DLA methods which we propose here.We then apply our approach on real-life data andobserve the impact it has on the the grammar cov-erage and its practical application.
In this way wetry to prove our assumption that the linguistic in-formation we incorporate into our DLA methodsis vital for the good performance of the acquisitionprocess and for the maintainability and re-usabilityof the grammar, as well for its successful practicalapplication.The remainder of the paper is organised as fol-lows.
In Section 2 we show that low (lexical) cov-erage is a serious issue for the GG when employedfor open domain natural language processing.
Sec-tion 3 presents the types in the lexical architectureof the GG that are considered to be relevant for thepurposes of our experiments.
Section 4 describesthe extensive linguistic analysis we perform in or-der to deal with the linguistic information thesetypes provide and presents the target type inven-tory for our DLA methods.
Section 5 reports onstatistical approaches towards automatic DLA andshows the importance of a good and linguistically-motivated feature selection.
Section 6 illustratesthe practical usage of the proposed DLA methodsand their impact on grammar coverage.
Section 7concludes the paper.2 Coverage Test with the GGWe start off adopting the automated error miningmethod described in (van Noord, 2004) for iden-tification of the major type of errors in the GG.As an HPSG grammar, the GG is based on typedfeature structures.
The GG types are strictly de-fined within a type hierarchy.
The GG also con-tains constructional and lexical rules and a lexiconwith its entries belonging to lexical types whichare themselves defined again within the type hier-archy.
The grammar originates from (Mu?ller andKasper, 2000), but continued to improve after theend of the Verbmobil project (Wahlster, 2000) andit currently consists of 5K types, 115 rules and thelexicon contains approximately 35K entries.
Theseentries belong to 386 distinct lexical types.In the experiments we report here two corporaof different kind and size have been used.
Thefirst one has been extracted from the FrankfurterRundschau newspaper and contains about 614Ksentences that have between 5 and 20 tokens.
Thesecond corpus is a subset of the German part of theWacky project (Kilgarriff and Grefenstette, 2003).The Wacky project aims at the creation of largecorpora for different languages, including German,from various web sources, such as online news-papers and magazines, legal texts, internet fora,university and science web sites, etc.
The Ger-man part, named deWaC (Web as Corpus), con-tains about 93M sentences and 1.65 billion tokens.The subset used in our experiments is extractedby randomly selecting 2.57M sentences that havebetween 4 and 30 tokens.
These corpora havebeen chosen because it is interesting to observethe grammar performance on a relatively balancednewspaper corpus that does not include so manylong sentences and sophisticated linguistic con-structions and to compare it with the performanceof the grammar on a random open domain text cor-pus.The sentences are fed into the PET HPSG parser(Callmeier, 2000) with the GG loaded.
The parserhas been configured with a maximum edge num-ber limit of 100K and it is running in the best-onlymode so that it does not exhaustively find all pos-sible parses.
The result of each sentence is markedas one of the following four cases:?
P means at least one parse is found for thesentence;?
L means the parser halted after the morpho-logical analysis and was not able to constructany lexical item for the input token;?
N means that the parser exhausted the search-ing and was not able to parse the sentence;58?
E means the parser reached the maximumedge number limit and was still not able tofind a parse.Table 1 shows the results of the experimentswith the two corpora.
From these results it canFR deWaCResult #Sentences % #Sentences %P 62,768 10.22% 109,498 4.3%L 464,112 75.55% 2,328,490 90.5%N 87,415 14.23% 134,917 5.2%E 3 ?
14 ?Total: 614,298 100% 2,572,919 100%Table 1: Parsing results with the GG and the testcorporabe seen that the GG has full lexical span for onlya small portion of the sentences?
about 25% and10% for the Frankfurter Rundschau and the deWaCcorpora, respectively.
The output of the error min-ing confirms our assumption that missing lexicalentries are the main problem when it comes torobust performance of the GG and illustrates theneed for efficient DLA methods.3 Atomic Lexical TypesBefore describing the proposed DLA algorithm,we should define what exactly is being learnt.Most of the so called deep grammars are stronglylexicalised.
As mentioned in the previous section,the GG employs a type inheritance system and itslexicon has a flat structure with each lexical entrymapped onto one type in the inheritance hierarchy.Normally, the types assigned to the lexical entriesare maximal on the type hierarchy, i.e., they do nothave any subtypes.
They provide the most specificinformation available for this branch of the hierar-chy.
These maximal types which the lexical entriesare mapped onto are called atomic lexical types.Thus, in our experiment setup, we can define thelexicon of the grammar as being a one-to-one map-ping from word stems to atomic lexical types.
It isthis mapping which must be automatically learnt(guessed) by the different DLA methods.We are interested in learning open-class words,i.e., nouns, adjectives, verbs and adverbs.
We as-sume that the close-class words are already in thelexicon or the grammar can handle them throughvarious lexical rules and they are not crucial forthe grammar performance in real life applications.Thus, for the purposes of our experiments, we con-sider only the open-class lexical types.
Moreover,we propose an inventory of open-class lexical typeswith sufficient type and token frequency.
The typefrequency of a given lexical type is defined asthe number of lexical entries in the lexicon of thegrammar that belong to this type and the token fre-quency is the number of words in some corpus thatbelong to this type.We use sentences from the Verbmobil corpuswhich have been treebanked with the GG in orderto determine the token frequency and to map thelexemes to their correct entries in the lexicon forthe purposes of the experiment.
This set contains11K sentences and about 73K tokens; this gives anaverage of 6.8 words per sentence.
The sentencesare taken from spoken dialogues.
Hence, they arenot long and most of them do not exhibit interest-ing linguistic properties which is a clear drawbackbut currently there is no other annotated data com-patible with the GG.We used a type frequency threshold of 10 entriesin the lexicon and a token frequency threshold of3 occurrences in the treebanked sentences to forma list of relevant open-class lexical types.
The re-sulting list contains 38 atomic lexical types with atotal of 32,687 lexical entries.4 Incorporation of Linguistic FeaturesHowever, in the case of the GG this type inventoryis not a sufficient solution.
As already mentioned,in the lexicon of the grammar much of the relevantlinguistic information is encoded not in the typedefinition itself but in the form of constraints in thefeature structures of the various types.
Moreover,given that German has a rich morphology, a givenattribute may have many different values amonglexical entries of the same type and it is crucial forthe DLA process to capture all the different com-binations.
That is why we expand the identified38 atomic lexical type definitions by including thevalues of various features into them.By doing this, we are trying to facilitate theDLA process because, in that way, it can ?learn?to differentiate not only the various lexical typesbut also significant morphosyntactic differencesamong entries that belong to the same lexical type.That gives the DLA methods access to much morelinguistic information and they are able to applymore linguistically fine-tuned classification crite-ria when deciding which lexical type the unknownword must be assigned to.
Furthermore, we en-sure that the learning process deliver linguistically59Feature Values MeaningSUBJOPT (subject options)+ in some cases the article for the noun can be omitted- the noun always goes with an article+ raising verb- non-raising verbKEYAGR (key agreement)?
case-number-gender information for nounsc-s-n underspecified-singular-neutralc-p-g underspecified-plural-underspecified... ...(O)COMPAGR ((oblique) a-n-g, d-n-g, etc.
case-number-gender informationcomplement ?
for (oblique) verb complementsagreement ?
case-number-gender of the modified noun (for adjectives)(O)COMPTOPT ((oblique) ?
verbs can take a different number of complementscomplement + the respective (oblique) complement is presentoptions - the respective (oblique) complement is absentKEYFORM?
the auxiliary verb used for the formation of perfect tensehaben the auxiliary verb is ?haben?sein the auxiliary verb is ?sein?Table 2: Relevant features used for type expansionplausible, precise and more practically useful re-sults.
The more the captured and used linguisticinformation is, the better and more useful the DLAresults will be.However, we have to avoid creating data sparseproblems.
We do so by making the assumptionthat not every feature could really contribute to theclassification process and by filtering out these fea-tures that we consider irrelevant for the enhance-ment of the DLA task.
Naturally, the questionwhich features are to be considered relevant arises.After performing an extensive linguistic analysis,we have decided to take the features shown in Ta-ble 2 into account.We have thoroughly analysed each of these fea-tures and selected them on the basis of their lin-guistic meaning and their significance and contri-bution to the DLA process.
The SUBJOPT fea-ture can be used to differentiate among nouns thathave a similar morphosyntactic behaviour but dif-fer only in the usage of articles; 4 out of the consid-ered 9 noun atomic lexical types do not define thisfeature.
Furthermore, using this feature, we canalso refine our classification within a single atomiclexical type.
For example, the entry ?adresse-n?
(address) of the type ?count-noun-le?1 has ?-?
forthe SUBJOPT value, whereas the value for the en-try ?anbindung-n?
(connection) of the same type is?+?
:(1) a. Dasdet.NEUT.NOMHotelhotelhathave.3PER.SGgutegoodAnbindungconnectionantodiedet.PL.ACCo?ffentlichenpublic1count noun lexeme; all lexical entries in the lexicon endwith le which stands for lexeme.Verkehrsmittel.transportation means?The hotel has a good connection to publictransportation.?b.
Diedet.FEM.NOMAnbindungconnectionantoRomRomemitwithdemdet.MASC.DATZugtrainistbe.3PER.SGgut.good?The train connection to Rome is good.
?The distinction between raising and non-raisingverbs that this feature expresses is also an impor-tant contribution to the classification process.The case-number-gender data the KEYAGR and(O)COMPAGR features provide allows for a bet-ter usage of morphosyntactic information for thepurposes of DLA.
Based on this data, the classifi-cation method is able to capture words with sim-ilar morphosyntactic behaviour and give variousindications for their syntactic nature; for instance,if the word is a subject, direct or indirect object.This is especially relevant and useful for languageswith rich morphology and relatively free word or-der such as German.
The same is also valid forthe (O)COMPOPT and KEYFORM features?
theyallow the DLA method to successfully learn andclassify verbs with similar syntactic properties.The values of the features are just attached to theold type name to form a new type definition.
In thisway, we ?promote?
them and these features are nowpart of the type hierarchy of the grammar whichmakes them accessible for the DLA process sincethis operates on the type level.
For example, theoriginal type of the entry for the noun ?abenteuer?
(adventure):abenteuer-n := count-noun-le &[ [ --SUBJOPT -,60KEYAGR c-n-n,KEYREL "_abenteuer_n_rel",KEYSORT situation,MCLASS nclass-2_-u_-e ] ].will become abenteuer-n := count-noun-le - c-n-n when we incorporate the values of the featuresSUBJOPT and KEYAGR into the original typedefinition.
The new expanded type inventory isshown in Table 3.Original Expandedlexicon lexiconNumber of lexical types 386 485Atomic lexical types 38 137-nouns 9 72-verbs 19 53-adjectives 3 5-adverbs 7 7Table 3: Expanded atomic lexical typesThe features we have ignored do not contributeto the learning process and are likely to cre-ate sparse data problems.
The (O)COMPFORM((oblique) complement form) features which de-note dependent to verbs prepositions are not con-sidered to be relevant.
An example of OCOMP-FORM is the lexical entry ?begru?nden mit-v?
(jus-tify with) where the feature has the preposition?mit?
(with) as its value.
Though for Germanprepositions can be considered as case markers, theDLA has already a reliable access to case informa-tion through the (O)COMPAGR features.
More-over, a given dependent preposition is distributedacross many types and it does not indicate clearlywhich type the respective verb belongs to.The same is valid for the feature VCOPMFORM(verb complement form) that denotes the separa-ble particle (if present) of the verb in question.An example of this feature is the lexical entry?abdecken-v?
(to cover) where VCOMPFORM hasthe separable particle ?ab?
as its value.
However,treating such discontinuous verb-particle combina-tions as a lexical unit could help for the acquisi-tion of subcategorizational frames.
For example,anho?ren (to listen to someone/something) takes anaccusative NP as argument, zuho?ren (to listen to)takes a dative NP and aufho?ren (to stop, to termi-nate) takes an infinitival complement.
Thus, ignor-ing VCOMPFORM could be a hindrance for theacquisition of some verb types2.We have also tried to incorporate some sort ofsemantic information into the expanded atomic2We thank the anonymous reviewer who pointed this outfor us.lexical type definitions by also attaching theKEYSORT semantic feature to them.
KEYSORTdefines a certain situation semantics category(?anything?, ?action sit?, ?mental sit?)
which thelexical entry belongs to.
However, this has causedagain a sparse data problem because the semanticclassification is too specific and, thus, the numberof possible classes is too large.
Moreover, seman-tic classification is done based on completely dif-ferent criteria and it cannot be directly linked to themorphosyntactic features.
That is why we have fi-nally excluded this feature, as well.Armed with this elaborate target type inventory,we now proceed with the DLA experiments for theGG.5 DLA Experiments with the GGFor our DLA experiments, we adopted the Max-imum Entropy based model described in (Zhangand Kordoni, 2006), which has been applied to theERG (Copestake and Flickinger, 2000), a wide-coverage HPSG grammar for English.
For the pro-posed prediction model, the probability of a lexicaltype t given an unknown word and its context c is:(2) p(t|c) = exp(?i?ifi(t,c))?t?
?Texp(?i?ifi(t?,c))where fi(t, c) may encode arbitrary characteristicsof the context and ?iis a weighting factor esti-mated on a training corpus.
Our experiments havebeen performed with the feature set shown in Table4.Featuresthe prefix of the unknown word(length is less or equal 4)the suffix of the unknown word(length is less or equal 4)the 2 words before and after the unknown wordthe 2 types before and after the unknown wordTable 4: Features for the DLA experimentWe have also experimented with prefix and suf-fix lengths up to 3.
To evaluate the contributionof various features and the overall precision of theME-based unknown word prediction model, wehave done a 10-fold cross validation on the Verb-mobil treebanked data.
For each fold, words thatdo not occur in the training partition are assumedto be unknown and are temporarily removed fromthe lexicon.For comparison, we have also built a baselinemodel that always assigns a majority type to each61unknown word according to its POS tag.
Specifi-cally, we tag the input sentence with a small POStagset.
It is then mapped to a most popular lexi-cal type for that POS.
Table 5 shows the relevantmappings.POS Majority lexical typenoun count-noun-le - c-n-fverb trans-nerg-str-verb-le haben-auxfadj adj-non-prd-leadv intersect-adv-leTable 5: POS tags to lexical types mappingAgain for comparison, we have built anothersimple baseline model using the TnT POS tagger(Brants, 2000).
TnT is a general-purpose HMM-based trigram tagger.
We have trained the taggingmodels with all the lexical types as the tagset.
Thetagger tags the whole sentence but only the outputtags for the unknown words are taken to generatelexical entries and to be considered for the eval-uation.
The precisions of the different predictionmodels are given in Table 6.The baseline achieves a precision of about 38%and the POS tagger outperforms it by nearly 10%.These results can be explained by the nature of theVerbmobil data.
The vast majority of the adjec-tives and the adverbs in the sentences belong tothe majority types shown in Table 5 and, thus, thebaseline model assigns the correct lexical types toalmost every adjective and adverb, which bringsup the overall precision.
The short sentence lengthfacilitates the tagger extremely, for TnT, as anHMM-based tagger, makes predictions based onthe whole sentence.
The longer the sentences are,the more challenging the tagging task for TnT is.The results of these models clearly show that thetask of unknown word type prediction for deepgrammars is non-trivial.Our ME-based models give the best results interms of precision.
However, verbs and adverbsremain extremely difficult for classification.
Thesimple morphological features we use in the MEmodel are not good enough for making good pre-dictions for verbs.
Morphology cannot capturesuch purely syntactic features as subcategoriza-tional frames, for example.While the errors for verbs are pretty random,there is one major type of wrong predictions foradverbs.
Most of them are correctly predicted assuch but they receive the majority type for adverbs,namely ?intersect-adv-le?.
Since most of the ad-verbs in the Verbmobil data we are using belongto the majority adverb type, the predictor is biasedtowards assigning it to the unknown words whichhave been identified as adverbs.The results in the top half of the Table 6 showthat morphological features are already very goodfor predicting adjectives.
In contrast with ad-verbs, adjectives occur in pretty limited number ofcontexts.
Moreover, when dealing with morpho-logically rich languages such as German, adjec-tives are typically marked by specific affixes cor-responding to a specific case-number-gender com-bination.
Since we have incorporated this kind oflinguistic information into our target lexical typedefinitions, this significantly helps the predictionprocess based on morphological features.Surprisingly, nouns seem to be hard to learn.Apparently, the vast majority of the wrong pre-dictions have been made for nouns that belong tothe expanded variants of the lexical type ?count-noun-le?
which is also the most common non-expanded lexical type for nouns in the original lex-icon.
Many nouns have been assigned the right lex-ical type except for the gender:(3) Betrieb (business, company, enterprise)prediction: count-noun-le - c-n-ncorrect type: count-noun-le - c-n-mAccording to the strict exact-match evaluate mea-sure we use, such cases are considered to be errorsbecause the predicted lexical type does not matchthe type of the lexical entry in the lexicon.The low numbers for verbs and adverbs showclearly that we also need to incorporate some sortof syntactic information into the prediction model.We adopt the method described in (Zhang and Ko-rdoni, 2006) where the disambiguation model ofthe parser is used for this purpose.
We also believethat the kind of detailed morphosyntactic informa-tion which the learning process now has accessto would facilitate the disambiguation model be-cause the input to the model is linguistically morefine-grained.
In another DLA experiment we letPET use the top 3 predictions provided by the lex-ical type predictor in order to generate sentenceanalyses.
Then we use the disambiguation model,trained on the Verbmobil data, to choose the bestone of these analyses and the corresponding lexicalentry is taken to be the final result of the predictionprocess.As shown in the last line of Table 6, we achievean increase of 19% which means that in manycases the correct lexical type has been ranked sec-62Model Precision Nouns Adjectives Verbs AdverbsBaseline 37.89% 27.03% 62.69% 33.57% 67.14%TnT 47.53% 53.76% 74.52% 26.94% 32.68%ME(affix length=3) 51.2% 48.25% 75.41% 44.06% 44.13%ME(affix length=4) 54.63% 53.55% 76.79% 47.10% 43.55%ME + disamb.
73.54% 75% 88.24% 65.98% 65.90%Table 6: Precision of unknown word type predictorsond or third by the predictor.
This proves thatthe expanded lexical types improve also the perfor-mance of the disambiguation model and allow forits successful application for the purposes of DLA.It also shows, once again, the importance of themorphology in the case of the GG and proves therightness of our decision to expand the type defini-tions with detailed linguistic information.36 Practical ApplicationSince our main claim in this paper is that forgood and practically useful DLA, which at thesame time may facilitate robustness and ensuremaintainability and re-usability of deep lexicalisedgrammars, we do not only need good machinelearning algorithms but also classification and fea-ture selection that are based on an extensive lin-guistic analysis, we apply our DLA methods to realtest data.
We believe that due to our expanded lex-ical type definitions, we provide much more lin-guistically accurate predictions.
With this type ofpredictions, we anticipate a bigger improvement ofthe grammar coverage and accuracy for the pre-diction process delivers much more linguisticallyrelevant information which facilitates parsing withthe GG.We have conducted experiments with PET andthe two corpora we have used for the error miningto determine whether we can improve coverage byusing our DLA method to predict the types of un-known words online.
We have trained the predic-tor on the whole set of treebanked sentences andextracted a subset of 50K sentences from each cor-pus.
Since lexical types are not available for thesesentences, we have used POS tags instead as fea-tures for our prediction model.
Coverage is mea-sured as the number of sentences that received atleast one parse and accuracy is measured as thenumber of sentences that received a correct analy-sis.
The results are shown in Table 7.The coverage for FR improves with more than12% and the accuracy number remains almost the3Another reason for this high result is the short averagelength of the treebanked sentences which facilitates the dis-ambiguation model of the parser.Parsed Corpus Coverage AccuracyFR with the vanilla version GG 8.89% 85%FR with the GG + DLA 21.08% 83%deWaC with the vanilla version GG 7.46% ?deWaC with the GG + DLA 16.95% ?Table 7: Coverage resultssame.
Thus, with our linguistically-oriented DLAmethod, we have managed to increase parsing cov-erage and at the same time to preserve the highaccuracy of the grammar.
It is also interesting tonote the increase in coverage for the deWaC cor-pus.
It is about 10%, and given the fact that deWaCis an open and unbalanced corpus, this is a clearimprovement.
However, we do not measure ac-curacy on the deWaC corpus because many sen-tences are not well formed and the corpus itselfcontains much ?noise?.
Still, these results showthat the incorporation of detailed linguistic infor-mation in the prediction process contributed to theparser performance and the robustness of the gram-mar without harming the quality of the deliveredanalyses.7 ConclusionIn this paper, we have tackled from a morelinguistically-oriented point of view the lexiconacquisition problem for a large-scale deep gram-mar for German, developed in HPSG.
We haveshown clearly that missing lexical entries are themain cause for parsing failures and, thus, illus-trated the importance of increasing the lexical cov-erage of the grammar.
The target type inventoryfor the learning process has been developed in alinguistically motivated way in an attempt to cap-ture significant morphosyntactic information and,thus, achieve a better performance and more prac-tically useful results.With the proposed DLA approach and our elab-orate target type inventory we have achieved nearly75% precision and this way we have illustrated theimportance of fine-grained linguistic informationfor the lexical prediction process.
In the end, wehave shown that with our linguistically motivatedDLA methods, the parsing coverage of the afore-63mentioned deep grammar improves significantlywhile its linguistic quality remains intact.The conclusion, therefore, is that it is vital tobe able to capture linguistic information and suc-cessfully incorporate it in DLA processes, for itfacilitates deep grammars and makes processingwith them much more robust for applications.
Atthe same time, the almost self-evident portabilityto new domains and the re-usability of the gram-mar for open domain natural language processingis significantly enhanced.The DLA method we propose can be used asan external module that can help the grammar beported and operate on different domains.
Thus,specifically in the case of HPSG, DLA can alsobe seen as a way for achieving more modular-ity in the grammar.
Moreover, in a future re-search, the proposed kind of DLA might also beused in order to facilitate the division and transi-tion from a core deep grammar with a core lex-icon towards subgrammars with domain specificlexicons/lexical constraints in a linguistically mo-tivated way.
The use of both these divisions nat-urally leads to a highly modular structure of thegrammar and the system using the grammar, whichat the same time helps in controlling its complex-ity.Our linguistically motivated approach providesfine-grained results that can be used in a numberof different ways.
It is a valuable linguistic tooland it is up to the grammar developer to choosehow to use the many opportunities it provides.ReferencesBaldwin, Timothy, Emily M. Bender, Dan Flickinger, AraKim, and Stephan Oepen.
2004.
Road-testing the EnglishResource Grammar over the British National Corpus.
InProceedings of the Fourth Internation Conference on Lan-guage Resources and Evaluation (LREC 2004), Lisbon,Portugal.Baldwin, Timothy.
2005.
Bootstrapping deep lexical re-sources: Resources for courses.
In Proceedings of theACL-SIGLEX 2005 Workshop on Deep Lexical Acquisi-tion, pages 67?76, Ann Arbor, USA.Brants, Thorsten.
2000.
TnT- a statistical part-of-speech tag-ger.
In Proceedings of the Sixth Conference on AppliedNatural Language Processing ANLP-2000, Seattle, WA,USA.Callmeier, Ulrich.
2000.
PET- a platform for experimenta-tion with efficient HPSG processing techniques.
In Jour-nal of Natural Language Engineering, volume 6(1), pages99?108.Copestake, Ann and Dan Flickinger.
2000.
An open-soursegrammar development environment and broad-coverageEnglish grammar using HPSG.
In Proceedings of the Sec-ond conference on Language Resources and Evaluation(LREC 2000), Athens, Greece.Crysmann, Berthold.
2003.
On the efficient implementationof German verb placement in HPSG.
In Proceedings ofRANLP 2003, pages 112?116, Borovets, Bulgaria.Kilgarriff, Adam and G Grefenstette.
2003.
Introduction tothe special issue on the web as corpus.
Computational Lin-guistics, 29:333?347.Mu?ller, Stephan and Walter Kasper.
2000.
HPSG analysis ofGerman.
In Wahlster, Wolfgang, editor, Verbmobil: Foun-dations of Speech-to-Speech Translation, pages 238?253.Springer-Verlag.Nicholson, Jeremy, Valia Kordoni, Yi Zhang, Timothy Bald-win, and Rebecca Dridan.
2008.
Evaluating and extend-ing the coverage of HPSG grammars.
In In proceedings ofLREC, Marrakesh, Marocco.van de Cruys, Tim.
2006.
Automatically extending the lexi-con for parsing.
In Huitink, Janneke and Sophia Katrenko,editors, Proceedings of the Student Session of the Euro-pean Summer School in Logic, Language and Information(ESSLLI), pages 180?191, Malaga, Spain.van Noord, Gertjan.
2004.
Error mining for wide coveragegrammar engineering.
In Proceedings of the 42nd Meetingof the Assiciation for Computational Linguistics (ACL?04),Main Volume, pages 446?453, Barcelona, Spain.Wahlster, Wolfgang, editor.
2000.
Verbmobil: Foundationsof Speech-to-Speech Translation.
Artificial Intelligence.Springer.Zhang, Yi and Valia Kordoni.
2006.
Automated deep lexicalacquisition for robust open text processing.
In Proceed-ings of the Fifth International Conference on LanguageResourses and Evaluation (LREC 2006), Genoa, Italy.64
