Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 98?106,Beijing, August 2010Broad Coverage Multilingual Deep Sentence Generation with aStochastic Multi-Level RealizerBernd Bohnet1, Leo Wanner1,2, Simon Mille1, Alicia Burga11Department of Information and Communication TechnologiesPompeu Fabra University2Institucio?
Catalana de Recerca i Estudis Avanc?ats (ICREA){first-name.last-name}@upf.eduAbstractMost of the known stochastic sentencegenerators use syntactically annotatedcorpora, performing the projection tothe surface in one stage.
However,in full-fledged text generation, sentencerealization usually starts from semantic(predicate-argument) structures.
To beable to deal with semantic structures,stochastic generators require semanticallyannotated, or, even better, multilevel an-notated corpora.
Only then can theydeal with such crucial generation issues assentence planning, linearization and mor-phologization.
Multilevel annotated cor-pora are increasingly available for multi-ple languages.
We take advantage of themand propose a multilingual deep stochasticsentence realizer that mirrors the state-of-the-art research in semantic parsing.
Therealizer uses an SVM learning algorithm.For each pair of adjacent levels of anno-tation, a separate decoder is defined.
Sofar, we evaluated the realizer for Chinese,English, German, and Spanish.1 IntroductionRecent years saw a significant increase of inter-est in corpus-based natural language generation(NLG), and, in particular, in corpus-based (orstochastic) sentence realization, i.e., that part ofNLG which deals with mapping of a formal (moreor less abstract) sentence plan onto a chain of in-flected words; cf., among others, (Langkilde andKnight, 1998; Oh and Rudnicky, 2000; Bangaloreand Rambow, 2000; Wan et al, 2009).
The advan-tage of stochastic sentence realization over tradi-tional rule-based realization is mainly threefold:(i) it is more robust, (ii) it usually has a signifi-cantly larger coverage; (iii) it is per se language-and domain-independent.
Its disadvantage is thatit requires at least syntactically annotated corporaof significant size (Bangalore et al, 2001).
Giventhe aspiration of NLG to start from numeric timeseries or conceptual or semantic structures, syn-tactic annotation even does not suffice: the cor-pora must also be at least semantically annotated.Up to date, deep stochastic sentence realizationwas hampered by the lack of multiple-level an-notated corpora.
As a consequence, availablestochastic sentence generators either take syntac-tic structures as input (and avoid thus the need formultiple-level annotation) (Bangalore and Ram-bow, 2000; Langkilde-Geary, 2002; Filippovaand Strube, 2008), or draw upon hybrid modelsthat imply a symbolic submodule which derivesthe syntactic representation that is then used bythe stochastic submodule (Knight and Hatzivas-siloglou, 1995; Langkilde and Knight, 1998).The increasing availability of multilevel anno-tated corpora, such as the corpora of the sharedtask of the Conference on Computational Natu-ral Language Learning (CoNLL), opens new per-spectives with respect to deep stochastic sentencegeneration?although the fact that these corporahave not been annotated with the needs of genera-tion in mind, may require additional adjustments,as has been, in fact, in the case of our work.98In this paper, we present a Support VectorMachine (SVM)-based multilingual dependency-oriented stochastic deep sentence realizer thatuses multilingual corpora of the CoNLL ?09shared task (Hajic?, 2009) for training.
The sen-tences of these corpora are annotated with shal-low semantic structures, dependency trees, andlemmata; for some of the languages involved,they also contain morphological feature annota-tions.
The multilevel annotation allows us to takeinto account all levels of representation neededfor linguistic generation and to model the pro-jection between pairs of adjacent levels by sep-arate decoders, which, in its turn, facilitates thecoverage of such critical generation tasks as sen-tence planning, linearization, and morphologiza-tion.
The presented realizer is, in principle,language-independent in that it is trainable on anymultilevel annotated corpus.
In this paper, we dis-cuss its performance for Chinese, English, Ger-man, and Spanish.The remainder of the paper is structured as fol-lows.
In Section 2, we discuss how the shallow se-mantic annotation in the CoNLL ?09 shared taskcorpora should be completed in order to be suit-able for generation.
Section 3 presents the train-ing setup of our realizer.
Section 4 shows the in-dividual stages of sentence realization: from thesemantic structure to the syntactic structure, fromthe syntactic structure to the linearized structureand from the linearized structure to a chain of in-flected word forms (if applicable for the languagein question).
Section 5 outlines the experimentalset up for the evaluation of our realizer and dis-cusses the results of this evaluation.
In Section 6,finally, some conclusions with respect to the char-acteristics of our realizer and its place in the re-search landscape are drawn.The amount of the material which comes intoplay makes it impossible to describe all stagesin adequate detail.
However, we hope that theoverview provided in what follows still suffices tofully assess our proposal.2 Completing the Semantic AnnotationThe semantic annotation of sentences in CoNLL?09 shared task corpora follows the PropBank an-notation guidelines (Palmer et al, 2005).
Prob-lematic from the viewpoint of generation is thatthis annotation is not always a connected acyclicgraph.
As a consequence, in these cases no valid(connected) syntactic tree can be derived.
Themost frequent cases of violation of the connectiv-ity principle are not attached adjectival modifiers,determiners, adverbs, and coordinations; some-times, the verb is not connected with its argu-ment(s).
Therefore, prior to starting the trainingprocedure, the semantic annotation must be com-pleted: non-connected adjectival modifiers mustbe annotated as predicates with their syntacticheads as arguments, determiners must be ?trans-lated?
into quantifiers, detached verbal argumentsmust be connected with their head, etc.Algorithm 1 displays the algorithm that com-pletes the semantic annotations of the corpora.Each sentence xi of the corpus I , with i =1, .
.
.
, |I|, is annotated with its dependency treeyi and its shallow semantic graph si.
The algo-rithm traverses yi breath-first, and examines foreach node n in yi whether n?s corresponding nodein si is connected with the node corresponding tothe parent of n. If not, the algorithm connects bothby a directed labeled edge.
The direction and thelabel of the edge are selected consulting a look uptable in which default labels and the orientationof the edges between different node categories arespecified.Figure 1 shows the semantic representation ofa sample English sentence obtained after the ap-plication of Algorithm 1.
The solid edges arethe edges available in the original annotation; thedashed edges have been introduced by the algo-rithm.
The edge labels ?A0?
and ?A1?
stand for?first argument?
and ?second argument?
(of thecorresponding head), respectively, ?R-A0?
for ?A0realized as a relative clause?, and ?AM-MNR?
for?manner modifier?.
As can be seen, 6 out of thetotal of 14 edges in the complete representationof this example have been added by Algorithm 1.We still did not finish the formal evaluation ofthe principal changes necessary to adapt the Prop-Bank annotation for generation, nor the quality ofour completion algorithm.
However, the need ofan annotation with generation in mind is obvious.99Algorithm 1: Complete semantic graph//si is a semantic graph and yi a dependency tree// si = ?Nsi , Lsi , Esi?, where Nsi is the set of nodes// Lsi the set of edge labels// Esi ?
Ns ?Ns ?
Ls is the set of edgesfor i?
1 to |I| // iteration over the training exampleslet ry ?
yi be the root node of the dependency tree// initialization of the queuenodeQueue ?
children(ry)while nodeQueue 6= ?
dony ?
removeFirst(nodeQueue)// breath first: add nodes at the end of the queuenodeQueue?
nodeQueue ?
children(ny)nys ?
sem(ny); pys ?
sem(parent(ny))//get the semantic equivalents of ny and of its parentif not exists path(nys , pys ) thenl?
label(ny ,parent(ny))ls ?
look-up-sem-label(nys , pys , l)if look-up-sem-direction(nys , pys , ls) = ???
then// add the semantic edgeEs?
Es ?
(pys , nys , ls)else // direction of the edge ??
?// add the semantic edgeEs?
Es ?
(nys , pys , ls)3 Realizer Training SetupFigure 2 shows the training setup of our realizer.For each level of annotation, an SVM feature ex-tractor and for each pair of adjacent levels of an-notation, an SVM decoder is defined.
The Sem-Synt decoder constructs from a semantic graphthe corresponding dependency tree.
The Synt-Linearization decoder derives from a dependencytree a chain of lemmata, i.e., determines the wordorder within the sentence.
The Linearization-Morph decoder generates the inflected word formfor each lemma in the chain.
Both the fea-ture extractors and the decoders are language-independent, which makes the realizer applicableto any language for which multilevel-annotatedcorpora are available.To compute the score of the alternative realiza-tions by each decoder, we apply MIRA (MarginInfused Relaxed Algorithm) to the features pro-vided by the feature extractors.
MIRA is oneof the most successful large-margin training tech-niques for structured data (Crammer et al, 2006).It has been used, e.g., for dependency parsing,semantic role labelling, chunking and tagging.Since we have similar feature sets (of compara-ble size) as those for which MIRA has proven towork well, we assume that it will also performa anA1A1A0A1A0A1A1A1A0AM-MNRA1A2A0beillustratebutPanamathatsubstutetheirsystemproducegridlockthatabsurdR-A0Figure 1: Semantic representation of the sentenceBut Panama illustrates that their substitute is asystem that produces an absurd gridlock.
aftercompletionwell for sentence realization.
Unfortunately, dueto the lack of space, we cannot present here theinstantiation of MIRA for all stages of our model.For illustration, Algorithm 2 outlines it for mor-phological realization.The morphologic realization uses the minimalstring edit distance (Levenshtein, 1966) to maplemmata to word forms.
As input to the MIRA-classifier, we use the lemmata of a sentence, itsdependency tree and the already ordered sentence.The characters of the input strings are reversedsince most of the changes occur at the end of thewords and the string edit scripts work relativelyto the beginning of the string.
For example, tocalculate the minimal string edit distance betweenthe lemma go and the form goes, both are firstreversed by the function compute-edit-dist andthen the minimal string edit script between og andseog is computed.
The resulting script is Ie0Is0.It translates into the operations ?insert e at the po-sition 0 of the input string?
and ?insert s at the po-sition 0?.Before MIRA starts, we compute all mini-mal edit distance scripts to be used as classes ofMIRA.
Only scripts that occur more often thantwice are used.
The number of the resulting editscripts is language-dependent; e.g., we get about100Semantic annotationSyntactic annotationLineariz.
annotationMorphol.
annotationsem.
feature extr.synt.
feature extr.lineariz.
feature extr.morph.
feature extr.Sem-Synt DECODERSynt-Lineariz.
DECODERLinear-Morph DECODERSVMFigure 2: Realizer training scenario setup1500 scripts for English and 2500 for German.The training algorithms typically perform 6 it-erations (epochs) over the training examples.
Foreach training example, a minimal edit script is se-lected.
If this script is different from the goldscript, the features of the gold script are calcu-lated and the weight vector of the SVM is adjustedaccording to the difference between the predictedvector and the gold feature vector.
The classifi-cation task consists then in finding the classifica-tion script that maps the lemma to the correct wordform.
For this purpose, the classifier scores eachof the minimal edit scripts according to the input,choosing the one with the highest score.4 Sentence GenerationSentence generation that starts from a given se-mantic structure as input consists in the applica-tion of the previously trained SVM decoders in se-quence in order to realize the following sequenceof mappings:SemStr?
SyntStr?
LinearStr?
Surface4.1 Semantic GenerationAlgorithm 3 shows the algorithm for semanticgeneration, i.e., the derivation of a dependencytree from a semantic structure.
It is a beam searchthat creates a maximum spanning tree.
In the firststep, a seed tree consisting of one edge is built.In each of the subsequent steps, this tree is ex-tended by one node.
For the decision, which nodeAlgorithm 2: Morphological realizationtraining with MIRA// yi, li; yi is a dependency tree, li lemmatized sentencescript-list?
{} //initialize the script-listfor i?
1 to |I| // iteration over the training examplesfor l?
1 to |li| do//// iteration over the lemmata of lilemmal?
lower-case (li,l)//ensure that all lemmata start with a lower case letterscript?
compute-edit-dist-script(lemmal, form(li,l))if script 6?
script-listscript-list?
script-list ?
{ script }for k?
1 to E // E = number of traininig epochsfor i?
1 to |I| // iteration over the training examplesfor l?
1 to |li| doscriptp?
predict-script(li,yi,l)scriptg ?
edit-dist-script(lemmal, form(li,l))if scriptp 6= scriptg then// update the weight vector v and the vector w, which// averages over all collected weight vectors acc.// to diff.
of the predicted and gold feature vectorupdate w, v according to ?(?
(scriptp), ?
(scriptg))//with ?
(scriptp), ?
(scriptg) as feature vectors of//scriptp and scriptg , respectivelyis to be attached next and to which node, we con-sider the highest scoring options.
This procedureworks well since nodes that are close in the se-mantic structure are usually close in the syntactictree as well.
Therefore subtrees that contain thosenodes are considered first.Unlike the traditional n-gram based stochasticrealizers such as (Langkilde and Knight, 1998),we use for the score calculation structured fea-tures composed of the following elements: (i) thelemmata, (ii) the distance between the startingnode s and the target node t, (iii) the directionof the path (if the path has a direction), (iv) thesorted bag of in-going edges labels without repi-tition, (v) the path of edge labels between sourceand target node.The composed structured features are:?
label+dist(s, t)+dir?
label+dist(s, t)+lemmas+dir?
label+dist(s, t)+lemmat+dir?
label+dist(s, t)+lemmas+lemmat+dir?
label+dist(s, t)+bags+dir?
label+dist(s, t)+bagt+dir?
label+path(s, t)+dir101# word-pairs(w1,w2) # n-grams1 labelw1+labelw2 13 PoS1+PoS2+PoS32 labelw1+lemma1 14 PoS1+PoS2+PoS3+dist3 labelw1+lemma2 15 lemma1+lemma2+lemma34 labelw2+lemma1 16 lemma1+lemma2+lemma3+dist5 labelw2+lemma2 17 lemma1+lemma3+head(w1,w2,w3)6 PoS1+PoS2 18 lemma1+lemma3+head(w1,w2,w3)+dist7 PoS1+PoS2+head(w1,w2) 19 label1+label2+label3+head(w1,w2,w3)8 labelw1+labelw2+PoS1+head(w1,w2) 20 label1+label2+label3+head(w1,w2,w3)+dist9 labelw1+labelw2+PoS2+head(w1,w2) 21 label1+label2+label3+lemma1+PoS2+head(w1,w2,w3)10 labelw1+labelw2+PoS1+PoS2+head(w1,w2) 22 label1+label2+label3+lemma1+PoS2+head(w1,w2,w3)+dist11 labelw1+labelw2+PoS1+#children2+head(w1,w2) 23 label1+label2+label3+lemma2+PoS1+head(w1,w2,w3)12 labelw1+labelw2+PoS2+#children1+head(w1,w2) 24 label1+label2+label3+lemma2+PoS1+head(w1,w2,w3)+dist# global features for constituents25 if |constituent| > 1 then label1st+labellast+labellast?1+PoSfirst+PoSlast+PoShead26 if |constituent| > 2 then label1st+label2d+label3d+PoSlast+PoSlast?1+PoShead+contains-?27 if |constituent| > 2 then label1st+label2d+label3d+PoSlast+PoSlast?1+lemmahead+contains-?28 if |constituent| > 3 then PoS1st+PoS2d+PoS3d+PoS4th+PoSlast+labelhead+contains-?+pos-head29 if |constituent| > 3 then PoSlast+PoSlast?1+PoSlast?2+PoSlast?3+PoSfirst+labelhead+contains-?+pos-head30 PoSfirst+PoSlast+lemmafirst+lemmalast+lemmahead+contains-?+pos-headTable 1: Feature schemas used for linearization (labelw is the label of the in-going edge to a word w inthe dependency tree; lemmaw is the lemma of w, and PoSw is the part-of-speech tag of w; head(w1,w2,.
.
. )
is a function which is 1 if w1 is the head, 2 if w2 is the head, etc.
and else 0; dist is the positionwithin the constituent; contains-?
is a boolean value which is true if the sentence contains a questionmark and false otherwise; pos-head is the position of the head in the constituent)4.2 Dependency Tree LinearizationSince we use unordered dependency trees as syn-tactic structures, our realizer has to find the opti-mal linear order for the lexemes of each depen-dency tree.
Algorithm 4 shows our linearizationalgorithm.
To order the dependency tree, we use aone classifier-approach for all languages?in con-trast to, e.g., Filippova and Strube (2009), who usea two-classifier approach for German.1The algorithm is again a beam search.
It startswith an elementary list for each node of the depen-dency tree.
Each elementary list is first extendedby the children of the node in the list; then, thelists are extended stepwise by the children of thenewly added nodes.
If the number of lists duringthis procedure exceeds the threshold of 1000, thelists are sorted in accordance with their score, andthe first 1000 are kept.
The remaining lists areremoved.
Afterwards, the score of each list is ad-justed according to a global score function whichtakes into account complex features such as thefirst word of a consitutent, last word, the head, andthe edge label to the head (cf.
Table 1 for the listof the features).
Finally, the nodes of the depen-1We decided to test at this stage of our work a uniformtechnology for all languages, even if the idiosyncrasies ofsome languages may be handled better by specific solutions.dency tree are ordered with respect to the highestranked lists.Only in a very rare case, the threshold of thebeam search is exceeded.
Even with a rich featureset, the procedure is very fast.
The linearizationtakes about 3 milliseconds in average per depen-dency tree on a computer with a 2.8 Ghz CPU.4.3 Morphological RealizationThe morphological realization algorithm selectsthe edit script in accordance with the highest scorefor each lemma of a sentence obtained duringtraining (see Algorithm 2 above) and applies thenthe scripts to obtain the word forms; cf.
Algo-rithm 5.Table 2 lists the feature schemas used for mor-phological realization.5 ExperimentsTo evaluate the performance of our realizer, wecarried out experiments on deep generation ofChinese, English, German and Spanish, startingfrom CoNLL ?09 shared task corpora.
The size ofthe test sets is listed in Table 3.22As in (Langkilde-Geary, 2002) and (Ringger et al,2004), we used Section 23 of the WSJ corpus as test set forEnglish.102Algorithm 3: Semantic generation//si, y semantic graph and its dependency treefor i?
1 to |I| // iteration over the training examples// build an initial treefor all n1 ?
si dotrees?
{} // initialize the constructed trees listfor all n2 ?
si doif n1 6= n2 thenfor all l ?
dependency-labels dotrees = trees ?
{(synt(n1),synt(n2),l)}trees?
sort-trees-descending-to-score(trees)trees?
look-forward(1000,sublist(trees,20))//assess at most 1000 edges of the 20 best treestree?
get-best-tree-due-to-score(trees)(s,t,l)?
first-added-edge(tree)// create the best treebest-tree?
(s,t,l)// compute the nodes that still need to be attachedrest?
nodes(si) - {s, t}while rest 6= ?
dotrees?
look-forward(1000,best-tree,rest)tree?
get-best-tree-due-to-score(trees)(s,t,l)?
first-added-edge(tree)best-tree?
best-tree ?
{ (s,t,l) }if (root(s,best-tree)) then rest?
rest - {s}else rest?
rest - {t}The performance of both the isolated stages andthe realizer as a whole has been assessed.5.1 Evaluation MetricsIn order to measure the correctness of the se-mantics to syntax mapping, we use the unlabeledand labeled attachment score as it commonly usedin dependency parsing.
The labeled attachmentscore (LAS) is the proportion of tokens that are as-signed both the correct head and the correct edgelabel.
The unlabeled attachment score (ULA) isthe proportion of correct tokens that are assignedthe correct head.To assess the quality of linearization, we usethree different evaluation metrics.
The first metricis the per-phrase/per-clause accuracy (acc snt.
),which facilitates the automatic evaluation of re-sults:acc = correct constituentsall constituentsAs second evaluation metric, we use a metricrelated to the edit distance:di = 1?
mtotal number of words(with m as the minimum number of deletionscombined with insertions to obtain the correct or-der (Ringger et al, 2004)).Algorithm 4: Dependency tree lineariza-tion//yi a dependency treefor i?
1 to |I| // iteration over the training examples// iterate over all nodes of the dependency tree yifor n?
1 to |yi| dosubtreen?
children(n) ?
{n}ordered-listsn?
{} // initializefor all m ?
subtreen dobeam?
{}for all l ?
ordered-lists dobeam?
beam ?
{ append(clone(l),m)}for all l ?
ordered-lists doscore(l)?
compute-score-for-word-list(l)sort-lists-descending-to-score(beam,score)if | beam | > beam-size thenbeam?
sublist(0,1000,beam)ordered-listsn?
beamscoreg(l)?
score(l) + compute-global-score(l)sort-lists-descending-in-score(beam,scoreg)Algorithm 5: Morphological realization// yi a dependency tree, and li an ordered list of lemmatafor l?
1 to |li| doscriptp?
predict-script(li,yi,l)forml?
apply-edit-dist-script(lemmal, scriptp)To be able to compare our results with (He etal., 2009) and (Ringger et al, 2004), we use theBLEU score as a third metric.For the asessment of the quality of the wordform generation, we use the accuracy score.
Theaccuracy is the ratio between correctly generatedword forms and the entire set of generated wordforms.For the evaluation of the sentence realizer as awhole, we use the BLEU metric.5.2 Experimental ResultsTable 4 displays the results obtained for the iso-lated stages of sentence realization and of the real-ization as a whole, with reference to a baseline andto some state-of-the-art works.
The baseline isthe deep sentence realization over all stages start-ing from the original semantic annotation in theCoNLL ?09 shared task corpora.Note, that our results are not fully comparablewith (He et al, 2009; Filippova and Strube, 2009)and (Ringger et al, 2004), respectively, since thedata are different.
Furthermore, Filippova andStrube (2009) linearize only English sentences103# features1 es+lemma2 es+lemma+m.feats3 es+lemma+m.feats+POS4 es+lemma+m.feats+POS+position5 es+lemma+(lemma+1)+m.feats6 es+lemma+(lemma+1)+POS7 es+lemma+(m.feats-1)+(POS-1)8 es+lemma+(m.feats-1)+(POS-1)+position9 es+m.feats+(m.feats-1)10 es+m.feats+(m.feats+1)11 es+lemma+(m.feats-1)12 es+m.feats+(m.feats-1)+(m.feats-2)13 es+m.feats+POS14 es+m.feats+(m.feats+1)15 es+m.feats+(m.feats+1)+lemma16 es+m.feats17 es+e0+e1+m.feats18 es+e0+e1+e2+m.feats19 es+e0+e1+e2+e3+m.feats20 es+e0+e1+e2+e3+e4+m.feats21 es+e0+m.featsTable 2: Feature schemas used for morphologicalrealizationChinese English German Spanish2556 2400 2000 1725Table 3: The number of sentences in the test setsused in the experimentsthat do not contain phrases that exceed 20,000 lin-earization options?which means that they filterout about 1% of the phrases.For Spanish, to the best of our knowledge, nolinearization experiments have been carried out sofar.
Therefore, we cannot contrast our results withany reference work.As far as morphologization is concerned, theperformance achieved by our realizer for Englishis somewhat lower than in (Minnen et al, 2001)(97.8% vs. 99.8% of accuracy).
Note, however,that Minnen et al describe a combined analyzer-generator, in which the generator is directly de-rived from the analyzer, which makes both ap-proaches not directly comparable.5.3 DiscussionThe overall performance of our SVM-based deepsentence generator ranges between 0.611 (for Ger-man) and 0.688 (for Chinese) of the BLEU score.HALogen?s (Langkilde-Geary, 2002) scores rangebetween 0.514 and 0.924, depending on the com-pleteness of the input.
The figures are not directlycomparable since HALogen takes as input syntac-tic structures.
However, it gives us an idea whereour generator is situated.Traditional linearization approaches are rule-based; cf., e.g., (Bro?ker, 1998; Gerdes and Ka-hane, 2001; Duchier and Debusmann, 2001), and(Bohnet, 2004).
More recently, statistic languagemodels have been used to derive word order, cf.
(Ringger et al, 2004; Wan et al, 2009) and (Fil-ippova and Strube, 2009).
Because of its partiallyfree order, which is more difficult to handle thanfixed word order, German has often been workedwith in the context of linearization.
Filippova andStrube (2009) adapted their linearization modeloriginally developed for German to English.
Theyuse two classifiers to determine the word orderin a sentence.
The first classifier uses a trigramLM to order words within constituents, and thesecond (which is a maximum entropy classifier)determines the order of constituents that dependon a finite verb.
For English, we achieve withour SVM-based classifier a better performance.As mentioned above, for German, Filippova andStrube (2009)?s two classifier approach pays offbecause it allows them to handle non-projectivestructures for the Vorfeld within the field model.It is certainly appropriate to optimize the perfor-mance of the realizer for the languages covered ina specific application.
However, our goal has beenso far different: to offer an off-the-shelf language-independent solution.The linearization error analysis, first of all ofGerman and Spanish, reveals that the annotationof coordinations in corpora of these languages as?X ?
and/or/.
.
.?
Y?
is a source of errors.
The?linear?
annotation used in the PropBank (?X ?and/or/.
.
.?
Y?)
appears to facilitate higher qual-ity linearization.
A preprocessing stage for au-tomatic conversion of the annotation of coordi-nations in the corpora would have certainly con-tributed to a higher quality.
We refrained fromdoing this because we did not want to distort thefigures.The morphologization error analysis indicatesa number of error sources that we will addressin the process of the improvement of the model.Among those sources are: quotes at the beginningof a sentence, acronyms, specific cases of start-ing capital letters of proper nouns (for English andSpanish), etc.104Chinese English German SpanishSemantics-Syntax (ULA/LAS) 95.71/86.29 94.77/89.76 95.46/82.99 98.39/93.00Syntax-Topology (di/acc) 0.88/64.74 0.91/74.96 0.82/50.5 0.83/52.77Syntax-Topology (BLEU) 0.85 0.894 0.735 0.78Topology-Morphology (accuracy=correct words/all words) ?
97.8 97.49 98.48All stages (BLEU) 0.688 0.659 0.611 0.68Baseline (BLEU) 0.12 0.18 0.11 0.14Syntax-Topology (He et al, 2009) (di/acc) 0.89/?
?
?
?Syntax-Topology (He et al, 2009) (BLEU) 0.887 ?
?
?Syntax-Topology (Filippova and Strube, 2009) (di/acc) ?
0.88/67 0.87/61 ?Syntax-Topology (Ringger et al, 2004) (BLEU) ?
0.836 ?
?Table 4: Quality figures for the isolated stages of deep sentence realization and the complete process.As far as the contrastive evaluation of the qual-ity of our morphologization stage is concerned,it is hampered by the fact that for the traditionalmanually crafted morphological generators, it isdifficult to find thorough quantitative evaluations,and stochastic morphological generators are rare.As already repeatedly pointed out above, so farwe intentionally refrained from optimizing the in-dividual realization stages for specific languages.Therefore, there is still quite a lot of room for im-provement of our realizer when one concentrateson a selected set of languages.6 ConclusionsWe presented an SVM-based stochastic deep mul-tilingual sentence generator that is inspired by thestate-of-the-art research in semantic parsing.
Ituses similar techniques and relies on the same re-sources.
This shows that there is a potential forstochastic sentence realization to catch up withthe level of progress recently achieved in parsingtechnologies.The generator exploits recently availablemultilevel-annotated corpora for training.
Whilethe availability of such corpora is a condition fordeep sentence realization that starts, as is usuallythe case, from semantic (predicate-argument)structures, we discovered that current annotationschemata do not always favor generation suchthat additional preprocessing is necessary.
Thisis not surprising since stochastic generation is avery young field.
An initiative of the generationcommunity would be appropriate to influencefuture multilevel annotation campaigns or to feedback the enriched annotations to the ?official?resources.3The most prominent features of our generatorare that it is per se multilingual, it achieves an ex-tremely broad coverage, and it starts from abstractsemantic structures.
The last feature allows us tocover a number of critical generation issues: sen-tence planning, linearization and morphologicalgeneration.
The separation of the semantic, syn-tactic, linearization and morphological levels ofannotation and their modular processing by sep-arate SVM decoders also facilitates a subsequentintegration of other generation tasks such as re-ferring expression generation, ellipsis generation,and aggregation.
As a matter of fact, this gen-erator instantiates the Reference Architecture forGeneration Systems (Mellish et al, 2006) for lin-guistic generation.A more practical advantage of the presenteddeep stochastic sentence generator (as, in prin-ciple, of all stochastic generators) is that, iftrained on a representative corpus, it is domain-independent.
As rightly pointed out by Belz(2008), traditional wide coverage realizers suchas KPML (Bateman et al, 2005), FUF/SURGE(Elhadad and Robin, 1996) and RealPro (Lavoieand Rambow, 1997), which were also intendedas off-the-shelf plug-in realizers still tend to re-quire a considerable amount of work for integra-tion and fine-tuning of the grammatical and lexicalresources.
Deep stochastic sentence realizers havethe potential to become real off-the-shelf modules.Our realizer is freely available for download athttp://www.recerca.upf.edu/taln.3We are currently working on a generation-oriented mul-tilevel annotation of corpora for a number of languages.
Thecorpora will be made available to the community.105AcknowledgmentsMany thanks to the three anonymous reviewers fortheir very valuable comments and suggestions.ReferencesBangalore, S. and O. Rambow.
2000.
Exploiting aProbabilistic Hierarchical Model for Generation.
InProceedings of COLING ?00, pages 42?48.Bangalore, S., J. Chen, and O. Rambow.
2001.
Impactof Quality and Quantity of Corpora on StochasticGeneration.
In Proceedings of the EMNLP Confer-ence, pages 159?166.Bateman, J.A., I.
Kruijff-Korbayova?, and G.-J.
Krui-jff.
2005.
Multilingual Resource Sharing AcrossBoth Related and Unrelated Languages: An Imple-mented, Open-Source Framework for Practical Nat-ural Language Generation.
Research on Languageand Computation, 15:1?29.Belz, A.
2008.
Automatic generation of weatherforecast texts using comprehensive probabilisticgeneration-space models.
Natural Language Engi-neering, 14(4):431?455.Bohnet, B.
2004.
A graph grammar approach to mapbetween dependency trees and topological models.In Proceedings of the IJCNLP, pages 636?645.Bro?ker, N. 1998.
Separating Surface Order and Syn-tactic Relations in a Dependency Grammar.
In Pro-ceedings of the COLING/ACL ?98.Crammer, K., O. Dekel, S. Shalev-Shwartz, andY.
Singer.
2006.
Online Passive-Aggressive Al-gorithms.
Journal of Machine Learning Research,7:551?585.Duchier, D. and R. Debusmann.
2001.
Topological de-pendency trees: A constraint-based account of lin-ear precedence.
In Proceedings of the ACL.Elhadad, M. and J. Robin.
1996.
An overview ofSURGE: A reusable comprehensive syntactic real-ization component.
Technical Report TR 96-03,Department of Mathematics and Computer Science,Ben Gurion University.Filippova, K. and M. Strube.
2008.
Sentence fusionvia dependency graph compression.
In Proceedingsof the EMNLP Conference.Filippova, K. and M. Strube.
2009.
Tree lineariza-tion in English: Improving language model basedapproaches.
In Proceedings of the NAACL ?09 andHLT, Short Papers, pages 225?228.Gerdes, K. and S. Kahane.
2001.
Word order in Ger-man: A formal dependency grammar using a topo-logical hierarchy.
In Proceedings of the ACL.Hajic?, J. et al 2009.
The CoNLL-2009 Shared Task:Syntactic and Semantic Dependencies in MultipleLanguages.
In Proceedings of the CoNLL.He, W., H. Wang, Y. Guo, and T. Liu.
2009.
De-pendency based chinese sentence realization.
InProceedings of the ACL and of the IJCNLP of theAFNLP, pages 809?816.Knight, K. and V. Hatzivassiloglou.
1995.
Two-level,many paths generation.
In Proceedings of the ACL.Langkilde, I. and K. Knight.
1998.
Generation thatexploits corpus-based statistical knowledge.
In Pro-ceedings of the COLING/ACL, pages 704?710.Langkilde-Geary, I.
2002.
An empirical verificationof coverage and correctness for a general-purposesentence generator.
In Proceedings of the SecondINLG Conference, pages 17?28.Lavoie, B. and O. Rambow.
1997.
A fast and portablerealizer for text generation systems.
In Proceedingsof the 5th Conference on ANLP.Levenshtein, V.I.
1966.
Binary codes capable of cor-recting deletions, insertions, and reversals.
SovietPhysics, 10:707?710.Mellish, C., D. Scott, L. Cahill, D. Paiva, R. Evans, andM.
Reape.
2006.
A reference architecture for natu-ral language generation systems.
Natural LanguageEngineering, 12(1):1?34.Minnen, G., J. Carroll, and D. Pearce.
2001.
Ap-plied morphological processing for English.
Nat-ural Language Engineering, 7(3):207?223.Oh, A.H. and A.I.
Rudnicky.
2000.
Stochastic lan-guage generation for spoken dialogue systems.
InProceedings of the ANL/NAACL Workshop on Con-versational Systems, pages 27?32.Palmer, M., D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?105.Ringger, E., M. Gamon, R.C.
Moore, D. Rojas,M.
Smets, and S. Corston-Oliver.
2004.
Linguis-tically informed statistical models of constituentstructure for ordering in sentence realization.
InProceedings of COLING, pages 673?679.Wan, S., M. Dras, Dale R., and C. Paris.
2009.
Im-proving Grammaticality in Statistical Sentence Gen-eration: Introducing a Dependency Spanning TreeAlgorithm with an Argument Satisfaction Model.
InProceedings of the EACL ?09, pages 852?860.106
