Coling 2010: Poster Volume, pages 1095?1103,Beijing, August 2010?Expresses-an-opinion-about?
: using corpus statistics in an informationextraction approach to opinion miningAsad B. Sayeed, Hieu C. Nguyen,and Timothy J. MeyerDepartment of Computer ScienceUniversity of Maryland, College Parkasayeed@cs.umd.edu,hcnguyen88@gmail.com,tmeyer1@umd.eduAmy WeinbergInstitute for Advanced Computer StudiesDepartment of LinguisticsUniversity of Maryland, College Parkweinberg@umiacs.umd.eduAbstractWe present a technique for identifying thesources and targets of opinions withoutactually identifying the opinions them-selves.
We are able to use an informa-tion extraction approach that treats opin-ion mining as relation mining; we iden-tify instances of a binary ?expresses-an-opinion-about?
relation.
We find thatwe can classify source-target pairs as be-longing to the relation at a performancelevel significantly higher than two relevantbaselines.This technique is particularly suited toemerging approaches in corpus-based so-cial science which focus on aggregatinginteractions between sources to determinetheir effects on socio-economically sig-nificant targets.
Our application is theanalysis of information technology (IT)innovations.
This is an example of amore general problem where opinion isexpressed using either sub- or supersetsof expressive words found in newswire.We present an annotation scheme and anSVM-based technique that uses the lo-cal context as well as the corpus-widefrequency of a source-target pair as datato determine membership in ?expresses-an-opinion-about?.
While the presenceof conventional subjectivity keywords ap-pears significant in the success of thistechnique, we are able to find the mostdomain-relevant keywords without sacri-ficing recall.1 IntroductionTwo problems in sentiment analysis consist ofsource attribution and target discovery?who hasan opinion, and about what?
These problems areusually presented in terms of techniques that re-late them to the actual opinion expressed.
We havea social science application in which the identifi-cation of sources and targets over a large volumeof text is more important than identifying the ac-tual opinions particularly in experimenting withsocial science models of opinion trends.
Con-sequently, we are able to use lightweight tech-niques to identify sources and targets without us-ing resource-intensive techniques to identify opin-ionated phrases.Our application for this work is the discoveryof networks of influence among opinion leadersin the IT field.
We are interested in answeringquestions about who the leaders in the field areand how their opinion matches the social and eco-nomic success of IT innovation.
Consequently,it became necessary for us to construct a system(figure 1) that finds the expressions in text that re-fer to an opinion leader?s activities in promotingor deprecating a technology.In this paper, we demonstrate an informationextraction (Mooney and Bunescu, 2005) approachbased in relation mining (Girju et al, 2007) thatis effective for this purpose.
We describe a tech-nique by which corpus statistics allow us to clas-sify pairs of entities and sentiment analysis targetsas instances of an ?expresses-an-opinion-about?relation in documents in the IT business press.This genre has the characteristic that many enti-ties and targets are represented within individualsentences and paragraphs.
Features based on the1095Figure 1: Opinion relation classification system.frequency counts of query results allow us to trainclassifiers that allow us to extract ?expresses-an-opinion-about?
instances, using a very simple an-notation strategy to acquire training examples.In the IT business press, the opinionated lan-guage is different from the newswire text forwhich many extant sentiment tools were devel-oped.
We use an existing sentiment lexicon along-side other non-sentiment-specific measures thatadapt resources from newswire-developed senti-ment analysis projects without imposing the fullcomplexity of those techniques.1.1 Corpus-based social scienceThe ?expresses-an-opinion-about?
relation is a bi-nary relation between opinion sources and tar-gets.
Sources include both people?typicallyknown experts, corporate representatives, andother businesspeople?as well as organizationssuch as corporations and government bodies.
Thetargets are the innovation terms.
Therefore, theuse of named-entity recognition in this projectonly focuses on persons and organizations, as thetargets are a fixed list.1.2 Reifying opinion in an applicationcontextA hypothesis implicit in our social science taskis that opinion leaders create trends in IT innova-tion adoption partly by the text that their activi-ties generate in the IT business press.
This texthas an effect on readers, and these readers act insuch a way that in turn may generate more or lessprominence for a given innovation?and may alsogenerate further text.Some of these text-generating activities includeexpressions of private states in an opinion source(e.g., ?I believe that Web 2.0 is the future?).
Thesekinds of expressions suggest a particular ontol-ogy of opinion analysis involving discourse re-lations across various types of clauses (Wilsonand Wiebe, 2005; Wilson et al, 2005a).
How-ever, if we are to track the relative adoption ofIT innovations, we must take into account theeffect of the text on the reader?s opinion aboutthese innovations?there are expressions otherthan those of private states that have an effect onthe reader.
These can be considered to be ?opin-ionated acts1.
?Opinionated acts can include things like pur-chasing and adoption decisions by organizations.For example:And like other top suppliers to Wal-Mart Stores Inc., BP has been in-volved in a mandate to affix radiofrequency identification tags with em-bedded electronic product codes to itscrates and pallets.
(ComputerWorld,January 2005)In this case, both Wal-Mart and BP have expressedimplicit approval for radio frequency identifica-tion by adopting it.
This may affect the reader?sown likelihood of support or adoption of the tech-nology.
In this context, we do not directly con-sider the subjectivity of the opinion source, eventhough that may be present.Opinionated acts include things like implica-tions of technology use, not just adoption.
Wethus define opinion expressions as follows: anyexpression involving some actor that is likely toaffect a reader?s own potential to adopt, reject, orspeak positively or negatively of a target.
Thiswould include ?conventional?
expressions of pri-vate states as well as opinionated acts.Our definition of ?expresses-an-opinion-about?follows immediately.
SourceA expresses an opin-ion about target B if an interested third party C?sactions towards B may be affected by A?s textu-ally recorded actions, in a context where actions1Somasundaran and Wiebe (2009) mention a related cate-gory of ?pragmatic opinions?
that involve world knowledge.1096have positive or negative weight (e.g.
purchasing,promotion, etc.
).1.3 Domain-specific sentiment detectionWe construct a system that uses named-entityrecognition and supervised machine learning viaSVMs to automatically discover instances of?expresses-an-opinion-about?
as a binary relationat reasonably high accuracy and precision.The advantage of our approach is that, outsideof HMM-based named-entity detection (BBN?sIdentiFinder), we evade the need for resource-intensive techniques such as sophsticated gram-matical models, sequence models, and semanticrole labelling (Choi et al, 2006; Kim and Hovy,2006) by removing the focus on the actual opinionexpressed.
Then we can use a simple superviseddiscriminative technique with a joint model of lo-cal term frequency information and corpus-wideco-occurrence distributions in order to discoverthe raw data for opinion trend modelling.
Themost complex instrument we use from sentimentanalysis research on conventional newswire is asentiment keyword lexicon (Wilson et al, 2005b);furthermore, our techniques allow us to distin-guish sentiment keywords that indicate opinion inthis domain from keywords that actually indicatethat there is no opinion relation between sourceand target.While we show that this lightweight techniqueworks well at a paragraph level, it can also be usedin conjunction with more resource-intensive tech-niques used to find ?conventional?
opinion ex-pressions.
Also, the use of topic aspects (Soma-sundaran and Wiebe, 2009) in conjunction withtarget names has been associated with an improve-ment in recall.
However, our technique still per-forms well above the baseline without these im-provements.2 Methodology2.1 Article preparationWe have a list of IT innovations on which ouropinion leader research effort is most closely fo-cused.
This list contains common names that re-fer to these technologies as well as some alternatenames and abbreviations.
We selected articles atrandom from the ComputerWorld IT journal thatcontained mentions of members of the given list.These direct mentions were tagged in the docu-ment as XML entities.Each article was processed by BBN?s Identi-Finder 3.3 (Bikel et al, 1999), a named entityrecognition (NER) system that tags named men-tions of person and organization entities2.The articles were then divided into paragraphs.For each paragraph, we generated candidate rela-tions from the entities and innovations mentionedtherein.
To generate candidates, we paired everyentity in the paragraph with every innovation.
Re-dundant pairs are sometimes generated when anentity is mentioned in multiple ways in the para-graph.
We eliminated most of these by removingentities whose mentions were substrings of othermentions.
For example, ?Microsoft?
and ?Mi-crosoft Corp.?
are sometimes found in the sameparagraph; we eliminate ?Microsoft.
?2.2 AnnotationWe processed 20 documents containing 157 rela-tions in the manner described in the previous sec-tion.
Then two domain experts (chosen from theauthors) annotated every candidate pair in everydocument according to the following scheme (il-lustrated in figure 2):?
If the paragraph associated with the candi-date pair describes a valid source-target rela-tion, the experts annotated it with Y.?
If the paragraph does not actually containthat source-target relation, the experts anno-tated it with N.?
If either the source or the target is misidenti-fied (e.g., errors in named entity recognition),the experts annotated it with X.The Cohen?s ?
score was 0.6 for two annotators.While this appears to be only moderate agree-ment, we are still able to achieve good perfor-mance in our experiments with this value.2In a separate research effort, we found that IdentiFinderhas a high error rate on IT business press documents, so webuilt a system to reduce the error post hoc.
We ran this sys-tem over the IdentiFinder annotations.1097Davis says she has especially enjoyed work-ing with the PowerPad?s bluetooth interfaces tophones and printers.
?It?s nice getting into newwireless technology,?
she says.
The bluetoothcapability will allow couriers to transmit datawithout docking their devices in their trucks.Source Target ClassDavis bluetooth Y/N/XPowerPad bluetooth Y/N/XFigure 2: Example paragraph annotation exercise.We then selected 75 different documents foreach annotator and processed and annotated themas above.
At this point we have the instances andthe classes to which they belong.
We labelled 466instances of Y, 325 instances of N, and 280 in-stances of X, for a total of 1071 relations.2.3 Feature vector generationWe have four classes of features for every rela-tion instance.
Each type of feature consists ofcounts extracted from an index of 77,227 Comput-erWorld articles from January 1988 to June 2008generated by the University of Massachusettssearch engine Indri (Metzler and Croft, 2004).Each vector is normalized to the unit vector.
Theindex is not stemmed for performance reasons.The first type of feature consists of simple doc-ument frequency statistics for source-target pairsthroughout the corpus.
The second type consistsof document frequency counts of source-targetpairs when they are in particularly close proxim-ity to one another.
The third type consists of docu-ment frequency counts of source target pairs prox-imate to keywords that reflect subjectivity.
Thefourth and final type consist of TFIDF scores ofvocabulary items in the paragraph containing theputative opinion-holding relation (unigram con-text features).
We use the first three features typesto represent the likelihood in the ?world?
that thesource has an opinion about the target and the lastfeature type to represent the likelihood of the spe-cific paragraph containing an opinion that reflectsthe source-target relation.We have a total of 7450 features.
Each vec-tor is represented as a sparse array.
806 featuresrepresent queries on the Indri index.
For all thefeatures, we therefore have 863,226 index queries.We perform the queries in parallel on 25 proces-sors to generate the full feature array, which takesapproximately an hour on processors running at8Ghz.
We eliminate all values that are smaller inmagnitude than 0.000001 after unit vector normal-ization.2.3.1 Frequency statisticsThere are two simple frequency statistics fea-tures generated from Indri queries.
The first isthe raw frequency counts of within-document co-occurrences of the source and target in the rela-tion.
The second is the mean co-occurrence fre-quency of the source and target per Computer-World document.2.3.2 Proximity countsFor every relation, we query Indri to check howoften the source and the target appear in the samedocument in the ComputerWorld corpus withinfour word ranges: 5, 25, 100, and 500.
That isto say, if a source and a target appear within fivewords of one another, this is included in the five-word proximity feature.
This generates four fea-tures per relation.2.3.3 Subjectivity keyword proximity countsWe augment the proximity counts feature witha third requirement: that the source and target ap-pear within one of the ranges with a ?subjectivitykeyword.?
The keywords are taken from Univer-sity of Pittsburgh subjectivity lexicon; the utilityof this lexicon is supported in recent work (Soma-sundaran and Wiebe, 2009).For performance reasons, we did not use all ofthe entries in the subjectivity lexicon.
Instead,we used a TFIDF-based measure to rank the key-words by their prevalence in the ComputerWorldcorpus where the term frequency is defined overthe entire corpus.
Then we selected 200 keywordswith the highest score.For each keyword, we use the same proximityranges (5, 25, 100, and 500) in queries to Indriwhere we obtain counts of each keyword-source-target triple for each range.
There are threfore 800subjectivity keyword features.1098Positive class Negative class System Prec / Rec / F AccuracyY N Random baseline 0.60 / 0.53 / 0.56 0.52Y N Maj.-class (Y) baseline 0.59 / 1.00 / 0.74 0.59Y N Linear kernel 0.70 / 0.73 / 0.72 0.66Y N RBF kernel 0.72 / 0.76 / 0.75 0.69Y N/X Random baseline 0.44 / 0.50 / 0.47 0.50Y N/X RBF kernel 0.65 / 0.55 / 0.59 0.67Table 1: Results with all features against majority class and random baselines.
All values are meanaverages under 10-fold cross validation.2.3.4 Word context (unigram) featuresFor each relation, we take term frequencycounts of the paragraph to which the relation be-longs.
We multiply them by the IDF of the termacross the ComputerWorld corpus.
This yields6644 features over all paragraphs.2.4 Machine learningOn these feature vectors, we trained SVM modelsusing Joachims?
(1999) svmlight tool.
We use aradial basis function kernel with an error cost pa-rameter of 100 and a ?
of 0.25.
We also use a lin-ear kernel with an error cost parameter of 100 be-cause it is straightforwardly possible with a linearkernel to extract the top features from the modelgenerated by svmlight.3 ExperimentsWe conducted most of our experiments with onlythe Y and N classes, discarding all X; this re-stricted most of our results to those assuming cor-rect named entity recognition.
Y was the posi-tive class for training the svmlight models, andN was the negative class.
We also performed ex-periments with N and X together being the nega-tive class; this represents the condition that we areseeking ?expresses-an-opinion-about?
even with ahigher named-entity error rate.We use two baselines.
One is a random base-line with uniform probability for the positive andnegative classes.
The other is a majority-class as-signer (Y is the majority class).The best system for the Y vs. N experiment wassubjected to feature ablation.
We first systemati-cally removed each of the four feature types indi-vidually.
The feature type whose removal had thelargest effect on performance was removed per-manently, and the rest of the features were testedwithout it.
This was done once more, at whichpoint only one feature type was present in themodels tested.3.1 EvaluationAll evaluation was performed under 10-fold crossvalidation, and we report the mean average of allperformance metrics (precision, recall, harmonicmean F-measure, and accuracy) across folds.We define these measures in the standard infor-mation retrieval form.
If tp represents true pos-itives, tn true negatives, fp false positives, andfn false negatives, then precision is tp/(tp+fp),recall tp/(tp + fn), F-measure (harmonic mean)is 2(prec ?
rec)/(prec + rec), and accuracy is(tp+ tn)/(tp+ fp+ fn+ tn).4 Results and discussionThe results of the experiments with all features arelisted in table 1.4.1 ?Perfect?
named entity recognitionWe achieve best results in the Y versus N case us-ing the radial basis function kernel.
We find im-provement in F-measure and accuracy at 19% and17% respectively.
Simply assigning the majorityclass to all test examples yields a very high re-call, by definition, but poor precision and accu-racy; hence its relatively high F-measure does notreflect high applicability to further processing, asthe false positives would amplify errors in our so-cial science application.The linear kernel has results that are below theRBF kernel for all measures, but are relativelyclose to the RBF results.1099Subjectivity Proximity Frequency Unigram Prec / Rec / F AccuracyX X X X 0.72 / 0.76 / 0.75 0.69X X X 0.67 / 0.89 / 0.76 0.67X X X 0.71 / 0.77 / 0.73 0.68X X X 0.70 / 0.78 / 0.74 0.67X X X 0.69 / 0.77 / 0.73 0.67X X 0.63 / 0.91 / 0.75 0.64X X 0.66 / 0.89 / 0.76 0.67X X 0.65 / 0.90 / 0.76 0.66X 0.61 / 0.92 / 0.73 0.60X 0.61 / 0.94 / 0.74 0.60Table 2: Feature ablation results for RBF kernel on Y vs. N case.
The first line is the RBF result withall features from table 1.4.2 Introducing erroneous named entitiesThe case of Y versus N and X together unsurpris-ingly performed worse than the case where namedentity errors were eliminated.
However, relative toits own random baseline, it performed well, witha 12% and 17% improvement in F-measure andaccuracy using the RBF kernel.
This suggests thatthe errors do not introduce enough noise into thesystem to produce a large decline in performance.As X instances are about 26% of the total andwe see a considerable drop in recall, we can saythat some of the X instances are likely to be similarto valid Y ones; indeed, examination of the namedentity recognizer?s errors suggests that some in-correct organizations (e.g.
product names) occurin contexts where valid organizations occur.
How-ever, precision and accuracy have not fallen nearlyas far, so that the quality of the output for furtherprocessing is not hurt in proportion to the intro-duction of X class noise.4.3 Feature ablationTable 2 contains the result of our feature abla-tion experiments.
Overall, the removal of featurescauses the SVM models to behave increasinglylike a majority class assigner.
As we mentionedearlier, higher recall at the expense of precisionand accuracy is not an optimal outcome for useven if the F-measure is preserved.
In our results,the F-measure values are remarkably stable.In the first round of feature removal, the sub-jectivity keyword features have the biggest ef-fect with the largest drop in precision and thelargest increase in recall; high-TFIDF words froma general-purpose subjectivity lexicon allow themodel to assign more items to the negative class.The next round of feature removal showsthat the proximity features have the next largestamount of influence on the classifier, as precisiondrops by 4%.
The proximity features are very sim-ilar to the subjectivity features in that they too in-volve queries over windows of limited word sizes;the subjectivity keyword features only differ inthat a subjectivity keyword must be within thewindow as well.
That the proximity features arenot more important than the subjectivity features,implies that the subjectivity keywords matter tothe classifier, even though they are not specific tothe IT domain.
However, the proximity of sourcesand targets also matters, even in the absence of thesubjectivity keywords.Finally, we are left with the frequency featuresand the unigram context features.
Either set offeatures supports a level of performance greaterthan the random baseline in table 1.
However,the unigram features allow for slightly better re-call than the frequency features without loss ofprecision, but this may not be very surprising, asthere are many more unigram features than fre-quency features.
More importantly, however, ei-ther of these feature types is sufficient to preventthe classifier from assigning the majority class allof the time, although they come close.1100Feature type Range KeywordSubjectivity 500 agreementSubjectivity 500 criticalSubjectivity 500 wantSubjectivity 100 willSubjectivity 100 ableSubjectivity 500 worthSubjectivity 500 benefitSubjectivity 100 tryingSubjectivity 500 largeSubjectivity 500 competitiveTable 3: The 10 most positive features via a linearkernel in descending order.Feature type Range KeywordSubjectivity 500 lowSubjectivity 500 ensureSubjectivity 25 wantSubjectivity 100 viceSubjectivity 500 slowSubjectivity 100 largeSubjectivity 500 readySubjectivity 100 actuallySubjectivity 100 readySubjectivity 100 againstTable 4: The 10 most negative features via a linearkernel in descending order.4.4 Most discriminative featuresThe models generated by svmlight under a lin-ear kernel allow for the extraction of featureweights by a script written by svmlight?s creator.We divided the instances into a single 70%/30%train/test split and trained a classifier with a linearkernel and an error cost parameter of 100, with re-sults similar to those reported under 10-fold cross-validation in table 1.
We used all features.Then we were able to extract the 10 most pos-itive (table 3) and 10 most negative (table 4) fea-tures from the model.Interestingly, all of these are subjectivity key-word features, even the negatively weighted fea-tures.
The top positive features are often evocativeof business language, such as ?agreement?, ?crit-ical?, and ?competitive?.
Most of them emergefrom queries at the 500-word range, suggestingthat their presence in the document itself is evi-dence that a source is expressing an opinion abouta target.
That most of them are subjectivity fea-tures is reflected in the feature ablation results inthe previous section.It is less clear why ?ensure?
and ?against?should be evidence that a source-target pair is notan instance of ?expresses-an-opinion-about?.
Onthe other hand, words like ?ready?
(which appearstwice) and ?actually?
can conceivably reflect sit-uations in the IT domain that are not matters ofopinion.
In either case, this demonstrates one ofthe advantages of our technique, as these are fea-tures that actively assist in classifying some rela-tion instances as not expressing sentiment.
For ex-ample, contrary to what we would expect, ?want?in a 25-word window with a source and a tar-get is actually evidence against an ?expresses-an-opinion-about?
relation in text about IT innova-tions (ComputerWorld, July 2007):But Klein, who is director of infor-mation services and technology, didn?twant IT to become the blog police.In this example, Klein is expressing a desire,but not about the innovation (blogs) in question.5 Conclusions and future work5.1 SummaryWe constructed and evaluated a system that de-tects at paragraph level whether entities relevantto the IT domain have expressed an opinion abouta list of IT innovations of interest to a larger socialscience research program.
To that end, we useda combination of co-occurrence statistics gleanedfrom a document indexing tool and TFIDF val-ues from the local term context.
Under thesenovel conditions, we successfully exceeded sim-ple baselines by large margins.Despite only moderate annotator agreement, wewere able to produce results coherent enough tosuccessfully train classifiers and conduct experi-ments.Our feature ablation study suggests that all ofthe feature types played a role in improving theperformance of the system over the random and1101majority-class baselines.
However, the subjec-tivity keyword features from an existing lexiconplayed the largest role, followed by the proxim-ity and unigram features.
Subjectivity keywordfeatures dominated the ranks of feature weightsunder a linear kernel, and the features most pre-dictive of membership in ?expresses-an-opinion-about?
are words with semantic significance in thecontext of the IT business press.5.2 Application to other domainsWe used somewhat na?
?ve statistics in a simplemachine learning system in order to implement aform of opinion mining for a particular domain.The most direct linguistic guidance we providedour system were the query ranges and the sub-jectivity lexicon.
The generality of this approachyields the advantage that it can be applied to otherdomains where there are ways of expressing senti-ment unique to those domains outside of newswiretext and product reviews.5.3 Improving the featuresOur use of an existing sentiment lexicon opens thedoor in future work for the use of techniques tobootstrap a larger sentiment lexicon that empha-sizes domain-specific language in the expressionof opinion, including opinionated acts.
In fact,our results suggest that terminology in the exist-ing lexicon that is most prominently weighted inour classifier also tends to be domain-relevant.
Ina further iteration, we might also improve perfor-mance by using terms outside the lexicon that tendto co-occur with terms from the lexicon.5.4 Data generationOur annotation exercise was a very simple one in-volving a short reading exercise and the selectionof one of three choices per relation instance.
Thistype of exercise is ideally suited to the ?crowd-sourcing?
technique of paying many individualssmall amounts of money to perform these simpleannotations over the Internet.
Previous research(Snow et al, 2008) suggests that we can generatevery large datasets very quickly in this way; thisis a requirement for expanding to other domains.5.5 ScalabilityIn order to classify on the order of 1000 instances,it took nearly a million queries to the Indri index,which took a little over an hour to do in parallelon 25 processors by calling the Indri query engineafresh at each query.
While each query is nec-essary to generate each feature value, there are anumber of optimizations we could implement toaccelerate the process.
Various types of dynamicprogramming and caching could be used to han-dle related queries.
One way of scaling up tolarger datasets would be to use the MapReduceand cloud computing paradigms on which textprocessing tools have already been implemented(Moreira et al, 2007).The application for this research is a social sci-ence exercise in exploring trends in IT adoptionby analysing the IT business press.
In the end, theperfect discovery of all instances of ?expresses-an-opinion-about?
is not as important as findingenough reliable data over a large number of docu-ments.
This work brings us several steps closer infinding the right combination of features in orderto acquire trend-representative data.AcknowledgementsThis paper is based upon work supported by theNational Science Foundation under Grant IIS-0729459.ReferencesBikel, Daniel M., Richard Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?sin a name.
Mach.
Learn., 34(1-3).Choi, Yejin, Eric Breck, and Claire Cardie.
2006.Joint extraction of entities and relations for opinionrecognition.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing (EMNLP).Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.Semeval-2007 task 04: classification of semantic re-lations between nominals.
In SemEval ?07: Pro-ceedings of the 4th International Workshop on Se-mantic Evaluations, pages 13?18, Morristown, NJ,USA.
Association for Computational Linguistics.Joachims, T. 1999.
Making large-scale SVM learn-ing practical.
In Scho?lkopf, B., C. Burges, and1102A.
Smola, editors, Advances in Kernel Methods -Support Vector Learning, chapter 11, pages 169?184.
MIT Press, Cambridge, MA.Kim, Soo-Min and Eduard Hovy.
2006.
Extractingopinions, opinion holders, and topics expressed inonline news media text.
In SST ?06: Proceedings ofthe Workshop on Sentiment and Subjectivity in Text,pages 1?8, Morristown, NJ, USA.
Association forComputational Linguistics.Metzler, Donald and W. Bruce Croft.
2004.
Combin-ing the language model and inference network ap-proaches to retrieval.
Information Processing andManagement, 40(5):735 ?
750.Mooney, Raymond J. and Razvan Bunescu.
2005.Mining knowledge from text using information ex-traction.
SIGKDD Explor.
Newsl., 7(1):3?10.Moreira, Jose?
E., Maged M. Michael, Dilma Da Silva,Doron Shiloach, Parijat Dube, and Li Zhang.
2007.Scalability of the nutch search engine.
In Smith,Burton J., editor, ICS, pages 3?12.
ACM.Rogers, Everett M. 2003.
Diffusion of Innovations,5th Edition.
Free Press.Snow, Rion, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: evaluating non-expert annotations for natu-ral language tasks.
In EMNLP 2008, Morristown,NJ, USA.Somasundaran, Swapna and Janyce Wiebe.
2009.Recognizing stances in online debates.
In ACL-IJCNLP ?09: Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 1.
Associationfor Computational Linguistics.Wilson, Theresa and Janyce Wiebe.
2005.
Annotatingattributions and private states.
In ACL 2005 Work-shop: Frontiers in Corpus Annotation II: Pie in theSky, pages 53?60.Wilson, Theresa, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patward-han.
2005a.
OpinionFinder: A system for subjec-tivity analysis.
In HLT/EMNLP.Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT/EMNLP.1103
