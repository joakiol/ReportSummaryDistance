IHMM Specialization with Selective Lexicalization*J i n -Dong K im and Sang-Zoo  Lee and Hae-Chang R imDept.
of Computer  Science and Engineering, Korea University,Anam-dong, Seongbuk-ku, Seoul 136-701, KoreaE-maih {jinlzoolrim}@nlp.korea.ac.krAbstractWe present a technique which complementsHidden Markov Models by incorporating somelexicalized states representing syntactically un-common words.
'Our approach examines thedistribution of transitions, selects the uncom-mon words, and makes lexicalized states for thewords.
We perfor'med a part-of-speech taggingexperiment on the Brown corpus to evaluate theresultant language model and discovered thatthis technique improved the tagging accuracyby 0.21% at the 95% level of confidence.1 In t roduct ionHidden Markov 'Models are widely used forstatistical language modelling in various fields,e.g., part-of-speech tagging or speech recogni-tion (Rabiner and Juang, 1986).
The modelsare based on Markov assumptions, which makeit possible to view the language prediction as aMarkov process.
'In general, we make the first-order Markov ass'umptions that the current agis only dependant on the previous tag and thatthe current word is only dependant on the cur-rent tag.
These are very 'strong' assumptions,so that the first-order Hidden Markov Modelshave the advantage of drastically reducing thenumber of its parameters.
On the other hand,the assumptions restrict the model from utiliz-ing enough constraints provided by the localcontext and the resultant model consults onlya single category 'as the contex.A lot of effort has been devoted in the pastto make up for the insufficient contextual in-formation of the first-order probabilistic model.The second order Hidden Markov Models with" The research underlying this paper was supportedt) 3" research grants fl'om Korea Science and EngineeringFoundation.appropriate smoothing techniques how betterperformance than the first order models and isconsidered a state-of-the-art technique (Meri-aldo, 1994; Brants, 1996).
The complexity ofthe model is however elatively very high con-sidering the small improvement of the perfor-mance.Garside describes IDIOMTAG (Garside etal., 1987) which is a component of a part-of-speech tagging system named CLAWS.
ID-IOMTAG serves as a front-end to the taggerand modifies ome initially assigned tags in or-der to reduce the amount of ambiguity to bedealt with by the tagger.
IDIOMTAG canlook at any combination of words and tags,with or without intervening words.
By usingthe IDIOMTAG, CLAWS system improved tag-ging accuracy from 94% to 96-97%.
However,the manual-intensive process of producing id-iom tags is very expensive although IDIOMTAGproved fruitful.Kupiec (Kupiec, 1992) describes a techniqueof augmenting the Hidden Markov Models forpart-of-speech tagging by the use of networks.Besides the original states representing eachpart-of-speech, the network contains additionalstates to reduce the noun/adjective confusion,and to extend the context for predicting pastparticiples from preceding auxiliary verbs whenthey are separated by adverbs.
By using theseadditional states, the tagging system improvedthe accuracy from 95.7% to 96.0%.
However,the additional context is chosen by analyzingthe tagging errors manually.An automatic refining technique for HiddenMarkov Models has been proposed by Brants(Brants, 1996).
It starts with some initial firstorder Markov Model.
Some states of the modelare selected to be split or merged to take intoaccount heir predecessors.
As a result, each of121new states represents a extended context.
Withthis technique, Brants reported a performancecquivalent o the second order Hidden MarkovModels.In this paper, we present an automatic re-fining technique for statistical language models.First, we examine the distribution of transitionsof lexicalized categories.
Next, we break out theuncommon ones from their categories and makenew states for them.
All processes are auto-mated and the user has only to determine theextent of the breaking-out.2 "S tandard"  Par t -o f -SpeechTagg ing  Mode l  based  on  HMMFrom the statistical point of view, the taggingproblem can be defined as the problem of find-ing the proper sequence of categories c:,r~ =Cl, c2, ..., cn (n _> 1) given the sequence of wordsw:,n = wl, w2, ...,wn (We denote the i'th wordby wi, and the category assigned to the wi byci), which is formally defined by the followingequation:"\]-(Wl,n) -= argmaxP(Cl,nlW:,~) (1)Charniak (Charniak et al, 1993) describesthe "standard" HMM-based tagging model asEquation 2, which is the simplified version ofEquation 1.nT(w:,~) = arg max I I  P(cilci-1)P(wilci) (2)i n . '
z - - - - IWith this model, we select the proper categoryfor each word by making use of the contextualprobabilities, P(citci_ 1), and the lexical prob-abilities, P(wilci).
This model has the advan-tages of a provided theoretical framework, auto-matic learning facility and relatively high per-formance.
It is thereby at the basis of most tag-ging programs created over the last few years.For this model, the first-order Markov assum-tions are made as follows:P(ci lcl , i - l ,Wl, i_l) ~ P(cilci-1) (3)P(wi\[cd (4)With Equation 3, we assume that the currentcategory is independent of the previous wordsand only dependent on the previous category.With Equation 4, we also assume that the cor-rect word is independent of everything exceptthe knowledge of its category.
Through theseassmnptions, the Hidden Markov Models havethe advantage of drastically reducing the num-ber of parameters, thereby alleviating the sparsedata problem.
However, as mentioned above,this model consults only a single category ascontext and does not utilize enough constraintsprovided by the local context.3 Some Ref in ing  Techn iques  forHMMTile first-order Hidden Markov Models de-scribed in the previous section provides onlya single category as context.
Sometimes, thisfirst-order context is sufficient to predict thefollowing parts-of-speech, but at other times(probably much more often) it is insufficient.The goal of the work reported here is to developa method that can automatically refine the Hid-den Markov Models to produce a more accu-rate language model.
We start with the care-ful observation on the assumptions which aremade for the "standard" Hidden Markov Mod-els.
With the Equation 3, we assume that thecurrent category is only dependent on the pre-ceding category.
As we know, it is not alwaystrue and this first-order Markov assumption re-stricts the disambiguation i formation witlfinthe first-order context.The immediate ways of enriching the contextare as follows:?
to lexicalize the context.?
to extend the context o higher-order.To lexiealize the context, we include the pre-ceding word into the context.
Contextual prob-abilities are then defined by P(eilci_l,Wi-1).Figure 1 illustrates the change of dependencywhen each method is applied respectively.
Fig-ure l(a) represents that each first-order contex-tual probability and lexical probability are in-dependent of each other in the "standard" Hid-den Markov Models, where Figure l(b) repre-sents that the lexical probability of the preced-ing word and the contextual probability of thecurrent category are tied into a lexicalized con-textual probability.To extend the context o higher-order, we ex-tend the contextual probability to the second-122Wi.1(a)Wi., I Wi.t(b) (c)Figure 1: Two Types of Weakening the MarkovAssumptionorder.
Contextual probabilities are then definedby P(cilci_l,Ci_2).
Figure l(c) represents thatthe two adjacent contextual probabilities aretied into the sec0nd-order contextual probabil-ity.The simple way of enriching the context is toextend or lexica!ize it uniformly.
The uniformextension of context to the second order is fea-sible with an appropriate smoothing techniqueand is considered a state-of-the-art technique,though its complexity is very high: In the caseof the Brown cerpus, we need trigrams up tothe number of 0.6 million.
An alternative to theuniform extension of context is the selective x-tension of context.
Brants(Brants, 1996) takesthis approach and reports a performance equiv-alent to the uniform extension with relativelymuch low complexity of the model.The uniform lexicalization of context is com-putationally prohibitively expensive: In thecase of the Brown corpus, we need lexicalizedbigrams up to the number of almost 3 billion.Moreover, manylof these bigrams neither con-tribute to the per~formance of the model, nor oc-cur frequently enough to be estimated properly.An alternative to the uniform lexicalization isthe selective lexicalization of context, which isthe main topic of this paper.4 Se lec t ive  Lex ica l i za t ion  o f  HMMThis section describes a new technique for re-fining the Hidden Markov Model, which we callselective lexicalization.
Our approach automat-ically finds out s'yntactically uncommon wordsand makes a new state (we call it a lexieal izedstate) for  each of the words.Given a fixed set of categories, {C 1 , C 2, ..., cC},e.g., {adjective,..., verb}, we assume the dis-crete random variable XcJ with domain the setof categories and range a set of conditional prob-abilities.
The random variable XcJ then repre-sents a process of assigning a conditional prob-ability p(cilc j) to every category c i (e i rangesover cl ...c C)xc (c = P (d= P(c21cJ)Xc) (c C) = p(cClc j)We convert the process of Xcj into the s ta tet rans i t ion  vector ,  VcJ , which consists of thecorresponding conditional probabilities, e.g.,Vprep - - - -  ( P(adjectiveiprep), ..., P(verbiprep)  T.The (squared) distance between two arbitraryvectors is then computed as follows:l~(V l ,  V2)  = (V l  -- v2)T (v1  - V2)  (5)Similarly, we define the lexical ized s ta tet rans i t ion  vector  1, VO,wk , e.g.,Vprep , i  n -~( P (adjectivelprep, in),..., P (verblprep, in)) YIn this situation, it is possible to regard eachlexicMized state transition vector, VcJ,wk, of thesame category cJ as members of a cluster whosecentroid is the state transition vector, Vc).
Wecan then compute the deviation of each lexi-calized state transition vector, Vc~,wk , from itscorresponding centroid.T D(Vc?,wk ) = (Vc~,~k-Vd) (VcJ,wJ-Vcj) (6)Figure 2 represents the distribution of lexical-ized state transition vectors according to theirdeviations.
As you can see in the figure, themajority of the vectors are near their centroidsand only a small number of vectors are very farfrom their centroids.
In the first-order contextmodel (without considering lexicalized context).1To alleviate the sparse data problenl, we smoothedthe lexicalized state transition probabilities by MacKayand Peto(MacKay and Peto, 1995)'s smoothing tech-nique.123180000160000140000120000100000~equency800006000040000200000f I I I I I I I I0.1 0.2 0.3 0.4Ai  ~ ?
_1  1 I0 0.5 0.6 0.7 0.8 0.9 1deviationFigure 2: Distribution of Lexicalized Vectors according to Deviationthe centroids represent all the members belong-ing to it.
In fact, the deviation of a vector is akind of (squared) error for the vector.
The errorfor a cluster ise(VcJ) = ~ D(Vd,wk ) (7)W kand the error for the overall model is simply thesum of the individual cluster errors:E = ~ e(Vg)  (8)cJNow, we could break out a few lexicalizedstate vectors which have large deviation (D > 0)and make them individual clusters to reduce theerror of the given model.As an example, let's consider the preposi -t ion cluster.
The value of each componentof the centroid, Vprep, is illustrated in Figure3(a) and that of the lexicalized vectors, Vprep,in,Vprep,wit h and Vprep,out are  in Figure 3(b), (c)and (d) respectively.
As you can see in these fig-ures, most of the prepositions including in andwith are immediately followed by article(AT),noun(NN) or pronoun(NP), but the word out aspreposition shows a completely different distri-bution.
Therefore, it would be a good choice tobreak out the lexicalized vector, Vprep,out , frolnits centroid, Vprep.outat  * - -  w i th(a) (b)Figure 4: Splitting the prepos i t ion  StateFrom the viewpoint of a network, the staterepresenting prepos i t ion  is split into twostates; the one is the state representing ordi-nary prepositions except out, and the other isthe state representing the special prepositionout, which we call a lex ical ized state.
Thisprocess of splitting is illustrated in Figure 4.Splitting a state results in some changes ofthe parameters.
The changes of the parame-ters resulting from lexicalizing a word, w k, in acategory, Cj, are  indicated in Table 1 (c i rangesover cl...cC).
This full splitting will increasethe complexity of the model rapidly, so thatestimating the parameters may suffer from thesparseness of the data.To alleviate it, we use the pseudo splittingwhich leads to relatively small increment of the1241I I I I I I l IT I  II1~1111111111~11 11111~11111171111111111 I I ITI I I  IT I I~I I I I I I INP AT NN PREP(a) preposition0 ,~llri71 I , l , l l l l , l l -r l l?
l l  , , , l l , l l l l l l l , i i i l i T i l i l  iliT, ii I I , i l l l lNP AT NN PREP(b) in as preposition1 !~0 JN, IiITI I I I t l , , l l l iT, IrT' l l l l l l~,,~ll~l l , l l l l l lTI l , I  li~Tllt ItT~ttttNP AT NN PREP(c) with as prepositionNP AT NN PREP(d) out as prepositionFigure 3: Transition Vectors in p repos i t ion  ClusterTable 1: Changes of Parameters in Full Split-tingbefore splitting after splittingP(w~lc y)P(cil~)P(dlc i )p(wilcJ, w k)p(wi lc  j , ~W k)P(dlcJ , w k)P(cilcJ, ~w k)P(cJ, w k Ic i)P(cJ, -~w~l ci)parameters.
The changes of the parameters inpseudo splitting ate indicated in Table 2.5 Exper imenta l  Resu l tWe have tested our technique through part-of-speech tagging eXperiments with the HiddenMarkov Models which are variously lexicalized.In ordcr to conduct he tagging experiments, wedivided the whole Brown (tagged) corpus con-taining 53,887 sentences (1,113,191 words) intotwo parts.
For tlle t ra in ing  set.
90% of theTable 2: Changes of Parameters in PseudoSplittingbefore splitting after splittingP(w' ld)  P(w~l d)P(cilc j) p(cilcJ, w k)p(cilc j, ~w k)P(elc P(c lc')sentences were chosen at random, from whichwe collected all of the statistical data.
We re-served the other 10% for testing.
Table 3 liststhe basic statistics of our corpus.Table 3: Overview of Our CorporaI # of sentences # of wordstraining set 48,499 1,001,712test set 5,388 111.479125We used a tag set containing 85 categories.The amount of ambiguity of the test set is sum-marized in Table 4.
The second column showsthat words to the ratio of 52% (the numberof 57,808) are not ambiguous.
The tagger at-tempts to resolve the ambiguity of the remain-ing words.Table 4: Amount of Ambiguity of Test SetI ambiguity(#) 3 4ratio(%) 5121:018 71 5 1 I total 100 IFigure 5 and Figure 6 show the results ofour part-of-speech tagging experiments with the"standard" Hidden Markov Model and vari-ously lexicalized Hidden Markov Models us-ing full splitting method and pseudo splittingmethod respectively.We got 95.7858% of the tags correct whenwe applied the standard Hidden Markov Modelwithout any lexicalized states.
As the num-ber of lexicalized states increases, the taggingaccuracy increases until the number of lexical-ized states becomes 160 (using full splitting)and 210 (using pseudo splitting).
As you cansee in these figures, the full splitting improvesthe performance of the model more rapidlybut suffer more sevelery from the sparsenessof the training data.
In this experiment, weemployed Mackay and Peto's smoothing tech-niques for estimating the parameters requiredfor the models.
The best precision has beenfound to be 95:9966% through the model withthe 210 lexcalized states using the pseudo split-ting method.6 Conc lus ionIn this paper, we present a method for comple-menting the Hidden Markov Models.
With thismethod, we lexicalize the Hidden Markov Modelseletively and automatically by examining thetransition distribution of each state relating tocertain words.Experimental results showed that the selec-tive lexicalization improved the tagging accu-rary from about 95.79% to about 96.00%.
Usingnormal tests for statistical significance we foundthat the improvement is significant at the 95%level of confidence.Tile cost for this imt~rovenmnt is minimal.The resulting network contains 210 additionallexicalized states which are found automati-cally.
Moreover, the lexicalization will not de-crease the tagging speed 2, because the lexi-calized states and their corresponding originalstates are exclusive in our lexicalized network,and thus the rate of ambiguity is not increasedeven if the lexicalized states are included.Our approach leaves much room for improve-ment.
We have so far considered only the outgo-ing transitions from the target states.
As a re-sult, we have discriminated only the words withright-associativity.
We could also discriminatethe words with left-associativity by examiningthe incoming transitions to the state.
Further-more, we could extend the context by using thesecond-order context as represented in Figurel(c).
We believe that the same technique pre-sented in this paper could be applied to the pro-posed extensions.Re ferencesT.
Brants.
1996.
Estimating markov modelstructures.
In Proceedings of the Fourth In-ternational Conference on Spoken LanguageProcessing, pages 893-896.E.
Charniak, C. Hendrickson, N. Jacobson,and M. Perkowitz.
1993.
Equations forpart-of-speech tagging.
In Proceedings of theEleventh National Conference on ArtificialIntelligence, pages 784-789.K.
Church.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.In Proceedings of the Second Conference onApplied Natural Language Processing, pages136-143.S.
Derose.
1988.
Grammatical category disam-biguation by statistical optimization.
Com-putational Linguistics, 14(1):31-39.R.
Garside, G. Leech, and G. Sampson.1987.
The Computational Analysis of En-glish.
Longman Group.J.
Kupiec.
1992.
Robust part-of-speech tag-ging using a hidden markoV model.
ComputerSpeech and Language, 6:225-242.D.
MacKay and L. Peto.
1995.
A hierarchicaldirichlet language model.
Natural LanguageEngineering, 1(3):289-307.2The Viterbi algorithm for finding the best tags runsin O(n 2) where n is the number of states.126iI0.960.9595.~ 0.9590.95850.9580.9575 I I I I50 100 150 200 250# of lexicalized statesFigure 5: POS tagging results with lexicalized HMM using full splitting method0.960.95950.959Q" 0.95850.958ff|0.9575 I I I I0 50 100 150 200# oflexicalized states250Figure 6: POS tagging results with lexicalized HMM using pseudo splitting methodB.
Merialdo.
1994.
Tagging english text with aprobabilistic model.
Computational Linguis-tics, 20(4):155-171.L.
Rabiner and B. Juang.
1986.
An introduc-tion to hiddeii markov models.
IEEE ASSPMagazine, pages 4-16, January.Appendix :Top 100 words with high deviationaccording(IN) l"ather(IN) out(IN) able(J )Aj(NP) no(QL) because(RB) however(RB)as(IN) per(IN) trying(VBG) however(WRB)likely(J ) Uuited(NP) Mrs.(NP) New(NP)Rhode(NP) .
National(NP) Miss(NP)tried(VBD) Dr)(NP) lack(NN) nmch(QL)Mr.(NP) North(NP) June(NP) A.(NP)J.
(NP) right(QL) May(NP) ready(J J)St.(NP) even(QL) various(J J) don't(DO)instead(RB) far(QL) B(NP) didn't(DOD)try(VB) available(JJ) William(NP)!(.)
?(.);(.)
number(NN) so(CS) due(J ) World(NP)Christian(NP) difficult (J J) tell(VB) go-ing(VBG) kind(NN) let(VB) continue(VB)series(NN) part(NN) radio(NN) sure(J )want(VB) front(NN) seem(VB) total(NN)decided(VBD) expected(VBN) right(NN)based(VBN) White(NP) except(IN) told(VBD)James(NP) fact(NN) March(NP) sort(NN)example(NN) designed(VBN) respect(NN)talk(VB) Department(NP) single(AP) Ne-gro(NP) wanted(VBD) Western(NP) yes(RB)become(VBN) necessary(J J) speak(VB)about(RB) amount(NN) down(IN)like(VB)S.(NP) same(AP) too(RB) General(NP)began(VBD) use(NN) tax(NN) got(VBN)127
