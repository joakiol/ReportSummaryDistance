Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 649?656,Sydney, July 2006. c?2006 Association for Computational LinguisticsWord Alignment in English-Hindi Parallel Corpus Using Recency-VectorApproach: Some StudiesNiladri ChatterjeeDepartment of MathematicsIndian Institute of Technology DelhiHauz Khas, New DelhiINDIA - 110016niladri iitd@yahoo.comSaumya AgrawalDepartment of MathematicsIndian Institute of TechnologyKharagpur, West BengalINDIA - 721302saumya agrawal2000@yahoo.co.inAbstractWord alignment using recency-vectorbased approach has recently become pop-ular.
One major advantage of these tech-niques is that unlike other approaches theyperform well even if the size of the par-allel corpora is small.
This makes thesealgorithms worth-studying for languageswhere resources are scarce.
In this workwe studied the performance of two verypopular recency-vector based approaches,proposed in (Fung and McKeown, 1994)and (Somers, 1998), respectively, for wordalignment in English-Hindi parallel cor-pus.
But performance of the above al-gorithms was not found to be satisfac-tory.
However, subsequent addition ofsome new constraints improved the perfor-mance of the recency-vector based align-ment technique significantly for the saidcorpus.
The present paper discusses thenew version of the algorithm and its per-formance in detail.1 IntroductionSeveral approaches including statistical tech-niques (Gale and Church, 1991; Brown et al,1993), lexical techniques (Huang and Choi, 2000;Tiedemann, 2003) and hybrid techniques (Ahren-berg et al, 2000), have been pursued to designschemes for word alignment which aims at estab-lishing links between words of a source languageand a target language in a parallel corpus.
Allthese schemes rely heavily on rich linguistic re-sources, either in the form of huge data of paralleltexts or various language/grammar related tools,such as parser, tagger, morphological analyser etc.Recency vector based approach has been pro-posed as an alternative strategy for word align-ment.
Approaches based on recency vectors typ-ically consider the positions of the word in thecorresponding texts rather than sentence bound-aries.
Two algorithms of this type can be found in(Fung and McKeown, 1994) and (Somers, 1998).The algorithms first compute the position vectorVw for the word w in the text.
Typically, Vw isof the form ?p1p2 .
.
.
pk?, where the pis indicatethe positions of the word w in a text T .
A newvector Rw, called the recency vector, is computedusing the position vector Vw, and is defined as?p2?p1, p3?p2, .
.
.
, pk?pk?1?.
In order to com-pute the alignment of a given word in the sourcelanguage text, the recency vector of the word iscompared with the recency vector of each targetlanguage word and the similarity between them ismeasured by computing a matching cost associ-ated with the recency vectors using dynamic pro-gramming.
The target language word having theleast cost is selected as the aligned word.The results given in the above references showthat the algorithms worked quite well in aligningwords in parallel corpora of language pairs con-sisting of various European languages and Chi-nese, Japanese, taken pair-wise.
Precision of about70% could be achieved using these algorithms.The major advantage of this approach is that it canwork even on a relatively small dataset and it doesnot rely on rich language resources.The above advantage motivated us to studythe effectiveness of these algorithms for aligningwords in English-Hindi parallel texts.
The corpusused for this work is described in Table 1.
It hasbeen made manually from three different sources:children?s storybooks, English to Hindi translationbook material, and advertisements.
We shall call649the three corpora as Storybook corpus, Sentencecorpus and Advertisement corpus, respectively.2 Word Alignment Algorithm: RecencyVector Based ApproachDK-vec algorithm given in (Fung and McKeown,1994) uses the following dynamic programmingbased approach to compute the matching costC(n,m) of two vectors v1 and v2 of lengths n andm, respectively.
The cost is calculated recursivelyusing the following formula,C(i, j) = |(v1(i)?
v2(j)|+min{C(i?
1, j),C(i?
1, j ?
1), C(i, j ?
1)}where i and j have values from 2 to n and 2 tom respectively, n and m being the number of dis-tinct words in source and target language corpusrespectively.
Note that vl(k) denotes the kth entryof the vector vl, for l = 1 and 2.
The costs areinitialised as follows.C(1, 1) = |v1(1)?
v2(1)|;C(i, 1) = |v1(i)?
v2(1)|+ C(i?
1, 1);C(1, j) = |v1(1)?
v2(j)|+ C(1, j ?
1);The word in the target language that has theminimum normalized cost (C(n,m)/(n + m)) istaken as the translation of the word considered inthe source text.One major shortcoming of the above scheme isits high computational complexity i.e.
O(mn).
Avariation of the above scheme has been proposedin (Somers, 1998) which has a much lower com-putational complexity O(min(m,n)).
In this newscheme, a distance called Levenshtein distance(S)is successively measured using :S = S +min{|v1(i+ 1)?
v2(j)|,|v1(i+1)?v2(j+1)|, |v1(i)?v2(j+1)|}The word in the target text having the minimumvalue of S (Levenshtein difference) is consideredto be the translation of the word in the source text.2.1 Constraints Used in the DynamicProgramming AlgorithmsIn order to reduce the complexity of the dynamicprogramming algorithm certain constraints havebeen proposed in (Fung and McKeown, 1994).1.
Starting Point Constraint: The constraint im-posed is: |first-occurrence of source languageword (w1) - first-occurrence of target lan-guage word w2| < 12?
(length of the text).2.
Euclidean distance constraint: The con-straint imposed is:?
(m1 ?m2)2 + (s1 ?
s2)2 < T , where mjand sj are the mean and standard deviation,respectively, of the recency vector of wj , j =1 or 2.
Here, T is some predefined threshold:3.
Length Constraint: The constraint imposedis: 12 ?
f2 < f1 < 2 ?
f2, where f1 and f2 arethe frequencies of occurrence of w1 and w2,in their respective texts.2.2 Experiments with DK-vec AlgorithmThe results of the application of this algorithmhave been very poor when applied on the threeEnglish to Hindi parallel corpora mentioned abovewithout imposing any constraints.We then experimented by varying the values ofthe parameters in the constraints in order to ob-serve their effects on the accuracy of alignment.As was suggested in (Somers, 1998), we also ob-served that the Euclidean distance constraint isnot very beneficial when the corpus size is small.So this constraint has not been considered in oursubsequent experiments.
Starting point constraintimposes a range within which the search for thematching word is restricted.
Although Fung andMcKeown suggested the range to be half of thelength of the text, we felt that the optimum valueof this range will vary from text to text depend-ing on the type of corpus, length ratio of the twotexts etc.
Table 2 shows the results obtained onapplying the DK vec algorithm on Sentence cor-pus for different lower values of range.
Similarresults were obtained for the other two corpora.The maximum increase observed in the F-score isaround 0.062 for the Sentence corpus, 0.03 for theStory book corpus and 0.05 for the Advertisementcorpus.
None of these improvements can be con-sidered to be significant.2.3 Experiments with Somers?
AlgorithmThe algorithm provided by Somers works by firstfinding all the minimum score word pairs usingdynamic programming, and then applying threefilters Multiple Alignment Selection filter, BestAlignment Score Selection filter and FrequencyRange constraint to the raw results to increase theaccuracy of alignment.The Multiple Alignment Selection(MAS) filtertakes care of situations where a single target lan-guage word is aligned with the number of source650Corpora English corpus Hindi corpusTotal words Distinct words Total words Distinct wordsStorybook corpus 6545 1079 7381 1587Sentence corpus 8541 1186 9070 1461Advertisement corpus 3709 1307 4009 1410Table 1: Details of English-Hindi Parallel CorporaRange Available Proposed Correct P% R% F-score50 516 430 34 7.91 6.59 0.077150 516 481 51 10.60 09.88 0.102250 516 506 98 19.37 18.99 0.192500 516 514 100 19.46 19.38 0.194700 516 515 94 18.25 18.22 0.182800 516 515 108 20.97 20.93 0.209900 516 515 88 17.09 17.05 0.1711000 516 516 100 19.38 19.38 0.1942000 516 516 81 15.70 15.70 0.1574535 516 516 76 14.73 14.73 0.147Table 2: Results of DK-vec Algorithm on Sentence Corpus for different rangelanguage words.
Somers has suggested that insuch cases only the word pair that has the mini-mum alignment score should be considered.
Table3 provides results (see column F-score old) whenthe raw output is passed through the MAS filtersfor the three corpora.
Note that for all the threecorpora a variety of frequency ranges have beenconsidered, and we have observed that the resultsobtained are slightly better when the MAS filterhas been used.The best F-score is obtained when frequencyrange is high i.e.
100-150, 100-200.
But herethe words are very few in number and are primar-ily pronoun, determiner or conjunction which arenot significant from alignment perspective.
Also,it was observed that when medium frequencyranges, such as 30-50, are used the best result,in terms of precision, is around 20-28% for thethree corpora.
However, since the corpus size issmall, here too the available and proposed alignedword pairs are very few (below 25).
Lower fre-quency ranges (viz.
2-20 and its sub-ranges) re-sult in the highest number of aligned pairs.
Wenoticd that these aligned word pairs are typicallyverb, adjective, noun and adverb.
But here toothe performance of the algorithm may be consid-ered to be unsatisfactory.
Although Somers hasrecommended words in the frequency ranges 10-30 to be considered for alignment, we have con-sidered lower frequency words too in our experi-ments.
This is because the corpus size being smallwe would otherwise have effectively overlookedmany small-frequency words (e.g.
noun, verb,adjective) that are significant from the alignmentpoint of view.Somers has further observed that if the BestAlignment Score Selection (BASS) filter is ap-plied to yield the first few best results of alignmentthe overall quality of the result improves.
Figure1 shows the results of the experiments done fordifferent alignment score cut-off without consid-ering the Frequency Range constraint on the threecorpora.
However, it was observed that the perfor-mance of the algorithm reduced slightly on intro-ducing this BASS filter.The above experiments suggest that the perfor-mance of the two algorithms is rather poor in thecontext of English-Hindi parallel texts as com-pared to other language pairs as shown by Fungand Somers.
In the following section we discussthe reasons for the low recall and precision values.2.4 Why Recall and Precision are LowWe observed that the primary reason for the poorperformance of the above algorithms in English- Hindi context is the presence of multiple Hindiequivalents for the same English word.
This canhappen primarily due to three reasons:651Figure 1: Results of Somers?
Algorithm and Improved approach for different score cut-offDeclension of Adjective: Declensions of adjec-tives are not present in English grammar.
No mor-phological variation in adjectives takes place alongwith the number and gender of the noun.
But,in Hindi, adjectives may have such declensions.For example, the Hindi for ?black?
is kaalaa whenthe noun is masculine singular number (e.g.
blackhorse ?
kaalaa ghodaa).
But the Hindi translationof ?black horses?
is kaale ghode; whereas ?blackmare?
is translated as kaalii ghodii.
Thus the sameEnglish word ?black?
may have three Hindi equiv-alents kaalaa, kaalii, and kale which are to be usedjudiciously by considering the number and genderof the noun concerned.Declensions of Pronouns and Nouns: Nouns orpronouns may also have different declensions de-pending upon the case endings and/or the genderand number of the object.
For example, the sameEnglish word ?my?
may have different forms (e.g.meraa, merii, mere) when translated in Hindi.For illustration, while ?my book?
is translated as?
merii kitaab, the translation of ?my name?
ismeraa naam.
This happens because naam is mas-culine in Hindi, while kitaab is feminine.
(Notethat in Hindi there is no concept of Neuter gen-der).
Similar declensions may be found with re-spect to nouns too.
For example, the Hindi equiv-alent of the word ?hour?
is ghantaa.
In pluralform it becomes ghante (e.g.
?two hours?
?
doghante).
But when used in a prepositional phrase,it becomes ghanto.
Thus the Hindi translation for?in two hours?
is do ghanto mein.Verb Morphology: Morphology of verbs inHindi depends upon the gender, number and per-son of the subject.
There are 11 possible suffixes(e.g taa, te, tii, egaa) in Hindi that may be at-tached to the root Verb to render morphologicalvariations.
For illustration,I read.
?
main padtaa hoon (Masculine) butmain padtii hoon (Feminine)You read.
?
tum padte ho (Masculine) ortum padtii ho (Feminine)He will read.
?
wah padegaa.Due to the presence of multiple Hindi equiva-lents, the frequencies of word occurrences differsignificantly, and thereby jeopardize the calcula-tions.
As a consequence, many English words arewrongly aligned.In the following section we describe certainmeasures that we propose for improving the effi-ciency of the recency vector based algorithms forword alignment in English - Hindi parallel texts.3 Improvements in Word AlignmentIn order to take care of morphological variations,we propose to use root words instead of variousdeclensions of the word.
For the present work thishas been done manually for Hindi.
However, al-gorithms similar to Porter?s algorithm may be de-veloped for Hindi too for cleaning a Hindi text ofmorphological inflections (Ramanathan and Rao,2003).
The modified text, for both English andHindi, are then subjected to word alignment.Table 4 gives the details about the root wordcorpus used to improve the result of word align-ment.
Here the total number of words for the threetypes of corpora is greater than the total numberof words in the original corpus (Table 1).
This isbecause of the presence of words like ?I?ll?
in theEnglish corpus which have been taken as ?I shall?in the root word corpus.
Also words like Unkaahave been taken as Un kaa in the Hindi root wordcorpus, leading to an increase in the corpus size.652Since we observed (see Section 2.2) that Eu-clidean distance constraint does not add signifi-cantly to the performance, we propose not to usethis constraint for English-Hindi word alignment.However, we propose to impose both frequencyrange constraint and length constraint (see Sec-tion 2.1 and Section 2.3).
Instead of the startingpoint constraint, we have introduced a new con-straint, viz.
segment constraint, to localise thesearch for the matching words.
The starting pointconstraint expresses range in terms of number ofwords.
However, it has been observed (see sec-tion 2.2) that the optimum value of the range varieswith the nature of text.
Hence no value for rangemay be identified that applies uniformly on differ-ent corpora.
Also for noisy corpora the segmentconstraint is expected to yield better results as thesearch here is localised better.
The proposed seg-ment constraint expresses range in terms of seg-ments.
In order to impose this constraint, first theparallel texts are aligned at sentence level.
Thesearch for a target language word is then restrictedto few segments above and below the current one.Use of sententially aligned corpora for wordalignment has already been recommended in(Brown et al, 1993).
However, the requirementthere is quite stringent ?
all the sentences are tobe correctly aligned.
The segment constraint pro-posed herein works well even if the text alignmentis not perfect.
Use of roughly aligned corpora hasalso been proposed in (Dagan and Gale, 1993) forword alignment in bilingual corpora, where statis-tical techniques have been used as the underlyingalignment scheme.
In this work, the sentence levelalignment algorithm given in (Gale and Church,1991) has been used for applying segment con-straint.
As shown in Table 5, the alignment ob-tained using this algorithm is not very good (only70% precision for Storybook corpus).
The threealigned root word corpora are then subjected tosegment constraint in our experiments.Next important decision we need to take whichdynamic programming algorithm should be used.Results shown in Section 2.2 and 2.3 demonstratethat the performance of DK-vec algorithm andSomers?
algorithm are almost at par.
Hence keep-ing in view the improved computational complex-ity, we choose to use Levenshtein distance as usedin Somers?
algorithm for comparing recency vec-tors.
In the following subsection we discuss theexperimental results of the proposed approach.3.1 Experimental Results and Comparisonwith Existing algorithmsWe have conducted experiments to determine thenumber of segments above and below the currentsegment that should be considered for searchingthe match of a word for each corpus.
In this re-spect we define i-segment constraint in which thesearch is restricted to the segments k ?
i to k + iof the target language corpus when the word un-der consideration is in the segment k of the sourcelanguage corpus.Evidently, the value of i depends on the accu-racy of sentence alignment.
Table 5 suggests thatthe quality of alignment is different for the threecorpora that we considered.
Due to the very highprecision and recall for Sentence corpus we haverestricted our search to the kth segment only, i.e.the value of i is 0.
However, since the results arenot so good for the Storybook and Advertisementcorpora we found after experimenting that the bestresults were obtained when i was 1.
During theexperiments it was observed that as the numberof segments was lowered or increased from theoptimum segment the accuracy of alignment de-creased continuously by around 10% for low fre-quency ranges for the three corpora and remainedalmost same for high frequency ranges.Table 3 shows the results obtained when seg-ment constraint is applied on the three corporaat optimum segment range for various frequencyranges.
A comparison between the F-score givenby algorithm in (Somers, 1998) (the column F-score old in the table) and the F-score obtainedby applying the improved scheme (the column F-score new in the table) indicate that the resultshave improved significantly for low frequencyranges.It is observed that the accuracy of alignment foralmost 95% of the available words has increasedsignificantly.
This accounts for words within lowfrequency range of 2?40 for Sentence corpus, 2?30 for Storybook corpus, and 2?20 for Advertise-ment corpus.
Also, most of the correct word pairsgiven by the modified approach are verbs, adjec-tives or nouns.
Also it was observed that as thenoise in the corpus increased the results becamepoorer.
This accounts for the lowest F-score val-ues for advertisement corpus.
The Sentence cor-pus, however, has been found to be the least noisy,and highest precision and recall values were ob-tained with this corpus.653Using Somers?
second filter on each corpus forthe optimum segment we found that the results atlow scores were better as shown in Figure 1.
Theword pairs obtained after applying the modifiedapproach can be used as anchor points for furtheralignment as well as for vocabulary extraction.
Incase of the Sentence corpus, best result for anchorpoints for further alignment lies at the score cutoff 1000 where precision and recall are 86.88%and 80.35% respectively.
Hence F-score is 0.835which is very high as compared to 0.173 obtainedby Somers?
approach and indicates an improve-ment of 382.65%.
Also, here the number of cor-rect word pairs is 198, whereas the algorithms in(Fung and McKeown, 1994) and (Somers, 1998)gave only 62 and 61 correct word pairs, respec-tively.
Hence the results are very useful for vo-cabulary extraction as well.
Similarly, Figure 2and Figure 3 show significant improvements forthe other two corpora.
At any score cut-off, themodified approach gives better results than the al-gorithms proposed in (Somers, 1998).4 ConclusionThis paper focuses on developing suitable wordalignment schemes in parallel texts where the sizeof the corpus is not large.
In languages, whererich linguistic tools are yet to be developed, oravailable freely, such an algorithm may prove tobe beneficial for various NLP activities, such as,vocabulary extraction, alignment etc.
This workconsiders word alignment in English - Hindi par-allel corpus, where the size of the corpus used isabout 18 thousand words for English and 20 thou-sand words for Hindi.The paucity of the resources suggests that sta-tistical techniques are not suitable for the task.On the other hand, Lexicon-based approaches arehighly resource-dependent.
As a consequence,they could not be considered as suitable schemes.Recency vector based approaches provide a suit-able alternative.
Variations of this approach havealready been used for word alignment in paralleltexts involving European languages and Chinese,Japanese.
However, our initial experiments withthese algorithms on English-Hindi did not producegood results.
In order to improve their perfor-mances certain measures have been taken.
Theproposed algorithm improved the performancemanifold.
This approach can be used for wordalignment in language pairs like English-Hindi.Since the available corpus size is rather smallwe could not compare the results obtained withvarious other word alignment algorithms proposedin the literature.
In particular we like to comparethe proposed scheme with the famous IBM mod-els.
We hope that with a much larger corpus sizewe shall be able to make the necessary compar-isons in near future.ReferencesL.
Ahrenberg, M. Merkel, A. Sagvall Hein, andJ.Tiedemann.
2000.
Evaluation of word alignmentsystems.
In Proc.
2nd International conference onLinguistic resources and Evaluation (LREC-2000),volume 3, pages 1255?1261, Athens, Greece.P.
Brown, S. A. Della Pietra, V. J. Della Pietra, , andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Compu-tational Linguistics, 19(2):263?311.K.
W. Church Dagan, I. and W. A. Gale.
1993.
Robustbilingual word alignment for machine aided transla-tion.
In Proc.
Workshop on Very Large Corpora:Academic and Industrial Perspectives, pages 1?8,Columbus, Ohio.P.
Fung and K. McKeown.
1994.
Aligning noisy par-allel corpora across language groups: Word pair fea-ture matching by dynamic time warping.
In Tech-nology Partnerships for Crossing the Language Bar-rier: Proc.
First conference of the Association forMachine Translation in the Americas, pages 81?88,Columbia, Maryland.W.
A. Gale and K. W. Church.
1991.
Identifying wordcorrespondences in parallel texts.
In Proc.
FourthDARPA Workshop on Speech and Natural Language,pages 152?157.
Morgan Kaufmann Publishers, Inc.Jin-Xia Huang and Key-Sun Choi.
2000.
Chinese ko-rean word alignment based on linguistic compari-son.
In Proc.
38th annual meeting of the associationof computational linguistic, pages 392?399, HongKong.Ananthakrishnan Ramanathan and Durgesh D. Rao.2003.
A lightweight stemmer for hindi.
In Proc.Workshop of Computational Linguistics for SouthAsian Languages -Expanding Synergies with Eu-rope, EACL-2003, pages 42?48, Budapest, Hungary.H Somers.
1998.
Further experiments in bilingual textalignment.
International Journal of Corpus Linguis-tics, 3:115?150.Jo?rg Tiedemann.
2003.
Combining clues word align-ment.
In Proc.
10th Conference of The EuropeanChapter of the Association for Computational Lin-guistics, pages 339?346, Budapest, Hungary.654Segment Constraint: 0-segment (Sentence Corpus)Frequency a p c P% R% F-score F-score %range (new) (old) increase2-5 285 181 141 77.90 49.74 0.61 0.118 416.903-5 147 108 81 75.00 55.10 0.64 0.169 278.693-10 211 152 119 78.29 56.40 0.61 0.168 263.105-20 146 103 79 76.70 54.12 0.64 0.216 196.2910-20 49 35 29 82.86 59.18 0.69 0.233 196.1420-30 19 12 9 75.00 47.37 0.58 0.270 114.6230-50 14 8 6 75.00 42.86 0.55 0.229 140.1740-50 4 2 2 100.00 50.00 0.67 0.222 201.8050-100 15 12 8 66.67 53.33 0.59 0.392 50.51100-200 6 5 5 100.00 83.33 0.91 0.91 -200-300 3 3 3 100.00 100.00 1.00 1.00 -Segment Constraint: 1-segment (Story book Corpus)2-5 281 184 89 48.37 31.67 0.38 0.039 874.353-5 143 108 52 48.15 36.36 0.41 0.042 876.195-10 125 89 35 39.39 28.00 0.33 0.090 266.6710-20 75 50 25 50.00 33.33 0.40 0.115 247.8310-30 117 76 39 51.32 33.33 0.41 0.114 259.6520-30 32 23 11 47.83 34.38 0.37 0.041 802.4330-40 14 8 2 25.00 14.29 0.18 0.100 8040-50 7 7 2 28.57 28.57 0.29 0.200 45.0050-100 11 10 2 20.00 18.18 0.19 0.110 72.72100-200 5 5 2 40.00 40.00 0.40 0.444 -Segment Constraint: 1-segment (Advertisement Corpus)2-5 411 250 67 26.80 16.30 0.20 0.035 471.433-5 189 145 41 28.28 21.69 0.25 0.073 242.473-10 237 172 48 27.91 20.03 0.23 0.075 206.675-20 107 73 27 36.99 25.23 0.30 0.141 112.7710-20 31 22 6 27.27 19.35 0.23 0.229 4.3710-30 40 28 8 32.14 22.50 0.26 0.247 5.2630-40 3 2 1 50.00 33.33 0.40 0.222 80.1830-50 3 2 1 50.00 33.33 0.40 0.222 80.1850-100 4 3 1 33.33 25.00 0.29 0.178 60.60100-200 2 2 0 0 0 - 1.000 -Table 3: Comparison of experimental results with Segment Constraint on the three Engish-Hindi parallelcorporaCorpora English corpus Hindi corpusTotal words Distinct words Total words Distinct wordsStorybook corpus 6609 895 7606 1100Advertisement corpus 3795 1213 4057 1198Sentence corpus 8540 1012 9159 1152Table 4: Experimental root word parallel corpora of English -Hindi655Different Corpora Actual alignment Alignment given Correct alignment R% P%in text by system given by systemAdvertisement corpus 323 358 253 78.32 70.68Storybook corpus 609 546 476 78.16 87.18Sentence corpus 4548 4548 4458 98.02 98.02Table 5: Results of Church and Gale Algorithm for Sentence level AlignmentFigure 2: Alignment Results for Sentence CorpusFigure 3: Alignment Results for Story Book Corpus656
