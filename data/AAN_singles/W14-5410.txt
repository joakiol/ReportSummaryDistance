Proceedings of the 25th International Conference on Computational Linguistics, pages 68?73,Dublin, Ireland, August 23-29 2014.Weakly supervised construction of a repository of iconic imagesLydia Weiland and Wolfgang Effelsberg and Simone Paolo PonzettoUniversity of MannheimMannheim, Germany{lydia,effelsberg,simone}@informatik.uni-mannheim.deAbstractWe present a first attempt at semi-automatically harvesting a dataset of iconic images, namelyimages that depict objects or scenes, which arouse associations to abstract topics.
Our methodstarts with representative topic-evoking images from Wikipedia, which are labeled with relevantconcepts and entities found in their associated captions.
These are used to query an online imagerepository (i.e., Flickr), in order to further acquire additional examples of topic-specific iconicrelations.
To this end, we leverage a combination of visual similarity measures, image clusteringand matching algorithms to acquire clusters of iconic images that are topically connected to theoriginal seed images, while also allowing for various degrees of diversity.
Our first results arepromising in that they indicate the feasibility of the task and that we are able to build a firstversion of our resource with minimal supervision.1 IntroductionFigurative language and images are a pervasive phenomenon associated with human communication.For instance, images used in news articles (especially on hot and sensitive topics) often make use ofnon-literal visual representations like iconic images, which are aimed at capturing the reader?s attention.For environmental topics, for instance, a windmill in an untouched and bright landscape surrounded bya clear sky is typically associated by humans with environmental friendliness, and accordingly causespositive emotions.
In a similar way, images of a polar bear on a drifting ice floe are typically associatedwith the topic of global warming (O?Neill and Smith, 2014).But while icons represent a pervasive device for visual communication, to date, there exists to the bestof our knowledge no approach aimed at their computational modeling.
In order to enable the overarchinggoal of producing such kind of models from real-world data, we focus, in this work, on the preliminarytask of semi-automatically compiling an electronic database of iconic images.
These consist, in ourdefinition, of images produced to create privileged associations between a particular visual representationand a referent.
Iconic images are highly recognizable for media users and typically induce negative orpositive emotions that have an impact on viewers?
attitudes and actions.
In order to model them from acomputational perspective, we initially formulate iconic image acquisition as a clustering task in which,given a set of initial, manually-selected ?seed?
images ?
e.g., a photo of a polar bear on a drifting ice floefor the topic of global warming, a smokestack for the topic of pollution, etc.
?
we use their associatedtextual descriptions in order to collect related images from the Web.
We then process these images usingstate-of-the-art image understanding techniques to produce clusters of semantically similar, yet differentimages depicting the same topic in an iconic way.The acquisition of a database of iconic images represents the first step towards a full-fledged model tocomputationally capture the phenomenon of iconic images in context.
Our long-term vision is to coverall three aspects of content (what makes an image iconic?
), usage (in which context are iconic imagesused?
), and effects (which negative/positive emotions do iconic images evoke on viewers?)
of iconicimages.
To make this challenging problem feasible, we opt in this preliminary step for an approach thatviews the task of understanding iconic images as the ability to build a dataset for further research.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/68Figure 1: Our framework for the semi-automatic acquisition of iconic images from the Web.2 MethodOur method for the semi-automatic acquisition of iconic images consists of five phases (Figure 1):Seed selection.
In the first phase of our approach we provide our pipeline with human-selected exam-ples of iconic images that help us bootstrap the image harvesting process.
To this end, we initially focuson a wide range of twelve different abstract topics that can be typically represented using iconic images(Table 1).
Selecting initial examples describing a visual iconic representation of a topic can be a dauntingtask, due to their volatile and topic-dependent nature.
In this work, we explore the use of Web encyclo-pedic resources in order to collect our initial examples.
We start with the encyclopedic entries fromNational Geographic Education1, an on-line resource in which human expert editors make explicit useof prototypical images to visually represent encyclopedic entries like ?agriculture?, ?climate change?,etc.. For instance, the encyclopedic entry for ?air pollution?
contains images of smokestacks, coolingtowers release steam, and so on (cf.
Table 1).
We use these (proprietary) images to provide us withhuman-validated examples of iconic images, and use these to identify (freely available) similar imageswithin Wikipedia pages based on a Google image search restricted to Wikipedia ?
e.g., by searching forsmokestack site:wikipedia.org.
We then use Wikipedia to create an initial dataset of iconicvisuals associated with the textual descriptions found in their captions.Text-based image search.
In the next step, we make use of a query-by-text approach in order to collectadditional data and enlarge our dataset with additional images depicting iconic relations.
To this end, westart by collecting the entities annotated within the image captions (e.g., ?Cumberland Power Plant atCumberland City?
), and manually determine their relevance to the associated topic (e.g., smokestacksand air pollution).
This is because, to build a good query, we need to provide the search systems witha good lexicalization (i.e., keywords) of the underlying information need (i.e., the topic).
Consequently,we extract entities from each caption of our initial set of images and use these to query additional data.For each seed, we generate a query by concatenating the entity labels in the captions and send it toFlickr2.
We then filter the data by retaining only photos with title, description, and tags where both, tagsand description (caption and title) contain the query words.
This method provides us with around 4000additional images and text pairs.Image clustering.
Text-based image search results can introduce noise in the dataset, e.g., cases of?semantic mismatch?
where the caption and tags do not appropriately describe the scene found in theimage.
In this work, we explore the application of image clustering techniques to cope with this issue.For each topic we start with a set of instances made up of the seed images and the crawled one, andgroup them into clusters based on image similarity measures.
Clusters are built by calculating the linearcorrelation ?
i.e., which we take as a proxy for a similarity measure ?
from the HSV-histograms of eachimage, and applying the K-Means algorithm.
Clustering on the basis of HSV-histograms does not takeinto account the semantic content of images, since images with different meanings can still have thesame HSV-histogram.
Nevertheless, this approach makes it possible to spot those outliers in the imagesets that do not relate well to the other images retrieved with the same query.Image filtering.
The next processing step focuses instead on rule-driven filtering to improve the initialclustering-based filtering.
We first apply a face detection and HoG (histogram of gradients) descriptor for1http://education.nationalgeographic.com2http://flickr.com69Topic Themes of seed imagesAdaption hummingbird, king snake, koalaAgriculture cattle, ploughing, rice terraces, tropical fruitsAir balloon, sky viewAir Pollution smokestack, Three Mile Island, wildfireBiodiversity Amazonas, blue starfish, cornflowers, fungi, Hopetoun FallsCapital Capitol Hill, Prac?a Dos Tr?es, Washington MonumentClimate Mykonos (mild climate), Sonoran Desert, tea plantation (cool climate)Climate Change polar bear, volcano, dry lakeClimate Refugee climate refugees from Indonesia, Haiti, Pakistan, etc.Ecosystem bison, flooded forest, Flynn Reef, harp seal, rainforest, thorn treeGlobal Warming deforestation, flooding, smokestackGreenhouse Effect smokestack, steam engine train (smoke emissions)Table 1: Overview of our covered topics and the themes associated with their seed images.detecting people (Viola and Jones, 2001; Dalal and Triggs, 2005)3.
Next, we filter our data as follows.If faces or people are recognized in the picture, and the caption is judged to be related to entities of typeperson (e.g., farmers iconically depicting the topic of agriculture), the instance is retained in the dataset.On the other hand, if faces and/or people are recognized, but the caption is not related to entities of typeperson (e.g., a blue linckia, which is a physical object), we filter out the image from the dataset.Image matching.
The filtered clusters we built so far are still problematic in that they do not accountfor diversity ?
i.e., we do not want as the outcome of our method to end up with clusters made up only ofvarious pictures of the very same object (e.g., the cooling towers of the Three Mile Island power complexfor the topic of air pollution, possibly seen from different perspectives, times of the day, etc.).
That is,in our scenario we would like to promote heterogeneous clusters which still retain a high-level semanticmatch with the initial seeds (e.g., smokestacks or cooling towers belonging to different plants).
To thisend, we explore in this work an approach that leverages different image matching methods together atthe same time to automatically capture these visual semantic matches.Initially, for each cluster we select the image that minimizes the sum over all squared distances fromthe other images in the cluster.
That is, given a cluster C = {c1.
.
.
cn}, we collect the image c?
=argminci?C?cj?C?{ci}(ci?
cj)2.
We call this the prototype of the cluster.
Several image processingmethods are then used to compare the prototype of each cluster with the original seed images, with theaim to detect high-level content similarity (i.e., distinct, yet similar objects such as the smokestacks ofdifferent plants, etc.)
and account for diversity with respect to our initial seeds.
The first method is atemplate matching approach, based on minimum and maximum values of gray levels, which, togetherwith their location are used to detect similar textures.
The matching method is based on a correlationcoefficient matching (Brunelli, 2009).
In parallel, we explore an alternative approach where images andprototypes are compared using SIFT-based features (Lowe, 2004).
Finally, we apply a contour matchingmethod: we use a manually chosen threshold of the largest 10% of contours of an image to reduce thenoise from non-characteristic contours like dots, points or other smaller structures.
The matching ofcontours is based on rotation invariant moments (Hu, 1962).
When a good match is found, boundingboxes are drawn around the contours.The three methods provide evidence for potential matches between regions of each input prototype andseed pair.
Information from each single method is then combined by intersecting their respective outputs:i) the patch, where the template matching is found is compared against the coordinates where relevantSIFT features are detected (SIFT-Template); ii) the template matching patch is tested for intersection withthe bounding boxes of the matched contours (Template-Contour); iii) the bounding boxes of the contours3We focus on face and people detection since these are both well studied areas in computer vision for which a wide rangeof state-of-the-art methods exist.702-matches all matchesTopicP R F1P R F1Adaption 100.0 57.2 72.8 66.7 10.9 18.7Agriculture 50.0 35.4 41.4 0.0 0.0 0.0Air 84.2 75.6 79.7 66.7 15.8 25.5Air Pollution 65.9 83.7 73.7 65.1 32.4 43.3Biodiversity 54.0 40.6 46.3 34.4 8.1 13.1Capital 61.7 54.6 57.9 50.6 12.6 20.2Climate 93.7 81.6 87.2 89.1 20.0 32.7Climate Change 88.5 78.1 83.0 50.0 21.4 30.0Climate Refugee 40.0 50.0 44.4 0.0 0.0 0.0Ecosystem 73.7 61.7 67.2 43.3 11.4 18.0Global Warming 65.9 71.0 68.3 43.0 21.7 28.8Greenhouse Effect 100.0 81.6 89.9 100.0 34.2 51.0Table 2: Performance results per topic on iconic image detection (percentages).are checked for relevant SIFT features (SIFT-Contour).
Finally, we group together the prototype withthe seed icon of the corresponding topic in case at least two or three of the single matching strategiesin i?iii) identify the same regions in the images.
This process is repeated until all prototypes have beenexamined: prototypes for which the no match can be found are filtered out as being not iconic.3 EvaluationDataset statistics.
We first provide statistics on the size of the datasets created with our approach.Using HSV correlation we initially generate 1232 clusters with an average size of 27.37 elements percluster.
Additional filtering based on at least two of our image matching methods produces 870 clusters(19.33 elements on average), whereas the more restrictive clustering based on all three methods gives261 small-sized clusters of only 5.8 instances on average.
This is because, naturally, applying matching-based filtering tends to produce a smaller number of clusters with fewer elements.Gold standard and filtering evaluation.
To produce a gold standard for our task, we annotated all ofthe 4,000 images we retrieved from Flickr.
Each image is associated with a keyword query (Section 2):accordingly, we annotated each instance as being iconic or not with respect to the topic expressed by thekeywords ?
e.g., given a picture of Hopetoun Falls, whether it captures the concept of waterfall or not.This is because, in our work, we take keywords as proxies of the underlying topics (e.g., biodiversity isdepicted using waterfalls): in this setting, negative instances consist of mismatches between the querytext and the picture ?
e.g., a photography taken near Hopetoun Falls, showing beech trees and thuscapturing a query search for ?forest?
rather than ?waterfalls?.We next evaluate our system on the binary classification task of detecting whether an image is iconicor not.
In our case, we can quantify performance by taking all images not filtered out in the last step ofimage matching (and thus deemed as iconic in the final system output), and comparing them against ourgold-standard annotations.
This way we can compute standard metrics of precision, recall and balancedF-measure.
Our results indicate that combining the output of two image matching techniques allows us toreach 59.5% recall and 68.5% precision, whereas requiring all three methods to match reduces precision(46.9%) while drastically decreasing recall (14.3%).
The results show that our system is precision-oriented, and that filtering based on the combination of all methods leads to an overall performancedegradation.
This is because requiring all methods to match gives an over-constrained filtering: ourmethods, in fact, tend to match all together only with those images which are highly similar to the seeds,thus not being able to produce heterogeneous clusters.We finally compute performance metrics for each single topic in turn, in order to experimentallyinvestigate the different degrees of performance of our system, and determine whether some topics are71more difficult than others (Table 2).
Our results indicate that some topics are indeed more difficult thanothers ?
e.g., our system exhibits perfect precision on ?adaptation?
and ?greenhouse effect?
vs. muchpoorer one on ?biodiversity?
or ?climate refugee?.
This is because some topics are bootstrapped fromless heterogeneous, and hence ?easier?, sets of seed images (e.g., all smokestacks, as in ?greenhouseeffect?, are very similar to each other).
In general, this seems to point out that one of the key challengesin our scenario is to produce highly precise clusters, while allowing for image diversity as a trade-off.Error analysis.
We finally looked at the output of our system, in order to better understand its per-formance, as well as problems and future challenges.
Examples of a few sample clusters are shown inFigure 2.
These clusters show that, thanks to our method, we are able to collect quite diverse, yet iconicimages retaining a topical affinity with the original seeds ?
e.g., the poster on fighting deforestation orthe drawing used to depict air pollution.
Due to the noise of our base image processing components, how-ever, we also suffer from wrong matches such as the picture of a mobile phone for the topic of wildfire,where the meaning of a rapidly spreading conflagration is related to air pollution, whereas the mobilephone is not.
Based on a random sample of 10% of the output clusters, we manually identified the mainsources of errors as related to: i) false image matching due to problems with contour detection; ii) SIFTperforming best for detecting different images of the same objects, but exhibiting lower performance onthe more complex task of detecting similar objects; iii) we applied our image matching methods usingdefault parameters and thresholds: further improvements could be obtained by in-domain tuning.4 ConclusionsIn this work, we presented some initial steps in developing a methodology to computationally modelthe challenging phenomenon of iconic images.
More specifically, we focused on the task of building arepository of iconic images in a minimally supervised way by bootstrapping based on images found onWeb encyclopedic resources.As future work, we plan to better combine heterogeneous information from text and images, as wellas use deeper representations for both information sources ?
cf.
joint semantic representations such asspecific LDA-based topic models and bags of visual (SIFT) features (Rasiwasia et al., 2010; Feng andLapata, 2010).
This, in turn, can be applied to a variety of different tasks such as the automatic se-mantification of captions, query generation and expansion.
On the computer vision side, we are insteadparticularly interested in exploring region-growing algorithms to detect textured or homogeneous re-gions, and to allow for a segmentation of textured regions without contours, e.g., a cloudy sky or a viewof a forest landscape.Downloads The dataset presented in this paper is freely available for research purposes at https://madata.bib.uni-mannheim.de/87/.ReferencesRoberto Brunelli.
2009.
Template Matching Techniques in Computer Vision: Theory and Practice.
Wiley Pub-lishing.Navneet Dalal and Bill Triggs.
2005.
Histograms of oriented gradients for human detection.
In Proc.
of CVPR,pages 886?893.Yansong Feng and Mirella Lapata.
2010.
Topic models for image annotation and text illustration.
In Proc.
of HLT?10, pages 831?839.Ming-Kuei Hu.
1962.
Visual pattern recognition by moment invariants.
IRE Transactions on Information Theory,8(2):179?187.David G. Lowe.
2004.
Distinctive image features from scale-invariant keypoints.
International Journal of Com-puter Vision, 60(2):91?110.S.
O?Neill and Nicholas Smith.
2014.
Climate change and visual imagery.
Wiley Interdisciplinary Reviews:Climate Change, 5(1):73?87.Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert R.G.
Lanckriet, Roger Levy, andNuno Vasconcelos.
2010.
A new approach to cross-modal multimedia retrieval.
In Proc.
of MM ?10, pages251?260.Paul Viola and Michael Jones.
2001.
Rapid object detection using a boosted cascade of simple features.
In Proc.of CVPR, pages 511?518.72Figure 2: Sample iconic image clusters.
Above a poor cluster on wildfire, below two good clusters onpollution and deforestation.73
