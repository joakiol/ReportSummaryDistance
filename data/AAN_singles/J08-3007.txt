Computational Approaches to Morphology and SyntaxBrian Roark and Richard Sproat(Oregon Health and Science University and University of Illinois atUrbana?Champaign)Oxford: Oxford University Press (Oxford surveys in syntax and morphology, edited byRobert D. Van Valin Jr, volume 4), 2007, xx+316 pp; hardbound, ISBN 978-0-19-927477-2,$110.00, ?60.00; paperbound, ISBN 978-0-19-927478-9, $45.00, ?24.99Reviewed byNoah A. SmithCarnegie Mellon UniversityBrian Roark and Richard Sproat have written a compact and very readable book survey-ing computational morphology and computational syntax.
This text is not introductory;instead, it will help bring computational linguists who do not work on morphologyor syntax up to date on these areas?
latest developments.
Certain chapters (in particu-lar, Chapters 2 and 8) provide especially good starting points for advanced graduatecourses or seminars.
The text is divided into an Introduction and Preliminaries chapter,four chapters on computational approaches to morphology, and four chapters on com-putational approaches to syntax.
The morphology chapters focus primarily on formaland theoretical issues, and are likely to be of interest to morphologists, computationaland not.
The syntax chapters are drivenmore by engineering goals, withmore algorithmdetails.
Because a good understanding of probabilistic modeling is assumed, thesechapters will also be useful for machine learning researchers interested in languageprocessing.Despite the authors?
former affiliations, this book is not an AT&T analogue ofBeesley and Karttunen?s (2003) pedagogically motivated text on the Xerox finite-statetools.
This text is not about the AT&T FSM libraries or the algorithms underlying them(cf.
Roche and Schabes 1997).1.
Chapter 1: Introduction and PreliminariesThe first chapter is a take-no-prisoners introduction to finite-state automata and trans-ducers and their semiring-weighted generalizations.
Algorithms (e.g., for FST compo-sition) are discussed but not presented in detail.
Epsilon removal, minimization, anddeterminization are mentioned but not defined.
This material is probably too cursory toserve as a lone introduction for those wishing to fully understand weighted FSTs, butthat lack of understanding will not be an impediment in the ensuing chapters becauseweights do not re-surface until chapter 6, in the context of n-gram models, and eventhen the algebraic view given here is not mentioned.The chapter concludes with a defense of the place of finite-state models in linguis-tics, followed by a clear explanation of the trade-offs in computational linguistics (e.g.,between computational cost, expressive power, and annotation cost).2.
Part I (Chapters 2?5): Computational Approaches to MorphologyThese chapters are primarily an argument for the effectiveness of finite-state transducersin modeling natural language morphology.Computational Linguistics Volume 34, Number 3Chapter 2 provides a laundry list of morphological phenomena, arguing that finite-state composition captures each of them, even in cases where there is a more obvioussolution (e.g., finite-state concatenation for concatenative phenomena).
Examples ofmany kinds of phenomena are given from diverse languages: prosodic restrictions inYowulmne, phonological effects of German affixes, and subsegmental morphology inWelsh, to name a few.
Importantly, the compile-reduce and merge operations are arguedto be syntactic sugar for effects achievable by finite-state composition, so that even root-and-pattern Arabic morphology is explained in the same algebraic framework.Reduplication effects, of course, challenge finite-state explanations, and so receivetheir own section.
Extended (non-regular) computational models are presented along-side data from Gothic, Dakota, and Sye.
The authors speculate that, in contrast with thecommonly accepted Correspondence theory, Morphological Doubling theory (Inkelasand Zoll 1999), if correct, would imply that a non-regular ?copying?
process is not atwork in reduplication.
It is at this point that the reader may experience some discomfort;should the reduplication problem be addressed in syntax rather than morphology?Where exactly does the boundary lie?
Readers hoping for a reassessment of this bound-ary, or even a new bridge over it, will not find it here.Chapter 3 begins with Stump?s (2001) two-dimensional taxonomy of morphologicaltheories, which appears rather divorced from the rich work on finite-state computationalmorphology in Chapter 2.
The subtleties among the four types of theories (lexical vs. in-ferential and incremental vs. realizational, a more nuanced breakdown of the debateover ?item-and-arrangement?
vs.
?item-and-process?)
may be difficult to understandfor the reader not trained in morphological theory, but resolution comes quickly.
Weare presented with a series of examples showing ?proof-of-concept?
fragmentary im-plementations (in AT&T?s lextools) of phenomena in Sanskrit, Swahili, and Breton toargue that lexical-incrementalist and inferential-realizational theories are computation-ally equivalent; both can be implemented using FSTs and can lead to the same models.Chapter 4 gives an algebraic analysis of Koskenniemi?s (1983) ?KIMMO?
two-level morphological analysis system.
Koskenniemi?s hand-coded morphology rules areargued to be a historical accident; if only computers had been more powerful in the1980s, compilation of those rules into FSTs might have been automated, and in factKaplan and Kay had already developed the algorithms.1 In the spirit of the previouschapter, Sproat and Roark also note that morphological accounts that use one, two, ormore ?cascaded?
levels are all computationally equivalent rational relations under thefinite-state approach, and that Optimality Theory can (under certain assumptions aboutconstraints) be implemented with finite-state operations as well (Ellison 1994).Chapter 5, ?Machine learning of morphology,?
focuses on unsupervised morphol-ogy induction methods.
There is about a page of discussion on statistical languagemodeling approaches for disambiguation in agglutinative languages; no mention ismade of the more recent use of discriminative machine learning in morphologicaldisambiguation (Kudo, Yamamoto, and Matsumoto 2004; Habash and Rambow 2005;Smith, Smith, and Tromble 2005).
The chapter focuses on the approaches of Goldsmith(2001), Schone and Jurafsky (2001), and Yarowsky and Wicentowski (2001).
Althougheach approach is interesting on its own, little effort is made to unify work in this area,and none to bring the reader back full circle to finite-state models or the problem ofinducing from data regular grammars (Stolcke and Omohundro 1993) or their weights1 The authors rightly point out that Koskenniemi deserves much credit for building an implementationthat aimed to have broad coverage, not merely a proof-of-concept.454Book Reviews(Eisner 2002).
Another missed opportunity here is the recent introduction of Bayesianlearning for word segmentation (Goldwater, Griffiths, and Johnson 2006).Part I, in summary, aims to reduce many accounts of morphological phenomenato finite-state transducer composition, drawing on a wealth of illustrative examples.Twenty-two languages are listed in the language index at the end of the book, and,tellingly, all of them are discussed exclusively in Part I.
These chapters are good diplo-macy toward theoretical linguistics, showing how computational arguments can havetheoretical implications.3.
Part II (Chapters 6?9): Computational Approaches to SyntaxIn Part II, Roark and Sproat turn to models of syntax in computational linguistics.Because most research in this area has been on English, English parsing is what theypresent.Chapter 6 covers finite-state approaches to syntax, including n-gram models andsmoothing, class-based language models, hidden Markov models (though without aformal definition), part-of-speech tagging, log-linear models, and shallow parsing/chunking.
The Forward, Viterbi, Viterbi n-best, Forward?Backward algorithms, and?Forward?Backward Decoding?
(also known as posterior or minimum Bayes risk de-coding) are covered with examples.
This chapter is not as leisurely as the treatmentsof HMMs by Manning and Schu?tze (1999) or Charniak (1993), and it omits basic back-ground on probabilistic modeling.
For example, why must we ensure that an n-grammodel?s total probability sums exactly to one?
The answer relies on an understandingof perplexity and its use in evaluation, now in decline (cf.
?stupid backoff?
in Brantset al 2007).
The chapter does not reconnect with the algebraic view presented in Chap-ter 1; for example, the connection between HMMs and WFSAs is never expressed.Chapter 7 introduces context-free grammars and their parsers, broken down into?deterministic?
and ?nondeterministic?
approaches.2 Probabilistic CFGs and treebanksare introduced informally alongside the latter, which may confuse some readers.
Am-biguity is only presented as a natural phenomenon, not a problem of crude, over-generating grammars.
The probabilistic CKY and Earley algorithms are presented.
TheInside?Outside algorithm is presented in the context of Goodman?s (1996) maximumexpected recall parsing (another instance of minimum Bayes risk).
As in the case of thedynamic programming algorithms for HMMs in Chapter 6, the exposition is probablytoo brisk to be an introduction to the topic.Chapter 8 contains a thoughtful discussion of many best practices in statistical pars-ing: treebank ?decoration?
techniques such as parent annotation and lexicalization, andthe probabilitymodels underlying the parsers of Collins (1997) and Charniak (1997).
De-pendency parsing, unsupervised grammar induction, and finite-state approximations toPCFGs are allotted short sections.Chapter 9 covers context-sensitive models of syntax.
Unification-based parsingis presented at a high level, without formal details of unification or the differencesbetween theories such as LFG and HPSG.
The ?lexicalized?
models (TAG and CCG)are treated more thoroughly; pseudocode for a TAG dynamic programming parser isprovided.
There is brief treatment of Data-Oriented Parsing, reranking (a section that2 These terms, though in wide use, are misnomers.
All of these parsers are deterministic, since noneinvolve randomness or nondeterministic behavior resulting from multiple processors.
Here?(non)deterministic?
refers to the grammar, not the parser.455Computational Linguistics Volume 34, Number 3would have been of more practical use in Chapter 8), and transduction grammars (i.e.,grammars over more than one string, most frequently used in machine translation).The abundance of dynamic programming algorithms in Part II leads to the questionof whether such algorithms can be more easily taught (and unified) using recursiveequations (Manning and Schu?tze 1999), or a more declarative framework (Shieber,Schabes, and Pereira 1995).
Readers who prefer procedural pseudocode will find it here,though the book does not address implementation tricks for storing and indexing parsecharts, or agenda-ordering methods to make parsing efficient.These chapters are neither a gentle introduction to probabilistic modeling of syntaxfor linguists nor a handbook for the language engineer who wants to build an efficient,competitive parser.
(There is also no advice on the relative merits of today?s parsersavailable for download.)
The audience that will find Part II most valuable will beresearchers who understand the principles of probabilistic modeling but want a moreup-to-date view of statistical parsing than offered by Manning and Schu?tze (1999), withmore coverage of advanced topics than Jurafsky and Martin (2008).
This group mightinclude structured machine-learning researchers interested in the nuances of naturallanguage parsing and computational linguists who do not work on syntax but want tokeep up with the area.4.
ConclusionThe two major parts of this book stand as clear, up-to-date, and concisely writtensummaries of particular sub-fields in computational linguistics: finite-state morphologyand English syntactic processing.
The book does a fine job of elucidating the trade-offs that make computational linguistics a tightrope act, and therefore serves as gooddiplomacy for researchers in related fields.
At 112 and 146 pages, respectively, either ofthe parts is readable on a half-day plane or train trip.Today, the strongest bridge between morphology and syntax is the Chomsky hier-archy, which is mentioned frequently in this book (but never depicted).
The contrastbetween Parts I and II implies blueprints for more bridges: data resources to supportmore powerful learning algorithms for morphology (as we have seen in syntax), astronger influence of non-English data on computational syntactic modeling (as wehave seen in morphology), and practical ways to accomplish the amalgamation ofmorphology and syntax.
This reviewer believes Computational Approaches to Morphologyand Syntax will re-introduce the two sub-communities to each other and help each toleverage the successes of the other.ReferencesBeesley, Kenneth R. and Lauri Karttunen.2003.
Finite State Morphology.
CSLIPublications, Stanford, CA.Brants, Thorsten, Ashok C. Popat, Peng Xu,Franz J. Och, and Jeffrey Dean.
2007.
Largelanguage models in machine translation.In Proceedings of the Joint Conference onEmpirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning, pages 858?867, Prague.Charniak, Eugene.
1993.
Statistical LanguageLearning.
MIT Press, Cambridge, MA.Charniak, Eugene.
1997.
Statistical parsingwith a context-free grammar and wordstatistics.
In Proceedings of the 14th NationalConference on Artificial Intelligence,pages 598?603, Providence, RI.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.
InProceedings of the 35th Annual Meeting of theAssociation for Computational Linguistics,pages 16?23, Madrid.Eisner, Jason.
2002.
Parameter estimation forprobabilistic finite-state transducers.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics,pages 1?8, Philadelphia, PA.Ellison, T. Mark.
1994.
Phonologicalderivation in Optimality Theory.
In456Book ReviewsProceedings of the 15th InternationalConference on Computational Linguistics,vol.
2, pages 1007?1013, Kyoto.Goldsmith, John.
2001.
Unsupervisedacquisition of the morphology of a naturallanguage.
Computational Linguistics27(2):153?198.Goldwater, Sharon, Thomas L. Griffiths, andMark Johnson.
2006.
Contextualdependencies in unsupervised wordsegmentation.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics,pages 673?680, Sydney.Goodman, Joshua.
1996.
Parsing algorithmsand metrics.
In Proceedings of the 34thAnnual Meeting of the Association forComputational Linguistics, pages 177?183,Santa Cruz, CA.Habash, Nizar and Owen Rambow.
2005.Arabic tokenization, part-of-speechtagging, and morphologicaldisambiguation in one fell swoop.
InProceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics,pages 573?580, Ann Arbor, MI.Inkelas, Sharon and Cheryl Zoll.
1999.Reduplication as morphological doubling.Technical report 412-0800, RutgersOptimality Archive.Jurafsky, Daniel and James H. Martin.
2008.Speech and Language Processing (2ndedition).
Prentice Hall, Upper SaddleRiver, NJ.Koskenniemi, Kimmo.
1983.
Two-LevelMorphology: A General Computational Modelfor Word-Form Recognition and Production.Ph.D.
thesis, Department of GeneralLinguistics, University of Helsinki,Helsinki, Finland.Kudo, Taku, Kaoru Yamamoto, and YujiMatsumoto.
2004.
Applying conditionalrandom fields to Japanese morphologicalanalysis.
In Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing, pages 230?237, Barcelona.Manning, Christopher D. and HinrichSchu?tze.
1999.
Foundations of StatisticalNatural Language Processing.
MIT Press,Cambridge, MA.Roche, Emmanuel and Yves Schabes(editors).
1997.
Finite-State LanguageProcessing.
MIT Press, Cambridge, MA.Schone, Patrick and Daniel Jurafsky.
2001.Knowledge-free induction of morphologyusing latent semantic analysis.
InProceedings of the 5th Conference onComputational Natural Language Learning,pages 67?72, Toulouse.Shieber, Stuart and Yves Schabes andFernando C. N. Pereira.
1995.
Principlesand implementation of deductiveparsing.
Journal of Logic Programming24(1?2):3?36.Smith, Noah A., David A. Smith, and RoyW.
Tromble.
2005.
Context-basedmorphological disambiguation withrandom fields.
In Proceedings of the HumanLanguage Technology Conference andConference on Empirical Methods in NaturalLanguage Processing, pages 475?482,Vancouver.Stolcke, Andreas and Stephen Omohundro.1993.
Hidden Markov model induction byBayesian model merging.
In Stephen Jose?Hanson, Jack D. Cowen, and C. Lee Giles,editors, Advances in Neural InformationProcessing Systems, vol.
5.
MorganKaufmann, San Mateo, CA, pages 11?18.Stump, Gregory T. 2001.
InflectionalMorphology: A Theory of ParadigmStructure.
Cambridge University Press,Cambridge, UK.Yarowsky, David and Richard Wicentowski.2001.
Minimally supervised morphologicalanalysis by multimodal alignment.
InProceedings of the 39th Annual Meeting of theAssociation for Computational Linguistics,pages 207?216, Toulouse.Noah A. Smith is an assistant professor at Carnegie Mellon University.
He conducts research instatistical models and learning algorithms for natural language processing, including morphol-ogy and syntax, as well as applications such as machine translation and question answering.Smith?s address is Language Technologies Institute, School of Computer Science, Carnegie Mel-lon University, 5000 Forbes Avenue, Pittsburgh, PA 15213; e-mail: nasmith@cs.cmu.edu; URL:www.cs.cmu.edu/?nasmith.457
