Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 50?59,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsAutomated Essay Scoring Based on Finite State Transducer: towards ASRTranscription of Oral English SpeechXingyuan Peng?, Dengfeng Ke?, Bo Xu??
?Digital Content Technology and Services Research Center?National Lab of Pattern RecognitionInstitute of Automation, Chinese Academy of SciencesNo.95 Zhongguancun East Road, Haidian district, Beijing 100190, China{xingyuan.peng,dengfeng.ke,xubo}@ia.ac.cnAbstractConventional Automated Essay Scoring(AES) measures may cause severe problemswhen directly applied in scoring AutomaticSpeech Recognition (ASR) transcriptionas they are error sensitive and unsuitablefor the characteristic of ASR transcription.Therefore, we introduce a framework ofFinite State Transducer (FST) to avoid theshortcomings.
Compared with the LatentSemantic Analysis with Support VectorRegression (LSA-SVR) method (stands forthe conventional measures), our FST methodshows better performance especially towardsthe ASR transcription.
In addition, we applythe synonyms similarity to expand the FSTmodel.
The final scoring performance reachesan acceptable level of 0.80 which is only 0.07lower than the correlation (0.87) betweenhuman raters.1 IntroductionThe assessment of learners?
language abilities is asignificant part in language learning.
In conven-tional assessment, the problem of limited teach-er availability has become increasingly seriouswith the population increase of language learn-ers.
Fortunately, with the development of com-puter techniques and machine learning techniques(natural language processing and automatic speechrecognition), Computer-Assisted Language Learn-ing (CALL) systems help people to learn languageby themselves.One form of CALL is evaluating the speech ofthe learner.
Efforts in speech assessment usually fo-cus on the integrality, fluency, pronunciation, andprosody (Cucchiarini et al, 2000; Neumeyer et al,2000; Maier et al, 2009; Huang et al, 2010) of thespeech, which are highly predictable like the examform of the read-aloud text passage.
Another formof CALL is textual assessment.
This work is alsonamed AES.
Efforts in this area usually focus on thecontent, arrangement and language usage (Landaueret al, 2003; Ishioka and Kameda, 2004; Kakkonenet al, 2005; Attali and Burstein, 2006; Burstein etal., 2010; Persing et al, 2010; Peng et al, 2010; At-tali, 2011; Yannakoudakis et al, 2011) of the textwritten by the learner under a certain form of exam-ination.In this paper, our evaluation objects are the oralEnglish picture compositions in English as a Sec-ond Language (ESL) examination.
This examina-tion requires students to talk about four successivepictures with at least five sentences in one minute,and the beginning sentence is given.
This examina-tion form combines both of the two forms describedabove.
Therefore, we need two steps in the scoringtask.
The first step is Automatic Speech Recognition(ASR), in which we get the speech scoring featuresas well as the textual transcriptions of the speech-es.
Then, the second step could grade the text-freetranscription in an (conventional) AES system.
Thepresent work is mainly about the AES system un-der the certain situation as the examination gradingcriterion is more concerned about the integrated con-tent of the speech (the reason will be given in sub-section 3.1).There are many features and techniques whichare very powerful in conventional AES systems, but50applying them in this task will cause two differen-t problems as the scoring objects are the ASR out-put results.
The first problem is that the inevitablerecognition errors of the ASR will affect the perfor-mance of the feature extractions and scoring system.The second problem is caused by the special charac-teristic of the ASR result.
As all these methods aredesigned under the normal AES situation that theyare not suitable for the characteristic.The impact of the first problem can be reducedby either perfecting the results of the ASR system orbuilding the AES system which is not sensitive to theASR errors.
Improving the performance of the ASRis not what we concern about, so building an errorinsensitive AES system is what we care about in thispaper.
This makes many conventional features nolonger useful in the AES system, such as spellingerrors, punctuation errors and even grammar errors.The second problem is caused by applying thebag-of-words (BOW) techniques to score the ASRtranscription.
The BOW are very useful in measur-ing the content features and are usually robust evenif there are some errors in the scoring transcription.However, the robustness would not exist anymorebecause of the characteristic of the ASR result.
It isknown that better performance of ASR (reduce theword error rate in ASR) usually requires a strongconstrain Language Model (LM).
It means that moremeaningless parts of the oral speeches would be rec-ognized as the words quite related to the topic con-tent.
These words will usually be the key words inthe BOW methods, which will lead to a great distur-brance for the methods.
Therefore, the conventionalBOW methods are no longer appropriate because ofthe characteristic of the ASR result.To tackle the two problems described above, weapply the FST (Mohri, 2004).
As the evaluating ob-jects are from an oral English picture compositionexamination, it has two important features that makethe FST algorithm quite suitable.?
Picture composition examinations require stu-dents to speak according to the sequence of thepictures, so there is strong sequentiality in thespeech.?
The sentences for describing the same pictureare very identical in expression, so there is ahierarchy between the word sequences in thesentences (the expression) and the sense for thesame picture.FST is designed to describe a structure mappingtwo different types of information sequences.
It isvery useful in expressing the sequences and the hi-erarchy in picture composition.
Therefore, we builda FST-based model to extract features related to thetranscription assessment in this paper.
As the FST-based model is similar to the BOW metrics, it is alsoan error insensitive model.
In this way, the impact ofthe first problem could be reduced.
The FST modelis very powerful in delivering the sequence informa-tion that a meaningless sequence of words related tothe topic content will get low score under the mod-el.
Therefore, it works well concerning the secondproblem.
In a word, the FST model can not only beinsensitive to the recognition error in the ASR sys-tem, but also remedy the weakness of BOW methodsin ASR result scoring.In the remainder of the paper, the related work ofconventional AES methods is addressed in section 2.The details of the speech corpus and the examinationgrading criterion are introduced in section 3.
TheFST model and its improved method are proposedin section 4.
The experiments and the results arepresented in section 5.
The final section presents theconclusion and future work.2 Related WorkConventional AES systems usually exploit textualfeatures to assess the quality of writing mainly inthree different facets: the content facet, the arrange-ment facet and the language usage facet.
In the con-tent facet, many existing BOW techniques have beenapplied, such as the content vector analysis (Attal-i and Burstein, 2006; Attali, 2011) and the LSA toreduce the dimension of content vector (Landauer etal., 2003; Ishioka and Kameda, 2004; Kakkonen etal., 2005; Peng et al, 2010).
In arrangement facet,Burstein et al (2010) modeled the coherence in s-tudent essays, while Persing et al (2010) modeledthe organization.
In language usage facet, grammar,spelling and punctuation are common features in as-sessment of the writing competence (Landauer et al,2003; Attali and Burstein, 2006), and so does the di-versity of words and clauses (Lonsdale and Strong-Krause, 2003; Ishioka and Kameda, 2004).
Besides51Grading levels Content Integrity Acoustic(18-20)passedDescribe the information in the four pictures with proper elaboration Perfect(15-17) Describe all the information in all of the four pictures Good(12-14) Describe most of the information in all of the four pictures Allow errors(9-11)failedDescribe most of the information in the pictures, but lose about 1 or 2 pictures(6-8) Describe some of the information in the pictures, but lose about 2 or 3 pictures(3-5) Describe little information in the four pictures(0-2) Describe some words related to the four picturesTable 1: Criterion of Gradingthe textual features, many methods are also proposedto evaluate the quality.
The cosine similarity is oneof the most common used similarity measures (Lan-dauer et al, 2003; Ishioka and Kameda, 2004; Attaliand Burstein, 2006; Attali, 2011).
Also, the regres-sion or the classification method is a good choice forscoring (Rudner and Liang, 2002; Peng et al, 2010).The rank preference techniques show excellent per-formance in grading essays (Yannakoudakis et al,2011).
Chen et al (2010) proposed an unsupervisedapproach to AES.As our work concerns more about the content in-tegrity, we applied the LSA-SVR approach (Peng etal., 2010) as the contrast experiment, which is veryeffective and robust.
In the LSA-SVR method, eachessay transcription is represented by a latent seman-tic space vector, which is regarded as the features inthe SVR model.
The LSA (Deerwester et al, 1990)considers the relations between the dimensions inconventional vector space model (VSM) (Salton etal., 1975), and it can order the importance of each di-mension in the Latent Semantic Space (LSS).
There-fore, it is useful in reducing the dimensions of thevector by truncate the high dimensions.
The sup-port vector machine can be performed for the func-tion estimation (Smola and Scho?lkopf, 2004).
TheLSA-SVR method takes the LSS vector as the fea-ture vector, and applies the SVR for the training da-ta to obtain the SVR model.
Each test transcriptionrepresented by the LSS vector can be scored by themodel.3 DataAs characteristics of the data determine the effec-tiveness of our methods, the details of it will be in-troduced first.
Our experimental data is acquired inan oral English examination for ESL students.
Threescore > 0 > 12 > 15 > 18WER(%) 58.86 50.58 45.56 36.36MR(%) 72.88 74.03 75.70 78.45Table 2: WER and MR of ASR resultclasses of students participated in the exam and 417valid speeches are obtained in the examination.
Asthe paper mainly focuses on scoring the text tran-scriptions, we have two ways to obtain them.
Oneis manually typing the text transcriptions which weregarded as the Correct Recognition Result (CRR)transcription, and another is the ASR result whichwe named ASR transcription.
We use the HTK (Y-oung et al, 2006), which stands for the state of artin speech recognition, to build the ASR system.To better reveal the differences of the methods?performance, all the experiments will be done inboth transcriptions.
A better understanding of thedifference in the CRR transcription and the ASRtranscription from the low score to the high scoreis shown in Table 2, where WER is the word errorrate and MR is the match rate which is the words?correct rate.3.1 Criterion of GradingAccording to the Grading Criterion of the exami-nation, the score of the examination ranges from 0to 20, and the grading score is divided into 7 levelswith 3 points?
interval for each level.
The criterionmainly concerns about two facets of the speech: theacoustic level and the content integrity.
The detailsof the criterion are shown in Table 1.
The criterionindicates that the integrity is the most important partin rating the speech.
The acoustic level only work-s well in excellent speeches (Huang et al, 2010).Therefore, this paper mainly focuses on the integrity52Correlation R1 R2 R3 ES OCR1 - 0.8966 0.8557 0.9620 0.9116R2 - - 0.8461 0.9569 0.9048R3 - - - 0.9441 0.8739Average 0.8661 0.9543 0.8968Table 3: Correlations of Human ScoresFigure 1: Distribution of Final Expert Scoresof content.
The acoustic level as well as other levelssuch as grammar errors is ignored.
Because the cri-terion is almost based on the content, our methodsobtain good performance although we ignore somefeatures.3.2 Human Score Correlation and DistributionEach speech in our experiments was scored by threeraters.
Therefore, we have three scores for eachspeech.
The final expert score is the average of thesethree scores.
The correlations between human s-cores are shown in Table 3.R1, R2, and R3 stand for the three raters, and ESis the final expert score.
The Open Correlation (OC)is the correlation between human rater scores andthe final scores, which are not related to the humanscores themselves (average of the other two scores).As most students are supposed to pass the ex-amination, the expert scores are mostly distributedabove 12 points, as shown in Figure 1.
In the rangeof the pass score, the distribution is close to normaldistribution, while in the range of failed score except0, the distribution is close to uniform distribution.4 ApproachThe approach used in this paper is to build a standardFST for the current examination topic.
However,the annotation of the corpus is necessary before theFigure 2: Distribution of Sentence Labelsbuilding.
After the annotation and the building, thefeatures are extracted based on the FST.
The auto-mated machine score is computed from the featuresat last.
Therefore, subsection 4.1 will show the cor-pus annotation, subsection 4.2 will introduce how tobuild the standard FST of the current topic, and sub-sections 4.3 and 4.4 will discuss how to extract thefeatures, at last, an improved method is proposed insubsection 4.5.4.1 Corpus AnnotationThe definitions of the sequences and hierarchy inthe corpus will be given before we apply the FSTalgorithm.
According to the characteristics of thepicture composition examination, each compositioncan be held as an orderly combination of the sensesof pictures.
The senses of pictures are called sense-groups here.
We define a sense-group as one sen-tence either describing the same one or two picturesor elaborating on the same pictures.
The descrip-tion sentence is labeled with a tag ?m?
(main sense ofthe picture) and the elaboration one is labeled with?s?
(subordinate sense of the picture).
The first giv-en sentence in the examination is labeled with 0mand the other describing sentences for the 1 to 4 pic-tures are labeled with 1m to 4m, while the elabo-ration ones for the 4 pictures are labeled with 1s to4s.
Therefore, each sentence in the composition islabeled as a sense-group.
For the entire 417 CRRtranscriptions, we manually labeled 274 transcrip-tions whose scores are higher than 15 points.
Wegained 8 types of labels from the manually labeledresults.
They are 0m, 1m, 2m, 3m, 34m (one sen-tence describes both of the third and the fourth pic-tures), 4m, 2s and 4s.
Other labels were discardedfor the number of their appearance is very low.
Thedistribution of sentences with each label is shown inFigure 2.
There are 1679 sentences in the 274 CRR53Figure 3: FST Buildingtranscriptions and 1667 are labeled in the eight sym-bols.4.2 FST BuildingIn this paper, we build three types of FST to extractscoring features with the help of openFST tool (Al-lauzen et al, 2007).
The first is the sense-group F-ST, the second is the words to each sense-group FSTand the last is the words to all the sense-groups FST.They are shown in Figure 3.The definition of the sense-group has been giv-en in subsection 4.1.
The sense-group FST can de-scribe all the possible proper sense-group sequencesof the current picture composition topic.
It is alsoan acceptor trained from the labeled corpus.
We usemanually labeled corpus, which are the sequencesof sense-groups of the CRR transcriptions with ex-pert scores higher than 15 points, to build the sense-group FST.
In the process, each CRR transcriptionsense-group sequence is a simple sense-group FST.Later, we unite these sense-group FSTs to get thefinal FST which considers every situation of sense-group sequences in the train corpus.
Also, we usethe operation of ?determinize?
and ?minimize?
inopenFST to optimize the final sense-group FST thatits states have no same input label and is a smallestFST.The second type is the words to sense-group F-ST.
It determines what word sequence input will re-sult in what sense-group output.
With the help ofthese FSTs, we can find out how students use lan-guage to describe a certain sense-group, or in otherwords, a certain sense-group is usually constructedwith what kind of word sequence.
All the differ-ent sentences with their sense-group labels are tak-en from the train corpus.
We regard each sentenceas a simple words to sense-group FST, and then u-nite these FSTs which have the same sense-group la-bel.
The final union FSTs can transform proper wordsequence into the right sense-group.
Like buildingthe sense-group FST, the optimization operations of?determinize?
and ?minimize?
are also done for theFSTs.The last type of FST is a words to sense-groupsFST.
We can also treat it as a words FSA, becauseany word sequence accepted by the words to sense-groups FST is considered to be an integrated com-position.
Meanwhile, it can transform the word se-quence into the sense-group label sequence whichis very useful in extracting the scoring features (de-tails will be presented in subsection 4.4).
The F-ST is built from the other two types of FST that wemade before.
We compute the composition of all thewords to each sense-group FSTs (the second type)and the sense-group FST (the first type) with the op-erations of ?compose?
in openFST.
Then, the com-position result is the words to sense-groups FST, thethird type of FST in this paper.4.3 Search for the Best Path in FSTNow we have successfully built the words to sense-groups FST, the third type described above.
Just likethe similarity methods mentioned in section 2 canscore essays from a have-been-scored similar essay,we need to find the best path, which is closest tothe to-be-scored transcription, in the FST.
Here, weapply the edit distance to measure how best the pathis.
This means the best path is the word sequencepath in the FST which has the smallest edit distancecompared with the to-be-scored transcription?s wordsequences .Here, we modify the Wagner-Fischer algorithm(Wagner and Fischer, 1974), which is a DynamicProgramming (DP) algorithm, to quest the best pathin the FST.
A simple example is illustrated in Figure4.
The best path can be described aspath = argminpath?allpathEDcost(path, transcription) (1)EDcost = ins+ del + sub (2)EDcost is the edit distance from the transcription tothe paths which start at state 0 and end at the end54Figure 4: Search the Best Path in the FST by DPstate.
The DP process can be described by equation(3):minEDcost(i) = argminj?X1,...,Xp?1(minEDcost(j) + cost(j, i))(3)The minEDcost(j) is the accumulated minimum ed-it distance from state 0 to state j, and the cost(i,j) isthe cost of insertion, deletion or substitution from s-tate j to state i.
The equation means the minED ofstate i can be computed by the accumulated minED-cost of state j in the phase p. The state j belongs tothe have-been-calculated state set {X0,.
.
.
,Xp?1} inphase p. In phrase p, we compute the best path andits edit distance from the transcription for all the to-be-calculated states which is the Xp shown in Fig-ure 4.
After computing all the phrases, the best pathand its edit distances of the end states are obtained.Then the final best path is the one with the smallestedit distance.4.4 Feature ExtractionAfter building the FST and finding the best pathfor the to-be-scored transcription, we can extrac-t some effective features from the path informationand the transcription.
Inspired by the similarity s-coring measures, our proposed features represent thesimilarity between the best path?s word sequenceand the to-be-scored transcription.The features used for the scoring model are as fol-lows:?
The Edit Distance (ED):The edit distance is the linear combination ofthe weights of insertion, deletion and substi-tution.
The relation is shown in equation (2),where ins, del and sub are the appearance timesof insertions, deletions and substitutions, re-spectively.
Normally, we set the cost of eachto be 1.?
The Normalized Edit Distance(NED):The NED is the ED normalized with the tran-scription?s length.NEDcost = EDcost/length (4)?
The Match Number(MN):The match number is the number of wordsmatched between the best path and the tran-scription.?
The Match Rate(MR):The match rate is the match number normalizedwith the transcription?s length.MR = MN/length (5)?
The Continuous Match Value(CMV):Continuous match should be better than thefragmentary match, so a higher value is givenfor the continuous situation.CMV =?OM + 2?SM + 3?LM (6)where OM (One Match) is the fragmentarymatch number, SM (Short Match) is the con-tinuous match number which is no more than 4,and LM (Long Match) is the continuous matchnumber which is more than 4.?
The Length(L):The length of transcription.
Length is alwaysa very effective feature in essay scoring (Attaliand Burstein, 2006).?
The Sense-group Scoring Feature(SSF):For each best path, we can transform the tran-scription?s word sequence into the sense-grouplabel sequence with the FST.
Then, the wordsmatch rate of each sense-group can be comput-ed.
The match rate of each sense-group can beregarded as one feature so that all the sense-group match rate in the transcription will becombined to a feature vector (called the Sense-group Match Rate vector (SMRv)), which isan 8-dimensional vector in the present experi-ments.
After that, we applied the SVR algorith-m to train a sense-group scoring model with thevectors and scores, and the transcription gets itsSSF from the model.554.5 Extend the FST model with the similarityof synonymBecause the FST is trained from the limited corpus,it does not contain all the possible situations prop-er for the current composition topic.
To completethe current FST model, we add the similarity of syn-onym to extend the FST model so that it can handlemore situations.The extension of the FST model is mainly reflect-ed in calculation of the edit distance of the best path.The previous edit distance, in equation (2), refersto the Levenshtein distance in which the insertion-s, deletions and substitutions have equal cost, but inthe edit distance in this section, the cost of substi-tutions is less than that of insertions and deletion-s.
Here, we assume that the cost of substitutions isbased on the similarity of the two words.
Then withthe help of different cost of substitutions, each wordedge is extended to some of its synonym word edgesunder the cost of similarity.
The new edit distance iscalculated by equation (7) as follows:EDcost = ins+ del + sub?
(1?
sim) (7)where, sim is the similarity of two words.We used the Wordnet::Similarity software pack-age (Pedersen et al, 2004) to calculate the similaritybetween every two words at first.
However, the per-formance?s reduction of the AES system indicatesthat the similarity is not good enough to extend theFST model.
Therefore, we seek for human helpto accurate the similarity calculation.
We manual-ly checked the similarity, and deleted some improp-er similarity.
Thus the final similarity applied in ourexperiment is the Wordnet::Similarity software com-puting result after the manual check.5 ExperimentsIn this section, the proposed features and our FSTmethods will be evaluated on the corpus we men-tioned above.
The contrasting approach, the LSA-SVR approach, will also be presented.5.1 Data SetupThe experiment corpus consists of 417 speeches.With the help of manual typing and the ASR system,417 CRR transcriptions and 417 ASR transcriptionsare obtained from the speeches after preprocessingFST SVR SVR CRR ASRbuild train test transcription transcriptionSet2 Set3Set10.7999 0.7505Set3 Set2 0.8185 0.7401Set1 Set3Set20.8557 0.7372Set3 Set1 0.8111 0.7257Set1 Set2Set30.9085 0.8086Set2 Set1 0.8860 0.8086Table 4: Correlation Between the SSF and the Expert S-coreswhich includes the capitalization processing and thestemming processing.
We divide them into 3 setsby the same distribution of their scores.
Therefore,there are totally 6 sets, and each of them has 139 ofthe transcriptions.
The FST building only uses theCRR transcriptions whose expert scores are higherthan 15 points.
While treating one set (one CRR set)as the FST building train set, we get the ED, NED,MN, MR, CMV features and the SMR vectors forthe other two sets(could be either CRR sets or ASRsets).
Then, the SSF is obtained by another set asthe SVR train set and the last set as the test set.
Theparameters of the SVR are trained through the gridsearch from the whole data sets (ASR or CRR set-s) by cross-validation.
Therefore, except the lengthfeature, the other six features of each set can be ex-tracted from the FST model.Also, we presented the result of using LSA-SVRapproach as a contrast experiment to show the im-provement of our FST model in scoring oral Englishpicture composition.To quantitatively assess the effectiveness of themethods, the Pearson correlation between the expertscores and the automated results is adopted as theperformance measure.5.2 Correlation of FeaturesThe correlations between the seven features and thefinal expert scores are shown in Tables 4 and 5 onthe three sets.The MN and CMV are very good features, whilethe NED is not.
This is mainly due to the nature ofthe examination.
When scoring the speech, humanraters concern more about how much valid informa-tion it contains and irrelevant contents are not takenfor penalty.
Therefore, the match features are morereasonable than the edit distance features.
This im-56Script Train Test L ED NED MN MR CMVCRRSet2Set1 0.74040.2410 -0.6690 0.8136 0.1544 0.7417Set3 0.3900 -0.4379 0.8316 0.1386 0.7792Set1Set2 0.78190.4029 -0.7667 0.8205 0.4904 0.7333Set3 0.4299 -0.5672 0.8370 0.5090 0.7872Set1Set3 0.86450.4983 -0.7634 0.8867 0.2718 0.8162Set2 0.3639 -0.6616 0.8857 0.3305 0.8035Average 0.7956 0.3877 -0.6443 0.8459 0.3158 0.7769ASRSet2Set1 0.1341-0.2281 -0.6375 0.7306 0.6497 0.7012Set3 -0.1633 -0.5110 0.7240 0.6071 0.6856Set1Set2 0.2624-0.0075 -0.4640 0.6717 0.5929 0.6255Set3 0.0294 -0.4389 0.6860 0.6259 0.6255Set1Set3 0.1643-0.1871 -0.5391 0.7419 0.6213 0.7001Set2 -0.1742 -0.4721 0.7714 0.6199 0.7329Average 0.1869 -0.1218 -0.5104 0.7209 0.6195 0.6785Table 5: Correlations Between the Six Features and the Expert ScoresScript Method Set1 Set2 Set3 AverageCRRLength 0.7404 0.7819 0.8645 0.7956LSA-SVR 0.7476 0.8024 0.8663 0.8054FST 0.8702 0.8852 0.9386 0.8980ASRLength 0.1341 0.2624 0.1643 0.1869LSA-SVR 0.5975 0.5643 0.5907 0.5842FST 0.7992 0.7678 0.8452 0.8041Table 6: Performance of the FST Method, the LSA-SVRApproach and the Length Featurepact is similar to the result displayed by the ASRoutput performance in Table 2 in section 3, wherethe WER has significant difference from the low s-core speeches to the high score ones while the MRdoes not, and the MR is much better than the WER.As the length feature is a strong correlation fea-ture in CRR transcription, the MR feature, which isnormalized by the length, is strongly affected.
How-ever, with the impact declining in the ASR transcrip-tion, the MR feature performs very well.
This alsoexplains the reason of different correlations of EDand NED in CRR transcription.The SSF is entirely based on the FST model, sothe impact of the length feature is very low.
Thedecline of it in different transcriptions is mainly be-cause of the ASR error.5.3 Performance of the FST ModelFor each test transcription, it has 12 dimensions of F-ST features.
The ED, NED, MN, MR and CMV fea-tures have two dimensions of each as trained fromtwo different FST building sets.
The SSF needs t-wo train sets as there are two train models: one isfor the FST building model and another is for theSVR model.
As different sets for different models,it also has two dimension features.
We use the linearregression to combine these 12 features to the finalautomated score.
The linear regression parameter-s were trained from all the data by cross-validation.After the weight of each feature and the linear biasare gained, we calculate the automated score of eachtranscription by the FST features.
The performanceof our FST model is shown in Table 6.
Comparedwith it, the performance of the LSA-SVR algorithm,the baseline in our paper, is also shown.
As a usualbest feature for AES, the length shows its outstand-ing performance in CRR transcription.
However, itfails in the ASR transcription.As we have predicted above, the BOW algorith-m (the LSA-SVR) performance declines drasticallyin the ASR transcription, which also happens to thelength feature.
By contrast, the decline of the per-formance of our FST method is acceptable consid-ering the impact of recognition errors in the ASRsystem.
This means the FST model is an error in-sensitive model that is very appropriate for the task.5.4 Improvement of FST by Adding theSimilarityThe improved FST extends the original FST modelby considering the word similarity in substitution-s.
In the extension, the similarities of the synonyms57Script Method Set1 Set2 Set3 AverageCRRFST 0.8702 0.8852 0.9386 0.8980IFST 0.8788 0.8983 0.9418 0.9063ASRFST 0.7992 0.7678 0.8452 0.8041IFST 0.8351 0.7617 0.8168 0.8045Table 7: Performance of the FST Method and the Im-proved FST Methoddescribe the invisible (extended) part of the FST, soit should be very accurate for the substitutions cost.Therefore, we added manual intervention to the sim-ilarity result calculated by the wordnet::similaritysoftware packet.After we added the similarity of synonym to ex-tend the FST model, the performance of the newmodel increased stably in the CRR transcription.However, the increase is not significant in the AS-R transcription (shown in Table 7).
We believe it isbecause the superiority of the improved model is dis-guised by the ASR error.
In other words, the impactof ASR error under the FST model is more signifi-cant than the improvement of the FST model.
Theperformance correlation of our FST model in theCRR transcription is about 0.9 which is very close tothe human raters?
(shown in Table 3).
Even thoughthe performance correlation in the ASR transcriptiondeclines compared with that in the CRR transcrip-tion, the FST methods still perform very well underthe current recognition errors of the ARS system.6 Conclusion and Future workThe aforementioned experiments indicate threepoints.
First, the BOW algorithm has its own weak-ness.
In regular text essay scoring, the BOW algo-rithm can have excellent performance.
However, incertain situations, such as towards ASR transcriptionof oral English speech, its weakness of sequence ne-glect will be magnified, leading to drastic decline ofperformance.
Second, the introduced FST model issuitable in our task.
It is an error insensitive mod-el under the task of automated oral English picturecomposition scoring.
Also, it considers the sequenceand the hierarchy information.
As we expected, theperformance of the FST model is more outstandingthan that of the BOW metrics in CRR transcription,and the decline of performance is acceptable in AS-R transcription scoring.
Third, adding the similarityof synonyms to extend the FST model improves thesystem performance.
The extension can completethe FST model, and achieve better performance inthe CRR transcription.The future work may focus on three facets.
First,as the extension of the FST model is a preliminarystudy, there is much work that can be done, suchas calculating the similarity more accurately withoutmanual intervention, or finding a balance betweenthe original FST model and the extended one to im-prove the performance in ASR transcription.
Sec-ond, as the task is speech evaluation, considering theacoustic features may give more information to theautomated scoring system.
Therefore, the featuresat the acoustic level could be introduced to com-plete the scoring model.
Third, the decline of theperformance in ASR transcription is derived fromthe recognition error of ASR system.
Therefore, im-proving the performance of the ASR system or mak-ing full use of the N-best lists may give more accu-rate transcription for the AES system.AcknowledgmentsThis work was supported by the National NaturalScience Foundation of China (No.
90820303 andNo.
61103152).
We thank the anonymous reviewersfor their insightful comments.ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut and Mehryar Mohri.
2007.
OpenFst: ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of International Conference onImplementation and Application of Automata, 4783:11-23.Yigal Attali.
2011.
A differential word use measure forcontent analysis in automated essay scoring.
ETS re-search report, ETS RR-11-36.Yigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater R?V.2.
The Journal of Technology,Learning, and Assessment, 4(3), 1-34.Jill Burstein, Joel Tetreault, and Slava Andreyev.
2010.Using entity-based features to model coherence in stu-dent essays.
In Human Language Technologies: TheAnnual Conference of the North American Chapter ofthe ACL, 681-684.Chih-Chung Chang, Chih-Jen Lin.
2011.
LIBSVM: a li-brary for support vector machines.
ACM Transactionson Intelligent Systems and Technology, Vol.
2.58Yen-Yu Chen, Chien-Liang Liu, Chia-Hoang Lee, andTao-Hsing Chang.
2010.
An unsupervised automatedessay scoring system.
IEEE Intelligent Systems, 61-67.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2000.Quantitative assessment of second language learner-s?
fluency by means of automatic speech recognitiontechnology.
Acoustical Society of America, 107(2):989-999.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science.
41(6): 391-407.Shen Huang, Hongyan Li, Shijin Wang, Jiaen Liang andBo Xu.
2010.
Automatic reference independent e-valuation of prosody quality using multiple knowledgefusions.
In INTERSPEECH, 610-613.Tsunenori Ishioka and Masayuki Kameda.
2004.
Auto-mated Japanese essay scoring system: jess.
In Pro-ceedings of the International Workshop on databaseand Expert Systems applications.Tuomo Kakkonen, Niko Myller, Jari Timonen, and ErkkiSutinen.
2005.
Automatic essay grading with prob-abilistic latent semantic analysis.
In Proceedings ofWorkshop on Building Educational Applications Us-ing NLP, 29-36.Thomas K. Landauer, Darrell Laham and Peter Foltz.2003.
Automatic essay assessment.
Assessment in E-ducation: Principles, Policy and Practice (10:3), 295-309.Deryle Lonsdale and Diane Strong-Krause.
2003.
Au-tomated rating of ESL essays.
In Proceedings of theHLT-NAACL Workshop: Building Educational Appli-cations Using NLP.Andreas Maier, F. Ho?nig, V. Zeissler, Anton Batliner,E.
Ko?rner, N. Yamanaka, P. Ackermann, Elmar No?th2009.
A language-independent feature set for the auto-matic evaluation of prosody.
In INTERSPEECH, 600-603.Mehryar Mohri.
2004.
Weighted finite-state transduceralgorithms: an overview.
Formal Languages and Ap-plications, 148 (620): 551-564.Leonardo Neumeyer, Horacio Franco, Vassilios Di-galakis, Mitchel Weintraub.
2000.
Automatic scor-ing of pronunciation quality.
Speech Communication,30(2-3): 83-94.Ted Pedersen, Siddharth Patwardhan and Jason Miche-lizzi.
2004.
WordNet::Similarity - measuring the re-latedness of concepts.
In Proceedings of the NationalConference on Artificial Intelligence, 144-152.Xingyuan Peng, Dengfeng Ke, Zhenbiao Chen and BoXu.
2010.
Automated Chinese essay scoring usingvector space models.
In Proceedings of IUCS, 149-153.Isaac Persing, Alan Davis and Vincent Ng.
2010.
Mod-eling organization in student essays.
In Proceedings ofEMNLP, 229-239.Lawrence M. Rudner and Tahung Liang.
2002.
Auto-mated essay scoring using Bayes?
theorem.
The Jour-nal of Technology, Learning and Assessment, 1(2):3-21.G.
Salton, C. Yang, A. Wong.
1975.
A vector spacemodel for automatic indexing.
Communications of theACM, 18(11): 613-620.Alex J. Smola and Bernhard Scho?lkopf.
2004.
A tutorialon support vector regression.
Statistics and Comput-ing 14(3): 199-222.Robert A. Wagner and Michael J. Fischer.
1974.
Thestring-to-string correction problem.
Journal of theACM, 21(1):168-173.Helen Yannakoudakis, Ted Briscoe and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of ACL, 180-189.Steve Young, Gunnar Evermann, Mark Gales, ThomasHain, Dan Kershaw, Xunying Liu, Gareth Moore,Julian Odell, Dave Ollason, Dan Povey, ValtchoValtchev, Phil Woodland.
2006.
The HTK book (forHTK version 3.4).
Cambridge University EngineeringDepartment.59
