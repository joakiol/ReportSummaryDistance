Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1006?1011,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAre Word Embedding-based Features Useful for Sarcasm Detection?Aditya Joshi1,2,3 Vaibhav Tripathi1 Kevin Patel1Pushpak Bhattacharyya1 Mark Carman21Indian Institute of Technology Bombay, India2Monash University, Australia3IITB-Monash Research Academy, India{adityaj,kevin.patel,pb}@cse.iitb.ac.in, mark.carman@monash.eduAbstractThis paper makes a simple increment to state-of-the-art in sarcasm detection research.
Existing ap-proaches are unable to capture subtle forms of con-text incongruity which lies at the heart of sarcasm.We explore if prior work can be enhanced using se-mantic similarity/discordance between word embed-dings.
We augment word embedding-based featuresto four feature sets reported in the past.
We also ex-periment with four types of word embeddings.
Weobserve an improvement in sarcasm detection, irre-spective of the word embedding used or the originalfeature set to which our features are augmented.
Forexample, this augmentation results in an improve-ment in F-score of around 4% for three out of thesefour feature sets, and a minor degradation in caseof the fourth, when Word2Vec embeddings are used.Finally, a comparison of the four embeddings showsthat Word2Vec and dependency weight-based fea-tures outperform LSA and GloVe, in terms of theirbenefit to sarcasm detection.1 IntroductionSarcasm is a form of verbal irony that is intended to ex-press contempt or ridicule.
Linguistic studies show thatthe notion of context incongruity is at the heart of sar-casm (Ivanko and Pexman, 2003).
A popular trend inautomatic sarcasm detection is semi-supervised extrac-tion of patterns that capture the underlying context in-congruity (Davidov et al, 2010; Joshi et al, 2015; Riloffet al, 2013).
However, techniques to extract these pat-terns rely on sentiment-bearing words and may not cap-ture nuanced forms of sarcasm.
Consider the sentence?With a sense of humor like that, you could make a liv-ing as a garbage man anywhere in the country.1?
Thespeaker makes a subtle, contemptuous remark about the1All examples in this paper are actual instances from our dataset.sense of humor of the listener.
However, absence of sen-timent words makes the sarcasm in this sentence difficultto capture as features for a classifier.In this paper, we explore use of word embeddings tocapture context incongruity in the absence of sentimentwords.
The intuition is that word vector-based sim-ilarity/discordance is indicative of semantic similar-ity which in turn is a handle for context incongruity.In the case of the ?sense of humor?
example above, thewords ?sense of humor?
and ?garbage man?
are seman-tically dissimilar and their presence together in the sen-tence provides a clue to sarcasm.
Hence, our set of fea-tures based on word embeddings aim to capture such se-mantic similarity/discordance.
Since such semantic simi-larity is but one of the components of context incongruityand since existing feature sets rely on sentiment-basedfeatures to capture context incongruity, it is imperativethat the two be combined for sarcasm detection.
Thus,our paper deals with the question:Can word embedding-based features when augmented tofeatures reported in prior work improve the performanceof sarcasm detection?To the best of our knowledge, this is the first attemptthat uses word embedding-based features to detect sar-casm.
In this respect, the paper makes a simple incrementto state-of-the-art but opens up a new direction in sarcasmdetection research.
We establish our hypothesis in caseof four past works and four types of word embeddings,to show that the benefit of using word embedding-basedfeatures holds across multiple feature sets and word em-beddings.2 MotivationIn our literature survey of sarcasm detection (Joshi et al,2016), we observe that a popular trend is semi-supervisedextraction of patterns with implicit sentiment.
One suchwork is by Riloff et al (2013) who give a bootstrap-ping algorithm that discovers a set of positive verbs and1006negative/undesirable situations.
However, this simplifi-cation (of representing sarcasm merely as positive verbsfollowed by negative situation) may not capture difficultforms of context incongruity.
Consider the sarcastic sen-tence ?A woman needs a man like a fish needs bicycle?2.The sarcasm in this sentence is understood from the factthat a fish does not need bicycle - and hence, the sentenceridicules the target ?a man?.
However, this sentence doesnot contain any sentiment-bearing word.
Existing sar-casm detection systems relying on sentiment incongruity(as in the case of our past work reported as Joshi et al(2015)) may not work well in such cases of sarcasm.To address this, we use semantic similarity as a han-dle to context incongruity.
To do so, we use word vectorsimilarity scores.
Consider similarity scores (as given byWord2Vec) between two pairs of words in the sentenceabove:similarity(man,woman) = 0.766similarity(fish,bicycle) = 0.131Words in one part of this sentence (?man?
and ?woman?
)are lot more similar than words in another part of the sen-tence (?fish?
and ?bicycle?).
This semantic discordancecan be a clue to presence of context incongruity.
Hence,we propose features based on similarity scores betweenword embeddings of words in a sentence.
In general, wewish to capture the most similar and most dissimilar wordpairs in the sentence, and use their scores as features forsarcasm detection.3 Background: Features from prior workWe augment our word embedding-based features to thefollowing four feature sets that have been reported:1.
Liebrecht et al (2013): They consider unigrams,bigrams and trigrams as features.2.
Gonza?lez-Iba?nez et al (2011a): They propose twosets of features: unigrams and dictionary-based.The latter are words from a lexical resource calledLIWC.
We use words from LIWC that have beenannotated as emotion and psychological processwords, as described in the original paper.3.
Buschmeier et al (2014): In addition to uni-grams, they propose features such as: (a) Hy-perbole (captured by three positive or negativewords in a row), (b) Quotation marks and ellipsis,(c) Positive/Negative Sentiment words followed byan exclamation mark or question mark, (d) Posi-tive/Negative Sentiment Scores followed by ellipsis(represented by a ?...?
), (e) Punctuation, (f) Interjec-tions, and (g) Laughter expressions.2This quote is attributed to Irina Dunn, an Australian writer(https://en.wikipedia.org/wiki/Irina_Dunn4.
Joshi et al (2015): In addition to unigrams, they usefeatures based on implicit and explicit incongruity.Implicit incongruity features are patterns with im-plicit sentiment as extracted in a pre-processing step.Explicit incongruity features consist of number ofsentiment flips, length of positive and negative sub-sequences and lexical polarity.4 Word Embedding-based FeaturesIn this section, we now describe our word embedding-based features.
We reiterate that these features will beaugmented to features from prior works (described inSection 3).As stated in Section 2, our word embedding-based fea-tures are based on similarity scores between word em-beddings.
The similarity score is the cosine similaritybetween vectors of two words.
To illustrate our features,we use our example ?A woman needs a man like a fishneeds a bicycle?.
The scores for all pairs of words in thissentence are given in Table 1.man woman fish needs bicycleman - 0.766 0.151 0.078 0.229woman 0.766 - 0.084 0.060 0.229fish 0.151 0.084 - 0.022 0.130needs 0.078 0.060 0.022 - 0.060bicycle 0.229 0.229 0.130 0.060 -Table 1: Similarity scores for all pairs of content words in ?A womanneeds a man like a fish needs bicycle?Using these similarity scores, we compute two sets offeatures:1.
Unweighted similarity features (S): We first com-pute similarity scores for all pairs of words (exceptstop words).
We then return four feature values persentence.3:?
Maximum score of most similar word pair?
Minimum score of most similar word pair?
Maximum score of most dissimilar word pair?
Minimum score of most dissimilar word pairFor example, in case of the first feature, we considerthe most similar word to every word in the sentence,and the corresponding similarity scores.
These mostsimilar word scores for each word are indicated inbold in Table 1.
Thus, the first feature in case of ourexample would have the value 0.766 derived fromthe man-woman pair and the second feature wouldtake the value 0.078 due to the needs-man pair.
Theother features are computed in a similar manner.3These feature values consider all words in the sentence, i.e., the?maximum?
is computed over all words10072.
Distance-weighted similarity features (WS): Likein the previous case, we first compute similarityscores for all pairs of words (excluding stop-words).For all similarity scores, we divide them by squareof distance between the two words.
Thus, the simi-larity between terms that are close in the sentence isweighted higher than terms which are distant fromone another.
Thus, for all possible word pairs, wecompute four features:?
Maximum distance-weighted score of mostsimilar word pair?
Minimum distance-weighted score of mostsimilar word pair?
Maximum distance-weighted score of mostdissimilar word pair?
Minimum distance-weighted score of most dis-similar word pairThese are computed similar to unweighted similarityfeatures.5 Experiment SetupWe create a dataset consisting of quotes on GoodReads 4.GoodReads describes itself as ?the world?s largest site forreaders and book recommendations.?
The website alsoallows users to post quotes from books.
These quotes aresnippets from books labeled by the user with tags of theirchoice.
We download quotes with the tag ?sarcastic?
assarcastic quotes, and the ones with ?philosophy?
as non-sarcastic quotes.
Our labels are based on these tags givenby users.
We ensure that no quote has both these tags.This results in a dataset of 3629 quotes out of which 759are labeled as sarcastic.
This skew is similar to skewsobserved in datasets on which sarcasm detection experi-ments have been reported in the past (Riloff et al, 2013).We report five-fold cross-validation results on theabove dataset.
We use SVMperf by Joachims (2006)with c as 20, w as 3, and loss function as F-score opti-mization.
This allows SVM to be learned while optimiz-ing the F-score.As described above, we compare features given in priorwork alongside the augmented versions.
This means thatfor each of the four papers, we experiment with four con-figurations:1.
Features given in paper X2.
Features given in paper X + unweighted similarityfeatures (S)3.
Features given in paper X + weighted similarity fea-tures (WS)4.
Features given in paper X + S+WS (i.e., weightedand unweighted similarity features)Features P R FBaselineUnigrams 67.2 78.8 72.53S 64.6 75.2 69.49WS 67.6 51.2 58.26Both 67 52.8 59.05Table 2: Performance of unigrams versus our similarity-based featuresusing embeddings from Word2VecWe experiment with four types of word embeddings:1.
LSA: This approach was reported in Landauer andDumais (1997).
We use pre-trained word em-beddings based on LSA5.
The vocabulary size is100,000.2.
GloVe: We use pre-trained vectors avaiable from theGloVe project6.
The vocabulary size in this case is2,195,904.3.
Dependency Weights: We use pre-trained vectors7weighted using dependency distance, as given inLevy and Goldberg (2014).
The vocabulary size is174,015.4.
Word2Vec: use pre-trained Google word vectors.These were trained using Word2Vec tool 8 on theGoogle News corpus.
The vocabulary size forWord2Vec is 3,000,000.
To interact with these pre-trained vectors, as well as compute various features,we use gensim library (R?ehu?r?ek and Sojka, 2010).To interact with the first three pre-trained vectors, we usescikit library (Pedregosa et al, 2011).6 ResultsTable 2 shows performance of sarcasm detection whenour word embedding-based features are used on their owni.e, not as augmented features.
The embedding in thiscase is Word2Vec.
The four rows show baseline setsof features: unigrams, unweighted similarity using wordembeddings (S), weighted similarity using word embed-dings (WS) and both (i.e., unweighted plus weighted sim-ilarities using word embeddings).
Using only unigramsas features gives a F-score of 72.53%, while only un-weighted and weighted features gives F-score of 69.49%and 58.26% respectively.
This validates our intuition4www.goodreads.com5http://www.lingexp.uni-tuebingen.de/z2/LSAspaces/6http://nlp.stanford.edu/projects/glove/7 https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/8https://code.google.com/archive/p/Word2Vec/1008LSA GloVe Dependency Weights Word2VecP R F P R F P R F P R FL 73 79 75.8 73 79 75.8 73 79 75.8 73 79 75.8+S 81.8 78.2 79.95 81.8 79.2 80.47 81.8 78.8 80.27 80.4 80 80.2+WS 76.2 79.8 77.9 76.2 79.6 77.86 81.4 80.8 81.09 80.8 78.6 79.68+S+WS 77.6 79.8 78.68 74 79.4 76.60 82 80.4 81.19 81.6 78.2 79.86G 84.8 73.8 78.91 84.8 73.8 78.91 84.8 73.8 78.91 84.8 73.8 78.91+S 84.2 74.4 79 84 72.6 77.8 84.4 72 77.7 84 72.8 78+WS 84.4 73.6 78.63 84 75.2 79.35 84.4 72.6 78.05 83.8 70.2 76.4+S+WS 84.2 73.6 78.54 84 74 78.68 84.2 72.2 77.73 84 72.8 78B 81.6 72.2 76.61 81.6 72.2 76.61 81.6 72.2 76.61 81.6 72.2 76.61+S 78.2 75.6 76.87 80.4 76.2 78.24 81.2 74.6 77.76 81.4 72.6 76.74+WS 75.8 77.2 76.49 76.6 77 76.79 76.2 76.4 76.29 81.6 73.4 77.28+S+WS 74.8 77.4 76.07 76.2 78.2 77.18 75.6 78.8 77.16 81 75.4 78.09J 85.2 74.4 79.43 85.2 74.4 79.43 85.2 74.4 79.43 85.2 74.4 79.43+S 84.8 73.8 78.91 85.6 74.8 79.83 85.4 74.4 79.52 85.4 74.6 79.63+WS 85.6 75.2 80.06 85.4 72.6 78.48 85.4 73.4 78.94 85.6 73.4 79.03+S+WS 84.8 73.6 78.8 85.8 75.4 80.26 85.6 74.4 79.6 85.2 73.2 78.74Table 3: Performance obtained on augmenting word embedding features to features from four prior works, for four word embeddings; L: Liebrechtet al (2013), G: Gonza?lez-Iba?nez et al (2011a), B: Buschmeier et al (2014) , J: Joshi et al (2015)that word embedding-based features alone are notsufficient, and should be augmented with other fea-tures.Following this, we show performance using featurespresented in four prior works: Buschmeier et al (2014),Liebrecht et al (2013), Joshi et al (2015) and Gonza?lez-Iba?nez et al (2011a), and compare them with augmentedversions in Table 3.Table 3 shows results for four kinds of word embed-dings.
All entries in the tables are higher than the sim-ple unigrams baseline, i.e., F-score for each of the fouris higher than unigrams - highlighting that these are bet-ter features for sarcasm detection than simple unigrams.Values in bold indicate the best F-score for a given priorwork-embedding type combination.
In case of Liebrechtet al (2013) for Word2Vec, the overall improvement inF-score is 4%.
Precision increases by 8% while recall re-mains nearly unchanged.
For features given in Gonza?lez-Iba?nez et al (2011a), there is a negligible degradation of0.91% when word embedding-based features based onWord2Vec are used.
For Buschmeier et al (2014) forWord2Vec, we observe an improvement in F-score from76.61% to 78.09%.
Precision remains nearly unchangedwhile recall increases.
In case of Joshi et al (2015) andWord2Vec, we observe a slight improvement of 0.20%when unweighted (S) features are used.
This shows thatword embedding-based features are useful, across fourpast works for Word2Vec.Table 3 also shows that the improvement holds acrossthe four word embedding types as well.
The maxi-mum improvement is observed in case of Liebrecht etal.
(2013).
It is around 4% in case of LSA, 5% in caseof GloVe, 6% in case of Dependency weight-based and4% in case of Word2Vec.
These improvements are notdirectly comparable because the four embeddings havedifferent vocabularies (since they are trained on differentdatasets) and vocabulary sizes, their results cannot be di-rectly compared.Therefore, we take an intersection of the vocabulary(i.e., the subset of words present in all four embeddings)and repeat all our experiments using these intersectionfiles.
The vocabulary size of these intersection files (forall four embeddings) is 60,252.
Table 4 shows the av-erage increase in F-score when a given word embed-ding and a word embedding-based feature is used, withthe intersection file as described above.
These gain val-ues are lower than in the previous case.
This is be-cause these are the values in case of the intersectionversions - which are subsets of the complete embed-dings.
Each gain value is averaged over the four priorworks.
Thus, when unweighted similarity (+S) basedfeatures computed using LSA are augmented to fea-tures from prior work, an average increment of 0.835%is obtained over the four prior works.
The values al-low us to compare the benefit of using these four kindsof embeddings.
In case of unweighted similarity-basedfeatures, dependency-based weights give the maximumgain (0.978%).
In case of weighted similarity-basedfeatures and ?+S+WS?, Word2Vec gives the maximumgain (1.411%).
Table 5 averages these values over the1009Word2Vec LSA GloVe Dep.Wt.+S 0.835 0.86 0.918 0.978+WS 1.411 0.255 0.192 1.372+S+WS 1.182 0.24 0.845 0.795Table 4: Average gain in F-Scores obtained by using intersection of thefour word embeddings, for three word embedding feature-types, aug-mented to four prior works; Dep.
Wt.
indicates vectors learned fromdependency-based weightsWord Embedding Average F-score GainLSA 0.452Glove 0.651Dependency 1.048Word2Vec 1.143Table 5: Average gain in F-scores for the four types of word embed-dings; These values are computed for a subset of these embeddingsconsisting of words common to all fourthree types of word embedding-based features.
UsingDependency-based and Word2Vec embeddings results ina higher improvement in F-score (1.048% and 1.143%respectively) as compared to others.7 Error AnalysisSome categories of errors made by our system are:1.
Embedding issues due to incorrect senses: Be-cause words may have multiple senses, some em-beddings lead to error, as in ?Great.
Relationshipadvice from one of America?s most wanted.?.2.
Contextual sarcasm: Consider the sarcastic quote?Oh, and I suppose the apple ate the cheese?.
Thesimilarity score between ?apple?
and ?cheese?
is0.4119.
This comes up as the maximum similar pair.The most dissimilar pair is ?suppose?
and ?apple?with similarity score of 0.1414.
The sarcasm in thissentence can be understood only in context of thecomplete conversation that it is a part of.3.
Metaphors in non-sarcastic text: Figurative lan-guage may compare concepts that are not directly re-lated but still have low similarity.
Consider the non-sarcastic quote ?Oh my love, I like to vanish in youlike a ripple vanishes in an ocean - slowly, silentlyand endlessly?.
Our system incorrectly predicts thisas sarcastic.8 Related WorkEarly sarcasm detection research focused on speech (Tep-perman et al, 2006) and lexical features (Kreuz andCaucci, 2007).
Several other features have been proposed(Kreuz and Caucci, 2007; Joshi et al, 2015; Khattri etal., 2015; Liebrecht et al, 2013; Gonza?lez-Iba?nez et al,2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wal-lace et al, 2014; Veale and Hao, 2010; Gonza?lez-Iba?nezet al, 2011b; Reyes et al, 2012).
Of particular relevanceto our work are papers that aim to first extract patternsrelevant to sarcasm detection.
Davidov et al (2010) use asemi-supervised approach that extracts sentiment-bearingpatterns for sarcasm detection.
Joshi et al (2015) extractphrases corresponding to implicit incongruity i.e.
the sit-uation where sentiment is expressed without use of sen-timent words.
Riloff et al (2013) describe a bootstrap-ping algorithm that iteratively discovers a set of positiveverbs and negative situation phrases, which are later usedin a sarcasm detection algorithm.
Tsur et al (2010) alsoperform semi-supervised extraction of patterns for sar-casm detection.
The only prior work which uses wordembeddings for a related task of sarcasm detection is byGhosh et al (2015).
They model sarcasm detection as aword sense disambiguation task, and use embeddings toidentify whether a word is used in the sarcastic or non-sarcastic sense.
Two sense vectors for every word arecreated: one for literal sense and one for sarcastic sense.The final sense is determined based on the similarity ofthese sense vectors with the sentence vector.9 ConclusionThis paper shows the benefit of features based on wordembedding for sarcasm detection.
We experiment withfour past works in sarcasm detection, where we augmentour word embedding-based features to their sets of fea-tures.
Our features use the similarity score values re-turned by word embeddings, and are of two categories:similarity-based (where we consider maximum/minimumsimilarity score of most similar/dissimilar word pair re-spectively), and weighted similarity-based (where weweight the maximum/minimum similarity scores of mostsimilar/dissimilar word pairs with the linear distancebetween the two words in the sentence).
We experi-ment with four kinds of word embeddings: LSA, GloVe,Dependency-based and Word2Vec.
In case of Word2Vec,for three of these past feature sets to which our featureswere augmented, we observe an improvement in F-scoreof at most 5%.
Similar improvements are observed incase of other word embeddings.
A comparison of thefour embeddings shows that Word2Vec and dependencyweight-based features outperform LSA and GloVe.This work opens up avenues for use of word embed-dings for sarcasm classification.
Our word embedding-based features may work better if the similarity scores arecomputed for a subset of words in the sentence, or usingweighting based on syntactic distance instead of lineardistance as in the case of our weighted similarity-basedfeatures.1010ReferencesKonstantin Buschmeier, Philipp Cimiano, and Roman Klinger.2014.
An impact analysis of features in a classification ap-proach to irony detection in product reviews.
In Proceedingsof the 5th Workshop on Computational Approaches to Sub-jectivity, Sentiment and Social Media Analysis, pages 42?49.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.
Semi-supervised recognition of sarcastic sentences in twitter andamazon.
In Proceedings of the Fourteenth Conference onComputational Natural Language Learning, pages 107?116.Association for Computational Linguistics.Debanjan Ghosh, Weiwei Guo, and Smaranda Muresan.
2015.Sarcastic or not: Word embeddings to predict the literal orsarcastic meaning of words.
In EMNLP.Roberto Gonza?lez-Iba?nez, Smaranda Muresan, and Nina Wa-cholder.
2011a.
Identifying sarcasm in twitter: a closer look.In Proceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human Language Technolo-gies: short papers-Volume 2, pages 581?586.
Association forComputational Linguistics.Roberto Gonza?lez-Iba?nez, Smaranda Muresan, and Nina Wa-cholder.
2011b.
Identifying sarcasm in twitter: a closer look.In Proceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human Language Technolo-gies: short papers-Volume 2, pages 581?586.
Association forComputational Linguistics.Stacey L Ivanko and Penny M Pexman.
2003.
Contextincongruity and irony processing.
Discourse Processes,35(3):241?279.Thorsten Joachims.
2006.
Training linear svms in linear time.In Proceedings of the 12th ACM SIGKDD international con-ference on Knowledge discovery and data mining, pages217?226.
ACM.Aditya Joshi, Vinita Sharma, and Pushpak Bhattacharyya.2015.
Harnessing context incongruity for sarcasm detec-tion.
In Proceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7th Inter-national Joint Conference on Natural Language Processing,volume 2, pages 757?762.Aditya Joshi, Pushpak Bhattacharyya, and Mark James Car-man.
2016.
Automatic sarcasm detection: A survey.
arXivpreprint arXiv:1602.03426.Anupam Khattri, Aditya Joshi, Pushpak Bhattacharyya, andMark James Carman.
2015.
Your sentiment precedes you:Using an authors historical tweets to predict sarcasm.
In6th Workshop on Computational Approaches to Subjectivity,Sentiment and Social Media Analysis (WASSA), page 25.Roger J Kreuz and Gina M Caucci.
2007.
Lexical influences onthe perception of sarcasm.
In Proceedings of the Workshopon computational approaches to Figurative Language, pages1?4.
Association for Computational Linguistics.Thomas K Landauer and Susan T. Dumais.
1997.
A solutionto platos problem: The latent semantic analysis theory of ac-quisition, induction, and representation of knowledge.
PSY-CHOLOGICAL REVIEW, 104(2):211?240.Omer Levy and Yoav Goldberg.
2014.
Dependency-basedword embeddings.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics, ACL2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2:Short Papers, pages 302?308.CC Liebrecht, FA Kunneman, and APJ van den Bosch.
2013.The perfect solution for detecting sarcasm in tweets# not.Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gramfort, Vin-cent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blon-del, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al2011.
Scikit-learn: Machine learning in python.
The Journalof Machine Learning Research, 12:2825?2830.Rachel Rakov and Andrew Rosenberg.
2013. ?
sure, i did theright thing?
: a system for sarcasm detection in speech.
InINTERSPEECH, pages 842?846.Radim R?ehu?r?ek and Petr Sojka.
2010.
Software Frameworkfor Topic Modelling with Large Corpora.
In Proceedingsof the LREC 2010 Workshop on New Challenges for NLPFrameworks, pages 45?50, Valletta, Malta, May.
ELRA.http://is.muni.cz/publication/884893/en.Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012.From humor recognition to irony detection: The figurativelanguage of social media.
Data & Knowledge Engineering,74:1?12.Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva,Nathan Gilbert, and Ruihong Huang.
2013.
Sarcasm as con-trast between a positive sentiment and negative situation.
InEMNLP, pages 704?714.Joseph Tepperman, David R Traum, and Shrikanth Narayanan.2006.
?
yeah right?
: sarcasm recognition for spoken dia-logue systems.
In INTERSPEECH.
Citeseer.Oren Tsur, Dmitry Davidov, and Ari Rappoport.
2010.
Icwsm-a great catchy name: Semi-supervised recognition of sarcas-tic sentences in online product reviews.
In ICWSM.Tony Veale and Yanfen Hao.
2010.
Detecting ironic intent increative comparisons.
In ECAI, volume 215, pages 765?770.Byron C Wallace, Laura Kertz Do Kook Choe, and EugeneCharniak.
2014.
Humans require context to infer ironic in-tent (so computers probably do, too).
In Proceedings of theAnnual Meeting of the Association for Computational Lin-guistics (ACL), pages 512?516.Byron C Wallace.
2015.
Sparse, contextually informed modelsfor irony detection: Exploiting user communities,entities andsentiment.
In ACL.1011
