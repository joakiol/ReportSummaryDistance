Proceedings of the SIGDIAL 2014 Conference, pages 151?160,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsThe Role of Polarity in Inferring Acceptance and Rejection in DialogueJulian J. Schl?oder and Raquel Fern?andezInstitute for Logic, Language & ComputationUniversity of Amsterdamjulian.schloeder@gmail.com, raquel.fernandez@uva.nlAbstractWe study the role that logical polarityplays in determining the rejection or ac-ceptance function of an utterance in dia-logue.
We develop a model inspired by re-cent work on the semantics of negationand polarity particles and test it on annota-ted data from two spoken dialogue corpo-ra: the Switchboard Corpus and the AMIMeeting Corpus.
Our experiments showthat taking into account the relative pola-rity of a proposal under discussion and ofits response greatly helps to distinguish re-jections from acceptances in both corpora.1 IntroductionIn order to establish and maintain coherence, dia-logue participants need to keep track of the infor-mation they jointly take for granted?their com-mon ground (Stalnaker, 1978).
As a dialogueprogresses, the common ground typically evolves.New information becomes shared as the interlocu-tors exchange moves (such as assertions, ques-tions, acceptances, and rejections) through the col-laborative process of grounding (Clark and Schae-fer, 1989; Clark, 1996).
To keep track of the com-mon ground, speakers must identify which infor-mation is accepted or rejected by their addressees.The basic idea is simple: If a proposal is rejected,its content does not enter the common ground,while if it is accepted, its content does becomecommon belief.Yet, determining whether a response to a movecounts as an acceptance or a rejection is far fromtrivial.
In many cases, the surface form of anutterance is not explicit enough to determine itsacceptance or rejection force and inference is re-quired (Horn, 1989; Lascarides and Asher, 2009;Walker, 1996).
For instance, B?s utterance in (1),extracted from the AMI Meeting Corpus (Carletta,2007), exemplifies what Walker (1996) calls im-plicature rejection (the rejection arises from aninferred scalar implicature: ?normal?
implicates?not interesting?
; see also Hirschberg (1985)).
(1) A: This is a very interesting design.B: It?s just the same as normal.The goal of this paper is to investigate the role oflogical polarity in distinguishing rejections fromacceptances.
Consider the following dialogue ex-cerpts, again from AMI, where the same utteranceform (?Yes it is?)
acts as an acceptance in (2) andas a rejection in (3):(2) A: But it?s uh yeah it?s uh original idea.B: Yes it is.
(3) A: the shape of a banana is not it?s not reallyhandy .B: Yes it is.To determine whether B?s utterance in either caseabove functions as an acceptance or a rejection,it is critical to not only look beyond the utteranceitself and take into account the proposal under dis-cussion (A?s utterance), but also to specify (a) thepolarity (positive vs. negative) of both the proposaland the response, and (b) how these polarities in-teract to give rise to a particular interpretation.
Ouraim in this paper is to develop a model of howlogical polarity influences acceptance/rejection in-terpretation, inspired by recent work on the se-mantics of negation and polarity particles (Cooperand Ginzburg, 2011; Cooper and Ginzburg, 2012;Farkas and Roelofsen, 2013), and to test it on an-notated data from two spoken dialogue corpora:the Switchboard Corpus (Godfrey et al., 1992) andthe AMI Meeting Corpus (Carletta, 2007).In the next section, we give an overview of re-lated computational work on acceptance/rejectiondetection.
In Section 3, we first briefly review re-cent formal semantics approaches to polarity andthen present our model of logical polarity in ac-ceptance and rejection moves.
Section 4 describes151our experiments: We derive machine learning fea-tures from our polarity theory and test them inSwitchboard and AMI datasets, achieving compet-itive F -scores of around 60 on the task of retriev-ing rejections.
We conclude in Section 5 with adiscussion of our results.2 Related Computational WorkThe first attempts to automatically identify accep-tances and rejections (often referred to as agree-ments and disagreements) were carried out in thecontext of multiparty meetings for the purposeof dialogue summarisation tasks.
Hillard et al.
(2003) and Hahn et al.
(2006) used the ICSI Meet-ing Corpus (Janin et al., 2003) to develop sys-tems that would classify utterances into agree-ments, disagreements, backchannels, and ?other?.While these authors only leveraged lexical andprosodic features of the utterance to be classified(i.e., local features), Galley et al.
(2004) showedthat accuracy could be improved by taking into ac-count contextual dependencies, in particular pre-vious (dis)agreements between the dialogue par-ticipants, achieving an overall accuracy of 86.9%.Subsequent work built on Galley et al.
?s approachshowed that detecting agreement acts helped toidentify public commitments to tasks (Purver etal., 2007) and other decisions made in a meeting(Fern?andez et al., 2008).A difficulty shared by all approaches mentionedabove is the skewness of the data, not only regard-ing (dis)agreement vs. other types of acts, but alsoagreement vs. disagreement.
In the dialogue set-tings considered, acceptance/agreement is muchmore common than rejection/disagreement (e.g.,11.9% vs. 6.7% in the portion of the ICSI Meet-ing Corpus used by Galley et al.
(2004) and 3.6%vs.
0.4% in the section of the AMI Meeting Cor-pus used by Germesin and Wilson (2009)).
Thiscan lead to reasonable overall accuracy but poorresults on recognising rejections.
Indeed, Ger-mesin and Wilson (2009), who apply an approachbased on Galley et al.
(2004) to the AMI Meet-ing Corpus, achieve 98.1% accuracy, but report0% recall for rejections/disagreements.
Wang etal.
(2011), who also work with AMI data, use dif-ferent resampling methods to balance their datasetand then apply Conditional Random Fields (usingtherefore contextual information from sequencesof utterances), achieving 56.9% recall and 55.9 F1for disagreement detection.Some recent work has moved away from spokendialogue to address similar tasks in online discus-sion forums.
An advantage of this kind of sce-narios is that they seem to offer more opportu-nity for disagreement/rejection, thereby yieldingmore inherently balanced datasets.
Abbott et al.
(2011) and Misra and Walker (2013) use the In-ternet Argument Corpus (Walker et al., 2012), anannotated collection of posts in discussion forumswith a balanced distribution of agreeing and dis-agreeing posts.
They address a 2-way classifica-tion task?determining whether each response toa post (or to a quoted portion of a post in the caseof Abbott et al.
(2011)) is either an agreement ora disagreement?using a collection of features in-spired by previous computational and theoreticalapproaches.
The system developed by Misra andWalker (2013) uses only local features of the to-be-classified post, achieving an accuracy of 66%(over a 50% baseline).
Abbott et al.
(2011)?s bestsystem uses features from both the quoted post andthe response post, achieving an accuracy of 68.2%.However adding this contextual information doesnot significantly outperform a system based onlyon local features of the response, which yields66.6% accuracy.
Using both features from the postand the post response, Yin et al.
(2012) obtain sim-ilar results: 68% accuracy on a different onlinecorpus (the Political Forum), where the datasetsare not balanced (they report a ratio of about 2 to1 for agreement vs. disagreement).All in all, this body of work has identifiedseveral linguistic features that are useful for in-ferring acceptances and rejections, often build-ing on observations made by conversational an-alysts (Pomerantz, 1984; Brown and Levinson,1987).
Furthermore, recent work by Bousmaliset al.
(2013) suggests that there are specific non-verbal behaviours associated with agreement anddisagreement, such as different types of head, lip,and hand movements.
However, to our knowl-edge, the role of logical polarity has not beeninvestigated in any detail by computational ap-proaches.
Several systems make use of subjectivepolarity, i.e., sentiment.
For instance, Galley et al.
(2004) use the list of subjective adjectives com-piled by Hatzivassiloglou and McKeown (1997)to assign a positive and a negative polarity valueto an utterance given the number of subjectivepositive/negative adjectives it contains.
Similarly,Misra and Walker (2013) use the MPQA Subjec-152tivity Lexicon (Wilson et al., 2005) to capture thelocal sentiment of an online post response giventhe number of words in the response with stronglysubjective positive/negative polarity according tothe subjectivity lexicon.
Yin et al.
(2012) assign apositive and a negative score to a post by aggregat-ing the sentiment scores of those words that can befound in SentiWordNet (Baccianella et al., 2010).Although subjective polarity may be helpful(e.g., utterances with a high positive sentimentscore may be more likely to be acceptances), thisis not the kind of polarity that concerns us in thispaper.
Note, furthermore, that local sentiment in-formation may be superseded by logical polarity.
(4) A: But then it wouldn?t sit as comfortably inyour hand.B: It would still be comfortable.Despite the fact that B?s utterance in (4)?extracted from the AMI corpus?would be as-signed a positive sentiment score (given the pres-ence of the word ?comfortable?, classified as pos-itive in the MPQA Subjectivity Lexicon, and theabsence of negative subjective words), the utter-ance acts as a rejection due to logical polarity con-straints, as we shall make clear in the next section.3 Polarity in Acceptances and RejectionsIn this section, we first give a brief overview ofsome of the main ideas put forward in recent theo-retical approaches to polarity.
Afterwards, we in-troduce our approach to logical polarity in the con-text of acceptance and rejection moves.3.1 Formal Semantics ApproachesPolarity and in particular negation are central con-cepts in formal semantics and pragmatics (Horn,1989).
Recent work independently put forwardwithin the frameworks of Type Theory withRecords (Cooper and Ginzburg, 2011; 2012) andof Inquisitive Semantics (Farkas and Roelofsen,2013) has proposed to semantically distinguish be-tween positive and negative propositions.
Such aproposal departs from the traditional view in for-mal semantics where propositions are taken to de-note sets of possible worlds (see Partee (1989) fora survey).
According to this traditional view, themeaning of (5a) would be indistinguishable fromthat of (5b), given that the two propositions aretrue in exactly the same possible worlds:(5) a.
Sue failed the exam.b.
Sue didn?t pass the exam.These utterances, however, license different typesof responses.
For instance, responding ?no?
to(5a) would assert that Sue did pass the exam,while the same response to (5b) would typicallybe understood as asserting the opposite.
Leavingaside many details that distinguish the two theo-ries, Cooper/Ginzburg and Farkas/Roelofsen pro-pose that polarity particles?words like ?yes?
and?no?
?are sensitive to the polarity of their an-tecedent: ?yes?
presupposes that a positive propo-sition is under discussion, while ?no?
presupposesa negative proposition.
If the presupposition ismet, both ?yes?
and ?no?
assert the propositionunder discussion (i.e., in our terms, they act as ac-ceptances); if the presupposition fails, they assertthe negation of the proposition under discussion(i.e., they act as rejections).This characterises the standard behaviour of po-larity particles.
However, the picture is slightlymore complicated since, when the proposition un-der discussion is negative, in English ?yes?
and?no?
can also be used to agree or disagree, respec-tively (contrary to the standard case):(6) Sue didn?t pass the exam.a.
No (she didn?t).
; standard acceptanceYes, she didn?t.b.
Yes, she did.
/ #Yes.
; standard rejectionNo, she did.According to Farkas and Roelofsen (2013), thisambiguity of use makes bare forms of ?yes?/?no?less likely in the non-standard cases exemplifiedin (6) and favours more explicit sentential formswhere the presence of the verb disambiguates theintended interpretation.
In this respect, however,the standard rejection in (6b) constitutes a spe-cial case: While in standard acceptances the sen-tential form is not required, in standard rejectionsit seems needed.
According to these authors, inEnglish the positive polarity particle ?yes?
has astrong preference for realising an agreement moveand therefore its use as a rejection is marked.1This makes the explicit sentential form ?Yes, shedid?
in (6b) more felicitous than the bare form?Yes?.
Thus, the two types of rejections to a neg-ative proposition we see in (6b)?with ?yes?
and1The special status of English ?yes?
for rejection seemsto be supported by cross-linguistic evidence.
For instance,German has a special positive polarity particle ?doch?
forrejecting a negative proposition: in response to the assertionin (6), ?doch?
would be used to disagree (?yes, she did?
)while ?ja?
would be used to agree (?yes, she didn?t?
).153Polarity of P -R Type Example from AMI Meeting Corpuspositive ?
positive default relative acceptance A: And then you can buy the covers.
B: Yesnegative?
negative reverse relative acceptance A: It?s not very well advertised.
B: No, it?s not.positive ?
negative default relative rejection A: It?s a frog.
B: No, it?s a turtle.negative?
positive reverse relative rejection A: TVs aren?t capable of sending.
B: Yes, they are.Table 1: Relative response types.?no?
?are expected to contain an explicit verbalconstituent.
We refer to this as the markedness ex-pectation.3.2 Our ModelOur aim is to exploit insights from the theo-ries sketched above to develop a model that canbe operationalised in a computational setting totest whether information regarding logical polaritycan contribute to automatically distinguish accep-tances from rejections.We focus on proposal-response pairs (P -R),where R either accepts or rejects P .
We pro-pose to assign both the proposal and the responsea logical polarity: either positive or negative.
Fur-thermore, we differentiate absolute (polarity inde-pendent) from relative (polarity independent) re-sponses.
A response type R is absolute if its ac-ceptance/rejection function does not depend on thepolarity of P , and it is relative if it does.
Formally,we say that a proposal P is rejected by a responseR if P ^ R is inconsistent.
This gives us the fol-lowing four possible responses to P :?
R ?
> : absolute acceptance?
R ?
?
: absolute rejection?
R ?
P : relative acceptance?
R ?
?P : relative rejectionOur focus of attention is on relative responses.Given aP -R pair with a relative response, we inferan acceptance if the polarities of P and R align,and a rejection if the polarities differ.
This givesus four possible relative responses, shown in Ta-ble 1.
In the default cases, where P is positive,positive responses act as acceptances and nega-tive responses as rejections?exactly as absoluteresponse types would act.
When P is negative(i.e., P ?
?P0), we are faced with what we callreverse relative responses: Negative polarity re-sponses act as acceptances and positive polarityresponses as rejections.
An acceptance can havethe form R ?
P ?
?P0while a rejection can havethe form R ?
?P ?
P0(with R being positive,i.e., with the double negation ?
?P0eliminated).We call these cases reverse responses because theirpolarity signature is precisely the negation of therespective default cases (cf.
Table 1).The next obvious question to address is how thepolarity of proposals and responses can be deter-mined.
Clearly, this will differ across languages.For the case of English, we shall assume that po-larity is linked to the presence of particular par-ticles and grammatical indicators.
In particular,we consider the words in Table 2 to be positiveand negative polarity markers.2Amongst negativepolarity markers, we distinguish between negativepolarity particles and negation indicators.positive particles: yes, yeah, yepnegative particles: no, nope, nahnegation: not, -n?t, never,nothing, nobody, nowhereTable 2: Polarity markers.All markers in Table 2 are key cues of polar-ity.
However, they do not straightforwardly deter-mine the polarity of a contribution.
Firstly, thereare cases where the presence of a marker does nothave the expected effect on polarity.
For instance,a negative tag question (?isn?t it??)
at the end ofan utterance does not mark that utterance as neg-ative.
Also, the polarity effect of a marker can beinvalidated if it is followed by the contrast con-nective ?but?.
For instance, in the following AMIexamples, ?but?
cancels out the effect of the neg-ative polarity particle ?no?
in (7), making B?s ut-terance positive, and the effect of the positive po-larity particle ?yeah?
in (8), making B?s utterancenegative (in conjunction with the verbal negationin this case):(7) Reverse rejection: negative?positiveA: Yes, but some televisions don?t support it.B: No, but then they would also support thatbutton, because it?s the same thing.
(8) Default rejection: positive?negativeA: Yeah, uh materials like wood thatB: Yeah, but wood is not a not a material youwhich you build a a remote control of .2We do not claim that this list is exhaustive.154Secondly, it is important to take into account thata large amount of acceptances and rejections donot include any marker of polarity at all.
For in-stance, in our datasets extracted from the AMI andSwitchboard (SWB) corpora (which we will de-scribe in detail in Section 4.1), 49% and 70% ofacceptances in AMI and SWB, respectively, do notcontain any explicit polarity marker; and similarlyfor 40% (AMI) and 15% (SWB) of rejections.In part this is due to the fact that in English (asin most languages) there is no morphologically-realised positive counterpart of verbal negation.Given the observations above, we adopt the heu-ristics in Figure 1 to assign a polarity to P and R.Since this heuristics is intended to be applicableto dialogue corpora, we forgo the use of deep se-mantic analysis, which is difficult to achieve whendealing with naturally occurring spoken language.3P -polarity: A proposal P has negative polarity if it con-tains a negation indicator (excluding tag questions); oth-erwise, P has positive polarity.R-polarity: We define a precedence order on polaritymarkers: negative polarity particles take precedence overpositive polarity particles, which in turn take precedenceover negation indicators.?
If a response R contains a negative polarity particle(not followed by ?but?
), its polarity is negative.?
Else, if R contains a positive polarity marker (not fol-lowed by ?but?
), its polarity is positive.?
Else, if R contains a negation indicator, its polarity isnegative.?
Otherwise, R has positive polarity.Figure 1: Heuristics for polarity determination.Drawing on the notion of markedness expecta-tion we introduced at the end of Section 3.1, wehypothesise that the lack of explicit positive polar-ity markers will be compensated for by the pres-ence of sentential similarity patterns between pro-posals and responses.
It follows from our descrip-tion of relative responses (see Table 1) that theywill either semantically mirror the proposal (ac-ceptances) or negate it (rejections).
In the absenceof an explicit positive polarity marker in the pro-posal or the response, therefore, we expect to findsome form of sentential parallelism, potentially inboth cases?when P -R polarities align, as in (9),and when they differ, as in (10):43Amongst other things, this means we do not account forthe scope of negation.4Both examples are extracted from the AMI corpus.
(9) A: It?s still it?s still working,B: It is.
(10) A: It?s a fat cat.B: It is not a fat cat.According to the markedness expectation, thistype of parallelism is expected in reverse relativeresponses even when polarity particles are presentas in (11) from Switchboard and in the reverse re-sponse examples in Table 1.
Hence, we conjec-ture that parallelism will be present with higherfrequency in the reverse cases.
(11) A: They wouldn?t be able to own a house.B: Yes, they would.4 ExperimentsIn order to automatically test the extent to whichlogical polarity plays a role in determining thefunction of naturally occurring acceptances andrejections, we conduct machine learning experi-ments on dialogue corpus data.
We first explainhow we create our dataset, then describe how wedevise features that encode polarity information,and finally report the results obtained.4.1 DatasetsWe test our model on two different corpora: TheSwitchboard Corpus (SWB) (Godfrey et al., 1992)and the AMI Meeting Corpus (Carletta, 2007).SWD is a collection of around 2400 recorded andtranscribed telephone conversations between twodialogue participants.
The speakers are providedwith a topic and then converse freely.
In con-trast, AMI contains transcriptions from around100 hours of recorded multiparty conversationsamongst four dialogue participants who interactface-to-face in a meeting setting.
The speakersconverse freely, but they play roles (such as in-dustrial designer or project manager) in a ficti-tious design team whose goal is to design a re-mote control.
Therefore the dialogue is mildlytask-oriented.
Both corpora have been annotatedwith dialogue acts (DAs), albeit with slightly dif-ferent DA annotation schemes: SWD is annotatedwith the SWBD-DAMSL tagset (Jurafsky et al.,1997), while AMI uses a coarser-grained tagsetbut includes relations between some DAs (looselycalled adjacency pair annotations).55The AMI DA annotation manual is available athttp://mmm.idiap.ch/private/ami/annotation/dialogue_acts_manual_1.0.pdf155acceptances rejections total P -RSWB 4534 (97%) 145 (3%) 4679AMI 7405 (91%) 697 (9%) 8102Table 3: Class distribution in our datasets.We use the DA annotations to extract adataset of proposal-response (P -R) pairs foreach corpus as follows.
To construct the SWBdataset, we extract all utterances u annotatedas Agree/Accept or Reject that are turn-initial and that are immediately preceded bya turn whose last utterance u0is annotatedas Statement-non-opinion, Statement-opinion or Summarize/Reformulate.
Toconstruct the AMI dataset, we extract all ut-terances u annotated as Assessment that areturn initial and that are linked with the re-lations Support/Positive Assessment orObjection/Negative Assessment to anearlier utterance u0that is not annotated asElicit Inform or Elicit Assessment (i.e.,that is not a question).
In both cases, P cor-responds to u0and R to the first five wordsof u.
We consider R an acceptance if u isannotated as Agree/Accept in SWB or asSupport/Positive Assessment in AMI, and arejection if it is annotated as Reject in SWB oras Objection/Negative Assessment in AMI.We take the first five words of a turn-initial ut-terance to be the most relevant ones for convey-ing acceptance or rejection.
This is motivated bythe fact that dialogue participants typically provideevidence of understanding?and, by extension, ofagreement or disagreement?at the earliest oppor-tunity in order to avoid misunderstandings on whatthey take to be common ground (Pomerantz, 1984;Clark, 1996).
However, when extracting our P -Rpairs we retain the entire utterance u (of which Ris a prefix) in order to be able to take its length intoaccount in the automatic classification experiment,as explained in the next section.Finally, we observe that in the two corpora allthe P -R pairs where R is just a single ?yeah?
areacceptances.
Thus, in the terminology we intro-duced in Section 3.2, bare ?yeah?
seems to be anabsolute response type, whose acceptance func-tion is independent of the polarity of P (in contrastto the relative response types in Table 1).
Sinceidentification of these acceptances is trivial, wediscard them from our datasets.
The final distri-bution of acceptances and rejections in each of thedatasets is shown in Table 3.
As can be seen, thedata is highly skewed, with less than 10% of P -Rpairs corresponding to rejections.4.2 FeaturesWe derive different types of features to test ourmodel.
We are not interested in using largeamounts of unmotivated features, but rather in ex-ploiting a small set of meaningful domain- andsetting-independent features that can help us to in-vestigate the impact of logical polarity.
The fea-ture we use are summarised in Figure 2.
We con-sider several local features of the response.
Mostof these features are inspired by earlier approachesreviewed in Section 2, such as those by Galley etal.
(2004) and Misra and Walker (2013).
We useseveral lexical features that act as cues for accep-tance or rejection.
For instance, the presence of?yeah?
is a good cue for acceptance, while thepresence of ?but?
is a strong cue for rejection.The bigram ?yeah, but?
is in turn a good indica-tor for rejection?the ?yeah?
in such cases seemsto be an attempt at politeness (Brown and Levin-son, 1987; Bousfield, 2008).
Since rejections aredispreferred moves, they are frequently initiatedwith a hedging such as ?well?
or with hesitationor stalling (Byron and Heeman, 1997).
Theseutterance-initial cues are aggregated into one fea-ture.
Rejections also tend to be longer than ac-ceptances since the speaker feels the need to jus-tify the unexpected move (Pomerantz, 1984).
Wetake into account the length of the entire utter-ance containing R with three binary features.6Wealso consider less frequent semantic indicators foracceptance and rejection, respectively, which wegroup into two aggregate features that record thepresence of agreement words such as ?okay?
or?correct?
and contrast words such as ?however?or ?although?.
Given our observations regard-ing polarity and polarity particles in Section 3, incontrast to previous approaches we don?t include?yes?
and ?no?
as local lexical cues.
Instead,we add a new local feature encoding the polarityof the response as determined by the R-polarityheuristics in Figure 1.76The use of Boolean features here is motivated by ourchoice of classifier, as we point out in the next subsection.The length thresholds have been set up manually after quali-tative examination of several examples.7We have tested other theoretically motivated local fea-tures, such as turn-length and number of disfluencies.
Thelocal R features in Fig.
2 combined with the local R polarityfeature correspond to the best performing local feature set.156Local features cannot capture the most interes-ting aspects of logical polarity, which originatefrom the interaction between the polarities of theproposal and the response in relative responsetypes.
To account for this, we introduce four rela-tive P -R polarity features corresponding to the re-sponse types described in Table 1.
Finally, we in-troduce a feature that records the presence of someform of parallelism between P andR.
As we men-tioned at the end of Section 3.2, the markednessexpectation predicts that sentential parallelismwill occur more frequently in reverse relative re-sponses, i.e., responses to negative proposals.
Theparallelism feature targets such cases.
We restrictourselves to strict identity between P and R ofa pronominal subject and a verb (in negative vs.positive form).8The feature therefore is only ableto capture examples such as (12a) but not (12b),where anaphora resolution would be required.9(12) a.
A: But it wouldn?t be very attractive.B: No, it would.b.
A: TVs aren?t capable of sending.B: Yes, they are.4.3 ResultsWe conducted the machine learning experimentusing BernoulliNB, the Bernoulli-distributedNaive Bayesian classifier from scikit-learn (Pe-dregosa et al., 2011), which outperformed sev-eral other classifiers, including Random Forestsand a Support Vector Machine.
We chose thisclassifier because our main features?the relativepolarities?are Boolean and our data is highly im-balanced.10Given the high relative frequency ofacceptances over rejections in our datasets (seeTable 3), measuring accuracy or retrieving accep-tances would yield very good results.
Hence, asdiscussed in section 2, we believe that the mostdiscerning task is the retrieval of rejections.
Pre-cision, recall and F -scores for this task, with theclassifier trained on different combinations of fea-ture sets, are shown in Table 4.
We developed theclassifier on the whole AMI dataset, as the smallnumber of rejections makes splitting up the cor-pus into a development and a test set infeasible.The SWB corpus was exclusively used for testing.In the AMI dataset we tested the classifier with8We use the NLTK POS tagger to implement this feature(Bird et al., 2009).9Given the high frequency of pronominal forms in spokendialogue, pronoun identity turns out to be reasonably useful.10The scikit-learn documentation indicates that this classi-fier is particularly suited for sparse data and Boolean features.LOCAL R FEATURESLength of utterance containing R in number of words:?
Three features: l>2, l>12, l>24Acceptance Indicators:?
R contains yeah?
R contains any of absolutely, okay, accept, agree, cor-rect, either, true, sure, not preceded by notRejection Indicators:?
R contains but?
R contains the bigram ?yeah, but??
R starts with any of well, oh, uh, mm?
R contains any of actually, however, though, althoughLOCAL R POLARITY FEATURE?
positive or negative, according to R-polarity in Fig.
1RELATIVE P -R POLARITY FEATURES (cf.
Fig.
1)?
positive?positive?
positive?negative?
negative?negative?
negative?positiveRELATIVE P -R PARALLELISM FEATUREOne of the following patterns appears in P -R, where apronoun p, an auxiliary verb aux and a main verb v areidentical in P and R:?
?p aux not?
?
?p aux?
not followed by {n?t| not}?
?p (aux) not v?
?
?p v??
?I do{n?t| not} {think|know} {that|if} p aux?
?
?p aux?
not followed by {n?t| not}Figure 2: Feature types (all features are Boolean).10-fold cross-validation and in the SWB datasetwith 5-fold cross-validation, due to the more lim-ited amount of rejections in this corpus.
Also, dueto the lack of training data, the more specific Rel-ative P -R Parallelism feature could not be appliedto the SWB corpus.For comparison we report the results of a sim-ple unigram baseline: Each content word that oc-curs at least 5 times in the dataset is used as aBoolean feature (occurrence vs. non-occurrence).This achieves F -scores of 31.66 in AMI and 16.63in SWB.
As a more substantial baseline we con-sider a system that uses only local features of theresponse, including local polarity.
This feature-set is expected to capture relatively well the ac-cepting/rejecting function of absolute responsesand default relative responses, since their functionaligns with their local polarity.
This yields an F -score of 52.24 in AMI and of 33 in SWB.
The Rel-ative Polarity features were conceived to reduceclassification confusion grounded in reverse polar-ity: If only local features are considered, a reversepolarity acceptance would appear to be a rejection,157AMI SWBFeature sets Precision Recall F1 Precision Recall F1Unigrams 35.61% 28.97% 31.66 24.20% 12.93% 16.63Local + Local Polarity 44.13% 64.12% 52.24 20.80% 82.46% 33.00Local + Relative Polarity 58.08% 61.63% 59.75 49.12% 72.93% 58.49Local + Relative Pol.
+ Parallelism 58.23% 64.04% 60.96 n/a n/a n/aTable 4: Precision, Recall, and F -scores for rejection identification.while a reverse polarity rejection would seem tobe an acceptance.
Moving from local to relativepolarity features should therefore reduce this con-fusion.
Indeed, in both corpora the precision is in-creased substantially (from 44.13% to 58.08% inAMI and from 20.8% to 49.12% in SWB), caus-ing a great increase in F -scores: 59.75 in AMIand 58.49 in SWB (paired t-tests show all theseincreases are significant, with p < 0.001).However, in both datasets we observe a reduc-tion in recall when moving from local to relativepolarity.
We believe that this is in part due to therelative polarity features ignoring some absoluteuses of polarity particles, which may have beencaptured by Local Polarity.11The Relative Paral-lelism feature should be able to help in such cases.For instance, in example (12a) B?s utterance wouldbe assigned negative polarity and therefore the rel-ative polarity features would contribute to clas-sify it as an acceptance (since in the large major-ity of cases negative-negative P -R pairs do cor-respond to acceptances).
In this case, however,?no?
is used absolutely, i.e., as a rejection.
Due tothe markedness expectation, this is likely to showup in the form of contrastive parallelism, whichwe can?at least in part?capture with our sim-ple feature.
Indeed, adding this feature to the AMIdataset raises recall back to baseline level: 64.04%vs.
61.63% (p < 0.005).
This, in turn, increase theAMI F -score from 59.75 to 60.96 (p < 0.05).5 ConclusionsThe overall aim of this paper has been to investi-gate the influence of logical polarity in interpret-ing utterances as acceptance or rejection movesin dialogue.
We have built on recent work onthe semantics of negation and polarity particles byCooper and Ginzburg (2011; 2012) and Farkas andRoelofsen (2013) to develop an approach to polar-ity that is theoretically motivated and that can becomputationally tested on corpus data.
Although11We note that the featureset Local + Local Polarity + Rel-ative Polarity does not outperform Local + Local Polarity inthe classification experiments.
We believe this indicates thatpolarity is indeed mostly a contextual phenomenon.there is a substantial amount of previous work onautomatically detecting agreement and disagree-ment in dialogue corpora, to our knowledge therole of logical polarity had not been explicitly in-vestigated before.Our focus has been on relative responses, i.e.,responses where simply taking into account cluesfrom the utterance to be classified is insufficient?or can even be misleading?to infer acceptance orrejection.
We have argued that relative responsesrequire taking into account how the polarities ofthe response and of the current proposal under dis-cussion interact, and have put forward a model thatcaptures such interaction.
Our experiments showthat the use of information on relative polarity sub-stantially helps to distinguish acceptances from re-jections.
This indicates, on the one hand, that ourmodel does a reasonably good job at capturing thisphenomenon, and on the other hand, that relativepolarity responses are not merely a theoreticallyinteresting phenomenon but are in fact widespreadin actual dialogue.There is certainly room for improving the im-plementation of our heuristics, for instance byusing finer-grained semantic and syntactic infor-mation: e.g., we cannot currently capture accep-tance/rejection of a subclause, implicature rejec-tions, rhetorical questions, nor sarcasm?all ofwhich affect the recall of our system.
Interestingly,the classification experiments yield very similarresults in the two corpora with the Local + RelativePolarity feature set?F -scores of 59.75 in AMIand 58.49 in SWB.
This indicates that our theo-retical observations are applicable independentlyof setting, domain and number of speakers.
Thereseem to be some differences across the two cor-pora, however, since the impact of relative polarityinformation is much higher in SWB than in AMI(the F -score goes up around 7 in AMI when mov-ing from local to relative polarity, while in SWB itincreases by 25).
A deeper investigation into theshortcomings of our implemented model and ofwhere these shortcomings affect AMI differentlythan SWB are issues we leave for future work.158ReferencesRob Abbott, Marilyn Walker, Pranav Anand, Jean E.Fox Tree, Robeson Bowmani, and Joseph King.2011.
How Can You Say Such Things?!?
: Rec-ognizing Disagreement in Informal Political Argu-ment.
In Proceedings of the Workshop on Lan-guages in Social Media, pages 2?11.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An Enhanced Lex-ical Resource for Sentiment Analysis and OpinionMining.
In Proceedings of LREC, pages 2200?2204.Steven Bird, Edward Loper, and Ewan Klein.2009.
Natural Language Processing with Python.O?Reilly Media Inc.Derek Bousfield.
2008.
Impoliteness in Interaction.John Benjamins.Konstantinos Bousmalis, Marc Mehu, and Maja Pan-tic.
2013.
Towards the Automatic Detection ofSpontaneous Agreement and Disagreement Basedon Nonverbal Behaviour: A Survey of Related Cues,Databases, and Tools.
Image Vision Computing,31(2):203?221.Penelope Brown and Stephen Levinson.
1987.
Po-liteness: Some universals in language usage.
Cam-bridge University Press.Donna K. Byron and Peter A. Heeman.
1997.
Dis-course marker use in task-oriented spoken dialog.
InProceedings of Eurospeech, pages 2223?2226.Jean Carletta.
2007.
Unleashing the killer corpus:experiences in creating the multi-everything AMIMeeting Corpus.
Language Resources and Evalu-ation, 41(2):181?190.Herbert H. Clark and Edward F. Schaefer.
1989.Contributing to discourse.
Cognitive Science,13(2):259?294.Herbert H. Clark.
1996.
Using language.
CambridgeUniversity Press.Robin Cooper and Jonathan Ginzburg.
2011.
Nega-tion in dialogue.
In Proceedings of the 15th SemDialWorkshop (Los Angelogue), pages 130?139.Robin Cooper and Jonathan Ginzburg.
2012.
Nega-tive inquisitiveness and alternatives-based negation.In Logic, Language and Meaning: Proceedings ofthe 18th Amsterdam Colloquium, Lecture Notes inComputer Science, pages 32?41.
Springer.Donka Farkas and Floris Roelofsen.
2013.
Polar ini-tiatives and polar particle responses in an inquisi-tive discourse model.
Available from http://www.illc.uva.nl/inquisitivesemantics/.Raquel Fern?andez, Matthew Frampton, Patrick Ehlen,Matthew Purver, and Stanley Peters.
2008.
Mod-elling and Detecting Decisions in Multi-Party Dia-logue.
In Proceedings of the 9th SIGdial Workshopon Discourse and Dialogue, pages 156?163.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech:Use of bayesian networks to model pragmatic de-pendencies.
In Proceedings of the 42nd Meetingof the Association for Computational Linguistics(ACL?04), pages 669?676.Sebastian Germesin and Theresa Wilson.
2009.Agreement detection in multiparty conversation.
InProceedings of the 2009 international conference onMultimodal interfaces, pages 7?14.
ACM.John J. Godfrey, E. C. Holliman, and J. McDaniel.1992.
SWITCHBOARD: Telephone Speech Cor-pus for Research and Development.
IEEE Confer-ence on Acoustics, Speech, and Signal Processing,1:517?520.Sangyun Hahn, Richard Ladner, and Mari Ostendorf.2006.
Agreement/disagreement classification: Ex-ploiting unlabeled data using contrast classifiers.
InProceedings of the HLT-NAACL, pages 53?56.Vasileios Hatzivassiloglou and Kathleen R McKeown.1997.
Predicting the semantic orientation of ad-jectives.
In Proceedings of the 35th Annual Meet-ing of the Association for Computational Linguis-tics and Eighth Conference of the European Chap-ter of the Association for Computational Linguistics,pages 174?181.Dustin Hillard, Mari Ostendorf, and ElizabethShriberg.
2003.
Detection of agreement vs. dis-agreement in meetings: training with unlabeled data.In Proceedings of the HLT-NAACL 2003, pages 34?36.Julia L. B. Hirschberg.
1985.
A theory of scalar impli-cature.
Ph.D. thesis, University of Pennsylvania.Laurence R. Horn.
1989.
A Natural History of Nega-tion.
University of Chicago Press.Adam Janin, Don Baron, Jane Edwards, Dan Ellis,David Gelbart, Nelson Morgan, Barbara Peskin,Thilo Pfau, Elisabeth Shriberg, Andreas Stolcke,and Chuck Wooters.
2003.
The ICSI Meeting Cor-pus.
In Proceedings of ICASSP?03, pages 364?367.Dan Jurafsky, Elizabeth Shriberg, and Debra Bi-asca.
1997.
Switchboard SWBD-DAMSL shallow-discourse-function-annotation coder?s manual, draft13.
Technical Report TR 97-02, Institute for Cogni-tive Science, University of Colorado at Boulder.Alex Lascarides and Nicholas Asher.
2009.
Agree-ment, disputes and commitments in dialogue.
Jour-nal of Semantics, 26(2):109?158.Amita Misra and Marilyn Walker.
2013.
Topic in-dependent identification of agreement and disagree-ment in social media dialogue.
In Proceedings ofthe SIGDIAL 2013 Conference, pages 41?50, Metz,France.
Association for Computational Linguistics.159Barbara Partee.
1989.
Possible worlds in model-theoretic semantics: A linguistic perspective.
InS.
Allen, editor, Possible Worlds in Humanities, Artsand Sciences, pages 93?123.
Walter de Gruyter.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Anita Pomerantz.
1984.
Agreeing and disagree-ing with assessments: Some features of pre-ferred/dispreferred turn shapes.
In Structures of So-cial Action.
Cambridge University Press.Matthew Purver, John Dowding, John Niekrasz,Patrick Ehlen, Sharareh Noorbaloochi, and StanleyPeters.
2007.
Detecting and summarizing actionitems in multi-party dialogue.
In Proceedings of the8th SIGdial Workshop on Discourse and Dialogue,pages 18?25.Robert Stalnaker.
1978.
Assertion.
In P. Cole, edi-tor, Pragmatics, volume 9 of Syntax and Semantics,pages 315?332.
New York Academic Press.Marilyn A Walker, Jean E Fox Tree, Pranav Anand,Rob Abbott, and Joseph King.
2012.
A corpus forresearch on deliberation and debate.
In LREC, pages812?817.Marilyn A. Walker.
1996.
Inferring acceptance and re-jection in dialog by default rules of inference.
Lan-guage and Speech, 39(2-3):265?304.Wen Wang, Sibel Yaman, Kristin Precoda, ColleenRichey, and Geoffrey Raymond.
2011.
Detectionof agreement and disagreement in broadcast conver-sations.
In Proceedings of ACL, pages 374?378.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP, pages 347?354.Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.2012.
Unifying Local and Global Agreement andDisagreement Classification in Online Debates.
InProceedings of the 3rd Workshop in ComputationalApproaches to Subjectivity and Sentiment Analysis,pages 61?69.160
