Statistical Language Modeling with Performance Benchmarksusing Various Levels of Syntactic-Semantic InformationDharmendra KANEJIYA?, Arun KUMAR?, Surendra PRASAD?
?Department of Electrical Engineering?Centre for Applied Research in ElectronicsIndian Institute of TechnologyNew Delhi 110016 INDIAkanejiya@hotmail.com, arunkm@care.iitd.ernet.in, sprasad@ee.iitd.ernet.inAbstractStatistical language models using n-gramapproach have been under the criticism ofneglecting large-span syntactic-semantic in-formation that influences the choice of thenext word in a language.
One of the ap-proaches that helped recently is the use oflatent semantic analysis to capture the se-mantic fabric of the document and enhancethe n-gram model.
Similarly there havebeen some approaches that used syntacticanalysis to enhance the n-gram models.
Inthis paper, we explain a framework calledsyntactically enhanced latent semantic anal-ysis and its application in statistical lan-guage modeling.
This approach augmentseach word with its syntactic descriptor interms of the part-of-speech tag, phrase typeor the supertag.
We observe that given thissyntactic knowledge, the model outperformsLSA based models significantly in terms ofperplexity measure.
We also present someobservations on the effect of the knowledgeof content or function word type in languagemodeling.
This paper also poses the prob-lem of better syntax prediction to achievethe benchmarks.1 IntroductionStatistical language models consist of estimat-ing the probability distributions of a word giventhe history of words so far used.
The standardn-gram language model considers two historiesto be equivalent if they end in the same n ?
1words.
Due to the tradeoff between predictivepower and reliability of estimation, n is typicallychosen to be 2 (bi-gram) or 3 (tri-gram).
Eventri-gram model suffers from sparse-data estima-tion problem, but various smoothing techniques(Goodman, 2001) have led to significant im-provements in many applications.
But still thecriticism that n-grams are unable to capture thelong distance dependencies that exist in a lan-guage, remains largely valid.In order to model the linguistic structure thatspans a whole sentence or a paragraph or evenmore, various approaches have been taken re-cently.
These can be categorized into two maintypes : syntactically motivated and semanti-cally motivated large span consideration.
Inthe first type, probability of a word is decidedbased on a parse-tree information like grammat-ical headwords in a sentence (Charniak, 2001)(Chelba and Jelinek, 1998), or based on part-of-speech (POS) tag information (Galescu andRingger, 1999).
Examples of the second typeare (Bellegarda, 2000) (Coccaro and Jurafsky,1998), where latent semantic analysis (LSA)(Landauer et al, 1998) is used to derive large-span semantic dependencies.
LSA uses word-document co-occurrence statistics and a matrixfactorization technique called singular value de-composition to derive semantic similarity mea-sure between any two text units - words or doc-uments.
Each of these approaches, when inte-grated with n-gram language model, has led toimproved performance in terms of perplexity aswell as speech recognition accuracy.While each of these approaches has been stud-ied independently, it would be interesting to seehow they can be integrated in a unified frame-work which looks at syntactic as well as seman-tic information in the large span.
Towards thisdirection, we describe in this paper a mathemat-ical framework called syntactically enhanced la-tent syntactic-semantic analysis (SELSA).
Thebasic hypothesis is that by considering a wordalongwith its syntactic descriptor as a unit ofknowledge representation in the LSA-like frame-work, gives us an approach to joint syntactic-semantic analysis of a document.
It also pro-vides a finer resolution in each word?s seman-tic description for each of the syntactic con-texts it occurs in.
Here the syntactic descriptorcan come from various levels e.g.
part-of-speechtag, phrase type, supertag etc.
This syntactic-semantic representation can be used in languagemodeling to allocate the probability mass towords in accordance with their semantic simi-larity to the history as well as syntactic fitnessto the local context.In the next section, we present the mathe-matical framework.
Then we describe its ap-plication to statistical language modeling.
Insection 4 we explain the the use of various lev-els of syntactic information in SELSA.
That isfollowed by experimental results and conclusion.2 Syntactically Enhanced LSALatent semantic analysis (LSA) is a statistical,algebraic technique for extracting and inferringrelations of expected contextual usage of wordsin documents (Landauer et al, 1998).
It isbased on word-document co-occurrence statis-tics, and thus is often called a ?bag-of-words?approach.
It neglects the word-order or syn-tactic information in a language which if prop-erly incorporated, can lead to better languagemodeling.
In an effort to include syntactic in-formation in the LSA framework, we have de-veloped a model which characterizes a word?sbehavior across various syntactic and semanticcontexts.
This can be achieved by augment-ing a word with its syntactic descriptor andconsidering as a unit of knowledge representa-tion.
The resultant LSA-like analysis is termedas syntactically enhanced latent semantic anal-ysis (SELSA).
This approach can better modelthe finer resolution in a word?s usage comparedto an average representation by LSA.
This finerresolution can be used to better discriminatesemantically ambiguous sentences for cognitivemodeling as well as to predict a word usingsyntactic-semantic history for language model-ing.
We explain below, a step-by-step procedurefor building this model.2.1 Word-Tag-Document StructureThe syntactic description of a word can be inmany forms like part-of-speech tag, phrase typeor supertags.
In the description hereafter wecall any such syntactic information as tag of theword.
Now, consider a tagged training corpusof sufficient size in the domain of interest.
Thefirst step is to construct a matrix whose rowscorrespond to word-tag pairs and columns cor-respond to documents in the corpus.
A docu-ment can be a sentence, a paragraph or a largerunit of text.
If the vocabulary V consists ofI words, tagset T consists of J tags and thenumber of documents in corpus is K, then thematrix will be IJ ?
K. Let ci j,k denote thefrequency of word wi with tag tj in the docu-ment dk.
The notation i j (i underscore j) insubscript is used for convenience and indicatesword wi with tag tj i.e., (i ?
1)J + jth row ofthe matrix.
Then we find entropy ?i j of eachword-tag pair and scale the corresponding rowof the matrix by (1?
?i j).
The document lengthnormalization to each column of the matrix isalso applied by dividing the entries of kth doc-ument by nk, the number of words in documentdk.
Let ci j be the frequency of i jth word-tagpair in the whole corpus i.e.
ci j =?Kk=1 ci j,k.Then,xi j,k = (1?
?i j)ci j,knk(1)?i j = ?1logKK?k=1ci j,kci jlogci j,kci j(2)Once the matrix X is obtained, we performits singular value decomposition (SVD) and ap-proximate it by keeping the largest R singularvalues and setting the rest to zero.
Thus,X ?
X?
= USVT (3)where, U(IJ ?R) and V(K ?R) are orthonor-mal matrices and S(R?R) is a diagonal matrix.It is this dimensionality reduction step throughSVD that captures major structural associa-tions between words-tags and documents, re-moves ?noisy?
observations and allows the samedimensional representation of words-tags anddocuments (albeit, in different bases).This samedimensional representation is used (eq.
12) tofind syntactic-semantic correlation between thepresent word and the history of words and thento derive the language model probabilities.
ThisR-dimensional space can be called either syntac-tically enhanced latent semantic space or latentsyntactic-semantic space.2.2 Document Projection in SELSASpaceAfter the knowledge is represented in the latentsyntactic-semantic space, we can project anynew document as a R dimensional vector v?selsain this space.
Let the new document consistof a word sequence wi1 , wi2 , .
.
.
, win and let thecorresponding tag sequence be tj1 , tj2 , .
.
.
, tjn ,where ip and jp are the indices of the pth wordand its tag in the vocabulary V and the tagsetT respectively.
Let d be the IJ ?
1 vector rep-resenting this document whose elements di j arethe frequency counts i.e.
number of times wordwi occurs with tag pj , weighted by its corre-sponding entropy measure (1 ?
?i j).
It can bethought of as an additional column in the ma-trix X, and therefore can be thought of as hav-ing its corresponding vector v in the matrix V.Then, d = USvT andv?selsa=vS=dTU=1nn?p=1(1?
?ip jp)uip jp (4)which is a 1?R dimensional vector representa-tion of the document in the latent space.
Hereuip jp represents the row vector of the SELSAU matrix corresponding to the word wip andtag tjp in the current document.We can also define a syntactic-semantic simi-larity measure between any two text documentsas the cosine of the angle between their pro-jection vectors in the latent syntactic-semanticspace.
With this measure we can address theproblems that LSA has been applied to, namelynatural language understanding, cognitive mod-eling, statistical language modeling etc.3 Statistical Language Modelingusing SELSA3.1 FrameworkWe follow the framework in (Bangalore, 1996)to define a class-based language model whereclasses are defined by the tags.
Here probabilityof a sequence Wn of n words is given byP (Wn) =?t1.
.
.
?tnn?q=1P (wq|tq,Wq?1, Tq?1)P (tq|Wq?1, Tq?1) (5)where ti is a tag variable for the word wi.
Tocompute this probability in realtime based onlocal information, we make certain assumptions:P (wq|tq,Wq?1, Tq?1) ?
P (wq|tq, wq?1, wq?2)P (tq|Wq?1, Tq?1) ?
P (tq|tq?1) (6)where probability of a word is calculated byrenormalizing the tri-gram probability to thosewords which are compatible with the tag in con-text.
Similarly, tag probability is modeled usinga bi-gram model.
Other models like tag basedlikelihood probability of a word or tag tri-gramscan also be used.
Similarly there is a motiva-tion for using the syntactically enhanced latentsemantic analysis method to derive the wordprobability given the syntax of tag and seman-tics of word-history.The calculation of perplexity is based on con-ditional probability of a word given the wordhistory, which can be derived in the followingmanner using recursive computation.P (wq|Wq?1)=?tqP (wq|tq,Wq?1)P (tq|Wq?1)?
?tqP (wq|tq, wq?1, wq?2)?tq?1P (tq|tq?1)P (tq?1|Wq?1)=?tqP (wq|tq, wq?1, wq?2)?tq?1P (tq|tq?1)P (Wq?1, tq?1)?tq?1 P (Wq?1, tq?1)(7)where,P (Wq, tq)=??
?tq?1P (Wq?1, tq?1)P (tq|tq?1)?
?P (wq|tq, wq?1, wq?2) (8)A further reduction in computation isachieved by restricting the summation over onlythose tags which the target word can anchor.
Asimilar expression using the tag tri-gram modelcan be derived which includes double summa-tion.
The efficiency of this model depends uponthe prediction of tag tq using the word his-tory Wq?1.
When the target tag is correctlyknown, we can derive a performance bench-mark in terms of lower bound on the perplexityachievable.
Furthermore, if we assume taggedcorpus, then tq?s and Tq?s become deterministicvariables and (5) and (7) can be written as,P (Wn) =n?q=1P (wq|tq,Wq?1, Tq?1) (9)P (wq|Wq?1) = P (wq|tq,Wq?1, Tq?1) (10)respectively in which case the next describedSELSA language model can be easily applied tocalculate the benchmarks.3.2 SELSA Language ModelSELSA model using tag information for eachword can also be developed and used along theline of LSA based language model.
We canobserve in the above framework the need forthe probability of the form P (wq|tq,Wq?1, Tq?1)which can be evaluated using the SELSA rep-resentation of the word-tag pair correspondingto wq and tq and the history Wq?1Tq?1.
Theformer is given by the row uiq jq of SELSA Umatrix and the later can be projected onto theSELSA space as a vector ?
?vq?1 using (4).
Thelength of history can be tapered to reduce theeffect of far distant words using the exponentialforgetting factor 0 < ?
< 1 as below:?
?vq?1 =1q ?
1q?1?p=1?q?1?p(1?
?ip jp)uip jp (11)The next step is to calculate the cosine mea-sure reflecting the syntactic-semantic ?closeness?between the word wq and the history Wq?1 asbelow:K(wq,Wq?1) =uiq jq ??vTq?1?
uiq jqS12 ??
?
?vq?1S?12 ?
(12)Then SELSA based probabilityP (sel)(wq|Wq?1) is calculated by allocatingtotal probability mass in proportion to thiscloseness measure such that least likely wordhas a probability of 0 and all probabilities sumto 1:Kmin(Wq?1) = minwi?VK(wi,Wq?1) (13)P?
(wq|Wq?1)=K(wq,Wq?1)?Kmin(Wq?1)?wi?V(K(wi,Wq?1)?Kmin(Wq?1))(14)But this results in a very limited dynamic rangefor SELSA probabilities which leads to poorperformance.
This is alleviated by raising theabove derived probability to a power ?
> 1 andthen normalizing as follows(Coccaro and Juraf-sky, 1998):P (sel)(wq|Wq?1) =P?
(wq|Wq?1)??wi?VP?
(wi|Wq?1)?
(15)This probability gives more importance to thelarge span syntactic-semantic dependencies andthus would be higher for those words which aresyntactic-semantically regular in the recent his-tory as compared to others.
But it will notpredict very well certain locally regular wordslike of, the etc whose main role is to supportthe syntactic structure in a sentence.
On theother hand, n-gram language models are ableto model them well because of maximum likeli-hood estimation from training corpus and var-ious smoothing techniques.
So the best perfor-mance can be achieved by integrating the two.One way to derive the ?SELSA + N-gram?
jointprobability P (sel+ng)(wq|Wq?1) is to use the ge-ometric mean based integration formula givenfor LSA in (Coccaro and Jurafsky, 1998) as fol-lows:P (sel+ng)(wq|Wq?1)=[P (sel)(wq |Wq?1)]?iq [P (wq |wq?1,...,wq?n+1)]1?
?iq?wi?V[P (sel)(wi|Wq?1)]?i [P (wi|wq?1,...,wq?n+1)]1?
?i(16)where, ?iq =1?
?iq jq2 and ?i =1?
?i jq2 are the ge-ometric mean weights for SELSA probabilitiesfor the current word wq and any word wi ?
Vrespectively.4 Various Levels of SyntacticInformationIn this section we explain various levels of syn-tactic information that can be incorporatedwithin SELSA framework.
They are supertags,phrase type and content/fuction word type.These are in decreasing order of complexity andprovide finer to coarser levels of syntactic infor-mation.4.1 SupertagsSupertags are the elementary structures of Lex-icalized Tree Adjoining Grammars (LTAGs)(Bangalore and Joshi, 1999).
They are com-bined by the operations of substitution and ad-junction to yield a parse for the sentence.
Eachsupertag is lexicalized i.e.
associated with atleast one lexical item - the anchor.
Further,all the arguments of the anchor of a supertagare localized within the same supertag which al-lows the anchor to impose syntactic and seman-tic (predicate-argument) constraints directly onits arguments.
As a result, a word is typicallyassociated with one supertag for each syntac-tic configuration the word may appear in.
Su-pertags can be seen as providing a much morerefined set of classes than do part-of-speech tagsand hence we expect supertag-based languagemodels to be better than part-of-speech basedlanguage models.4.2 Phrase-typeWords in a sentence are not just strung to-gether as a sequence of parts of speech, butrather they are organized into phrases, group-ing of words that are clumped as a unit.
Asentence normally rewrites as a subject nounphrase (NP) and a verb phrase (VP) which arethe major types of phrases apart from proposi-tional phrases, adjective phrases etc (Manningand Schutze, 1999).
Using the two major phrasetypes and the rest considered as other type, weconstructed a model for SELSA.
This model as-signs each word three syntactic descriptions de-pending on its frequency of occurrence in eachof three phrase types across a number of doc-uments.
This model captures the semantic be-haviour of each word in each phrase type.
Gen-erally nouns accur in noun phrases and verbsoccur in verb phrases while prepositions occurin the other type.
So this framework brings inthe finer syntactic resolution in each word?s se-mantic description as compared to LSA basedaverage description.
This is particularly moreimportant for certain words occurring as bothnoun and verb.4.3 Content or Function Word TypeIf a text corpus is analyzed by counting wordfrequencies, it is observed that there are cer-tain words which occur with very high frequen-cies e.g.
the, and, a, to etc.
These words havea very important grammatical behaviour, butthey do not convey much of the semantics.
Thsewords are called function or stop words.
Sim-ilarly in a text corpus, there are certain wordswith frequencies in moderate to low range e.g.car, wheel, road etc.
They each play an impor-tant role in deciding the semantics associatedwith the whole sentence or document.
Thusthey are known as content words.
Generallya list of vocabulary consists of a few hundredfunction words and a few tens of thousands ofcontent words.
However, they span more or lessthe same frequency space of a corpora.
So it isalso essential to give them equal importance bytreating them separately in a language model-ing framework as they both convey some sortof orthogonal information - syntactic vs seman-tic.
LSA is better at predicting topic bearingcontent words while parsing based models arebetter for function words.
Even n-gram mod-els are quite better at modeling function words,but they lack the large-span semantic that canbe achieved by LSA.
On the other hand, SELSAmodel is suitable for both types of words as itcaptures semantics of a word in a syntactic con-text.We performed experiments with LSA andSELSA with various levels of syntactic informa-tion in both the situations - content words onlyvs content and function words together.
In theformer case, the function words are treated byn-gram model only.5 Experiments and DiscussionA statistical language model is evaluated byhow well it predicts some hitherto unseen text- test data - generated by the source to bemodeled.
A commonly used quality measurefor a given model M is related to the en-tropy of the underlying source and is knownas perplexity(PPL).
Given a word sequencew1, w2, .
.
.
, wN to be used as a test corpus, theperplexity of a language model M is given by:PPL =exp??
?1NN?q=1logP (M)(wq|Wq?1)??
(17)Perplexity also indicates the (geometric) aver-age branching factor of the language accordingto the modelM and thus indicates the difficultyof a speech recognition task(Jelinek, 1999).
Thelower the perplexity, the better the model; usu-ally a reduction in perplexity translates into areduction in word error rate of a speech recog-nition system.We have implemented both the LSA andSELSA models using the BLLIP corpus1 whichconsists of machine-parsed English new storiesfrom the Wall Street Journal (WSJ) for theyears 1987, 1988 and 1989.
We used the su-pertagger (Bangalore and Joshi, 1999) to su-pertag each word in the corpus.
This had a tag-ging acuracy of 92.2%.
The training corpus con-sisted of about 40 million words from the WSJ1987, 1988 and some portion of 1989.
This con-sists of about 87000 documents related to newsstories.
The test corpus was a section of WSJ1989 with around 300, 000 words.
The baselinetri-gram model had a perplexity of 103.12 andbi-gram had 161.06.
The vocabulary size forwords was 20106 and for supertags was 449.5.1 Perplexity ResultsIn the first experiment, we performed SELSAusing supertag information for each word.
Theword-supertag vocabulary was about 60000.This resulted in a matrix of about 60000X87000for which we performed SVD at various dimen-sions.
Similarly we trained LSA matrix and per-formed its SVD.
Then we used this knowledge tocalculate language model probability and then1Available from the Linguistic Data Consor-tium(LDC) www.ldc.upenn.eduintegrated with tri-gram probability using geo-metric interpolation method (Coccaro and Ju-rafsky, 1998).
In the process, we had assumedthe knowledge of the content/function wordtype for the next word being predicted.
Fur-thermore, in this experiment, we had used onlycontent words for LSA as well as SELSA repre-sentation, while the function words were treatedby tri-gram model only.
We also used the su-pertagged test corpus, thus we knew the su-pertag of the next word being predicted.
Theseresults thus sets benchmarks for content wordbased SELSA model.
With these assumptions,we obtained the perplexity values as shown inTable 1.SVD dimensions LSA+ SELSA+R tri-gram tri-gramtri-gram only 103.12 103.120 (uniform prob) 78.92 60.832 78.05 60.8810 74.92 57.8820 72.91 56.1550 69.85 52.80125 68.42 50.39200 67.79 49.50300 67.34 48.84Table 1: Perplexity at different SVD dimensionswith content/function word type knowledge as-sumed.
For SELSA, these are benchmarks withcorrect supertag knowledge.These benchmark results show that given theknowledge of the content or function word aswell as the supertag of the word being predicted,SELSA model performs far better than the LSAmodel.
This improvement in the performance isattributed to the finer level of syntactic infor-mation available now in the form of supertag.Thus given the supertag, the choice of the wordbecomes very limited and thus perplexity de-creases.
The decrease in perplexity across theSVD dimension shows that the SVD also playsan important role and thus for SELSA it is tru-ely a latent syntactic-semantic analysis.
Thus ifwe devise an algorithm to predict the supertagof the next word with a very high accuracy, thenthere is a gurantee of performance improvementby this model compared to LSA.Our next experiment, was based on no knowl-edge of content or function word type of thenext word.
Thus the LSA and SELSA matriceshad all the words in the vocabulary.
We alsokept the SVD dimensions for both SELSA andLSA to 125.
The results are shown in Table2.
In this case, we observe that LSA achievesthe perplexity of 88.20 compared to the base-line tri-gram 103.12.
However this is more thanLSA perplexity of 68.42 when the knowledge ofcontent/function words was assumed.
This rel-ative increase is mainly due to poor modelingof function words in the LSA-space.
Howeverfor SELSA, we can observe that its perplexityof 36.37 is less than 50.39 value in the case ofknowledge about content/function words.
Thisis again attributed to better modeling of syntac-tically regular function words in SELSA.
Thiscan be better understood from the observationthat there were 305 function words comparedto 19801 content words in the vocabulary span-ning 19.8 and 20.3 million words respectively inthe training corpus.
Apart from this, there were152, 145 and 147 supertags anchoring functionword only, content word only and both typesof words respectively.
Thus given a supertagbelonging to function word specific supertags,the ?vocabulary?
for the target word is reducedby orders of magnitude compared to the case forcontent word specific supertags.
It is also worthobserving that the 125-dimensional SVD case ofSELSA is better than the 0-dimensional SVDor uniform SELSA case.
Thus the SVD playsa role in deciphering the syntactic-semanticallyimportant dimensions of the information space.Model Perplexitytri-gram only 103.12LSA(125)+tri-gram 88.20SELSA(125)+tri-gram 36.37uniform-SELSA+tri-gram 41.79Table 2: Perplexity without content/functionword knowledge.
For SELSA, these are bench-marks with correct supertag knowledge.We also performed experiments using thephrase-type (NP, VP, others) knowledge andincorporated them within SELSA framework.The resultant model was also used to calcu-late perplexity values and the results on con-tent/function type assumption set comparesfavourably with LSA by improving the perfor-mance.
In another experiment we used the part-of-speech tag of the previous word (prevtag)within SELSA, but it couldn?t improve againstthe plain LSA.
These results shows that phraselevel information is somewhat useful if it can bepredicted correctly, but previous POS tags arenot useful.Model Perplexitytri-gram only 103.12LSA(125)+tri-gram 68.42phrase-SELSA(125)+tri-gram 64.78prevtag-SELSA(125)+tri-gram 69.12Table 3: Perplexity of phrase/prevtag basedSELSA with the knowledge of content/functionword type and the correct phrase/prevtagFinally the utility of this language model canbe tested in a speech recognition experiment.Here it can be most suitably applied in a second-pass rescoring framework where the output offirst-pass could be the N-best list of either jointword-tag sequences (Wang and Harper, 2002) orword sequences which are then passed througha syntax tagger.
Both these approaches allow adirect application of the results shown in aboveexperiments, however there is a possibility oferror propagation if some word is incorrectlytagged.
The other approach is to predict thetag left-to-right from the word-tag partial prefixfollowed by word prediction and then repeatingthe procedure for the next word.6 Conclusions and ResearchDirectionWe presented the effect of incorporating vari-ous levels of syntactic information in a statisti-cal language model that uses the mathematicalframework called syntactically enhanced LSA.SELSA is an attempt to develop a unified frame-work where syntactic and semantic dependen-cies can be jointly represented.
It general-izes the LSA framework by incorporating var-ious levels of the syntactic information alongwith the current word.
This provides a mech-anism for statistical language modeling wherethe probability of a word given the semanticsof the preceding words is constrained by theadjacent syntax.
The results on WSJ corpussets a set of benchmarks for the performanceimprovements possible with these types of syn-tactic information.
The supertag based infor-mation is very fine-grained and thus leads to alarge reduction in perplexity if correct supertagis known.
It is also observed that the knowledgeof the phrase type also helps to reduce the per-plexity compared to LSA.
Even the knowledgeof the content/function word type helps addi-tionally in each of the SELSA based languagemodels.
These benchmarks can be approachedwith better algorithms for predicting the nec-essary syntactic information.
Our experimentsare still continuing in this direction as well astoward better understanding of the overall sta-tistical language modeling problem with appli-cations to speech recognition.ReferencesS.
Bangalore and A. K. Joshi.
1999.
Supertag-ging:an approach to almost parsing.
Compu-tational Linguistics, 25(2):237?265.S.
Bangalore.
1996.
?almost parsing?
techniquefor language modeling.
In Proc.
Int.
Conf.Spoken Language Processing, Philadeplphia,PA, USA.J.
R. Bellegarda.
2000.
Exploiting la-tent semantic information in statistical lan-guage modeling.
Proceedings of the IEEE,88(8):1279?1296.E.
Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proc.
39th Annual Meet-ing of the Association for Computational Lin-guistics.C.
Chelba and F. Jelinek.
1998.
Exploiting syn-tactic structure for language modeling.
InProc.
COLING-ACL, volume 1, Montreal,Canada.N.
Coccaro and D. Jurafsky.
1998.
To-wards better integration of semantic predic-tors in statistical language modeling.
In Proc.ICSLP-98, volume 6, pages 2403?2406, Syd-ney.L.
Galescu and E. R. Ringger.
1999.
Aug-menting words with linguistic information forn-gram language models.
In Proc.
6th Eu-roSpeech, Budapest, Hungary.J.
T. Goodman.
2001.
A bit of progress in lan-guage modeling.
Microsoft Technical ReportMSR-TR-2001-72.F.
Jelinek.
1999.
Statistical methods for speechrecognition.
The MIT Press.T.
K. Landauer, P. W. Foltz, and D. Laham.1998.
Introduction to latent semantic analy-sis.
Discourse Processes, 25:259?284.C.
Manning and H. Schutze.
1999.
Foundationsof statistical natural language processing.
TheMIT Press.W.
Wang and M. P. Harper.
2002.
The super-ARV language model: Investigating the effec-tiveness of tightly integrating multiple knowl-edge sources.
In Proc.
Conf.
Empirical Meth-ods in Natural Language Processing, pages238?247, Philadelphia.
