Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87?96,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsHandling Noisy Queries In Cross Language FAQ RetrievalDanish Contractor Govind Kothari Tanveer A. FaruquieL.
Venkata Subramaniam Sumit NegiIBM Research IndiaVasant Kunj, Institutional AreaNew Delhi, India{dcontrac,govkotha,ftanveer,lvsubram,sumitneg}@in.ibm.comAbstractRecent times have seen a tremendous growthin mobile based data services that allow peo-ple to use Short Message Service (SMS) toaccess these data services.
In a multilin-gual society it is essential that data servicesthat were developed for a specific languagebe made accessible through other local lan-guages also.
In this paper, we present a ser-vice that allows a user to query a Frequently-Asked-Questions (FAQ) database built in a lo-cal language (Hindi) using Noisy SMS En-glish queries.
The inherent noise in the SMSqueries, along with the language mismatchmakes this a challenging problem.
We handlethese two problems by formulating the querysimilarity over FAQ questions as a combina-torial search problem where the search spaceconsists of combinations of dictionary varia-tions of the noisy query and its top-N transla-tions.
We demonstrate the effectiveness of ourapproach on a real-life dataset.1 IntroductionThere has been a tremendous growth in the numberof new mobile subscribers in the recent past.
Mostof these new subscribers are from developing coun-tries where mobile is the primary information de-vice.
Even for users familiar with computers and theinternet, the mobile provides unmatched portability.This has encouraged the proliferation of informa-tion services built around SMS technology.
Severalapplications, traditionally available on Internet, arenow being made available on mobile devices usingSMS.
Examples include SMS short code services.Short codes are numbers where a short message ina predesignated format can be sent to get specificinformation.
For example, to get the closing stockprice of a particular share, the user has to send amessage IBMSTOCKPR.
Other examples are search(Schusteritsch et al, 2005), access to Yellow Pageservices (Kopparapu et al, 2007), Email 1, Blog 2 ,FAQ retrieval 3 etc.
The SMS-based FAQ retrievalservices use human experts to answer SMS ques-tions.Recent studies have shown that instant messag-ing is emerging as the preferred mode of commu-nication after speech and email.4 Millions of usersof instant messaging (IM) services and short mes-sage service (SMS) generate electronic content in adialect that does not adhere to conventional gram-mar, punctuation and spelling standards.
Words areintentionally compressed by non-standard spellings,abbreviations and phonetic transliteration are used.Typical question answering systems are built for usewith languages which are free from such errors.
Itis difficult to build an automated question answer-ing system around SMS technology.
This is trueeven for questions whose answers are well docu-mented like in a Frequently-Asked-Questions (FAQ)database.
Unlike other automatic question answer-ing systems that focus on searching answers froma given text collection, Q&A archive (Xue et al,2008) or the Web (Jijkoun et al, 2005), in a FAQdatabase the questions and answers are already pro-1http://www.sms2email.com/2http://www.letmeparty.com/3http://www.chacha.com/4http://www.whyconverge.com/87Figure 1: Sample SMS queries with Hindi FAQsvided by an expert.
The main task is then to iden-tify the best matching question to retrieve the rel-evant answer (Sneiders, 1999) (Song et al, 2007).The high level of noise in SMS queries makes this adifficult problem (Kothari et al, 2009).
In a multi-lingual setting this problem is even more formidable.Natural language FAQ services built for users in onelanguage cannot be accessed in another language.In this paper we present a FAQ-based question an-swering system over a SMS interface that solves thisproblem for two languages.
We allow the FAQ to bein one language and the SMS query to be in another.Multi-lingual question answering and informationretrieval has been studied in the past (Sekine andGrishman, 2003)(Cimiano et al, 2009).
Such sys-tems resort to machine translation so that the searchcan be performed over a single language space.
Inthe two language setting, it involves building a ma-chine translation system engine and using it suchthat the question answering system built for a sin-gle language can be used.Typical statistical machine translation systemsuse large parallel corpora to learn the translationprobabilities (Brown et al, 2007).
Traditionallysuch corpora have consisted of news articles andother well written articles.
Since the translation sys-tems are not trained on SMS language they performvery poorly when translating noisy SMS language.Parallel corpora comprising noisy sentences in onelanguage and clean sentences in another languageare not available and it would be hard to build suchlarge parallel corpora to train a machine translationsystem.
There exists some work to remove noisefrom SMS (Choudhury et al, 2007) (Byun et al,2008) (Aw et al, 2006) (Neef et al, 2007) (Kobuset al, 2008).
However, all of these techniques re-quire an aligned corpus of SMS and conventionallanguage for training.
Such data is extremely hardto create.
Unsupervised techniques require hugeamounts of SMS data to learn mappings of non-standard words to their corresponding conventionalform (Acharyya et al, 2009).Removal of noise from SMS without the use ofparallel data has been studied but the methods usedare highly dependent on the language model and thedegree of noise present in the SMS (Contractor etal., 2010).
These systems are not very effective ifthe SMSes contain grammatical errors (or the sys-tem would require large amounts of training data inthe language model to be able to deal with all pos-sible types of noise) in addition to misspellings etc.Thus, the translation of a cleaned SMS, into a secondlanguage, will not be very accurate and it would notgive good results if such a translated SMS is used toquery an FAQ collection.Token based noise-correction techniques (such asthose using edit-distance, LCS etc) cannot be di-rectly applied to handle the noise present in the SMSquery.
These noise-correction methods return a listof candidate terms for a given noisy token (E.g.?gud?
?
> ?god?,?good?,?guide? )
.
Considering allthese candidate terms and their corresponding trans-lations drastically increase the search space for anymulti-lingual IR system.
Also , naively replacing thenoisy token in the SMS query with the top matchingcandidate term gives poor performance as shown byour experiments.
Our algorithm handles these andrelated issues in an efficient manner.In this paper we address the challenges arisingwhen building a cross language FAQ-based ques-tion answering system over an SMS interface.
Ourmethod handles noisy representation of questions ina source language to retrieve answers across targetlanguages.
The proposed method does not requirehand corrected data or an aligned corpus for explicitSMS normalization to mitigate the effects of noise.It also works well with grammatical noise.
To thebest of our knowledge we are the first to addressissues in noisy SMS based cross-language FAQ re-trieval.
We propose an efficient algorithm that canhandle noise in the form of lexical and semantic cor-ruptions in the source language.2 Problem formulationConsider an input SMS Se in a source languagee.
We view Se as a sequence of n tokens Se =s1, s2, .
.
.
, sn.
As explained in the introduction, theinput is bound to have misspellings and other lexicaland semantic distortions.
Also let Qh denote the set88of questions in the FAQ corpus of a target languageh.
Each question Qh ?
Qh is also viewed as a se-quence of tokens.
We want to find the question Q?hfrom the corpus Qh that best matches the SMS Se.The matching is assisted by a source dictionaryDe consisting of clean terms in e constructed froma general English dictionary and a domain dictio-nary of target language Dh built from all the termsappearing in Qh.
For a token si in the SMS in-put, term te in dictionary De and term th in dictio-nary Dh we define a cross-lingual similarity mea-sure ?
(th, te, si) that measures the extent to whichterm si matches th using the clean term te.
We con-sider th a cross lingual variant of si if for any te thecross language similarity measure ?
(th, te, si) > .We denote this as th ?
si.We define a weight function ?
(th, te, si) using thecross lingual similarity measure and the inverse doc-ument frequency (idf) of th in the target languageFAQ corpus.
We also define a scoring function to as-sign a score to each question in the corpusQh usingthe weight function.
Consider a question Qh ?
Qh.For each token si, the scoring function chooses theterm from Qh having the maximum weight usingpossible clean representations of si; then the weightof the n chosen terms are summed up to get thescore.
The score measures how closely the questionin FAQ matches the noisy SMS string Se using thecomposite weights of individual tokens.Score(Qh) =n?i=1maxth?Qh,te?De & th?si?
(th, te, si)Our goal is to efficiently find the question Q?h havingthe maximum score.3 Noise removal from queriesIn order to process the noisy SMS input we first haveto map noisy tokens in Se to the possible correct lex-ical representations.
We use a similarity measure tomap the noisy tokens to their clean lexical represen-tations.3.1 Similarity MeasureFor a term te ?
De and token si of the SMS inputSe, the similarity measure ?
(te, si) between them is?
(te, si) =??????????????
?LCSRatio(te,si)EditDistanceSMS(te,si)if te and si sharesame startingcharacter *0 otherwise(1)Where LCSRatio(te, si) =length(LCS(te,si))length(te)and LCS(te, si)is the Longest common subsequence between te and si.
* The intuition behind this measure is that people typically type thefirst few characters of a word in an SMS correctly.
This way we limitthe possible variants for a particular noisy tokenThe Longest Common Subsequence Ratio (LC-SRatio) (Melamed et al, 1999) of two strings is theratio of the length of their LCS and the length of thelonger string.
Since in the SMS scenario, the dictio-nary term will always be longer than the SMS token,the denominator of LCSRatio is taken as the lengthof the dictionary term.The EditDistanceSMS (Figure 2) compares theConsonant Skeletons (Prochasson et al, 2007) of thedictionary term and the SMS token.
If the Leven-shtein distance between consonant skeletons is smallthen ?
(te, si) will be high.
The intuition behind us-ing EditDistanceSMS can be explained throughan example.
Consider an SMS token ?gud?
whosemost likely correct form is ?good?.
The longestcommon subsequence for ?good?
and ?guided?
with?gud?
is ?gd?.
Hence the two dictionary terms?good?
and ?guided?
have the same LCSRatio of 0.5w.r.t ?gud?, but the EditDistanceSMS of ?good?is 1 which is less than that of ?guided?, which hasEditDistanceSMS of 2 w.r.t ?gud?.
As a result thesimilarity measure between ?gud?
and ?good?
willbe higher than that of ?gud?
and ?guided?.
Higherthe LCSRatio and lower the EditDistanceSMS ,higher will be the similarity measure.
Hence, fora given SMS token ?byk?, the similarity measure ofword ?bike?
is higher than that of ?break?.4 Cross lingual similarityOnce we have potential candidates which are thelikely disambiguated representations of the noisy89Procedure EditDistanceSMS(te, si)Beginreturn LevenshteinDistance(CS(si), CS(te)) + 1EndProcedure CS (t): // Consonant Skeleton GenerationBeginStep 1. remove consecutive repeated characters in t// (fall?
fal)Step 2. remove all vowels in t//(painting ?
pntng, threat?
thrt)return tEndFigure 2: EditDistanceSMSterm, we map these candidates to appropriate termsin the target language.
We use a statistical dictionaryto achieve this cross lingual mapping.4.1 Statistical DictionaryIn order to build a statistical dictionary we usethe statistical translation model proposed in (Brownet al, 2007).
Under IBM model 2 the transla-tion probability of source language sentence e?
={t1e, .
.
.
, tje, .
.
.
, tme } and a target language sentenceh?
= {t1h, .
.
.
, tih, .
.
.
, tle} is given byPr(h?|e?)
= ?(l|m)l?i=1m?j=0?
(tih|tje)a(j|i,m, l).
(2)Here the word translation model ?
(th|te) gives theprobability of translating the source term to targetterm and the alignment model a(j|i,m, l) gives theprobability of translating the source term at positioni to a target position j.
This model is learnt using analigned parallel corpus.Given a clean term tie in source language we getall the corresponding terms T = {t1h, .
.
.
, tkh, .
.
.
}from the target language such that word translationprobability ?
(tkh|tie) > ?.
We rank these terms ac-cording to the probability given by the word trans-lation model ?
(th|te) and consider only those tar-get terms that are part of domain dictionary i.e.tkh ?
Dh.4.2 Cross lingual similarity measureFor each term si in SMS input query, we find allthe clean terms te in source dictionary De for whichsimilarity measure ?
(te, si) > ?.
For each of theseterm te, we find the cross lingual similar terms Tteusing the word translation model.
We compute thecross lingual similarity measure between these termsas?
(si, te, th) = ?
(te, si).?
(th, te) (3)The measure selects those terms in target lan-guage that have high probability of being translatedfrom a noisy term through one or more valid cleanterms.4.3 Cross lingual similarity weightWe combine the idf and the cross lingual similaritymeasure to define the cross lingual weight function?
(th, te, si) as?
(th, te, si) = ?
(th, te, si).idf(th) (4)By using idf we give preference to terms that arehighly discriminative.
This is necessary becausequeries are distinguished from each other using in-formative words.
For example for a given noisytoken ?bck?
if a word translation model producesa translation output ?wapas?
(as in came back) or?peet?
or ?qamar?
(as in back pain) then idf willweigh ?peet?
more as it is relatively more discrim-inative compared to ?wapas?
which is used fre-quently.5 Pruning and matchingIn this section we describe our search algorithm andthe preprocessing needed to find the best questionQ?h for a given SMS query.5.1 IndexingOur algorithm operates at a token level and its corre-sponding cross lingual variants.
It is therefore nec-essary to be able to retrieve all questions Qhth thatcontain a given target language term th.
To do thisefficiently we index the questions in FAQ corpus us-ing Lucene5.
Each question in FAQ is treated as adocument.
It is tokenized using whitespace as de-limiter before indexing.5http://lucene.apache.org/java/docs/90The cross lingual similarity weight calculation re-quires the idf for a given term th.
We query on thisindex to determine the number of documents f thatcontain th.
The idf of each term in Dh is precom-puted and stored in a hashtable with th as the key.The cross lingual similarity measure calculation re-quires the word translation probability for a giventerm te.
For every te in dictionary De, we storeTte in a hashmap that contains a list of terms in thetarget language along with their statistically deter-mined translation probability ?
(th|te) > ?, whereth ?
Dh.Since the query and the FAQs use terms from dif-ferent languages, the computation of IDF becomes achallenge (Pirkola, 1998) (Oard et al, 2007).
Priorwork uses a bilingual dictionary for translations forcalculating the IDF.
We on the other hand rely ona statistical dictionary that has translation probabil-ities.
Applying the method suggested in the priorwork on a statistical dictionary leads to errors as thetranslations may themselves be inaccurate.We therefore calculate IDFs for target languageterm (translation) and use it in the weight measurecalculation.
The method suggested by Oard et al(Oard et al, 2007) is more useful in retrieval tasksfor multiple documents, while in our case we needto retrieve a specific document (FAQ).5.2 List CreationGiven an SMS input string Se, we tokenize it onwhite space and replace any occurrence of digits totheir string based form (e.g.
4get, 2day) to get a se-ries of n tokens s1, s2, .
.
.
, sn.
A list Lei is createdfor each token si using terms in the monolingual dic-tionary De.
The list for a single character SMS to-ken is set to null as it is most likely to be a stop word.A term te from De is included in Lei if it satisfies thethreshold condition?
(te, si) > ?
(5)The threshold value ?
is determined experimen-tally.
For every te ?
Lei we retrieve Tte and thenretrieve the idf scores for every th ?
Tte .
Using theword translation probabilities and the idf score wecompute the cross lingual similarity weight to createa new list Lhi .
A term th is included in the list onlyif?
(th|te) > 0.1 (6)This probability cut-off is used to prevent poorquality translations from being included in the list.If more than one term te has the same transla-tion th, then th can occur more than once in a givenlist.
If this happens, then we remove repetitive oc-currences of th and assign it a weight equal to themaximum weight amongst all occurrences in the list,multiplied by the number of times it occurs.
Theterms th in Lhi are sorted in decreasing order of theirsimilarity weights.
Henceforth, the term ?list?
im-plies a sorted list.For example given a SMS query ?hw mch ds it cstto stdy in india?
as shown in Fig.
3, for each tokenwe create a list of possible correct dictionary wordsby dictionary look up.
Thus for token ?cst?
we getdictionary words lik ?cost, cast, case, close?.
Foreach dictionary word we get a set of possible wordsin Hindi by looking at statistical translation table.Finally we merged the list obtained to get single listof Hindi words.
The final list is ranked according totheir similarity weights.5.3 Search algorithmGiven Se containing n tokens, we create n sortedlists Lh1 , Lh2 , .
.
.
, Lhn containing terms from the do-main dictionary and sorted according to their crosslingual weights as explained in the previous section.A naive approach would be to query the index usingeach term appearing in all Lhi to build a Collectionset C of questions.
The best matching question Q?hwill be contained in this collection.
We compute thescore of each question in C using Score(Q) and thequestion with highest score is treated as Q?h.
How-ever the naive approach suffers from high runtimecost.Inspired by the Threshold Algorithm (Fagin etal., 2001) we propose using a pruning algorithmthat maintains a much smaller candidate set C ofquestions that can potentially contain the maximumscoring question.
The algorithm is shown in Fig-ure 4.
The algorithm works in an iterative manner.In each iteration, it picks the term that has maxi-mum weight among all the terms appearing in thelists Lh1 , Lh2 , .
.
.
, Lhn.
As the lists are sorted in thedescending order of the weights, this amounts topicking the maximum weight term amongst the firstterms of the n lists.
The chosen term th is queried tofind the set Qth .
The set Qth is added to the candi-91Figure 3: List creationdate set C. For each question Q ?
Qth , we computeits score Score(Q) and keep it along with Q. Afterthis the chosen term th is removed from the list andthe next iteration is carried out.
We stop the iterativeprocess when a thresholding condition is met and fo-cus only on the questions in the candidate set C. Thethresholding condition guarantees that the candidateset C contains the maximum scoring question Q?h.Next we develop this thresholding condition.Let us consider the end of an iteration.
Sup-pose Q is a question not included in C. Atbest, Q will include the current top-most tokensLh1 [1], Lh2 [1], .
.
.
, Lhn[1] from every list.
Thus, theupper bound UB on the score of Q isScore(Q) ?n?i=0?
(Lhi [1]).Let Q?
be the question in C having the maximumscore.
Notice that if Q?
?
UB, then it is guaranteedthat any question not included in the candidate set Ccannot be the maximum scoring question.
Thus, thecondition ?Q?
?
UB?
serves as the termination cri-terion.
At the end of each iteration, we check if thetermination condition is satisfied and if so, we canstop the iterative process.
Then, we simply pick thequestion in C having the maximum score and returnit.Procedure Search AlgorithmInput: SMS S = s1, s2, .
.
.
, snOutput: Maximum scoring question Q?h.Begin?si, construct Lei for which ?
(si, te) > // Li lists variants of siConstruct lists Lh1 , Lh2 , .
.
.
, Lhn //(see Section 5.2).// Lhi lists cross lingual variants of si in decreasing//order of weight.Candidate list C = ?.repeatj?
= argmaxi?
(Lhi [1])t?h = Lhj?
[1]// t?h is the term having maximum weight among// all terms appearing in the n lists.Delete t?h from the list Lhj?
.Retrieve Qt?h using the index// Qt?h : the set of all questions in Qh//having the term t?hFor each Q ?
Qt?hCompute Score(Q) andadd Q with its score into CUB =?ni=1 ?
(Lhi [1])Q?
= argmaxQ?CScore(Q).if Score(Q?)
?
UB, then// Termination condition satisfiedOutput Q?
and exit.foreverEndFigure 4: Search Algorithm with Pruning6 ExperimentsTo evaluate our system we used noisy English SMSqueries to query a collection of 10, 000 Hindi FAQs.These FAQs were collected from websites of vari-ous government organizations and other online re-sources.
These FAQs are related to railway reser-vation, railway enquiry, passport application andhealth related issues.
For our experiments we asked6 human evaluators, proficient in both English andHindi, to create English SMS queries based on thegeneral topics that our FAQ collection dealt with.We found 60 SMS queries created by the evaluators,had answers in our FAQ collection and we desig-nated these as the in-domain queries.
To measurethe effectiveness of our system in handling out ofdomain queries we used a total of 380 SMSes part ofwhich were taken from the NUS corpus (How et al,92whch metro statn z nr pragati maidan ?dus metro goes frm airpot 2 new delhi rlway statn?is dere any special metro pas 4 delhi uni students?whn is d last train of delhi metro?whr r d auto stands N delhi?Figure 5: Sample SMS queries2005) and the rest from the ?out-of-domain?
queriescreated by the human evaluators.
Thus the total SMSquery data size was 440.
Fig 5 shows some of thesample queries.Our objective was to retrieve the correct HindiFAQ response given a noisy English SMS query.
Agiven English SMS query was matched against thelist of indexed FAQs and the best matching FAQ wasreturned by the Pruning Algorithm described in Sec-tion 5.
A score of 1 was assigned if the retrievedanswer was indeed the response to the posed SMSquery else we assigned a score of 0.
In case of outof domain queries a score of 1 was assigned if theoutput was NULL else we assigned a score of 0.6.1 Translation SystemWe used the Moses toolkit (Koehn et al, 2007) tobuild an English-Hindi statistical machine transla-tion system.
The system was trained on a collec-tion of 150, 000 English and Hindi parallel sentencessourced from a publishing house.
The 150, 000 sen-tences were on a varied range of subjects such asnews, literature, history etc.
Apart from this thetraining data also contained an aligned parallel cor-pus of English and Hindi FAQs.
The FAQs werecollected from government websites on topics suchas health, education, travel services etc.Since an MT system trained solely on a collectionof sentences would not be very accurate in translat-ing questions, we trained the system on an English-Hindi parallel question corpus.
As it was difficultto find a large collection of parallel text consistingof questions, we created a small collection of par-allel questions using 240 FAQs and multiplied themto create a parallel corpus of 50, 000 sentences.
Thisset was added to the training data and this helped fa-miliarize the language model and phrase tables usedby the MT systems to questions.
Thus in total theMT system was trained on a corpus of 200, 000 sen-tences.Experiment 1 and 2 form the baseline againstwhich we evaluated our system.
For our experi-ments the lexical translation probabilities generatedby Moses toolkit were used to build the word trans-lation model.
In Experiment 1 the threshold ?
de-scribed in Equation 5 is set to 1.
In Experiment 2and 3 this is set to 0.5.
The Hindi FAQ collectionwas indexed using Lucene and a domain dictionaryDh was created from the Hindi words in the FAQcollection.6.2 System EvaluationWe perform three sets of experiments to show howeach stage of the algorithm contributes in improvingthe overall results.6.2.1 Experiment 1For Experiment 1 the threshold ?
in Equation 5is set to 1 i.e.
we consider only those tokens in thequery which belong to the dictionary.
This setup il-lustrates the case when no noise handling is done.The results are reported in Figure 6.6.2.2 Experiment 2For Experiment 2 the noisy SMS query wascleaned using the following approach.
Given a noisytoken in the SMS query it?s similarity (Equation 1)with each word in the Dictionary is calculated.
Thenoisy token is replaced with the Dictionary wordwith the maximum similarity score.
This gives usa clean English query.For each token in the cleaned English SMS query,we create a list of possible Hindi translations of thetoken using the statistical translation table.
EachHindi word was assigned a weight according toEquation 4.
The Pruning algorithm in Section 5 wasthen applied to get the best matching FAQ.6.2.3 Experiment 3In this experiment, for each token in the noisy En-glish SMS we obtain a list of possible English vari-ations.
For each English variation a correspondingset of Hindi words from the statistical translation ta-ble was obtained.
Each Hindi word was assigneda weight according to Equation 4.
As described inSection 5.2, all Hindi words obtained from Englishvariations of a given SMS token are merged to create93Experiment 1 Experiment 2 Experiment 3MRR Score 0.41 0.68 0.83Table 1: MRR ScoresF1 ScoreExpt 1 (Baseline 1) 0.23Expt 2 (Baseline 2) 0.68Expt 3 (Proposed Method) 0.72Table 2: F1 Measurea list of Hindi words sorted in terms of their weight.The Pruning algorithm as described in Section 5 wasthen applied to get the best matching FAQ.We evaluated our system using two different cri-teria.
We used MRR (Mean reciprocal rank) andthe best matching accuracy.
Mean reciprocal rankis used to evaluate a system by producing a list ofpossible responses to a query, ordered by probabil-ity of correctness.
The reciprocal rank of a queryresponse is the multiplicative inverse of the rank ofthe first correct answer.
The mean reciprocal rankis the average of the reciprocal ranks of results for asample of queries Q.MRR = 1/|Q|Q?i=11/ranki (7)Best match accuracy can be considered as a spe-cial case of MRR where the size of the ranked list is1.
As the SMS based FAQ retrieval system will beused via mobile phones where screen size is a ma-jor constraint it is crucial to have the correct resulton the top.
Hence in our settings the best match ac-curacy is a more relevant and stricter performanceevaluation measure than MRR.Table 1 compares the MRR scores for all threeexperiments.
Our method reports the highest MRRof 0.83.
Figure 6 shows the performance using thestrict evaluation criterion of the top result returnedbeing correct.We also experimented with different values ofthe threshold for Score(Q) (Section 5.3).
The ROCcurve for various threshold is shown in Figure 7.
Theresult for both in-domain and out-of-domain queriesfor the three experiments are shown in Figure 6 forScore(Q) = 8.
The F1 Score for experiments 1, 2 and3 are shown in Table 2.Figure 6: Comparison of resultsFigure 7: ROC Curve for Score(Q)6.3 Measuring noise level in SMS queriesIn order to quantify the level of noise in the col-lected SMS data, we built a character-level languagemodel(LM) using the questions in the FAQ data-set(vocabulary size is 70) and computed the perplexityof the language model on the noisy and the cleanedSMS test-set.
The perplexity of the LM on a cor-pus gives an indication of the average number of bitsneeded per n-gram to encode the corpus.
Noise re-Cleaned SMS Noisy SMSEnglish FAQ collectionbigram 16.64 55.19trigram 9.75 69.41Table 3: Perplexity for Cleaned and Noisy SMS94sults in the introduction of many previously unseenn-grams in the corpus.
Higher number of bits areneeded to encode these improbable n-grams whichresults in increased perplexity.
From Table 3 we cansee the difference in perplexity for noisy and cleanSMS data for the English FAQ data-set.
Large per-plexity values for the SMS dataset indicates a highlevel of noise.For each noisy SMS query e.g.
?hw 2 prvnt ty-phd?
we manually created a clean SMS query ?howto prevent typhoid?.
A character level languagemodel using the questions in the clean English FAQdataset was created to quantify the level of noise inour SMS dataset.
We computed the perplexity of thelanguage model on clean and noisy SMS queries.7 ConclusionThere has been a tremendous increase in informationaccess services using SMS based interfaces.
How-ever, these services are limited to a single languageand fail to scale for multilingual QA needs.
Theability to query a FAQ database in a language otherthan the one for which it was developed is of greatpractical significance in multilingual societies.
Au-tomatic cross-lingual QA over SMS is challengingbecause of inherent noise in the query and the lackof cross language resources for noisy processing.
Inthis paper we present a cross-language FAQ retrievalsystem that handles the inherent noise in source lan-guage to retrieve FAQs in a target language.
Our sys-tem does not require an end-to-end machine transla-tion system and can be implemented using a sim-ple dictionary which can be static or constructedstatistically using a moderate sized parallel corpus.This side steps the problem of building full fledgedtranslation systems but still enabling the system tobe scaled across multiple languages quickly.
Wepresent an efficient algorithm to search and matchthe best question in the large FAQ corpus of tar-get language for a noisy input question.
We havedemonstrated the effectiveness of our approach on areal life FAQ corpus.ReferencesSreangsu Acharyya, Sumit Negi, L Venkata Subrama-niam, Shourya Roy.
2009.
Language independentunsupervised learning of short message service di-alect.
International Journal on Document Analysisand Recognition, pp.
175-184.Aiti Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for SMS text normaliza-tion.
In Proceedings of COLING-ACL, pp.
33-40.Peter F. Brown, Vincent J.Della Pietra, Stephen A. DellaPietra, Robert.
L. Mercer 1993.
The Mathematics ofStatistical Machine Translation: Parameter EstimationComputational Linguistics, pp.
263-311.Jeunghyun Byun, Seung-Wook Lee, Young-In Song,Hae-Chang Rim.
2008.
Two Phase Model for SMSText Messages Refinement.
AAAI Workshop on En-hanced Messaging.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, Anupam Basu.
2007.Investigation and modeling of the structure of textinglanguage.
International Journal on Document Analy-sis and Recognition, pp.
157-174.Philipp Cimiano, Antje Schultz, Sergej Sizov, PhilippSorg, Steffen Staab.
2009.
Explicit versus latent con-cept models for cross-language information retrieval.In Proceeding of IJCAI, pp.
1513-1518.Danish Contractor, Tanveer A. Faruquie, L. Venkata Sub-ramaniam.
2010.
Unsupervised cleansing of noisytext.
In Proceeding of COLING 2010: Posters, pp.189-196.R.
Fagin, A. Lotem, and M. Naor.
2001.
Optimal aggre-gation algorithms for middleware.
In Proceedings ofthe 20th ACM SIGMOD-SIGACT-SIGART symposiumon Principles of database systems, pp.
102-113.Yijue How and Min-Yen Kan. 2005.
Optimizing pre-dictive text entry for short message service on mobilephones.
In M. J. Smith and G. Salvendy (Eds.)
Proc.
ofHuman Computer Interfaces International,LawrenceErlbaum AssociatesValentin Jijkoun and Maarten de Rijke.
2005.
Retrievinganswers from frequently asked questions pages on theweb.
In Proceedings of the Tenth ACM Conference onInformation and Knowledge Management,CIKM, pp.76-83.Catherine Kobus, Francois Yvon and Grraldine Damnati.2008.
Normalizing SMS: Are two metaphors betterthan one?
In Proceedings of COLING, pp.
441-448.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, Evan Herbst 2007.
Moses:Open source toolkit for statistical machine translation.Annual Meeting of the Association for ComputationLinguistics (ACL), Demonstration Session .Sunil Kumar Kopparapu, Akhilesh Srivastava and ArunPande.
2007.
SMS based Natural Language Interface95to Yellow Pages Directory.
In Proceedings of the 4thinternational conference on mobile technology, appli-cations, and systems and the 1st international sympo-sium on Computer human interaction in mobile tech-nology, pp.
558-563 .Govind Kothari, Sumit Negi, Tanveer Faruquie, VenkatChakravarthy and L V Subramaniam 2009.
SMSbased Interface for FAQ Retrieval.
Annual Meetingof the Association for Computation Linguistics (ACL).I.
D. Melamed.
1999.
Bitext maps and alignment via pat-tern recognition.
Computational Linguistics, pp.
107-130.Guimier de Neef, Emilie, Arnaud Debeurme, andJungyeul Park.
2007.
TILT correcteur de SMS : Eval-uation et bilan quantitatif.
In Actes de TALN, pp.
123-132.Douglas W. Oard, Funda Ertunc.
2002.
Translation-Based Indexing for Cross-Language Retrieval In Pro-ceedings of the ECIR, pp.
324-333.A.
Pirkola 1998.
The Effects of Query Structureand Dictionary Setups in Dictionary-Based Cross-Language Information Retrieval SIGIR ?98: Proceed-ings of the 21st Annual International ACM SIGIR Con-ference on Research and Development in InformationRetrieval , pp.
55-63.E.
Prochasson, C. Viard-Gaudin, and E. Morin.
2007.Language models for handwritten short message ser-vices.
In Proceedings of the 9th International Confer-ence on Document Analysis and Recognition, pp.
83-87.Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.2005.
Mobile Search with Text Messages: Designingthe User Experience for Google SMS.
In Proceedingsof ACM SIGCHI, pp.
1777-1780.Satoshi Sekine, Ralph Grishman.
2003.
Hindi-Englishcross-lingual question-answering system.
ACM Trans-actions on Asian Language Information Processing,pp.
181-192.E.
Sneiders.
1999.
Automated FAQ Answering: Contin-ued Experience with Shallow Language Understand-ing Question Answering Systems.
Papers from the1999 AAAI Fall Symposium.
Technical Report FS-99-02, AAAI Press, pp.
97-107.W.
Song, M. Feng, N. Gu, and L. Wenyin.
2007.
Ques-tion similarity calculation for FAQ answering.
In Pro-ceeding of SKG 07, pp.
298-301.X.
Xue, J. Jeon, and W.B Croft.
2008.
Retrieval Modelsfor Question and Answer Archives.
In Proceedings ofSIGIR, pp.
475-482.96
