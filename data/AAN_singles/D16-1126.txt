Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1183?1191,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsGenerating Topical PoetryMarjan Ghazvininejad?, Xing Shi?, Yejin Choi?, and Kevin Knight?
?Information Sciences Institute & Computer Science DepartmentUniversity of Southern California{ghazvini,xingshi,knight}@isi.edu?Computer Science & Engineering, University of Washingtonyejin@cs.washington.eduAbstractWe describe Hafez, a program that gener-ates any number of distinct poems on a user-supplied topic.
Poems obey rhythmic andrhyme constraints.
We describe the poetry-generation algorithm, give experimental dataconcerning its parameters, and show its gener-ality with respect to language and poetic form.1 IntroductionAutomatic algorithms are starting to generate in-teresting, creative text, as evidenced by recent dis-tinguishability tests that ask whether a given story,poem, or song was written by a human or a com-puter.1 In this paper, we describe Hafez, a programthat generates any number of distinct poems on auser-supplied topic.
Figure 1 shows an overview ofthe system, which sets out these tasks:?
Vocabulary.
We select a specific, large vocabu-lary of words for use in our generator, and wecompute stress patterns for each word.?
Related words.
Given a user-supplied topic, wecompute a large set of related words.?
Rhyme words.
From the set of related words,we select pairs of rhyming words to end lines.?
Finite-state acceptor (FSA).
We build an FSAwith a path for every conceivable sequenceof vocabulary words that obeys formal rhythmconstraints, with chosen rhyme words in place.?
Path extraction.
We select a fluent path throughthe FSA, using a recurrent neural network(RNN) for scoring.1For example, in the 2016 Dartmouth test bit.ly/20WGLF3,no automatic sonnet-writing system passed indistinguishability,though ours was selected as the best of the submitted systems.Figure 1: Overview of Hafez converting a user-supplied topicword (wedding) into a four-line iambic pentameter stanza.1183Sections 3-7 describe how we address these tasks.After this, we show results of Hafez generating 14-line classical sonnets with rhyme scheme ABABCDCD EFEF GG, written in iambic pentameter (tensyllables per line with alternating stress: ?da-DUMda-DUM da-DUM .
.
.
?).
We then show experimentson Hafez?s parameters and conclude by showing thegenerality of the approach with respect to languageand poetic form.2 Prior WorkAutomated poem generation has been a popular butchallenging research topic (Manurung et al, 2000;Gervas, 2001; Diaz-Agudo et al, 2002; Manurung,2003; Wong and Chun, 2008; Jiang and Zhou, 2008;Netzer et al, 2009).
Recent work attempts to solvethis problem by applying grammatical and seman-tic templates (Oliveira, 2009; Oliveira, 2012), orby modeling the task as statistical machine trans-lation, in which each line is a ?translation?
of theprevious line (Zhou et al, 2009; He et al, 2012).Yan et al (2013) proposes a method based on sum-marization techniques for poem generation, retriev-ing candidate sentences from a large corpus of po-ems based on a user?s query and clustering the con-stituent terms, summarizing each cluster into a lineof a poem.
Greene et al (2010) use unsupervisedlearning to estimate the stress patterns of words ina poetry corpus, then use these in a finite-state net-work to generate short English love poems.Several deep learning methods have recently beenproposed for generating poems.
Zhang and Lapata(2014) use an RNN model to generate 4-line Chi-nese poems.
They force the decoder to rhyme thesecond and fourth lines, trusting the RNN to controlrhythm.
Yi et al (2016) also propose an attention-based bidirectional RNN model for generating 4-line Chinese poems.
The only such work which triesto generate longer poems is from Wang et al (2016),who use an attention-based LSTM model for gener-ation iambic poems.
They train on a small datasetand do not use an explicit system for constrainingrhythm and rhyme in the poem.Novel contributions of our work are:?
We combine finite-state machinery with deeplearning, guaranteeing formal correctness ofour poems, while gaining coherence of long-distance RNNs.?
By using words related to the user?s topic asrhyme words, we design a system that can gen-erate poems with topical coherence.
This al-lows us to generate longer topical poems.?
We extend our method to other poetry formatsand languages.3 VocabularyTo generate a line of iambic pentameter poetry, wearrange words to form a sequence of ten syllablesalternating between stressed and unstressed.
For ex-ample:010 1 0 10 101Attending on his golden pilgramageFollowing Ghazvininejad and Knight (2015), werefer to unstressed syllables with 0 and stressed syl-lables with 1, so that the form of a Shakespeareansonnet is ((01)5)14.
To get stress patterns for in-dividual words, we use CMU pronunciation dictio-nary,2 collapsing primary and secondary stresses.For example:CAFETERIA K AE2 F AH0 T IH1 R IY0 AH0becomesCAFETERIA 10100The first two columns of Table 1 show other ex-amples.
From the 125,074 CMU dictionary wordtypes, we can actually only use words whose stresspattern matches the iambic pattern (alternating 1sand 0s).
However, we make an exception for wordsthat end in ...100 (such as spatula).
To mimic howhuman poets employ such words, we convert all?...100?
patterns to ?...101?.
This leaves us with a106,019 word types.Words with multiple syllable-stress patternspresent a challenge.
For example, our programmay use the word record in a ?...10...?
context,but if it is a verb in that context, a human readerwill pronounce it as ?01?, breaking the intendedrhythm.
To guarantee that our poems scan properly,we eject all ambiguous words from our vocabulary.This problem is especially acute with monosyllabicwords, as most have a stress that depends on context.Greene et al (2010) apply the EM algorithm to align2http://www.speech.cs.cmu.edu/cgi-bin/cmudict1184word stress pattern strict rhyme class slant rhyme class (coarse version)needing 10 IY1 D IH0 NG IY1 * IH0 NGordinary 1010 EH1 R IY0 EH1 * IY0obligate 101 EY1 T last syllable stressed, no slant rhymeTable 1: Sample word analyses.human-written sonnets with assumed meter, extract-ing P(0|word) and P(1|word) probabilities.
Usingtheir method, we eject all monosyllabic words ex-cept those with P(0|word) > 0.9 or P(1|word) > 0.9.A consequence is that our poetry generator avoidsthe words to, it, in, and is, which actually forcesthe system into novel territory.
This yields 16,139monosyllabic and 87,282 multisyllabic words.Because our fluency module (Section 7) is re-stricted to 20,000 word types, we further pare downour vocabulary by removing words that are notfound in the 20k-most-frequent list derived from thesong lyrics corpus we use for fluency.
After this step,our final vocabulary contains 14,368 words (4833monosyllabic and 9535 multisyllabic).4 Topically Related Words and PhrasesAfter we receive a user-supplied topic, the first stepin our poem generation algorithm is to build a scoredlist of 1000 words/phrases that are related to thattopic.
For example:?
User-supplied input topic: colonel?
Output: colonel (1.00), lieutenant colonel(0.77), brigadier general (0.73), commander(0.67) ... army (0.55) ...This problem is different from finding synonymsor hypernyms in WordNet (Miller, 1995).
For exam-ple, while Banerjee and Pedersen (2003) use Word-Net to assign a 1.0 similarity score between car andautomobile, they only give a 0.3 similarity betweencar and gasoline.A second method is to use pointwise mutual in-formation (PMI).
Let t be the topic/phrase, and letw be a candidate related word.
We collect a set ofsentences S that contain t, and sort candidates byProportion of sentences in S containing wP(w) in general textTable 2 shows that PMI has a tendency to assign ahigh score to low frequency words (Bouma, 2009;Role and Nadif, 2011; Damani, 2013).A third method is word2vec (Mikolov et al,2013a), which provides distributed word represen-tations.
We train a continuous-bag-of-words model3with window size 8 and 40 and word vector dimen-sion 200.
We score candidate related words/phraseswith cosine to topic-word vector.
We find that alarger window size works best (Pennington et al,2014; Levy and Goldberg, 2014).Table 2 shows examples.
The training corpus forword2vec has a crucial effect on the quality of the re-lated words.
We train word2vec models on the En-glish Gigaword corpus,4 a song lyrics corpus, andthe first billion characters from Wikipedia.5 The Gi-gaword corpus produces related words that are toonewsy, while the song lyrics corpus does not coverenough topics.
Hence, we train on Wikipedia.
Toobtain related phrases as well as words, we apply themethod of Mikolov et al (2013b) to the Wikipediacorpus, which replaces collocations like Los Ange-les with single tokens like Los Angeles.
Word2vecthen builds vectors for phrases as well as words.When the user supplies a multi-word topic, we useits phrase vector if available.
Otherwise, we cre-ate the vector topic by element wise addition of itswords?
vectors.5 Choosing Rhyme WordsWe next fill in the right-hand edge of our poem byselecting pairs of rhyming words/phrases and as-signing them to lines.
In a Shakespearean sonnetwith rhyme scheme ABAB CDCD EFEF GG, thereare seven pairs of rhyme words to decide on.5.1 Strict RhymeThe strict definition of English rhyme is that thesounds of two words must match from the laststressed vowel onwards.
In a masculine rhyme,3https://code.google.com/archive/p/word2vec/4https://catalog.ldc.upenn.edu/LDC2011T075http://mattmahoney.net/dc/enwik9.zip1185Method Window Corpus Phrases?
Related wordsPMI n/a Gigaword no croquet, Romai, Carisbo, NTTF, showcourts ...CBOW 8 Gigaword no squash, badminton, golf, soccer, racquetball ...CBOW 40 Gigaword no singles, badminton, squash, ATP, WTA ...CBOW 40 Song Lyrics no high-heel, Reebok, steel-toed, basketball, Polos ...CBOW 40 Wikipedia no volleyball, racquet, Wimbledon, athletics, doubles ...CBOW 40 Wikipedia yes singles titles, grass courts, tennis club, hardcourt ...Table 2: Different methods for extracting words related to the topic tennis.the last syllable is stressed; in a feminine rhyme,the penultimate syllable is stressed.
We collectphoneme and stress information from the CMU pro-nunciation dictionary.
We pre-compute strict rhymeclasses for words (see Table 1) and hash the vocab-ulary into those classes.5.2 Slant RhymeIn practice, human poets do not always use strictrhymes.
To give ourselves more flexibility in choos-ing rhyme pairs, we allow for slant (or half) rhymes.By inspecting human rhyming data, we develop thisoperational definition of slant rhyme:1.
Let s1 and s2 be two potentially-rhymingphoneme sequences.2.
Replace ER with UH R in both sequences.3.
Let v1 and v2 be the last stressed vowels in s1and s2.4.
Let w1 and w2 be last vowels in s1 and s2.5.
Let s1 = a1 v1 x1 w1 c1.
Likewise, let s2 = a2v2 x2 w2 c2.6.
Output NO under any of these circumstances:(a) v1 6= v2, (b) w1 6= w2, (c) c1 6= c2, (d) a16= NULL and a2 6= NULL and a1 = a2.7.
If x1 and x2 are single phonemes:(a) If x1 ?
x2, then output YES.6(b) Otherwise, output NO.8.
If x1 and x2 contain different numbers of vow-els, output NO.9.
Let p1 and q1 be the first and last phonemes ofx1.
Let p2 and q2 be the same for x2.10.
If (p1 = p2) and (q1 ?
q2), output YES.11.
If (p1 ?
p2) and (q1 = q1), output YES.12.
Otherwise, output NO.6x ?
y if phonemes x and y are similar.
Two phonemes aresimilar if their pairwise score according to (Hirjee and Brown,2010) is greater than -0.6.
This includes 98 pairs, such as L/R,S/SH, and OY/UH.Words whose last syllable is stressed do not partici-pate in slant rhymes.Example slant rhymes taken from our gener-ated poems include Viking/fighting, snoopy/spooky,baby/crazy and comic/ironic.
We pre-compute acoarse version of slant rhyme classes (Table 1) withthe pattern ?vi * wi ci?.
If two words hash to thesame coarse class, then we subsequently accept orreject depending on the similarity of the intermedi-ate phonemes.5.3 Non-Topical Rhyming WordsFor rare topics, we may not have enough relatedwords to locate seven rhyming pairs.
For exam-ple, we generate 1000 related words for the topicViking, but only 32 of them are found in our 14,368-word vocabulary.
To give a chance for all topicalwords/phrases to be used as rhyme words, for eachstrict rhyme class, we add the most common word inour song lyric corpus to the list of related words.
Inaddition, we add words from popular rhyme pairs7(like do/you and go/know) to the list of related wordswith a low topic similarity score.5.4 Rhyme word selectionWe first hash all related words/phrases into rhymeclasses.
Each collision generates a candidate rhymepair (s1, s2), which we score with the maximumof cosine(s1, topic) and cosine(s2, topic).
So thatwe can generate many different sonnets on the sametopic, we choose rhyme pairs randomly with prob-ability proportional to their score.
After choosing apair (s1, s2), we remove it, along with any other can-didate pair that contains s1 or s2.
Because a poem?sbeginning and ending are more important, we assignthe first rhyme pair to the last two lines of the sonnet,7http://slate.me/OhTKCA1186Figure 2: An FSA compactly encoding all word sequences that obey formal sonnet constraints, and dictating the right-hand edgeof the poem via rhyming, topical words delight, chance, ... and joy.then assign other pairs from beginning of the sonnettowards the end.6 Constructing FSA of Possible PoemsAfter choosing rhyme words, we create a largefinite-state acceptor (FSA) that compactly encodesall word sequences that use these rhyme words andalso obey formal sonnet constraints:?
Each sonnet contains 14 lines.?
Lines are in iambic pentameter, with stress pat-tern (01)5.
Following poetic convention, wealso use (01)50, allowing feminine rhyming.?
Each line ends with the chosen rhymeword/phrase for that line.?
Each line is punctuated with comma or period,except for the 4th, 8th, 12th, and 14th lines,which are punctuated with period.To implement these constraints, we create FSAstates that record line number and syllable count.For example, FSA state L2-S3 (Figure 2) signifies?I am in line 2, and I have seen 3 syllables so far?.From each state, we create arcs for each feasibleword in the vocabulary.
For example, we can movefrom state L1-S1 to state L1-S3 by consuming anyword with stress pattern 10 (such as table or active).When moving between lines (e.g., from L1-S10 toL2-S1), we employ arcs labeled with punctuationmarks.To fix the rhyme words at the end of each line,we delete all arcs pointing to the line-final state, ex-cept for the arc labeled with the chosen rhyme word.For speed, we pre-compute the entire FSA; once wereceive the topic and choose rhyme words, we onlyneed to carry out the deletion step.In the resulting FSA, each path is formally a son-net.
However, most of the paths through the FSA aremeaningless.
One FSA generated from the topic nat-ural language contains 10229 paths, including thisrandomly-selected one:Of pocket solace ammunition grammar.An tile pretenders spreading logical.An stories Jackie gallon posing banner.An corpses Kato biological ...Hence, we need a way to search and rank this largespace.7 Path extraction through FSA with RNNTo locate fluent paths, we need a scoring functionand a search procedure.
For example, we can build an-gram word language model (LM)?itself a largeweighted FSA.
Then we can take a weighted in-tersection of our two FSAs and return the highest-scoring path.
While this can be done efficiently withdynamic programming, we find that n-gram modelshave a limited attention span, yielding poor poetry.Instead, we use an RNN language model (LM).We collect 94,882 English songs (32m word tokens)as our training corpus,8 and train9 a two-layer recur-rent network with long short-term memory (LSTM)units (Hochreiter and Schmidhuber, 1997).10When decoding with the LM, we employ a beam8http://www.mldb.org/9We use the toolkit: https://github.com/isi-nlp/Zoph RNN10We use a minibatch of 128, a hidden state size of 1000, anda dropout rate of 0.2.
The output vocabulary size is 20,000.
Thelearning rate is initially set as 0.7 and starts to decay by 0.83once the perplexity on a development set starts to increase.
Allparameters are initialized within range [?0.08,+0.08], and thegradients are re-scaled when the global norm is larger than 5.1187search that is further guided by the FSA.
Each beamstate Ct,i is a tuple of (h, s, word, score), where h isthe hidden states of LSTM at step t in ith state, ands is the FSA state at step t in ith state.
The modelgenerates one word at each step.At the beginning, h0,0 is the initial hidden stateof LSTM, s0,0 is the start state of FSA, word0,0 =<START> and score0,0 = 0.
To expand a beamstate Ct,i, we first feed ht,i and word into the LMand get an updated hidden state hnext.
The LMalso returns a probability distribution P (V ) overthe entire vocabulary V for next word.
Then, foreach succeeding state ssuc of st,i in the FSA andthe word wnext over each edge from st,i to ssuc,we form a new state (hnext, ssuc, wnext, scoret,i +log(P (wnext))) and push it into next beam.Because we fix the rhyme word at the end ofeach line, when we expand the beam states immedi-ately before the rhyme word, the FSA states in thosebeam states have only one succeeding state?LN-S10, where N = [1, 14], and only one succeedingword, the fixed rhyme word.
For our beam size b= 50, the chance is quite low that in those b wordsthere exists any suitable word to precede that rhymeword.
We solve this by generating the whole sonnetin reverse, starting from the final rhyme word.
Thus,when we expand the state L1-S8, we can choosefrom almost every word in vocabulary instead of justb possible words.
The price to pay is that at thebeginning of each line, we need to hope in those bwords there exists some that are suitable to succeedcomma or period.Because we train on song lyrics, our LM tends togenerate repeating words, like never ever ever everever.
To solve this problem, we apply a penalty tothose words that already generated in previous stepsduring the beam search.To create a poem that fits well with the pre-determined rhyme words at the end of each line, theLM model tends to choose ?safe?
words that are fre-quent and suitable for any topic, such as pronouns,adverbs, and articles.
During decoding, we apply areward on all topically related words (generated inSection 4) in the non-rhyming portion of the poem.Finally, to further encourage the system to followthe topic, we train an encoder-decoder sequence-to-sequence model (Sutskever et al, 2014).
For train-ing, we select song lyric rhyme words and assembleBipolar DisorderExistence enters your entire nation.A twisted mind reveals becoming manic,An endless modern ending medication,Another rotten soul becomes dynamic.Or under pressure on genetic tests.Surrounded by controlling my depression,And only human torture never rests,Or maybe you expect an easy lesson.Or something from the cancer heart disease,And I consider you a friend of mine.Without a little sign of judgement please,Deliver me across the borderline.An altered state of manic episodes,A journey through the long and winding roads.Figure 3: Sample sonnet generated from the topic phrase bipo-lar disorder.them in reverse order (encoder side), and we pair thiswith the entire reversed lyric (decoder side).
At gen-eration time, we put all the selected rhyme words onthe source side, and let the model to generate thepoem conditioned on those rhyme words.
In thisway, when the model tries to generate the last lineof the poem, it already knows all fourteen rhymewords, thus possessing better knowledge of the re-quested topic.
We refer to generating poems usingthe RNN LM as the ?generation model?
and to thismodel as the ?translation model?.8 Results and AnalysisSample outputs produced by our best system areshown in Figures 3 and 4.
We find that they gen-erally stay on topic and are fairly creative.
If we re-quest a poem on the topic Vietnam, we may see thephrase Honky Tonkin Resolution; a different topicleads the system to rhyme Dirty Harry with BloodyMary.
In this section, we present experiments weused to select among different versions of our poemgenerator.The first experiment tests the effect of encourag-ing topical words in the body of the poem, via adirect per-word bonus.
For 40 different topics, wegenerate 2 sonnets with and without encouragement,using the same set of rhyme words.
Then we ask23 human judges to choose the better sonnet.
Eachjudge compares sonnets for 10 different topics.
Ta-ble 3 shows that using topical words increases the1188Love at First SightAn early morning on a rainy night,Relax and make the other people happy,Or maybe get a little out of sight,And wander down the streets of Cincinnati.GirlfriendAnother party started getting heavy.And never had a little bit of Bobby,Or something going by the name of Eddie,And got a finger on the trigger sloppy.NoodlesThe people wanna drink spaghetti alla,And maybe eat a lot of other crackers,Or sit around and talk about the salsa,A little bit of nothing really matters.Civil WarCreating new entire revolution,An endless nation on eternal war,United as a peaceful resolution,Or not exist together any more.Figure 4: Sample stanzas generated from different topicphrases.Preference Encourages Does Not CannotEncourage DecideSonnets 54% 18% 28%Table 3: Users prefer the system that encourages the use ofrelated words in the body (non-rhyme) portion of the poem.
40poems are tested with 23 judges.quality of the sonnets.Next, we compare the translation model with gen-eration model.
For each of 40 topics, we gener-ate one poem with generation model and one poemwith translation model, using the same set of rhymewords.
We ask 25 human judges to chose the bet-ter poem.
Each judge compares sonnets for 10 dif-ferent topics.
This experiment is run separately forsonnets and stanzas.
Table 4 shows how the trans-lation model generates better poems, and Figure 5compares two stanzas.We check for plagiarism, as it is common foroptimal-searching RNNs to repeat large sections ofthe training data.
We hypothesize that strong condi-tions on rhyme, meter, repetition, and ambiguously-stressed words will all mitigate against plagiarism.Gen Another tiny thousand ashes scattered.And never hardly ever really own,Or many others have already gathered,The only human being left alone.Trans Being buried under ashes scattered,Many faces we forgotten own,About a hundred thousand soldiers gathered,And I remember standing all alone.Figure 5: Stanzas generated with and without a encoder-decoder translation model for topic death.Preference Generation Translation CannotModel Model DecideStanzas 26% 43% 31%Sonnets 21% 57% 22%Table 4: Users prefer poems created with the encoder-decodertranslation model over those that use only the RNN languagemodel in generation mode.
40 poems are tested with 25 judges.We find that on average, each sonnet copies only1.2 5-grams from the training data.
If we relaxthe repeated-word penalty and the iambic meter,this number increases to 7.9 and 10.6 copied 5-grams, respectively.
Considering the lack of copy-ing, we find the RNN-generated grammar to bequite good.
The most serious?and surprisinglycommon?grammatical error is the wrong use of aand an, which we fix in a post-processing step.9 Other Languages and FormatsTo show the generality of our approach, we mod-ify our system to generate Spanish-language poetryfrom a Spanish topic.
We use these resources:?
A song lyric corpus for training our RNN.We download 97,775 Spanish song lyrics fromLyricWikia,11 which amounts to 20m word to-kens and 219k word types.?
A Spanish Wikipedia dump12 consisting of885m word tokens, on which we run word2vecto find words and phrases related to the topic.Our vocabulary consists of the 20k most frequentlyric words.
For each word, we compute its syllable-stress pattern and its rhyme class (see Figure 6).
Be-cause Spanish writing is quite phonetic, we can re-trieve this from the letter strings of the vocabulary.11http://lyrics.wikia.com/wiki/Category:Language/Spanish12https://dumps.wikimedia.org/eswiki/20160305/eswiki-20160305-pages-meta-current.xml.bz21189word stress rhyme v- -vconsultado 0010 -ado yesaduciendo 0010 -endo yes yesre?gimen 100 -egimenhospital 001 -al yesFigure 6: Sample word analyses needed to construct SpanishHafez.
v- and -v indicate whether the word starts and/or endswith a vowel sound.For any given vocabulary word:131.
We remove silent h, and convert y into i.2.
We count the number of syllables by isolat-ing vowel groups.
In such groups, weak vow-els (i, u) attached to strong vowels (a, e, o) donot form separate syllables, unless they are ac-cented (d?
?-as versus dios).
Strong clusters arebroken into separate syllables (eg, ca-er).3.
We determine which vowel (and therefore syl-lable) is stressed.
If any vowel is accented, it isstressed.
If the word is accent-free, then thesecond-to-last syllable is stressed, unless theword ends in a consonant other than n or s, inwhich case the last syllable is stressed.4.
We form the word?s rhyme class by breakingoff a letter suffix starting at the last stressedvowel (as in English).
Weak vowels do not par-ticipate (e.g., tienda?
-enda, not -ienda).
Weremove h from the rhyme, so bu?ho rhymes withcontinu?o.
Because rhyming is easier in Spanishthan English, we do not need slant rhyme.Most Spanish poetic formats enforce some num-ber of syllables per line, without meter.
However,there are two caveats when counting syllables:1.
Sinalefa merges vowels across word bound-aries.
Thus, la obra is counted as two syllablesinstead of three, and va a hacer is counted astwo syllables instead of four.
A line may there-fore have more words than syllables.2.
For the last word of a line (only), we count upto its last stressed syllable, then add one.
Thismeans that even though iambic meter is not em-ployed, we still need stress patterns to correctlycount syllables.We implement these constraints in the FSAframework, now with separate states for ?I have seenM syllables, and the last word ended in a vowelsound?
and ?I have seen M syllables, and the last13http://community.dur.ac.uk/m.p.thompson/verse.htmMariposaQuieres saber do?nde esta?
el escorpio?n,Ni ayer ni antes vos sos corona dorada.Ya os ves ma?s tal cual tortuga pintada,A e?l nos gusta andar con cola marro?n.Ella es quie?n son las alas de algu?n gorrio?n.Si al fin pode?s ver tu imagen manchada,O hoy vas bajo un cielo azul plateada,Por que?
esta?s tan lejos del aguijo?n.No hay luz que al sol se enreda en tus palmera.Ay por que?
eres v?
?bora venenosa,Sin querer igual a un enredadera.Y si au?n suen?as con ser mariposa,En vez de abrir los ojos y espera,Sabes muy bien que el amor no es gran cosa.Figure 7: Sample Spanish poem generated in classical sonetaform, on the topic mariposa (butterfly).word ended in a consonant sound.?
Technicallyspeaking, the FSA includes single-state cycles forthe Spanish word a, due to sinalefa.
Line-endingstates can only be reached by words that have theirsyllable count adjusted as in point 2 above.Figure 7 shows a sample Spanish output.
The for-mat is the classical Spanish soneta, which consistsof 14 eleven-syllable lines under the rhyme schemeABBA ABBA CDC DCD.
This scheme requires usto choose up to four words with the same rhyme.Overall, we also find Spanish outputs to be flu-ent, fairly creative, and on topic.
Grammatical prob-lems are a bit more common than in our Englishgenerator?for example, adjacent words sometimesdisagree in number or gender.
The RNN generaliza-tions that permit these errors no doubt also permitcreative phrasings.10 ConclusionWe have described Hafez, a poetry generation sys-tem that combines hard format constraints with adeep-learning recurrent network.
The system usesspecial techniques, such as rhyme-word choice andencoder-decoder modeling, to keep the poem ontopic.
We hope that future work will provide morediscourse structure and function to automatic poetry,while maintaining the syntax, semantics, and cre-ative phrasing we observe.1190AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful comments.
This work was sup-ported by DARPA (W911NF-15-1-0543) and NSF(IIS-1524371).ReferencesSatanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proc.
IJCAI.Gerlof Bouma.
2009.
Normalized (pointwise) mutual in-formation in collocation extraction.
In Proc.
BiennialGSCL Conference.Om P. Damani.
2013.
Improving pointwise mutualinformation (PMI) by incorporating significant co-occurrence.
In Proc.
ACL.Belen Diaz-Agudo, Pablo Gervas, and Pedro Gonzalez-Calero.
2002.
Poetry generation in COLIBRI.
InProc.
ECCBR.Pablo Gervas.
2001.
An expert system for the composi-tion of formal Spanish poetry.
Knowledge-Based Sys-tems, 14(3).Marjan Ghazvininejad and Kevin Knight.
2015.
How tomemorize a random 60-bit string.
In Proc.
NAACL.Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010.Automatic analysis of rhythmic poetry with applica-tions to generation and translation.
In Proc.
EMNLP.Jing He, Ming Zhou, and Long Jiang.
2012.
Generat-ing Chinese classical poems with statistical machinetranslation models.
In Proc.
AAAI.Hussein Hirjee and Daniel Brown.
2010.
Using auto-mated rhyme detection to characterize rhyming stylein rap music.
In Empirical Musicology Review.Sepp Hochreiter and Jurgen Schmidhuber.
1997.
Longshort-term memory.
Neural Computation, 9(8).Long Jiang and Ming Zhou.
2008.
Generating Chinesecouplets using a statistical MT approach.
In Proc.COLING.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proc.
ACL.Hisar Manurung, Graeme Ritchie, and Henry Thompson.2000.
Towards a computational model of poetry gen-eration.
In Proc.
AISB Symposium on Creative andCultural Aspects and Applications of AI and CognitiveScience.Hisar Manurung.
2003.
An evolutionary algorithm ap-proach to poetry generation.
Ph.D. thesis, Universityof Edinburgh.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
In Proc.
NIPS.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,and Jeff Dean.
2013b.
Distributed representations ofwords and phrases and their compositionality.
In Proc.NIPS.George Miller.
1995.
WordNet: A lexical database forEnglish.
Communications of the ACM.Yael Netzer, David Gabay, Yoav Goldberg, and MichaelElhadad.
2009.
Gaiku: Generating haiku with wordassociations norms.
In Proc.
NAACL Workshop onComputational Approaches to Linguistic Creativity.Hugo Oliveira.
2009.
Automatic generation of poetry:an overview.
In Proc.
1st Seminar of Art, Music, Cre-ativity and Artificial Intelligence.Hugo Oliveira.
2012.
PoeTryMe: a versatile platform forpoetry generation.
Computational Creativity, ConceptInvention, and General Intelligence, 1.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for word rep-resentation.
In Proc.
EMNLP.Franois Role and Mohamed Nadif.
2011.
Handlingthe impact of low frequency events on co-occurrencebased measures of word similarity?a case study ofpointwise mutual information.
In Knowledge Discov-ery and Information Retrieval.I.
Sutskever, O. Vinyals, and Q. V. Le.
2014.
Sequenceto sequence learning with neural networks.
In Proc.NIPS.Qixin Wang, Tianyi Luo, Dong Wang, and Chao Xing.2016.
Chinese song iambics generation with neuralattention-based model.
arXiv:1604.06274.Martin Wong and Andy Chun.
2008.
Automatic haikugeneration using VSM.
In Proc.
ACACOS.Rui Yan, Han Jiang, Mirella Lapata, Shou-De Lin, Xue-qiang Lv, and Xiaoming Li.
2013.
I, Poet: AutomaticChinese poetry composition through a generative sum-marization framework under constrained optimization.In Proc.
IJCAI.Xiaoyuan Yi, Ruoyu Li, and Maosong Sun.
2016.
Gen-erating chinese classical poems with RNN encoder-decoder.
arXiv:1604.01537.Xingxing Zhang and Mirella Lapata.
2014.
Chinesepoetry generation with recurrent neural networks.
InProc.
EMNLP.Ming Zhou, Long Jiang, and Jing He.
2009.
Generat-ing Chinese couplets and quatrain using a statisticalapproach.
In Proc.
Pacific Asia Conference on Lan-guage, Information and Computation.1191
