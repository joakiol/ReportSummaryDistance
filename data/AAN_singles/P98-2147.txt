Dynamic compilation of weighted context-free grammarsMehryar  Mohr i  and Fernando C. N. Pere i raAT&T Labs - Research180 Park AvenueFlorham Park, NJ 07932, USA{mohri, pereira}@research, att.
comAbst rac tWeighted context-free grammars are a conve-nient formalism for representing rammaticalconstructions and their likelihoods in a varietyof language-processing applications.
In partic-ular, speech understanding applications requireappropriate grammars both to constrain speechrecognition and to help extract the meaningof utterances.
In many of those applications,the actual languages described are regular, butcontext-free r presentations are much more con-cise and easier to create.
We describe an effi-cient algorithm for compiling into weighted fi-nite automata n interesting class of weightedcontext-free grammars that represent regularlanguages.
The resulting automata can then becombined with other speech recognition compo-nents.
Our method allows the recognizer to dy-namically activate or deactivate grammar ulesand substitute a new regular language for someterminal symbols, depending on previously rec-ognized inputs, all without recompilation.
Wealso report experimental results showing thepracticality of the approach.1.
Mot ivat ionContext-free grammars (CFGs) are widely usedin language processing systems.
In many appli-cations, in particular in speech recognition, inaddition to recognizing rammatical sequencesit is necessary to provide some measure of theprobability of those sequences.
It is then natu-ral to use weighted CFGs, in which each rule isgiven a weight from an appropriate weight alge-bra (Salomaa nd Soittola, 1978).
Weights canencode probabilities, for instance by setting arule's weight to the negative logarithm of theprobability of the rule.
Rule probabilities canbe estimated in a variety of ways, which we willnot discuss further in this paper.Since speech recognizers cannot be fully cer-tain about the correct ranscription of a spokenutterance, they instead generate a range of al-ternative hypotheses with associated probabil-ities.
An essential function of the grammar isthen to rank those hypotheses according to theprobability that they would be actually uttered.The grammar is thus used together with otherinformation sources - pronunciation dictionary,phonemic ontext-dependency model, acousticmodel (Bahl et al, 1983; Rabiner and Juang,1993) - to generate an overall set of transcrip-tion hypotheses with corresponding probabili-ties.General CFGs are computationally too de-manding for real-time speech recognition sys-tems, since the amount of work required to ex-pand a recognition hypothesis in the way justdescribed would in general be unbounded foran unrestricted grammar.
Therefore, CFGsused in spoken-dialogue applications often rep-resent regular languages (Church, 1983; Brownand Buntschuh, 1994), either by construction oras a result of a finite-state approximation of ?more general CFG (Pereira and Wright, 1997).
1Assuming that the grammar can be efficientlyconverted into a finite automaton, appropriatetechniques can then be used to combine it withother finite-state recognition models for use inreal-time recognition (Mohri et al, 1998b).There is no general algorithm that would mapan arbitrary CFG generating a regular languageinto a corresponding finite-state automaton (UI-lian; 1967).
However, we will describe a use-ful class of grammars that can be so trans-formed, and a transformation algorithm thatavoids some of the potential for combinatorial1 Grammars representing regular languages have alsobeen used successfully in other areas of computationallinguistics (Karlsson et al, 1995).891explosion in the process.Spoken dialogue systems require grammarsor language models to change as the dialogueproceeds, because previous interactions set thecontext for interpreting new utterances.
For in-stance, a previous request for a date might ac-tivate the date grammar and lexicon and inac-tivate the location grammar and lexicon in anautomated reservations task.
Without such dy-namic grammars, efficiency and accuracy wouldbe compromised because many irrelevant wordsand constructions would be available when eval-uating recognition hypotheses.
We consider twodynamic grammar mechanisms: activation anddeactivation of grammar ules, and the substi-tution of a new regular language for a terminalsymbol when recognizing the next utterance.We describe a new algorithm for compil-ing weighted CFGs, based on representing thegrammar as a weighted transducer.
Thisrepresentation provides opportunities for op-timization, including optimizations involvingweights, which are not possible for generalCFGs.
The algorithm also supports dynamicgrammar changes without recompilation.
Fur-thermore, the algorithm can be executed on de-mand: states and transitions of the automa-ton are expanded only as needed for the recog-nition of the actual input utterances.
More-over, our lazy compilation algorithm is opti-mal in the sense that the construction requireswork linear in the size of the input grammar,which is the best one can expect given thatany algorithm needs to inspect the whole in-put grammar.
It is however possible to speed-up grammar compilation further by applyingpre-compilation optimizations to the grammar,as we will see later.
The class of grammarsto which our algorithm applies includes right-linear grammars, left-linear grammars and cer-tain combinations thereof.The algorithm has been fully implementedand evaluated experimentally, demonstratingits effectiveness.2.
A lgor i thmWe will start by giving a precise definition ofdynamic grammars.
We will then explain eachstage of grammar compilation.
Grammar com-pilation takes as input a weighted CFG repre-sented as a weighted transducer (Salomaa ndSoittola, 1978), which may have been opti-mized prior to compilation (preoptimized).
Theweighted transducer is analyzed by the com-pilation algorithm, and the analysis, if suc-cessful, outputs a collection of weighted au-tomata that are combined at runtime accordingto the current dynamic grammar configurationand the strings being recognized.
Since not allCFGs can be compiled into weighted automata,the compilation algorithm may reject an inputgrammar.
The class of allowed grammars willbe defined later.2.1.
Dynamic  grammarsThe following notation will be used in the restof the paper.
A weighted CFG G = (V,P)over the alphabet E, with real-number weightsconsists of a finite alphabet V of variables ornonterminals disjoint from ~, and a finite setP C V ?
R ?
(V U Z)* of productions or deriva-tion rules (Autebert et al, 1997).
Given stringsu, v E (V U ~)*, and real numbers c and c', wewrite (u, c) 2+ (v, c') when there is a derivationfrom u with weight c to v with weight c'.
Wedenote by La(X)  the weighted language gener-ated by a nonterminal X:LG(X) = {(w,c) E ~* x R :  (X, 0) -~ (w,c)}We can now define the two grammar-changingoperations that we use.Dynamic  act ivat ion or  deact ivat ion  o frules 2 We augment he grammar with a setof active nonterminals, which are those avail-able as start symbols for derivations.
More pre-cisely, let A C_ V be the set of active nonter-minals.
The language generated by G is thenLG = \[.JxEA LG(X).
Note that inactive nonter-minals, and the rules involving them, are avail-able for use in derivations; they are just notavailable as start symbols.
Dynamic rule acti-vation or deactivation is just the dynamic re-definition of the set A in successive uses of thegrammar.Dynamic  subst i tu t ion  Let a be a weightedrational transduction of ~* to A* x R, ~ C_ A,that is a regular weighted substitution (Berstel,1979).
a is a monoid morphism verifying:2This is the terminology used in this area, though amore appropriate xpression would be dynamic activa-tion or deactivation of nonterminal symbols.892Vx E ~, a(x) C Reg(A" ?
R)where Reg(A* x R) denotes the set ofweighted regular languages over the alphabetA.
Thus a simply substitutes for each symbola E ~ a weighted regular expression a(a).
Adynamic substitution consists of the applicationof the substitution a to ~, during the processof recognition of a word sequence.
Thus, aftersubstitution, the language generated by the newgrammar GI is: 3La, = a( Lc)Our algorithm allows for both of those dy-namic grammar changes without recompilingthe grammar.2.2.
Preprocess ingOur compilation algorithm operates on aweighted transducer v(G) encoding a factoredrepresentation f the weighted CFG G, whichis generated from G by a separate preproces-sor.
This preprocessor is not strictly needed,since we could use a version of the main algo-rithm that works directly on G. However, pre-processing can improve dramatically the timeand space needed for the main compilation step,since the preprocessor uses determinization a dminimization algorithms for weighted transduc-ers (Mohri, 1997) to increase the sharing - -  fac-to r ing -  among rammar rules that start or endthe same way.The preprocessing step builds a weightedtransducer in which each path corresponds to agrammar rule.
Rule X(~ -+ Y1 --.Y~ has a cor-responding path that maps X to the sequenceI/1 ...Y~ with weight ~.
For example, the smallCFG in Figure 1 is preprocessed into the com-pacted transducer shown in Figure 2.2.3.
Compilat ionThe compilation of weighted left-linear or right-linear grammars into weighted automata isstraightforward (Aho and Ullman, 1973).
Inthe right-linear case, for instance, the states ofthe automaton are the grammar nonterminalstogether with a new final state F. There is a3a can be extended as usual to map ~* ?
R toReg( A * ?
R ).Z .1 -~ XYX .2 -~ aYY .3 --+ bXY .4 -~c(i)Figure 1: Grammar G1.
:?10.1Figure 2: Weighted transducer r(G1).transition labeled with a E E and weight a E Rfrom X E V to Y E V iff the grammar con-tains the rule Xa --+ aY.
There is a transitionfrom X to F labeled with a and weight a iffXa --~ a is a rule of the grammar.
The initialstates are the states corresponding to the activenonterminals.
For example, Figure 3 shows theweighted automaton for grammar G2 consistingof the last three rules of G1 with start symbolX.However, the standard methods for left- andright-linear grammars cannot be used for gram-mars such as G1 that generate regular sets buthave rules that are neither left- nor right-linear.But we can use the methods for left- and right-linear grammars as subroutines if the grammarcan be decomposed into left-linear and right-linear components hat do not call each otherrecursively (Pereira and Wright, 1997).
Moreprecisely, define a dependency graph Dc forG's nonterminals and examine the set of itsstrongly-connected components (SCCs).
4 Thenodes of Da are G's nonterminals, and thereis a directed edge from X to Y if Y appearsin the right-hand side of a rule with left-handside X, that is, if the definition of X dependson Y.
Each SCC S of DG has a correspondingsubgrammar ofG consisting of those rules with4 Recall that  the strongly connected components of adirected graph are the equivalence classes of graph nodesunder the relation R defined by: q R q~ if q~ can bereached from q and q from q~.893Figure 3: Compilation of G2.Figure 4: Dependency graph DG1 for grammarG1.left-hand nonterminals in S, with nonterminalsnot in S treated as terminal symbols.
If each ofthese subgrammars is either left-linear or right-linear, we shall see that compilation into a singlefinite automaton is possible.The dependency graph DG can be obtainedeasily from the transducer (G).
For exam-ple, Figure 4 shows the dependency graph forour example grammar G1, with SCCs {Z} and(X, Y}.
It is clear that G1 satisfies our condi-tion, and Figure 5 shows the result of compilingG1 with A = (Z}.The SCCs of Da can be obtained in time lin-ear in the size of G (Aho et hi., 1974).
Be-fore starting the compilation, we check thateach subgrammar is left-linear or right-linear(as noted above, nonterminals not in the SCCof a subgrammar e treated as terminals).
Forexample, if (X1, X2} is an SCC, then the sub-grammarXt -'~ aYlbY2X1X1 --~ bY2aY1X2X2 -~ bbYlabX1(2)Figure 5: Compilation of G1 with start symbolZ.Figure 6: Weighted automaton K((X, Y}) cor-responding to the strongly connected compo-nent {X, Y} of G1.with X1,X2, Y1,Y2 E V and a,b E ~ is right-linear, since expressions such as aYlbY2 can betreated as elements of the terminal alphabet ofthe subgrammar.When the compilation condition holds, foreach SCC S we can build a weighted automa-ton K(S) representing the language of S's sub-grammar using the standard methods.
Sincesome nonterminals of G are treated as termi-nal symbols within a subgrammar, the transi-tions of an automaton K(S) may be labeledwith nonterminals not in S. 5 The nontermi-nals not in S can then be replaced by their cor-responding automata.
The replacement opera-tion is lazy, that is, the states and transitions ofthe replacing automata re only expanded whenneeded for a given input string.
Another inter-esting characteristic ofour algorithm is that theweighted automata K(S) can be made smallerby determinization and minimization, leadingto improvements in runtime performance.The automaton M(X) that represents thelanguage generated by nonterminal symbol Xcan be defined using K(S), where S is thestrongly connected component containing X,X E S. For instance, when the subgrammarof S is right-linear, M(X)is the automatonthat has the same states, transitions, and finalstates as K(S) and has the state correspond-ing to X as initial state.
For example, Figure6 shows K((X,Y)) for G1.
M(X) is then ob-tained from K((X,Y}) by taking X as initialstate.
The left-linear case can be treated in asimilar way.
Thus, M(X) can always be de-fined in constant time and space by editing theautomaton K(S).
We use a lazy implementa-tion of this editing operation for the definition5More precisely, they can only be part of otherstrongly connected components that come before S ina reverse topological sort of the components.
This guar-antees the convergence of the replacement of the nonter-minals by the corresponding automata.894xt0Figure 7: Automaton Ma with activated non-terminals: A = {X, Y, Z}.of the automata M(X) :  the states and transi-tions of M(X) are determined using K(S) onlywhen necessary for the given input string.
Thisallows us to save both space and time by avoid-ing a copy of K(S) for each X E S.Once the automaton representing the lan-guage generated by each nonterminal is cre-ated, we can define the language generated by Gby building an automaton Ma with one initialstate and one final state, and transitions labeledwith active nonterminals from the initial to thefinal state.
Figure 7 illustrates this in the casewhere A -- {X, Y, Z}.Given this construction, the dynamic activa-tion or deactivation f nonterminals can be doneby modifying the automaton MG.
This opera-tion does not require any recompilation, since itdoes not affect he automaton M(X) built foreach nonterminal X.All the steps in building the automata M(X)-- construction ofDG, finding the SCCs, andcomputing for K(S) for each SCC S - -  requirelinear time and space with respect to the sizeof G. In fact, since we first convert G intoa compact weighted transducer (G),  the to-tal work required is linear in the size of r(G).
6This leads to significant gains as shown by ourexperiments.In summary, the compilation algorithm hasthe following steps:1.
Build the dependency graph Da of thegrammar G.2.
Compute the SCCs of Da.
73.
For each SCC S, construct he automatonK(S).
For each X E S, build M(X) fromSApplying the algorithm to a compacted weightedtransducer r(G) involves various ubtleties that we omitfor simplicity.TWe order the SCCs in reverse topological order, butthis is not necessary for the correctness ofthe algorithm.K(X).
84.
Create a simple automaton MG acceptingexactly the set of active nonterminals A.5.
The automaton is then expanded on-the-flyfor each input string using lazy replacementand editing.The dynamic substitution of a terminal sym-bol a by a weighted automaton 9 aa is done byreplacing the symbol aby the automaton aa, us-ing the replacement operation discussed earlier.This replacement is also done on demand, withonly the necessary part of aa being expanded fora given input string.
In practice, the automatonaa can be large, a list of city or person ames forexample.
Thus a lazy implementation is crucialfor dynamic substitutions.3.
Optimizations, Exper iments andResultsWe have a full implementation f the compila-tion algorithm presented in the previous section,including the lazy representations that are cru-cial in reducing the space requirements of peechrecognition applications.
Our implementationof the compilation algorithm is part of a gen-era\] set of grammar tools, the GRM Library(Mohri, 1998b), currently used in speech pro-cessing projects at AT&T Labs.
The GRM Li-brary also includes an efficient compilation too\]for weighted context-dependent rewrite rules(Mohri and Sproat, 1996) that is used in text-to-speech projects at Lucent Bell Laboratories.Since the GRM library is compatible with theFSM general-purpose finite-state machine li-brary (Mohri et al, 1998a), we were able to usethe tools provided in FSM library to optimizethe input weighted transducers r(G) and theweighted automata in the compilation output.We did several experiments that show the ef-ficiency of our compilation method.
A key fea-ture of our grammar compilation method is therepresentation of the grammar by a weightedtransducer that can then be preoptimized usingweighted transducer determinization and mini-mization (Mohri, 1997; Mohri, 1998a).
To showSFor any X, this is a constant ime operation.
Forinstance, if K(S) is right-llnear, we just need to pick outthe state associated to X in K(X).9In fact, our implementation allows more generallydynamic substitutions by weighted transducers.895O// "//no op6mization ///- (Un~/10) /optimization i Ii II = //i /i /  / /i / =//7 / ?
~ "  .~:'~.~50 100 1~50 2()0 250VOCABULARY/"i I/ 'i Ino optimization // / - (size / 25) /optimization ,// '////// ///,=/ // // /  //VOCABULARYFigure 8: Advantage of transducer representation combined with preoptimization: time and space.the benefits of this representation, we comparedthe compilation time and the size of the re-sulting lazy automata with and without preop-timization.
The advantage of preoptimizationwould be even greater if the compilation outputwere fully expanded rather than on-demand.We did experiments with full bigram modelswith various vocabulary sizes, and with two un-weighted grammars derived by feature instanti-ation from hand-built feature-based grammars(Pereira and Wright, 1997).
Figure 8 showsthe compilation times of full bigram modelswith and without preoptimization, demonstrat-ing the importance of the optimization allowedby using a transducer epresentation of thegrammar.
For a 250-word vocabulary model,the compilation time is about 50 times fasterwith the preoptimized representation.
1?
Figure8 also shows the sizes of the resulting lazy au-tomata in the two cases.
While in the preop-timized case time and s_~ace grow linearly withvocabulary size (O(x/IGI)), they grow quadrat-ically in the unoptimized case (O(\[G\[)).The bigram examples also show the advan-tages of lazy replacement and editing over thefull expansion used in previous work (Pereiraand Wright, 1997).
Indeed, the size of thefully-expanded automaton for the preoptimized1?For convenience, the compilation time for the unop-timized case in Figure 8 was divided by 10, and the sizeof the result by 25.Table 1: Feature-based grammars.I \[GI I optim, time expanded (s) states \[expanded transitions \[14 1\]no 04  14'?
i431 yes .02 1535 2002i1 0 ,i 0,0  ,40141 i 12657 yes 2.02 112795 144083case grows quadratically with the vocabularysize (O(IGI)), while it grows with the cube ofthe vocabulary size in the unoptimized case(0(IGt3/2)).
For example, compilation is about700 times faster in the optimized case for a fullyexpanded automaton even for a 40-word vo-cabulary model, and the result about 39 timessmaller.Our experiments with a small and a medium-sized CFG obtained from feature-based gram-mars confirm these observations (Table 1).If dynamic grammars and lazy expansion arenot needed, we can expand the result fully andthen apply weighted eterminization and min-imization algorithms.
Additional experimentsshow that this can yield dramatic reductions inautomata size.4.
ConclusionA new weighted CFG compilation algorithm hasbeen presented.
It can be used to compile effi-896ciently an interesting class of grammars repre-senting weighted regular languages and allowsfor dynamic modifications that are crucial inmany speech recognition applications.While we focused here on CFGs with realnumber weights, which are especially relevant inspeech recognition, weighted CFGs can be de-fined more generally over an arbitrary semiring(Salomaa nd Soittola, 1978).
Our compilationalgorithm applies to general semirings withoutchange.
Both the grammar compilation algo-rithms (GRM library) and our automata opti-mization tools (FSM library) work in the mostgeneral case.AcknowledgementsWe thank Bruce Buntschuh and Ted Roycraftfor their help with defining the dynamic gram-mar features and for their comments on thiswork.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1973.The Theory of Parsing, Translation andCompiling.
Prentice-Hall.Alfred V. Aho, John E. Hopcroft, and Jeffrey D.Ullman.
1974.
The design and analysis ofcomputer algorithms.
Addison Wesley: Read-ing, MA.Jean-Michel Autebert, Jean Berstel, and LucBoasson.
1997.
Context-free languages andpushdown automata.
In Grzegorz Rozenbergand Arto Salomaa, editors, Handbook of For-mal Languages, volume 1, pages 111-172.Springer.Lalit R. Bahl, Fred Jelinek, and Robert Mercer.1983.
A maximum likelihood approach tocontinuous peech recognition.
IEEE Trans-actions on Pattern Analysis and Machine In-telligence (PAUl), 5(2):179-190.Jean Berstel.
1979.
Transductions and Context-Free Languages.
Teubner Studienbucher:Stuttgart.Michael K. Brown and Bruce M. Buntschuh.1994.
A context-free grammar compiler forspeech understanding systems.
In Proceed-ings of the International Conference on Spo-ken Language Processing (ICSLP '94), pages21-24, Yokohama, Japan.Kenneth W. Church.
1983.
A finite-state parserfor use in speech recognition.
In 21 st Meet-ing of the Association for Computational Lin-guistics (ACL '88), Proceedings of the Con-ference.
ACL.Fred Karlsson, Atro Voutilalnen, Juha Heikkila,and Atro Anttila.
1995.
Constraint Gram-mar, A language-Independent System forParsing Unrestricted Tezt.
Mouton deGruyter.Mehryar Mohri and Richard Sproat.
1996.
Anefficient compiler for weighted rewrite rules.In 34th Meeting of the Association for Com-putational Linguistics (ACL 96), Proceedingsof the Conference, Santa Cruz, California.ACL.Mehryar Mohri, Fernando C. N. Pereira, andMichael Riley.
1998a.
A rational design for aweighted finite-state transducer library.
Lec-tune Notes in Computer Science, to appear.Mehryar Mohri, Michael Riley, Don Hindle,Andrej Ljolje, and Fernando C. N. Pereira.1998b.
Full expansion of context-dependentnetworks in large vocabulary speech recogni-tion.
In Proceedings of the International Con-ference on Acoustics, Speech, and Signal Pro-cessing (ICASSP '98), Seattle, Washington.Mehryar Mohri.
1997.
Finite-state transducersin language and speech processing.
Compu-tational Linguistics, 23:2.Mehryar Mohri.
1998a.
Minimization algo-rithms for sequential transducers.
TheoreticalComputer Science, to appear.Mehryar Mohri.
1998b.
Weighted GrammarTools: the GRM Library.
In preparation.Fernando C. N. Pereira and Rebecca N. Wright.1997.
Finite-state approximation of phrase-structure grammars.
In Emmanuel Rocheand Yves Schabes, editors, Finite-State Lan-guage Processing, pages 149-173.
MIT Press,Cambridge, Massachusetts.Lawrence Rabiner and Biing-Hwang Juang.1993.
Fundamentals of Speech Recognition.Prentice-Hall, Englewood Cliffs, NJ.Arto Salomaa and Matti Soittola.
1978.Automata-Theoretic Aspects of Formal PowerSeries.
Springer-Verlag: New York.Joseph S. Ullian.
1967.
Partial algorithm prob-lems for context free languages.
Informationand Control, 11:90-101.897
