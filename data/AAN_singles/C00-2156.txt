Dec is ion -Tree  based Error  Cor rec t ion  for Stat is t ica l  Phrase  BreakPred ic t ion  in Korean  *Byeongchang K im and Geunbae LeeDel)artment of Computer  Science & EngineeringPohang University of Science & TechnologyPohang, 790-784, South Korea{bckim, gblee}((~postech.ac.krAbst racttn this paper, we present a new 1)hrase breakprediction architecture that integrates proba-bilistic apt)roach with decision-tree based errorcorrection.
The probabilistic method alone usu-ally sufl'crs fronl performance degradation dueto inherent data sparseness l)rolflems and it onlycovers a limited range of contextual informa-tion.
Moreover, the module can not utilize theselective morpheme tag and relative distanceto the other phrase breaks.
The decision-treebased error correction was tightly integrated toovert:ohm these limitations.The initially phrase break tagged morphcnm se-quence is corrected with the error correcting de-cision tree which was induced by C4.5 fl'om thecorrectly tagged corpus with the outtmt of the15mbabilistic predictor.
The decision tree-basedpost error correction l)rovided improved resultseven with the phrase break predictor that hasl)oor initial performance.
Moreover, tim systemcan be flexibly tamed to new corI)uS withoutmassive retraining.1 I n t roduct ionDuring 1;15(; past thw years, there has l)een agreat deal of interest in high quality text-to-speech (TTS) systelns (van Santen et al, 1997).One of the essential prolflenlS ill developing highquality TTS systems is to predict phrase breaksflora texts.
Phrase breaks are especially es-sential fbr subsequent processing in the TTSsystems uch as grapheme-to-iflloneme conver-sion and prosodic feature generation.
More-over, gral)helnes in the phrase,-break bound-aries are not phonologically changed and shouldbe i)ronommed as their original correspondingp honenles.There have been two apln'oaches to predictphrase breaks (Taylor and Black, 1998).
The* This paper was supported by the University ResearchProgram of the Ministry of Intbrmation & Communica-tion in South Korea through the IITA(1998.7-2000.6).first: uses some sort of syntactic information toIn:edict prosodic boundaries based on the factthat syntactic structure and prosodic structureare co-related.
This method needs a reliableparser and syntax-to-prosody 1nodule.
Thesemodules are usnally implemented in rule-drivenmethods, consequently, they are difficult towrite, modi(y, maintain and adapt to new do-mains and languages.
Ill addition, a greater useof syntactic information will require, more con>lmtation for finding n more detailed syntacticparse.
Considering these shortcomings, the sec-ond approach uses some probabilistic methodson the crude POS sequence of the text:, and thislnethod will be fln:ther developed in this paper.However, t:he.
probabilistic method alone usu-ally sufl'ers front pertbrmance degradation dueto inherent data sparseness problems.So we adopted decision tree-based error COl'-re, ction to overconm these training data limi-tations.
Decision tree induction iv 1;t5(; mostwidely used \]calming reel;hod.
Espcci~flly in lla~l;-m:al language and speech processing, decisiontree learning has been apt)lied to many prob-h,.nls including stress acquisition fl'om texts,gralflmme to phonenm conversion and prosodicphrase, modeling (Daelemans et al, 1994) (vanSanten et al, 1997) (Lee and Oh, 1999).In the next section, linguistic fb, atures of Ko-rean relevant o phrase break prediction are de-scribed.
Section 3 presents the probabilisticphrase break prediction method and the tree-based error correction method.
Section 4 showsexperimental results to demonstrate he t>erfor -mam:e of the method and section 5 draws st)meconclusions.2 Features  o f  KoreanThis section brMly explains the linguistic char-acterists of spoken Korean before describing thephrase break prediction.1) A Korean word consists of more thanone morpheme with clear-cut morphenm bound-aries (Korean is all agglutinative language).10512) Korean is a postpositional language withmany kinds of noun-endings, verb-endings, andprefinal verb-endings.
These functional mor-phemes determine a noun's case roles, a verb'stenses, modals, and modification relations be-twcen words.
3) Korean is basically an SOVlanguage but has relatively free word ordercompared to other rigid word-order languagessuch as English ,except br the constraints thatthe verb must appear in a sentence-final posi-tion.
However, in Korean, some word-order con-straints actually do exist such that the auxiliaryverbs representing modalities must follow themain verb, and modifiers must be placed betbrethe word (called head) they modify.
4) Phono-logical changes can occur in a morpheme, be-tween morphemes in a word, and even betweenwords in a phrase, but not between phrases.3 Hybr id  Phrase  Break  Detect ionPart-of speech (POS) tagging is a basic stepto phrase break prediction.
POS tagging sys-tems have to handle out-of vocabulary (OOV)words for an unlimited vocabulary TTS sys-tem.
Figure 1 shows the architecture of ourphrase break predictor integrated with the POStagging system.
The POS tagging system era-ploys generalized OOV word handling mecha-nisms in the morphological analysis and cas-cades statistical and rule-based approaches inthe two-phase training architecture tbr POS dis-ambiguation.Morphological !
;iI analyzc r~{M~pl me ..... ~se~i~ ':~, :  .
/ Protmbilistlc I rtgram ~z'~*t  $IFigure 1: Architecture of the hybrid phrasebreak prediction.Tire probabilistic phrase break predictor seg-ments the POS sequences into several phrasesaccording to word trigram probabilities.
Tireirdtial phrase break tagged morpheme sequenceis corrected with the error correcting treelearned by the C4.5 (Quinlan, 1983).Tire next two subsections will give detaileddescriptions of the probabilistic phrase predic-tion and error correcting tree learning.
The hy-brid POS tagging system will not l)e explainedin this paper, and the interested readers can see(Cha et al, 1998) tbr further reference.3.1 Probabi l i s t ie  Phrase BreakDetect ion3.1.1 Probabi l l s t ie  ModelsFor phrase break prediction, we develop tireword POS tag trigrmn model.
Some experi-ments are performed on all the possible trigramsequences and 'word-tag word-tag break word-tag' sequence turns out to be the most fl'uitfulof any others, which are the same results as theprevious tudies in English (Sanders, 1995).The probability of a phrase break bi appear-ing after the second word POS tag is given byP(bilt?,2ta) = C(tlt2bit3)Ej=o,~,2 C(ht2b j t3)  'where C is a frequency count flmction and b0,bl and b2 mean no break, minor break and ma-jor break, respectively.
Even with a large num-ber of training patterns it is very clear thatthere will be a number of word POS tag se-quences that never occur or occur only once inthe training corpus.
One solution to this datasparseness problem is to smooth the probabili-ties by using the bigram and unigram probabil-ities, which adjusts the fl'equency counts of rareor non-occurring POS tag sequences.
We usethe smoothed probabilities:P(biltlt,2ta) = )q C(trt2bit3)~j=o,l,2 C(t~ t2bjt3)C(t2bita)q- A2 ~j=0,1,2 C(t2bjt3)+ C(t2bi))t3 j=_0,1,2 C( 2bj) 'where )~1, A2 and ),a are three nommgative con-stants such that h I q- ~2 Jr- ~3 = 1.
In someexperiments, we can get the weights ~1, ~2 andA3 as 0.2, 0.7 and 0.1, respectively.3.1.2 Adjust ing the POS TagSequences of  WordsPrevious researchers of phrase break predic-tion used mainly content-flnmtion word rule,wherel)y a phrase break is placed before everyflmction word that follows a content word (Allenand Hmmicut, 1987) (Taylor et al, 1991).
The1052researchers used tag set size of only 3, includingfunction, content ~md t)lmctuation i  the rule.However, Korean is a post-positional aggln-tinative language.
If the eontent-t'unction wordrule is to be adapted in Korean, the rule nmstbe changed so that a phrase break is placedbefore every content mort/henm that R)llows afimction morl)heme.
Unfortunately this ruleis very inet\[icient in Korean since it tends tocreate too many pauses.
In our works, onlythe POS tags of Nnction mort)heroes are usedbe, cause the function morphelnes constrain theclasses of precedent n:orpheanes and t)b\y impor-tant roles in syntactic relation.
So, each wordis represented by the I?OS tag of its fimctionmorpheme.
In the case of the word which hasno function mort)heine , simplified POS tags ofcontent mort)henms are used.
The nmnber ofPOS tags use, d in this rese, m'ch is a2.3.2 Dec is ion -Tree  Based  ErrorCorrect ionThe t)robabilistic phrase break prediction onlycovers a limited range of contextual infornm-tion, i.e.
two preceding words and one.
follow-ing word.
Moreove, r the module can not utilizethe morl)heme tag se.lectively and relative dis-tance to the other phrase breaks.
For this rea-son we designed error correcting tree to con>pensate for |;tie limitations of the.
probal)ilisticphrase break prediction.
However, designing er-ror corre, cting rules with knowledge ngineeringis te, dious and error-prone,, lTnstead, we, adopte, ddecision tree learning ai)proa(:h to auton~aticallylearn the error correcting rules froln a correctlyt:,hrase break tagged eorlms.Most algorithms th:~t have t)een develope, dfor lmilding decision trees employ a top-down,greedy search through the space of possible deci-sion trees (Mitchell, 1997).
The 04.5 (Quinlan,1983) is adequate to buiht a decision tree easilyfor successively dividing the regions of featurevector to minimize the prediction error.
It alsouses intbrmation gain which lneasures how wella given attril)ute separates the training vectorsaccording to their target classification in orderto select he most critical attrilmtes at each stepwhile growing |;tit tree (hence the nmne is IG-~l'ree).
Now, we utilize it for correcting the ini-tially phrase break tagged POS tag sequencesgenerated by probabilistic predictor.However, wc invented novel way of using thedecision tree as trmlsibrmation-t)ased rule in-duction (Brill, 1992).
l?igure, 2 shows the treelearning architecture tbr phrase break error cor-rection.
The initial phrase break tagged POStag sequences upport the ti;ature vectors tbrattributes which are used tbr decision mak-ing.
Because the ii;atm'e vectors include phrasebreak sequences as well as POS tag sequences,a learned decision tree can check |;lie morphenmtag selectively and utilize |;lie relative distaneeto the other phrase breaks.
The correctly phrasebreak tagged POS tag sequences upport theclasses into which the feature vectors are classi-fied.
C4.5 lmilds a decision tree fl'om the t)airswhich consist of the feature vectors and theirclasses.. .
.
.
I Prol,al,ilislic 11l'I,rase l,reak ta~gedPOS tag seque nee.
.
.
.
.
Correctly plmlse break tagged~-~\ ] : :  ........ ...... POS tag sequenceErrOr Correcting decision treeFigure 2: Architecture of the error correctingdecision tree learner.4 Exper imenta l  Resu l ts4.1 CorpusThe, experiments are t)ertbrmed on a Koreannews story database,, called MBCNF, WS\])I~, ofspoken Korean directly recorded f rom broad-casting news.
The size of th(; database is now6,111 sentences (75,647 words) and it is eontin-nously growing.
'12) lm used in the phrase breakprediction experiments, |;tie database has beenPOS tagged and break-b~beled with major andminor phrase breaks.4.2 Phrase  Break  Detect ion  and ErrorCorrect ionWe, I)eribrmed three experiments o show syner-gistic results of probabilistic method and tree-based error correction method.
First, only prob-abilistic method was used to predict phrasebreaks.
%'igrams, bigrams and unigrams forphrase break prediction were trained fl:om thebreak-labeled an(1 POS tagged 5,492 sentencesof the MBCNEWSDB by adjusting the POSsequences of words as described in sut)section3.1.2.
The other 619 sentences are used totest the t)ertbrnum(:e of the probabilistic I)hrasebreak predictor.
In the second experiment, wemade a decision tree, which can be used onlyto predict phrase breaks and cannot be used to1053correct phrase breaks, from the 5,429 sentences.Also the 619 sentences were used to test theperformance of the decision tree-based phrasebreak predictor.
The size of feature vector (thesize of the window) is w~ried fi'om 7 (the POStag of current word, preceding 3 words and fol-lowing 3 words) to 15 (the POS tag of currentword, preceding 7 words and following 7 words).The third experiment utilized a decision tree aspost error corrector as presented in this paper.We trained trigrams, bigrams and unigrams us-ing 60% of totM sentences, and learned the deci-sion tree using 3(1% of total sentences.
For theother experiment, 50% aim 40% of total sen-fences are used tbr probability training and tbrdecision tree learning, respectively.
Tim other10% of total sentences were used to test as inthe prevkms ext)eriments(Figure 3).
For the de-cision tree in the tlfird experiment, hough thesize of the window is also varied from 7 wordsto 15 words, the size of feature vector is variedfrom 14 to 30 because phrase breaks tagged byprobabilistic predictor are include in the featurevector.El I :oI  prababililies trai,ling D For decision Iree induction \[3 I:(!r lest(}9; .
.
.
.
.PI ol~:lbilislic Iil~,lhodonly\[\]GITtee mlly Prolmbilisfic ii~01\]lod Pmballilistic iii0thodlind post error ~tlltl IX)st ?IlOlcDrl ocliOll(6:3 ) t'olteclJoll(4:5 )Fignre 3: The number of sentences for the prob-ability training, the decision tree learning andthe test in the experiments.Tit(; performance is assessed with reference toN, the total number of junctures (spaces in textincluding any type of phrase breaks), and B, thetotal number of phrase breaks (only minor(b1)and major(b,)) breaks) in the test set.
The er-rors can be divided into insertions, deletions andsubstitutions.
An insertion (I) is a break in-serted in the test sentence, where there is not abreak in the reference sentence.
A deletion (D)occurs when a break is marked in the rethrencesentence but not in the test sentence.
A substi-tution (S) is an error between major break andminor break or vice versa.
Since there is no sin-gle way to measure the performance of phrasebreak prediction, we use the following peribr-mance measures (Taylor and Black, 1998).Break_Cor rect  -B -D-SBx 100%,N - D - S - IJuncture_Cor rect  = x 100%NWe use another pertbrmance nmasure, cMledadjusted score, which refer to the prediction ac-curacy in proportion to the total nmnber ofphrase breaks as following performance measureproposed by Sanders (Sanders, 1995).Adjus ted_Score  -, IC  - NB1-N I3  'where NB 1 means the proportion of nobreaks to the number of interword spaces and, lC  means the Juncture_Correct / lO0.Table 1. shows the experimental results of ourphrase break prediction and error con:ectionmethod on the 619 open test sentences (10%of the total corpus).
In the table, W means thethature vector size tbr the decision tree, and 6:3and 4:5 mean ratio of the number of sentencesused in the probabilistic train and the decisiontree induction.The performance of probabilistic method isbetter than that of IG-tree method with anywindow size in U'reak_Cor'rect.
However, as theti;ature vector size is growing in IO-tree method,.lv, nctv, re_Co'rrect  and Ad j , ( s led_Score  becomebetter than those of the l)robM)ilitic method.From the fact that the attribute located in thefirst level of the decision trees is the POS tag ofpreceding word, we can see that the POS tag ofpreceding word iv the most useful attribute forpredicting phrase breaks.The pertbrmance before the error correctionin hyl)rid experiments iv worse ttlan that of theoriginal 1)robabilistic method because the size oftraining corlms for probabilistic method is only66.6% and 44.4:% of that of the original one, re-spectively.
However, the performance sets im-proved by the post error correction tree, and be-corns finally higher than that of both the prob-abilistic nmthod and the IG-tree method.
Theattribute located in the first level of the decisiontree is the phrase break that was predicted inthe probatfilistic method phase.
Although theinitial pertbrmmme (beibre error correction) ofthe exImriment using 4:5 corpus ratio is worsethan that of the experiment using 6:3 corlmsratio, the final perfbrmance gets impressivelyimproved as the decision tree induction corpus1NB _ N- I~N1054'li~l)l( ~, 1: 1)hrase break t)\]e{ti(:tion ~md error eorr{',ction results.\]3r(;~k_Correet1}rol)al)ilis|;ic method onlyIG-Tree onlyProl)al)ilisl;iemel;ho(t 6:3~I~15(Il)ost erroreorr(~,e|; iol l4:5W=7W- 11W= 15l)efore error eorre{;t;iol~W=7W=l lW= 151)~;~, ore, error con'e(;tionW=7W- - l lW- -1552.17%5O.58%51.66%51.77%52.03%57.34{~)59.8O%60.75%51.30%59.O4%61.83%62.7/1%J un{:l;{~re_CorreeI;81.39%81.39%81.65%81.71.%81.29~/~083.67%84.69%85.O6%80.85'~84.42%85.16%85.57%Adj us te ( l _ -~0.48O0.4800,4870.4880.4770.5430.5720.5820.4650.564:0.5850.597incre,~ses from 30% 1;o 50% of the to|;al (:()rims.~l)his result; shows t;h~t |;he prol)osed ~rehilx~e-ture c~m 1)rovi(te, improved results evell with thephrase 1)re~k \])re(tie|:or |:h~l; h~s I)oor |nit|a,1 per-f()z'51 I~L51(;(~,.5 Conc lus ionThis t)~l)er l)r{;s(:nts ;~ new 1)hr;tse t)rea.k predic-|;ion ~rt:hil;eel;ure l;h~l; inl;egr~tes the t}rob~bilis-tie ;~t)t)ro~eh wii;h the (le(:ision-tree 1)~s{'(t ~tl)-t)ro~teh in ;~ synergistic w~y.
Our m~in contl'it)u-|,ions include presenl;ing (leeision |;ree-1);~se(1 r-ror correction for 1)hr~se t)re~k prediel;ion.
Also,i)rol)~fl)ilistic t)hrase break prediction w~ts im-t)leme, nt;e(l a.s ;I,51 inil:i~tl am~ot~l;or f the (tet:i-si()n tree-t)~tse(t e, rror (:orre('tiolL The m:ehite,('-ture ('~m t)rovi(te imt)l"ove(t results even with l;he1)\]u:;~se t)re;~k l)redi{:tor |;h;t|; ha,s l)o{)r inil;i~d t}er-t'orm~mee.
Moreover, I;he, syslxun (:~m 1)e ttexit)lyt;u, ned t;o new eort)us wil;houl; nmssive rel;r;~iningwhich is necess~ry in the t)rol}~bilisti(: metho(t.As shown in the result, t)erli)rmamce, of the hy-t)rid t)hr~se 1)re~k t)redietion is dctermilmd t)yhow well the error eorre('|;or ( ; t in  (;Ollll)ellS;~l;ei;he defi(:iencies of the 1)rol)~d)ilistie t)hrase 1)re~kpredict;ion.The next sl;el) will 1)e to :m~lyze the le~rnedde(:ision trees eareflflly I;() exi;r;~(:l; more desir~t)leti',~tl;ur(', veel;ors.
W(', ~re now working 055 in(:or-l)or~|;ing this i)hl'~se, break 1)redi('tion 5he|hodinto the ext)erimenl;~l Korean 'I?
'\]?S sysl;em.ReferencesJ.
Alle, n ~md S. Humli{'ut.
1987. l'"ro~tt !l'ext toSt)eech: the: MITalk  Systcnt.
Cmnl)ridge Uni-versity Press.IB.
\]\]rill.
1992.
A simple rule-based p~rl;-ot/-speech t~gg{'r.
In \]}roccediT~,.qs of the co~:fer-c'~tcc o~, applied ~,at'u,r'rd la~zg'aage processi~zg..\]eos\]gwon Ch~, Oeunbae \]~(;e, ~md Jong-HyeokLee.
:1998.
Ge, nc, r~dized mlknown morphelneguessing for hybrid P()S l;~tgging of Kor(';m.In P'lw('ecdi'l~,gs of th.
(: Sizth, Wo'r~:.sh, op o'l~, l&-"cqha'rqc Co'rpora, t}~g('s 85 93.Dvr~llx'r l)a(;lem~ms, St;ev{'n Gills, ~md Gerl;\])urieux.
1994.
'l'he ~cquisil;ion of sl;ress: Ad~l;a-orienl;ed ~l)l)ro~{:h. Co~tI)~tl, al, io'l~,al Liw,-g'ltil, ics, 20(3):421 451.Sangho Lee ;rod Yung-Itw~m O15.
1999.
~.lS:ee-t)~rse(t mo(l(;ling of 1)roso(tie 1)hr,(sing ~1~(tsegmei~l;~tl ( m:a.t;ion for kore~m |.t.q ,qy,qJ;elll,q.,~peech, Co'll~,~n,'w~,icatio~,, 28(4):283 300.q'om M. Mil;ehell.
1!)97.
Mach, i~,c Lea'~"H,i~,g.MeGr~w-Ilill.J.
l/,.
{~uinDm.
1.983.
C~.5: l'ro!tr(~ms fo'~" M(z-ch, i'~,e Lea'r'~,i'~ 9.
Morgan K~mflnlmn.\]Brie S~mtlers.
1.9!t5.
Using prot}al)ilistic mel:h-()<Is |;o t)re(lict phr~se t)oundaries for ~t text-to-sl)eeeh system.
Master's thesis, Universityof Nijmegen.P~ml T~,ylor ;md Alum W. BD~t:k.
1!)98.
As-signing phrase l)rc,~ks kom 1)ar|;-of-st)e, eeh sc,-qllelIces.
Cotlt\])'lttcr Speech (t~td La~tg'~t(,,gc,12(2):9!}
1~7.lhml A.
'l?aylor, I.
A. N~irn, A. M. fiutherl;md,~md M. A..l~ck.
1991.
A re~l time speechsynthesis ystem.
1151 l~rot:ce, dirt.q.s of th, c \]~'~t-rospecch, '9.l.,l~m P.II.
wm Santen, l/.iehard W. Sl)roat,3osel)h P. Olive, ~md .luli~ Hirsehl)erg.1997.
l}rog'r(;ss in Speech Sy~,th, esis.
Springer-VerLzg.1055
