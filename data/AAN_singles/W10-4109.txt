Bigram HMM with Context Distribution Clustering for UnsupervisedChinese Part-of-Speech taggingLidan ZhangDepartment of Computer Sciencethe University of Hong KongHong Konglzhang@cs.hku.hkKwok-Ping ChanDepartment of Computer Sciencethe University of Hong KongHong Kongkpchan@cs.hku.hkAbstractThis paper presents an unsupervisedChinese Part-of-Speech (POS) taggingmodel based on the first-order HMM.Unlike the conventional HMM, the num-ber of hidden states is not fixed and willbe increased to fit the training data.
Infavor of sparse distribution, the Dirich-let priors are introduced with variationalinference method.
To reduce the emis-sion variables, words are represented bytheir contexts and clustered based on thedistributional similarities between con-texts.
Experiment results show the out-put state sequence of HMM are highlycorrelated to the latent annotations ofgold POS tags, in context of clusteringsimilarity measures.
The other exper-iments on a real application, unsuper-vised dependency parsing, reveal that theoutput sequence can replace the manu-ally annotated tags without loss of accu-racies.1 IntroductionRecently latent variable model has shown greatpotential in recovering the underlying structures.For example, the task of POS tagging is to re-cover the appropriate sequence structure giventhe input word sequence (Goldwater and Grif-fiths, 2007).
One of the most popular exam-ple of latent models is Hidden Markov Model(HMM), which has been extensively studied formany years (Rabiner, 1989).
The key problemof HMM is how to find an optimal hidden statenumber and the topology appropriately.In most cases, the topology of HMM is pre-defined by exploiting the domain or empiricalknowledge.
This topology will be fixed duringthe whole process.
Therefore how to select theoptimal topology for a certain application or a setof training data is still a problem, because manyresearches show that varying the size of the statespace greatly affects the performance of HMM.Generally there are two ways to adjust the statenumber: top-down and bottom-up methods.
Inthe bottom-up methods (Brand, 1999), the statenumber is initialized with a relatively large num-ber.
During the training, the states are merged ortrimmed and ended with a small set of states.
Onthe other hand, the top-down methods (Siddiqi etal., 2007) start from a small state set and split oneor some states until no further improvement canbe obtained.
The bottom-up approaches requirehuge computational cost in deciding the states tobe merged, which makes it impractical for appli-cations with large state space.
In this paper, wefocus on the latter approaches.Another problem in HMM is that EM algo-rithm might yield local maximum value.
John-son (2007) points out that training HMM withEM gives poor results because it leads to a fairlyflat distribution of hidden states when the empiri-cal distribution is highly skewed.
A multinomialprior, which favors sparse distribution, is a goodchoice for natural language tasks.
In this paper,we proposed a new procedure for inferring theHMM topology and estimating its parameters si-multaneously.
Gibbs sampling has been used ininfinite HMM (iHMM) (Beal et al, 2001; Fox etal., 2008; Van Gael et al, 2008) for inference.Unfortunately Gibbs sampling is slow and diffi-cult to be converged.
In this paper, we proposedthe variational Bayesian inference for the adap-tive HMMmodel with Dirichlet prior.
It involvesa modification to the Baum-Welch algorithm.
Ineach iteration, we replaced only one hidden statewith two new states until convergence.To reduce the number of observation vari-ables, the words are pre-clustered and repre-sented by the exemplar within the same clus-ter.
It is a one-to-many clustering, because thesame word play different roles under differentcontexts.
We evaluate the similarity between thedistribution of contexts, with the assumption thatthe context distribution implies syntactic patternof the given word (Zelling, 1968; Weeds andWeir, 2003).
With this clustering, more contex-tual information can be considered without in-creasing the model complexity.
A relatively sim-ple model is important for unsupervised task interms of computational burden and data sparse-ness.
This is the reason why we do not increasethe order of HMM(Kaji and Kitsuregawa, 2008;Headden et al, 2008).With unsupervised algorithms, there are twoaspects to be evaluated (Van Gael et al, 2009).Fist one is how good the outcome clusters are.We compare the HMM results with the manu-ally POS tags and report the similarity measuresbased on information theory.
On the other hand,we test how good the outputs act as an interme-diate results.
In many natural language tasks, theinputs are word class, not the actual lexical item,for reason of sparsity.
In this paper, we choosethe unsupervised dependency parsing as the ap-plication to investigate whether our clusters canreplace the manual labeled tags or not.The paper is organized as below: in section 2,we describe the definition of HMM and its vari-ance inference.
We present our dynamic HMMin section 3.
To overcome the context limitationin the first-order HMM, we present our distribu-tional similarity clustering in section 4.
In sec-tion 5, we reported the results of the mentionedexperiments while section 6 concludes the paper.2 TerminologyThe task of POS tagging is to assign a syntac-tic category sequence to the input words.
LetS be defined as the set of all possible hiddenstates, which are expected to be highly correlatedto POS tags.
?
represents the set of all words.Therefore the task is to find a sequence of tagsequence S = s1...sn ?
S given a sequence ofwords (i.e.
a sentence, W = w1...wn ?
?).
Theoptimal tags is to maximize the conditional prob-ability p(S |W), which is equal to:maxSp(S |W) = maxSp(S )p(W |S )= maxSp(W, S )(1)In this paper,we consider the first-order HMM,where the POS tags are regarded as hidden statesand words as observed variables.
According tothe Markov assumption, the best sequence oftags S for a given sequence of words W is doneby maximizing (with s0 = 0) the joint probabil-ity:p(W, S ) =n?i=1p(si|si?1)p(wi|si) (2)where w0 is the special boundary marker of sen-tences.2.1 Variational Inference for HMMLet the HMM be modeled with parameter ?
=(A, B, pi), where A = {ai j} = {P(st = j|st?1 = i)}is the transition matrix governing the dynamic ofthe HMM.
B = {bt(i)} = {P(wt = i|st}) is the stateemission matrix and pi = {pii} = {P(s1 = i)} as-signs the initial probabilities to all hidden states.In favor of sparse distributions, a natural choiceis to encode Dirichlet prior into parameters p(?
).In particular, we have:p(A) =N?i=1Dir({ai1, ..., aiN} |u(A))p(B) =N?i=1Dir({bi1, ..., biN} |u(B))p(pi) = Dir({pi1, ..., piN} |u(pi))(3)where the Dirichlet distribution of order N withhyperparameter vector u is defined as:Dir(x|u) =?
(?Ni=1 ui)?Ni=1 ?
(ui)N?i=1xui?1i .
(4)In this paper, we consider the symmetricDirichlet distribution with a fixed length, i.e.u = [?Ni=1 ui/N, ...,?Ni=1 ui/N].In the Bayesian framework, the model param-eters are also regarded as hidden variables.
Themarginal likelihood can be calculated by sum-ming up all hidden variables.
According to theJensen?s inequality, the lower bound of marginallikelihood is defined as:ln p(W) = ln??Sp(?
)p(W, S |?)d???
?Sq(?, S ) lnp(W, S , ?
)q(?, S )d?= F(5)Generally, Variational Bayesian Inferenceaims to find a tractable distribution q(?, s) thatmaximizes the lower bound F .
To make infer-ence flexible, the posterior distribution can beassumed to be factorized according to the mean-field assumption.
We have:p(W, S , ?)
?
q(S , ?)
= q?(?
)qS (S ) (6)Then an extension of EM algorithm (calledBaum-Welch algorithm) can be used to alter-nately optimize the qS and q?.
The EM processis described as follows:?
E Step: Forward-Backward algorithm tofind the optimal state sequence S (t+1) =argmax p(S (t)|W, ?(t))?
M Step: The parameters ?
(t+1) are re-estimated given the optimal state S (t+1)The E and M steps are repeated until a conver-gence criteria is satisfied.
Beal (2003) provedthat only need to do minor modifications in Mstep (in 1) is needed, when Dirichlet prior is in-troduced.3 Adaptive Hidden Markov ModelAs aforementioned, the key problem of HMM ishow to initialize the number of hidden states andselect the topology of HMM.
In this paper, weuse the top-down scheme: starting from a smallnumber of states, only one state is chosen in eachstep and splitted into two new states.
This binarysplit scheme is described in Figure 1.Algorithm 1 Outline of our adpative HMMInitialization: Initialize: t = 0, N(t)repeatOptimization: Find the optimal parametersfor current NtCandidate Generation: Split states andgenerate candidate HMMsCandidate Selection: Select the optimalHMM from the candidates, whose hiddenstate number is Nt+1untilNo further improvement can be achievedafter splittingIn the following, we will discuss the details ofeach step one by one.3.1 Candidate GenerationLet N(t) represent the number of hidden states attimestep t. The problem is how to choose thestates for splitting.
A straightforward way is toselect all states and generate N(t) + 1 candidateHMMs, including the original un-splitted one.Obviously the exhaustive search is inefficient es-pecially for large state space.
To make the algo-rithm more efficient, some constraints must beset to narrow the search space.Intuitively entropy implies uncertainty.
Sohidden states with large conditional entropies aredesirable to be splitted.
We can define the con-ditional entropy of the state sequences given ob-servation W as:H(S |W) = ?
?S[P(S |W) log P(S |W)] (8)Our assumption is the state to be splitted mustbe the states sequence with the highest condi-tional entropy value.
This entropy can be recur-sively calculated with complexity O(N2T ) (Her-nando et al, 2005).
Here N is the number ofA(t+1) = {a(t+1)i j } = exp[?(?
(A)i j ) ?
?(N?j=1?
(A)i j )] ; ?
(A)i j = u(A)j + Eq(s)[ni j]B(t+1) = {b(t+1)ik } = exp[?(?
(B)ik ) ?
?(T?k=1?
(B)ik )] ; ?
(B)ik = u(B)k + Eq(s)[n?ik]pi(t+1)= {pi(t+1)i } = exp[?(?
(pi)i ) ?
?(N?i=1?
(pi)j )]; ?
(pi)i = u(pi)i + Eq(s)[n?
?i ](7)Figure 1: Parameters update equations in M-step.
Here E is the expectation with respect to themodel parameters.
And ni j is the expected number of transition from state si to state s j; n?ik is theexpected number of times word wk occurs with state si; n?
?i is the occurrence of s0 = istates and T is the length of sequence.
Usingthis entropy constraint, the size of candidate stateset is always smaller than the minimal value be-tween N and T .3.2 Candidate SelectionGiven the above candidate set, the parameters ofeach HMM are to be updated.
Note that we justupdate the parameters related to the split state,whilst keep the others fixed.
Suppose the i-thhidden state is replaced by two new states.
Firstthe transition matrix is enlarged from N(t) ?
N(t)dimension to (N(t) + 1) ?
(N(t) + 1) dimension,by inserting one column and row after the i-thcolumn and row.
In the process of update, weonly change the items in the two (i and i + 1)rows and columns.
The other elements irrelevantto the split state are not involved in the updateprocedure.
Similarly EM algorithm is used tofind the optimal parameters.
Note that most ofthe calculations can be skipped by making useof the forward and backward probability matrixachieved in the previous step.
Therefore the con-vergence is fast.Given the candidate selection, we can use amodified Baum-Welch algorithm to find optimalstates and parameters.
Here we use the algorithmin (Siddiqi et al, 2007) with some modificationsfor the Dirichlet prior.
In particular, in E step,we follow their partial Forward-Background al-gorithm to calculate E[ni j] and E[n?ik], if si or s jis candidate state to be splitted.
Then in M-step,only rows and columns related to the candidatestate are updated according to equation (7).
Thedetailed description is given as appendix.Finally it is natural to use variational boundof marginal likelihood in equation (5) for modelscoring and convergence criterion.4 Distributional ClusteringTo reduce the number of observation variables,the words are clustered before HMM training.Intuitively, the words share the similar contextshave similar syntactic property.
The categoriesof many words are varied in different contexts.In other words, the cluster of a given word isheavily dependent on the context it appears.
Forexample,??
can be a noun (meaning: discov-ery) if it acts as an object, or a verb (meaning: todiscover) if it is followed with a noun.
Further-more the introduction of context can overcomethe limited context in the first-order HMM.The underlying hypothesis of clustering basedon distributional similarity is that the words oc-curring in similar contexts behave as similar syn-tactic roles.
In this work, the context of a wordis a trigram consist of the word immediately pre-ceding the target and the word immediately fol-lowing it.
The similarity between two wordsis measured by Pointwise Mutual Information(PMI) between the context pair in which they ap-pear:PMI(wi,w j) = logP(ci, c j)P(ci)P(c j)(9)where ci denotes the context of wi.
P(ci, c j) isthe co-occurrence probability of ci and c j, andP(ci) =?j P(ci, c j) is the occurrence probabil-ity of ci.
In our experiments, the cutoff contextcount is set to 10, which means the frequencyless than the threshold is labeled as the unknowncontext.The above distributional similarity can beused as a distance measure.
Hence any cluster-ing algorithm can be adopted.
In this paper, weuse the affinity propagation algorithm (Frey andDueck, 2007).
Its parameter ?dampfact?
is setto 0.9, and the other parameters are set as de-fault.
After running the clustering algorithm, thecontexts are clustered into 1869 clusterings.
Itis noted that one word might be classified intoseveral clusters , if its contexts are clustered intoseveral clusters.5 ExperimentsAs aforementioned, the outputs of our HMMmodel are evaluated in two ways, clustering met-ric and parsing performance.
The data used in allexperiments are the Chinese data set in CoNLL-2007 shared task.
The number of tokens intraining, development and test sets are 609,060,49,620 and 73,153 respectively.
We use all train-ing data set for training the model, whose maxi-mum length is 242.The hyper parameters of Dirichlet priors areinitialized in a homogeneous way.
The initialhidden state is set to 40 in all experiments.
Afterseveral iterations, the hidden states number con-verged to 247, which is much larger than the sizeof the manually defined POS tags.
Our expec-tation is the refinement variables can reveal thedeep granularity of the POS tags.5.1 Clustering EvaluationIn this paper, we use information theoretic basedmetrics to quantify the information shared bytwo clusters.
The most common information-based clustering metric is the variational of In-formation (VI)(Meila?, 2007).
Given the cluster-ing resultCr and the gold clusteringCg, VI sumsup the conditional entropy of one cluster distri-bution given the other one:VI(Cr,Cg) = H(Cr) + H(Cg) ?
2I(Cr,Cg)= H(Cr |Cg) + H(Cg|Cr)(10)where H(Cr) is the entropy associated with theclustering Cr, and mutual information I(Cr,Cg)quantifies the mutual dependence between twoclusterings, or say the shared information be-tween two variables.
It is easy to see thatVI?
[0, log(N)], where N is the number of datapoints.
However, the standard VI is not normal-ized, which favors clusterings with a small num-ber of clusters.
It can be normalized by divid-ing by log(N), because the number of traininginstances are fixed.
However the normalized VIscore is misleadingly large, if the N is very largewhich is the case in our task.
In this paper onlyun-normalized VI scores are reported to show thescore ranking.To standardize the measures to have fixedbounds, (Strehl and Ghosh, 2003) defined thenormalized Mutual Information (NMI) as:NMI(Cr,Cg) =I(Cr,Cg)?H(Cr)H(Cg)(11)NMI takes its lower bound of 0 if no informationis shared by two clusters and the upper boundof 1 if two clusterings are identical.
The NMIhowever, still has problems, whose variation issensitive to the choice of the number of clusters.Rosenberg and Hirschberg (2007) proposedV-measure to combine two desirable propertiesof clustering: homogeneity (h) and completeness(c) as follows:h = 1 ?
H(Cg|Cr)/H(Cg)c = 1 ?
H(Cr |Cg)/H(Cr)V = 2hc/(h + c)(12)Generally homogeneity and completenessruns in opposite way, whose harmonic mean (i.e.V-measure) is a comprise score, just like F-scorefor the precision and recall.Let us first examine the contextual word clus-tering performance.
The VI score between dis-tributional word categories and gold standard is2.39.
The NMI and V-measure score are 0.53and 0.48, respectively.The clustering performance of the HMM out-puts are reported in Figure 2.
The best VIscore achieved was 3.9524, while V-measurewas 62.09% and NMI reached 0.8051.
Previous40 60 80 100 120 140 160 180 200 220 2403.844.24.44.64.85(a) VI score40 60 80 100 120 140 160 180 200 220 24000.10.20.30.40.50.60.7NMIhomogeneitycompletenessV?measure(b) normalized scoresFigure 2: Clustering evaluation metrics against number of hidden stateswork of Chinese tagging focuses on the taggingaccuracies, e.g.
Wang (Wang and Schuurmans,) and Huang et al (Huang et al, 2007).
Toour knowledge, this is the first work to reportthe distributional clustering similarity measuresbased on informatics view for Chinese .
Simi-lar works can be found on English of WSJ cor-pus (Van Gael et al, 2009).
Their best results ofVI, V-measure, achieved with Pitman-Yor prior,were 3.73 and 59%.
We believe the Chinese re-sults are not good as English correspondencesbecause of the rich unknown words in Chinese(Tseng et al, 2005).5.2 Dependency Parsing EvaluationThe next experiment is to test the goodness of theoutcome states of our model in the context of realtasks.
In this work, we consider unsuperviseddependency parsing for a fully unsupervised sys-tem.
The dependency parsing is to extract thedependency graph whose nodes are the words ofthe given sentence.
The dependency graph is adirected acyclic graph in which every edge linksfrom a head word to its dependent.
Because wework on unsupervised methods in this paper, wechoose a simple generative head-outward model(DependencyModel with Valence, DMV) (Kleinand Manning, 2004; Headden III et al, 2009) forparsing.
The data through the experiment is re-stricted to the sentences up to length 10 (exclud-ing punctuation).Because the main purpose is to test the HMMoutput rather than to improve the parsing perfor-mance, we select the original DMV model with-out extensions or modifications.
Starting fromthe root, DMV generates the head, and then eachhead recursively generates its left and right de-pendents.
In each direction, the possible depen-dents are repeatedly chosen until a STOP markeris seen.
DMV use inside-outside algorithm forre-estimation.
We choose the ?harmonic?
ini-tializer proposed in (Klein and Manning, 2004)for initialization.
The valence information is thesimplest binary value indicating the adjacency.For different HMM candidates with varied hid-den state number, we directly use the outputs asthe input of the DMV and trained a set of models.Performing test on these individual models, wereport the directed dependency accuracies (thefraction of words assigned the correct parent) inFigure 3.40 60 80 100 120 140 160 180 200 220 2403540455055Figure 3: Directed accuracies for different hid-den statesIt is noted that the accuracy monotonicallyincreases when the number of states increases.The most drastic increase happened when statechanges from 40 to 120.
The accuracy increasedfrom 38.56% to 50.60%.
If the state number islarger than 180, the increase is not obvious.
Thefinal best accuracy is 54.20%, which improve thestandard DMV model by 5.6%.
Therefore wecan see that the introduction of more annotationscan help the parsing results.
However, the im-provement is limited and stable when the num-ber of state number is large.
To further improvethe parsing performance, one might turn to theextension of DMV model, e.g.
introducing moreknowledge (prior or lexical information) or moresophistical smoothing techniques.
However, thedevelopment of parser is not the focus of this pa-per.6 Conclusion and Future WorkThis paper works on the unsupervised ChinesePOS tagging based on the first-order HMM.
Ourcontributions are: 1).
The number of hiddenstates can be adjusted to fit the data.
2).
For in-ference, we use the variational inference, whichis faster and is guaranteed theoretically to con-vergence.
3).
To overcome the context limitationin HMM, the words are clustered based on dis-tributional similarities.
It is a 1-to-many cluster-ing, which means one word might be classifiedinto different clusters under different contexts.Finally, experiments show the hidden states arecorrelated to the latent annotations of the stan-dard POS tags.The future work includes to improve the per-formance by incorporating a small amount of su-pervision.
The typical supervision used beforeis dictionary extracted from a large corpus likeChinese Gigaword.
Another interesting idea isto select some exemplars (Haghighi and Klein,2006).ReferencesBeal, Matthew J., Zoubin Ghahramani, and Carl Ed-ward Rasmussen.
2001.
The infinite hiddenmarkov model.
In NIPS, pages 577?584.Beal, M. J.
2003.
Variational algorithms forapproximate bayesian inference.
Phd Thesis.Gatsby Computational Neuroscience Unit, Uni-versity College London.Brand, Matthew.
1999.
An entropic estimator forstructure discovery.
In Proceedings of the 1998conference on Advances in neural information pro-cessing systems II, pages 723?729, Cambridge,MA, USA.
MIT Press.Fox, Emily B., Erik B. Sudderth, Michael I. Jordan,and Alan S. Willsky.
2008.
An hdp-hmm for sys-tems with state persistence.
In ICML ?08: Pro-ceedings of the 25th international conference onMachine learning.Frey, Brendan J. and Delbert Dueck.
2007.
Clus-tering by passing messages between data points.Science, 315:972?976.Goldwater, Sharon and Tom Griffiths.
2007.
Afully bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 744?751, Prague, Czech Republic,June.
Association for Computational Linguistics.Haghighi, Aria and Dan Klein.
2006.
Prototype-driven learning for sequence models.
In Pro-ceedings of the main conference on Human Lan-guage Technology Conference of the North Amer-ican Chapter of the Association of ComputationalLinguistics, pages 320?327.Headden, III, William P., David McClosky, and Eu-gene Charniak.
2008.
Evaluating unsupervisedpart-of-speech tagging for grammar induction.
InCOLING ?08: Proceedings of the 22nd Interna-tional Conference on Computational Linguistics,pages 329?336, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Headden III, William P., Mark Johnson, and DavidMcClosky.
2009.
Improving unsupervised depen-dency parsing with richer contexts and smoothing.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 101?109, Boulder, Colorado,June.
Association for Computational Linguistics.Hernando, D., V. Crespi, and G. Cybenko.
2005.
Ef-ficient computation of the hidden markov modelentropy for a given observation sequence.
vol-ume 51, pages 2681?2685.Huang, Zhongqiang, Mary Harper, and Wen Wang.2007.
Mandarin part-of-speech tagging and dis-criminative reranking.
In Proceedings of the 2007Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Nat-ural Language Learning (EMNLP-CoNLL), pages1093?1102, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Johnson, Mark.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In Proceedings of the 2007Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Nat-ural Language Learning (EMNLP-CoNLL), pages296?305, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Kaji, Nobuhiro and Masaru Kitsuregawa.
2008.
Us-ing hidden markov random fields to combine dis-tributional and pattern-based word clustering.
InCOLING ?08: Proceedings of the 22nd Interna-tional Conference on Computational Linguistics,pages 401?408, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Klein, Dan and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models ofdependency and constituency.
In Proceedings ofthe 42nd Meeting of the Association for Computa-tional Linguistics (ACL?04), Main Volume, pages478?485, Barcelona, Spain, July.Meila?, Marina.
2007.
Comparing clusterings?an in-formation based distance.
volume 98, pages 873?895.Rabiner, Lawrence R. 1989.
A tutorial on hiddenmarkov models and selected applications in speechrecognition.
In Proceedings of the IEEE, pages257?286.Rosenberg, Andrew and Julia Hirschberg.
2007.V-measure: A conditional entropy-based exter-nal cluster evaluation measure.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 410?420.Siddiqi, Sajid, Geoffrey Gordon, and Andrew Moore.2007.
Fast state discovery for hmm model selec-tion and learning.
In Proceedings of the EleventhInternational Conference on Artificial Intelligenceand Statistics (AI-STATS).Strehl, Alexander and Joydeep Ghosh.
2003.
Clus-ter ensembles ?
a knowledge reuse frameworkfor combining multiple partitions.
Journal of Ma-chine Learning Research, 3:583?617.Tseng, Huihsin, Daniel Jurafsky, and ChristopherManning.
2005.
Morphological features help postagging of unknown words across language vari-eties.
pages 32?39.Van Gael, Jurgen, Yunus Saatci, Yee Whye Teh, andZoubin Ghahramani.
2008.
Beam sampling forthe infinite hidden markov model.
In ICML ?08:Proceedings of the 25th international conferenceon Machine learning.Van Gael, Jurgen, Andreas Vlachos, and ZoubinGhahramani.
2009.
The infinite HMM for unsu-pervised PoS tagging.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 678?687, Singapore, Au-gust.
Association for Computational Linguistics.Wang, Qin Iris and Dale Schuurmans.
Improved es-timation for unsupervised part-of-speech tagging.page 2005, Wuhan, China.Weeds, Julie and David Weir.
2003.
A generalframework for distributional similarity.
In Pro-ceedings of the 2003 conference on Empiricalmethods in natural language processing, pages81?88, Morristown, NJ, USA.
Association forComputational Linguistics.Zelling, Harris.
1968.
Mathematical sturcture of lan-guage.
NewYork:Wiley.APPENDIXPseudo-code of the extended Baum-Welch Al-gorithm in our dynamic HMMInput: Time step t:State Candidate: k ?
(k(1), k(2)) ;Sate Number: Nt;Model Parameter: ?
(t) = (A(t), B(t), pi(t));Initializeu(l)[k(1), k(2)]?
[ u(l)[k]2 ,u(l)[k]2 ], l ?
{A, B, pi}pik(1) ?
12pik; pik(2) ?12pikak?k(i) ?
12ak?k(i) ; ak(i)k?
?
ak(i)k?
;ak(i)k( j) ?
12ak(i)k( j) , here i, j ?
1, 2, k?
, krepeatE step:update forward: ?t(k(1)) and ?t(k(2))backward: ?t(k(1)) and ?t(k(2))update ?t(i, j) and ?t(i); if i, j ?
{k(1), k(2)}update E[ni j] =?t ?t(i, j)/?t ?t(i)E[nik] =?t,wt=k ?t( j)/?t ?t( j)M step:update ?
(t+1) using equation (7)until (4F < ?
)Output: ?
(t+1), F
