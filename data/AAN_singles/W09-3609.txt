Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 71?79,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPFireCite: Lightweight real-time reference string extraction from webpagesChing Hoi Andy Hong Jesse Prabawa GozaliSchool of ComputingNational University of Singapore{hongchin,jprabawa,kanmy}@comp.nus.edu.sgMin-Yen KanAbstractWe present FireCite, a Mozilla Firefoxbrowser extension that helps scholars as-sess and manage scholarly references onthe web by automatically detecting andparsing such reference strings in real-time.FireCite has two main components: 1)a reference string recognizer that has ahigh recall of 96%, and 2) a referencestring parser that can process HTML webpages with an overall F1 of .878 and plain-text reference strings with an overall F1of .97.
In our preliminary evaluation, wepresented our FireCite prototype to fouracademics in separate unstructured inter-views.
Their positive feedback gives evi-dence to the desirability of FireCite?s cita-tion management capabilities.1 IntroductionOn the Web, many web pages like researchers?
orconference homepages contain references to aca-demic papers much like citations in a bibliogra-phy.
These references do not always follow a spe-cific reference style.
Usually, they make use ofHTML formatting to differentiate fields and em-phasize keywords.
For example in Figure 1, papertitles are displayed in bold.Depending on personal preference and habit,references found on the Web may be processed invarious ways.
This process however, can possiblybe quite a long chain of events:1.
A researcher finds a PDF copy of the paperand downloads it.2.
He reads the abstract of the paper, then de-cides to read the rest of it.3.
He prints out the paper and reads it, makingannotations along the margin as he reads.4.
He produces a BibTeX entry for the paper.Figure 1: A web page with a list of references.Paper titles are displayed in bold.5.
He cites the paper in his own work.This process is too time-consuming for re-searchers to do for each reference, one at a time.One solution is to collect all the references of in-terest first.
These references can then be processedat a later time.
Bibliographic Management Appli-cations (BMAs) do exactly this by allowing the re-searcher to record interesting references for lateruse.
Alternatively, the references can be recordedmanually on paper or in a text file.
The paper foreach reference can also be printed and organizedphysically in folders or piles.Each method has its own disadvantages.
Usingnotebooks, text files or printouts imposes consid-erable cognitive load on the researcher especiallywhen hundreds of references need to be managed.BMAs seek to relieve researchers from this prob-lem, but are often too complicated to use andmaintain.
A popular BMA, EndNote, for example,retrieves metadata from online library cataloguesand databases, but experience is necessary to knowwhich database or catalogue to search.
Consider-able time can be lost searching for a computer sci-ence paper in a medical database.
An automatic,yet lightweight solution is needed.Since the references are found on the Web, themost suitable location for a BMA is within the web71browser itself.
In this paper, we propose FireCite1,a Firefox browser extension which embodies thisidea.
FireCite 1) automatically recognizes refer-ences on web pages, 2) parses these referencesinto title, authors, and date fields, 3) allows theresearcher to save these references for later use,and 4) allows a local PDF copy of the paper to besaved for each reference.At its core, FireCite consists of a referencestring recognizer and a reference string parser withaccuracies comparable to other systems.
Unlikethese systems however, as a browser extension,FireCite needs to be fast and lightweight.
Bloatedextensions can cause the browser?s memory foot-print to grow significantly, lowering overall per-formance.
An extension must also perform its op-erations fast.
Otherwise, it will detract users fromtheir primary task with the browser.
Nah (2004)suggests latencies should be kept within two sec-onds.In the next section, we review related work.
Wethen discuss reference string recognition, followedby parsing in Section 3.
After component evalua-tions, we conclude by discussing the user interfaceof FireCite.2 Related WorkRecognizing and parsing reference strings hasbeen a task tackled by many, as it is a necessarytask in modern digital libraries.Past work has dealt primarily with clean data,where reference strings are already delimited (e.g.,in the References or Bibliography section of ascholarly work).
Many works consider both refer-ence string recognition and reference string pars-ing as a single combined problem.
With regardsto the task, IEPAD (Chang et al, 2003) looks forpatterns among the HTML tags, while (Zhai andLiu, 2005) looks for patterns among the presenta-tion features of the web page.
A machine learningapproach using Conditional Random Fields is alsodiscussed in a few works (Xin et al, 2008; Zhu etal., 2006).CRE (Yang et al, 2008) is an automatic ref-erence string recognizer that works on publica-tion list pages.
Given such a page, CRE iden-tifies individual reference strings by looking forcontiguous common style patterns.
The system isbased on the authors?
two observations: 1) ?refer-1The latest version of the extension is at:https://addons.mozilla.org/en-US/firefox/addon/10766/ence string records are usually presented in one ormore contiguous regions?, and 2) ?reference stringrecords are usually presented by using similar tagsequences and organized under a common parentnode?.
Therefore, the system examines the DOM2tree of the web page and identifies adjacent sub-trees that are similar.
The system then removessubtrees that are unlikely to be reference strings,by comparing their word count against a databaseof reference strings?
word counts.
The authors re-port an F1 of around 90% for pages where refer-ence strings make up at least 80% of the text onthe page, and an F1 of at least 70% when refer-ence strings make up at least 30% of the page.Of note is that their testing dataset consistssolely of computer science researchers?
home-pages and publication list pages.
There is no indi-cation of how their system will perform for othertypes of web pages.
Although there are many pub-lished works on the extraction of semi-structureddata from web pages, very few of them deal di-rectly with the issue of reference string extraction.Also, none of the works deal directly with the is-sue of web pages that do not contain any relevantdata.
In FireCite?s case, this is an important issueto consider, because false positives will be parsed,and as stated previously, almost all web pages willhave elements that are not part of any referencestring.As for reference string parsing, the field of In-formation Extraction (IE) has treated this task asone of its sample applications.
As such, manydifferent IE approaches involving different super-vised classifiers have been tried.Such classification methods require a gold stan-dard corpus to train on.
The CORA InformationExtraction dataset, introduced in (Seymore et al,1999) consists of a corpus of 500 classified refer-ence strings extracted from computer science re-search papers, is used as training data.
The CORAdataset is annotated with thirteen fields, includingauthor, title and date.As for classification approaches, (Hetzner,2008; Seymore et al, 1999) and AutoBib (Gengand Yang, 2004) makes use of Hidden MarkovModels (HMM), while ParsCit (Councill et al,2008) and (Peng and McCallum, 2004) make useof Conditional Random Fields (CRF).ParsCit?s reference string parsing system makesuse of CRF to learn a model that can apply meta-2Document Object Model.
http://www.w3.org/DOM/72data labels to individual word tokens of a referencestring.
ParsCit?s labeling model consists of 7 lex-ical features (features that make use of the mean-ing/category of a word, such as whether the wordis a place, a month, or a first name) and 16 localand contextual features (features that makes useof formatting information about the current andneighbouring tokens, such as whether the word isin all caps).
Its lexical features require the use ofan extensive dictionary of names, places, publish-ers and months.
ParsCit achieves an overall field-level F1 of .94.Another competitive method, FLUX-CiM (Cortez et al, 2007) also parses plain-textreference strings, based on a knowledge base ofreference strings.
Initially, labels are assigned totokens based on the (label, token) pair?s likelihoodof appearance in the knowlege base.
For tokensthat do not occur in the knowledge base, a bindingstep is used to associate them with neighbouringtokens that have already been labelled.
Theauthors report a very high token-level accuracy interms of F1 of 98.4% for reference strings in theComputer Science (CS) domain, and 97.4% forreference strings in the Health Sciences domain.A key difference from other parsing methods isthat tokens in FLUX-CiM are strings delimited bypunctuation rather than single words (see an ex-ample in Figure 2).
This comes from an observa-tion by the authors that ?in general, in a referencestring, every field value is bounded by a delimiter,but not all delimiters bound a field.
?Atlas , L ., and S .
Shamma ,?
Joint Acoustic and Modulation Frequency ,?EURASIP JASP , 2003 .Figure 2: A tokenised reference string.
Each boxcontains one token.While both ParsCit and FLUX-CiM have highlevels of performance, they are not suitable for ouruse for two reasons:?
Both systems are large.
ParsCit?s classi-fier model plus dictionaries add up to about10MB.
FLUX-CiM requires a database of3000 reference strings for each knowledgedomain, for best performance.
Databases ofthis size will take a significant amount of timeto load and to access, negatively impactingthe user experience.?
Both systems are not designed to handle webreference strings.
Neither system is able tocorrectly parse a reference string such as theone shown in Figure 3 due to its lack of punc-tuation and the misleading tokens that resem-ble publication dates.Doe, J.
2000 1942-1945: World War Twoand its effects on economy and technology.Generic Publisher.
Generic Country.Figure 3: A reference string that FLUX-CiM andParsCit cannot parse correctly.3 MethodologyFireCite performs its task of reference extractionin two logically separate stages: recognition andparsing.
Reference string recognition locates anddelimits the start and end of reference strings on aweb page, while parsing delimits the internal fieldswithin a recognized reference.3.1 RecognitionReference recognition itself can be logically seg-mented into two tasks: deciding whether refer-ences could occur on a page; and if so, delimitingthe individual reference strings.
We build a roughfilter for the first task, and solve the second taskusing a three stage heuristic cascade.Algorithm 1 Reference recognition.1: Exclude pages based on URL and absence ofkeywords2: Split token stream into a set S of (non-overlapping) sequences, where each sequencecontains at most one reference string, and noreference string is split across two token se-quences.3: Select sequences likely to be reference strings,forming a set S?
which is parsed into a set ofreference strings C.4: Remove sequences with nonsensical parse re-sults from the set of reference strings C.We now detail these stages.Stage 1 immediately discards webpages thatdo not meet three criteria from subsequent auto-matic processing.
For a page to be automaticallyprocessed by subsequent recognition and parsingphases, FireCite requires that the webpage:73?
Is from a .edu, .org, or .ac domain.
Do-mains with country identifiers, such aswww.monash.edu.au, are also accepted;?
Contains one or more of the words ?Publica-tions?, ?Readings?, ?Citations?, ?Papers?, and?References?.?
Contains one or more of the words ?Confer-ence?, ?Academic?, ?Journal?, and ?Research?.The included domains include web pages fromacademic institutions, digital libraries such asCiteseerX 3 and ACM Portal 4, and online ency-clopedias such as Wikipedia 5 ?
basically, webpages where reference strings are likely to befound.
The keywords serve to further filter awaypages unlikely to contain lists of reference strings,by requiring words that are likely to appear in theheadings of such lists.Stage 1 runs very quickly and filters most non-scholarly web pages away from the subsequent,more expensive processing.
This is crucial in im-proving the extension?s efficiency, and ensuringthat the extension does not incur significant la-tency for normal browsing activity.Stage 2 splits the web page text into distinctchunks.
In plain-text documents, we differentiatechunks by the use of blank lines.
In HTML webpages, we use formatting tags: <p> and <br>.Other tags might also indicate a fresh chunk withinordered (<ol>) and unordered (<ul>) lists, listitems are marked by the <li> tag.
A horizon-tal rule (<hr>) is used to separate sections in theweb page.
Stage 2 makes use of all these HTMLtags to split the web page text into distinct, non-overlapping sequences.Stage 3 removes sequences that are unlikely tobe reference strings, based on their length.
Se-quences that are too long or short are removed(i.e., with word length 5 < wl < 64, and tokenlengths 4 < tl < 48).
These limits are basedon the maximum and minimum word and tokenlengths of reference strings in the CORA corpus.The sequences that survive this stage are sent tothe parsing system, discussed in the next subsec-tion to be parsed.Stage 4 further removes sequences that are ill-formed.
We require that all reference strings in-clude a title and a list of authors after being parsed.3hosted at http://citeseerx.ist.psu.edu4http://portal.acm.org5www.wikipedia.orgSequences that do not meet these requirements arediscarded.
Remaining sequences are accepted asvalid reference strings.3.2 ParsingBetween Steps 3 and 4 in the recognition pro-cess, a reference string is parsed into fields.
Wetreat this problem as a standard classification prob-lem for which a supervised machine learning algo-rithm can be trained to perform.
In implementingour parsing algorithm, recall that we have to meetthe criterion of a lightweight solution, which heav-ily influenced the resulting design.While a full-fledged reference string parser willextract all available metadata from the referencestring, including fields such as publisher name,publisher address and page numbers, we con-sciously designed our parser to only extract threefields: the title, the authors, and the date of pub-lication.
All other tokens are classified as Miscel-laneous.
There are two reasons for this: 1) for thepurposes of sorting the reference strings and sub-sequently searching for them, these three fields aremost likely to be used; 2) restricting classificationspace to four classes also simplifies the solution,shrinking the model size.Another simplification was to use a decisiontree classifier, as 1) the trained model is easilycoded in any declarative programming language(including Javascript, the programming languageused by Firefox extensions), and 2) classificationis computationally inexpensive, consisting of a se-ries of conditional statements.Also, instead of the common practice of to-kenising a string into individual words, we followFLUX-CiM?s design and use punctuation (exceptfor hyphens and apostrophes) and HTML tags astoken delimiters (as seen in the example in Fig-ure 2).
This tokenization scheme often leads tophrases.
There are a few advantages to this styleof tokenisation: 1) considering multiple words asa token allows more complex features to be used,thus giving a better chance of making a correctclassification; and 2) reducing the number of to-kens per reference string reduces the computa-tional cost of this task.To classify each phrase, we compile a set of tenfeatures for use in the decision tree, comprising:1) Lexical (dictionary) features that contain infor-mation about the meaning of the words within thetoken; 2) Local features that contain non-lexical74Feature Name DescriptionPfieldLabel(String)The label of the previous tokenhasNumber(Boolean)Whether the token contains any num-bershasYear(Boolean)Whether the token contains any 4-digit number between 1940 and 2040fieldLength (In-teger)The number of characters the tokenhashasMonth(Boolean)Whether the token contains anymonth words (e.g.
?January?, ?Jan?
)oneCap(Boolean)Whether the token consists of onlyone capital letter e.g.
?B?position (Float) A number between 0 and 1 that indi-cates the relative position of the tokenin the reference string.hasAbbreviation(Boolean)Whether the token contains anywords with more than one capitalletter.
Examples are ?JCDL?, and?ParsCit?startPunctuation(String)The punctuation that preceded thistoken.
Accepted values are pe-riod, comma, hyphen, double quotes,opening brace, closing brace, colon,others, and noneendPunctuation(String)The punctuation that is immediatelyafter this token.
Accepted values arethe same as for startPunctuationTable 1: List of classifier featuresinformation about the token; 3) Contextual fea-tures, which are lexical or local features of a to-ken?s neighbours.
Table 1 gives an exhaustive listof features used in FireCite.We had to exclude lexical features that requirea large dictionary, such as place names and firstnames, as such features would add significantly tothe loading and execution times of FireCite.FireCite uses its trained model to tag inputphrases with their output class.
Before acceptingthe classification results, we make one minor re-pair to them.
The repair stems from the observa-tion that in gold standard reference strings, boththe author and title fields are contiguous.
If morethan one contiguous sequence of Title or Authorclassification labels exist, there must be a classifi-cation error.
When the extension encounters sucha situation, FireCite will accept the first encoun-tered sequence as correct, and change subsequentsequences?
labels to Miscellaneous (Figure 4).The parser joins all contiguous tokens for eachcategory into a string, and returns the set of stringsas the result.4 Evaluation4.1 RecognitionWe took faculty homepages from the domains offour universities at random, until a set of 20 home-pages with reference strings and 20 homepageswithout reference strings were obtained.
Note thatthese homepages were sampled from all faculties,not merely from computer science.Tests were conducted using these 40 pages toobtain the reference string recognition algorithm?saccuracy.
A reference string is considered foundif there exists, in the set of confirmed referencestrings C, a parsed text segment c that contains theentire title as well as all the authors?
names.
Eachparsed text segment can only be used to identifyone reference string, so if any text segments con-tain more than one reference string, only one ofthose reference strings will be considered found.Active stages Recall Precision F11, 2, 3, 4 96.0% 57.5% .7192, 3, 4 96.6% 53.6% .6891, 2, 4 96.3% 51.6% .6721, 2, 3 98.4% 40.9% .5781, 2 99.2% 16.1% .278Table 2: Results of reference string recognitionover forty web pages for five variations of Fire-Cite?s reference string recognitionIn order to determine the effect of each stageon overall recognition accuracy, some stages ofthe recognition algorithm were disabled in testing.The results are presented in Table 2.
As all testpages come from university domains, all pass thefirst URL test.
When the keyword search is deac-tivated, all 40 test pages pass Stage 1.
Otherwise,19 pages with reference strings and 6 pages with-out reference strings pass Stage 1.The results show that disabling individualstages of the algorithm increases recall slightly,but increases the number of false positives dispro-portionately more.
The fully-enabled algorithmstrikes a balance between the number of referencestrings found and the number of false positives.From the above results, we can also see thatfalse positives make up around 40% of the textsegments that are recognised as reference strings.However, the majority of reference strings arerecognised by the algorithm.
In our usage sce-nario, our output will eventually be viewed by ahuman user, who will be the final judge of what isa reference string and what is not.
Therefore, it is75Figure 4: An example of an incorrectly labelled (highlighted) reference string segmentPage (# of references) Title Authors Date All TokensA (72) .902 .893 .988 .708B (52) .953 .957 .990 .960C (29) .684 .304 .774 .651D (68) .753 .968 .889 .917E (8) .692 .875 1.000 .889F (45) .847 1.000 .989 .966Overall .836 .916 .948 .878Table 3: Results of FireCite reference string pars-ing.
Performance figures given are Token F1.Overall F1 includes tokens classified as Miscella-neous, and is micro-averaged.more important that we have a high recall ratherthan high precision.
In that respect, this algorithmcan be said to fulfill its purpose.4.2 ParsingTo evaluate the reference string parsing algorithm,we randomly selected six staff publication pagesfrom a computer science faculty.
The presenta-tion of each page, as well as the presentation ofreference strings on each page, were all chosen todiffer from each other.
There are a total of 274reference strings in these six pages.
We annotatedthe reference strings by hand; this set of annota-tions is used as the gold standard.
The six pagesare loaded using a browser with FireCite installed.FireCite processes each page and produces a out-put file with the parsed reference strings.
Theseparsing results are then compared against the goldstandard.
Table 3 shows the token level results,broken down by web page.The FireCite reference string parser is able tohandle plain-text reference strings as well.
A set ofplain-text reference strings can be converted intoa form understandable by FireCite, simply by en-closing the set of reference strings with <html>tags, and replacing line breaks with <br> tags.Table 4 shows the token F1 of the Firecite ref-erence string parser compared FLUX-CiM, whileTable 5 shows the field F1 of FireCite, FLUX-CiMand ParsCit.
The test dataset used by all three sys-tems is the FLUX-CiMComputer Science dataset66available at http://www.dcc.ufam.edu.br/ e?ccv/flux-cim/ Computer-Science/System Title Authors Date OverallFireCite .940 .994 .982 .979FLUX-CiM .974 .994 .986 .984Table 4: Token F1 of FireCite and FLUX-CiM.System Title Authors Date OverallFireCite .92 .96 .97 .94ParsCit .96 .99 .97 .94FLUX-CiM .93 .95 .98 .97Table 5: Field F1 of FireCite and other referencestring parsers.of 300 reference strings randomly selected fromthe ACM Digital Library.
Note that in FireCiteand FLUX-CiM, tokens are punctuation delimitedwhereas in ParsCit, tokens are word delimited.We feel that above results show that FireCite?sreference string parser is comparable to the re-viewed systems (although statistically worse), de-spite its use of a fast and simple classifier and thelack of lexical features that require large dictio-naries.
The disparity of results between handlingweb page reference strings and handling plain-textreference strings can generally be attributed to thedifferences between web page reference stringsand plain-text reference strings.
Specifically:?
Among the testing data used, the referencestrings on one web page (Page C) all beginwith the title.
However, in the CORA train-ing corpus, all reference strings begin withthe authors?
names.
As a result, in the trainedclassifier, the first token of every referencestring is classified as ?authors?.
This error isthen propagated through the entire referencestring, because each token makes use of theprevious token?s class as a classifier feature.As shown in Table 3 above, the performancefor page C is much worse than the perfor-mance for the other pages.?
When web pages are created and editedusing a WYSIWIG editor, such as AdobeDreamweaver or Microsoft Office FrontPage,multiple nested and redundant HTML tags76Min.
time Max.
time Avg.
timeWith references 90 544 192W/o references 6 222 74All pages 6 544 133Table 6: FireCite execution time tests over 40 webpages.
Times given in milliseconds.tend to be added to the page.
Because Fire-Cite treats HTML tags as token delimiters,these redundant tags increase the number oftokens in the string, thus affecting the to-ken position feature of the classifier, causingsome tokens to become incorrectly classified.Some of the inaccuracies can also be attributedto mistakes from reference string recognition.When the reference string is not correctly delim-ited, text that occurs before or after the actual ref-erence string is also sent to the reference stringparser.
This affects the token position and previ-ous token label features.The competitive advantage of FireCite?s refer-ence string parser is that it is very small comparedto the other systems.
FireCite?s reference stringparser consists only of a decision tree coded intoJavaScript if-then-else statements, and a couple ofJavaScript functions, taking up a total of around38KB of space.
On the other hand, as mentionedabove, FLUX-CiM optimally requires a databaseof around 3000 reference strings, while ParsCit?sclassifier model and dictionaries require a total of10MB of space.
These characteristics also makethe reference string parser fast.
Speed tests wereconducted over 40 web pages taken from the do-mains of four universities, 20 of which contain ref-erence strings and 20 of which do not.
The resultsare summarised in Table 6.
From these results wecan infer with some confidence that FireCite willadd no more than one second to the existing timea page takes to load.5 Extension Front EndWe thus implemented a prototype BMA as a Fire-fox extension that uses the recognizer and parseras core modules.
As such an extension interactswith users directly, the extension?s front end de-sign concentrated on functionality and usabilityissues that go beyond the aforementioned naturallanguage processing issues.Browser extension based BMAs are not new.Zotero7 as well as Mendeley8 both offer BMAsthat manage reference (and other bookmark) in-formation for users.
However, neither recognizesor delimits free formed reference strings found ongeneral webpages.
Both rely on predefined tem-plates to process specific scholarly websites (e.g.Google Scholar, Springer).In developing our front end, our design hopesto complement such existing BMAs.
We followeda rapid prototyping design methodology.
The cur-rent user interface, shown in Figure 5, is the re-sult of three cycles of development.
Up to now,feedback gathering has been done through focusgroups with beginning research students and indi-vidual interviews with faculty members.
Ratherthan concentrate on the design process, we give aquick synopsis of the major features that the Fire-Cite prototype implements.One-Click Addition of References: FireCiteappends a clickable button to each reference stringit detects through the recognition and parsingmodules.
Clicking this button adds the referencestring?s metadata to the reference library.
The de-sign draws attention to the presence of a referencewithout disrupting the layout of the webpage.Reference Library: The reference libraryopens as a sidebar in the browser.
It is a localdatabase containing the metadata of the saved ref-erences.
The library allows reference strings to beedited or deleted, and sorted according to the threeextracted metadata fields.Manual recognition and addition: The coremodules occasionally miss valid references.
Toremedy this, users can manually highlight a spanof text, and through the right click context menu,ask FireCite to parse the span and append an ?addcitation?
button.
The user may also manually addor edit reference metadata directly in the sidebar.This feature allows the user to add entries from hisexisting collections of papers, or to add entries forwhich no reference string can be found (such aspapers that have not been published).PDF download: When a reference is added tothe local library, any Portable Document Format(PDF) file associated with the reference string isdownloaded as well.
Appropriate PDF files arefound heuristically by finding a hyperlink lead-ing to a PDF file within the text segment.
Thedownloaded PDF files are stored in a single folder7http://www.zotero.org8http://www.mendeley.com77Figure 5: Screenshot of FireCite prototype illus-trating (a) the reference string library, (b) buttonappended to each reference string, and (c) buttonstate after the reference string has been added tothe list.within Firefox?s storage location for the extension,and can be opened or deleted through the sidebarinterface.
With this feature, the user will not needto juggle his PDF files and reference string libraryseparately.As a preliminary evaluation, we presented Fire-Cite to four academics in separate unstructuredinterviews.
All four subjects saw the potentialof FireCite as a BMA, but not the usefulness ofrecognising reference strings on the Web.
Two ofthem pointed out that they rarely encounter refer-ence strings while browsing the Web, while an-other only needs to search for specific, known pa-pers.
When asked in detail, it was apparent thatsubjects do actually visit web pages that containmany reference strings.
In DBLP, each entry isactually a reference string.
In the ACM Digital Li-brary, in every article information page, there isa list of reference strings that have been extractedfrom the bibliography of the article using OpticalCharacter Recognition (OCR).From our study, we conclude that integrationwith template based recognition (a la Zotero) ofsites such as DBLP, Google Scholar and ACMPortal, has better potential.
As expected, sincethe subjects all have significant research expe-rience, they have already developed suitable re-search methods.
The challenge is for FireCite tofit into their workflow.6 ConclusionThis paper describes FireCite, a Firefox extensionthat can recognise and delimit metadata from ref-erence strings on freeform web pages.
FireCite?s?Liquidity-Based Model of Security Design,?with Darrell Duffie, Econometrica, 1999, 67,65-99.Figure 6: A reference string with one author?sname omitted.Michael Collins and Terry Koo.Discriminative Reranking for Natural Lan-guage Parsing.Computational Linguistics 31(1):25-69.Figure 7: A reference string with its year omitted.Part of a list of reference strings organised by theiryear of publication.implementation demonstrates it is possible to dothese tasks in real-time and with a usable level ofaccuracy.We have validated the accuracy of FireCite?sembedded recognition and parsing modules bycomparing against the state-of-the-art systems,both on web based reference strings that useHTML tags as well as gold-standard referencestrings in plain text.
FireCite achieves a usablelevel of reference string recognition and parsingaccuracy, while remaining small in size, a criti-cal requirement in building a browser extension.This small model allows FireCite to complete itsprocessing of reference heavy webpages in un-der one second, an acceptable level of latency formost users.
Preliminary user studies show thatthe FireCite system should incorporate templatebased recognition of large scholarly sites as wellfor maximum effectiveness.Future work on the parsing and recognition willfocus on capturing implied contextual informa-tion.
On some web pages the author may omittheir own name, or place the year of publication ina section head (Figures 6 and 7).
We are workingtowards recognizing and incorporating such con-textual information in processing.AcknowledgementsThis work was partially supported by a NationalResearch Foundation grant ?Interactive MediaSearch?
(grant # R 252 000 325 279).ReferencesChia-Hui Chang, Chun-Nan Hsu, and Shao-Cheng Lui.2003.
Automatic information extraction from semi-78structured web pages by pattern discovery.
Decis.Support Syst., 35(1):129?147.Eli Cortez, Altigran S. da Silva, Marcos Andre?Gonc?alves, Filipe Mesquita, and Edleno S.de Moura.
2007.
FLUX-CIM: flexible unsuper-vised extraction of citation metadata.
In Proc.
JCDL?07, pages 215?224, New York, NY, USA.
ACM.Isaac G. Councill, C. Lee Giles, and Min-Yen Kan.2008.
ParsCit: An open-source CRF referencestring parsing package.
In LREC ?08, Marrakesh,Morrocco, May.Junfei Geng and Jun Yang.
2004.
Autobib: automaticextraction of bibliographic information on the web.pages 193?204, July.Erik Hetzner.
2008.
A simple method for citationmetadata extraction using hidden markov models.In Proc.
JCDL ?08, pages 280?284, New York, NY,USA.
ACM.Fiona Fui-Hoon Nah.
2004.
A study on tolerable wait-ing time: how long are web users willing to wait?Behaviour & Information Technology Special Issueon HCI in MIS, 23(3), May-June.Fuchun Peng and Andrew McCallum.
2004.
Accu-rate information extraction from research papers us-ing conditional random fields.
pages 329?336.
HLT-NAACL.Kristie Seymore, Andrew McCallum, and Roni Rosen-feld.
1999.
Learning hidden markov model struc-ture for information extraction.
In AAAI?99 Work-shop on Machine Learning for Information Extrac-tion.Xin Xin, Juanzi Li, Jie Tang, and Qiong Luo.
2008.Academic conference homepage understanding us-ing constrained hierarchical conditional randomfields.
In Proc.
CIKM ?08, pages 1301?1310, NewYork, NY, USA.
ACM.Kai-Hsiang Yang, Shui-Shi Chen, Ming-Tai Hsieh,Hahn-Ming Lee, and Jan-Ming Ho.
2008.
CRE:An automatic citation record extractor for publica-tion list pages.
In Proc.
WMWA?08 of PAKDD-2008,Osaka, Japan, May.Yanhong Zhai and Bing Liu.
2005.
Web data extrac-tion based on partial tree alignment.
In Proc.
WWW?05, pages 76?85, New York, NY, USA.
ACM.Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, andWei-Ying Ma.
2006.
Simultaneous record detec-tion and attribute labeling in web data extraction.In Proc.
KDD ?06, pages 494?503, New York, NY,USA.
ACM.79
