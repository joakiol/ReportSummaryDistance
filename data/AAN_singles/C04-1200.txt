Determining the Sentiment of OpinionsSoo-Min KimInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292-6695skim@isi.eduEduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292-6695hovy@isi.eduAbstractIdentifying sentiments (the affective partsof opinions) is a challenging problem.
Wepresent a system that, given a topic,automatically finds the people who holdopinions about that topic and the sentimentof each opinion.
The system contains amodule for determining word sentimentand another for combining sentimentswithin a sentence.
We experiment withvarious models of classifying andcombining sentiment at word and sentencelevels, with promising results.1 IntroductionWhat is an opinion?The many opinions on opinions are reflectedin a considerable literature (Aristotle 1954;Perelman 1970; Toulmin et al 1979; Wallace1975; Toulmin 2003).
Recent computationalwork either focuses on sentence ?subjectivity?
(Wiebe et al 2002; Riloff et al 2003),concentrates just on explicit statements ofevaluation, such as of films (Turney 2002; Panget al 2002),  or focuses on just one aspect ofopinion, e.g., (Hatzivassiloglou and McKeown1997) on adjectives.
We wish to study opinionin general; our work most closely resemblesthat of (Yu and Hatzivassiloglou 2003).Since an analytic definition of opinion isprobably impossible anyway, we will notsummarize past discussion or try to defineformally what is and what is not an opinion.For our purposes, we describe an opinion as aquadruple [Topic, Holder, Claim, Sentiment] inwhich the Holder believes a Claim about theTopic, and in many cases associates aSentiment, such as good or bad, with the belief.For example, the following opinions containClaims but no Sentiments:?I believe the world is flat?
?The Gap is likely to go bankrupt?
?Bin Laden is hiding in Pakistan?
?Water always flushes anti-clockwise inthe southern hemisphere?Like Yu and Hatzivassiloglou (2003), wewant to automatically identify Sentiments,which in this work we define as an explicit orimplicit expression in text of the Holder?spositive, negative, or neutral regard toward theClaim about the Topic.
(Other sentiments weplan to study later.)
Sentiments always involvethe Holder?s emotions or desires, and may bepresent explicitly or only implicitly:?I think that attacking Iraq would put theUS in a difficult position?
(implicit)?The US attack on Iraq is wrong?
(explicit)?I like Ike?
(explicit)?We should decrease our dependence onoil?
(implicit)?Reps.
Tom Petri and William F.Goodling asserted that counting illegal aliensviolates citizens?
basic right to equalrepresentation?
(implicit)In this paper we address the followingchallenge problem.
Given a Topic (e.g.,?Should abortion be banned??)
and a set oftexts about the topic, find the Sentimentsexpressed about (claims about) the Topic (butnot its supporting subtopics) in each text, andidentify the people who hold each sentiment.To avoid the problem of differentiatingbetween shades of sentiments, we simplify theproblem to: identify just expressions ofpositive, negative, or neutral sentiments,together with their holders.
In addition, forsentences that do not express a sentiment butsimply state that some sentiment(s) exist(s),return these sentences in a separate set.
Forexample, given the topic ?What should be donewith Medicare??
the sentence ?After years ofempty promises, Congress has rolled out twoMedicare prescription plans, one from HouseRepublicans and the other from the DemocraticSentencePOS Taggerverbs nounsAdjectivesAdjective Senti mentclassifiersentiment sentimentSentence sentiment classifierOpinion region + polarity + holderHolder finderNamed EntityTaggerSentenceSentencetexts + topicsentiment sentiment sentimentV rbsVerb Senti mentclassifierNounsNoun Senti mentclassifierWordNetSentence :Figure 1: System architecture.Sens.
Bob Graham of Florida and Zell Miller ofGeorgia?
should be returned in the separate set.We approach the problem in stages, startingwith words and moving on to sentences.
Wetake as unit sentiment carrier a single word, andfirst classify each adjective, verb, and noun byits sentiment.
We experimented with severalclassifier models.
But combining sentimentsrequires additional care, as Table 1 shows.California Supreme Court agreed that the state?snew term-limit law was constitutional.California Supreme Court disagreed that thestate?s new term-limit law was constitutional.California Supreme Court agreed that the state?snew term-limit law was unconstitutional.California Supreme Court disagreed that thestate?s new term-limit law was unconstitutional.Table 1: Combining sentiments.A sentence might even express opinions ofdifferent people.
When combining word-levelsentiments, we therefore first determine foreach Holder a relevant region within thesentence and then experiment with variousmodels for combining word sentiments.We describe our models and algorithm inSection 2, system experiments and discussionin Section 3, and conclude in Section 4.2 AlgorithmGiven a topic and a set of texts, the systemoperates in four steps.
First it selects sentencesthat contain both the topic phrase and holdercandidates.
Next, the holder-based regions ofopinion are delimited.
Then the sentencesentiment classifier calculates the polarity of allsentiment-bearing words individually.
Finally,the system combines them to produce theholder?s sentiment for the whole sentence.Figure 1 shows the overall system architecture.Section 2.1 describes the word sentimentclassifier and Section 2.2 describes the sentencesentiment classifier.2.1 Word Sentiment Classifier2.1.1 Word Classification ModelsFor word sentiment classification wedeveloped two models.
The basic approach isto assemble a small amount of seed words byhand, sorted by polarity into two lists?positiveand negative?and then to grow this by addingwords obtained from WordNet (Miller et al1993; Fellbaum et al 1993).
We assumesynonyms of positive words are mostly positiveand antonyms mostly negative, e.g., thepositive word ?good?
has synonyms ?virtuous,honorable, righteous?
and antonyms ?evil,disreputable, unrighteous?.
Antonyms ofnegative words are added to the positive list,and synonyms to the negative one.To start the seed lists we selected verbs (23positive and 21 negative) and adjectives (15positive and 19 negative), adding nouns later.Since adjectives and verbs are structureddifferently in WordNet, we obtained from itsynonyms and antonyms for adjectives but onlysynonyms for verbs.
For each seed word, weextracted from WordNet its expansions andadded them back into the appropriate seed lists.Using these expanded lists, we extracted anadditional cycle of words from WordNet, toobtain finally 5880 positive adjectives, 6233negative adjectives, 2840 positive verbs, and3239 negative verbs.However, not all synonyms and antonymscould be used: some had opposite sentiment orwere neutral.
In addition, some common wordssuch as ?great?, ?strong?, ?take?, and ?get?occurred many times in both positive andnegative categories.
This indicated the need todevelop a measure of strength of sentimentpolarity (the alternative was simply to discardsuch ambiguous words)?to determine howstrongly a word is positive and also howstrongly it is negative.
This would enable us todiscard sentiment-ambiguous words but retainthose with strengths over some threshold.Armed with such a measure, we can alsoassign strength of sentiment polarity to as yetunseen words.
Given a new word, we useWordNet again to obtain a synonym set of theunseen word to determine how it interacts withour sentiment seed lists.
That is, we compute(1)                 ).....,|(maxarg)|(maxarg21 nccsynsynsyncPwcP?where c is a sentiment category (positive ornegative), w is the unseen word, and synn are theWordNet synonyms of w.  To computeEquation (1), we tried two different models:(2)   )|()(maxarg)|()(maxarg)|()(maxarg)|(maxarg1))(,(...3 2 1?====mkwsynsetfcountkcnccckcfPcPcsynsynsynsynPcPcwPcPwcPwhere fk is the kth feature (list word) ofsentiment class c which is also a member of thesynonym set of w, and count(fk,synset(w)) is thetotal number of occurrences of fk in thesynonym set of w.  P(c) is the number of wordsin class c divided by the total number of wordsconsidered.
This model derives from documentclassification.
We used the synonym andantonym lists obtained from Wordnet instead oflearning word sets from a corpus, since theformer is simpler and does not requiremanually annotated data for training.Equation (3) shows the second model for aword sentiment classifier.
(3))(),()(maxarg)|()(maxarg)|(maxarg1ccountcsyncountcPcwPcPwcPniiccc?===To compute the probability P(w|c) of word wgiven a sentiment class c, we count theoccurrence of w?s synonyms in the list of c.The intuition is that the more synonymsoccuring in c, the more likely the word belongs.We computed both positive and negativesentiment strengths for each word andcompared their relative magnitudes.
Table 2shows several examples of the system output,computed with Equation (2), in which ?+?represents positive category strength and ?-?negative.
The word ?amusing?, for example,was classified as carrying primarily positivesentiment, and ?blame?
as primarily negative.The absolute value of each category representsthe strength of its sentiment polarity.
Forinstance, ?afraid?
with strength -0.99 representsstrong negavitity while ?abysmal?
with strength-0.61 represents weaker negativity.abysmal : NEGATIVE[+ : 0.3811][- : 0.6188]adequate : POSITIVE[+ : 0.9999][- : 0.0484e-11]afraid : NEGATIVE[+ : 0.0212e-04][- : 0.9999]ailing : NEGATIVE[+ : 0.0467e-8][- : 0.9999]amusing : POSITIVE[+ : 0.9999][- : 0.0593e-07]answerable : POSITIVE[+ : 0.8655][- : 0.1344]apprehensible: POSITIVE[+ : 0.9999][- : 0.0227e-07]averse : NEGATIVE[+ : 0.0454e-05][- : 0.9999]blame : NEGATIVE[+ : 0.2530][- : 0.7469]Table 2: Sample output of word sentimentclassifier.2.2 Sentence Sentiment ClassifierAs shows in Table 1, combining sentimentsin a sentence can be tricky.
We are interestedin the sentiments of the Holder about theClaim.
Manual analysis showed that suchsentiments can be found most reliably close tothe Holder; without either Holder orTopic/Claim nearby as anchor points, evenhumans sometimes have trouble reliablydetermining the source of a sentiment.
Wetherefore included in the algorithm steps toidentify the Topic (through direct matching,since we took it as given) and any likelyopinion Holders (see Section 2.2.1).
Near eachHolder we then identified a region in whichsentiments would be considered; anysentiments outside such a region we take to beof undetermined origin and ignore (Section2.2.2).
We then defined several models forcombining the sentiments expressed within aregion (Section 2.2.3).2.2.1 Holder IdentificationWe used BBN?s named entity taggerIdentiFinder to identify potential holders of anopinion.
We considered PERSON andORGANIZATION as the only possible opinionholders.
For sentences with more than oneHolder, we chose the one closest to the Topicphrase, for simplicity.
This is a very crude step.A more sophisticated approach would employ aparser to identify syntactic relationshipsbetween each Holder and all dependentexpressions of sentiment.2.2.2 Sentiment RegionLacking a parse of the sentence, we werefaced with a dilemma: How large should aregion be?
We therefore defined the sentimentregion in various ways (see Table 3) andexperimented with their effectiveness, asreported in Section 3.Window1: full sentenceWindow2: words between Holder and TopicWindow3: window2 ?
2 wordsWindow4: window2 to the end of sentenceTable 3: Four variations of region size.2.2.3 Classification ModelsWe built three models to assign a sentimentcategory to a given sentence, each combiningthe individual sentiments of sentiment-bearingwords, as described above, in a different way.Model 0 simply considers the polarities ofthe sentiments, not the strengths:Model 0: ?
(signs in region)The intuition here is something like?negatives cancel one another out?.
Here thesystem assigns the same sentiment to both ?theCalifornia Supreme Court agreed that thestate?s new term-limit law was constitutional?and ?the California Supreme Court disagreedthat the state?s new term-limit law wasunconstitutional?.
For this model, we alsoincluded negation words such as not and neverto reverse the sentiment polarity.Model 1 is the harmonic mean (average) ofthe sentiment strengths in the region:Model 1:cwcpwcpcnscPijnii== ?=)|(argmax if,)|()(1)|(j1Here n(c) is the number of words in the regionwhose sentiment category is c.  If a regioncontains more and stronger positive thannegative words, the sentiment will be positive.Model 2 is the geometric mean:Model 2:cwcpifwcpscPijniicn=?= ?=?
)|(argmax,)|(10)|(j11)(2.2.4 ExamplesThe following are two example outputs.Public officials throughout California havecondemned a U.S. Senate vote Thursday toexclude illegal aliens from the 1990 census,saying the action will shortchange California inCongress and possibly deprive the state ofmillions of dollars of federal aid for medicalemergency services and other programs for poorpeople.TOPIC : illegal alienHOLDER : U.S. SenateOPINION REGION: vote/NN Thursday/NNPto/TO exclude/VB illegal/JJ aliens/NNS from/INthe/DT 1990/CD census,/NNSENTIMENT_POLARITY: negativeFor that reason and others, the ConstitutionalConvention unanimously rejected term limitsand the First Congress soundly defeated twosubsequent term-limit proposals.TOPIC : term limitHOLDER : First CongressOPINION REGION: soundly/RB defeated/VBDtwo/CD subsequent/JJ term-limit/JJproposals./NNSENTIMENT_POLARITY: negative3 ExperimentsThe first experiment examines the two wordsentiment classifier models and the second thethree sentence sentiment classifier models.3.1 Word Sentiment ClassifierFor test material, we asked three humans toclassify data.
We started with a basic Englishword list for foreign students preparing for theTOEFL test and intersected it with an adjectivelist containing 19748 English adjectives and averb list of 8011 verbs to obtain commonadjectives and verbs.
From this we randomlyselected 462 adjectives and 502 verbs forhuman classification.
Human1 and human2each classified 462 adjectives, and human2 andhuman3 502 verbs.The classification task is defined as assigningeach word to one of three categories: positive,negative, and neutral.3.1.1 Human?Human AgreementAdjectives VerbsHuman1 : Human2 Human1 : Human3Strict 76.19% 62.35%Lenient 88.96% 85.06%Table 4: Inter-human classificationagreement.Table 4 shows inter-human agreement.
Thestrict measure is defined over all threecategories, whereas the lenient measure is takenover only two categories, where positive andneutral have been merged, should we choose tofocus only on differentiating words of negativesentiment.3.1.2 Human?Machine AgreementTable 5 shows results, using Equation (2) ofSection 2.1.1, compared against a baseline thatrandomly assigns a sentiment category to eachword (averaged over 10 iterations).
The systemachieves lower agreement than humans buthigher than the random process.Of the test data, the algorithm classified93.07% of adjectives and 83.27% of verbs aseither positive and negative.
The remainder ofadjectives and verbs failed to be classified,since they did not overlap with the synonym setof adjectives and verbs.In Table 5, the seed list included just a fewmanually selected seed words (23 positive and21 negative verbs and 15 and 19 adjectives,repectively).
We decided to investigate theeffect of more seed words.
After collecting theannotated data, we added half of it (231adjectives and 251 verbs) to the training set,retaining the other half for the test.
As Table 6shows, agreement of both adjectives and verbswith humans improves.
Recall is alsoimproved.Adjective(Train: 231  Test : 231)Verb(Train: 251  Test : 251)Lenient agreement Lenient agreementH1:M H2:MrecallH1:M H3:Mrecall75.66% 77.88% 97.84% 81.20% 79.06% 93.23%Table 6: Results including manual data.3.2 Sentence Sentiment Classifier3.2.1 Data100 sentences were selected from the DUC2001 corpus with the topics ?illegal alien?,?term limits?, ?gun control?, and ?NAFTA?.Two humans annotated the 100 sentences withthree categories (positive, negative, and N/A).To measure the agreement between humans, weused the Kappa statistic (Siegel and CastellanJr.
1988).
The Kappa value for the annotationtask of 100 sentences was 0.91, which isconsidered to be reliable.3.2.2 Test on Human Annotated DataWe experimented on Section 2.2.3?s 3models of sentiment classifiers, using the 4different window definitions and 4 variations ofword-level classifiers (the two word sentimentequations introduced in Section 2.1.1, first withand then without normalization, to compareperformance).Since Model 0 considers not probabilities ofwords but only their polarities, the two word-level classifier equations yield the same results.Consequently, Model 0 has 8 combinations andModels 1 and 2 have 16 each.To test the identification of opinion Holder,we first ran models with holders that wereannotated by humans then ran the same modelswith the automatic holder finding strategies.The results appear in Figures 2 and 3.
Themodels are numbered as follows: m0 throughm4 represent 4 sentence classifier models,Table 5.
Agreement between humans and system.Adjective  (test: 231 adjectives) Verb (test : 251 verbs)Lenient agreement Lenient agreementH1:M H2:MrecallH1:M H3:MrecallRandom selection(average of 10 iterations) 59.35% 57.81% 100% 59.02% 56.59% 100%Basic method 68.37% 68.60% 93.07% 75.84% 72.72% 83.27%p1/p2 and p3/p4 represent the word classifiermodels in Equation (2) and Equation (3) withnormalization and without normalizationrespectively.0.30.40.50.60.70.80.9m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4accuracyWindow 1 Window 2 Window 3 Window 40.30.40.50.60.70.80.9m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4accuracyWindow 1 Window 2 Window 3 Window 4Human 1 : MachineHuman 2 : MachineFigure 2: Results with manually annotatedHolder.0.30.40.50.60.70.80.9m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4accuracyWindow 1 Window 2 Window 3 Window 40.30.40.50.60.70.80.9m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4accuracyWindow 1 Window 2 Window 3 Window 4Human 1 : MachineHuman 2 : MachineFigure 3: Results with automatic Holderdetection.Correctness of an opinion is determinedwhen the system finds both a correct holder andthe appropriate sentiment within the sentence.Since human1 classified 33 sentences positiveand 33 negative, random classification gives 33out of 66 sentences.
Similarly, since human2classified 29 positive and 34 negative, randomclassification gives 34 out of 63 when thesystem blindly marks all sentences as negativeand 29 out of 63 when it marks all as positive.The system?s best model performed at 81%accuracy with the manually provided holderand at 67% accuracy with automatic holderdetection.3.3 Problems3.3.1 Word Sentiment ClassificationAs mentioned, some words have both strongpositive and negative sentiment.
For thesewords, it is difficult to pick one sentimentcategory without considering context.
Second,a unigram model is not sufficient: commonwords without much sentiment alone cancombine to produce reliable sentiment.
Forexample, in ?
?Term limits really hit atdemocracy,?
says Prof.
Fenno?, the commonand multi-meaning word ?hit?
was used toexpress a negative point of view about termlimits.
If such combinations occur adjacently,we can use bigrams or trigrams in the seedword list.
When they occur at a distance,however, it is more difficult to identify thesentiment correctly, especially if one of thewords falls outside the sentiment region.3.3.2 Sentence Sentiment ClassificationEven in a single sentence, a holder mightexpress two different opinions.
Our systemonly detects the closest one.Another difficult problem is that the modelscannot infer sentiments from facts in asentence.
?She thinks term limits will givewomen more opportunities in politics?expresses a positive opinion about term limitsbut the absence of adjective, verb, and nounsentiment-words prevents a classification.Although relatively easy task for people,detecting an opinion holder is not simple either.As a result, our system sometimes picks awrong holder when there are multiple plausibleopinion holder candidates present.
Employinga parser to delimit opinion regions and moreaccurately associate them with potential holdersshould help.3.4 DiscussionWhich combination of models is best?The best overall performance is provided byModel 0.
Apparently, the mere presence ofnegative words is more important thansentiment strength.
For manually tagged holderand topic, Model 0 has the highest singleperformance, though Model 1 averages best.Which is better, a sentence or a region?With manually identified topic and holder,the region window4 (from Holder to sentenceend) performs better than other regions.How do scores differ from manual toautomatic holder identification?Table 7 compares the average results withautomatic holder identification to manuallyannotated holders in 40 different models.Around 7 more sentences (around 11%) weremisclassified by the automatic detectionmethod.positive negative totalHuman1 5.394 1.667 7.060Human2 4.984 1.714 6.698Table 7: Average difference betweenmanual and automatic holder detection.How does adding the neutral sentiment as aseparate category affect the score?It is very confusing even for humans todistinguish between a neutral opinion and non-opinion bearing sentences.
In previousresearch, we built a sentence subjectivityclassifier.
Unfortunately, in most cases itclassifies neutral and weak sentiment sentencesas non-opinion bearing sentences.4 ConclusionSentiment recognition is a challenging anddifficult part of understanding opinions.
Weplan to extend our work to more difficult casessuch as sentences with weak-opinion-bearingwords or sentences with multiple opinionsabout a topic.
To improve identification of theHolder, we plan to use a parser to associateregions more reliably with holders.
We plan toexplore other learning techniques, such asdecision lists or SVMs.Nonetheless, as the experiments show,encouraging results can be obtained even withrelatively simple models and only a smallamount of manual seeding effort.ReferencesAristotle.
The Rhetorics and Poetics (trans.
W.Rhys Roberts), Modern Library, 1954.Fellbaum, C., D. Gross, and K. Miller.
1993.Adjectives in WordNet.
http://www.cosgi.princeton.edu/~wn.Hatzivassiloglou, V. and K. McKeown 1997.Predicting the Semantic Orientation ofAdjectives.
Proceedings of the 35th ACLconference, 174?181.Miller, G.A., R. Beckwith, C. Fellbaum, D.Gross, and K. Miller.
1993.
Introduction toWordNet: An On-Line Lexical Database.http://www.cosgi.princeton.edu/~wn.Pang, B. L. Lee, and S. Vaithyanathan, 2002.Thumbs up?
Sentiment classification usingMachine Learning Techniques.
Proceedingsof the EMNLP conference.Perelman, C. 1970.
The New Rhetoric: ATheory of Practical Reasoning.
In The GreatIdeas Today.
Chicago: EncyclopediaBritannica.Riloff, E., J. Wiebe, and T. Wilson 2003.Learning Subjective Nouns Using ExtractionPattern Bootstrapping.
Proceedings of theCoNLL-03 conference.Siegel, S. and N.J. Castellan Jr. 1988.Nonparametric Statistics for the BehavioralSciences.
McGraw-Hill.Toulmin, S.E., R. Rieke, and A. Janik.
1979.An Introduction to Reasoning.
Macmillan,New York.Toulmin, S.E.
2003.
The Uses of Argument.Cambridge University Press.Turney, P. 2002.
Thumbs Up or ThumbsDown?
Semantic Orientation Applied toUnsupervised Classification of Reviews.Proceedings of the 40th Annual Meeting ofthe ACL, Philadelphia, 417?424.Wallace, K. 1975.
Topoi and the Problem ofInvention.
In W. Ross Winterowd (ed),Contemporary Rhetoric.
Harcourt BraceJovanovich.Wiebe, J. et al 2002.
NRRC summer study JanWiebe and group (University of Pittsburgh)on ?subjective?
statements.Yu, H. and V. Hatzivassiloglou.
2003.
TowardsAnswering Opinion Questions: SeparatingFacts from Opinions and Identifying thePolarity of Opinion Sentences.
Proceedingsof the EMNLP conference.
