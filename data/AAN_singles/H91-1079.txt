Adaptive Natural Language ProcessingRalph WeischedelBBN Systems and Technologies10 Moulton StreetCambridge, MA 02138OBJECTIVESimprovement in error rate resulted.
Therefore, muchless training data than theoretically required provedadequate.The objective of this project is a pilot study of several new 3.ideas for the automatic adaptation and improvement ofnatural anguage processing (NLP) systems.
The effortfocuses particularly on automatically inferring the meaningof new words in context and on developing partialinterpretations of language that is either fragmentary orbeyond the capability of the NLP system to understand.The techniques are being evaluated in a message processingdomain, such as automatic data base update based onarticles from The Wall Street Journal on corporate takeoverbids.
4.The NLP system uses large annotated corpora, such as thosebeing developed under the DARPA-funded TREE-BANKproject at the University of Pennsylvania, to adapt byacquiring syntactic and semantic information from theannotated examples.
Statistical language modeling, basedon probability estimates derived from the large corpora,will provide a means of ranking alternative interpretations 5.of fragments.This pilot study, running from March, 1990 throughMarch, 1991, is designed to test the feasibility of such anew approach.RECENT RESULTSWe have run pilot experiments on the effectiveness ofprobability models for (1) ranking interpretations ofsentences, (2) predicting the part of speech of words, (3)predicting the part of speech of unknown words, and (4)classifying text.
Additionally, we are experimenting withusing unification algorithms to infer properties of anunknown word from examples.1.2.We obtained a reduction in error rate in selecting thecorrect interpretation f a sentence by a factor of twocompared to no model.
A context-free probabilitymodel on supervised training of only 80 sentences wasused in the experiments.Using supervised training with a tri-tag probabilisticmodel, we achieved a 3-5% error rate in picking thecorrect part of speech on a test set including bothknown and unknown words.
As little as 64,000 wordsof supervised training data was used; with 1,000,000words of supervised training, less than a 1%In processing unknown words, the best errors rate onpredicted part of speech as reported in the literature isonly 75%.
Using a tri-gram model, we found an errorrate of 50%.
Adding an estimate of the probability ofa word ending, given the part of speech, reduced theerror rate to 18%.
Adding a probability estimatefactoring in the likelihood of capitalization, given apart of speech, reduced the error rate for unknownwords to 15%.It is well known that a unification parser can processan unknown word by collecting the assumptions itmakes while trying to find an interpretation for asentence.
Adding a context-free probability improvedthe unification predictions of syntactic and semanticproperties of an unknown word, reducing the error rateby a factor of two compared to no model.One set of experiments was classification of MUCarticles into a specific subcategory or not.
We traineda simple classification algorithm to generate aboolean classification tree for each relevantsubeategory, that is, a tree which says whether thearticle is or is not in the category.
Given an article,the different classification trees can be applied to it todetermine which relevant categories the article isrelated to.
In the following results, "recalled" is theprobability that a message in the class would beclassified correctly, and "filtered" is the probabilitythat a message not in the class would be classifiedcorrectly.BOMBINGMURDERKIDNAPARSON100% recalled, 83% filtered87% recalled, 53% filtered76% recalled, 93% filtered97% recalled, 97% filteredPLANS FOR THE COMING YEAR?
Document results in the project final report.405
