2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 773?782,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Translation Sense ClusteringMohit Bansal?UC Berkeleymbansal@cs.berkeley.eduJohn DeNeroGoogledenero@google.comDekang LinGooglelindek@google.comAbstractWe propose an unsupervised method for clus-tering the translations of a word, such thatthe translations in each cluster share a com-mon semantic sense.
Words are assigned toclusters based on their usage distribution inlarge monolingual and parallel corpora usingthe softK-Means algorithm.
In addition to de-scribing our approach, we formalize the taskof translation sense clustering and describe aprocedure that leverages WordNet for evalu-ation.
By comparing our induced clusters toreference clusters generated from WordNet,we demonstrate that our method effectivelyidentifies sense-based translation clusters andbenefits from both monolingual and parallelcorpora.
Finally, we describe a method for an-notating clusters with usage examples.1 IntroductionThe ability to learn a bilingual lexicon from aparallel corpus was an early and influential areaof success for statistical modeling techniques innatural language processing.
Probabilistic wordalignment models can induce bilexical distributionsover target-language translations of source-languagewords (Brown et al, 1993).
However, word-to-wordcorrespondences do not capture the full structure ofa bilingual lexicon.
Consider the example bilingualdictionary entry in Figure 1; in addition to enumerat-ing the translations of a word, the dictionary authorhas grouped those translations into three sense clus-ters.
Inducing such a clustering would prove use-ful in generating bilingual dictionaries automaticallyor building tools to assist bilingual lexicographers.
?Author was a summer intern with Google Research whileconducting this research project.Colocar [co?lo?car?
], va. 1.
To arrange, to put indue place or order.
2.
To place, to put in any place,rank condition or office, to provide a place or em-ployment.
3.
To collocate, to locate, to lay.Figure 1: This excerpt from a bilingual dictionary groupsEnglish translations of the polysemous Spanish word colocarinto three clusters that correspond to different word senses(Vela?zquez de la Cadena et al, 1965).This paper formalizes the task of clustering a setof translations by sense, as might appear in a pub-lished bilingual dictionary, and proposes an unsu-pervised method for inducing such clusters.
We alsoshow how to add usage examples for the translationsense clusters, hence providing complete structureto a bilingual dictionary.The input to this task is a set of source words anda set of target translations for each source word.
Ourproposed method clusters these translations in twosteps.
First, we induce a global clustering of the en-tire target vocabulary using the soft K-Means algo-rithm, which identifies groups of words that appearin similar contexts (in a monolingual corpus) and aretranslated in similar ways (in a parallel corpus).
Sec-ond, we derive clusters over the translations of eachsource word by projecting the global clusters.We evaluate these clusters by comparing them toreference clusters with the overlapping BCubed met-ric (Amigo et al, 2009).
We propose a clustering cri-terion that allows us to derive reference clusters fromthe synonym groups of WordNet R?
(Miller, 1995).1Our experiments using Spanish-English andJapanese-English datasets demonstrate that the au-tomatically generated clusters produced by ourmethod are substantially more similar to the1WordNet is used only for evaluation; our sense clusteringmethod is fully unsupervised and language-independent.773Sense cluster WordNet sense description Usage examplecollocate group or chunk together in a certainorder or place side by sidecolocar juntas todas los libroscollocate all the booksinvest, place, put make an investment capitales para colocarcapital to investlocate, place assign a location to colocar el nu?mero de serielocate the serial numberplace, position, put put into a certain place or abstractlocationcolocar en un lugarput in a placeFigure 2: Correct sense clusters for the translations of Spanish verb s = colocar, assuming that it has translation set Ts ={collocate, invest, locate, place, position, put}.
Only the sense clusters are outputs of the translation sense clustering task; theadditional columns are presented for clarity.WordNet-based reference clusters than naive base-lines.
Moreover, we show that bilingual featurescollected from parallel corpora improve clusteringaccuracy over monolingual distributional similarityfeatures alone.Finally, we present a method for annotating clus-ters with usage examples, which enrich our automat-ically generated bilingual dictionary entries.2 Task DescriptionWe consider a three-step pipeline for generatingstructured bilingual dictionary entries automatically.
(1) The first step is to identify a set of high-qualitytarget-side translations for source lexical items.
Inour experiments, we ask bilingual human annota-tors to create these translation sets.2 We restrict ourpresent study to word-level translations, disallowingmulti-word phrases, in order to leverage existing lex-ical resources for evaluation.
(2) The second step is to cluster translations of eachword according to common word senses.
This clus-tering task is the primary focus of the paper, and weformalize it in this section.
(3) The final step annotates clusters with usage ex-amples to enrich the structure of the output.
Sec-tion 7 describes a method of identifying cluster-specific usage examples.In the task of translation sense clustering, thesecond step, we assume a fixed set of source lexi-cal items of interest S, each with a single part of2We do not use automatically extracted translation sets inour experiments, in order to isolate the clustering task on cleaninput.speech3, and for each s ?
S a set Ts of target trans-lations.
Moreover, we assume that each target wordt ?
Ts has a set of senses in common with s. Thesesenses may also be shared among different targetwords.
That is, each target word may have multiplesenses and each sense may be expressed by multiplewords.Given a translation set Ts, we define a clusterG ?Ts to be a correct sense cluster if it is both coherentand complete.?
A sense cluster G is coherent if and only ifthere exists some sense B shared by all of thetarget words in G.?
A sense clusterG is complete if and only if, forevery sense B shared by all words in G, thereis no other word in Ts but not in G that alsoshares that sense.The full set of correct clusters for a set of translationsconsists of all sense clusters that are both coherentand complete.The example translation set for the Spanish wordcolocar in Figure 2 is shown with four correct senseclusters.
For descriptive purposes, these clusters areannotated by WordNet senses and bilingual usageexamples.
However, the task we have defined doesnot require the WordNet sense or usage exampleto be identified: we must only produce the correctsense clusters within a set of translations.
In fact, acluster may correspond to more than one sense.Our definition of correct sense clusters has sev-eral appealing properties.
First, we do not attemptto enumerate all senses of the source word.
Sense3A noun and verb that share the same word form would con-stitute two different source lexical items.774NotationTs : The set of target-language translations (given)Dt : The set of synsets in which t appears (given)C : A synset; a set of target-language wordsB : A source-specific synset; a subset of TsB : A set of source-specific synsetsG : A set of correct sense clusters for TsThe Cluster Projection Algorithm:B ?
{C ?
Ts : C ?
?t?TsDt}G ?
?for B ?
B doif @B?
?
B such that B ?
B?
thenadd B to Greturn GFigure 3: The Cluster Projection (CP) algorithm projectslanguage-level synsets (C) to source-specific synsets (B) andthen filters the set of synsets for redundant subsets to producethe complete set of source-specific synsets that are both coher-ent and complete (G).distinctions are only made when they affect cross-lingual lexical choice.
If a source word has manyfine-grained senses but translates in the same wayregardless of the sense intended, then there is onlyone correct sense cluster for that translation.Second, no correct sense cluster can be a super-set of another, because the subset would violate thecompleteness condition.
This criterion encourageslarger clusters that are easier to interpret, as theirunifying senses can be identified as the intersectionof senses of the translations in the cluster.Third, the correct clusters need not form a parti-tion of the input translations.
It is common in pub-lished bilingual dictionaries for a translation to ap-pear in multiple sense clusters.
In our example, thepolysemous English verbs place and put appear inmultiple clusters.3 Generating Reference ClustersTo construct a reference set for the translationsense clustering task, we first collected Englishtranslations of Spanish and Japanese nouns, verbs,and adverbs.
Translation sets were curated by hu-man annotators to keep only high-quality single-word translations.Rather than gathering reference clusters via an ad-ditional annotation effort, we leverage WordNet, alarge database of English lexical semantics (Miller,1995).
WordNet groups words into sets of cogni-Synsetscollocate collocate, lump, chunkinvest, put, commit, place invest, clothe, adorn invest, vest, enthrone ?locate, turn up situate, locate locate, place, site ?put, set, place, pose, position, lay rate, rank, range, order, grade, place locate, place, site invest, put, commit, place?position put, set, place, pose, position, layput, set, place, pose, position, lay put frame, redact, cast, put, couch invest, put, commit, place ?WordscollocateinvestlocateplacepositionputSense Clusterscollocateinvest, place, putlocate, placeplace, position, putFigure 4: An example of cluster projection on WordNet, for theSpanish source word colocar.
We show the target translationwords to be clustered, their WordNet synsets (with words not inthe translation set grayed out), and the final set of correct senseclusters.tive synonyms called synsets, each expressing a dis-tinct concept.
We use WordNet version 2.1, whichhas wide coverage of nouns, verbs, and adverbs, butsparser coverage of adjectives and prepositions.4Reference clusters for the set of translations Tsof some source word s are generated algorithmi-cally from WordNet synsets via the Cluster Projec-tion (CP) algorithm defined in Figure 3.
An inputto the CP algorithm is the translation set Ts of somesource word s. Also, each translation t ?
Ts be-longs to some set of synsets Dt, where each synsetC ?
Dt contains target-language words that mayor may not be translations of s. First, the CP algo-rithm constructs a source-specific synset B for eachC, which contains only translations of s. Second,it identifies all correct sense clusters G that are bothcoherent and complete with respect to the source-specific senses B.
A sense cluster must correspondto some synset B ?
B to be coherent, and it must4WordNet version 2.1 is almost identical to ver-sion 3.0, for Unix-like systems, as described inhttp://wordnetcode.princeton.edu/3.0/CHANGES.
The lat-est version 3.1 is not yet available for download.775not have a proper superset in B to be complete.5Figure 4 illustrates the CP algorithm for the trans-lations of the Spanish source word colocar that ap-pear in our input dataset.4 Clustering with K-MeansIn this section, we describe an unsupervised methodfor inducing translation sense clusters from the us-age statistics of words in large monolingual and par-allel corpora.
Our method is language independent.4.1 Distributed SoftK-Means ClusteringAs a first step, we cluster all words in the target-language vocabulary in a way that relates words thathave similar distributional features.
Several methodsexist for this task, such as the K-Means algorithm(MacQueen, 1967), the Brown algorithm (Brownet al, 1992) and the exchange algorithm (Kneserand Ney, 1993; Martin et al, 1998; Uszkoreit andBrants, 2008).
We use a distributed implementa-tion of the ?soft?
K-Means clustering algorithm de-scribed in Lin and Wu (2009).
Given a feature vec-tor for each element (a word type) and the numberof desired clusters K, the K-Means algorithm pro-ceeds as follows:1.
Select K elements as the initial centroids forK clusters.repeat2.
Assign each element to the top M clusterswith the nearest centroid, according to a simi-larity function in feature space.3.
Recompute each cluster?s centroid by aver-aging the feature vectors of the elements in thatcluster.until convergence4.2 Monolingual FeaturesFollowing Lin and Wu (2009), each word to be clus-tered is represented as a feature vector describing thedistributional context of that word.
In our setup, the5One possible shortcoming of our approach to constructingreference sets for translation sense clustering is that a clustermay correspond to a sense that is not shared by the originalsource word used to generate the translation set.
All translationsmust share some sense with the source word, but they may notshare all senses with the source word.
It is possible that twotranslations are synonymous in a sense that is not shared by thesource.
However, we did not observe this problem in practice.context of a word w consists of the words immedi-ately to the left and right of w. The context featurevector of w is constructed by first aggregating thefrequency counts of each word f in the context ofeach w. We then compute point-wise mutual infor-mation (PMI) features from the frequency counts:PMI(w, f) = logc(w, f)c(w)c(f)where w is a word, f is a neighboring word, andc(?)
is the count of a word or word pair in the cor-pus.6 A feature vector for w contains a PMI featurefor each word type f (with relative position left orright) for all words that appears a sufficient numberof times as a neighbor of w. The similarity of twofeature vectors is the cosine of the angle between thevectors.
We follow Lin and Wu (2009) in applyingvarious thresholds during K-Means, such as a fre-quency threshold for the initial vocabulary, a total-count threshold for the feature vectors, and a thresh-old for PMI scores.4.3 Bilingual FeaturesIn addition to the features described in Lin and Wu(2009), we introduce features from a bilingual par-allel corpus that encode reverse-translation informa-tion from the source-language (Spanish or Japanesein our experiments).
We have two types of bilin-gual features: unigram features capture source-sidereverse-translations ofw, while bigram features cap-ture both the reverse-translations and source-sideneighboring context words to the left and right.
Fea-tures are expressed again as PMI computed fromfrequency counts of aligned phrase pairs in a par-allel corpus.
For example, one unigram feature forplace would be the PMI computed from the numberof times that place was in the target side of a phrasepair whose source side was the unigram lugar.
Sim-ilarly, a bigram feature for place would be the PMIcomputed from the number of times that place wasin the target side of a phrase pair whose source sidewas the bigram lugar de.
These features characterizethe way in which a word is translated, an indicationof its meaning.6PMI is typically defined in terms of probabilities, but hasproven effective previously when defined in terms of counts.7764.4 Predicting Translation ClustersAs a result of softK-Means clustering, each word inthe target-language vocabulary is assigned to a list ofup to M clusters.
To predict the sense clusters for aset of translations of a source word, we apply the CPalgorithm (Figure 3), treating the K-Means clustersas synsets (Dt).5 Related WorkTo our knowledge, the translation sense clusteringtask has not been explored previously.
However,much prior work has explored the related task ofmonolingual word and phrase clustering.
Uszkor-eit and Brants (2008) uses an exchange algorithmto cluster words in a language model, Lin and Wu(2009) uses distributed K-Means to cluster phrasesfor various discriminative classification tasks, Vla-chos et al (2009) uses Dirichlet Process MixtureModels for verb clustering, and Sun and Korhonen(2011) uses a hierarchical Levin-style clustering tocluster verbs.Previous word sense induction work (Diab andResnik, 2002; Kaji, 2003; Ng et al, 2003; Tufiset al, 2004; Apidianaki, 2009) relates to our workin that these approaches discover word senses au-tomatically through clustering, even using multilin-gual parallel corpora.
However, our task of clus-tering multiple words produces a different type ofoutput from the standard word sense induction taskof clustering in-context uses of a single word.
Theunderlying notion of ?sense?
is shared across thesetasks, but the way in which we use and evaluate in-duced senses is novel.6 ExperimentsThe purpose of our experiments is to assess whetherour unsupervised soft K-Means clustering methodcan effectively recover the reference sense clustersderived from WordNet.6.1 DatasetsWe conduct experiments using two bilingualdatasets: Spanish-to-English (S?E) and Japanese-to-English (J?E).
Table 1 shows, for each dataset,the number of source words and the total numberof target words in their translation sets.
The datasetsDataset No.
of src-words Total no.
of tgt-wordsS?E 52 230J?E 369 1639Table 1: Sizes of the Spanish-to-English (S?E) and Japanese-to-English (J?E) datasets.are limited in size because we solicited human anno-tators to filter the set of translations for each sourceword.
The S?E dataset has 52 source-words with apart-of-speech-tag distribution of 38 nouns, 10 verbsand 4 adverbs.
The J?E dataset has 369 source-words with 319 nouns, 38 verbs and 12 adverbs.
Weincluded only these parts of speech because Word-Net version 2.1 has adequate coverage for them.Most source words have 3 to 5 translations each.Monolingual features for K-Means clusteringwere computed from an English corpus of Webdocuments with 700 billion tokens of text.
Bilin-gual features were computed from 0.78 (S?E) and1.04 (J?E) billion tokens of parallel text, primar-ily extracted from the Web using automated paral-lel document identification (Uszkoreit et al, 2010).Word alignments were induced from the HMM-based alignment model (Vogel et al, 1996), initial-ized with the bilexical parameters of IBM Model 1(Brown et al, 1993).
Both models were trained us-ing 2 iterations of the expectation maximization al-gorithm.
Phrase pairs were extracted from alignedsentence pairs in the same manner used in phrase-based machine translation (Koehn et al, 2003).6.2 Clustering Evaluation MetricsThe quality of text clustering algorithms can be eval-uated using a wide set of metrics.
For evaluationby set matching, the popular measures are Purity(Zhao and Karypis, 2001) and Inverse Purity andtheir harmonic mean (F measure, see Van Rijsber-gen (1974)).
For evaluation by counting pairs, thepopular metrics are the Rand Statistic and JaccardCoefficient (Halkidi et al, 2001; Meila, 2003).Metrics based on entropy include Cluster Entropy(Steinbach et al, 2000), Class Entropy (Bakus et al,2002), VI-measure (Meila, 2003), Q0 (Dom, 2001),V-measure (Rosenberg and Hirschberg, 2007) andMutual Information (Xu et al, 2003).
Lastly, thereexist the BCubed metrics (Bagga and Baldwin,1998), a family of metrics that decompose the clus-777tering evaluation by estimating precision and recallfor each item in the distribution.Amigo et al (2009) compares the various clus-tering metrics mentioned above and their properties.They define four formal but intuitive constraints onsuch metrics that explain which aspects of clusteringquality are captured by the different metric families.Their analysis shows that of the wide range of met-rics, only BCubed satisfies those constraints.
Afterdefining each constraint below, we briefly describeits relevance to the translation sense clustering task.Homogeneity: In a cluster, we should not mix itemsbelonging to different categories.Relevance: All words in a proposed cluster shouldshare some common WordNet sense.Completeness: Items belonging to the same cate-gory should be grouped in the same cluster.Relevance: All words that share some commonWordNet sense should appear in the same cluster.Rag Bag: Introducing disorder into a disorderedcluster is less harmful than introducing disorder intoa clean cluster.Relevance: We prefer to maximize the number oferror-free clusters, because these are most easily in-terpreted and therefore most useful.Cluster Size vs.
Quantity: A small error in a bigcluster is preferable to a large number of small er-rors in small clusters.Relevance: We prefer to minimize the total numberof erroneous clusters in a dictionary.Amigo et al (2009) also show that BCubed ex-tends cleanly to settings with overlapping clusters,where an element can simultaneously belong tomore than one cluster.
For these reasons, we focuson BCubed for cluster similarity evaluation.7The BCubed metric for scoring overlapping clus-ters is computed from the pair-wise precision andrecall between pairs of items:P(e, e?)
=min(|C(e) ?
C(e?
)|, |L(e) ?
L(e?
)|)|C(e) ?
C(e?
)|R(e, e?)
=min(|C(e) ?
C(e?
)|, |L(e) ?
L(e?
)|)|L(e) ?
L(e?
)|where e and e?
are two items, L(e) is the set of ref-erence clusters for e and C(e) is the set of predicted7An evaluation using purity and inverse purity (extended tooverlapping clusters) has been omitted for space, but leads tothe same conclusions as the evaluation using BCubed.clusters for e (i.e., clusters to which e belongs).
Notethat P(e, e?)
is defined only when e and e?
sharesome predicted cluster, and R(e, e?)
when e and e?share some reference cluster.The BCubed precision associated to one item is itsaveraged pair-wise precision over other items shar-ing some of its predicted clusters, and likewise forrecall8; and the overall BCubed precision (or recall)is the averaged precision (or recall) of all items:PB3 = Avge[Avge?s.t.C(e)?C(e?)6=?
[P(e, e?
)]]RB3 = Avge[Avge?s.t.L(e)?L(e?)6=?
[R(e, e?
)]]6.3 ResultsFigure 5 shows the F?-score for various ?
values:F?
=(1 + ?2) ?
PB3 ?
RB3?2 ?
PB3 + RB3This graph gives us a trade-off between precisionand recall (?
= 0 is exact precision and ?
?
?tends to exact recall).9Each curve in Figure 5 represents a particularclustering method.
We include three naive baselines:ewnc: Each word in its own clusteraw1c: All words in one clusterRandom: Each target word is assigned M randomcluster id?s in the range 1 to K, then translationsets are clustered with the CP algorithm.The curves for K-Means clustering include onecondition with monolingual features alone and twocurves that include bilingual features as well.10 Thebilingual curves correspond to two different featuresets: the first includes only unigram features (t1),while the second includes both unigram and bigramfeatures (t1t2).Each point on an F?
curve in Figure 5 (includingthe baseline curves) represents a maximum over two8The metric does include in this computation the relation ofeach item with itself.9Note that we use the micro-averaged version of F-scorewhere we first compute PB3 and RB3 for each source-word,then compute the average PB3 and RB3 over all source-words,and finally compute the F-score using these averaged PB3 andRB3.10All bilingual K-Means experiments include monolingualfeatures also.
K-Means with only bilingual features does notproduce accurate clusters.7780.650.70.750.80.850.90.950.5 1 1.5 2 2.5 3 3.5 4 4.5 5F !
score!Spanish-English BCubed Resultsewnc aw1c Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t20.70.750.80.850.90.950.5 1 1.5 2 2.5 3 3.5 4 4.5 5F !
score!Japanese-English BCubed Resultsewnc aw1c Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2Figure 5: BCubed F?
plot for the Spanish-English dataset (top) and Japanese-English dataset (bottom).Source word: ayudarMonolingual [[aid], [assist, help]] P=1.0, R=0.56Bilingual [[aid, assist, help]] P=1.0, R=1.0Source word: concursoMonolingual [[competition, contest, match], [concourse], [contest, meeting]] P=0.58, R=1.0Bilingual [[competition, contest], [concourse], [match], [meeting]] P=1.0, R=1.0Table 2: Examples showing improvements in clustering when we move from K-Means clustering with only monolingual featuresto clustering with additional bilingual features.7790.790.840.890.940.990.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0RecallPrecisionJapanese-English BCubed ResultsRandom Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 ewnc aw1cFigure 6: BCubed Precision-Recall scatter plot for the Japanese-English dataset.
Each point represents a particular choice of clustercount K and clusters per word M .parameters: K, the number of clusters created in thewhole corpus andM , the number of clusters allowedper word (in M -best soft K-Means).
As both therandom baseline and proposed clustering methodscan be tuned to favor precision or recall, we showthe best result from each technique across this spec-trum of F?
metrics.
We vary ?
to highlight differentpotential objectives of translation sense clustering.An application that focuses on synonym discoverywould favor recall, while an application portrayinghighly granular sense distinctions would favor pre-cision.Clustering accuracy improves over the baselineswith monolingual features alone, and it improvesfurther with the addition of bilingual features, for awide range of ?
values.
Our unsupervised approachwith bilingual features achieves up to 6-8% absoluteimprovement over the random baseline, and is par-ticularly effective for recall-weighted metrics.11 Asan example, in a S?E experiment with a K-Meanssetting ofK = 4096 : M = 3, the overall F1.5 score11It is not surprising that a naive baseline like random clus-tering can achieve a high precision: BCubed counts each worditself as correctly clustered, and so even trivial techniques thatcreate many singleton clusters will have high precision.
Highrecall (without very low precision) is harder to achieve, becauseit requires positing larger clusters, and it is for recall-focusedobjectives that our technique substantially outperforms the ran-dom baseline.increases from 80.58% to 86.12% upon adding bilin-gual features.
Table 2 shows two examples from thatexperiment for which bilingual features improve theoutput clusters.The parameter values we use in our experimentsare K ?
{23, 24, .
.
.
, 212} and M ?
{1, 2, 3, 4, 5}.To provide additional detail, Figure 6 shows theBCubed precision and recall for each induced clus-tering, as the values of K and M vary, for Japanese-English.12 Each point in this scatter plot represents aclustering methodology and a particular value for Kand M .
Soft K-Means with bilingual features pro-vides the strongest performance across a broad rangeof cluster parameters.6.4 Evaluation DetailsCertain special cases needed to be addressed in orderto complete this evaluation.Target words not in WordNet: Words that did nothave any synset in WordNet were each assigned to asingleton reference cluster.13 The S?E dataset hasonly 2 out of 225 target types missing in WordNetand the J?E dataset has only 55 out of 1351 target12Spanish-English precision-recall results are omitted due tospace constraints, but depict similar trends.13Note that certain words with WordNet synsets also end upin their own singleton cluster because all other words in theircluster are not in the translation set.780types missing.Target words not clustered by K-Means: The K-Means algorithm applies various thresholds duringdifferent parts of the process.
As a result, thereare some target word types that are not assignedany cluster at the end of the algorithm.
For ex-ample, in the J?E experiment with K = 4096and with bilingual (t1 only) features, only 49 outof 1351 target-types are not assigned any cluster byK-Means.
These unclustered words were each as-signed to a singleton cluster in post-processing.7 Identifying Usage ExamplesWe now briefly consider the task of automaticallyextracting usage examples for each predicted clus-ter.
We identify these examples among the extractedphrase pairs of a parallel corpus.Let Ps be the set of source phrases containingsource word s, and letAt be the set of source phrasesthat align to target phrases containing target wordt.
For a source word s and target sense cluster G,we identify source phrases that contain s and trans-late to all words in G. That is, we collect the setof phrases Ps ??t?GAt.
We use the same parallelcorpus as we used to compute bilingual features.For example, if we consider the cluster [place, po-sition, put] for the Spanish word colocar, then wefind Spanish phrases that contain colocar and alsoalign to English phrases containing place, position,and put somewhere in the parallel corpus.
Sampleusage examples extracted by this approach appear inFigure 7.
We have not performed a quantitative eval-uation of these extracted examples, although quali-tatively we have found that the technique surfacesuseful phrases.
We look forward to future researchthat further explores this important sub-task of auto-matically generating bilingual dictionaries.8 ConclusionWe presented the task of translation sense clustering,a critical second step to follow translation extractionin a pipeline for generating well-structured bilingualdictionaries automatically.
We introduced a methodof projecting language-level clusters into clusters forspecific translation sets using the CP algorithm.
Weused this technique both for constructing referenceclusters, via WordNet synsets, and constructing pre-debajo["below","beneath"]    ?
debajo de la superficie (below the surface)["below","under"]     ?
debajo de la l?nea (below the line)["underneath"]     ?
debajo de la piel (under the skin)??
["break"]     ?
????
??
?
??
??
??
?
?
??
??
.
(I worked hard and I deserve a good break.
)["recreation"]     ?
??
?
??
?
??
??
(Traditional healing and recreation activities)["rest"]     ?
???
?
??
??
??
?
??
??
.
(Bed rest is the only treatment required.)??
["application"]     ?
???????
??
??
(Computer-aided technique)["use","utilization"]     ?
??
?
??
??
?
??
??
(Promote effective use of land)??
["draw","pull"]     ?
????
?
??
(Draw the curtain)["subtract"]     ?
A ??
B ?
??
(Subtract B from A)["tug"]     ?
?
?
???
??
(Tug at someone's sleeve)Figure 7: Usage examples for Spanish and Japanese words andtheir English sense clusters.
Our approach extracts multipleexamples per cluster, but we show only one.
We also showthe translation of the examples back into English produced byGoogle Translate.dicted clusters from the output of a vocabulary-levelclustering algorithm.Our experiments demonstrated that the soft K-Means clustering algorithm, trained using distribu-tional features from very large monolingual andbilingual corpora, recovered a substantial portion ofthe structure of reference clusters, as measured bythe BCubed clustering metric.
The addition of bilin-gual features improved clustering results over mono-lingual features alone; these features could proveuseful for other clustering tasks as well.
Finally, weannotated our clusters with usage examples.In future work, we hope to combine our cluster-ing method with a system for automatically gen-erating translation sets.
In doing so, we will de-velop a system that can automatically induce high-quality, human-readable bilingual dictionaries fromlarge corpora using unsupervised learning methods.AcknowledgmentsWe would like to thank Jakob Uszkoreit, AdamPauls, and the anonymous reviewers for their helpfulsuggestions.781ReferencesEnrique Amigo, Julio Gonzalo, Javier Artiles, and FelisaVerdejo.
2009.
A comparison of extrinsic clusteringevaluation metrics based on formal constraints.
Infor-mation Retrieval, 12(4):461486.Marianna Apidianaki.
2009.
Data-driven semantic anal-ysis for multilingual WSD and lexical selection intranslation.
In Proceedings of EACL.A.
Bagga and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector space model.In Proceedings of COLING-ACL.J.
Bakus, M. F. Hussin, and M. Kamel.
2002.
A SOM-based document clustering using phrases.
In Proceed-ings of ICONIP.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467479.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel corpora.In Proceedings of ACL.B.E.
Dom.
2001.
An information-theoretic externalcluster-validity measure.
In IBM Technical Report RJ-10219.M.
Halkidi, Y. Batistakis, and M. Vazirgiannis.
2001.
Onclustering validation techniques.
Journal of IntelligentInformation Systems, 17(2-3):107?145.Hiroyuki Kaji.
2003.
Word sense acquisition from bilin-gual comparable corpora.
In Proceedings of NAACL.Reinherd Kneser and Hermann Ney.
1993.
Improvedclustering techniques for class-based statistical lan-guage modelling.
In Proceedings of the 3rd EuropeanConference on Speech Communication and Technol-ogy.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of NAACL.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of ACL.J.
B. MacQueen.
1967.
Some methods for classifica-tion and analysis of multivariate observations.
In Pro-ceedings of 5th Berkeley Symposium on MathematicalStatistics and Probability.Sven Martin, Jorg Liermann, and Hermann Ney.
1998.Algorithms for bigram and trigram word clustering.Speech Communication, 24:19?37.M.
Meila.
2003.
Comparing clusterings by the variationof information.
In Proceedings of COLT.George A. Miller.
1995.
Wordnet: A lexical database forEnglish.
In Communications of the ACM.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.
Ex-ploiting parallel texts for word sense disambiguation:An empirical study.
In Proceedings of ACL.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clusterevaluation measure.
In Proceedings of EMNLP.Michael Steinbach, George Karypis, and Vipin Kumar.2000.
A comparison of document clustering tech-niques.
In Proceedings of KDD Workshop on TextMining.Lin Sun and Anna Korhonen.
2011.
Hierarchical verbclustering using graph factorization.
In Proceedingsof EMNLP.Dan Tufis, Radu Ion, and Nancy Ide.
2004.
Fine-grainedword sense disambiguation based on parallel corpora,word alignment, word clustering and aligned word-nets.
In Proceedings of COLING.Jakob Uszkoreit and Thorsten Brants.
2008.
Distributedword clustering for large scale class-based languagemodeling in machine translation.
In Proceedings ofACL.Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-biner.
2010.
Large scale parallel document mining formachine translation.
In Proceedings of COLING.C.
Van Rijsbergen.
1974.
Foundation of evaluation.Journal of Documentation, 30(4):365?373.Mariano Vela?zquez de la Cadena, Edward Gray, andJuan L. Iribas.
1965.
New Revised Vela?zques Spanishand English Dictionary.
Follet Publishing Company.Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-mani.
2009.
Unsupervised and constrained Dirichletprocess mixture models for verb clustering.
In Pro-ceedings of the Workshop on Geometrical Models ofNatural Language Semantics.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the Conference on Computa-tional linguistics.W.
Xu, X. Liu, and Y. Gong.
2003.
Document-clusteringbased on non-negative matrix factorization.
In Pro-ceedings of SIGIR.Y.
Zhao and G. Karypis.
2001.
Criterion functions fordocument clustering: Experiments and analysis.
InTechnical Report TR 01-40, Department of ComputerScience, University of Minnesota, Minneapolis, MN.782
