A New Chinese Natural Language Understanding ArchitectureBased on Multilayer Search MechanismWanxiang Che Ting Liu Sheng LiSchool of Computer Science and TechnologyHarbin Institute of TechnologyP.O.
Box 321, HITHarbin China, 150001{car, tliu, ls}@ir.hit.edu.cnAbstractA classical Chinese Natural Language Under-standing (NLU) architecture usually includesseveral NLU components which are executedwith some mechanism.
A new Multilayer SearchMechanism (MSM) which integrates and quan-tifies these components into a uniform multi-layer treelike architecture is presented in thispaper.
The mechanism gets the optimal re-sult with search algorithms.
The componentsin MSM affect each other.
At last, the per-formance of each component is enhanced.
Webuilt a practical system ?
CUP (Chinese Under-standing Platform) based on MSM with threelayers.
By the experiments on Word Segmen-tation, a better performance was achieved.
Intheory the normal cascade and feedback mech-anism are just some special cases of MSM.1 IntroductionAt present a classical Chinese NLU architec-ture usually includes several components, suchas Word Segmentation (Word-Seg), POS Tag-ging, Phrase Analysis, Parsing, Word Sense Dis-ambiguation (WSD) and so on.
These compo-nents are executed one by one from lower layers(such as Word-Seg, POS Tagging) to higher lay-ers (such as Parsing, WSD) to form a kind ofcascade mechanism.
But when people build aNLU system based on these complex languageanalysis, it is a very serious problem since theerrors of each layer component are multiplied.With more and more analysis components, thefinal result becomes too bad to be applicable.Another problem is that the components inthe system affect each other when people build apractical but toy NLU system.
Here the toy sys-tem means that each component is ideal enoughwith perfect input.
But in fact, on the one handthe lower layer components need the informa-tion of higher layer components; on the otherhand the incorrect analysis of lower layers mustreduce the accuracy of higher layers.
In ChineseWord-Seg component, many segmentation am-biguities which cannot be solved using only lexi-cal information.
In order to improve the perfor-mance of Word-Seg, we have to use some syntaxand even semantic information.
Without cor-rect Word-Seg results, however the syntax andsemantic parser cannot obtain a correct analy-sis.
It is a chain debts problem.People have tried to solve the error-multipliedproblem by integrating multi-layers into a uni-form model (Gao et al, 2001; Nagata, 1994).But with the increasing number of integratedlayers, the model becomes too complex to buildor solve.The feedback mechanism (Wu and Jiang,1998) helps to use the information of high lay-ers to control the final result.
If the analysisat feedback point cannot be passed, the wholeanalysis will be denied.
This mechanism placestoo much burden on the function of feedbackpoint.
This leads to the problems that a correctlower layer result may be rejected or an errorresult may be accepted.We propose a new Multilayer Search Mecha-nism (MSM) to solve the problems mentionedabove.
Based on the mechanism, we build apractical Chinese NLU platform ?
CUP (Chi-nese Understanding Platform).
Section 2 intro-duces the background and architecture of thenew mechanism and how to build it up.
Exper-imental results with CUP is given in Section 3.In Section 4, we discuss why the new mechanismgets better results than the old ones.
Conclu-sions and the some future work follow in Sec-tion 5.2 Multilayer Search MechanismThe novel Multilayer Search Mechanism (MSM)integrates and quantifies NLU components intoa uniform multilayer treelike platform, such asWord-Seg, POS Tagging, Parsing and so on.These components affect each other by comput-ing the final score and then get better results.2.1 BackgroundConsidering a Chinese sentence, the sen-tence analysis task can be formally definedas finding a set of word segmentation se-quence (W ), a POS tagging sequence (POS),a syntax dependency parsing tree (DP ) andso on which maximize their joint probabilityP (W,POS,DP, ?
?
?).
In this paper, we assumethat there are only three layers W , POS andDP in MSM.
It is relatively straightforward,however, to extend the method to the case forwhich there are more than three layers.
There-fore, the sentence analysis task can be describedas finding a triple < W,POS,DP > that max-imize the joint probability P (W,POS,DP ).< W,POS,DP >= arg maxW,POS,DPP (W,POS,DP )The joint probability distributionP (W,POS,DP ) can be written in the fol-lowing form using the chain rule of probability:P (W,POS,DP )=P (W )P (POS|W )P (DP |W,POS)Where P (W ) is considered as the probabil-ity of the word segmentation layer, P (POS|W )is the conditional probability of POS Tag-ging with a given word segmentation result,P (DP |W,POS) is the conditional probabilityof a dependency parsing tree with a given wordsegmentation and POS Tagging result similarly.So the form of < W,POS,DP > can be trans-formed into:< W,POS,DP >= arg maxW,POS,DPP (W,POS,DP )= arg maxW,POS,DPP (W )P (POS|W )P (DP |W,POS)= arg maxW,POS,DPlogP (W ) + logP (POS|W )+ logP (DP |W,POS)= arg minW,POS,DP?
logP (W )?
logP (POS|W )?
logP (DP |W,POS)We consider that each inversion of probability?slogarithm at the last step of the above equationis a score given by a component (Such as Word-Seg, POS Tagging and so on).
So at last, we findan n-tuple < W,POS,DP, ?
?
?
> that minimizesthe last score Sn of a sentence analysis resultwith n layers.
Sn is defined as:Sn = s1 + s2 + ?
?
?+ sn (1)si denotes the score of the ith layer compo-nent.2.2 The Architecture of MultilayerSearch MechanismBecause there are lots of analysis results at eachlayer, it?s a combinatorial explosion problem tofind the optimal result.
Assuming that eachcomponent produces m results for an input onaverage and there are n layers in a NLU system,the final search space is mn.
With the increas-ing of n, it?s impossible for a system to find theoptimal result in the huge search space.The classical cascade mechanism uses agreedy algorithm to solve the problem.
It onlykeeps the optimal result at each layer.
But ifit?s a fault analysis result for the optimal resultat a layer, it?s impossible for this mechanism tofind the final correct analysis result.To overcome the difficulty, we build a newMultilayer Search Mechanism (MSM).
Differentfrom the cascade mechanism, MSM maintains anumber of results at each component, so thatthe correct analysis should be included in theseresults with high probability.
Then MSM triesto use the information of all layer componentsto find out the correct analysis result.
Differentfrom the feedback mechanism, the acceptanceof an analysis is not based on a higher layercomponents alone.
The lower layer componentsprovide some information to help to find thecorrect analysis result as well.According to the above idea, we design thearchitecture of MSM with multilayer treelikestructure.
The original input is root and theseveral analysis results of the input becomebranches.
Iterating this progress, we get a big-ger analysis tree.
Figure 1 gives an analysis ex-ample of a Chinese sentence ???????????
(He likes beautiful flowers).
For the in-put sentence, there are several Word-Seg resultswith scores (the lower the better).
Then for eachof Word-Seg results, there are several POS Tag-ging results, too.
And for each of POS Taggingresult, the same thing happens.
So we get a bigtree structure and the correct analysis result is apath in the tree from the root to the leaf exceptfor there is no correct analysis result in someanalysis components.A search algorithm can be used to find out thecorrect analysis result among the lowest score inthe tree.
But because each layer cannot give theexact score in Equation 1 as the standard scoreand the ability of analysis are different withdifferent layers, we should weight every score.Then the last score is the linear weighted sum(Equation 2).Sn = w1s1 + w2s2 + ?
?
?+ wnsn (2)si denotes the score of the ith layer compo-nent which we will introduce in Section 3; widenotes the weight of the ith layer componentswhich we will introduce in the next section.In order to get the optimal result, all kindsof tree search algorithms can be used.
Herethe BEST-FIRST SEARCH Algorithm (Rus-sell and Norvig, 1995) is used.
Figure 2 showsthe main algorithm steps.POS TagWord-Seg???????????56.2?
??
??
?
???
87.3?
?
??
?
?
???108.3?
??
??
?
???
r      v        a      u      n187.4?
??
??
?
???
b      v        a      u      nFigure 1: An Example of Multilayer SearchMechanism1.
Add the initial node (starting point) to thequeue.2.
Compare the front node to the goal state.
Ifthey match then the solution is found.3.
If they do not match then expand the frontnode by adding all the nodes from its links.4.
If all nodes in the queue are expanded then thegoal state is not found (e.g.there is no solution).Stop.5.
According to Equation 2 evaluate the score ofexpanded nodes and reorder the nodes in thequeue.6.
Go to step 2.Figure 2: BEST-FIRST SEARCH Algorithm2.3 Layer WeightWe should find out a group of appropriatew1, w2, ?
?
?
, wn in Equation 2 to maximize thenumber of the optimal paths in MSM which canget the correct results.
They are expressed byW ?.W ?
= arg maxWObjFun(minSn) (3)Here W ?
is named as Whole Layer Weight.ObjFun(?)
denotes a function to value the re-sult that a group ofW can get.
Here we can con-sider that the performance of each layer is pro-portional to the last performance of the wholesystem in MSM.
So it maybe the F-Score ofWord-Seg, precision of POS Tagging and so on.minSn returns the optimal analysis results withthe lowest score.Here, the F-Score of Word-Seg can be definedas the harmonic mean of recall and precision ofWord-Seg.
That is to say:Seg.F -Score = 2 ?
Seg.Pre ?
Seg.RecSeg.Pre+ Seg.RecSeg.Pre = #words correctly segmented#words segmentedSeg.Rec = #words correctly segmented#words in input textsFinding out the most suitable group of Wis an optimization problem.
Genetic Algo-rithms (GAs) (Mitchell, 1996) is just an adap-tive heuristic search algorithm based on the evo-lutionary ideas of natural selection and geneticsto solve optimization problems.
It exploits his-torical information to direct the search into theregion of better performance within the searchspace.To use GAs to solve optimization prob-lems (Wall, 1996) the following three questionsshould be answered:1.
How to describ genome?2.
What is the objective function?3.
Which controlling parameters to be se-lected?A solution to a problem is represented as agenome.
The genetic algorithm then creates apopulation of solutions and applies genetic op-erators such as mutation and crossover to evolvethe solutions in order to find the best one(s) af-ter several generations.
The numbers of popula-tion and generation are given by controlling pa-rameters.
The objective function decides whichsolution is better than others.In MSM, the genome is just the group of Wwhich can be denoted by real numbers between0 and 1.
Because the result is a linear weightedsum, we should normalize the weights to let w1+w2+ ?
?
?+wn = 1.
The objective function is justObjFun(?)
in Equation 3.
Here the F-Scoreof Word-Seg is used to describe it.
We set thegenetic generations as 10 and the populations inone generation as 30.
The Whole Layer Weightshows in the row of WLW in Table 4.
The F-Score of Word-Seg shows as Table 3.We can see that the Word-Seg layer gets anobviously large weight.
So the final result isinclined to the result of Word-Seg.2.4 Self ConfidenceOur analysis indicates that the method ofweighting a whole layer uniformly cannot re-flect the individual information of each sen-tence to some component.
So the F-Score ofWord-Seg drops somewhat comparing with us-ing Only Word-Seg.
For example, the mostsentences which have ambiguities in Word-Segcomponent are still weighted high with Word-Seg layer weight.
Then the final result may stillbe the same as the result of Word-Seg compo-nent.
It is ambiguous, too.
So we must use aparameter to decrease the weight of a compo-nent with ambiguity.
It is used to describe theanalysis ability of a component for an input.
Wename it as Self Confident (SC) of a component.It is described by the difference between the firstand the second score of a component.
Then thebigger SC of a component, the larger weight ofit.There are lots of methods to value the differ-ence between two numbers.
So there are manykinds of definitions of SC.
We use A and B todenote the first and the second score of a compo-nent respectively.
Then the SC can be definedas B?A, BA and so on.
We must select the bet-ter one to represent SC.
The better means thata method which gets a lower Error Rate with athreshold t?
which gets the Minimal Error Rate.t?
= arg mintErrRate(t)ErrRate(t) denotes the Error Rate with thethreshold t. An error has two definitions:?
SC is higher than t but the first result isfault?
SC is lower than t but the first result isrightThen the Error Rate is the ratio between theerror number and the total number of sentences.Table 2 is the comparison list between differ-ent definitions of SC and their Minimal ErrorRate of Word-Seg.
By this table we select B?Aas the last SC because it gets the minimal Min-imal Error Rate within the different definitionsof SC.SC is added into Equation 2 to describe theindividual information of each sentence inten-sively.
Equation 4 shows the new score methodof a path.Sn = w1sc1s1 + w2sc2s2 + ?
?
?+ wnscnsn (4)sci denotes the SC of a component in the ithlayer.3 Experimental Results3.1 Score of ComponentsWe build a practical system CUP (ChineseUnderstanding Platform) based on MSM withthree layers ?
Word-Seg, POS Tagging andParsing.
Each component not only provides then-best analysis result, but also the score of eachresult.In the Word-Seg component, we use the uni-gram model (Liu et al, 1998) to value differentresults of Word-Seg.
So the score of a result is:ScoreWord?Seg = ?
logP (W ) = ?
?logP (wi)wi denotes the ith word in the Word-Seg re-sult of a sentence.In the POS Tagging component the classicalMarkov Model (Manning and Schu?tze, 1999) isused to select the n-best POS results of eachWord-Seg result.
So the score of a result is:ScorePOS =?
logP (POS|W )=?
log P (W |POS)P (POS)P (W )=?
?logP (wi|ti)?
?logP (ti|ti?1)+ logP (W )ti denotes the POS of the ith word in a Word-Seg result of a sentence.In the Parsing component, we use a ChineseDependency Parser System developed by HIT-IRLab1.
The score of a result is:ScoreParsing =?
logP (DP |W,POS)=?
log P (W,POS,DP )P (W,POS)=?
?logP (lij)+ logP (W,POS)lij denotes a link between the ith and jthword in a Word-Seg and POS Tagging resultof a sentence.Table 1 gives the one and five-best results ofeach component with a correct input.
The testdata comes from Beijing Univ.
and Fujitsu Chi-nese corpus (Huiming et al, 2000).
The F-Scoreis used to value the performance of the Word-Seg, Precision to POS Tagging and the correctrate of links to Parsing.Table 1: The five-best results of each compo-nent1-best 5-bestWord-Seg 87.83% 94.45%POS Tag 85.34% 93.28%Parsing 80.25% 82.13%3.2 Self Confidence SelectionIn order to select a better SC, we test all kinds ofdefinition form to calculate their Minimal ErrorRate.
For example B?A, BA and so on.
A and Bdenote the first and the second score of a com-ponent respectively.
Table 2 shows the relation-ship between definition forms of SC and theirMinimal Error Rate.
Here, we experimentedwith the first and the second Word-Seg resultsof more than 7100 Chinese sentences.3.3 F-Score of Word-SegThe result of Word-Seg is used to test oursystem?s performance, which means that theObjFun(?)
returns the F-Score of Word-Seg.There are 1,500 sentences as training dataand 500 sentences as test data.
Among thesedata about 10% sentences have ambiguities andthe others come from Beijing Univ.
and Fujitsu1The Parser has not been published still.Chinese corpus (Huiming et al, 2000).
In CUPthe five-best results of each component are se-lected.
Table 3 lists the F-Score of Word-Seg.They use Only Word-Seg (OWS), Whole LayerWeight (WLW), SC (SC) and FeedBack mecha-nism (FB) separately.
Using the feedback mech-anism means that the last analysis result of asentence is decided by the Parsing.
We selectthe result which has the lowest score of Pars-ing.
Table 4 shows the weight distributions inWLW and SC weighting methods.3.4 The Efficiency of CUPThe efficiency test of CUP was done with 7112sentences with 20 Chinese characters averagely.It costs 58.97 seconds on a PC with PIV 2.0CPU and 512M memory.
The average cost of asentence is 0.0083 second.4 DiscussionsAccording to Table 1, we can see that the per-formance of each component improved with theincreasing of the number of results.
But at thesame time, the processing time must increase.So we should balance the efficiency and effec-tiveness with an appropriate number of results.Thus, it?s more possible for CUP to find out thecorrect analysis than the original cascade mech-anism if we can invent an appropriate method.We define SC as B ?
A which gets the mini-mal Minimal Error Rate with the analysis of theTable 2: SC and Minimal Error RateDefinition Form of SC MinimalError Rate1A ?
1B 23.85%B ?A 21.07%BA 23.98%BA ?
AB 23.98%B?Alength of a sentence 24.12%B?Alength of a sentence+100 23.71%Table 3: F-Score of Word-SegOWS WLW SC FBF-Score 86.99% 85.80% 88.13% 80.72%Table 4: Layer Weight1-layer 2-layer 3-layerIn WLW 0.84 0.12 0.04In SC 0.44 0.40 0.16Table 2.
Take the case of Word Segmentation:B ?A =?ilogP (wAi )?
?jlogP (wBj )It?s just the difference between logarithms ofdifferent word results?
probability of the firstand the second result of Word Segmentation.Table 3 shows that MSM using SC gets a bet-ter performance than other methods.
For a Chi-nese sentence ???????????.
(Thereare some drinks under the table).
The CUPgets the correct analysis ?
??
?/n ?/nd ?/v?/u ?/m ?/q ?/n ?/w?.
But the cascadeand feedback mechanism?s result is ??
?/n ?
?/v ?/u ?/m ?/q ?/n ?/w?.The cascade mechanism uses the Only Word-Seg result.
In this method P (??)
is morethan P (?)
?
P (?).
At the same time, thewrong analysis is a grammatical sentence andis accepted by Parsing.
These create that thesetwo mechanisms cannot get the correct result.But the MSM synthesizes all the information ofWord-Seg, POS Tagging and Parsing.
Finallyit gets the correct analysis result.Now, CUP integrates three layers and its effi-ciency is high enough for practical applications.5 Conclusions and Future WorkA new Chinese NLU architecture based on Mul-tilayer Search Mechanism (MSM) integrates al-most all of NLU components into a uniformmultilayer treelike platform and quantifies thesecomponents to use the search algorithm to findout the optimal result.
Thus any componentcan be added into MSM conveniently.
Theyonly need to accept an input and give severaloutputs with scores.
By experiments we can seethat a practical system ?
CUP based on MSMimproves the performance of Word-Seg to a cer-tain extent.
And its efficiency is high enough formost practical applications.The cascade and the feedback mechanism areJUST the special cases of MSM.
If greedy algo-rithm is used at each layer to expand the resultwith the lowest score, MSM becomes the cas-cade mechanism.
If the weight of each layerexcept the feedback point is set 0, the MSM be-comes the feedback mechanism.In the future we are going to add the PhraseAnalysis, WSD (Word Sense Disambiguation)and Semantic Analysis components into CUP,because it is impossible to analyze some sen-tences correctly without semantic understand-ing and the Phrase Analysis helps to en-hance the performance of Parsing.
At last,CUP becomes a whole Chinese NLU platformwith Word-Seg, POS Tagging, Phrase Analy-sis, Parsing, WSD and Semantic Analysis, sixcomponents from lower layers to higher layers.Under the framework of MSM, it becomes veryeasy to add these components.With the increasing of layers the handle speedmust decrease.
So some heuristic search algo-rithms will be used to improve the speed ofsearching while enhancing the speed of eachcomponent.
Under the MSM framework, we cando these easily.The performance of each component shouldbe improved in the future.
At least, it is impos-sible for MSM to find out the correct analysisresult if there is a component which cannot givea correct result within n-best results with a cor-rect input.
In addition, we are going to evalu-ate the performance of each component not justWord-Seg only.6 AcknowledgementsWe thank Liqi Gao and Zhuoran Wang providethe Word Segmentation tool, Wei He providethe POS Tagging tool and Jinshan Ma providethe Parser tool for us.
We acknowledge DekangLin for his valuable comments on the earlier ver-sions of this paper.
This work was supported byNSFC 60203020.ReferencesShan Gao, Yan Zhang, Bo Xu, ChengQingZong, ZhaoBing Han, and RangShen Zhang.2001.
The research on integrated chinesewords segmentation and labeling based on tri-gram statistic model.
In Proceedings of IJCL-2001, Tai Yuan, Shan Xi, China.Duan Huiming, Song Jing, Xu Guowei,Hu Guoxin, and Yu Shiwen.
2000.
The de-velopment of a large-scale tagged chinese cor-pus and its applications.
Applied Linguistics,(2):72?77.Ting Liu, Yan Wu, and Kaizhu Wang.
1998.The problem and algorithm of maximal prob-ability word segmentation.
Journal of HarbinInstitute of Technology, 30(6):37?41.Christopher D. Manning and Hinrich Schu?tze.1999.
Foundations of Statistical Natural Lan-guage Processing.
The MIT Press, Cam-bridge, Massachusetts.Melanie Mitchell.
1996.
An Introduction toGenetic Algorithms.
The MIT Press, Cam-bridge, Massachusetts.Masaaki Nagata.
1994.
A stochastic japanesemorphological analyzer using a forward-dpbackward-A* n-best search algorithm.
InProceedings of the 15th International Con-ference on Computational Linguistics, pages201?207.Stuart Russell and Peter Norvig.
1995.
Artifi-cial Intelligence: A Modern Approach.
Pren-tice Hall Series in Artificial Intelligence, En-glewood Cliffs, NJ, USA.Matthew Wall.
1996.
GAlib: A C++ Li-brary of Genetic Algorithms components.http://lancet.mit.edu/ga/.Andi Wu and Zixin Jiang.
1998.
Word segmen-tation in sentence analysis.
In Proceedingsof the 1998 International Conference on Chi-nese Information Processing, pages 169?180,Beijing, China.
