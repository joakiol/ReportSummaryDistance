Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 552?561,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsSemantic Topic Models: Combining Word Distributional Statistics andDictionary DefinitionsWeiwei GuoDepartment of Computer Science,Columbia University,weiwei@cs.columbia.eduMona DiabCenter for Computational Learning Systems,Columbia University,mdiab@ccls.columbia.eduAbstractIn this paper, we propose a novel topicmodel based on incorporating dictionarydefinitions.
Traditional topic models treatwords as surface strings without assumingpredefined knowledge about word mean-ing.
They infer topics only by observingsurface word co-occurrence.
However, theco-occurred words may not be semanti-cally related in a manner that is relevantfor topic coherence.
Exploiting dictionarydefinitions explicitly in our model yieldsa better understanding of word semanticsleading to better text modeling.
We exploitWordNet as a lexical resource for sensedefinitions.
We show that explicitly mod-eling word definitions helps improve per-formance significantly over the baselinefor a text categorization task.1 IntroductionLatent Dirichlet Allocation (LDA) (Blei et al,2003) serves as a data-driven framework in model-ing text corpora.
The statistical model allows vari-able extensions to integrate linguistic features suchas syntax (Griffiths et al, 2005), and has been ap-plied in many areas.In LDA, there are two factors which determinethe topic of a word: the topic distribution of thedocument, and the probability of a topic to emitthis word.
This information is learned in an unsu-pervised manner to maximize the likelihood of thecorpus.
However, this data-driven approach hassome limitations.
If a word is not observed fre-quently enough in the corpus, then it is likely tobe assigned the dominant topic in this document.For example, the word grease (a thick fatty oil) ina political domain document should be assignedthe topic chemicals.
However, since it is an in-frequent word, LDA cannot learn its correct se-mantics from the observed distribution, the LDAmodel will assign it the dominant document topicpolitics.
If we look up the semantics of the wordgrease in a dictionary, we will not find any of itsmeanings indicating the politics topic, yet there isample evidence for the chemical topic.
Accord-ingly, we hypothesize that if we know the seman-tics of words in advance, we can get a better in-dication of their topics.
Therefore, in this paper,we test our hypothesis by exploring the integrationof word semantics explicitly in the topic modelingframework.In order to incorporate word semantics fromdictionaries, we recognize the need to modelsense-topic distribution rather than word-topic dis-tribution, since dictionaries are constructed at thesense level.
We use WordNet (Fellbaum, 1998)as our lexical resource of choice.
The notion ofa sense in WordNet goes beyond a typical wordsense in a traditional dictionary since a WordNetsense links senses of different words that havesimilar meanings.
Accordingly, the sense for thefirst verbal entry for buy and for purchase willhave the same sense id (and same definition) inWordNet, while they could have different mean-ing definitions in a traditional dictionary such asthe Merriam Webster Dictionary or LDOCE.
Inour model, a topic will first emit a WordNet sense,then the sense will generate a word.
This is in-spired by the intuition that words are instantiationsof concepts.The paper is organized as follows: In Sections 2and 3, we describe our models based on WordNet.In Section 4, experiment results on text catego-rization are presented.
Moreover, we analyze bothqualitatively and quantitatively the contribution ofmodeling definitions (by teasing out the contribu-tion of explicit sense modeling in a word sense dis-ambiguation task).
Related work is introduced inSection 5.
We conclude in Section 6 by discussingsome possible future directions.552d(a)dT Ssensen ?/Nsends(b)Figure 1: (a) LDA: Latent Dirichlet Allocation(Blei et al, 2003).
(b) STM: Semantic topicmodel.
The dashed arrows indicate the distribu-tions (?
and ?)
and nodes (z) are not influenced bythe values of pointed nodes.2 Semantic Topic Model2.1 Latent Dirichlet AllocationWe briefly introduce LDA where Collapsed GibbsSampling (Griffiths and Steyvers, 2004) is usedfor inference.
In figure 1a, given a corpus withD documents, LDA will summarize each docu-ment as a normalized T -dimension topic mixture?.
Topic mixture ?
is drawn from a Dirichlet distri-bution Dir(?)
with a symmetric prior ?.
?
con-tains T multinomial distribution, each represent-ing the probability of a topic z generating word wp(w|z).
?
is drawn from a Dirichlet distributionDir(?)
with prior ?.In Collapsed Gibbs Sampling, the distributionof a topic for the word wi = w based on values ofother data is computed as:P (zi = z|z?i,w) ?n(d)?i,z + ?n(d)?i + T??
nw?i,z + ?n?i,z +W?
(1)In this equation, n(d)?i,z is a count of how manywords are assigned topic z in document d, exclud-ing the topic of the ith word; nw?i,z is a count ofhow many words = w are assigned topic z, alsoexcluding the topic of the ith word.
Hence, thefirst fraction is the proportion of the topic in thisdocument p(z|?).
The second fraction is the prob-ability of topic z emitting wordw.
After the topicsbecome stable, all the topics in a document con-struct the topic mixture ?.2.2 Applying Word Sense DisambiguationTechniquesWe add a sense node between the topic node andthe word node based on two linguistic observa-tions: a) Polysemy: many words have more thanone meaning.
A topic is more directly relevant toa word meaning (sense) than to a word due to pol-ysemy; b) Synonymy: different words may sharethe same sense.
WordNet explicitly models syn-onymy by linking synonyms to the same sense.
InWordNet, each sense has an associated definition.It is worth noting that we model the sense-wordrelation differently from (Boyd-Graber and Blei,2007), where in their model words are generatedfrom topics, then senses are generated from words.In our model, we assume that during the genera-tive process, the author picks a concept relevant tothe topic, then thinks of a best word that representsthat concept.
Hence the word choice is dependenton the relatedness of the sense and its fit to thedocument context.In standard topic models, the topic of a wordis sampled from the document level topic mixture?.
The underlying assumption is that all words in adocument constitute the context of the target word.However, it is not the case in real world corpora.Titov and McDonald (2008) find that using globaltopic mixtures can only extract global topics in on-line reviews (e.g., Creative Labs MP3 players andiPods) and ignores local topics (product featuressuch as portability and battery).
They design theMulti-grain LDA where the local topic of a wordis only determined by topics of surrounding sen-tences.
In word sense disambiguation (WSD), aneven narrower context is taken into consideration,for instance in graph based WSD models (Mihal-cea, 2005), the choice of a sense for a word onlydepends on a local window whose size equals thelength of the sentence.
Later in (Sinha and Mihal-cea, 2007; Guo and Diab, 2010; Li et al, 2010),people use a fixed window size containing around12 neighbor words for WSD.Accordingly, we adopt the WSD inspired localwindow strategy in our model.
However, we do553not employ the complicated schema in (Titov andMcDonald, 2008).
We simply hypothesize that thesurrounding words are semantically related to theconsidered word, and they construct a local slid-ing window for that target word.
For a documentd with Nd words, we represent it as Nd local win-dows ?
a window is created for each word.
Themodel is illustrated in the left rectangle in figure1b.
The window size is fixed for each word: itcontains /2 preceding words, and /2 followingwords.
Therefore, a word in the original documentwill have  copies, existing in +1 local windows.Similarly, there are  + 1 pairs of topics/sensesassigned for each word in the original document.Each window has a distribution ?i over topics.
?iwill emit the topics of words in the window.This approach enables us to exploit differentcontext sizes without restricting it to the sentencelength, and hence spread topic information acrosssentence boundaries.2.3 Integrating DefinitionsIntuitively, a sense definition reveals some priorknowledge on the topic domain: the definition ofsense [crime, offense, offence] indicates a legaltopic; the definition of sense [basketball] indicatesa sports topic, etc.
Therefore, during inference, wewant to choose a topic/sense pair for each word,such that the topic is supported by the context ?and the sense definition also matches that topic.Given that words used in the sense definitionsare strongly relevant to the sense/concept, we setout to find the topics of those definition words, andaccordingly assign the sense sen itself these top-ics.
We treat a sense definition as a document andperform Gibbs sampling on it.
We normalize def-inition length by a variable ?.
Therefore, beforethe topic model sees the actual documents, eachsense s has been sampled ?
times.
The ?
topicsare then used as a ?training set?, so that given asense, ?
has some prior knowledge of which topicit should be sampled from.Consider the sense [party, political party] witha definition ?an organization to gain politicalpower?
of length 6 when ?
= 12.
If topicmodel assigns politics topic to the words ?orga-nization political power?, then sense [party, polit-ical party] will be sampled from politics topic for3 ?
?/definitionLength = 6 times.We refer to the proposed model as SemanticTopic Model (figure 1b).
For each window vi inthe document set, the model will generate a distri-bution of topics ?i.
It will emit the topics of + 1words in the window.
For a word wij in windowvi, a sense sij is drawn from the topic, and then sijgenerates the word wi.
Sense-topic distribution ?contains T multinomial distributions over all pos-sible senses in the corpus drawn from a symmetricDirichlet distribution Dir(?).
From WordNet weknow the set of words W (s) that have a sense sas an entry.
A sense s can only emit words fromW (s).
Hence, for each sense s, there is a multi-nomial distribution ?s over W (s).
All ?
are drawnfrom symmetric Dir(?
).On the definition side, we use a different prior?s to generate a topic mixture ?.
Aside from gen-erating si, zi will deterministically generate thecurrent sense sen for ?/Nsen times (Nsen is thenumber of words in the definition of sense sen),so that sen is sampled ?
times in total.The formal procedure of generative process isthe following:For the definition of sense sen:?
choose topic mixture ?
?
Dir(?s).?
for each word wi:?
choose topic zi ?Mult(?).?
choose sense si ?Mult(?zi).?
deterministically choose sense sen ?Mult(?zi) for ?/Nsen times.?
choose word wi ?Mult(?si).For each window vi in a document:?
choose local topic mixture ?i ?
Dir(?d).?
for each word wij in vi:?
choose topic zij ?Mult(?i).?
choose sense sij ?Mult(?zij ).?
choose word wij ?Mult(?sij ).2.4 Using WordNetSince definitions and documents are in differentgenre/domains, they have different distributionson senses and words.
Besides, the definition setscontain topics from all kinds of domains, many ofwhich are irrelevant to the document set.
Hencewe prefer ?
and ?
that are specific for the doc-ument set, and we do not want them to be ?cor-rupted?
by the text in the definition set.
There-fore, as in figure 1b, the dashed lines indicate thatwhen we estimate ?
and ?, the topic/sense pair andsense/word pairs in the definition set are not con-sidered.WordNet senses are connected by relations suchas synonymy, hypernymy, similar attributes, etc.554We observe that neighboring sense definitions areusually similar and are in the same topic domain.Hence, we represent the definition of a sense asthe union of itself with its neighboring sense def-initions pertaining to WordNet relations.
In thisway, the definition gets richer as it considers moredata for discovering reliable topics.3 InferenceWe still use Collapsed Gibbs Sampling to find la-tent variables.
Gibbs Sampling will initialize allhidden variables randomly.
In each iteration, hid-den variables are sequentially sampled from thedistribution conditioned on all the other variables.In order to compute the conditional probabilityP (zi = z, si = s|z?i, s?i,w) for a topic/sensepair, we start by computing the joint probabilityP (z, s,w) = P (z)P (s|z)P (w|s).
Since the gen-erative processes are not exactly the same for def-initions and documents, we need to compute thejoint probability differently.
We use a type spe-cific subscript to distinguish them: Ps(?)
for sensedefinitions and Pd(?)
for documents.Let sen be a sense.
Integrating out ?
we have:Ps(z) =(?(T?s)?
(?s)T)S S?sen=1?z ?
(n(sen)z + ?s)?
(n(sen) + T?)
(2)where n(sen)z means the number of times a wordin the definition of sen is assigned to topic z, andn(sen) is the length of the definition.
S is all thepotential senses in the documents.We have the same formula of P (s|z) andP (w|s) for definitions and documents.
Similarly,let nz be the number of words in the documentsassigned to topic z, and nsz be the number of timessense s assigned to topic z.
Note that when sappears in the superscript surrounded by bracketssuch as n(s)z , it denotes the number of words as-signed to topics z in the definition of sense s. Byintegrating out ?
we obtain the second term:P (s|z) =(?(S?)?(?
)S)T T?z=1?s ?
(nsz + n(s)z ?/n(s) + ?)?
(nz +?s?
n(s?
)z ?/n(s?)
+ S?
)(3)At last, assume ns denotes the number of senses in the documents, and nws denotes the number ofsense s to generate the word w, then integratingout ?
we have:P (w|s) =S?s=1?
(|W (s)|?)?(?
)|W (s)|?W (s)w ?
(nws + ?)?
(ns + |W (s)|?
)(4)With equation 2-4, we can compute the condi-tional probability Ps(zi = z, si = s|z?i, s?i,w)for a sense-topic pair in the sense definition.
Letseni be the sense definition containing word wi,then we have:Ps(zi = z, si = s|z?i, s?i,w) ?n(seni)?i,z + ?sn(seni)?i + T?snsz + n(s?)?i,z?/n(s?)
+ ?nz +?s?
n(s?)?i,z?/n(s?)
+ S?nws + ?ns + |W (s)|?
(5)The subscript ?i in expression n?i denotesthe number of certain events excluding word wi.Hence the three fractions in equation 5 correspondto the probability of choosing z from ?sen, choos-ing s from z and choosingw from s. Also note thatour model defines s that can only generate wordsin W (s), therefore for any word w /?
W (s), thethird fraction will yield a 0.The probability for documents is similar to thatfor definitions except that there is a topic mixturefor each word, which is estimated by the topics inthe window.
Hence Pd(z) is estimated as:Pd(z) =?i?(T?d)?
(?d)T?z ?
(n(vi)z + ?d)?
(n(vi) + T?d)(6)Thus, the conditional probability for documentscan be estimated by cancellation terms in equation6, 3, and 4:Pd(zij = z, sij = s|z?ij, s?ij,w) ?n(vi)?ij,z + ?dn(vi)?ij + T?dns?ij,z + n(s?
)z ?/n(s?)
+ ?n?ij,z +?s?
n(s?
)z ?/n(s?)
+ S?nw?ij,s + ?n?ij,s + |W (s)|?
(7)3.1 ApproximationIn current model, each word appears in + 1 win-dows, and will be generated  + 1 times, so therewill be  + 1 pairs of topics/senses sampled foreach word, which requires a lot of additional com-putation (proportional to context size ).
On theother hand, it can be imagined that the set of val-ues {zij , sij |j ?
/2 ?
i ?
j + /2} in dif-ferent windows vi should roughly be the same,since they are hidden values for the same wordwj .Therefore, to reduce computation complexity dur-ing Gibbs sampling, we approximate the values of{zij , sij | i 6= j} by the topic/sense (zjj , sjj) thatare generated from window vj .
That is, in Gibbssampling, the algorithm does not actually samplethe values of {zij , sij , | i 6= j}; instead, it directlyassumes the sampled values are zjj , sjj .5554 Experiments and AnalysisData: We experiment with several datasets,namely, the Brown Corpus (Brown), New YorkTimes (NYT) from the American National Cor-pus, Reuters (R20) and WordNet definitions.
In apreprocessing step, we remove all the non-contentwords whose part of speech tags are not one ofthe following set {noun, adjective, adverb, verb}.Moreover, words that do not have a valid lemma inWordNet are removed.
For WordNet definitions,we remove stop words hence focusing on relevantcontent words.Corpora statistics after each step of preprocess-ing is presented in Table 1.
The column WN tokenlists the number of word#pos tokens after prepro-cessing.
Note that now we treat word#pos as aword token.
The column word types shows cor-responding word#pos types, and the total numberof possible sense types is listed in column sensetypes.
The DOCs size for WordNet is the totalnumber of senses defined in WordNet.Experiments: We design two tasks to test ourmodels: (1) text categorization task for evaluat-ing the quality of values of topic nodes, and (2) aWSD task for evaluating the quality of the valuesof the sense nodes, mainly as a diagnostic tool tar-geting the specific aspect of sense definitions in-corporation and distinguish that component?s con-tribution to text categorization performance.
Wecompare the performance of four topic models.
(a) LDA: the traditional topic model proposed in(Blei et al, 2003) except that it uses Gibbs Sam-pling for inference.
(b) LDA+def: is LDA withsense definitions.
However they are not explic-itly modeled; rather they are treated as documentsand used as augmented data.
(c) STM0: the topicmodel with an additional explicit sense node in themodel, but we do not model the sense definitions.And finally (d) STMn is the full model with defi-nitions explicitly modeled.
In this setting n is the?
value.
We experiment with different ?
valuesin the STM models, and investigate the semanticscope of words/senses by choosing different win-dow size .
We report mean and standard deviationbased on 10 runs.It is worth noting that a larger window size suggests documents have larger impact on themodel (?, ?)
than definitions, since each documentword has  copies.
This is not a desirable propertywhen we want to investigate the weight of defi-nitions by choosing different ?
values.
Accord-ingly, we only use zjj , sjj , wjj to estimate ?, ?, sothat the impact of documents is fixed.
This makesmore sense, in that after the approximation in sec-tion 3.1, there is no need to use {zij , sij , | i 6= j}(they have the same values as zjj , sjj).4.1 Text CategorizationWe believe our model can generate more ?correct?topics by looking into dictionaries.
In topic mod-els, each word is generalized as a topic and eachdocument is summarized as the topic mixture ?,hence it is natural to evaluate the quality of in-ferred topics in a text categorization task.
We fol-low the classification framework in (Griffiths etal., 2005): first run topic models on each datasetindividually without knowing label informationto achieve document level topic mixtures, then weemploy Naive Bayes and SVM (both implementedin the WEKA Toolkit (Hall et al, 2009)) to per-form classification on the topic mixtures.
For alldocument, the features are the percentage of top-ics.
Similar to (Griffiths et al, 2005), we assess in-ferred topics by the classification accuracy of 10-fold cross validation on each dataset.We evaluate our models on three datasets in thecross validation manner: The Brown corpus whichcomprises 500 documents grouped into 15 cate-gories (same set used in (Griffiths et al, 2005));NYT comprising 800 documents grouped into the16 most frequent label categories; Reuters (R20)comprising 8600 documents labeled with the mostfrequent 20 categories.
In R20, combination ofcategories is treated as separate category labels,so money, interest and interest are considereddifferent labels.For the three datasets, we use the Brown cor-pus only as a tuning set to decide on the topicmodel parameters for all of our experimentation,and use the optimized parameters directly on NYTand R20 without further optimization.4.1.1 Classification ResultsSearching ?
and  on Brown: The classificationaccuracy on the Brown corpus with different  and?
values using Naive Bayes and SVM are pre-sented in figure 2a and 2b.
In this section, thenumber of topics T is set to 50.
The possible values in the horizontal axis are 2, 10, 20, 40,all.
The possible ?
values are 0, 1, 2.
Note that = all means that no local window is used, and?
= 0 means definitions are not used.
The hyper-556Corpus DOCs size orig tokens content tokens WN tokens word types sense typesBrown 500 1022393 580882 547887 27438 46645NYT 800 743665 436988 393120 19025 37631R20 8595 901691 450935 417331 9930 24834SemCor 352 676546 404460 352563 28925 45973WordNet 117659 1447779 886923 786679 42080 60567Table 1: Corpus statistics0 10 20 30 40 50 all40455055606570window sizeaccuracy%LDALDA+defSTM0STM1STM2(a) Naive Bayes on Brown0 10 20 30 40 50 all4045505560657075window sizeaccuracy%LDALDA+defSTM0STM1STM2(b) SVM on Brown0 10 20 30 40 50 all556065707580window sizeaccuracy%STM0STM1STM2(c) SVM on NYTFigure 2: Classification accuracy at different parameter settingsparameters are tuned as ?d = 0.1, ?s = 0.01, ?
=0.01, ?
= 0.1.From figure 2, we observe that results usingSVM have the same trend as Naive Bayes exceptthat the accuracies are roughly 5% higher for SVMclassifier.
The results of LDA and LDA+def sug-gest that simply treating definitions as documentsin an augmented data manner does not help.
Com-paring SMT0 with LDA in the same  values, wefind that explicitly modeling the sense node in themodel greatly improves the classification results.The reason may be that words in LDA are inde-pendent isolated strings, while in STM0 they areconnected by senses.STM2 prefers smaller window sizes ( less than40).
That means two words with a distance largerthan 40 are not necessarily semantically related orshare the same topic.
This  number also corre-lates with the optimal context window size of 12reported in WSD tasks (Sinha and Mihalcea, 2007;Guo and Diab, 2010).Classification results: Table 2 shows the resultsof our models using best tuned parameters of  =10, ?
= 2 on 3 datasets.
We present three base-lines in Table 2: (1) WEKA uses WEKA?s classi-fiers directly on bag-of-words without topic mod-eling.
The values of features are simply term fre-quency.
(2) WEKA+FS performs feature selectionusing information gain before applying classifica-tion.
(3) LDA, is the traditional topic model.
Notethat Griffiths et al?s (2005) implementation ofLDA achieve 51% on Brown corpus using NaiveBayes .
Finally the Table illustrates the resultsobtained using our proposed models STM0 (?=0)and STM2 (?
= 2).It is worth noting that R20 (compared to NYT)is a harder condition for topic models.
This isbecause fewer words (10000 distinct words ver-sus 19000 in NYT) are frequently used in a largetraining set (8600 documents versus 800 in NYT),making the surface word feature space no longeras sparse as in the NYT or Brown corpus, whichimplies simply using surface words without con-sidering the words distributional statistics ?
topicmodeling ?
is good enough for classification.
In(Blei et al, 2003) figure 10b they also show worsetext categorization results over the SVM baselinewhen more than 15% of the training labels ofReuters are available for the SVM classifiers, indi-cating that LDA is less necessary with large train-ing data.
In our investigation, we report resultson SVM classifiers trained on the whole Reuterstraining set.
In our experiments, LDA fails to cor-rectly classify nearly 10% of the Reuters docu-ments compared to the WEKA baseline, howeverSTM2 can still achieve significantly better accu-racy (+4%) in the SVM classification condition.Table 2 illustrates that despite the difference be-tween NYT, Reuters and Brown (data size, genre,domains, category labels), exploiting WSD tech-niques (namely using a local window size cou-pled with explicitly modeling a sense node) yields557Brown NYT R20NB SVM NB SVM NB SVMWEKA 48 47.8 57 54.1 72.4 82.9WEKA+FS 50 47.2 56.9 55.1 72.9 83.4LDA 47.8?4.3 53.9?3.8 48.5?5.5 53.8?3.5 61.0?3.3 72.5?2.5STM0 68.6?3.5 70.7?3.9 66.7?3.8 74.2?4.0 72.7?3.5 85.2?0.9STM2 69.3?3.3 75.4?3.7 74.6?3.3 79.3?2.5 73?3.7 86.9?1.2Table 2: Classification results on 3 datasets using hyperparameters tuned on Brown.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 165060708090100accuracy%STM0STM21.Sports  2.Politics  3.National News  4.Entertainment  5.International News6.Society  7.Business  8.Miscellaneous  9.Finance  10.Culture  11.Science12.Health  13.Law  14.Technology  15.Religion  16.EnvironmentFigure 3: SVM accuracy on each category of NYTsignificantly better results than all three baselinesincluding LDA.
Furthermore, explicit definitionmodeling as used in STM2 yields the best perfor-mance consistently overall.Finally, in Figure 2c we show the SVM clas-sification results on NYT in different parame-ter settings.
We find that the NYT classifica-tion accuracy trend is consistent with that on theBrown corpus for each parameter setting of  ?
{2, 10, 20, 40, all} and ?
?
{0, 1, 2}.
This furtherproves the robustness of STMn.4.2 Analysis on the Impact of ModelingDefinitions4.2.1 Qualitative AnalysisTo understand why definitions are helpful in textcategorization, we analyze the SVM performanceof STM0 and STM2 ( = 10) on each cate-gory of NYT dataset (figure 3).
We find STM2outperforms STM0 in all categories.
However,the largest gain is observed in Society, Miscel-laneous, Culture, Technology.
For Technology,we should credit WordNet definitions, since Tech-nology may contain many infrequent technicalterms, and STM0 cannot generalize the meaningof words only by distributional information due totheir low frequency usage.
However in some otherdomains, fewer specialized words are repeatedlyused, hence STM0 can do as well as STM2.For the other 3 categories, we hypothesize thatthese documents are likely to be a mixture of mul-tiple topics.
For example, a Culture news couldcontain topics pertaining to religion, history, art;while a Society news about crime could relate tolaw, family, economics.
In this case, it is veryimportant to sample a true topic for each word,so that ML algorithms can distinguish the Cul-ture documents from the Religion ones by the pro-portion of topics.
Accordingly, adding definitionsshould be very helpful, since it specifically definesthe topic of a sense, and shields it from the influ-ence of other ?incorrect/irrelevant?
topics.4.2.2 Quantitative Analysis with Word SenseDisambiguationA side effect of our model is that it sense disam-biguates all words.
As a means of analyzing andgaining some insight into the exact contribution ofexplicitly incorporating sense definitions (STMn)versus simply a sense node (STM0) in the model,we investigate the quality of the sense assignmentsin our models.
We believe that the choice of thecorrect sense is directly correlated with the choiceof a correct topic in our framework.
Accord-ingly, a relative improvement of STMn over STM0(where the only difference is the explicit sense def-inition modeling) in WSD task is an indicator ofthe impact of using sense definitions in the textcategorization task.WSD Data: We choose the all-words WSD task inwhich an unsupervised WSD system is required todisambiguate all the content words in documents.Our models are evaluated against the SemCordataset.
We prefer SemCor to all-words datasetsavailable in Senseval-3 (Snyder and Palmer, 2004)or SemEval-2007 (Pradhan et al, 2007), sinceit includes many more documents than either set(350 versus 3) and therefore allowing more reli-able results.
Moreover, SemCor is also the datasetused in (Boyd-Graber et al, 2007), where a Word-Net based topic model for WSD is introduced.
The558Total Noun Adjective Adverb Verbsense annotated words 225992 86996 31729 18947 88320polysemous words 187871 70529 21989 11498 83855TF-IDF - 0.422 0.300 0.153 0.182Table 3: Statistics of SemCor per POSstatistics of SemCor is listed in table 3.We use hyperparameters tuned from the text cat-egorization task: ?d=0.1, ?s=0.01, ?=0.01, ?=1,T=50, and try different values of  ?
{10, 20, 40}and ?
?
{0, 2, 10}.
The Brown corpus and Word-Net definitions corpus are used as augmented data,which means the dashed line in figure 1c will be-come bold.
Finally, we choose the most frequentanswer for each word in the last 10 iterations of aGibbs Sampling run as the final sense choice.WSD Results: Disambiguation per POS resultsare presented in table 4.
We only report resultson polysemous words.
We can see that modelingdefinitions (STM2 and STM10) improves perfor-mance significantly over STM0?s across the boardper POS and overall.
The fact that STMn picksmore correct senses helps explain why STMn clas-sifies more documents correctly than STM0.
Alsoit is interesting to see that unlike in the text cate-gorization task, larger values of ?
generate betterWSD results.
However, the window size , doesnot make a significant difference, yet we note that=10 is still the optimal value, similar to our ob-servation in the text categorization task.STM10 achieves similar results as in LDAWN(Boyd-Graber et al, 2007) which was specificallydesigned for WSD.
LDAWN needs a fine grainedhypernym hierarchy to perform WSD, hence theycan only disambiguate nouns.
They report differ-ent performances under various parameter setting.We cite their best performance of 38% accuracyon nouns as a comparison point to our best perfor-mance for nouns of 38.5%.An interesting feature of STM10 is that itperforms much better in nouns than adverbs andverbs, compared to a random baseline in Table4.
This is understandable since topic informationcontent is mostly borne by nouns and adjectives,while adverbs and verbs tend to be less informa-tive about topics (e.g., even, indicate, take), andused more across different domain documents.Hence topic models are weaker in their abilityto identify clear cues for senses for verbs andadverbs.
In support of our hypothesis about thePOS distribution, we compute the average TF-IDFscores for each POS (shown in Table 3 accordingto the equation illustrated below).
The averageTF-IDF clearly indicate the positive skewness ofthe nouns and adjectives (high TF-IDF) correlateswith the better WSD performance.TF-IDF(pos) =?i?d TF-IDF(wi,d)# of wi,dwhere wi,d ?
pos.At last, we notice that the most frequent sensebaseline performs much better than our models.This is understandable since: (1) most frequentsense baseline can be treated as a supervisedmethod in the sense that the sense frequency iscalculated based on the sense choice as presentin sense annotated data; (2) our model is not de-signed for WSD, therefore it discards a lot of in-formation when choosing the sense: in our model,the choice of a sense si is only dependent on twofacts: the corresponding topic zi and word wi,while in (Li et al, 2010; Banerjee and Pedersen,2003), they consider all the senses and words inthe context words.5 Related workVarious topic models have been developed formany applications.
Recently there is a trendof modeling document dependency (Dietz et al,2007; Mei et al, 2008; Daume, 2009).
How-ever, topics are only inferred based on word co-occurrence, while word semantics are ignored.Boyd-Graber et al (2007) are the first to inte-grate semantics into the topic model framework.They propose a topic model based on WordNetnoun hierarchy for WSD.
A word is assumed to begenerated by first sampling a topic, then choosinga path from the root node of hierarchy to a sensenode corresponding to that word.
However, theyonly focus on WSD.
They do not exploit word def-initions, neither do they report results on text cat-egorization.Chemudugunta et al (2008) also incorporate asense hierarchy into a topic model.
In their frame-work, a word may be directly generated from atopic (as in standard topic models), or it can be559Total Noun Adjective Adverb Verbrandom 22.1 26.2 27.9 32.2 15.8most frequent sense 64.7 74.7 77.5 74.0 59.6STM0  = 10 24.1?1.4 29.3?4.3 28.7?1.1 34.1?3.1 17.1?1.6 = 20 24?1.3 30.2?3.3 29.1?1.4 34.9?3.1 15.9?0.7 = 40 24?2.4 28.4?4.3 28.7?1.1 36.4?4.7 17.3?2.4STM2  = 10 27.5?1.1 36.1?3.8 34.0?1.2 33.4?1.8 17.8?1.4 = 20 25.7?1.3 32.0?4.2 33.5?0.7 34.2?3.4 17.3?0.7 = 40 26.1?1.3 32.5?3.9 33.6?0.9 34.2?3.4 17.5?1.4STM10  = 10 28.8?1.1 38.5?2.3 34.7?0.8 34.0?3.3 18.4?1.2 = 20 27.7?1.0 36.8?2.2 34.5?0.7 33.0?3.1 17.6?0.7 = 40 28.1?1.5 38.4?3.1 34.0?1.0 35.1?5.4 17.0?0.9Table 4: Disambiguation results per POS on polysemous words.generated by choosing a sense path in the hierar-chy.
Note that no topic information is on the sensepath.
If a word is generated from the hierarchy,then it is not assigned a topic.
Their models basedon different dictionaries improve perplexity.Recently, several systems have been proposedto apply topic models to WSD.
Cai et al (2007)incorporate topic features into a supervised WSDframework.
Brody and Lapata (2009) place thesense induction in a Baysian framework by assum-ing each context word is generated from the targetword?s senses, and a context is modeled as a multi-nomial distribution over the target word?s sensesrather than topics.
Li et al (2010) design sev-eral systems that use latent topics to find a mostlikely sense based on the sense paraphrases (ex-tracted from WordNet) and context.
Their WSDmodels are unsupervised and outperform state-of-art systems.Our model borrows the local window idea fromword sense disambiguation community.
In graph-based WSD systems (Mihalcea, 2005; Sinha andMihalcea, 2007; Guo and Diab, 2010), a node iscreated for each sense.
Two nodes will be con-nected if their distance is less than a predefinedvalue; the weight on the edge is a value returnedby sense similarity measures, then the PageR-ank/Indegree algorithm is applied on this graph todetermine the appropriate senses.6 Conclusion and Future WorkWe presented a novel model STM that combinesexplicit semantic information and word distribu-tion information in a unified topic model.
STMis able to capture topics of words more accuratelythan traditional LDA topic models.
In future work,we plan to model the WordNet sense network.
Webelieve that WordNet senses are too fine-grained,hence we plan to use clustered senses, instead ofcurrent WN senses, in order to avail the model ofmore generalization power.AcknowledgmentsThis research was funded by the Ofce of the Direc-tor of National Intelligence (ODNI), IntelligenceAdvanced Research Projects Activity (IARPA),through the U.S. Army Research Lab.
All state-ments of fact, opinion or conclusions containedherein are those of the authors and should not beconstrued as representing the ofcial views or poli-cies of IARPA, the ODNI or the U.S. Government.ReferencesSatanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proceedings of the 18th International Joint Con-ference on Artificial Intelligence, pages 805?810.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jordan Boyd-Graber and David M. Blei.
2007.
Putop:turning predominant senses into a topic model forword sense disambiguation.
In Proceedings of the4th International Workshop on Semantic Evalua-tions, pages 277?281.Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambiguation.In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 1024?1033.Samuel Brody and Mirella Lapata.
2009.
Bayesianword sense induction.
In Proceedings of the 12thConference of the European Chapter of the ACL,pages 103?111.Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.
2007.Improving word sense disambiguation using topicfeatures.
In Proceedings of 2007 Joint Confer-ence on Empirical Methods in Natural Language560Processing and Computational Natural LanguageLearning, pages 1015?1023.Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2008.
Combining concept hierarchiesand statistical topic models.
In Proceedings of the17th ACM conference on Information and knowl-edge management, pages 1469?1470.Hal Daume.
2009.
Markov random topic fields.
InProceedings of the ACL-IJCNLP Conference, pages293?296.Laura Dietz, Steffen Bickel, and Tobias Scheffer.
2007.Unsupervised prediction of citation influence.
InProceedings of the 24th international conference onMachine learning, pages 233?240.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101:5228?5235.Thomas L. Griffiths, Mark Steyvers, David M. Blei,and Joshua B. Tenenbaum.
2005.
Integrating top-ics and syntax.
In Advances in Neural InformationProcessing Systems.Weiwei Guo and Mona Diab.
2010.
Combining or-thogonal monolingual and multilingual sources ofevidence for all words wsd.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1542?1551.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11:10?18.Linlin Li, Benjamin Roth, and Caroline Sporleder.2010.
Topic models for word sense disambiguationand token-based idiom detection.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics, pages 1138?1147.Qiaozhu Mei, Deng Cai, Duo Zhang, and ChengxiangZhai.
2008.
Topic modeling with network regu-larization.
In Proceedings of the 17th internationalconference on World Wide Web, pages 101?110.Rada Mihalcea.
2005.
Unsupervised large-vocabularyword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In Proceedingsof the Joint Conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 411?418.Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,and Martha Palmer.
2007.
Semeval-2007 task 17:English lexical sample, srl and all words.
In Pro-ceedings of the 4th International Workshop on Se-mantic Evaluations, pages 87?92.
ACL.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervisedgraph-based word sense disambiguation using mea-sures of word semantic similarity.
In Proceedingsof the IEEE International Conference on SemanticComputing, pages 363?369.Benjamin Snyder and Martha Palmer.
2004.
The en-glish all-words task.
In Senseval-3: Third Interna-tional Workshop on the Evaluation of Systems for theSemantic Analysis of Text, pages 41?43.
ACL.Ivan Titov and Ryan McDonald.
2008.
Modelingonline reviews with multi-grain topic models.
InProceedings of the 17th international conference onWorld Wide Web, pages 111?120.561
