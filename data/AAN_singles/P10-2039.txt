Proceedings of the ACL 2010 Conference Short Papers, pages 209?214,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEfficient Optimization of an MDL-Inspired Objective Function forUnsupervised Part-of-Speech TaggingAshish Vaswani1 Adam Pauls2 David Chiang11Information Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292{avaswani,chiang}@isi.edu2Computer Science DivisionUniversity of California at BerkeleySoda HallBerkeley, CA 94720adpauls@eecs.berkeley.eduAbstractThe Minimum Description Length (MDL)principle is a method for model selectionthat trades off between the explanation ofthe data by the model and the complexityof the model itself.
Inspired by the MDLprinciple, we develop an objective func-tion for generative models that capturesthe description of the data by the model(log-likelihood) and the description of themodel (model size).
We also develop a ef-ficient general search algorithm based onthe MAP-EM framework to optimize thisfunction.
Since recent work has shown thatminimizing the model size in a HiddenMarkov Model for part-of-speech (POS)tagging leads to higher accuracies, we testour approach by applying it to this prob-lem.
The search algorithm involves a sim-ple change to EM and achieves high POStagging accuracies on both English andItalian data sets.1 IntroductionThe Minimum Description Length (MDL) princi-ple is a method for model selection that provides ageneric solution to the overfitting problem (Barronet al, 1998).
A formalization of Ockham?s Razor,it says that the parameters are to be chosen thatminimize the description length of the data giventhe model plus the description length of the modelitself.It has been successfully shown that minimizingthe model size in a Hidden Markov Model (HMM)for part-of-speech (POS) tagging leads to higheraccuracies than simply running the Expectation-Maximization (EM) algorithm (Dempster et al,1977).
Goldwater and Griffiths (2007) employ aBayesian approach to POS tagging and use sparseDirichlet priors to minimize model size.
More re-cently, Ravi and Knight (2009) alternately mini-mize the model using an integer linear programand maximize likelihood using EM to achieve thehighest accuracies on the task so far.
However, inthe latter approach, because there is no single ob-jective function to optimize, it is not entirely clearhow to generalize this technique to other prob-lems.
In this paper, inspired by the MDL princi-ple, we develop an objective function for genera-tive models that captures both the description ofthe data by the model (log-likelihood) and the de-scription of the model (model size).
By using asimple prior that encourages sparsity, we cast ourproblem as a search for the maximum a poste-riori (MAP) hypothesis and present a variant ofEM to approximately search for the minimum-description-length model.
Applying our approachto the POS tagging problem, we obtain higher ac-curacies than both EM and Bayesian inference asreported by Goldwater and Griffiths (2007).
On aItalian POS tagging task, we obtain even largerimprovements.
We find that our objective functioncorrelates well with accuracy, suggesting that thistechnique might be useful for other problems.2 MAP EM with Sparse Priors2.1 Objective functionIn the unsupervised POS tagging task, we aregiven a word sequence w = w1, .
.
.
,wN and wantto find the best tagging t = t1, .
.
.
, tN , whereti ?
T , the tag vocabulary.
We adopt the problemformulation of Merialdo (1994), in which we aregiven a dictionary of possible tags for each wordtype.We define a bigram HMMP(w, t | ?)
=N?i=1P(w, t | ?)
?
P(ti | ti?1) (1)In maximum likelihood estimation, the goal is to209find parameter estimates??
= arg max?log P(w | ?)
(2)= arg max?log?tP(w, t | ?)
(3)The EM algorithm can be used to find a solution.However, we would like to maximize likelihoodand minimize the size of the model simultane-ously.
We define the size of a model as the numberof non-zero probabilities in its parameter vector.Let ?1, .
.
.
, ?n be the components of ?.
We wouldlike to find??
= arg min?(?
log P(w | ?)
+ ???
?0)(4)where ??
?0, called the L0 norm of ?, simply countsthe number of non-zero parameters in ?.
Thehyperparameter ?
controls the tradeoff betweenlikelihood maximization and model minimization.Note the similarity of this objective function withMDL?s, where ?
would be the space (measuredin nats) needed to describe one parameter of themodel.Unfortunately, minimization of the L0 normis known to be NP-hard (Hyder and Mahata,2009).
It is not smooth, making it unamenableto gradient-based optimization algorithms.
There-fore, we use a smoothed approximation,??
?0 ?
?i(1 ?
e??i?
)(5)where 0 < ?
?
1 (Mohimani et al, 2007).
Forsmaller values of ?, this closely approximates thedesired function (Figure 1).
Inverting signs and ig-noring constant terms, our objective function isnow:??
= arg max???????
?log P(w | ?)
+ ??ie??i????????
(6)We can think of the approximate model size asa kind of prior:P(?)
=exp?
?i e?
?i?Z(7)log P(?)
= ?
??ie??i?
?
log Z (8)where Z =?d?exp?
?i e??i?
is a normalizationconstant.
Then our goal is to find the maximum00.20.40.60.8 1  00.20.40.60.81Function Values?
i?=0.005 ?=0.05 ?=0.5 1-||?
i|| 0Figure 1: Ideal model-size term and its approxima-tions.a posterior parameter estimate, which we find us-ing MAP-EM (Bishop, 2006):??
= arg max?log P(w, ?)
(9)= arg max?
(log P(w | ?)
+ log P(?
))(10)Substituting (8) into (10) and ignoring the constantterm log Z, we get our objective function (6) again.We can exercise finer control over the sparsityof the tag-bigram and channel probability distri-butions by using a different ?
for each:arg max?
(log P(w | ?)
+?c?w,te?P(w|t)?
+ ?t?t,t?e?P(t?
|t)?
)(11)In our experiments, we set ?c = 0 since previ-ous work has shown that minimizing the numberof tag n-gram parameters is more important (Raviand Knight, 2009; Goldwater and Griffiths, 2007).A common method for preferring smaller mod-els is minimizing the L1 norm,?i |?i|.
However,for a model which is a product of multinomial dis-tributions, the L1 norm is a constant.
?i|?i| =?i?i=?t???????
?wP(w | t) +?t?P(t?
| t)??????
?= 2|T |Therefore, we cannot use the L1 norm as part ofthe size term as the result will be the same as theEM algorithm.2102.2 Parameter optimizationTo optimize (11), we use MAP EM, which is an it-erative search procedure.
The E step is the same asin standard EM, which is to calculate P(t | w, ?t),where the ?t are the parameters in the current iter-ation t. The M step in iteration (t + 1) looks like?t+1 = arg max?
(EP(t|w,?t)[log P(w, t | ?)]+?t?t,t?e?P(t?
|t)?)
(12)Let C(t,w; t,w) count the number of times theword w is tagged as t in t, and C(t, t?
; t) the numberof times the tag bigram (t, t?)
appears in t. We canrewrite the M step as?t+1 = arg max?
(?t?wE[C(t,w)] log P(w | t)+?t?t?
(E[C(t, t?)]
log P(t?
| t) + ?te?P(t?
|t)?)???????
(13)subject to the constraints?w P(w | t) = 1 and?t?
P(t?
| t) = 1.
Note that we can optimize eachterm of both summations over t separately.
Foreach t, the term?wE[C(t,w)] log P(w | t) (14)is easily optimized as in EM: just let P(w | t) ?E[C(t,w)].
But the term?t?
(E[C(t, t?)]
log P(t?
| t) + ?te?P(t?
|t)?
)(15)is trickier.
This is a non-convex optimization prob-lem for which we invoke a publicly availableconstrained optimization tool, ALGENCAN (An-dreani et al, 2007).
To carry out its optimization,ALGENCAN requires computation of the follow-ing in every iteration:?
Objective function, defined in equation (15).This is calculated in polynomial time usingdynamic programming.?
Constraints: gt =?t?
P(t?
| t) ?
1 = 0 foreach tag t ?
T .
Also, we constrain P(t?
| t) tothe interval [, 1].11We must have  > 0 because of the log P(t?
| t) termin equation (15).
It seems reasonable to set   1N ; in ourexperiments, we set  = 10?7.?
Gradient of objective function:?F?P(t?
| t)=E[C(t, t?)]P(t?
| t)??t?e?P(t?
|t)?
(16)?
Gradient of equality constraints:?gt?P(t??
| t?)=??????
?1 if t = t?0 otherwise(17)?
Hessian of objective function, which is notrequired but greatly speeds up the optimiza-tion:?2F?P(t?
| t)?P(t?
| t)= ?E[C(t, t?)]P(t?
| t)2+ ?te?P(t?
|t)?
?2(18)The other second-order partial derivatives areall zero, as are those of the equality con-straints.We perform this optimization for each instanceof (15).
These optimizations could easily be per-formed in parallel for greater scalability.3 ExperimentsWe carried out POS tagging experiments on En-glish and Italian.3.1 English POS taggingTo set the hyperparameters ?t and ?, we preparedthree held-out sets H1,H2, and H3 from the PennTreebank.
Each Hi comprised about 24, 000 wordsannotated with POS tags.
We ran MAP-EM for100 iterations, with uniform probability initializa-tion, for a suite of hyperparameters and averagedtheir tagging accuracies over the three held-outsets.
The results are presented in Table 2.
We thenpicked the hyperparameter setting with the highestaverage accuracy.
These were ?t = 80, ?
= 0.05.We then ran MAP-EM again on the test data withthese hyperparameters and achieved a tagging ac-curacy of 87.4% (see Table 1).
This is higher thanthe 85.2% that Goldwater and Griffiths (2007) ob-tain using Bayesian methods for inferring bothPOS tags and hyperparameters.
It is much higherthan the 82.4% that standard EM achieves on thetest set when run for 100 iterations.Using ?t = 80, ?
= 0.05, we ran multiple ran-dom restarts on the test set (see Figure 2).
We findthat the objective function correlates well with ac-curacy, and picking the point with the highest ob-jective function value achieves 87.1% accuracy.211?t?0.75 0.5 0.25 0.075 0.05 0.025 0.0075 0.005 0.002510 82.81 82.78 83.10 83.50 83.76 83.70 84.07 83.95 83.7520 82.78 82.82 83.26 83.60 83.89 84.88 83.74 84.12 83.4630 82.78 83.06 83.26 83.29 84.50 84.82 84.54 83.93 83.4740 82.81 83.13 83.50 83.98 84.23 85.31 85.05 83.84 83.4650 82.84 83.24 83.15 84.08 82.53 84.90 84.73 83.69 82.7060 83.05 83.14 83.26 83.30 82.08 85.23 85.06 83.26 82.9670 83.09 83.10 82.97 82.37 83.30 86.32 83.98 83.55 82.9780 83.13 83.15 82.71 83.00 86.47 86.24 83.94 83.26 82.9390 83.20 83.18 82.53 84.20 86.32 84.87 83.49 83.62 82.03100 83.19 83.51 82.84 84.60 86.13 85.94 83.26 83.67 82.06110 83.18 83.53 83.29 84.40 86.19 85.18 80.76 83.32 82.05120 83.08 83.65 83.71 84.11 86.03 85.39 80.66 82.98 82.20130 83.10 83.19 83.52 84.02 85.79 85.65 80.08 82.04 81.76140 83.11 83.17 83.34 85.26 85.86 85.84 79.09 82.51 81.64150 83.14 83.20 83.40 85.33 85.54 85.18 78.90 81.99 81.88Table 2: Average accuracies over three held-out sets for English.system accuracy (%)Standard EM 82.4+ random restarts 84.5(Goldwater and Griffiths, 2007) 85.2our approach 87.4+ random restarts 87.1Table 1: MAP-EM with a L0 norm achieves highertagging accuracy on English than (2007) and muchhigher than standard EM.system zero parameters bigram typesmaximum possible 1389 ?EM, 100 iterations 444 924MAP-EM, 100 iterations 695 648Table 3: MAP-EM with a smoothed L0 normyields much smaller models than standard EM.We also carried out the same experiment with stan-dard EM (Figure 3), where picking the point withthe highest corpus probability achieves 84.5% ac-curacy.We also measured the minimization effect of thesparse prior against that of standard EM.
Since ourmethod lower-bounds all the parameters by , weconsider a parameter ?i as a zero if ?i ?
.
Wealso measured the number of unique tag bigramtypes in the Viterbi tagging of the word sequence.Table 3 shows that our method produces muchsmaller models than EM, and produces Viterbitaggings with many fewer tag-bigram types.3.2 Italian POS taggingWe also carried out POS tagging experiments onan Italian corpus from the Italian Turin Univer-0.780.79 0.80.810.820.830.840.850.860.870.880.89 -53200-53000-52800-52600-52400-52200-52000-51800-51600-51400Tagging accuracyobjectivefunctionvalue?
t=80,?=0.05,Test Set24115WordsFigure 2: Tagging accuracy vs. objective func-tion for 1152 random restarts of MAP-EM withsmoothed L0 norm.sity Treebank (Bos et al, 2009).
This test set com-prises 21, 878 words annotated with POS tags anda dictionary for each word type.
Since this is allthe available data, we could not tune the hyperpa-rameters on a held-out data set.
Using the hyper-parameters tuned on English (?t = 80, ?
= 0.05),we obtained 89.7% tagging accuracy (see Table 4),which was a large improvement over 81.2% thatstandard EM achieved.
When we tuned the hyper-parameters on the test set, the best setting (?t =120, ?
= 0.05 gave an accuracy of 90.28%.4 ConclusionA variety of other techniques in the literature havebeen applied to this unsupervised POS taggingtask.
Smith and Eisner (2005) use conditional ran-dom fields with contrastive estimation to achieve212?t?0.75 0.5 0.25 0.075 0.05 0.025 0.0075 0.005 0.002510 81.62 81.67 81.63 82.47 82.70 84.64 84.82 84.96 84.9020 81.67 81.63 81.76 82.75 84.28 84.79 85.85 88.49 85.3030 81.66 81.63 82.29 83.43 85.08 88.10 86.16 88.70 88.3440 81.64 81.79 82.30 85.00 86.10 88.86 89.28 88.76 88.8050 81.71 81.71 78.86 85.93 86.16 88.98 88.98 89.11 88.0160 81.65 82.22 78.95 86.11 87.16 89.35 88.97 88.59 88.0070 81.69 82.25 79.55 86.32 89.79 89.37 88.91 85.63 87.8980 81.74 82.23 80.78 86.34 89.70 89.58 88.87 88.32 88.5690 81.70 81.85 81.00 86.35 90.08 89.40 89.09 88.09 88.50100 81.70 82.27 82.24 86.53 90.07 88.93 89.09 88.30 88.72110 82.19 82.49 82.22 86.77 90.12 89.22 88.87 88.48 87.91120 82.23 78.60 82.76 86.77 90.28 89.05 88.75 88.83 88.53130 82.20 78.60 83.33 87.48 90.12 89.15 89.30 87.81 88.66140 82.24 78.64 83.34 87.48 90.12 89.01 88.87 88.99 88.85150 82.28 78.69 83.32 87.75 90.25 87.81 88.50 89.07 88.41Table 4: Accuracies on test set for Italian.0.760.78 0.80.820.840.860.88 0.9 -147500-147400-147300-147200-147100-147000-146900-146800-146700-146600-146500-146400Tagging accuracyobjectivefunctionvalueEM, Test Set24115WordsFigure 3: Tagging accuracy vs. likelihood for 1152random restarts of standard EM.88.6% accuracy.
Goldberg et al (2008) providea linguistically-informed starting point for EM toachieve 91.4% accuracy.
More recently, Chiang etal.
(2010) use GIbbs sampling for Bayesian in-ference along with automatic run selection andachieve 90.7%.In this paper, our goal has been to investi-gate whether EM can be extended in a genericway to use an MDL-like objective function thatsimultaneously maximizes likelihood and mini-mizes model size.
We have presented an efficientsearch procedure that optimizes this function forgenerative models and demonstrated that maxi-mizing this function leads to improvement in tag-ging accuracy over standard EM.
We infer the hy-perparameters of our model using held out dataand achieve better accuracies than (Goldwater andGriffiths, 2007).
We have also shown that the ob-jective function correlates well with tagging accu-racy supporting the MDL principle.
Our approachperforms quite well on POS tagging for both En-glish and Italian.
We believe that, like EM, ourmethod can benefit from more unlabeled data, andthere is reason to hope that the success of theseexperiments will carry over to other tasks as well.AcknowledgementsWe would like to thank Sujith Ravi, Kevin Knightand Steve DeNeefe for their valuable input, andJason Baldridge for directing us to the ItalianPOS data.
This research was supported in part byDARPA contract HR0011-06-C-0022 under sub-contract to BBN Technologies and DARPA con-tract HR0011-09-1-0028.ReferencesR.
Andreani, E. G. Birgin, J. M. Martnez, and M. L.Schuverdt.
2007.
On Augmented Lagrangian meth-ods with general lower-level constraints.
SIAMJournal on Optimization, 18:1286?1309.A.
Barron, J. Rissanen, and B. Yu.
1998.
The min-imum description length principle in coding andmodeling.
IEEE Transactions on Information The-ory, 44(6):2743?2760.C.
Bishop.
2006.
Pattern Recognition and MachineLearning.
Springer.J.
Bos, C. Bosco, and A. Mazzei.
2009.
Converting adependency treebank to a categorical grammar tree-bank for italian.
In Eighth International Workshopon Treebanks and Linguistic Theories (TLT8).D.
Chiang, J. Graehl, K. Knight, A. Pauls, and S. Ravi.2010.
Bayesian inference for Finite-State transduc-ers.
In Proceedings of the North American Associa-tion of Computational Linguistics.213A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Computational Linguistics, 39(4):1?38.Y.
Goldberg, M. Adler, and M. Elhadad.
2008.
EM canfind pretty good HMM POS-taggers (when given agood start).
In Proceedings of the ACL.S.
Goldwater and T. L. Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the ACL.M.
Hyder and K. Mahata.
2009.
An approximate L0norm minimization algorithm for compressed sens-ing.
In Proceedings of the 2009 IEEE InternationalConference on Acoustics, Speech and Signal Pro-cessing.B.
Merialdo.
1994.
Tagging English text with aprobabilistic model.
Computational Linguistics,20(2):155?171.H.
Mohimani, M. Babaie-Zadeh, and C. Jutten.
2007.Fast sparse representation based on smoothed L0norm.
In Proceedings of the 7th International Con-ference on Independent Component Analysis andSignal Separation (ICA2007).S.
Ravi and K. Knight.
2009.
Minimized models forunsupervised part-of-speech tagging.
In Proceed-ings of ACL-IJCNLP.N.
Smith.
and J. Eisner.
2005.
Contrastive estima-tion: Training log-linear models on unlabeled data.In Proceedings of the ACL.214
