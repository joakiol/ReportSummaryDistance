Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1192?1202,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Sentiment-aligned Topic Model for Product Aspect Rating PredictionHao WangSchool of Computing ScienceSimon Fraser UniversityBurnaby, BC, Canadahwa63@sfu.caMartin EsterSchool of Computing ScienceSimon Fraser UniversityBurnaby, BC, Canadaester@sfu.caAbstractAspect-based opinion mining has attractedlots of attention today.
In this paper, weaddress the problem of product aspect rat-ing prediction, where we would like to ex-tract the product aspects, and predict as-pect ratings simultaneously.
Topic mod-els have been widely adapted to jointlymodel aspects and sentiments, but exist-ing models may not do the prediction taskwell due to their weakness in sentimentextraction.
The sentiment topics usuallydo not have clear correspondence to com-monly used ratings, and the model mayfail to extract certain kinds of sentimentsdue to skewed data.
To tackle this prob-lem, we propose a sentiment-aligned topicmodel(SATM), where we incorporate twotypes of external knowledge: product-level overall rating distribution and word-level sentiment lexicon.
Experiments onreal dataset demonstrate that SATM is ef-fective on product aspect rating prediction,and it achieves better performance com-pared to the existing approaches.1 IntroductionOnline reviews have become an important sourceof information for consumers.
People tend to readreviews to help them compare products, and makeinformed decisions.
As the volume of product re-views continues to grow, it is often impossible toread all of them, which calls for efficient methodsfor opinion mining.
Nowadays, for each product,many websites aggregate the overall rating of re-views, and display its distribution.
However, thiscannot provide detailed information.
For exam-ple, two products may have similar overall ratingdistributions, while people talk about different un-satisfactory aspects.
This problem has inspired anew line of research on aspect-level opinion min-ing(Hu and Liu, 2004).An aspect refers to a rateable feature, such asstaff and location in hotel reviews, or size andbattery for digital camera reviews.
In this paper,we deal with the problem of product aspect rat-ing prediction.
The input is a collection of prod-ucts, and each product is associated with a set ofreviews.
The goal is to extract the corpus-level as-pects, and predict the aspect ratings for each prod-uct.
This kind of fine-grained sentiment analysiswill help users efficiently digest the reviews, andgain more insight into the product quality.The product aspect rating prediction problemusually involves two subtasks: aspect extractionand sentiment identification(Titov and McDonald,2008b).
Given some text, we would like to knowwhat aspects it talks about, and what kind of sen-timents are expressed.
For example, given a sen-tence ?the room is filthy?, we would like to knowthat it talks about the aspect ?room?.
Also, ?filthy?is a sentiment word, and it expresses strongly neg-ative sentiment towards the aspect ?room?.Topic models(Blei et al., 2003; Hofmann, 1999)have been popular in aspect-based opinion min-ing(Liu, 2012).
Existing works have used topicmodels to extract only aspects(Titov and McDon-ald, 2008a; Brody and Elhadad, 2010; Chen etal., 2013), or jointly model aspects and sentiments(Mei et al., 2007; Lin and He, 2009; Li et al.,2010; Jo and Oh, 2011; Moghaddam and Ester,2011; Lakkaraju et al., 2011; Sauper et al., 2011;Mukherjee and Liu, 2012; Lazaridou et al., 2013;Moghaddam and Ester, 2013; Kim et al., 2013).
Inthe joint modelling approaches, a sentiment topicis usually modelled as a sentiment label-word dis-tribution, analogous to the topic-word distributionin standard topic models.
However, the differenceis that the sentiment topics need to be ordered.
Ifthe model is to be applied for aspect rating predic-tion, the sentiment topics should have clear cor-1192respondence to the ratings.
Suppose there are 5sentiment topics with sentiment labels from 1 to5.
The sentiment topic with label i is expectedto correspond to the rating i on the 1-5 ratingscale.
For example, the sentiment topic with label5 should have high probability over positive sen-timent words, so it expresses highly positive sen-timent, which matches our natural interpretationfor the rating 5.
In this case, sentiment labels andratings are aligned.
However, in a standard topicmodel, the learned sentiment topics may not haveclear correspondence with different ratings.
Also,if the positive reviews are dominant in the data, thetopic model may fail to capture the negative sen-timents with any sentiment topic, so no sentimentlabels are matched with low ratings.
If the senti-ment labels are not correctly aligned to the ratings,we cannot use these sentiment labels to predict as-pect ratings.
Consequently, the aspect rating pre-diction accuracy is compromised, and the methodis less practical.
We call this the sentiment labelalignment problem.
To tackle this problem, mod-els in the literature usually use some seed wordsfor each sentiment topic to define Dirichlet priorswith asymmetric concentration parameter vectors(Sauper et al., 2011; Kim et al., 2013), or use seedwords to initialize word assignment to sentimenttopic(Lin and He, 2009), or both(Li et al., 2010;Jo and Oh, 2011).
However, these seed wordsare usually arbitrarily selected, and how to defineasymmetric priors is not clear, especially when wewould like to capture more than two (positive andnegative) kinds of sentiments.In this paper, we propose a sentiment-alignedtopic model(SATM) for product aspect rating pre-diction, which focuses the sentiment label align-ment problem.
We use two kinds of externalknowledge: the product overall rating distribution,and a sentiment lexicon.
For each product, theoverall rating distribution is available on most on-line review websites.
It provides the big picture ofthe product-level sentiments.
In SATM, for eachproduct and each aspect, we define a multinomialdistribution over sentiment labels, with prior pa-rameterized by the overall rating distribution.
Sen-timent lexicon is constructed by linguistic experts,and every word in the lexicon is associated with asentiment polarity score(Taboada et al., 2011).
Wetreat the polarity score as an extra word featurein a semi-supervised framework.
By incorporat-ing both product-level and word-level knowledgeinto the model, the sentiment labels can be alignedwith ratings, and the extracted sentiment topicscan capture different kinds of sentiments, rangingfrom highly positive to highly negative.
Experi-ments on a TripAdvisor dataset demonstrate thatour method can effectively deal with the sentimentlabel alignment problem, and outperforms state-of-the-art methods in terms of product aspect rat-ing prediction accuracy.2 Related workSeveral methods have been proposed for productaspect rating prediction, and many of them arebased on topic models.In (Lu et al., 2009), the authors studied the prob-lem of generating an aspect rating summary forshort comments.
The text was first preprocessedinto phrases of the format <headterm, sentimentword>, and the headterms are clustered by Struc-tured PLSA to find K major aspects.
Then, phraseratings are predicted by either Local Predictionor Global Prediction, and they are aggregated toget aspect ratings.
The method in (Brody and El-hadad, 2010) also first uses topic models to find as-pects.
Then, for each aspect, it extracts all the rel-evant adjectives, and builds a conjunction graph.A label propagation algorithm(Zhu and Ghahra-mani, 2002) is used on the graph to learn the senti-ment polarity score of adjective words.
Althoughthis approach is not proposed for aspect rating pre-diction, it can be used for this task if the polar-ity scores of adjective words are aggregated foreach aspect.
All the methods above perform as-pect extraction and sentiment identification sepa-rately, while our approach takes a joint modellingapproach so that different subtasks can potentiallyreinforce with each other.
To demonstrate this, weuse these methods as baselines in our experiments.Wang et al.
worked on the Latent Aspect Rat-ing Analysis problem(Wang et al., 2010; Wanget al., 2011), the task of inferring aspect ratingsfor each review and the relative weights review-ers have placed on each aspect.
In (Wang et al.,2010), aspect keywords are provided as user input,and a two-stage method, called Latent Rating Re-gression(LRR), is proposed.
The first stage usesa bootstrapping algorithm to obtain more relatedwords for each aspect, and segments the documentcontent.
In the second stage, the overall rating is?generated?
as weighted combination of the latentaspect ratings, and LRR is used to infer both the1193weights and aspect ratings.
Their follow-up work(Wang et al., 2011) does not need keyword speci-fication from users, and replaces the bootstrappingmethod with a topic model.
However, both meth-ods implicitly require that each review talks aboutall aspects, which is not always true due to the datasparsity in online reviews.In (Moghaddam and Ester, 2011), ILDA wasproposed for product aspect rating prediction.Later, it was extended to FLDA (Moghaddamand Ester, 2013) to address the cold start prob-lem, when there are few reviews associated witha product.
Similar to (Lu et al., 2009), in ILDAand FLDA, a preprocessing step parses the textinto phrases of the format <headterm, sentimentword>, and a review is modelled as a bag ofphrases.
We also adopt this assumption in ourmodel.
The method in (Sauper et al., 2011; Sauperand Barzilay, 2013) does not use phrases, but in-stead uses ?snippets?, and an snippet is a shortsentence or phrase.
However, the sentiment labelalignment problem is not well addressed in thesemodels, which limits their practicality.
ILDAand FLDA did not deal with this problem.
Themodel in (Sauper et al., 2011; Sauper and Barzi-lay, 2013) follows the most common approach ofusing seed words to define asymmetric priors.
Itsupports only two kinds of sentiment topics: pos-itive and negative, while how to define asymmet-ric priors for more sentiment topics becomes un-clear.
More importantly, the prior approach maynot work well in practice(see Experiment Section).Lakkaraju et al.
try to tackle the sentiment labelalignment problem by assuming that the overallrating is generated as response variable(Lakkarajuet al., 2011), with the sentiment topic propor-tions as features.
However, how the sentiment la-bels are related to ratings is still unknown untillearned, and we may not get the desired alignment.Lazaridou et al.
attempt to connect sentiment la-bels with ratings by Kronecker symbol, but thismethod only applies to three sentiment polarities:?1(negative), 0(neutral), +1(positive), and it doesnot explore the word-level lexicon, which is alsoan important source of knowledge.Another line of research on product aspect rat-ing prediction or summarization does not use topicmodels, but relies mainly on word frequency andgrammatical relations(Hu and Liu, 2004; Popescuand Etzioni, 2005; Blair-goldensohn et al., 2008),or specialized review selection(Long et al., 2014).In this case, the extracted aspect words need tobe clustered manually.
For example, picture andphoto may refer to the same aspect in digital cam-era reviews.
By comparison, topic modelling ap-proaches extract aspect words and cluster them si-multaneously.Our method incorporates the product overallrating distributions and sentiment lexicons intothe model, so it is also related to topic modelswhich use observed features or domain knowl-edge(Mimno and McCallum, 2008; Andrzejewskiet al., 2009; Andrzejewski et al., 2011).
Mimno etal.
introduces two general frameworks to integrateobserved features into the generative process:downstream and upstream topic models(Mimnoand McCallum, 2008).
In the context of aspect-based opinion mining, MaxEnt-LDA(Zhao et al.,2010) integrates a discriminative maximum en-tropy component to help separate aspect wordsand sentiment words.
The SAS model (Mukherjeeand Liu, 2012) uses seed words to provide guid-ance for aspect discovery, and MC-LDA (Chen etal., 2013) uses must-links and cannot-links to ex-tract coherent aspects.
However, MaxEnt-LDA,SAS and MC-LDA cannot be used for aspect rat-ing prediction, since they fail to identify the senti-ment polarity of sentiment words.3 Method3.1 PreliminariesWe first introduce several key concepts used in ourmodel.Products: Let P = {P1, P2, .
.
. }
be a set ofproducts.
Each product Piis associated with aset of reviews Di= {d1, d2, .
.
.
dNi}, and also anoverall rating distribution Yi.
Yiis a multinomialdistribution on R ratings.
It is available on mostonline review websites, and usually R = 5.Aspects: An aspect is a rateable feature of aproduct, and each aspect is modelled as a distribu-tion over aspect words.
The number of aspects ispredefined as K.Sentiment topics: A sentiment topic is mod-elled as a distribution over sentiment words, andeach sentiment topic is associated with a sentimentlabel.
To make it consistent with commonly usedrating scale, we assume there are R sentiment la-bels, corresponding to the R ratings.
The chal-lenge is that sentiment labels with higher valuesare expected to be associated with sentiment top-ics which express more positive sentiments, so that1194we can match sentiment labels with ratings.Phrases: An opinion phrase f =< h,m > is apair of aspect word h and sentiment word m, suchas < room, filthy >(Lu et al., 2009; Moghad-dam and Ester, 2011).
For each product Pi, wefirst parse the related reviews Diinto phrases Fi,and each product can be modelled as a bag ofphrases.Sentiment lexicons : A sentiment lexicon L isa list of sentiment words, and each word m ?
L isassociated with a sentiment polarity score sm.
smcan take T values.
Note that the lexicon L usuallyonly covers a small subset of sentiment words inthe whole vocabulary.Sentiment association: The sentiment labeltakesR values, and there are T different values forthe polarity score in the sentiment lexicon.
How-ever, the relation between sentiment labels and po-larity scores are unknown.
If we have training in-stances where a sentiment word m is associatedwith both a sentiment label rmand polarity scoresm, we can build a classifier, where the explana-tory variable for the classifier is a sentiment label,and outcome is the polarity score.
In this case,H(sm|rm) can be interpreted as the probability ofobserving a polarity score sm, given its sentimentlabel rm.
We refer to this probability H as senti-ment association.
This is a key component in ourmodel.
It naturally bridges the gap between sen-timent labels and polarity scores, and captures theuncertainty in their relations.
Note that H can betrained independent of the topic model part.
Foreach training instance, suppose the sentiment wordis m ?
L, we need to know its sentiment labelrmand polarity score sm.
smcan be retrieved di-rectly from the sentiment lexicon, and rmcan beeither manually or automatically annotated.
Forexample, suppose the word m appears in reviewd, we can assign the overall rating of d as its sen-timent label.
In this case, each word m ?
L canbe associated with multiple training instances thathave the same value for smbut different sentimentlabels rm.
We adopt this approach to automati-cally annotate sentiment labels, and details are de-scribed in the Experiments section.3.2 Problem definitionThe product aspect rating prediction problem canbe defined as follows.
The input is a set of prod-ucts P .
Each product Pihas a bag of phrases Fi,and an overall rating distribution Yiover R rat-Figure 1: Graphical model of SATMings.
The output is the K corpus-level aspects,and for each product, we predict its ratings on theK aspects, also in the [1, R] rating scale.
We as-sume products in P are in the same category sothey share the same aspects.3.3 The SATM modelWe introduce the Sentiment-aligned TopicModel(SATM) in this section, and its graphicalrepresentation is shown in Figure 1.
Note that thesentiment association H is observed, because it istrained independently of the topic model part.At the word level, each observed phrase <h,m > is associated with two latent variables:aspect z and sentiment label r. Aspect z modelswhat aspect this phrase talks about, and r deter-mines the sentiment of m. If m is in the senti-ment lexicon, we assume r is also responsible forgenerating a word feature vm, based on the senti-ment association H , which is equal to its polarityscore smin the lexicon.
In this case, the observeddata becomes (< h,m >, vm), and the latent sen-timent label r is responsible for generating bothword m, and word feature vm.
For example, forthe phrase<room, filthy>, we observe a word fea-ture v = ?5, since the sentiment polarity score forthe word ?filthy?
is?5.
GivenH , sentiment labels1 or 2 are more likely to generate a word feature?5.
Also, people tend to use ?filthy?
to expresslow ratings, like 1 or 2, so the sentiment labels andratings can be aligned.At the product level, for each product p andeach aspect k, we define a multinomial distribu-tion ?p,koverR sentiment labels.
Since Ypalreadygives us the big picture about the overall senti-1195ment expressed on this product, we assume ?p,kis drawn from a dirichlet distribution Dir(pip,k)with asymmetric concentration parameters, wherepip,k = f(Yp,?k, ?b).
We can use a linearparametrization, and setf(Yp,?k, ?b) = ?1kYp + ?0k+ ?b(1)?1kcaptures the influence of the product overallrating distribution, and can favour certain sen-timent labels in the prior.
?0kand ?bare theaspect-specific and corpus-level bias, respectively.Through this linear parametrization, we build a di-rect matching between sentiment label i and ratingi.
For example, for a product p, if its overall rat-ing distribution Yphas high probability over rat-ing 4, for aspect k, we assume its product-aspect-sentiment label distribution also has high probabil-ity on sentiment label 4 in the prior.
The actual as-pect rating is affected by both the text which talksabout aspect k, and also the prior.To sum up, we assume the generative process asfollows:?
For each aspect k = 1, 2, .
.
.K,?
draw an aspect-word distribution ?ak?Dir(?a)?
For each sentiment label r = 1, 2, .
.
.
R,draw an aspect-sentiment label-worddistribution ?sk,r?
Dir(?s)?
For each product p ?
P ,?
draw a product-aspect distribution ?p?Dir(?)?
for each aspect k, draw a product-aspect-sentiment label distribu-tion ?p,k?
Dir(pip,k) wherepip,k = f(Yp,?k, ?b)?
For each phrase f =< h,m > of product p,1.
Draw an aspect z from ?p2.
Draw a sentiment label r from ?p,z3.
Draw an aspect word h from ?az4.
Draw a sentiment word m from ?sz,r.If m ?
L, generate a word feature vmbased on H .By integrating out ?, ?
and ?, the joint proba-bility can be defined as:P (z, r,h,m,v|?,?a,?s,pi,H) =P (z|?
)P (r|z,pi)P (h|z,?a)P (m|z, r,?s)P (v|r,H) (2)3.4 InferenceWe use Gibbs Sampling(Griffiths and Steyvers,2004) to estimate the posterior distribution giventhe observed data.We jointly sample the aspect z and sentimentlabel r for the ith phrase < h,m > of product p,given the assignments of other phrases:P (zi= k, ri= l|z?i, r?i,h,m,v) ?
(np,k+ ?
)nak,h+ ?a?h?
(nak,h?+ ?a)np,k,l+ pip,k,l?l?
(np,k,l?+ pip,k,l?
)nsk,l,m+ ?s?m?
(nsk,l,m?+ ?s)g(m, l)(3)where g(m, l) = H(vm|l) if m ?
L. In thiscase, when we sample the sentiment label r for thisphrase, the probability of generating word featurevmfrom r is also considered.
For example, theword ?excellent?
has a word feature value vm= 5.Based on H , the probability of generating a wordfeature 5 is higher for sentiment labels with largervalues.
If m /?
L, there is no g(m, l) term, sinceno word feature is associated with this phrase.
InEquation 3, np,kis the number of times aspect kis assigned to phrases of product p, and nak,histhe number of times aspect word h is assigned toaspect k. np,k,lis the number of times sentimentlabel l is assigned to aspect k for product p, andnsk,l,mis the number of times sentiment word mis assigned to aspect k and sentiment label l. Allthese counts exclude assignments for the currentphrase < h,m >.Based on the samples, we can estimate ?p,k,ras:?p,k,r=np,k,r+ pip,k,r?r?
(np,k,r?+ pip,k,r?
)(4)Since sentiment labels and atings are aligned, theaspect rating tpkof product p on aspect k can besimply calculated as the expectation of ?p,k:tpk=?r?p,k,r?
r (5)4 ExperimentsIn this section, we describe the experiments andanalyze the results.11964.1 DatasetWe use the TripAdvisor dataset1(Wang et al.,2010) for evaluation, since in this dataset, reviewsare not only associated with overall ratings, butalso with ground truth aspect ratings on 7 aspects:value, room, location, cleanliness, check in/frontdesk, service, business service.
All the ratings inthe dataset are in the range from 1 star to 5 stars.We first remove reviews with any missing aspectratings or very short reviews(less than three sen-tences).
Then we adopt the dependency parsertechnique to identify opinion phrases, and collectphrases with adjective sentiment words.
The de-pendency parser can deal with conjunctions, nega-tions and bigram aspect words, and it results in thebest performance according to (Moghaddam andEster, 2012).
Some sample phrases are shown inTable 1.
All words are converted into lower case,and we remove phrases containing words that ap-pear no more than 10 times or stop words.
Sincewe are only interested in product-level aspect rat-ing prediction, for each product, we aggregate allthe review overall ratings to get the overall ratingdistribution.
The statistics of the dataset is shownin Table 2.
The average rating is the rating av-eraged over all reviews and all products.
As wecan see, positive reviews are dominant in the data,which raises the challenge of discovering negativesentiment topics.Sentences PhrasesThe room, facing thecourtyard, was large andcomfortable.<room, large>,<room, comfortable>The room was not reallyclean.<room, no clean>Internet access wasavailable.<Internet access,available>Table 1: Sample extracted phrases#Products #Reviews Avg rating #Phrases1850 61306 4.03 740982Table 2: Statistics of the dataset4.2 MetricsWe use three evaluation metrics for comparison.RMSE: Root-mean-square error is used to mea-sure the difference between the predicted aspect1http://sifaka.cs.uiuc.edu/?wang296/Data/index.htmlratings and ground truth aspect ratings.
It is de-fined as:RMSE =??p?k(tpk?
?tpk)2|P | ?K(6)where tpkis the predicted aspect rating for productp on aspect k, and?tpkis the ground truth.Precision@N: For each aspect k, we rank thehotels based on their predicted aspect ratings, andget the top N results.
A hotel is considered rele-vant if its ground truth aspect rating is in the top10% of the ground truth aspect ratings of all ho-tels.
Precision@N is defined as the percentage ofthe top N results that are relevant:Precision@N =|{relevant hotels} ?
{top N ranked hotels}|N(7)We use N = 10, and the result is averaged over Kaspects.
?hotel: Pearson correlation across hotels(Wanget al., 2010) is defined as:?hotel =?k?
(tk, ?tk)K(8)where tk is the predicted aspect rating vector forall hotels on aspect k, and?tk is the correspondingground truth vector.
?
(tk, ?tk) is the Pearson cor-relation between these two vectors.
It measureshow the predicted ratings of aspect k can preservethe order in the ground truth(Wang et al., 2010).If we can predict an aspect-specific ranking sim-ilar to the ground truth, we can use the predictedaspect ratings to answer questions like ?Is hotel abetter than hotel b on aspect k?
?4.3 BaselinesThe first three baselines are Local Prediction,Global Prediction and Graph Propagation.They all separate aspect extraction and sentimentidentification.
For each phrase f =< h,m > fromreview d of product p, we first find the aspect as-signment of this phrase.
Then, we use three meth-ods to get the phrase rating.
Local Prediction(Lu etal., 2009) simply uses the overall rating of d as itsphrase rating.
Global Prediction(Lu et al., 2009)trains a multi-class classifier to classify the senti-ment word m into a rating category r ?
1, 2 .
.
.
R,1197Figure 2: Method for aspect extraction in LocalPrediction, Global Prediction and Graph Propaga-tionthen assigns r as the phrase rating.
Graph Prop-agation(Brody and Elhadad, 2010) builds a con-junction graph for sentiment words, and uses a La-bel Propagation algorithm on the graph to learn thesentiment polarity score for each sentiment word.The score of m is set as phrase rating.
Finally,we aggregate all the phrases of each aspect to pre-dict the aspect ratings.
To apply these methods inour experiments, in the aspect extraction step, weadapt our model to extract only aspects, as shownin Figure 2.
In this simplified model, no sentimentlabels is involved, and the latent aspect explainsboth the aspect word and sentiment word.ILDA(Moghaddam and Ester, 2011) was pro-posed for aspect rating prediction, but it fails todeal with the sentiment label alignment problem,so it cannot be directly used for this task.
We adoptthe common approach of providing seed words toset priors for each sentiment topic.LRR(Wang et al., 2010) was proposed to pre-dict aspect ratings for each review, but it can alsobe used to predict product aspect ratings by ag-gregating all the reviews of a product into a single?h-review?
(Wang et al., 2011).
First, we can runa topic model to learn aspects, and annotate eachsentence with an aspect.
Then LRR is applied onthe annotated sentences to predict aspect ratings.This approach provided the best result, accordingto (Wang et al., 2011).
In the first step we use thesentence-LDA(Jo and Oh, 2011) to annotate sen-tences, which is slightly different from the originalmethod, but still provides a good analogy.We also test two simplified version of the SATMmodel.
First, we remove the part which involvessentiment lexicons, so we only use the productoverall rating distribution.
We call this methodSATM-O.
Second, we use only sentiment lexi-cons, ignoring the influence of overall rating dis-tribution.
We call it SATM-L.
These two baselinescan help us identify how the sentiment lexicon andoverall rating distribution can improve the results,if used separately.Our last baseline simply uses the overall ratingof a hotel as its aspect ratings.
For each hotel, itsoverall rating is defined as the average overall rat-ing of its reviews.
This method is referred to asOverall.4.4 Experimental SetupFor all topic modelling based approaches, thenumber of aspects is set to 7.
Since we can evalu-ate aspect rating prediction only on the predefinedaspects, we need to ensure the discovered aspectsmatch the predefined aspects.
To do this, we adoptthe common approach of providing a few seedwords for each aspect as priors, as in (Wang et al.,2010).
The seed words are listed in Table 3.
Theremay be better methods to use seed words for as-pect discovery (Jagarlamudi et al., 2012; Mukher-jee and Liu, 2012), and it would be interesting tocombine their methods with ours.
However, thisis beyond the scope of this paper, and we list it asfuture work.Aspects Seed wordsValue value, price, worthRoom room, roomsLocation locationCleanliness room, dirty, smelled, cleanCheck in/front desk staffService service, breakfast, foodBusiness service internet, wifiTable 3: Seed words for aspect discoveryWe use 5 sentiment labels in SATM, SATM-L and SATM-O, as this is the number of dis-tinct ratings.
The lexicon L used in our experi-ment is part of (Taboada et al., 2011) where wordsare associated with polarity scores in the range[?5,?1] ?
[1, 5].
We observe that words with po-larity score 1 and?1 express too weak sentiments,so we discard them in our experiment.
To gettraining instances for sentiment association H , wetreat each appearance of wordm ?
L in the data asone training instance.
The polarity score smis di-rectly retrieved from L, and the sentiment label rmis the overall rating of review d where m appears.This approach avoids the need for manual annota-tion of sentiment labels, and the annotation resultcaptures the characteristics of the dataset.
How-1198ever, all training instances in a review will havethe same sentiment label, which means that we as-sume all sentiment words in a review express thesame sentiment, no matter what aspects they talkabout.
This is not true, thus will introduce noise tothe training.
To reduce noise, for words with pos-itive polarity score, we ignore their appearance inreviews with rating 1 and 2, since we assume pos-itive sentiment words rarely express negative sen-timents, even if they appear in negative reviews.Therefore, H(sm|rm) = 0 for rm= 1, 2 and smin the range [2, 5].
A similar method is used to dealwith words with negative polarity score.For Global Prediction, in (Lu et al., 2009), theprior for the multi-class classifier is uniform, whilein our experiment, for product p, we used productoverall rating distribution on r as the prior for rat-ing category r, which achieves better results thanuniform prior.The Graph Propagation method requires a smallset of sentiment words as seeds, from which the al-gorithm can learn sentiment score for other words.The method in (Brody and Elhadad, 2010) con-structs these seed words based on morphology inan unsupervised way, and can only support twokinds of sentiment: positive and negative.
In ourexperiment, since the sentiment lexicon is avail-able, the sentiment seed words are from the lexi-con, and we update the polarity score for those notin the lexicon.For ILDA, since we need to provide seed wordsas priors for sentiment topics, we have two op-tions, and we use both for experiment.
First,we can employ the common approach of usingtwo sentiment labels(R=2, positive and negative).Then, words with positive polarity scores in lexi-con L are used as priors for the positive sentimenttopic, and similarly words with negative polarityscores for negative sentiment topic.
An alternativeapproach is to use 5 sentiment labels(R=5).
It pro-vides finer grained sentiment extraction, but raisesthe question of how to choose seed words for eachsentiment topic.
To do this, we use the full senti-ment lexicon in (Taboada et al., 2011), where sen-timent words have polarity score in the range of[?5,?1] ?
[1, 5].
We divide the lexicon, and usewords with polarity score 4 and 5 as prior for thesentiment topic with label 5.
Then, words with po-larity score 2 and 3 are used for the sentiment topicwith label 4, and so on.For all topic modelling based approaches, weset the number of iterations for Gibbs Samplingto 3000, and take samples from the markov chainevery 50 iterations after a burn-in period of 1000iterations.
In SATM and SATM-O, for all aspectsk, we need to choose the parameters ?k and alsowb.
We use a small portion of dataset with groundtruth to choose the best value, and we set ?1k= 20,w0k= 0.01, wb= 0.
Automatically learning theseparameters are feasible.
One possible option is touse stochastic EM sampling scheme, as in (Mimnoand McCallum, 2008).
For the LRR implementa-tion2, we use the default parameters included inthe package, and train the model with seed wordsprovided by the author(Wang et al., 2010).4.5 ResultsThe experimental results are listed in Table 4.
ForRMSE, the smaller the better, while for the othertwo measures, the larger the better.
Graph Prop-agation, ILDA and SATM-L do not use the over-all ratings(except for training sentiment associa-tion H), so we group them together.
Similarlywe group Local Prediction, Global Prediction,SATM-O and SATM.
The Overall method is a spe-cial baseline that does not do any aspect based pre-diction.
For the LRR method, after the first step ofsentence annotation, we notice that sentence-LDAfails to annotate the ?h-review?
of some hotel withall 7 aspects, mainly because these hotels are as-sociated with less reviews.
In this case, the LRRmodel will fail in the second step, so we do notinclude LRR in Table 4.
Instead, we comparedour method with LRR on a subset of products thatcomment on all aspects based on the sentence an-notation.
There are 1533 hotels in this subset, andthe result is shown in Table 5.
Note that our exper-imental results for LRR are far worse than thosereported in the original paper(Wang et al., 2011).We believe this maybe due to different parametersettings, or due to the choice of different reviews.We observe that SATM achieves the best RMSEvalue, i.e., it produces the most accurate aspect rat-ing prediction.
The Overall method does better inranking all the hotels(?hotel), but SATM is betterat ranking top hotels(P@10).
When we comparethe results of SATM with SATM-L and SATM-O, we find that the good performance of SATMis mainly due to the use of the overall rating distri-bution.
On one hand, this is reasonable, since in-2http://sifaka.cs.uiuc.edu/?wang296/Codes/LARA.zip1199Sentiment label Top sentiment words1 old, dirty, worn, older, dark, stained, broken, dated, outdated, bad2 small, tiny, little, noisy, single, double, uncomfortable, smaller, larger, narrow3 large, double, big, mini, hard, main, huge, twin, single, jacuzzi4 nice, comfortable, modern, clean, new, good, great, flat, big, comfy5 large, huge, great, beautiful, big, lovely, separate, spacious, wonderful, excellentTable 6: Top sentiment words for aspect ?room?
with different sentiment labelsMethods RMSE P@10 ?hotelILDA,R=2 1.202 0.30 0.193ILDA,R=5 1.096 0.257 0.222Graph Propagation 0.718 0.271 0.442SATM-L 0.774 0.443 0.483Local Prediction 0.572 0.486 0.761Global Prediction 0.625 0.30 0.778SATM-O 0.429 0.80 0.841SATM 0.384 0.814 0.854Overall 0.415 0.80 0.863Table 4: Experimental results except LRRMethods RMSE P@10 ?hotelLRR 1.018 0.3 0.404SATM 0.373 0.829 0.849Table 5: Experimental comparison with LRRtuitively aspect ratings usually do not diverge toofar from the overall rating, especially for hotelswith higher overall ratings.
As we can see fromthe result of Overall, the overall rating has goodcorrelation with aspect ratings, and using overallrating only is already a strong predictor for as-pect ratings.
Also, in most cases, methods usingoverall ratings(Overall and the four methods in themiddle of Table 4) are better than others(first fourmethods).
On the other hand, we should not relyonly on the overall rating distribution.
By incor-porating the sentiment lexicon, for RMSE, SATMachieves 10% improvement over SATM-O and 7%improvement than Overall.
Also, the overall ratingmay not always be a good aspect rating predictor,depending on the dataset.To take a closer look at cases where the over-all rating is not a good aspect rating predictor,we evaluate the RMSE on different subsets of ho-tels.
We divide the hotels into different overall rat-ing ranges: [1.2), [2,3), [3,4) and [4,5].
The re-sults are shown in Table 7.
Going from the [4,5]group to [1,2) group, the overall rating becomesless and less reliable to predict aspect ratings, andthe gain of SATM increases compared to SATM-Methods [1,2) [2,3) [3,4) [4-5]Local Prediction 0.789 0.772 0.621 0.456Global Prediction 1.013 0.884 0.584 0.567SATM-O 0.703 0.564 0.446 0.359SATM 0.606 0.494 0.394 0.332Overall 0.735 0.612 0.431 0.320Table 7: RMSE on hotels with different overallrating rangesO and Overall.
For a hotel with higher overallrating(good hotel), its aspect ratings are closer tothe overall rating.
This matches our intuition thatgood hotels are expected to be good on most as-pects, if not on all aspects.
For a hotel with av-erage and lower overall rating, the average differ-ence between aspect ratings and overall rating islarger.
In this case, the overall rating can not tellus the whole story, which calls for aspect basedprediction.
Our method achieves the best RMSEgain on this group of hotels.4.6 Qualitative analysisTo provide a qualitative analysis, we can list thetop words for the aspect-sentiment label-word dis-tributions.
In Table 6, we list them for the aspect?room?, with 5 different sentiment labels.
We ob-serve that, as the sentiment label value increases,the sentiment topics express more and more pos-itive sentiments.
This means the sentiment labelsand ratings are indeed aligned, so that we can usethese sentiment labels to predict ratings.5 Conclusion and future workIn this paper, we proposed a sentiment alignedtopic model(SATM) for product aspect rating pre-diction.
By incorporating the overall rating distri-bution and a sentiment lexicon, our SATM modelcan align sentiment labels with ratings.
Experi-ments on a TripAdvisor dataset demonstrate theeffectiveness of SATM on aspect rating prediction.In SATM, for each product and each aspect, themultinomial distribution over sentiment labels has1200prior parameterized by product overall rating dis-tribution.
We assume linear dependency, but it willbe interesting to explore other dependencies.
An-other direction is to learn the parameters ?k auto-matically, so that ?k can be different for differentk, capturing the influence of the overall rating ondifferent aspects.Finally, we assume each phrase is associatedwith one latent aspect.
However, aspects maybe correlated.
For example, the phrase <room,filthy> gives us information about the aspect roomand also the aspect cleanliness.
To deal with thisproblem, we can relax the assumption that onephrase talks about one aspect, or we can modelcorrelation among aspects.AcknowledgmentsThis research is supported by NSERC DiscoveryGrant.
The authors thank Dr. Maite Taboada forproviding the sentiment lexicon.ReferencesDavid Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topicmodeling via dirichlet forest priors.
In Proceedingsof the 26th Annual International Conference on Ma-chine Learning, ICML ?09, pages 25?32, New York,NY, USA.
ACM.David Andrzejewski, Xiaojin Zhu, Mark Craven, andBenjamin Recht.
2011.
A framework for incorpo-rating general domain knowledge into latent dirich-let allocation using first-order logic.
In Proceedingsof the Twenty-Second International Joint Conferenceon Artificial Intelligence - Volume Volume Two, IJ-CAI?11, pages 1171?1177.
AAAI Press.Sasha Blair-goldensohn, Tyler Neylon, Kerry Hannan,George A. Reis, Ryan Mcdonald, and Jeff Reynar.2008.
Building a sentiment summarizer for localservice reviews.
In WWW Workshop on NLP in theInformation Explosion Era.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
J. Mach.
Learn.Res., 3:993?1022, March.Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 804?812, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Zhiyuan Chen, Arjun Mukherjee, Bing Liu, MeichunHsu, Mal?u Castellanos, and Riddhiman Ghosh.2013.
Exploiting domain knowledge in aspect ex-traction.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP 2013, 18-21 October 2013, Grand Hy-att Seattle, Seattle, Washington, USA, A meeting ofSIGDAT, a Special Interest Group of the ACL, pages1655?1667.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(Suppl.
1):5228?5235,April.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22Nd Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR ?99,pages 50?57, New York, NY, USA.
ACM.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Jagadeesh Jagarlamudi, Hal Daum?e, III, andRaghavendra Udupa.
2012.
Incorporating lex-ical priors into topic models.
In Proceedings ofthe 13th Conference of the European Chapter ofthe Association for Computational Linguistics,EACL ?12, pages 204?213, Stroudsburg, PA, USA.Association for Computational Linguistics.Yohan Jo and Alice H. Oh.
2011.
Aspect and sen-timent unification model for online review analy-sis.
In Proceedings of the Fourth ACM Interna-tional Conference on Web Search and Data Mining,WSDM ?11, pages 815?824, New York, NY, USA.ACM.Suin Kim, Jianwen Zhang, Zheng Chen, Alice H.Oh, and Shixia Liu.
2013.
A hierarchical aspect-sentiment model for online reviews.
In Proceedingsof the Twenty-Seventh AAAI Conference on ArtificialIntelligence, July 14-18, 2013, Bellevue, Washing-ton, USA.Himabindu Lakkaraju, Chiranjib Bhattacharyya, Indra-jit Bhattacharya, and Srujana Merugu.
2011.
Ex-ploiting coherence for the simultaneous discoveryof latent facets and associated sentiments.
In Pro-ceedings of the Eleventh SIAM International Con-ference on Data Mining, SDM 2011, April 28-30,2011, Mesa, Arizona, USA, pages 498?509.Angeliki Lazaridou, Ivan Titov, and CarolineSporleder.
2013.
A bayesian model for jointunsupervised induction of sentiment, aspect anddiscourse representations.
In Proceedings of the51st Annual Meeting of the Association for Com-putational Linguistics, ACL 2013, 4-9 August 2013,Sofia, Bulgaria, Volume 1: Long Papers, pages1630?1639.1201Fangtao Li, Minlie Huang, and Xiaoyan Zhu.
2010.Sentiment analysis with global topics and local de-pendency.
In Proceedings of the Twenty-FourthAAAI Conference on Artificial Intelligence, AAAI2010, Atlanta, Georgia, USA, July 11-15, 2010.Chenghua Lin and Yulan He.
2009.
Joint senti-ment/topic model for sentiment analysis.
In Pro-ceedings of the 18th ACM Conference on Informa-tion and Knowledge Management, CIKM ?09, pages375?384, New York, NY, USA.
ACM.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Synthesis Lectures on Human Language Tech-nologies.
Morgan & Claypool Publishers.Chong Long, Jie Zhang, Minlie Huang, Xiaoyan Zhu,Ming Li, and Bin Ma.
2014.
Estimating feature rat-ings through an effective review selection approach.Knowl.
Inf.
Syst., 38(2):419?446.Yue Lu, ChengXiang Zhai, and Neel Sundaresan.2009.
Rated aspect summarization of short com-ments.
In Proceedings of the 18th InternationalConference on World Wide Web, WWW ?09, pages131?140, New York, NY, USA.
ACM.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic sentiment mix-ture: Modeling facets and opinions in weblogs.
InProceedings of the 16th International Conference onWorld Wide Web, WWW ?07, pages 171?180, NewYork, NY, USA.
ACM.David M. Mimno and Andrew McCallum.
2008.Topic models conditioned on arbitrary features withdirichlet-multinomial regression.
In the Conferenceon Uncertainty in Artificial Intelligence, pages 411?418.Samaneh Moghaddam and Martin Ester.
2011.
Ilda:Interdependent lda model for learning latent aspectsand their ratings from online product reviews.
InProceedings of the 34th International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, SIGIR ?11, pages 665?674, NewYork, NY, USA.
ACM.Samaneh Moghaddam and Martin Ester.
2012.
On thedesign of lda models for aspect-based opinion min-ing.
In Proceedings of the 21st ACM InternationalConference on Information and Knowledge Man-agement, CIKM ?12, pages 803?812, New York,NY, USA.
ACM.Samaneh Moghaddam and Martin Ester.
2013.
Theflda model for aspect-based opinion mining: Ad-dressing the cold start problem.
In Proceedings ofthe 22Nd International Conference on World WideWeb, WWW ?13, pages 909?918.Arjun Mukherjee and Bing Liu.
2012.
Aspect extrac-tion through semi-supervised modeling.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Long Papers - Vol-ume 1, ACL ?12, pages 339?348, Stroudsburg, PA,USA.
Association for Computational Linguistics.Ana-Maria Popescu and Oren Etzioni.
2005.
Ex-tracting product features and opinions from reviews.In Proceedings of the Conference on Human Lan-guage Technology and Empirical Methods in Natu-ral Language Processing, HLT ?05, pages 339?346,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Christina Sauper and Regina Barzilay.
2013.
Auto-matic aggregation by joint modeling of aspects andvalues.
J. Artif.
Int.
Res., 46(1):89?127, January.Christina Sauper, Aria Haghighi, and Regina Barzilay.2011.
Content models with attitude.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 350?358,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Comput.
Lin-guist., 37(2):267?307, June.Ivan Titov and Ryan McDonald.
2008a.
Modelingonline reviews with multi-grain topic models.
InProceedings of the 17th International Conference onWorld Wide Web, WWW ?08, pages 111?120, NewYork, NY, USA.
ACM.Ivan Titov and Ryan T. McDonald.
2008b.
A jointmodel of text and aspect ratings for sentiment sum-marization.
In ACL 2008, Proceedings of the 46thAnnual Meeting of the Association for Computa-tional Linguistics, June 15-20, 2008, Columbus,Ohio, USA, pages 308?316.Hongning Wang, Yue Lu, and Chengxiang Zhai.
2010.Latent aspect rating analysis on review text data:A rating regression approach.
In Proceedings ofthe 16th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, KDD ?10,pages 783?792, New York, NY, USA.
ACM.HongningWang, Yue Lu, and ChengXiang Zhai.
2011.Latent aspect rating analysis without aspect key-word supervision.
In Proceedings of the 17th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, KDD ?11, pages 618?626, New York, NY, USA.
ACM.Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-ing Li.
2010.
Jointly modeling aspects and opin-ions with a maxent-lda hybrid.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?10, pages 56?65, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learningfrom labeled and unlabeled data with label propa-gation.
Technical report, Technical Report CMU-CALD-02-107, Carnegie Mellon University.1202
