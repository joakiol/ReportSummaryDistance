Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 32?42,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsCombining Morpheme-based Machine Translation withPost-processing Morpheme PredictionAnn Clifton and Anoop SarkarSimon Fraser UniversityBurnaby, British Columbia, Canada{ann clifton,anoop}@sfu.caAbstractThis paper extends the training and tun-ing regime for phrase-based statistical ma-chine translation to obtain fluent trans-lations into morphologically complex lan-guages (we build an English to Finnishtranslation system).
Our methods useunsupervised morphology induction.
Un-like previous work we focus on morpho-logically productive phrase pairs ?
ourdecoder can combine morphemes acrossphrase boundaries.
Morphemes in the tar-get language may not have a correspondingmorpheme or word in the source language.Therefore, we propose a novel combina-tion of post-processing morphology pre-diction with morpheme-based translation.We show, using both automatic evaluationscores and linguistically motivated analy-ses of the output, that our methods out-perform previously proposed ones and pro-vide the best known results on the English-Finnish Europarl translation task.
Ourmethods are mostly language independent,so they should improve translation intoother target languages with complex mor-phology.1 Translation and MorphologyLanguages with rich morphological systemspresent significant hurdles for statistical ma-chine translation (SMT), most notably datasparsity, source-target asymmetry, and prob-lems with automatic evaluation.In this work, we propose to address the prob-lem of morphological complexity in an English-to-Finnish MT task within a phrase-based trans-lation framework.
We focus on unsupervisedsegmentation methods to derive the morpholog-ical information supplied to the MT model inorder to provide coverage on very large data-sets and for languages with few hand-annotatedresources.
In fact, in our experiments, unsuper-vised morphology always outperforms the useof a hand-built morphological analyzer.
Ratherthan focusing on a few linguistically motivatedaspects of Finnish morphological behaviour, wedevelop techniques for handling morphologicalcomplexity in general.
We chose Finnish as ourtarget language for this work, because it ex-emplifies many of the problems morphologicallycomplex languages present for SMT.
Among allthe languages in the Europarl data-set, Finnishis the most difficult language to translate fromand into, as was demonstrated in the MT Sum-mit shared task (Koehn, 2005).
Another reasonis the current lack of knowledge about how to ap-ply SMT successfully to agglutinative languageslike Turkish or Finnish.Our main contributions are: 1) the intro-duction of the notion of segmented translationwhere we explicitly allow phrase pairs that canend with a dangling morpheme, which can con-nect with other morphemes as part of the trans-lation process, and 2) the use of a fully seg-mented translation model in combination witha post-processing morpheme prediction system,using unsupervised morphology induction.
Bothof these approaches beat the state of the arton the English-Finnish translation task.
Mor-phology can express both content and functioncategories, and our experiments show that it isimportant to use morphology both within thetranslation model (for morphology with content)and outside it (for morphology contributing tofluency).Automatic evaluation measures for MT,BLEU (Papineni et al, 2002), WER (WordError Rate) and PER (Position IndependentWord Error Rate) use the word as the basicunit rather than morphemes.
In a word com-32prised of multiple morphemes, getting even asingle morpheme wrong means the entire word iswrong.
In addition to standard MT evaluationmeasures, we perform a detailed linguistic anal-ysis of the output.
Our proposed approachesare significantly better than the state of the art,achieving the highest reported BLEU scores onthe English-Finnish Europarl version 3 data-set.Our linguistic analysis shows that our modelshave fewer morpho-syntactic errors compared tothe word-based baseline.2 Models2.1 Baseline ModelsWe set up three baseline models for compari-son in this work.
The first is a basic word-based model (called Baseline in the results);we trained this on the original unsegmentedversion of the text.
Our second baseline is afactored translation model (Koehn and Hoang,2007) (called Factored), which used as factorsthe word, ?stem?1 and suffix.
These are de-rived from the same unsupervised segmenta-tion model used in other experiments.
The re-sults (Table 3) show that a factored model wasunable to match the scores of a simple word-based baseline.
We hypothesize that this maybe an inherently difficult representational formfor a language with the degree of morphologi-cal complexity found in Finnish.
Because themorphology generation must be precomputed,for languages with a high degree of morpho-logical complexity, the combinatorial explosionmakes it unmanageable to capture the full rangeof morphological productivity.
In addition, be-cause the morphological variants are generatedon a per-word basis within a given phrase, itexcludes productive morphological combinationacross phrase boundaries and makes it impossi-ble for the model to take into account any long-distance dependencies between morphemes.
Weconclude from this result that it may be moreuseful for an agglutinative language to use mor-phology beyond the confines of the phrasal unit,and condition its generation on more than justthe local target stem.
In order to compare the1see Section 2.2.performance of unsupervised segmentation fortranslation, our third baseline is a segmentedtranslation model based on a supervised segmen-tation model (called Sup), using the hand-builtOmorfi morphological analyzer (Pirinen and Lis-tenmaa, 2007), which provided slightly higherBLEU scores than the word-based baseline.2.2 Segmented TranslationFor segmented translation models, it cannot betaken for granted that greater linguistic accu-racy in segmentation yields improved transla-tion (Chang et al, 2008).
Rather, the goal insegmentation for translation is instead to maxi-mize the amount of lexical content-carrying mor-phology, while generalizing over the informationnot helpful for improving the translation model.We therefore trained several different segmenta-tion models, considering factors of granularity,coverage, and source-target symmetry.We performed unsupervised segmentation ofthe target data, using Morfessor (Creutz andLagus, 2005) and Paramor (Monson, 2008), twotop systems from the Morpho Challenge 2008(their combined output was the Morpho Chal-lenge winner).
However, translation modelsbased upon either Paramor alone or the com-bined systems output could not match the word-based baseline, so we concentrated on Morfes-sor.
Morfessor uses minimum description lengthcriteria to train a HMM-based segmentationmodel.
When tested against a human-annotatedgold standard of linguistic morpheme segmen-tations for Finnish, this algorithm outperformscompeting unsupervised methods, achieving anF-score of 67.0% on a 3 million sentence cor-pus (Creutz and Lagus, 2006).
Varying the per-plexity threshold in Morfessor does not segmentmore word types, but rather over-segments thesame word types.
In order to get robust, com-mon segmentations, we trained the segmenteron the 5000 most frequent words2; we then usedthis to segment the entire data set.
In orderto improve coverage, we then further segmented2For the factored model baseline we also used thesame setting perplexity = 30, 5,000 most frequent words,but with all but the last suffix collapsed and called the?stem?.33Training Set Test SetTotal 64,106,047 21,938Morph 30,837,615 5,191Hanging Morph 10,906,406 296Table 1: Morpheme occurences in the phrase tableand in translation.any word type that contained a match from themost frequent suffix set, looking for the longestmatching suffix character string.
We call thismethod Unsup L-match.After the segmentation, word-internal mor-pheme boundary markers were inserted intothe segmented text to be used to reconstructthe surface forms in the MT output.
Wethen trained the Moses phrase-based system(Koehn et al, 2007) on the segmented andmarked text.
After decoding, it was a sim-ple matter to join together all adjacent mor-phemes with word-internal boundary markersto reconstruct the surface forms.
Figure 1(a)gives the full model overview for all the vari-ants of the segmented translation model (super-vised/unsupervised; with and without the Un-sup L-match procedure).Table 1 shows how morphemes are being usedin the MT system.
Of the phrases that includedsegmentations (?Morph?
in Table 1), roughly athird were ?productive?, i.e.
had a hanging mor-pheme (with a form such as stem+) that couldbe joined to a suffix (?Hanging Morph?
in Ta-ble 1).
However, in phrases used while decodingthe development and test data, roughly a quar-ter of the phrases that generated the translatedoutput included segmentations, but of these,only a small fraction (6%) had a hanging mor-pheme; and while there are many possible rea-sons to account for this we were unable to finda single convincing cause.2.3 Morphology GenerationMorphology generation as a post-processing stepallows major vocabulary reduction in the trans-lation model, and allows the use of morpholog-ically targeted features for modeling inflection.A possible disadvantage of this approach is thatin this model there is no opportunity to con-sider the morphology in translation since it isremoved prior to training the translation model.Morphology generation models can use a vari-ety of bilingual and contextual information tocapture dependencies between morphemes, of-ten more long-distance than what is possible us-ing n-gram language models over morphemes inthe segmented model.Similar to previous work (Minkov et al, 2007;Toutanova et al, 2008), we model morphologygeneration as a sequence learning problem.
Un-like previous work, we use unsupervised mor-phology induction and use automatically gener-ated suffix classes as tags.
The first phase of ourmorphology prediction model is to train a MTsystem that produces morphologically simplifiedword forms in the target language.
The outputword forms are complex stems (a stem and somesuffixes) but still missing some important suffixmorphemes.
In the second phase, the output ofthe MT decoder is then tagged with a sequenceof abstract suffix tags.
In particular, the out-put of the MT decoder is a sequence of complexstems denoted by x and the output is a sequenceof suffix class tags denoted by y.
We use a listof parts from (x,y) and map to a d -dimensionalfeature vector ?
(x,y), with each dimension be-ing a real number.
We infer the best sequenceof tags using:F (x) = argmaxyp(y | x,w)where F (x ) returns the highest scoring outputy?.
A conditional random field (CRF) (Laffertyet al, 2001) defines the conditional probabilityas a linear score for each candidate y and a globalnormalization term:log p(y | x,w) = ?
(x,y) ?w ?
logZwhere Z =?y?
?GEN(x) exp(?(x,y?)
?
w).
Weuse stochastic gradient descent (using crfsgd3)to train the weight vector w. So far, this isall off-the-shelf sequence learning.
However, theoutput y?
from the CRF decoder is still only asequence of abstract suffix tags.
The third andfinal phase in our morphology prediction model3http://leon.bottou.org/projects/sgd34Morphological Pre-ProcessingEnglish Training Data Finnish Training Datawordsstem+ +morphwordsPost-Process:Morph Re-Stitchingstem+ +morphEvaluation against original referenceFully inflected surface formMT SystemAlignment:word          word          wordstem+      +morph        stem(a) Segmented Translation ModelMorphlgica Pe-ci-gsnETDrrrrrrrrrrnETDrrrrrrrrrrnETDlgictrrrrrtcETFwdtrrrrrlgicMETFwE EePm+ r:TiR:TEmillP-erdEnglish?Training?Data Finnish?Training?DatanETDllgictrtcETFwdtnETDl:ElgR:TEmillrdsMETFwrSiRpgPgmwP-elgictrtcETFwdt:ElgR:TEmillrvsruSfMETFwE Eehryi-iT+gPE-mEcF iArlgicsrlgictcETFwdt ?+-e?+eirMEDi l?T?+mir?ETcrc+FFP-elgictcETFwdtrtcETFwv??
?+gPE-r+e+P-lgrETPeP-+ rTi?iTi-mif?
hrP-?
imgiDrl?T?+mir?ETcMETFwE EePm+ r:TiR:TEmillP-ervlgictrtcETFwdtrtcETFwv(b) Post-Processing Model Translation & GenerationFigure 1: Training and testing pipelines for the SMT models.is to take the abstract suffix tag sequence y?
andthen map it into fully inflected word forms, andrank those outputs using a morphemic languagemodel.
The abstract suffix tags are extractedfrom the unsupervised morpheme learning pro-cess, and are carefully designed to enable CRFtraining and decoding.
We call this model CRF-LM for short.
Figure 1(b) shows the full pipelineand Figure 2 shows a worked example of all thesteps involved.We use the morphologically segmented train-ing data (obtained using the segmented corpusdescribed in Section 2.24) and remove selectedsuffixes to create a morphologically simplifiedversion of the training data.
The MT model istrained on the morphologically simplified train-ing data.
The output from the MT system isthen used as input to the CRF model.
TheCRF model was trained on a ?210,000 Finnishsentences, consisting of ?1.5 million tokens; the2,000 sentence Europarl test set consisted of41,434 stem tokens.
The labels in the outputsequence y were obtained by selecting the mostproductive 150 stems, and then collapsing cer-tain vowels into equivalence classes correspond-ing to Finnish vowel harmony patterns.
Thus4Note that unlike Section 2.2 we do not use UnsupL-match because when evaluating the CRF model on thesuffix prediction task it obtained 95.61% without usingUnsup L-match and 82.99% when using Unsup L-match.variants -ko?
and -ko become vowel-generic en-clitic particle -kO, and variants -ssa?
and -ssabecome the vowel-generic inessive case marker-ssA, etc.
This is the only language-specific com-ponent of our translation model.
However, weexpect this approach to work for other agglu-tinative languages as well.
For fusional lan-guages like Spanish, another mapping from suf-fix to abstract tags might be needed.
These suf-fix transformations to their equivalence classesprevent morphophonemic variants of the samemorpheme from competing against each other inthe prediction model.
This resulted in 44 possi-ble label outputs per stem which was a reason-able sized tag-set for CRF training.
The CRFwas trained on monolingual features of the seg-mented text for suffix prediction, where t is thecurrent token:Word Stem st?n, .., st, .., st+n(n = 4)Morph Prediction yt?2, yt?1, ytWith this simple feature set, we were able touse features over longer distances, resulting ina total of 1,110,075 model features.
After CRFbased recovery of the suffix tag sequence, we usea bigram language model trained on a full seg-mented version on the training data to recoverthe original vowels.
We used bigrams only, be-cause the suffix vowel harmony alternation de-pends only upon the preceding phonemes in theword from which it was segmented.35original training data:koskevaa mietinto?a?
ka?sitella?a?nsegmentation:koske+ +va+ +a mietinto?+ +a?
ka?si+ +te+ +lla?+ +a?+ +n(train bigram language model with mapping A = { a, a?
})map final suffix to abstract tag-set:koske+ +va+ +A mietinto?+ +A ka?si+ +te+ +lla?+ +a?+ +n(train CRF model to predict the final suffix)peeling of final suffix:koske+ +va+ mietinto?+ ka?si+ +te+ +lla?+ +a?+(train SMT model on this transformation of training data)(a) Trainingdecoder output:koske+ +va+ mietinto?+ ka?si+ +te+ +lla?+ +a?+decoder output stitched up:koskeva+ mietinto?+ ka?sitella?a?+CRF model prediction:x = ?koskeva+ mietinto?+ ka?sitella?a?+?, y = ?+A +A +n?koskeva+ +A mietinto?+ +A ka?sitella?a?+ +nunstitch morphemes:koske+ +va+ +A mietinto?+ +A ka?si+ +te+ +lla?+ +a?+ +nlanguage model disambiguation:koske+ +va+ +a mietinto?+ +a?
ka?si+ +te+ +lla?+ +a?+ +nfinal stitching:koskevaa mietinto?a?
ka?sitella?a?n(the output is then compared to the reference translation)(b) DecodingFigure 2: Worked example of all steps in the post-processing morphology prediction model.3 Experimental ResultsFor all of the models built in this paper, we usedthe Europarl version 3 corpus (Koehn, 2005)English-Finnish training data set, as well as thestandard development and test data sets.
Ourparallel training data consists of ?1 million sen-tences of 40 words or less, while the develop-ment and test sets were each 2,000 sentenceslong.
In all the experiments conducted in thispaper, we used the Moses5 phrase-based trans-lation system (Koehn et al, 2007), 2008 version.We trained all of the Moses systems herein usingthe standard features: language model, reorder-ing model, translation model, and word penalty;in addition to these, the factored experimentscalled for additional translation and generationfeatures for the added factors as noted above.We used in all experiments the following set-tings: a hypothesis stack size 100, distortionlimit 6, phrase translations limit 20, and maxi-mum phrase length 20.
For the language models,we used SRILM 5-gram language models (Stol-cke, 2002) for all factors.
For our word-basedBaseline system, we trained a word-based modelusing the same Moses system with identical set-tings.
For evaluation against segmented trans-lation systems in segmented forms before wordreconstruction, we also segmented the baselinesystem?s word-based output.
All the BLEUscores reported are for lowercase evaluation.We did an initial evaluation of the segmentedoutput translation for each system using the no-5http://www.statmt.org/moses/Segmentation m-BLEU No UniBaseline 14.84?0.69 9.89Sup 18.41?0.69 13.49Unsup L-match 20.74?0.68 15.89Table 2: Segmented Model Scores.
Sup refers to thesupervised segmentation baseline model.
m-BLEUindicates that the segmented output was evaluatedagainst a segmented version of the reference (thismeasure does not have the same correlation with hu-man judgement as BLEU).
No Uni indicates the seg-mented BLEU score without unigrams.tion of m-BLEU score (Luong et al, 2010) wherethe BLEU score is computed by comparing thesegmented output with a segmented referencetranslation.
Table 2 shows the m-BLEU scoresfor various systems.
We also show the m-BLEUscore without unigrams, since over-segmentationcould lead to artificially high m-BLEU scores.In fact, if we compare the relative improvementof our m-BLEU scores for the Unsup L-matchsystem we see a relative improvement of 39.75%over the baseline.
Luong et.
al.
(2010) reportan m-BLEU score of 55.64% but obtain a rel-ative improvement of 0.6% over their baselinem-BLEU score.
We find that when using agood segmentation model, segmentation of themorphologically complex target language im-proves model performance over an unsegmentedbaseline (the confidence scores come from boot-strap resampling).
Table 3 shows the evalua-tion scores for all the baselines and the methodsintroduced in this paper using standard word-based lowercase BLEU, WER and PER.
We do36Model BLEU WER TERBaseline 14.68 74.96 72.42Factored 14.22 76.68 74.15(Luong et.al, 2010) 14.82 - -Sup 14.90 74.56 71.84Unsup L-match 15.09?
74.46 71.78CRF-LM 14.87 73.71 71.15Table 3: Test Scores: lowercase BLEU, WER andTER.
The ?
indicates a statistically significant im-provement of BLEU score over the Baseline model.The boldface scores are the best performing scoresper evaluation measure.better than (Luong et al, 2010), the previousbest score for this task.
We also show a bet-ter relative improvement over our baseline whencompared to (Luong et al, 2010): a relative im-provement of 4.86% for Unsup L-match com-pared to our baseline word-based model, com-pared to their 1.65% improvement over theirbaseline word-based model.
Our best perform-ing method used unsupervised morphology withL-match (see Section 2.2) and the improvementis significant: bootstrap resampling provides aconfidence margin of ?0.77 and a t-test (Collinset al, 2005) showed significance with p = 0.001.3.1 Morphological Fluency AnalysisTo see how well the models were doing at get-ting morphology right, we examined several pat-terns of morphological behavior.
While we wishto explore minimally supervised morphologicalMT models, and use as little language spe-cific information as possible, we do want touse linguistic analysis on the output of our sys-tem to see how well the models capture essen-tial morphological information in the target lan-guage.
So, we ran the word-based baseline sys-tem, the segmented model (Unsup L-match),and the prediction model (CRF-LM) outputs,along with the reference translation through thesupervised morphological analyzer Omorfi (Piri-nen and Listenmaa, 2007).
Using this analy-sis, we looked at a variety of linguistic construc-tions that might reveal patterns in morphologi-cal behavior.
These were: (a) explicitly markednoun forms, (b) noun-adjective case agreement,(c) subject-verb person/number agreement, (d)transitive object case marking, (e) postposi-tions, and (f) possession.
In each of these cat-egories, we looked for construction matches ona per-sentence level between the models?
outputand the reference translation.Table 4 shows the models?
performance on theconstructions we examined.
In all of the cat-egories, the CRF-LM model achieves the bestprecision score, as we explain below, while theUnsup L-match model most frequently gets thehighest recall score.A general pattern in the most prevalent ofthese constructions is that the baseline tendsto prefer the least marked form for noun cases(corresponding to the nominative) more thanthe reference or the CRF-LM model.
The base-line leaves nouns in the (unmarked) nominativefar more than the reference, while the CRF-LMmodel comes much closer, so it seems to farebetter at explicitly marking forms, rather thandefaulting to the more frequent unmarked form.Finnish adjectives must be marked with thesame case as their head noun, while verbs mustagree in person and number with their subject.We saw that in both these categories, the CRF-LM model outperforms for precision, while thesegmented model gets the best recall.In addition, Finnish generally marks di-rect objects of verbs with the accusativeor the partitive case; we observed moreaccusative/partitive-marked nouns followingverbs in the CRF-LM output than in the base-line, as illustrated by example (1) in Fig.
3.While neither translation picks the same verb asin the reference for the input ?clarify,?
the CRF-LM-output paraphrases it by using a grammat-ical construction of the transitive verb followedby a noun phrase inflected with the accusativecase, correctly capturing the transitive construc-tion.
The baseline translation instead follows?give?
with a direct object in the nominativecase.To help clarify the constructions in question,we have used Google Translate6 to provide back-6http://translate.google.com/37Construction Freq.
Baseline Unsup L-match CRF-LMP R F P R F P R FNoun Marking 5.5145 51.74 78.48 62.37 53.11 83.63 64.96 54.99 80.21 65.25Trans Obj 1.0022 32.35 27.50 29.73 33.47 29.64 31.44 35.83 30.71 33.07Noun-Adj Agr 0.6508 72.75 67.16 69.84 69.62 71.00 70.30 73.29 62.58 67.51Subj-Verb Agr 0.4250 56.61 40.67 47.33 55.90 48.17 51.48 57.79 40.17 47.40Postpositions 0.1138 43.31 29.89 35.37 39.31 36.96 38.10 47.16 31.52 37.79Possession 0.0287 66.67 70.00 68.29 75.68 70.00 72.73 78.79 60.00 68.12Table 4: Model Accuracy: Morphological Constructions.
Freq.
refers to the construction?s average numberof occurrences per sentence, also averaged over the various translations.
P, R and F stand for precision,recall and F-score.
The constructions are listed in descending order of their frequency in the texts.
Thehighlighted value in each column is the most accurate with respect to the reference value.translations of our MT output into English; tocontextualize these back-translations, we haveprovided Google?s back-translation of the refer-ence.The use of postpositions shows another dif-ference between the models.
Finnish postposi-tions require the preceding noun to be in thegenitive or sometimes partitive case, which oc-curs correctly more frequently in the CRF-LMthan the baseline.
In example (2) in Fig.
3,all three translations correspond to the Englishtext, ?with the basque nationalists.?
However,the CRF-LM output is more grammatical thanthe baseline, because not only do the adjectiveand noun agree for case, but the noun ?bask-ien?
to which the postposition ?kanssa?
belongs ismarked with the correct genitive case.
However,this well-formedness is not rewarded by BLEU,because ?baskien?
does not match the reference.In addition, while Finnish may express pos-session using case marking alone, it has anotherconstruction for possession; this can disam-biguate an otherwise ambiguous clause.
This al-ternate construction uses a pronoun in the geni-tive case followed by a possessive-marked noun;we see that the CRF-LM model correctly marksthis construction more frequently than the base-line.
As example (3) in Fig.
3 shows, while nei-ther model correctly translates ?matkan?
(?trip?
),the baseline?s output attributes the inessive?yhteydess?
(?connection?)
as belonging to ?tu-lokset?
(?results?
), and misses marking the pos-session linking it to ?Commissioner Fischler?.Our manual evaluation shows that the CRF-LM model is producing output translations thatare more morphologically fluent than the word-based baseline and the segmented translationUnsup L-match system, even though the wordchoices lead to a lower BLEU score overall whencompared to Unsup L-match.4 Related WorkThe work on morphology in MT can be groupedinto three categories, factored models, seg-mented translation, and morphology generation.Factored models (Koehn and Hoang, 2007)factor the phrase translation probabilities overadditional information annotated to each word,allowing for text to be represented on multi-ple levels of analysis.
We discussed the draw-backs of factored models for our task in Sec-tion 2.1.
While (Koehn and Hoang, 2007; Yangand Kirchhoff, 2006; Avramidis and Koehn,2008) obtain improvements using factored mod-els for translation into English, German, Span-ish, and Czech, these models may be less usefulfor capturing long-distance dependencies in lan-guages with much more complex morphologicalsystems such as Finnish.
In our experimentsfactored models did worse than the baseline.Segmented translation performs morphologi-cal analysis on the morphologically complex textfor use in the translation model (Brown et al,1993; Goldwater and McClosky, 2005; de Gis-pert and Marin?o, 2008).
This method unpackscomplex forms into simpler, more frequently oc-curring components, and may also increase thesymmetry of the lexically realized content be-38(1) Input: ?the charter we are to approve today both strengthens and gives visible shape to the common fundamental rightsand values our community is to be based upon.?a.
Reference: perusoikeuskirja , jonka ta?na?a?n aiomme hyva?ksya?
, seka?
vahvistaa etta?
selventa?a?
(sel-venta?a?/VERB/ACT/INF/SG/LAT-clarify) niita?
(ne/PRONOUN/PL/PAR-them) yhteisia?
perusoikeuksia ja -arvoja , joiden on oltava yhteiso?mme perusta.Back-translation: ?Charter of Fundamental Rights, which today we are going to accept that clarify and strengthenthe common fundamental rights and values, which must be community based.?b.
Baseline: perusoikeuskirja me hyva?ksymme ta?na?a?n molemmat vahvistaa ja antaa (antaa/VERB/INF/SG/LAT-give) na?kyva?
(na?kya?/VERB/ACT/PCP/SG/NOM-visible) muokata yhteista?
perusoikeuksia ja arvoja on perustut-tava.Back-translation: ?Charter today, we accept both confirm and modify to make a visible and common values, funda-mental rights must be based.?c.
CRF-LM: perusoikeuskirja on hyva?ksytty ta?na?a?n , seka?
vahvistaa ja antaa (antaa/VERB/ACT/INF/SG/LAT-give)konkreettisen (konkreettinen/ADJECTIVE/SG/GEN,ACC-concrete) muodon (muoto/NOUN/SG/GEN,ACC-shape) yhteisia?
perusoikeuksia ja perusarvoja , yhteiso?n on perustuttava.Back-translation: ?Charter has been approved today, and to strengthen and give concrete shape to the commonbasic rights and fundamental values, the Community must be based.?
(2) Input: ?with the basque nationalists?a.
Reference: baskimaan kansallismielisten kanssabasque-SG/NOM+land-SG/GEN,ACC nationalists-PL/GEN with-POSTb.
Baseline: baskimaan kansallismieliset kanssabasque-SG/NOM-+land-SG/GEN,ACC kansallismielinen-PL/NOM,ACC-nationalists POST-withc.
CRF-LM: kansallismielisten baskien kanssanationalists-PL/GEN basques-PL/GEN with-POST(3) Input: ?and in this respect we should value the latest measures from commissioner fischler , the results of his trip tomorocco on the 26th of last month and the high level meetings that took place, including the one with the kinghimself?a.
Reference: ja ta?ssa?
mielessa?
osaamme myo?s arvostaa komission ja?sen fischlerin viimeisimpia?
toimia , jotka ovatha?nen (ha?nen/GEN-his) marokkoon 26 lokakuuta tekemns (tekema?nsa?/POSS-his) matkan (matkan/GEN-tour) ja korkean tason kokousten jopa itsensa?
kuninkaan kanssa tulostaBack-translation: ?and in this sense we can also appreciate the Commissioner Fischler?s latest actions, which are histo Morocco 26 October trip to high-level meetings and even the king himself with the resultb.
Baseline: ja ta?ssa?
yhteydessa?
olisi arvoa viimeisin toimia komission ja?sen fischler , tulokset monitulkintaisia marokonyhteydessa?
(yhteydess/INE-connection) , ja viime kuussa pidettiin korkean tason kokouksissa , mukaan luettunakuninkaan kanssaBack-translation: ?and in this context would be the value of the last act, Commissioner Fischler, the results of theMoroccan context, ambiguous, and last month held high level meetings, including with the king?c.
CRF-LM: ja ta?ssa?
yhteydessa?
meida?n olisi lisa?arvoa viimeista?
toimenpiteita?
kuin komission ja?sen fischler , etta?
ha?nen(ha?nen/GEN-his) kokemuksensa (kokemuksensa/POSS-experience) marokolle (marokolle-Moroccan) viime kuun26 ja korkean tason tapaamiset ja?rjestettiin, kuninkaan kanssaBack-translation: ?and in this context, we should value the last measures as the Commissioner Fischler, that hisexperience in Morocco has on the 26th and high-level meetings took place, including with the king.
?Figure 3: Morphological fluency analysis (see Section 3.1).tween source and target.
In a somewhat or-thogonal approach to ours, (Ma et al, 2007) usealignment of a parallel text to pack together ad-jacent segments in the alignment output, whichare then fed back to the word aligner to boot-strap an improved alignment, which is then usedin the translation model.
We compared our re-sults against (Luong et al, 2010) in Table 3since their results are directly comparable toours.
They use a segmented phrase table andlanguage model along with the word-based ver-sions in the decoder and in tuning a Finnish tar-get.
Their approach requires segmented phrasesto match word boundaries, eliminating morpho-logically productive phrases.
In their work a seg-mented language model can score a translation,but cannot insert morphology that does notshow source-side reflexes.
In order to performa similar experiment that still allowed for mor-phologically productive phrases, we tried train-ing a segmented translation model, the outputof which we stitched up in tuning so as to tuneto a word-based reference.
The goal of this ex-periment was to control the segmented model?stendency to overfit by rewarding it for usingcorrect whole-word forms.
However, we found39that this approach was less successful than us-ing the segmented reference in tuning, and couldnot meet the baseline (13.97% BLEU best tun-ing score, versus 14.93% BLEU for the base-line best tuning score).
Previous work in seg-mented translation has often used linguisticallymotivated morphological analysis selectively ap-plied based on a language-specific heuristic.
Atypical approach is to select a highly inflectingclass of words and segment them for particularmorphology (de Gispert and Marin?o, 2008; Ra-manathan et al, 2009).
Popovic?
and Ney (2004)perform segmentation to reduce morphologicalcomplexity of the source to translate into an iso-lating target, reducing the translation error ratefor the English target.
For Czech-to-English,Goldwater and McClosky (2005) lemmatized thesource text and inserted a set of ?pseudowords?expected to have lexical reflexes in English.Minkov et.
al.
(2007) and Toutanova et.
al.
(2008) use a Maximum Entropy Markov Modelfor morphology generation.
The main draw-back to this approach is that it removes morpho-logical information from the translation model(which only uses stems); this can be a prob-lem for languages in which morphology ex-presses lexical content.
de Gispert (2008) usesa language-specific targeted morphological clas-sifier for Spanish verbs to avoid this issue.
Tal-bot and Osborne (2006) use clustering to groupmorphological variants of words for word align-ments and for smoothing phrase translation ta-bles.
Habash (2007) provides various methodsto incorporate morphological variants of wordsin the phrase table in order to help recognize outof vocabulary words in the source language.5 Conclusion and Future WorkWe found that using a segmented translationmodel based on unsupervised morphology in-duction and a model that combined morphemesegments in the translation model with a post-processing morphology prediction model gave usbetter BLEU scores than a word-based baseline.Using our proposed approach we obtain betterscores than the state of the art on the English-Finnish translation task (Luong et al, 2010):from 14.82% BLEU to 15.09%, while using asimpler model.
We show that using morpho-logical segmentation in the translation modelcan improve output translation scores.
Wealso demonstrate that for Finnish (and possi-bly other agglutinative languages), phrase-basedMT benefits from allowing the translation modelaccess to morphological segmentation yieldingproductive morphological phrases.
Taking ad-vantage of linguistic analysis of the output weshow that using a post-processing morphologygeneration model can improve translation flu-ency on a sub-word level, in a manner that isnot captured by the BLEU word-based evalua-tion measure.In order to help with replication of the resultsin this paper, we have run the various morpho-logical analysis steps and created the necessarytraining, tuning and test data files needed in or-der to train, tune and test any phrase-based ma-chine translation system with our data.
The filescan be downloaded from natlang.cs.sfu.ca.In future work we hope to explore the utility ofphrases with productive morpheme boundariesand explore why they are not used more per-vasively in the decoder.
Evaluation measuresfor morphologically complex languages and tun-ing to those measures are also important futurework directions.
Also, we would like to explorea non-pipelined approach to morphological pre-and post-processing so that a globally trainedmodel could be used to remove the target sidemorphemes that would improve the translationmodel and then predict those morphemes in thetarget language.AcknowledgementsThis research was partially supported byNSERC, Canada (RGPIN: 264905) and aGoogle Faculty Award.
We would like to thankChristian Monson, Franz Och, Fred Popowich,Howard Johnson, Majid Razmara, BaskaranSankaran and the anonymous reviewers fortheir valuable comments on this work.
Wewould particularly like to thank the developersof the open-source Moses machine translationtoolkit and the Omorfi morphological analyzerfor Finnish which we used for our experiments.40ReferencesEleftherios Avramidis and Philipp Koehn.
2008.
En-riching morphologically poor languages for statis-tical machine translation.
In Proceedings of the46th Annual Meeting of the Association for Com-putational Linguistics: Human Language Tech-nologies, page 763?770, Columbus, Ohio, USA.Association for Computational Linguistics.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and R. L. Mercer.
1993.
Themathematics of statistical machine translation:Parameter estimation.
Computational Linguis-tics, 19(2):263?311.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word seg-mentation for machine translation performance.In Proceedings of the Third Workshop on Statisti-cal Machine Translation, pages 224?232, Colum-bus, Ohio, June.
Association for ComputationalLinguistics.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL05).
Association for Computational Lin-guistics.Mathias Creutz and Krista Lagus.
2005.
Inducingthe morphological lexicon of a natural languagefrom unannotated text.
In Proceedings of the In-ternational and Interdisciplinary Conference onAdaptive Knowledge Representation and Reason-ing (AKRR?05), pages 106?113, Espoo, Finland.Mathias Creutz and Krista Lagus.
2006.
Morfes-sor in the morpho challenge.
In Proceedings ofthe PASCAL Challenge Workshop on Unsuper-vised Segmentation of Words into Morphemes.Adria?
de Gispert and Jose?
Marin?o.
2008.
On theimpact of morphology in English to Spanish sta-tistical MT.
Speech Communication, 50(11-12).Sharon Goldwater and David McClosky.
2005.Improving statistical MT through morphologicalanalysis.
In Proceedings of the Human LanguageTechnology Conference and Conference on Em-pirical Methods in Natural Language Processing,pages 676?683, Vancouver, B.C., Canada.
Associ-ation for Computational Linguistics.Philipp Koehn and Hieu Hoang.
2007.
Factoredtranslation models.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 868?876, Prague,Czech Republic.
Association for ComputationalLinguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical ma-chine translation.
In ACL ?07: Proceedings ofthe 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, pages177?108, Prague, Czech Republic.
Association forComputational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof Machine Translation Summit X, pages 79?86,Phuket, Thailand.
Association for ComputationalLinguistics.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th Inter-national Conference on Machine Learning, pages282?289, San Francisco, California, USA.
Associ-ation for Computing Machinery.Minh-Thang Luong, Preslav Nakov, and Min-YenKan.
2010.
A hybrid morpheme-word repre-sentation for machine translation of morphologi-cally rich languages.
In Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 148?157, Cam-bridge, Massachusetts.
Association for Computa-tional Linguistics.Yanjun Ma, Nicolas Stroppa, and Andy Way.
2007.Bootstrapping word alignment via word packing.In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages304?311, Prague, Czech Republic.
Association forComputational Linguistics.Einat Minkov, Kristina Toutanova, and HisamiSuzuki.
2007.
Generating complex morphologyfor machine translation.
In In Proceedings of the45th Annual Meeting of the Association for Com-putational Linguistics (ACL07), pages 128?135,Prague, Czech Republic.
Association for Compu-tational Linguistics.Christian Monson.
2008.
Paramor and morpho chal-lenge 2008.
In Lecture Notes in Computer Science:Workshop of the Cross-Language Evaluation Fo-rum (CLEF 2008), Revised Selected Papers.Habash Nizar.
2007.
Four techniques for online han-dling of out-of-vocabulary words in arabic-englishstatistical machine translation.
In Proceedings ofthe 46th Annual Meeting of the Association ofComputational Linguistics, Columbus, Ohio.
As-sociation for Computational Linguistics.41Kishore Papineni, Salim Roukos, Todd Ward, andWei jing Zhu.
2002.
BLEU: A method for auto-matic evaluation of machine translation.
In Pro-ceedings of 40th Annual Meeting of the Associ-ation for Computational Linguistics ACL, pages311?318, Philadelphia, Pennsylvania, USA.
Asso-ciation for Computational Linguistics.Tommi Pirinen and Inari Listenmaa.2007.
Omorfi morphological analzer.http://gna.org/projects/omorfi.Maja Popovic?
and Hermann Ney.
2004.
Towardsthe use of word stems and suffixes for statisti-cal machine translation.
In Proceedings of the 4thInternational Conference on Language Resourcesand Evaluation (LREC), pages 1585?1588, Lis-bon, Portugal.
European Language Resources As-sociation (ELRA).Ananthakrishnan Ramanathan, Hansraj Choudhary,Avishek Ghosh, and Pushpak Bhattacharyya.2009.
Case markers and morphology: Address-ing the crux of the fluency problem in English-Hindi SMT.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the Associa-tion for Computational Linguistics and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the Asian Federation of Natural Lan-guage Processing, pages 800?808, Suntec, Singa-pore.
Association for Computational Linguistics.Andreas Stolcke.
2002.
Srilm ?
an extensible lan-guage modeling toolkit.
7th International Confer-ence on Spoken Language Processing, 3:901?904.David Talbot and Miles Osborne.
2006.
Modellinglexical redundancy for machine translation.
InProceedings of the 21st International Conferenceon Computational Linguistics and 44th AnnualMeeting of the Association for Computational Lin-guistics, pages 969?976, Sydney, Australia, July.Association for Computational Linguistics.Kristina Toutanova, Hisami Suzuki, and AchimRuopp.
2008.
Applying morphology generationmodels to machine translation.
In Proceedingsof the 46th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 514?522, Columbus, Ohio,USA.
Association for Computational Linguistics.Mei Yang and Katrin Kirchhoff.
2006.
Phrase-basedbackoff models for machine translation of highlyinflected languages.
In Proceedings of the Eu-ropean Chapter of the Association for Computa-tional Linguistics, pages 41?48, Trento, Italy.
As-sociation for Computational Linguistics.42
