Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1732?1740,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsOrthonormal Explicit Topic Analysis for Cross-lingual Document MatchingJohn Philip McCraeUniversity BielefeldInspiration 1Bielefeld, GermanyPhilipp CimianoUniversity BielefeldInspiration 1Bielefeld, Germany{jmccrae,cimiano,rklinger}@cit-ec.uni-bielefeld.deRoman KlingerUniversity BielefeldInspiration 1Bielefeld, GermanyAbstractCross-lingual topic modelling has applicationsin machine translation, word sense disam-biguation and terminology alignment.
Multi-lingual extensions of approaches based on la-tent (LSI), generative (LDA, PLSI) as well asexplicit (ESA) topic modelling can induce aninterlingual topic space allowing documentsin different languages to be mapped into thesame space and thus to be compared acrosslanguages.
In this paper, we present a novelapproach that combines latent and explicittopic modelling approaches in the sense thatit builds on a set of explicitly defined top-ics, but then computes latent relations betweenthese.
Thus, the method combines the ben-efits of both explicit and latent topic mod-elling approaches.
We show that on a cross-lingual mate retrieval task, our model signif-icantly outperforms LDA, LSI, and ESA, aswell as a baseline that translates every word ina document into the target language.1 IntroductionCross-lingual document matching is the task of,given a query document in some source language,estimating the similarity to a document in some tar-get language.
This task has important applications inmachine translation (Palmer et al 1998; Tam et al2007), word sense disambiguation (Li et al 2010)and ontology alignment (Spiliopoulos et al 2007).An approach that has become quite popular in re-cent years for cross-lingual document matching isExplicit Semantics Analysis (ESA, Gabrilovich andMarkovitch (2007)) and its cross-lingual extensionCL-ESA (Sorg and Cimiano, 2008).
ESA indexesdocuments by mapping them into a topic space de-fined by their similarity to predefined explicit top-ics ?
generally articles from an encyclopaedia ?
insuch a way that there is a one-to-one correspondencebetween topics and encyclopedic entries.
CL-ESAextends this to the multilingual case by exploitinga background document collection that is alignedacross languages, such as Wikipedia.
A feature ofESA and its extension CL-ESA is that, in contrast tolatent (e.g.
LSI, Deerwester et al(1990)) or genera-tive topic models (such as LDA, Blei et al(2003)),it requires no training and, nevertheless, has beendemonstrated to outperform LSI and LDA on cross-lingual retrieval tasks (Cimiano et al 2009).A key choice in Explicit Semantic Analysis is thedocument space that will act as the topic space.
Thestandard choice is to regard all articles from a back-ground document collection ?
Wikipedia articles area typical choice ?
as the topic space.
However, itis crucial to ensure that these topics cover the se-mantic space evenly and completely.
In this pa-per, we present an alternative approach where weremap the semantic space defined by the topics insuch a manner that it is orthonormal.
In this way,each document is mapped to a topic that is distinctfrom all other topics.
Such a mapping can be con-sidered as equivalent to a variant of Latent Seman-tic Indexing (LSI) with the main difference that ourmodel exploits the matrix that maps topic vectorsback into document space, which is normally dis-carded in LSI-based approaches.
We dub our modelONETA (OrthoNormal Explicit Topic Analysis) andempirically show that on a cross-lingual retrieval1732task it outperforms ESA, LSI, and Latent DirichletAllocation (LDA) as well as a baseline consisting oftranslating each word into the target language, thusreducing the task to a standard monolingual match-ing task.
In particular, we quantify the effect of dif-ferent approximation techniques for computing theorthonormal basis and investigate the effect of vari-ous methods for the normalization of frequency vec-tors.The structure of the paper is as follows: we situateour work in the general context of related work ontopic models for cross-lingual document matchingin Section 2.
We present our model in Section 3 andpresent our experimental results and discuss theseresults in Section 4.2 Related WorkThe idea of applying topic models that map docu-ments into an interlingual topic space seems a quitenatural and principled approach to tackle severaltasks including the cross-lingual document retrievalproblem.Topic modelling is the process of finding a rep-resentation of a document d in a lower dimensionalspace RK where each dimension corresponds to onetopic that abstracts from specific words and thus al-lows us to detect deeper semantic similarities be-tween documents beyond the computation of thepure overlap in terms of words.Three main variants of document models havebeen mainly considered for cross-lingual documentmatching:Latent methods such as Latent Semantic Indexing(LSI, Deerwester et al(1990)) induce a de-composition of the term-document matrix ina way that reduces the dimensionality of thedocuments, while minimizing the error in re-constructing the training data.
For example,in Latent Semantic Indexing, a term-documentmatrix is approximated by a partial singu-lar value decomposition, or in Non-NegativeMatrix Factorization (NMF, Lee and Seung(1999)) by two smaller non-negative matrices.If we append comparable or equivalent doc-uments in multiple languages together beforecomputing the decomposition as proposed byDumais et al(1997) then the topic model isessentially cross-lingual allowing to comparedocuments in different languages once theyhave been mapped into the topic space.Probabilistic or generative methods instead at-tempt to induce a (topic) model that has thehighest likelihood of generating the documentsactually observed during training.
As with la-tent methods, these topics are thus interlin-gual and can generate words/terms in differ-ent languages.
Prominent representatives ofthis type of method are Probabilistic Latent Se-mantic Indexing (PLSI, Hofmann (1999)) orLatent Dirichlet Allocation (LDA, Blei et al(2003)), both of which can be straightforwardlyextended to the cross-lingual case (Mimno etal., 2009).Explicit topic models make the assumption thattopics are explicitly given instead of being in-duced from training data.
Typically, a back-ground document collection is assumed to begiven whereby each document in this corpuscorresponds to one topic.
A mapping from doc-ument to topic space is calculated by comput-ing the similarity of the document to every doc-ument in the topic space.
A prominent exam-ple for this kind of topic modelling approach isExplicit Semantic Analysis (ESA, Gabrilovichand Markovitch (2007)).Both latent and generative topic models attempt tofind topics from the data and it has been found thatin some cases they are equivalent (Ding et al 2006).However, this approach suffers from the problemthat the topics might be artifacts of the training datarather than coherent semantic topics.
In contrast, ex-plicit topic methods can use a set of topics that arechosen to be well-suited to the domain.
The princi-ple drawback of this is that the method for choosingsuch explicit topics by selecting documents is com-paratively crude.
In general, these topics may beoverlapping and poorly distributed over the seman-tic topic space.
By comparison, our method takes theadvantage of the pre-specified topics of explicit topicmodels, but incorporates a training step to learn la-tent relations between these topics.17333 Orthonormal explicit topic analysisOur approach follows Explicit Semantic Analysis inthe sense that it assumes the availability of a back-ground document collection B = {b1, b2, ..., bN}consisting of textual representations.
The map-ping into the explicit topic space is defined by alanguage-specific function ?
that maps documentsinto RN such that the jth value in the vector is givenby some association measure ?j(d) for each back-ground document bj .
Typical choices for this associ-ation measure ?
are the sum of the TF-IDF scores oran information retrieval relevance scoring functionsuch as BM-25 (Sorg and Cimiano, 2010).For the case of TF-IDF, the value of the j-th ele-ment of the topic vector is given by:?j(d) =???
?tf-idf(bj)T ???
?tf-idf(d)Thus, the mapping function can be represented asthe product of a TF-IDF vector of document dmulti-plied by anW?N matrix, X, each element of whichcontains the TF-IDF value of word i in document bj :?
(d) =????????tf-idf(b1)T...???
?tf-idf(bN )T???????
?tf-idf(d) = XT ????
?tf-idf(d)For simplicity, we shall assume from this point onthat all vectors are already converted to a TF-IDFor similar numeric vector form.
In order to com-pute the similarity between two documents di anddj , typically the cosine-function (or the normalizeddot product) between the vectors ?
(di) and ?
(dj) iscomputed as follows:sim(di, dj) = cos(?(di),?
(dj)) =?(di)T?(dj)||?(di)||||?
(dj)||If we represent the above using our above definedW ?N matrix X then we get:sim(di, dj) = cos(XTdi,XTdj) =dTi XXTdj||XTdi||||XTdj ||The key challenge with ESA is choosing a goodbackground document collection B = {b1, ..., bN}.A simple minimal criterion for a good backgrounddocument collection is that each document in thiscollection should be maximally similar to itself andless similar to any other document:?i 6= j 1 = sim(bj , bj) > sim(bi, bj) ?
0While this criterion is trivially satisfied if we haveno duplicate documents in our collection, our intu-ition is that we should choose a background collec-tion that maximizes the slack margin of this inequal-ity, i.e.
|sim(bj , bj) ?
sim(bi, bj)|.
We can see thatmaximizing this margin for all i,j is the same asminimizing the semantic overlap of the backgrounddocuments, which is given as follows:overlap(B) =?i = 1, .
.
.
, Nj = 1, .
.
.
, Ni 6= jsim(bi, bj)We first note that we can, without loss of general-ity, normalize our background documents such that||Xbj || = 1 for all j, and in this case we can re-define the semantic overlap as the following matrixexpression1overlap(X) = ||XTXXTX?
I||1It is trivial to verify that this equation has a mini-mum when XTXXTX = I.
This is the case whenthe topics are orthonormal:(XTbi)T(XTbj) = 0 if i 6= j(XTbi)T(XTbi) = 1Unfortunately, this is not typically the case as thedocuments have significant word overlap as well assemantic overlap.
Our goal is thus to apply a suitabletransformation to X with the goal of ensuring thatthe orthogonality property holds.Assuming that this transformation of X is doneby multiplication with some other matrix A, we candefine the learning problem as finding that matrix Asuch that:(AXTX)T(AXTX) = I1||A||p =?i,j |aij |p is the p-norm.
||A||F =?||A||2 isthe Frobenius norm.1734If we have the case that W ?
N and that the rankof X is N , then XTX is invertible and thus A =(XTX)?1 is the solution to this problem.2We define the projection function of a documentd, represented as a normalized term frequency vec-tor, as follows:?ONETA(d) = (XTX)?1XTdFor the cross-lingual case we assume that we havetwo sets of background documents of equal size,B1 = {b11, .
.
.
, b1N}, B2 = {b21, .
.
.
, b2N} in lan-guages l1 and l2, respectively and that these doc-uments are aligned such that for every index i, b1iand b2i are documents on the same topic in eachlanguage.
Using this we can construct a projec-tion function for each language which maps into thesame topic space.
Thus, as in CL-ESA, we obtainthe cross-lingual similarity between a document diin language l1 and a document dj in language l2 asfollows:sim(di, dj) = cos(?l1ONETA(di),?l2ONETA(dj))We note here that we assume that ?
could be rep-resented as a symmetric inner product of two vec-tors.
However, for many common choices of asso-ciation measures, including BM25, this is not thecase.
In this case the expression XTX can be re-placed with a kernel matrix specifying the associ-ation of each background document to each otherbackground document.3.1 Relationship to Latent Semantic IndexingIn this section we briefly clarify the relationship be-tween our method ONETA and Latent Semantic In-dexing.
Latent Semantic Indexing defines a map-ping from a document represented as a term fre-quency vector to a vector in RK .
This transforma-tion is defined by means of calculating the singu-lar value decomposition (SVD) of the matrix X asabove, namely2In the case that the matrix is not invertible we can in-stead solve ||XTXA ?
I||F , which has a minimum at A =V?
?1UT where XTX = U?VT is the singular value de-composition of XTX.As usual we do not in fact compute the inverse for our exper-iments, but instead the LU Decomposition and solve by Gaus-sian elimination at test time.X = U?VTWhere ?
is diagonal and U V are the eigenvec-tors of XXT and XTX., respectively.
Let ?K de-note the K ?
K submatrix containing the largesteigenvalues, and UK ,VK denote the correspondingeigenvectors.
Thus LSI can be defined as:?LSI(d) = ?
?1K UKdWith regards to orthonormalized topics, we seethat using the SVD, we can simply derive the fol-lowing:(XTX)?1XT = V?
?1UTWhen we set K = N and thus choose the maxi-mum number of topics, ONETA is equivalent to LSImodulo the fact that it multiplies the resulting topicvector by V, thus projecting back into documentspace, i.e.
into explicit topics.In practice, both methods differ significantly inthat the approximations they make are quite differ-ent.
Furthermore, in the case that W  N and Xhas n non-zeroes, the calculation of the SVD is ofcomplexity O(nN + WN2) and requires O(WN)bytes of memory.
In contrast, ONETA requires com-putation time ofO(Na) for a > 2, which is the com-plexity of the matrix inversion algorithm3, and onlyO(n+N2) bytes of memory.3.2 ApproximationsThe computation of the inverse has a complexitythat, using current practical algorithms, is approxi-mately cubic and as such the time spent calculatingthe inverse can grow very quickly.
There are sev-eral methods for obtaining an approximate inverse.The most commonly used are based on the SVD oreigendecomposition of the matrix.
As XTX is sym-metric positive definite, it holds that:XTX = U?UTWhere U are the eigenvectors of XTX and ?
isa diagonal matrix of the eigenvalues.
With UK ,?K3Algorithms with a = 2.3727 are known but practical algo-rithms have a = 2.807 or a = 3 (Coppersmith and Winograd,1990)1735as the first K eigenvalues and eigenvectors, respec-tively, we have:(XTX)?1 ' UK?
?1K UTK (1)We call this the orthonormal eigenapproxima-tion or ON-Eigen.
The complexity of calculating(XTX)?1XT from this is O(N2K + Nn), wheren is the number of non-zeros in X.Similarly, using the formula derived in the previ-ous section we can derive an approximation of thefull model as follows:(XTX)?1XT ' UK?
?1K VTK (2)We call this approximation Explicit LSI as it firstmaps into the latent topic space and then into theexplicit topic space.We can consider another approximation by notic-ing that X is typically very sparse and moreoversome rows of X have significantly fewer non-zeroesthan others (these rows are for terms with low fre-quency).
Thus, if we take the first N1 columns (doc-uments) in X, it is possible to rearrange the rowsof X with the result that there is some W1 suchthat rows with index greater than W1 have only ze-roes in the columns up to N1.
In other words, wetake a subset of N1 documents and enumerate thewords in such a way that the terms occurring in thefirst N1 documents are enumerated 1, .
.
.
,W1.
LetN2 = N ?
N1, W2 = W ?W1.
The result of thisrow permutation does not affect the value of XTXand we can write the matrix X as:X =(A B0 C)where A is a W1 ?
N1 matrix representing termfrequencies in the first N1 documents, B is a W1 ?N2 matrix containing term frequencies in the re-maining documents for terms that are also found inthe firstN1 documents, and C is aW2?N2 contain-ing the frequency of all terms not found in the firstN1 documents.Application of the well-known divide-and-conquer formula (Bernstein, 2005, p. 159) formatrix inversion yields the following easily verifi-able matrix identity, given that we can find C?
suchthat C?C = I.
((ATA)?1AT ?
(ATA)?1ATBC?0 C?
)(A B0 C)= I(3)We denote the above equation using a matrix Las LTX = I.
We note that L 6= (XTX)?1X,but for any document vector d that is representableas a linear combination of the background doc-ument set (i.e., columns of X) we have thatLd = (XTX)?1XTd and in this sense L '(XTX)?1XT.We further relax the assumption so that we onlyneed to find a C?
such that C?C ' I.
For this,we first observe that C is very sparse as it containsonly terms not contained in the first N1 documentsand we notice that very sparse matrices tend to beapproximately orthogonal, hence suggesting that itshould be very easy to find a left-inverse of C. Thefollowing lemma formalizes this intuition:Lemma: If C is a W ?
N matrix with M non-zeros, distributed randomly and uniformly across thematrix, and all the non-zeros are 1, then DCTC hasan expected value on each non-diagonal value of MN2and a diagonal value of 1 if D is the diagonal matrixwhose values are given by ||ci||?2, the square of thenorm of the corresponding column of C.Proof: We simply observe that if D?
= DCTC,then the (i, j)th element of D?
is given bydij =cTi cj||ci||2If i 6= j then the cTi cj is the number of non-zeroesoverlapping in the ith and jth column of C and undera uniform distribution we expect this to be M2N3 .
Sim-ilarly, we expect the column norm to be MN such thatthe overall expectation is MN2 .
The diagonal value isclearly equal to 1.As long as C is very sparse, we can use the fol-lowing approximation, which can be calculated inO(M) operations, where M is the number of non-zeroes.C?
'??
?||c1||?2 0. .
.0 ||cN2 ||?2??
?CTWe call this method L-Solve.
The complexityof calculating a left-inverse by this method is of1736DocumentNormalizationFrequency Normalization No YesTF 0.31 0.78Relative 0.23 0.42TFIDF 0.21 0.63SQRT 0.28 0.66Table 1: Effect of Term Frequency and Document Nor-malization on Top-1 Precisionorder O(Na1 ), being much more efficient than theeigenvalue methods.
However, it is potentially moreerror-prone as it requires that a left-inverse of C ex-ists.
On real data this might be violated if we do nothave linear independence of the rows of C, for ex-ample if W2 < N2 or if we have even one documentwhich has only words that are also contained in thefirst N1 documents and hence there is a row in Cthat consists of zeros only.
This can be solved byremoving documents from the collection until C isrow-wise linear independent.43.3 NormalizationA key factor in the effectiveness of topic-basedmethods is the appropriate normalization of the el-ements of the document matrix X.
This is evenmore relevant for orthonormal topics as the matrixinversion procedure can be very sensitive to smallchanges in the matrix.
In this context, we con-sider two forms of normalization, term and docu-ment normalization, which can also be consideredas row/column normalizations of X.A straightforward approach to normalization is tonormalize each column of X to obtain a matrix asfollows:X?
=(x1||x1||.
.
.xN||xN ||)If we calculate X?TX?
= Y then we get that the(i, j)-th element of Y is:yij =xTi xj||xi||||xj ||4In the experiments in the next section we discarded 4.2% ofdocuments at N1 = 1000 and 47% of documents at N1 = 5000llllllll l ll ll l l llll l100 200 300 400 5000.00.20.40.60.8Approximation ratePrecisionl lllll lll l ll ll llll?ON?EigenL?SolveExplicit LSILSIESAFigure 1: Effect on Top-1 Precision by various approxi-mation methodThus, the diagonal of Y consists of ones only anddue to the Cauchy-Schwarz inequality we have that|yij | ?
1, with the result that the matrix Y is al-ready close to I.
Formally, we can use this to statea bound on ||X?TX?
?
I||F , but in practice it meansthat the orthonormalizing matrix has more small orzero values.A further option for normalization is to considersome form of term frequency normalization.
Forterm frequency normalization, we use TF (tfwn),Relative ( tfwnFw ), TFIDF (tfwn log(Ndfw)), and SQRT( tfwn?Fw).
Here, tfwn is the term frequency of word win document n, Fw is the total frequency of wordw in the corpus, and dfw is the number of docu-ments containing the words w. The first three ofthese normalizations have been chosen as they arewidely used in the literature.
The SQRT normaliza-tion has been shown to be effective for explicit topicmethods in previous experiments not reported here.4 Experiments and ResultsFor evaluation, we consider a cross-lingual mate re-trieval task from English/Spanish on the basis ofWikipedia as aligned corpus.
The goal is to, for eachdocument of a test set, retrieve the aligned documentor mate.
For each test document, on the basis of1737Method Top-1 Prec.
Top-5 Prec.
Top-10 Prec.
MRR Time MemoryONETA L-Solve (N1 = 1000) 0.290 0.501 0.596 0.390 73s 354MBONETA L-Solve (N1 = 2000) 0.328 0.531 0.600 0.423 2m18s 508MBONETA L-Solve (N1 = 3000) 0.462 0.662 0.716 0.551 4m12s 718MBONETA L-Solve (N1 = 4000) 0.599 0.755 0.781 0.667 7m44s 996MBONETA L-Solve (N1 = 5000) 0.695 0.817 0.843 0.750 12m28s 1.30GBONETA L-Solve (N1 = 6000) 0.773 0.883 0.905 0.824 18m40s 1.69GBONETA L-Solve (N1 = 7000) 0.841 0.928 0.937 0.881 26m31s 2.14GBONETA L-Solve (N1 = 8000) 0.896 0.961 0.968 0.927 37m39s 2.65GBONETA L-Solve (N1 = 9000) 0.924 0.981 0.987 0.950 52m52s 3.22GBONETA (No Approximation) 0.929 0.987 0.990 0.956 57m10s 3.42GBWord Translation 0.751 0.884 0.916 0.812 n/a n/aESA (SQRT Normalization) 0.498 0.769 0.835 0.621 72s 284MBLDA (K=1000) 0.287 0.568 0.659 0.417 4h12m 8.4GBLSI (K=4000) 0.615 0.756 0.783 0.676 13h51m 19.7GBONETA + Word Translation 0.932 0.987 0.993 0.958 n/a n/aTable 2: Result on large-scale mate-finding studies for English to Spanish matchingthe similarity of the query document to all indexeddocuments, we compute the value ranki indicatingat which position the mate of the ith document oc-curs.
We use two metrics: Top-k Precision, definedas the percentage of documents for which the mate isretrieved among the first k elements, and MinimumReciprocal Rank, defined asMRR =?i?test1rankiFor our experiments, we first extracted a subsetof documents (every 20th) from Wikipedia, filteringthis set down to only those that have aligned pagesin both English and Spanish with a minimum lengthof 100 words.
This gives us 10,369 aligned doc-uments in total, which form the background docu-ment collection B.
We split this data into a trainingand test set of 9,332 and 1,037 documents, respec-tively.
We then removed all words whose total fre-quencies were below 50.
This resulted in corpus of6.7 millions words in English and 4.2 million wordsin Spanish.Normalization Methods: In order to investigatethe impact of different normalization methods, weran small-scale experiments using the first 500 doc-uments from our dataset to train ONETA and thenevaluate the resulting models on the mate-findingtask on 100 unseen documents.
The results are pre-sented in Table 1, which shows the Top-1 Precisionfor the different normalization methods.
We see thatthe effect of applying document normalization inall cases improves the quality of the overall result.Surprisingly, we do not see the same result for fre-quency normalization yielding the best result for thecase where we do no normalization at all5 .
In the re-maining experiments we thus employ document nor-malization and no term frequency normalization.ApproximationMethods: In order to evaluate thedifferent approximation methods, we experimen-tally compare 4 different approximation methods:standard LSI, ON-Eigen (Equation 1), Explicit LSI(Equation 2), L-Solve (Equation 3) on the samesmall-scale corpus.
For convenience we plot an ap-proximation rate which is either K or N1 dependingon method; at K = 500 and N1 = 500, these ap-proximations become exact.
This is shown in Figure1.
We also observe the effects of approximation andsee that the performance increases steadily as weincrease the computational factor.
We see that theorthonormal eigenvector (Equation 1) method andthe L-solve (Equation 3) method are clearly simi-lar in approximation quality.
We see that the explicitLSI method (Equation 2) and the LSI method bothperform significantly worse for most of the approxi-5A likely explanation for this is that low frequency terms areless evenly distributed and the effect of calculating the matrixinverse magnifies the noise from the low frequency terms1738mation amounts.
Explicit LSI is worse than the otherapproximations as it first maps the test documentsinto a K-dimensional LSI topic space, before map-ping back into theN -dimensional explicit space.
Asexpected this performs worse than standard LSI forall but high values of K as there is significant errorin both mappings.
We also see that the (CL-)ESAbaseline, which is very low due to the small numberof documents, is improved upon by even the least ap-proximation of orthonormalization.
In the remain-ing of this section, we report results using the L-Solve method as it has a very good performance andis computationally less expensive than ON-Eigen.Evaluation and Comparison: We compareONETA using the L-Solve method with N1 valuesfrom 1000 to 9000 topics with (CL-)ESA (usingSQRT normalization), LDA (using 1000 topics)and LSI (using 4000 topics).
We choose the largesttopic count for LSI and LDA we could to providethe best possible comparison.
For LSI, the choice ofK was determined on the basis of operating systemmemory limits, while for LDA we experimentedwith higher values for K without any performanceimprovement, likely due to overfitting.
We alsostress that for L-Solve ONETA, N1 is not the topiccount but an approximation rate of the mapping.
Inall settings we use N topics as with standard ESA,and so should not be considered directly comparableto the K values of these methods.We also compare to a baseline system that re-lies on word-by-word translation, where we use themost likely single translation of a word as given by aphrase table generated by the Moses system (Koehnet al 2007) on the EuroParl corpus (Koehn, 2005).Top 1, Top 5 and Top 10 Precision as well as MeanReciprocal Rank are reported in Table 2.Interestingly, even for a small number of docu-ments (e.g., N1 = 6000) our results improve boththe word-translation baseline as well as all othertopic models, ESA, LDA and LSI in particular.
Wenote that at this level the method is still efficientlycomputable and calculating the inverse in practicetakes less time than training the Moses system.
Thesignificance for results (N1 ?
7000) have beentested by means of a bootstrap resampling signifi-cance test, finding out that our results significantlyimprove on the translation base line at a 99% level.Further, we consider a straightforward combina-tion of our method with the translation system con-sisting of appending the topic vectors and the trans-lation frequency vectors, weighted by the relativeaverage norms of the vectors.
We see that in thiscase the translations continue to improve the perfor-mance of the system (albeit not significantly), sug-gesting a clear potential for this system to help in im-proving machine translation results.
While we havepresented results for English and Spanish here, simi-lar results were obtained for the German and Frenchcase but are not presented here due to space limita-tions.In Table 2 we also include the user time and peakresident memory of each of these processes, mea-sured on an 8 Core Intel Xeon 2.50 GHz server.We do not include the results for Word Translationas many hours were spent learning a phrase table,which includes translations for many phrases not inthe test set.
We see that the ONETA method signif-icantly outperforms LSI and LDA in terms of speedand memory consumption.
This is in line with thetheoretical calculations presented earlier where weargued that inverting the N ?N dense matrix XTXwhen W  N is computationally lighter than find-ing an eigendecomposition of the W ?
W sparsematrix XXT.
In addition, as we do not multiply(XTX)?1 and XT, we do not need to allocate alarge W ?
K matrix in memory as with LSI andLDA.The implementations of ESA, ONETA,LSI and LDA used as well as the datafor the experiments are available athttp://github.com/jmccrae/oneta.5 ConclusionWe have presented a novel method for cross-lingualtopic modelling, which combines the strengths ofexplicit and latent topic models and have demon-strated its application to cross-lingual documentmatching.
We have in particular shown that themethod outperforms widely used topic models suchas Explicit Semantic Analysis (ESA), Latent Seman-tic Indexing (LSI) and Latent Dirichlet Allocation(LDA).
Further, we have shown that it outperformsa simple baseline relying on word-by-word transla-tion of the query document into the target language,1739while the induction of the model takes less timethan training the machine translation system from aparallel corpus.
We have also presented an effec-tive approximation method, i.e.
L-Solve, which sig-nificantly reduces the computational cost associatedwith computing the topic models.AcknowledgementsThis work was funded by the Monnet Projectand the Portdial Project under the EC Sev-enth Framework Programme, Grants No.248458 and 296170.
Roman Klinger has beenfunded by the ?Its OWL?
project (?Intelli-gent Technical Systems Ostwestfalen-Lippe?,http://www.its-owl.de/), a leading-edgecluster of the German Ministry of Education andResearch.ReferencesDennis S Bernstein.
2005.
Matrix mathematics, 2nd Edi-tion.
Princeton University Press Princeton.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Philipp Cimiano, Antje Schultz, Sergej Sizov, PhilippSorg, and Steffen Staab.
2009.
Explicit versus la-tent concept models for cross-language information re-trieval.
In IJCAI, volume 9, pages 1513?1518.Don Coppersmith and Shmuel Winograd.
1990.
Matrixmultiplication via arithmetic progressions.
Journal ofsymbolic computation, 9(3):251?280.Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
JASIS,41(6):391?407.Chris Ding, Tao Li, and Wei Peng.
2006.
NMF andPLSI: equivalence and a hybrid algorithm.
In Pro-ceedings of the 29th annual international ACM SIGIR,pages 641?642.
ACM.Susan T Dumais, Todd A Letsche, Michael L Littman,and Thomas K Landauer.
1997.
Automatic cross-language retrieval using latent semantic indexing.
InAAAI spring symposium on cross-language text andspeech retrieval, volume 15, page 21.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using Wikipedia-based ex-plicit semantic analysis.
In Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence,volume 6, page 12.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd annual interna-tional ACM SIGIR conference, pages 50?57.
ACM.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, et al2007.
Moses: Open source toolkit for sta-tistical machine translation.
In Proceedings of the 45thAnnual Meeting of the ACL, pages 177?180.
Associa-tion for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In MT summit, volume 5.Daniel D Lee and H Sebastian Seung.
1999.
Learningthe parts of objects by non-negative matrix factoriza-tion.
Nature, 401(6755):788?791.Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010.Topic models for word sense disambiguation andtoken-based idiom detection.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, pages 1138?1147.
Association forComputational Linguistics.David Mimno, Hanna M Wallach, Jason Naradowsky,David A Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 880?889.
Association forComputational Linguistics.Martha Palmer, Owen Rambow, and Alexis Nasr.
1998.Rapid prototyping of domain-specific machine trans-lation systems.
In Machine Translation and the Infor-mation Soup, pages 95?102.
Springer.Philipp Sorg and Philipp Cimiano.
2008.
Cross-lingualinformation retrieval with explicit semantic analysis.In Proceedings of the Cross-language Evaluation Fo-rum 2008.Philipp Sorg and Philipp Cimiano.
2010.
An experi-mental comparison of explicit semantic analysis im-plementations for cross-language retrieval.
In NaturalLanguage Processing and Information Systems, pages36?48.
Springer.Vassilis Spiliopoulos, George A Vouros, and VangelisKarkaletsis.
2007.
Mapping ontologies elements us-ing features in a latent space.
In IEEE/WIC/ACMInternational Conference on Web Intelligence, pages457?460.
IEEE.Yik-Cheung Tam, Ian Lane, and Tanja Schultz.
2007.Bilingual LSA-based adaptation for statistical machinetranslation.
Machine Translation, 21(4):187?207.1740
