Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 181?185,Dublin, Ireland, August 23-24, 2014.CMUQ-Hybrid: Sentiment ClassificationBy Feature Engineering and Parameter TuningKamla Al-Mannai1, Hanan Alshikhabobakr2,Sabih Bin Wasi2, Rukhsar Neyaz2, Houda Bouamor2, Behrang Mohit2Texas A&M University in Qatar1, Carnegie Mellon University in Qatar2almannaika@hotmail.com1{halshikh, sabih, rukhsar, hbouamor, behrang}@cmu.eduAbstractThis paper describes the system we sub-mitted to the SemEval-2014 shared taskon sentiment analysis in Twitter.
Our sys-tem is a hybrid combination of two systemdeveloped for a course project at CMU-Qatar.
We use an SVM classifier and cou-ple a set of features from one system withfeature and parameter optimization frame-work from the second system.
Most of thetuning and feature selection efforts wereoriginally aimed at task-A of the sharedtask.
We achieve an F-score of 84.4% fortask-A and 62.71% for task-B and the sys-tems are ranked 3rd and 29th respectively.1 IntroductionWith the proliferation of Web2.0, people increas-ingly express and share their opinion through so-cial media.
For instance, microblogging websitessuch as Twitter1are becoming a very popular com-munication tool.
An analysis of this platform re-veals a large amount of community messages ex-pressing their opinions and sentiments on differ-ent topics and aspects of life.
This makes Twit-ter a valuable source of subjective and opinionatedtext that could be used in several NLP researchworks on sentiment analysis.
Many approachesfor detecting subjectivity and determining polarityof opinions in Twitter have been proposed (Pangand Lee, 2008; Davidov et al., 2010; Pak andParoubek, 2010; Tang et al., 2014).
For instance,the Twitter sentiment analysis shared task (Nakovet al., 2013) is an interesting testbed to developand evaluate sentiment analysis systems on socialmedia text.
Participants are asked to implementThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1http://twitter.coma system capable of determining whether a giventweet expresses positive, negative or neutral sen-timent.
In this paper, we describe the CMUQ-Hybrid system we developed to participate in thetwo subtasks of SemEval 2014 Task 9 (Rosenthalet al., 2014).
Our system uses an SVM classifierwith a rich set of features and a parameter opti-mization framework.2 Data PreprocessingWorking with tweets presents several challengesfor NLP, different from those encountered whendealing with more traditional texts, such asnewswire data.
Tweet messages usually containdifferent kinds of orthographic and typographicalerrors such as the use of special and decorativecharacters, letter duplication used generally foremphasis, word duplication, creative spelling andpunctuation, URLs, #hashtags as well as the useof slangs and special abbreviations.
Hence, beforebuilding our classifier, we start with a preprocess-ing step on the data, in order to normalize it.
Allletters are converted to lower case and all wordsare reduced to their root form using the WordNetLemmatizer in NLTK2(Bird et al., 2009).
We keptonly some punctuation marks: periods, commas,semi-colons, and question and exclamation marks.The excluded characters were identified to be per-formance boosters using the best-first branch andbound technique described in Section 3.3 Feature ExtractionOut of a wide variety of features, we selected themost effective features using the best-first branchand bound method (Neapolitan, 2014), a searchtree technique for solving optimization problems.We used this technique to determine which punc-tuation marks to keep in the preprocessing step and2http://www.nltk.org/api/nltk.stem.html181in selecting features as well.
In the feature selec-tion step, the root node is represented by a bag ofwords feature, referred as textual tokens.At each level of the tree, we consider a set ofdifferent features, and iteratively we carry out thefollowing steps: we process the current feature bygenerating its successors, which are all the otherfeatures.
Then, we rank features according to thef-score and we only process the best feature andprune the rest.
We pass all the current pruned fea-tures as successors to the next level of the tree.
Theprocess iterates until all partial solutions in the treeare processed or terminated.
The selected featuresare the following:Sentiment lexicons : we used the Bing Liu Lex-icon (Hu and Liu, 2004), the MPQA SubjectivityLexicon (Wilson et al., 2005), and NRC HashtagSentiment Lexicon (Mohammad et al., 2013).
Wecount the number of words in each class, result-ing in three features: (a) positive words count, (b)negative words count and (c) neutral words count.Negative presence: presence of negative wordsin a term/tweet using a list of negative words.
Thelist used is built from the Bing Liu Lexicon (Huand Liu, 2004).Textual tokens: the target term/tweet is seg-mented into tokens based on space.
Token identityfeatures are created and assigned the value of 1.Overall polarity score: we determine the polar-ity scores of words in a target term/tweet using theSentiment140 Lexicon (Mohammad et al., 2013)and the SentiWordNet lexicon (Baccianella et al.,2010).
The overall score is computed by addingup all word scores.Level of association: indicates whether theoverall polarity score of a term is greater than 0.2or not.
The threshold value was optimized on thedevelopment set.Sentiment frequency: indicates the most fre-quent word sentiment in the tweet.
We determinethe sentiment of words using an automaticallygenerated lexicon.
The lexicon comprises 3,247words and their sentiments.
Words were obtainedfrom the provided training set for task-A and sen-timents were generated using our expression-levelclassifier.We used slightly different features for Task-Aand Task-B.
The features extracted for each taskare summarized in Table 1.Feature Task A Task BPositive words count XNegative words count XNeutral words count XNegative presence X XTextual tokens X XOverall polarity score X XLevel of association XSentiment frequency XTable 1: Feature summary for each task.4 Modeling Kernel FunctionsInitially we experimented with both logisticregression and the Support Vector Machine(SVM) (Fan et al., 2008), using the StochasticGradient Descent (SGD) algorithm for parame-ter optimization.
In our development experiments,SVM outperformed and became our single classi-fier.
We used the LIBSVM package (Chang andLin, 2011) to train and test our classifier.An SVM kernel function and associated param-eters were optimized for best F-score on the de-velopment set.
In order to avoid the model over-fitting the data, we select the optimal parametervalue only if there are smooth gaps between thenear neighbors of the corresponded F-score.
Oth-erwise, the search will continue to the second op-timal value.In machine learning, the difference between thenumber of training samples, m, and the numberof features, n, is crucial in the selection processof SVM kernel functions.
The Gaussian kernel issuggested when m is slightly larger than n. Other-wise, the linear kernel is recommended.
In Task-B, the n : m ratio was 1 : 3 indicating a largedifference between the two numbers.
Whereas inTask-A, a ratio of 5 : 2 indicated a small differ-ence between the two numbers.
We selected thetheoretical types, after conducting an experimen-tal verification to identify the best kernel functionaccording to the f-score.We used a radical basis function kernel for theexpression-level task and the value of its gammaparameter was adjusted to 0.319.
Whereas, weused a linear function kernel for the message-leveltask and the value of its cost parameter was ad-justed to 0.053.1825 Experiments and ResultsIn this section, we describe the data and the sev-eral experiments we conducted for both tasks.
Wetrain and evaluate our classifier with the training,development and testing datasets provided for theSemEval 2014 shared task.
A short summary ofthe data distribution is shown in Table 2.Dataset Postive Negative NeutralTask-A:Train (9,451) 62% 33% 5%Dev (1,135) 57% 38% 5%Test (10,681) 60% 35% 5%Task-B:Train (9,684) 38% 15% 47%Dev (1,654) 35% 21% 44%Test (5,754) 45% 15% 40%Table 2: Datasets distribution percentage per class.Our test dataset is composed of five differentsets: The test dataset is composed of five dif-ferent sets: Twitter2013 a set of tweets collectedfor the SemEval2013 test set, Twitter2014, tweetscollected for this years version, LiveJournal2014consisting of formal tweets, SMS2013, a collectionof sms messages, TwitterSarcasm, a collection ofsarcastic tweets.5.1 Task-AFor this task, we train our classifier on 10,586terms (9,451 terms in the training set and 1,135in the development set), tune it on 4,435 terms,and evaluate it using 10,681 terms.
The averageF-score of the positive and negative classes foreach dataset is given in the first part of Table 3.The best F-score value of 88.94 is achieved on theTwitter2013.We conducted an ablation study illustrated inthe second part of Table 3 shows that all the se-lected features contribute well in our system per-formance.
Other than the textual tokens feature,which refers to a bag of preprocessed tokens, thestudy highlights the role of the term polarity scorefeature: ?4.20 in the F-score, when this feature isnot considered on the TwitterSarcasm dataset.Another study conducted is a feature correlationanalysis, in which we grouped features with sim-ilar intuitions.
Namely the two features negativepresence and negative words count are groupedas ?negative features?, and the features positivewords count and negative words count are groupedas ?words count?.
We show in Table 4 the effecton f-score after removing each group from the fea-tures set.
Also we show the f-score after remov-ing each individual feature within the group.
Thishelps us see whether features within a group areredundant or not.
For the Twitter2014 dataset, wenotice that excluding one of the features in any ofthe two groups leads to a significant drop, in com-parison to the total drop by its group.
The uncor-related contributions of features within the samegroup indicate that features are not redundant toeach other and that they are indeed capturing dif-ferent information.
However, in the case of theTwitterSarcasm dataset, we observe that the neg-ative presence feature is not only not contributingto the system performance but also adding noiseto the feature space, specifically, to the negativewords count feature.5.2 Task-BFor this task, we trained our classifier on 11,338tweets (9,684 terms in the training set and 1,654in the development set), tuned it on 3,813 tweets,and evaluated it using 8,987 tweets.
Results fordifferent feature configurations are reported in Ta-ble 5.It is important to note that if we exclude the tex-tual tokens feature, all datasets benefit the mostfrom the polarity score feature.
It is interesting tonote that the bag of words, referred to as textualtokens, is not helping in one of the datasets, theTwitterSarcasm set.
For all datasets, performancecould be improved by removing different features.In Table 5, we observe that the Negative pres-ence feature decreases the F-score on the Twitter-Sarcasm dataset.
This could be explained by thefact that negative words do not usually appear ina negative implication in sarcastic messages.
Forexample, this tweet: Such a fun Saturday catch-ing up on hw.
which has a negative sentiment, isclassified positive because of the absence of neg-ative words.
Table 5 shows that the textual tokensfeature increases the classifier?s performance up to+21.07 for some datasets.
However, using a largenumber of features in comparison to the numberof training samples could increase data sparsenessand lower the classifier?s performance.We conducted a post-competition experiment toexamine the relationship between the number offeatures and the number of training samples.
We183Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013F-score 84.40 76.99 84.21 88.94 87.98Negative presence -0.45 0.00 -0.45 -0.23 +0.30Positive words count -0.52 -1.37 -0.11 -0.02 +0.38Negative words count -0.50 -2.20 -0.61 -0.47 -1.66Polarity score -1.83 -4.20 -0.23 -2.14 -3.00Level of association -0.18 0.00 -0.18 -0.07 +0.57Textual tokens -8.74 -2.40 -3.02 -4.37 -6.06Table 3: Task-A feature ablation study.
F-scores calculated on each set along with the effect whenremoving one feature at a time.Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013F-score 84.40 76.99 84.21 88.94 87.98Negative features -1.53 -0.84 -3.05 -1.88 -0.67Negative presence -0.45 0.00 -0.45 -0.23 +0.3Negative words count -0.50 -2.20 -0.61 -0.47 -1.66Words count -1.07 -2.2 -0.79 -0.62 -2.01Positive words count -0.52 -1.37 -0.11 -0.02 +0.38Negative words count -0.50 -2.20 -0.61 -0.47 -1.66Table 4: Task-A features correlation analysis.
We grouped features with similar intuitions and we calcu-lated F-scores on each set along with the effect when removing one feature at a time.fixed the size of our training dataset.
Then, wecompared the performance of our classifier usingonly the bag of tokens feature, in two differentsizes.
In the first experiment, we included all to-kens collected from all tweets.
In the second, weonly considered the top 20 ranked tokens fromeach tweet.
Tokens were ranked according to thedifference between their highest level of associa-tion into one of the sentiments and the sum of therest.
The level of associations for tokens were de-termined using the Sentiment140 and SentiWord-Net lexicons.
The threshold number of tokens wasidentified empirically for best performance.
Wefound that the classifier?s performance has beenimproved by 2 f-score points when the size of to-kens bag is smaller.
The experiment indicates thatthe contribution of the bag of words feature can beincreased by reducing the size of vocabulary list.6 Error AnalysisOur efforts are mostly tuned towards task-A,hence our inspection and analysis is focused ontask-A.
The error rate calculated per sentimentclass: positive, negative and neutral are 6.8%,14.9% and 93.8%, respectively.
The highest errorrate in the neutral class, 93.8%, is mainly due tothe few neutral examples in the training data (only5% of the data).
Hence the system could not learnfrom such a small set of neutral class examples.In the case of negative class error rate, 14.9%,most of which were classified as positive.
An ex-ample of such classification: I knew it was toogood to be true OTL.
Since our system highly re-lies on lexicon, hence looking at lexicon assignedpolarity to the phrase too good to be true which ispositive, happens because the positive words goodand true has dominating positive polarity.Lastly for the positive error rate, which is rel-atively lower, 6%, most of which were classifiednegative instead of positive.
An example of suchclassification: Looks like we?re getting the heavi-est snowfall in five years tomorrow.
Awesome.
I?llnever get tired of winter.
Although the phrase car-ries a positive sentiment, the individual negativewords of the phrase never and tired again domi-nates over the phrase.7 ConclusionWe described our systems for Twitter SentimentAnalysis shared task.
We participated in bothtasks, but were mostly focused on task-A.
Our hy-brid system was assembled by integrating a richset of lexical features into a framework of fea-ture selection and parameter tuning, The polarity184Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013F-score 62.71 40.95 65.14 63.22 61.75Negative presence -1.65 +1.26 -3.37 -3.66 -0.95Neutral words count +0.05 0.00 -0.72 -0.57 -0.54Polarity score -4.03 -6.92 -3.82 -3.83 -4.84Sentiment frequency +0.10 0.00 +0.18 -0.12 -0.05textual tokens -17.91 +6.5 -21.07 -19.97 -15.8Table 5: Task B feature ablation study.
F-scores calculated on each set along with the effect whenremoving one feature at a time.score feature was the most important feature forour model in both tasks.
The F-score results wereconsistent across all datasets, except the Twitter-Sarcasm dataset.
It indicates that feature selectionand parameter tuning steps were effective in gen-eralizing the model to unseen data.AcknowledgmentWe would like to thank Kemal Oflazer and also theshared task organizers for their support throughoutthis work.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An Enhanced Lex-ical Resource for Sentiment Analysis and OpinionMining.
In Proceedings of the Seventh conferenceon International Language Resources and Evalua-tion (LREC?10), pages 2200?2204, Valletta, Malta.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media, Inc.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A Library for Support Vector Machines.ACM Transactions on Intelligent Systems and Tech-nology, 2:27:1?27:27.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics:Posters, pages 241?249, Uppsala, Sweden.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Minqing Hu and Bing Liu.
2004.
Mining and Sum-marizing Customer Reviews.
In Proceedings of theTenth ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages 168?177, Seattle, WA, USA.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets.
In SecondJoint Conference on Lexical and Computational Se-mantics (*SEM), Volume 2: Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 321?327, Atlanta, Geor-gia, USA.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 Task 2: Sentiment Analysis inTwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, USA.Richard E. Neapolitan, 2014.
Foundations of Algo-rithms, pages 257?262.
Jones & Bartlett Learning.Alexander Pak and Patrick Paroubek.
2010.
TwitterBased System: Using Twitter for DisambiguatingSentiment Ambiguous Adjectives.
In Proceedingsof the 5th International Workshop on Semantic Eval-uation, pages 436?439, Uppsala, Sweden.Bo Pang and Lillian Lee, 2008.
Opinion Mining andSentiment Analysis, volume 2, pages 1?135.
NowPublishers Inc.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Proceedings of theEighth International Workshop on Semantic Evalu-ation (SemEval?14), Dublin, Ireland.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, TingLiu, and Bing Qin.
2014.
Learning Sentiment-Specific Word Embedding for Twitter SentimentClassification.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics, pages 1555?1565, Baltimore, Maryland.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing Contextual Polarity in Phrase-level Sentiment Analysis.
In Proceedings of theHuman Language Technology Conference and Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 347?354, Vancouver, B.C.,Canada.185
