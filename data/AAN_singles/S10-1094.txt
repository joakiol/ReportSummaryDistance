Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 421?426,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsCFILT: Resource Conscious Approaches for All-Words Domain SpecificWSDAnup Kulkarni Mitesh M. Khapra Saurabh Sohoney Pushpak BhattacharyyaDepartment of Computer Science and Engineering,Indian Institute of Technology Bombay,Powai, Mumbai 400076,India{anup,miteshk,saurabhsohoney,pb}@cse.iitb.ac.inAbstractWe describe two approaches for All-wordsWord Sense Disambiguation on a Spe-cific Domain.
The first approach is aknowledge based approach which extractsdomain-specific largest connected com-ponents from the Wordnet graph by ex-ploiting the semantic relations between allcandidate synsets appearing in a domain-specific untagged corpus.
Given a testword, disambiguation is performed byconsidering only those candidate synsetsthat belong to the top-k largest connectedcomponents.The second approach is a weakly super-vised approach which relies on the ?OneSense Per Domain?
heuristic and uses afew hand labeled examples for the mostfrequently appearing words in the targetdomain.
Once the most frequent wordshave been disambiguated they can pro-vide strong clues for disambiguating otherwords in the sentence using an iterativedisambiguation algorithm.
Our weaklysupervised system gave the best perfor-mance across all systems that participatedin the task even when it used as few as 100hand labeled examples from the target do-main.1 IntroductionDomain specific WSD exhibits high level of ac-curacy even for the all-words scenario (Khapra etal., 2010) - provided training and testing are on thesame domain.
However, the effort of creating thetraining corpus - annotated sense marked corpora- for every domain of interest has always been amatter of concern.
Therefore, attempts have beenmade to develop unsupervised (McCarthy et al,2007; Koeling et al, 2005) and knowledge basedtechniques (Agirre et al, 2009) for WSD whichdo not need sense marked corpora.
However, suchapproaches have not proved effective, since theytypically do not perform better than the Wordnetfirst sense baseline accuracy in the all-words sce-nario.Motivated by the desire to develop annotation-lean all-words domain specific techniques forWSD we propose two resource conscious ap-proaches.
The first approach is a knowledge basedapproach which focuses on retaining only domainspecific synsets in the Wordnet using a two steppruning process.
In the first step, the Wordnetgraph is restricted to only those synsets whichcontain words appearing in an untagged domain-specific corpus.
In the second step, the graph ispruned further by retaining only the largest con-nected components of the pruned graph.
Each tar-get word in a given sentence is then disambiguatedusing an iterative disambiguation process by con-sidering only those candidate synsets which ap-pear in the top-k largest connected components.Our knowledge based approach performed betterthan current state of the art knowledge based ap-proach (Agirre et al, 2009).
Also, the precisionwas better than the Wordnet first sense baselineeven though the F-score was slightly lower thanthe baseline.The second approach is a weakly supervised ap-proach which uses a few hand labeled examplesfor the most frequent words in the target domainin addition to the publicly available mixed-domainSemCor (Miller et al, 1993) corpus.
The underly-ing assumption is that words exhibit ?One SensePer Domain?
phenomenon and hence even as fewas 5 training examples per word would be suffi-cient to identify the predominant sense of the mostfrequent words in the target domain.
Further, oncethe most frequent words have been disambiguatedusing the predominant sense, they can providestrong clues for disambiguating other words in the421sentence.
Our weakly supervised system gave thebest performance across all systems that partici-pated in the task even when it used as few as 100hand labeled examples from the target domain.The remainder of this paper is organized as fol-lows.
In section 2 we describe related work ondomain-specific WSD.
In section 3 we discuss anIterative Word Sense Disambiguation algorithmwhich lies at the heart of both our approaches.
Insection 4 we describe our knowledge based ap-proach.
In section 5 we describe our weakly su-pervised approach.
In section 6 we present resultsand discussions followed by conclusion in section7.2 Related WorkThere are two important lines of work for do-main specific WSD.
The first focuses on targetword specific WSD where the results are reportedon a handful of target words (41-191 words) onthree lexical sample datasets, viz., DSO corpus(Ng and Lee, 1996), MEDLINE corpus (Weeber etal., 2001) and the corpus of Koeling et al (2005).The second focuses on all-words domain specificWSD where the results are reported on large anno-tated corpora from two domains, viz., TOURISMand HEALTH (Khapra et al, 2010).In the target word setting, it has been shown thatunsupervised methods (McCarthy et al, 2007) andknowledge based methods (Agirre et al, 2009)can do better than wordnet first sense baseline andin some cases can also outperform supervised ap-proaches.
However, since these systems have beentested only for certain target words, the question oftheir utility in all words WSD it still open .In the all words setting, Khapra et al (2010)have shown significant improvements over thewordnet first sense baseline using a fully super-vised approach.
However, the need for sense anno-tated corpus in the domain of interest is a matter ofconcern and provides motivation for adapting theirapproach to annotation scarce scenarios.
Here, wetake inspiration from the target-word specific re-sults reported by Chan and Ng (2007) where byusing just 30% of the target data they obtained thesame performance as that obtained by using theentire target data.We take the fully supervised approach of(Khapra et al, 2010) and convert it to a weakly su-pervised approach by using only a handful of handlabeled examples for the most frequent words ap-pearing in the target domain.
For the remainingwords we use the sense distributions learnt fromSemCor (Miller et al, 1993) which is a publiclyavailable mixed domain corpus.
Our approach isthus based on the ?annotate-little from the targetdomain?
paradigm and does better than all the sys-tems that participated in the shared task.Even our knowledge based approach does betterthan current state of the art knowledge based ap-proaches (Agirre et al, 2009).
Here, we use an un-tagged corpus to prune the Wordnet graph therebyreducing the number of candidate synsets for eachtarget word.
To the best of our knowledge such anapproach has not been tried earlier.3 Iterative Word Sense DisambiguationThe Iterative Word Sense Disambiguation (IWSD)algorithm proposed by Khapra et al (2010) lies atthe heart of both our approaches.
They use a scor-ing function which combines corpus based param-eters (such as, sense distributions and corpus co-occurrence) and Wordnet based parameters (suchas, semantic similarity, conceptual distance, etc.
)for ranking the candidates synsets of a word.
Thealgorithm is iterative in nature and involves thefollowing steps:?
Tag all monosemous words in the sentence.?
Iteratively disambiguate the remaining wordsin the sentence in increasing order of their de-gree of polysemy.?
At each stage rank the candidate senses of aword using the scoring function of Equation(1).S?= argmaxi(?iVi+?j?JWij?
Vi?
Vj) (1)where,i ?
Candidate SynsetsJ = Set of disambiguated words?i= BelongingnessToDominantConcept(Si)Vi= P (Si|word)Wij= CorpusCooccurrence(Si, Sj)?
1/WNConceptualDistance(Si, Sj)?
1/WNSemanticGraphDistance(Si, Sj)The scoring function as given above cleanlyseparates the self-merit of a synset (P (Si|word))422as learnt from a tagged corpus and its interaction-merit in the form of corpus co-occurrence, con-ceptual distance, and wordnet-based semantic dis-tance with the senses of other words in the sen-tence.
The scoring function can thus be easilyadapted depending upon the amount of informa-tion available.
For example, in the weakly su-pervised setting, P (Si|word) will be available forsome words for which either manually hand la-beled training data from environment domain isused or which appear in the SemCor corpus.
Forsuch words, all the parameters in Equation (1) willbe used for scoring the candidate synsets and forremaining words only the interaction parameterswill be used.
Similarly, in the knowledge basedsetting, P (Si|word) will never be available andhence only the wordnet based interaction parame-ters (i.e., WNConceptualDistance(Si, Sj) andWNSemanticGraphDistance(Si, Sj)) will beused for scoring the pruned list of candidatesynsets.
Please refer to (Khapra et al, 2010) forthe details of how each parameter is calculated.4 Knowledge-Based WSD using GraphPruningWordnet can be viewed as a graph where synsetsact as nodes and the semantic relations betweenthem act as edges.
It should be easy to seethat given a domain-specific corpus, synsets fromsome portions of this graph would be more likelyto occur than synsets from other portions.
Forexample, given a corpus from the HEALTH do-main one might expect synsets belonging to thesub-trees of ?doctor?, ?medicine?, ?disease?
toappear more frequently than the synsets belongingto the sub-tree of ?politics?.
Such dominance ex-hibited by different components can be harnessedfor domain-specific WSD and is the motivation forour work.The crux of the approach is to identify such do-main specific components using a two step prun-ing process as described below:Step 1: First, we use an untagged corpus fromthe environment domain to identify the uniquewords appearing in the domain.
Note that, byunique words we mean all content words whichappear at least once in the environment corpus(these words may or may not appear in a gen-eral mixed domain corpus).
This untagged corpuscontaining 15 documents (22K words) was down-loaded from the websites of WWF1 and ECNC2and contained articles on Climate Change, De-forestation, Species Extinction, Marine Life andEcology.
Once the unique words appearing inthis environment-specific corpus are identified, werestrict the Wordnet graph to only those synsetswhich contain one or more of these unique wordsas members.
This step thus eliminates all spurioussynsets which are not related to the environmentdomain.Step 2: In the second step, we perform a Breadth-First-Search on the pruned graph to identify theconnected components of the graph.
Whiletraversing the graph we consider only those edgeswhich correspond to the hypernymy-hyponymy re-lation and ignore all other semantic relations as weobserved that such relations add noise to the com-ponents.
The top-5 largest components thus iden-tified were considered to be environment-specificcomponents.
A subset of synsets appearing in onesuch sample component is listed in Table 1.Each target word in a given sentence is then disam-biguated using the IWSD algorithm described insection 3.
However, now the argmax of Equation(1) is computed only over those candidate synsetswhich belong to the top-5 largest components andall other candidate synsets are ignored.
The sug-gested pruning technique is indeed very harsh andas a result there are many words for which noneof their candidate synsets belong to these top-5largest components.
These are typically domain-invariant words for which pruning does not makesense as the synsets of such generic words donot belong to domain-specific components of theWordnet graph.
In such cases, we consider all thecandidate synsets of these words while computingthe argmax of Equation (1).5 Weakly Supervised WSDWords are known to exhibit ?One Sense Per Do-main?.
For example, in the HEALTH domain theword cancer will invariably occur in the diseasesense and almost never in the sense of a zodiacsign.
This is especially true for the most frequentlyappearing nouns in the domain as these are typi-cally domain specific nouns.
For example, nounssuch as farmer, species, population, conservation,nature, etc.
appear very frequently in the envi-ronment domain and exhibit a clear predominant1http://www.wwf.org2http://www.ecnc.org423{ safety} - NOUN - the state of being certain that adverse effects will not be caused by some agentunder defined conditions; ?insure the safety of the children?
; ?the reciprocal of safety is risk?
{preservation, saving} - NOUN - the activity of protecting something from loss or danger{environment} - NOUN - the totality of surrounding conditions; ?he longed for the comfortableenvironment of his living room?
{animation, life, living, aliveness} - NOUN - the condition of living or the state of being alive;?while there?s life there?s hope?
; ?life depends on many chemical and physical processes?
{renovation, restoration, refurbishment} - NOUN - the state of being restored to its former goodcondition; ?the inn was a renovation of a Colonial house?
{ecology} - NOUN - the environment as it relates to living organisms; ?it changed the ecology ofthe island?
{development} - NOUN - a state in which things are improving; the result of developing (as in theearly part of a game of chess); ?after he saw the latest development he changed his mind and be-came a supporter?
; ?in chess your should take care of your development before moving your queen?
{survival, endurance} - NOUN - a state of surviving; remaining alive.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.Table 1: Environment specific component identified after pruningsense in the domain.
As a result as few as 5 handlabeled examples per noun are sufficient for find-ing the predominant sense of these nouns.
Further,once these most frequently occurring nouns havebeen disambiguated they can help in disambiguat-ing other words in the sentence by contributing tothe interaction-merit of Equation (1) (note that inEquation (1), J = Set of disambiguated words).Based on the above intuition, we slightly mod-ified the IWSD algorithm and converted it to aweakly supervised algorithm.
The original algo-rithm as described in section 3 uses monosemouswords as seed input (refer to the first step of the al-gorithm).
Instead, we use the most frequently ap-pearing nouns as the seed input.
These nouns aredisambiguated using their pre-dominant sense ascalculated from the hand labeled examples.
Ourweakly supervised IWSD algorithm can thus besummarized as follows?
If a word w in a test sentence belongs tothe list of most frequently appearing domain-specific nouns then disambiguate it first us-ing its self-merit (i.e., P (Si|word)) as learntfrom the hand labeled examples.?
Iteratively disambiguate the remaining wordsin the sentence in increasing order of their de-gree of polysemy.?
While disambiguating the remaining wordsrank the candidate senses of a word usingthe self-merit learnt from SemCor and theinteraction-merit based on previously disam-biguated words.The most frequent words and the correspondingexamples to be hand labeled are extracted from thesame 15 documents (22K words) as described insection 4.6 ResultsWe report the performance of our systems in theSEMEVAL task on All-words Word Sense Dis-ambiguation on a Specific Domain (Agirre et al,2010).
The task involved sense tagging 1398nouns and verbs from 3 documents extracted fromthe environment domain.
We submitted one runfor the knowledge based system and 2 runs for theweakly supervised system.
For the weakly super-vised system, in one run we used 5 training ex-amples each for the 80 most frequently appear-ing nouns in the domain and in the second run we424used 5 training examples each for the 200 mostfrequently appearing nouns.
Both our submis-sions in the weakly supervised setting performedbetter than all other systems that participated inthe shared task.
Post-submission we even exper-imented with using 5 training examples each foras few as 20 most frequent nouns and even inthis case we found that our weakly supervised sys-tem performed better than all other systems thatparticipated in the shared task.The precision of our knowledge based systemwas slightly better than the most frequent sense(MFS) baseline reported by the task organizersbut the recall was slightly lower than the baseline.Also, our approach does better than the currentstate of the art knowledge based approach (Person-alized Page Rank approach of Agirre et al (2009)).All results are summarized in Table 2.
The fol-lowing guide specifies the systems reported:?
WS-k: Weakly supervised approach using 5training examples for the k most frequentlyappearing nouns in the environment domain.?
KB: Knowledge based approach using graphbased pruning.?
PPR: Personalized PageRank approach ofAgirre et al (2009).?
MFS: Most Frequent Sense baseline pro-vided by the task organizers.?
Random: Random baseline provided by thetask organizers.System Precision Recall Rank in shared taskWS-200 0.570 0.555 1WS-80 0.554 0.540 2WS-20 0.548 0.535 3 (Post submission)KB 0.512 0.495 7PPR 0.373 0.368 24 (Post submission)MFS 0.505 0.505 6Random 0.23 0.23 30Table 2: The performance of our systems in theshared taskIn Table 3 we provide the results of WS-200 foreach POS category.
As expected, the results fornouns are much better than those for verbs mainlybecause nouns are more likely to stick to the ?Onesense per domain?
property than verbs.Category Precision RecallVerbs 45.37 42.89Nouns 59.64 59.01Table 3: The performance of WS-200 on eachPOS category7 ConclusionWe presented two resource conscious approachesfor All-words Word Sense Disambiguation on aSpecific Domain.
The first approach is a knowl-edge based approach which retains only domainspecific synsets from the Wordnet by using a twostep pruning process.
This approach does betterthan the current state of the art knowledge basedapproaches although its performance is slightlylower than the Most Frequent Sense baseline.
Thesecond approach which is a weakly supervised ap-proach based on the ?annotate-little from the tar-get domain?
paradigm performed better than allsystems that participated in the task even when itused as few as 100 hand labeled examples fromthe target domain.
This approach establishes theveracity of the ?One sense per domain?
phe-nomenon by showing that even as few as five ex-amples per word are sufficient for predicting thepredominant sense of a word.AcknowledgmentsWe would like to thank Siva Reddy and AbhilashInumella (from IIIT Hyderabad, India) for provid-ing us the results of Personalized PageRank (PPR)for comparison.ReferencesEneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.2009.
Knowledge-based wsd on specific domains:Performing better than generic supervised wsd.Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-baum, Shu kai Hsieh, Maurizio Tesconi, Mon-ica Monachini, Piek Vossen, and Roxanne Segers.2010.
Semeval-2010 task 17: All-words word sensedisambiguation on a specific domain.Yee Seng Chan and Hwee Tou Ng.
2007.
Domainadaptation with active learning for word sense dis-ambiguation.
In In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 49?56.Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-pak Bhattacharyya.
2010.
Domain-specific word425sense disambiguation combining corpus based andwordnet based parameters.
In 5th InternationalConference on Global Wordnet (GWC2010).Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In HLT ?05: Proceed-ings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 419?426, Morristown, NJ, USA.Association for Computational Linguistics.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of predom-inant word senses.
Comput.
Linguist., 33(4):553?590.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InHLT ?93: Proceedings of the workshop on HumanLanguage Technology, pages 303?308, Morristown,NJ, USA.
Association for Computational Linguis-tics.Hwee Tou Ng and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsenses: An exemplar-based approach.
In In Pro-ceedings of the 34th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages40?47.Marc Weeber, James G. Mork, and Alan R. Aronson.2001.
Developing a test collection for biomedicalword sense disambiguation.
In In Proceedings of theAmerican Medical Informatics Association AnnualSymposium (AMIA 2001), pages 746?750.426
