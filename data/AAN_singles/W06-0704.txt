Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 24?31,Sydney, July 2006. c?2006 Association for Computational LinguisticsSituated Question Answering in the Clinical Domain:Selecting the Best Drug Treatment for DiseasesDina Demner-Fushman1,3 and Jimmy Lin1,2,31Department of Computer Science2College of Information Studies3Institute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742, USAdemner@cs.umd.edu, jimmylin@umd.eduAbstractUnlike open-domain factoid questions,clinical information needs arise within therich context of patient treatment.
This en-vironment establishes a number of con-straints on the design of systems aimedat physicians in real-world settings.
Inthis paper, we describe a clinical ques-tion answering system that focuses on aclass of commonly-occurring questions:?What is the best drug treatment for X?
?,where X can be any disease.
To evalu-ate our system, we built a test collectionconsisting of thirty randomly-selected dis-eases from an existing secondary source.Both an automatic and a manual evalua-tion demonstrate that our system comparesfavorably to PubMed, the search systemmost commonly-used by physicians today.1 IntroductionOver the past several years, question answering(QA) has emerged as a general framework for ad-dressing users?
information needs.
Instead of re-turning ?hits?, as information retrieval systems do,QA systems respond to natural language questionswith concise, targeted information.
Recently, re-search focus has shifted away from so-called fac-toid questions such as ?What are pennies madeof??
and ?What country is Aswan High Dam lo-cated in??
to more complex questions such as?How have South American drug cartels been us-ing banks in Liechtenstein to launder money??
and?What was the Pentagon panel?s position with re-spect to the dispute over the US Navy trainingrange on the island of Vieques??
?so-called ?re-lationship?
and ?opinion?
questions, respectively.These complex information needs differ fromfactoid questions in many important ways.
Un-like factoids, they cannot be answered by named-entities and other short noun phrases.
They do notoccur in isolation, but are rather embedded withina broader context, i.e., a ?scenario?.
These com-plex questions set forth parameters of the desiredknowledge, which may include additional factsabout the motivation of the information seeker,her assumptions, her current state of knowledge,etc.
Presently, most systems that attempt to tacklesuch complex questions are aimed at serving in-telligence analysts, for activities such as counter-terrorism and war-fighting.Systems for addressing complex informationneeds are interesting because they provide an op-portunity to explore the role of semantic struc-tures in question answering, e.g., (Narayanan andHarabagiu, 2004).
Opportunities include explicitsemantic representations for capturing the con-tent of questions and documents, deep inferentialmechanisms (Moldovan et al, 2002), and attemptsto model task-specific influences in information-seeking environments (Freund et al, 2005).Our own interest in question answering fallsin line with these recent developments, but wefocus on a different type of user?the primarycare physician.
The need to answer questions re-lated to patient care at the point of service hasbeen well studied and documented (Gorman etal., 1994; Ely et al, 1999; Ely et al, 2005).However, research has shown that existing searchsystems, e.g., PubMed, are often unable to sup-ply clinically-relevant answers in a timely man-ner (Gorman et al, 1994; Chambliss and Conley,1996).
Clinical question answering represents ahigh-impact application that has the potential toimprove the quality of medical care.24From a research perspective, the clinical do-main is attractive because substantial medicalknowledge has already been codified in the Uni-fied Medical Language System (UMLS) (Lind-berg et al, 1993).
This large ontology en-ables us to explore knowledge-rich techniques andmove beyond question answering methods primar-ily driven by keyword matching.
In this work, wedescribe a paradigm of medical practice known asevidence-based medicine and explain how it canbe computationally captured in a semantic domainmodel.
Two separate evaluations demonstrate thatsemantic modeling yields gains in question an-swering performance.2 Considerations for Clinical QAWe begin our exploration of clinical question an-swering by first discussing design constraints im-posed by the domain and the information-seekingenvironment.
The practice of evidence-basedmedicine (EBM) provides a well-defined processmodel for situating our system.
EBM is a widely-accepted paradigm for medical practice that in-volves the explicit use of current best evidence,i.e., high-quality patient-centered clinical researchreported in the primary medical literature, to makedecisions about patient care.
As shown by pre-vious work (De Groote and Dorsch, 2003), cita-tions from the MEDLINE database maintained bythe National Library of Medicine serve as a goodsource of evidence.Thus, we conceive of clinical question answer-ing systems as fulfilling a decision-support roleby retrieving highly-relevant MEDLINE abstractsin response to a clinical question.
This repre-sents a departure from previous systems, which fo-cus on extracting short text segments from largersources.
The implications of making potentiallylife-altering decisions mean that all evidence mustbe carefully examined in context.
For example, theefficacy of a drug in treating a disease is alwaysframed in the context of a specific study on a sam-ple population, over a set duration, at some fixeddosage, etc.
The physician simply cannot recom-mend a particular course of action without consid-ering all these complex factors.
Thus, an ?answer?without adequate support is not useful.
Given thata MEDLINE abstract?on the order of 250 words,equivalent to a long paragraph?generally encap-sulates the context of a clinical study, it serves as alogical answer unit and an entry point to the infor-mation necessary to answer the physician?s ques-tion (e.g., via drill-down to full text articles).In order for a clinical QA system to be success-ful, it must be suitably integrated into the daily ac-tivities of a physician.
Within a clinic or a hos-pital setting, the traditional desktop application isnot the most ideal interface for a retrieval system.In most cases, decisions about patient care mustbe made by the bedside.
Thus, a PDA is an idealvehicle for delivering question answering capabil-ities (Hauser et al, 2004).
However, the form fac-tor and small screen size of such devices placesconstraints on system design.
In particular, sincethe physician is unable to view large amounts oftext, precision is of utmost importance.In summary, this section outlines considerationsfor question answering in the clinical domain: thenecessity of contextualized answers, the rationalefor adopting MEDLINE abstract as the responseunit, and the importance of high precision.3 EBM and Clinical QAEvidence-based medicine not only supplies a pro-cess model for situating question answering capa-bilities, but also provides a framework for codify-ing the knowledge involved in retrieving answers.This section describes how the EBM paradigmprovides the basis of the semantic domain modelfor our question answering system.Evidence-based medicine offers three facets ofthe clinical domain, that, when taken together,describe a model for addressing complex clini-cal information needs.
The first facet, shown inTable 1 (left column), describes the four maintasks that physicians engage in.
The secondfacet pertains to the structure of a well-built clin-ical question.
Richardson et al (1995) identifyfour key elements, as shown in Table 1 (middlecolumn).
These four elements are often refer-enced with a mnemonic PICO, which stands forPatient/Problem, Intervention, Comparison, andOutcome.
Finally, the third facet serves as a toolfor appraising the strength of evidence, i.e., howmuch confidence should a physician have in theresults?
For this work, we adopted a system withthree levels of recommendations, as shown in Ta-ble 1 (right column).By integrating these three perspectives ofevidence-based medicine, we conceptualize clin-ical question answering as ?semantic unifica-tion?
between information needs expressed in a25Clinical Tasks PICO Elements Strength of EvidenceTherapy: Selecting effective treat-ments for patients, taking into accountother factors such as risk and cost.Diagnosis: Selecting and interpret-ing diagnostic tests, while consideringtheir precision, accuracy, acceptabil-ity, cost, and safety.Prognosis: Estimating the patient?slikely course with time and anticipat-ing likely complications.Etiology: Identifying the causes for apatient?s disease.Patient/Problem: What is the pri-mary problem or disease?
What arethe characteristics of the patient (e.g.,age, gender, co-existing conditions,etc.
)?Intervention: What is the main inter-vention (e.g., diagnostic test, medica-tion, therapeutic procedure, etc.
)?Comparison: What is the main in-tervention compared to (e.g., no inter-vention, another drug, another thera-peutic procedure, a placebo, etc.
)?Outcome: What is the effect of theintervention (e.g., symptoms relievedor eliminated, cost reduced, etc.
)?A-level evidence is based on con-sistent, good quality patient-orientedevidence presented in systematic re-views, randomized controlled clini-cal trials, cohort studies, and meta-analyses.B-level evidence is inconsistent, lim-ited quality patient-oriented evidencein the same types of studies.C-level evidence is based on disease-oriented evidence or studies less rigor-ous than randomized controlled clin-ical trials, cohort studies, systematicreviews and meta-analyses.Table 1: The three facets of evidence-based medicine.PICO-based knowledge structure and correspond-ing structures extracted fromMEDLINE abstracts.Naturally, this matching process should be sensi-tive to the clinical task and the strength of evidenceof the retrieved abstracts.
As conceived, clini-cal question answering is a knowledge-intensiveendeavor that requires automatic identification ofPICO elements from MEDLINE abstracts.Ideally, a clinical question answering systemshould be capable of directly performing thissemantic match on abstracts, but the size ofthe MEDLINE database (over 16 million ci-tations) makes this approach currently unfeasi-ble.
As an alternative, we rely on PubMed,1a boolean search engine provided by the Na-tional Library of Medicine, to retrieve an initialset of results that we then postprocess in greaterdetail?this is the standard two-stage architecturecommonly-employed by many question answer-ing systems (Hirschman and Gaizauskas, 2001).The complete architecture of our system isshown in Figure 1.
The query formulation mod-ule converts the clinical question into a PubMedsearch query, identifies the clinical task, and ex-tracts the appropriate PICO elements.
PubMed re-turns an initial list of MEDLINE citations, whichis analyzed by the knowledge extractor to identifyclinically-relevant elements.
These elements serveas input to the semantic matcher, and are com-pared to corresponding elements extracted fromthe question.
Citations are then scored and the topranking ones are returned as answers.1http://www.ncbi.nih.gov/entrez/Figure 1: Architecture of our clinical question an-swering system.Although we have outlined a general frameworkfor clinical question answering, the space of allpossible patient care questions is immense, and at-tempts to develop a comprehensive system is be-yond the scope of this paper.
Instead, we focus ona subset of therapy questions: specifically, ques-tions of the form ?What is the best drug treatmentfor X?
?, where X can be any disease.
We have cho-sen to tackle this class of questions because studiesof physicians?
question-asking behavior in naturalsettings have revealed that this question type oc-curs frequently (Ely et al, 1999).
By leveragingthe natural distribution of clinical questions, wecan make the greatest impact with the least amount26of development effort.
For this class of questions,we have implemented a working system with thearchitecture described in Figure 1.
The next threesections detail each module.4 Query FormulatorSince our system only handles one question type,the query formulator is relatively simple: the taskis known in advance to be therapy and the Prob-lem PICO element is the disease asked about in theclinical question.
In order to facilitate the semanticmatching process, we employMetaMap (Aronson,2001) to identify the concept in the UMLS ontol-ogy that corresponds to the disease; UMLS alsoprovides alternative names and other expansions.The query formulator also generates a queryto PubMed, the National Library of Medicine?sboolean search engine for MEDLINE.
As an ex-ample, the following query is issued to retrieve hitsfor the disease ?meningitis?
:(Meningitis[mh:noexp]) AND drug therapy[sh]AND hasabstract[text] AND Clinical Trial[pt]AND English[Lang] AND humans[mh] AND(1900[PDAT] : 2003/03[PDAT])In order to get the best possible set of initial ci-tations, we employ MeSH (Medical Subject Head-ings) terms when available.
MeSH terms are con-trolled vocabulary concepts assigned manually bytrained medical librarians in the indexing process(based on the full text of the article), and encodea substantial amount of knowledge about the con-tents of the citation.
PubMed allows searches onMeSH headings, which usually yield highly accu-rate results.
In addition, we limit retrieved cita-tions to those that have the MeSH heading ?drugtherapy?and those that describe a clinical trial (an-other metadata field).
By default, PubMed orderscitations chronologically in reverse.5 Knowledge ExtractorThe knowledge extraction module provides thebasic frame elements used in the semanticmatching process, described in the next sec-tion.
We employ previously-implemented com-ponents (Demner-Fushman and Lin, 2005) thatidentify PICO elements within a MEDLINE cita-tion using a combination of knowledge-based andstatistical machine-learning techniques.
Of thefour PICO elements prescribed by evidence-basedmedicine practitioners, only the Problem and Out-come elements are relevant for this application(there are no Interventions and Comparisons forour question type).
The Problem is the main dis-ease under consideration in an abstract, and out-comes are statements that assert clinical findings,e.g., efficacy of a drug or a comparison betweentwo drugs.
The ability to precisely identify theseclinically-relevant elements provides the founda-tion for semantic question answering capabilities.6 Semantic MatcherEvidence-based medicine identifies three differ-ent sets of factors that must be taken into accountwhen assessing citation relevance.
These consid-erations are computationally operationalized in thesemantic matcher, which takes as input elementsidentified by the knowledge extractor and scoresthe relevance of each PubMed citation with re-spect to the question.
After matching, the top-scoring abstracts are presented to the physician asanswers.
The individual score of a citation is com-prised of three components:SEBM = SPICO + SSoE + SMeSH (1)By codifying the principles of evidence-basedmedicine, our semantic matcher attempts to sat-isfy information needs through conceptual analy-sis, as opposed to simple keyword matching.
Inthe following subsections, we describe each ofthese components in detail.6.1 PICO MatchingThe score of an abstract based on PICO elements,SPICO, is broken up into two separate scores:SPICO = Sproblem + Soutcome (2)The first component in the above equation,Sproblem, reflects a match between the primary prob-lem in the query frame and the primary problemidentified in the abstract.
A score of 1 is given ifthe problems match exactly, based on their uniqueUMLS concept id (as provided by MetaMap).Matching based on concept ids addresses the issueof terminological variation.
Failing an exact matchof concept ids, a partial string match is given ascore of 0.5.
If the primary problem in the queryhas no overlap with the primary problem from theabstract, a score of ?1 is given.The outcome-based score Soutcome is the value as-signed to the highest-scoring outcome sentence,27as determined by the knowledge extractor.
Sincethe desired outcome (i.e., improve the patient?scondition) is implicit in the clinical question, oursystem only considers the inherent quality of out-come statements in the abstract.
Given a match onthe primary problem, most clinical outcomes arelikely to be of interest to the physician.For the drug treatment scenario, there is no in-tervention or comparison, and so these elementsdo not contribute to the semantic matching.6.2 Strength of EvidenceThe relevance score of a citation based on thestrength of evidence is calculated as follows:SSoE = Sjournal + Sstudy + Sdate (3)Citations published in core and high-impactjournals such as Journal of the American MedicalAssociation (JAMA) get a score of 0.6 for Sjournal,and 0 otherwise.
In terms of the study type, Sstudy,clinical trials receive a score of 0.5; observationalstudies, 0.3; all non-clinical publications, ?1.5;and 0 otherwise.
The study type is directly en-coded as metadata in a MEDLINE citation.Finally, recency factors into the strength of evi-dence score according to the formula below:Sdate = (yearpublication ?
yearcurrent)/100 (4)A mild penalty decreases the score of a citationproportionally to the time difference between thedate of the search and the date of publication.6.3 MeSH MatchingThe final component of the EBM score reflectstask-specific considerations, and is computed fromMeSH terms associated with each citation:SMeSH =?t?MeSH?
(t) (5)The function ?
(t) maps MeSH terms to positivescores for positive indicators, negative scores fornegative indicators, or zero otherwise.Negative indicators include MeSH headings as-sociated with genomics, such as ?genetics?
and?cell physiology?.
Positive indicators for therapywere derived from the clinical query filters used inPubMed searches (Haynes et al, 1994); examplesinclude ?drug administration routes?
and any of itschildren in the MeSH hierarchy.
A score of ?1 isgiven if theMeSH descriptor or qualifier is markedas the main theme of the article (indicated via thestar notation by indexers), and ?0.5 otherwise.7 Evaluation MethodologyClinical Evidence (CE) is a periodic report cre-ated by the British Medical Journal (BMJ) Pub-lishing Group that summarizes the best treatmentsfor a few dozen diseases at the time of publica-tion.
We were able to mine the June 2004 editionto create a test collection to evaluate our system.Note that the existence of such secondary sourcesdoes not obviate the need for clinical question an-swering because they are perpetually falling out ofdate due to rapid advances in medicine.
Further-more, such reports are currently created by highly-experienced physicians, which is an expensive andtime-consuming process.
From CE, we randomlyextracted thirty diseases, creating a developmentset of five questions and a test set of twenty-fivequestions.
Some examples include: acute asthma,chronic prostatitis, community acquired pneumo-nia, and erectile dysfunction.We conducted two evaluations?one auto-matic and one manual?that compare the origi-nal PubMed hits and the output of our semanticmatcher.
The first evaluation is based on ROUGE,a commonly-used summarization metric that com-putes the unigram overlap between a particulartext and one or more reference texts.2 The treat-ment overview for each disease in CE is accompa-nied by a number of citations (used in writing theoverview itself)?the abstract texts of these citedarticles serve as our references.
We adopt this ap-proach because medical journals require abstractsthat provide factual information summarizing themain points of the studies.
We assume that thecloser an abstract is to these reference abstracts (asmeasured by ROUGE-1 precision), the more rele-vant it is.
On average, each disease overview con-tains 48.4 citations; however, we were only ableto gather abstracts of those that were contained inMEDLINE (34.7 citations per disease, min 8, max100).
For evaluation purposes, we restricted ab-stracts under consideration to those that were pub-lished before our edition of CE.
To quantify theperformance of our system, we computed the av-erage ROUGE score over the top one, three, five,and ten hits of our EBM and baseline systems.To supplement our automatic evaluation, wealso conducted a double-blind manual evaluation2We ran ROUGE-1.5.5 with DUC 2005 settings.28PubMed EBM PICO SoE MeSH1 0.160 0.205 (+27.7%)M 0.186 (+16.1%)?
0.192 (+20.0%)?
0.166 (+3.6%)?3 0.162 0.202 (+24.6%)N 0.192 (+18.0%)N 0.204 (+25.5%)N 0.172 (+6.1%)?5 0.166 0.198 (+19.5%)N 0.196 (+18.0%)N 0.201 (+21.3%)N 0.168 (+1.2%)?10 0.170 0.196 (+15.5%)N 0.191 (+12.5%)N 0.195 (+15.1%)N 0.174 (+2.8%)?Table 2: Results of automatic evaluation: average ROUGE score using cited abstracts in CE as references.The EBM column represents performance of our complete domain model.
PICO, SoE, and MeSH rep-resent performance of each component.
(?
denotes n.s., M denotes sig.
at 0.95, N denotes sig.
at 0.99)PubMed results EBM-reranked resultsEffect of vitamin A supplementation on childhood morbid-ity and mortality.Intrathecal chemotherapy in carcinomatous meningitis frombreast cancer.Isolated leptomeningeal carcinomatosis (carcinomatousmeningitis) after taxane-induced major remission in patientswith advanced breast cancer.A comparison of ceftriaxone and cefuroxime for the treat-ment of bacterial meningitis in children.Randomised comparison of chloramphenicol, ampicillin,cefotaxime, and ceftriaxone for childhood bacterial menin-gitis.The beneficial effects of early dexamethasone administra-tion in infants and children with bacterial meningitis.Table 3: Titles of the top abstracts retrieved in response to the question ?What is the best treatment formeningitis?
?, before and after applying our semantic reranking algorithm.of the system.
The top five citations from boththe original PubMed results and the output of oursemantic matcher were gathered, blinded, and ran-domized (see Table 3 for an example of top resultsobtained by PubMed and our system).
The firstauthor of this paper, who is a medical doctor, man-ually evaluated the abstracts.
Since the sources ofthe abstracts were hidden, judgments were guar-anteed to be impartial.
All abstracts were evalu-ated on a four point scale: not relevant, marginallyrelevant, relevant, and highly relevant, which cor-responds to a score of zero to three.8 ResultsThe results of our automatic evaluation are shownin Table 2: the rows show average ROUGE scoresat one, three, five, and ten hits, respectively.
Inaddition to the PubMed baseline and our com-plete EBM model, we conducted a component-level analysis of our semantic matching algorithm.Three separate ablation studies isolate the effectsof the PICO-based score, the strength of evi-dence score, and the MeSH-based score (columns?PICO?, ?SoE?, and ?MeSH?
).At all document cutoffs, the quality of theEBM-reranked hits is higher than that of the origi-nal PubMed hits, as measured by ROUGE.
The dif-ferences are statistically significant, according tothe Wilcoxon signed-rank test, the standard non-parametric test employed in IR.Based on the component analysis, we can seethat the strength of evidence score is responsi-ble for the largest performance gain, althoughthe combination of all three components outper-forms each one individually (for the most part).All three components of our semantic model con-tribute to the overall QA performance, which isexpected because clinical relevance is a multi-faceted property that requires a multitude of con-siderations.
Evidence-based medicine provides atheory of these factors, and we have shown that aquestion answering algorithm which operational-izes EBM yields good results.The distribution of human judgments from ourmanual evaluation is shown in Figure 2.
Forthe development set, the average human judg-ment of the original PubMed hits is 1.52 (be-tween ?marginally relevant?
and ?relevant?
); aftersemantic matching, 2.32 (better than ?relevant?
).For the test set, the averages are 1.49 before rank-ing and 2.10 after semantic matching.
These re-sults show that our system performs significantlybetter than the PubMed baseline.The performance improvement observed in ourexperiments is encouraging, considering that wewere starting off with a strong state-of-the-art29Figure 2: Results of our manual evaluation: distribution of judgments, for development set (left) and testset (right).
(0=not relevant, 1=marginally relevant, 2=relevant, 3=highly relevant)PubMed baseline that leverages MeSH terms.
Allinitial citations retrieved by PubMed were clinicaltrials and ?about?
the disease in question, as deter-mined by human indexers.
Our work demonstratesthat principles of evidence-based medicine can becodified in an algorithm.Since a number of abstracts were both auto-matically evaluated with ROUGE and manuallyassessed, it is possible to determine the degreeto which automatic metrics predict human judg-ments.
For the 125 human judgments gatheredon the test set, we computed a Pearson?s r scoreof 0.544, which indicates moderate predictiveness.Due to the structure of our PubMed query, the key-word content of retrieved abstracts are relativelyhomogeneous.
Nevertheless, automatic evaluationwith ROUGE appears to be useful.9 Discussion and Related WorkRecently, researchers have become interestedin restricted-domain question answering becauseit provides an opportunity to explore the useof knowledge-rich techniques without havingto tackle the commonsense reasoning problem.Knowledge-based techniques dependent on richsemantic representations contrast with TREC-style factoid question answering, which is primar-ily driven by keyword matching and named-entitydetection.Our work represents a successful case study ofhow semantic models can be employed to capturedomain knowledge (the practice of medicine, inour case).
The conception of question answer-ing as the matching of knowledge frames providesus with an opportunity to experiment with seman-tic representations that capture the content of bothdocuments and information needs.
In our case,PICO-based scores were found to have a positiveimpact on performance.
The strength of evidenceand the MeSH-based scores represent attempts tomodel user requirements by leveraging meta-levelinformation not directly present in either questionsor candidate answers.
Both contribute positivelyto performance.
Overall, the construction of oursemantic model is enabled by the UMLS ontol-ogy, which provides an enumeration of relevantconcepts (e.g., the names of diseases, drugs, etc.
)and semantic relations between those concepts.Question answering in the clinical domain is anemerging area of research that has only recentlybegun to receive serious attention.
As a result,there exist relatively few points of comparison toour own work, as the research space is sparselypopulated.The idea that information systems shouldbe sensitive to the practice of evidence-basedmedicine is not new.
Many researchers have stud-ied MeSH terms associated with basic clinicaltasks (Mendonc?a and Cimino, 2001; Haynes et al,1994).
Although originally developed as a tool toassist in query formulation, Booth (2000) pointedout that PICO frames can be employed to struc-ture IR results for improving precision; PICO-based querying is merely an instance of facetedquerying, which has been widely used by librari-ans since the invention of automated retrieval sys-tems.
The feasibility of automatically identifyingoutcome statements in secondary sources has beendemonstrated by Niu and Hirst (2004), but ourwork differs in its focus on the primary medical lit-erature.
Approaching clinical needs from a differ-ent perspective, the PERSIVAL system leveragespatient records to rerank search results (McKeownet al, 2003).
Since the primary focus is on person-30alization, this work can be viewed as complemen-tary to our own.The dearth of related work and the lack of a pre-existing clinical test collection to a large extent ex-plains the ad hoc nature of some aspects of oursemantic matching algorithm.
All weights wereheuristically chosen to reflect our understandingof the domain, and were not optimized in a prin-cipled manner.
Nevertheless, performance gainsobserved in the development set carried over tothe blind held-out test collection, providing con-fidence in the generality of our methods.
Devel-oping a more formal scoring model for evidence-based medicine will be the subject of future work.10 ConclusionWe see this work as having two separate contribu-tions.
From the viewpoint of computational lin-guistics, we have demonstrated the effectivenessof a knowledge-rich approach to QA based onmatching questions with answers at the semanticlevel.
From the viewpoint of medical informat-ics, we have shown how principles of evidence-based medicine can be operationalized in a sys-tem to support physicians.
We hope that this workpaves the way for future high-impact applications.11 AcknowledgmentsThis work was supported in part by the NationalLibrary of Medicine.
The second author wishes tothank Esther and Kiri for their loving support.ReferencesA.
Aronson.
2001.
Effective mapping of biomedi-cal text to the UMLS Metathesaurus: The MetaMapprogram.
In Proceeding of the AMIA 2001.A.
Booth.
2000.
Formulating the question.
InA.
Booth and G. Walton, editors, Managing Knowl-edge in Health Services.
Facet Publishing.M.
Chambliss and J. Conley.
1996.
Answering clinicalquestions.
The Journal of Family Practice, 43:140?144.S.
De Groote and J. Dorsch.
2003.
Measuring usepatterns of online journals and databases.
Journalof the Medical Library Association, 91(2):231?240,April.D.
Demner-Fushman and J. Lin.
2005.
Knowledge ex-traction for clinical question answering: Preliminaryresults.
In Proceedings of the AAAI-05 Workshop onQuestion Answering in Restricted Domains.J.
Ely, J. Osheroff, M. Ebell, G. Bergus, B. Levy,M.
Chambliss, and E. Evans.
1999.
Analysis ofquestions asked by family doctors regarding patientcare.
BMJ, 319:358?361.J.
Ely, J. Osheroff, M. Chambliss, M. Ebell, andM.
Rosenbaum.
2005.
Answering physicians?
clin-ical questions: Obstacles and potential solutions.Journal of the American Medical Informatics Asso-ciation, 12(2):217?224, March-April.L.
Freund, E. Toms, and C. Clarke.
2005.
Modelingtask-genre relationships for IR in the Workplace.
InProceedings of SIGIR 2005.P.
Gorman, J. Ash, and L. Wykoff.
1994.
Can pri-mary care physicians?
questions be answered usingthe medical journal literature?
Bulletin of the Medi-cal Library Association, 82(2):140?146, April.S.
Hauser, D. Demner-Fushman, G. Ford, andG.
Thoma.
2004.
PubMed on Tap: Discoveringdesign principles for online information delivery tohandheld computers.
In Proceedings of MEDINFO2004.R.
Haynes, N. Wilczynski, K. McKibbon, C. Walker,and J. Sinclair.
1994.
Developing optimal searchstrategies for detecting clinically sound studies inMEDLINE.
Journal of the American Medical In-formatics Association, 1(6):447?458.L.
Hirschman and R. Gaizauskas.
2001.
Naturallanguage question answering: The view from here.Natural Language Engineering, 7(4):275?300.D.
Lindberg, B. Humphreys, and A. McCray.
1993.The Unified Medical Language System.
Methods ofInformation in Medicine, 32(4):281?291, August.K.
McKeown, N. Elhadad, and V. Hatzivassiloglou.2003.
Leveraging a common representation for per-sonalized search and summarization in a medicaldigital library.
In Proceedings JCDL 2003.E.
Mendonc?a and J. Cimino.
2001.
Building a knowl-edge base to support a digital library.
In Proceedingsof MEDINFO 2001.D.
Moldovan, M. Pas?ca, S. Harabagiu, and M. Sur-deanu.
2002.
Performance issues and error analysisin an open-domain question answering system.
InProceedings of ACL 2002.S.
Narayanan and S. Harabagiu.
2004.
Question an-swering based on semantic structures.
In Proceed-ings of COLING 2004.Y.
Niu and G. Hirst.
2004.
Analysis of semanticclasses in medical text for question answering.
InProceedings of the ACL 2004 Workshop on QuestionAnswering in Restricted Domains.W.
Richardson, M. Wilson, J. Nishikawa, and R. Hay-ward.
1995.
The well-built clinical question: Akey to evidence-based decisions.
American Col-lege of Physicians Journal Club, 123(3):A12?A13,November-December.31
