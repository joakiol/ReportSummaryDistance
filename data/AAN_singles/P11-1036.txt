Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 350?358,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsContent Models with AttitudeChristina Sauper, Aria Haghighi, Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technologycsauper@csail.mit.edu, me@aria42.com, regina@csail.mit.eduAbstractWe present a probabilistic topic model forjointly identifying properties and attributes ofsocial media review snippets.
Our modelsimultaneously learns a set of properties ofa product and captures aggregate user senti-ments towards these properties.
This approachdirectly enables discovery of highly rated orinconsistent properties of a product.
Ourmodel admits an efficient variational mean-field inference algorithm which can be paral-lelized and run on large snippet collections.We evaluate our model on a large corpus ofsnippets from Yelp reviews to assess propertyand attribute prediction.
We demonstrate thatit outperforms applicable baselines by a con-siderable margin.1 IntroductionOnline product reviews have become an increasinglyvaluable and influential source of information forconsumers.
Different reviewers may choose to com-ment on different properties or aspects of a product;therefore their reviews focus on different qualities ofthe product.
Even when they discuss the same prop-erties, their experiences and, subsequently, evalua-tions of the product can differ dramatically.
Thus,information in any single review may not providea complete and balanced view representative of theproduct as a whole.
To address this need, online re-tailers often use simple aggregation mechanisms torepresent the spectrum of user sentiment.
For in-stance, product pages on Amazon prominently dis-play the distribution of numerical scores across re-Coherent property cluster+The martinis were very good.The drinks - both wine and martinis - were tasty.-The wine list was pricey.Their wine selection is horrible.Incoherent property cluster+The sushi is the best I?ve ever had.Best paella I?d ever had.The fillet was the best steak we?d ever had.It?s the best soup I?ve ever had.Table 1: Example clusters of restaurant review snippets.The first cluster represents a coherent property of the un-derlying product, namely the cocktail property, and as-sesses distinctions in user sentiment.
The latter clustersimply shares a common attribute expression and doesnot represent snippets discussing the same product prop-erty.
In this work, we aim to produce the first type ofproperty cluster with correct sentiment labeling.views, providing access to reviews at different levelsof satisfaction.The goal of our work is to provide a mechanismfor review content aggregation that goes beyond nu-merical scores.
Specifically, we are interested inidentifying fine-grained product properties acrossreviews (e.g., battery life for electronics or pizza forrestaurants) as well as capturing attributes of theseproperties, namely aggregate user sentiment.For this task, we assume as input a set of prod-uct review snippets (i.e., standalone phrases such as?battery life is the best I?ve found?)
rather than com-plete reviews.
There are many techniques for ex-tracting this type of snippet in existing work; we usethe Sauper et al (2010) system.350At first glance, this task can be solved using ex-isting methods for review analysis.
These methodscan effectively extract product properties from indi-vidual snippets along with their corresponding sen-timent.
While the resulting property-attribute pairsform a useful abstraction for cross-review analysis,in practice direct comparison of these pairs is chal-lenging.Consider, for instance, the two clusters of restau-rant review snippets shown in Figure 1.
While bothclusters have many words in common among theirmembers, only the first describes a coherent prop-erty cluster, namely the cocktail property.
The snip-pets of the latter cluster do not discuss a single prod-uct property, but instead share similar expressionsof sentiment.
To solve this issue, we need a methodwhich can correctly identify both property and sen-timent words.In this work, we propose an approach that jointlyanalyzes the whole collection of product reviewsnippets, induces a set of learned properties, andmodels the aggregate user sentiment towards theseproperties.
We capture this idea using a Bayesiantopic model where a set of properties and corre-sponding attribute tendencies are represented as hid-den variables.
The model takes product review snip-pets as input and explains how the observed textarises from the latent variables, thereby connectingtext fragments with corresponding properties and at-tributes.The advantages of this formulation are twofold.First, this encoding provides a common ground forcomparing and aggregating review content in thepresence of varied lexical realizations.
For instance,this representation allows us to directly comparehow many reviewers liked a given property of aproduct.
Second, our model yields an efficientmean-field variational inference procedure whichcan be parallelized and run on a large number of re-view snippets.We evaluate our approach in the domain of snip-pets taken from restaurant reviews on Yelp.
In thiscollection, each restaurant has on average 29.8 snip-pets representing a wide spectrum of opinions abouta restaurant.
The evaluation we present demon-strates that the model can accurately retrieve clustersof review fragments that describe the same property,yielding 20% error reduction over a standalone clus-tering baseline.
We also show that the model can ef-fectively identify binary snippet attributes with 9.2%error reduction over applicable baselines, demon-strating that learning to identify attributes in the con-text of other product reviews yields significant gains.Finally, we evaluate our model on its ability to iden-tify product properties for which there is significantsentiment disagreement amongst user snippets.
Thistests our model?s capacity to jointly identify proper-ties and assess attributes.2 Related WorkOur work on review aggregation has connections tothree lines of work in text analysis.First, our work relates to research on extraction ofproduct properties with associated sentiment fromreview text (Hu and Liu, 2004; Liu et al, 2005a;Popescu et al, 2005).
These methods identify rele-vant information in a document using a wide rangeof methods such as association mining (Hu and Liu,2004), relaxation labeling (Popescu et al, 2005) andsupervised learning (Kim and Hovy, 2006).
Whileour method also extracts product properties and sen-timent, our focus is on multi-review aggregation.This task introduces new challenges which werenot addressed in prior research that focused on per-document analysis.A second related line of research is multi-document review summarization.
Some ofthese methods directly apply existing domain-independent summarization methods (Seki et al,2006), while others propose new methods targetedfor opinion text (Liu et al, 2005b; Carenini et al,2006; Hu and Liu, 2006; Kim and Zhai, 2009).
Forinstance, these summaries may present contrastiveview points (Kim and Zhai, 2009) or relay averagesentiment (Carenini et al, 2006).
The focus of thisline of work is on how to select suitable sentences,assuming that relevant review features (such as nu-merical scores) are given.
Since our emphasis is onmulti-review analysis, we believe that the informa-tion we extract can benefit existing summarizationsystems.Finally, a number of approaches analyze reviewdocuments using probabilistic topic models (Lu andZhai, 2008; Titov and McDonald, 2008; Mei et al,2007).
While some of these methods focus primar-351ily on modeling ratable aspects (Titov and McDon-ald, 2008), others explicitly capture the mixture oftopics and sentiments (Mei et al, 2007).
These ap-proaches are capable of identifying latent topics inthe collection in opinion text (e.g., weblogs) as wellas associated sentiment.
While our model capturessimilar high-level intuition, it analyzes fine-grainedproperties expressed at the snippet level, rather thandocument-level sentiment.
Delivering analysis atsuch a fine granularity requires a new technique.3 Problem FormulationIn this section, we discuss the core random variablesand abstractions of our model.
We describe the gen-erative models over these elements in Section 4.Product: A product represents a reviewable ob-ject.
For the experiments in this paper, we userestaurants as products.Snippets: A snippet is a user-generated short se-quence of tokens describing a product.
Input snip-pets are deterministically taken from the output ofthe Sauper et al (2010) system.Property: A property corresponds to some fine-grained aspect of a product.
For instance, the snippet?the pad thai was great?
describes the pad thai prop-erty.
We assume that each snippet has a single prop-erty associated with it.
We assume a fixed numberof possible properties K for each product.For the corpus of restaurant reviews, we assumethat the set of properties are specific to a given prod-uct, in order to capture fine-grained, relevant proper-ties for each restaurant.
For example, reviews from asandwich shop may contrast the club sandwich withthe turkey wrap, while for a more general restau-rant, the snippets refer to sandwiches in general.
Forother domains where the properties are more consis-tent, it is straightforward to alter our model so thatproperties are shared across products.Attribute: An attribute is a description of a prop-erty.
There are multiple attribute types, which maycorrespond to semantic differences.
We assume afixed, pre-specified number of attributes N .
Forexample, in the case of product reviews, we selectN = 2 attributes corresponding to positive and neg-ative sentiment.
In the case of information extrac-tion, it may be beneficial to use numeric and alpha-betic types.One of the goals of this work in the review do-main is to improve sentiment prediction by exploit-ing correlations within a single property cluster.
Forexample, if there are already many snippets with theattribute representing positive sentiment in a givenproperty cluster, additional snippets are biased to-wards positive sentiment as well; however, data canalways override this bias.Snippets themselves are always observed; thegoal of this work is to induce the latent property andattribute underlying each snippet.4 ModelOur model generates the words of all snippets foreach product in a collection of products.
We usesi,j,w to represent the wth word of the jth snippetof the ith product.
We use s to denote the collec-tion of all snippet words.
We also assume a fixedvocabulary of words V .We present an overview of our generative modelin Figure 1 and describe each component in turn:Global Distributions: At the global level, wedraw several unigram distributions: a global back-ground distribution ?B and attribute distributions?aA for each attribute.
The background distributionis meant to encode stop-words and domain white-noise, e.g., food in the restaurants domain.
In thisdomain, the positive and negative attribute distribu-tions encode words with positive and negative senti-ments (e.g., delicious or terrible).Each of these distributions are drawn from Dirich-let priors.
The background distribution is drawnfrom a symmetric Dirichlet with concentration?B = 0.2.
The positive and negative attribute dis-tributions are initialized using seed words (Vseedain Figure 1).
These seeds are incorporated intothe attribute priors: a non-seed word gets  hyper-parameter and a seed word gets  + ?A, where = 0.25 and ?A = 1.0.Product Level: For the ith product, we drawproperty unigram distributions ?i,1P , .
.
.
, ?i,KP foreach of the possibleK product properties.
The prop-erty distribution represents product-specific contentdistributions over properties discussed in reviews ofthe product; for instance in the restaurant domains,properties may correspond to distinct menu items.Each ?i,kP is drawn from a symmetric Dirichlet prior352Global Level:- Draw background distribution ?B ?
DIRICHLET(?BV )- For each attribute type a,- Draw attribute distribution ?aA ?
DIRICHLET(V + ?AVseeda)Product Level:- For each product i,- Draw property distributions ?kP ?
DIRICHLET(?PV ) for k = 1, .
.
.
,K- Draw property attribute binomial ?i,k ?
BETA(?A, ?A) for k = 1, .
.
.
,K- Draw property multinomial ?i ?
DIRICHLET(?MK)Snippet Level:- For each snippet j in ith product,- Draw snippet property Zi,jP ?
?i- Draw snippet attribute Zi,jA ?
?ZijP- Draw sequence of word topic indicators Zi,j,wW ?
?|Zi,j,w?1W- Draw snippet word given property Zi,jP and attribute Zi,jAsi,j,w ????????
?i,Zi,jPP , when Zi,j,wW = P?Zi,jAA , when Zi,j,wW = A?B, when Zi,j,wW = B?B?aA?
?kZi?1WZiWZi+1Wwi?1wiwi+1HMM over snippet wordsBackground worddistributionAttribute worddistributionsProductSnippetZPZAPropertymultinomialProperty attributebinomials?kPProperty worddistributionsPropertySnippet attributeSnippet property?aAZP, ?PZA, ?A?BAttributeFigure 1: A high-level verbal and graphical description for our model in Section 4.
We use DIRICHLET(?V ) to denotea finite Dirichlet prior where the hyper-parameter counts are a scalar times the unit vector of vocabulary items.
Forthe global attribute distribution, the prior hyper-parameter counts are  for all vocabulary items and ?Afor Vseeda , thevector of vocabulary items in the set of seed words for attribute a.with hyper-parameter ?P = 0.2.For each property k = 1, .
.
.
,K. ?i,k, we draw abinomial distribution ?i,k.
This represents the dis-tribution over positive and negative attributes forthat property; it is drawn from a beta prior usinghyper-parameters ?A = 2 and ?A = 2.
We alsodraw a multinomial ?i over K possible propertiesfrom a symmetric Dirichlet distribution with hyper-parameter ?M = 1, 000.
This distribution is used todraw snippet properties.Snippet Level: For the jth snippet of the ith prod-uct, a property random variable Zi,jP is drawn ac-cording to the multinomial ?i.
Conditioned on thischoice, we draw an attribute Zi,jA (positive or nega-tive) from the property attribute distribution ?i,Zj,jP .Once the property Zi,jP and attribute Zi,jA havebeen selected, the tokens of the snippet are gener-ated using a simple HMM.
The latent state underly-ing a token, Zi,j,wW , indicates whether the wth wordcomes from the property distribution, attribute dis-tribution, or background distribution; we use P , A,or B to denote these respective values of Zi,j,wW .The sequence Zi,j,1W , .
.
.
, Zi,j,mW is generated us-ing a first-order Markov model.
The full transitionparameter matrix ?
parametrizes these decisions.Conditioned on the underlying Zi,j,wW , a word, si,j,wis drawn from ?i,jP , ?i,Zi,jPA , or ?B for the values P ,A,or B respectively.5 InferenceThe goal of inference is to predict the snippet prop-erty and attribute distributions over each snippetgiven all the observed snippets P (Zi,jP , Zi,jA |s) forall products i and snippets j.
Ideally, we would liketo marginalize out nuisance random variables anddistributions.
Specifically, we approximate the full353model posterior using variational inference:1P (?,?P , ?B,?A,?, |s) ?Q(?,?P , ?B,?A,?
)where?, ?P ,?
denote the collection of latent distri-butions in our model.
Here, we assume a full mean-field factorization of the variational distribution; seeFigure 2 for the decomposition.
Each variationalfactor q(?)
represents an approximation of that vari-able?s posterior given observed random variables.The variational distribution Q(?)
makes the (incor-rect) assumption that the posteriors amongst factorsare independent.
The goal of variational inference isto set factors q(?)
so that it minimizes the KL diver-gence to the true model posterior:minQ(?
)KL(P (?,?P , ?B,?A,?, |s)?Q(?,?P , ?B,?A,?
)We optimize this objective using coordinate descenton the q(?)
factors.
Concretely, we update each fac-tor by optimizing the above criterion with all otherfactors fixed to current values.
For instance, the up-date for the factor q(Zi,j,wW ) takes the form:q(Zi,j,wW )?EQ/q(Zi,j,wW )lgP (?,?P , ?B,?A,?, s)The full factorization of Q(?)
and updates forall random variable factors are given in Figure 2.Updates of parameter factors are omitted; howeverthese are derived through simple counts of the ZA,ZP , and ZW latent variables.
For related discussion,see Blei et al (2003).6 ExperimentsIn this section, we describe in detail our data set andpresent three experiments and their results.Data Set Our data set consists of snippets fromYelp reviews generated by the system described inSauper et al (2010).
This system is trained to ex-tract snippets containing short descriptions of usersentiment towards some aspect of a restaurant.2 We1See Liang and Klein (2007) for an overview of variational tech-niques.2For exact training procedures, please reference that paper.The [P noodles ] and the [P meat ] were actually [+ pretty good ].I [+ recommend ] the [P chicken noodle pho ].The [P noodles ] were [- soggy ].The [P chicken pho ] was also [+ good ].The [P spring rolls ] and [P coffee ] were [+ good ] though.The [P spring roll wrappers ] were a [- little dry tasting ].My [+ favorites ] were the [P crispy spring rolls ].The [P Crispy Tuna Spring Rolls ] are [+ fantastic ]!The [P lobster roll ] my mother ordered was [- dry ] and [- scant ].The [P portabella mushroom ] is my [+ go-to ] [P sandwich ].The [P bread ] on the [P sandwich ] was [- stale ].The slice of [P tomato ] was [- rather measly ].The [P shumai ] and [P California maki sushi ] were [+ decent ].The [P spicy tuna roll ] and [P eel roll ] were [+ perfect ].The [P rolls ] with [P spicy mayo ] were [- not so great ].I [+ love ] [P Thai rolls ].Figure 3: Example snippets from our data set, groupedaccording to property.
Property words are labeled P andcolored blue, NEGATIVE attribute words are labeled - andcolored red, and POSITIVE attribute words are labeled +and colored green.
The grouping and labeling are notgiven in the data set and must be learned by the model.select only the snippets labeled by that system as ref-erencing food, and we ignore restaurants with fewerthan 20 snippets.
There are 13,879 snippets in to-tal, taken from 328 restaurants in and around theBoston/Cambridge area.
The average snippet lengthis 7.8 words, and there are an average of 42.1 snip-pets per restaurant, although there is high variancein number of snippets for each restaurant.
Figure 3shows some example snippets.For sentiment attribute seed words, we use 42 and33 words for the positive and negative distributionsrespectively.
These are hand-selected based on therestaurant review domain; therefore, they includedomain-specific words such as delicious and gross.Tasks We perform three experiments to evaluateour model?s effectiveness.
First, a cluster predic-tion task is designed to test the quality of the learnedproperty clusters.
Second, an attribute analysis taskwill evaluate the sentiment analysis portion of themodel.
Third, we present a task designed to testwhether the system can correctly identify propertieswhich have conflicting attributes, which tests bothclustering and sentiment analysis.354Mean-field FactorizationQ(?,?P , ?B,?A,?)
= q(?B)(N?a=1q(?aA))?
?n?i(K?k=1q(?i,kP )q(?i,k))??
?jq(Zi,jA )q(Zi,jP )?wq(Zi,j,wW )???
?Snippet Property Indicatorlg q(Zi,jP = k) ?
Eq(?i) lg?i(p) +?wq(Zi,j,wW = P )Eq(?i,kP )lg ?i,kP (si,j,w) +N?a=1q(Zi,jA = a)Eq(?i,k) lg ?i,k(a)Snippet Attribute Indicatorlg q(Zi,jA = a) =?kq(Zi,jP = k)Eq(?i,k) lg ?i,k(a) +?wq(Zi,j,wW = A)Eq(?aA) lg ?aA(si,j,w)Word Topic Indicatorlg q(Zi,j,wW = P ) ?
lgP (ZW = P ) +?kq(Zi,jP = k)Eq(?i,kP )lg ?i,jP (si,j,w)lg q(Zi,j,wW = A) ?
lgP (ZW = A) +?a?{+,?
}q(Zi,jA = a)Eq(?aA) lg ?aA(si,j,w)lg q(Zi,j,wW = B) ?
lgP (ZW = B) + Eq(?B) lg ?B(si,j,w)Figure 2: The mean-field variational algorithm used during learning and inference to obtain posterior predictions oversnippet properties and attributes, as described in Section 5.
Mean-field inference consists of updating each of the latentvariable factors as well as a straightforward update of latent parameters in round robin fashion.6.1 Cluster predictionThe goal of this task is to evaluate the quality ofproperty clusters; specifically the Zi,jP variable inSection 4.
In an ideal clustering, the predicted clus-ters will be cohesive (i.e., all snippets predicted fora given property are related to each other) and com-prehensive (i.e., all snippets which are related to aproperty are predicted for it).
For example, a snip-pet will be assigned the property pad thai if and onlyif that snippet mentions some aspect of the pad thai.Annotation For this task, we use a set of goldclusters over 3,250 snippets across 75 restaurantscollected through Mechanical Turk.
In each task, aworker was given a set of 25 snippets from a singlerestaurant and asked to cluster them into as manyclusters as they desired, with the option of leavingany number unclustered.
This yields a set of goldclusters and a set of unclustered snippets.
For verifi-cation purposes, each task was provided to two dif-ferent workers.
The intersection of both workers?judgments was accepted as the gold standard, so themodel is not evaluated on judgments which disagree.In total, there were 130 unique tasks, each of whichwere provided to two workers, for a total output of210 generated clusters.Baseline The baseline for this task is a cluster-ing algorithm weighted by TF*IDF over the data setas implemented by the publicly available CLUTOpackage.3 This baseline will put a strong connec-tion between things which are lexically similar.
Be-cause our model only uses property words to tietogether clusters, it may miss correlations betweenwords which are not correctly identified as propertywords.
The baseline is allowed 10 property clustersper restaurant.We use the MUC cluster evaluation metric forthis task (Vilain et al, 1995).
This metric measuresthe number of cluster merges and splits required torecreate the gold clusters given the model?s output.3Available at http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overviewwith agglomerative clustering, using the cosine similaritydistance metric.355Precision Recall F1Baseline 80.2 61.1 69.3Our model 72.2 79.1 75.5Table 2: Results using the MUC metric on the clusterprediction task.
Note that while the precision of the base-line is higher, the recall and overall F1 of our model out-weighs that.
While MUC has a deficiency in that puttingeverything into a single cluster will artificially inflate thescore, parameters on our model are set so that the modeluses the same number of clusters as the baseline system.Therefore, it can concisely show how accurate ourclusters are as a whole.
While it would be possibleto artificially inflate the score by putting everythinginto a single cluster, the parameters on our modeland the likelihood objective are such that the modelprefers to use all available clusters, the same numberas the baseline system.Results Results for our cluster prediction task arein Table 2.
While our system does suffer on preci-sion in comparison to the baseline system, the recallgains far outweigh this loss, for a total error reduc-tion of 20% on the MUC measure.The most common cause of poor cluster choicesin the baseline system is its inability to distinguishproperty words from attribute words.
For example,if many snippets in a given restaurant use the worddelicious, there may end up being a cluster based onthat alone.
Because our system is capable of dis-tinguishing which words are property words (i.e.,words relevant to clustering), it can choose clusterswhich make more sense overall.
We show an exam-ple of this in Table 3.6.2 Attribute analysisWe also evaluate the system?s predictions of snip-pet attribute using the predicted posterior over theattribute distribution for the snippet (i.e., Zi,jA ).
Forthis task, we consider the binary judgment to be sim-ply the one with higher value in q(Zi,jA ) (see Sec-tion 5).
The goal of this task is to evaluate whetherour model correctly distinguishes attribute words.Annotation For this task, we use a set of 260 to-tal snippets from the Yelp reviews for 30 restaurants,evenly split into a training and test sets of 130 snip-pets each.
These snippets are manually labeled POS-The martini selection looked deliciousThe s?mores martini sounded excellentThe martinis were goodThe martinis are very goodThe mozzarella was very freshThe fish and various meets were very well madeThe best carrot cake I?ve ever eatenCarrot cake was deliciously moistThe carrot cake was delicious.It was rich, creamy and delicious.The pasta Bolognese was rich and robust.Table 3: Example phrases from clusters in both the base-line and our model.
For each pair of clusters, the dashedline indicates separation by the baseline model, while thesolid line indicates separation by our model.
In the firstexample, the baseline mistakenly clusters some snippetsabout martinis with those containing the word very.
Inthe second example, the same occurs with the word deli-cious.ITIVE or NEGATIVE.
Neutral snippets are ignoredfor the purpose of this experiment.Baseline We use two baselines for this task, onebased on a standard discriminative classifier and onebased on the seed words from our model.The DISCRIMINATIVE baseline for this task isa standard maximum entropy discriminative bi-nary classifier over unigrams.
Given enough snip-pets from enough unrelated properties, the classifiershould be able to identify that words like great in-dicate positive sentiment and those like bad indi-cate negative sentiment, while words like chickenare neutral and have no effect.The SEED baseline simply counts the number ofwords from the positive and negative seed lists usedby the model, Vseed+ and Vseed?
.
If there are morewords from Vseed+ , the snippet is labeled positive,and if there are more words from Vseed?
, the snip-pet is labeled negative.
If there is a tie or there areno seed words, we split the prediction.
Becausethe seed word lists are specifically slanted towardrestaurant reviews (i.e., they contain words such asdelicious), this baseline should perform well.Results For this experiment, we measure the over-all classification accuracy of each system (see Table356AccuracyDISCRIMINATIVE baseline 75.9SEED baseline 78.2Our model 80.2Table 4: Attribute prediction accuracy of the full systemcompared to the DISCRIMINATIVE and SEED baselines.The advantage of our system is its ability to distinguishproperty words from attribute words in order to restrictjudgment to only the relevant terms.The naan was hot and freshAll the veggies were really fresh and crisp.Perfect mix of fresh flavors and comfort foodThe lo main smelled and tasted rancidMy grilled cheese sandwich was a little grossTable 5: Examples of sentences correctly labeled by oursystem but incorrectly labeled by the DISCRIMINATIVEbaseline; the key sentiment words are highlighted.
No-tice that these words are not the most common sentimentwords; therefore, it is difficult for the classifier to make acorrect generalization.
Only two of these words are seedwords for our model (fresh and gross).4).
Our system outperforms both supervised base-lines.As in the cluster prediction case, the main flawwith the DISCRIMINATIVE baseline system is its in-ability to recognize which words are relevant for thetask at hand, in this case the attribute words.
Bylearning to separate attribute words from the otherwords in the snippets, our full system is able to moreaccurately judge their sentiment.
Examples of thesecases are found in Table 5.The obvious flaw in the SEED baseline is the in-ability to pre-specify every possible sentiment word;our model?s performance indicates that it is learningsomething beyond just these basic words.6.3 Conflict identificationOur final task requires both correct cluster predictionand correct sentiment judgments.
In many domains,it is interesting to know not only whether a productis rated highly, but also whether there is conflictingsentiment or debate.
In the case of restaurant re-views, it is relevant to know whether the dishes areconsistently good or whether there is some variationin quality.JudgmentP A Attribute / SnippetYes Yes- The salsa isn?t great+ Chips and salsa are sublime- The grits were good, but not great.+ Grits were the perfect consistency- The tom yum kha was bland+ It?s the best Thai soup I ever had- The naan is a bit doughy and undercooked+ The naan was pretty tasty- My reuben was a little dry.+ The reuben was a good reuben.Yes No- Belgian frites are crave-able+ The frites are very, very good.No Yes- The blackened chicken was meh+ Chicken enchiladas are yummy!- The taste overall was mediocre+ The oysters are tremendousNo No - The cream cheese wasn?t bad+ Ice cream was just deliciousTable 6: Example property-attribute correctness for theconflict identification task, over both property and at-tribute.
Property judgment (P) indicates whether the snip-pets are discussing the same item; attribute judgment (A)indicates whether there is a correct difference in attribute(sentiment), regardless of properties.To evaluate this, we examine the output clusterswhich contain predictions of both positive and neg-ative snippets.
The goal is to identify whether theseare true conflicts of sentiment or there was a failurein either property clustering or attribute classifica-tion.For this task, the output clusters are manually an-notated for correctness of both property and attributejudgments, as in Table 6.
As there is no obviousbaseline for this experiment, we treat it simply as ananalysis of errors.Results For this task, we examine the accuracy ofconflict prediction, both with and without the cor-rectly identified properties.
The results by property-attribute correctness are shown in Table 7.
Fromthese numbers, we can see that 50% of the clustersare correct in both property (cohesiveness) and at-tribute (difference in sentiment) dimensions.Overall, the properties are correctly identified(subject of NEG matches the subject of POS) 68%of the time and a correct difference in attribute isidentified 67% of the time.
Of the clusters whichare correct in property, 74% show a correctly labeled357JudgmentP A # ClustersYes Yes 52Yes No 18No Yes 17No No 15Table 7: Results of conflict analysis by correctness ofproperty label (P) and attribute conflict (A).
Examplesof each type of correctness pair are show in in Table 6.50% of the clusters are correct in both labels, and thereare approximately the same number of errors toward bothproperty and attribute.difference in attribute.7 ConclusionWe have presented a probabilistic topic model foridentifying properties and attitudes of product re-view snippets.
The model is relatively simple andadmits an efficient variational mean-field inferenceprocedure which is parallelized and can be run ona large number of snippets.
We have demonstratedon multiple evaluation tasks that our model outper-forms applicable baselines by a considerable mar-gin.AcknowledgmentsThe authors acknowledge the support of the NSF(CAREER grant IIS-0448168), NIH (grant 5-R01-LM009723-02), Nokia, and the DARPA Ma-chine Reading Program (AFRL prime contract no.FA8750-09-C-0172).
Thanks to Peter Szolovits andthe MIT NLP group for their helpful comments.Any opinions, findings, conclusions, or recommen-dations expressed in this paper are those of the au-thors, and do not necessarily reflect the views of thefunding organizations.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Giuseppe Carenini, Raymond Ng, and Adam Pauls.2006.
Multi-document summarization of evaluativetext.
In Proceedings of EACL, pages 305?312.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of SIGKDD,pages 168?177.Minqing Hu and Bing Liu.
2006.
Opinion extraction andsummarization on the web.
In Proceedings of AAAI.Soo-Min Kim and Eduard Hovy.
2006.
Automatic iden-tification of pro and con reasons in online reviews.
InProceedings of COLING/ACL, pages 483?490.Hyun Duk Kim and ChengXiang Zhai.
2009.
Generat-ing comparative summaries of contradictory opinionsin text.
In Proceedings of CIKM, pages 385?394.P.
Liang and D. Klein.
2007.
Structured Bayesian non-parametric models with variational inference (tutorial).In Proceedings of ACL.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005a.Opinion observer: Analyzing and comparing opinionson the web.
In Proceedings of WWW, pages 342?351.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005b.Opinion observer: analyzing and comparing opinionson the web.
In Proceedings of WWW, pages 342?351.Yue Lu and ChengXiang Zhai.
2008.
Opinion integra-tion through semi-supervised topic modeling.
In Pro-ceedings of WWW, pages 121?130.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, andChengXiang Zhai.
2007.
Topic sentiment mixture:modeling facets and opinions in weblogs.
In Proceed-ings of WWW, pages 171?180.Ana-Maria Popescu, Bao Nguyen, and Oren Etzioni.2005.
OPINE: Extracting product features and opin-ions from reviews.
In Proceedings of HLT/EMNLP,pages 339?346.Christina Sauper, Aria Haghighi, and Regina Barzilay.2010.
Incorporating content structure into text anal-ysis applications.
In Proceedings of EMNLP, pages377?387.Yohei Seki, Koji Eguchi, Noriko K, and Masaki Aono.2006.
Opinion-focused summarization and its analysisat DUC 2006.
In Proceedings of DUC, pages 122?130.Ivan Titov and Ryan McDonald.
2008.
A joint model oftext and aspect ratings for sentiment summarization.In Proceedings of ACL, pages 308?316.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof MUC, pages 45?52.358
