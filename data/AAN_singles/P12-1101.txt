Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 959?968,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsLarge-Scale Syntactic Language Modeling with TreeletsAdam Pauls Dan KleinComputer Science DivisionUniversity of California, BerkeleyBerkeley, CA 94720, USA{adpauls,klein}@cs.berkeley.eduAbstractWe propose a simple generative, syntacticlanguage model that conditions on overlap-ping windows of tree context (or treelets) inthe same way that n-gram language modelscondition on overlapping windows of linearcontext.
We estimate the parameters of ourmodel by collecting counts from automati-cally parsed text using standard n-gram lan-guage model estimation techniques, allowingus to train a model on over one billion tokensof data using a single machine in a matter ofhours.
We evaluate on perplexity and a rangeof grammaticality tasks, and find that we per-form as well or better than n-gram models andother generative baselines.
Our model evencompetes with state-of-the-art discriminativemodels hand-designed for the grammaticalitytasks, despite training on positive data alone.We also show fluency improvements in a pre-liminary machine translation experiment.1 IntroductionN -gram language models are a central componentof all speech recognition and machine translationsystems, and a great deal of research centers aroundrefining models (Chen and Goodman, 1998), ef-ficient storage (Pauls and Klein, 2011; Heafield,2011), and integration into decoders (Koehn, 2004;Chiang, 2005).
At the same time, because n-gramlanguage models only condition on a local windowof linear word-level context, they are poor models oflong-range syntactic dependencies.
Although sev-eral lines of work have proposed generative syntac-tic language models that improve on n-gram mod-els for moderate amounts of data (Chelba, 1997; Xuet al, 2002; Charniak, 2001; Hall, 2004; Roark,2004), these models have only recently been scaledto the impressive amounts of data routinely used byn-gram language models (Tan et al, 2011).In this paper, we describe a generative, syntac-tic language model that conditions on local con-text treelets1 in a parse tree, backing off to smallertreelets as necessary.
Our model can be trained sim-ply by collecting counts and using the same smooth-ing techniques normally applied to n-gram mod-els (Kneser and Ney, 1995), enabling us to applytechniques developed for scaling n-gram models outof the box (Brants et al, 2007; Pauls and Klein,2011).
The simplicity of our training procedure al-lows us to train a model on a billion tokens of data ina matter of hours on a single machine, which com-pares favorably to the more involved training algo-rithm of Tan et al (2011), who use a two-pass EMtraining algorithm that takes several days on severalhundred CPUs using similar amounts of data.The simplicity of our approach also contrasts withrecent work on language modeling with tree sub-stitution grammars (Post and Gildea, 2009), wherelarger treelet contexts are incorporated by using so-phisticated priors to learn a segmentation of parsetrees.
Such an approach implicitly assumes that a?correct?
segmentation exists, but it is not clear thatthis is true in practice.
Instead, we build upon thesuccess of n-gram language models, which do notassume a segmentation and instead score all over-lapping contexts.We evaluate our model in terms of perplexity, andshow that we achieve the same performance as astate-of-the-art n-gram model.
We also evaluate ourmodel on several grammaticality tasks proposed in1We borrow the term treelet from Quirk et al (2005), whouse it to refer to an arbitrary connected subgraph of a tree.959(a) The index fell 109.85 Monday .
(b) ROOTS-VBD?ROOTNP-NNDT-theTheNNindexVP-VBD?SVBDfellCD-DC109.85NNTPMonday..(c) ROOTS-VBD?ROOTNP-NNDT-theTheNNindexVP-VBD?SVBDfellCD-DC109.85NNTPMonday..5-GRAMThe board ?s will soon be feasible , from everyday which Coke ?s cabinet hotels .They are all priced became regulatory action by difficulty caused nor Aug. 31 of Helmsley-Spear :Lakeland , it may take them if the 46-year-old said the loss of the Japanese executives at him :But 8.32 % stake in and Rep. any money for you got from several months , ?
he says .TREELETWhy a $ 1.2 million investment in various types of the bulk of TVS E. August ??
One operating price position has a system that has Quartet for the first time , ?
he said .He may enable drops to take , but will hardly revive the rush to develop two-stroke calculations .
?The centers are losses of meals , and the runs are willing to like them .Table 1: The first four samples of length between 15 and 20 generated from the 5-GRAM and TREELET models.rule context would need its own state in the gram-mar), and extensive pruning would be in order.In practice, however, language models are nor-mally integrated into a decoder, a non-trivial taskthat is highly problem-dependent and beyond thescope of this paper.
However, we note that formachine translation, a model that builds target-sideconstituency parses, such as that of Galley et al(2006), combined with an efficient pruning strategylike cube pruning (Chiang, 2005), should be able tointegrate our model without much difficulty.That said, for evaluation purposes, whenever weneed to query our model, we use the simple strategyof parsing a sentence using a black box parser, andsumming over our model?s probabilities of the 1000-best parses.4 Note that the bottleneck in this caseis the parser, so our model can essentially score asentence at the speed of a parser.5 ExperimentsWe evaluate our model along several dimensions.We first show some sample sentences generated byour model in Section 5.1.
We report perplexity re-4We found that using the 1-best worked just as well as the1000-best on our grammaticality tasks, but significantly overes-timated our model?s perplexities.sults in Section 5.2.
In Section 5.3, we measureits ability to distinguish between grammatical En-glish and various types of automatically generated,or pseudo-negative,5 English.
We report machinetranslation reranking results in Section 5.4.5.1 Generating SamplesBecause our model is generative, we can qualita-tively assess it by generating samples and verifyingthat they are more syntactically coherent than otherapproaches.
In Table 1, we show the first four sam-ples of length between 15 and 20 generated fromboth model and a 5-gram model trained on the PennTreebank.5.2 PerplexityPerplexity is the standard intrinsic evaluation metricfor language models.
It measures the inverse of theper-word probability a model assigns to some held-out set of grammatical English (so lower is better).For training data, we constructed a large treebank byconcatenating the Penn Treebank, the Brown Cor-pus, the 50K BLLIP training sentences from Post(2011), and the AFP and APW portions of English5We follow Okanohara and Tsujii (2007) in using the termpseudo-negative to highlight the fact that automatically gener-ated negative examples might not actually be ungrammatical.Figure 1: Conditioning contexts and back-off strategies for Markov models.
The bolded symbol indicates the part of thetree/sentence being generated, and the dotted lines represent the conditioning contexts; back-off proceeds from the largest to thesmallest context.
(a) A trigram model.
(b) The context used for non-terminal productions in our treelet model.
For this context,P=VP-VBD?S, P ?=S-VBD?ROOT, and r?=S-VBD?ROOT?NP-NN VP-VBD?S .
(c) The context used for terminal productionsin our treelet model.
Here, P=VBD, R=CD-DC, r?=VP-VBD?S?VBD CD-DC NNTP, w?1=index, and w?2=The.
Note that thetree is a modified version of a standard Penn Treebank parse ?
see Section 3 for details.the literature (Okanohara and Tsujii, 2007; Foster etal., 2008; Cherry and Quirk, 2008) and show thatit consistently outperforms an n-gram model as wellas other head-driven and tree-driven generative base-lines.
Our model even competes with state-of-the-artdiscriminative classifiers specifically designed foreach task, despite being estimated on positive dataalone.
We also show fluency improvements in a pre-liminary machine translation reranking experiment.2 Treelet Language ModelingThe common denominator of most n-gram languagemodels is that they assign probabilities roughly ac-cording to empirical frequencies for observed n-grams, but fall back to distributions conditioned onsmaller contexts for unobserved n-grams, as shownin Figure 1(a).
This type of smoothing is both highlyrobust and easy to implement, requiring only the col-lection of counts from data.We would like to apply the same smoothing tech-niques to distributions over rule yields in a con-stituency tree, conditioned on contexts consistingof previously generated treelets (rules, nodes, etc.
).Formally, let T be a constituency tree consisting ofcontext-free rules of the form r = P ?
C1 ?
?
?Cd,where P is the parent symbol of rule r and Cd1 =C1 .
.
.
Cd are its children.
We wish to assign proba-bilities to trees22A distribution over trees also induces a distribution oversentences w`1 given by p(w`1) = PT :s(T )=w`1 p(T ), wherep(T ) =?r?Tp(Cd1 |h)where the conditioning context h is some portion ofthe already-generated parts of the tree.
In this paper,we assume that the children of a rule are expandedfrom left to right, so that when generatin th yi ldCd1 , all treelets above and left of th parent P areavailable.
Note that a raw PCFG would conditiononly on P , i.e.
h = P .As in the n-gram case, we would like to pick hto be large enough to capture relevant dependencies,but small enough that we can obtain meaningful es-timates from data.
We start with a straightforwardchoice of context: we condition on P , as well as therule r?
that generated P , as shown in in Figure 1(b).Conditioning on the parent rule r?
allows us tocapture several important dependencies.
First, itcaptures both P and its parent P ?, which predictsthe distribution over child symbols far better thanjust P (Johnson, 1998).
Second, it captures posi-tional effects.
For example, subject and object nounphrases (NPs) have different distributions (Klein andManning, 2003), and the position of an NP relativeto a verb is a good indicator of this distinction.
Fi-nally, the generation of words at preterminals cancondition on siblings, allowing the model to capture,for example, verb subcategorization frames.We should be clear that we are not the firsts(T ) is the terminal yield of T .960to use back-off-based smoothing for syntactic lan-guage modeling ?
such techniques have been ap-plied to models that condition on head-word con-texts (Charniak, 2001; Roark, 2004; Zhang, 2009).Parent rule context has also been employed in trans-lation (Vaswani et al, 2011).
However, to ourknowledge, we are the first to apply these techniquesfor language modeling on large amounts of data.2.1 Lexical contextAlthough it is tempting to think that we can replacethe left-to-right generation of n-gram models withthe purely top-down generation of typical PCFGs,in practice, words are often highly predictive of thewords that follow them ?
indeed, n-gram modelswould be terrible language models if this were notthe case.
To capture linear effects, we extend thecontext for terminal (lexical) productions to includethe previous two wordsw?2 andw?1 in the sentencein addition to r?
; see Figure 1(c) for a depiction.
Thisallows us to capture collocations and other lexicalcorrelations.2.2 Backing offAs with n-gram models, counts for rule yields con-ditioned on r?
are sparse, and we must choose an ap-propriate back-off strategy.
We handle terminal andnon-terminal productions slightly differently.For non-terminal productions, we back off fromr?
to P and its parent P ?, and then to just P .That is, we back off from a rule-annotated gram-mar p(Cd1 |P, P ?, r?)
to a parent-annotated gram-mar (Johnson, 1998) p(Cd1 |P, P ?
), then to a rawPCFG p(Cd1 |P ).
In order to generalize to unseenrule yields Cd1 , we further back off from the ba-sic PCFG probability p(Cd1 |P ) to p(Ci|Ci?1i?3 , P ), a4-gram model over symbols C conditioned on P ,interpolated with an unconditional 4-gram modelp(Ci|Ci?1i?3 ).
In other words, we back off from a rawPCFG to?d?i=1p(Ci|Ci?1i?3 , P ) + (1?
?
)d?i=1p(Ci|Ci?1i?3 )where ?
= 0.9 is an interpolation constant.For terminal (i.e lexical) productions, wefirst remove lexical context, backing off fromp(w|P,R, r?, w?1, w?2) to p(w|P,R, r?, w?1) andthen p(w|P,R, r?).
From there, we back off top(w|P,R) whereR is the sibling immediately to theright of P , then to a raw PCFG p(w|P ), and finallyto a unigram distribution.
We chose this scheme be-cause p(w|P,R) allows, for example, a verb to begenerated conditioned on the non-terminal categoryof the argument it takes (since arguments usually im-mediately follow verbs).
We depict these two back-off schemes pictorially in Figure 1(b) and (c).2.3 EstimationEstimating the probabilities in our model can bedone very simply using the same techniques (in fact,the same code) used to estimate n-gram languagemodels.
Our model requires estimates of four distri-butions: p(Cd1 |P, P ?, r?
), p(w|P,R, r?, w?1, w?2),p(Ci|Ci?1i?n+1, P ), and p(Ci|Ci?1i?n+1).
In each case,we require empirical counts of treelet tuples in thesame way that we require counts of word tuples forestimating n-gram language models.There is one additional hurdle in the estimation ofour model: while there exist corpora with human-annotated constituency parses like the Penn Tree-bank (Marcus et al, 1993), these corpora are quitesmall ?
on the order of millions of tokens ?
and wecannot gather nearly as many counts as we can for n-grams, for which billions or even trillions (Brants etal., 2007) of tokens are available on the Web.
How-ever, we can use one of several high-quality con-stituency parsers (Collins, 1997; Charniak, 2000;Petrov et al, 2006) to automatically generate parses.These parses may contain errors, but not all parsingerrors are problematic for our model, since we onlycare about the sentences generated by our model andnot the parses themselves.
We show in our experi-ments that the addition of data with automatic parsesdoes improve the performance of our language mod-els across a range of tasks.3 Tree TransformationsIn the previous section, we described how to condi-tion on rich parse context to better capture the dis-tribution of English trees.
While such context al-lows our model to capture many interesting depen-dencies, several important dependencies require ad-ditional attention.
In this section, we describe a961ROOTS-VB?ROOTPRP-heHeVP-VB?SVBresetNP-NNSJJopeningNNSargumentsPP-forIN-forforNNTtoday..Figure 2: A sample parse from the Penn Treebank afterthe tree transformations described in Section 3.
Note thatwe have not shown head tag annotations on preterminalsbecause in that case, the head tag is the preterminal itself.number of transformations of Treebank constituencyparses that allow us to capture such dependencies.We list the annotations and deletions in the order inwhich they are performed.
A sample transformedtree is shown in Figure 2.Temporal NPs Following Klein and Manning (2003),we attempt to annotate temporal noun phrases.
Althoughthe Penn Treebank annotates temporal NPs, most off-the-shelf parsers do not retain these tags, and we do not as-sume their presence.
Instead, we mark any noun that isthe head of a NP-TMP constituent at least once in theTreebank as a temporal noun, so for example today wouldbe tagged as NNT and months would be tagged as NNTS.Head Annotations We annotate every non-terminal orpreterminal with its head word if the head is a closed-class word3 and with its head tag otherwise.
Klein andManning (2003) used head tag annotation extensively,though they applied their splits much more selectively.NP Flattening We delete NPs dominated byother NPs, unless the child NPs are in coordi-nation or apposition.
These NPs typically oc-cur when nouns are modified by PPs, as in(NP (NP (NN stock) (NNS sales)) (PP (IN by) (NNS traders))).
Byremoving the dominated NP, we allow the productionNNS?sales to condition on the presence of a modifyingPP (here a PP head-annotated with by).Number Annotations Numbers are divided into fiveclasses: CD-YR for numbers that consist of four digits(which are usually years); CD-NM for entirely numericnumbers; CD-DC for numbers that have a decimal; CD-3We define the following to be closed class words: any punc-tuation; all inflections of the verbs do, be, and have; and anyword tagged with IN, WDT, PDT, WP, WP$, TO, WRB, RP,DT, SYM, EX, POS, PRP, AUX, or CC.MX for numbers that mix letters and digits; and CD-ALfor numbers that are entirely alphabetic.SBAR Flattening We remove any sentential (S) nodesimmediately dominated by an SBAR.
S nodes underSBAR have very distinct distributions from other senten-tial nodes, mostly due to empty subjects and/or objects.VP Flattening We remove any VPs immediately domi-nating a VP, unless it is conjoined with another VP.
In theTreebank, chains of verbs (e.g.
will be going) have a sep-arate VP for each verb.
By flattening such structures, weallow the main verb and its arguments to condition on thewhole chain of verbs.
This effect is particularly importantfor passive constructions.Gapped Sentence Annotation Collins (1999) andKlein and Manning (2003) annotate nodes which haveempty subjects.
Because we only assume the presenceof automatically derived parses, which do not producethe empty elements in the original Treebank, we mustidentify such elements on our own.
We use a very simpleprocedure: we annotate all S or SBAR nodes that have aVP before any NPs.Parent Annotation We annotate all VPs with their par-ent symbol.
Because our treelet model already conditionson the parent, this has the effect of allowing verbs to con-dition on their grandparents.
This was important for VPsunder SBAR nodes, which often have empty objects.
Wealso parent-annotated any child of the ROOT.Unary Deletion We remove all unary productions ex-cept the root and preterminal productions, keeping onlythe bottom-most symbol.
Because we are not interestedin the internal labels of the trees, unaries are largely anuisance, and their removal brings many symbols into thecontext of others.4 Scoring a SentenceComputing the probability of a sentence w`1 underour model requires summing over all possible parsesof w`1.
Although our model can be formulated as astraightforward PCFG, allowing O(`3) computationof this sum, the grammar constant for this PCFGwould be unmanageably large (since every parentrule context would need its own state in the gram-mar), and extensive pruning would be in order.In practice, however, language models are nor-mally integrated into a decoder, a non-trivial taskthat is highly problem-dependent and beyond thescope of this paper.
For machine translation, a modelthat builds target-side constituency parses, such asthat of Galley et al (2006), combined with an ef-ficient pruning strategy like cube pruning (Chiang,9625-GRAMThe board ?s will soon be feasible , from everyday which Coke ?s cabinet hotels .They are all priced became regulatory action by difficulty caused nor Aug. 31 of Helmsley-Spear :Lakeland , it may take them if the 46-year-old said the loss of the Japanese executives at him :But 8.32 % stake in and Rep. any money for you got from several months , ?
he says .TREELETWhy a $ 1.2 million investment in various types of the bulk of TVS E. August ??
One operating price position has a system that has Quartet for the first time , ?
he said .He may enable drops to take , but will hardly revive the rush to develop two-stroke calculations .
?The centers are losses of meals , and the runs are willing to like them .Table 1: The first four samples of length between 15 and 20 generated from the 5-GRAM and TREELET models.2005), should be able to integrate our model withoutmuch difficulty.That said, for evaluation purposes, whenever weneed to query our model, we use the simple strategyof parsing a sentence using a black box parser, andsumming over our model?s probabilities of the 1000-best parses.4 Note that the bottleneck in this caseis the parser, so our model can essentially score asentence at the speed of a parser.5 ExperimentsWe evaluate our model along several dimensions.We first show some sample generated sentences inSection 5.1.
We report perplexity results in Sec-tion 5.2.
In Section 5.3, we measure its ability todistinguish between grammatical English and var-ious types of automatically generated, or pseudo-negative,5 English.
We report machine translationreranking results in Section 5.4.5.1 Generating SamplesBecause our model is generative, we can qualita-tively assess it by generating samples and verifyingthat they are more syntactically coherent than otherapproaches.
In Table 1, we show the first four sam-ples of length between 15 and 20 generated from ourmodel and a 5-gram model trained on the Penn Tree-bank.4We found that using the 1-best worked just as well as the1000-best on our grammaticality tasks, but significantly overes-timated our model?s perplexities.5We follow Okanohara and Tsujii (2007) in using the termpseudo-negative to highlight the fact that automatically gener-ated negative examples might not actually be ungrammatical.5.2 PerplexityPerplexity is the standard intrinsic evaluation metricfor language models.
It measures the inverse of theper-word probability a model assigns to some held-out set of grammatical English (so lower is better).For training data, we constructed a large treebank byconcatenating the WSJ and Brown portions of thePenn Treebank, the 50K BLLIP training sentencesfrom Post (2011), and the AFP and APW portionsof English Gigaword version 3 (Graff, 2003), total-ing about 1.3 billion tokens.
We used the human-annotated parses for the sentences in the Penn Tree-bank, but parsed the Gigaword and BLLIP sentenceswith the Berkeley Parser.
Hereafter, we refer to thistraining data as our 1B corpus.
We used Section 0of the WSJ as our test corpus.
Results are shown inTable 2.
In addition to our TREELET model, we alsoshow results for the following baselines:5-GRAM A 5-gram interpolated Kneser-Ney model.PCFG-LA The Berkeley Parser in language model mode.HEADLEX A head-lexicalized model similar to, butmore powerful6 than, Collins Model 1 (Collins, 1999).PCFG A raw PCFG.TREELET-TRANS A PCFG estimated on the trees afterthe transformations of Section 3.TREELET-RULE The TREELET-TRANS model with theparent rule context described in Section 2.
This is equiv-alent to the full TREELET model without the lexical con-text described in Section 2.1.6Specifically, like Collins Model 1, we generate a rule yieldconditioned on parent symbol P and head word h by first gen-erating its head symbol Ch, then generating the head words andsymbols for left and right modifiers outwards from Ch.
UnlikeModel 1, which generates each modifier head and symbol con-ditioned only on Ch, h, and P , we additionally condition on thepreviously generated modifier?s head and symbol and back offto Model 1.963Model PerplexityPCFG 1772TREELET-TRANS 722TREELET-RULE 329TREELET 198?PCFG-LA 330**HEADLEX 2995-GRAM 207?Table 2: Perplexity of several generative models on Sec-tion 0 of the WSJ.
The differences between scores markedwith ?
are not statistically significant.
PCFG-LA (markedwith **) was only trained on the WSJ and Brown corporabecause it does not scale to large amounts of data.We used the Berkeley LM toolkit (Paulsand Klein, 2011), which implements Kneser-Neysmoothing, to estimate all back-off models for bothn-gram and treelet models.
To deal with unknownwords, we use the following strategy: after the first10000 sentences, whenever we see a new word inour training data, we replace it with a signature710% of the time.Our model outperforms all other generative mod-els, though the improvement over the n-gram modelis not statistically significant.
Note that because weuse a k-best approximation for the sum over trees,all perplexities (except for PCFG-LA and 5-GRAM)are pessimistic bounds.5.3 Classification of Pseudo-Negative SentencesWe make use of three kinds of automatically gener-ated pseudo-negative sentences previously proposedin the literature: Okanohara and Tsujii (2007) pro-posed generating pseudo-negative examples from atrigram language model; Foster et al (2008) create?noisy?
sentences by automatically inserting a sin-gle error into grammatical sentences with a scriptthat randomly deletes, inserts, or misspells a word;and Och et al (2004) and Cherry and Quirk (2008)both use the 1-best output of a machine translationsystem.
Examples of these three types of pseudo-negative data are shown in Table 3.
We evaluate ourmodel?s ability to distinguish positive from pseudo-negative data, and compare against generative base-lines and state-of-the-art discriminative methods.7We use signatures generated by the Berkeley Parser.These signatures capture surface features such as capitalization,presents of digits, and common suffixes.
For example, the wordvexing would be replaced with the signature UNK-ing.Noisy There was were many contributors.Trigram For years in dealer immediately .MT we must further steps .Table 3: Sample pseudo-negative sentences.We would like to use our model to make grammat-icality judgements, but as a generative model it canonly provide us with probabilities.
Simply thresh-olding generative probabilities, even with a separatethreshold for each length, has been shown to be veryineffective for grammaticality judgements, both forn-gram and syntactic language models (Cherry andQuirk, 2008; Post, 2011).
We used a simple measurefor isolating the syntactic likelihood of a sentence:we take the log-probability under our model andsubtract the log-probability under a unigram model,then normalize by the length of the sentence.8 Thismeasure, which we call the syntactic log-odds ratio(SLR), is a crude way of ?subtracting out?
the se-mantic component of the generative probability, sothat sentences that use rare words are not penalizedfor doing so.5.3.1 Trigram ClassificationTo facilitate comparison with previous work, weused the same negative corpora as Post (2011) fortrigram classification.
They randomly selected 50Ktrain, 3K development, and 3K positive test sen-tences from the BLLIP corpus, then trained a tri-gram model on 450K BLLIP sentences and gener-ated 50K train, 3K development, and 3K negativesentences.
We parsed the 50K positive training ex-amples of Post (2011) with the Berkeley Parser andused the resulting treebank to train a treelet languagemodel.
We set an SLR threshold for each model onthe 6K positive and negative development sentences.Results are shown in Table 4.
In addition to ourgenerative baselines, we show results for the dis-criminative models reported in Cherry and Quirk(2008) and Post (2011).
The former train a latentPCFG support vector machine for binary classifica-tion (LSVM).
The latter report results for two bi-nary classifiers: RERANK uses the reranking fea-tures of Charniak and Johnson (2005), and TSG uses8Och et al (2004) also report using a parser probability nor-malized by the unigram probability (but not length), and did notfind it effective.
We assume this is either because the length-normalization is important, or because their choice of syntacticlanguage model was poor.964GenerativeBLLIP 1BPCFG 81.5 81.8TREELET-TRANS 87.7 90.1TREELET-RULE 89.8 94.1TREELET 88.9 93.3PCFG-LA 87.1* ?HEADLEX 87.6 92.05-GRAM 67.9 87.5DiscriminativeBLLIP 1BLSVM 81.42** ?TSG 89.9 ?RERANK 93.0 ?Table 4: Classification accuracy for trigram pseudo-negativesentences on the BLLIP corpus.
The number reported forPCFG-LA is marked with a * to indicate that this model wastrained on the training section of the WSJ, not the BLLIP cor-pus.
The number reported for LSVM (marked with **) was eval-uated on a different random split of the BLLIP corpus, and so isnot directly comparable.indicator features extracted from a tree substitutiongrammar derivation of each sentence.Our TREELET model performs nearly as well asthe TSG method, and substantially outperforms theLSVM method, though the latter was not tested onthe same random split.
Interestingly, the TREELET-RULE baseline, which removes lexical context fromour model, outperforms the full model.
This is likelybecause the negative data is largely coherent at thetrigram level (because it was generated from a tri-gram model), and the full model is much more sen-sitive to trigram coherence than the TREELET-RULEmodel.
This also explains the poor performance ofthe 5-GRAM model.We emphasize that the discriminative baselinesare specifically trained to separate trigram text fromnatural English, while our model is trained on pos-itive examples alone.
Indeed, the methods in Post(2011) are simple binary classifiers, and it is notclear that these models would be properly calibratedfor any other task, such as integration in a decoder.One of the design goals of our system was thatit be scalable.
Unlike some of the discriminativebaselines, which require expensive operations9 on9It is true that in order train our system, one must parse largeamounts of training data, which can be costly, though it onlyneeds to be done once.
In contrast, even with observed train-ing trees, the discriminative algorithms must still iteratively per-form expensive operations (like parsing) for each sentence, anda new model must be trained for new types of negative data.Model Pairwise IndependentWSJ 1B WSJ 1BPCFG 79.1 77.0 58.9 58.6TREELET-RULE 90.3 94.4 63.8 66.2TREELET 90.7 94.5 63.4 65.55-GRAM 86.3 93.5 55.7 60.1HEADLEX 90.7 94.0 59.5 62.0PCFG-LA 91.3 ?
59.7 ?Foster et al (2008) ?
?
65.9 ?Table 5: Classification accuracies on the noisy WSJ for mod-els trained on WSJ Sections 2-21 and our 1B token corpus.?Pairwise?
accuracy is the fraction of correct sentences whoseSLR score was higher than its noisy version, and ?independent?refers to standard binary classification accuracy.each training sentence, we can very easily scaleour model to much larger amounts of data.
In Ta-ble 4, we also show the performance of the gener-ative models trained on our 1B corpus.
All gener-ative models improve, but TREELET-RULE remainsthe best, now outperforming the RERANK system,though of course it is likely that RERANK would im-prove if it could be scaled up to more training data.5.3.2 ?Noisy?
ClassificationWe also evaluate the performance of our modelon the task of distinguishing the noisy WSJ sen-tences of Foster et al (2008) from their originalversions.
We use the noisy versions of Section 0and 23 produced by their error-generating proce-dure.
Because they only report classification re-sults on Section 0, we used Section 23 to tune anSLR threshold, and tested our model on Section 0.We show the results of both independent and pair-wise classification for the WSJ and 1B training setsin Table 5.
Note that independent classification ismuch more difficult than for the trigram data, be-cause sentences contain at most one change, whichmay not even result in an ungrammaticality.
Again,our model outperforms the n-gram model for bothtypes of classification, and achieves the same per-formance as the discriminative system of Foster etal.
(2008), which is state-of-the-art for this data set.The TREELET-RULE system again slightly outper-forms the full TREELET model at independent clas-sification, though not at pairwise classification.
Thisprobably reflects the fact that semantic coherencecan still influence the SLR score, despite our effortsto subtract it out.
Because the TREELET model in-cludes lexical context, it is more sensitive to seman-965French German Chinese5-GRAM 44.8 37.8 60.0TREELET 57.9 66.0 83.8Table 6: Pairwise comparison accuracy of MT outputagainst a reference translation for French, German, andChinese.
The BLEU scores for these outputs are 32.7,27.8, and 20.8.
This task becomes easier, at least for ourTREELET model, as translation quality drops.
Cherry andQuirk (2008) report an accuracy of 71.9% on a similarexperiment with German a source language, though thetranslation system and training data were different so thenumbers are not comparable.
In particular, their transla-tions had a lower BLEU score, making their task easier.tic coherence and thus more likely to misclassifysemantically coherent but ungrammatical sentences.For pairwise comparisons, where semantic coher-ence is effectively held constant, such sentences arenot problematic.5.3.3 Machine Translation ClassificationWe follow Och et al (2004) and Cherry and Quirk(2008) in evaluating our language models on theirability to distinguish the 1-best output of a machinetranslation system from a reference translation in apairwise fashion.
Unfortunately, we do not haveaccess to the data used in those papers, so a di-rect comparison is not possible.
Instead, we col-lected the English output of Moses (Hoang et al,2007), using both French and German as source lan-guage, trained on the Europarl corpus used by WMT2009.10 We also collected the output of Joshua (Liet al, 2009) trained on 500K sentences of GALEChinese-English parallel newswire.
We trained bothour TREELET model and a 5-GRAM model on theunion of our 1B corpus and the English sides of ourparallel corpora.In Table 6, we show the pairwise comparison ac-curacy (using SLR) on these three corpora.
We seethat our system prefers the reference much more of-ten than the 5-GRAM language model.11 However,we also note that the easiness of the task is corre-lated with the quality of translations (as measured inBLEU score).
This is not surprising ?
high-qualitytranslations are often grammatical and even a per-10http://www.statmt.org/wmt0911We note that the n-gram language model used by the MTsystem was much smaller than the 5-GRAM model, as they wereonly trained on the English sides of their parallel data.fect language model might not be able to differenti-ate such translations from their references.5.4 Machine Translation FluencyWe also carried out reranking experiments on 1000-best lists from Moses using our syntactic languagemodel as a feature.
We did not find that the useof our syntactic language model made any statis-tically significant increases in BLEU score.
How-ever, we noticed in general that the translations fa-vored by our model were more fluent, a useful im-provement to which BLEU is often insensitive.
Toconfirm this, we carried out an Amazon Mechan-ical Turk experiment where users from the UnitedStates were asked to compare translations using ourTREELET language model as the language modelfeature to those using the 5-GRAM model.12 We had1000 such translation pairs rated by 4 separate Turk-ers each.
Although these two hypothesis sets hadthe same BLEU score (up to statistical significance),the Turkers preferred the output obtained using oursyntactic language model 59% of the time, indicat-ing that our model had managed to pick out morefluent hypotheses that nonetheless were of the sameBLEU score.
This result was statistically significantwith p < 0.001 using bootstrap resampling.6 ConclusionWe have presented a simple syntactic languagemodel that can be estimated using standard n-gramsmoothing techniques on large amounts of data.
Ourmodel outperforms generative baselines on severalevaluation metrics and achieves the same perfor-mance as state-of-the-art discriminative classifiersspecifically trained on several types of negative data.AcknowledgmentsWe would like to thank David Hall for some modelingsuggestions and the anonymous reviewers for their com-ments.
We thank both Matt Post and Jennifer Foster forproviding us with their corpora.
This work was partiallysupported by a Google Fellowship to the first author andby BBN under DARPA contract HR0011-12-C-0014.12We used translations from the baseline Moses system ofSection 5.3.3 with German as the input language.
For each lan-guage model, we took k-best lists from the baseline system andreplaced the baseline LM score with the new model?s score.
Wethen retrained all feature weights with MERT on the tune set,and selected the 1-best output on the test set.966ReferencesThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, Jeffrey Dean, and Google Inc. 2007.
Large lan-guage models in machine translation.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the Association for Computa-tional Linguistics.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the North American chapterof the Association for Computational Linguistics.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of the Associationfor Computational Linguistics.Ciprian Chelba.
1997.
A structured language model.
InProceedings of the Association for Computational Lin-guistics.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of the Association for Computa-tional Linguistics.Colin Cherry and Chris Quirk.
2008.
Discriminative,syntactic language modeling through latent SVMs.
InProceedings of The Association for Machine Transla-tion in the Americas.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In The Annual Con-ference of the Association for Computational Linguis-tics.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of As-sociation for Computational Linguistics.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Jennifer Foster, Joachim Wagner, and Josef van Genabith.2008.
Adapting a wsj-trained parser to grammaticallynoisy text.
In Proceedings of the Association for Com-putational Linguistics: Short Paper Track.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In The An-nual Conference of the Association for ComputationalLinguistics (ACL).David Graff.
2003.
English gigaword, version 3.
In Lin-guistic Data Consortium, Philadelphia, Catalog Num-ber LDC2003T05.Keith Hall.
2004.
Best-first Word-lattice Parsing: Tech-niques for Integrated Syntactic Language Modeling.Ph.D.
thesis, Brown University.Kenneth Heafield.
2011.
Kenlm: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation.Hieu Hoang, Alexandra Birch, Chris Callison-burch,Richard Zens, Rwth Aachen, Alexandra Constantin,Marcello Federico, Nicola Bertoldi, Chris Dyer,Brooke Cowan, Wade Shen, Christine Moran, and On-dej Bojar.
2007.
Moses: Open source toolkit for sta-tistical machine translation.
In Proceedings of the As-sociation for Computational Linguistics: Demonstra-tion Session,.Mark Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics, 24.Dan Klein and Chris Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of the North AmericanChapter of the Association for Computational Linguis-tics (NAACL).Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In IEEEInternational Conference on Acoustics, Speech andSignal Processing.Philipp Koehn.
2004.
Pharaoh: A beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of The Association for MachineTranslation in the Americas.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenN.
G. Thornton, Jonathan Weese, and Omar F. Zaidan.2009.
Joshua: an open source toolkit for parsing-based machine translation.
In Proceedings of theFourth Workshop on Statistical Machine Translation.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
In Computational Linguistics.Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, AnoopSarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,Libin Shen, David Smith, Katherine Eng, Viren Jain,Zhen Jin, and Dragomir Radev.
2004.
A Smorgas-bord of Features for Statistical Machine Translation.In Proceedings of the North American Association forComputational Linguistic.Daisuke Okanohara and Jun?ichi Tsujii.
2007.
Adiscriminative language model with pseudo-negativesamples.
In Proceedings of the Association for Com-putational Linguistics.Adam Pauls and Dan Klein.
2011.
Faster and smallern-gram language models.
In Proceedings of the Asso-ciation for Computational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of COLING-ACL 2006.Matt Post and Daniel Gildea.
2009.
Language model-ing with tree substitution grammars.
In Proceedings967of the Conference on Neural Information ProcessingSystems.Matt Post.
2011.
Judging grammaticality with tree sub-stitution grammar.
In Proceedings of the Associationfor Computational Linguistics: Short Paper Track.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of the Association ofComputational Linguistics.Brian Roark.
2004.
Probabilistic top-down parsing andlanguage modeling.
Computational Linguistics.Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.2011.
A large scale distributed syntactic, semanticand lexical language model for machine translation.In Proceedings of the Association for ComputationalLinguistics.Ashish Vaswani, Haitao Mi, Liang Huang, and DavidChiang.
2011.
Rule markov models for fast tree-to-string translation.
In Proceedings of the Associationfor Computations Linguistics.Peng Xu, Ciprian Chelba, and Fred Jelinek.
2002.
Astudy on richer syntactic dependencies for structuredlanguage modeling.
In Proceedings of the Associationfor Computational Linguistics.
Association for Com-putational Linguistics.Ying Zhang.
2009.
Structured language models for sta-tistical machine translation.
Ph.D. thesis, Johns Hop-kins University.968
