Discriminative Training of a Neural Network Statistical ParserJames HENDERSONSchool of Informatics, University of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LWUnited Kingdomjames.henderson@ed.ac.ukAbstractDiscriminative methods have shown significantimprovements over traditional generative meth-ods in many machine learning applications, butthere has been difficulty in extending them tonatural language parsing.
One problem is thatmuch of the work on discriminative methodsconflates changes to the learning method withchanges to the parameterization of the problem.We show how a parser can be trained with a dis-criminative learning method while still param-eterizing the problem according to a generativeprobability model.
We present three methodsfor training a neural network to estimate theprobabilities for a statistical parser, one gen-erative, one discriminative, and one where theprobability model is generative but the trainingcriteria is discriminative.
The latter model out-performs the previous two, achieving state-of-the-art levels of performance (90.1% F-measureon constituents).1 IntroductionMuch recent work has investigated the applica-tion of discriminative methods to NLP tasks,with mixed results.
Klein and Manning (2002)argue that these results show a pattern wherediscriminative probability models are inferiorto generative probability models, but that im-provements can be achieved by keeping a gener-ative probability model and training accordingto a discriminative optimization criteria.
Weshow how this approach can be applied to broadcoverage natural language parsing.
Our estima-tion and training methods successfully balancethe conflicting requirements that the trainingmethod be both computationally tractable forlarge datasets and a good approximation to thetheoretically optimal method.
The parser whichuses this approach outperforms both a genera-tive model and a discriminative model, achiev-ing state-of-the-art levels of performance (90.1%F-measure on constituents).To compare these different approaches, weuse a neural network architecture called Sim-ple Synchrony Networks (SSNs) (Lane and Hen-derson, 2001) to estimate the parameters of theprobability models.
SSNs have the advantagethat they avoid the need to impose hand-craftedindependence assumptions on the learning pro-cess.
Training an SSN simultaneously trains afinite representations of the unbounded parsehistory and a mapping from this history repre-sentation to the parameter estimates.
The his-tory representations are automatically tuned tooptimize the parameter estimates.
This avoidsthe problem that any choice of hand-crafted in-dependence assumptions may bias our resultstowards one approach or another.
The indepen-dence assumptions would have to be differentfor the generative and discriminative probabil-ity models, and even for the parsers which usethe generative probability model, the same setof independence assumptions may be more ap-propriate for maximizing one training criteriaover another.
By inducing the history represen-tations specifically to fit the chosen model andtraining criteria, we avoid having to choose in-dependence assumptions which might bias ourresults.Each complete parsing system we proposeconsists of three components, a probabilitymodel for sequences of parser decisions, a Sim-ple Synchrony Network which estimates the pa-rameters of the probability model, and a proce-dure which searches for the most probable parsegiven these parameter estimates.
This paperoutlines each of these components, but more de-tails can be found in (Henderson, 2003b), and,for the discriminative model, in (Henderson,2003a).
We also present the training methods,and experiments on the proposed parsing mod-els.2 Two History-Based ProbabilityModelsAs with many previous statistical parsers (Rat-naparkhi, 1999; Collins, 1999; Charniak, 2000),we use a history-based model of parsing.
De-signing a history-based model of parsing in-volves two steps, first choosing a mapping fromthe set of phrase structure trees to the set ofparses, and then choosing a probability modelin which the probability of each parser decisionis conditioned on the history of previous deci-sions in the parse.
We use the same mappingfor both our probability models, but we use twodifferent ways of conditioning the probabilities,one generative and one discriminative.
As wewill show in section 6, these two different waysof parameterizing the probability model have abig impact on the ease with which the parame-ters can be estimated.To define the mapping from phrase structuretrees to parses, we use a form of left-corner pars-ing strategy (Rosenkrantz and Lewis, 1970).
Ina left-corner parse, each node is introduced afterthe subtree rooted at the node?s first child hasbeen fully parsed.
Then the subtrees for thenode?s remaining children are parsed in theirleft-to-right order.
Parsing a constituent startsby pushing the leftmost word w of the con-stituent onto the stack with a shift(w) action.Parsing a constituent ends by either introducingthe constituent?s parent nonterminal (labeledY ) with a project(Y) action, or attaching to theparent with an attach action.1 A complete parseconsists of a sequence of these actions, d1,..., dm,such that performing d1,..., dm results in a com-plete phrase structure tree.Because this mapping from phrase structuretrees to sequences of decisions about parseractions is one-to-one, finding the most prob-able phrase structure tree is equivalent tofinding the parse d1,..., dm which maximizesP (d1,..., dm|w1,..., wn).
This probability is onlynonzero if yield(d1,..., dm) = w1,..., wn, so wecan restrict attention to only those parseswhich actually yield the given sentence.
Withthis restriction, it is equivalent to maximizeP (d1,..., dm), as is done with our first probabilitymodel.The first probability model is generative, be-cause it specifies the joint probability of the in-put sentence and the output tree.
This jointprobability is simply P (d1,..., dm), since the1More details on the mapping to parses can be foundin (Henderson, 2003b).probability of the input sentence is included inthe probabilities for the shift(wi) decisions in-cluded in d1,..., dm.
The probability model isthen defined by using the chain rule for con-ditional probabilities to derive the probabilityof a parse as the multiplication of the proba-bilities of each decision di conditioned on thatdecision?s prior parse history d1,..., di?1.P (d1,..., dm) = ?iP (di|d1,..., di?1)The parameters of this probability model arethe P (di|d1,..., di?1).
Generative models are thestandard way to transform a parsing strategyinto a probability model, but note that we arenot assuming any bound on the amount of in-formation from the parse history which mightbe relevant to each parameter.The second probability model is discrimina-tive, because it specifies the conditional proba-bility of the output tree given the input sen-tence.
More generally, discriminative modelstry to maximize this conditional probability, butoften do not actually calculate the probabil-ity, as with Support Vector Machines (Vapnik,1995).
We take the approach of actually calcu-lating an estimate of the conditional probabilitybecause it differs minimally from the generativeprobability model.
In this form, the distinc-tion between our two models is sometimes re-ferred to as ?joint versus conditional?
(John-son, 2001; Klein and Manning, 2002) ratherthan ?generative versus discriminative?
(Ng andJordan, 2002).
As with the generative model,we use the chain rule to decompose the entireconditional probability into a sequence of prob-abilities for individual parser decisions, whereyield(dj ,..., dk) is the sequence of words wi fromthe shift(wi) actions in dj ,..., dk.P (d1,..., dm|yield(d1,..., dm)) =?iP (di|d1,..., di?1, yield(di,..., dm))Note that d1,..., di?1 specifies yield(d1,..., di?1),so it is sufficient to only add yield(di,..., dm) tothe conditional in order for the entire input sen-tence to be included in the conditional.
Wewill refer to the string yield(di,..., dm) as thelookahead string, because it represents all thosewords which have not yet been reached by theparse at the time when decision di is chosen.The parameters of this model differ from thoseof the generative model only in that they in-clude the lookahead string in the conditional.Although maximizing the joint probability isthe same as maximizing the conditional proba-bility, the fact that they have different param-eters means that estimating one can be muchharder than estimating the other.
In general wewould expect that estimating the joint probabil-ity would be harder than estimating the condi-tional probability, because the joint probabilitycontains more information than the conditionalprobability.
In particular, the probability distri-bution over sentences can be derived from thejoint probability distribution, but not from theconditional one.
However, the unbounded na-ture of the parsing problem means that the in-dividual parameters of the discriminative modelare much harder to estimate than those of thegenerative model.The parameters of the discriminative modelinclude an unbounded lookahead string in theconditional.
Because these words have not yetbeen reached by the parse, we cannot assignthem any structure, and thus the estimationprocess has no way of knowing what words inthis string will end up being relevant to the nextdecision it needs to make.
The estimation pro-cess has to guess about the future role of anunbounded number of words, which makes theestimate quite difficult.
In contrast, the param-eters of the generative model only include wordswhich are either already incorporated into thestructure, or are the immediate next word tobe incorporated.
Thus it is relatively easy todetermine the significance of each word.3 Estimating the Parameters with aNeural NetworkThe most challenging problem in estimat-ing P (di|d1,..., di?1, yield(di,..., dm)) andP (di|d1,..., di?1) is that the conditionalsinclude an unbounded amount of information.Both the parse history d1,..., di?1 and thelookahead string yield(di,..., dm) grow withthe length of the sentence.
In order to applystandard probability estimation methods, weuse neural networks to induce finite repre-sentations of both these sequences, which wewill denote h(d1,..., di?1) and l(yield(di,..., dm)),respectively.
The neural network trainingmethods we use try to find representationswhich preserve all the information about thesequences which are relevant to estimating thedesired probabilities.P (di|d1,..., di?1) ?
P (di|h(d1,..., di?1))P (di|d1,..., di?1, yield(di,..., dm)) ?P (di|h(d1,..., di?1), l(yield(di,..., dm)))Of the previous work on using neural net-works for parsing natural language, by far themost empirically successful has been the workusing Simple Synchrony Networks.
Like otherrecurrent network architectures, SSNs computea representation of an unbounded sequence byincrementally computing a representation ofeach prefix of the sequence.
At each position i,representations from earlier in the sequence arecombined with features of the new position i toproduce a vector of real valued features whichrepresent the prefix ending at i.
This repre-sentation is called a hidden representation.
Itis analogous to the hidden state of a HiddenMarkov Model.
As long as the hidden repre-sentation for position i ?
1 is always used tocompute the hidden representation for positioni, any information about the entire sequencecould be passed from hidden representation tohidden representation and be included in thehidden representation of that sequence.
Whenthese representations are then used to estimateprobabilities, this property means that we arenot making any a priori hard independence as-sumptions (although some independence maybe learned from the data).The difference between SSNs and most otherrecurrent neural network architectures is thatSSNs are specifically designed for process-ing structures.
When computing the his-tory representation h(d1,..., di?1), the SSN usesnot only the previous history representationh(d1,..., di?2), but also uses history representa-tions for earlier positions which are particularlyrelevant to choosing the next parser decision di.This relevance is determined by first assigningeach position to a node in the parse tree, namelythe node which is on the top of the parser?sstack when that decision is made.
Then therelevant earlier positions are chosen based onthe structural locality of the current decision?snode to the earlier decisions?
nodes.
In this way,the number of representations which informa-tion needs to pass through in order to flow fromhistory representation i to history representa-tion j is determined by the structural distancebetween i?s node and j?s node, and not just thedistance between i and j in the parse sequence.This provides the neural network with a lin-guistically appropriate inductive bias when itlearns the history representations, as explainedin more detail in (Henderson, 2003b).When computing the lookahead representa-tion l(yield(di,..., dm)), there is no structural in-formation available to tell us which positions aremost relevant to choosing the decision di.
Prox-imity in the string is our only indication of rele-vance.
Therefore we compute l(yield(di,..., dm))by running a recurrent neural network backwardover the string, so that the most recent input isthe first word in the lookahead string, as dis-cussed in more detail in (Henderson, 2003a).Once it has computed h(d1,..., di?1) and (forthe discriminative model) l(yield(di,..., dm)), theSSN uses standard methods (Bishop, 1995) toestimate a probability distribution over the setof possible next decisions di given these repre-sentations.
This involves further decomposingthe distribution over all possible next parser ac-tions into a small hierarchy of conditional prob-abilities, and then using log-linear models toestimate each of these conditional probabilitydistributions.
The input features for these log-linear models are the real-valued vectors com-puted by h(d1,..., di?1) and l(yield(di,..., dm)), asexplained in more detail in (Henderson, 2003b).Thus the full neural network consists of a recur-rent hidden layer for h(d1,..., di?1), (for the dis-criminative model) a recurrent hidden layer forl(yield(di,..., dm)), and an output layer for thelog-linear model.
Training is applied to this fullneural network, as described in the next section.4 Three Optimization Criteria andtheir Training MethodsAs with many other machine learning methods,training a Simple Synchrony Network involvesfirst defining an appropriate learning criteriaand then performing some form of gradient de-scent learning to search for the optimum valuesof the network?s parameters according to thiscriteria.
In all the parsing models investigatedhere, we use the on-line version of Backprop-agation to perform the gradient descent.
Thislearning simultaneously tries to optimize the pa-rameters of the output computation and theparameters of the mappings h(d1,..., di?1) andl(yield(di,..., dm)).
With multi-layered networkssuch as SSNs, this training is not guaranteed toconverge to a global optimum, but in practicea network whose criteria value is close to theoptimum can be found.The three parsing models differ in the crite-ria the neural networks are trained to optimize.Two of the neural networks are trained using thestandard maximum likelihood approach of opti-mizing the same probability which they are esti-mating, one generative and one discriminative.For the generative model, this means maximiz-ing the total joint probability of the parses andthe sentences in the training corpus.
For thediscriminative model, this means maximizingthe conditional probability of the parses in thetraining corpus given the sentences in the train-ing corpus.
To make the computations easier,we actually minimize the negative log of theseprobabilities, which is called cross-entropy er-ror.
Minimizing this error ensures that trainingwill converge to a neural network whose outputsare estimates of the desired probabilities.2 Foreach parse in the training corpus, Backpropaga-tion training involves first computing the proba-bility which the current network assigns to thatparse, then computing the first derivative of(the negative log of) this probability with re-spect to each of the network?s parameters, andthen updating the parameters proportionatelyto this derivative.3The third neural network combines the ad-vantages of the generative probability modelwith the advantages of the discriminative opti-mization criteria.
The structure of the networkand the set of outputs which it computes areexactly the same as the above network for thegenerative model.
But the training procedureis designed to maximize the conditional proba-bility of the parses in the training corpus giventhe sentences in the training corpus.
The con-ditional probability for a sentence can be com-puted from the joint probability of the gener-ative model by normalizing over the set of allparses d?1,..., d?m?
for the sentence.P (d1,..., dm|w1,..., wn) = P (d1,...,dm)?d?1,...,d?m?P (d?1,...,d?m?
)So, with this approach, we need to maximizethis normalized probability, and not the proba-bility computed by the network.The difficulty with this approach is that thereare exponentially many parses for the sentence,so it is not computationally feasible to com-pute them all.
We address this problem byonly computing a small set of the most prob-able parses.
The remainder of the sum is es-timated using a combination of the probabili-ties from the best parses and the probabilities2Cross-entropy error ensures that the minimum of theerror function converges to the desired probabilities asthe amount of training data increases (Bishop, 1995),so the minimum for any given dataset is considered anestimate of the true probabilities.3A number of additional training techniques, such asregularization, are added to this basic procedure, as willbe specified in section 6.from the partial parses which were pruned whensearching for the best parses.
The probabilitiesof pruned parses are estimated in such a wayas to minimize their effect on the training pro-cess.
For each decision which is part of some un-pruned parses, we calculate the average proba-bility of generating the remainder of the sen-tence by these un-pruned parses, and use thisas the estimate for generating the remainder ofthe sentence by the pruned parses.
With thisestimate we can calculate the sum of the prob-abilities for all the pruned parses which origi-nate from that decision.
This approach gives usa slight overestimate of the total sum, but be-cause this total sum acts simply as a weightingfactor, it has little effect on learning.
What isimportant is that this estimate minimizes theeffect of the pruned parses?
probabilities on thepart of the training process which occurs afterthe probabilities of the best parses have beencalculated.After estimating P (d1,..., dm|w1,..., wn), train-ing requires that we estimate the first derivativeof (the negative log of) this probability with re-spect to each of the network?s parameters.
Thecontribution to this derivative of the numera-tor in the above equation is the same as in thegenerative case, just scaled by the denominator.The difference between the two learning meth-ods is that we also need to account for the con-tribution to this derivative of the denominator.Here again we are faced with the problem thatthere are an exponential number of derivationsin the denominator, so here again we approxi-mate this calculation using the most probableparses.To increase the conditional probability of thecorrect parse, we want to decrease the total jointprobabilities of the incorrect parses.
Probabilitymass is only lost from the sum over all parses be-cause shift(wi) actions are only allowed for thecorrect wi.
Thus we can decrease the total jointprobability of the incorrect parses by makingthese parses be worse predictors of the words inthe sentence.4 The combination of training thecorrect parses to be good predictors of the wordsand training the incorrect parses to be bad pre-dictors of the words results in prediction prob-4Non-prediction probability estimates for incorrectparses can make a small contribution to the derivative,but because pruning makes the calculation of this con-tribution inaccurate, we treat this contribution as zerowhen training.
This means that non-prediction outputsare trained to maximize the same criteria as in the gen-erative case.abilities which are not accurate estimates, butwhich are good at discriminating correct parsesfrom incorrect parses.
It is this feature whichgives discriminative training an advantage overgenerative training.
The network does not needto learn an accurate model of the distributionof words.
The network only needs to learn anaccurate model of how words disambiguate pre-vious parsing decisions.When we apply discriminative training onlyto the most probable incorrect parses, we trainthe network to discriminate between the correctparse and those incorrect parses which are themost likely to be mistaken for the correct parse.In this sense our approximate training methodresults in optimizing the decision boundary be-tween correct and incorrect parses, rather thanoptimizing the match to the conditional prob-ability.
Modifying the training method to sys-tematically optimize the decision boundary (asin large margin methods such as Support VectorMachines) is an area of future research.5 Searching for the most probableparseThe complete parsing system uses the probabil-ity estimates computed by the SSN to search forthe most probable parse.
The search incremen-tally constructs partial parses d1,..., di by takinga parse it has already constructed d1,..., di?1 andusing the SSN to estimate a probability distri-bution P (di|d1,..., di?1, ...) over possible next de-cisions di.
These probabilities are then used tocompute the probabilities for d1,..., di.
In gen-eral, the partial parse with the highest proba-bility is chosen as the next one to be extended,but to perform the search efficiently it is nec-essary to prune the search space.
The mainpruning is that only a fixed number of themost probable derivations are allowed to con-tinue past the shifting of each word.
Settingthis post-word beam width to 5 achieves fastparsing with reasonable performance in all mod-els.
For the parsers with generative probabilitymodels, maximum accuracy is achieved with apost-word beam width of 100.6 The ExperimentsWe used the Penn Treebank (Marcus et al,1993) to perform empirical experiments on theproposed parsing models.
In each case the inputto the network is a sequence of tag-word pairs.55We used a publicly available tagger (Ratnaparkhi,1996) to provide the tags.
For each tag, there is anWe report results for three different vocabularysizes, varying in the frequency with which tag-word pairs must occur in the training set in or-der to be included explicitly in the vocabulary.A frequency threshold of 200 resulted in a vo-cabulary of 508 tag-word pairs, a threshold of 20resulted in 4215 tag-word pairs, and a thresholdof 5 resulted in 11,993 tag-word pairsFor the generative model we trained net-works for the 508 (?GSSN-Freq?200?)
and 4215(?GSSN-Freq?20?)
word vocabularies.
Theneed to calculate word predictions makes train-ing times for the 11,993 word vocabulary verylong, and as of this writing no such networktraining has been completed.
The discrimina-tive model does not need to calculate word pre-dictions, so it was feasible to train networks forthe 11,993 word vocabulary (?DSSN-Freq?5?
).Previous results (Henderson, 2003a) indicatethat this vocabulary size performs better thanthe smaller ones, as would be expected.For the networks trained with the discrimi-native optimization criteria and the generativeprobability model, we trained networks for the508 (?DGSSN-Freq?200?)
and 4215 (?DGSSN-Freq?20?)
word vocabularies.
For this train-ing, we need to select a small set of the mostprobable incorrect parses.
When we tried usingonly the network being trained to choose thesetop parses, training times were very long andthe resulting networks did not outperform theirgenerative counterparts.
In the experiments re-ported here, we provided the training with alist of the top 20 parses found by a network ofthe same type which had been trained with thegenerative criteria.
The network being trainedwas then used to choose its top 10 parses fromthis list, and training was performed on these10 parses and the correct parse.6 This reducedthe time necessary to choose the top parses dur-ing training, and helped focus the early stagesof training on learning relevant discriminations.Once the training of these networks was com-plete, we tested both their ability to parse ontheir own and their ability to re-rank the topunknown-word vocabulary item which is used for allthose words which are not sufficiently frequent with thattag to be included individually in the vocabulary (aswell as other words if the unknown-word case itself doesnot have at least 5 instances).
We did no morphologicalanalysis of unknown words.6The 20 candidate parses and the 10 training parseswere found with post-word beam widths of 20 and 10,respectively, so these are only approximations to the topparses.20 parses of their associated generative model(?DGSSN-.
.
., rerank?
).We determined appropriate training param-eters and network size based on intermediatevalidation results and our previous experience.7We trained several networks for each of theGSSN models and chose the best ones based ontheir validation performance.
We then trainedone network for each of the DGSSN modelsand for the DSSN model.
The best post-wordbeam width was determined on the validationset, which was 5 for the DSSN model and 100for the other models.To avoid repeated testing on the standardtesting set, we first compare the different mod-els with their performance on the validation set.Standard measures of accuracy are shown in ta-ble 1.8 The largest accuracy difference is be-tween the parser with the discriminative proba-bility model (DSSN-Freq?5) and those with thegenerative probability model, despite the largervocabulary of the former.
This demonstratesthe difficulty of estimating the parameters of adiscriminative probability model.
There is alsoa clear effect of vocabulary size, but there is aslightly larger effect of training method.
Whentested in the same way as they were trained(for reranking), the parsers which were trainedwith a discriminative criteria achieve a 7% and8% reduction in error rate over their respec-tive parsers with the same generative probabil-ity model.
When tested alone, these DGSSNparsers perform only slightly better than theirrespective GSSN parsers.
Initial experiments ongiving these networks exposure to parses out-side the top 20 parses of the GSSN parsers atthe very end of training did not result in any im-provement on this task.
This suggests that atleast some of the advantage of the DSSN mod-els is due to the fact that re-ranking is a simplertask than parsing from scratch.
But additionalexperimental work would be necessary to makeany definite conclusions about this issue.7All the best networks had 80 hidden units for thehistory representation (and 80 hidden units in the looka-head representation).
Weight decay regularization wasapplied at the beginning of training but reduced to near0 by the end of training.
Training was stopped whenmaximum performance was reached on the validationset, using a post-word beam width of 5.8All our results are computed with the evalb pro-gram following the standard criteria in (Collins, 1999),and using the standard training (sections 2?22, 39,832sentences, 910,196 words), validation (section 24, 1346sentence, 31507 words), and testing (section 23, 2416sentences, 54268 words) sets (Collins, 1999).LR LP F?=1?DSSN-Freq?5 84.9 86.0 85.5GSSN-Freq?200 87.6 88.9 88.2DGSSN-Freq?200 87.8 88.8 88.3GSSN-Freq?20 88.2 89.3 88.8DGSSN-Freq?200, rerank 88.5 89.6 89.0DGSSN-Freq?20 88.5 89.7 89.1DGSSN-Freq?20, rerank 89.0 90.3 89.6Table 1: Percentage labeled constituent recall(LR), precision (LP), and a combination of both(F?=1) on validation set sentences of length atmost 100.LR LP F?=1?Ratnaparkhi99 86.3 87.5 86.9Collins99 88.1 88.3 88.2Collins&Duffy02 88.6 88.9 88.7Charniak00 89.6 89.5 89.5Collins00 89.6 89.9 89.7DGSSN-Freq?20, rerank 89.8 90.4 90.1Bod03 90.7 90.8 90.7* F?=1 for previous models may have rounding errors.Table 2: Percentage labeled constituent recall(LR), precision (LP), and a combination of both(F?=1) on the entire testing set.For comparison to previous results, table 2lists the results for our best model (DGSSN-Freq?20, rerank)9 and several other statisti-cal parsers (Ratnaparkhi, 1999; Collins, 1999;Collins and Duffy, 2002; Charniak, 2000;Collins, 2000; Bod, 2003) on the entire testingset.
Our best performing model is more accu-rate than all these previous models except (Bod,2003).
This DGSSN parser achieves this resultusing much less lexical knowledge than other ap-proaches, which mostly use at least the wordswhich occur at least 5 times, plus morphologicalfeatures of the remaining words.
However, thefact that the DGSSN uses a large-vocabularytagger (Ratnaparkhi, 1996) as a preprocessingstage may compensate for its smaller vocabu-lary.
Also, the main reason for using a smallervocabulary is the computational complexity ofcomputing probabilities for the shift(wi) actionson-line, which other models do not require.9On sentences of length at most 40, the DGSSN-Freq?20-rerank model gets 90.1% recall and 90.7% pre-cision.7 Related WorkJohnson (2001) investigated similar issues forparsing and tagging.
His maximal conditionallikelihood estimate for a PCFG takes the sameapproach as our generative model trained witha discriminative criteria.
While he shows anon-significant increase in performance over thestandard maximal joint likelihood estimate ona small dataset, because he did not have a com-putationally efficient way to train this model,he was not able to test it on the standarddatasets.
The other models he investigates con-flate changes in the probability models withchanges in the training criteria, and the discrim-inative probability models do worse.In the context of part-of-speech tagging,Klein and Manning (2002) argue for the samedistinctions made here between discriminativemodels and discriminative training criteria, andcome to the same conclusions.
However, theirarguments are made in terms of independenceassumptions.
Our results show that these gen-eralizations also apply to methods which do notrely on independence assumptions.While both (Johnson, 2001) and (Klein andManning, 2002) propose models which use theparameters of the generative model but trainto optimize a discriminative criteria, neitherproposes training algorithms which are com-putationally tractable enough to be used forbroad coverage parsing.
Our proposed trainingmethod succeeds in being both tractable andeffective, demonstrating both a significant im-provement over the equivalent generative modeland state-of-the-art accuracy.Collins (2000) and Collins and Duffy (2002)also succeed in finding algorithms for trainingdiscriminative models which balance tractabil-ity with effectiveness, showing improvementsover a generative model.
Both these methodsare limited to reranking the output of anotherparser, while our trained parser can be usedalone.
Neither of these methods use the param-eters of a generative probability model, whichmight explain our better performance (see ta-ble 2).8 ConclusionsThis article has investigated the application ofdiscriminative methods to broad coverage nat-ural language parsing.
We distinguish betweentwo different ways to apply discriminative meth-ods, one where the probability model is changedto a discriminative one, and one where theprobability model remains generative but thetraining method optimizes a discriminative cri-teria.
We find that the discriminative proba-bility model is much worse than the generativeone, but that training to optimize the discrimi-native criteria results in improved performance.Performance of the latter model on the stan-dard test set achieves 90.1% F-measure on con-stituents, which is the second best current ac-curacy level, and only 0.6% below the currentbest (Bod, 2003).This paper has also proposed a neural net-work training method which optimizes a dis-criminative criteria even when the parametersbeing estimated are those of a generative prob-ability model.
This training method success-fully satisfies the conflicting constraints that itbe computationally tractable and that it be agood approximation to the theoretically optimalmethod.
This approach contrasts with previousapproaches to scaling up discriminative meth-ods to broad coverage natural language pars-ing, which have parameterizations which departsubstantially from the successful previous gen-erative models of parsing.ReferencesChristopher M. Bishop.
1995.
Neural Networksfor Pattern Recognition.
Oxford UniversityPress, Oxford, UK.Rens Bod.
2003.
An efficient implementation ofa new DOP model.
In Proc.
10th Conf.
of Eu-ropean Chapter of the Association for Com-putational Linguistics, Budapest, Hungary.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proc.
1st Meeting of NorthAmerican Chapter of Association for Compu-tational Linguistics, pages 132?139, Seattle,Washington.Michael Collins and Nigel Duffy.
2002.
Newranking algorithms for parsing and tagging:Kernels over discrete structures and the votedperceptron.
In Proc.
35th Meeting of Asso-ciation for Computational Linguistics, pages263?270.Michael Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania, Philadel-phia, PA.Michael Collins.
2000.
Discriminative rerank-ing for natural language parsing.
In Proc.17th Int.
Conf.
on Machine Learning, pages175?182, Stanford, CA.James Henderson.
2003a.
Generative ver-sus discriminative models for statistical left-corner parsing.
In Proc.
8th Int.
Workshop onParsing Technologies, pages 115?126, Nancy,France.James Henderson.
2003b.
Inducing historyrepresentations for broad coverage statisti-cal parsing.
In Proc.
joint meeting of NorthAmerican Chapter of the Association forComputational Linguistics and the HumanLanguage Technology Conf., pages 103?110,Edmonton, Canada.Mark Johnson.
2001.
Joint and conditional es-timation of tagging and parsing models.
InProc.
39th Meeting of Association for Compu-tational Linguistics, pages 314?321, Toulouse,France.Dan Klein and Christopher D. Manning.
2002.Conditional structure versus conditional es-timation in NLP models.
In Proc.
Conf.
onEmpirical Methods in Natural Language Pro-cessing, pages 9?16, Univ.
of Pennsylvania,PA.Peter Lane and James Henderson.
2001.
In-cremental syntactic parsing of natural lan-guage corpora with Simple Synchrony Net-works.
IEEE Transactions on Knowledge andData Engineering, 13(2):219?231.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.A.
Y. Ng and M. I. Jordan.
2002.
On discrim-inative vs. generative classifiers: A compari-son of logistic regression and naive bayes.
InT.
G. Dietterich, S. Becker, and Z. Ghahra-mani, editors, Advances in Neural Informa-tion Processing Systems 14, Cambridge, MA.MIT Press.Adwait Ratnaparkhi.
1996.
A maximum en-tropy model for part-of-speech tagging.
InProc.
Conf.
on Empirical Methods in NaturalLanguage Processing, pages 133?142, Univ.
ofPennsylvania, PA.Adwait Ratnaparkhi.
1999.
Learning to parsenatural language with maximum entropymodels.
Machine Learning, 34:151?175.D.J.
Rosenkrantz and P.M. Lewis.
1970.
De-terministic left corner parsing.
In Proc.
11thSymposium on Switching and Automata The-ory, pages 139?152.Vladimir N. Vapnik.
1995.
The Nature ofStatistical Learning Theory.
Springer-Verlag,New York.
