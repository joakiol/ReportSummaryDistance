Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 473?481,Sydney, July 2006. c?2006 Association for Computational LinguisticsBoosting Unsupervised Relation Extraction by Using NERRonen FeldmanComputer Science DepartmentBar-Ilan UniversityRamat-Gan, ISRAELfeldman@cs.biu.ac.ilBenjamin RosenfeldComputer Science DepartmentBar-Ilan UniversityRamat-Gan, ISRAELgrurgrur@gmail.comAbstractWeb extraction systems attempt to usethe immense amount of unlabeled textin the Web in order to create large listsof entities and relations.
Unliketraditional IE methods, the Webextraction systems do not label everymention of the target entity or relation,instead focusing on extracting as manydifferent instances as possible whilekeeping the precision of the resultinglist reasonably high.
URES is a Webrelation extraction system that learnspowerful extraction patterns fromunlabeled text, using short descriptionsof the target relations and theirattributes.
The performance of URES isfurther enhanced by classifying itsoutput instances using the properties ofthe extracted patterns.
The features weuse for classification and the trainedclassification model are independentfrom the target relation, which wedemonstrate in a series of experiments.In this paper we show how theintroduction of a simple rule basedNER can boost the performance ofURES on a variety of relations.
Wealso compare the performance ofURES to the performance of the state-of-the-art KnowItAll system, and to theperformance of its pattern learningcomponent, which uses a simpler andless powerful pattern language thanURES.1   IntroductionInformation Extraction (IE) (Riloff 1993;Cowie and Lehnert 1996; Grishman 1996;Grishman 1997; Kushmerick, Weld et al 1997;Freitag 1998; Freitag and McCallum 1999;Soderland 1999)  is the task of extractingfactual assertions from text.Most IE systems rely on knowledgeengineering or on machine learning to generateextraction patterns ?
the mechanism thatextracts entities and relation instances fromtext.
In the machine learning approach, adomain expert labels instances of the targetrelations in a set of documents.
The systemthen learns extraction patterns, which can beapplied to new documents automatically.Both approaches require substantial humaneffort, particularly when applied to the broadrange of documents, entities, and relations onthe Web.
In order to minimize the manualeffort necessary to build Web IE systems, wehave designed and implemented URES(Unsupervised Relation Extraction System).URES takes as input the names of the targetrelations and the types of their arguments.
Itthen uses a large set of unlabeled documentsdownloaded from the Web in order to learn theextraction patterns.URES is most closely related to theKnowItAll system developed at University ofWashington by Oren Etzioni and colleagues(Etzioni, Cafarella et al 2005), since both areunsupervised and both leverage relation-independent extraction patterns toautomatically generate seeds, which are thenfed into a pattern-learning component.KnowItAll is based on the observation that theWeb corpus is highly redundant.
Thus, itsselective, high-precision extraction patternsreadily ignore most sentences, and focus onsentences that indicate the presence of relationinstances with very high probability.In contrast, URES is based on theobservation that, for many relations, the Webcorpus has limited redundancy, particularlywhen one is concerned with less prominentinstances of these relations (e.g., theacquisition of Austria Tabak).
Thus, URESutilizes a more expressive extraction patternlanguage, which enables it to extractinformation from a broader set of sentences.URES relies on a sophisticated mechanism to473assess its confidence in each extraction,enabling it to sort extracted instances, therebyimproving its recall without sacrificingprecision.Our main contributions are as follows:?
We introduce the first domain-independent system to extract relationinstances from the Web with both highprecision and high recall.?
We show how to minimize the humaneffort necessary to deploy URES foran arbitrary set of relations, includingautomatically generating and labelingpositive and negative examples of therelation.?
We show how we can integrate asimple NER component into theclassification scheme of URES inorder to boost recall between 5-15%for similar precision levels.?
We report on an experimentalcomparison between URES, URES-NER and the state-of-the-artKnowItAll system, and show thatURES can double or even triple therecall achieved by KnowItAll forrelatively rare relation instances.The rest of the paper is organized asfollows:  Section 2 describes previous work.Section 3 outlines the general design principlesof URES, its architecture, and then describeseach URES component in detail.
Section 4presents our experimental evaluation.
Section5 contains conclusions and directions for futurework.2   Related WorkThe IE systems most similar to URES arebased on bootstrap learning: MutualBootstrapping (Riloff and Jones 1999), theDIPRE system (Brin 1998), and the Snowballsystem (Agichtein and Gravano 2000 ).
(Ravichandran and Hovy 2002) also usebootstrapping, and learn simple surfacepatterns for extracting binary relations from theWeb.Unlike those unsupervised IE systems,URES patterns allow gaps that can be matchedby any sequences of tokens.
This makes URESpatterns much more general, and allows torecognize instances in sentences inaccessibleto the simple surface patterns of systems suchas (Brin 1998; Riloff and Jones 1999;Ravichandran and Hovy 2002).
The greaterpower of URES requires different and morecomplex methods for learning, scoring, andfiltering of patterns.Another direction for unsupervised relationlearning was taken in (Hasegawa, Sekine et al2004; Chen, Ji et al 2005).
These systems usea NER system to identify pairs of entities andthen cluster them based on the types of theentities and the words appearing between theentities.
Only pairs that appear at least 30 timeswere considered.
The main benefit of thisapproach is that all relations between twoentity types can be discovered simultaneouslyand there is no need for the user to supply therelations definitions.
Such a system could havebeen used as a preliminary step to URES,however its relatively low precision makes itunfeasible.
Unlike URES, the evaluationsperformed in these papers ignored errors thatwere introduced by the underlying NERcomponent.
The precision reported by thesesystems (77% breakeven for the COM-COMdomain) is inferior to that of URES.We compared our results directly to twoother unsupervised extraction systems, theSnowball (Agichtein and Gravano 2000 ) andKnowItAll.
Snowball is an unsupervisedsystem for learning relations from documentcollections.
The system takes as input a set ofseed examples for each relation, and uses aclustering technique to learn patterns from theseed examples.
It does rely on a full fledgedNamed Entity Recognition system.
Snowballachieved fairly low precision figures (30-50%)on relations such as Merger and Acquisition onthe same dataset we used in our experiments.KnowItAll is a system developed atUniversity of Washington by Oren Etzioni andcolleagues (Etzioni, Cafarella et al 2005).
Weshall now briefly describe it and its patternlearning component.Brief description of KnowItAllKnowItAll uses a set of generic extractionpatterns, and automatically instantiates rules bycombining those patterns with user suppliedrelation labels.
For example, KnowItAll haspatterns for a generic ?of?
relation:NP1  <relation>  NP2NP1  's  <relation>  ,  NP2NP2  ,  <relation>  of  NP1474where NP1 and NP2 are simple noun phrasesthat extract values of attribute1 and attribute2of a relation, and <relation> is a user-suppliedstring associated with the relation.
The rulesmay also constrain NP1 and NP2 to be propernouns.The rules have alternating context strings(exact string match) and extraction slots(typically an NP or head of an NP).
Each rulehas an associated query used to automaticallyfind candidate sentences from a Web searchengine.KnowItAll also includes mechanisms tocontrol the amount of search, to mergeredundant extractions, and to assign aprobability to each extraction based onfrequency of extraction or on Web statistics(Downey, Etzioni et al 2004).KnowItAll-PL.
While those generic ruleslead to high precision extraction, they tend tohave low recall, due to the wide variety ofcontexts describing a relation.
KnowItAllincludes a simple pattern learning scheme(KnowItAll-PL) that builds on the genericextraction mechanism (KnowItAll-baseline).Like URES, this is a self-supervised methodthat bootstraps from seeds that areautomatically extracted by the baseline system.KnowItAll-PL creates a set of positivetraining sentences by downloading sentencesthat contain both argument values of a seedtuple and also the relation label.
Negativetraining is created by downloading sentenceswith only one of the seed argument values, andconsidering a nearby NP as the other argumentvalue.
This does not guarantee that thenegative example will actually be false, butworks well in practice.Rule induction tabulates the occurrence ofcontext tokens surrounding the argumentvalues of the positive training sentences.
Eachcandidate extraction pattern has a left contextof zero to k tokens immediately to the left ofthe first argument, a middle context of alltokens between the two arguments, and a rightcontext of zero to k tokens immediately to theright of the second argument.
A pattern can begeneralized by dropping the furthest termsfrom the left or right context.
KnowItAll-PLretains the most general version of each patternthat has training frequency over a thresholdand training precision over a threshold.3   Description of URESThe goal of URES is extracting instances ofrelations from the Web without humansupervision.
Accordingly, the input of thesystem is limited to (reasonably short)definition of the target relations (composed ofthe relation's schema and a few keywords thatenable gathering relevant sentences).
Forexample, this is the description of theacquisition relation:Acquisition(ProperNP, ProperNP) orderedkeywords={"acquired" "acquisition"}The word ordered indicates that Acquisitionis not a symmetric relation and the order of itsarguments matters.
The ProperNP tokensindicate the types of the attributes.
In theregular mode, there are only two possibleattribute types ?
ProperNP and CommonNP,meaning proper and common noun phrases,respectively.
When using the NER Filtercomponent described in the section 4.1 weallow further subtypes of ProperNP, and thepredicate definition becomes:acquisition(Company, Company) ?The keywords are used for gatheringsentences from the Web and for instantiatingthe generic patterns for seeds generation.Additional keywords (such as ?acquire?,?purchased?, ?hostile takeover?, etc), whichcan be used for gathering more sentences, areadded automatically by using WordNet [18].URES consists of several largelyindependent components; their layout is shownon the Figure 1.
The Sentence Gatherergenerates (e.g., downloads from the Web) alarge set of sentences that may contain targetinstances.
The Seeds Generator, which isessentially equal to the KnowItAll-baselinesystem, uses a small set of generic patternsinstantiated with the predicate keywords toextract a small set of high-confidence instancesof the target relations.
The Pattern Learner usesthe seeds to learn likely patterns of relationoccurrences.
Then, the Instance Extractor usesthe patterns to extracts the instances from thesentences.
Those instances can be filtered by aNER Filter, which is an optional part of thesystem.
Finally, the Classifier assigns theconfidence score to each extraction.475SentenceGathererInput:Target RelationsDefinitionsWebSentenceskeywordsPatternLearnerInstanceExtractorOutput:ExtractionsSeedsGeneratorseedspatternsNER Filter(optional)instancesClassifierFigure 1.
The architecture of URES3.1  Pattern LearnerThe task of the Pattern Learner is to learn thepatterns of occurrence of relation instances.This is an inherently supervised task, becauseat least some occurrences must be known inorder to be able to find patterns among them.Consequently, the input to the Pattern Learnerincludes a small set (10 instances in ourexperiments) of known instances for eachtarget relation.
Our system assumes that theseeds are a part of the target relation definition.However, the set of seeds need not be createdmanually.
Instead, the seeds can be takenautomatically from the top-scoring results of ahigh-precision low-recall unsupervisedextraction system, such as KnowItAll.
Theseeds for our experiments were produced inexactly this way: we used two generic patternsinstantiated with the relation name andkeywords.
Those patterns have a relativelyhigh precision (although low recall), and thetop-confidence results, which are the onesextracted many times from different sentences,have close to 100% probability of beingcorrect.The Pattern Learner proceeds as follows:first, the gathered sentences that contain theseed instances are used to generate the positiveand negative sets.
From those sets the patternsare learned.
Finally, the patterns are post-processed and filtered.
We shall now describethose steps in detail.PREPARING THE POSITIVE AND NEGATIVESETSThe positive set of a predicate (the termspredicate and relation are interchangeable inour work) consists of sentences that contain aknown instance of the predicate, with theinstance attributes changed to ?<AttrN>?,where N is the attribute index.
For example,assuming there is a seed instanceAcquisition(Oracle, PeopleSoft), the sentenceThe Antitrust Division of the U.S. Department ofJustice evaluated the likely competitive effects ofOracle's proposed acquisition of PeopleSoft.will be changed toThe Antitrust Division?
?of <Attr1>'s proposedacquisition of <Attr2>.The positive set of a predicate P is generatedstraightforwardly, using substring search.
Thenegative set of a predicate consists ofsentences with known false instances of thepredicate similarly marked (with <AttrN>substituted for attributes).
The negative set isused by the pattern learner during the scoringand filtering step, to filter out the patterns thatare overly general.
We generate the negativeset from the sentences in the positive set by476changing the assignment of one or bothattributes to other suitable entities in thesentence.
In the shallow parser based mode ofoperation, any suitable noun phrase can beassigned to an attribute.GENERATING THE PATTERNSThe patterns for the predicate P aregeneralizations of pairs of sentences from thepositive set of P. The function Generalize(s1,s2)  is applied to each pair of sentences s1 ands2 from the positive set of the predicate.
Thefunction generates a pattern that is the best(according to the objective function definedbelow) generalization of its two arguments.The following pseudocode shows theprocess of generating the patterns for thepredicate P:For each pair s1, s2 from PositiveSet(P)Let Pattern = Generalize(s1, s2).Add Pattern to PatternsSet(P).The patterns are sequences of tokens, skips(denoted *), limited skips (denoted *?)
andslots.
The tokens can match only themselves,the skips match zero or more arbitrary tokens,and slots match instance attributes.
Thelimited skips match zero or more arbitrarytokens, which must not belong to entities of thetypes equal to the types of the predicateattributes.
In the shallow parser based mode,there are only two different entity types ?ProperNP and CommonNP, standing forproper and common noun phrases.The Generalize(s1, s2) function takes twosentences and generates the least (mostspecific) common generalization of both.
Thefunction does a dynamical programmingsearch for the best match between the twopatterns (Optimal String Alignment algorithm),with the cost of the match defined as the sumof costs of matches for all elements.
The exactcosts of matching elements are not importantas long as their relative order is maintained.We use the following numbers:  two identicalelements match at cost 0, a token matches askip or an empty space at cost 10, a skipmatches an empty space at cost 2, and differentkinds of skip match at cost 3.
All othercombinations have infinite cost.
After the bestmatch is found, it is converted into a pattern bycopying matched identical elements andadding skips where non-identical elements arematched.
For example, assume the sentencesareToward this end, <Attr1> in July acquired<Attr2>Earlier this year, <Attr1> acquired <Attr2> fromXAfter the dynamic programming-basedsearch, the following match will be found:Toward (cost 10)Earlier   (cost 10)this this (cost 0)end (cost 10)year (cost 10), , (cost 0)<Attr1 > <Attr1 > (cost 0)in   July (cost 20)acquired acquired (cost 0)<Attr2 > <Attr2 > (cost 0)from (cost 10)X (cost 10)at total cost = 80.
Assuming that ?X?belongs to the same type as at least one of theattributes while the other tokens are notentities, the match will be converted to thepattern*?
this  *?
,  <Attr1>  *?
acquired  <Attr2>*3.2  Classifying the ExtractionsThe goal of the final classification stage is tofilter the list of all extracted instances, keepingthe correct extractions and removing mistakesthat would always occur regardless of thequality of the patterns.
It is of courseimpossible to know which extractions arecorrect, but there exist properties of patternsand pattern matches that increase or decreasethe confidence in the extractions that theyproduce.
Thus, instead of a binary classifier,we seek a real-valued confidence function c,mapping the set of extracted instances into the[0, 1] segment.Since confidence value depends on theproperties of particular sentences and patterns,it is more properly defined over the set ofsingle pattern matches.
Then, the overallconfidence of an instance is the maximum ofthe confidence values of the matches thatproduce the instance.Assume that an instance E was extractedfrom a match of a pattern P at a sentence S.477The following set of binary features mayinfluence the confidence c(E, P, S):f1(E, P, S) = 1,  if the number of sentencesproducing E  is greater than one.f2(E, P, S) = 1,  if the number of sentencesproducing E is greater than two.f3(E, P, S) = 1,  if at least one slot of the pattern P isadjacent to a non-stop-word token.f4(E, P, S) = 1,  if both slots of the pattern P areadjacent to non-stop-word tokens.f5?f9(E, P, S)  = 1,  if the number of nonstopwords in P is 0 (f5), 1 or greater (f6),2 or greater (f7), 3 or greater (f8), and4 or greater (f9).f10?f15(E, P, S)  = 1, if the number of wordsbetween the slots of the match Mthat were matched to skips of thepattern P is 0 (f10), 1 or less (f11), 2or less (f12) , 3 or less(f13),  5 or less(f14), and 10 or less (f15).Utilizing the NERIn the URES-NER version the entities of eachcandidate instance are passed through a simplerule-based NER filter, which attaches a score(?yes?, ?maybe?, or ?no?)
to the argument(s)and optionally fixes the arguments boundaries.The NER is capable of identifying entities oftype PERSON and COMPANY (and can beextended to identify additional types).The scores mean:?yes?
?
the argument is of the correctentity type.?no?
?
the argument is not of the rightentity type, and hencethe candidate instance should beremoved.?maybe?
?
the argument type is uncertain,can be eithercorrect or no.If ?no?
is returned for one of the arguments,the instance is removed.
Otherwise, anadditional binary feature is added to theinstance's vector:f16 = 1 iff the score for both arguments is?yes?.For bound predicates, only the secondargument is analyzed, naturally.As can be seen, the set of features above issmall, and is not specific to any particularpredicate.
This allows us to train a model usinga small amount of labeled data for onepredicate, and then use the model for all otherpredicates:Training: The patterns for a single modelpredicate are run over a relatively small set ofsentences (3,000-10,000 sentences in ourexperiments), producing a set of extractions(between 150-300 extractions in ourexperiments).The extractions are manually labeledaccording to whether they are correct or not.For each pattern match Mk = (Ek, Pk, Sk), thevalue of the feature vector fk = (f1(Mk), ?,f15(Mk)) is calculated, and the label Lk = ?1is set according to whether the extraction Ek iscorrect or no.A regression model estimating the functionL(f) is built from the training data {(fk, Lk)}.For our classifier we used the BBR (Genkin,Lewis et al 2004), but other models, such asSVM or NaiveBayes are of course alsopossible.Confidence estimation: For each patternmatch M, its score L(f(M)) is calculated by thetrained regression model.
Note that we do notthreshold the value of L, instead using the rawprobability value between zero and one.The final confidence estimates c(E) for theextraction E is set to the maximum of L(f(M))over all matches M that produced E.4   Experimental EvaluationOur experiments aim to answer threequestions:1.
Can we train URES?s classifier once, andthen use the results on all other relations?2.
What boost will we get by introducing asimple NER into the classification scheme ofURES?3.
How does URES?s performance comparewith KnowItAll and KnowItAll-PL?Our experiments utilized five relations:Acquisition(BuyerCompany,AcquiredCompany),Merger(Company1, Company2),CEO_Of(Company, Person),MayorOf(City, Person),InventorOf(Person, Invention).Merger is a symmetric predicate, in thesense that the order of its attributes does notmatter.
Acquisition is antisymmetric, and theother three are tested as bound in the first478attribute.
For the bound predicates, we are onlyinterested in the instances with particularprespecified values of the first attribute.
TheInvention attribute of the InventorOf predicateis of type CommonNP.
All other attributes areof type ProperName.The data for the experiments were collectedby the KnowItAll crawler.
The data for theAcquisition and Merger predicates consist ofabout 900,000 sentences for each of the twopredicates, where each sentence contains atleast one predicate keyword.
The data for thebounded predicates consist of sentences thatcontain a predicate keyword and one of ahundred values of the first (bound) attribute.Half of the hundred are frequent entities(>100,000 search engine hits), and another halfare rare (<10,000 hits).The pattern learning for each of thepredicates was performed using the wholecorpus of sentences for the predicate.
Fortesting the precision of each of the predicatesin each of the systems we manually evaluatedsets of 200 instances that were randomlyselected out of the full set of instancesextracted from the whole corpus.In the first experiment, we test theperformance of the classification componentusing different predicates for building themodel.
In the second experiment we evaluatethe full system over the whole dataset.4.1  Cross-Predicate ClassificationPerformanceIn this experiment we test whether the choiceof the model predicate for training theclassifier is significant.The pattern learning for each of thepredicates was performed using the wholecorpus of sentences for the predicate.
Fortesting we used a small random selection ofsentences, run the Instance Extractor overthem, and manually evaluated each extractedinstance.
The results of the evaluation forAcquisition, CEO_Of, and Merger aresummarized in Figure 2.
As can be seen, usingany of the predicates as the model producessimilar results.
The graphs for the other twopredicates are similar.
We have used only thefirst 15 features, as the NER-based feature (f16)is predicate-dependent.Acquisition0.70.750.80.850.90.9510 50 100 150PrecisionCEO_Of0 50 100 150 200 250Extractions countMerger0 50 100 150 200 250Acq.CEOInventorMayorMergerFigure 2.
Cross-predicate classification performance results.
Each graph shows the five precision-recall curves produced byusing the five different model predicates.
As can be seen, the curves on each graph are very similar.479CeoOf0.600.650.700.750.800.850.900.951.000 50 100 150 200 250 300Correct ExtractionsPrecisionKIA KIA-PL URES U_NERInventorOf0.600.650.700.750.800.850.900.951.000 200 400 600 800 1,000 1,200Correct ExtractionsPrecisionKIA KIA-PL URESAcquisition0.600.650.700.750.800.850.900.951.000 2,000 4,000 6,000 8,000 10,000Correct ExtractionsPrecisionKIA KIA-PL URES U_NERMerger0.600.650.700.750.800.850.900.951.000 2,000 4,000 6,000 8,000 10,000 12,000 14,000Correct ExtractionsPrecisionKIA KIA-PL URES U_NERFigure 3.
Comparision between URES, URES-NER, KnowItAll-baseline, and KnowItAll-PL.4.2  Performance of the whole systemIn this experiment we compare theperformance of URES with classification to theperformance of KnowItAll.
To carry out theexperiments, we used extraction data kindlyprovided by the KnowItAll group.
Theyprovided us with the extractions obtained bythe KnowItAll system and by its patternlearning component (KnowItAll-PL).
Both aresketched in Section 2.1 and are described indetail in (Etzioni, Cafarella et al 2005).In this experiment we used Acquisition asthe model predicate for testing all otherpredicates except itself.
For testingAcquisition we used CEO_Of as the modelpredicate.
The results are summarized in thefive graphs in the Figure 3.For three relations (Acquisition, Merger, andInventorOf) URES clearly outperformsKnowItAll.
Yet for the other two (CEO_Ofand MayorOf), the simpler method ofKnowItAll-PL or even the KnowItAll-baselinedo as well as URES.
Close inspection revealsthat the key difference is the amount ofredundancy of instances of those relations inthe data.
Instances of CEO_Of and MayorOfare mentioned frequently in a wide variety ofsentences whereas instances of the otherrelations are relatively infrequent.KnowItAll extraction works well whenredundancy is high and most instances have agood chance of appearing in simple forms thatKnowItAll is able to recognize.
The additionalmachinery in URES is necessary whenredundancy is low.
Specifically, URES is moreeffective in identifying low-frequencyinstances, due to its more expressive rulerepresentation, and its classifier that inhibitsthose rules from overgeneralizing.In the same graphs we can see that URES-NER outperforms URES by 5-15% in recallfor similar precision levels.
We can also seethat for Person-based predicates theimprovement is much more pronounced,because Person is a much simpler entity torecognize.
Since in the InventorOf predicatethe 2nd attribute is of type CommonNP, theNER component adds no value and URES-NER and URES results are identical for thispredicate.4805   ConclusionsWe have presented the URES system forautonomously extracting relations from theWeb.
We showed how to improve theprecision of the system by classifying theextracted instances using the properties of thepatterns and sentences that generated theinstances and how to utilize a simple NERcomponent.
The cross-predicate tests showedthat classifier that performs well for allrelations can be built using a small amount oflabeled data for any particular relation.
Weperformed an experimental comparisonbetween URES, URES-NER and the state-of-the-art KnowItAll system, and showed thatURES can double or even triple the recallachieved by KnowItAll for relatively rarerelation instances, and get an additional 5-15%boost in recall by utilizing a simple NER.
Inparticular we have shown that URES is moreeffective in identifying low-frequencyinstances, due to its more expressive rulerepresentation, and its classifier (augmented byNER) that inhibits those rules fromovergeneralizing.ReferencesAgichtein, E. and L. Gravano (2000 ).
Snowball:Extracting Relations from Large Plain-TextCollections.
Proceedings of the 5th ACMInternational Conference on Digital Libraries(DL).Brin, S. (1998).
Extracting Patterns and Relationsfrom the World Wide Web.
WebDB Workshop at6th International Conference on ExtendingDatabase Technology, EDBT?98, Valencia,Spain.Chen, J., D. Ji, et al (2005).
Unsupervised FeatureSelection for Relation Extraction IJCNLP-05, JejuIsland, Korea.Cowie, J. and W. Lehnert (1996).
"InformationExtraction."
Communications of the Associationof Computing Machinery 39(1): 80-91.Downey, D., O. Etzioni, et al (2004).
LearningText Patterns for Web Information Extraction andAssessment (Extended Version).
TechnicalReport UW-CSE-04-05-01.Etzioni, O., M. Cafarella, et al (2005).
"Unsupervised named-entity extraction from theWeb: An experimental study."
ArtificialIntelligence 165(1): 91-134.Freitag, D. (1998).
Machine Learning forInformation Extraction in Informal Domains.Computer Science Department.
Pittsburgh, PA,Carnegie Mellon University: 188.Freitag, D. and A. K. McCallum (1999).Information extraction with HMMs andshrinkage.
Proceedings of the AAAI-99Workshop on Machine Learning for InformationExtraction.Genkin, A., D. D. Lewis, et al (2004).
Large-ScaleBayesian Logistic Regression for TextCategorization.
New Brunswick, NJ, DIMACS:1-41.Grishman, R. (1996).
The role of syntax inInformation Extraction.
Advances in TextProcessing: Tipster Program Phase II, MorganKaufmann.Grishman, R. (1997).
Information Extraction:Techniques and Challenges.
SCIE: 10-27.Hasegawa, T., S. Sekine, et al (2004).
DiscoveringRelations among Named Entities from LargeCorpora.
ACL 2004.Kushmerick, N., D. S. Weld, et al (1997).
WrapperInduction for Information Extraction.
IJCAI-97:729-737.Ravichandran, D. and E. Hovy (2002).
LearningSurface Text Patterns for a Question AnsweringSystem.
40th ACL Conference.Riloff, E. (1993).
Automatically Constructing aDictionary for Information Extraction Tasks.AAAI-93.Riloff, E. and R. Jones (1999).
LearningDictionaries for Information Extraction by Multi-level Boot-strapping.
AAAI-99.Soderland, S. (1999).
"Learning InformationExtraction Rules for Semi-Structured and FreeText."
Machine Learning 34(1-3): 233-272.481
