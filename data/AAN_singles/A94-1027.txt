A Probab i l i s t i c  Mode l  for Text  Categor i za t ion :Based  on  a S ing le  Random Var iab le  w i th  Mu l t ip le  Va luesMakoto IWAYAMAAdvanced Research LaboratoryHitachi Ltd.HATOYAMASAITAMA 350-03, JAPANiwayama?har l ,  h i t  achi .
co. j pAbst ractText categorization is the classification ofdocuments with respect to a set of prede-fined categories.
In this paper, we proposea new probabilistic model for text catego-rization, that is based on a Single randomVariable with Multiple Values (SVMV).
Com-pared to previous probabilistic models, ourmodel has the following advantages; 1) itconsiders within-document term frequencies,2) considers term weighting for target docu-ments, and 3) is less affected by having insuf-ficient training cases.
We verify our model'ssuperiority over the others in the task of cat-egorizing news articles from the "Wall StreetJournal".1 IntroductionText categorization is the classification of documentswith respect o a set of predefined categories.
As anexample, let us take a look at the following article fromthe "Wall Street Journal" (1989/11/2).McDermott International, Inc. said its Bab-cock & Wilcox unit completed the sale of itsBailey Controls Operations to FinmeccanicaS.p.A for $295 million.
Finmeccanica is anItalian state-owned holding company with in-terests in the mechanical engineering indus-try.
Bailey Controls, based in Wickliffe, Ohio,makes computerized industrial controls sys-tems.
It employs 2,700 people and has annualrevenue of about $370 million.Two categories (topics) are manually assigned tothis article; "TENDER OFFERS, MERGERS, ACQUISI-TIONS (TNM)" and "COMPUTERS AND INFORMA-TION TECHNOLOGY (CPK)."
While there may be cer-tain rules or standards for categorization, it is verydifficult for human experts to assign categories con-sistently and efficiently to large numbers of daily in-coming documents.
The purpose of this paper is topropose a new probabilistic model for automatic textcategorization.Takenobu TOKUNAGADepartment of Computer ScienceTokyo Institute of Technology2-12-1, OOKAYAMA, MEGURO-KUTOKYO 152, JAPANt ake@cs,  t i t  ech.
ac.
jpWhile many text categorization models have beenproposed so far, in this paper, we concentrate onthe probabilistic models (Robertson and Sparck Jones,1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft,1981; Wong and Yao, 1989; Yu et al, 1989) becausethese models have solid formal grounding in probabil-ity theory.
Section 2 quickly reviews the probabilis-tic models and lists their individual problems.
In sec-tion 3, we propose anew probabilistic model based on aSingle random Variable with Multiple Values (SVMV).Our model is very simple, but solves some problemsof the previous models.
In section 4, we verify ourmodel's superiority over the others through experi-ments in which we categorize "Wall Street Journal"articles.2 A Brief Survey of Probabilistic TextCategorizationIn this section, we will briefly review three major prob-abilistic models for text categorization.
Originally,these models have been exploited for information re-trieval, but the adaptation to text categorization isstraightforward.In a model of probabilistic text categorization,P(cld ) = "the probability that a document d iscategorized into a category c" (1)is calculated.
Usually, a set of categories i defined be-forehand.
For every document di, probability P(cldi ) iscalculated and all the documents are ranked in decreas-ing order according to their probabilities.
The largerP(cldi) a document di has, the more probably it willbe categorized into category c. This is called the Prob-abilistic Ranking Principle (PRP) (Robertson, 1977).Several strategies can be used to assign categories to adocument based on PRP (Lewis, 1992).There are several ways to calculate P(c\[d).
Threerepresentatives are (Robertson and Sparck Jones,1976), (Kwok, 1990), and (Fuhr, 1989).2.1 Probabil ist lc Relevance Weighting(PRW)Robertson and Sparck Jones (1976) make use of thewell-known logistic (or log-odds) transformation f the162probability P(c\]d).P(cld)g(cld ) = log P(~ld) (2)where ~ means "not c", that is "a document is notcategorized into c." Since this is a monotonic transfor-mation of P(cld), PRP is still satisfied after transfor-mation.Using Bayes' theorem, Eq.
(2) becomesP(d\]c) P(c) g(cld ) = log ~ + log p(~).
(3)Here, P(c) is the prior probability that a document iscategorized into c. This is estimated from given train-ing data, i.e., the number of documents assigned to thecategory c. P(dl c) is calculated as follows.
If we as-sume that a document consists of a set of terms (usuallynouns are used for the first approximation) and eachterm appears independently in a document, P(dlc ) isdecomposed toP(dlc) = l-~ P(T/ = lie) l '~ P(~ = 01e) (4)tied tjEe-dwhere "c -  d" is a set of terms that do not appearin d but appear in the training cases assigned to c."ti" represents the name of a term and "T/ = 1, 0"represents whether or not the corresponding term '2i"appears in a document.
Therefore, P (T /= 1, 0\[c) is theprobability that a document does or does not containthe term ti, given that the document is categorized intoc.
This probability is estimated from the training data;the number of documents that are categorized into cand have the term tl.
Substituting Eq.
(4) into Eq.
(3)yieldsP (~ = lie )g(cld) = log P(T, 1re)tiedP(e) + log+ ,--, P(Tj = 01c )\ - logtieZe-'Ce_d P (~ = 0l~ )(5)We refer to Robertson and Sparck Jones' formulationas Probabilistic Relevance Weighting (PRW).While PRW is the first attempt to formalize well-known relevance weighting (Sparck Jones, 1972; Saltonand McGill, 1983) by probability theory, there are sev-eral drawbacks in PRW.\ [P rob lem 1\] no wi th in-document  te rm fre-quenc iesPRW does not make use of within-document termfrequencies.
P(T = 1, 01c) in Eq.
(5) takes into accountonly the existence/absence of the term t in a document.In general, frequently appearing terms in a documentplay an important role in information retrieval (Saltonand McGill, 1983).
Salton and Yang experimentallyverified the importance of within-document term fre-quencies in their vector model (Salton and Yang, 1973).\ [Problem 2\] no term weight ing for targetdocumentsIn the PRW formulation, there is no factor of termweighting for target documents (i.e., P(.Id)).
Accord-ing to Eq.
(5), even if a term exists in a target docu-ment, only the importance of the term in a category(i.e., P(T = lie)) is considered for overall probability.Term weighting for target documents would also benecessary for sophisticated information retrieval (Fuhr,1989; Kwok, 1990).\ [P rob lem 3\] af fected by  having insufficienttraining casesIn practical situations, the estimation of P(T =1, 01c ) is not always straightforward.
Let us considerthe following case.
In the training data, we are givenR documents that are assigned to c. Among them,r documents have the term t. In this example, thestraightforward estimate of P(T -- llc ) is "r/R."
If"r = 0" (i.e., none of the documents in c has t) andthe target document d contains the term t, g(c\[d) be-comes -0% which means that d is never categorizedinto c. Robertson and Sparck Jones mentioned otherspecial cases like the above example (Robertson andSparck Jones, 1976).
A well-known remedy for thisproblem is to use "(r + 0.5)/(R + 1)" as the estimateof P(T = lie ) (Robertson and Sparck Jones, 1976).While various smoothing methods (Church and Gale,1991; Jelinek, 1990) are also applicable to these situa-tions and would be expected to work better, we usedthe simple "add one" remedy in the following experi-ments.2.2 Component  Theory  (CT)To solve problems 1 and 2 of PRW, Kwok (1990)stresses the assumption that a document consists ofterms.
This theory is called the Component Theory(CT).To introduce within-document term frequencies (i.e.,to solve problem 1), CT assumes that a documentis completely decomposed into its constituting terms.Therefore, rather than counting the number of docu-ments, as in PRW, CT counts the number of terms ina document for probability estimation.
This leads towithin-document term frequencies.
Moreover, to in-corporate term weighting for target documents (i.e., tosolve problem 2), CT defines g(cld ) as the geometricmean probabilities over components of the target doc-ument d;P(dlc) = \[YI P(TIc)\]~~ P(Tl'c) J "Following Kwok's derivation, g(cld) becomesg(cld) = E P(T = tld)(log P(T - tic )P(T ?
tic) tedP(T ~ tl'd)~ P(c)log P(T -- ~-~" + log p(~).
(6)+(7)163For precise derivation, refer to (Kwok, 1990).Here, note that P(T  = rid ) and P(T  = tic ) representthe frequency of a term t within a target documentd and that within a category c respectively.
There-fore, CT is not subject to problems 1 and 2.
However,problem 3 still affects CT.
Furthermore, Fuhr (1989)pointed out that transformation, as in Eq.
(6), is notmonotonic of P(cld ).
It follows then, that CT does notsatisfy the probabilistic ranking principle (PRP) anymore.2.3 Ret r ieva l  w i th  P robab i l i s t i c  Index ing(RPI)Fuhr (1989) solves problem 2by assuming that a doc-ument is probabilistically indexed by its term vectors.This model is called Retrieval with Probabilistie Index-ing (RPI).In RPI, a document d has a binary vector-- (T1, .
.
.
,  Tn) where each component corresponds toa term.
7} = 1 means that the document d containsthe term ti.
X is defined as the set of all possible in-dexings, where IX I = 2".
Conditioning P(cld ) for eachpossible indexing givesP(c\[d) = Z P(cld'x)P(:rld)" (8)~EXBy assuming conditional independence between c andd given a~ 1 , and using Bayes' theorem, Eq.
(8) becomes,P(~lc)P(zld)P(cld ) = P(c) Z P (z )  (9):~EXAssuming that each term appears independently in atarget document d and in a document assigned to c,Eq.
(9) is rewritten as1-I(P(7} = llc)P(7} = lid ) P(c\]d) = P(c) P(7} = 1)i-t P(7} = 0Ic)P(T~ = 01d)).P(7} -- 0)(lo)Here, all the probabilities are estimated from the train-ing data using the same method described in Sec-tion 2.1.Since Eq.
(10) includes the factor P(T  = 1,01d) aswell as P(T  = 1,01c), RPI takes into account termweighting for target documents.
While this in prin-ciple solves problem 2, if we use a simple estimationmethod counting the number of documents which havea term, P(T  = 1,0\]d) reduces to 1 or 0 (i.e, binary,not weighted).
For example, when a target docu-ment d has a term t, P(t  = 1\]d) = 1 and when not,P(T  = l id ) = 0.
In the following experiments we usedthis binary estimation method, but non binary esti-mates could be used as in (Fuhr, 1989).1More precisely, P( cld , x) = P( c\[x ) which assumes thatif we know x, information for c is independent of that ford.
This assumption sounds valid because x is a kind ofrepresentation f d.As far as other problems are concerned, RPI stillproblematic.
In particular, because of problem 3,P(cld) would become an illegitimate value.
In ourexperiments, as well as in Lewis' experiments (1992),P(cld ) ranges from 0 to more than 101?.3 A P robab i l i s t i c  Mode l  Based  on  aS ing le  Random Var iab le  w i thMu l t ip le  Va lues  (SVMV)In this section, we propose a new probabilistic modelfor text categorization, and compare it to the previousthree models from several viewpoints.
Our model isvery simple, but yet solves problems 1, 2, and 3 inPRW.Document representation of our model is basicallythe same as CT, that is a document is a set of itsconstituting terms.
The major difference between ourmodel and others is the way of document characteriza-tion through probabilities.
While almost all previousmodels assume that an event space for a document iswhether the document is indexed or not by a term 2,our model characterizes a document as random sam-pling of a term from the term set that represents thedocument.
For example, an event "T = ti" means thata randomly selected term from a document is ti.
If wewant to emphasis indexing process like other models, itis possible to interpret "T = ti" as a randomly selectedelement from a document being indexed by the termti.Formally, our model can be seen as modifying Fuhr'sderivation of P(cld ) by replacing an index vector witha single random variable whose value is one of possi-ble terms.
Conditioning P(cld ) for each possible eventgivesP(cld) = ~_~ P(c\]d, T = t i ) f (T  = tild).
(11)tiIf we assume conditional independence between c andd, given T = ti, that is P(cid, T = ti) = P(c\]T = tl),we obtainP(cld) = Z P(cIT = t i )P (T  = tild).
(12)tiUsing Bayes' theorem, this becomesZ P(T  = t i lc)P(T = ti Id) P(cld) P(c) t, P (T  = ti) (13)All the probabilities in Eq.
(13) can be estimated fromgiven training data based on the following definitions.?
P (T  =t i l c )  is the probability that a randomlyselected term in a document is ti, given that thedocument is assigned to c. We used ~c  as theestimator.
NCi is the frequency of the term ti inthe category c, and NC is the total frequency ofterms in c.2In section 2 explaining previous models, we simplified"a document is indexed by a term" as "a document containsa term" for ease of explanation.164,, P (T  = tild) is the probability that a randomlyselected term in a target document d is ti.
Weused ~D as the estimator.
NDi  is the frequencyof the term ti in the document d, and ND is thetotal frequency of terms in d.* P (T  = ti) is the prior probability that a randomlyselected term in a randomly selected ocument isti.
We used ~t as the estimator.
Ni is the fre-quency of the term ti in the given training doc-uments, and N is the total frequency of terms inthe training documents.?
P(c) is the prior probability that a randomly se-lected document is categorized into c. We usedD_~ as the estimator.
Dc is the frequency of docu- Dmeats that is categorized to c in the given trainingdocuments, and D is the frequency of documentsin the training documents.Here, let us recall the three problems of PRW.
SinceSVMV's primitive probabilities are based on within-document term frequencies, SVMV does not have prob-lem 1.
Furthermore, SVMV does not have problem 2either because Eq.
(13) includes a factor P(T  = tld),which accomplishes term weighting for a target docu-ment d.For problem 3, let us reconsider the previous exam-ple; R documents in the training data are categorizedinto a category c, none of the R documents has termti, but a target document d does.
If the straightfor-ward estimate of P (T /= llc ) = 0 or P(T  = tilc) = 0 isadopted, the document d would never be categorizedinto c in the previous models (PRW, CT, and RPI).In SVMV, the probability P(c\[d) is much less affectedby such estimates.
This is because P(cld ) in Eq.
(13)takes the sum of each term's weight.
In this example,the weight for ti is estimated to be 0 as in the othermodels, but this little affect the total value of P(c\[d).A similar argument applies to all other problems in(Robertson and Sparck Jones, 1976) that are causedby having insufficient training cases.
SVMV is formallyproven not to suffer from the serious effects (like neverbeing assigned to a category or always being assignedto a category) by having insufficient training cases.
Inother words, SVMV can directly use the straightfor-ward estimates.
In addition, we experimentally verifiedthat the value of P(dlc ) in SVMV is always a legitimatevalue (i.e., 0 to 1) unlike in RPI.Table 1 summarizes the characteristics of the fourprobabilistic models.Table 1 Summary of the four probabilistic modelsPRW CT RPI SVMVProblem 1 considered no yes no yesProblem 2 considered no yes (yes) yesProblem 3 considered no no no yesPRP satisfied yes no ye s yesAs illustrated in the table, SVMV has better character-istics for text categorization compared to the previousmodels.
In the next section, we will experimentallyverify SVMV's superiority.4 Exper imentsThis section describes experiments conducted to eval-uate the performance of our model (SVMV) comparedto the other three (PRW, CT, and RPI).4.1 Data and Preprocess ingA collection of Wall Street Journal (WSJ) full-textnews stories (Liberman, 1991) 3 was used in the experi-ments.
We extracted all 12,380 articles from 1989/7/25to 1989/11/2.The WSJ articles from 1989 are indexed with 78categories (topics).
Articles having no category wereexcluded.
8,907 articles remained; each having 1.94categories on the average.
The largest category is"TENDER OFFERS, MERGERS, ACQUISITIONS (TNM)"which encompassed 2,475 articles; the smallest one is"RUBBER (RUB)", assigned to only 2 articles.
On theaverage, one category is assigned to 443 articles.All 8,907 articles were tagged by the Xerox Part-of-Speech Tagger (Cutting et al, 1992) 4.
From the taggedarticles, we extracted the root words of nouns usingthe "ispell" program 5.
As a result, each article has aset of root words representing it, and each element inthe set (i.e.
root word of a noun) corresponds to aterm.
We did not reduce the number of terms by usingstop words list or feature selection method, etc.
Thenumber of terms amounts to 32,975.Before the experiments, we divided 8,907 articlesinto two sets; one for training (i.e., for probabilityestimation), and the other for testing.
The divisionwas made according to chronology.
All articles thatappeared from 1989/7/25 to 1989/9/29 went into atraining set of 5,820 documents, and all articles from1989/10/2 to 1989/11/2 went into a test set of 3,087documents.4.2 Category  Ass ignment  StrategiesIn the experiments, the probabilities, P(c), P(Ti =llc), P(T  =ti lc) ,  and so forth, were estimated fromthe 5,820 training documents, as described in the pre-vious sections.
Using these estimates, we calculated theposterior probability (P(cld)) for each document (d) ofthe 3,087 test documents and each of the 78 categories3We used "ACL/DCI (September 1991)" CD-ROMwhich is distributed from the Linguistic Data Consortium(LDC).
For more details, please contact Mark Liberman(myl@lmagi.
cis.
upena, edu).4The xerox part-of-speech tagger version 1.0 is availablevia anonymous FTP from the host parc~tp.xerox.cora inthe directory pub/tagger.5Ispell is a program for correcting English spelling.
Weused the "ispell version 3.0" which is available via anony-mous FTP from the host f tp .
as  .ucla.
edu in the directorypub/ispell.165(c).
The four probabilistic models are compared in thiscalculation.There are several strategies for assigning categoriesto a document based on the probability P(cld ).
Thesimplest one is the k-per-doc strategy (Field, 1975) thatassigns the top k categories to each document.
A moresophisticated one is the probability threshold strategy,in which all the categories above a user-defined thresh-old are assigned to a document.Lewis proposed the proportional assignment strat-egy based on the probabilistic ranking principle (Lewis,1992).
Each category is assigned to its top scoring doc-uments in proportion to the number of times the cat-egory was assigned in the training data.
For example,a category assigned to 2% of the training documentswould be assigned to the top scoring 0.2% of the testdocuments if the proportionality constant was 0.1, orto 10% of the test documents if the proportionalityconstant was 5.0.4.3 Results and DiscussionsBy using a category assignment s rategy, several cat-egories are assigned to each test document.
Thebest known measures for evaluating text categoriza-tion models are recall and precision, calculated by thefollowing equations (Lewis, 1992):the number of categories that areRecall : correctly assigned to documentsthe number of categories that should be'assigned to documentsthe number of categories that arePrecision = correctly assigned to documentsthe number of categories that are"assigned to documentsNote that recall and precision have somewhat mutuallyexclusive characteristics.
To raise the recall value, onecan simply assign many categories to each document.However, this leads to a degradation i precision; i.e.,almost all the assigned categories are false.
A breakevenpoint might be used to summarize the balance betweenrecall and precision, the point at which they are equal.For each strategy, we calculated breakeven points byusing the four probabilistic models.
Table 2 shows thebest breakeven points identified for the three strategiesalong with the used models.Table 2 Best breakeven points for three categoryassignment s rategiesBreakeven Pts.Prop.
assignment 0.63 (by SVMV)Prob.
thresholding 0.47 (by SVMV)k-per-doe 0.43 (by SVMV)From Table 2, we find that SVMV with proportionalassignment gives the best result (0.63).
The superior-ity of proportional ssignment over the other strategieshas already been reported by Lewis (1992).
Our ex-periment verified Lewis' assumption.
In addition, forany of the three strategies, SVMV gives the highestbreakeven point among the four probabilistic models.Figure 1 shows the recall/precision trade off forthe four probabilistic models with proportional ssign-ment strategy.
As a reference, the recall/precisioncurve of a well-known vector model (Salton and Yang,1973) ("TF.IDF")6 is also presented.
Table 3 lists thebreakeven point for each model.
All the breakevenpoints were obtained when proportionality constantwas about 1.0.Fig.
1OJ~-Recall/precision with proportionalassignment s rategy?
:_ot 0.0A-0.7-0.1L0.6-0A-0.3 /0.2-0.10.1* ' ~  Breakeve~.
.
.
.
.
RM "?L " .. .
.
.
~ .
- ,  PmlVTFJOFI I I 0., oi,, oI, olo o., 0'.0 0.0RecallTable 3 Breakeven points with proportionalassignment s rategySVMVCTRPIPRWTF.IDFBreakeven Pts.0.630.600.510.530.48From Figure 1 and Table 3, we can see that:?
as far as this dataset is concerned, SVMV withproportional ssignment s rategy gives the best re-sult among the four probabilistic models,?
the models that consider within-document termfrequencies (SVMV, CT) are better than thosethat do not (PRW, RPI),Sin the model we used, each element of document vectoris the "term frequency" multiplied by the "inverted ocu-ment frequency."
Similarity between every pair of vectorsis measured by cosine.
Note that this is the simplest versionof TF.IDF model, and there has been many improvementswhich we did not consider in the experiments.166?
the models that consider term weighting for tar-get documents (SVMV, CT) are better than thosethat do not (PRW, (RPI)), and?
the models that are less affected by having insuffi-cient training cases (SVMV) are better than thosethat are (CT, RPI, PRW).5 ConclusionWe have proposed a new probabilistic model for textcategorization.
Compared to previous models, ourmodel has the following advantages; 1) it considerswithin document term frequencies, 2) considers termweighting for target documents, and 3) is less affectedby having insufficient training cases.
We have also pro-vided empirical results verifying our model's uperior-ity over the others in the task of categorizing newsarticles from the "Wall Street Journal.
"There are several directions along which this workcould be extended.?
We have to compare our probabilistic modelto other non probabilistic models like decisiontree/rule based models, one of which has recentlybeen reported to be promising (Apt4 et al, 1994).?
While we used simple document representation inwhich a document is defined as a set of nouns,there could be considered several improvements,such as using phrasal information (Lewis, 1992),clustering terms (Sparck Jones, 1973), reducingthe number of features by using local dictio-nary (Apt4 et al, 1994), etc.?
We are incorporating our probabilistic model intocluster-based text categorization that offers an ef-ficient and effective search strategy.AcknowledgmentsThe authors are grateful to Hiroshi Motoda for bene-ficial discussions, and would like to thank the anony-mous reviewers for their useful comments.ReferencesC.
Apt4, F. Damerau, and S. M. Weiss.
1994.
Au-tomated learning of decision rules for text catego-rization.
ACM Transactions on Office InformationSystems.
(to appear).K.
W. Church and W. A. Gale.
1991.
A comparisonof the enhanced Good-Turing and deleted estima-tion methods for estimating probabilities of Englishbigrams.
Computer Speech and Language, 5:19-54.W.
B. Croft.
1981.
Document representation in prob-abilistic models of information retrieval.
Journalof the American Society for Information Science,32(6):451-457.D.
Cutting, J. Kupiec, J. Pedersen, and P. Sibun.
1992.A practical part-of-speech tagger.
In In Proc.
of theThird Conference on Applied Natural Language Pro-cessing.B.
Field.
1975.
Towards automatic indexing: Au-tomatic assignment of controlled language indexingand classification from free indexing.
Journal ofDocumentation, 31(4):246-265.N.
Fuhr.
1989.
Models for retrieval with probabilis-tic indexing.
Information Processing ~ Retrieval,25(1):55-72.F.
Jelinek.
1990.
Self-organized language modelingfor speech recognition.
In A. Waibel and K. Lee,editors, Readings in Speech Recognition, pages 450-506.
Morgan Kaufmann.K.
L. Kwok.
1990.
Experiments with a componenttheory of probabilistic nformation retrieval based onsingle terms as document components.
A CM Trans-actions on Information Systems, 8(4):363-386.D.
D. Lewis.
1992.
An evaluation of phrasal and clus-tered representation  a text categorization task.
InProceedins of the Annual International A CM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 37-50.M.
Liberman, editor.
1991.
ACL/DCI (CD-ROM).Association for Computational Linguistics DataCollection Initiative, University of Pennsylvania,September.S.
E. Robertson and K. Sparck Jones.
1976.
Relevanceweighting of search terms.
Journal of the AmericanSociety for Information Science, 27:129-146.S.
E. Robertson.
1977.
The probability ranking prin-ciple in IR.
Journal of Documentation, 33:294-304.G.
Salton and M. J. McGill.
1983.
Introduction toModern Information Retrieval.
McGraw-Hill Pub-lishing Company.G.
Salton and C. S. Yang.
1973.
On the specificationof term values in automatic indexing.
Journal ofDocumentation, 29(4):351-372.K.
Sparck Jones.
1972.
A statistical interpretationof term specificity and its application in retrieval.Journal of Documentation, 28(1):11-21.K.
Sparck Jones.
1973.
Collection properties influenc-ing automatic term classification performance.
In-formation Storage and Retrieval, 9:499-513.S.
K. M. Wong and Y. Y. Yao.
1989.
A probabilitydistribution model for information retrieval.
Infor-mation Processing ~ Management, 25(1):39-53.C.
T. Yu, W. Meng, and S. Park.
1989.
A frame-work for effective retrieval.
ACM Transactions onDatabase Systems, 14(2):147-167.167
