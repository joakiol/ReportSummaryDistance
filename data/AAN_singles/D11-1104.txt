Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1128?1136,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsEfficient Subsampling for Training Complex Language ModelsPuyang Xupuyangxu@jhu.eduAsela Gunawardana#aselag@microsoft.comSanjeev Khudanpurkhudanpur@jhu.eduDepartment of Electrical and Computer EngineeringCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USA#Microsoft ResearchRedmond, WA 98052, USAAbstractWe propose an efficient way to train maximumentropy language models (MELM) and neuralnetwork language models (NNLM).
The ad-vantage of the proposed method comes froma more robust and efficient subsampling tech-nique.
The original multi-class language mod-eling problem is transformed into a set of bi-nary problems where each binary classifierpredicts whether or not a particular word willoccur.
We show that the binarized model isas powerful as the standard model and allowsus to aggressively subsample negative trainingexamples without sacrificing predictive per-formance.
Empirical results show that we cantrain MELM and NNLM at 1% ?
5% of thestandard complexity with no loss in perfor-mance.1 IntroductionLanguage models (LM) assign probabilities to se-quences of words.
They are widely used in manynatural language processing applications.
The prob-ability of a sequence can be modeled as a product oflocal probabilities, as shown in (1), where wi is theith word, and hi is the word history preceding wi.P (w1, w2, ..., wl) =l?i=1P (wi|hi) (1)Therefore the task of language modeling reducesto estimating a set of conditional distributions{P (w|h)}.
The n-gram LM is a dominant way toparametrizeP (w|h), where it is assumed thatw onlydepends on the previous n?1 words.
More complexmodels have also been proposed?MELM (Rosen-feld, 1996) and NNLM (Bengio et al, 2003) are twoexamples.Modeling P (w|h) can be seen as a multi-classclassification problem.
Given the history, we haveto choose a word in the vocabulary, which can eas-ily be a few hundred thousand words in size.
Forcomplex models such as MELM and NNLM, thisposes a computational challenge for learning, be-cause the resulting objective functions are expensiveto normalize.
In contrast, n-gram LMs do not suf-fer from this computational challenge.
In the webera, language modelers have access to virtually un-limited amounts of data, while the computing poweravailable to process this data is limited.
Therefore,despite the demonstrated effectiveness of complexLMs, the n-gram is still the predominant approachfor most real world applications.Subsampling is a simple solution to get aroundthe constraint of computing resources.
For the pur-pose of language modeling, it amounts to takingonly part of the text corpus to train the LM.
Forcomplex models such as NNLM, it has been shownthat subsampling can speed up training greatly, atthe cost of some degradation in predictive perfor-mance (Schwenk, 2007), allowing for trade-off be-tween computational cost and LM quality.Our contribution is a novel way to train com-plex LMs such as MELM and NNLM which allowsmuch more aggressive subsampling without incur-ring as high a cost in predictive performance.
Thekey to our approach is reducing the multi-class LMproblem into a set of binary problems.
Instead oftraining a V -class classifier, where V is the size of1128the vocabulary, we train V binary classifiers, eachone of which performs a one-against-all classifica-tion.
The V trained binary probabilities are then re-normalized to obtain a valid distribution over the Vwords.
Subsampling here can be done in the nega-tive examples.
Since the majority of training exam-ples are negative for each of the binary classifiers,we can achieve substantial computational saving byonly keeping subsets of them.
We will show that thebinarized LM is as powerful as its multi-class coun-terpart, while being able to sustain much more ag-gressive subsampling.
For certain types of LMs suchas MELM, there are more benefits?the binarizationleads to a set of completely independent classifiersto train, which allows easy parallelization and sig-nificantly lowers the memory requirement.Similar one-against-all approaches are often usedin the machine learning community, especially bySVM (support vector machine) practitioners to solvemulti-class problems (Rifkin and Klautau, 2004;Allwein et al, 2000).
The goal of this paper is toshow that a similar technique can also be used forlanguage modeling and that it enables us to sub-sample data much more efficiently.
We show thatthe proposed approach is useful when the dominantmodeling constraint is computing power as opposedto training data.The rest of the paper is organized as follows.
Insection 2, we describe our binarization and subsam-pling techniques for language models with MELMand NNLM as two specific examples.
Experimentalresults are presented in Section 3, followed by dis-cussion in Section 4.2 Approximating Language Models withBinary ClassifiersSuppose we have an LM that can be written in theformP (w|h) = exp aw(h; ?)?w?
exp aw?
(h; ?
), (2)where aw(h; ?)
is a parametrized history representa-tion for word w.Given a training corpus of word history pairs withempirical distribution P?
(h,w), the regularized loglikelihood of the training set can be written asL =?hP?
(h)?wP?
(w|h) logP (w|h)?
r(?
), (3)where r(?)
is the regularizing function over the pa-rameters.Assuming that r(?)
can be written as a sum overper-word regularizers, namely r(?)
= ?w rw(?
),we can take the gradient of the log likelihood w.r.t ?to show that the regularized MLE for the LM satis-fies?hP?
(h)?wP (w|h)?
?aw(h; ?)=?h,wP?
(w, h)?
?aw(h; ?)??w??rw(?).
(4)For each word w, we can define a binary classifierthat predicts whether the next word is w byPb(w|h) =exp aw(h; ?
)1 + exp aw(h; ?).
(5)The regularized training set log likelihood for allthe binary classifiers is given byLb =?w?hP?
(h)[P?
(w|h) logPb(w|h)+P?
(w?|h) logPb(w?|h)]??wrw(?
), (6)where Pb(w?|h) = 1?
Pb(w|h) is the probability ofw not occurring.
Here we assume the same structureof the regularizer r(?
).The regularized MLE for the binary classifierssatisfies?hP?
(h)?wPb(w|h)?
?aw(h; ?)=?h,wP?
(w, h)?
?aw(h; ?)??w??rw(?).
(7)Notice the right hand sides of (4) and (7) are thesame.
Thus, taking P ?
(w|h) = Pb(w|h) from MLtrained binary classifiers gives an LM that meets theMLE constraints for language models.
Therefore,if ?w Pb(w|h) = 1, ML training for the languagemodel is equivalent to ML training of the binaryclassifiers and using the probabilities given by theclassifiers as our LM probabilities.Note that in practice, the probabilities given bythe binary classifiers are not guaranteed to sum upto one.
For tasks such as measuring perplexity,1129these probabilities have to be normalized explicitly.Our hope is that for large enough data sets and richenough history representation aw(h; ?
), we will get?w Pb(w|h) ?
1 so that renormalizing the classi-fiers to getP ?
(w|h) = Pb(w|h)?w?
?V Pb(w?|h)(8)will not change the MLE constraint too much.2.1 Stratified SamplingWe note that iterative estimation of the LM shownin (2) in general requires enumerating over the Ttraining cases in the training set and computing thedenominator of (2) for each case at a cost of O(V ).Thus, each iteration of training takesO(V T ) in gen-eral.
The complexity of estimating each of the Vbinary classifiers is O(T ) per iteration, also givingO(V T ) per iteration in total.However, as mentioned earlier, we are able tomaximally subsample negative examples for eachclassifier.
Thus the classifier for w is trained us-ing the C(w) positive examples and a proportion?
of the T ?
C(w) negative examples.
The totalnumber of training examples for all V classifiers isthen (1 ?
?
)T + ?V T .
For large V , we choose?
>> 11+V so that this is approximately ?V T .Thus, our complexity for estimating all V classifiersis O(?V T ).The resulting training set for each binary classi-fier is a stratified sample (Neyman, 1934), and ourestimate needs to be calibrated to account for this.Since the training set subsamples negative examplesby ?, the resulting classifier will have a likelihoodratioPb(w|h)1?
Pb(w|h)= exp aw(h; ?)
(9)that is overestimated by a factor of 1?
.
This can becorrected by simply adding log?
to the bias (uni-gram) weight of the classifier.2.2 Maximum Entropy LMMELM is an effective alternative to the standard n-gram LM.
It provides a flexible framework to incor-porate different knowledge sources in the form offeature constraints.
Specifically, MELM takes theform of (2), for wordw following history h, we havethe following probability definition,P (w|h) = exp?i ?ifi(h,w)?w?
?V exp?i ?ifi(h,w?).
(10)fi is the ith feature function defined over theword-history pair, ?i is the feature weight associatedwith fi.
By defining general features, we have a nat-ural framework to go beyond n-grams and capturemore complex dependencies that exist in language.Previous research has shown the benefit of includingvarious kinds of syntactic and semantic informationinto the LM (Khudanpur and Wu, 2000).
However,despite providing a promising avenue for languagemodeling, MELM are computationally expensive toestimate.
The bottleneck lies in the denominatorof (10).To estimate ?is, gradient based methods can beused.
The derivative of the likelihood function Lw.r.t ?i has a simple form, namely?L?
?i=?kfi(wk, hk)??k?w?
?VP (w?|h)fi(w?, hk),(11)where k is the index of word-history pair in the train-ing corpus.
The first term in the derivative is the ob-served feature count in the training corpus, the sec-ond term is the expected feature count according tothe model.
In order to obtain P (w?|h) in the secondterm, we need to compute the normalizer, which in-volves a very expensive summation over the entirevocabulary.
As described earlier, the complexity foreach iteration of training is at O(V T ), where T isthe size of training corpus.For feature sets that can be expressed hierarchi-cally, for example n-gram feature set, where higherorder n-grams imply lower order n-grams, Wu andKhudanpur (2000) exploit the structure of the nor-malizer, and precompute components that can beshared by different histories.
For arbitrary featuresets, however, it may not be possible to establishthe required hierarchical relations and the normal-izer still needs to be computed explicitly.
Good-man (2001) changes the original LM into a class-based LM, where each one of the two-step predic-tions only involves a much smaller summation in thenormalizer.
In addition, MELM estimation can beparallelized, with expected count computation done1130separately for different parts of the training data andmerged together at the end of each iteration.
Formodels with massive parametrizations, this mergestep can be expensive due to communication costs.Obviously, a different way to expedite MELMtraining is to simply train on less data.
We propose away to do this without incurring a significant loss ofmodeling power, by reframing the problem in termsof binary classification.
As mentioned above, webuild V binary classifiers of the form in (5) to modelthe distribution over the V words.
The binary clas-sifiers use the same features as the MELM of (10),and are given by:Pb(w|h) =exp?i ?ifi(h,w)1 + exp?i ?ifi(h,w).
(12)We assume the features are partitioned over the vo-cabulary, so that each feature fi has an associatedw such that fi(h,w?)
= 0 for all w?
6= w. There-fore, the corresponding ?i affects only the binaryclassifier for w. This gives an important advan-tage in terms of parallelization?we have a set of bi-nary classifiers with no feature sharing, and can betrained separately on different machines.
The par-allelized computations are completely independentand do not require the tedious communication be-tween machines.
Memory-wise, since the compu-tations are independent, each word trainer only haveto store features that are associated with the word, sothe memory requirement for each individual workeris significantly reduced.2.3 Neural Network LMNeural Network Language Models (NNLM) havegained a lot of interest since their introduction (Ben-gio et al, 2003).
While in standard language mod-eling, words are treated as discrete symbols, NNLMmap them into a continuous space and learn theirrepresentations automatically.
It is often believedthat NNLM can generalize better to sequences thatare not seen in the training data.
However, despitehaving been shown to outperform standard n-gramLM (Schwenk, 2007), NNLM are computationallyexpensive to train.Figure 1 shows the standard feed-forward NNLMarchitecture.
Starting from the left part of the figure,each word of the n?
1 words history is mapped to aFigure 1: Feed-forward NNLMcontinuous vector and concatenated.
Through a non-linear hidden layer, the neural network constructs amultinomial distribution at the output layer.
Denot-ing the concatenated d-dimensional word represen-tations r, we have the following probability defini-tion:P (wi = k|wi?1, ..., wi?n+1) =eak?m eam, (13)ak = bk +h?l=1Wkl tanh(cl +(n?1)d?j=1Uljrj), (14)where h denotes the hidden layer size, b and c arethe bias vectors for the output nodes and hiddennodes respectively.
Note that NNLM also has theform of (2).Stochastic gradient descent is often used to max-imize the training data likelihood under such amodel.
The gradient can be computed using theback-propagation method.
To analyze the complex-ity, computing an n-gram conditional probability re-quires approximatelyO((n?
1)dh+ h+ V h+ V ) (15)operations, where V is the size of the vocabu-lary.
The four terms in the complexity correspondto computing the hidden layer, applying the nonlin-earity, computing the output layer and normaliza-tion, respectively.
The error propagation stage canbe analyzed similarly and takes about the same num-ber of operations.
For typical values as used in ourexperiments, namely n = 3, d = 50, h = 200,1131V = 10000, the majority of the complexity per iter-ation comes from the term hV .
For large scale tasks,it may be impractical to train an NNLM.A lot of previous research has focused onspeeding up NNLM training.
It usually aimsat removing the computational dependency on V .Schwenk (2007) used a short list of frequent wordssuch that a large number of out-of-list words aretaken care of by a back-off LM.
To reduce thegradient computation introduced by the normal-izer, Bengio and Senecal (2008) proposed a dif-ferent kind of importance sampling technique.
Arecent work (Mikolov et al, 2011) applied Good-man?s class MELM trick (2001) to NNLM, in or-der to avoid the gigantic normalization.
A similartechnique has been introduced even earlier whichtook the idea of factorizing output layer to the ex-treme (Morin, 2005) by replacing the V -way predic-tion by a tree-style hierarchical prediction.
The au-thors show a theoretical complexity reduction fromO(V ) to (log V ), but the technique requires a care-ful clustering which may not be easily attainable inpractice.Subsampling has also been proposed to acceler-ate NNLM training (Schwenk, 2007).
The idea is toselect random subsets of the training data in eachepoch of stochastic gradient descent.
After someepochs, it is very likely that all of the training exam-ples have been seen by the model.
We will show thatour binary classifier representation leads to a morerobust and promising subsampling strategy.As with MELM, we notice that the parametersof (14) can be interpreted as also defining a set ofV per-word binary classifiersPb(wi = k|wi?1, ..., wi?n+1) =eak1 + eak , (16)but with a common hidden layer representation.
Asin MELM, we will train the classifiers, and renor-malize them to obtain an NNLM over the V words.In order to train the classifiers, we need to com-pute all V output nodes and propagate the errorsback.
Since the hidden layer is shared, the classifiersare not independent, and the computations can notbe easily parallelized to multiple machines.
How-ever, subsampling can be done differently for eachclassifier.
Each training instance serves as a positiveexample for one classifier and as a negative exam-ple for only a fraction ?
of the others.
The rest ofthe nodes are not computed and do not produce er-ror signal for the hidden representation.
We calibratethe classifiers after subsampled training as describedabove for MELM.It is straightforward to show that the dominatingterm V h in the complexity is reduced to ?V h. Wewant to point out that compared with MELM, sub-sampling the negatives here does not always reducethe complexity proportionally.
In cases where thevocabulary is very small, as shown in (15), com-puting the hidden layer can no longer be ignored.Nonetheless, real world applications such as speechrecognition, usually involves a vocabulary of consid-erable size, therefore, subsampling in the binary set-ting can still achieve substantial speedup for NNLM.3 Experimental Results3.1 MELMWe evaluate the proposed technique on two datasetsof different sizes.
Our first dataset is obtainedfrom Penn Treebank.
Section 00-20 are used fortraining(972K tokens), section 21-22 are the val-idation set(77K), section 23-24(86K) are the testset.
The vocabulary size of the experiment is10, 000.
This is one of the standard setups on whichmany researchers have reported perplexity results on(Mikolov et al, 2011).The binary MELM is trained using stochasticgradient descent, no explicit regularization is per-formed (Zhang, 2004).
The learning rate starts at 0.1and is halved every time the perplexity on the vali-dation set stops decreasing.
It usually takes around20 iterations before no significant improvement canbe obtained on the validation set.
The training stopsat that time.We compare perplexity with both the standard in-terpolated Kneser-Ney trigram model and the stan-dard MELM.
The MELM is L2 regularized and es-timated using a variant of generalized iterative scal-ing, the regularizer is tuned on the validation data.To demonstrate the effectiveness of our subsamplingapproach, we compare the subsampled versions ofthe binary MELM and the standard MELM.
In orderto obtain valid perplexities, the binary LMs are firstrenormalized explicitly according to equation (8) foreach test history.1132Model PPLKN Trigram 153.0Standard MELM, Feat-I 154.2Binary MELM, Feat-I 153.7Standard MELM, Feat-II 140.2Binary MELM, Feat-II 141.1Table 1: Binary MELM vs. Standard MELMWe consider two kinds of feature sets: Feat-I con-tains only n-gram features, namely unigram, bigramand trigram features, with no count cutoff, the totalnumber of features is 0.9M .
Feat-II is augmentedwith skip-1 bigrams and skip-1 trigrams (Goodman,2001), as well as word trigger features as describedin (Rosenfeld, 1996).
The total number of featuresin this set is 1.9M .
Note that the speedup trick de-scribed in (Wu and Khudanpur, 2000) can be usedfor feat-I , but not feat-II .Table 1 shows the perplexity results when no sub-sampling is performed.
With only n-gram features,the binary MELM is able to match both standardMELM and the Kneser-Ney model.
We can also seethat by adding features that are known to be able toimprove the standard MELM, we can get the sameimprovement in the binary setting.Figure 2 shows the comparisons of the two typesof MELM when the training data are subsampled.The standard MELM with n-gram features suffersdrastically as we sample more aggressively.
In con-trast, the binary n-gram MELM(Feat-I) does notappear to be hurt by aggressive subsampling, evenwhen 99% of the negative examples are discarded.The robustness also holds for Feat-II where morecomplicated features are added into the model.
Thissuggests a very efficient way of training MELM?with only 1% of the computational cost, we are ableto train an LM as powerful as the standard MELM.We further test our approach on a second datasetwhich comes from Wall Street Journal corpus.
Itcontains 26M training tokens and a test set of 22Ktokens.
We also have a held-out validation set totune parameters.
This set of experiments is intendedto demonstrate that the binary subsampling tech-nique is useful on a large text corpus where traininga standard MELM is not practical, and gives a betterLM than the commonly used Kneser-Ney baseline.Figure 2: Subsampled Binary MELM vs. SubsampledStandard MELMModel PPLKN Trigram 117.7Standard MELM, Trigram 116.5Binary MELM, Feat-III, 10% 110.2Binary MELM, Feat-III, 5% 110.8Binary MELM, Feat-III, 2% 112.1Binary MELM, Feat-III, 1% 112.4Table 2: Binary Subsampled MELM on WSJThe binary MELM is trained in the same way asdescribed in the previous experiment.
Besides un-igram, bigram and trigram features, we also addedskip-1 bigrams and skip-1 trigrams, this gives us7.5M features in total.
We call this set of featuresfeat-III .
We were unable to train a standard MELMwith feat-III or a binary MELM without subsam-pling because of the computational cost.
However,with our binary subsampling technique, as shown inTable 2, we are able to benefit from skip n-gram fea-tures with only 5% of the standard MELM complex-ity.
Also the performance does not degrade much aswe discard more negative examples.To show that such improvement in perplexitytranslates into gains in practical applications, weconducted a set of speech recognition experiments.The task is on Wall Street Journal, the LMs aretrained on 37M tokens and are used to rescore the n-best list generated by the first pass recognizer witha trigram LM.
The details of the experimental setupcan be found in (Xu et al, 2009).
Our baseline LMis an interpolated Kneser-Ney 4-gram model.Note that the size of the vocabulary for the task1133Model Dev WER Eval WERKN 4-gram 11.8 17.2Binary MELM, Feat-IV, 5% 11.0 16.7Binary MELM, Feat-IV, 2% 11.2 16.7Binary MELM, Feat-IV, 1% 11.2 16.7Table 3: WSJ WER improvement.
Binary MELM areinterpolated with KN 4-gramis 20K, for the purpose of rescoring, we are onlyinterested in the words that exist in the n-best list,therefore, for the binary MELM, we only have totrain about 5300 binary classifiers.
For comparison,the KN 4-gram also uses the same restricted vocabu-lary.
The features for the binary MELM are n-gramfeatures up to 4-grams plus skip-1 bigrams and skip-1 trigrams.
The total number of features is 10M.
Wecall this set of features Feat-IV.Table 3 demonstrates the word error rate(WER)improvement enabled by our binary subsamplingtechnique.
Note that we can achieve 0.5% abso-lute WER improvement on the test set at only 1%of the standard MELM complexity.
More specifi-cally, with only 50 machines, such a reduction incomplexity allows us to train a binary MELM withskip n-gram features in less than two hours, which isnot possible for the standard MELM on 37M words.Obviously, with more machines, the estimationcan be even faster, it?s also reasonable to expect thatwith more kinds of features, the improvement canbe even larger.
We think that the proposed techniqueopens the door for the utilization of the modelingframework provided by MELM at a scale that hasnot been possible before.3.2 NNLMWe evaluate our binary subsampling technique onthe same Penn Treebank corpus as described for theMELM experiments.
Taking random subsets of thetraining data with the standard model is our primarybaseline to compare with.
The NNLM we train isa trigram LM with tanh hidden units.
The size ofword representation and the size of hidden layer aretuned minimally on the validation set(Hidden layersize 200; Representation size 50).
We adopt thesame learning rate strategy as for training MELM,and the validation set is used to track perplexity per-formance and adjust learning rate correspondingly.Model PPL100% 20% 10% 5%Standard NNLM 154.3 239.8 297.0 360.3Binary NNLM - 152.7 160.0 176.2KN trigram 153.0 - - -Table 4: Binary NNLM vs. Standard NNLM.
Fixed ran-dom subset.Model Interpolated PPL100% 20% 10% 5%Standard NNLM 132.7 145.6 148.6 150.7Binary NNLM - 132.1 134.2 138.0KN trigram 153.0 - - -Table 5: Binary NNLM vs. Standard NNLM.
Fixed ran-dom subset.
Interpolated with KN trigram.All parameters are initialized randomly with mean 0and variance 0.01.
As with binary MELM, binaryNNLM are explicitly renormalized to obtain validperplexities.In our first experiment, we keep the subsampleddata fixed as we did for MELM.
For the standardNNLM, it means only a subset of the data is seen bythe model and it does not change through epochs;For binary NNLM, it means the subset of negativeexamples for each binary classifier does not change.Table 4 shows the perplexity results by NNLM itselfand the interpolated results are shown in Table 5.We can see that both models exhibit a tendencyto deteriorate as we subsample more aggressively.However, the standard NNLM is clearly impactedmore severely.
With binary NNLM, we are able toretain all the gain after interpolation with only 20%of the negative examples.Notice that with a fixed random subset, we are notreplicating the experiments of Schwenk (Schwenk,2007) exactly, although it is reasonable to expectboth models are able to benefit from seeing differentrandom subsets of the training data.
This is verifiedby results in Table 6 and Table 7.The standard NNLM benefits quite a lot goingfrom using a fixed random subset to a variable ran-dom subset, but still demonstrates a clear tendencyto deteriorate as we discard more and more data.
Onthe constrast, the binary NNLM maintains all theperformance gain with only 5% of the negative ex-amples and still clearly outperforms its counterpart.1134Model PPL100% 20% 10% 5%Standard NNLM 154.3 157.7 172.2 186.5Binary NNLM - 151.7 150.1 152.1Table 6: Binary NNLM vs. Standard NNLM.
Variablerandom subset.Model Interpolate PPL100% 20% 10% 5%Standard NNLM 132.7 133.9 138.1 141.2Binary NNLM - 132.2 131.7 132.2Table 7: Binary NNLM vs. Standard NNLM.
Variablerandom subset.
Interpolated with KN trigram.4 DiscussionFor the standard models, the amount of existent pat-terns fed into training heavily depends on the sub-sampling rate ?.
For a small ?, the models will in-evitably lose some training patterns given any rea-sonable number of epochs of training.
Taking vari-able random subsets in each epoch can alleviate thisproblem to some extent, but still can not solve thefundamental problem.
In the binary setting, we areable to do subsampling differently.
While the com-plexity remains the same without subsampling, themajority of the complexity comes from processingnegatives examples for each binary classifier.
There-fore, we can achieve the same level of speedup asstandard subsampling by only subsampling negativeexamples, and most importantly, it allows us to keepall the existent patterns(positive examples) in thetraining data.
Of course, negative examples are im-portant and even in the binary case, we benefit fromincluding more of them, but since we have so manyof them, they might not be as critical as positive ex-amples in determining the distribution.A similar conclusion can be drawn from Google?swork on large LMs (Brants et al, 2007).
Not havingto properly smooth the LM, they are still able to ben-efit from large volumes of web text as training data.It is probably more important to have a high n-gramcoverage than having a precise distribution.The explanation here might lead us to wonderwhether for the multi-class problem, subsamplingthe terms in the normalizer would achieve the sameresults.
More specifically, instead of summing overall words in the vocabulary, we may choose to onlyconsider ?
of them.
In fact, the short-list approachin (Schwenk, 2007) and the adaptive importancesampling in (Bengio and Senecal, 2008) have ex-actly this intuition.
However, in the multi-classsetup, subsampling like this has to be very careful.We have to either have a good estimate of how muchprobability mass we?ve thrown away, as in the short-list approach, or have a good estimate of the entirenormalizer, as in the importance sampling approach.It is very unlikely that an arbitrary random subsam-pling will not harm the model.
Fortunately, in the bi-nary case, the effect of random subsampling is mucheasier to analyze.
We know exactly how much nega-tive examples we?ve discarded, and they can be com-pensated easily in the end.It is worth pointing out that the proposed tech-nique is not restricted to MELM and NNLM.
Wehave done experiments to binarize the class tricksometimes used for language modeling (Goodman,2001; Mikolov et al, 2011), and it also proves tobe useful.
We plan to report these results in the fu-ture.
More generally, for many large-scale multi-class problems, binarization and subsampling can bean effective combination to consider.5 ConclusionWe propose efficient subsampling techniques fortraining large multi-class classifiers such as maxi-mum entropy language models and neural networklanguage models.
The main idea is to replace amulti-way decision by a set of binary decisions.Since most of the training instances in the binarysetting are negatives examples, we can achieve sub-stantial speedup by subsampling only the negatives.We show by extensive experiments that this is morerobust than subsampling subsets of training data forthe original multi-class classifier.
The proposedmethod can be very useful for building large lan-guage models and solving more general multi-classproblems.AcknowledgmentsThis work is partially supported by National ScienceFoundation Grant No?
0963898, the DARPA GALEProgram and JHU/HLTCOE.1135ReferencesAllwein, Erin, Robert Schapire, Yoram Singer and PackKaelbling.
2000.
Reducing Multiclass to Binary: AUnifying Approach for Margin Classifiers.
Journal ofMachine Learning Research, 1:113-141.Bengio, Yoshua, Rejean Ducharme and Pascal Vincent2003.
A neural probabilistic language model Journalof Machine Learning research, 3:1137?1155.Bengio, Yoshua and J. Senecal 2008.
Adaptive impor-tance sampling to accelerate training of a neural prob-abilistic language model IEEE Transaction on NeuralNetwork, Apr.
2008.Berger, Adam, Stephen A. Della Pietra and Vicent J.Della Pietra 1996.
A Maximum Entropy approachto Natural Language Processing.
Computational Lin-guistics, 1996, 22:39-71.Brants, Thorsten, Ashok C. Popat, Peng Xu, Frank J. Ochand Jeffrey Dean 2007.
Large language models in ma-chine translation.
In Proceedings of 2007 Conferenceon Empirical Methods in Natural Language Process-ing, 858?867.Goodman, Joshua 2001.
Classes for Fast MaximumEntropy Training.
Proceedings of 2001 IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing.Goodman, Joshua 2001.
A bit of Progress in LanguageModeling.
Computer Speech and Language, 403-434.Khudanpur, Sanjeev and Jun Wu 2000.
Maximum En-tropy Techniques for Exploiting Syntactic, Semanticand Collocational Dependencies in Language Model-ing.
Computer Speech and Language, 14(4):355-372.Mikolov, Tomas, Stefan Kombrink, Lukas Burget, Jan?Honza?
Cernocky and Sanjeev Khudanpur 2011.
Ex-tensions of recurrent neural network language model.Proceedings of 2011 IEEE International Conferenceon Acoustics, Speech and Signal Processing.Morin, Frederic 2005.
Hierarchical probabilistic neuralnetwork language model.
AISTATS?05, pp.
246-252.Neyman, Jerzy 1934.
On the Two Different Aspectsof the Representative Method: The Method of Strati-fied Sampling and the Method of Purposive Selection.Journal of the Royal Statistical Society, 97(4):558-625.Rifkin, Ryan and Aldebaro Klautau 2004.
In Defense ofOne-Vs-All Classification.
Journal of Machine Learn-ing Research.Rosenfeld, Roni.
1996.
A maximum entropy approachto adaptive statistical language modeling.
ComputerSpeech and Language, 10:187?228.Schwenk, Holger 2007.
Continuous space languagemodel.
Computer Speech and Language, 21(3):492-518.Wu, Jun and Sanjeev Khudanpur.
2000.
Efficient train-ing methods for maximum entropy language model-ing.
Proceedings of the 6th International Conferenceon Spoken Language Technologies, pp.
114?117.Xu, Puyang, Damianos Karakos and Sanjeev Khudanpur.2009.
Self-supervised discriminative training of statis-tical language models.
Proceedings of 2009 IEEE Au-tomatic Speech Recognition and Understanding Work-shop.Zhang, Tong 2004.
Solving large scale linear predictionproblems using stochastic gradient descent algorithms.Proceedings of 2004 International Conference on Ma-chine Learnings.1136
