Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 807?817,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsRobust Domain Adaptation for Relation Extraction via ClusteringConsistencyMinh Luan Nguyen?
?, Ivor W.
Tsang?, Kian Ming A.
Chai?, and Hai Leong Chieu?
?Institute for Infocomm Research, Singapore?Centre for Quantum Computation & Intelligent Systems, University of Technology, Sydney, Australia?DSO National Laboratories, Singaporemlnguyen@i2r.a-star.edu.sg, ivor.tsang@gmail.com{ckianmin,chaileon}@dso.org.sgAbstractWe propose a two-phase framework toadapt existing relation extraction classi-fiers to extract relations for new target do-mains.
We address two challenges: neg-ative transfer when knowledge in sourcedomains is used without considering thedifferences in relation distributions; andlack of adequate labeled samples for rarerrelations in the new domain, due to asmall labeled data set and imbalance rela-tion distributions.
Our framework lever-ages on both labeled and unlabeled datain the target domain.
First, we determinethe relevance of each source domain tothe target domain for each relation type,using the consistency between the clus-tering given by the target domain labelsand the clustering given by the predic-tors trained for the source domain.
Toovercome the lack of labeled samples forrarer relations, these clusterings operateon both the labeled and unlabeled data inthe target domain.
Second, we trade-offbetween using relevance-weighted source-domain predictors and the labeled targetdata.
Again, to overcome the imbalancedistribution, the source-domain predictorsoperate on the unlabeled target data.
Ourmethod outperforms numerous baselinesand a weakly-supervised relation extrac-tion method on ACE 2004 and YAGO.1 IntroductionThe World Wide Web contains information onreal-world entities, such as persons, locations and?The work is done while Nguyen was a research staff inNanyang Technological University, Singapore.organizations, which are interconnected by vari-ous semantic relations.
Detecting these relationsbetween two entities is important for many taskson the Web, such as information retrieval (Saltonand McGill, 1986) and information extraction forquestion answering (Etzioni et al, 2008).
Recentwork on relation extraction has demonstrated thatsupervised machine learning coupled with intelli-gent feature engineering can provide state-of-the-art performance (Jiang and Zhai, 2007b).
How-ever, most supervised learning algorithms requireadequate labeled data for every relation type to beextracted.
Due to the large number of relationsamong entities, it may be costly to annotate a largeenough set of training data to cover each relationtype adequately in every new domain of interest.Instead, it can be more cost-effective to adapt anexisting relation extraction system to the new do-main using a small set of labeled data.
This paperconsiders relation adaptation, where a relation ex-traction system trained on many source domains isadapted to a new target domain.There are at least three challenges when adapt-ing a relation extraction system to a new domain.First, the same semantic relation between two en-tities can be expressed using different lexical orsyntactic patterns.
For example, the acquisition ofcompany A by company B can be expressed with?B bought over by A?, ?A buys B?
and ?A pur-chases B?.
To extract a relation, we need to cap-ture the different ways in which it can be expressedacross different open domains on the Web.Second, the emphasis or interest on the differentrelation types varies from domain to domain.
Forexample, in the organization domain, we may bemore interested in extracting relations such as lo-catedIn (between a company and a location) andfounderOf (between a company and a person),807whereas in the person domain we may be more in-terested in extracting relations such as liveIn (be-tween a person and a location) and workAt (be-tween a person and a company).
Therefore, al-though the two domains may have the same setof relations, they probably have different marginaldistributions on the relations.
This can produce anegative transfer phenomenon (Rosenstein et al,2005), where using knowledge from other do-mains degrades the performance on the target do-main.
Hence, when transferring knowledge frommultiple domains, it is overly optimistic to believethat all source domains will contribute positively.We call a source domain irrelevant when it has noor negative contribution to the performance of thetarget domain.
One example is named entities ex-traction adaptation, where na?
?ve transfer of infor-mation from a mixed-case domain with capitaliza-tion information (e.g., news-wire) to a single-casedomain (e.g., conversational speech transcripts)will miss most names in the single-case domaindue to the absence of case information, which istypically important in the mixed-case domain.Third, the annotated instances for the target do-main are typically much fewer than those for thesource domains.
This is primarily due to the lackof resources such as raw target domain documents,time, and people with the expertise.
Together withimbalanced relation distributions inherent in thedomain, this can cause some rarer relations to con-stitute only a very small proportion of the labeleddata set.
This makes learning a relation classifierfor the target domain challenging.To tackle these challenges, we propose a two-phase Robust Domain Adaptation (RDA) frame-work.
In the first phase, Supervised Voting is usedto determine the relevance of each source domainto each region in the target domain, using both la-beled and unlabeled data in the target domain.
Byusing also unlabeled data, we alleviate the lack oflabeled samples for rarer relations due to imbal-anced distributions in relation types.The second phase uses the relevances deter-mined the first phase to produce a reference pre-dictor by weighing the source-domain predictorsfor each target domain sample separately.
The in-tention is to alleviate the effect of mismatched dis-tributions.
The final predictor in the target domainis trained on the labeled target domain data whiletaking reference from the reference predictions onthe unlabeled target domain data.
This ensuresreasonable predictive performance even when allthe source domains are irrelevant and augmentsthe rarer classes with examples in the unlabeleddata.
We compare the proposed two-phase frame-work with state-of-the-art domain adaptation base-lines for the relation extraction task, and we findthat our method outperforms the baselines.2 Related WorkRelation extraction is usually considered a classifi-cation problem: determine if two given entities ina sentence have a given relation.
Kernel-based su-pervised methods such as dependency tree kernels(Culotta and Sorensen, 2004), subsequence ker-nels (Bunescu and Mooney, 2006) and convolutiontree kernels (Qian et al, 2008) have been rathersuccessful in learning this task.
However, purelysupervised relation extraction methods assume theavailability of sufficient labeled data, which maybe costly to obtain for new domains.
We addressthis by augmenting a small labeled data set withother information in the domain adaptation setting.Bootstrapping methods (Zhu et al, 2009;Agichtein and Gravano, 2000; Xu et al, 2010;Pasca et al, 2006; Riloff and Jones, 1999) to re-lation extraction are attractive because they re-quire fewer training instances than supervised ap-proaches.
Bootstrapping methods are either ini-tialized with a few instances (often designated asseeds) of the target relation (Zhu et al, 2009;Agichtein and Gravano, 2000) or a few extractionpatterns (Xu et al, 2010).
In subsequent itera-tions, new extraction patterns are discovered, andthese are used to extract new instances.
The qual-ity of the extracted relations depends heavily onthe seeds (Kozareva and Hovy, 2010).
Differentfrom bootstrapping, we not only use labeled tar-get domain data as seeds, but also leverage on ex-isting source-domain predictors to obtain a robustrelation extractor for the target domain.Open Information Extraction (Open IE) (Et-zioni et al, 2008; Banko et al, 2008; Mesquitaet al, 2013) is a domain-independent informa-tion extraction paradigm to extract relation tu-ples from collected corpus (Shinyama and Sekine,2006) and Web (Etzioni et al, 2008; Banko et al,2008).
Open IE systems are initialized with a fewdomain-independent extraction patterns.
To cre-ate labeled data, the texts are dependency-parsed,and the domain-independent patterns on the parsesform the basis for extractions.
Recently, to reduce808labeling effort for relation extraction, distant su-pervision (Mintz et al, 2009; Takamatsu et al,2012; Min et al, 2013; Xu et al, 2013) has beenproposed.
This is an unsupervised approach thatexploits textual features in large unlabeled cor-pora.
In contrast to Open IE, we tune the relationpatterns for a domain of interest, using labeled re-lation instances in source and target domains andunlabeled instances in the target domain.Our work is also different from the multi-schema matching in database integration (Doanet al, 2003).
Multi-schema matching finds rela-tions between columns of schemas, which havethe same semantic.
In addition, current weightedschema matching methods do not address negativetransfer and imbalance class distribution.Domain adaptation methods can be classi-fied broadly into weakly-supervised adaptation(Daume and Marcu, 2007; Blitzer et al, 2006;Jiang and Zhai, 2007a; Jiang, 2009), and unsuper-vised adaptation (Pan et al, 2010; Blitzer et al,2006; Plank and Moschitti, 2013).
In the weakly-supervised approach, we have plenty of labeleddata for the source domain and a few labeled in-stances in the target domain; in the unsupervisedapproach, the data for the target domain are not la-beled.
Among these studies, Plank and Moschitti?sis the closest to ours because it adapts relationextraction systems to new domains.
Most otherworks focused on adapting from old to new re-lation types.
Typical relation adaptation methodsfirst identify a set of common features in sourceand target domains and then use those features aspivots to map source domain features to the targetdomain.
These methods usually assume that eachsource domain is relevant to the task on the targetdomain.
In addition, these methods do not handlethe imbalanced distribution of relation data explic-itly.
In this work, we study how to learn the targetprediction using only a few seed instances, whiledealing with negative transfer and imbalanced re-lation distribution explicitly.
These issues are sel-dom explored in relation adaptation.3 Problem StatementThis section defines the domain adaptation prob-lem and describes our feature extraction scheme.3.1 Relation Extraction Domain AdaptationGiven two entities A and B in a sentence S, rela-tion extraction is the task of selecting the relationy between A and B from a fixed set of c relationtypes, which includes the not-a-relation type.
Weintroduce a feature extraction ?
that maps the triple(A,B,S) to its feature vector x.
Learning relationextraction can then be abstracted to finding a func-tion p such that p(?
(A,B,S)) = p(x) = y.For adaptation, we have k source domains anda target domain.
We shall assume that all domainshave the same set of relation types.
The target do-main has a few labeled data Dl= {(xi,yi)}nli=1andplenty of unlabeled data Du= {(xi)}nl+nunl+1, wherenland nuare the number of labeled and unla-beled samples respectively, xiis the feature vec-tor, yiis the corresponding label (if available).
Letn = nl+ nu.
For the sth source domain, we havean adequate labeled data set Ds.
We define domainadaptation as the problem of learning a classifierp for relation extraction in the target domain usingthe data sets Dl, Duand Ds, s = 1, .
.
.
,k.3.2 Relational Feature RepresentationWe consider relation extraction as a classifica-tion problem, where each pair of entities A andB within a sentence S is a candidate relation in-stance.
The contexts in which entities A and B co-occur provide useful features to the relations be-tween them.
We use the term context to refer awindow of text in which two entities co-occur.
Acontext might not necessarily be a complete sen-tence S. Retrieving contexts in which two entitiesco-occur has been studied in previous works in-vestigating the relations between entities.Given a pair of entities (A,B) in S, the first stepis to express the relation between A and B withsome feature representation using a feature ex-traction scheme ?.
Lexical or syntactic patternshave been successfully used in numerous natu-ral language processing tasks, including relationextraction.
Jiang and Zhai (2007b) have shownthat selected lexical and syntactic patterns can givegood performance for relation extraction.
Follow-ing their work1, we also use lexical and syntacticpatterns extracted from the contexts to representthe relations between entities.
We extract featuresfrom a sequence representation and a parse treerepresentation of each relation instance.
The de-tails are as follows.Entity Features Entity types and entity mentiontypes are very useful for relation extraction.
We1The source code for extracting entity features is providedby the authors (Jiang and Zhai, 2007b).809use a subgraph in the relation instance graph(Jiang and Zhai, 2007b) that contains only thenode presenting the head word of the entity A, la-beled with the entity type or entity mention types,to describe a single entity attribute.Sequence Features The sequence representationpreserves the order of the tokens as they occur inthe original sentence.
Each node in the graph is atoken augmented with its relevant attributes.Syntactic Features The syntactic parse tree ofthe relation instance sentence can be augmented torepresent the relation instance.
Each node is aug-mented with relevant part-of-speech (POS) usingthe Python Natural Language Processing Tool Kit.Each node in the sequence or the parse treeis augmented by an argument tag that indicateswhether the node corresponds to entity A, B, both,or neither.
The nodes that represent the argumentare also labeled with the entity type, subtype andmention type.
We trim the parse tree of a relationinstance so that it contains only the most essentialtree components based on constituent dependen-cies (Qian et al, 2008).
We also use unigram fea-tures and bigram features from a relation instancegraph.4 Robust Domain AdaptationIn this section, we describe our two-phase ap-proach, which comprises of a Supervised Votingscheme and a combined classifier learning phase.4.1 Phase 1: Clustering Consistency viaSupervised VotingIn this section, we use the concept of clusteringconsistency to determine the relevance of a sourcedomain to particular regions in the target domain.Figure 1 illustrates this.
There, both enclosing cir-cles in the left and right figures denote the sameinput space of the target domain.
There are fourdisjoint regions within the input space, located atthe left, right, top and bottom of the space.
Thereare four classes of labels: plus (+), cross (?
), cir-cle (?)
and asterisk (?).
The labels in the left fig-ure are given by a preliminary predictor in the tar-get domain data, while the labels in the right fig-ure are given by a predictor trained on the sourcedomain data.
Comparing the figures, we see thepreliminary predictor and source domain predic-tor are consistent for the bottom and right regions,Target domain inputspace with transduc-tive learning using la-beled and unlabeledtarget domain data.Target domain inputspace with labels fromthe predictor trainedon the source domaindata set.Figure 1: Clustering consistency is used to deter-mine the relevance of a source domain to a regionin the target domain data.
The bottom and rightregions are more relevant than the top and left re-gions.
See text for explanation.but are inconsistent for the top and left regions.This suggests that the source domain is very rele-vant for the bottom and right regions of the targetinput space, but less so for the top and left regions.To apply this idea to relation classification, wehave to (i) partition the target domain input spaceinto regions and (ii) assign preliminary labels forall the examples.
We approximate the target do-main input space with all the samples from DlandDu.
With data from both the labeled and unlabeleddata sets, we apply transductive inference or semi-supervised learning (Zhou et al, 2003) to achieveboth (i) and (ii).
By augmenting with unlabeleddata Du, we aim to alleviate the effect of imbal-anced relation distribution, which causes a lackof labeled samples for rarer classes in a small setof labeled data.
Briefly, the known labels in Dlare propagated to the entire target input space byencouraging label smoothness in neighborhoods.The next three paragraphs give more details.At present, we assume a similarity matrix W ,where Wi jis the similarity between the ith and thejth input samples in Dl?Du.
Matrix W then de-termines the neighborhoods.
Let ?
be a diagonalmatrix where the (i, i)th entry is the sum of theith row of W .
Let us also encode the the labeleddata Dlin an n-by-c matrix H, such that Hi j= 1if sample i is labeled with relation class j in Dl,and Hi j= 0 otherwise.
Our objective is the c-dimensional relation-class indicator vector Fiforthe ith sample, for every sample.
This is achieved810via a regularization framework (Zhou et al, 2003):min{Fi}ni=1(n?i, j=1Wi j???Fi??ii?Fj?
?j j??
?2+?n?i=1?Fi?Hi?2).This trades off two criteria: the first term encour-ages nearby samples (under distance metric W ) tohave the same labels, while the second encouragessamples to take their labels from the labeled data.The closed-form solution isF?= (I?
(1+?
)?1L)?1H, (1)where L = ??1/2W?
?1/2; and the n-by-c matrixF?is the concatenation of the Fis.Using vector F?i, we now assign preliminary la-bels to the samples.
For a sample i, we transformF?iinto probabilities p1i, p2i, .
.
.
, pciusing softmax.Our propagated label `ifor sample i is then`i={not-a-relation if (maxjpji)< ?,argmaxjpjiotherwise.
(2)The second clause is self-evident, but the firstneeds further explanation.
Because not-a-relationis a background or default relation type in the re-lation classification task, and because it has ratherhigh variation when manifested in natural lan-guage, we have found it difficult to obtain a dis-tance metric W that allows the not-a-relation sam-ples to form clusters naturally using transductiveinference.
Therefore, we introduce the first clauseto assign the not-a-relation label to a sample whenthere is no strong evidence for any of the positiverelation types.
The amount of evidence needed isquantified by the parameter ?> 1/c.
In addition,the second clause will also assign not-a-relation toa sample if that probability is the highest.Next, we partition the data in Dl?Duinto c re-gions, R1,R2, .
.
.
,Rc, corresponding to the c rela-tion types.
The intuition is to use the true label inDlwhen available, or otherwise resort to using thepropagated label.
That is,xi?
{Ryiif xi?
Dl,R`iif xi?
Du.We now have the necessary ingredients to quan-tify the clustering consistency between a sourcePhy Per Emp Agt Aff GPE Dis N/ABCBNNWCTSWLFigure 2: Heat map of the relevance scores ws, jbetween the target domain Usenet (UN) with theother domains on ACE 2004 data set.
A lightershade means a higher score, or more relevant.
N/Arefers to not-a-relation; for the other abbrevia-tions, see the second paragraph in section 5.domain and a region in the target domain.
Intu-itively, this is the agreement between the source-domain predictor and the preliminary predictorwithin the target domain.
We use supervised vot-ing in the following manner.
For every source do-main, say domain s, we first train a relation-typepredictor psbased on its training data Ds.
Then,for every region Rj, we compute the relevancescore ws, j=?xi?RjJps(xi) = `iK/|Rj|, where J?K isthe Iverson bracket.Figure 2 shows the heat map of the relevancescores ws, jbetween the target domain Usenet(UN) with the other domains in the ACE 2004 cor-pus.
We observe, for example, that the Broad-cast News (BN) domain is more relevant in thePersonal-Social region of the target domain thanthe Broadcast Conversation (BC) domain.
Theserelevance scores will be used in the next phaseof the framework to weigh the contributions ofsource-domain predictors to the eventual target-domain relation classifier.4.2 Phase 2: Target Classifier LearningThe second phase uses both the weighted predic-tions from all sources and the target labeled dataDlto learn a relation classifier.
This ensures thateven when most of the source domains are irrele-vant, the performance of our method is no worsethan using the target-domain labeled data alone.The previous phase has computed the relevancews, jfor source domain s in region Rj.
We trans-late this to the relevance weight us,ifor an ex-ample xi: if xi?
Rj, then us,i= ws, j.
At our dis-posal from the previous phase are also k source-domain predictors psthat have been trained onDs.
Combining and weighing the predictions frommultiple sources, we obtain the reference predic-811tion r?ji=?ks=1us,i(2Jps(xi) = jK?1) for examplexibelonging to relation j, using the ?1 encoding.The relation classifier consists of c functionsf1, .
.
.
, fcusing the one-versus-rest decoding formulti-class classification.2Inspired by the Do-main Adaptive Machine (Duan et al, 2009), wecombine the reference predictions and the labeleddata of the target domain to learn these functions:min{ fj}cj=1c?j=1{1nlnl?i=1( fj(xi)?
rji)2+ ??
fj?2H+?2n?i=nl+1?
fj(xi)?
r?ji?2}, (3)where rji= 2Jyi= jK?1 is the ?1 binary encod-ing for the i labeled sample belonging to relation j.Here, we have multiple objectives: the first termcontrols the training error; the second regularizesthe complexity of the functions fjs in the Repro-ducing Kernel Hilbert Space (RKHS) H ; and thethird prefers the predicted labels of the unlabeleddata Dlto be close to the reference predictions.The third term provides additional pseudo-trainingsamples for the rarer relation classes, if these areavailable in Du.
Parameters ?
and ?
govern thetrade-offs between these objectives.Let K(?, ?)
be the reproducing kernel for H .
Bythe Representer Theorem (Smola and Scholkopf,1998), the solution for Eq.
3 is linear in K(xi, ?
):fj(x) =?ni=1?jiK(xi,x).
Putting this into Eq.
3,parameter vectors ?jare (Belkin et al, 2006):?
?j= (JK + ?(nl+?nu)I)?1JRj.
(4)Here, Rjis an (nl+ nu)-vector, where Rji= rjiifsample i belongs to the labeled set, and Rji= r?i jifit belongs to the unlabeled set; and J is an (nl+nu)-by-(nl+nu) diagonal matrix where the first nldiagonal entries are ones and the rest are ?s.5 ExperimentsWe evaluate our algorithm on two corpora: Au-tomatic Content Extraction (ACE) 2004 andYAGO3.
Table 1 provides some statistics on them.ACE 2004 consists of six domains: Broad-cast Conversations (BC), Broadcast News(BN), Conversational Telephone Speech (CTS),Newswire (NW), Usenet (UN) and Weblog(WL).
There are seven positive relation types:2For two-classes, though, only one function is needed.3http://www.mpi-inf.mpg.de/yago-naga/yago/Table 1: Statistics on ACE 2004 and YAGOProperties ACE 2004 YAGO# relation types 7 20# candidate relations 48,625 68,822# gold relations 4,296 2,000# mentions per entity pair 6 11% mentions with +ve relations 8.8% 21%Physical (Phy), Personal/Social (Per), Employ-ment/Membership/Subsidiary (Emp), Agent-Artifact (Agt), PER/ORG Affiliation (Aff), GPEAffiliation (GPE) and Discourse (Dis).YAGO is an open information extraction dataset.
The relation types of YAGO are built fromWikipedia and WordNet, while the labeled text forYAGO is from Bollegala et al (2011).
It consistsof twenty relation types such as ceo company,bornIn and isMarriedTo, and each of them is con-sidered as a domain in this work.
YAGO is dif-ferent from ACE 2004 in two aspects: there isless overlapping of topics, entity types and rela-tion types between domains; and it has more rela-tion mentions with 11 mentions per pair of entitieson the average.We used Collins parser (Collins, 1999) to parsethe sentences.
The constituent parse trees werethen transformed into dependency parse trees, us-ing the head of each constituent (Jiang and Zhai,2007b).
The candidate relation instances weregenerated by considering all pairs of entities thatoccur in the same sentence.
For the similarity ma-trix W in section 4.1 and the kernel K(?, ?)
in sec-tion 4.2, we used the composite kernel function(Zhang et al, 2006), which is based on structuredfeatures and entity-related features.F1is used to measure the performance of the al-gorithms.
This is the harmonic mean of precisionTP/(TP + FP) and recall TP/(TP + FN), whereTP, FP and FN are the numbers of correct, missingand wrongly recognized relations.5.1 Experimental SettingsFor ACE 2004, we used each of the six domainsas the target domain and the remaining domainsas source domains.
For YAGO, each relation typein YAGO was considered as a domain.
For eachdomain in YAGO, we have a binary classifica-tion task: whether the instance has the relationcorresponding to the domain.
Five-fold cross-validation was used to evaluate the performance.812For every target domain, we divided all data into5 subsets, and we used each subset for testing andthe other four subsets for training.
In the trainingset, we randomly removed most of the positive in-stances of the target domain from the training setexcept for 10% of the labeled data.
This gave usthe weakly-supervised setting.
This was repeatedfive times with different training and test sets.
Wereport the average performance over the five runs.In our experiments, we set ?
= 0.8 in Eq.
1;?= 0.18 in Eq.
2; and ?= 0.1 and ?= 0.3 in Eq.
3.For each target domain, we used k ?
{1,3,5} dif-ferent source domains chosen randomly from theremaining domains.
Thus, the relevance of thesource domains to the target domain varies fromexperiment to experiment.5.2 BaselinesWe compare our framework with several othermethods, including state-of-the-art machine learn-ing, relation extraction and common domain adap-tation methods.
These are described below.In-domain multiclass classifier This is Support-vector-machine (Fan et al, 2008, SVM) usingthe one-versus-rest decoding without removingpositive labeled data (Jiang and Zhai, 2007b)from the target domain.
Its performance can beregarded as an upper bound on the performanceof the cross-domain methods.No-transfer classifier (NT) We only use the fewlabeled instances of the target relation type to-gether with the negative relation instances totrain a binary classifier.Alternate no-transfer classifier (NT-U) We usethe union of the k source-domain labeled datasets Dss and the small set of target-domain la-beled data Dlto train a binary classifier.
It isthen applied directly to predict on the unlabeledtarget-domain data Duwithout any adaptation.Laplacian SVM (L-SVM) This is a semi-supervised learning method based on labelpropagation (Melacci and Belkin, 2011).Multi-task transfer (MTL) This is a learningmethod for weakly-supervised relation extrac-tion (Jiang, 2009).Adaptive domain bootstrapping (DAB) This isan instance-based domain adaptation methodfor relation extraction (Xu et al, 2010).Structural correspondence learning (SCL) Weuse the feature-based domain adaptation ap-proach by Blitzer et al (2007).
We apply SCLon the Dss and Dlto train a model.
The learnedmodel then makes predictions on Du.Domain Adaptation Machine (DAM) We usethe framework of Duan et al (2009), which is amultiple-sources domain adaptation method.For the kernel-based methods above, we use thesame composite kernel used in our method.
Thesource codes of L-SVM, MTL, SCL and DAMwere obtained from the authors.
The others werere-implemented.5.3 Experimental ResultsTables 2, 3 and 4 present the results on ACE 2004(corresponding to k = 1,3,5), and Tables 5 presentthose on YAGO (corresponding to k = 5).From Table 3 and Table 5, we see that theproposed method has the best F1among all theother methods, except for the supervised upperbound (In-domain).
We first notice that NT-Ugenerally does not perform well, and sometimesit performs worse than NT.
The reason is thatNT-U aims to obtain a consensus among the do-mains, and this will give a worse label than NTwhen there are enough irrelevant sources to influ-ence the classification decision wrongly.
In fact,one can roughly deduce that a target domain hasfew relevant source domains by simply comparingcolumns NT with columns NT-U in the tables: adecrease in F1from NT to NT-U suggests that thesource domains are mainly irrelevant.
For exam-ple, for domain BC in ACE 2004, we find that itsF1decreases from NT to NT-U consistently in Ta-bles 2, 3 and 4, which suggests that BN, NW, CTS,UN and WL are generally irrelevant to it; and sim-ilarly for domain CTS.
We investigate this furtherby examining the relevance scores ws, js, and wefind that the decreases in F1from NT to NT-U hap-pen when there are more regions in the target do-main to which source-domains are irrelevant.We find that MTL, DAB and SCL are better thanNT-U when the majority of source domains arerelevant.
This shows that MTL, DAB and SCL areable to make more effective use of relevant sourcesthan NT-U.
Howevever, we find that their perfo-mances are not stable: for example, MTL for tar-get UN in Table 2.
In contrast, we find the per-formance of L-SVM and DAM to be more sta-ble.
The reason is their reduced vulnerability to813Table 2: The F1of different methods on ACE 2004 with k = 1 source domain.
The best performance foreach target domain is in bold.Target In-domain NT NT-U L-SVM MTL DAB SCL DAM RDABC 55.74 30.00 20.31 32.42 32.74 32.12 30.41 33.07 35.43BN 67.24 33.43 38.31 35.40 44.81 27.32 45.27 43.26 47.28NW 68.32 41.48 39.35 41.50 42.28 43.27 44.16 41.69 45.41CTS 72.92 36.60 29.90 36.15 45.06 37.50 44.68 39.40 44.27UN 45.16 21.67 17.55 25.10 18.69 18.78 28.77 26.57 31.07WL 46.46 28.53 23.84 29.90 26.13 24.78 23.71 27.01 30.80Average 57.58 31.95 28.21 33.41 35.02 30.46 29.57 33.50 39.00Table 3: The F1of different methods on ACE 2004 with k = 3 source domains.Target In-domain NT NT-U L-SVM MTL DAB SCL DAM RDABC 55.74 30.00 24.55 32.42 35.26 34.12 37.83 36.08 39.43BN 67.24 33.43 38.31 35.40 49.76 32.15 49.25 45.89 51.28NW 68.32 41.48 43.35 42.50 43.28 43.71 44.16 44.01 46.41CTS 72.92 36.60 30.25 36.15 45.06 37.50 44.68 42.51 49.27UN 45.16 21.67 27.55 25.10 19.72 35.78 31.77 33.29 35.07WL 46.46 28.53 30.72 30.90 33.21 32.81 26.37 32.46 35.11Average 57.58 31.95 32.46 34.20 37.72 36.01 39.01 39.10 42.76Table 4: The F1of different methods on ACE 2004 with k = 5 source domains.Target In-domain NT NT-U L-SVM MTL DAB SCL DAM RDABC 55.74 30.00 27.32 33.07 37.76 35.08 40.38 38.70 42.90BN 67.24 33.43 40.83 36.42 52.69 42.76.
50.47 48.23 53.40NW 68.32 41.48 44.35 43.69 47.80 44.09 45.50 46.06 49.13CTS 72.92 36.60 34.60 38.90 45.06 38.71 47.35 45.69 52.63UN 45.16 21.67 29.34 26.34 35.47 35.44 33.21 34.13 36.02WL 46.46 28.53 32.41 31.56 34.72 32.81 36.89 32.29 37.90Average 57.58 31.95 34.80 35.0 42.25 38.15 42.30 40.84 45.33negative transfer from irrelevant sources by rely-ing on similarity of feature vectors between sourceand target domains based on labeled and unlabeleddata.
Further improvements can still be made, asshown by the better performance of RDA over L-SVM and DAM.
This is achieved by further ad-justing the relevances between source and targetdomains according to regions in the target-domaininput space.We analyzed histogram of the relation types toorder the domains according to the imbalance ofthe class distributions.
Using this, we observethat MTL, DAB and SCL perform relatively badlywhen the target-domain distribution is more im-balanced.
In constrast, L-SVM, DAM and RDAare more robust.Comparing with the baselines, RDA achievesthe best performance on almost all the experi-ments.
Using the two-phase framework, RDAcan successfully transfer useful knowledge even inthe pressence of irrelevant sources and imbalanceddistributions.
For ACE 2004, the improvement inF1over the best baseline can be up to 4.0% andis on average 3.6%.
Similarly for YAGO, the im-provement in F1over the best baseline can be upto 5.5% and is on average 4.3%.Impact of Number of Source Domains Tables2, 3, 4 and 6 also demonstrate that RDA improvesmonotonically as the number of source domainsincreases for both ACE 2004 and YAGO.814Table 5: The F1of different methods on YAGO with k = 5 source domains.Target In-domain NT NT-U L-SVM MTL DAB SCL DAM RDAacquirer acquiree 58.74 32.12 33.19 43.16 45.28 39.08 44.19 45.07 51.15actedIn 77.36 40.73 44.32 50.45 57.18 49.61 58.23 56.37 63.40bornIn 68.32 42.39 40.35 44.38 49.80 48.36 50.67 48.12 56.93ceo company 82.92 47.60 51.27 55.27 61.06 58.33 57.41 59.08 66.71company headquarters 75.16 48.92 52.15 50.13 59.47 61.23 58.36 56.65 64.36created 74.26 46.37 43.58 60.45 60.74 55.08 59.42 57.34 65.28diedIn 81.45 42.78 47.37 57.37 62.69 57.16 65.28 60.44 71.15directed 70.11 44.42 48.29 50.57 54.29 49.09 52.31 50.30 57.71discovered 68.13 37.34 42.51 48.77 53.04 49.82 53.73 51.21 59.12graduatedFrom 69.37 39.28 45.74 51.56 58.22 54.38 56.32 51.17 60.37hasChild 74.56 49.14 50.98 56.07 64.82 53.41 62.38 61.12 66.83hasWonPrize 69.41 38.75 45.72 53.47 57.38 52.76 58.29 54.03 63.13isLeaderOf 79.18 46.31 52.66 58.88 63.49 60.27 63.75 61.51 70.27isMarriedTo 73.33 47.85 48.16 52.31 56.39 50.73 55.35 52.10 62.58livesIn 66.93 36.16 35.15 40.28 50.27 41.72 43.59 48.11 56.91participatedIn 85.38 46.22 48.33 62.48 67.51 61.08 65.38 61.12 71.72person birthplace 77.62 43.43 45.27 49.66 58.47 59.32 57.55 52.14 65.80person field 68.32 36.25 37.93 47.69 54.22 50.46 50.47 48.89 59.47politicianOf 79.10 39.17 42.25 53.38 64.56 62.11 60.74 58.82 68.12worksAt 84.29 45.78 49.78 59.34 65.33 65.44 66.53 63.24 73.31Average 74.20 42.55 45.25 52.28 58.21 53.97 56.80 54.84 63.72Performance Gap From Tables 2 to 4, we ob-serve that the smallest performance gap betweenRDA and the in-domain settings is still high (about12% with k = 5) on ACE 2004.
This is because wehave used a lot less labeled instances in the targetdomains: only 10% are used.
However, the gapsreduces when the number of source domains in-creases.
Comparing with the in-domain results inTable 5 (which is constant with k), Table 6 alsoshows a similar trend on YAGO.
By exploiting thelabeled data in ten source domains in YAGO, ourRDA algorithm can reduce the gap between thecross-domain and in-domain settings to 9%.6 Conclusion and Future WorkIn this paper, we have proposed a robust domainadaptation (RDA) approach for the relation extrac-tion problem where labeled data is scarce.
Ex-isting domain adaptation approaches suffer fromnegative transfer and under imbalanced distribu-tions.
To overcome these, we have proposed atwo-phase approach to transfer only relevant in-formation from multiple source domains, and thusderive accurate and robust predictions on the un-labeled target-domain data.
Experimental resultsTable 6: Average F1of RDA on YAGO# source domains F1k = 1 53.81k = 3 59.43k = 5 63.72k = 10 65.55on ACE 2004 and YAGO have shown that the ourdomain adaptation method achieves the best per-formance on F1measure compared with the otherbaselines when only few labeled target instancesare used.
Because of the practical importance ofdomain adaptation for relation extraction due tolack of labeled data in new domains, we hope ourstudy and findings will lead to further investiga-tions into this problem.AcknowledgmentsThis work is supported by DSO grantDSOCL10021.
We thank Jiang for provid-ing the source code for feature extraction andBollegala for sharing his YAGO dataset.815ReferencesEugene Agichtein and Luis Gravano.
2000.
Snow-ball: Extracting relations from large plain-text col-lections.
In Proceedings of the fifth ACM conferenceon Digital libraries, pages 85?94.
ACM.Michele Banko, Oren Etzioni, and Turing Center.2008.
The tradeoffs between open and traditionalrelation extraction.
Proceedings of ACL-08: HLT,pages 28?36.Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.2006.
Manifold regularization: A geometric frame-work for learning from labeled and unlabeled exam-ples.
The Journal of Machine Learning Research,7:2399?2434.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 120?128.
ACL.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In ACL.Danushka Bollegala, Yutaka Matsuo, and MitsuruIshizuka.
2011.
Relation adaptation: learning toextract novel relations with minimum supervision.In Proceedings of the Twenty-Second internationaljoint conference on Artificial Intelligence-VolumeVolume Three, pages 2205?2210.
AAAI Press.Razvan Bunescu and Raymond Mooney.
2006.
Subse-quence kernels for relation extraction.
Advances inneural information processing systems, 18:171?178.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedingsof the 42nd Annual Meeting on Association for Com-putational Linguistics, pages 423?429.
ACL.Hal Daume and D Marcu.
2007.
Frustratingly easydomain adaptation.
In Annual meeting-associationfor computational linguistics, pages 256?263.Anhai Doan, Pedro Domingos, and Alon Halevy.2003.
Learning to match the schemas of datasources: A multistrategy approach.
Machine Learn-ing, 50(3):279?301.Lixin Duan, Ivor W Tsang, Dong Xu, and Tat-SengChua.
2009.
Domain adaptation from multiplesources via auxiliary classifiers.
In Proceedings ofthe 26th Annual ICML, pages 289?296.
ACM.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S Weld.
2008.
Open information extrac-tion from the web.
Communications of the ACM,51(12):68?74.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Jing Jiang and ChengXiang Zhai.
2007a.
Instanceweighting for domain adaptation in nlp.
In An-nual Meeting-Association For Computational Lin-guistics, pages 264?271.Jing Jiang and ChengXiang Zhai.
2007b.
A systematicexploration of the feature space for relation extrac-tion.
In HLT-NAACL, pages 113?120.Jing Jiang.
2009.
Multi-task transfer learning forweakly-supervised relation extraction.
In Proceed-ings of the 47th Annual Meeting of the ACL: Volume2-Volume 2, pages 1012?1020.Zornitsa Kozareva and Eduard Hovy.
2010.
Not allseeds are equal: Measuring the quality of text min-ing seeds.
In HLT: The 2010 Annual Conference ofthe North American Chapter of the ACL, pages 618?626.
ACL.Stefano Melacci and Mikhail Belkin.
2011.
Laplaciansupport vector machines trained in the primal.
Jour-nal of Machine Learning Research, 12:1149?1184.Filipe Mesquita, Jordan Schmidek, and Denilson Bar-bosa.
2013.
Effectiveness and efficiency of openrelation extraction.
In Proceedings of EMNLP-13,volume 500, pages 447?457.Bonan Min, Ralph Grishman, Li Wan, Chang Wang,and David Gondek.
2013.
Distant supervision forrelation extraction with an incomplete knowledgebase.
In Proceedings of NAACL-HLT, pages 777?782.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1003?1011.
Association forComputational Linguistics.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain sen-timent classification via spectral feature alignment.In Proceedings of the 19th international conferenceon World wide web, pages 751?760.
ACM.Marius Pasca, Dekang Lin, Jeffrey Bigham, AndreiLifchits, and Alpa Jain.
2006.
Organizing andsearching the world wide web of facts - step one:The one-million fact extraction challenge.
In Pro-ceedings of the 21st National Conference on Artifi-cial Intelligence - Volume 2, AAAI?06, pages 1400?1405.
AAAI Press.816Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding semantic similarity in tree kernels for do-main adaptation of relation extraction.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics, pages 1498?1507.Longhua Qian, Guodong Zhou, Fang Kong, Qiaom-ing Zhu, and Peide Qian.
2008.
Exploiting con-stituent dependencies for tree kernel-based semanticrelation extraction.
In Proceedings of the 22nd Con-ference on Computational Linguistics, pages 697?704.
ACL.Ellen Riloff and Rosie Jones.
1999.
Learning dic-tionaries for information extraction by multi-levelbootstrapping.
In Proceedings of the National Con-ference on AI, pages 474?479.Michael T Rosenstein, Zvika Marx, Leslie Pack Kael-bling, and Thomas G Dietterich.
2005.
To transferor not to transfer.
In NIPS 2005 Workshop on Trans-fer Learning, volume 898, pages ?.Gerard Salton and Michael J. McGill.
1986.
Intro-duction to Modern Information Retrieval.
McGraw-Hill, Inc., New York, NY, USA.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In Proceedings of the HLT Confer-ence of the North American Chapter of the ACL,pages 304?311.Alex J Smola and Bernhard Scholkopf.
1998.
Learn-ing with kernels.
Citeseer.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge unifying wordnet and wikipedia.
In Proceed-ings of the 16th international conference on WorldWide Web, pages 697?706.
ACM.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.2012.
Reducing wrong labels in distant supervi-sion for relation extraction.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers-Volume 1, pages721?729.
Association for Computational Linguis-tics.Feiyu Xu, Hans Uszkoreit, Sebastian Krause, and HongLi.
2010.
Boosting relation extraction with lim-ited closed-world knowledge.
In Proceedings ofthe 23rd Conference on Computational Linguistics,pages 1354?1362.
ACL.Wei Xu, Raphael Hoffmann Le Zhao, and Ralph Grish-man.
2013.
Filling knowledge base gaps for distantsupervision of relation extraction.
In Proceedings ofEMNLP-13, pages 665?670.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.2006.
A composite kernel to extract relations be-tween entities with both flat and structured features.In Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 825?832.
Association for Computa-tional Linguistics.Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,Jason Weston, and Bernhard Sch?olkopf.
2003.Learning with local and global consistency.
InNIPS.Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, andJi-Rong Wen.
2009.
Statsnowball: a statistical ap-proach to extracting entity relationships.
In Pro-ceedings of the 18th international conference onWorld wide web, pages 101?110.
ACM.817
