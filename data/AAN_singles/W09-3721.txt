Proceedings of the 8th International Conference on Computational Semantics, pages 235?247,Tilburg, January 2009. c?2009 International Conference on Computational SemanticsAn Ordering of Terms Based on SemanticRelatednessPeter WittekDepartment of Computer ScienceNational University of SingaporeSa?ndor Dara?nyiSwedish School of Library and Information ScienceGo?teborg UniversitySandor.Daranyi@hb.seChew Lim TanDepartment of Computer ScienceNational University of Singaporetancl@comp.nus.edu.sgAbstractTerm selection methods typically employ a statistical measure tofilter or weight terms.
Term expansion for IR may also depend onstatistics, or use some other, non-metric method based on a lexicalresource.
At the same time, a wide range of semantic similarity mea-sures have been developed to support natural language processing taskssuch as word sense disambiguation.
This paper combines the two ap-proaches and proposes an algorithm that provides a semantic order ofterms based on a semantic relatedness measure.
This semantic ordercan be exploited by term weighting and term expansion methods.1 IntroductionSince the early days of the vector space model, it has been debated whetherit is a proper carrier of meaning of texts [23], arguing if distributional sim-ilarity is an adequate proxy for lexical semantic relatedness [3].
With thestatistical, i.e.
devoid of word semantics approaches there is generally noway to improve both precision and recall at the same time, increasing oneis done at the expense of the other.
For example, casting a wider net of235search terms to improve recall of relevant items will also bring in an evengreater proportion of irrelevant items, lowering precision.
In the meantime,practical applications in information retrieval and text classification havebeen proliferating, especially with developments in kernel methods in thelast decade [9, 4].Ordering of terms based on semantic relatedness seeks an answer to thesimple question, can statistical term weighting be eclipsed?
Namely, vari-ants of weighting schemes based on term occurrences and co-occurrencesdominate the information retrieval and text classification scenes.
However,they also have a number of limitations.
The connection between statisticsand word semantics is in general not understood very well.
In other words,a systematic discussion of mappings between theories of word meaning andmodeling them by mathematical objects is missing for the time being.
Fur-ther, enriching weighting schemes by importing their sense content fromlexical resources such as WordNet lacks a theoretical interpretation in termsof lexical semantics.
Combining co-occurrence and lexical resource-basedapproaches for term weighting and term expansion may offer further theo-retical insights, as well as performance benefits.Using vectors in the vector space model as such mathematical objectsfor the representation of term, document or query meaning necessarily ex-presses content mapped on form as a set of coordinates.
These coordinates,at least in the case of the tfidf scheme, are corpus-specific, i.e.
term weightsare neither constant over time nor database independent.
Introducing a se-mantic ordering of terms, hence loading a coordinate with semantic content,reduces the dependence on a specific corpus.In what follows, we will argue that:?
By assigning specific scalar values to terms in an ontology, terms rep-resented by sets of geometric coordinates can be outdone;?
Such values result from a one-dimensional ordering based on the ideaof a sense-preserving distance between terms in a conceptual hierarchy;?
Sense-preserving distances mapped onto a line condense lexical rela-tions and express them as a kind of within-language referential mean-ing pertinent to individual terms, quasi charging their occurrencesindependent of their occurrence rates, i.e.
from the outside;?
This linear order can be used to assist term expansion and term weight-ing.236This paper is organized as follows.
Section 2 discusses the most impor-tant measures for semantic relatedness with regard to the major linguistictheories.
Section 3 introduces an algorithm that creates a linear semanticorder of terms of a corpus, and Section 4 both offers first results in textclassification and discusses some implications.
Finally, Section 5 concludesthe paper.2 Measuring Semantic RelatednessSeveral methods have been proposed for measuring similarity.
One of suchearly proposals was the semantic differential which analyzes the affectivemeaning of terms into a range of different dimensions with the opposedadjectives at both ends, and locates the terms within semantic space [20].Semantic similarity as proposed by Miller and Charles is a continuousvariable that describes the degree of synonymy between two terms [16].
Theyargue that native speakers can order pairs of terms by semantic similarity,for example ship-vessel, ship-watercraft, ship-riverboat, ship-sail, ship-house,ship-dog, ship-sun.
This concept may be extended to quantify relationsbetween non-synonymous but closely related terms, for example airplane-wing.
Semantic distance is the inverse of semantic similarity [17].Semantic relatedness is defined between senses of terms.
Given a relat-edness formula rel(s1, s2) between two senses s1and s2, term relatednessbetween two terms t1and t2can be calculated asrel(t1, t2) = maxs1?sen(t1),s2?sen(t2)rel(s1, s2),where sen(t) is a set of senses of term t [3].Automated systems assign a score of semantic relatedness to a given pairof terms calculated from a relatedness measure.
The absolute score itselfis typically irrelevant on its own, what is important is that the measureassigns a higher score to term pairs which humans think are more relatedand comparatively lower scores to term pairs that are less related [17].The best known theories of word semantics fall in three major groups:1.
?Meaning is use?
[30]: habitual usage provides indirect contextual in-terpretation of any term.
In accord with Carnap, frequency of useexpresses aspects of a conceptual hierarchy.
In terms of logical seman-tics, one regards document groups as value extensions (classes) andindex terms as value intensions (properties) of a (semantic) function237?f?.
Extensions and intensions are inverse proportional: the more prop-erties defined, the less entities they apply to - there are more flowersin general than tulips in particular, for instance.2.
?Meaning is change?
: the stimulus-response theory by Bloomfield andthe biological theory of meaning by von Uexku?ll both stress that themeaning of any action is its consequences.3.
?Meaning is equivalence?
: referential or ostensional theories of mean-ing suggest that ?X = Y for/as long as Z?
[22].Point 2 refers to theories which assign a temporal structure to wordmeaning, they are not discussed here.
Measures that rely on distributionalmeasures (Point 1) and those that use knowledge-rich resources (Point 3)both exist, and they have been individually shown to good quantifiers ofterm similarity each [17], These theories have been individually shown to begood, therefore their combination must be a valid research alternative.A lexical resource in computer science is a structure that captures se-mantic relations among terms.
Such a resource necessarily entails some sortof world view with respect to a given domain.
This is often conceived as a setof concepts, their definitions and their inter-relationships; this is referred toas a conceptualization.
The following types of resources are commonly usedin measuring semantic similarity between terms: dictionary [12], semanticnetworks, such as WordNet [5], thesauri modeled on Roget?s Thesaurus [19].All approaches to measuring semantic relatedness that use a lexical re-source regard the resource as a network or a directed graph, making use ofthe structural information embedded in the graph [8, 3].Distributional similarity, as studied by language technology, covers animportant kind of theories of word meaning and can be hence seen as con-tributing to semantic document indexing and retrieval.
Its predecessors goback a long way, building on the notion of term dependence and structuresderived therefrom [2, 18].
Also called the contextual theory of meaning (see[15] for the historical development of the concept), the underlying distri-butional hypothesis is often cited for explaining how word meaning entersinformation processing [10], and basically equals the claim ?meaning is use?in language philosophy.
Before attempts to utilize lexical resources for thesame purpose, this used to be the sole source of word semantics in informa-tion retrieval, inherent in the exploitation of term occurrences (tfidf) andterm co-occurrences [7, 21, 27], including multiple-level term co-occurrences[11].238Statistical techniques typically suffer from the sparse data problem: theyperform poorly when the terms are relatively rare, due to the scarcity of data.Hybrid approaches attempt to address this problem by supplementing sparsedata with information from a lexical database [24, 8].
In a semantic network,to differentiate between the weights of edges connecting a node and all itschild nodes, one needs to consider the link strength of each specific childlink.
This is a situation in which corpus statistics can contribute.
Ideallythe method chosen should be both theoretically sound and computationallyefficient [8].Following the notation in information theory, the information content(IC) of a concept c can be quantified as follows.IC(c) =1logP (c).where P (c) is the probability of encountering an instance of concept c. In thecase of the hierarchical structure, where a concept in the hierarchy subsumesthose ones below it, this implies that P (c) is monotonic as one moves up inthe hierarchy.
As the node?s probability increases, its information content orits informativeness decreases.
If there is a unique top node in the hierarchy,then its probability is 1, hence its information content is 0.
Given themonotonic feature of the information content value, the similarity of twoconcepts can be formally defined as follows.sim(c1, c2) = maxc?Sup(c1,c2)IC(c) = maxc?Sup(c1,c2)?
log p(c)where Sup(c1, c2) is the set of concepts that subsume both c1and c2.
Tomaximize the representativeness, the similarity value is the information con-tent value of the node whose IC value is the largest among those higher orderclasses.The information content method requires less information on the detailedstructure of a lexical resource and it is insensitive to varying link types [24].On the other hand, it does not differentiate between the similarity values ofany pair of concepts in a sub-hierarchy as long as their lowest super-ordinateclass is the same.
Moreover, in the calculation of information content, apolysemous term will have a large content value if only term frequency dataare used.The distance function between two terms can be written as follows:d(t1, t2) = IC(c1) + IC(c2)?
2IC(LSuper(c1, c2)),239where LSuper(c1, c2) denotes the lowest super-ordinate of c1and c2in alexical resource.
This distance measure also satisfies the properties of ametric [8].3 A Semantic Ordering of TermsTraditional distributional term clustering methods do not provide signifi-cantly improved text representation [13].
Distributional clustering has alsobeen employed to compress the feature space while compromising documentclassification accuracy [1].
Applying the information bottleneck method tofind term clusters that preserve the information about document categorieshas been shown to increase text classification accuracy in certain cases [28].On the other hand, term expansion has been widely researched, withvarying results [21].
These methods generate new features for each docu-ment in the data set.
These new features can be synonyms or homonyms ofdocument terms [26], or expanded features for terms, sentences and docu-ments as in [6].
Several distributional criteria have been used to select termsrelated to the query.
For instance, [25] proposed the principle that the se-lected terms should have a higher probability in the relevant documents thanin the irrelevant documents.
Others examined the impact of determining ex-pansion terms using a minimum spanning tree and some simple linguisticanalysis [29].This section proposes an algorithm that connects term clustering andterm expansion.
It employs a pairwise comparison between the terms tofind a linear order, instead of finding clusters.
In this order, the transitionfrom a term to an adjacent one is ?smooth?
if the semantic distance betweentwo neighboring terms is small.
The dimension of the feature space is notcompressed, yet, groups of adjacent terms can be regarded as semantic clus-ters.
Hence, following the idea of term expansion, adjacent terms can helpto improve the effectiveness of any vector space-based language technology.Let V denote a set of terms {t1, t2, .
.
.
, tn} and let d(ti, tj) denote thesemantic distance between the terms tiand tj.Let G = (V,E) denote a weighted undirected graph, where the weightson the set E are defined by the distances between the terms.Finding a semantic ordering of terms can be translated to a graph prob-lem: a minimum-weight Hamiltonian path S of G gives the ordering byreading the nodes from one end of the path to the other.
G is a completegraph, therefore such a path always exists, but finding it is an NP-completeproblem.
The following greedy algorithm is similar to the nearest neighbor240heuristic for the solution of the traveling salesman problem.
It creates agraph G?= (S, T ), where S = V and T ?
E. This G?graph is a span-ning tree of G in which the maximum degree of a node is two, that is, theminimum spanning tree is a path between two nodes.Step 1 Find the term at the highest stage of the hierarchy in a lexicalresource.ts= argminti?Vdepth(ti).This seed term is the first element of V?, V?= {ts}.
Remove it fromthe set V :V := V \{ts}.Step 2 Let tldenote the leftmost term of the ordering and trthe rightmostone.
Find the next two elements of the ordering:t?l= argminti?Vd(ti, tl),t?r= argminti?V \{t?l}d(ti, tr).Step 3 If d(tl, t?l) < d(tr, t?r) then add t?lto V?, E?
:= E??
{e(tl, t?l)}, andV := V \{t?l}.
Else add t?rto V?, E?
:= E??
{e(tr, t?r)} and V := V \{t?r}.Step 4 Repeat from Step 2 until V = ?.The computational cost of the algorithm is O(n2).
The above algorithmcan be thought of as a modified Prim?s algorithm, but it does not find theoptimal minimum-weight spanning tree.The validity of the ordering algorithm is discussed as follows.1.
The ordering is possible.
Starting from the seed term, the candidatesets will always contain elements, which either share the same hyper-nym or are hypernyms of each other.2.
The ordering is good enough.
The quality will also depend on thelexical resource in question.
Further, the complexity of human lan-guages makes the creation of even a near perfect semantic network ofits concepts impossible.
Thus in many ways the lexical resource-basedmeasures are as good as the networks on which they are based.3.
The distance between adjacent terms is uniform.
By the constructionof the ordering, it is obvious that the distances will not be uniform.2414 DiscussionWe were interested in how the distances of consecutive index terms change ifwe apply the semantic ordering.
We indexed the ModApte split of Reuters-21578 benchmark corpus with a WordNet-based stemmer.
The indexingfound 12643 individual terms.
Prior to the semantic ordering, terms wereassumed to be in an arbitrary order.
Measuring the Jiang-Conrath distancebetween the arbitrarily ordered terms, the average distance was 1.68.
Notethat the Jiang-Conrath distance was normalized to the interval [0, 2].
Fig-ure 1 shows the distribution of distances.
The histogram has a high peakat the maximum distance, indicating that the original arrangment had littleto do with semantic distance.
However, there were few terms with zero orlittle distance between them.
This is due to terms which are related andstart with the same word or stem.
For example, account, account execu-tive, account for, accountable, accountant, accounting principle, accountingstandard, accounting system, accounts payable, accounts receivable.Figure 1: Distribution of Distances Between Adjacent Terms in an ArbitraryOrderAfter the semantic ordering of the term by the proposed algorithm, boththe average distance and the Jiang-Conrath distance were 0.56.
About onethird of the terms had very little distance between each other (see Figure 2).Nevertheless, over 10 % of the total terms still had the maximum distance.This is due to the non-optimal nature of the proposed term-ordering algo-rithm.
These terms add noise to the classification.
The noisy terms occur242typically at the two sides of the scale, being the leftmost and the rightmostones.
While it is easy to find terms close to each other in the beginning, asthe algorithm proceeds, fewer terms remain in the pool to be chosen.
Forinstance, brand, brand name, trade name, label are in the 33rd, 34th, 35thand 36th position on the left side counting from the seed respectively, whilewindy, widespread, willingly, whatsoever, worried, worthwhile close the leftside, apparently sharing little in common.Figure 2: Distribution of Distances Between Adjacent Terms in a SemanticOrder Based on Jiang-Conrath DistanceWe conducted experiments on the ten most common categories of theModApte split of Reuters-21578.
We trained support vector machines witha linear kernel to compare the micro- and macro-average F1measures fordifferent methods.
Table 1 summarizes the results.
The baseline vectorspace model has zero expansion terms.
Neighboring terms of the semanticorder were chosen as expansion terms.
We found that increasing the numberof expansion terms also increases the effectiveness of classification, however,effectiveness decreases after 4 expansions for micro-F1 and after 6 expansionsfor macro-F1.5 ConclusionsTerms can be corpus- or genre-specific.
Manually constructed general-purposelexical resources include many usages that are infrequent in a particular cor-243Number ofExpansion Micro-F1Macro-F1Terms0 0.900 0.8262 0.901 0.8264 0.905 0.8286 0.898 0.8308 0.896 0.827Table 1: Micro-Average and Macro F1-measure, Reuters-21578pus or genre of documents, and therefore of little use.
For example, one ofthe 8 senses of company in WordNet is a visitor/visitant, which is a hyponymof person [14].
This usage of the term is practically never used in newspaperarticles, hence distributional attributes should be taken into considerationwhen creating a linear ordering of terms.Integrating lexical resources into an upgraded semantic weighting schemethat could augment statistical term weighting is a prospect that cannot beoverlooked in information retrieval and text categorization.
Our first resultswith such a scheme in text categorization.
At the same time, the resultsalso raise the question, does assigning specific scalar values to terms in anontology, this far represented by their geometric coordinates only, turn themmetaphorically into band lines of elements in a conceptual spectrum.
Weanticipate that applying other types of kernels to the task may bring a newset of challenging results.References[1] L.D.
Baker and A.K.
McCallum.
Distributional clustering of wordsfor text classification.
In Proceedings of SIGIR-98, 21st ACM Inter-national Conference on Research and Development in Information Re-trieval, pages 96?103, Melbourne, Australia, August 1998.
ACM Press,New York, NY, USA.
[2] M.A.
Ba?rtschi.
Term dependence in information retrieval models.
Mas-ter?s thesis, Swiss Federal Institute of Technology, 1984.244[3] A. Budanitsky and G. Hirst.
Evaluating WordNet-based measures oflexical semantic relatedness.
Computational Linguistics, 32(1):13?47,2006.
[4] N. Cristianini, J. Shawe-Taylor, and H. Lodhi.
Latent semantic kernels.Journal of Intelligent Information Systems, 18(2):127?152, 2002.
[5] C. Fellbaum.
WordNet: An Electronic Lexical Database.
MIT Press,Cambridge, MA, USA, 1998.
[6] E. Gabrilovich and S. Markovitch.
Feature generation for text cate-gorization using world knowledge.
In Proceedings of IJCAI-05, 19thInternational Joint Conference on Artificial Intelligence, volume 19,Edinburgh, UK, 2005.
Lawrence Erlbaum Associates Ltd.[7] S. I. Gallant.
A practical approach for representing context and forperforming word sense disambiguation using neural networks.
NeuralComputation, 3:293?309, 1991.
[8] J.J. Jiang and D.W. Conrath.
Semantic similarity based on corpusstatistics and lexical taxonomy.
In Proceedings of the International Con-ference on Research in Computational Linguistics, pages 19?33, Taipei,Taiwan, 1997.
[9] T. Joachims.
Text categorization with support vector machines: Learn-ing with many relevant features.
In Proceedings of ECML-98, 10thEuropean Conference on Machine Learning, pages 137?142, Chemnitz,Germany, April 1998.
Springer-Verlag, London, UK.
[10] J. Karlgren and M. Sahlgren.
From words to understanding.
Founda-tions of Real-World Intelligence, pages 294?308, 2001.
[11] A. Kontostathis and W.M.
Pottenger.
A framework for understandinglatent semantic indexing (LSI) performance.
Information Processingand Management, 42(1):56?73, 2006.
[12] M. Lesk.
Automatic sense disambiguation using machine readable dic-tionaries: How to tell a pine cone from an ice cream cone?
In Proceed-ings of SIGDOC-86, 5th Annual International Conference on SystemsDocumentation, pages 24?26, New York, NY, USA, 1986.
ACM Press.
[13] D.D.
Lewis.
An evaluation of phrasal and clustered representationson a text categorization task.
In Proceedings of SIGIR-92, 15th ACM245International Conference on Research and Development in InformationRetrieval, pages 37?50, Copenhagen, Denmark, June 1992.
ACM Press,New York, NY, USA.
[14] D. Lin.
Automatic retrieval and clustering of similar words.
In Pro-ceedings of COLING-ACL Workshop on Usage of WordNet in Natu-ral Language Processing Systems, volume 98, pages 768?773, Montre?al,Que?bec, Canada, August 1998.
ACL, Morristown, NJ, USA.
[15] J. Lyons.
Semantics.
Cambridge University Press, New York, NY,USA, 1977.
[16] G. Miller and W. Charles.
Contextual correlates of semantic similarity.Language and Cognitive Processes, 6(1):1?28, 1991.
[17] S. Mohammad and G. Hirst.
Distributional measures as proxies forsemantic relatedness.
Submitted for publication, 2005.
[18] J. Morris, C. Beghtol, and G. Hirst.
Term relationships and their contri-bution to text semantics and information literacy through lexical cohe-sion.
In Proceedings of the 31st Annual Conference of the Canadian As-sociation for Information Science, Halifax, Nova Scotia, Canada, May2003.
[19] J. Morris and G. Hirst.
Lexical cohesion computed by thesaural rela-tions as an indicator of the structure of text.
Computational Linguistics,17(1):21?48, 1991.
[20] C.E.
Osgood.
The nature and measurement of meaning.
PsychologicalBulletin, 49(3):197?237, 1952.
[21] H.J.
Peat and P. Willett.
The limitations of term co-occurrence data forquery expansion in document retrieval systems.
Journal of the Ameri-can Society for Information Science, 42(5):378?383, 1991.
[22] C.S.
Peirce.
Logic as semiotic: The theory of signs.
PhilosophicalWritings of Peirce, pages 98?119, 1955.
[23] V.V.
Raghavan and S.K.M.
Wong.
A critical analysis of vector spacemodel for information retrieval.
Journal of the American Society forInformation Science, 37(5):279?287, 1986.246[24] P. Resnik.
Using information content to evaluate semantic similarity ina taxonomy.
In Proceedings of IJCAI-95, 14th International Joint Con-ference on Artificial Intelligence, volume 1, pages 448?453, Montre?al,Que?bec, Canada, August 1995.
[25] S.E.
Robertson.
On term selection for query expansion.
Journal ofDocumentation, 46(4):359?364, 1990.
[26] M.D.E.B.
Rodriguez and J.M.G.
Hidalgo.
Using WordNet to com-plement training information in text categorisation.
In Procedings ofRANLP-97, 2nd International Conference on Recent Advances in Natu-ral Language Processing.
John Benjamins Publishing, Amsterdam, TheNetherlands, 1997.
[27] H. Schutze and T. Pedersen.
A co-occurrence-based thesaurus andtwo applications to information retrieval.
Information Processing andManagement, 3(33):307?318, 1997.
[28] N. Slonim and N. Tishby.
The power of word clusters for text clas-sification.
In Proceedings of ECIR-01, 23rd European Colloquium onInformation Retrieval Research, Darmstadt, Germany, 2001.
[29] A.F.
Smeaton and C.J.
van Rijsbergen.
The retrieval effects of queryexpansion on a feedback document retrieval system.
The ComputerJournal, 26(3):239?246, 1983.
[30] L. Wittgenstein.
Philosophical Investigations.
Blackwell Publishing,Oxford, UK, 1967.247
