Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1089?1099,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsEfficient Left-to-Right Hierarchical Phrase-based Translation withImproved ReorderingMaryam Siahbani, Baskaran Sankaran, Anoop SarkarSimon Fraser UniversityBurnaby BC.
CANADA{msiahban,baskaran,anoop}@cs.sfu.caAbstractLeft-to-right (LR) decoding (Watanabe et al2006b) is a promising decoding algorithm forhierarchical phrase-based translation (Hiero).It generates the target sentence by extendingthe hypotheses only on the right edge.
LR de-coding has complexity O(n2b) for input of nwords and beam size b, compared toO(n3) forthe CKY algorithm.
It requires a single lan-guage model (LM) history for each target hy-pothesis rather than two LM histories per hy-pothesis as in CKY.
In this paper we present anaugmented LR decoding algorithm that buildson the original algorithm in (Watanabe et al2006b).
Unlike that algorithm, using experi-ments over multiple language pairs we showtwo new results: our LR decoding algorithmprovides demonstrably more efficient decod-ing than CKY Hiero, four times faster; and byintroducing new distortion and reordering fea-tures for LR decoding, it maintains the sametranslation quality (as in BLEU scores) ob-tained phrase-based and CKY Hiero with thesame translation model.1 IntroductionHiero (Chiang, 2007) models translation using a lex-icalized synchronous context-free grammar (SCFG)extracted from word aligned bitexts.
Typically,CKY-style decoding is used for Hiero with timecomplexity O(n3) for source input with n words.Scoring the target language output using a lan-guage model within CKY-style decoding requirestwo histories per hypothesis, one on the left edgeof each span and one on the right, due to the factthat the target side is not generated in left to rightorder, but rather built bottom-up from sub-spans.This leads to complex problems in efficient lan-guage model integration and requires state reduc-tion techniques (Heafield et al 2011; Heafield etal., 2013).
The size of a Hiero SCFG grammar istypically larger than phrase-based models extractedfrom the same data creating challenges in rule ex-traction and decoding time especially for largerdatasets (Sankaran et al 2012).In contrast, the LR-decoding algorithm couldavoid these shortcomings such as faster time com-plexity, reduction in the grammar size and the sim-plified left-to-right language model scoring.
Itmeans LR decoding has the potential to replaceCKY decoding for Hiero.
Despite these attractiveproperties, we show that the original LR-Hiero de-coding proposed by (Watanabe et al 2006b) doesnot perform to the same level of the standard CKYHiero with cube pruning (see Table 3).
In addition,the current LR decoding algorithm does not obtainBLEU scores comparable to phrase-based or CKY-based Hiero models for different language pairs (seeTable 4).
In this paper we propose modifications tothe LR decoding algorithm that addresses these limi-tations and provides, for the first time, a true alterna-tive to the standard CKY Hiero algorithm that usesleft-to-right decoding.We introduce a new extended version of the LRdecoding algorithm presented in (Watanabe et al2006b) which is demonstrably more efficient thanthe CKY Hiero algorithm.
We measure the effi-ciency of the LR Hiero decoder in a way that isindependent of the choice of system and program-ming language by measuring the number of lan-guage model queries.
Although more efficient, thenew LR decoding algorithm suffered from lowerBLEU scores compared to CKY Hiero.
Our anal-ysis of left to right decoding showed that it has morepotential for search errors due to early pruning ofgood hypotheses.
This is unlike bottom-up decoding(CKY) which keeps best hypotheses for each span.To address this issue, we introduce two novel fea-tures into the Hiero SMT model that deal with re-ordering and distortion.
Our experiments show thatLR decoding with these features using prefix lexi-1089calized target side rules equals the scores obtainedby CKY decoding with prefix lexicalized target siderules and phrase-based translation system.
It per-forms four times fewer language model queries onaverage, compare to CKY Hiero decoding with un-restricted Hiero rules: 6466.7 LM queries for CKYHiero (with cube pruning) compared to 1500.45 LMqueries in LR Hiero (with cube pruning).
Whiletranslation quality suffers by only about 0.67 inBLEU score on average, across two different lan-guage pairs.2 Left-to-Right Decoding for HieroHierarchical phrase-based SMT (Chiang, 2005; Chi-ang, 2007) uses a synchronous context free gram-mar (SCFG), where the rules are of the form X ??
?, ?
?, where X is a non-terminal, ?
and ?
arestrings of terminals and non-terminals.Chiang (2007) places certain constraints on theextracted rules in order to simplify decoding.
Thisincludes limiting the maximum number of non-terminals (rule arity) to two and disallowing any rulewith consecutive non-terminals on the foreign lan-guage side.
It further limits the length of the initialphrase-pair as well as the number of terminals andnon-terminals in the rule.
For translating sentenceslonger than the maximum phrase-pair length, the de-coder relies on additional glue rules S ?
?X,X?and S ?
?SX,SX?
that allows monotone combi-nation of phrases.
The glue rules are used when norules could match or the span length is larger thanthe maximum phrase-pair length.2.1 Rule Extraction for LR DecodingLeft-to-right Hiero (Watanabe et al 2006b) gener-ates the target hypotheses left to right, but for syn-chronous context-free grammar (SCFG) as used inHiero.
The target-side rules are constrained to beprefix lexicalized.
These constrained SCFG rulesare defined as:X ?
?
?,?b ??
(1)where ?
is a mixed string of terminals and non-terminals.
?b is a terminal sequence prefixed to thepossibly empty non-terminal sequence ?.
For thesake of simplicity, We refer to these type of rules astheir workstudentsX1X2X6X4 X5X3havenot yetdone.schuler ihre noch nicht gemacht haben .arbeitstudents have done their workyet .not(b)(a)gemachtschulerX1X2X6X5 X4X3 habennoch nichtihre arbeit.123 645Figure 1: (a): A word-aligned German-English sentencepair.
The bars above the source words indicate phrase-pairs having at least two words.
(b): its correspondingleft-to-right target derivation tree.
Superscripts on thesource non-terminals show the indices of the rules (seeFig 2) used in derivation.GNF rules1 in this paper.Rule extraction is similar to Hiero, except anyrules violating GNF form on the target side areexcluded.
Rule extraction considers each smallersource-target phrase pair within a larger phrase pairand replaces the spans with non-terminal X , yield-ing hierarchical rules.
Figure 1(a) shows a word-aligned German-English sentence with a phrasepair ?ihre arbeit noch nicht gemacht haben,have not yet done their work?
that will lead to aSCFG rule.
Given other smaller phrases (marked bybars above the source side), we extract a GNF rule2:X ?
?X1noch nicht X2haben, have not yet X2X1?
(2)In order to avoid data sparsity and for better gen-eralization, Watanabe et al(2006b) adds four gluerules for each lexical rule ?
?f, e??
which are analo-gous to the glue rules defined in (Chiang, 2007) (seeabove) except that these glue rules for LR decoding1Griebach Normal Form (GNF), although the synchronousgrammar is not in this normal form, rather only the target sideis prefix lexicalized as if it were in GNF form.2 LR-Hiero rule extraction excludes non-GNF rules such asX ?
?X1 noch nicht gemacht X2, X2 not yet done X1?.1090allow reordering as well.X ?
?
?fX1, e?X1?
X ?
?X1?fX2, e?X1X2?X ?
?X1?f, e?X1?
X ?
?X1?fX2, e?X2X1?
(3)It might appear that the restriction that target-siderules be GNF is a severe restriction on the cover-age of possible hypotheses compared to the full setof rules permitted by the Hiero extraction heuris-tic.
However there is some evidence in the liter-ature that discontinuous spans on the source sidein translation rules is a lot more useful than dis-continuous spans in the target side (which is disal-lowed in the GNF).
For instance, (Galley and Man-ning, 2010) do an extensive study of discontinuousspans on source and target side and show that sourceside discontinuous spans are very useful but remov-ing discontinuous spans on the target side only low-ers the BLEU score by 0.2 points (using the JoshuaSMT system on Chinese-English).
Removing dis-continuous spans means that the target side ruleshave the form: uX,Xu,XuX,XXu, or uXX ofwhich we disallow Xu,XuX,XXu.
Zhang andZong (2012) also conduct a study on discontinuousspans on source and target side of Hiero rules andconclude that source discontinuous spans are alwaysmore useful than discontinuities on the target sidewith experiments on four language pairs (zh-en, fr-en, de-en and es-en).
As we shall also see in ourexperimental results (see Table 4) we can get closeto the BLEU scores obtained using the full set of Hi-ero rules by using only target lexicalized rules in ourLR decoder.2.2 LR-Hiero DecodingLR-Hiero decoding uses a top-down depth-firstsearch, which strictly grows the hypotheses in targetsurface ordering.
Search on the source side followsan Earley-style search (Earley, 1970), the dot jumpsaround on the source side of the rules based on theorder of nonterminals on the target side.
This searchis integrated with beam search or cube pruning toefficiently find the k-best translations.Several important details about the algorithm ofLR-Hiero decoding are implicit and unexplainedin (Watanabe et al 2006b).
In this section we de-scribe the LR-Hiero decoding algorithm in more de-tail than the original description in (Watanabe et alAlgorithm 1: LR-Hiero Decoding1: Input sentence: f = f0f1.
.
.
fn2: F = FutureCost(f) (Precompute future cost forspans)3: for i = 0, .
.
.
, n do4: Si = {} (Create empty stacks)5: h0= (?s?, [[0, n]], ?,F[0,n]) (Initial hypothesis4-tuple)6: Add h0to S0(Push initial hyp into first Stack)7: for i = 0, .
.
.
, n?
1 do8: for each h in Si do9: [u, v] = pop(hs) (Pop first uncovered spanfrom list)10: R = GetSpanRules([u, v]) (Extract rulesmatching the entire span [u, v])11: for r ?
R do12: h?
= GrowHypothesis(h, r, [u, v],F) (Newhypothesis)13: Add h?
to Sl, where l = |h?cov| (Add newhyp to stack)14: return arg max(Sn)15: GrowHypothesis(h, r, [u, v],F)16: h?
= (h?t = ?, h?s = hs, h?cov = ?, h?c = 0)17: rX = {Xj , Xk, .
.
.
|j C k C .
.
.}
(Get NTs insurface order)18: for each X in reverse(rX) do19: push(h?s, span(X)) (Push uncovered spans toLIFO list)20: h?t = Concatenate(ht, rt)21: h?cov = UpdateCoverage(hcov, rs)22: h?c = ComputeCost(g(h?
),F?h?cov )23: return h?2006b).
We explain our own modified algorithm forLR decoding with cube pruning in Section 2.3.Algorithm 1 shows the pseudocode for LR de-coding.
Decoding the example in Figure 1(b)is explained using a walk-through shown in Fig-ure 2.
Each partial hypothesis h is a 4-tuple(ht, hs, hcov, hc): consisting of a translation prefixht, a (LIFO-ordered) list hs of uncovered spans,source words coverage set hcov and the hypothesiscost hc.
The initial hypothesis is a null string withjust a sentence-initial marker ?s?
and the list hs con-taining a span of the whole sentence, [0, n].
The hy-potheses are stored in stacks S0, .
.
.
, Sn, where eachstack corresponds to a coverage vector of same size,covering same number of source words (Koehn etal., 2003).At the beginning of beam search the initial hy-1091?
X ?
schuler ihre arbeit nochnicht gemacht haben .
?schuler ?
X11?ihrearbeit nochnicht gemacht haben .
?schuler ?
X12?ihre arbeit nochnicht gemacht ?
haben X 22?.
?schuler X 13?ihrearbeit ?
nochnicht ?
X23?gemacht?
haben X 22?.
?schuler ?
X13 ?ihre arbeit?
nochnicht gemacht haben X 22?.
?schuler ihre arbeit nochnicht gemacht haben ?X 22?.
?schuler ihre arbeit nochnicht gemacht haben .1) X?
?schuler X1/ students X1?2) X?
?X1heban X 2/have X 1X 2?3 )X?
?X 1nochnicht X2/not yet X 2X 1?4 ) X?
?gemacht /done ?5 )X??
ihre arbeit / their work ?6 )X??
./ .
?
[0,8]students [1,8 ]students have [1,6 ][7,8]students have not yet [5,6] [1,3 ][7,8]students have not yet done [1,3 ][7,8]students have not yet done their work [7,8]students have not yet done their work .rules source side coverage hypothesisGG <s><s><s><s><s><s><s></s>Figure 2: Illustration of the LR-Hiero decoding process in Figure 1.
(a) Rules pane show the rules used in the derivation(glue rules are marked byG) (b) Decoder state using Earley dot notation (superscripts show rule#) (c) Hypotheses paneshowing translation prefix and ordered list of yet-to-be-covered spans.pothesis h0 is added to the decoder stack S0 (line 6in Algoorithm 1).
Hypotheses in each decoder stackare expanded iteratively, generating new hypotheses,which are added to the latter stacks corresponding tothe number of source words covered.
In each step itpops from the LIFO list hs, the span [u, v] of thenext hypothesis h to be processed.All rules that match the entire span [u, v] are thenobtained efficiently via pattern matching (Lopez,2007).
GetSpanRules addresses possible ambigui-ties in matched rules to the given span [u, v].
Forexample, given a rule r, with source side rs :?X1 the X2?
and source phrase p : ?ok, the morethe better?.
There is ambiguity in matching r top.
GetSpanRules returns a distinct matched rule foreach possible matching.The GrowHypothesis routine creates a new can-didate by expanding given hypothesis h using ruler and computes the complete hypothesis score in-cluding language model score.
Since the target-siderules are in GNF, the translation prefix of the newhypothesis is obtained by simply concatenating theterminal prefixes of h and r in same order (line 20).UpdateCoverage updates source word coverage setusing the source side of r. The hs list is built bypushing the non-terminal spans of rule r in a reverseorder (lines 17 and 18).
The reverse ordering main-tains the left-to-right generation of the target side.In the walk-through in Figure 2, the derivationprocess starts by expanding the initial hypothesis h0(first item in the right pane of Fig 2) with the rule(rule #1 in left pane) to generate a new partial candi-date having a terminal prefix of ?s?
students (seconditem in right pane).
The second item in the middlepane shows the current position of the parser em-ploying Earley?s dot notation, indicating that the firstword has already been translated.
Now the decoderconsiders the second hypothesis and pops the span[1, 8].
It then matches the rule (#2) and pushes thespans [1, 6] and [7, 8] into the list hs in the reverseorder of their appearance in the target-side rule.
Ateach step the new hypothesis is added to the decoderstack Sl depending on the number of covered wordsin the new hypothesis (line 13 in Algorithm 1).For pruning we use an estimate of the future cost3of the spans uncovered by current hypothesis to-gether with the hypothesis cost.
The future cost isprecomputed (line 2 Algorithm 1) in a way simi-lar to the phrase-based models (Koehn et al 2007)using only the terminal rules of the grammar.
TheComputeCost method (line 22 in Algorithm 1) usesthe usual log-linear model and scores a hypothesisbased on its different feature scores g(h?)
and thefuture cost of the yet to be covered spans (F?h?cov ).Time complexity of left to right Hiero decoding withbeam search is O(n2b) in practice where n is thelength of source sentence and b is the size of beam(Huang and Mi, 2010).2.3 LR-Hiero Decoding with Cube PruningThe Algorithm 1 presented earlier does an ex-haustive search as it generates all possible partialtranslations for a given stack that are reachable fromthe hypotheses in previous stacks.
However only afew of these hypotheses are retained, while majorityof them are pruned away.
The cube pruning tech-nique (Chiang, 2007) avoids the wasteful generationof poor hypotheses that are likely to be pruned awayby efficiently restricting the generation to only highscoring partial translations.We modify the cube pruning for LR-decodingthat takes into account the next uncovered span to3 Watanabe et al(2006b) also use a similar future cost, eventhough it is not discussed in the paper (p.c.
).1092Algorithm 2: LR-Hiero Decoding with Cube Pruning1: Input sentence: f = f0f1.
.
.
fn2: F = FutureCost(f) (Precompute future cost forspans)3: S0= {} (Create empty initial stack)4: h0= (?s?, [[0, n]], ?,F[0,n]) (Initial hypothesis4-tuple)5: Add h0to S0(Push initial hyp into first Stack)6: for i = 1, .
.
.
, n do7: cubeList = {} (MRL is max rule length)8: for p = max(i?
MRL, 0), .
.
.
, i?
1 do9: {G} = Grouped(Sp) (Group based on the firstuncovered span)10: for g ?
{G} do11: [u, v] = gspan12: R = GetSpanRules([u, v])13: for Rs ?
R do14: cube = [ghyps, Rs]15: Add cube to cubeList16: Si = Merge(cubeList,F) (Create stack Si andadd new hypotheses to it, see Figure 3)17: return arg max(Sn)18: Merge(CubeList,F)19: heapQ = {}20: for each (H,R) in cubeList do21: [u, v] = span of rule R22: h?
= GrowHypothesis(h1, r1, [u, v],F) (fromAlgorithm 1)23: push(heapQ, (h?c, h?, [H,R])24: hypList = {}25: while |heapQ| > 0 and |hypList| < K do26: (h?c, h?, [H,R]) = pop(heapQ)27: push(heapQ,GetNeighbours([H,R])28: Add h?
to hypList29: return hypListbe translated indicated by the Earley?s dot nota-tion.
The Algorithm 2 shows the pseudocode forLR-decoding using cube pruning.
The structure ofstacks and hypotheses and computing the future costis similar to Algorithm 1 (lines 1-5).
To fill stackSi, it iterates over previous stacks (line 8 in Algo-rithm 2) 4.
All hypotheses in each stack Sp (cov-ering p words on the source-side) are first parti-tioned into a set of groups, {G}, based on theirfirst uncovered span (line 9) 5.
Each group g is a4As the length of rules are limited (at most MRL), we canignore stacks with index less than i?
MRL5The beam search decoder in Phrase-based system (Huangand Chiang, 2007; Koehn et al 2007; Sankaran et al 2010)2-tuple (gspan, ghyps), where ghyps is a list of hy-potheses which share the same first uncovered spangspan.
Rules matching the span gspan are obtainedfrom routine GetSpanRules, which are then groupedbased on unique source side rules (i.e.
each Rs con-tains rules that share the same source side s but havedifferent target sides).
Each ghyps and possible Rs6create a cube which is added to cubeList.In LR-Hiero, each hypothesis is developed withonly one uncovered span, therefore each cube al-ways has just two dimensions: (1) hypotheses withthe same number of covered words and similar firstuncovered span, (2) rules sharing the same sourceside.
In Figure 3(a), each group of hypotheses,ghyps, is shown in a green box (in stacks), and eachrectangle on the top is a cube.
Figure 3 is using theexample in Figure 2.The Merge routine is the core function of cubepruning which generates the best hypotheses fromall cubes (Chiang, 2007).
For each possible cube,(H,R), the best hypothesis is generated by callingGrowHypothesis(h1, r1, span,F) where h1 andr1 are the best hypothesis and rule in H and R re-spectively (line 22).
Figure 3 (b) shows a more de-tailed view of a cube (shaded cube in Figure 3(a)).Rows are hypotheses and columns are rules whichare sorted based on their scores.The first best hypotheses, h?, along with theirscore, h?c and corresponding cube, (H,R) areplaced in a priority queue, heapQ (triangle in Fig-ure 3).
Iteratively the best hypothesis is poppedfrom the queue (line 26) and its neighbours inthe cube are added to the priority queue (usingGetNeighbours([H,Q])).
It continues to generateall K best hypotheses.
Using cube pruning tech-nique, each stack is filled with K best hypotheseswithout generating all possible hypotheses in eachcube.groups the hypotheses in a given stack based on their coveragevector.
But this idea does not work in LRHiero decoding inwhich the expansion of each hypothesis is restricted to its firstuncovered span.
We have also tried another way of groupinghypotheses: group by all uncovered spans, hs.
Our experimentsdid not show any significant difference between the final results(BLEU score), therefore we decided to stick to the simpler idea:using first uncovered span for grouping.6Note that, just rules whose number of terminals in theirsource side is equal to i?
p can be used.1093...1 2 3 4 5[1,8][1,8][0,3][0,3][5,8][1,6][1,6][1,6][0,3][0,3][5,6][5,6][1,4][6,8][6,8][5,6][5,6][5,6][1,3][7,8][1,3]theitrerthe thewtheo tiertiektoeitreorehtetseudnX126nd453a246vn4y2n4.ocwl4tsehdnX126n453d46vn43gm231y4.ocwl4thekbXb(gd453a246vn4y2n4.ocwl4tt o)312 1v62 1vtrerthe thewtheo tiertiektoeitreothei(a) (b)Figure 3: Example of generating hypotheses in cube pruning using Figure 2: (a) Hypotheses in previous stacks aregrouped based on their first uncovered span, and build cubes (grids on top).
Cubes are in different sizes becauseof different number of rules and group sizes.
Cubes are fed to a priority queue (triangle) and new hypotheses areiteratively popped from the queue and added to the current stack, S5.
(b) Generating hypotheses from a cube.
The topside of the grid denotes the target side of rules sharing the same source side (Rs) along with their scores.
Left side ofthe grid shows the hypotheses in a same group, their first uncovered span and their scores.
Hypothesis generated fromrow 1 and column 1 is added to the queue at first.
Once it is popped from the queue, its neighbours (in the grid) aresubsequently added to the queue.Figure 3 (b) shows the derivation of the two besthypotheses from the cube.
The best hypothesis ofthis cube which is likely created from the best hy-pothesis and rule (left top most entry) is poppedat first step.
Then, GetNeighbours calls GrowHy-pothesis to generate next potential best hypothesesof this cube (neighbours of the popped entry whichare shaded in Figure 3(b)).
These hypotheses areadded to the priority queue.
In the next iteration, thebest hypothesis is popped from all candidates in thequeue and algorithm continues.3 FeaturesWe use the following standard SMT features for thelog-linear model of LR-Hiero: relative-frequencytranslation probabilities p(f |e) and p(e|f), lexicaltranslation probabilities pl(f |e) and pl(e|f), a lan-guage model probability, word count and phrasecount.
In addition we also use the glue rule countand the two reordering penalty features employedby Watanabe et al(2006b; 2006a).
These featurescompute the height and width (span size of the en-tire subtree) of all subtrees which are backtraced inthe derivation of a hypothesis.
A non-terminal Xiis pushed into the LIFO list of a partial hypothesis;it?s backtrace refers to the set of NTs that must bepopped before Xi.In Figure 1(b), X2 has two subtrees X3 and X6,where X3 should be processed before X6.
The sub-tree rooted atX3 in Figure 1(b) has a height of 2 andspan [1, 6] having a width of 5.
Similarly, X4 shouldbe backtraced beforeX5 and has height and width of1.
Backtracing applies only for rules having at leasttwo non-terminals.
Thus the total height and widthpenalty for this derivation are 3 and 6 respectively.However, the height and width features do notdistinguish between a rule that reorders the non-terminals in source and target from one that pre-serves the ordering.
Rules #2 and #3 in Figure 2are treated equally although they have different or-derings.
The decoder is thus agnostic to this dif-ference and would not be able to exploit this ef-fectively to control reordering and instead wouldrely on the partial LM score.
This issue is exac-erbated for glue rules, where the decoder has tochoose from different possibilities without any wayto favour one over the others.
Instead of the rule#2, the decoder could use its reordered version?X1 haben X2, have X2 X1?
leading to a poortranslation.1094The features we introduce can be used to learnif the model should favour monotone translations atthe cost of re-orderings or vice versa and hence caneasily adapt to different language pairs.
Further, ourexperiments (see Section 4) suggest that the featuresh andw are not sufficient by themselves to model re-ordering for language pairs exhibiting very differentsyntactic structure.3.1 Distortion FeaturesOur distortion features are inspired by their name-sake in phrase-based system, with some modifica-tions to adapt the idea for the discontiguous phrasesin LR-Hiero grammar.r : hf1X1f2X2f3, tX2X1i I = [`, f1, f2, f3, X2, X1,a]f2 f3 X1 f1 X2 (a)r : ?
X1noch nicht X 2/not yet X2 X1?I=[(1,1) ,(3,5) ,(5,6) ,(1,3) ,(6,6)].1ihre2arbeit3noch4nicht5gemacht 6 (b)Figure 4: (a) Distortion feature computation using a ruler.
(b) Example of distortion computation for applying r3on phrase ?ihre arbeit noch nicht gemacht haben?.
sub-scripts between words show the indices which are used tobuild I .
Distortion would be: d = 2 + 0 + 5 + 3.Consider a rule r = ?
?,?b ?
?, with the sourceterm ?
being a mixed string of terminals and non-terminals.
Representing the non-terminal spans andeach sequence of terminals in ?
as distinct items, ourdistortion feature counts the total length of jumps be-tween the items during Earley parsing.Figure 4 (a) explains the computation of our dis-tortion feature for an example rule r. Let I =[I0, .
.
.
, Ik] be the items denoting the terminal se-quences and non-terminal spans with I0 and Ik be-ing dummy items (` and a in Fig) marking the leftand right indices of the rule r in input sentence f .Other items are arranged by their realization orderon the target-side with the terminal sequences pre-ceding non-terminal spans.
The items for the exam-ple rule are shown in Figure 4 (a).
The distortionfeature is computed as follows:d(r) =k?j=1|ILj ?
IRj?1| (4)where superscripts refer to position of left (L) andright (R) edge of each item in the source sentencef .
These are then aggregated across the rules of aderivation D as: d =?r?D d(r).
For each itemIj , we count the jump from the end of previous itemto the beginning of the current.
In Figure 4 (a) thejumps are indicated by the arrows above the rule.Figure 4 (b) shows an example of distortion com-putation for r3 and phrase ?ihre arbeit noch nichtgemacht haben?
from Figure 2.Since the glue rules are likely to be used in the toplevels (possibly with large distortion) of the deriva-tion, we would want the decoder to learn the distor-tion for regular and glue rules separately.
We thususe two distortion features for the two rule types andwe call them dp and dg.These features do not directly model the source-target reordering, but only capture the source-sidejumps.
Furthermore they apply for both monotoneand reordering rules.
We now introduce a new fea-ture for exclusively modelling the reordering.3.2 Reordering FeatureThis feature simply counts the number of reorderingrules, where the non-terminals in source and targetsides are reordered.
Thus r??
= rule(D, ??
), whererule(D, ??)
is the number of reordering rules in D.Similar to width and height, this feature is appliedfor rule having at least two non-terminals.
This fea-ture is applied to regular and glue rules.4 ExperimentsWe conduct different types of experiments to evalu-ate LR-Hiero decoding developed by cube pruningand integrating new features into LR-Hiero systemfor two language pairs: German-English (de-en) andCzech-English (cs-en).Table 1 shows the dataset de-tails.4.1 System SetupIn our experiments we use four baselines as wellas our implementation of LR-Hiero (written inPython):1095Corpus Train/Dev/Testcs-en Europarl(v7), CzEng(v0.9);News commentary7.95M/3000/3003de-en Europarl(v7); Newscommentary1.5M/2000/2000Table 1: Corpus statistics in number of sentencesModel cs-en de-enPhrase-based 233.0 77.2Hiero 1,961.6 858.5LR-Hiero 230.5 101.3Table 2: Model sizes (millions of rules).
We do not countglue rules for LR-Hiero which are created at runtime asneeded.?
Hiero: we used Kriya, our open-source im-plementation of Hiero in Python, which per-forms comparably to other open-source Hierosystems (Sankaran et al 2012).
Kriya canobtain statistically significantly equal BLEUscores when compared with Moses (Koehn etal., 2007) for several language pairs (Razmaraet al 2012; Callison-Burch et al 2012).?
Hiero-GNF: where we use Hiero decoder withthe restricted LR-Hiero grammar (GNF rules).?
LR-Hiero: our implementation of LR-Hiero(Watanabe et al 2006b) in Python.?
phrase-based: Moses (Koehn et al 2007)?
LR-Hiero+CP: LR-Hiero decoding with cubepruning.We use a 5-gram LM trained on the Gigaword cor-pus and use KenLM (Heafield, 2011) for LM scor-ing during decoding.
We tune weights by minimiz-ing BLEU loss on the dev set through MERT (Och,2003) and report BLEU scores on the test set.
Weuse comparable pop limits in each of the decoders:1000 for Moses and LR-Hiero and 500 with cubepruning for CKY Hiero and LR-Hiero+CP.
Otherextraction and decoder settings such as maximumphrase length, etc.
were identical across settings sothat the results are comparable.Table 2 shows how the LR-Hiero grammar ismuch smaller than CKY-based Hiero.Model cs-en de-en#queries / time(ms) #queries / time(ms)Hiero 5,679.7 / 16.12 7,231.62 / 20.33Hiero-GNF 4,952.5 / 14.71 5,858.74 / 18.23LR-Hiero (1000) 46,333.21 / 163.6 83,518.63 / 328.11LR-Hiero (500) 24,141.03 / 97.61 42,783.12 / 192.23LR-Hiero+CP 1,303.2 / 4.2 1,697.7 / 5.67Table 3: Comparing average number and time of lan-guage model queries.4.2 Time Efficiency ComparisonTo evaluate the performance of LR-Hiero decod-ing with cube pruning (LR-Hiero+CP), we compareit with three baselines: (i) CKY Hiero, (ii) CKYHiero-GNF, and (iii) LR-Hiero (without cube prun-ing) with two different beam size 500 and 1000.When it comes to instrument timing results, there arelots of system level details that we wish to abstractaway from, and focus only on the number of ?edges?processed by the decoder.
In comparison of parsingalgorithms, the common practice is to measure thenumber of edges processed by different algorithmsfor the same reason (Moore and Dowding, 1991).By analogy to parsing algorithm comparisons, wecompare the different decoding algorithms with re-spect to the number of calls made to the languagemodel (LM) since that directly corresponds to thenumber of hypotheses considered by the decoder.A decoder is more time efficient if it can considerfewer translation hypotheses while maintaining thesame BLEU score.
All of the baselines use the samewrapper to query the language model, and we haveinstrumented the wrapper to count the statistics weneed and thus we can say this is a fair comparison.For this experiment we use a sample set of 50 sen-tences taken from the test sets.Table 3 shows the results in terms of average num-ber of language model queries and times in millisec-onds.4.3 Reordering FeaturesTo evaluate the new reordering features proposedto LR-Hiero (Section 3.2), LR-Hiero+CP with newfeatures is compared to all baselines.
Table 4 showsthe BLEU scores of different models in two lan-guage pairs.
The baseline (Watanabe et al 2006b)model uses all the features mentioned therein but is1096Model cs-en de-enPhrase-based 20.32 24.71CKY Hiero 20.64 25.52CKY Hiero-GNF 20.04 24.84LR-Hiero 18.30 23.47LR-Hiero + reordering feats 20.20 24.90LR-Hiero + CP + reordering feats 20.15 24.83CKY Hiero-GNF + reordering feats 20.52 25.09CKY Hiero + reordering feats 20.77 25.72Table 4: BLEU scores.
The rows are grouped such thateach group use the same model.
The last row in part 2 oftable shows LR-Hiero+CP using our new features in ad-dition to the baseline Watanabe features (line LR-Hierobaseline).
The last part shows CKY Hiero using new re-ordering features.
The reordering features used are dp, dgand r??.
LR-Hiero+CP has a beam size of 500 while LR-Hiero has a beam size of 1000, c.f.
with the LM callsshown in Table 3.worse than both phrase-based and CKY-Hiero base-lines by up to 2.3 BLEU points.All the reported results are obtained from a singleoptimizer run.
However we observed insignificantchanges in different tuning runs in our experiments.We find a gain of about 1 BLEU point when we adda single distortion feature d and a further gain of0.3 BLEU (not shown due to lack of space) whenwe split the distortion feature for the two rule types(dp and dg).
The last line in part two of Table 4shows a consistent gain of 1.6 BLEU over the LR-Hiero baseline for both language pairs.
It shows thatLR-Hiero maintains the BLEU scores obtained by?phrase-based?
and ?CKY Hiero-GNF?.We performed statistical significance tests us-ing two different tools: Moses bootstrap resam-pling and MultEval (Clark et al 2011).
The dif-ference between ?LR-Hiero+CP+reordering feat?and three baselines: ?phrase-based?, ?CKY Hiero-GNF?, ?LR-Hiero+reordering feat?
are not statis-tically significant even for p-value of 0.1 for bothtools.To investigate the impact of proposed reorderingfeatures with other decoder or models.
We add thesefeatures to both Hiero and Hiero-GNF7.
The lastpart of Table 4 shows the performance CKY decoder7Feature r??
is defined for SCFG rules and cannot beadopted to phrase-based translation systems; and Moses usesdistortion feature therefore we omit Moses from this experi-ment.with different models (full Hiero and GNF) with thenew reordering features in terms of BLEU score.The results show that these features are helpful inboth models.
Although, they do not make a big dif-ference in Hiero with full model, they can alleviatethe lack of non-GNF rules in Hiero-GNF.Nguyen and Vogel (2013) integrate traditionalphrase-based features: distortion and lexicalized re-ordering into Hiero as well.
They show that suchfeatures can be useful to boost the translation qualityof CKY Hiero with the full rule set.
Nguyen and Vo-gel (2013) compute the distortion feature in a differ-ent way, only applicable to CKY.
The distortion foreach cell is computed after the translation for non-terminal sub-spans is complete.
In LR-decoding,we compute distortion for rules even though we areyet to translate some of the sub-spans.
Thus our ap-proach computes the distortion incrementally for theuntranslated sub-spans which are later added.
Un-like (Nguyen and Vogel, 2013), our distortion fea-ture can be applied to both LR and CKY-decoding(Table 4).
We have also introduced another reorder-ing feature (Section 3.2) not proposed previously.5 Conclusion and Future WorkWe provided a detailed description of left-to-rightHiero decoding, many details of which were onlyimplicit in (Watanabe et al 2006b).
We presentedan augmented LR decoding algorithm that builds onthe original algorithm in (Watanabe et al 2006b)but unlike that algorithm, using experiments overmultiple language pairs we showed two new results:(i) Our LR decoding algorithm provides demonstra-bly more efficient decoding than CKY Hiero and theoriginal LR decoding algorithm in (Watanabe et al2006b).
And, (ii) by introducing new distortion andreordering features for LR decoding we show thatit maintains the BLEU scores obtained by phrase-based and CKY Hiero-GNF.CKY Hiero uses standard Hiero-style translationrules capturing better reordering model than prefixlexicalized target-side translation rules used in LR-Hiero.
Our LR-decoding algorithm is 4 times fasterin terms of LM calls while translation quality suffersby about 0.67 in BLEU score on average.Unlike Watanabe et al(2006b), our new featurescan easily adapt to the reordering requirements ofdifferent language pairs.
We also introduce the use1097of future cost in decoding algorithm which is an es-sential part in decoding.
We have shown in this pa-per that left-to-right (LR) decoding can be consid-ered as a potential faster alternative to CKY decod-ing for Hiero-style machine translation systems.In future work, we plan to apply lexicalized re-ordering models to LR-Hiero.
It has been shown tobe useful for Hiero in some languages therefore itis promising to improve translation quality in LR-Hiero which suffers from lack of modeling powerof non-GNF target side rules.
We also plan to ex-tend the glue rules in LR-Hiero to provide a bet-ter reordering model.
We believe such an exten-sion would be very effective in reducing search er-rors and capturing better reordering models in lan-guage pairs involving complex reordering require-ments like Chinese-English.AcknowledgmentsThis research was partially supported by an NSERC,Canada (RGPIN: 264905) grant and a Google Fac-ulty Award to the third author.
The authors wishto thank Taro Watanabe and Marzieh Razavi fortheir valuable discussions and suggestions, and theanonymous reviewers for their helpful comments.ReferencesChris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical machinetranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 10?51, Montre?al, Canada, June.
Association for Compu-tational Linguistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In In ACL, pages263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisti-cal machine translation: controlling for optimizer in-stability.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies: short papers - Volume2, HLT ?11, pages 176?181, Stroudsburg, PA, USA.Association for Computational Linguistics.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Commun.
ACM, 13(2):94?102, February.Michel Galley and Christopher D. Manning.
2010.
Ac-curate non-hierarchical phrase-based translation.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 966?974, Los Angeles, California, June.
Association forComputational Linguistics.Kenneth Heafield, Hieu Hoang, Philipp Koehn, TetsuoKiso, and Marcello Federico.
2011.
Left languagemodel state for syntactic machine translation.
In Pro-ceedings of the International Workshop on SpokenLanguage Translation, pages 183?190, San Francisco,California, USA, 12.Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013.Grouping language model boundary words to speed K-Best extraction from hypergraphs.
In Proceedings ofthe 2013 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, Atlanta, Georgia, USA,6.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In In Proc.
of the Sixth Work-shop on Statistical Machine Translation.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InIn ACL 07.Liang Huang and Haitao Mi.
2010.
Efficient incrementaldecoding for tree-to-string translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 273?283, Cambridge,MA, October.
Association for Computational Linguis-tics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07,pages 177?180, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In EMNLP-CoNLL, pages976?985.Robert C. Moore and John Dowding.
1991.
Efficientbottom-up parsing.
In HLT.
Morgan Kaufmann.Thuylinh Nguyen and Stephan Vogel.
2013.
Integrat-ing phrase-based reordering features into chart-baseddecoder for machine translation.
In Proc.
of ACL.1098Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics - Volume 1, ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Majid Razmara, Baskaran Sankaran, Ann Clifton, andAnoop Sarkar.
2012.
Kriya - the sfu system for trans-lation task at wmt-12.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, WMT?12, pages 356?361, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.2010.
Incremental decoding for phrase-based statis-tical machine translation.
In Proceedings of the JointFifth Workshop on Statistical Machine Translation andMetricsMATR, WMT ?10, pages 216?223, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.2012.
Kriya - an end-to-end hierarchical phrase-basedmt system.
The Prague Bulletin of Mathematical Lin-guistics (PBML), (97):83?98, apr.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2006a.
NTT statistical machine translationfor iwslt 2006.
In Proceedings of IWSLT 2006, pages95?102.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006b.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proc.
of ACL.Jiajun Zhang and Chenqqing Zong.
2012.
A Compar-ative Study on Discontinuous Phrase Translation.
InNLPCC 2012, pages 164?175.1099
