Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 435?439,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsDCU Participation in WMT2013 Metrics TaskXiaofeng Wu?, Hui Yu?,Qun Liu?
?CNGL, Dublin City University, Ireland?ICT, Chinese Academy of Sciences, China?
{xfwu, qliu}@computing.dcu.ie?yuhui@ict.ac.cnAbstractIn this paper, we propose a novel syntac-tic based MT evaluation metric which onlyemploys the dependency information inthe source side.
Experimental results showthat our method achieves higher correla-tion with human judgments than BLEU,TER, HWCM and METEOR at both sen-tence and system level for all of the fourlanguage pairs in WMT 2010.1 IntroductionAutomatic evaluation plays a more important rolein the evolution of machine translation.
At the ear-liest stage, the automatic evaluation metrics onlyuse the lexical information, in which, BLEU (Pap-ineni et al 2002) is the most popular one.
BLEUis simple and effective.
Most of the researchersregard BLEU as their primary evaluation metricto develop and compare MT systems.
However,BLEU only employs the lexical information andcannot adequately reflect the structural level sim-ilarity.
Translation Error Rate (TER) (Snover etal., 2006) measures the number of edits required tochange the hypothesis into one of the references.METEOR (Lavie and Agarwal, 2007), which de-fines loose unigram matching between the hypoth-esis and the references with the help of stem-ming and Wordnet-looking-up, is also a lexicalbased method and achieves the first-class human-evaluation-correlation score.
AMBER (Chen andKuhn, 2011; Chen et al 2012) incorporates recall,extra penalties and some text processing variantson the basis of BLEU.
The main weakness of allthe above lexical based methods is that they cannotadequately reflect the structural level similarity.To overcome the weakness of the lexical basedmethods, many syntactic based metrics were pro-posed.
Liu and Gildea (2005) proposed STM, aconstituent tree based approach, and HWCM, adependency tree based approach.Both of the two methods compute the similar-ity between the sub-trees of the hypothesis and thereference.
Owczarzak et al2007a; 2007b; 2007c)presented a method using the Lexical-FunctionalGrammar (LFG) dependency tree.
MAXSIM(Chan and Ng, 2008) and the method proposedby Zhu et al2010) also employed the syntac-tic information in association with lexical infor-mation.With the syntactic information which canreflect structural information, the correlation withthe human judgments can be improved to a certainextent.As we know that the hypothesis is potentiallynoisy, and these errors expand through the parsingprocess.
Thus the power of syntactic informationcould be considerably weakened.In this paper, we attempt to overcome the short-coming of the syntactic based methods and pro-pose a novel dependency based MT evaluationmetric.
The proposed metric only employs the ref-erence dependency tree which contains both thelexical and syntactic information, leaving the hy-pothesis side unparsed to avoid the error propaga-tion.
In our metric, F-score is calculated using thestring of hypothesis and the dependency based n-grams which are extracted from the reference de-pendency tree.Experimental results show that our methodachieves higher correlation with human judgmentsthan BLEU, HWCM, TER and METEOR at bothsentence level and system level for all of the fourlanguage pairs in WMT 2010.2 Background: HWCMHWCM is a dependency based metric which ex-tracts the headword chains, a sequence of wordswhich corresponds to a path in the dependencytree, from both the hypothesis and the referencedependency tree.
The score of HWCM is obtained435Figure 1: The dependency tree of the referenceFigure 2: The dependency tree of the hypothesisby formula (1).HWCM = 1DD?n=1?g?chainn(hyp) countclip(g)?g?chainn(hyp) count(g)(1)In formula (1), D is the maximum length of theheadword chain.
chainn(hyp) denotes the set ofthe headword chains with length of n in the tree ofhypothesis.
count(g) denotes the number of timesg appears in the headword chain of the hypothe-sis dependency tree and countclip(g) denotes theclipped number of times when g appears in the theheadword chain of the reference dependency trees.Clipped means that the count computed from theheadword chain of the hypothesis tree should notexceed the maximum number of times when g oc-curs in headword chain of any single referencetree.
The following are two sentences represent-ing as reference and hypothesis, and Figure 1 andFigure 2 are the dependency trees respectively.reference: It is not for want of trying .hypothesis: This is not for lack of trying .In the example above, there are 8 1-word, 7 2-word and 3 3-word headword chains in the hy-pothesis dependency tree.
The number of 1-wordand 2-word headword chains in the hypothesis treewhich can match their counterparts in the refer-ence tree is 5 and 4 respectively.
The 3-word head-word chains in the hypothesis dependency tree areis for lack, for lack of and lack of trying.
Due tothe difference in the dependency structures, theyhave no matches in the reference side.3 A Novel Dependency Based MTEvaluation MethodIn this new method, we calculate F-score using thestring of hypothesis and the dep-n-grams whichare extracted from the reference dependency tree.The new method is named DEPREF since it isa DEPendency based method only using depen-dency tree of REference to calculate the F-score.In DEPREF, after the parsing of the reference sen-tences, there are three steps below being carriedout.
1) Extracting the dependency based n-gram(dep-n-gram) in the dependency tree of the refer-ence.
2) Matching the dep-n-gram with the stringof hypothesis.
3) Obtaining the final score of a hy-pothesis.
The detail description of our method willbe found in paper (Liu et al 2013) .
We only givethe experiment results in this paper.4 ExperimentsBoth the sentence level evaluation and the systemlevel evaluation are conducted to assess the per-formance of our automatic metric.
At the sentencelevel evaluation, Kendall?s rank correlation coeffi-cient ?
is used.
At the system level evaluation, theSpearman?s rank correlation coefficient ?
is used.4.1 DataThere are four language pairs in our experimentsincluding German-to-English, Czech-to-English,French-to-English and Spanish-to-English, whichare all derived from WMT2010.
Each of thefour language pairs consists of 2034 sentences andthe references of the four language pairs are thesame.
There are 24 translation systems for French-to-English, 25 for German-to-English, 12 forCzech-to-English and 15 for Spanish-to-English.We parsed the reference into constituent tree byBerkeley parser and then converted the constituenttree into dependency tree by Penn2Malt 1.
Pre-sumably, we believe that the performance will beeven better if the dependency trees are manuallyrevised.In the experiments, we compare the perfor-mance of our metric with the widely used lexicalbased metrics BLEU, TER, METEOR and a de-pendency based metric HWCM.
In order to makea fair comparison with METEOR which is knownto perform best when external resources like stemand synonym are provided, we also provide resultsof DEPREF with external resources.1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html436Metrics Czech-English German-English Spanish-English French-EnglishBLEU 0.2554 0.2748 0.2805 0.2197TER 0.2526 0.2907 0.2638 0.2105HWCMN=1 0.2067 0.2227 0.2188 0.2022N=2 0.2587 0.2601 0.2408 0.2399N=3 0.2526 0.2638 0.2570 0.2498N=4 0.2453 0.2672 0.2590 0.2436DEPREF 0.3337 0.3498 0.3190 0.2656Table 1.A Sentence level correlations of the metrics without external resources.Metrics Czech-English German-English Spanish-English French-EnglishMETEOR 0.3186 0.3482 0.3258 0.2745DEPREF 0.3281 0.3606 0.3326 0.2834Table 1.B Sentence level correlations of the metrics with stemming and synonym.Table 1: The sentence level correlations with the human judgments for Czech-to-English, German-to-English, Spanish-to-English and French-to-English.
The number in bold is the maximum value in eachcolumn.
N stands for the max length of the headword chains in HWCM in Table 1.A.4.2 Sentence-level EvaluationKendall?s rank correlation coefficient ?
is em-ployed to evaluate the correlation of all the MTevaluation metrics and human judgements at thesentence level.
A higher value of ?
means a bet-ter ranking similarity with the human judges.
Thecorrelation scores of the four language pairs andthe average scores are shown in Table 1.A (withoutexternal resources) and Table 1.B (with stemmingand synonym), Our method performs best whenmaximum length of dep-n-gram is set to 3, so wepresent only the results when the maximum lengthof dep-n-gram equals 3.From Table 1.A, we can see that all our methodsare far more better than BLEU, TER and HWCMwhen there is no external resources applied on allof the four language pairs.
In Table 1.B, externalresources is considered.
DEPREF is also betterthan METEOR on the four language pairs.
Fromthe comparison between Table 1.A and Table 1.B,we can conclude that external resources is help-ful for DEPREF on most of the language pairs.When comparing DEPREF without external re-sources with METEOR, we find that DEPREF ob-tains better results on Czech-English and German-English.4.3 System-level EvaluationWe also evaluated the metrics with the humanrankings at the system level to further investigatethe effectiveness of our metrics.
The matching ofthe words in DEPREF is correlated with the posi-tion of the words, so the traditional way of com-puting system level score, like what BLEU does,is not feasible for DEPREF.
Therefore, we resortto the way of adding the sentence level scores to-gether to obtain the system level score.
At systemlevel evaluation, we employ Spearman?s rank cor-relation coefficient ?.
The correlations of the fourlanguage pairs and the average scores are shownin Table 2.A (without external resources) and Ta-ble 2.B (with stem and synonym).From Table 2.A, we can see that the correla-tion of DEPREF is better than BLEU, TER andHWCM on German-English, Spanish-English andFrench-English.
On Czech-English, our metricDEPREF is better than BLEU and TER.
In Table2.B (with stem and synonym), DEPREF obtainsbetter results than METEOR on all of the languagepairs except one case that DEPREF gets the sameresult as METEOR on Czech-English.
When com-paring DEPREF without external resources withMETEOR, we can find that DEPREF gets bet-ter result than METEOR on Spanish-English andFrench-English.From Table 1 and Table 2, we can concludethat, DEPREF without external resources can ob-tain comparable result with METEOR, and DE-PREF with external resources can obtain better re-sults than METEOR.
The only exception is that atthe system level evaluation, Czech-English?s bestscore is abtained by HWCM.
Notice that there areonly 12 systems in Czech-English, which meansthere are only 12 numbers to be sorted, we believe437Metrics Czech-English German-English Spanish-English French-EnglishBLEU 0.8400 0.8808 0.8681 0.8391TER 0.7832 0.8923 0.9033 0.8330HWCMN=1 0.8392 0.7715 0.7231 0.6730N=2 0.8671 0.8600 0.7670 0.8026N=3 0.8811 0.8831 0.8286 0.8209N=4 0.8811 0.9046 0.8242 0.8148DEPREF 0.8392 0.9238 0.9604 0.8687Table 2.A System level correlations of the metrics without external resources.Metrics Czech-English German-English Spanish-English French-EnglishMETEOR 0.8392 0.9269 0.9516 0.8652DEPREF 0.8392 0.9331 0.9692 0.8730Table 2.B System level correlations of the metrics with stemming and synonym.Table 2: The system level correlations with the human judgments for Czech-to-English, German-to-English, Spanish-to-English and French-to-English.
The number in bold is the maximum value in eachcolumn.
N stands for the max length of the headword chains in HWCM in Table 2.A.the spareness issure is more serious in this case.5 ConclusionIn this paper, we propose a new automatic MTevaluation method DEPREF.
The experiments arecarried out at both sentence-level and system-levelusing four language pairs from WMT 2010.
Theexperiment results indicate that DEPREF achievesbetter correlation than BLEU, HWCM, TER andMETEOR at both sentence level and system level.ReferencesYee Seng Chan and Hwee Tou Ng.
2008.
Maxsim: Amaximum similarity metric for machine translationevaluation.
In Proceedings of ACL-08: HLT, pages55?62.Boxing Chen and Roland Kuhn.
2011.
Amber: Amodified bleu, enhanced ranking metric.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 71?77, Edinburgh, Scot-land, July.
Association for Computational Linguis-tics.Boxing Chen, Roland Kuhn, and George Foster.
2012.Improving amber, an mt evaluation metric.
In Pro-ceedings of the Seventh Workshop on Statistical Ma-chine Translation, WMT ?12, pages 59?63, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Alon Lavie and Abhaya Agarwal.
2007.
Meteor: anautomatic metric for mt evaluation with high levelsof correlation with human judgments.
In Proceed-ings of the Second Workshop on Statistical MachineTranslation, StatMT ?07, pages 228?231, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Ding Liu and Daniel Gildea.
2005.
Syntactic featuresfor evaluation of machine translation.
In Proceed-ings of the ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for Machine Translationand/or Summarization, pages 25?32.Q.
Liu, H. Yu, X. Wu, J. Xie, Y. Lu, and S. Lin.2013.
A Novel Dependency Based MT EvaluationMethod.
Under Review.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007a.
Dependency-based automatic eval-uation for machine translation.
In Proceedings ofthe NAACL-HLT 2007/AMTA Workshop on Syntaxand Structure in Statistical Translation, SSST ?07,pages 80?87, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007b.
Evaluating machine translation withlfg dependencies.
Machine Translation, 21(2):95?119, June.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007c.
Labelled dependencies in machinetranslation evaluation.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation,StatMT ?07, pages 104?111, Stroudsburg, PA, USA.Association for Computational Linguistics.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th annualmeeting on association for computational linguis-tics, pages 311?318.
Association for ComputationalLinguistics.438Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas, pages 223?231.Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, andTiejun Zhao.
2010.
All in strings: a powerful string-based automatic mt evaluation metric with multi-ple granularities.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics:Posters, COLING ?10, pages 1533?1540, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.439
