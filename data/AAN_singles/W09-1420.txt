Proceedings of the Workshop on BioNLP: Shared Task, pages 137?140,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExploring ways beyond the simple supervised learning approach forbiological event extractionGyo?rgy Mo?ra1, Richa?rd Farkas1, Gyo?rgy Szarvas2?, Zsolt Molna?r3gymora@gmail.com, rfarkas@inf.u-szeged.hu,szarvas@tk.informatik.tu-darmstadt.de, zsolt@acheuron.hu1 Hungarian Academy of Sciences, Research Group on Artificial IntelligenceAradi ve?rtanuk tere 1., H-6720 Szeged, Hungary2 Ubiquitous Knowledge Processing Lab, Technische Universita?t DarmstadtHochschulstra?e 10., D-64289 Darmstadt, Germany3 Acheuron Hungary Ltd., Chemo-, and Bioinformatics group,Tiszavira?g u.
11., H-6726 Szeged, HungaryAbstractOur paper presents the comparison of amachine-learnt and a manually constructedexpert-rule-based biological event extractionsystem and some preliminary experiments toapply a negation and speculation detectionsystem to further classify the extracted events.We report results on the BioNLP?09 SharedTask on Event Extraction evaluation datasets,and also on an external dataset for negationand speculation detection.1 IntroductionWhen we consider the sizes of publicly availablebiomedical scientific literature databases for re-searchers, valuable biological knowledge is acces-sible today in enormous amounts.
The efficient pro-cessing of these large text collections is becomingan increasingly important issue in Natural LanguageProcessing.
For a survey on techniques used in bio-logical Information Extraction, see (Tuncbag et al,2009).The BioNLP?09 Shared Task (Kim et al, 2009)involved the recognition of bio-molecular events inscientific abstracts.
In this paper we describe oursystems submitted to the event detection and charac-terization (Task1) and the recognition of negationsand speculations (Task3) subtasks.
Our experimentscan be regarded as case studies on i) how to definea framework for a hybrid human-machine biologicalinformation extraction system, ii) how the linguis-tic scopes of negation/speculation keywords relateto biological event annotations.
?On leave from RGAI of Hungarian Acad.
Sci.2 Event detectionWe formulated the event extraction task as a classifi-cation problem for each event-trigger-word/proteinpair.
A domain expert collected 140 keywordswhich he found meaningful and reliable by manualinspection of the corpus.
This set of high-precisionkeywords covered 69.8% of the event annotations inthe training data.We analysed each occurrence of these keywordsin two different approaches.
We used C4.5 deci-sion tree classifier to predict one of the event typesconsidered in the shared task or the keyword/proteinpair being unrelated; and we also developed a hand-crafted expert system with a biological expert.
Weobserved that the two systems extract markedly dif-ferent sets of true positive events.
Our final submis-sion was thus the union of the events extracted bythe expert-rule-based and the statistical systems (wecall this hybrid system later on).2.1 The statistical event classifierThe preprocessing of the data was performed us-ing the UltraCompare (Kano et al, 2008) repositoryprovided by the organizers of the challenge: Geniasentence splitter, Genia tagger for POS coding andNER.The statistical system classified each key-word/protein pair into 9 event and 2 non-eventclasses.
A pair was either labeled according tothe predicted event type (the keyword as an eventtrigger and the protein name as the theme of theevent), non-event (keyword not an event trigger)or wrong-protein (the theme of the event is adifferent protein).
We chose to use two non-event137classes to make the decision tree more human read-able (the negative cases being separated).
This madethe comparison of the statistical model and the rule-based system easier.The features we used were the following: 1) thewords and POS codes in a window (?
3 tokens)around the keyword, preserving position informa-tion relative to the keyword; 2) the distances be-tween the keyword and the two nearest annotatedproteins (left and right) and the theme candidate asnumeric features1.
The protein annotations were re-placed by the term $protein, Genia tagger anno-tations by $genia-protein (mainly complexes),to enable the classifier to learn the difference be-tween events involved in the shared task, and eventsout of the scope of the task.
Events with proteincomplexes and families often had the same linguisticstructure as events with annotated proteins.
As com-plexes did not form events in the shared task, theysometimes misled our local-context-based classifier.For example ?the binding of ISGF3?
was not anno-tated as an event because the theme is not a ?protein?
(as defined by the shared task guidelines), while ?thebinding of TRAF2?
was (TRAF2 being a protein,and not a complex as in the former example).We trained a C4.5 decision tree classifier usingWeka (Witten and Frank, 2005).
The human read-able models and fast training time motivated ourselection of a learning algorithm which allowed astraightforward comparison with the expert system.2.2 Expert-rule-based systemThe expert system was constructed by a biologistwho had over 4 years of experience in similar tasks.The main idea was to define rules ?
which have avery high precision ?
in order to compare them withthe learnt decision trees and to increase the cover-age of the final system by adding these annotationsto the output of the statistical system.
We only man-aged to prepare expert rules for the Phosphorylationand Gene expression classes due to time constraints(a total of 46 patterns).
The expert was asked toconstruct high-precision rules (they were tested onthe train set to keep the false positive rate near zero)in order to gain insight into the structure of reliable1More information on the featuresand parameters used can be found atwww.inf.u-szeged.hu/rgai/BioEventExtractionrules.Here each rule is bound to a specific keyword.
Ev-ery rule is a sequence of ?word patterns?
(with orwithout a suffix).
A word pattern can match a pro-tein, an arbitrary word, an exact word or the key-word.
Every pattern can have a Regular Expressionstyle suffix:Table 1: Word pattern types and suffixes<keyword> matching the keyword of the event"word" matching regular wordsmatching any token$protein matching any annotated protein?
zero or one of the word pattern* zero or more of the word pattern+ one or more of the word pattern{a,b} definite number of word patternsFor example the ?<expression> ?
"of"?
$protein?
pattern recognizes an event withthe keyword expression, followed by an arbitraryword and then the word of, or immediately by of andthen a protein (or immediately by the protein name).An obvious drawback of this system is that nega-tion is not allowed, so the expert was unable to de-fine a word pattern like !
"of" to match any to-ken besides of.
This extension would have been astraightforward way of improving the system.2.3 Experimental resultsWe expected the recall of the hybrid system to benear the sum of the recalls of the individual systems,meaning that they had recognized different events,as the pattern matching was mainly based on theorder of the tokens, while the statistical classifierlearned position-oriented contextual clues.
Thanksto the high precision of the rule-based system, theoverall precision also increased.
The two eventclasses which were included in the expert systemhad a significantly better precision score.
The cov-erage of the Phosphorylation class was lower thanthat for the Gene expression class because its pat-terns were still incomplete2.2A discussion on comparing the contribution of thetwo approaches and individual rules can be found atwww.inf.u-szeged.hu/rgai/BioEventExtraction138Table 2: Results of rule based-system compared to thestatistical and combined systems (R/P/fscore)All Event Gene exp.
Phosph.stat.
16 / 31 / 21 36 / 41 / 38 73 / 37 / 49rule 5 / 80 / 10 20 / 85 / 33 17 / 58 / 26hybrid 22 / 37 / 27 56 / 51 / 54 81 / 40 / 533 Recognition of negations andspeculationsFor negation and speculation detection, we applieda model trained on a different dataset (Vincze et al,2008) of scientific abstracts, which had been spe-cially annotated for negative and uncertain keywordsand their linguistic scope.
Due to time constraintswe used our model to produce annotations for Task3without any sort of fine tuning to the shared task goldstandard annotations.The only exception here was a subclass of specu-lative annotations that were not triggered by a wordused to express uncertainty, but were judged to bespeculative because the sentence itself reported onsome experiments performed, the focus of the in-vestigations described in the article, etc.
That is,it was not the meaning of the text that was uncer-tain, but ?
as saying that something has been exam-ined does not mean it actually exists ?
the sentenceimplicitly contained uncertain information.
Sincesuch sentences were not covered by our corpus, forthese cases we collected the most reliable text cuesfrom the shared task training data and applied adictionary-lookup-based approach.
We did this soas to get a comprehensive model for the Genia nega-tion and speculation task.As for the explicit uncertain and negative state-ments, we applied a more sophisticated approachthat exploited the annotations of the BioScope cor-pus (Vincze et al, 2008).
For each frequent and am-biguous keyword found in the approximately 1200abstracts annotated in BioScope, we trained a sepa-rate classifier to discriminate keyword/non-keyworduses of each term, using local contextual patterns(neighbouring lemmas, their POS codes, etc.)
asfeatures.
In others words, for the most commonuncertain and negative keywords, we attempted acontext-based disambiguation, instead of a simplekeyword lookup.
Having the keywords, we pre-dicted their scope using simple heuristics (?to theend of the sentence?, ?to the next punctation markin both directions?, etc.).
In the shared task we ex-amined each extracted event and they were said tobe negated or hedged when some of their arguments(trigger word, theme or clause) were within a lin-guistic scope.3.1 Experimental resultsFirst we evaluated our negation and speculationkeyword/non-keyword classification models on theBioScope corpus by 5-fold cross-validation.
Wetrained models for 15 negation and 41 speculativekeywords.
We considered different word forms ofthe same lemma to be different keywords becausethey may be used in a different meaning/context.For instance, different keyword/non-keyword deci-sion rules must be used for appear, appears and ap-peared.
We trained a C4.5 decision tree using worduni- and bigram features and POS codes to discrim-inate keyword/non-keyword uses and compared theresults with the most frequent class (MFC) baseline.Overall, our context-based classification methodoutperformed the baseline algorithm by 3.7% (giv-ing an error reduction of 46%) and 3.1% (giving anerror reduction of 27%) on the negation and specula-tion keywords, respectively.
The learnt models weretypically very small decision trees i.e.
they repre-sented very simple rules indicating collocations (like?hypothesis is a keyword if and only if followed bythat, etc.).
More complex rules (e.g.
?clear is a key-word if and only if not is in ?3 environment?)
werelearnt just in a few cases.Our second set of experiments focused on Task3of the shared task (Kim et al, 2009).
As the offi-cial evaluation process of Task3 was built upon thedetected events of Task1, it did not provide any use-ful feedback about our negation and speculation de-tection approach.
Thus instead of our Task1 out-put, we evaluated our model on the gold standardTask1 annotation of the training and the develop-ment datasets.
The statistical parts of the systemwere learnt on the BioScope corpus, thus the trainset was kept blind as well.
Table 3 summarises theresults obtained by the explicit negation, speculationand by the full speculation (both explicit and implicitkeywords) detection methods.Analysing the errors of the system, we found that139Table 3: Negation and speculation detection resultsTrain (R/P/F) Dev.
(R/P/F)negation 46.9 / 61.3 / 52.8 42.8 / 57.9 / 49.2exp.
spec.
15.4 / 39.5 / 23.6 15.4 / 32.6 / 20.1full spec.
25.5 / 71.1 / 37.5 27.9 / 65.3 / 39.1most of the false positives came from the differentapproaches of the BioScope and the Genia annota-tions (see below for a detailed discussion).
Most ofthe false negative predictions were a consequence ofthe incompleteness of our keyword list.3.2 DiscussionWe applied this negation and speculation detectionmodel more as a case study to assess the usabilityof the BioScope corpus.
This means that we did notfine-tune the system to the Genia annotations.
Ourexperiments revealed some fundamental and inter-esting differences between the Genia-interpretationof negation and speculation, and the corpus used byus.
The chief difference is that the BioScope corpuswas constructed following more linguistic-orientedprinciples than the Genia negation and speculationannotation did, which sought to extract biologicalinformation.
These differences taken together ex-plain the relatively poor results we got for the sharedtask.There are significant differences in the interpreta-tion of both at the keyword level (i.e.
what triggersnegation/uncertainty and what does not) and in thedefinition of the scope of keywords.
For example,in a sentence like ?have NO effect on the inducibil-ity of the IL-2 promoter?, Genia annotation just con-siders the effect to be negated.
This means that theinducibility of IL-2 is regarded as an assertive eventhere.
In BioScope, the complements of effect arealso placed within the scope of no, thus it would alsobe annotated as a negative one.
We argue here thatthe above example is not a regular sentence to ex-press the fact: IL-2 is inducible.
We rather thinkthat if the paper has some result (evidence) regard-ing this event, it should be stated elsewhere in thetext, and we should not retrieve this information as afact just based on the above sentence.
Thus we arguethat more sophisticated guidelines are needed for theconsistent annotation and efficient handling of nega-tion and uncertainty in biomedical text mining.4 ConclusionsWe described preliminary experiments on two dif-ferent approaches which take us beyond the ?take-goldstandard-data, extract-some-features, train-a-classifier?
approach for biomedical event extractionfrom scientific texts (incorporating rule-based sys-tems and linguistic negation/uncertainty detection).The systems introduced here participated in the Ge-nia Event annotation shared task.
They achieved rel-atively poor results on this dataset, mainly due to1) the special annotation guidelines of the sharedtask (like disregarding events with protein complexor family arguments, and treating subevents as as-sertive information) and 2) the limited resources wehad to allocate for the task during the challengetimeline.
We consider that the lessons learnt hereare still useful and we also plan to improve our sys-tem in the near future.5 AcknowledgementsThe authors would like to thank the organizers ofthe shared task for their efforts.
This work was sup-ported in part by the NKTH grant of the Hungariangovernment (codename BELAMI).ReferencesY.
Kano, N. Nguyen, R. Saetre, K. Yoshida, Y. Miyao,Y.
Tsuruoka, Y. Matsubayashi, S. Ananiadou, andJ.
Tsujii.
2008.
Filling the gaps between tools andusers: a tool comparator, using protein-protein inter-action as an example.
Pac Symp Biocomput.J-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.2009.
Overview of bionlp?09 shared task on eventextraction.
In Proceedings of Natural Language Pro-cessing in Biomedicine (BioNLP) NAACL 2009 Work-shop.
To appear.N.
Tuncbag, G. Kar, O. Keskin, A. Gursoy, and R. Nussi-nov. 2009.
A survey of available tools and web serversfor analysis of protein-protein interactions and inter-faces.
Briefings in Bioinformatics.V.
Vincze, Gy.
Szarvas, R. Farkas, Gy.
Mo?ra, andJ.
Csirik.
2008.
The bioscope corpus: biomedi-cal texts annotated for uncertainty, negation and theirscopes.
BMC Bioinformatics, 9(Suppl 11):S9.I.
H. Witten and E. Frank.
2005.
Data Mining: Practi-cal Machine Learning Tools and Techniques, SecondEdition.
Morgan Kaufmann.140
