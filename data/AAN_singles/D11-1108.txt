Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168?1179,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLearning Sentential Paraphrases from Bilingual Parallel Corporafor Text-to-Text GenerationJuri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van DurmeCenter for Language and Speech Processing, and HLTCOEJohns Hopkins UniversityAbstractPrevious work has shown that high qualityphrasal paraphrases can be extracted frombilingual parallel corpora.
However, it is notclear whether bitexts are an appropriate re-source for extracting more sophisticated sen-tential paraphrases, which are more obviouslylearnable from monolingual parallel corpora.We extend bilingual paraphrase extraction tosyntactic paraphrases and demonstrate its abil-ity to learn a variety of general paraphrastictransformations, including passivization, da-tive shift, and topicalization.
We discuss howour model can be adapted to many text gener-ation tasks by augmenting its feature set, de-velopment data, and parameter estimation rou-tine.
We illustrate this adaptation by usingour paraphrase model for the task of sentencecompression and achieve results competitivewith state-of-the-art compression systems.1 IntroductionParaphrases are alternative ways of expressing thesame information (Culicover, 1968).
Automaticallygenerating and detecting paraphrases is a crucial as-pect of many NLP tasks.
In multi-document sum-marization, paraphrase detection is used to collapseredundancies (Barzilay et al, 1999; Barzilay, 2003).Paraphrase generation can be used for query expan-sion in information retrieval and question answer-ing systems (McKeown, 1979; Anick and Tipirneni,1999; Ravichandran and Hovy, 2002; Riezler et al,2007).
Paraphrases allow for more flexible matchingof system output against human references for taskslike machine translation and automatic summariza-tion (Zhou et al, 2006; Kauchak and Barzilay, 2006;Madnani et al, 2007; Snover et al, 2010).Broadly, we can distinguish two forms of para-phrases: phrasal paraphrases denote a set of surfacetext forms with the same meaning:the committee?s second proposalthe second proposal of the committeewhile syntactic paraphrases augment the surfaceforms by introducing nonterminals (or slots) that areannotated with syntactic constraints:the NP1?s NP2the NP2 of the NP1It is evident that the latter have a much higher poten-tial for generalization and for capturing interestingparaphrastic transformations.A variety of different types of corpora (and se-mantic equivalence cues) have been used to auto-matically induce paraphrase collections for English(Madnani and Dorr, 2010).
Perhaps the most nat-ural type of corpus for this task is a monolingualparallel text, which allows sentential paraphrases tobe extracted since the sentence pairs in such corporaare perfect paraphrases of each other (Barzilay andMcKeown, 2001; Pang et al, 2003).
While rich syn-tactic paraphrases have been learned from monolin-gual parallel corpora, they suffer from very limiteddata availability and thus have poor coverage.Other methods obtain paraphrases from rawmonolingual text by relying on distributional simi-larity (Lin and Pantel, 2001; Bhagat and Ravichan-dran, 2008).
While vast amounts of data arereadily available for these approaches, the distri-butional similarity signal they use is noisier thanthe sentence-level correspondency in parallel cor-pora and additionally suffers from problems such asmistaking cousin expressions or antonyms (such as{boy , girl} or {rise, fall}) for paraphrases.1168Abundantly available bilingual parallel corporahave been shown to address both these issues, ob-taining paraphrases via a pivoting step over foreignlanguage phrases (Bannard and Callison-Burch,2005).
The coverage of paraphrase lexica extractedfrom bitexts has been shown to outperform thatobtained from other sources (Zhao et al, 2008a).While there have been efforts pursuing the extrac-tion of more powerful paraphrases (Madnani etal., 2007; Callison-Burch, 2008; Cohn and Lapata,2008; Zhao et al, 2008b), it is not yet clear to whatextent sentential paraphrases can be induced frombitexts.
In this paper we:?
Extend the bilingual pivoting approach to para-phrase induction to produce rich syntactic para-phrases.?
Perform a thorough analysis of the types ofparaphrases we obtain and discuss the para-phrastic transformations we are capable of cap-turing.?
Describe how training paradigms for syntac-tic/sentential paraphrase models should be tai-lored to different text-to-text generation tasks.?
Demonstrate our framework?s suitability for avariety of text-to-text generation tasks by ob-taining state-of-the-art results on the exampletask of sentence compression.2 Related WorkMadnani and Dorr (2010) survey a variety of data-driven paraphrasing techniques, categorizing thembased on the type of data that they use.
Theseinclude large monolingual texts (Lin and Pantel,2001; Szpektor et al, 2004; Bhagat and Ravichan-dran, 2008), comparable corpora (Barzilay and Lee,2003; Dolan et al, 2004), monolingual parallel cor-pora (Barzilay and McKeown, 2001; Pang et al,2003), and bilingual parallel corpora (Bannard andCallison-Burch, 2005; Madnani et al, 2007; Zhao etal., 2008b).
We focus on the latter type of data.Paraphrase extraction using bilingual parallel cor-pora was proposed by Bannard and Callison-Burch(2005) who induced paraphrases using techniquesfrom phrase-based statistical machine translation(Koehn et al, 2003).
After extracting a bilingualphrase table, English paraphrases are obtained bypivoting through foreign language phrases.
Sincemany paraphrases can be extracted for a phrase,Bannard and Callison-Burch rank them using a para-phrase probability defined in terms of the translationmodel probabilities p(f |e) and p(e|f):p(e2|e1) =?fp(e2, f |e1) (1)=?fp(e2|f, e1)p(f |e1) (2)?
?fp(e2|f)p(f |e1).
(3)Several subsequent efforts extended the bilin-gual pivoting technique, many of which introducedelements of more contemporary syntax-based ap-proaches to statistical machine translation.
Mad-nani et al (2007) extended the technique to hier-archical phrase-based machine translation (Chiang,2005), which is formally a synchronous context-freegrammar (SCFG) and thus can be thought of as aparaphrase grammar.
The paraphrase grammar canparaphrase (or ?decode?)
input sentences using anSCFG decoder, like the Hiero, Joshua or cdec MTsystems (Chiang, 2007; Li et al, 2009; Dyer et al,2010).
Like Hiero, Madnani?s model uses just onenonterminal X instead of linguistic nonterminals.Three additional efforts incorporated linguisticsyntax.
Callison-Burch (2008) introduced syntac-tic constraints by labeling all phrases and para-phrases (even non-constituent phrases) with CCG-inspired slash categories (Steedman and Baldridge,2011), an approach similar to Zollmann and Venu-gopal (2006)?s syntax-augmented machine transla-tion (SAMT).
Callison-Burch did not formally de-fine a synchronous grammar, nor discuss decoding,since his presentation did not include hierarchicalrules.
Cohn and Lapata (2008) used the GHKMextraction method (Galley et al, 2004), which islimited to constituent phrases and thus producesa reasonably small set of syntactic rules.
Zhaoet al (2008b) added slots to bilingually extractedparaphrase patterns that were labeled with part-of-speech tags, but not larger syntactic constituents.Before the shift to statistical natural language pro-cessing, paraphrasing was often treated as syntactictransformations or by parsing and then generating1169from a semantic representation (McKeown, 1979;Muraki, 1982; Meteer and Shaked, 1988; Shem-tov, 1996; Yamamoto, 2002).
Indeed, some workgenerated paraphrases using (non-probabilistic) syn-chronous grammars (Shieber and Schabes, 1990;Dras, 1997; Dras, 1999; Kozlowski et al, 2003).After the rise of statistical machine translation, anumber of its techniques were repurposed for para-phrasing.
These include sentence alignment (Galeand Church, 1993; Barzilay and Elhadad, 2003),word alignment and noisy channel decoding (Brownet al, 1990; Quirk et al, 2004), phrase-based models(Koehn et al, 2003; Bannard and Callison-Burch,2005), hierarchical phrase-based models (Chiang,2005; Madnani et al, 2007), log-linear models andminimum error rate training (Och, 2003a; Madnaniet al, 2007; Zhao et al, 2008a), and here syntax-based machine translation (Wu, 1997; Yamada andKnight, 2001; Melamed, 2004; Quirk et al, 2005).Beyond cementing the ties between paraphrasingand syntax-based statistical machine translation, thenovel contributions of our paper are (1) an in-depthanalysis of the types of structural and sententialparaphrases that can be extracted with bilingual piv-oting, (2) a discussion of how our English?Englishparaphrase grammar should be adapted to specifictext-to-text generation tasks (Zhao et al, 2009) with(3) a concrete example of the adaptation procedurefor the task of paraphrase-based sentence compres-sion (Knight and Marcu, 2002; Cohn and Lapata,2008; Cohn and Lapata, 2009).3 SCFGs in TranslationThe model we use in our paraphrasing approach isa syntactically informed synchronous context-freegrammar (SCFG).
The SCFG formalism (Aho andUllman, 1972) was repopularized for statistical ma-chine translation by Chiang (2005).
Formally, aprobabilistic SCFG G is defined by specifyingG = ?N , TS , TT ,R, S?,whereN is a set of nonterminal symbols, TS and TTare the source and target language vocabularies, Ris a set of rules and S ?
N is the root symbol.
Therules inR take the form:C ?
?
?, ?,?, w?,PP/NN ?
mit einer  |  with aNP ?
das leck  |  the leakVP ?
NP PP/NN detonation zu schliessen  |  closing NP PP/NN blasttheyVPVPPRP VBD NNDTNNNP NPNPclosing tried theSsie versuchten das zu schliessenleakleckwith a   blastDT INPPVBGeinermit detonationFigure 1: Synchronous grammar rules for translation areextracted from sentence pairs in a bixtext which havebeen automatically parsed and word-aligned.
Extractionmethods vary on whether they extract only minimal rulesfor phrases dominated by nodes in the parse tree, or morecomplex rules that include non-constituent phrases.where the rule?s left-hand side C ?
N is a nonter-minal, ?
?
(N?TS)?
and ?
?
(N?TT )?
are stringsof terminal and nonterminal symbols with an equalnumber of nonterminals cNT (?)
= cNT (?)
and?
: {1 .
.
.
cNT (?)}
?
{1 .
.
.
cNT (?
)}constitutes a one-to-one correspondency functionbetween the nonterminals in ?
and ?.
A non-negative weight w ?
0 is assigned to each rule, re-flecting the likelihood of the rule.Rule Extraction Phrase-based approaches to sta-tistical machine translation (and their successors)extract pairs of (e, f) phrases from automaticallyword-aligned parallel sentences.
Och (2003b)described various heuristics for extracting phrasealignments from the Viterbi word-level alignmentsthat are estimated using Brown et al (1993) word-alignment models.These phrase extraction heuristics have been ex-tended so that they extract synchronous grammarrules (Galley et al, 2004; Chiang, 2005; Zollmannand Venugopal, 2006; Liu et al, 2006).
Most ofthese extraction methods require that one side of theparallel corpus be parsed.
This is typically done au-tomatically with a statistical parser.Figure 1 shows examples of rules obtained froma sentence pair.
To extract a rule, we first choose asource side span f like das leck.
Then we use phraseextraction techniques to find target spans e that areconsistent with the word alignment (in this case the1170leak is consistent with our f ).
The nonterminal sym-bol that is the left-hand side of the SCFG rule is thendetermined by the syntactic constituent that domi-nates e (in this case NP).
To introduce nonterminalsinto the right-hand side of the rule, we can applyrules extracted over sub-phrases of f , synchronouslysubstituting the corresponding nonterminal symbolfor the sub-phrases on both sides.
The synchronoussubstitution applied to f and e then yields the corre-spondency ?.One significant differentiating factor between thecompeting ways of extracting SCFG rules is whetherthe extraction method generates rules only for con-stituent phrases that are dominated by a node inthe parse tree (Galley et al, 2004; Cohn andLapata, 2008) or whether they include arbitraryphrases, including non-constituent phrases (Zoll-mann and Venugopal, 2006; Callison-Burch, 2008).We adopt the extraction for all phrases, includingnon-constituents, since it allows us to cover a muchgreater set of phrases, both in translation and para-phrasing.Feature Functions Rather than assigning a singleweight w, we define a set of feature functions ~?
={?1...?N} that are combined in a log-linear model:w = ?N?i=1?i log?i.
(4)The weights ~?
of these feature functions are set tomaximize some objective function like BLEU (Pap-ineni et al, 2002) using a procedure called minimumerror rate training (MERT), owing to Och (2003a).MERT iteratively adjusts the weights until the de-coder produces output that best matches referencetranslations in a development set, according to theobjective function.
We will examine appropriate ob-jective functions for text-to-text generation tasks inSection 6.2.Typical features used in statistical machine trans-lation include phrase translation probabilities (cal-culated using maximum likelihood estimation overall phrase pairs enumerable in the parallel cor-pus), word-for-word lexical translation probabili-ties (which help to smooth sparser phrase transla-tion estimates), a ?rule application penalty?
(whichgoverns whether the system prefers fewer longerthey can not be dangerous to the rest of the villageVP/PPVB+JJSNPNP/NNsie k?nnengef?hrlich werdennichtdem rest des dorfesVP/PPVB+JJSNPNP/NNNP/NN ?
dem rest des  |   the rest of theNP ?
NP/NN dorfes  |  NP/NN villageVP/PP ?
nicht VB+JJ k?nnen  |  can not VB+JJVB+JJ ?
gef?hrlich werden  |  be dangerousS ?
sie NP VP/PP  |  they VP/PP to NPFigure 2: An example derivation produced by a syntacticmachine translation system.
Although the synchronoustrees are unlike the derivations found in the Penn Tree-bank, their yield is a good translation of the German.phrases or a greater number of shorter phrases), anda language model probability.Decoding Given an SCFG and an input sourcesentence, the decoder performs a search for the sin-gle most probable derivation via the CKY algorithm.In principle the best translation should be the En-glish sentence e that is the most probable after sum-ming over all d ?
D derivations, since many deriva-tions yield the same e. In practice, we use a Viterbiapproximation and return the translation that is theyield of the single best derivation:e?
= arg maxe?Trans(f)?d?D(e,f)p(d, e|f)?
yield(arg maxd?D(e,f)p(d, e|f)).
(5)Derivations are simply successive applications of theSCFG rules such as those given in Figure 2.4 SCFGs in ParaphrasingRule Extraction To create a paraphrase grammarfrom a translation grammar, we extend the syntac-tically informed pivot approach of Callison-Burch(2008) to the SCFG model.
For this purpose, weassume a grammar that translates from a given for-eign language to English.
For each pair of trans-lation rules where the left-hand side C and foreign1171string ?
match:C ?
?
?, ?1,?1, ~?1?C ?
?
?, ?2,?2, ~?2?,we create a paraphrase rule:C ?
?
?1, ?2,?, ~?
?,where the nonterminal correspondency relation ?has been set to reflect the combined nonterminalalignment:?
= ?
?11 ?
?2 .Feature Functions In the computation of the fea-tures ~?
from ~?1 and ~?2 we follow the approximationin Equation 3, which yields lexical and phrasal para-phrase probability features.
Additionally, we add aboolean indicator for whether the rule is an iden-tity paraphrase, ?identity .
Another indicator feature,?reorder , fires if the rule swaps the order of two non-terminals, which enables us to promote more com-plex paraphrases that require structural reordering.Decoding With this, paraphrasing becomes anEnglish-to-English translation problem which canbe formulated similarly to Equation 5 as:e?2 ?
yield(arg maxd?D(e2,e1)p(d, e2|e1)).Figure 3 shows an example derivation produced as aresult of applying our paraphrase rules in the decod-ing process.
Another advantage of using the decoderfrom statistical machine translation is that n-gramlanguage models, which have been shown to be use-ful in natural language generation (Langkilde andKnight, 1998), are already well integrated (Huangand Chiang, 2007).5 AnalysisA key motivation for the use of syntactic paraphrasesover their phrasal counterparts is their potential tocapture meaning-preserving linguistic transforma-tions in a more general fashion.
A phrasal system islimited to memorizing fully lexicalized transforma-tions in its paraphrase table, resulting in poor gener-alization capabilities.
By contrast, a syntactic para-phrasing system intuitively should be able to addressthis issue and learn well-formed and generic patternsthat can be easily applied to unseen data.twelve cartoons insulting theprophetmohammadCDNNSJJ DTNNPNPNPVPNPDT+NNP12 the prophet mohammadCDNNSJJ DTNNPNPNPVPNPDT+NNPcartoons offensiveForeign Pivot PhraseParaphrase RuleJJ ?
offensive  |   insultingLexical paraphrase:NP ?
NP that VP  |  NP VPReduced relative clause:NP ?
CD of the NNS  |  CD NNSPartitive construction:VP ?
are JJ to NP  |  JJ NPPred.
adjective copula deletion:JJ -> beleidigend  |  offensiveJJ -> beleidigend  |  insultingNP -> NP die VP  |  NP VPNP -> NP die VP  |  NP that VPNP -> CD der NNS  |  CD of the NNSNP -> CD der NNS  |  CD NNSVP ?
sind JJ f?r NP  |  are JJ to NPVP ?
sind JJ f?r NP  |  JJ NPof the that are toFigure 3: An example of a synchronous paraphrasticderivation.
A few of the rules applied in the parse areshow in the left column, with the pivot phrases that gaverise to them on the right.To put this expectation to the test, we investigatehow our grammar captures a number of well-knownparaphrastic transformations.1 Table 1 shows thetransformations along with examples of the genericgrammar rules our system learns to represent them.When given a transformation to extract a syntacticparaphrase for, we want to find rules that neitherunder- nor over-generalize.
This means that, whilereplacing the maximum number of syntactic argu-ments with nonterminals, the rules ideally will bothretain enough lexicalization to serve as sufficient ev-idence for the applicability of the transformation andimpose constraints on the nonterminals to ensure thearguments?
well-formedness.The paraphrases implementing the possessive ruleand the dative shift shown in Table 1 are a goodexamples of this: the two noun-phrase argumentsto the expressions are abstracted to nonterminalswhile each rule?s lexicalization provides an appro-priate frame of evidence for the transform.
This isimportant for a good representation of dative shift,which is a reordering transformation that fully ap-plies to certain ditransitive verbs while other verbsare uncommon in one of the forms:1The data and software used to extract the grammar we drawthese examples from is described in Section 6.5.1172Possessive rule NP ?
the NN of the NNP | the NNP ?s NNNP ?
the NNS 1 made by NNS 2 | the NNS 2?s NNS 1Dative shift VP ?
give NN to NP | give NP the NNVP ?
provide NP1 to NP2 | give NP2 NP1Adv./adj.
phrase move S/VP ?
ADVP they VBP | they VPB ADVPS ?
it is ADJP VP | VP is ADJPVerb particle shift VP ?
VB NP up | VB up NPReduced relative clause SBAR/S ?
although PRP VBP that | although PRP VBPADJP ?
very JJ that S | JJ SPartitive constructions NP ?
CD of the NN | CD NNNP ?
all DT\NP | all of the DT\NPTopicalization S ?
NP , VP .
| VP , NP .Passivization SBAR?
that NP had VBN | which was VBN by NPLight verbs VP ?
take action ADVP | to act ADVPVP ?
TO take a decision PP | TO decide PPTable 1: A selection of meaning-preserving transformations and hand-picked examples of syntactic paraphrases thatour system extracts capturing these.give decontamination equipment to Japangive Japan decontamination equipmentprovide decontamination equipment to Japan?
provide Japan decontamination equipmentNote how our system extracts a dative shift rule forto give and a rule that both shifts and substitutes amore appropriate verb for to provide.The use of syntactic nonterminals in our para-phrase rules to capture complex transforms alsomakes it possible to impose constraints on their ap-plication.
For comparison, as Madnani et al (2007)do not impose any constraints on how the nontermi-nal X can be realized, their equivalent of the topi-calization rule would massively overgeneralize:S ?
X1, X2 .
| X2, X1 .Additional examples of transforms our use of syn-tax allows us to capture are the adverbial phraseshift and the reduction of a relative clause, as wellas other phenomena listed in Table 1.Unsurprisingly, syntactic information alone is notsufficient to capture all transformations.
For in-stance it is hard to extract generic paraphrases for allinstances of passivization, since our syntactic modelcurrently has no means of representing the morpho-logical changes that the verb undergoes:the reactor leaks radiationradiation is leaking from the reactor .Still, for cases where the verb?s morphology doesnot change, we manage to learn a rule:the radiation that the reactor had leakedthe radiation which leaked from the reactor .Another example of a deficiency in our synchronousgrammar models are light verb constructs such as:to take a walkto walk .Here, a noun is transformed into the correspondingverb ?
something our synchronous syntactic CFGsare not able to capture except through memorization.Our survey shows that we are able to extract ap-propriately generic representations for a wide rangeof paraphrastic transformations.
This is a surpris-ing result which shows that bilingual parallel cor-pora can be used to learn sentential paraphrases, andthat they are a viable alternative to other data sourceslike monolingual parallel corpora, which more obvi-ously contain sentential paraphrases, but are scarce.6 Text-to-Text ApplicationsThe core of many text-to-text generation tasks issentential paraphrasing, augmented with specificconstraints or goals.
Since our model borrows muchof its machinery from statistical machine translation?
a sentential rewriting problem itself ?
it is straight-forward to use our paraphrase grammars to generatenew sentences using SMT?s decoding and param-eter optimization techniques.
Our framework canbe adapted to many different text-to-text generationtasks.
These could include text simplification, sen-1173tence compression, poetry generation, query expan-sion, transforming declarative sentences into ques-tions, and deriving hypotheses for textual entail-ment.
Each individual text-to-text application re-quires that our framework be adapted in severalways, by specifying:?
A mechanism for extracting synchronousgrammar rules (in this paper we argue thatpivot-based paraphrasing is widely applicable).?
An appropriate set of rule-level features thatcapture information pertinent to the task (e.g.whether a rule simplifies a phrase).?
An appropriate ?objective function?
that scoresthe output of the model, i.e.
a task-specificequivalent to the BLEU metric in SMT.?
A development set with examples of the sen-tential transformations that we are modeling.?
Optionally, a way of injecting task-specificrules that were not extracted automatically.In the remainder of this section, we illustrate howour bilingually extracted paraphrases can be adaptedto perform sentence compression, which is the taskof reducing the length of sentence while preservingits core meaning.
Most previous approaches to sen-tence compression focused only on the deletion ofa subset of words from the sentence (Knight andMarcu, 2002).
Our approach follows Cohn and La-pata (2008), who expand the task to include substi-tutions, insertions and reorderings that are automat-ically learned from parallel texts.6.1 Feature DesignIn Section 4 we discussed phrasal probabilities.While these help quantify how good a paraphraseis in general, they do not make any statement ontask-specific things such as the change in languagecomplexity or text length.
To make this informationavailable to the decoder, we enhance our paraphraseswith four compression-targeted features.
We add thecount features csrc and ctgt , indicating the number ofwords on either side of the rule as well as two differ-ence features: cdcount = ctgt ?
csrc and the anal-ogously computed difference in the average wordlength in characters, cdavg .6.2 Objective FunctionGiven our paraphrasing system?s connection toSMT, the naive/obvious choice for parameter op-timization would be to optimize for BLEU over aset of paraphrases, for instance parallel English ref-erence translations for a machine translation task(Madnani et al, 2007).
For a candidate C and a ref-erence R, (with lengths c and r) BLEU is defined as:BLEUN (C,R)={e(1?c/r) ?
e?Nn=1 logwnpn if c/r ?
1e?Nn=1 logwnpn otherwise ,where pn is the modified n-gram precision of Cagainst R, with typically N = 4 and wn = 1N .
The?brevity penalty?
term e(1?c/r) is added to preventshort candidates from achieving perfect scores.Naively optimizing for BLEU, however, will re-sult in a trivial paraphrasing system heavily biasedtowards producing identity ?paraphrases?.
This isobviously not what we are looking for.
Moreover,BLEU does not provide a mechanism for directlyspecifying a per-sentence compression rate, whichis desirable for the compression task.Instead, we propose PRE?CIS, an objective func-tion tailored to the text compression task:PRE?CIS?,?
(I, C,R)={e?(?
?c/i) ?
BLEU(C,R) if c/i ?
?BLEU(C,R) otherwiseFor an input sentence I , an output C and ref-erence compression R (with lengths i, c and r),PRE?CIS combines the precision estimate of BLEUwith an additional ?verbosity penalty?
that is ap-plied to compressions that fail to meet a given targetcompression rate ?.
We rely on the BLEU brevitypenalty to prevent the system from producing overlyaggressive compressions.
The scaling term ?
deter-mines how severely we penalize deviations from ?.In our experiments we use ?
= 10.It is straightforward to find similar adaptations forother tasks.
For text simplification, for instance, thepenalty term can include a readability metric.
Forpoetry generation we can analogously penalize out-puts that break the meter (Greene et al, 2010).6.3 Development DataTo tune the parameters of our paraphrase system forsentence compression, we need an appropriate cor-1174pus of reference compressions.
Since our model isdesigned to compress by paraphrasing rather thandeletion, the commonly used deletion-based com-pression data sets like the Ziff-Davis corpus are notsuitable.
We have thus created a corpus of com-pression paraphrases.
Beginning with 9570 tuplesof parallel English?English sentences obtained frommultiple reference translations for machine transla-tion evaluation, we construct a parallel compressioncorpus by selecting the longest reference in each tu-ple as the source sentence and the shortest referenceas the target sentence.
We further retain only thosesentence pairs where the compression rate cr falls inthe range 0.5 < cr ?
0.8.
From these, we randomlyselect 936 sentences for the development set, as wellas 560 sentences for a test set that we use to gaugethe performance of our system.6.4 Grammar AugmentationsAs we discussed in Section 5, the paraphrase gram-mar we induce is capable of representing a wide va-riety of transformations.
However, the formalismand extraction method are not explicitly geared to-wards a compression application.
For instance, thesynchronous nature of our grammar does not allowus to perform deletions of constituents as done byCohn and Lapata (2007)?s tree transducers.
One wayto extend the grammar?s capabilities towards the re-quirements of a given task is by injecting additionalrules designed to capture appropriate operations.For the compression task, this could includeadding rules to delete target-side nonterminals:JJ ?
JJ | ?This would render the grammar asynchronous andrequire adjustments to the decoding process.
Al-ternatively, we can generate rules that specificallydelete particular adjectives from the corpus:JJ ?
superfluous | ?
.In our experiments we evaluate the latter approachby generating optional deletion rules for all adjec-tives, adverbs and determiners.6.5 Experimental SetupWe extracted a paraphrase grammar from theFrench?English Europarl corpus (v5).
The bitextwas aligned using the Berkeley aligner and the En-glish side was parsed with the Berkeley parser.
WeGrammar # Rulestotal 42,353,318w/o identity 23,641,016w/o complex constituents 6,439,923w/o complex const.
& identity 5,097,250Table 2: Number and distribution of rules in our para-phrase grammar.
Note the significant number of identityparaphrases and rules with complex nonterminal labels.obtained the initial translation grammar using theSAMT toolkit (Venugopal and Zollmann, 2009).The grammars we extract tend to be extremelylarge.
To keep their size manageable, we only con-sider translation rules that have been seen more than3 times and whose translation probability exceeds10?4 for pivot recombination.
Additionally, we onlyretain the top 25 most likely paraphrases of eachphrase, ranked by a uniformly weighted combina-tion of phrasal and lexical paraphrase probabilities.We tuned the model parameters to our PRE?CISobjective function, implemented in the Z-MERTtoolkit (Zaidan, 2009).
For decoding we used theJoshua decoder (Li et al, 2010).
The languagemodel used in our paraphraser and the Clarke andLapata (2008) baseline system is a Kneser-Ney dis-counted 5-gram model estimated on the Gigawordcorpus using the SRILM toolkit (Stolcke, 2002).6.6 EvaluationTo assess the output quality of the resulting sentencecompression system, we compare it to two state-of-the-art sentence compression systems.
Specifically,we compare against our implementation of Clarkeand Lapata (2008)?s compression model which usesa series of constraints in an integer linear program-ming (ILP) solver, and Cohn and Lapata (2007)?stree transducer toolkit (T3) which learns a syn-chronous tree substitution grammar (STSG) frompaired monolingual sentences.
Unlike SCFGs, theSTSG formalism allows changes to the tree topol-ogy.
Cohn and Lapata argue that this is a naturalfit for sentence compression, since deletions intro-duce structural mismatches.
We trained the T3 soft-ware2 on the 936 ?full, compressed?
sentence pairsthat comprise our development set.
This is equiva-lent in size to the training corpora that Cohn and La-pata (2007) used (their training corpora ranged from2www.dcs.shef.ac.uk/people/T.Cohn/t3/1175882?1020 sentence pairs), and has the advantage ofbeing in-domain with respect to our test set.
Boththese systems reported results outperforming previ-ous systems such as McDonald (2006).
To showcasethe value of the adaptations discussed above, we alsocompare variants of our paraphrase-based compres-sion systems: using Hiero instead of syntax, usingsyntax with or without compression features, usingan augmented grammar with optional deletion rules.We solicit human judgments of the compres-sions along two five-point scales: grammaticalityand meaning.
Judges are instructed to decide howmuch the meaning from a reference translation isretained in the compressed sentence, with a scoreof 5 indicating that all of the important informationis present, and 1 being that the compression doesnot retain any of the original meaning.
Similarly, agrammar score of 5 indicates perfect grammaticality,and a grammar score of 1 is assigned to sentencesthat are entirely ungrammatical.
To ensure fairness,we perform pairwise system comparisons with com-pression rates strictly tied on the sentence-level.
Forany comparison, a sentence is only included in thecomputation of average scores if the difference be-tween both systems?
compression rates is < 0.05.3Table 4 shows a set of pairwise comparisons forcompression rates ?
0.5.
We see that going froma Hiero-based to a syntactic paraphrase grammaryields a significant improvement in grammatical-ity.
Adding compression-specific features improvesgrammaticality even further.
Further augmenting thegrammar with deletion rules significantly helps re-tain the core meaning at compression rates this high,however compared to the un-augmented syntacticsystem grammaticality scores drop.
While our ap-proach significantly outperforms the T3 system, weare not able to match ILP?s results in grammaticality.In Table 3 we compare our system to the ILP ap-proach at a modest compression rate of?
0.8.
Here,we significantly outperform ILP in meaning reten-tion while achieving comparable results in gram-maticality.
This improvement is significant at p <0.0001, using the sign test, while the better gram-maticality score of the ILP system is not statisti-3Because evaluation quality correlates linearly with com-pression rate, the community-accepted practice of not compar-ing based on a closely tied compression rate is potentially sub-ject to erroneous interpretation (Napoles et al, 2011).CR Meaning GrammarReference 0.73 4.26 4.35Syntax+Feat.
0.80 3.67 3.38ILP 0.80 3.50 3.49Random Deletions 0.50 1.94 1.57Table 3: Results of the human evaluation on longer com-pressions: pairwise compression rates (CR), meaning andgrammaticality scores.
Bold indicates a statistically sig-nificance difference at p < 0.05.CR Meaning GrammarHiero 0.56 2.57 2.35Syntax 0.56 2.76 2.67Syntax 0.53 2.70 2.49Syntax+Feat.
0.53 2.71 2.54Syntax+Feat.
0.54 2.79 2.71Syntax+Aug.
0.54 2.96 2.52Syntax+Aug.
0.52 2.87 2.40ILP 0.52 2.83 3.09Syntax+Aug.
0.50 2.41 2.20T3 0.50 2.01 1.93Table 4: Human evaluation for shorter compressions andfor variations of our paraphrase system.
+Feat.
includesthe compression features from Section 6.1, +Aug.
in-cludes optional deletion rules from Section 6.4.cally significant (p < 0.088).
These results indi-cate that, over a variety of compression rates, ourframework for text-to-text generation is performingas well as or better than specifically tailored state-of-the-art methods.Table 5 shows an example sentence drawn fromour test set and the compressions produced by thedifferent systems.
We see that both the paraphraseand ILP systems produce good quality results, withthe paraphrase system retaining the meaning of thesource sentence more accurately.7 ConclusionIn this work we introduced a method to learn syntac-tically informed paraphrases from bilingual paralleltexts.
We discussed the expressive power and limita-tions of our formalism and outlined straightforwardadaptation strategies for applications in text-to-textgeneration.
We demonstrated when our paraphras-ing system was adapted to do sentence compression,it achieved results competitive with state-of-the-artcompression systems with only minimal effort.1176Source he also expected that he would have a role in the future at the level of the islamic movementacross the palestinian territories , even if he was not lucky enough to win in the elections .Reference he expects to have a future role in the islamic movement in the palestinian territories if he isnot successful in the elections .Syntax+Feat.
he also expected that he would have a role in the future of the islamic movement in thepalestinian territories , although he was not lucky enough to win elections .ILP he also expected that he would have a role at the level of the islamic movement , even if hewas not lucky enough to win in the elections .Source in this war which has carried on for the last 12 days , around 700 palestinians , which includea large number of women and children , have died .Reference about 700 palestinians , mostly women and children , have been killed in the israeli offensiveover the last 12 days .Syntax+Feat.
in this war has done for the last 12 days , around 700 palestinians , including women andchildren , died .ILP in this war which has carried for the days palestinians , which include a number of womenand children died .Source hala speaks arabic most of the time with her son , taking into consideration that he can speakenglish with others .Reference hala speaks to her son mostly in arabic , as he can speak english to others .Syntax+Feat.
hala speaks arabic most of the time with her son , considering that he can speak english withothers .ILP hala speaks arabic most of the time , taking into consideration that he can speak english withothers .Table 5: Example compressions produced by the two systems in Table 3 for three input sentences from our test data.AcknowledgmentsWe would like to thank Trevor Cohn for kindly pro-viding us with the T3 compression system.
This re-search was supported by the NSF under grant IIS-0713448.
Opinions, interpretations, and conclusionsare the authors?
alone.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The Theoryof Parsing, Translation, and Compiling.
Prentice Hall.Peter G. Anick and Suresh Tipirneni.
1999.
The para-phrase search assistant: terminological feedback foriterative information seeking.
In Proceedings of SI-GIR.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL.Regina Barzilay and Noemie Elhadad.
2003.
Sentencealignment for monolingual comparable corpora.
InProceedings of EMNLP.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT/NAACL.Regina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceedingsof ACL.Regina Barzilay, Kathleen R. McKeown, and MichaelElhadad.
1999.
Information fusion in the contextof multi-document summarization.
In Proceedings ofACL.Regina Barzilay.
2003.
Information Fusion for Mutli-document Summarization: Paraphrasing and Genera-tion.
Ph.D. thesis, Columbia University, New York.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL/HLT.Peter Brown, John Cocke, Stephen Della Pietra, VincentDella Pietra, Frederick Jelinek, Robert Mercer, and1177Paul Poossin.
1990.
A statistical approach to languagetranslation.
Computational Linguistics, 16(2), June.Peter Brown, Stephen Della Pietra, Vincent Della Pietra,and Robert Mercer.
1993.
The mathematics of ma-chine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311, June.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: An integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, 31:273?381.Trevor Cohn and Mirella Lapata.
2007.
Large marginsynchronous generation and its application to sentencecompression.
In Proceedings of EMNLP-CoLing.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings of theCOLING.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of Artificial In-telligence Research (JAIR), 34:637?674.Peter W. Culicover.
1968.
Paraphrase generation andinformation retrieval from stored text.
Mechani-cal Translation and Computational Linguistics, 11(1-2):78?88.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
In Pro-ceedings of the COLING.Mark Dras.
1997.
Representing paraphrases using syn-chronous tree adjoining grammars.
In Proceedings ofACL.Mark Dras.
1999.
Tree Adjoining Grammar and the Re-luctant Paraphrasing of Text.
Ph.D. thesis, MacquarieUniversity, Australia.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proceed-ings of ACL.William Gale and Kenneth Church.
1993.
A programfor aligning sentences in bilingual corpora.
Compu-atational Linguistics, 19(1):75?90.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT/NAACL.Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010.Automatic analysis of rhythmic poetry with applica-tions to generation and translation.
In Proceedings ofEMNLP.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of ACL.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedings ofEMNLP.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139:91?107.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT/NAACL.Raymond Kozlowski, Kathleen McCoy, and K. Vijay-Shanker.
2003.
Generation of single-sentenceparaphrases from predicate/argument structure usinglexico-grammatical resources.
In Workshop On Para-phrasing.Irene Langkilde and Kevin Knight.
1998.
The practi-cal value of n-grams in generation.
In Workshop OnNatural Language Generation, Ontario, Canada.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenThornton, Jonathan Weese, and Omar Zaidan.
2009.Joshua: An open source toolkit for parsing-based ma-chine translation.
In Proceedings of WMT09.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Ann Irvine, Sanjeev Khudanpur, LaneSchwartz, Wren Thornton, Ziyuan Wang, JonathanWeese, and Omar Zaidan.
2010.
Joshua 2.0: Atoolkit for parsing-based machine translation with syn-tax, semirings, discriminative training and other good-ies.
In Proceedings of WMT10.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules from text.
Natural Language Engineering,7(3):343?360.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment templates for statistical machinetranslation.
In Proceedings of the ACL/Coling.Nitin Madnani and Bonnie Dorr.
2010.
Generat-ing phrasal and sentential paraphrases: A surveyof data-driven methods.
Computational Linguistics,36(3):341?388.Nitin Madnani, Necip Fazil Ayan, Philip Resnik, andBonnie Dorr.
2007.
Using paraphrases for parametertuning in statistical machine translation.
In Proceed-ings of WMT07.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceedingsof EACL.1178Kathleen R. McKeown.
1979.
Paraphrasing using givenand new information in a question-answer system.
InProceedings of ACL.Dan Melamed.
2004.
Statistical machine translation byparsing.
In Proceedings of ACL.Marie W. Meteer and Varda Shaked.
1988.
Strategies foreffective paraphrasing.
In Proceedings of COLING.Kazunori Muraki.
1982.
On a semantic model for multi-lingual paraphrasing.
In Proceedings of COLING.Courtney Napoles, Chris Callison-Burch, and Ben-jamin Van Durme.
2011.
Evaluating sentence com-pression: Pitfalls and suggested remedies.
In Work-shop on Monolingual Text-To-Text Generation.Franz Josef Och.
2003a.
Minimum error rate training forstatistical machine translation.
In Proceedings of ACL.Franz Josef Och.
2003b.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL.Karolina Owczarzak, Declan Groves, Josef Van Gen-abith, and Andy Way.
2006.
Contextual bitext-derivedparaphrases in automatic MT evaluation.
In Proceed-ings of WMT06.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.
InProceedings of HLT/NAACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings of ACL.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In Proceedings of EMNLP.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of ACL.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsufrace text patterns for a question answering system.In Proceedings of ACL.Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisticalmachine translation for query expansion in answer re-trieval.
In Proceedings of ACL.Hadar Shemtov.
1996.
Generation of paraphrases fromambiguous logical forms.
In Proceedings of COLING.Stuart Shieber and Yves Schabes.
1990.
Generation andsynchronous tree-adjoining grammars.
In WorkshopOn Natural Language Generation.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2010.
Ter-plus: paraphrase, se-mantic, and alignment enhancements to translationedit rate.
Machine Translation, 23(2-3):117?127.Mark Steedman and Jason Baldridge.
2011.
Combi-natory categorial grammar.
In Non-TransformationalSyntax: Formal and Explicit Models of Grammar.Wiley-Blackwell.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceeding of the InternationalConference on Spoken Language Processing.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisition ofentailment relations.
In Proceedings of EMNLP, Pro-ceedings of EMNLP.Ashish Venugopal and Andreas Zollmann.
2009.
Gram-mar based statistical MT on Hadoop: An end-to-endtoolkit for large scale PSCFG based MT.
Prague Bul-letin of Mathematical Linguistics, 91.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3).Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of ACL.Kazuhide Yamamoto.
2002.
Machine translation by in-teraction between paraphraser and transfer.
In Pro-ceedings of COLING.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and ShengLi.
2008a.
Combining multiple resources to improveSMT-based paraphrasing model.
In Proceedings ofACL/HLT.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2008b.
Pivot approach for extracting paraphrasepatterns from bilingual corpora.
In Proceedings ofACL/HLT.Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.
2009.Application-driven statistical paraphrase generation.In Proceedings of ACL.Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,and Eduard Hovy.
2006.
Paraeval: Using paraphrasesto evaluate summaries automatically.
In Proceedingsof HLT/NAACL.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of WMT06.1179
