CRF-based Hybrid Model for Word Segmentation, NER and evenPOS TaggingZhiting Xu, Xian Qian, Yuejie Zhang,  Yaqian ZhouDepartment of Computer Science & Engineering,Shanghai Key Laboratory of Intelligent Information Processing,Fudan University, Shanghai 200433, P. R. China{zhiting, qianxian, yjzhang, zhouyaqian}@fudan.edu.cnAbstractThis paper presents systems submitted tothe close track of Fourth SIGHAN Bakeoff.We built up three systems based on Condi-tional Random Field for Chinese WordSegmentation, Named Entity Recognitionand Part-Of-Speech Tagging respectively.Our systems employed basic features aswell as a large number of linguistic features.For segmentation task, we adjusted the BIOtags according to confidence of each char-acter.
Our final system achieve a F-score of94.18 at CTB, 92.86 at NCC, 94.59 at SXUon Segmentation, 85.26 at MSRA onNamed Entity Recognition, and 90.65 atPKU on Part-Of-Speech Tagging.1 IntroductionFourth SIGHAN Bakeoff includes three tasks, thatis, Word Segmentation, Named Entity Recognition(NER) and Part-Of-Speech (POS) Tagging.
In thePOS Tagging task, the testing corpora are pre-segmented.
Word Segmentation, NER and POSTagging could be viewed as classification prob-lems.
In a Segmentation task, each charactershould be classified into three classes, B, I, O, in-dicating whether this character is the Beginning ofa word, In a word or Out of a word.
For NER, eachcharacter is assigned a tag indicating what kind ofNamed Entity (NE) this character is (Beginning ofa Person Name (PN), In a PN, Beginning of a Lo-cation Name (LN), In a LN, Beginning of an Or-ganization Name (ON), In an ON or not-a-NE).
InPOS tagging task defined by Fourth SIGHAN Ba-keoff, we only need to give a POS tag for eachgiven word in a context.We attended the close track of CTB, NCC, SXUon Segmentation, MSRA on NER and PKU onPOS Tagging.
In the close track, we cannot useany external resource, and thus we extracted sev-eral word lists from training corpora to form multi-ple features beside basic features.
Then we trainedCRF models based on these feature sets.
In CRFmodels, a margin of each character can be gotten,and the margin could be considered as the confi-dence of that character.
For the Segmentation task,we performed the Maximum Probability Segmen-tation first, through which each character is as-signed a BIO tag (B represents the Beginning of aword, I represents In a word and O represents Outof a word).
If the confidence of a character is lowerthan the threshold, the tag of that character will beadjusted to the tag assigned by the MaximumProbability Segmentation (R. Zhang et al, 2006).2 Conditional Random FieldsConditional Random Fields (CRFs) are a class ofundirected graphical models with exponent distri-bution (Lafferty et al, 2001).
A common used spe-cial case of CRFs is linear chain, which has a dis-tribution of:)),,,(exp(1)|(11??=??
=Tt kttkkxtxyyfZxyP rrrr?
(1)where ),,( 1 txyyf ttkr?
is a function which is usu-ally an indicator function; k?
is the learned weightof feature kf ; and xZ r is the normalization factor.The feature function actually consists of two kindsof features, that is, the feature of single state andthe feature of transferring between states.
Featureswill be discussed in section 3.167Sixth SIGHAN Workshop on Chinese Language ProcessingSeveral methods (e.g.
GIS, IIS, L-BFGS) couldbe used to estimate k?
, and L-BFGS has beenshowed to converge faster than GIS and IIS.
Tobuild up our system, we used Pocket CRF1.3 Feature RepresentationWe used three feature sets for three tasks respec-tively, and will describe them respectively.3.1 Word SegmentationWe mainly adopted features from (H. T. Ng et al,2004, Y. Shi et al, 2007), as following:a) Cn(n=-2, -1, 0, 1, 2)b) CnCn+1(n=-2,-1,0,1)c) C-1C1d) CnCn+1Cn+2 (n=-1, 0, 1)e) Pu(C0)f) T(C-2)T(C-1)T(C0)T(C1)T(C2)g) LBegin(C0), Lend(C0)h) Single(C0)where C0 represents the current character and Cnrepresents the nst character from the current charac-ter.
Pu(C0) indicates whether current word is apunctuation.
this feature template helps to indicatethe end of a sentence.
T(C) represents the type ofcharacter C. There are four types we used: (1) Chi-nese Number (?
?/one?, ?
?/two?, ??/ten?
); (2)Chinese Dates (?
?/day?, ?
?/month?, ??/year?
);(3) English letters; and (4) other characters.
The (f)feature template is used to recognize the Chinesedates for the construction of Chinese dates maycause the sparseness problem.
LBegin(C0) representsthe maximum length of the word beginning withthe character C0, and Lend(C0) presents the maxi-mum length of the word ending with the characterC0.
The (g) feature template is used to decide theboundary of a word.
Single(C0) shows whether cur-rent character can form a word solely.3.2 Named Entity RecognitionMost features described in (Y. Wu et al, 2005) areused in our systems.
Specifically, the following isthe feature templates we used:a) Surname(C0): Whether current character is ina Surname List, which includes all first char-acters of PNs in the training corpora.1http://sourceforge.net/project/showfiles.php?group_id=201943b) PersonName(C0C1C2, C0C1): Whether C0C1C2,C0C1 is in the Person Name List, which con-tains all PNs in the training corpora.c) PersonTitle(C-2C-1): Whether C-2C-1 is in thePerson Title List, which is extracted from theprevious two characters of each PN in thetraining corpora.d) LocationName(C0C1,C0C1C2,C0C1C2C3):Whether C0C1,C0C1C2,C0C1C2C3 is in the Lo-cation Name List, which includes all LNs inthe training corpora.e) LocationSuffix(C0): Whether current characteris in the Location Suffix List, which is con-structed using the last character of each LN inthe training corpora.f) OrgSuffix(C0): Whether current character is inthe Organization Suffix List, which containsthe last-two-character of each ON in the train-ing corpora.3.3 Part-Of-Speech TaggingWe employed part of feature templates describedin (H. T. Ng et al, 2004, Y. Shi et al, 2007).
Sincewe are in the close track, we cannot use morpho-logical features from external resources such asHowNet, and we used features that are availablejust from the training corpora.a) Wn, (n=-2,-1,0,1,2)b) WnWn+1, (n=-2,-1,0,1)c) W-1W1d) Wn-1WnWn+1 (n=-1, 1)e) Cn(W0) (n=0,1,2,3)f) Length(W0)where Cn represents the nth character of the currentword, and Length(W0) indicates the length of thecurrent word.4 Reliability EvaluationIn the task of Word Segmentation, the label of eachcharacter is adjusted according to their reliability.For each sentence, we perform Maximum Prob-ability Segmentation first, through which we canget a BIO tagging for each character in the sen-tence.After that, the features are extracted accordingto the feature templates, and the weight of eachfeature has already been estimated in the step oftraining.
Then marginal probability for each char-acter can be computed as follows:168Sixth SIGHAN Workshop on Chinese Language Processing)),(exp()(1)|( yxfxZxyp iirr ?=     (2)The value of )|( xyprbecomes the original re-liability value of BIO label y for the current char-acter under the current contexts.
If the probabilityof y  with the largest probability is lower than 0.75,which is decided according to the experiment re-sults, the tag given by Maximum Probability Seg-mentation will be used instead of tag given by CRF.The motivation of this method is to use the Maxi-mum Probability method to enhance the F-measureof In-Vocabulary (IV) Words.
According to theresults reported in (R. Zhang et al, 2006), CRFperforms relatively better on Out-of-Vocabulary(OOV) words while Maximum Probability per-forms well on IV words, so a model combining theadvantages of these two methods is appealing.
Onesimplest way to combine them is the method wedescribed.
Besides, there are some complex meth-ods, such as estimation using Support Vector Ma-chine (SVM) for CRF, CRF combining boostingand combining Margin Infused Relaxed Algorithm(MIRA) with CRF, that might perform better.However, we did not have enough time to imple-ment these methods, and we will compare themdetailedly in the future work.5 Experiments5.1 Results on Fourth SIGHAN BakeoffWe participated in the close track on Word Seg-mentation on CTB, NCC and SXU corpora, NERon MSRA corpora and POS Tagging on PKU cor-pora.For Word Segmentation and NER, our memorywas enough to use all features.
However, for POStagging, we did not have enough memory to use allfeatures, and we set a frequency cutoff of 10; thatis, we could only estimate variables for those fea-tures that occurred more than ten times.Our results of Segmentation are listed in the Ta-bel 1, the results of NER are listed in the Tabel 2,and the results of POS Tagging are listed in theTabel 3.R P F Roov RivCTB 0.9459 0.9418 0.9439 0.6589 0.9628NCC 0.9396 0.9286 0.9341 0.5007 0.9614SXU 0.9554 0.9459 0.9507 0.6206 0.9735Tabel 1.
Results of Word SegmentationMSRA P R FPER 0.8084 0.8557 0.8314LOC 0.9138 0.8576 0.8848ORG 0.8666 0.773 0.8171Overall 0.873 0.8331 0.8526Tabel 2.
Results of NERTotal-A IV-R OOV-R MT-RPKU 0.9065 0.9259 0.5836 0.8903Tabel 3.
Results of POS Tagging5.2 Errors AnalysisObserving our results of Word Segmentation andPOS Tagging, we found that the recall of OOV isrelatively low, this may be improved through in-troducing features aiming to enhance the perform-ance of OOV.On NER task, we noticed that precision of PNrecognition is relative low, and we found that oursystem may classify some ONs as PNs, such as ????(Guinness)/ORG?
and ?????
(World Re-cord)/)?.
Besides, the bound of PN is sometimesconfusing and may cause problems.
For example,??
?/PER ?/ ?/ ???
may be segmented as???
?/PER ?/ ???.
Further, some words be-ginning with Chinese surname, such as ?????
?, may be classified as PN.For List may not be the real suffix.
For example,??????
should be a LN, but it is very likelythat ?????
is recognized as a LN for its suffix???.
Another problem involves the characters inthe Location Name list may not a LN all the time.In the context ??
?/ ?
?/?, for example, ??
?means Chinese rather than China.For ONs, the correlative dictionary also exists.Consider sequence ?????
?, which should be asingle word, ????
is in the Organization NameList and thus it is recognized as an ON in our sys-tem.
Another involves the subsequence of a word.For example, the sequence ??????????
?, which should be a person title, but ??????????
is an ON.
Besides, our recall of ON islow for the length of an ON could be very long.6 Conclusions and Future WorksWe built up our systems based on the CRF modeland employed multiple linguistics features basedon the knowledge extracted from training corpora.169Sixth SIGHAN Workshop on Chinese Language ProcessingWe found that these features could greatly improvethe performance of all tasks.
Besides, we adjustedthe tag of segmentation result according to the reli-ability of each character, which also helped to en-hance the performance of segmentation.As many other NLP applications, feature plays avery important role in sequential labeling tasks.
Inour POS tagging task, we could only use featureswith high frequency, but some low-frequency fea-tures may also play a vital role in the task; goodnon-redundant features could greatly improve clas-sification performance while save memory re-quirement of classifiers.
In our further research, wewill focus on feature selection on CRFs.AcknowledgementThis research was sponsored by National NaturalScience Foundation of China (No.
60773124, No.60503070).ReferencesO.
Bender, F. J. Och, and H. Ney.
2003.
Maximum En-tropy Models for Named Entity Recognition.
Pro-ceeding of CoNLL-2003.A.
L. Berger, S. A. Della Pietra, and V. J. Della Pietra.1996.
A Maximum Entropy Approach to NaturalLanguage Processing.
Computational Linguistics,22(1).H.
L. Chieu, H. T. Ng.
2002.
Named Entity Recognition:A Maximum Entropy Approach Using Global Infor-mation.
International Conference on ComputationalLinguistics (COLING).J.
N. Darroch and D. Ratcliff.
1972.
Generalized Itera-tive Scaling for Log-Linear Models.
The Annals ofMathematical Statistics, 43(5).J.
Lafferty, A McCallum, and F. Pereira..2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proceed-ings of the 18th International Conf.
on MachineLearning (ICML).R.
Li, J. Wang, X. Chen, X. Tao, and Y. Hu.
2004.
Us-ing Maximum Entropy Model for Chinese TextCategorization.
Computer Research and Develop-ment, 41(4).H.
T. Ng and J. K. Low.
2004.
Chinese Part-Of-SpeechTagging: One-at-a-Time or All-at-Once?
Word-Baseor Character-Based?
Proceedings of Conference onEmpirical Methods in Natural Language Processing(EMNLP).A.
Ratnaparkhi.
1997.
A Simple Introduction to Maxi-mum Entropy Models for Natural Language Process-ing.
Institute for Research in Cognitive Science Re-port, 97(8).F.
Sha and F.Pereira.
2003.
Shallow parsing with condi-tional random fields.
In Proceedings of HLT-NAACL.Y.
Shi and M. Wang.
2007.
A Dual-Layer CRFs BasedJoint Decoding Method for Cascaded Segmentationand Labeling Tasks.
In International Joint Confer-ences on Artificial Intelligence (IJCAI).C.
A. Sutton, K. Rohanimanesh, A. McCallum.
2004.Dynamic conditional random fields: factorized prob-abilistic models for labeling and segmenting se-quence data.
In International Conference on MachineLearning (ICML).M.
Volk, and S. Clematide.
2001.
Learn - Filter - Apply-- Forget Mixed Approaches to Named Entity Rec-ognition.
Proceeding of the 6th International Work-shop on Applications of Natural Language for Infor-mation Systems.Y.
Wu, J. Zhao, B. Xu and H. Yu.
2005.
ChineseNamed Entity Recognition Based on Multiple Fea-tures.
Proceedings of Human Language TechnologyConference and Conference on Empirical Methods inNatural Language Processing (HLT/EMNLP).H.
Zhang, Q. Liu, H. Zhang, and X. Cheng.
2002.
Au-tomatic Recognition of Chinese Unknown WordsBased on Roles Tagging.
Proceeding of the 19th In-ternational Conference on Computational Linguistics.R.
Zhang, G. Kikui and E. Sumita.
2006.
Subword-based tagging by conditional random fields for Chi-neseword segmentation.
Companion volume to the-proceedings of the North American chapter of theAssociation for Computational Linguistics (NAACL).Y.
Zhou, Y. Guo, X. Huang, and L. Wu.
2003.
Chineseand English BaseNP Recognition Based on a Maxi-mum Entropy Model.
Journal of Computer Researchand Development, 40(3).170Sixth SIGHAN Workshop on Chinese Language Processing
