Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 30?37,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsIncremental Reference Resolution: The Task, Metrics for Evaluation, anda Bayesian Filtering Model that is Sensitive to DisfluenciesDavid Schlangen, Timo Baumann, Michaela AttererDepartment of LinguisticsUniversity of Potsdam, Germany{das|timo|atterer}@ling.uni-potsdam.deAbstractIn this paper we do two things: a) we dis-cuss in general terms the task of incre-mental reference resolution (IRR), in par-ticular resolution of exophoric reference,and specify metrics for measuring the per-formance of dialogue system componentstackling this task, and b) we present a sim-ple Bayesian filtering model of IRR thatperforms reasonably well just using wordsdirectly (no structure information and nohand-coded semantics): it picks the rightreferent out of 12 for around 50% of real-world dialogue utterances in our test cor-pus.
It is also able to learn to interpret notonly words but also hesitations, just as hu-mans have shown to do in similar situa-tions, namely as markers of references tohard-to-describe entities.1 IntroductionLike other tasks involved in language comprehen-sion, reference resolution?that is, the linking ofnatural language expressions to contextually givenentities?is performed incrementally by humanlisteners.
This was shown for example by Tanen-haus et al (1995) in a famous experiment whereaddressees of utterances containing referring ex-pressions made eye movements towards target ob-jects very shortly after the end of the first wordthat unambiguously specified the referent, even ifthat wasn?t the final word of the phrase.
In fact, ashas been shown in later experiments (Brennan andSchober, 2001; Bailey and Ferreira, 2007; Arnoldet al, 2007), such disambiguating material doesn?teven have to be lexical: under certain circum-stances, a speaker?s hesitating already seems to beunderstood as increasing the likelihood of subse-quent reference to hard-to-describe entities.Recently, efforts have begun to build dialoguesystems that make use of incremental processingas well (Aist et al, 2006; Skantze and Schlangen,2009).
These efforts have so far focused on as-pects other than resolution of references ((Stonesset al, 2004) deals with the interaction of referenceand parsing).
In this paper, we discuss in gen-eral terms the task of incremental reference res-olution (IRR) and specify metrics for evaluatingincremental components for this task.
To makethe discussion more concrete, we also describe asimple Bayesian filtering model of IRR in a do-main with a small number of possible referents,and show that it performs better wrt.
our metricsif given information about hesitations?thus pro-viding computational support for the rationality ofincluding observables other than words into mod-els of dialogue meaning.The remainder of the paper is structured as fol-lows: We discuss the IRR task in Section 2, andsuitable evaluation metrics in Section 3.
In Sec-tion 4 we describe and analyse the data for whichwe present results with our Bayesian model forIRR in Section 5.2 Incremental Reference ResolutionTo a first approximation, IRR can be modeled asthe ?inverse?
as it were of the task of generating re-ferring expressions (GRE; which is well-studied incomputational linguistics, see e. g. (Dale and Re-iter, 1995)).
Where in GRE words are added thatexpress features which reduce the size of the setof possible distractors (with which the object thatthe expression is intended to pick out can be con-fused), in IRR words are encountered that expressfeatures that reduce the size of the set of possible30referents.
To give a concrete example, for the ex-pression in (1-a), we could imagine that the logicalrepresentation in (1-b) is built on a word-by-wordbasis, and at each step the expression is checkedagainst the world model to see whether the refer-ence has become unique.
(1) a. the red crossb.
?x(red(x) ?
cross(x))To give an example, in a situation where thereare available for reference only one red cross, onegreen circle, and two blue squares, we can saythat after ?the red?
the referent should have beenfound; in a world with two red crosses, we wouldneed to wait for further restricting information(e. g. ?.
.
.
on the left?
).This is one way to describe the task, then: acomponent for incremental reference resolutiontakes expressions as input in a word-by-word fash-ion and delivers for each new input a set (possiblya singleton set) as output which collects those dis-course entities that are compatible with the expres-sion up to that point.
(This description is meantto be neutral as to whether reference is exophoric,i.
e. directly to entities in the world, or anaphoric,via previous mentions; we will mainly discuss theformer case, though.
)As we will see below, this does howevernot translate directly into a usable metric forevaluation.
While it is easy to identify thecontributions of individual words in simple,constructed expressions like (1-a), reference inreal conversations is often much more complex,and is a collaborative process that isn?t confinedto single expressions (Clark and Schaefer, 1987):referring is a pragmatic action that is not reducibleto denotation.
In our corpus (see below), we oftenfind descriptions as in (2), where the speakercontinuously adds (rather vague) material, typi-cally until the addressee signals that she identifiedthe item, or proposes a different way to describe it.
(2) Also das S Teil sieht so aus dass es eineinzelnes .
Teilchen hat .
dann .
vier am Stu?ckim rechten Winkel .. dazu nee .
nee warte ..dann noch ein einzelnes das guckt auf der an-deren Seite raus.well, the S piece looks so that it has a single .
piece .and then .
four together in a 90 degree angle .. and also.
no .. wait .. and then a single piece that sticks out onthe other side.While it?s difficult to say in the individual casewhat the appropriate moment is to settle on a hy-pothesis about the intended referent, and what the?correct?
time-course of the development of hy-potheses is, it?s easy to say what we want to be truein general: we want a referent to be found as earlyas possible, with as little change of opinion as pos-sible during the utterance.1 Hence a model thatfinds the correct referent earlier and makes fewerwrong decisions than a competing one will be con-sidered better.
The metrics we develop in the nextsection spell out this idea.3 Evaluation Metrics for IRRIn previous work, we have discussed metrics forevaluating the performance of incremental speechrecognition (Baumann et al, 2009).
There, ourmetrics could rely on time-aligned gold-standardinformation against which the incremental resultscould be measured.
For the reasons discussedin the previous section, we do not assume thatwe have such temporally-aligned information forevaluating IRR.
Our measures described here sim-ply assume that there is one intention behind thereferring utterances (namely to identify a certainentity), and that this intention is there from the be-ginning of the utterance and stays constant.2 Thisis not to be understood as the claim that it is rea-sonable to expect an IRR component to pick out areferent even if the only part of the utterance thathas already been processed for example is ?nowtake the?
?it just facilitates the ?earlier is better?ranking discussed above.We use two kinds of metrics for IRR: posi-tional metrics, which measure when (which per-centage into the utterance) a certain event happens,and edit metrics which capture the ?jumpiness?of the decision process (how often the componentchanges its mind during an utterance).Figure 1 shows a constructed example that il-1We leave open here what ?as early as possible?
means?a well-trained model might be able to resolve a referencebefore the speaker even deems that possible, and hence ap-pear to do unnatural (or supernatural?)
?mind reading?.
Con-versely, frequent changes of opinion might be something thathuman listeners would exhibit as well (e. g. in their gaze di-rection).
We abstract away from these finer details in ourheuristic.2Note that our metrics would also work for corpora wherethe correct point-of-identification is annotated; this wouldsimply move the reference point from the beginning of theutterance to that point.
Gallo et al (2007) describe an anno-tation effort in a simpler domain where entities can easily bedescribed which would make such information available.31X F W F Fsil?modelno?sil?model X ?
?X Ffirst finalfirst correctedit phasetake thetimegold reference Fwords (sil)F F F(sil)FfFigure 1: Simple constructed example that illus-trates the evaluation measureslustrates these ideas.
We assume that reference isto an object that is internally represented by theletter F. The example shows two models, no-siland sil (what exactly they are doesn?t matter fornow).
The former model guesses that reference isto object X already after the first word, and stayswith this opinion until it encounters the final word,when it chooses F as most likely referent.
(Whythe decision for the items sil is ?-?
will be ex-plained below; here this can be read as ?repetitionof previous decision?.)
The other model changesits mind more often, but also is correct for the firsttime earlier and stays correct earlier.
Our metricsmake this observation more precise:?
average fc (first correct): how deep into the ut-terance do we make the first correct guess?
(If thedecision component delivers n-best lists instead ofsingle guesses, ?correct?
means here and below ?ismember of n-best list?.)E.
g., if the referent is recognised only after thefinal word of the expression, the score for this met-ric would be 1.
In our example it is 2/5 for thesil-model and 1 for the non-sil model.?
fc applicable: since the previous measure canonly be specified for cases where the correct refer-ent has been found, we also specify for how manyutterances this is the case.?
average ff (first final): how deep into the utter-ance do we make the correct guess and don?t sub-sequently change our mind?
This would be 4/5 forthe sil-model in our example and 1 for the no-sil-model.?
ff applicable: again, the previous measure canonly be given where the final guess of the compo-nent is correct, so we also need to specify how of-ten this is the case.
Note that whenever ff is appli-cable, fc is applicable as well, so ff applicable?fcapplicable.?
ed-utt (mean edits per utterance): an IRR mod-ule may still change its mind even after it has al-ready made a correct guess.
This metric measureshow often the module changes its mind before itcomes back to the right guess (if at all).
Since suchdecision-revisions (edits) may be costly for latermodules, which possibly need to retract their ownhypotheses that they?ve built based on the outputof this module, ideally this number should be low.In our example the number of edits between fcand ff is 2 for the sil-model and 0 for the non-silmodel (because here fc and ff are at the same po-sition).?
eo (edit overhead): ratio unnecessary edits / nec-essary edits.
(In the ideal case, there is exactly oneedit, from ?no decision?
to the correct guess.)?
correctness: how often the model guesses cor-rectly.
This is 3/5 for the sil-model in the exampleand 1/5 for the non-sil-model.?
sil-correctness: how often the model guessescorrectly during hesitations.
The correctness mea-sure applied only to certain data-points; we usethis to investigate whether informing the modelabout hesitations is helpful.?
adjusted error: some of our IRR models can re-turn ?undecided?
as reply.
The correctness mea-sures defined above would punish this in the sameway as a wrong guess.
The adjusted error measureimplements the idea that undecidedness is betterthan a wrong guess, at least early in the utterance.More precisely, it?s defined to be 0 if the guess iscorrect, pos / posmax if the reply is ?undecided?
(with pos denoting the position in the utterance),and 1 if the guess is incorrect.
That way uncer-tainty is not punished in the beginning of the utter-ance and counted like an error towards its end.Note that these metrics characterise different as-pects of the performance of a model.
In practi-cal cases, they may not be independent from eachother, and a system designer will have to decidewhich one to optimize.
If it is helpful to be in-formed about a likely referent early, for exampleto prepare a reaction, and is not terribly costly tolater have to revise hypotheses, then a low first cor-rect may be the target.
If hypothesis revisions arecostly, then a low edit overhead may be preferredover a low first correct.
(first final and ff applicable,however, are parameters that are useful for globaloptimisation.
)32Figure 2: The Twelve Pentomino Pieces with theircanonical names (which were not known to the di-alogue participants).
The pieces used in the dia-logues all had the same colour.In the remaining sections, we describe a prob-abilistic model of IRR that we have implemented,and evaluate it in terms of these metrics.
We beginwith describing the data from which we learnt ourmodel.4 Data4.1 Our CorporaAs the basis for training and testing of our modelwe used data from three corpora of task-orienteddialogue that differ in some details of the set-up,but use the same task: an Instruction Giver (IG) in-structs an Instruction Follower (IF) on which puz-zle pieces (from the ?Pentomino?
game, see Fig-ure 2) to pick up.
In detail, the corpora were:?
The Pento Naming corpus described in (Siebertand Schlangen, 2008).
In this variant of the task,IG records instructions for an absent IF; so thesearen?t fully interactive dialogues.
The corpus con-tained 270 utterances out of which we selectedthose 143 that contained descriptions of puzzlepieces (and not of their position on the game-board).?
Selections from the FTT/PTT corpus describedin (Ferna?ndez et al, 2007), where IF and IG areconnected through an audio-only connection, andin some dialogues a simplex / push-to-talk one.We selected all utterances from IG that containedreferences to puzzle pieces (286 altogether).?
The third part of our corpus was constructedspecifically for the experiments described here.We set-up a Wizard of Oz experiment where userswere given the task to describe puzzle pieces forthe ?dialogue system?
to pick up.
The system(i. e. the wizard) had available a limited numberof utterances and hence could conduct only a lim-ited form of dialogue.
We collected 255 utter-ances containing descriptions of puzzle pieces inthis way.0.00.10.20.30.4tileslrtF I L N P T U V W X Y ZFigure 3: Silence rate per referent and corpus(WOz:black, PentoNaming:red, FTT:green)All utterances were hand-transcribed and thetranscriptions were automatically aligned with thespeech data using the MAUS system (Schiel,2004); this way, we could automatically identifypauses during utterances and measure their length.For some experiments (see below), pauses were?re-ified?
through the addition of silence pseudo-words (one for each 333ms of silence).The resulting corpus is not fully balanced interms of available material for the various piecesor contributions by sub-corpora.4.2 Descriptive StatisticsWe were interested to see whether intra-utterancesilences (hesitations) could potentially be used asan information source in our (more or less) real-world data in the same way as was shown inthe much more controlled situations described inthe psycholinguistics literature mentioned abovein the introduction (Arnold et al, 2007).
Fig-ure 3 shows the mean ratio of within-utterance si-lences per word for the different corpora and dif-ferent referents.
We can see that there are cleardifferences between the pieces.
For example, ref-erences to the piece whose canonical name is Xcontain very few or short hesitations, whereas ref-erences to Y tend to contain many.
We can alsosee that the tendencies seem to be remarkably sim-ilar between corpora, but with relatively stable off-sets between them, PentoDescr having the longest,PTT/FTT the shortest silences.
We speculate thatthis is the result of the differing degrees of inter-activity (none in PentoDescr, restricted in WOz,less restricted in PTT, free in FTT) which puts dif-ferent pressures on speakers to avoid silences.
Tobalance our data with respect to this difference, weperformed some experiments with adjusted data33where silence lengths in PentoDescr were adjustedby 0.7 and in PTT/FTT by 1.3.
This brings the si-lence rates in the corpora, if plotted in the style ofFigure 3, almost in congruence.To test whether the differences in silence ratebetween utterances referring to different piecesare significant, we performed an ANOVA andfound a main effect of silence rate, F (11, 672) =6.2102, p < 8.714?10.
A post-hoc t-test revealsthat there are roughly two groups whose membersare not significantly different within-group, but areacross groups: I, L, U, W and X form one groupwith relatively low silence rate, F, N, P, T, V, Y, andZ another with relatively high silence rate.
We willsee in the next section whether our model pickedup on these differences.5 A Bayesian Filtering Model of IRRTo explore incremental reference resolution, andas part of a larger incremental dialogue system weare building, we implemented a probabilistic refer-ence resolver that works in the pentomino domain.At its base, the resolver has a Bayesian Filteringmodel (see e. g. (Thrun et al, 2005)) that with eachnew observation (word) computes a belief distri-bution over the available objects (the twelve puz-zle pieces); in a second step, a decision for a piece(or a collection of pieces in the n-best case) is de-rived from this distribution.
This model is incre-mental in a very natural and direct way: new inputincrements are simply treated as new observationsthat update the current belief state.
Note that thismodel does not start with any assumptions aboutsemantic word classes: whether an observed wordcarries information about what is being referred towill be learnt from data.5.1 The Belief-Update ModelWe use a Bayesian model which treats the in-tended referent as a latent variable generating asequence of observations (w1:n is the sequence ofwords w1, w2, .
.
.
, wn):P (r|w1:n) = ?
?
P (wn|r, w1:n?1) ?
P (r|w1:n?1)where?
P (wn|r, w1:n?1) is the likelihood of the newobservation (see below for how we approximatethat); and?
the prior P (r|w1:n?1) at step n is the posteriorof the previous step.
Before the first observation ismade (i. e., the first word is seen), the prior is sim-ply a distribution over the possible referents, P (r).F I L N P T U V W X Y Zintended referent:  Nnimm <sil?0> <sil?1> <sil?2> das teil <sil?0> <sil?1> <sil?2> <sil?3> das aus einer0.00.10.20.30.4Figure 4: Example of Belief Distribution after Ob-servationIn our experiment, we set this to a uniform distri-bution, but if there is prior information from othersources (e. g., because the dialogue state makescertain pieces more salient), this can be reflected.?
?
is a normalising constant, ensuring that the re-sult is indeed a probability distribution.The output of the model is a distribution of be-lief over the 12 available entities, as shown in Fig-ure 4.
Figure 5 shows in a 3D plot the devel-opment of the belief state (pieces from front toback, strength of belief as height of the peaks) overthe course of a whole utterance (with observationsfrom left to right).5.2 The Decision StepWe implemented several ways to derive a decisionfor a referent from such a distribution:i) In the arg max approach, at each state the ref-erent with the highest posterior probability is cho-sen. For Figure 4, that would be F (and hence,a wrong decision).
As Figure 5 shows (and theexample is quite representative for the model be-haviour), there often are various local maximaover the course of an utterance, and hence a modelthat takes as its decision always the maximum canbe expected to perform many edits.ii) In the adaptive threshold approach, we startwith a default decision for a special 13th class,?undecided?, and a new decision is only made ifthe maximal value at the current step is above acertain threshold, where this threshold is reset ev-ery time this condition is met.
In other words, thisdraws a plane into the belief space and only makesa new decision when a peak rises above this planeand hence above the previous peak.
In effect, thisapproach favours strong convictions and reduces34utterance #: 230 intended referent:  Nhast eine lange ule mit drei teilen <sil?0> <sil?1> und eine kurze mit zweiZ, Y, X, W, V, U, T, P, N, L, I, Fas.matrix(norm.vect[, 1:12])0.00.20.40.60.81.0Figure 5: Belief Update over Course of Utterancethe ?jitter?
in the decision making.In our example from Figure 4, this would meanthat the maximum, F, would only be the decisionif its value was higher than the threshold and therewas no previous guess that was even higher.iii) The final model implements a threshold n-best approach, where not just a single piece is se-lected but all pieces that are above a certain thresh-old.
Assuming that the threshold is 0.1 for exam-ple this would select F, I, N, Y, and Z?and hencewould include the correct reference in Figure 4.5.3 ImplementationTo learn and query the observation likelihoodsP (wn|r, w1:n?1), we used referent-specific lan-guage models.
More precisely, we computed thelikelihood as P (r, w1:n)/P (r, w1:n?1) (definitionconditional probability), and approximated thejoint probabilities of referent and word sequencevia n-grams with specialised words.
E. g., an ut-terance like ?take the long, narrow piece?
refer-ring to piece I (or tested for reference to this piece)would be rewritten as ?take I the I long I narrow Ipiece I?
and presented to the n-gram learner / in-ference component.
(Both taken from the SRI LMpackage, (Stolcke, 2002).
)During evaluation of the models, the test utter-ances are fed word-by-word to the model and thedecision is evaluated against the known intendedreferent.
Since we were interested in testingwhether disfluencies contained information thatwould be learned, for one variant of the systemwe also fed pseudo-words for silences and hesi-tation markers like uhm, numbered by their posi-tion (i. e., ?take the ..?
becomes ?take the sil-1 sil-2?
), to both learning and inference for the silence-sensitive variant; the silence-ignorant variant sim-ply repeats the previous decision at such pointsand does not update its belief state; this way, itis guaranteed that both variants generate the samenumber of decisions and can be compared directly.(Cf.
the dashes in the ?no-sil-model?
in Figure 1above: those are points where no real computationis made in the no-sil case.
)5.4 ExperimentsAll experiments were performed with 10-foldcross-validation.
We always ran both versions, theone that showed silences to the model and the onethat didn?t.
We tested various combinations of lan-guage model parameters and deciders, of whichthe best-performing ones are discussed in the nextsection.5.5 ResultsTable 1 shows the results for the different deci-sion methods and for models where silences areincluded as observations and where they aren?t,and, as a baseline, the result for a resolver thatmakes a random decision after each observation.As we can see, the different decision methodshave different characteristics wrt.
individual mea-sures.
The threshold n-best approach performsbest across the board?but of course has a slightlyeasier job since it does not need to make unam-biguous decisions.
We will look into the develop-ment of the n-best lists in a second, but for nowwe note that this model is for almost all utterancescorrect at least once (97% fc applicable) and ifso, typically very early (after 30% of the utter-ance).
In over half of the cases (54.68%), the fi-nal decision is correct (i. e. is an n-best list thatcontains the correct referent), and similarly for agood third of all silence observations.
Interest-ingly, silence-correctness is decidedly higher forthe silence model (which does actually make newdecisions during silences and hence based on theinformation that the speaker is hesitating) than forthe non-sil model (which at these places only re-peats the previously made decision).
The modelperforms significantly bettern than a baseline thatrandomly selects n-best lists of the same size (seernd-nb in Table 1).As can be expected, the adaptive threshold ap-proach is more stable with its decisions, as wit-nessed by the low edit overhead.
The fact that itchanges its decision not as often has an impact onthe other measures, though: in more cases, themodel is correct not even once (fc applicable is35n-best rnd-nb adapt max randomMeasure / Model w/ h w/o h w/ h w/ h w/o h w/ h w/o h w/ hfc applicable 97.22% 95.03% 85.38% 63.15% 66.67% 86.55% 82.89% 59.94%average fc 30.43% 33.73% 29.61% 53.87% 55.25% 46.55% 49.31% 42.60%ff applicable 54.68% 54.24% 17.54% 48.68% 53.07% 39.77% 40.64% 9.65%average ff 87.74% 85.01% 97.08% 71.24% 70.89% 96.08% 94.28% 98.44%edit overhead 93.49% 90.65% 96.65% 69.61% 67.66% 92.57% 89.44% 93.16%correctness 37.81% 36.81% 23.37% 23.01% 26.61% 17.83% 20.23% 7.83%sil-correctness 36.60% 31.09% 26.39% 18.71% 22.58% 13.67% 19.34% 8.63%adjusted error 60.07% 56.96% 76.63% 76.29% 70.90% 82.17% 79.42% 92.16%Table 1: Results for different decision methods (n-best, adaptive, max arg and random) and for modelswith and without silence-observations (w/ h and w/o h, respectively)lower than for the other two models).
But it isstill correct with almost half of its final decisions,and these come even earlier than for the n-bestmodel.
Silence information does not seem to helpthis model; this suggests that the information pro-vided by knowledge about the fact that the speakerhesitates is too subtle to push through the thresh-old in order to change decisions.The arg max approach fares worst.
Since nei-ther the relative strength of the strongest belief (ascompared to that in the competing pieces) nor theglobal strength (have I been more convinced be-fore?)
is taken into account, the model changesits mind too often, as evidenced by the edit over-head, and does not settle on the correct referent of-ten (and if, then late).
Again, silence informationdoes not seem to be helpful for this model.As a more detailed look at what happens dur-ing silence sequences, Figure 6 plots the averagechange in probability from onset of silence to apoint at 1333ms of silence.
(Recall that the un-derlying Bayesian model is the same for all mod-els evaluated above, they differ only in how theyderive a decision.)
We can see that the gains andlosses are roughly as expected from the analysis ofthe corpora: pieces like L and P become more ex-pected after a silence of that length, pieces like Xless.
So the model does indeed seem to learn thathesitations systematically occur together with cer-tain pieces.
(The reader can convince herself withthe help of Figure 2 that these shapes are indeedcomparatively hard-to-describe; but the interestingpoint here is that this categorisation does not haveto be brought to the model but rather is discoveredby it.
)Finally, a look at the distribution and the sizes ofthe n-best groupings: the most frequent decision isF I L N P T U V W X Y Z?0.050.000.05Figure 6: Average change in probability from on-set of silence to 1333ms into silence?undecided?
(474 times), followed by the group-ings F N, N Y, and N Y P (343, 342 and 196, re-spectively).
Here again we find groupings that re-flect the differences w.r.t.
hesitation rate.
The av-erage size of the n-best lists is 2.58 (sd = 1.4).6 Conclusions and Further WorkWe discussed the task of incremental referenceresolution (IRR), in particular with respect to ex-ophoric reference.
From a theoretical perspective,it might seem easy to specify what the ideal be-haviour of an IRR component should be, namelyto always produce the set of entities (the exten-sion) that is compatible with the part of the ex-pression seen so far.
In practice, however, this isdifficult to annotate, for both practical reasons aswell as theoretical (referring is a pragmatic activ-ity that is not reducible to denotation).
The met-rics we defined for evaluation of IRR componentsaccount for this in that they do not require a gold36standard annotation that fixes the dynamics of theresolution process; they simply make it possibleto quantify the assumption that ?early and withstrong convictions?
is best.We then presented our probabilistic model ofIRR that works directly on word observationswithout any further processing (POS tagging,parsing).
It achieves a reasonable success (as mea-sured with our metrics); for example, in over halfof the cases, the final guess of the model is correct,and comes before the utterance is over.
As an ad-ditional interesting feature, the model is able to in-terpret hesitations (silences lifted to pseudo-wordstatus) in a way shown before only in controlledpsycholinguistic experiments, namely as makingreference to hard-to-describe pieces more likely.3In future work, we want to explore the model?sperformance on ASR output.
It is not clear apriori that this would degrade performance much,as it can be expected that the learning componentsare quite robust against noise.
Connected tothis, we want to explore more complex statis-tical models, e. g. a hierarchical model whereone level generates parts of the utterance (e. g.non-referential parts and referential parts) and thesecond the actual words.
We also want to test howthis approach scales up to worlds with a largernumber of possible referents, where consequentlyapproximation methods like particle filtering haveto be used.
Finally, we will test how the modulecontributes to a working dialogue system, wherefurther decisions (e. g. for clarification requests)can be built on its output.Acknowledgments This work was funded bya grant from DFG in the Emmy Noether Pro-gramme.
We would like to thank the anonymousreviewers for their detailed comments.ReferencesG.S.
Aist, J. Allen, E. Campana, L. Galescu, C.A.Gomez Gallo, S. Stoness, M. Swift, and M Tanenhaus.2006.
Software architectures for incremental understand-ing of human speech.
In Proceedings of the Interna-tional Conference on Spoken Language Processing (IC-SLP), Pittsburgh, PA, USA, September.Jennifer E. Arnold, Carla L. Hudson Kam, and Michael K.Tanenhaus.
2007.
If you say thee uh you are describ-ing something hard: The on-line attribution of disfluency3It is interesting to speculate whether this could have im-plications for generation of referring expressions as well.
Itmight be a good strategy to make your planning problemsobservable or even to fake planning problems that are under-standable to humans.during reference comprehension.
Journal of ExperimentalPsychology.Karl Bailey and F. Ferreira.
2007.
The processing of filledpause disfluencies in the visual world.
In R. P. G. vonGompel, M H. Fischer, W. S. Murray, and R. L. Hill,editors, Eye Movements: A Window on Mind and Brain,chapter 22.
Elsevier.Timo Baumann, Michaela Atterer, and David Schlangen.2009.
Assessing and Improving the Performance ofSpeech Recognition for Incremental Systems.
In Proceed-ings of NAACL-HLT 2009, Boulder, USA.Susan E. Brennan and Michael F. Schober.
2001.
How lis-teners compensate for disfluencies in spontaneous speech.Journal of Memory and Language, 44:274?296.Herbert H. Clark and Edward F. Schaefer.
1987.
Collabo-rating on contributions to conversations.
Language andCognitive Processes, 2(1):19?41.Robert Dale and Ehud Reiter.
1995.
Computational interpre-tations of the gricean maxims in the generation of referringexpressions.
Cognitive Science, 19:233?263.Raquel Ferna?ndez, David Schlangen, and Tatjana Lucht.2007.
Push-to-talk ain?t always bad!
comparing differ-ent interactivity settings in task-oriented dialogue.
In Pro-ceeding of DECALOG (SemDial?07), Trento, Italy, June.Carlos Go?mez Gallo, Gregory Aist, James Allen, Williamde Beaumont, Sergio Coria, Whitney Gegg-Harrison,Joana Paulo Pardal, and Mary Swift.
2007.
Annotatingcontinuous understanding in a multimodal dialogue cor-pus.
In Proceeding of DECALOG (SemDial07), Trento,Italy, June.Florian Schiel.
2004.
Maus goes iterative.
In Proc.
of theIV.
International Conference on Language Resources andEvaluation, Lisbon, Portugal.Alexander Siebert and David Schlangen.
2008.
A simplemethod for resolution of definite reference in a shared vi-sual context.
In Procs of SIGdial, Columbus, Ohio.Gabriel Skantze and David Schlangen.
2009.
Incrementaldialogue processing in a micro-domain.
In Proceedings ofEACL 2009, Athens, Greece, April.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings Intl.
Conf.
Spoken Lan-guage Processing (ICSLP?02), Denver, Colorado, USA,September.Scott C. Stoness, Joel Tetreault, and James Allen.
2004.
In-cremental parsing with reference interaction.
In Proceed-ings of the Workshop on Incremental Parsing at the ACL2004, pages 18?25, Barcelona, Spain, July.Michael K. Tanenhaus, Michael J. Spivey-Knowlton, Kath-llen M. Eberhard, and Julie C. Sedivy.
1995.
Intergrationof visual and linguistic information in spoken languagecomprehension.
Science, 268.Sebastian Thrun, Wolfram Burgard, and Dieter Fox.
2005.Probabilistic Robotics.
MIT Press, Cambridge, Mas-sachusetts, USA.37
