Web-Based List Question AnsweringHui Yang, Tat-Seng ChuaSchool of ComputingNational University of Singapore3 Science Drive 2, 117543, Singaporeyangh@lycos.co.uk,chuats@comp.nus.edu.sgAbstractWhile research on question answering has be-come popular in recent years, the problem of ef-ficiently locating a complete set of distinctanswers to list questions in huge corpora or theWeb is still far from being solved.
This paper ex-ploits the wealth of freely available text and linkstructures on the Web to seek complete answersto list questions.
We introduce our system,FADA, which relies on question parsing, webpage classification/clustering, and content extrac-tion to find reliable distinct answers with high re-call.1 IntroductionThe Text REtrieval Conference Series (TREC) hasgreatly encouraged Question Answering (QA) re-search in the recent years.
The QA main task in therecent TREC-12 involved retrieving short conciseanswers to factoid and list questions, and answernuggets for definition questions (Voorhees, 2003).The list task in TREC-12 required systems to as-semble a set of distinct and complete exact answersas responses to questions like ?What are the brandnames of Belgian chocolates??.
Unlike the questionsin previous TREC conferences, TREC-12 list ques-tions did not specify a target number of instances toreturn but expected all answers contained in the cor-pus.
Current QA systems (Harabagiu et al, 2003;Katz et al, 2003) usually extract a ranked list of fac-toid answers from the top returned documents byretrieval engines.
This is actually the traditional wayto find factoid answers.
The only difference betweenanswering list questions and factoid questions here isthat list QA systems allow for multiple answers,whose scores are above a cut-off threshold.An analysis of the results of TREC-12 list QAsystems (Voorhees, 2003) reveals that many of themseverely suffer from two general problems: low re-call and non-distinctive answers.
The median aver-age F1 performance of list runs was only 21.3%while the best performer could only achieve 39.6%(Table 1).
This unsatisfactory performance exposesthe limitation of using only traditional InformationRetrieval and Natural Language Processing tech-niques to find an exhaustive set of factoid answers ascompared to only one.TREC-12 Run Tag Avg F1LCCmainS03 0.396nusmml03r2 0.319MITCSAIL03c 0.134isi03a 0.118BBN2003B 0.097Average 0.213Table 1: TREC-12 Top 5 Performers (Voorhees, 2003)In contrast to the traditional techniques, the Webis used extensively in systems to rally round factoidquestions.
QA researchers have explored a variety ofuses of the Web, ranging from surface pattern min-ing (Ravichandran et al, 2002), query formulation(Yang et al, 2003), answer validation (Magnini etal., 2002), to directly finding answers on the Web bydata redundancy analysis (Brill et al, 2001).
Thesesystems demonstrated that with the help of the Webthey could generally boost baseline performance by25%-30% (Lin 2002).The well-known redundancy-based approach iden-tifies the factoid answer as an N-gram appearingmost frequently on the Web (Brill et al 2001).
Thisidea works well on factoid questions because factoidquestions require only one instance and web docu-ments contains a large number of repeated informa-tion about possible answers.
However, when dealingwith list questions, we need to find all distinct in-stances and hence we cannot ignore the less frequentanswer candidates.
The redundancy-based approachfails to spot novel or unexpectedly valuable informa-tion in lower ranked web pages with few occur-rences.In this paper, we propose a novel framework toemploy the Web to support list question answering.Based on the observations that multiple answer in-stances often appear in the list or table of a singleweb page while multiple web pages may also con-tain information about the same instance, we differ-entiate these two types of web pages.
For the firstcategory, which we call Collection Page (CP), weneed to extract table/list content from the web page.For the second category, which we call Topic Page(TP), we need to find distinct web pages relating todifferent answer instances.
We will demonstrate thatthe resulting system, FADA (Find All Distinct An-swers), could achieve effective list question answer-ing in the TREC corpus.Figure 1: Examples of Collection Page (top)and Topic Page (bottom)The remainder of this paper is organized as fol-lowing.
Section 2 gives the design considerations ofour approach.
Section 3 details our question analysisand web query formulation.
Section 4 describes theweb page classification and web document featuresused in FADA.
Section 5 shows the algorithm oftopic page clustering while Section 6 details the an-swer extraction process.
Section 7 discusses experi-mental results.
Section 8 concludes the paper.2 Design ConsiderationsOur goal is to find as many distinct exact answers onthe Web as possible.
This requires us to:?
perform effective and exhaustive search; and?
extract distinct answers.In order to perform effective search, we employquestion transformation to get effectual web queries.However, this is not a trivial task.
If the query is toogeneral, too many documents may be retrieved andthe system would not have sufficient resources toscan through all of them.
If the query is too specific,no pages may be retrieved.Given millions of web pages returned by searchengines, our strategy is to divide-and-conquer byfirst identify Collection Pages (CP) that contain a listof answer instances.
For example, for the question?What breeds of dog have won the "Best in Show"award at the Westminster Dog Show?
?, we can finda Collection Page as shown in Figure 1 (top).
Such aweb page is a very good resource of answers.
Ingeneral, we observe that there is a large number ofnamed entities of the type desired appearing in aCollection Page, typically in a list or table.
Our in-tuition is that if we can find a Collection Page thatcontains almost all the answers, then the rest of thework is simply to extract answers from it or relatedweb pages by wrapper rule induction.Another kind of ?good?
web page is a Topic Page,that contains just one answer instance (Figure 1, bot-tom).
It typically contains many named entities,which correspond to our original query terms andsome other named entities of the answer target type.Given the huge amount of web data, there will bemany Topic Pages that refer to the same answer in-stance.
There is hence a need to group the pages andto identify a pertinent and distinctive page in orderto represent a distinct answer.Table 2: Web Page ClassesThe rest of the top returned web pages could beeither relevant or irrelevant to the question.
In sum-mary, we need to classify web pages into fourclasses: Collection Page, Topic Page, Relevant Page,and Irrelevant Page (Table 2), based on their func-tionality and contribution in finding list answers.Based on the above considerations, we propose ageneral framework to find list answers on the Webusing the following steps:a) Retrieve a good set of web documents.b) Identify Collection Pages and distinct TopicPages as main resources of answers.c) Perform clustering on other web pages based ontheir similarities to distinct Topic Pages to formclusters that correspond to distinct answer in-stances.d) Extract answers from Collection Pages and TopicPage clusters.3 Question Transformation and Web PageRetrievalAgichtein et al (2001) presented a technique onlearning search engine specific query transforma-tions for question answering.
A set of transformationrules are learned from a training corpus and appliedto the questions at the search time.
Related workcould also be found in Kwok et al (2001) where theuser?s question is processed by a parser to learn itssyntactic structure and various query modulationtechniques are applied to the initial questions to gethigh quality results for later answer extraction.FADA performs question parsing to identify keyquestion words and the expected answer type.
It ex-tracts several sets of words from the original ques-tion and identifies the detailed question classes.
ItWeb page class DescriptionCollection Page Containing a list of  answersTopic Page The best page to represent an answerinstanceRelevant Page Relevant to an  answer instance byproviding either support or objection tothe Topic PageIrrelevant Page Not related to any answer instancethen formulates a number of queries by combiningthe known facets together with heuristic patterns forlist questions.We perform both shallow and full parsing on aquestion followed by Named Entity Recognition(NER) to get the known query facets and their types.The shallow parser we used is the free online mem-ory-based chunker1 and the full parser is MINIPAR2.Both parsers are very efficient and usually parse 300words within a second.
The procedure of query pars-ing is as follows:a) Remove head words.
The head words in a ques-tion could be wh-question words and leadingverbs.
The list of head words includes ?who, what,when, where, which, how, how much, how many,list, name, give, provide, tell?, etc.
Removingthem enables us to get the correct subject/objectrelation and verb in the question.
For example, forquestion ?What breeds of dog have won the ?Bestin Show?
award at the Westminster Dog Show?
?,after removing the head word, the question be-comes ?breeds of dog have won the ?Best inShow?
award at the Westminster Dog Show?.b) Detect subject and object for the remaining ques-tion segments by shallow parsing.
For example,after parsing the above question, we get:[NP1Subject breeds//NNS NP1Subject] {PNP [Pof/IN P] [NP dog/NN NP] PNP} [VP1have/VBP won/VBN VP1] [NP1Object the/DT``/`` Best/JJS NP1Object] {PNP [P in/INP] [NP Show/NNP ''/'' award/NN NP]PNP} {PNP [P at/IN P] [NP the/DTWestminster/NNP Dog//NNP Show/NNP NP]PNP}From the parsed sentence, we want to get the logi-cal subject as the sentence subject or its immedi-ate modifiers.
Here we have the logical subject-?breeds of dog?, verb-?won?, and logical object-?the best in show award?.
If the resulting logicalsubject/object is the term ?that?
as in the follow-ing parsed query for ?U.S.
entertainers that laterbecame politicians?
:[NP U.S./NNP entertainers//NNS NP][NP1Subject that/WDT NP1Subject] [ADVPlater/RB ADVP] [VP1 became/VBD VP1][NP1Object politicians//NNS NP1Object]we get the noun or noun phrase before the clauseas the logical subject/object.
Hence, we have thelogical subject-?entertainers?, action-?became?,and logical object-?politician?.c) Extract all the noun phrases as potential descrip-tions from the remaining question segments,which are usually prepositional phrases or clauses.For the ?dog breeds?
example, we get the descrip-tions-?Westminster Dog Show?.1 http://ilk.kub.nl/cgi-bin/tstchunk/demo.pl2 http://www.cs.ualberta.ca/~lindek/minipar.htmd) Apply named entity recognition to the resultingdescription phrases by using NEParser, a fine-grained named entity recognizer used in ourTREC-12 system (Yang et al, 2003).
It assignstags like ?person?, ?location?, ?time?, ?date?,?number?.
For the ?dog breed?
example, ?West-minster?
gets the location tag.After the above analysis, we obtain all the knownfacets provided in the original question.
We thenmake use of this knowledge to form web queries toget the right set of pages.
This is a crucial task indealing with the Web.
One of the query transforma-tion rules is given as follows:(list|directoty|category|top|favorite)?
(:|of)?
<subj> <action>?
<object>?<description1>?
<description2>?
?<descriptionN>?The rule starts the query optionally with leadingwords (list, directory, category), optionally followedby a colon or ?of?, followed by subject phrase(<subj>), optionally followed by action (<action>),optionally followed by object (<object>) and de-scription phrases (<description1>?<descriptionN>).In the above pattern, ???
denotes optional, ???
omit,and ?|?
alternative.
For example, for the ?dog breed?question, we form queries ?breed of dog won best inshow Westminster Dog Show?, ?directory breed ofdog best in show Westminster Dog Show?, and ?listbreed of dog won best in show?
etc.Transforming the initial natural language ques-tions into a good query can dramatically improve thechances of finding good answers.
FADA submitsthese queries to well-known search engines (Google,AltaVista, Yahoo) to get the top 1,000 Web pagesper search engine per query.
Here we attempt to re-trieve a large number of web pages to serve our goal- find All Distinct answers.
Usually, there are a largenumber of web pages which are redundant as theycome from the same URL addresses.
We remove theredundant web pages using the URL addresses as theguide.
We also filter out files whose formats are nei-ther HTML nor plain text and those whose lengthsare too short or too long.
Hence the size of the re-sulting document set for each question varies from afew thousands to ten of thousands.4 Web Page ClassificationIn order to group the web pages returned by thesearch engines into the four categories discussedearlier, it is crucial to find a good set of features torepresent the web pages.
Many techniques such astd.idf (Salton and Buckley, 1988) and a stop wordlist have been proposed to extract lexical features tohelp document clustering.
However, they do notwork well for question answering.
As pointed out byYe et al (2003) in their discussion on the per-son/organization finding task, given two resumepages about different persons, it is highly possiblethat they are grouped into one cluster because theyshare many similar words and phrases.
On the otherhand, it is difficult to group together a news pageand a resume page about the same target entity, dueto the diversity in subject matter, word choice, liter-ary styles and document format.
To overcome thisproblem, they used mostly named entity and linkinformation as the basis for clustering.
Compared totheir task, our task of finding good web documentscontaining answers is much more complex.
The fea-tures are more heterogeneous, and it is more difficultto choose those that reflect the essential characteris-tics of list answers.In our approach, we obtain the query wordsthrough subject/object detection and named entityrecognition.
We found that there are a large numberof named entities of the same type appearing in aCollection Page, typically within a list or table.
Andin a Topic Page, there is also typically a group ofnamed entities, which could correspond to our origi-nal query terms or answer target type.
Therefore,named entities play important roles in semantic ex-pression and should be used to reflect the content ofthe pages.The Web track in past TREC conferences showsthat URL, HTML structure, anchor text, hyperlinks,and document length tend to contain important heu-ristic clues for web clustering and information re-trieval (Craswell and Hawking, 2002).
We havefound that a Topic Page is highly likely to repeat thesubject in its URL, title, or at the beginning of itspage.
In general, if the subject appears in importantlocations, such as in HTML tags <title>, <H1> and<H2>, or appears frequently, then the correspondingpages should be Topic Pages and their topic is aboutthe answer target.Followed the above discussion, we design a set of 29features based on Known Named Entity Type, An-swer Named Entity Type, ordinary Named Entities,list, table, URL, HTML structure, Anchor, Hyper-links, and document length to rep resent the webpages.
Table 3 lists the features used in our system.In the table and subsequent sections, NE refers toNamed Entity.We trained two classifiers: the Collection Pageclassifier and the Topic Page classifier.
The formerclassifies web pages into Collection Pages and non-collection pages while the later further classifies thenon-collection pages into Topic Pages and Others.Both Classifiers are implemented using DecisionTree C4.5 (Quinlan 1993).
We used 50 list questionsfrom TREC-10 and TREC-11 for training andTREC-12 list questions for testing.
We parse thequestions, formulate web queries and collect webpages by using the algorithm described in Section 2.Table 3: Web Page FeaturesEach sample is represented using the featureslisted in Table 3.
Some of the decision rules are asfollows:a) OUT_Link >= 25 & NE > 78 &b) Answer_NE >= 30 -> Class CP OUT_Link<= 25 & Answer_NE <= 5 & NE > 46 ->Class TPc) OUT_Link >= 25 & URL_Depth > 3 ->Othersd) NE <= 4  -> OthersRule a) implies that good Collection Pages shouldhave many outlinks, NEs and especially answer NEs.Rule b) implies that good Topic Pages should havemany NEs but relatively few links and answer NEs.Rule c) show that Others have deeper URL depth;while Rule d) shows that they have fewer NEs.Feature Meaning1 |PER| # of Person NEs2 |ORG| # of Organization NEs3 |LOC| # of Location NEs4 |TME| # of Time NEs, including date, year, month5 |NUM| # of Numer NEs6 |COD| # of Code NEs, including phone number,zip code, etc7 |OBJ| # of Object NEs, including animal, planet,book,  etc8 |NE| Total number of the above NEs9 |Known_NE| Total # of NEs within the same NE type asin the question.
In the ?dog breed?
example,it is the number of Location NEs since?Westminster?
is identified as Location byNER.10 |Unknown_NE|# of NEs belonging to other NE type.
In the?dog breed?
example, it is the total numberof Time and Breed NEs11 |Answer_NE| # of NEs belonging to expected answertype.
In the ?dog breed?
example, it is thenumber of Breed NEs12 |Known_NE|/ |NE|Ratio of  | Known _NE| to |NE|13 |Unknown_NE| / |NE|Ratio of  | Unknown _NE| to |NE|14 |Answer_NE|/ |NE|Ratio of  |Answer_NE| to |NE|15 Length # of tokens in a page16 Content_Length# of words in a page excluding HTMLtags17 |NE|/Length Ratio of |NE| to |Token|18 |NE|/Content_LengthRatio of |NE| to |Word|19 |In_Link| # of in-links20 |Out_Link| # of out-links21 |All_Link| The sum of in-links and out-links22 Keyword_in_TitleBoolean indicating presence of keywords inpage title23 Keyword_in_URLBoolean indicating presence of keywords inURL24 Keyword_in_PageBoolean indicating presence of keywords inthe page25 |Answer_NE_in_Title|# of NEs belonging to expected answer typepresenting in page title26 |Answer_NE_in_URL|# of NEs belonging to expected answer typepresenting in URL27 |<li>| # of HTML tags representing a list or table,including <li>, <ol>, <ul>, <br>,<td>28 |<li><ahref=|# of HTML tags, including <li>, <ol>,<ul>, <br>,<td> to represent a list/table ofanchors,29 URL_Depth The depth of URLWeb page classification enables us to get Collec-tion Pages, Topic Pages and the rest of the pages.Our experiments on TREC-12 list questions showedthat we can achieve a classification precision of91.1% and 92% for Collection Pages and TopicPages respectively.5 Finding Answer SourcesBased on Web page classification, we form the ini-tial sets of Collection Pages CPSet, Topic PagesTPSet and OtherSet.
In order to boost the recall, wefirst use the outgoing links of Collection Pages tofind more Topic Pages.
These outgoing pages arepotential Topic Pages but not necessarily appearingamong the top returned web documents.
Our subse-quent tests reveal that the new Topic Pages intro-duced by links from Collection Pages greatlyincrease the overall answer recall by 23%.
The newTopic Page set becomes:TPSet?
= TPSet + {outgoing pages of CPs}Second, we select distinct Topic Pages.
We com-pare the page similarity between each pair of TopicPages using the algorithm below.for each pair {tpi, tpj} in TPSet?if (sim(tpi,tpj)> ?
)if ?ANE_in_tpi > ?ANE_in_tpjmove tpj into OtherSet;Here the page similarity function sim() is a linearcombination of overlaps between Known_NE, An-swer_NE, URL similarity and link similarity.
?
ispreset at 0.75 and may be overridden by the user.
?ANE_in_tpi is the number of named entities ofanswer type in Topic Page tpi.
For those pairs withhigh similarity, we keep the page that contains morenamed entities of answer type in TPSet?
and movethe other into OtherSet.
The resulting Topic Pagesin TPSet?
are distinct and will be used as clusterseeds for the next step.Third, we identify and dispatch Relevant Pagesfrom OtherSet into appropriate clusters based ontheir similarities with the cluster seeds.for each rpi in OtherSet {k = argmax {sim(rpi , tpk) }if (sim(rpi , tpk ) >  ?
)insert rpi into clusterk;elseinsert rpi into IrrelevantSet; }Here ?
is preset at 0.55, and sim() is defined asabove.
Each cluster corresponds to a distinct answerinstance.
The Topic Page provides the main factsabout that answer instance while Relevant Pagesprovide supporting materials for the unique answerinstance.
The average ratio of correct clustering is54.1% in our experiments.Through web page clustering, we avoid early an-swer redundancy, and have a higher chance to find-ing distinct answers on the noisy Web.6 Answer Extraction6.1 HTML Source Page CleaningMany HTML web pages contain common HTMLmistakes, including missing or unmatched tags, endtags in the wrong order, missing quotes round attrib-utes, missed / in end tags, and missing > closing tags,etc.
We use HtmlTidy3 to clean up the web pagesbefore classification and clustering.
FADA also usesan efficient technique to remove advertisements.
Weperiodically update the list from Accs-Net4, a sitethat specializes in creating such blacklists of adver-tisers.
If a link address matches an entry in a black-list, the HTML portion that contained the link isremoved.6.2 Answer Extraction from CPCollection Pages are very good answer resources forlist QA.
However, to extract the ?exact?
answersfrom the resource page, we need to perform wrapperrule induction to extract the useful content.
There isa large body of related work in content extraction,which enables us to process only extracted contentrather than cluttered data coming directly from theweb.
Gupta et al (2003) parsed HTML documentsto a Document Object Model tree and to extract themain content of a web page by removing the linklists and empty tables.
In contrast, our link list ex-tractor finds all link lists, which are table cells orlists for which the ratio of the number of links to thenumber of non-linked words is greater than a spe-cific ratio.
We have written separate extractors foreach answer target type.
The answers obtained inCollection Pages are then ?projected?
onto theTREC AQUAINT corpus to get the TREC answers(Brill et al, 2001).6.3 Answer Extraction from TP ClusterHaving web pages clustered for a certain question,especially when the clusters nicely match distinctanswer, facilitates the task of extracting the possibleanswers based on the answer target type.
We per-form this by first analyzing the main Topic Pages ineach cluster.
In case we find multiple passages con-taining different answer candidates in the sameTopic Page, we select the answer candidate from thepassage that has the most variety of NE types sinceit is likely to be a comprehensive description aboutdifferent facets of a question topic.
The answerfound in the Topic Page is then ?projected?
onto theQA corpus to get the TREC answers as with the Col-lection Page.
In case no TREC answers can be found3 http://htmltrim.sourceforge.net/tidy.html4 http://www.accs-net.com/hosts/get_hosts.htmlbased on the Topic Page, we go to the next mostrelevant page in the same cluster to search for theanswer.
The process is repeated until either an an-swer from the cluster is found in the TREC corpusor when all Relevant Pages in the cluster have beenexhausted.For the question ?Which countries did the firstlady Hillary Clinton visit?
?, we extracted the Loca-tions after performing Named Entity analysis oneach cluster and get 38 country names as answers.The recall is much higher than the best performingsystem (Harabagiu et al, 2003) in TREC-12 whichfound 26 out of 44 answers.7 Evaluation on TREC-12 Question SetWe used the 37 TREC-12 list questions to test theoverall performance of our system and compare theanswers we found in the TREC AQUAINT corpus(after answer projection (Brill et al 2001)) with theanswers provided by NIST.7.1 Tests of Web Page ClassificationIn Section 3, the web pages are classified into threeclasses: Collection Pages, Topic Pages, and Others.Table 4 shows the system performance of the classi-fication.
We then perform a redistribution of classi-fied pages, where the outgoing pages from CPs go toTP collection, and the Relevant Pages are grouped assupportive materials into clusters, which are basedon distinct Topic Page.
Nevertheless, the perform-ance of web page classification will influence thelater clustering and answer finding task.
Table 4shows that we could achieve an overall classificationaverage precision of 0.897 and average recall of0.851.
This performance is adequate to support thesubsequent steps of finding complete answers.Table 4: Performance of Web Page Classification7.2 Performance and Effects of Web PageClusteringRelevant Pages are put into clusters to provide sup-portive material for a certain answer instance.
Theperformance of Relevant Page dispatch/clustering is54.1%.
We also test different clustering thresholdsfor our web page clustering as defined in Section 5.We use the F1 measure of the TREC-12 list QA re-sults as the basis to compare the performance of dif-ferent clustering threshold combinations as shown inxx.
We obtain the best performance of F1 = 0.464when ?=0.55 and ?=0.75.?
(0.55) ?
(0.65) ?
(0.75) ?=0.85?=0.25 0.130 0.234 0.
324 0.236?=0.35 0.136 0.244 0.
338 0.232?=0.45 0.148 0.332 0.
428 0.146?=0.55 0.166 0.408 0.
464 0.244?=0.65 0.200 0.322 0.
432 0.236Table 5: Clustering Threshold Effects7.3 Overall PerformanceTable 6 compares a baseline list question answeringsystem with FADA.
The baseline is based on a sys-tem which we used in participation in the TREC-12QA task (Yang et al, 2003).
It extends the tradi-tional IR/NLP approach for factoid QA to performlist QA, as is done in most other TREC-12 systems.It achieves an average F1 of 0.319, and is ranked 2ndin the list QA task.We test two variants of FADA ?
one without in-troducing the outgoing pages from CPs as potentialTPs (FADA1), and one with (FADA2).
The twovariants are used to evaluate the effects of CPs in thelist QA task.
The results of these two variants ofFADA on the TREC-12 list task are presented inTable 6.Table 6: Performance on TREC-12 Test SetWithout the benefit of the outgoing pages fromCPs to find potential answers, FADA1 could boostthe average recall by 30% and average F1 by 16.6%as compared to the baseline.
The great improvementin recall is rather encouraging because it is crucialfor a list QA system to find a complete set of an-swers, which is how list QA differ from factoid QA.By taking advantage of the outgoing pages fromCPs, FADA2 further improves performance to anaverage recall of 0.422 and average F1 of 0.464.
Itoutperforms the best TREC-12 QA system (Voorhees,2003) by 19.6% in average F1 score.From Table 6, we found that the outgoing pagesfrom the Collection Pages (or resource pages) con-tribute much to answer finding task.
It gives rise toan improvement in recall of 22.7% as compared tothe variant of FADA1 that does not take advantageof outgoing pages.
We think this is mainly due to thecharacteristics of the TREC-12 questions.
Mostquestions ask for well-known things, and famousevents, people, and organization.
For this kind ofquestions, we can easily find a Collection Page thatcontains tabulated answers since there are web sitesthat host and maintain such information.
For in-stance, ?Westminster Dog Show?
has an officialAvg P Avg R Avg F1Baseline 0.568 0.264 0.319FADA1 (w/o outgoing pages) 0.406 0.344 0.372FADA2 (w/ outgoing pages) 0.516 0.422 0.464TREC-12 best run - - 0.396Page Class Avg Prec.
Avg Rec.Collection 91.1% 89.5%Topic 92.0% 88.4%Relevant 86.5% 83.4%Overall 89.7% 85.1%web site5.
However, for those questions that lackCollection Pages, such as ?Which countries did thefirst lady Hillary Clinton visit?
?, we still need to relymore on Topic Pages and Relevant Pages.With the emphasis on answer completeness anduniqueness, FADA uses a large set of documentsobtained from the Web to find answers.
As com-pared to the baseline system, this results in a drop inaverage answer precision although both recall and F1are significantly improved.
This is due to the factthat we seek most answers from the noisy Web di-rectly, whereas in the baseline system, the Web ismerely used to form new queries and the answers arefound from the TREC AQUAINT corpus.
We arestill working to find a good balance between preci-sion and recall.The idea behind FADA system is simple: SinceWeb knowledge helps in answering factoid ques-tions, why not list questions?
Our approach inFADA demonstrates that this is possible.
We believethat list QA should benefit even more than factoidQA from using Web knowledge.8 ConclusionWe have presented the techniques used in FADA,a system which aims to find complete and distinctanswers on the Web using question parsing, webpage classification/clustering and content extraction.By using the novel approach, we can achieve a recallof 0.422 and F1 of 0.464, which is significantly bet-ter than the top performing systems in the TREC-12List QA task.
The method has been found to be ef-fective.
Our future work includes discovering an-swers on non-text web information, such as images.Much text information is stored as images on theweb, and hence, cannot be accessed by our approach,and some do contain valuable information.ReferencesE.
Agichtein, S. Lawrence, and L. Gravano.
2001.
"Learning search engine specific query transforma-tions for question answering.?
In the Proceedings ofthe 10th ACM World Wide Web Conference (WWW2001).E.
Brill, J. Lin, M. Banko, S. Dumais, and A. Ng.
2001.?Data-intensive question answering?.
In the Pro-ceedings of the 10th  Text REtrieval Conference(TREC 2001).N.
Craswell, D Hawking.
2002.
?Overview of theTREC-2002 Web Track?, In the Proceedings of the11th Text REtrieval Conference.
(TREC 2002).S.
Gupta, G. Kaiser, D. Neistadt, P. Grimm, 2003.?DOM-based Content Extraction of HTML Docu-ments?, In the Proceedings of the 12th ACM WorldWide Web conference.
(WWW 2003).5 http://www.westminsterkennelclub.org/S.
Harabagiu, D. Moldovan, C. Clark, M. Bowden, J.Williams, J. Bensley, 2003 ?Answer Mining byCombining Extraction Techniques with AbductiveReasoning,?
In the notebook of the 12th Text RE-trieval Conference (TREC 2003), 46-53.B.
Katz, J. Lin, D. Loreto, W. Hildebrandt, M. Bilotti,S.
Felshin, A. Fernandes, G. Marton, F. Mora, 2003,?Integrating Web-Based and Corpus-Based Tech-niques for Question Answering?, In the notebook ofthe 12th Text REtrieval Conference (TREC 2003),472-480.C.
Kwok, O. Etzioni, and D. S. Weld, 2001, ?ScalingQuestion Answering to the Web?, In the Proceed-ings of the 10th ACM World Wide Web conference.
(WWW 2001).C.
Y. Lin, ?The Effectiveness of Dictionary and Web-Based Answer Reranking.?
In the Proceedings of the19th International Conference on ComputationalLinguistics (COLING 2002).B.
Magnini, M. Negri, R. Prevete and H. Tanev.
2002.?Is it the Right Answer?
Exploiting Web Redun-dancy for Answer Validation?.
In the Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics.
(ACL 2002), 425-432.J.
R. Quinlan, 1993.
C4.5: Programs for MachineLearning.
Morgan-Kaufmann, San Francisco.D.
Ravichandran, and E. H. Hovy.
2002.
?LearningSurface Text Patterns for a Question AnsweringSystem.?
In the Proceedings of the 40th  ACL con-ference.
(ACL 2002).G.
Salton and C. Buckley, "Term-weighting ap-proaches in automatic text retrieval", InformationProcessing and Management: an International Jour-nal, v.24 n.5, 1988E.M.Voorhees.
2003.
?Overview of the TREC 2003Question Answering Track.?
In the notebook of the12th Text REtrieval Conference (TREC 2003), 14-27.H.
Yang, T. S. Chua, S Wang, C. K. Koh.
2003.?Structured Use of External Knowledge for Event-based Open Domain Question Answering?, In theProceedings of the 26th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval (SIGIR 2003).H.
Yang, H. Cui, M. Maslennikov, L. Qiu, M. Y. Kan,T.
S. Chua.
2003.
?QUALIFIER in the TREC12 QAMain Task?, In the notebook of the 12th Text RE-trieval Conference (TREC 2003).S.
Ye, T. S. Chua, J. R. Kei.
2003.
?Querying andClustering Web Pages about Persons and Organiza-tions?.
Web Intelligence 2003, 344-350.
