Using model-theoretic semantic interpretation to guide statisticalparsing and word recognition in a spoken language interfaceWilliam SchulerDepartment of Computer and Information ScienceUniversity of Pennsylvania200 S. 33rd StreetPhiladelphia, PA 19104schuler@linc.cis.upenn.eduAbstractThis paper describes an extension of the se-mantic grammars used in conventional sta-tistical spoken language interfaces to allowthe probabilities of derived analyses to beconditioned on the meanings or denotationsof input utterances in the context of aninterface's underlying application environ-ment or world model.
Since these denota-tions will be used to guide disambiguationin interactive applications, they must be ef-ciently shared among the many possibleanalyses that may be assigned to an inpututterance.
This paper therefore presents aformal restriction on the scope of variablesin a semantic grammar which guaranteesthat the denotations of all possible analy-ses of an input utterance can be calculatedin polynomial time, without undue con-straints on the expressivity of the derivedsemantics.
Empirical tests show that thismodel-theoretic interpretation yields a sta-tistically signicant improvement on stan-dard measures of parsing accuracy over abaseline grammar not conditioned on deno-tations.1 IntroductionThe development of speaker-independent mixed-initiative speech interfaces, in which users not onlyanswer questions but also ask questions and giveinstructions, is currently limited by the perfor-mance of language models based largely on word co-occurrences.
Even under ideal circumstances, withlarge application-specic corpora on which to train,The author would like to thank David Chiang, KarinKipper, and three anonymous reviewers for particularlyhelpful comments on this material.
This work was sup-ported by NSF grant EIA 0224417.conventional language models are not su?cientlypredictive to correctly analyze a wide variety of in-puts from a wide variety of speakers, such as mightbe encountered in a general-purpose interface for di-recting robots, o?ce assistants, or other agents withcomplex capabilities.
Such tasks may involve unla-beled objects that must be precisely described, and awider range of actions than a standard database in-terface would require (which also must be preciselydescribed), introducing a great deal of ambiguity intoinput processing.This paper therefore explores the use of a statis-tical model of language conditioned on the mean-ings or denotations of input utterances in the contextof an interface's underlying application environmentor world model.
This use of model-theoretic inter-pretation represents an important extension to the`semantic grammars' used in existing statistical spo-ken language interfaces, which rely on co-occurrencesamong lexically-determined semantic classes and slotllers (Miller et al, 1996), in that the probabilityof an analysis is now also conditioned on the exis-tence of denoted entities and relations in the worldmodel.
The advantage of the interpretation-baseddisambiguation advanced here is that the probabil-ity of generating, for example, the noun phrase `thelemon next to the safe' can be more reliably esti-mated from the frequency with which noun phraseshave non-empty denotations { given the fact that`the lemon next to the safe' does indeed denote some-thing in the world model { than it can from the rel-atively sparse co-occurrences of frame labels such aslemon and next-to, or of next-to and safe.Since there are exponentially many word stringsattributable to any utterance, and an exponential(Catalan-order) number of possible parse tree anal-yses attributable to any string of words, this useof model-theoretic interpretation for disambiguationmust involve some kind of sharing of partial resultsbetween competing analyses if interpretation is to beperformed on large numbers of possible analyses in apractical interactive application.
This paper there-fore also presents a formal restriction on the scope ofvariables in a semantic grammar (without untowardconstraints on the expressivity of the derived seman-tics) which guarantees that the denotations of allpossible analyses of an input utterance can be calcu-lated in polynomial time.
Empirical tests show thatthis use of model-theoretic interpretation in disam-biguation yields a statistically signicant improve-ment on standard measures of parsing accuracy overa baseline grammar not conditioned on denotations.2 Model-theoretic interpretationIn order to determine whether a user's directions de-note entities and relations that exist in the worldmodel { and of course, in order to execute thosedirections once they are disambiguated { it is nec-essary to precisely represent the meanings of inpututterances.Semantic grammars of the sort employed in cur-rent spoken language interfaces foright reservationtasks (Miller et al, 1996; Sene et al, 1998) asso-ciate fragments of logical { typically relational alge-bra { expressions with recursive transition networksencoding lexicalized rules in a context-free grammar(the independent probabilities of these rules can thenbe estimated from a training corpus and multipliedtogether to give a probability for any given analysis).Inight reservation systems, these associated seman-tic expressions usually designate entities through axed set of constant symbols used as proper names(e.g.
for cities and numberedights); but in applica-tions with unlabeled (perhaps visually-represented)environments, entities must be described by pred-icating one or more modiers over some variable,narrowing the set of potential referents by specify-ing colors, spatial locations, etc., until only the de-sired entity or entities remain.
A semantic grammarfor interacting with this kind of unlabeled environ-ment might contain the following rules, using vari-ables x1::: xn(over entities in the world model) inthe associated logical expressions:VP !
VP PP : x1::: xn: $1(x1::: xm)^$2(x1; xm+1::: xn)VP !
hold NP : x1:Hold (Agent; x1) ^ $2(x1)NP !
a glass : x1:Glass(x1)PP !
under NP : x1x2:Under(x1; x2) ^ $2(x2)NP !
the faucet : x1:Faucet(x1)in which m and n are integers and 0  m  n. Eachlambda expression x1::: xn:  indicates a functionfrom a tuple of entities he1::: eni to a truth valuedened by the remainder of the expression  (sub-VP !
VP PPx1::: xn=2: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn)fhf1; g1i; hf2; g2i; : : : gVP !
hold NPx1: H(A;x1) ^ $2(x1)fg1; g2; : : : ghold NP !
.
.
.x1: G(x1)fg1; g2; : : : ga glassPP !
under NPx1x2: U(x1; x2) ^ $2(x2)fhf1; g1i; hf2; g2i; : : : gunder NP !
.
.
.x1: F (x1)ff1; f2; : : : gthe faucetFigure 1: Semantic grammar derivation showing theassociated semantics and denotation of each con-stituent.
Entire rules are shown at each step in thederivation in order to make the semantic associationsexplicit.stituting e1::: enfor x1::: xn), which denotes a set oftuples satisfying , drawn from En(where E is theset of entities in the world model).The pseudo-variables $1; $2; : : : in this notation in-dicate the sites at which the semantic expressionsassociated with each rule's nonterminal symbols areto compose (the numbers correspond to the relativepositions of the symbols on the right hand side ofeach rule, numbered from left to right).
Semantic ex-pressions for complete sentences are then formed bycomposing the sub-expressions associated with eachrule at the appropriate sites.1Figure 1 shows the above rules assembled in aderivation of the sentence `hold a glass under thefaucet.'
The denotation annotated beneath each con-stituent is simply the set of variable assignments(for each free variable) that satisfy the constituent'ssemantics.
These denotations exactly capture themeaning (in a given world model) of the assem-bled semantic expressions dominated by each con-stituent, regardless of how many sub-expressions aresubsumed by that constituent, and can therefore beshared among competing analyses in lieu of the se-mantic expression itself, as a partial result in model-theoretic interpretation.2.1 Variable scopeNote, however, that the adjunction of the preposi-tional phrase modier `under the faucet' adds an-other free variable (x2) to the semantics of the verb1This use of pseudo-variables is intended to resemblethat of the unix program `yacc,' which has a similar pur-pose (associating syntax with semantics in constructingcompilers for programming languages).VP !
VP PPx1::: xn=1: $1(x1::: xm=0) ^ $2(x1; xm+1::: xn)VP !
hold NPQx1: H(A;x1) ^ $2(x1)hold NP !
.
.
.PP !
under NPx1: Qx2: U(x1; x2) ^ $2(x2)under NP !
.
.
.Figure 2: Derivation with minimal scoping.
Thevariable x1in the semantic expression associatedwith the prepositional phrase `under the faucet' can-not be identied with the variable in the verb phrase.phrase, and therefore another factor of jEj to the car-dinality of its denotation.
Moreover, under this kindof global scoping, if additional prepositional phrasesare adjoined, they would each contribute yet an-other free variable, increasing the complexity of thedenotation by an additional factor of jEj, makingthe shared interpretation of such structures poten-tially exponential on the length of the input.
Thisproliferation of free variables means that the vari-ables introduced by the noun phrases in an utter-ance, such as `hold a glass under the faucet,' can-not all be given global scope, as in Figure 1.
Onthe other hand, the variables introduced by quanti-ed noun phrases cannot be bound as soon as thenoun phrases are composed, as in Figure 2, becausethese variables may need to be used in modierscomposed in subsequent (higher) rule applications.Fortunately, if these non-immediate variable scop-ing arrangements are expressed structurally, as dom-inance relationships in the elementary tree struc-tures of some grammar, then a structural restrictionon this grammar can be enforced that preserves asmany non-immediate scoping arrangements as possi-ble while still preventing an unbounded proliferationof free variables.The correct scoping arrangements (e.g.
for the sen-tence `hold a glass under the faucet,' shown Fig-ure 3) can be expressed using ordered sets of parserules grouped together in such a way as to allowother structural material to intervene.
In this case,a group would include a rule for composing a verband a noun phrase with some associated predicate,and one or more rules for binding each of the pred-icate's variables in a quantier somewhere above it(thereby ensuring that these rules always occur to-gether with the quantier rules dominating the pred-icate rule), while still allowing rules adjoining prepo-sitional phrase modiers to apply in between them(so that variables in their associated predicates canVP !
VPx2::: xn=1: Qx1: $1(x1::: xn)fhigVP !
VP PPx1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn)fg1; g2; : : : gVP !
hold NPx1: H(A;x1) ^ $2(x1)fg1; g2; : : : ghold NP !
.
.
.PP !
PPx2::: xn: Qx1: $1(x1::: xn)fg1; g2; : : : gPP !
under NPx1x2: U(x2; x1) ^ $2(x1)fhf1; g1i; hf2; g2i; : : : gunder NP !
.
.
.Figure 3: Derivation with desired scoping.be bound by the same quantiers).These `grouped rules' can be formalized using atree-rewriting system whose elementary trees cansubsume several ordered CFG rule applications (orsteps in a context-free derivation), as shown in Fig-ure 4.
Each such elementary tree contains a rule(node) associated with a logical predicate and rules(nodes) associated with quantiers binding each ofthe predicate's variables.
These trees are then com-posed by rewriting operations (dotted lines), whichsplit them up and either insert them between or iden-tify them with (if demarcated with dashed lines) therules in another elementary tree { in this case, theelementary tree anchored by the word `under.'
Thesetrees are considered elementary in order to excludethe possibility of generating derivations that containunbound variables or quantiers over unused vari-ables, which would have no intuitive meaning.
Thecomposition operations will be presented in furtherdetail in Section 2.2.2.2 Semantic composition as tree-rewritingA general class of rewriting systems can be denedusing sets of allowable expansions of some type ofobject to incorporate zero or more other instancesof the same type of object, each of which is simi-larly expandable.
Such a system can generate ar-bitrarily complex structure by recursively expand-ing or `rewriting' each new object, concluding with aset of zero-expansions at the frontier.
For example,a context-free grammar may be cast as a rewritingsystem whose objects are strings, and whose allow-able expansions are its grammar productions, eachof which expands or rewrites a certain string as a setVP !
VPx2::: xn: Qx1: $1(x1::: xn)VP !
hold NPx1::: xn: $1(x1::: xn) ^ $2(x1)V !
holdx1:Hold(A; x1)NP !
.
.
.. .
.1VP !
VPx2::: xn: Qx1: $1(x1::: xn)VP !
VP PPx1::: xn: $1(x1::: xm) ^ $2(x1; xm+1::: xn)VP1PP !
PPx2::: xn: Qx1: $1(x1::: xn)PP !
P NPx1::: xn: $1(x1::: xn) ^ $2(x1)P !
underx1x2:Under(x2; x1)NP212PP !
PPx2::: xn: Qx1: $1(x1::: xn)NP !
Q N$2Q !
a N !
faucetx1:Faucet(x1)Figure 4: Complete elementary tree for `under' showing argument insertion sites.of zero or more sub-strings arranged around certain`elementary' strings contributing terminal symbols.A class of tree-rewriting systems can similarlybe dened as rewriting systems whose objects aretrees, and whose allowable expansions are produc-tions (similar to context-free productions), each ofwhich rewrite a tree A as some function f appliedto zero or more sub-trees A1; : : : As; s  0 arrangedaround some `elementary' tree structure dened byf (Pollard, 1984; Weir, 1988):A !
f(A1; : : : As) (1)This elementary tree structure can be used to ex-press the dominance relationship between a logicalpredicate and the quantiers that bind its variables(which must be preserved in any meaningful derivedstructure); but in order to allow the same instanceof a quantier to bind variables in more than onepredicate, the rewriting productions of such a se-mantic tree-rewriting system must allow expandedsubtrees to identify parts of their structure (speci-cally, the parts containing quantiers) with parts ofeach other's structure, and with that of their hostelementary tree.In particular, a rewriting production in such a sys-tem would rewrite a tree A as an elementary tree0with a set of sub-trees A1; : : : Asinserted into it, eachof which is rst partitioned into a set of contiguouscomponents (in order to isolate particular quantiernodes and other kinds of sub-structure) using a treepartition function g at some sequence of split pointsh#i1,...#icii, which are node addresses in Ai(the rstof which simply species the root).2The resulting se-quence of partitioned components of each expanded2The node addresses encode a path from the root ofsub-tree are then inserted into0at a correspond-ing sequence of insertion site addresses hi1,... iciidened by the rewriting function f :f(A1; : : : As) =0[h11,... 1c1i; g#11,...#1c1(A1)] : : :[hs1,... scsi; g#s1,...#scs(As)] (2)Since each address can only host a single insertedcomponent, any components from dierent sub-treearguments of f that are assigned to the same inser-tion site address are constrained to be identical in or-der for the production to apply.
Additionally, someaddresses may be `pre-lled' as part of the elemen-tary structure dened in f , and therefore may alsobe identied with components of sub-tree argumentsof f that are inserted at the same address.Figure 4 shows the set of insertion sites (designatedwith boxed indices) for each argument of an elemen-tary tree anchored by `under.'
The sites labeled 1 ,associated with the rst argument sub-tree (in thiscase, the tree anchored by `hold'), indicate that it iscomposed by partitioning it into three components,each dominating or dominated by the others, the low-est of which is inserted at the terminal node labeled`VP,' the middle of which is identied with a pre-lled component (delimited by dashed lines), con-taining the quantier node labeled `VP !
VP,' andthe uppermost of which (empty in the gure) is in-serted at the root, while preserving the relative dom-inance relationships among the nodes in both trees.Similarly, sites labeled 2 , associated with the sec-ond argument sub-tree (for the noun phrase comple-the tree in which every address i species the ithchildof the node at the end of path .ment to the preposition), indicate that it is composedby partitioning it into two components { again, onedominating the other { the lowest of which is insertedat the terminal node labeled `NP,' and the uppermostof which is identied with another pre-lled compo-nent containing the quantier node labeled `PP !PP,' again preserving the relative dominance rela-tionships among the nodes in both trees.2.3 Shared interpretationRecall the problem of unbounded variable prolif-eration described in Section 2.1.
The advantageof using a tree-rewriting system to model semanticcomposition is that such systems allow the appli-cation of well-studied restrictions to limit their re-cursive capacity to generate structural descriptions(in this case, to limit the unbounded overlappingof quantier-variable dependencies that can produceunlimited numbers of free variables at certain steps ina derivation), without limiting the multi-level struc-ture of their elementary trees, used here for captur-ing the well-formedness constraint that a predicatebe dominated by its variables' quantiers.One such restriction, based on the regular formrestriction dened for tree adjoining grammars(Rogers, 1994), prohibits a grammar from allowingany cycle of elementary trees, each intervening insidea spine (a path connecting the insertion sites of anyargument) of the next.
This restriction is denedbelow:Denition 2.1 Let a spine in an elementary tree bethe path of nodes (or object-level rule applications)connecting all insertion site addresses of the sameargument.Denition 2.2 A grammar G is in regular form if adirected acyclic graph hV;Ei can be drawn with ver-tices vH; vA2 V corresponding to partitioned ele-mentary trees of G (partitioned as described above),and directed edges hvH; vAi 2 E  V  V from eachvertex vH, corresponding to a partitioned elementarytree that can host an argument, to each vertex vA,corresponding to a partitioned elementary tree thatcan function as its argument, whose partition inter-sects its spine at any place other than the top nodein the spine.This restriction ensures that there will be no un-bounded `pumping' of intervening tree structure inany derivation, so there will never be an unboundedamount of unrecognized tree structure to keep trackof at any step in a bottom-up parse, so the numberof possible descriptions of each sub-span of the in-put will be bounded by some constant.
It is called a`regular form' restriction because it ensures that theset of root-to-leaf paths in any derived structure willform a regular language.A CKY-style parser can now be built that rec-ognizes each context-free rule in an elementary treefrom the bottom up, storing in order the unrecog-nized rules that lie above it in the elementary tree(as well as any remaining rules from any composedsub-trees) as a kind of promissory note.
The factthat any regular-form grammar has a regular pathset means that only a nite number of states will berequired to keep track of this promised, unrecognizedstructure in a bottom-up traversal, so the parser willhave the usual O(n3) complexity (times a constantequal to the nite number of possible unrecognizedstructures).Moreover, since the parser can recognize any stringderivable by such a grammar, it can create a sharedforest representation of every possible analysis of agiven input by annotating every possible applica-tion of parse rules that could be used in the deriva-tion of each constituent (Billot and Lang, 1989).This polynomial-sized shared forest representationcan then be interpreted determine which constituentsdenote entities and relations in the world model, inorder to allow model-theoretic semantic informationto guide disambiguation decisions in parsing.Finally, the regular form restriction also has theimportant eect of ensuring that the number of un-recognized quantier nodes at any step in a bottom-up analysis { and therefore the number of free vari-ables in any word or phrase constituent of a parse { isalso bounded by some constant, which limits the sizeof any constituent's denotation to a polynomial or-der of E , the number of entities in the environment.The interpretation of any shared forest derived bythis kind of regular-form tree-rewriting system cantherefore be calculated in worst-case polynomial timeon E .A denotation-annotated shared forest for the nounphrase `the girl with the hat behind the counter' isshown in Figure 5, using the noun and prepositiontrees from Figure 4, with alternative applications ofparse rules represented as circles below each derivedconstituent.
This shared structure subsumes twocompeting analyses: one containing the noun phrase`the girl with the hat', denoting the entity g1, andthe other containing the noun phrase `the hat be-hind the counter', which does not denote anythingin the world model.
Assuming that noun phrasesrarely occur with empty denotations in the trainingdata, the parse containing the phrase `the girl withthe hat' will be preferred, because there is indeed agirl with a hat in the world model.This formalism has similarities with two ex-NP !
girlx1:Girl(x1)fg1; g2; g3gP !
withx1x2:With(x2; x1)fhh1; g1i; hh2; b1igNP !
hatx1:Hat(x1)fh1; h2; h3; h4gP !
behindx1x2:Behind(x2; x1)fhc1; g1igNP !
counterx1:Counter(x1)fc1; c2gPP !
P NPx1::: xn=2: $1(x1::: xn) ^ $2(x1)fhh1; g1i; hh2; b1igPP !
PPx2::: xn=2: Qx1: $1(x1::: xn)fg1; b1gPP !
P NPx1::: xn=2: $1(x1::: xn) ^ $2(x1)fhc1; g1igPP !
PPx2::: xn=2: Qx1: $1(x1::: xn)fg1gNP !
NP PPx1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn)fg1gNP !
NP PPx1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn);PP !
P NPx1::: xn=2: $1(x1::: xn) ^ $2(x1);PP !
PPx2::: xn=2: Qx1: $1(x1::: xn);NP !
NP PPx1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn); or fg1gFigure 5: Shared forest for `the girl with the hat behind the counter.
'tensions of tree-adjoining grammar (Joshi, 1985),namely multi-component tree adjoining grammar(Becker et al, 1991) and description tree substitu-tion grammar (Rambow et al, 1995), and indeedrepresents something of a combination of the two:1.
Like description tree substitution grammars,but unlike multi-component TAGs, it allowstrees to be partitioned into any desired set ofcontiguous components during composition,2.
Like multi-component TAGs, but unlike descrip-tion tree substitution grammars, it allows thespecication of particular insertion sites withinelementary trees, and3.
Unlike both, it allow duplication of structure(which is used for merging quantiers from dif-ferent elementary trees).The use of lambda calculus functions to dene de-composable meanings for input sentences draws ontraditions of Church (1940) and Montague (1973),but this approach diers from the Montagovian sys-tem by introducing explicit limits on computationalcomplexity (in order to allow tractable disambigua-tion).This approach to semantics is very similar to thatdescribed by Shieber (1994), in which syntactic andsemantic expressions are assembled synchronouslyusing paired tree-adjoining grammars with isomor-phic derivations, except that in this approach thederived structures are isomorphic as well, hence thereduction of synchronous tree pairs to semantically-annotated syntax trees.
This isomorphism restric-tion on derived trees reduces the number of quantierscoping congurations that can be assigned to anygiven input (most of which are unlikely to be usedin a practical application), but its relative parsimonyallows syntactically ambiguous inputs to be seman-tically interpreted in a shared forest representationin worst-case polynomial time.
The interleaving ofsemantic evaluation and parsing for the purpose ofdisambiguation also has much in common with thatof Dowding et al (1994), except that in this case,constituents are not only semantically type-checked,but are also fully interpreted each time they are pro-posed.
There are also commonalities between the un-derspecied semantic representation of structurally-ambiguous elementary tree constituents in a sharedforest and the underspecied semantic representa-tion of (e.g.
quantier) scope ambiguity describedby Reyle (1993).33 EvaluationThe contribution of this model-theoretic semantic in-formation toward disambiguation was evaluated ona set of directions to animated agents collected in acontrolled but spatially complex 3-D simulated en-vironment (of children running a lemonade stand).In order to avoid priming them towards particu-lar linguistic constructions, subjects were shown un-narrated animations of computer-simulated agentsperforming dierent tasks in this environment (pick-ing fruit, operating a juicer, and exchanging lemon-ade for money), which were described only as the `de-sired behavior' of each agent.
The subjects were thenasked to direct the agents, using their own words, toperform the desired behaviors as shown.340 utterances were collected and annotated withbrackets and elementary tree node addresses as de-scribed in Section 2.2, for use as training data andas gold standard data in testing.
Some sample direc-tions are shown in Figure 6.
Most elementary treeswere extracted, with some simplications for parsinge?ciency, from an existing broad-coverage grammarresource (XTAG Research Group, 1998), but someelementary trees for multi-word expressions had tobe created anew.
In all, a complete annotation of thiscorpus required a grammar of 68 elementary treesand a lexicon of 288 lexicalizations (that is, wordsor sets of words with indivisible semantics, formingthe anchors of a given elementary tree).
Each lex-icalization was then assigned a semantic expressiondescribing the intended geometric relation or class ofobjects in the simulated 3-D environment.4The interface was tested on the rst 100 collectedutterances, and the parsing model was trained onthe remaining utterances.
The presence or absenceof a denotation of each constituent was added to thelabel of each constituent in the denotation-sensitiveparsing model (for example, statistics were collectedfor the frequency of `NP:  !
NP:+ PP:+' events,meaning a noun phrase that does not denote any-3Denotation of competing applications of parse rulescan be unioned (though this eectively treats ambiguityas a form of disjunction), or stored separately to somenitie beam (though some globally preferable but locallydispreferred structures would be lost).4Here it was assumed that the intention of the userwas to direct the agent to perform the actions shown inthe `desired behavior' animation.Walk towards the tree where you see a yellow lemonon the ground.Pick up the lemon.Place the lemon in the pool.Take the dollar bill from the person in front of you.Walk to the left towards the big black cube.Figure 6: Sample utterances from collected corpus.thing in the environment expands to a noun phraseand a prepositional phrase that do have a denota-tion in the environment), whereas the baseline sys-tem used a parsing model conditioned on only con-stituent labels (for example, `NP !
NP PP' events).The entire word lattice output of the speech recog-nizer was fed directly into the parser, so as to al-low the model-theoretic semantic information to bebrought to bear on word recognition ambiguity aswell as on structural ambiguity in parsing.Since any derivation of elementary trees uniquelydenes a semantic expression at each node, the taskof evaluating this kind of semantic analysis is reducedto the familiar task of evaluating a the accuracy ofa labeled bracketing (labeled with elementary treenames and node addresses).
Here, the standard mea-sures of labeled precision and recall are used.
Notethat there may be multiple possible bracketings foreach gold standard tree in a given word lattice thatdier only in the start and end frames of the com-ponent words.
Since neither the baseline nor testparsing models are sensitive to the start and endframes of the component words, the gold standardbracketing is simply assumed to use the most likelyframe segmentation in the word lattice that yieldsthe correct word sequence.The results of the experiment are summarizedbelow.
The environment-based model shows astatistically signicant (p<.05) improvement of 3points in labeled recall, a 12% reduction in error.Most of the improvement can be attributed to thedenotation-sensitive parser dispreferring noun phraseconstituents with mis-attached modiers, which donot denote anything in the world model.Model LR LPbaseline model 82% 78%baseline + denotation bit 85% 81%4 ConclusionThis paper has described an extension of the seman-tic grammars used in conventional spoken languageinterfaces to allow the probabilities of derived anal-yses to be conditioned on the results of a model-theoretic interpretation.
In particular, a formal re-striction was presented on the scope of variables in asemantic grammar which guarantees that the deno-tations of all possible analyses of an input utterancecan be calculated in polynomial time, without un-due constraints on the expressivity of the derivedsemantics.
Empirical tests show that this model-theoretic interpretation yields a statistically signif-icant improvement on standard measures of parsingaccuracy over a baseline grammar not conditionedon denotations.ReferencesTilman Becker, Aravind Joshi, and Owen Rambow.1991.
Long distance scrambling and tree adjoininggrammars.
In Fifth Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL'91), pages 21{26.Sylvie Billot and Bernard Lang.
1989.
The structureof shared forests in ambiguous parsing.
In Proceed-ings of the 27thAnnual Meeting of the Associationfor Computational Linguistics (ACL '89), pages143{151.Alonzo Church.
1940.
A formulation of the sim-ple theory of types.
Journal of Symbolic Logic,5(2):56{68.John Dowding, Robert Moore, Francois Andery, andDouglas Moran.
1994.
Interleaving syntax and se-mantics in an e?cient bottom-up parser.
In Pro-ceedings of the 32nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL'94).Aravind K. Joshi.
1985.
How much context sensi-tivity is necessary for characterizing structural de-scriptions: Tree adjoining grammars.
In L. Kart-tunen D. Dowty and A. Zwicky, editors, Natu-ral language parsing: Psychological, computationaland theoretical perspectives, pages 206{250.
Cam-bridge University Press, Cambridge, U.K.Scott Miller, David Stallard, Robert Bobrow, andRichard Schwartz.
1996.
A fully statistical ap-proach to natural language interfaces.
In Pro-ceedings of the 34th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL'96),pages 55{61.Richard Montague.
1973.
The proper treatmentof quantication in ordinary English.
In J. Hin-tikka, J.M.E.
Moravcsik, and P. Suppes, editors,Approaches to Natural Langauge, pages 221{242.D.
Riedel, Dordrecht.
Reprinted in R. H. Thoma-son ed., Formal Philosophy, Yale University Press,1994.Carl Pollard.
1984.
Generalized phrase structuregrammars, head grammars and natural langauge.Ph.D.
thesis, Stanford University.Owen Rambow, David Weir, and K. Vijay-Shanker.1995.
D-tree grammars.
In Proceedings of the 33rdAnnual Meeting of the Association for Computa-tional Linguistics (ACL '95).Uwe Reyle.
1993.
Dealing with ambiguities by un-derspecication: Construction, representation anddeduction.
Journal of Semantics, 10:123{179.James Rogers.
1994.
Capturing CFLs with tree ad-joining grammars.
In Proceedings of the 32nd An-nual Meeting of the Association for ComputationalLinguistics (ACL '94).Stephanie Sene, Ed Hurley, Raymond Lau, Chris-tine Pao, Philipp Schmid, and Victor Zue.
1998.Galaxy-II: a reference architecture for conversa-tional system development.
In Proceedings of the5th International Conference on Spoken LanguageProcessing (ICSLP '98), Sydney, Australia.Stuart M. Shieber.
1994.
Restricting the weak-generative capability of synchronous tree adjoininggrammars.
Computational Intelligence, 10(4).David Weir.
1988.
Characterizing mildly context-sensitive grammar formalisms.
Ph.D. thesis, De-partment of Computer and Information Science,University of Pennsylvania.XTAG Research Group.
1998.
A lexicalized treeadjoining grammar for english.
Technical report,IRCS, University of Pennsylvania.
