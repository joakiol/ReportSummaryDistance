Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1236?1244,Beijing, August 2010A Working Report on Statistically Modeling Dative Variation inMandarin ChineseYao YaoUniversity of California, BerkeleyDepartment of Linguisticsyaoyao@berkeley.eduFeng-hsi LiuUniversity of ArizonaDepartment of East Asian Studiesfliu@u.arizona.eduAbstractDative variation is a widely observed syn-tactic phenomenon in world languages(e.g.
I gave John a book and I gave a bookto John).
It has been shown that whichsurface form will be used in a dative sen-tence is not a completely random choice,rather, it is conditioned by a wide rangeof linguistic factors.
Previous work byBresnan and colleagues adopted a statis-tical modeling approach to investigate theprobabilistic trends in English dative alter-nation.
In this paper, we report a similarstudy on Mandarin Chinese.
We furtherdeveloped Bresnan et al?s models to suitthe complexity of the Chinese data.
Ourmodels effectively explain away a largeproportion of the variation in the data, andunveil some interesting probabilistic fea-tures of Chinese grammar.
Among otherthings, we show that Chinese dative varia-tion is sensitive to heavy NP shift in bothleft and right directions.1 Introduction1.1 OverviewIn traditional linguistic research, the study of syn-tax is most concerned with grammaticality.
Sen-tences are either grammatical or ungrammatical,and syntactic theories are proposed to explain thestructural features that cause (un)grammaticality.Meanwhile, little attention has been paid to therelative acceptability of grammatical sentences.
Iftwo sentences are both grammatical and basicallyexpress the same meaning, are they equally likelyto occur in the language?
The answer is proba-bly no.
For example, in English, the sentence Ihave read that book is much more frequent thanThat book I have read.
The latter topicalized sen-tence is only used when the entity denoted by Thatbook is in focus.
This indicates that the choiceof surface sentence form is not entirely random,but conditioned by some factors including infor-mation status.Thus, instead of categorizing sentences asgrammatical or ungrammatical, a better way toexpress the degree of grammaticality would be touse a likelihood continuum, from 0 to 1, where un-grammatical sentences have zero likelihood andgrammatical sentences fall somewhere between0 and 1, with some being more likely than oth-ers.
The idea of associating linguistic forms withvarious probabilities has been around for a while(see Jurafsky, 2003 and Manning, 2003 for an ex-tensive review).
Recent psycholinguistic researchhas shown that just like grammaticality, the likeli-hoods of sentence forms are also part of the user?slinguistic knowledge.
Sentences with high proba-bilities are in general easier to comprehend andproduce, and their production is more prone tophonetic reduction (Bresnan, 2007; Gahl and Gar-nsey, 2004; Levy, 2008; among others).
The fa-mous example of garden path sentences also ex-emplifies the difficulty of comprehension in low-probability sentence forms.If we accept the premise of probabilistic syn-tax, then an immediate question is what deter-mines these probabilities.
In the current work, weaddress this question by investigating a particulartype of probabilistic phenomenon, i.e.
dative vari-ation in Chinese.
We show that the probabilities of1236various surface forms of Chinese dative sentencescan be well estimated by a linear combination ofa set of formal and semantic features.The remainder of this paper is organized as fol-lows.
Section 1.2 briefly reviews previous workon English dative variation.
Section 1.3 intro-duces dative variation in Chinese.
Section 2 de-scribes the dataset and the statistical models usedin the current study.
Section 3 presents model-ing results, followed by a discussion in Section 4.Section 5 concludes the paper with a short sum-mary.
To preview the results, we show that dativevariation in Chinese is more complicated than inEnglish, in that it features two levels of variation,which exhibit different (sometimes even opposite)probabilistic patterns.1.2 Dative variation in EnglishA dative sentence is a sentence that encodes atransfer event.
Typical verbs of transfer in En-glish include give, send, mail, etc.
A characteriz-ing property of transfer events is that they often in-volve two objects.
In addition to the direct object(DO), the verb also takes an indirect object (IO)which usually denotes the recipient of the trans-fer action.
For instance, in sentence 1a, the directobject is a book and the indirect object is John.Cross-linguistically, it has been documentedthat many languages in the world have multiplesyntactic forms for encoding the same transferevent (Margetts and Austin, 2007 , among oth-ers).
In English, both 1a and 1b describe the sameevent, but 1a is a double object form (V IO DO)while 1b takes a prepositional phrase (V DO toIO).
(1) a. I gave John a book.
?
V IO DOb.
I gave a book to John.
?
V DO to IOA number of conditioning factors have beenidentified for the alternation between the two sur-face forms.
For instance, when the indirect ob-ject is a pronoun (e.g.
him), it is more likely tohave the double object form (i.e.
I gave him abook) than the PP form (i.e.
I gave a book tohim).
On the other hand, if the indirect objectis a complex NP (with relative clauses), it tendsto occur at the end of the sentence.
Since mostof these effects are subtle and often correlatedwith each other (e.g.
definiteness, pronominalityand syntactic complexity), investigating individ-ual factors can give convoluted and unreliable re-sults.
To avoid this problem, many recent works inthe field adopted a statistical modeling approach(Bresnan et al, 2007; Wasow and Arnold, 2003,among others).
Instead of investigating separatefactors, statistical models are built on large-scaledatasets, using all potential conditioning factorsto predict the surface form.
In Bresnan et al(2007), a dozen predictors relating to the verb(type of transfer event), the two object NPs (ac-cessibility, pronominality, definiteness, syntacticcomplexity, etc), and the discourse (presence ofparallel structures) were used to make the predic-tion.
Using data input from 2,360 dative sentencesfrom the Switchboard corpus, the model correctlypredicted surface form in 97% of the sentences,which was a great improvement over the baselineprediction accuracy of 79% (i.e.
the percentageof correct responses if the model knows nothingbut which variant is more frequently used).
It alsoshowed that dative variation in English was indeedsensitive to all the predictors in the model.1.3 Dative variation in ChineseDative variation in Chinese is much more compli-cated than in English.
In addition to the two wordorders that exist in English (2a, 2b), it is also com-mon for direct object to appear before the verb,as in a BA construction or a topicalized sentence(2c).
Besides, indirect object can also precede theverb, as shown in 2d.
Another dimension of vari-ation is in the use of coverbs gei and ba, both ofwhich can be optional (2b, 2c; see Li and Thomp-son, 1981 for a detailed discussion on this), or re-placed by other morphemes (zhu, yu, jiang, etc).
(2) a. JohnJohnsong-legive-ASPshubookgeitoMary.MaryJohn gave one/some book(s) to Mary.?
V DO IOb.
JohnJohnsonggave(gei)(to)MaryMaryyibenoneshu.bookJohn gave Mary a book.?
V IO DOc.
JohnJohnbaBAshubooksonggave(gei)(to)Mary,Mary(ba)(BA)1237jiuwinesonggave(gei)(to)Kate.KateJohn gave the book(s) to Mary andgave the wine to Kate.?
DO V IOd.
TaHemeireneveryonefa-leallocatedyibenoneshu.bookHe gave everyone a book.?
IO V DOFor the purpose of the current study, we willignore the existence (hence also the variation) ofgei and ba, and concentrate on the variation in therelative order of V, DO and IO.
In addition, ourcorpus search shows that sentences in the form ofIO V DO are the least frequent (<9%) and mostlylimited to a small set of verbs (mostly fa ?to al-locate?
and banfa ?to award?
), so we drop thiscategory from the current study.
Thus the threeremaining word order variants are: DO V IO, VDO IO, and V IO DO.Generally speaking, there are two ways ofmodeling a variation phenomenon involving threevariants.
One way is to assume that the three vari-ants are equally dissimilar from one another andthe selection process is just to pick one out of three(Fig.
1a).
The other approach is to assume a hi-erarchical structure: two of the variants are moresimilar to each other than they are to the third oneand thus form a subcategory first before they jointhe third variant (Fig.
1b).
In the selection pro-cess, the user first selects the subcategory (i.e.
x1or x?
in Fig 1b), and depending on which subcate-gory is chosen, they might need to make a secondchoice between two end nodes (i.e.
x2 and x3).
(a) (b)Figure 1: Two possible schemasWe argue that the variation among the threeword order variants in the current study is bettermodeled by a schema like Fig 1b, for both theoret-ical and methodological reasons.
First, V DO IOand V IO DO are structurally more similar to eachother than they are to DO V IO.
Both V DO IO andV IO DO are in canonical word order of Chinesebut the form DO V IO features the preposing (ortopicalization) of the DO, whether or not the BAmorpheme is present.
Object preposing also existsoutside ditransitive sentences (e.g.
3).
Previousresearch has associated object preposing with thedisposal meaning of the verb phrase, and the def-initeness, givenness and weight of the object NP(Li and Thompson, 1981; Liu, 2007).
(3) a. WoIbaBAfanricechieatwanfinishle.SEPI have finished the rice.b.
Tahezhethisdianyingmoviekan-lesawhenduomanybian.timeHe has watched this movie for manytimes.There is also a methodological motivation foradopting a hierarchical schema.
Though it is notimpossible to model a categorical variation withmore than two variants (using multinomial logis-tic regression), binary variation is much easier tomodel and the interpretation of the results is morestraightforward (this is especially true when ran-dom effects are present).In view of the above, we propose the schemain Fig 2 for modeling the current variation phe-nomenon.
We refer to sentences in the form of DOV IO as preverbal ditransitive sentences (since DOis before the verb), while both V DO IO and V IODO are postverbal ditransitives.
The distinctionbetween the latter two forms regards whether DOis before or after IO, therefore one is termed aspre-IO and the other post-IO.
Compared with theupper-level preverbal-postverbal distinction, thelower-level variation is much less studied in theliterature (though see Liu, 2006 for a relevant dis-cussion).Corresponding to the schema in Fig 2, we con-structed two separate models, one for the upper-level variation (?upper model?)
and the other forthe lower-level variation (?lower model?
).1238Figure 2: A two-level schema for Chinese dativevariation2 Methodology2.1 Corpus and datasetThe data we use are from the Sinica Corpus ofModern Chinese (v3.1; Huang et al, 1995).
Wefirst compiled a list of 36 verbs that could be usedditransitively (see Appendix A) and then extractedfrom the corpus all sentences containing thesewords (n= 48,825 sentences).
We then manu-ally went through the sentences and selected thosethat (a) featured the ditransitive sense of the tar-get verb, with both object NPs being overt, and(b) were in the form of any of the three form vari-ants.
1,574 sentences remained after step (a) 1 and1,433 after step (b) 2.Further removal was conducted on verbs thatwere too sparse in the dataset.
In each varia-tion model, we removed verbs with fewer thantwo occurrences under either form variant.
Thefinal dataset for the upper model has 1149 sen-tences (of 20 verb types) while the dataset forthe lower model has 801 sentences (of 14 verbtypes).
The latter dataset is largely but not fullycontained in the former due to the elimination oflow-frequency verbs.2.2 Data annotationSimilar to Bresnan et al?s work on English, weannotated each data sentence for a wide range offeatures pertaining to the verb and the two NPs(see Appendix B for a complete list of annotated1A vast number of sentences were removed because thetarget verb was not used as a verb, or used with a differentsense, or used as part of a different verb phrase, e.g.
fa toallocate could also mean to bloom or be used in fazhan todevelop, faxian to discover, etc.2141 sentences were removed because they were in theform of IO V DO.factors).
Specifically, the verb was coded eitheras expressing a canonical transfer event, such asji ?to mail?, or an extended transfer event, suchas jieshao ?to introduce?.
Semantic annotation ofthe two NPs is much trickier in Chinese than inEnglish due to the lack of morphology.
In prac-tice, we used Bresnan et al?s criteria for English,whenever applicable (e.g.
accessibility, person,concreteness, animacy).
In cases where the En-glish rules did not apply (e.g.
definiteness andnumber of bare NPs in Chinese), we developedworking principles based on phrasal substitution.For example, if a bare NP can take a specifierlike yige/yizhi ?one?
without changing sentencemeaning, it is considered to be indefinite.
Con-versely, if a bare NP is better replaced with afull NP with a demonstrative zhege ?this?
or nage?that?, it is coded as definite.
Similar rules wereused to assist annotating the number feature, usingspecifiers yige/nage ?one?/?that?
and yixie/naxie?some?/?those?.In addition to the factors in the English model,we also coded a set of structural features, includ-ing the presence of a following verb after theditransitive construction, the presence of quanti-fiers/numerals in the NPs, and whether or not theditransitive structure is embedded, nominalized,or relativized, etc.
We suspect that since seman-tic features are often covert in Chinese words, it ispossible that overt marking (e.g.
the use of quan-tifiers/numerals) plays a more important role inconditioning surface form variation.Finally we also included genre in the model.Sentences listed under the categories of dialogueand speech in the Sinica corpus were coded as?spoken?
and the rest are coded as ?written?.Altogether 24 factors were annotated and in-cluded in the statistical models as predictor vari-ables.
All variables are categorical except for the(log) length difference between DO and IO, whichis numerical.2.3 Statistical modelsThe statistical tool we use is mixed-effects lo-gistic regression models.
Compared with regu-lar logistic regression models, mixed-effects mod-els are more sophisticated in that they allow theuser to specify factors that might introduce ran-1239dom variation in the dataset.
In the current study,the datasets in both models contain sentences withdifferent verbs.
It is possible that different verbshave different intrinsic tendencies toward a cer-tain word order variant.3 Incorporating this pieceof information into the model makes it more pow-erful and less affected by the unbalanced distri-bution of verb types.
The mathematical formulaof the mixed-effects logistic regression model isgiven below.
(4) Probability(V DO IO) = 11+e?(?i+x?)
,where ?i is the verb-specific intercept ofthe verb vi , x is a vector of predictorsand ?
is a vector of corresponding coef-ficients.Using the annotated datasets described in 2.2,we built an upper model and a lower model, corre-sponding to the schema in Fig 2.
The general pro-cedure of statistical analysis (which is the samefor both models) is described as follows.We first run the model with all 24 predictors,which will generate a coefficient and a p value foreach predictor.
Then we refit the model with onlysignificant predictors (i.e.
p < 0.05).
The purposeof doing so is to filter out the noise in the model fitcreated by the large number of insignificant pre-dictors.
Only predictors that remain significant inthe simplified model with largely unchanged co-efficients are considered to be reliably significant.Two model evaluation techniques are used tocheck the model results: cross-validation and sep-arate analysis of high-frequency verbs.
A poten-tial problem in any statistical model is that it mightoverfit the data.
After all, what we are interestedin is the general probabilistic trends in dative vari-ation, not the trends in a particular set of sen-tences featuring a particular set of verbs.
A cross-validation test helps us evaluate the generalizabil-ity of model results by running the same modelon a randomly sampled subset of the data.
In do-ing so, it simulates the effect of having differentdatasets.
In practice, we use two types of cross-3The same can be said about individual speakers, as somespeakers might be more inclined to use certain forms thanother speakers.
However, since the sentences in the currentdatasets were sampled from a vast pool of speakers/writers(given the way the corpus is developed), individual differ-ences among speakers is not considered in the current model.validation procedures: one randomly samples sen-tences and the other samples verbs.
Each proce-dure is executed on 100 randomly sampled subsetof half the sentences/verbs.
Only predictors withconsistent performance over all iterations in bothtests will be considered as stable.Another concern in the model design is the ef-fect of verb frequency.
In the current dataset, oneverb, i.e.
tigong ?to provide?, is extremely fre-quent.
37.3% of the sentences in the upper modeland 50.9% in the lower model come from thisverb.
Though in theory, verb frequency is alreadytaken care of by using mixed-effects models andrunning cross-validation on samples of the verbset, it is still necessary to test tigong separatelyfrom the rest of the verbs, due to its extremelyhigh frequency.
In the next section, we will re-port in detail the results from the two regressionmodels.3 Results3.1 Upper model: predicting preverbal andpostverbal variationIn the upper model, the distinction is between pre-verbal (DO V IO; coded as 1) and postverbal di-transitives (V DO IO and V IO DO; both coded as0).
The dataset in this model contains 1,149 sen-tences (of 20 verb types), with 379 preverbal and770 postverbal.
The distribution of the verbs ishighly skewed.
The most frequent verb is tigong?to provide?
(n=428 tokens), followed by song ?tosend?
(135) and jiao ?to hand; to transfer?
(117).The remaining 17 verbs have between 5 and 54occurrences in the dataset.10 out of 24 predictors in the full model aresignificant and most of them remain significantwhen the other 14 predictors are removed fromthe model.
Table 1 below summarizes the resultsof the simplified model.Judging from the signs of the coefficients inTable 1, a dative sentence is more likely to takethe preverbal form (as opposed to the postverbalform) when (a) the verb expresses canonical trans-fer event, (b) DO is definite, plural, abstract andgiven in the previous context, with no quantifiersor numerals, (c) IO is not a pronoun and is notgiven in the previous context, and (d) DO is longer1240Predictor ?
pverb is canonical 1.71 0.03DO is given 1.22 <0.001DO is definite 4.89 <0.001DO is plural 1.4 <0.001DO is concrete -1.13 0.004quan/num in DO -0.99 0.005IO is pronoun -1.64 <0.001IO is given -0.9 0.007quan/num in IO 1.32 0.07 (n.s.
)Len(DO)-Len(IO) 0.53 0.002Table 1: Fixed effects in the simplified uppermodelthan IO.Table 2 shows the accuracy of the sim-plified model.
If 0.5 is used as the cut-off probability, the model correctly predictsfor (737+338)/1149=93.6% of the sentences.For comparison, the baseline accuracy is only770/1049=-67% (i.e.
by guessing postverbal ev-ery time).
In other words, the model only needsto include 10 predictors to achieve an increase ofaround 39% (93.6-67)/67) in model accuracy.Predictedpreverbal postverbalobserved preverbal 338 41postverbal 33 737Table 2: Prediction accuracy of the simplified up-per modelResults from the two cross-validation tests con-firm all the predictors regarding DO in Table 1,as well as the pronominality of IO and the lengthdifference between DO and IO.
Verb categoryand the givenness of IO do not survive the cross-validation tests.Separate analysis of tigong shows that indeed,the extremely high-frequency verb exhibits vastlydifferent patterns than other verbs.
Only one pre-dictor turns out to be significant for tigong sen-tences, that is, the definiteness of DO (?
= 6.17,p < 0.001).
A closer look at these sentences sug-gests that they are strongly biased toward postver-bal word order, in that 400 out of 428 (95.4%)tigong sentences are postverbal (compared withthe average level of 67% in all sentences).
In otherwords, just by guessing postverbal every time, oneis able to make the correct prediction for tigongover 95% of the time.
Not surprisingly, thereis little need for additional predictors.
For non-tigong sentences, all factors in Table 1 are signif-icant except for verb category and the presenceof quantifiers/numerals in IO.
Overall, the non-tigong model has an accuracy of 91.5% (baseline= 50.6%).To sum up, we are confident to say that thesemantic features of DO, as well as pronominal-ity of IO and the length difference between thetwo objects, play important roles in conditioningthe preverbal-postverbal variation.
Knowing thesefactors boosts the model s predicting power by agreat deal.3.2 Lower model: predicting pre-IO andpost-IO variationIn the lower model, the distinction is between pre-IO sentences (i.e.
V DO IO; coded as 1) and post-IO sentences (i.e.
V IO DO; coded as 0).
Thedataset consists of 801 sentences of 14 verb types,among which 161 are pre-IO and 640 are post-IO.The most frequent verb is again, tigong (n=408tokens), followed by dai ?to bring?
(137) and song?to send?
(89).Table 3 below summarizes the results of thesimplified version of the lower model (constructedin the same fashion as described in Section 3.1).Predictor ?
pDO is definite 1.59 0.006DO is concrete 1.06 <0.001DO is plural -0.57 0.04followed by a verb 2.29 <0.001normalized verbphrase1.36 0.13 (n.s.
)Len(DO) - Len(IO) -1.37 <0.001Table 3: Fixed effects in the simplified lowermodelCompared to the upper model, fewer predictorsare significant in the lower model.
Everything else1241being equal, a postverbal ditransitive sentence ismore likely to take the pre-IO form (V DO IO) if(a) DO is definite and concrete, (b) IO is singu-lar, (c) DO is shorter than IO, and (d) the ditransi-tive construction is followed by another verb.
Thelast point is illustrated in sentence 5a, which isadapted from a real sentence in the corpus.
In5a, the NP women ?we?
is both the recipient ofthe first verb song ?to send?
and the agent of thesecond verb chi ?to eat?.
Thus, by using a pre-IO form, the NP women is in effect adjacent tothe second verb chi, which might give an advan-tage in sentence processing.
Notice though, if theother form (V IO DO) is used, the sentence is stillgrammatical (see 5b).
(5) a. Tahehaialsosongsentxiaoyesnacksgeitowomechi.eatHe also sent snacks for me to eat.b.
Tahehaialsosongsent(gei)(to)womexiaoyesnackschi.eatHe also sent me snacks to eat.Overall the lower model is not as successfulas the upper model.
The prediction accuracy is87.7% (baseline accuracy is 79.9%; see Table 4).Predictedpre-IO post-IOobserved pre-IO 85 76post-IO 22 618Table 4: Prediction accuracy of the simplifiedlower modelMoreover, cross-validation and the analysis oftigong show that only two factors, the presence ofthe following verb and length difference, are sta-ble across subsets of the data.
In fact, with lengthdifference alone, the model generates correct pre-dictions for 86.8% of the sentences (only 1% lessthan the accuracy reported in Table 4).However, before we hastily conclude thatlength difference is the only thing that matters inthe lower-level variation, it is important to pointout that when the length factor is removed fromthe model, some predictors (such as the accessi-bility of DO) turn out to be significant and themodel still manages to achieve an accuracy of85.3%.
Therefore, a more plausible explanationis that length difference is the strongest predictorsfor lower-level dative variation.
Though the partof variation it accounts for can also be explainedby other predictors, it is more effective in doingso.
Therefore the existence of this variable tendsto mask other predictors in the model.4 Discussion4.1 Comparing he two modelsIn the current study, we propose a two-level hier-archical schema for modeling the variation amongthree major word orders of Chinese dative sen-tences.
On the upper level, there is a distinctionbetween sentences with preverbal DOs and thosewith postverbal DOs.
On the lower level, amongpostverbal sentences, there is a further distinctionbetween pre-IO sentences (i.e.
with prepositionalphrases), and post-IO sentences (i.e.
double ob-ject forms).
This schema is promoted by structuralas well as methodological concerns.Our modeling results show that the two lev-els of variation are indeed characterized by dif-ferent probabilistic patterns, which in turn pro-vide evidence for our original proposal.
As pre-sented in Section 3, the upper-level distinction ismostly conditioned by the semantic features ofthe DO.
However, in the lower-level variation, thetwo best predictors are length difference and thepresence of a following verb.
Overall, the upper-level model is more successful (accuracy = 93.6%,baseline = 67%) than the lower-level model (accu-racy = 87.7%, baseline = 79.9%).A more striking difference between the twomodels is that they exhibit weight effects in op-posite directions.
In both models, length differ-ence between DO and IO plays an important role.Nevertheless, in the upper model, length differ-ence has a positive sign (?
= 0.53), meaning thatthe longer the DO is (compared to the IO), themore likely it is to prepose DO before the verb.Conversely, in the lower-level model, this factorhas a negative sign (?
= - 1.37), which means thatthe longer the DO is (compared to the IO), the lesslikely it is for DO to be before IO.
That is to say,everything else being equal, if a DO is long, it willprobably be preposed before the verb, but if it is1242already after the verb, then it will more likely beplaced after IO, at the end of the construction.The difference in directionality explains why itis only in the lower-level model that the weighteffect overshadows other predictors.
Featureslike pronominality, definiteness, and accessibil-ity are inherently correlated with weight.
Pro-nouns are shorter than full NPs; definite NPs tendto be shorter than indefinite NPs (which oftentake quantifiers and numerals); NPs that have ap-peared before tend to be in shorter forms than theirfirst occurrences.
In both models, a general trendis that NPs that are more prominent in the con-text (e.g.
pronouns, definite NPs, NPs with an-tecedents) tend to occur earlier in the construc-tion.
Thus, in the lower model, the general trendof prominence is confluent with the short beforelong weight effect, but in the upper model, it ispulling away from the long before short weighteffect.
As a result, weight effect only masks se-mantic predictors in the lower model, not in theupper model.4.2 Comparing with English dative variationCompared with Bresnan et als models, the currentresults reveal a number of interesting differencesbetween Chinese and English dative variation.First, the variation phenomenon in Chinese in-volves at least one more major variant, that is,the preverbal word order, which significantly in-creases the complexity of the phenomenon.
Thefact that overall the English model has greater pre-diction accuracy than the Chinese models mighthave to do with the fact that the variation phe-nomenon is more complicated and harder tomodel in Chinese.Second, dative variation in Chinese seems to beless sensitive to semantic features.
If we only con-sider the lower-level variation in Chinese, whichinvolves the same form variants as in English (i.e.V DO IO and V IO DO), the Chinese model isbest predicted by the length difference betweenDO and IO and most other predictors are muted bythe presence of this factor.
In the English model,semantic features are still significant even whenlength difference is controlled.Last but not least, as discussed at length in theprevious section, the two levels of dative variationin Chinese exhibit weight effects in opposite di-rections.
The English variation is also sensitiveto weight, but only in the short before long direc-tion, which is the same as the lower-level variationin Chinese.5 ConclusionIn this work, we present a corpus-based statisti-cal modeling study on Chinese dative variation.In doing so, we show that this new methodology,which combines corpus data and statistical model-ing, is a powerful tool for studying complex vari-ation phenomena in Chinese.
The statistical mod-els built in the current study achieve high accu-racy in predicting surface forms in Chinese dativesentences.
More importantly, the models unveilprobabilistic tendencies in Chinese grammar thatare otherwise hard to notice.A remaining question in the current studyis why would Chinese dative variation exhibitweight effects in both directions.
The answer tothis question awaits further investigation.AcknowledgementWe would like to thank three anonymous review-ers for helpful comments on an earlier version ofthe paper.
We owe special thanks to Joan Bresnanand her colleagues in the Spoken Syntax Lab atStanford University, for sharing working manualsand for valuable discussions.ReferencesBresnan, J.
(2007).
Is syntactic knowledge prob-abilistic?
Experiments with the English dativealternation.
In Featherston, S. and Sternefeld,W., editors, Roots: Linguistics in search of itsevidential base, Studies in generative gramar,pages 77?96.
Mouton de Gruyter, Berlin.Bresnan, J., Cueni, A., Nikitina, T., and Baayen,H.
(2007).
Predicting the dative alternation.
InBoume, G., Kraemer, I., and Zwarts, J., ed-itors, Cognitive foundations of interpretation,pages 69?94.
Royal Netherlands Academy ofScience, Amsterdam.Gahl, S. and Garnsey, S. (2004).
Knowledge ofgrammar, knowledge of usage: Syntactic prob-1243abilities affect pronunciation variation.
Lan-guage, 80(4):748?775.Huang, C., Chen, K., Chang, L., and Hsu, H.(1995).
An introduction to Academia SinicaBalanced Corpus.
[in chinese].
In Proceedingsof ROCLING VIII, pages 81?99.Jurafsky, D. (2003).
Probabilistic modeling inpsycholinguistics: Linguistic comprehensionand production.
In Rens Bod, J. H. and Jannedy,S., editors, Probabilistic Linguistics, pages 39?96.
MIT Press, Cambridge, Massachusetts.Levy, R. (2008).
Expectation-based syntacticcomprehension.
Cognition, 106(3):1126?1177.Li, C. N. and Thompson, S. A.
(1981).
MandarinChinese: A functional reference grammar.
Uni-versity of California Press, Berkeley.Liu, F. (2006).
Dative constructions in Chinese.Language and Linguistics, 7(4):863?904.Liu, F. H. (2007).
Word order variation and basentences in Chinese.
Studies in Language,31(3):649 ?
682.Manning, C. D. (2003).
Probabilistic syntax.
InRens Bod, J. H. and Jannedy, S., editors, Proba-bilistic Linguistics, pages 289?341.
MIT Press,Cambridge, Massachusetts.Margetts, A. and Austin, P. (2007).
Three par-ticipant events in the languages of the world:toward a cross-linguistic typology.
Linguistics,45(3):393?451.Wasow, T. and Arnold, J.
(2003).
Post-verbal con-stituent ordering in english.
In Rohdenburg,G.
and Mondorf, B., editors, Determinants ofGrammatical Variation in English, pages 119?154.
Mouton.AppendicesA Complete verb list 4song ?to send?, tigong ?to provide?, jie ?to lend(to)?, fu ?to pay?, ban ?to award?, banfa ?toaward?, zengsong ?to send (as a gift)?, shang ?to4The verb gei ?to give?
is not included in the list, becauseit has the same form as the coverb gei and therefore has dif-ferent properties than other ditransitive verbs.
Among otherthings, the verb gei cannot take the V DO IO form in Man-darin (e.g.
*gei yiben shu gei wo ?give a book to me?
).award?, jieshao ?to introduce?, huan ?to return?,fa ?to distribute/allocate?,jiao ?to transfer?, ji ?tomail?, liu ?to leave (behind)?, liuxia ?to leave (be-hind)?,reng ?to throw?, diu ?to throw?, diuxia ?tothrow (behind)?, juan ?to donate?, juanzeng ?todonate?, juanxian ?to donate?, bo ?to allocate?,di ?to hand (to)?, zu ?to rent (to)?, fen ?to dis-tribute?, na ?to hand (to)?, dai ?to bring?, dailai?to bring?, jiao ?to teach?, chuan ?to deliver?,chuanran ?to pass around (a disease)?, chuanda?to deliver (a message)?, chuansong ?to deliver?, chuanshou ?to deliver (knowledge)?,ci ?to give(as a reward)?, pei ?to pay (compensation)?B Predictors in the full modelPredictor Codinggenre 1=spoken; 0=writtenverb category 1=canonical transfer;0=otherwisedefiniteness of DO 1=definite; 0=indefinitepronominality of DO 1=pronoun; 0=otherwisenumber of DO 1=plural; 0=singularperson of DO 1=1st and 2nd person;0=otherwiseconcreteness of DO 1=concrete; 0=abstractgivenness of DO 1=given; 0=otherwisequan/num in DO 1=yes; 0=nodefiniteness of IO 1=definite; 0=indefinitepronominality of IO 1=pronoun; 0=otherwisenumber of IO 1=plural; 0=singularperson of IO 1=1st and 2nd person;0=otherwiseconcreteness of IO 1=concrete; 0=abstractgivenness of IO 1=given; 0=otherwisefollowed by another verb 1=yes; 0=noembedded under anotherverb1=yes; 0=nopart of a copular sentence 1=yes; 0=noadverbial phrase after theverb1=yes; 0=noparticle after the verb 1=yes; 0=noquestion form 1=yes; 0=nosentence negation 1=yes; 0=norelativization 1=yes; 0=nonominalization 1=yes; 0=nolog(len(DO)- log(len(IO)) numerical1244
