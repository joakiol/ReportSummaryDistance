Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 875?882, Vancouver, October 2005. c?2005 Association for Computational LinguisticsInducing a multilingual dictionaryfrom a parallel multitext in related languagesDmitriy GenzelDepartment of Computer ScienceBox 1910Brown UniversityProvidence, RI 02912, USAdg@cs.brown.eduAbstractDictionaries and word translation modelsare used by a variety of systems, espe-cially in machine translation.
We builda multilingual dictionary induction systemfor a family of related resource-poor lan-guages.
We assume only the presenceof a single medium-length multitext (theBible).
The techniques rely upon lexicaland syntactic similarity of languages aswell as on the fact that building dictionar-ies for several pairs of languages providesinformation about other pairs.1 Introduction and MotivationModern statistical natural language processing tech-niques require large amounts of human-annotateddata to work well.
For practical reasons, the requiredamount of data exists only for a few languages ofmajor interest, either commercial or governmental.As a result, many languages have very little com-putational research done in them, especially outsidethe borders of the countries in which these languagesare spoken.
Some of these languages are, however,major languages with hundreds of millions of speak-ers.
Of the top 10 most spoken languages, Lin-guistic Data Consortium at University of Pennsyl-vania, the premier U.S. provider of corpora, offerstext corpora only in 7 (The World Factbook (2004),2000 estimate) Only a few of the other languages(French, Arabic, and Czech) have resources pro-vided by LDC.
Many Asian and Eastern Europeanlanguages number tens of millions of speakers, yetvery few of these seem to have any related compu-tational linguistics work, at least as presented at theinternational conferences, such as the ACL.1The situation is not surprising, nor is it likely tosignificantly change in the future.
Luckily, mostof these less-represented languages belong to lan-guage families with several prominent members.
Asa result, some of these languages have siblings withmore resources and published research.
2 Inter-estingly, the better-endowed siblings are not alwaysthe ones with more native speakers, since politicalconsiderations are often more important.3 If oneis able to use the resources available in one lan-guage (henceforth referred to as source) to facilitatethe creation of tools and resource in another, relatedlanguage (target), this problem would be alleviated.This is the ultimate goal of this project, but in thefirst stage we focus on multi-language dictionary in-duction.Building a high-quality dictionary, or even bet-ter, a joint word distribution model over all the lan-guages in a given family is very important, becauseusing such a model one can use a variety of tech-niques to project information across languages, e.g.to parse or to translate.
Building a unified model formore than a pair of languages improves the qualityover building several unrelated pairwise models, be-cause relating them to each other provides additionalinformation.
If we know that word a in language Ahas as its likely translation word b in language B,and b is translated as c in C, then we also know thata is likely to be translated as c, without looking at1The search through ACL Anthology, for e.g., Telugu (?70million speakers) shows only casual mention of the language.2Telugu?s fellow Dravidian language Tamil (?65 millionspeakers) has seen some papers at the ACL3This is the case with Tamil vs. Telugu.875the A to C model.2 Previous WorkThere has been a lot of work done on building dic-tionaries, by using a variety of techniques.
Onegood overview is Melamed (2000).
There is workon lexicon induction using string distance or otherphonetic/orthographic comparison techniques, suchas Mann and Yarowsky (2001) or semantic com-parison using resources such as WordNet (Kondrak,2001).
Such work, however, primarily focuses onfinding cognates, whereas we are interested in trans-lations of all words.
Moreover, while some tech-niques (e.g., Mann and Yarowsky (2001)) use mul-tiple languages, the languages used have resourcessuch as dictionaries between some language pairs.We do not require any dictionaries for any languagepair.An important element of our work is focusing onmore than a pair of languages.
There is an activeresearch area focusing on multi-source translation(e.g., Och and Ney (2001)).
Our setting is the re-verse: we do not use multiple dictionaries in orderto translate, but translate (in a very crude way) inorder to build multiple dictionaries.Many machine translation techniques require dic-tionary building as a step of the process, and there-fore have also attacked this problem.
They use a va-riety of approaches (a good overview is Koehn andKnight (2001)), many of which require advancedtools for both languages which we are not able touse.
They also use bilingual (and to some extentmonolingual) corpora, which we do have available.They do not, however, focus on related languages,and tend to ignore lexical similarity 4, nor are theyable to work on more than a pair of languages at atime.It is also worth noting that there has been someMT work on related languages which explores lan-guage similarity in an opposite way: by using dic-tionaries and tools for both languages, and assum-ing that a near word-for-word approach is reasonable(Hajic et al, 2000).4Much of recent MT research focuses on pairs of languageswhich are not related, such as English-Chinese, English-Arabic,etc.3 Description of the ProblemLet us assume that we have a group of related lan-guages, L1 .
.
.
Ln, and a parallel sentence-alignedmultitext C, with corresponding portions in eachlanguage denoted as C1 .
.
.
Cn.
Such a multitext ex-ists for virtually all the languages in the form of theBible.
Our goal is to create a multilingual dictionaryby learning the joint distribution P (x1 .
.
.
xn)xi?Liwhich is simply the expected frequency of the n-tuple of words in a completely word-aligned mul-titext.
We will approach the problem by learningpairwise language models, although leaving someparameters free, and then combine the models andlearn the remaining free parameters to produce thejoint model.Let us, therefore, assume that we have a set ofmodels {P (x, y|?ij)x?Li,y?Lj}i6=j where ?ij is aparameter vector for pairwise model for languagesLi and Lj .
We would like to learn how to combinethese models in an optimal way.
To solve this prob-lem, let us first consider a simpler and more generalsetting.3.1 Combining Models of Hidden DataLet X be a random variable with distributionPtrue(x), such that no direct observations of it exist.However, we may have some indirect observationsof X and have built several models of X?s distri-bution, {Pi(x|?i)}ni=1, each parameterized by someparameter vector ?i.
Pi also depends on some otherparameters that are fixed.
It is important to note thatthe space of models obtained by varying ?i is only asmall subspace of the probability space.
Our goal isto find a good estimate of Ptrue(x).The main idea is that if some Pi and Pj are close(by some measure) to Ptrue, they have to be closeto each other as well.
We will therefore make theassumption that if some models of X are close toeach other (and we have reason to believe they arefair approximations of the true distribution) they arealso close to the true distribution.
Moreover, wewould like to set the parameters ?i in such a waythat P (xi|?i) is as close to the other models as pos-sible.
This leads us to look for an estimate that isas close to all of our models as possible, under the876optimal values of ?i?s, or more formally:Pest = argminP?
(?)min?1.
.
.min?nd(P?
(?
), P1(?|?1), .
.
.
Pn(?|?n))where d measures the distance between P?
and all thePi under the parameter setting ?i.
Since we have noreason to prefer any of the Pi, we choose the follow-ing symmetric form for d:n?i=1D(P?
(?
)||Pi(?|?i))where D is a reasonable measure of distance be-tween probability distributions.
The most appro-priate and the most commonly used measure insuch cases in the Kullback-Leibler divergence, alsoknown as relative entropy:D(p||q) =?xp(x) logp(x)q(x)It turns out that it is possible to find the optimal P?under these circumstances.
Taking a partial deriva-tive and solving, we obtain:P?
(x) =?ni=1 Pi(x|?i)1/n?x?
?X?ni=1 Pi(x?|?i)1/nSubstituting this value into the expression forfunction d, we obtain the following distance mea-sure between the Pi?s:d?
(P1(X|?1) .
.
.
Pn(X|?n))= minP?
d(P?
, P1(X|?1), .
.
.
Pn(X|?n))= ?
log?x?X?ni=1 Pi(x|?i)1/nThis function is a generalization of the well-known Bhattacharyya distance for two distributions(Bhattacharyya, 1943):b(p, q) =?i?piqiThese results suggest the following Algorithm 1to optimize d (and d?):?
Set al ?i randomly?
Repeat until change in d is very small:?
Compute P?
according to the above for-mula?
For i from 1 to n?
Set ?i in such a way as to minimizeD(P?
(X)||Pi(X|?i))?
Compute d according to the above for-mulaEach step of the algorithm minimizes d. It is alsoeasy to see that minimizing D(P?
(X)||Pi(X|?i)) isthe same as setting the parameters ?i in order to max-imize?x?X Pi(x|?i)P?
(x), which can be interpretedas maximizing the probability under Pi of a cor-pus in which word x appears P?
(x) times.
In otherwords, we are now optimizing Pi(X) given an ob-served corpus of X , which is a much easier problem.In many types of models for Pi the Expectation-Maximization algorithm is able to solve this prob-lem.3.2 Combining Pairwise ModelsFollowing the methods outlined in the previoussection, we can find an optimal joint probabilityP (x1 .
.
.
xn)xi?Li if we are given several modelsPj(x1 .
.
.
xn|?j).
Instead, we have a number of pair-wise models.
Depending on which independence as-sumptions we make, we can define a joint distribu-tion over all the languages in various ways.
For ex-ample, for three languages, A, B, and C, and we canuse the following set of models:P1(A,B,C) = P (A|B)P (B|C)P (C)P2(A,B,C) = P (C|A)P (A|B)P (B)P3(A,B,C) = P (B|C)P (C|A)P (A)andd?(P?
, P1, P2, P3)= D(P?
||P1) + D(P?
||P2) + D(P?
||P3)= 2H(P?
(A,C), P (A,C))+ 2H(P?
(A,B), P (A,B))+ 2H(P?
(B,C), P (B,C)) ?
3H(P?
)?
H(P?
(A), P (A)) ?H(P?
(B), P (B))?
H(P?
(C), P (C))where H(?)
is entropy, H(?, ?)
is cross-entropy, andP?
(A,B) means P?
marginalized to variables A,B.The last three cross-entropy terms involve monolin-gual models which are not parameterized.
The en-tropy term does not involve any of the pairwise dis-tributions.
Therefore, if P?
is fixed, to maximize d?877we need to maximize each of the bilingual cross-entropy terms.This means we can apply the algorithm fromthe previous section with a small modification(Algorithm 2):?
Set al ?ij (for each language pair i, j) ran-domly?
Repeat until change in d is very small:?
Compute Pi for i = 1 .
.
.
k where k is thenumber of the joint models we have cho-sen?
Compute P?
from {Pi}?
For i, j such that i 6= j?
Marginalize P?
to (Li, Lj)?
Set ?ij in such a way as to minimizeD(P?
(Li, Lj)||Pi(Li, Lj |?ij))?
Compute d according to the above for-mulaMost of the ?
parameters in our models can beset by performing EM, and the rest are discrete withonly a few choices and can be maximized over bytrying all combinations of them.4 Building Pairwise ModelsWe now know how to combine pairwise translationmodels with some free parameters.
Let us now dis-cuss how such models might be built.Our goal at this stage is to take a parallel bitextin related languages A and B and produce a jointprobability model P (x, y), where x ?
A, y ?
B.Equivalently, since the models PA(x) and PB(y)are easily estimated by maximum likelihood tech-niques from the bitext, we can estimate PA?B(y|x)or PB?A(x|y).
Without loss of generality, we willbuild PA?B(y|x).The model we are building will have a number offree parameters.
These parameters will be set by thealgorithm discussed above.
In this section we willassume that the parameters are fixed.Our model is a mixture of several components,each discussed in a separate section below:PA?B(y|x) = ?fw(x)PfwA?B(y|x)+ ?bw(x)PbwA?B(y|x)+ ?char(x)PcharA?B(y|x)+ ?pref (x)PprefA?B(y|x)+ ?suf (x)PsufA?B(y|x)+ ?cons(x)PconsA?B(y|x)(1)where all ?s sum up to one.
The ?s are free pa-rameters, although to avoid over-training we tie the?s for x?s with similar frequencies.
These lambdasform a part of the ?ij parameter mentioned previ-ously, where Li = A and Lj = B.The components represent various constraints thatare likely to hold between related languages.4.1 GIZA (forward)This component is in fact GIZA++ software, origi-nally created by John Hopkins University?s SummerWorkshop in 1999, improved by Och (2000).
Thissoftware can be used to create word alignments forsentence-aligned parallel corpora as well as to in-duce a probabilistic dictionary for this language pair.The general approach taken by GIZA is as fol-lows.
Let LA and LB be the portions of the par-allel text in languages A and B respectively, andLA = (xi)i=1...n and LB = (yi)i=1...m. We candefine P (LB|LA) asmaxPA?BmaxPalignsn?i=1m?j=1PA?B (yj |xi)Paligns (xi|j)The GIZA software does the maximization bybuilding a variety of models, mostly described byBrown et al (1993).
GIZA can be tuned in variousways, most importantly by choosing which modelsto run and for how many iterations.
We treat theseparameters as free, to be set alng with the rest at alater stage.As a side effect of GIZA?s optimization, we obtainthe PA?B(y|x) that maximizes the above expres-sion.
It is quite reasonable to believe that a modelof this sort is also a good model for our purposes.This model is what we refer to as PfwA?B(y|x) inthe model overview.GIZA?s approach is not, however, perfect.
GIZAbuilds several models, some quite complex, yet it878does not use all the information available to it, no-tably the lexical similarity between the languages.Furthermore, GIZA tries to map words (especiallyrare ones) into other words if possible, even if thesentence has no direct translation for the word inquestion.These problems are addressed by using othermodels, described in the following sections.4.2 GIZA (backward)In the previous section we discussed using GIZA totry to optimize P (LB|LA).
It is, however, equallyreasonable to try to optimize P (LA|LB) instead.
Ifwe do so, we can obtain PfwB?A(x|y) that pro-duces maximal probability for P (LA|LB).
We,however need a model of PA?B(y|x).
This is easilyobtained by using Bayes?
rule:PbwA?B(y|x) =PfwB?A(x|y)PB(y)PA(x)which requires us to have PB(y) and PA(x).
Thesemodels can be estimated directly from LB and LA,by using maximum likelihood estimators:PA(x) =?i ?
(xi, x)nandPB(y) =?i ?
(yi, y)mwhere ?
(x, y) is the Kronecker?s delta function,which is equal to 1 if its arguments are equal, andto 0 otherwise.4.3 Character-based modelThis and the following models all rely on having amodel of PA?B(y|x) to start from.
In practice itmeans that this component is estimated followingthe previous components and uses the models theyprovide as a starting point.The basic idea behind this model is that in relatedlanguages words are also related.
If we have a modelPc of translating characters in language A into char-acters in language B, we can define the model fortranslating entire words.Let word x in language A consists of charactersx1 through xn, and word y in language B consist ofcharacters y1 through ym.Let us define (the unnormalized) character model:Puchar(y|x) = Pcharlen(y|x,m)Plength(m|x)i.e., estimating the length of y first, and y itself af-terward.
We make an independence assumption thatthe length of y depends only on length of x, and areable to estimate the second term above easily.
Thefirst term is harder to estimate.First, let us consider the case where lengths of xand y are the same (m = n).
Then,Pcharlen(y|x, n) =n?i=1Pc(yi|xi)Let yj be word y with j?s character removed.
Letus now consider the case when m > n. We define(recursively):Pcharlen(y|x,m) =m?i=11mPcharlen(yi|x,m?
1)Similarly, if n > m:Pcharlen(y|x) =n?i=11nPcharlen(y|xi,m)It is easy to see that this is a valid probabilitymodel over all sequences of characters.
However,y is not a random sequence of characters, but a wordin language B, moreover, it is a word that can serveas a potential translation of word x.
So, to define aproper distribution over words y given a word x anda set of possible translations of x, T (x)Pchar(y|x) = Puchar (y|x, y ?
T (x))= ?y?
?T (x)Puchar(y,y?T (x)|x)?y?
?T (x)Puchar(y?|x)This is the complete definition of Pchar, exceptfor the fact that we are implicitly relying upon thecharacter-mapping model, Pc, which we need tosomehow obtain.
To obtain it, we rely upon GIZAagain.
As we have seen, GIZA can find a good word-mapping model if it has a bitext to work from.
If wehave a PA?B word-mapping model of some sort, itis equivalent to having a parallel bitext with words yand x treated as a sequence of characters, instead ofindivisible tokens.
Each (x, y) word pair would oc-cur PA?B(x, y) times in this corpus.
GIZA wouldthen provide us with the Pc model we need, by opti-mizing the probability B language part of the modelgiven the language A part.8794.4 Prefix ModelThis model and the two models that follow are builton the same principle.
Let there be a function f :A ?
CA and a function g : B ?
CB .
These func-tions group words in A and B into some finite set ofclasses.
If we have some PA?B(y|x) to start with,we can definePfgA?B(y|x)= P (y|g(y))P (g(y)|f(x))P (f(x)|x)= P (y)?x?:f(x?)=f(x)?y?:g(y?
)=g(y)P (x?,y?)(?x?:f(x?
)=f(x)P (x?))(?y?:g(y?
)=g(y)P (y?
))For the prefix model, we rely upon the followingidea: words that have a common prefix often tend tobe related.
Related words probably should translateas related words in the other language as well.
Inother words, we are trying to capture word-level se-mantic information.
So we define the following setof f and g functions:fn(x) = prefix(x, n)gm(y) = prefix(y,m)where n and m are free parameters, whose values wewill determine later.
We therefore define PprefA?Bas Pfg with f and g specified above.4.5 Suffix ModelSimilarly to a prefix model mentioned above, it isalso useful to have a suffix model.
Words that havethe same suffixes are likely to be in the same gram-matical case or share some morphological featurewhich may persist across languages.
In either case,if a strong relationship exists between the result-ing classes, it provides good evidence to give higherlikelihood to the word belonging to these classes.
Itis worth noting that this feature (unlike the previousone) is unlikely to be helpful in a setting where lan-guages are not related.The functions f and g are defined based on a set ofsuffixes SA and SB which are learned automatically.f(x) is defined as the longest possible suffix of xthat is in the set SA, and g is defined similarly, forSB .The sets SA and SB are built as follows.
We startwith all one-character suffixes.
We then considertwo-letter suffixes.
We add a suffix to the list if itoccurs much more often than can be expected basedon the frequency of its first letter in the penultimateposition, times the frequency of its second letter inthe last position.
We then proceed in a similar wayfor three-letter suffixes.
The threshold value is a freeparameter of this model.4.6 Constituency ModelIf we had information about constituent boundariesin either language, it would have been useful tomake a model favoring alignments that do not crossconstituent boundaries.
We do not have this infor-mation at this point.
We can assume, however, thatany sequence of three words is a constituent of sorts,and build a model based on that assumption.As before, let LA = (xi)i=1...n and LB =(yi)i=1...m. Let us define as CA(i) a tripleof words (xi?1, xi, xi+1) and as CB(j) a triple(yj?1, yj , yj+1).
If we have some model PA?B , wecan definePCA?CB (j|i) =1CPA?B(yj?1|xi?1)PA?B(yj |xi)?
PA?B(yj+1|xi+1)where C is the sum over j of the above products, andserves to normalize the distribution.PconsA?B(y|x)=?ni=1?mj=1P (y|CB(j))PCA?CB (j|i)P (CA(i)|x)=?i:xi=x?mj=1 P (y|CB(j))PCA?CB (j|i)= 1?j=1?
(yj ,y)?i:xi=x?j:yi=y PCA?CB (j|i)5 EvaluationThe output of the system so far is a multi-lingualword translation model.
We will evaluate it by pro-ducing a tri-lingual dictionary (Russian-Ukrainian-Belorussian), picking a highest probability transla-tion for each word, from the corresponding Bibles.Unfortunately, we do not have a good hand-built tri-lingual dictionary to compare it to, but only onegood bilingual one, Russian-Ukrainian5.
We willtherefore take the Russian-Ukrainian portion of ourdictionary and compare it to the hand-built one.Our evaluation metric is the number of entries thatmatch between these dictionaries.
If a word has sev-eral translations in the hand-built dictionary, match-5The lack of such dictionaries is precisely why we do thiswork880ing any of them counts as correct.
It is worth not-ing that for all the dictionaries we generate, the to-tal number of entries is the same, since all the wordsthat occur in the source portion of the corpus have anentry.
In other words, precision and recall are pro-portional to each other and to our evaluation metric.Not all of the words that occur in our dictionaryoccur in the hand-built dictionary and vice versa.
Anabsolute upper limit of performance, therefore, forthis evaluation measure is the number of left-hand-side entries that occur in both dictionaries.In fact, we cannot hope to achieve this number.First, because the dictionary translation of the wordin question might never occur in the corpus.
Second,even if it does, but never co-occurs in the same sen-tence as its translation, we will not have any basisto propose it as a translation.6.
Therefore we havea ?achievable upper limit?, the number of wordsthat have their ?correct?
translation co-occur at leastonce.
We will compare our performance to this up-per limit.Since there is no manual tuning involved we donot have a development set, and use the whole biblefor training (the dictionary is used as a test set, asdescribed above).We evaluate the performance of the model withjust the GIZA component as the baseline, and addall the other components in turn.
There are two pos-sible models to evaluate at each step.
The pairwisemodel is the model given in equation 1 under theparameter setting given by Algorithm 2, with Be-lorussian used as a third language.
The joint modelis the full model over these three languages as es-timated by Algorithm 2.
In either case we pick ahighest probability Ukrainian word as a translationof a given Russian word.The results for Russian-Ukrainian bibles are pre-sented in Table 1.
The ?oracle?
setting is the set-ting obtained by tuning on the test set (the dictio-nary).
We see that using a third language to tuneworks just as well, obtaining the true global max-imum for the model.
Moreover, the joint model(which is more flexible than the model in Equation1) does even better.
This was unexpected for us, be-6Strictly speaking, we might be able to infer the word?s exis-tence in some cases, by performing morphological analysis andproposing a word we have not seen, but this seems too hard atthe momentTable 1: Evaluation for Russian-Ukrainian (with Be-lorussian to tune)Stage Pair JointForward (baseline) 62.3% 71.7%Forward+chars 77.1% 84.2%Forward+chars+backward 81.3% 84.1%Fw+chars+bw+prefix 83.5% 84.5%Fw+chars+bw+prefix+suffix 84.5% 85%Fw+chars+bw+pref+suf+const 84.5% 85.2%?Oracle?
setting for ?
?s 84.6%Table 2: Evaluation for Russian-Ukrainian (with Be-lorussian and Polish)Tuned by Pair JointBelorussian (prev.
table) 84.5% 85.2% &Polish 84.6% 78.6%Both 84.5% 85.2%?Oracle?
tuning 84.5%cause the joint model relies on three pairwise mod-els equally, and Russian-Belorussian and Ukrainian-Belorussian models are bound to be less reliable forRussian-Ukrainian evaluation.
It appears, however,that our Belorussian bible is translated directly fromRussian rather than original languages, and parallelsRussian text more than could be expected.To insure our results are not affected by this factwe also try Polish separately and in combinationwith Belorussian (i.e.
a model over 4 languages),as shown in Table 2.These results demonstrate that the joint modelis not as good for Polish, but it still finds theoptimal parameter setting.
This leads us to pro-pose the following extension: let us marginalizejoint Russian-Ukrainian-Belorussian model into justRussian-Ukrainian, and add this model as yet an-other component to Equation 1.
Now we cannot useBelorussian as a third language, but we can use Pol-ish, which we know works just as well for tuning.The resulting performance for the model is 85.7%,our best result to date.8816 Discussion and Future WorkWe have built a system for multi-dictionary in-duction from parallel corpora which significantlyimproves quality over the standard existing tool(GIZA) by taking advantage of the fact that lan-guages are related and we have a group of morethan two of them.
Because the system attempts tobe completely agnostic about the languages it workson, it might be used successfully on many languagegroups, requiring almost no linguistic knowledge onthe part of the user.
Only the prefix and suffix com-ponents are somewhat language-specific, but eventhey are sufficiently general to work, with varyingdegree of success, on most inflective and agglutina-tive languages (which form a large majority of lan-guages).
For generality, we would also need a modelof infixes, for languages such as Hebrew or Arabic.We must admit, however, that we have not testedour approach on other language families yet.
It isour short term plan to test our model on several Ro-mance languages, e.g.
Spanish, Portuguese, French.Looking at the first lines of Table 1, one can seethat using more than a pair of languages with amodel using only a small feature set can dramat-ically improve performance (compare second andthird columns), while able to find the optimal val-ues for all internal parameters.As discussed in the introduction, the ultimate goalof this project is to produce tools, such as a parser,for languages which lack them.
Several approachesare possible, all involving the use of the dictionarywe built.
While working on this project, we wouldno longer be treating all languages in the same way.We would use the tools available for that language tofurther improve the performance of pairwise mod-els involving that language and, indirectly, even thepairs not involving this language.
Using these tools,we may be able to improve the word translationmodel even further, simply as a side effect.Once we build a high-quality dictionary for a spe-cial domain such as the Bible, it might be possible toexpand to a more general setting by mining the Webfor potential parallel texts.Our technique is limited in the coverage of theresulting dictionary which can only contain wordswhich occur in our corpus.
Whatever the corpusmay be, however, it will include the most commonwords in the target language.
These are the wordsthat tend to vary the most between related (and evenunrelated) languages.
The relatively rare words (e.g.domain-specific and technical terms) can often betranslated simply by inferring morphological rulestransforming words of one language into another.Thus, one may expand the dictionary coverage us-ing non-parallel texts in both languages, or even injust one language if its morphology is sufficientlyregular.ReferencesThe Central Intelligence Agency.
2004.
The world fact-book.A.
Bhattacharyya.
1943.
On a measure of divergence be-tween two statistical populations defined by their prob-ability distributions.
Bull.
Calcutta Math.
Soc., 35:99?109.P.F.
Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.J.
Hajic, J. Hric, and V. Kubon.
2000.
Machine transla-tion of very close languages.
In Proccedings of the 6thApplied Natural Language Processing Conference.P.
Koehn and K. Knight.
2001.
Knowledge sourcesfor word-level translation models.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.G.
Kondrak.
2001.
Identifying cognates by phoneticand semantic similarity.
In Proceedings of the SecondMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics, Pittsburgh, PA,pages 103?110.G.
Mann and D. Yarowsky.
2001.
Multipath transla-tion lexicon induction via bridge languages.
In Pro-ceedings of the Second Meeting of the North AmericanChapter of the Association for Computational Linguis-tics, Pittsburgh, PA, pages 151?158.I.
D. Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26:221?249, June.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proceedings of the 38th Annual Meet-ing of the Association for Computational Linguistics,pages 440?447, Hongkong, China, October.F.
J. Och and H. Ney.
2001.
Statistical multi-sourcetranslation.
In Proccedings of MT Summit VIII, pages253?258.882
