Adaptive Chinese Word Segmentation withOnline Passive-Aggressive AlgorithmWenjun GaoSchool of Computer ScienceFudan UniversityShanghai, Chinawjgao616@gmail.comXipeng QiuSchool of Computer ScienceFudan UniversityShanghai, Chinaxpqiu@fudan.edu.cnXuanjing HuangSchool of Computer ScienceFudan UniversityShanghai, Chinaxjhuang@fudan.edu.cnAbstractIn this paper, we describe our system1for CIPS-SIGHAN-2010 bake-off task ofChinese word segmentation, which fo-cused on the cross-domain performanceof Chinese word segmentation algorithms.We use the online passive-aggressive al-gorithm with domain invariant informa-tion for cross-domain Chinese word seg-mentation.1 IntroductionIn recent years, Chinese word segmentation(CWS) has undergone great development (Xue,2003; Peng et al, 2004).
The popular method isto regard word segmentation as a sequence label-ing problems.
The goal of sequence labeling is toassign labels to all elements of a sequence.Due to the exponential size of the outputspace, sequence labeling problems tend to bemore challenging than the conventional classifi-cation problems.
Many algorithms have beenproposed and the progress has been encourag-ing, such as SVMstruct (Tsochantaridis et al,2004), conditional random fields (CRF) (Laffertyet al, 2001), maximum margin Markov networks(M3N) (Taskar et al, 2003) and so on.
After yearsof intensive researches, Chinese word segmenta-tion achieves a quite high precision.
However, theperformance of segmentation is not so satisfyingfor out-of-domain text.There are two domains in domain adaptionproblem, a source domain and a target domain.When we use the machine learning methods for1Available at http://code.google.com/p/fudannlp/Chinese word segmentation, we assume that train-ing and test data are drawn from the same distri-bution.
This assumption underlies both theoreti-cal analysis and experimental evaluations of learn-ing algorithms.
However, the assumption doesnot hold for domain adaptation(Ben-David et al,2007; Blitzer et al, 2006).
The challenge is thedifference of distribution between the source andtarget domains.In this paper, we use online margin max-imization algorithm and domain invariant fea-tures for domain adaptive CWS.
The online learn-ing algorithm is Passive-Aggressive (PA) algo-rithm(Crammer et al, 2006), which passively ac-cepts a solution whose loss is zero, while it ag-gressively forces the new prototype vector to stayas close as possible to the one previously learned.The rest of the paper is organized as follows.Section 2 introduces the related works.
Then wedescribe our algorithm in section 3 and 4.
Thefeature templates are described in section 5.
Sec-tion 6 gives the experimental analysis.
Section 7concludes the paper.2 Related WorksThere are several approaches to deal with the do-main adaption problem.The first approach is to use semi-supervisedlearning (Zhu, 2005).The second approach is to incorporate super-vised learning with domain invariant information.The third approach is to improve the presentmodel with a few labeled domain data.Altun et al (2006) investigated structured clas-sification in a semi-supervised setting.
They pre-sented a discriminative approach that utilizes theintrinsic geometry of inputs revealed by unlabeleddata points and we derive a maximum-margin for-mulation of semi-supervised learning for struc-tured variables.Self-training (Zhu, 2005) is also a popular tech-nology.
In self-training a classifier is first trainedwith the small amount of labeled data.
The clas-sifier is then used to classify the unlabeled data.Typically the most confident unlabeled points, to-gether with their predicted labels, are added to thetraining set.
The classifier is re-trained and theprocedure repeated.
Note the classifier uses itsown predictions to teach itself.
Yarowsky (1995)uses self-training for word sense disambiguation,e.g.
deciding whether the word plant means a liv-ing organism or a factory in a given context.Zhao and Kit (2008) integrated unsupervisedsegmentation and CRF learning for Chinese wordsegmentation and named entity recognition.
Theyfound word accessory variance (Feng et al, 2004)is useful to CWS.3 Online Passive-Aggressive AlgorithmSequence labeling, the task of assigning labelsy = y1, .
.
.
, yL to an input sequence x =x1, .
.
.
, xL.Give a sample (x,y), we define the feature is?(x,y).
Thus, we can label x with a score func-tion,y?
= argmaxzF (w,?
(x, z)), (1)where w is the parameter of function F (?
).The score function of our algorithm is linearfunction.Given an example (x,y), y?
is denoted as theincorrect label with the highest score,y?
= argmaxz 6=ywT?
(x, z).
(2)The margin ?
(w; (x,y)) is defined as?
(w; (x,y)) = wT?(x,y)?wT?
(x, y?).
(3)Thus, we calculate the hinge loss.`(w; (x,y) ={0, ?
(w; (x,y)) > 11?
?
(w; (x,y)), otherwise(4)We use the online PA learning algorithm tolearn the weights of features.
In round t, we findnew weight vector wt+1 bywt+1 = arg minw?Rn12||w ?wt||2 + C ?
?,s.t.
`(w; (xt,yt)) <= ?
and ?
>= 0 (5)where C is a positive parameter which controlsthe influence of the slack term on the objectivefunction.The algorithms goal is to achieve a margin atleast 1 as often as possible, thus the Hamming lossis also reduced indirectly.
On rounds where thealgorithm attains a margin less than 1 it suffers aninstantaneous loss.We abbreviate `(wt; (x, y)) to `t. If `t = 0then wt itself satisfies the constraint in Eq.
(5)and is clearly the optimal solution.
We thereforeconcentrate on the case where `t > 0.First, we define the Lagrangian of the optimiza-tion problem in Eq.
(5) to beL(w, ?, ?, ?)
= 12||w ?wt||2 + C ?
?+ ?
(`t ?
?)?
??s.t.
?
>= 0, ?
>= 0.
(6)where ?, ?
is a Lagrange multiplier.Setting the partial derivatives of L with respectto the elements of ?
to zero gives?
+ ?
= C. (7)The gradient of w should be zero,w ?
wt ?
?(?
(x,y) ?
?
(x, y?))
= 0, (8)we getw = wt + ?(?
(x,y) ?
?
(x, y?)).
(9)Substitute Eq.
(7) and Eq.
(9) to dual objectivefunction Eq.
(6), we getL(?)
= ?12||?(?(x,y)?
?
(x, y?))||2?
?
(wtT (?(x,y)?
?
(x, y?))
+ ?
(10)Differentiate with ?, and set it to zero, we get?||?(x,y)?
?
(x, y?
)||2+wtT (?(x,y)?
?
(x, y?))?
1 = 0.
(11)So,??
= 1?wtT (?(x,y)?
?
(x, y?))||?(x,y)?
?
(x, y?)||2.
(12)From ?
+ ?
= C, we know that ?
< C, so???
= min(C, ??).
(13)Finally, we get update strategy,wt+1 = wt + ???(?(x,y)?
?
(x, y?)).
(14)Our final algorithm is shown in Algorithm 1.
Inorder to avoiding overfitting, the averaging tech-nology is employed.input : training data set:(xn,yn), n = 1, ?
?
?
, N , andparameters: C,Koutput: wInitialize: cw?
0,;for k = 0 ?
?
?K ?
1 dow0 ?
0 ;for t = 0 ?
?
?T ?
1 doreceive an example (xt,yt);predict:y?t = argmaxz 6=yt?wt,?
(xt, z)?
;calculate `(w; (x,y));update wt+1 with Eq.
(14);endcw = cw +wT ;endw = cw/K ;Algorithm 1: Labelwise Margin Maxi-mization Algorithm4 InferenceThe PA algorithm is used to learn the weights offeatures in training procedure.
In inference pro-cedure, we use Viterbi algorithm to calculate themaximum score label.Let ?
(n) be the best score of the partial labelsequence ending with yn.
The idea of the Viterbialgorithm is to use dynamic programming to com-pute ?(n):?
(n) = maxn?1(?(n?
1) +wT?
(x, yn, yn?1))(15)+wt?
(x, yn)Using this recursive definition, we can evalu-ate ?
(N) for all yN , where N is the input length.This results in the identification of the best labelsequence.The computational cost of the Viterbi algorithmis O(NL2), where L is the number of labels.5 Feature TemplatesAll feature templates used in this paper are shownin Table 1.
C represents a Chinese character whilethe subscript of C indicates its position in the sen-tence relative to the current character, whose sub-script is 0.
T represents the character-based tag:?B?, ?B2?, ?B3?, ?M?, ?E?
and ?S?, which repre-sent the beginning, second, third, middle, end orsingle character of a word respectively.The type of character includes: digital, letter,punctuation and other.We also use the word accessor variance for do-main adaption.
Word accessor variance (AV) wasproposed by (Feng et al, 2004) and was used toevaluate how independently a string is used, andthus how likely it is that the string can be a word.The accessor variety of a string s of more than onecharacter is defined asAV (s) = min{Lav(s), Rav(s)} (16)Lav(s) is called the left accessor variety and isdefined as the number of distinct characters (pre-decessors) except ?S?
that precede s plus the num-ber of distinct sentences of which s appears atthe beginning.
Similarly, the right accessor va-riety Rav(s) is defined as the number of distinctcharacters (successors) except ?E?
that succeed splus the number of distinct sentences in which sappears at the end.
The characters ?S?
and ?E?are defined as the begin and end of a sentence.The word accessor variance was found effectivefor CWS with unsegmented text (Zhao and Kit,2008).Table 1: Feature templatesCi, T0, (i = ?1, 0, 1, 2)Ci, Ci+1, T0, (i = ?2,?1, 0, 1)T?1,0Tc: Type of CharacterAV : word accessor variance6 CIPS-SIGHAN-2010 BakeoffCIPS-SIGHAN-2010 bake-off task of Chineseword segmentation focused on the cross-domainperformance of Chinese word segmentation algo-rithms.
There are two subtasks for this evaluation:(1) Word Segmentation for Simplified ChineseText;(2) Word Segmentation for Traditional ChineseText.The test corpus of each subtask covers four do-mains: literature, computer science, medicine andfinance.We participate in closed training evaluation ofboth subtasks.Firstly, we calculate the word accessor varianceAVL(s)of the continuous string s from labeledcorpus.
Here, we set the largest length of strings to be 4.Secondly, we train our model with feature tem-ples and AVL(s).Thirdly, when we process the different domainunlabeled corpus, we recalculate the word ac-cessory variance AVU (s) from the correspondingcorpus.Fourthly, we segment the domain corpus withnew word accessory variance AVU (s) instead ofAVL(s).The results are shown in Table 2 and 3.
Theresults show our method has a poor performancein OOV ( Out-Of-Vocabulary) word.The running environment is shown in Table 4.Table 4: Experimental environmentOS Win 2003CPU Intel Xeon 2.0GMemory 4GWe set the max iterative number is 20.
Our run-ning time is shown in Table 5.
?s?
represents sec-ond, ?chars?
is the number of Chinese character,and ?MB?
is the megabyte.
In practice, we foundthe system can achieve the same performance af-ter 7 loops.
Therefore, we just need less half thetime in Table 5 actually.7 ConclusionIn this paper, we describe our system in CIPS-SIGHAN-2010 bake-off task of Chinese wordsegmentation.
Although our method just achievea consequence of being average and not outstand-ing, it has an advantage of faster training thanother batch learning algorithm, such as CRF andM3N.In the future, we wish to improve our methodin the following aspects.
Firstly, we will investi-gate more effective domain invariant feature rep-resentation.
Secondly, we will integrate our algo-rithm with self-training and other semi-supervisedlearning methods.AcknowledgmentsThis work was (partially) funded by 863 Pro-gram (No.
2009AA01A346), 973 Program (No.2010CB327906), and Shanghai Science and Tech-nology Development Funds (No.
08511500302).ReferencesAltun, Y., D. McAllester, and M. Belkin.
2006.
Max-imum margin semi-supervised learning for struc-tured variables.
Advances in neural informationprocessing systems, 18:33.Ben-David, S., J. Blitzer, K. Crammer, and F. Pereira.2007.
Analysis of representations for domain adap-tation.
Advances in Neural Information ProcessingSystems, 19:137.Blitzer, J., R. McDonald, and F. Pereira.
2006.Domain adaptation with structural correspondencelearning.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Process-ing, pages 120?128.
Association for ComputationalLinguistics.Crammer, Koby, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.Table 2: Evaluation results on simplified corpusR P F1 OOV RR IV RRBest 0.945 0.946 0.946 0.816 0.954Literature Our 0.915 0.925 0.92 0.577 0.94Best 0.953 0.95 0.951 0.827 0.975Computer Our 0.934 0.919 0.926 0.739 0.969Best 0.942 0.936 0.939 0.75 0.965Medicine Our 0.927 0.924 0.925 0.714 0.953Best 0.959 0.96 0.959 0.827 0.972Finance Our 0.94 0.942 0.941 0.719 0.961Table 3: Evaluation results on traditional corpusR P F1 OOV RR IV RRBest 0.942 0.942 0.942 0.788 0.958Literature Our 0.869 0.91 0.889 0.698 0.887Best 0.948 0.957 0.952 0.666 0.977Computer Our 0.933 0.949 0.941 0.791 0.948Best 0.953 0.957 0.955 0.798 0.966Medicine Our 0.908 0.932 0.92 0.771 0.919Best 0.964 0.962 0.963 0.812 0.975Finance Our 00.925 0.939 0.932 0.793 0.935Table 5: Execution time of training and test phase.Task A B C DTraining Simp 817.2s 795.6s 774.0s 792.0sTrad 903.6s 889.2s 885.6s 874.8sTest 20327 chars/s, or 17.97 s/MBFeng, H., K. Chen, X. Deng, and W. Zheng.
2004.
Ac-cessor variety criteria for chinese word extraction.Computational Linguistics, 30(1):75?93.Lafferty, John D., Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In ICML ?01: Proceedings of theEighteenth International Conference on MachineLearning.Peng, F., F. Feng, and A. McCallum.
2004.
Chinesesegmentation and new word detection using condi-tional random fields.
Proceedings of the 20th inter-national conference on Computational Linguistics.Taskar, Ben, Carlos Guestrin, and Daphne Koller.2003.
Max-margin markov networks.
In Proceed-ings of Neural Information Processing Systems.Tsochantaridis, I., T. Hofmann, T. Joachims, and Y Al-tun.
2004.
Support vector machine learning for in-terdependent and structured output spaces.
In Pro-ceedings of the International Conference on Ma-chine Learning(ICML).Xue, N. 2003.
Chinese word segmentation as charac-ter tagging.
Computational Linguistics and ChineseLanguage Processing, 8(1):29?48.Yarowsky, D. 1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceed-ings of the 33rd annual meeting on Association forComputational Linguistics, pages 189?196.
Associ-ation for Computational Linguistics.Zhao, H. and C. Kit.
2008.
Unsupervised segmenta-tion helps supervised learning of character taggingfor word segmentation and named entity recogni-tion.
In The Sixth SIGHAN Workshop on ChineseLanguage Processing, pages 106?111.
Citeseer.Zhu, Xiaojin.
2005.
Semi-supervised learningliterature survey.
Technical Report 1530, Com-puter Sciences, University of Wisconsin-Madison.http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
