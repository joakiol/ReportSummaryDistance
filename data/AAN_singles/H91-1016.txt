EVALUATION OF THE CMU ATIS SYSTEMWayne WardSchoo l  o f  Computer  Sc ienceCarneg ie  Me l lon  Un ivers i tyP i t t sburgh ,  Pa  15213ABSTRACTThe CMU Phoenix system is an experiment in understanding spontaneousspeech.
It has been implemented for the Air Travel Information Servicetask.
In this task, casual users are asked to obtain information from adatabase of air travel information.
Users are not given a vocabulary,grammar or set of sentences to read.
They compose queries themselves ina spontaneous manner.
This task presents peech recognizers with manynew problems compared to the Resource Management task.
Not only isthe speech not fluent, but the vocabulary and grammar are open.
Also,the task is not just to produce a transcription, but to produce an action,retrieve data from the database.
Taking such actions requires parsing and"understanding" the utteraoce.
Word error rate is not as important asutterance understanding rate.Phoenix attempts to deal with phenomena that occur in spontaneousspeech.
Unknown words, restarts, repeats, and poody formed or unusualgrammar are common is spontaneous speech and are very disruptive tostandard recognizers.
These events lead to misrecognitions which oftencause a total parse failure.
Our strategy is to apply grammatical con-straints at the phrase level and to use semantic rather than lexicalgrammars.
Semantics provide more constraint han parts of speech andmust ultimately be delt with in order to take actions.
Applying constraintsat the phrase level is more flexible than recognizing sentences as a wholewhile providing much more constraint than word-spotting, Restarts andrepeats are most often between phase occurences, o individual phrasescan still be recognized correctly.
Poorly constructed grammar oftenconsists of well-formed phrases, and is often semantically well-formed.
Itis only syntactically incorrect.
We associate phrases by frame-basedsemantics.
Phrases represent word strings that can fill slots in frames.
Theslots represent information which the frame is able to act on.The current Phoenix system uses a bigram language model with theSphinx speech recognition system.
The top-scoring word string is passedto a flexible frame-based parser, The parser assigns phrases (word strings)from the input to slots in frames.
The slots represent information contentneeded for the frame.
A beam of frame hypotheses i produced and thebest scoring one is used to produce an SQL query.INTRODUCTIONUnderstanding spontaneous speech presents everal problemsnot found in transcribing read speech input.
Spontaneous speechis often not fluent.
It contains stutters, filled pauses, restarts,repeats, interjections, etc.
Casual users do not know the lexiconand grammar used by the system.
It is therefore very difficuk fora speech understanding system to achieve good coverage of thelexicon and grammar that subjects might use.
Also, the task ofthe system is not just to produce a transcription, but to produce anaction.
Taking such actions requires parsing and "understanding"the utterance.
Word error rate is not as important as utteranceunderstanding rate.The Air Travel Information Service task is being used byseveral DARPA-funded sites to develop and evaluate speech un-derstanding systems for database query tasks.
In the ATIS task,novice users are asked to perform a task that requires gettinginformation from the Air Travel database.
This database containsinformation about flights and their fares, airports, aircraft, etc.The only input to the system is by voice.
Users compose thequestions themselves, and are allowed to phrase the queries anyway they choose.
No explicit grammar or lexicon is given to thesubject.At Carnegie Mellon University, we have been developing asystem, called Phoenix, to understand spontaneous peech\[1\] \[2\] \[3\].
We have implemented an initial version of thissystem for the ATIS task, This paper presents the design of thePhoenix system and its current status.
We also report systemevaluation results for the DARPA Feb91 test.THE PHOENIX  SYSTEMSome problems posed by spontaneous speech are:?
User noise - breath noise, filled pauses and other usergenerated noise?
Environment noise - door slams, phone rings, etc.?
Out-of-vocabulary words - The subject says wordsthat the system doesn't know.?
Grammatical coverage - Subjects often use gram-matically ill-formed utterances and restart and repeatphrases.Phoenix address these problems by using non-verbal soundmodels, an out-of-vocabulary word model and flexible parsing.Non.Verba l  Sound Mode lsModels for sounds other than speech have been shown tosignificantly increase performance of HMM-based recognizersfor noisy input.
\[2\] \[4\] In this technique, additional models areadded to the system that represent non-verbal sounds, just asword models represent verbal sounds.
These models are trainedexactly as ff they were word models, but using the noisy input.Thus, sounds that are not words are allowed to map onto tokensthat are also not words.Out -o f -vocabu lary  Word  Mode lThis module has not yet been implemented, In order to dealwith out-of-vocabulary words, we will use a technique ssentiallylike the one presented by BBN.
\[5\] We will create an explicitmodel for out-of-vocabulary words.
This model allows anytriphone (context dependent phone) to follow any other triphone(given of course that the context is the same) with a bigrami01probability model.
The bigrams are to be trained from a largedictionary of English pronunciations.Flexible ParsingOur concept of flexible parsing combines ~ame based seman-tics with a semantic phrase grammar.
We use a frame basedparser similar to the DYPAR parser used by Carbonell, et al toprocess ill-formed text, \[6\] and the MINDS system previouslydeveloped at CMU.
\[7\] Semantic information is represented in aset of frames.
Each blame contains a set of slots representingpieces of information.
In order to fill the slots in the frames, weuse a partitioned semantic phrase grammar.
Each slot type isrepresented by a separate finite-state network which specifies allways of saying the meaning represented by the slot.
The gram-mar is a semantic grammar, non-terminals are semantic onceptsinstead of parts of speech.
The grammar is also written so thatphrases can stand alone (be recognized by a net) as well as beingembedded in a sentence.
Strings of phrases which do not form agrammatical English sentence are still parsed by the system.
Thegrammar is compiled into a set of finite-state networks.
It ispartitioned in the sense that, instead of one big network, there aremany small networks.
Networks can "call" other networks,thereby significantly reducing the overall size of the system.These networks are used to perform pattern matches against inputword strings.
This general approach as been described in earlierpapers.
\[1\] \[3\]The operation of the parser can be viewed as "phrase spotting".A beam of possible interpretations are pursued simultaneously.An interpretation is a frame with some of its slots filled.
Thef'mite-state networks perform pattern matches against he inputstring.
When a phrase is recognized, it attempts to extend allcurrent interpretations.
That is, it is assigned to slots in activeinterpretations that it can fill.
Phrases assigned to slots in thesame interpretation are not allowed to overlap.
In ease of overlap,multiple interpretations are produced.
When two interpretationsfor the same frame end with the same phrase, the lower scoringone is pruned.
This amounts to dynamic programming on seriesof phrases.
The score for an interpretation is the number of inputwords that it accounts for.
At the end of the utterance, the bestscoring interpretation is output.In our system, slots (pattern specifications) can be at differentlevels in a hierarchy.
Higher level slots can contain the infor-mation specified in several lower level slots.
These higher levelforms allow more specific relations between the lower level slotsto be specified.
In the utterance "leaving denver and arriving inboston after five pro", "leaving denver" is a \[deparUloc\] and"arriving in boston" is an \[arrive loci, but there is ambiguity as towhether "after 5 pro" is \[depart_time_range\] or\[arrive_timejange\].
The existence of the higher level slot\[ARRIVE\] allows this to be resolved.
One rewrite for the slot\[ARRIVE\] is (\[arrive loc\] \[arrive_time range\]) in which the twolower level slots are specfically associated.
Thus two interpreta-tions for this utterance are produced,leaving denver and arrivingin boston after 5 pmi\[ depart_loc \] leaving denver\[arrive_loc\] arriving in boston\[depart time_range\] after 5 pm2\[depart_loc \] leaving denver\[ARRIVE\]\[arrive_loc\] arriving in boston\[arrive time range\] after 5 pmIn picking which interpretation is correct, higher level slots arepreferred to lower level ones because the associations betweenconcepts is more tightly bound, thus the second (correct) inter-pretation is picked here.Our strategy is to apply grammatical constraints at the phraselevel and to associate phrases in frames.
Phrases represent wordstrings that can fill slots in frames.
The slots represent infor-mation which, taken together, the frame is able to act on.
Wealso use semantic rather than lexical grammars, Semanticsprovide more constraint than parts of speech and must ultimatelybe delt with in order to take actions.
Applying constraints at thephrase level is more flexible than recognizing sentences as awhole while providing much more constraint than word-spotting.Restarts and repeats are most often between phases, so individualphrases can still be recognized correctly.
Poorly constructedgrammar often consists of well-formed phrases, and is oftensemantically well-formed.
It is only syntactically incorrect.System StructureThe overall structure of our current system is shown in Figure1.
We use the Sphinx system as our recognizer module \[8\].Sphinx is a speaker independent continuous peech recognitionsystem.Curremly the recognizer and parser are not integrated.
Thespeech input is digitized and vector quantized and then passed tothe Sphinx recognizer.
The recognizer uses a bigram languagemodel to produce asingle best word string from the speech input.This word string is then passed to the frame-based parser whichassigns word slxings to slots in frames as explained above.The slots in the best scoring frame are then used to buildobjects.
In this process, all dates, times, names, etc.
are mappedinto a standard form for the routines that build the databasequery.
The objects represent the information that was extractedfrom the utterance.
There is also a currently active set of objectswhich represent constraints from previous utterances.
The newobjects created from the frame are merged with the current set ofobjects.
At this step ellipsis and anaphora re resolved.
Resolu-tion of ellipsis and anaphora is relatively simple in this system.The slots in frames are semantic, thus we know the type of objectneeded for the resolution.
For ellipsis, we add the new objects.For anaphora, we simply have to check that an object of that typealready exists.Each frame has an associated function.
After the information is102I % No WeightedInput % True % False Answer ScoreTranscript 80.7 16.6 2.8 64.0spe h 61.4 26.9 11.7 34.5Table 1: Phoenix results for Feb91 Class-A test setSrcWordStringSubs  Del Ins Error19.3 6.8 2.6 28.779.1 79.1Table 2: Recognition error rates for Class-Aextracted and objects built, the frame function is executed.
Thisfunction takes the action appropriate for the frame.
It builds adatabase query (if appropriate) from objects, sends it to SYBASE(the DataBase Management System we use) and displays outputto the user.RESULTSOur current system has a lexicon of 710 words and uses abigram language model of perplexity 49.
Six noise models areincluded in the lexicon.
We used the version of Sphinx producedby Hun \[9\], which includes between-word triphone models.
Thevocabulary-independent phone models generated by Hon\[9\] were used to compile the word models for the system.
Notask specific acoustic training was done.
We have not yet addedthe out-of-vocabulary models to the system.The DARPA ATIS0 training set consists of approximately 700utterances gathered by Texas Instruments and distributed byNIST.
This data was gathered and distributed before the June1990 evaluations.
The data was gathered using a "wizard"paradigm.
Subjects were asked to perform an ATIS scenario.They were given a task to perform and told that they were to usea speech understanding computer to get information.
A hiddenexperimenter listened to the subjects and provided the appropriateinformation from the database.
The transcripts from this set wereused to train our language model.
This includes the bigrammodel for the recognizer and the grammar for the parser.
Sincethis amount of data is not nearly enough to train a languagemodel, we chose to "pad" our bigrams.
Bigrams were generatedbased on tag pairs rather than word pairs.
Words in our lexiconwere put into categories represented bytags.
The June90 trainingcorpus was tagged according to this mapping.
We then generateda word-pair file from the Phoenix finite-state ATIS grammar,This file was used to initiafize the tag bigram counts.
The taggedcorpus was then used to add to the counts and the bigram file wasgenerated.
It is a "padded" bigram in the sense that the grammaris used to insure a count of at least 1 for all "legal" tag pairs.
Thisprocedure yielded a bigrarn language model which has perplexity39 for the ATIS0 test set.The DARPA ATIS1 test (for the February 1991 evaluations)has two mandatory test sets, the class A set and the class D1 set.Structure of PhoenixA Spoken Language Understanding System;,, .-h,---speech: "show me...ah.,.l want to see all the flights toDenver after two pro"digitize: 16 KHz, 16 bit samplesDSPVQ codes: A vector of 3 bytes, each 1O msS~axwords: "show me I want to see all flights toDenver after two pro"Error CorrectingParser \[list\]: I want o tee| frame: \[flights\]: all flights\[arrive loci: to DenverDialog-Based \[depart_time range\]: after two pmcan~r~m,~: \[mlOt, l: mght,\[errive_loc\]: "DEN"ATIS \[depart Joe\]: "PIT"Application \[depar t_time._range\]: 1400 2400~?~ SQL: select airline_code, flight_nmnberfrom flight_tableTravel where (from_alrport ='l iT'and toairport ='DEN') Databaseand (departure_time > 1400)Figure 1: Structure of the Phoenix systemThe class A set contains 145 utterances that are processed in-dividually without context.
All utterances in the test set were"Class-A", that is, answerable, context independent and with nodisfluencies.
The class D1 set contains 38 utterance pairs.
Theseare intended to test dialog capability.
The first utterance of a pairis a Class-A utterance that sets the context for the second.
Onlyscores for the second utterance are reported for this set.We processed both transcript and speech input for each set.Tables 1-4 show the results of this evaluation.Utterances were scored correct if the answer output by the103No WeightedInput True False Answer ScoreTranscript 60.5 34.2 5.2 26.3Speech 39.4 55.2 5.2 -15.8Table 3: Phoenix results for Feb91 Class-D1 test setSrc SubsWord 17.6String 77.6Table 4: Recognition error rates for Class-D1Del8.6Ins0.7Error26.977.6Source % of Total ErrorsGrammatical CoverageSemantic CoverageWrong CAS FieldUnanswerableApplication Coding Errors2520251020Table 5: Analysis of Errors for Class-A NLsystem matched the reference answer for the utterance.
The refer-ence answer is database output, not a word string.
Systems areallowed to output a NO_ANSWER response, indicating that theutterance was misunderstood.
Any output hat was not correct orNOANSWER was scored incorrect.
The Weighted Score iscomputed as ( 1- ( 2*percent false + percentNO_ANSWER) ).Table 1 shows the results for class A utterances.
For these, thesystem produced the correct answer for 80.7 percent of thetranscript input and 61.4 percent of the speech input.
The perfor-mance for transcript input reflects the grammatical nd semanticcoverage of the parser and application program.
The perfor-mance for the speech input reflects additional errors made in therecognition stage.
Recognition performance for these utterancesis shown in Table 2.
Word substitutions, deletions and insertionsare summed to give the word error measure of 28.7 percent.
Astring error rate of 79 percent means that only twenty one percentof the utterances contained no errors.
However, 61 percent of theutterances gave correct answers.
This illustrates the ability of theparser to handle minor misrecognitions i  the recognized string.The D1 test set is designed to provide a test of dialogcapability.
The utterances are specified in pairs.
The first ut-terance is processed normally and is used to set the context forthe second utterance of the pair.
Missing the first utterance canlead to incorrectly interpreting the second.
Tables 3 and 4 showthe understanding performance and speech recognition rates forthe D1 test set.
While the recognition results are comparable tothose for set A, the understanding performance is significantlyworse.
This is due in large part to utterances in which we missedthe first utterance, causing the context for the second to be wrong.We feel that recognition error rates for spontaneous input willimprove considerably with the addition of out-of-vocabularymodels and with better lexical and grammatical coverage.ERROR ANALYS ISIn order to interpret the performance of the system, it is usefulto look at the source of the errors.
Table 5 shows the percentageof errors from various ources.Twenty five percent of our errors were a result of lack ofgrammatical coverage.
This includes unknown words for con-cepts that the system has.
For example, the system knew daynames (Monday, Tuesday, ere) but nov plural day names (Mon-days, etc) since these had not been seen in the training data.
Thiscategory also contains errors where all words were known but thespecific word sequence used did not match any phrase patterns.Twenty percent of the errors were due to a lack of semanticcoverage.
In this case, there were no frames for the type ofquestion being asked or no slots for the type of information beingprovided.
For example, one utterance requested "a generaldescription of the aircraft".
Our" system allows you to ask aboutspecific attributes of an aircraft but does not have the notion of"general description" which maps to a subset of these attributes.Twenty five percent of the errors were due to outputting thewrong field from the database for the CAS answer.
In these cases,the utterance was correctly understood and a reasonable answerwas output, but it was not the specific answer equired by theCAS specifications.
For example, when asked for cities near theDenver airport, we output he city name "DENVER" rather thanthe city code "DDEN" as required by CAS.104Ten percent of the errors were due to utterances that oursystem considered unanswerable.
For CAS evaluation runs, wemap all system error messages toa NO_ANSWER response.
Forexample, one utterance asked for ground transportation from At-lanta to Baltimore.
Our system recognized that this was outsidethe abilities of the database and generated an error message thatwas mapped to NO~kNSWER.
The reference answer was thenull list "0".The other twenty percent of the errors were due to coding bugsin the back end.The first two categories (grammatical and semantic errors) areerrors in the "understanding" part of the system.
Forty five per-cent of our total errors were due to not correctly interpreting theinput.
The other fifty five percent of the errors were generationerrors.
That is, the utterance was correctly interpreted but thecorrect answer was not generated.FUTURE PLANSOur next step in the evolution of the Phoenix system will be tointegrate the recognition and parsing.
We will use the patternmatching networks to drive the word Izansifions in the recog-nition search rather than a bigram grammar.ACKNOWLEDGMENTSThis research was sponsored by the Defense AdvancedResearch Projects Agency (DOD), ARPA Order No.
5167, undercontract number N00039-85-C-0163.
The views and conclusionscontained in this document are those of the authors and shouldnot be interpreted as representing the official policies, eitherexpressed or implied, of the Defense Advanced Research ProjectsAgency or the US Government.REFERENCES1.
Ward, W., "Understanding Spontaneous Speech", Proceedings ofthe DARPA Speech and Natural Language Workshop, 1989, pp.137, 141.2.
Ward, W., "Modelling Non-verbal Sounds for Speech Recog-nition", Proceedings of the DARPA Speech and Natural Lan-guage Workshop, 1989, pp.
47, 50.3.
Ward, W., "The CMU Air Travel Information Service: Under-standing Spontaneous Speech", Proceedings of the DARPASpeech and Natural Language Workshop, 1990.4.
Wilpon, J.G., Rabiner, L.R., Lee, C.H., Goldman, E.R.,"Automatic Recognition of Vocabulary Word Sets in Uncon-strained Speech Using Hidden Markov Models", in press Trans-actions ASSP , 1990.5.
Asadi, A., Schwartz, R., Makhoul, J., "Automatic Detection OfNew Words In A Large Vocabulmy Continuous Speech Recog-nition System", Proceedings ofthe DARPA Speech and NaturalLanguage Workshop, 1989. pp.
263, 265.6.
Carbonell, J.G.
and Hayes, P.J., "Recovery Strategies for ParsingExtragrammafical L nguage", Tech.
report CMU-CS-84-107,Carnegie-Mellon U iversity Computer Science Technical Report,1984.7.
Young, S.R., Hauptmann, A. G., Ward, W. H., Smith, E. T. andWemer, P., "High Level Knowledge Sources in Usable SpeechRecognition Systems", Communications f the ACM, Vol.
32, No.2, 1989, pp.
183-194.8.
Lee, K.-F., Automatic Speech Recognition: The Development ofthe SPHINX System, Kluwer Academic Pubfishers, Boston, 1989.9.
Hon, H,W., Lee, K.F., Weide, R., "Towards Speech RecognitionWithout Vocabulary-Specific Training", Proceedings of theDARPA Speech and Natural Language Workshop, 1989, pp.
271,275.105
