Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 129?139,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Joint Model for Extended Semantic Role LabelingVivek Srikumar and Dan RothUniversity of Illinois, Urbana-ChampaignUrbana, IL 61801{vsrikum2, danr}@illinois.eduAbstractThis paper presents a model that extends se-mantic role labeling.
Existing approaches in-dependently analyze relations expressed byverb predicates or those expressed as nominal-izations.
However, sentences express relationsvia other linguistic phenomena as well.
Fur-thermore, these phenomena interact with eachother, thus restricting the structures they artic-ulate.
In this paper, we use this intuition todefine a joint inference model that capturesthe inter-dependencies between verb seman-tic role labeling and relations expressed us-ing prepositions.
The scarcity of jointly la-beled data presents a crucial technical chal-lenge for learning a joint model.
The keystrength of our model is that we use existingstructure predictors as black boxes.
By en-forcing consistency constraints between theirpredictions, we show improvements in the per-formance of both tasks without retraining theindividual models.1 IntroductionThe identification of semantic relations betweensentence constituents has been an important task inNLP research.
It finds applications in various naturallanguage understanding tasks that require complexinference going beyond the surface representation.In the literature, semantic role extraction has beenstudied mostly in the context of verb predicates, us-ing the Propbank annotation of Palmer et al (2005),and also for nominal predicates, using the Nombankcorpus of Meyers et al (2004).However, sentences express semantic relationsthrough other linguistic phenomena.
For example,consider the following sentence:(1) The field goal by Brien changed the game in thefourth quarter.Verb centered semantic role labeling would identifythe arguments of the predicate change as (a) Thefield goal by Brien (A0, the causer of the change),(b) the game (A1, the thing changing), and (c) inthe fourth quarter (temporal modifier).
However,this does not tell us that the scorer of the field goalwas Brien, which is expressed by the preposition by.Also, note that the in indicates a temporal relation,which overlaps with the verb?s analysis.In this paper, we propose an extension of the stan-dard semantic role labeling task to include relationsexpressed by lexical items other than verbs and nom-inalizations.
Further, we argue that there are interac-tions between the different phenomena which sug-gest that there is a benefit in studying them together.However, one key challenge is that large jointly la-beled corpora do not exist.
This motivates the needfor novel learning and inference schemes that ad-dress the data problem and can still benefit from theinteractions among the phenomena.This paper has two main contributions.1.
From the machine learning standpoint, we pro-pose a joint inference scheme to combine exist-ing structure predictors for multiple linguisticphenomena.
We do so using hard constraintsthat involve only the labels of the phenomena.The strength of our model is that it is easily129extensible, since adding new phenomena doesnot require fully retraining the joint model fromscratch.
Furthermore, our approach minimizesthe need for extensive jointly labeled corporaand, instead, uses existing predictors as blackboxes.2.
From an NLP perspective, we motivate the ex-tension of semantic role labeling beyond verbsand nominalizations.
We instantiate our jointmodel for the case of extracting preposition andverb relations together.
Our model uses exist-ing systems that identify verb semantic rolesand preposition object roles and jointly pre-dicts the output of the two systems in the pres-ence of linguistic constraints that enforce co-herence between the predictions.
We show thatusing constraints to combine models improvesthe performance on both tasks.
Furthermore,since the constraints depend only on the labelsof the two tasks and not on any specific dataset,our experiments also demonstrate that enforc-ing them allows for better domain adaptation.The rest of the paper is organized as follows: Wemotivate the need for extending semantic role label-ing and the necessity for joint inference in Section 2.In Section 3, we describe the component verb SRLand preposition role systems.
The global model isdefined in Section 4.
Section 5 provides details onthe coherence constraints we use and demonstratesthe effectiveness of the joint model through experi-ments.
Section 6 discusses our approach in compar-ison to existing work and Section 7 provides con-cluding remarks.2 Problem Definition and MotivationSemantic Role Labeling has been extensively stud-ied in the context of verbs and nominalizations.While this analysis is crucial to understanding asentence, it is clear that in many natural languagesentences, information is conveyed via other lexi-cal items.
Consider, for example, the following sen-tences:(2) Einstein?s theory of relativity changed physics.
(3) The plays of Shakespeare are widely read.
(4) The bus, which was heading for Nairobiin Kenya, crashed in the Kabale district ofUganda.The examples contain information that cannot becaptured by analyzing the verbs and the nominaliza-tions.
In sentence (2), the possessive form tells usthat the theory of relativity was discovered by Ein-stein.
Furthermore, the theory is on the subject ofrelativity.
The usage of the preposition of is dif-ferent in sentence (3), where it indicates a creator-creation relationship.
In the last sentence, the samepreposition tells us that the Kabale district is locatedin Uganda.
Prepositions, compound nouns, posses-sives, adjectival forms and punctuation marks of-ten express relations, the identification of which iscrucial for text understanding tasks like recognizingtextual entailment, paraphrasing and question an-swering.The relations expressed by different linguisticphenomena often overlap.
For example, consider thefollowing sentence:(5) Construction of the library began in 1968.The relation expressed by the nominalization con-struction recognizes the library as the argument ofthe predicate construct.
However, the same analy-sis can also be obtained by identifying the sense ofthe preposition of, which tells us that the subject ofthe preposition is a nominalization of the underlyingverb.
A similar redundancy can be observed withanalyses of the verb began and the preposition in.The above example motivates the following key in-tuition: The correct interpretation of a sentence isthe one that gives a consistent analysis across allthe linguistic phenomena expressed in it.An inference mechanism that simultaneously pre-dicts the structure for different phenomena shouldaccount for consistency between the phenomena.
Amodel designed to address this has the followingdesiderata:1.
It should account for the dependencies betweenphenomena.2.
It should be extensible to allow easy addition ofnew linguistic phenomena.1303.
It should be able to leverage existing state-of-the-art models with minimal use of jointly la-beled data, which is expensive to obtain.Systems that are trained on each task indepen-dently do not account for the interplay betweenthem.
One approach for tackling this is to definepipelines, where the predictions for one of the tasksacts as the input for another.
However, a pipelinedoes not capture the two-way dependency betweenthe tasks.
Training a fully joint model from scratchis also unrealistic because it requires text that is an-notated with all the tasks, thus making joint train-ing implausible from a learning theoretic perspective(See Punyakanok et al (2005) for a discussion aboutthe learning theoretic requirements of joint training.
)3 Tasks and Individual SystemsBefore defining our proposed model that capturesthe requirements listed in the previous section, weintroduce the tasks we consider and their indepen-dently trained systems that we improve using thejoint system.
Though the model proposed here isgeneral and can be extended to several linguisticphenomena, in this paper, we focus on relations ex-pressed by verbs and prepositions.
This section de-scribes the tasks, the data sets we used for our exper-iments and the current state-of-the-art systems forthese tasks.We use the following sentence as our running ex-ample to illustrate the phenomena: The companycalculated the price trends on the major stock mar-kets on Monday.3.1 Preposition RelationsPrepositions indicate a relation between the attach-ment point of the preposition and its object.
As wehave seen, the same preposition can indicate dif-ferent types of relations.
In the literature, the pol-ysemy of prepositions is addressed by The Prepo-sition Project1 of Litkowski and Hargraves (2005),which is a large lexical resource for English that la-bels prepositions with their sense.
This sense inven-tory formed the basis of the SemEval-2007 task ofpreposition word sense disambiguation of Litkowskiand Hargraves (2007).
In our example, the first on1http://www.clres.com/prepositions.htmlwould be labeled with the sense 8(3) which identifiesthe object of the preposition as the topic, while thesecond instance would be labeled as 17(8), whichindicates that argument is the day of the occurrence.The preposition sense inventory, while useful toidentify the fine grained distinctions between prepo-sition usage, defines a unique sense label for eachpreposition by indexing the definitions of the prepo-sitions in the Oxford Dictionary of English.
For ex-ample, in the phrase at noon, the at would be labeledwith the sense 2(2), while the preposition in I willsee you in an hour will be labeled 4(3).
Note thatboth these (and also the second on in our running ex-ample) indicate a temporal relation, but are assigneddifferent labels based on the preposition.
To counterthis problem we collapsed preposition senses thatare semantically similar to define a new label space,which we refer to as Preposition Roles.We retrained classifiers for preposition sense forthe new label space.
Before describing the prepo-sition role dataset, we briefly describe the datasetsand the features for the sense problem.
The bestperforming system at the SemEval-2007 shared taskof preposition sense disambiguation (Ye and Bald-win (2007)) achieves a mean precision of 69.3% forpredicting the fine grained senses.
Tratz and Hovy(2009) and Hovy et al (2010) attained significantimprovements in performance using features derivedfrom the preposition?s neighbors in the parse tree.We extended the feature set defined in the formerfor our independent system.
Table 1 summarizes therules for identifying the syntactically related wordsfor each preposition.
We used dependencies fromthe easy-first dependency parser of Goldberg and El-hadad (2010).For each word extracted from these rules, the fea-tures include the word itself, its lemma, the POStag, synonyms and hypernyms of the first WordNetsense and an indicator for capitalization.
These fea-tures improved the accuracy of sense identificationto 75.1% on the SemEval test set.
In addition, wealso added the following new features for each word:1.
Indicators for gerunds and nominalizations ofverbs.2.
The named entity tag (Person, Location or Or-ganization) associated with a word, if any.
We131Id.
Feature1.
Head noun/verb that dominates thepreposition along with its modifiers2.
Head noun/verb that is dominated bythe preposition along with its modifiers3.
Subject, negator and object(s) of theimmediately dominating verb4.
Heads of sibling prepositions5.
Words withing a window of 5 centeredat the prepositionTable 1: Features for preposition relation from Tratz andHovy (2009).
These rules were used to identify syntacti-cally related words for each preposition.used the state-of-the-art named entity tagger ofRatinov and Roth (2009) to label the text.3.
Gazetteer features, which are active if a word isa part of a phrase that belongs to a gazetteer list.We used the gazetteer lists which were usedby the NER system.
We also used the CBCword clusters of Pantel and Lin (2002) as ad-ditional gazetteers and Brown cluster featuresas used by Ratinov and Roth (2009) and Koo etal.
(2008).Dahlmeier et al (2009) annotated senses for theprepositions at, for, in, of, on, to and with in the sec-tions 2-4 and 23 of the Wall Street Journal portion ofthe Penn Treebank2.
We trained sense classifiers onboth datasets using the Averaged Perceptron algo-rithm with the one-vs-all scheme using the LearningBased Java framework of Rizzolo and Roth (2010)3.Table 2 reports the performance of our sense disam-biguation systems for the Treebank prepositions.As mentioned earlier, we collapsed the sense la-bels onto the newly defined preposition role labels.Table 3 shows this label set alng with frequenciesof the labels in the Treebank dataset.
According tothis labeling scheme, the first on in our running ex-ample will be labeled TOPIC and the second one will2This dataset does not annotate all prepositions and re-stricts itself mainly to prepositions that start a Propbank ar-gument.
The data is available at http://nlp.comp.nus.edu.sg/corpora3Learning Based Java can be downloaded from http://cogcomp.cs.illinois.edu.Test setTrain Treebank Sec.
23 SemEvalPenn Treebank 61.41 38.22SemEval 47.00 78.25Table 2: Preposition sense performance.
This table re-ports accuracy of sense prediction on the prepositions thathave been annotated for the Penn Treebank dataset.Role Train TestACTIVITY 57 23ATTRIBUTE 119 51BENEFICIARY 78 17CAUSE 255 116CONCOMITANT 156 74ENDCONDITION 88 66EXPERIENCER 88 42INSTRUMENT 37 19LOCATION 1141 414MEDIUMOFCOMMUNICATION 39 30NUMERIC/LEVEL 301 174OBJECTOFVERB 365 112OTHER 65 49PARTWHOLE 485 133PARTICIPANT/ACCOMPANIER 122 58PHYSICALSUPPORT 32 18POSSESSOR 195 56PROFESSIONALASPECT 24 10RECIPIENT 150 70SPECIES 240 58TEMPORAL 582 270TOPIC 148 54Table 3: Preposition role data statistics for the Penn Tree-bank preposition dataset.be labeled TEMPORAL4.
We re-trained the sensedisambiguation system to predict preposition roles.When trained on the Treebank data, our system at-tains an accuracy of 67.82% on Section 23 of theTreebank.
We use this system as our independentbaseline for preposition role identification.3.2 Verb SRLThe goal of verb Semantic Role Labeling (SRL)is to identify the predicate-argument structure de-fined by verbs in sentences.
The CoNLL SharedTasks of 2004 and 2005 (See Carreras and Ma`rquez4The mapping from the preposition senses to the roles de-fines a new dataset and is available for download at http://cogcomp.cs.illinois.edu/.132(2004), Carreras and Ma`rquez (2005)) studied theidentification of the predicate-argument structure ofverbs using the PropBank corpus of Palmer et al(2005).
Punyakanok et al (2008) and Toutanova etal.
(2008) used global inference to ensure that thepredictions across all arguments of the same predi-cate are coherent.
We re-implemented the system ofPunyakanok et al (2008), which we briefly describehere, to serve as our baseline verb semantic role la-beler 5.
We refer the reader to the original paper forfurther details.The verb SRL system of Punyakanok et al (2008)consists of four stages ?
candidate generation, argu-ment identification, argument classification and in-ference.
The candidate generation stage involves us-ing the heuristic of Xue and Palmer (2004) to gener-ate an over-complete set of argument candidates foreach predicate.
The identification stage uses a clas-sifier to prune the candidates.
In the argument clas-sification step, the candidates that remain after theidentification step are assigned scores for the SRLarguments using a multiclass classifier.
One of thelabels of the classifier is ?, which indicates that thecandidate is, in fact, not an argument.
The inferencestep produces a combined prediction for all argu-ment candidates of a verb proposition by enforcingglobal constraints.The inference enforces the following structuraland linguistic constraints: (1) Each candidate canhave at most one label.
(2) No duplicate core argu-ments.
(3) No overlapping or embedding arguments.
(4) Given the predicate, some argument classes areillegal.
(5) If a candidate is labeled as an R-arg,then there should be one labeled as arg.
(6) If acandidate is labeled as a C-arg, there should be onelabeled arg that occurs before the C-arg.Instead of using the identifier to filter candidatesfor the classifier, in our SRL system, we addedthe identifier to the global inference and enforcedconsistency constraints between the identifier andthe argument classifier predictions ?
the identifiershould predict that a candidate is an argument if,and only if, the argument classifier does not predictthe label ?.
This change is in keeping with the ideaof using joint inference to combine independently5The verb SRL system be downloaded from http://cogcomp.cs.illinois.edu/page/softwarelearned systems, in this case, the argument identifierand the role classifier.
Furthermore, we do not needto explicitly tune the identifier for high recall.We phrase the inference task as an integer lin-ear program (ILP) following the approach devel-oped in Roth and Yih (2004).
Integer linear pro-grams were used by Roth and Yih (2005) to add gen-eral constraints for inference with conditional ran-dom fields.
ILPs have since been used successfullyin many NLP applications involving complex struc-tures ?
Punyakanok et al (2008) for semantic rolelabeling, Riedel and Clarke (2006) and Martins et al(2009) for dependency parsing and several others6.Let vCi,a be the Boolean indicator variable that de-notes that the ith argument candidate for a predicateis assigned a label a and let ?Ci,a represent the scoreassigned by the argument classifier for this decision.Similarly, let vIi denote the identifier decision for theith argument candidate of the predicate and ?Ii de-note its identifier score.
Then, the objective of infer-ence is to maximize the total score of the assignmentmaxvC ,vI?i,a?Ci,avCi,a +?i?Ii vIi (1)Here, vC and vI denote all the argument classifierand identifier variables respectively.
This maximiza-tion is subject to the constraints described above,which can be transformed to linear (in)equalities.We denote these constraints as CSRL.
In additionto CSRL which were defined by Punyakanok et al(2008), we also have the constraints linking the pre-dictions of the identifier and classifier:vCv,i,?
+ vIv,i = 1; ?v, i.
(2)Inference in our baseline SRL system is, thus, themaximization of the objective defined in (1) sub-ject to constraints CSRL, the identifier-classifier con-straints defined in (2) and the restriction of the vari-ables to take values in {0, 1}.To train the classifiers, we used parse trees fromthe Charniak and Johnson (2005) parser with the6The primary advantage of using ILP for inference is thatthis representation enables us to add arbitrary coherence con-straints between the phenomena.
If the underlying optimizationproblem itself is tractable, then so is the corresponding integerprogram.
However, other approaches to solve the constrainedmaximization problem can also be used for inference.133same feature representation as in the original sys-tem.
We trained the classifiers on the standardPropbank training set using the one-vs-all extensionof the average Perceptron algorithm.
As with thepreposition roles, we implemented our system usingLearning Based Java of Rizzolo and Roth (2010).We normalized all classifier scores using the soft-max function.
Compared to the 76.29% F1 scorereported by Punyakanok et al (2008) using singleparse tree predictions from the parser, our systemobtained 76.22% F1 score on section 23 of the PennTreebank.4 A Joint Model for Verbs andPrepositionsWe now introduce our model that captures the needsidentified in Section 2.
The approach we developin this paper follows the one proposed by Roth andYih (2004) of training individual models and com-bining them at inference time.
Our joint modelis a Constrained Conditional Model (See Chang etal.
(2011)), which allows us to build upon existinglearned models using declarative constraints.We represent our component inference problemsas integer linear program instances.
As we saw inSection 3.2, the inference for SRL is instantiated asan ILP problem.
The problem of predicting prepo-sition roles can be easily transformed into an ILPinstance.
Let vRp,r denote the decision variable thatencodes the prediction that the preposition p is as-signed a role r and let ?Rp,r denote its score.
LetvR denote all the role variables for a sentence.
Thenrole prediction is equivalent to the following maxi-mization problem:maxvR?p,r?Rp,r ?
vRp,r (3)subj.
to ?rvRp,r = 1, ?p (4)vRp,r ?
{0, 1}, ?p, r. (5)In general, let p denote a linguistic structure pre-diction task of interest and let P denote all suchtasks.
Let Zp denote the set of labels that the partsof the structure associated with phenomenon p cantake.
For example, for the SRL argument classifica-tion component, the parts of the structure are all thecandidates that need to be labeled for a given sen-tence and the set Zp is the set of all argument labels.For each phenomenon p ?
P , we use vp to denoteits set of inference variables for a given sentence.Each inference variable vpZ,y ?
vp corresponds tothe prediction that the part y has the label Z in thefinal structure.
Each variable is associated with ascore ?pZ,y that is obtained from a learned score pre-dictor.
Let Cp denote the structural constraints thatare ?local?
to the phenomenon.
Thus, for verb SRL,these would be the constraints defined in the previ-ous section, and for preposition role, the only localconstraint would be the constraint (4) defined above.The independent inference problem for the phe-nomenon p is the following integer program:maxvp?Z?Zp?vpvpZ,y ?
?pZ,y, (6)subj.
to Cp(vp), (7)vpZ,y ?
{0, 1}, ?vpZ,y.
(8)As a technical point, this defines one inferenceproblem per sentence, rather than per predicateas in the verb SRL system of Punyakanok et al(2008).
This simple extension enabled Surdeanu etal.
(2007) to study the impact of incorporating cross-predicate constraints for verb SRL.
In this work, thisextension allows us to incorporate cross-phenomenainference.4.1 Joint inferenceWe consider the problem of jointly predicting sev-eral phenomena incorporating linguistic knowledgethat enforce consistency between the output labels.Suppose p1 and p2 are two phenomena.
If zp11 is a la-bel associated with the former and zp21 , zp22 , ?
?
?
arelabels associated with the latter, we consider con-straints of the formzp11 ?
zp21 ?
zp22 ?
?
?
?
?
zp2n (9)We expand this language of constraints by allowingthe specification of pre-conditions for a constraint toapply.
This allows us to enforce constraints of theform ?If an argument that starts with the preposi-tion ?at?
is labeled AM-TMP, then the prepositioncan be labeled either NUMERIC/LEVEL or TEMPO-RAL.?
This constraint is universally quantified for134all arguments that satisfy the precondition of start-ing with the preposition at.Given a first-order constraint in this form and aninput sentence, suppose the inference variable vp11 isa grounding of zp11 and vp21 , vp22 , ?
?
?
are groundingsof the right hand labels such that the preconditionsare satisfied, then the constraint can be phrased asthe following linear inequality.
?vp11 +?ivp2i ?
0In the context of the preposition role and verbSRL, we consider constraints between labels for apreposition and SRL argument candidates that beginwith that preposition.
This restriction forms the pre-condition for all the joint constraints considered inthis paper.
Since the joint constraints involve onlythe labels, they can be derived either manually fromthe definition of the tasks or using statistical rela-tion learning techniques.
In addition to mining con-straints of the form (9), we also use manually spec-ified joint constraints.
The constraints used in ourexperiments are described further in Section 5.In general, let J denote a set of pairwise jointconstraints.
The joint inference problem can bephrased as that of maximizing the score of the as-signment subject to the structural constraints of eachphenomenon (Cp) and the joint linguistic constraints(J).
However, since, the individual tasks were nottrained on the same datasets, the scoring functionsneed not be in the same numeric scale.
In our model,each labelZ for a phenomenon p is associated with ascoring function ?pZ,y for a part y.
To scale the scor-ing functions, we associate each label with a param-eter ?pZ .
This gives us the following integer linearprogram for joint inference:maxv?p?P?Z?Zp?pZ(?ypvpZ,y ?
?pZ,y), (10)subj.
to Cp(vp), ?p ?
P (11)J(v), (12)vpZ,y ?
{0, 1}, ?vpZ,y.
(13)Here, v is the vector of inference variables whichis obtained by stacking all the inference variables ofeach phenomena.For our experiments, we use a cutting plane solverto solve the integer linear program as in Riedel(2009).
This allows us to solve the inference prob-lem without explicitly having to instantiate all thejoint constraints.4.2 Learning to rescale the individual systemsGiven the individual models and the constraints, weonly need to learn the scaling parameters ?pZ .
Notethat the number of scaling parameters is the totalnumber of labels.
When we jointly predict verb SRLand preposition role, we have 22 preposition roles(from table 3), one SRL identifier label and 54 SRLargument classifier labels.
Thus we learn only 77parameters for our joint model.
This means that weonly need a very small dataset that is jointly anno-tated with all the phenomena.We use the Structure Perceptron of Collins (2002)to learn the scaling weights.
Note that for learningthe scaling weights, we need each label to be associ-ated with a real-valued feature.
Given an assignmentof the inference variables v, the value of the featurecorresponding to the label Z of task p is given by thesum of scores of all parts in the structure for p thathave been assigned this label, i.e.
?ypvpZ,y??pZ,y.
Thisfeature is computed for the gold and the predictedstructures and is used for updating the weights.5 ExperimentsIn this section, we describe our experimental setupand evaluate the performance of our approach.
Theresearch question addressed by the experiments isthe following: Given independently trained systemsfor verb SRL and preposition roles, can their per-formance be improved using joint inference betweenthe two tasks?
To address this, we report the resultsof the following two experiments:1.
First, we compare the joint system against thebaseline systems and with pipelines in both di-rections.
In this setting, both base systems aretrained on the Penn Treebank data.2.
Second, we show that using joint inference canprovide strong a performance gain even whenthe underlying systems are trained on differentdomains.In all experiments, we report the F1 measure forthe verb SRL performance using the CoNLL 2005135evaluation metric and the accuracy for the preposi-tion role labeling task.5.1 Data and ConstraintsFor both the verb SRL and preposition roles, weused the first 500 sentences of section 2 of the PennTreebank corpus to train our scaling parameters.
Forthe first set of experiments, we trained our underly-ing systems on the rest of the available Penn Tree-bank training data for each task.
For the adaptationexperiment, we train the role classifier on the Se-mEval data (restricted to the same Treebank prepo-sitions).
In both cases, we report performance onsection 23 of the Treebank.We mined consistency constraints from the sec-tions 2, 3 and 4 of the Treebank data.
As mentionedin Section 4.1, we considered joint constraints re-lating preposition roles to verb argument candidatesthat start with the preposition.
We identified the fol-lowing types of constraints: (1) For each preposi-tion, the set of invalid verb arguments and prepo-sition roles.
(2) For each preposition role, the setof allowed verb argument labels if the role occurredmore than ten times in the data, and (3) For eachverb argument, the set of allowed preposition roles,similarly with a support of ten.
Note that, while theconstraints were obtained from jointly labeled data,the constraints could be written down because theyencode linguistic intuition about the labels.The following is a constraint extracted from thedata, which applies to the preposition with:srlarg(A2) ?
prep-role(ATTRIBUTE)?
prep-role(CAUSE)?
prep-role(INSTRUMENT)?
prep-role(OBJECTOFVERB)?
prep-role(PARTWHOLE)?
prep-role(PARTICIPANT/ACCOMPAINER)?
prep-role(PROFESSIONALASPECT).This constraint says that if any candidate that startswith with is labeled as an A2, then the prepositioncan be labeled only with one of the roles on the righthand side.Some of the mined constraints have negated vari-ables to enforce that a role or an argument labelshould not be allowed.
These can be similarly con-verted to linear inequalities.
See Rizzolo and Roth(2010) for a further discussion about converting log-ical expressions into linear constraints.In addition to these constraints that were minedfrom data, we also enforce the following hand-written constraints: (1) If the role of a verb at-tached preposition is labeled TEMPORAL, then thereshould be a verb predicate for which this preposi-tional phrase is labeled AM-TMP.
(2) For verb at-tached prepositions, if the preposition is labeled withone of ACTIVITY, ENDCONDITION, INSTRUMENTor PROFESSIONALASPECT, there should be at leastone predicate for which the corresponding preposi-tional phrase is not labeled ?.The conversion of the first constraint to a linearinequality is similar to the earlier cases.
For eachof the roles in the second constraint, let r denote arole variable that assigns the label to some prepo-sition.
Suppose there are n SRL candidates acrossall verb predicates begin with that preposition, andlet s1, s2, ?
?
?
, sn denote the SRL variables that as-sign these candidates to the label ?.
Then the secondconstraint corresponds to the following inequality:r +n?i=1si ?
n5.2 Results of joint learningFirst, we compare our approach to the performanceof the baseline independent systems and to pipelinesin both directions in Table 4.
For one pipeline, weadded the prediction of the baseline preposition rolesystem as an additional feature to both the identifierand the argument classifier for argument candidatesthat start with a preposition.
Similarly, for the sec-ond pipeline, we added the SRL predictions as fea-tures for prepositions that were the first word of anSRL argument.
In all cases, we performed five-foldcross validation to train the classifiers.The results show that both pipelines improve per-formance.
This justifies the need for a joint sys-tem because the pipeline can improve only one ofthe tasks.
The last line of the table shows that thejoint inference system improves upon both the base-lines.
We achieve this improvement without retrain-ing the underlying models, as done in the case of thepipelines.On analyzing the output of the systems, we foundthat the SRL precision improved by 2.75% but the136Setting SRL Preposition Role(F1) (Accuracy)Baseline SRL 76.22 ?Baseline Prep.
?
67.82Prep.
?
SRL 76.84 ?SRL?
Prep.
?
68.55Joint inference 77.07 68.39Table 4: Performance of the joint system, compared tothe individual systems and the pipelines.
All performancemeasures are reported on Section 23 of the Penn Tree-bank.
The verb SRL systems were trained on sections2-21, while the preposition role classifiers were trainedon sections 2-4.
For the joint inference system, the scal-ing parameters were trained on the first 500 sentences ofsection 2, which were held out.
All the improvements inthis table are statistically significant at the 0.05 level.recall decreased by 0.98%, contributing to the over-all F1 improvement.
The decrease in recall is due tothe joint hard constraints that prohibit certain assign-ments to the variables which would have otherwisebeen possible.
Note that, for a given sentence, evenif the joint constraints affect only a few argumentcandidates directly, they can alter the labels of theother candidates via the ?local?
SRL constraints.Consider the following example of the systemoutput which highlights the effect of the constraints.
(6) Weatherford said market conditions led to thecancellation of the planned exchange.The independent preposition role system incor-rectly identifies the to as a LOCATION.
The semanticrole labeling component identifies the phrase to thecancellation of the planned exchange as the A2 ofthe verb led.
One of the constraints mined from thedata prohibits the label LOCATION for the preposi-tion to if the argument it starts is labeled A2.
Thisforces the system to change the preposition labelto the correct one, namely ENDCONDITION.
Boththe independent and the joint systems also label thepreposition of as OBJECTOFVERB, which indicatesthat the phrase the planned exchange is the object ofthe deverbal noun cancellation.5.3 Effect of constraints on adaptationOur second experiment compares the performanceof the preposition role classifier that has been trainedon the SemEval dataset with and without joint con-straints.
Note that Table 2 in Section 3, showsthe drop in performance when applying the prepo-sition sense classifier.
We see that the SemEval-trained preposition role classifier (baseline in the ta-ble) achieves an accuracy of 53.29% when tested onthe Treebank dataset.
Using this classifier jointlywith the verb SRL classifier via joint constraints getsan improvement of almost 3 percent in accuracy.Setting Preposition Role(Accuracy)Baseline 53.29Joint inference 56.22Table 5: Performance of the SemEval-trained prepositionrole classifier, when tested on the Treebank dataset withand without joint inference with the verb SRL system.The improvement, in this case is statistically significantat the 0.01 level using the sign test.The primary reason for this improvement, evenwithout re-training the classifier, is that the con-straints are defined using only the labels of the sys-tems.
This avoids the standard adaptation problemsof differing vocabularies and unseen features.6 Discussion and Related workRoth and Yih (2004) formulated the problem of ex-tracting entities and relations as an integer linearprogram, allowing them to use global structural con-straints at inference time even though the componentclassifiers were trained independently.
In this pa-per, we use this idea to combine classifiers that weretrained for two different tasks on different datasetsusing constraints to encode linguistic knowledge.In the recent years, we have seen several jointmodels that combine two or more NLP tasks .
An-drew et al (2004) studied verb subcategorizationand sense disambiguation of verbs by treating it asa problem of learning with partially labeled struc-tures and proposed to use EM to train the jointmodel.
Finkel and Manning (2009) modeled the taskof named entity recognition together with parsing.Meza-Ruiz and Riedel (2009) modeled verb SRL,predicate identification and predicate sense recogni-tion jointly using Markov Logic.
Henderson et al(2008) was designed for jointly learning to predictsyntactic and semantic dependencies.
Dahlmeier et137al.
(2009) addressed the problem of jointly learningverb SRL and preposition sense using the Penn Tree-bank annotation that was introduced in that work.The key difference between these and the modelpresented in this paper lies in the simplicity of ourmodel and its easy extensibility because it leveragesexisting trained systems.
Moreover, our model hasthe advantage that the complexity of the joint param-eters is small, hence does not require a large jointlylabeled dataset to train the scaling parameters.Our approach is conceptually similar to that ofRush et al (2010), which combined separatelytrained models by enforcing agreement using globalinference and solving its linear programming relax-ation.
They applied this idea to jointly predict de-pendency and phrase structure parse trees and on thetask of predicting full parses together with part-of-speech tags.
The main difference in our approach isthat we treat the scaling problem as a separate learn-ing problem in itself and train a joint model specifi-cally for re-scaling the output of the trained systems.The SRL combination system of Surdeanu et al(2007) studied the combination of three differentSRL systems using constraints and also by trainingsecondary scoring functions over the individual sys-tems.
Their approach is similar to the one presentedin this paper in that, unlike standard reranking, asin Collins (2000), we entertain all possible solutionsduring inference, while reranking approaches traina discriminative scorer for the top-K solutions ofan underlying system.
Unlike the SRL combinationsystem, however, our approach spans multiple phe-nomena.
Moreover, in contrast to their re-scoringapproaches, we do not define joint features drawnfrom the predictions of the underlying componentsto define our global model.We consider the tasks verb SRL and prepositionroles and combine their predictions to provide aricher semantic annotation of text.
This approachcan be easily extended to include systems that pre-dict structures for other linguistic phenomena be-cause we do not retrain the underlying systems.
Thesemantic relations can be enriched by incorporatingmore linguistic phenomena such as nominal SRL,defined by the Nombank annotation scheme of Mey-ers et al (2004), the preposition function analysisof O?Hara and Wiebe (2009) and noun compoundanalysis as defined by Girju (2007) and Girju et al(2009) and others.
This presents an exciting direc-tion for future work.7 ConclusionThis paper presents a strategy for extending seman-tic role labeling without the need for extensive re-training or data annotation.
While standard seman-tic role labeling focuses on verb and nominal re-lations, sentences can express relations using otherlexical items also.
Moreover, the different relationsinteract with each other and constrain the possiblestructures that they can take.
We use this intuitionto define a joint model for inference.
We instanti-ate our model using verb semantic role labeling andpreposition role labeling and show that, using lin-guistic constraints between the tasks and minimaljoint learning, we can improve the performance ofboth tasks.
The main advantage of our approachis that we can use existing trained models withoutre-training them, thus making it easy to extend thiswork to include other linguistic phenomena.AcknowledgmentsThe authors thank the members of the CognitiveComputation Group at the University of Illinois forinsightful discussions and the anonymous reviewersfor valuable feedback.This research is supported by the Defense Ad-vanced Research Projects Agency (DARPA) Ma-chine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181.
Any opinions, ndings, and conclusionor recommendations expressed in this material arethose of the authors and do not necessarily reect theview of the DARPA, AFRL, or the US government.ReferencesG.
Andrew, T. Grenager, and C. D. Manning.
2004.Verb sense and subcategorization: Using joint infer-ence to improve performance on complementary tasks.In Proceedings of EMNLP.X.
Carreras and L. Ma`rquez.
2004.
Introduction to theCoNLL-2004 shared tasks: Semantic role labeling.
InProceedings of CoNLL-2004.X.
Carreras and L. Ma`rquez.
2005.
Introduction to theCoNLL-2005 shared task: Semantic role labeling.
InProceedings of CoNLL-2005.138M.
Chang, L. Ratinov, and D. Roth.
2011.
Structuredlearning with constrained conditional models.
Ma-chine Learning (To appear).E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InACL.M.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In ICML.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In EMNLP.D.
Dahlmeier, H. T. Ng, and T. Schultz.
2009.
Jointlearning of preposition senses and semantic roles ofprepositional phrases.
In EMNLP.J.
R. Finkel and C. D. Manning.
2009.
Joint parsing andnamed entity recognition.
In NAACL.R.
Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-ney, and D. Yuret.
2009.
Classification of semanticrelations between nominals.
Language Resources andEvaluation.R.
Girju.
2007.
Improving the interpretation of nounphrases with cross-linguistic information.
In ACL.Y.
Goldberg and M. Elhadad.
2010.
An efficient algo-rithm for easy-first non-directional dependency pars-ing.
In NAACL.J.
Henderson, P. Merlo, G. Musillo, and I. Titov.
2008.
Alatent variable model of synchronous parsing for syn-tactic and semantic dependencies.
In CoNLL.D.
Hovy, S. Tratz, and E. Hovy.
2010.
What?s in a prepo-sition?
dimensions of sense disambiguation for an in-teresting word class.
In Coling 2010: Posters.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In ACL.K.
Litkowski and O. Hargraves.
2005.
The prepositionproject.
In Proceedings of the Second ACL-SIGSEMWorkshop on the Linguistic Dimensions of Preposi-tions and their Use in Computational Linguistics For-malisms and Applications.K.
Litkowski and O. Hargraves.
2007.
Semeval-2007task 06: Word-sense disambiguation of prepositions.In SemEval-2007: 4th International Workshop on Se-mantic Evaluations.A.
Martins, N. A. Smith, and E. Xing.
2009.
Conciseinteger linear programming formulations for depen-dency parsing.
In ACL.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
The Nom-Bank project: An interim report.
In HLT-NAACL 2004Workshop: Frontiers in Corpus Annotation.I Meza-Ruiz and S. Riedel.
2009.
Jointly identifyingpredicates, arguments and senses using markov logic.In NAACL.T.
O?Hara and J. Wiebe.
2009.
Exploiting semantic roleresources for preposition disambiguation.
Computa-tional Linguistics, 35(2), June.M.
Palmer, P. Kingsbury, and D. Gildea.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1).P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In The Eighth ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2005.Learning and inference over constrained output.
In IJ-CAI.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics.L.
Ratinov and D. Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InCoNLL.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In EMNLP.S.
Riedel.
2009.
Cutting plane map inference for markovlogic.
In SRL 2009.N.
Rizzolo and D. Roth.
2010.
Learning based java forrapid development of nlp systems.
In Language Re-sources and Evaluation.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InCoNLL.D.
Roth and W. Yih.
2005.
Integer linear programminginference for conditional random fields.
In ICML.A.M.
Rush, D. Sontag, M. Collins, and T. Jaakkola.2010.
On dual decomposition and linear program-ming relaxations for natural language processing.
InEMNLP.
Association for Computational Linguistics.M.
Surdeanu, L. Ma`rquez, X. Carreras, and P. R. Comas.2007.
Combination strategies for semantic role label-ing.
J. Artif.
Int.
Res., 29:105?151, June.K.
Toutanova, A. Haghighi, and C. D. Manning.
2008.
Aglobal joint model for semantic role labeling.
Compu-tational Linguistics, 34(2).S.
Tratz and D. Hovy.
2009.
Disambiguation of prepo-sition sense using linguistically motivated features.In NAACL: Student Research Workshop and DoctoralConsortium.N.
Xue and M. Palmer.
2004.
Calibrating features forsemantic role labeling.
In EMNLP.P.
Ye and T. Baldwin.
2007.
MELB-YB: PrepositionSense Disambiguation Using Rich Semantic Features.In SemEval-2007.139
