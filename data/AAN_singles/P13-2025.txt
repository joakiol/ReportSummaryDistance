Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 137?142,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsOn the Predictability of Human Assessment: when Matrix CompletionMeets NLP EvaluationGuillaume WisniewskiUniversite?
Paris SudLIMSI?CNRSOrsay, Franceguillaume.wisniewski@limsi.frAbstractThis paper tackles the problem of collect-ing reliable human assessments.
We showthat knowing multiple scores for each ex-ample instead of a single score results ina more reliable estimation of a systemquality.
To reduce the cost of collect-ing these multiple ratings, we propose touse matrix completion techniques to pre-dict some scores knowing only scores ofother judges and some common ratings.Even if prediction performance is prettylow, decisions made using the predictedscore proved to be more reliable than de-cision based on a single rating of each ex-ample.1 IntroductionHuman assessment is often considered as the best,if not the only, way to evaluate ?subjective?
NLPtasks like MT or speech generation.
However,human evaluations are doomed to be noisy and,sometimes, even contradictory as they depend onindividual perception and understanding of thescore scale that annotators generally use in re-markably different ways (Koehn and Monz, 2006).Moreover, annotation is known to be a long andfrustrating process and annotator fatigue has beenidentified as another source of noise (Pighin et al,2012).In addition to defining and enforcing stricterguidelines, several solutions have been proposedto reduce the annotation effort and produce morereliable ratings.
For instance, to limit the impactof the score scale interpretation, in the WMT eval-uation campaign (Callison-Burch et al, 2012), an-notators are asked to rank translation hypothesesfrom best to worst instead of providing absolutescores (e.g.
in terms of adequacy or fluency).
Gen-eralizing this approach, several works (Pighin etal., 2012; Lopez, 2012) have defined novel annota-tion protocols to reduce the number of judgmentsthat need to be collected.
However, all these meth-ods suffer from several limitations: first, they pro-vide no interpretable information about the qualityof the system (only a relative comparison betweentwo systems is possible); second, (Koehn, 2012)has recently shown that the ranking they induce isnot reliable.In this work, we study an alternative approachto the problem of collecting reliable human as-sessments.
Our basic assumption, motivated bythe success of ensemble methods, is that hav-ing several judgments for each example, even ifthey are noisy, will result in a more reliable de-cision than having a single judgment.
An evalu-ation campaign should therefore aim at gatheringa score matrix, in which each example is rated byall judges instead of having each judge rate onlya small subset of examples, thereby minimizingredundancy.
Obviously, the former approach re-quires a large annotation effort and is, in practice,not feasible.
That is why, to reduce the numberof judgments that must be collected, we proposeto investigate the possibility of using matrix com-pletion techniques to recover the entire score ma-trix from a sample of its entries.
The questionwe try to answer is whether the missing scores ofone judge can be predicted knowing only scores ofother judges and some shared ratings.The contributions of this paper are twofold: i)we show how knowing the full score matrix in-stead of a single score for each example provides amore reliable estimation of a system quality (Sec-tion 3); ii) we present preliminary experiments137showing that missing data techniques can be usedto recover the score matrix from a sample of itsentries despite the low inter-rater agreement (Sec-tion 4).2 Matrix CompletionThe recovering of a matrix from a sampling of itsentries is a task of considerable interest (Cande`sand Recht, 2012).
It can be used, for instance, inrecommender systems: rows of the matrix repre-sent users that are rating movies (columns of thematrix); the resulting matrix is mostly unknown(each user only rates a few movies) and the taskconsists in completing the matrix so that moviesthat any user is likely to like can be predicted.Matrix completion generally relies on the lowrank hypothesis: because of hidden factors be-tween the observations (the columns of the ma-trix), the matrix has a low rank.
For instance,in recommender systems it is commonly believedthat only a few factors contribute to an individual?stastes.
Formally, recovering a matrix M amountsat solving:minimize rank Xsubject to Xij = Mij (i, j) ?
?
(1)whereX is the decision variable and ?
is the set ofknown entries.
This optimization problem seeksthe simplest explanation fitting the observed data.Solving the rank minimization problem hasbeen proved to be NP-hard (Chistov andGrigor?ev, 1984).
However several convex relax-ations of this program have been proposed.
Inthis work, we will consider the relaxation of therank by the nuclear norm1 that can be efficientlysolved by semidefinite programming (Becker etal., 2011).
This relaxation enjoys many theoret-ical guarantees with respect to the optimality ofits solution (under mild assumptions its solution isalso the solution of the original problem), the con-ditions under which the matrix can be recoveredand the number of entries that must be sampledto recover the original matrix.
In our experimentswe used TFOCS,2 a free implementation of thismethod.1The nuclear norm of a matrix is the sum of its singularvalues; the relation between rank an nuclear norm is similarto the one between `0 and `1 norms.2http://cvxr.com/tfocs/3 CorporaFor our experiments we considered two publiclyavailable corpora in which multiple human ratings(i.e.
scores on an ordinal scale) were available.The CECorpus The first corpus of human judg-ments we have considered has been collectedfor the WMT12 shared task on quality estima-tion (Callison-Burch et al, 2012).3 The data set ismade of 2, 254 English sentences and their auto-matic translations in Spanish predicted by a stan-dard Moses system.
Each sentence pair is accom-panied by three estimates in the range 1 to 5 ofits translation quality expressed in terms of post-editing effort.
These human grades are in the range1 to 5, the latter standing for a very good trans-lation that hardly requires post-editing, while theformer identifies very poor automatic translationsthat are not deemed to be worth the post-editingeffort.As pointed out by the task organizers, despitethe special care that was taken to ensure the qualityof the data, the inter-raters agreement was muchlower than what is typically observed in NLPtasks (Artstein and Poesio, 2008): the weighted?
ranged from 0.39 to 0.50 depending on the pairof annotators considered4; the Fleiss coefficient (ageneralization of ?
to multi-raters) was 0.25 andthe Kendall ?b correlation coefficient5 between0.64 and 0.68, meaning that, on average, two ratersdo not agree on the relative order of two transla-tions almost two out of five times.
In fact, as of-ten observed for the sentence level human evalua-tion of MT outputs, the different judges have usedthe score scale differently: the second judge hada clear tendency to give more ?medium?
scoresthan the others, and the variance of her scoreswas low.
Because theirs distributions are differ-ent, standardizing the scores has only a very lim-ited impact on the agreement.If, as in many manual evaluations, each exam-ple had been rated by a single judge chosen ran-domly, the resulting scores would have been onlymoderately correlated with the average of the threescores which is, intuitively, a better estimate of the?true?
quality: the 95% confidence interval of the3The corpus is available from http://www.statmt.org/wmt12/quality-estimation-task.html4The weighted ?
is a generalization of the ?
to ordinaldata; a linear weighting schema was used.5Note that, in statistics, agreement is a stronger notionthan correlation, as the former compare the actual values.138?b between the averaged scores and the ?sampled?score is 0.754?0.755.TIDES The second corpus considered was col-lected for the DARPA TIDES program: a team ofhuman judges provided multiple assessments ofadequacy and fluency for Arabic to English andChinese to English automatic translations.6 Forspace reasons, only results on the Chinese to En-glish fluency corpus will be presented; similar re-sults were achieved on the other corpora.In the considered corpus, 31 sets of automatictranslations, generated by three systems, havebeen rated by two judges on a scale of 1 to 5.
Theinter-rater agreement is very low: depending onthe pair of judges, the weighted ?
is between -0.05and 0.2, meaning that agreement occurs less of-ten than predicted by chance alone.
More impor-tantly, if the ratings of a pair of judges were usedto decide which is the best system among two, thetwo judges will disagree 36% of the time.
This?agreement?
score is computed as follows: if mA,iis the mean of the scores given to system A bythe i-th annotator, we say that there is no agree-ment in a pairwise comparison if mA,i > mB,iand mA,j < mB,j , i.e.
if two judges rank two sys-tems in a different order; the score is then the per-centage of agreement when considering all pairsof systems and judges.Considering the full scoring matrix instead ofsingle scores has a large impact: if each example israted by a single judge (chosen randomly), the re-sulting comparison between the two systems willbe different from the decision made by averagingthe two scores of the full score matrix in almost20% of the comparisons.4 Experimental Results4.1 Testing the Low-Rank HypothesisMatrix completion relies on the hypothesis thatthe matrix has a low rank.
We first propose totest this hypothesis on simulated data, using amethod similar to the one proposed in (Mathetet al, 2012), to evaluate the impact of noise inhuman judgments on the score matrix rank.
Ar-tificial ratings are generated as follows: a MTsystem is producing n translations the quality ofwhich, qi, is estimated by a continuous value,that represents, for instance, a hTER score.
This6These corpora are available from LDC under the refer-ences ldc2003t17 and ldc2003t18value is drawn from N (?, ?2).
Based on this?intrinsic?
quality, two ratings, ai and bi, aregenerated according to three strategies: in thefirst, ai and bi are sampled from N (qi, ?
); inthe second, ai ?
N(qi + ?2 , ?
?2) and bi ?N(qi ?
?2 , ?
?2) and in the third, ai ?
N(qi, ?
?2)and the bi is drawn from a bimodal distribu-tion 12(N(qi ?
?2 , ?
?2)+N(qi + ?2 , ?
?2)) (with?
?2 < ?2 ).
?
describes the noise level.Each of these strategies models a different kindof noise that has been observed in different evalua-tion campaigns (Koehn and Monz, 2006): the firstone describes random noise in the ratings; the sec-ond a systematic difference in the annotators?
in-terpretation of the score scale and the third, the sit-uation in which one annotator gives medium scorewhile the other one tend to commit more stronglyto whether she considered the translation good orbad.
Stacking all these judgments results in a n?2score matrix.
To test whether this matrix has a lowrank or not, we assess how close it is to its ap-proximation by a rank 1 matrix.
A well-knownresult (Lawson and Hanson, 1974) states that theFrobenius norm of the difference of these matri-ces is equal to the 2nd singular value of the orig-inal matrix; the quality of the approximation canthus be estimated by ?, defined as the 2nd eigen-value of the matrix normalized by its norm (Leon,1994).
Intuitively, the smaller ?, the better the ap-proximation.Figure 1 represents the impact of the noise levelon the condition number.
As a baseline, we havealso represented ?
for a random matrix.
All valuesare averaged over 100 simulations.
As it could beexpected, ?
is close to 0 for small noise level; buteven for moderate noise level, the second eigen-value continue to be small, suggesting that the ma-trix can still be approximated by a matrix of rank 1without much loss of information.
As a compari-son, on average, ?
= 0.08 for the CE score matrix,in spite of the low inter-rater agreement.4.2 Prediction PerformanceWe conducted several experiments to evaluate thepossibility to use matrix completion to recover ascore matrix.
Experiments consist in choosingrandomly k% of the entries of a matrix; these en-tries are considered unknown and predicted usingthe method introduced in Section 2 denoted predin the following.
In our experiments k varies from10% to 40%.
Note that, when, as in our exper-1390 0.1 0.2 0.3 0.400.10.20.30.4random?
?1st strat.2nd strat.3rd strat.Figure 1: Evolution of the condition number ?with the noise level ?
for the different strategies(see text for details)iments, only two judges are involved, k = 50%would mean that each example is rated by a sin-gle judge.
Two simple methods for handling miss-ing data are used as baselines: in the first one, de-noted rand, missing scores are chosen randomly;the second one, denoted mean, predicts for all themissing scores of a judge the mean of her knownscores.We propose to evaluate the quality of the recov-ery, first by comparing the predicted score to theirtrue value and then by evaluating the decision thatwill be made when considering the recovered ma-trix instead of the full matrix.Prediction Performance Comparing the com-pleted matrix to the original score matrix can bedone in terms of Mean Absolute Error (MAE) de-fined as 1N?Ni=1 |yi ?
y?i|where y?i is the predictedvalue and yi the corresponding ?true?
value; thesum runs over all unknown values of the matrix.Table 1 presents the results achieved by the dif-ferent methods.
All reported results are averagedover 10 runs (i.e.
: sampling of the score matrixand prediction of the missing scores) and over allpairs of judges.
All tables also report the 95% con-fidence interval.
The MAE of the rand method isalmost constant, whatever the number of samplesis.
Performance of the matrix completion tech-nique is not so good: predicted scores are quitedifferent than true scores.
In particular, perfor-mance falls quickly when the number of missingdata increases.
This observation is not surprising:when 40% of the scores are missing, only a fewexamples have more than a single score and manyhave no score at all.
In these conditions recoveringmissing data pred mean40% 0.78 ?6.21 ?
10?3 0.72 ?8.86 ?
10?330% 0.83 ?3.19 ?
10?3 0.80 ?5.42 ?
10?320% 0.88 ?2.49 ?
10?3 0.87 ?3.54 ?
10?310% 0.93 ?1.76 ?
10?3 0.92 ?1.51 ?
10?3Table 2: Correlation between the rankings inducedby the recovered matrix and the original score ma-trix for the CE corpusthe matrix is almost impossible.
The performanceof the simple mean technique is, comparatively,pretty good, especially when only a few entriesare known.
However, the pred method alwaysoutperform the rand method showing that thereare dependencies between the two ratings even ifstatistical measures of agreement are low.Impact on the Decision The negative results ofthe previous paragraph only provide indirect mea-sure of the recovery quality as it is not the value ofthe score that is important but the decision that itwill support.
That is why, we also evaluated ma-trix recovery in a more task-oriented way by com-paring the decision made when considering the re-covered score matrix instead of the ?true?
scorematrix.For the CE corpus, a task-oriented evaluationcan be done by comparing the rankings inducedby the recovered matrix and by the original matrixwhen examples are ordered according to their av-eraged score.
Such a ranking can be used by a MTuser to set a quality threshold granting her con-trol over translation quality (Soricut and Echihabi,2010).
Table 2 shows the correlation between thetwo rankings as evaluated by ?b.
The two rankingsappear to be highly correlated, the matrix comple-tion technique outperforming slightly the meanbaseline.
More importantly, even when 40% ofthe data are missing, the ranking induced by thetrue scores is better correlated to the ranking in-duced by the predicted scores than to the rankinginduced when each example is only rated once: asreported in Section 3, the ?b is, in this case, 0.75.For the TIDES corpus, we computed the num-ber of pairs of judges for which the results of apairwise comparison between two systems is dif-ferent when the systems are evaluated using thepredicted scores and the true scores.
Results pre-sented in Table 3 show that considering the pre-dicted matrix is far better than having judges rate140QE TIDESk pred mean rand pred mean rand40% 1.14 ?2.9 ?
10?2 0.78 ?6.6 ?
10?3 1.45 ?
?
?30% 0.94 ?2.9 ?
10?2 0.78 ?7.4 ?
10?3 1.44 0.95 ?2.7 ?
10?2 0.43 ?2.6 ?
10?2 1.3720% 0.77 ?3.4 ?
10?2 0.78 ?1.0 ?
10?2 1.45 0.76 ?2.6 ?
10?2 0.41 ?2.5 ?
10?2 1.3810% 0.65 ?2.1 ?
10?2 0.79 ?1.9 ?
10?2 1.47 0.48 ?3.0 ?
10?2 0.41 ?2.5 ?
10?2 1.36Table 1: Completion performance as evaluated by the MAE for the three prediction methods and thethree corpora considered.random samples of the examples: the number ofdisagreement falls from 20% (Sect.
3) to less than4%.
While the mean method outperforms thepred method, this result shows that, even in caseof low inter-rater agreement, there is still enoughinformation to predict the score of one annotatorknowing only the score of the others.For the tasks considered, decisions based on arecovered matrix are therefore more similar to de-cisions made considering the full score matrix thandecisions based on a single rating of each example.5 ConclusionThis paper proposed a new way of collecting reli-able human assessment.
We showed, on two cor-pora, that knowing multiple scores for each exam-ple instead of a single score results in a more reli-able estimation of the quality of a NLP system.
Weproposed to used matrix completion techniquesto reduce the annotation effort required to collectthese multiple ratings.
Our experiments showedthat while scores predicted using these techniquesare pretty different from the true scores, decisionsconsidering them are more reliable than decisionsbased on a single score.Even if it can not predict scores accurately, webelieve that the connection between NLP evalua-tion and matrix completion has many potential ap-plications.
For instance, it can be applied to iden-tify errors made when collecting scores by com-paring the predicted and actual scores.6 AcknowledgmentsThis work was partly supported by ANR projectTrace (ANR-09-CORD-023).
The author wouldlike to thank Franc?ois Yvon and Nicolas Pe?cheuxfor their helpful questions and comments on thevarious drafts of this work.% missing data pred mean30% 9.24% 3.53 %20% 6.45% 2.10 %10% 3.66% 1.20 %Table 3: Disagreements in a pairwise comparisonof two systems of the TIDES corpus, when thesystems are evaluated using the predicted scoresand the true scoresReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Comput.Linguist., 34(4):555?596, December.Stephen R. Becker, Emmanuel J. Cande`s, andMichael C. Grant.
2011.
Templates for convex coneproblems with applications to sparse signal recovery.Math.
Prog.
Comput., 3(3):165?218.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proc.
of WMT, pages 10?51,Montre?al, Canada, June.
ACL.Emmanuel Cande`s and Benjamin Recht.
2012.
Exactmatrix completion via convex optimization.
Com-mun.
ACM, 55(6):111?119, June.A.
Chistov and D. Grigor?ev.
1984.
Complexity ofquantifier elimination in the theory of algebraicallyclosed fields.
In M. Chytil and V. Koubek, editors,Math.
Found.
of Comp.
Science, volume 176, pages17?31.
Springer Berlin / Heidelberg.Philipp Koehn and Christof Monz.
2006.
Manual andautomatic evaluation of machine translation betweeneuropean languages.
In Proc.
WMT, pages 102?121,New York City, June.
ACL.Philipp Koehn.
2012.
Simulating human judgment inmachine translation evaluation campaigns.
In Proc.of IWSLT.Charles L. Lawson and Richard J. Hanson.
1974.
Solv-ing Least Squares Problems.
Prentice Hall.141Stephen J: Leon.
1994.
Linear Algebra with Applica-tions.
Macmillan,.Adam Lopez.
2012.
Putting human assessments ofmachine translation systems in order.
In Proc.
ofWMT, pages 1?9, Montre?al, Canada, June.
ACL.Yann Mathet, Antoine Widlcher, Kare?n Fort, ClaireFranc?ois, Olivier Galibert, Cyril Grouin, JulietteKahn, Sophie Rosset, and Pierre Zweigenbaum.2012.
Manual corpus annotation: Giving meaningto the evaluation metrics.
In Proceedings of COL-ING 2012: Posters, pages 809?818, Mumbai, India,December.Daniele Pighin, Llu?
?s Formiga, and Llu?
?s Ma`rquez.2012.
A graph-based strategy to streamline trans-lation quality assessments.
In Proc.
of AMTA.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments witha tunable MT metric.
In Proc.
of WMT, pages 259?268, Athens, Greece, March.
ACL.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translationsvia ranking.
In Proc.
of ACL, pages 612?621, Upp-sala, Sweden, July.
ACL.142
