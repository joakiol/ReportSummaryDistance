Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 198?206,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsFormatting Time-Aligned ASR Transcripts for ReadabilityMaria Shugrina?Google Inc.New York, NY 10011shumash@google.comAbstractWe address the problem of formatting the out-put of an automatic speech recognition (ASR)system for readability, while preserving word-level timing information of the transcript.
Oursystem enriches the ASR transcript with punc-tuation, capitalization and properly writtendates, times and other numeric entities, andour approach can be applied to other format-ting tasks.
The method we describe combineshand-crafted grammars with a class-based lan-guage model trained on written text and relieson Weighted Finite State Transducers (WF-STs) for the preservation of start and end timeof each word.1 Introduction and Prior WorkThe output of a typical ASR system lacks punctua-tion, capitalization and proper formatting of entitiessuch as phone numbers, time expressions and dates.Even if such automatic transcript is free of recogni-tion errors, it is difficult for a human to parse.
Theproper formatting of the transcript gains particularimportance in applications where the user relies onASR output for information and where information-rich numeric entities (e.g.
time expressions, mone-tary amounts) are common.
A good example of suchapplication is a voicemail transcription system.
Thegoal of our work is to transform the raw transcriptinto its proper written form in order to optimize it forthe visual scanning task by the end user.
We presentquantitative and qualitative evaluation of our systemwith a focus on numeric entity formatting, punctua-tion and capitalization (See Fig.
1).Apart from text, the ASR output usually con-tains word-level metadata such as time-alignmentand confidence.
Such quantities may be useful for avariety of applications.
Although simple to recover?Thank you to Michiel Bacchiani, Martin Jansche, MichaelRiley and Cyril Allauzen for discussion and support.Raw Transcript:hi bill it?s tracy at around three thirty P M just gotan apartment for one thousand three thirty one thou-sand four hundred a month my number is five five fiveeight eight eight eight extension is three thirty byeOur Result:Hi Bill, it?s Tracy at around 3:30 PM, just got anapartment for 1,330 1,400 a month.
My number is555-8888 extension is 330.
Bye.Figure 1: An example of a raw transcript with ambiguouswritten forms and the output of our formatting system.via word alignment after some types of formatting,word-level quantities may be difficult to preserve ifthe original text has undergone a significant transfor-mation.
We present a formal and general augmen-tation of our WFST-based technique that preservesword-level timing and confidence information dur-ing arbitrary formatting.The problems of sentence boundary detection andpunctuation of transcripts have received a substantialamount of attention, e.g.
(Beeferman et al, 1998;Shriberg et al, 2000; Christensen et al, 2001; Liuet al, 2006; Gravano et al, 2009).
Capitalization ofASR transcripts received less attention (Brown andCoden, 2002; Gravano et al, 2009), but there hasalso been work on case restoration in the contextof machine translation (Chelba and Acero, 2006;Wang et al, 2006).
Our work does not proposecompeting methods for transcript punctuation andcapitalization.
Instead, we aim to provide a com-mon framework for a wide range of formatting tasks.Our method extends the approach of Gravano et al(2009) with a general WFST formulation suitablefor formatting monetary amounts, time expressions,dates, phone numbers, honorifics and more, in addi-tion to punctuation and capitalization.To our knowledge, this scope of the problem hasnot been addressed in literature.
Yet such format-ting can have a high impact on transcript readabil-ity.
In this paper we focus on numeric entity format-198ting.
In general, context independent rules fail toadequately perform this task due to its inherent am-biguity (See Fig.
1).
For example, the spoken words?three thirty?
should be written differently in thesethree contexts:?
meet me at 3:30?
you owe me 330?
dinner for three 30 minutes laterThe proper written form of a numeric entity dependson its class (time, monetary amount, etc).
In thissense, formatting is related to the problem of namedentity (NE) detection and value extraction, as de-fined by MUC-7 (Chinchor, 1997).
Several authorshave considered the problem of NE value extractionfrom raw transcripts (Huang et al, 2001; Janscheand Abney, 2002; Be?chet et al, 2004; Levit et al,2004).
This is an information extraction task that in-volves identifying transcript words corresponding toa particular NE class and extracting an unambigu-ous value of that NE (e.g.
the value of the dateNE ?december first oh nine?
is ?12/01/2009?).
Al-though relevant, this information extraction does notdirectly address the problem of proper formattingand ordinarily requires a tagged corpus for training.A parallel corpus containing raw transcriptionsand the corresponding formatted strings would facil-itate the solution to the transcript formatting prob-lem.
However, there is no such corpus available.Therefore, we follow the approach of Gravano et aland provide an approximation that exploits readilyavailable written text instead.
In section 2 we de-tail our method, provide a probabilistic interpreta-tion and present a practical formulation of the solu-tion in terms of WFSTs.
Section 3 shows how toaugment the WFST formulation to preserve word-level timing and confidence.
Section 4 presents bothqualitative and quantitative evaluation of our system.2 MethodFirst, handwritten grammars are used to generateall plausible written forms.
These variants are thenscored with a language model (LM) approximatingprobability over written strings.
To overcome datasparsity associated with written numeric strings, weintroduce numeric classes into the LM.
In section2.1 we give a probabilistic formulation of this ap-proach.
In section 2.2 we comment on the hand-written grammars, and in section 2.3 we discuss theclass-based language model used for scoring.
Sec-tion 2.4 provides the WFST formulation of the solu-tion.2.1 Probabilistic FormulationThe problem of estimating the best written form w?of a spoken sequence of words s can be formulatedas a Machine Translation (MT) problem of translat-ing a string s from the language of spoken stringsinto a language of written strings.
From a statisticalstandpoint, w?
can be estimated as follows:w?
= argmaxw{P (w|s)} ?
argmaxw{P ?
(s|w)P ?
(w)},where P (?)
denotes probability, and P ?(?)
a prob-ability approximation.
The probability over writtenstrings P (w) can be estimated by training an n-gramlanguage model on amply available written text.
Theabsence of a parallel corpus containing sequences ofspoken words and their written renditions makes theconditional distribution P (s|w) impossible to esti-mate.
An approximation P ?
(s|w) can be obtained bydefining handwritten grammars that generate multi-ple unweighted written variants for any spoken se-quence.
For a given s, a collection of grammars en-codes a uniform probability distribution across theset of all written variants generated for s and as-signs a zero probability to any string not in this set.Such grammar-based modeling of P (s|w) combinedwith statistical estimation of P (w) takes advantageof prior knowledge, but does not share the disadvan-tages of rigid, fully rule-based systems.2.2 Handwritten GrammarsHandwritten grammars G1...Gm are used to gener-ate unweighted written variants for a raw string s. InGravano?s work (Gravano et al, 2009) the generatedvariants include optional punctuation between everytwo words and an optional capitalization for everyword.
Our system supports a wider range of vari-ants, including but not limited to multiple variantsof number formatting.The handwritten grammars can be very restrictiveor very liberal, depending on the application require-ments.
For example, a grammar we use to generatepunctuation and capitalization only generates sen-tences with the first word capitalized.
This enforcesconventions and consistency, which the best scor-ing variant could occasionally violate.
On the other19902three133<space><space><column><period>?5thirty43 0Figure 2: An FSA encoding all variants generated by thenumber grammar for a spoken string ?three thirty?.hand, the grammar for number formatting couldbe very liberal in producing written variants (SeeFig.
2).
Jansche and Abney (2002) observe thathandwritten rules deterministically tagging numericstrings of certain length as phone numbers performsurprisingly well on phone number NE identificationin voicemail.
If appropriate to the task, determinis-tic grammars can be incorporated into the grammarstack.
The unweighted written variants generated byapplying G1...Gm to s are then scored with the lan-guage model.2.3 Language ModelThe probability distribution over written text P (w)can be approximated by a Katz back-off n-gram lan-guage model trained on written text in a domainsemantically similar to the domain for which theASR engine is deployed.
Unlike some of the ap-proaches used for NE identification (Jansche andAbney, 2002; Levit et al, 2004) and sentence bound-ary detection (Christensen et al, 2001; Shriberg etal., 2000; Liu et al, 2006), LM-based scoring can-not exploit a larger context than n tokens or prosodicfeatures.
The advantage of the LM approach is theease of applying it to new formatting tasks: no newtagged corpus, and only trivial changes to the pre-processing of the training text would be required.If the LM is to score written numeric strings, caremust be taken in modeling numbers.
Representingeach written number as a token (e.g.
tokens ?1,235?,?15?)
during training results in a very large modeland suffers from data sparsity even with very largetraining corpora.
An alternative approach of model-ing every digit as a token (e.g.
?15?
is comprised oftokens ?1?
and ?2?)
fails to model sufficient contextfor longer digit strings.
A partially class-based LMremedies the drawbacks of both approaches, and hasbeen used for tasks such as NE tagging (Be?chet etClass Set ANumeric range Interpretation2-9 single digits10-12 up to hour in a 12-hour system13-31 up to the largest day of the month32-59 up to the largest minute in a timeexpressionother 2-digit all other 2-digit numbersother 3-digit all 3-digit numbers1900 - 2099 common year numbersother 4-digit all other 4-digit numbers10000-99999 all 5-digit numbers; e.g.
US zip-codes?
100000 all large numbersClass Set BNumeric range Interpretation0-9 one digit string10-99 two digit string... ...109 ?
(1010 ?
1) ten-digit string?
1010 longer digit stringTable 1: Two sets of number classes used in our system.Each sequence of consecutive digit characters is mappedto the appropriate class.
For example, ?$1,235.12?
wouldbecome ??dollar?
1 ?comma?
?num 100 999?
?period?
?num 10 12??
in Class Set A and ??dollar?
?num 1D??comma?
?num 3D?
?period?
?num 2D?
in Class Set B.al., 2004).
The generalization provided by classeseliminates data sparsity, and is able to model suffi-cient context.We experiment with two sets of classes (See Ta-ble 1).
Class Set B, based on (Be?chet et al, 2004),marks strings of n consecutive digits as belonging toan n-digit class, assuming nothing about the num-ber distribution.
Class Set A is based on intuitionabout number distribution in text (See Table 1, Inter-pretation).
In section 4.4 we show that Class Set Aachieves better performance on number formatting.Now that it is established that the choice of classesaffects performance, future research could focus onfinding an optimal set of number classes automat-ically.
Clustering techniques, often used to deriveclass definitions from training text, could be applied.Although more punctuation marks could be con-sidered, we focus on periods and commas.
Similarlyto Gravano et al (2009), we map all other punctua-tion marks in the training text to these two.
In manyformatting scenarios (e.g.
spelled out acronyms, nu-meric ranges), spaces are ambiguous and significant,200and it is therefore important to consider whitespacewhen scoring the written variants.
Because of this,we model space as a token in the LM.2.4 WFST FormulationThe one-best1 ASR output s can be represented by aFinite State Acceptor (FSA) S. We describe a seriesof standard WFST operations on S resulting in theFSA Wbest encoding the best estimated formattedvariant w?.
Current section assumes familiarity withWFSTs; for background see (Mohri, 2009).
(a) S FSA(b) W variants FST(c) Wout FSA(d) Wclass FST(e) Wbest FSAFigure 3: An example showing transducers produced dur-ing formatting.We encode each grammar Gi as an unweightedFST Ti that transduces the raw transcript to its for-matted versions.
The necessity to encode themas FSTs restricts the set of grammars to regulargrammars (Hopcroft and Ullman, 1979), sufficientlypowerful for most formatting tasks.
The back-offn-gram LM is naturally represented as a weighteddeterministic FSA G with negative log probabilityweights (Mohri et al, 2008).
The deterministic map-ping of digit strings to number class tokens can also1This WFST formulation can also be applied to the ASR lat-tice or n-best list with some modification to the scoring phase.be accomplished by an unweighted transducer K,which passes all non-numeric strings unchanged.Composing the input acceptor S with the gram-mar transducers Ti results in a transducer W withall written variants on the output.
Projected onto itsoutput labels, W becomes an acceptor Wout.
Wclass,the result of the composition of Wout with K, hasall formatted written variants on the input side andthe formatted variants with digit strings replaced byclass tokens on the output.
The output side of Wclasscan then be scored via composition with G to pro-duce a weighted transducer Wscored.
The shortestpath in the Tropical Semiring on Wscored containsthe estimate of the best written variant on the inputside.
This algorithm can be summarized as follows(See Fig.
3):1.
W = S ?
T1 ?
T2... ?
Tm2.
Wout = Projout(W )3.
Wclass = Wout ?K4.
Wscored = Wclass ?G5.
Wbest = Projin(BestPath(Wscored))where ?
denotes FST composition, Projin andProjout denote projection on input and output la-bels respectively, and BestPath(X) as a functionreturning an FST encoding the shortest path of X .The key Step 2 ensures that the target written vari-ants are not consumed in the consequent composi-tion operations.
For efficiency reasons it is advisableto apply optimizations such as epsilon removal anddeterminization to the intermediate results.
23 Preserving Word-Level MetadataWe extend the WFST formulation to preserve word-level timing and confidence information.3.1 BackgroundA WFST is a finite set of states and transitionsconnecting them.
Each transition has an input la-bel, an output label and a weight in some semir-ing K. A semiring is informally defined as a tou-ple (K,?,?, 0, 1), where K is the set of elements,?
and ?
are the addition and multiplication opera-tions, 0 is the additive identity and multiplicative an-nihilator, 1 is the multiplicative identity (See (Mohri,2Our system implements proper failure transitions availablein the OpenFST Library (Allauzen et al, 2007).2012009)).
By defining new semirings we can use stan-dard FST operations to accomplish a wide range ofgoals.3.2 Timing SemiringIn order to formulate time preservation within theFST formalism, we define the timing semiring Ktwhere each element is a pair (s, e) that can be inter-preted as the start and end time of a word:Wt ={(s, e) : s, e ?
R+ ?
{0,?
}}(s1, e1) ?
(s2, e2) = (max(s1, s2), min(e1, e2))(s1, e1) ?
(s2, e2) = (min(s1, s2), max(e1, e2))0 = (0,?)
1 = (?, 0)Intuitively, the addition operation takes the largestinterval contained by both operand intervals, whilemultiplication returns the smallest interval fully con-taining both operand intervals.
3 This definition ful-fills all the semiring properties as defined in (Mohri,2009).
Note that encoding only the duration of eachword is not sufficient, as there may be time gapsbetween the words due to the segmentation of thesource audio.
Let S?
denote the Weighted FiniteState Acceptor (WFSA) encoding the raw ASR out-put with the start and end time stored in the weightof each arc.In order to preserve word-level confidence in ad-dition to timing information, a Cartesian product ofKt and the Log semiring can be used to store bothtime and confidence in an arc weight.3.3 Weight SynchronizationThe goal is to associate the timing/confidenceweights of S?
with the word labels of Wbest, the bestformatted string (See Sec.
2.4).
Because the weightof each transition in S?
already expresses the tim-ing/confidence corresponding to its word label, it issufficient to associate the labels of S?
with the labelsof Wbest.
This is equivalent to identifying the outputlabels to which each input label is transduced duringStep 1 in section 2.4.
However, in general WFST op-erations may desynchronize input and output labels3Note that this is just a Cartesian product of min-max andmax-min semirings.
The elements of Kt are not proper inter-vals, as it is possible for s to exceed e.and weights, as the FST structure itself does not in-dicate a semantic correspondence between them.
Toalleviate this, we guarantee such a correspondencein our grammars by enforcing that for all paths inany grammar FST Ti:?
an input label appears before any of the corre-sponding output labels, and?
output labels corresponding to a given input la-bel appear before the next input label.In practice, these assumptions are usually met byhandwritten grammars.
Even if these assumptionsare violated for a small number of paths, only smallword-level timing discrepancies will be incurred.Each path in W can be thought of as a sequence ofsubpaths with only the first transition containing anon-?
input label.
We say that the input label of eachsuch subpath corresponds to that subpath?s output la-bels.0 1ten/(1,2) 2<sp> 3six/(3,4)(a) S?
FSA0 11 20 3<sp> 46(b) Wbest FSA0 1ten:1 2?
:0 3<sp>:<sp> 4six:6(c) Wraw:best FST0 1ten:1/(1,2) 2?
:0 3<sp>:<sp> 4six:6/(3,4)(d) W?best FSTFigure 4: A small example of time preservation section ofthe algorithm.
Arcs with non-unity timing weights showparenthesized pair of start and end time.The best path that has input labels correspondingto the raw ASR output can be obtained by compos-ing the variants FST W with the best formatted FSAWbest and picking any path.
The timing weights arerestored to by composing the weighted S?
with thisresult.
To preserve timing we add two more steps toSteps 1?5 in section 2.4:6.
Wraw:best = RmEps(AnyPath(W ?Wbest))7.
W?best = S?
?
Mapt(Wraw:best)where RmEps(X) applies the epsilon-removal al-gorithm to X (Mohri, 2009), and Mapt(X) maps202all non-zero weights of X to the unity weight in thetiming semiring.
Because S?
is an epsilon-free accep-tor, the result W?best will contain the original weightsof S?
on the arcs with the corresponding input labels(See Fig.
4 for an example).
The space-delimitedwords and the corresponding weights can then beread off by walking W?best.4 EvaluationSection 4.1 presents our datasets and an evaluationmetric specific to number formatting, and section4.2 describes our experimental system.
We presentquantitative evaluation of capitalization/punctuationperformance and number formatting performanceseparately in sections 4.3 and 4.4.
Because the ul-timate goal of our work is to improve the readabilityof ASR transcripts, we also present the result of auser study of transcript readability in section 4.5.4.1 Data and MetricsThe training corpus contains 185M tokens of writtentext normalized to contain only comma and periodpunctuation marks.
A set of 176M tokens (TRS) isused for training and a set of 7M tokens (PTS) isheld back for testing punctuation and capitalization(See Table 3).
To obtain a test input (NPTS) for oursystem, PTS is lowercased and all punctuation is re-moved.words commas periods capitalsTRS 176M 10.6M 11.8M 24.3MPTS 7M 420K 440K 880KTable 3: Training set TRS and test set PTS.Number formatting is evaluated on a manuallyformatted test set.
We manually processed the setof raw manual transcripts (NNTS) from the LDCVoicemail Part I training set (Padmanabhan et al,1998) to obtain a reference number formatting set(NTS).
All numeric entities in NTS were formattedaccording to the following conventions:?
all quantities under 10 are spelled out?
time is written in a 12-hour system as ?xx:xx?or ?xx??
dollar amounts are written as ?$x,xxx.xx?
withcents included if spoken?
US phone numbers are written as ?
(xxx) xxx-xxxx?
or ?xxx-xxxx??
other phone numbers are written as digit strings?
decimals are written as ?x.x??
large amounts include commas: ?x,xxx,xxx?All contiguous sequences of words in NTS thatcould be a target for number formatting were markedas numeric entities, whether or not these words wereformatted by the labeler (for example ?six?
is a nu-meric entity).
To evaluate number formatting per-formance, we process NNTS with our full experi-mental system, then remove all capitalization andinter-word punctuation.
This result is aligned withNTS, and each entity is scored separately as totallycorrect or totally incorrect (See Table 2), yielding:Numeric Entity Error Rate = 100 ?
INwhere I is the count of entities that did not matchthe reference entity string exactly and N is the totalentity count.
This error rate is independent of thenumeric entity density in the test set.
The errors arebroken down into three types:?
incorrect formatting - when the system incor-rectly formats an entity that is formatted in thereference?
overformatting - when the system formats anentity that stays unformatted in the reference?
underformatting - when the system does notformat an entity formatted in the referenceOut of 1801 voicemail transcripts in NTS, 1347 con-tain at least one entity for a total of 3563 entities,signifying a frequent occurrence of numeric entitiesin voicemail.
There is an average of 7 raw transcriptwords per entity, suggesting that in many cases en-tity formatting is non-trivial.4.2 Experimental SystemThe experimental system includes a 5-gram LMtrained on TRS with spaces treated as tokens.
Num-ber evaluation is performed with two sets of numberclasses, listed in Table 1.
System A contains LMwith classes from set A, and System B contains LMwith classes from set B.
The experimental setup alsoincludes the following grammars:?
Gphone - deterministically formats as a phonenumber any string spoken like a US 7 or 10digit phone number?
Gnumber - expands all spoken numbers to a fullrange of variants, with support for time expres-sions, ordinals, decimals, dollar amounts?
Gcap punct - generates all possible combina-tions of commas, periods and capitals; alwayscapitalizes the first word of a sentence203Raw: for six people at five five thirty cost is eleven hundred dollarsRef: for six people at 5 5:30 cost is $1,100Hyp: for 6 people at 5 5:30 cost is 11 $100Score: - incorrect - correct - incorrectTable 2: A example of a raw transcript, reference transcript with number formatting and the hypothesis produced bythe system.
The entities (bold) in reference and hypothesis are aligned and scored.4.3 Evaluation of PunctuationTo evaluate the performance of capitalization andpunctuation we run System A on NPTS with onlythe Gcap punct (in order not to introduce errors dueto numeric formatting).
The precision, recall and F-measure rates for periods, commas and capitals arecomputed using PTS as reference (See Fig.
5).Precision Recall F-MeasureCapitals 0.7902 0.5356 0.6385Comma 0.5527 0.3129 0.3996Period 0.6672 0.6783 0.6727Figure 5: Punctuation and capitalization results.It should be noted that a 5-gram language modelthat treats spaces as words models the same historyas a 3-gram model that omits the spaces from train-ing data.
When this is taken into account, our re-sults with a much smaller training set are compara-ble to Gravano et al (2009).
The F-measure scoresfor commas and periods are also comparable to theprosody-based work of (Christensen et al, 2001),with the precision of the period slightly lower, butcompensated by recall.
Thus, our system can per-form additional formatting, while retaining a reason-able capitalization and punctuation performance.4.4 Evaluation of Number FormattingWe evaluate number formatting performance of Sys-tems A and B, which use different sets of classes forthe language modeling (See Table 1).
We processNNTS with both systems and score against the ref-erence formatted set NTS to obtain Numeric EntityError Rate (NEER).
Class Set B naively breaks num-bers into classes by digit count.
System B using thisclass set performs worse than System A by 1.7% ab-solute (See Table 4).
In particular, the overformat-ting rate (OFR) is higher by 1.2% absolute in SystemB than in System A.
An example of overformattingis the mis-formatting of the English impersonal pro-noun ?one?
as the digit ?1?.
Such overformattingerrors are much more noticeable than the underfor-NEER IFR OFR UFRSystem Aexact 16.1% 9.7% 5.4% 1.0%ignore space 11.2% 4.9% 5.4% 1.0%System Bexact 17.8% 10.6% 6.6% 0.6%ignore space 13.2% 6.0% 6.6% 0.6%Table 4: The total NEER score, NEER due to incorrectformatting (IFR), NEER due to overformatting (OFR)and NEER due to underformatting (UFR); NEER rateswith whitespace errors ignored are also listed.matting errors, which are higher by 0.4% absolutein System A.
This result shows that the choice ofclasses for the class-based LM significantly impactsnumber formatting performance.
Superior overallperformance of System A suggests that prior knowl-edge in the choice of classes favorably impacts per-formance.In order to estimate the error rate not caused bywhitespace errors, we also compute the NEER withwhitespace errors ignored.
It turns out that between4 and 5% absolute of the errors are whitespace er-rors.
Even if all whitespace errors are significant,the 83.9% of perfectly formatted entities suggeststhat the proposed formatting approach can achievegood performance on the number formatting task.entities : .
$ ,Reference totals 3563 310 50 39 17System A correct 3161 232 36 33 10System B correct 2923 204 35 31 5Table 5: The count of formatted entities in NTS contain-ing various formatting characters; the counts of these en-tities correctly formatted by the systems A and B.To estimate how well the systems perform on spe-cific number formatting tasks we count the numberof reference entities containing certain formattingcharacters and compute the number of these enti-ties correctly formatted by Systems A and B (SeeTable 5).
The count of different formatting charac-ters in NTS is small, but still provides an estimate ofthe number formatting performance for a real appli-204cation like voicemail transcription.
System A per-forms significantly better on the formatting of timeexpressions containing a colon, getting 74.8% cor-rect.
The NEER of System A for entities containingspecial formatting characters is under 28% for allformatting characters except comma, which is usedinconsistently in training text.4.5 Qualitative EvaluationIn addition to quantitative evaluation we have con-ducted a small-scale study of transcript readability.The study aims to compare raw ASR transcripts,ASR transcripts formatted by our system and rawmanual transcripts.
We have processed LDC Voice-mail Part 1 with our ASR engine achieving an er-ror rate of 30%, and have selected 50 voicemailswith error rate under 30% and high informationalcontent.
Messages containing names, addresses andnumbers were preferred.
The word error rate onthe selected voicemails is 20%.
For each voicemailwe have constructed three semantic multiple-choicequestions, aimed at information extraction.
We haveasked each of 15 volunteers to answer all 3 questionsabout half of the voicemails.
The questions wereshown in sequence, while the transcript remained onthe screen.
The transcript for each voicemail wasrandomly selected to be ASR raw, ASR formattedor manual raw.
The response time was measured in-dividually for each question.The analysis of the responses reveals a statis-tically significant difference in response time be-tween formatted and raw ASR transcripts (p = 0.02,even allowing for per-item and per-subject effects;see also Fig.
6) and comparable accuracy.
The re-sponse times for formatted ASR were comparableto the response times for manual unformatted tran-scripts.
This suggests that for transcripts with lowerror rates the formatting of the ASR output signif-icantly impacts readability.
This disagrees with asimilar study (Jones et al, 2003), which found nosignificant difference in the comprehension rates be-tween raw ASR transcripts and capitalized, punctu-ated ASR output with disfluencies removed.
Thiscould be due to a number of factors, including a dif-ferent type of transformation performed on the ASRtranscript, a different corpus, and a lower word errorrate of transcripts in our user study.ASR Formatted ASR Raw Manual Raw020406080100Timetoanswer (seconds)90.0% 90.7% 94.4%Figure 6: The standard R box plot of the response time fordifferent transcript types and the corresponding accuracy.5 ConclusionWe present a statistical approach suitable for a widerange of formatting tasks, including but not lim-ited to punctuation, capitalization and numeric en-tity formatting.
The average of 2 numeric enti-ties per voicemail in the manually processed LDCVoicemail corpus shows that number formatting isimportant for applications such as voicemail tran-scription.
Our best system achieves a Numeric En-tity Error Rate of 16.1% on the ambiguous task ofnumeric entity formatting, while retaining capital-ization and punctuation performance comparable toother published work.
Our algorithm is conciselyformulated in terms of WFSTs and is easily ex-tended to new formatting tasks without the need foradditional training data.
In addition, the WFST for-mulation allows word-level timing and confidenceto be retained during formatting.
In order to over-come data sparsity associated with written numbers,we use a class-based language model and show thatthe choice of number classes significantly impactsnumber formatting performance.
Finally, a statis-tically significant difference in question answeringtime for raw and formatted ASR transcripts in ouruser study demonstrates the positive impact of thetranscript formatting on the readability of errorfulASR transcripts.205ReferencesC.
Allauzen, M. Riley, J. Schalkwyk, W. Skut, andM.
Mohri.
2007.
Openfst: A general and efficientweighted finite-state transducer library.
In Proceed-ings of CIAA, pages 11?23.F.
Be?chet, A. Gorin, J. Wright, and D. Hakkani-Tu?r.2004.
Detecting and extracting named entities fromspontaneous speech in a mixed-initiative spoken dia-logue context: How may i help you?
Speech Commu-nication, 42(2):207?225.D.
Beeferman, A. Berger, and J. Lafferty.
1998.
Cyber-punc: A lightweight punctuation annotation system forspeech.
In Proceedings of ICASSP, pages 689?692.E.
Brown and A. Coden.
2002.
Capitalization re-covery for text.
In Information Retrieval Techniquesfor Speech Applications, pages 11?22, London, UK.Springer-Verlag.C.
Chelba and A. Acero.
2006.
Adaptation of maximumentropy capitalizer: Little data can help a lot.
Com-puter Speech and Language, 20(4):382?399.N.
Chinchor.
1997.
Muc-7 named entity task definition.In Proceedings of MUC-7.H.
Christensen, Y. Gotoh, and S. Renals.
2001.
Punc-tuation annotation using statistical prosody models.
InISCA Workshop on Prosody in Speech Recognition andUnderstanding.A.
Gravano, M. Jansche, and M. Bacchiani.
2009.Restoring punctuation and capitalization in transcribedspeech.
In Proceedings of ICASSP, pages 4741?4744.IEEE Computer Society.J.
Hopcroft and J. Ullman, 1979.
Introduction to au-tomata theory, languages, and computation, pages218?219.
Addison-Wesley.J.
Huang, G. Zweig, and M. Padmanabhan.
2001.
Infor-mation extraction from voicemail.
In Proceedings ofthe Conference of the ACL, pages 290?297.M.
Jansche and S. P. Abney.
2002.
Information extrac-tion from voicemail transcripts.
In In EMNLP.D.
Jones, F. Wolf, E. Gibson, E. Williams, E. Fedorenko,D.
Reynolds, and M. Zissman.
2003.
Measuring thereadability of automatic speech-to-text transcripts.
InProceedings of EUROSPEECH, pages 1585?1588.M.
Levit, P. Haffner, A. Gorin, H. Alshawi, and E. No?th.2004.
Aspects of named entity processing.
In Pro-ceedings of INTERSPEECH.Y.
Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf,and M. Harper.
2006.
Enriching speech recognitionwith automatic detection of sentence boundaries anddisfluencies.
IEEE Transactions on Audio, Speech,and Language Processing, 14(5):1526?1540.M.
Mohri, F. Pereira, and M. Riley.
2008.
Speech recog-nition with weighted finite-state transducers.
In Hand-book on Speech Processing and Speech Communica-tion.
Springer.M.
Mohri.
2009.
Weighted automata algorithms.
InHandbook of Weighted Automata.
Monographs in The-oretical Computer Science., pages 213?254.
Springer.M.
Padmanabhan, G. Ramaswamy, B. Ramabhadran,P.
Gopalakrishnan, and C. Dunn.
1998.
Voicemailcorpus part i. Linguistic Data Consortium, Philadel-phia.E.
Shriberg, A. Stolcke, D. Hakkani-Tu?r, and G. Tu?r.2000.
Prosody-based automatic segmentation ofspeech into sentences and topics.
Speech Communi-cations, 32(1-2):127?154.W.
Wang, K. Knight, and D. Marcu.
2006.
Capitaliz-ing machine translation.
In Proceedings of HLT/ACL,pages 1?8.
Association for Computational Linguistics.206
