Proceedings of the 10th Conference on Parsing Technologies, pages 83?92,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSymbolic Preference Using Simple ScoringPaula S. Newmannewmanp@acm.orgAbstractDespite the popularity of stochastic parsers,symbolic parsing still has some advantages,but is not practical without an effectivemechanism for selecting among alternativeanalyses.
This paper describes the symbolicpreference system of a hybrid parser thatcombines a shallow parser with an overlayparser that builds on the chunks.
The hy-brid currently equals or exceeds most sto-chastic parsers in speed and is approachingthem in accuracy.
The preference system isnovel in using a simple, three-valued scor-ing method (-1, 0, or +1) for assigningpreferences to constituents viewed in thecontext of their containing constituents.The approach addresses problems associ-ated with earlier preference systems, andhas considerably facilitated development.
Itis ultimately based on viewing preferencescoring as an engineering mechanism, andonly indirectly related to cognitive princi-ples or corpus-based frequencies.1 IntroductionDespite the popularity of stochastic parsers, sym-bolic parsing still has some advantages, but is notpractical without an effective mechanism for se-lecting among alternative analyses.
Without it, ac-cept/fail grammar rules must either be overlystrong or admit very large numbers of parses.
.Symbolic parsers have recently been augmentedby stochastic post-processors for output disam-biguation, which reduces their independence fromcorpora.
Both the LFG XLE parser (Kaplan et.al.2004), and the HPSG LinGO ERG parser (Tou-tanova et al 2005) have such additions.This paper examines significant aspects of apurely symbolic alternative: the preference andpruning system of the RH (Retro-Hybrid) parser(Newman, 2007).
The parser combines a pre-existing, efficient shallow parser with an overlayparser that builds on the emitted chunks.
The over-lay parser is "retro" in that the grammar is relatedto ATNs (Augmented Transition Networks) origi-nated by Woods (1970).RH delivers single "best" parses providing syn-tactic categories, syntactic functions, head features,and other information (Figure 1).
The parenthe-sized numbers following the category labels in thefigure are preference scores, and are explained fur-ther on.
While the parses are not quite as detailedas those obtained using "deep" grammars, themissing information, mostly relating to long dis-tance dependencies, can be added at far less cost ina post-parse phase that operates only on a singlebest parse.
Methods for doing so, for stochasticparser output, are described by Johnson (2002) andCahill et al(2004).The hybrid parser exceeds most stochastic pars-ers in speed, and approaches them in accuracy,even based on limited manual "training" on a par-ticular idiom, so the preference system is a suc-cessful one  (see Section 6), and continues to im-prove.The RH preference system builds on earliermethods.
The major difference is a far simplerscoring system, which has considerably facilitatedoverlay parser development.
Also, the architectureallows the use of large numbers of preference testswithout impacting parser speed.
Finally, the treat-ment of coordination exploits the lookaheads af-forded by the shallow parser to license or bar alter-native appositive readings.Section 2 below discusses symbolic preferencesystems in general, and section 3 provides an over-view of RH parser structure.
Section 4 describesthe organization of the RH preference system andthe simplified scoring mechanism.
Section 5 dis-cusses the training approach and Section 6 pro-vides some experimental results.
Section 7 sum-marizes, and indicates directions for further work.83Figure 1.
Output Parse Tree for "Rumsfeld micromanaged daily briefings and rode roughshodover people."
* indicates head.
Mouseover shows head features for "micromanaged".2 Background: Symbolic Preference2.1 PrinciplesPreference-based parsing balances necessarilypermissive syntactic rules by preference rules thatpromote more likely interpretations.
One of theearliest works in the area is by Wilks (1975),which presented a view of preference as based onsemantic templates.
Throughout the 1980's therewas a considerable amount of work devoted tofinding general principles, often cognitively ori-ented, for preference rules, and then to devisemechanisms for using them in practical systems.Hobbs and Bear (1990) provide a useful summaryof the evolved principles.
Slightly restated, theseprinciples are:1.
Prefer attachments in the "most restrictivecontext".2.
If that doesn't uniquely determine the result,attach low and parallel, and finally3.
Adjust the above based on considerations ofpunctuationPrinciple 1 suggests that the preference for aconstituent in a construction should depend on theextent to which the constituent meets a narrow setof expectations.
Most of the examples given byHobbs and Bear use either (a) sub-categorizationinformation, e.g., preferring the attachment of aprepositional phrase to a head that expects that par-ticular preposition, or (b) limited semantic infor-mation, for example, preferring the attachment of atime expression to an event noun.Principle 2 implies that in the absence of coor-dination, attachment should be low, and in thepresence of coordination, parallel constituentsshould be preferred.
Principle 3 relates primarilyto the effect of commas in modifying attachmentpreferences.2.2 ImplementationsAbstractly, symbolic preference systems can bethought of as regarding a set of possible parses as acollection of spanning trees over a network of po-tential relationships, with each edge having a nu-meric value, and attempting to find the highestscoring tree.1However, for syntactic parsers, in contrast withdependency parsers, it is convenient to associatescores with constituents as they are built, for con-sistency with the parser structure, and to permitwithin-parse pruning.
A basic model for a prefer-ence system assigns preference scores to rules.
Fora ruleC ?
c1, c2, ?, cnthe preference score PS(CC) of a resultant con-stituent CC is the sum:PS(cc1) + PS(cc2) +  +PS(ccn)+ TRS (C, cc1, cc2, ?, ccn)where PS(cci) is the non-contexted score of con-stituent cci, and the total relationship score TRS is avalue that assesses the relationships among the sib-ling constituents of CC.
The computation of TRSdepends on the parser approach.
For a top-downparser, TRS may be the sum of contexted relation-ship scores CRS, for example:TRS = CRS (cc1|C) +CRS (cc2|C, cc1), +CRS (cc3|C, cc1, cc2) + ?..+ CRS (cn |C,  cc1,?.ccn-1)where each CRS (cci|_ ) evaluates cci in the contextof the prior content of the constituent CC and thecategory C..Few publications specify details of how prefer-ence scores are assigned and combined.
For exam-ple, Hobbs and Bear (1990) say only that "When a1The idea has also been used directly in stochastic pars-ers that consider all possible attachments, for example,by McDonald et al (2005).84non-terminal node of a parse tree is constructed, itis given an initial score which is the sum of thescores of its child nodes.
Various conditions arechecked during the construction of the node and, asa result, a score of 20, 10, 3, -3, -10, or -20 may beadded to the initial score.
"McCord (1993), however, carefully describeshow the elements of TRS are computed in his slotgrammar system.
Each element value is the sum ofthe results of up to 8 optional, typed tests, relatingto structural, syntactic, and semantic conditions.One of these tests, relating to coordination, is acomplex test involving 7 factors assessing parallel-ism.2.3 Multi-Level Contexted ScoringThe scores assigned by symbolic preference sys-tems to particular relationships or combinationsusually indicate not just whether they are preferredor dispreferred, but to what degree.
For example, ascore of 1 might indicate that a relationship isgood, and 2 that it is better.Such multi-level scores create problems in tun-ing parsers to remove undesirable interactions,both in the grammar and the preference system.Even for interactions foreseen in advance, onemust remember or find out the sizes of the prefer-ences involved, to decide how to compensate.Yamabana et al (1993) give as an example a bot-tom-up parser, where an S constituent with a tran-sitive verb head but lacking an object is initiallygiven a strong negative preference, but when it isdiscovered that the constituent actually functionsas a relative clause, the appropriate compensationmust be found.
(Their solution uses a vector ofpreference scores, with the vector positions corre-sponding to specific types of preference features,together with an accumulator.
It allows the contentof vector elements to be erased based on subse-quently discovered compensating features.
)For unforeseen interactions, for example when areview of parser outputs finds that the best parse isnot given the highest preference score, multi-levelcontexted scoring requires complex tracing of thecontribution of each score to the total, remember-ing at each point what the score should be, to de-termine the necessary adjustments.A different sort of problem of multi-level scor-ing stems from the unavoidable incompleteness ofinformation.
For example, in Figure 1, the attach-ment of an object to the "guessed" verb "micro-managed" is dispreferred because the verb is notidentified as transitive.
Here, the correct readingsurvives because there are no higher scoring ones.But in some situations, if such a dispreferencewere given a large negative score, the parser couldbe forced into very odd readings not compensatedfor by other factors.2.4 Corpus-Based PreferenceIn the early 1990's, the increasing availability anduse of corpora, together with a sense that multi-level symbolic preference scores were based on ad-hoc judgments, led to experiments and systems thatused empirical methods to obtain preferenceweights.
Examples of this work include a systemby Liu et al(1990), and experiments by Hindle andRooth (1993), and Resnik and Hearst (1993).2These efforts had mixed success, suggesting thatwhile multi-level preference scores are problem-atic, integrating some corpus data does not solvethe problems.
In light of later developments, thismight be expected.
Full-scale contemporary sto-chastic parsers use a broad range of interacting fea-tures to obtain their fine-grained results; frequen-cies of particular relationships are just one aspect.2.5 OT-based PreferenceA more recent approach to symbolic preferenceadapts optimality theory to parser and generatorpreference.
Optimality Theory (OT) was origi-nally developed to explain phonological rules(Prince and Smolensky, 1993).
In that use, poten-tial rules are given one "optimality mark" for eachconstraint they violate.
The marks, all implicitlynegative, are ranked by level of severity.
A bestrule R is one for which (a) the most severe level ofconstraint violation L is ?
the level violated by anyother rule, and (b) if other rules also violate level Lconstraints, the number of such violations is ?
thenumber of violations by R.As adapted for use in the XLE processor forLFG (Frank et al 1998) optimality marks are asso-ciated with parser and generator outputs.
Positivemarks are added, and also labeled inter-mark posi-tions within the optimality mark ranking.
The la-beled positions influence processor behavior.
Forgeneration, they are used to disprefer infelicitousstrings accepted in a parse direction.
And for pars-2McCord (1993) also includes some corpus-based in-formation, but to a very limited extent.85ing they can be used to disprefer (actually ignore)rarely-applicable rules, in order to reduce parsetime (Kaplan et al 2004).However, because the optimality marks areglobal, a single dispreference can rule out an entireparse.
To partially overcome this limitation, a fur-ther extension (see XLE Online Documentation)allows direct comparisons of alternative readingsfor the same input extent.
A different optimalitymark can be set for each reading, and the use ofone such mark in the ranking can be conditionedon the presence of another particular mark for thesame extent.
For example, a conditional disprefer-ence can be set for an adjunct reading if an argu-ment reading also exists.
The extension does notaddress more global interactions, and is said (Forstet al 2005) to be used mostly as a pre-filter to limitthe readings disambiguated by a follow-on stochas-tic process.2.6 A Slightly Different ViewA slightly different view of preference?based pars-ing is that the business of a preference system is towork in tandem with a permissive syntactic gram-mar, to manipulate outcomes.The difference focuses on the pragmatic role ofpreference in coercing the parser.
In this light, theprinciples of section 2.1 are guidelines for desiredoutcomes, not bases for judging the goodness of arelationship or setting preference values.
Instead,preference values should be set based on their ef-fectiveness in isolating best parses.
Also, in thislight, the utility of a preference system lies notonly in its contribution to accuracy, but also in itssoftware-engineering convenience.
These consid-erations led to the simpler, more practical scoringsystem of the RH overlay parser, described  in sec-tion 4 below, in which contexted preference scoresCRS can have one of only 3 values, -1, 0, or +1.3 Background: The RH ParserThe RH parser consists of three major components,outlined below: the shallow parser, a mediating"locator" phase, and the overlay parser.3.1 Shallow ParserThe shallow parser used, XIP, was developed byXRCE (Xerox Research Center Europe).
It isactually a full parser, whose per-sentence outputconsists of a single tree of basic chunks, togetherwith identifications of (sometimes alternative)typed dependences among the chunk heads  (Ait-Mokhtar et al 2002, Gala 2004).
But because theXIP dependency analysis for English was notmature at the time that work on RH began, andbecause a classic parse tree annotated by syntacticfunctions is more convenient for someapplications, we focused on the output chunks.XIP is astonishingly fast, contributing very littleto parse times (about 20%).
It consists of the XIPprocessor, plus grammars for a number oflanguages.
The grammar for a particular languageconsists of:(a) a finite-state lexicon producing alternativepart-of-speech and morphological analyses foreach token, together with bit-expressedsubcategorization and control features, and(some) semantic features,(b) a substitutable tagger identifying the mostprobable part of speech for each token, and(c) sequentially applied rule sets that extend andmodify lexical information, disambiguate tags,identify named entities and other  multiwords,and produce output chunks and inter-chunkhead dependences (the latter not used in thehybrid).Work on the hybrid parser has included largescale extensions to the XIP English rule sets.3.2 Locator phaseThe locator phase accumulates and analyses someof the shallow parser results to expedite thegrammar and preference tests of the overlay parser.For preference tests,  for any input position, thepositions of important leftward and rightwardtokens are identified.
These "important" tokensinclude commas, and leftward phrase heads thatmight serve as alternative attachment points.Special attention is given to coordination, aconstant source of inefficiency and inaccuracy forall parsers.
To limit this problem, an input string isdivided into spans ending at coordinating conjunc-tions, and the chunks following a span are exam-ined to determine what kinds of coordination mightbe present in the span.
For example, if a chunkfollowing a span Sp is a noun phrase, and there areno verbs in the input following that noun phrase,only noun phrase coordination is considered withinSp.
Also, with heuristic exceptions, the locatorphase disallows searching for appositives within86long sequences of noun and prepositional phrasesending with a coordinating conjunction.3.3 Overlay ParserThe overlay parser uses a top-down grammar,expressed as a collection of ATN-like grammarnetworks.
A recursive control mechanism traversesthe grammar networks depth-first to buildconstituents.
The labels on the grammar networkarcs represent specialized categories, and areassociated with tests that, if  successful, eitherreturn  a chunk or reinvoke the control to attemptto build a constituents for the category.
The label-specific tests include both context-free tests, andtests taking into account the current context.
Fordetails see (Newman, 2007).If an invocation of the control is successful, itreturns an output network containing one or morepaths, with each path representing an alternativesequence of immediate children of the constituent.An example output network is shown in figure 2.Each arc of the network references either a basicchunk, or a final state of a subordinate output net-work.
Unlike the source grammar networks, theoutput networks do not contain cycles or converg-ing arcs, so states represent unique paths.The states contain both (a) information aboutmaterial already encountered along the path, in-cluding syntactic functions and head features, and(b) a preference score for the path to that point.Thus the figure 2 network represents twoalternative noun phrases, one represented by thepath containing OS0 and OS1, and one containingOS0, OS1, and OS2.
State OS2 contains thepreference score (+1), because attaching a locativepp to a feature of the landscape is preferred.From To Cat Synfun ReferenceOSo OS1 NP HEAD NPChunk(The park)OS1 OS2 PP NMOD Final state ofPP net for(in Paris)States Score Final?OS0 0 NoOS1 0 YesOS2 +1 YesFigure 2.
Output network for "The park in Paris"Before an output network is returned from aninvocation of the control mechanism, it is prunedto remove lower-scoring paths, and cached.Output from the overlay parser is a single tree(Figure 1) derived from a highest scoring full path(i.e.
final state) of a topmost output network.
Ifthere are several highest scoring paths, low attachconsiderations select a "best" one.
The preferencescores shown in Figure 1 in parentheses after thecategory labels are the scores at the succeedingstates of the underlying output networks.4 Preference SystemAny path in an output network has the form:S0, Ref1, S1, Ref2, ?, Sn-1 , Refn, Snwhere Si is a state, and Refi labels an arc, and refer-ences either a basic chunk, or a final state of an-other output network.
A state Si has total prefer-ence score TPS(i) where:?
TPS(0) = 0?
TPS(i),  i>0 =TPS( i-1) + PS(Refi) +CRS(Refi)?
PS(Refi) is the non-contexted score of theconstituent referenced by Refi, that is, thescore at the referenced final state.?
CRS(Refi) is the contexted score for Refi, inthe context of the network category and thepath ending at the previous state i-1.For example, if Refi refers to a noun phrase con-sidered a second object within a path, and the syn-tactic head along the path does not expect a secondobject, CRS(Refi) might be (-1).Each value CRS is limited to values in {-1, 0,+1}.
Therefore, no judgment is needed to decidethe degree to which a contexted reference is to bedispreferred or preferred.
Also, if the desired parseresult does not receive the highest overall score, itis relatively easy to trace the reason.
Pruning (seebelow) can be disabled and all parses can be dis-played, as in Figure 1, which shows the scoresTPS(i) in parentheses after the category labels foreach Refi (with zero scores not shown).
Then, ifTPS(i) >  ( TPS(i-1) + PS(Refi))it is clear that the contexted reference is preferred.If multi-level contexted scoring were used instead,it would be necessary to determine whether thereference was preferred to exactly the right degree.87Test BlockTypeLengthIndependent?Indexed ByCoordinate Y Parent syncatSubcat Y No indexFN1 Y synfunTAG1 Y syncatFN2 N synfunTAG2 N syncatTable 1.
Preference Test Block Types4.1 Preference test organizationTo compute the contexted score CRS for a refer-ence, relevant tests are applied until either (a) ascore of -1 is obtained, which is used as CRS forthe reference, or (b) the tests are exhausted.
In thelatter case, CRS is the higher of the values {0, +1}returned by any test.For purposes of efficiency, the preference testsare divided into typed blocks, as shown in Table 1.At most one block of each type can be applied to areference.
Four of the blocks contain tests that areindependent of referenced constituent length.
Theyare applied at most once for a returned output net-work and the results are assumed for all paths.
Theother two blocks are length dependent.Referring to Table 1, the length-independent co-ordinate tests are applied only to non-first siblingsof coordinated constituents.
The parent categoryindicates the type of constituents being coordinatedand selects the appropriate test block.
Tests inthese blocks focus on the semantic consistency of acoordinated sibling with the first one.Subcategorization tests are applied to preposi-tional, particle, and clausal dependents of the cur-rent head.
These tests consist to a large extent ofbit-vector implemented operations, comparing theexpected dependent types of the head with lexicalfeatures of the prospective dependent.
The testsare made somewhat more complex because ofvarious exceptions, such as (a) temporal and loca-tive phrases, and (b) the presence of a nearer po-tential head also expecting the dependent type.The other test block types are selected and ac-cessed either by the syntactic category or the syn-tactic function of the reference, depending on thefocus of the test.
The length-dependent tests in-clude tests of noun-phrases within coordinations todetermine whether post modifiers should be ap-plied to the individual phrase or to the coordinationas a whole.The test blocks are expressed in proceduralcode.
This has allowed the parser to be developedwithout advance prediction of the types of infor-mation needed for the tests, and also has contrib-uted some efficiency.
The blocks, usually shortbut occasionally long, generally consist of ordered(if-then-else) subtests.4.2 Preference test scopeA contexted preference test can refer to material onthree levels of the developing parse tree: (a) thesyntactic category of the parent (available becauseof the top-down parser direction) (b) informationabout the current output network path, includinghead features, already-encountered syntactic func-tions, and a small collection of special-purposeinformation, and (c) information about the refer-enced constituent, specifically its head and a list ofthe immediately contained syntactic functions.
Thetests can also reference lookahead information fur-nished by the locator phase.
This material is suffi-cient for most purposes.
Limiting the kind of ref-erenced information, particularly not permittingaccess to sibling constituents or deep elements ofthe referenced constituent, contributes to perform-ance.4.3 PruningBefore an output network is completed, it is prunedto remove lower-scoring output network paths.Any path with the same length as another but witha lower score is pruned.
Also, paths having otherlengths but considerably lower preference scoresthan the best-scoring path are often pruned as well.4.4 Usage ExampleTo illustrate how the simple scores and modulartests are used to detect and repair problems in thepreference system, Figure 1 shows, as noted be-fore, that the attachment of an object to the guessedverb "micromanaged" is dispreferred.
In this casethe probable reason is the lack of a transitive fea-ture for the verb.
To check this, we would look atthe FN1 test block for OBJ and find that in fact thetest assigns (-1) in this case.
The required modifi-cation is best made by adding a transitive feature toguessed verbs.But there is another problem here: the attach-ment of the pp "over people" is not given a positivepreference.
Checking the FN1 test block for88VMOD and the TAG1 test block for PP finds thatthere is in fact no subtest that prefers combinationsof motion verbs and "over".
While this doesn'tcause trouble in the example, it could if there werea prior object in the verb phrase.
A subtest or sub-categorization feature could be added.5 Training the Preference SystemTo obtain the preference system, an initial set oftests is identified, based primarily on subcategori-zation considerations, and then refined and ex-tended based on manual "training" on large num-bers of documents.
Several problem situationsresult in changes to the system, besides randominspection of scores:(a) the best parse identified is not the correct one,either because the correct parse is not the high-est scoring one, or because another parse withthe same score was considered "best" becauseof low-attach considerations.
(b) The best parse obtained is the correct one, butthere are many other parses with the samescore, suggesting a need for refinement, bothto improve performance and to avoid errors inrelated circumstances when the correct parsedoes not "float" to the top.
(c) No parse is returned for an input, because ofimposed space constraints, which indirectlycontrol the amount of time that can be spent toobtain a parse.In some cases the above problems can be solvedby adjusting the base grammar, or by extendinglexical information to obtain the appropriate pref-erences.
For example, the preference scoring prob-lems of Figure 1 can be corrected by adding sub-categorization information, as described above.In other cases, one or more modifications to thepreference system are made, adding positive teststo better distinguish best parses, adding negativetests to disprefer incorrect parses, and/or refiningexisting tests to narrow or expand applicability.Positive tests often just give credit to expectedstructures not previously considered to require rec-ognition beyond acceptance by the grammar.Negative tests fall into many classes, such as:(a) Tests for "ungrammatical" phenomena thatshould not be ruled out entirely by the gram-mar.
These include lack of agreement, lack ofexpected punctuation, and presence of unex-pected punctuation (such as a comma betweena subject and a verb when there is no commawithin the subject).
(b) Tests for probably incomplete constituents,based on the chunk types that follow them.
(c) Tests for unexpected arguments, except insome circumstances.
For example, "benefac-tive" indirect objects ("John baked Mary acake") are dispreferred if they are not in ap-propriate semantic classes.Also, a large, complex collection of positive andnegative tests, based on syntactic and semantic fac-tors, are used to distinguish among coordinated andappositive readings, and among alternative attach-ments of appositives.If the addition or modification of preferencetests does not solve a particular problem, thensome more basic changes can be made, such as theintroduction of new semantic classes.
And, in rarecases, new features are added to output networkstates in order to make properties of non-head con-stituents encountered along a path available fortesting both further along the path and in the de-velopment of higher-level constituents.
An exam-ple is the person and number of syntactic subjects,allowing contexted preference tests for finite verbphrases to check for subject consistency.5.1 Relationship to "supervised" trainingTo illustrate the relationship between the abovesymbolic training method for preference scoringand corpus-based methods, perhaps the easiest wayis to compare it to an adaptation (Collins andRoark, 2004) of the perceptron training method tothe problem of obtaining a best parse (either di-rectly, or for parse reranking), because the twomethods are analogous in a number of ways.The basic adapted perceptron training assumes agenerator function producing parses for inputs.Each such parse is associated with a vector of fea-ture values that express the number of times thefeature appears in the input or parse.
The featuresused are those identified by Roark (2001) for a top-down stochastic parser.The training method obtains a weight vector W(initially 0) for the feature values, by iterating mul-tiple times over pairs <xi, yi> where xi is a traininginput, and yi is the correct parse for xi.
For eachpair, the best current parse zi for xi  produced by thegenerator, with feature value vector V(zi),  is se-lected based on the current value of (W ?
V(zi)).Then if zi ?
yi, W is incremented by V(yi), and dec-89remented by V(zi).
After training, the weights in Ware divided by the number of training steps (# in-puts * # iterations).The method is analogous to the RH manualtraining process for preference in a number ofways.
First, the features used were developed forsuitability to a top-down parser, for example takinginto account superordinate categories at severallevels, some lexical information associated withnon-head, left-side siblings of a node, and someright-hand lookahead.
Although only one su-perordinate category is routinely used in RH pref-erence tests, in order to allow caching of outputnetworks for a category, the preference system al-lows for and occasionally uses the promotion ofnon-head features of nested constituents to providesimilar capability.Also, the feature weights obtained by the per-ceptron training method can be seen to focus onpatterns that actually matter in distinguishing cor-rect from incorrect parses, as does RH preferencetraining.
Intuitively, the difference is that whilesymbolic training for RH explicitly pinpoints pat-terns that distinguish among parses, the perceptrontraining method accomplishes something similarby postulating some more general features as nega-tive or positive based on particular examples, butallowing the iterations over a large training set tofilter out potentially indicative patterns that do notactually serve as such.These analogies highlight the fact that prefer-ence system training, whether symbolic or corpus-based, is ultimately an empirical engineering exer-cise.6 Some Experimental ResultsTables 2, 3, and 4 summarize some recent resultsas obtained by testing on Wall Street Journal sec-tion 23 of the Penn Treebank (Marcus et al 1994).The RH results were obtained by about 8 weeks ofmanual training on the genre.Table 2 compares speed and coverage for RHand Collins Model3 (Collins, 1999) run on thesame CPU.
The table also extrapolates the resultsto two other parsers, based on reported compari-sons with Collins.
One extrapolation is to a veryfast stochastic parser by Sagae and Lavie (2005).The comparison indicates that the RH parser speedis close to that of the best contemporary parsers.The second extrapolation is to the LFG XLEparser (Kaplan et al 2004) for English, consistingof a highly developed symbolic parser and gram-mar, an OT-based preference component, and astochastic back end to select among remaining al-ternative parser outputs.
Two sets of values aregiven for XLE, one obtained using the full Englishgrammar, and one obtained using a reduced gram-mar ignoring less-frequently applicable rules.
Theextrapolation indicates that the coverage of RH isquite good for a symbolic parser with limited train-ing on an idiom.While the most important factor in RH parserspeed is the enormous speed of the shallow parser,the preference and pruning approach of the overlayparser make contributions to both speed and cover-age.
This can be seen in Table 2 by the differencebetween RH parser results with and without prun-ing.
Pruning increases coverage because without itmore parses exceed imposed resource limits.Table 3 compares accuracy.
The values forCollins and Sagae/Lavie are based on comparisonwith treebank data for the entire section 23.
How-ever, because RH does not produce treebank-styletags,  the RH values are  based only on a randomTime No full parseSagae/Lavie ~ 4 min 1.1%RH Prune 5 min 14 sec 10.8%RH NoPrune  7 min 5 sec  13.9 %Collins m3 16 min  .6%XLE reduced ~24 minutes unknownXLE full ~80 minutes ~21%Table 2.
Speeds and Extrapolated speedsFullyaccurateF-score Avg crossbktsSagae/Lavie unknwn 86% unknwnCollins Lbl 33.6% 88.2% 1.05CollinsNoLbl 35.4% 89.4 % 1.05RH NoLbl 46% 86 % .59Table 3.
Accuracy ComparisonAverage  MedianRH Base 137.10 11RH Pref    5.04 2Table 4.
Highest Scoring Parses per Input90100-sentence sample from section 23, and com-pared using a different unlabeled bracketing stan-dard.
For details see Newman (2007).
For non-parsed sentences the chunks are bracketed.
Accu-racy is not extrapolated to XLE because availablemeasurements give f-scores (all ?
80%) for de-pendency relations rather than for bracketed con-stituents.As a partial indication of the role and effective-ness of the RH preference system, if non-parsedsentences are ignored, the percentage of fully accu-rate bracketings shown in Table 3 rises to ap-proximately 46/89 = 51.6% (it is actually largerbecause coverage is higher on the 100-sentencesample).
As further indication, Table 4 compares,for section 23, the average and median number ofparses per sentence obtained by the base grammaralone (RH Base), and the base grammar plus thepreference system (RH Pref).3  The table demon-strates that the preference system is a crucial parsercomponent.
Also, the median of 2 parses per sen-tence obtained using the preference system ex-plains why the fallback low-attach strategy is suc-cessful in many cases.7 Summary and DirectionsThe primary contribution of this work is in demon-strating the feasibility of a vastly simplified sym-bolic preference scoring method.
The preferencescores assigned are neither "principle-based", nor"ad-hoc", but explicitly engineered to facilitate themanagement of undesirable interactions in thegrammar and in the preference system itself.
Re-stricting individual contexted scores to {-1, 0, +1}addresses the problems of multi-level contextedscoring discussed in Section 2, as follows:?
No abstract judgment is required to assign avalue to a preference or dispreference.?
Information deficiencies contribute onlysmall dispreferences, so they can often beovercome by preferences.?
Compensating for interactions that are fore-seen does not require searching the rules tofind necessary compensating values.?
For unforeseen interactions discovered whenreviewing parser results, the simplified pref-3The values are somewhat inflated because they includeduplicate parses, which have not yet been entirelyeliminated.erence scores facilitate finding the sources ofthe problems and potential methods of solv-ing them.This approach to symbolic preference has facili-tated development and maintenance of the RHparser, and has enabled the production of resultswith a speed and accuracy comparable to the beststochastic parsers, even with limited training on anidiom.An interesting question is why this very simpleapproach does not seem to have been used previ-ously.
Part of the answer may lie in the lack ofexplicit recognition that symbolic preference scor-ing is ultimately an engineering problem, and isonly indirectly based on cognitive principles orapproximations to frequencies of particular rela-tionships.Ongoing development of the RH preference sys-tem includes continuing refinement based on"manual" training, and continuing expansion of theset of semantic features used as the parser is ap-plied to new domains.
Additional developmentwill also include more encoding of, and attentionto, the expected semantic features of arguments.Experiments are also planned to examine the accu-racy/performance tradeoffs of using additionalcontext information in the preference tests.ReferencesSalah A?t-Mokhtar, Jean-Pierre Chanod, Claude Roux.2002.
Robustness beyond shallowness: incrementaldeep parsing, Natural Language Engineering 8:121-144, Cambridge University Press.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josefvan Genabith, and Andy Way.
2004.
Long-DistanceDependency Resolution in Automatically AcquiredWide-Coverage PCFG-Based LFG Approximations,In Proc of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL'04), Barce-lonaMichael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Michael Collins and Brian Roark.
2004.
IncrementalParsing with the Perceptron Algorithm.
In Proc of the42nd Annual Meeting of the Association for Compu-tational Linguistics (ACL'04),  Barcelona.Martin Forst, Jonas Kuhn, and Christian Rohrer.
2005.Corpus-based Learning of OT Constraint Rankingsfor Large-scale LFG Grammars.
In Proc of the91LFG05 Conference, Bergen.
Available athttp://cslipublications.stanford.edu/LFG/10/lfg05.pdfAnette Frank., Tracy H. King, Jonas Kuhn, and JohnMaxwell.
1998.
Optimality theory style constraintranking in large-scale LFG grammars.
In M. Butt andT.
H. King, Eds.
Proc of the Third LFG Conference.Available http://csli-publications.stanford.edu/LFG3/Revised version in Peter Sells, ed.
Formal and Theo-retical Issues in Optimality Theoretic Syntax, CSLIPublications, 2001.Nuria Gala.
2004.
Using a robust parser grammar toautomatically generate UNL graphs.
In Proc Work-shop on Robust Methods for Natural Language Dataat COLING'04, GenevaDonald Hindle and Mats Rooth.
1993.
Structural Ambi-guity and Lexical Relations.
Computational Linguis-tics, 19:1,103?120.Jerry R. Hobbs and John Bear.
1990.
Two Principles ofParse Preference.
In Proceedings of the 13th Interna-tional Conference on Computational Linguistics(COLING'90), Helsinki, Finland, August 1990.Jerry.
R. Hobbs, Douglas E. Appelt, John Bear, MabryTyson, and David Magerman.
1992.
Robust process-ing of real-world natural language texts.
In Paul S.Jacobs, ed., Text-Based Intelligent Systems: CurrentResearch and Practice in Information Extraction andRetrieval.
Lawrence Erlbaum, New Jersey, 1992.Mark Johnson.
2002.
A simple pattern-matching algo-rithm for recovering empty nodes and their antece-dents.
Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics (ACL'02),Philadelphia, July 2002, pp.
136-143.Ronald M. Kaplan, Stephan Riezler, Tracy H. King,John T. Maxwell, Alex Vasserman.
2004.
Speed andaccuracy in shallow and deep stochastic parsing.
InProc HLT/NAACL'04, Boston, MA.Chao-Lin Liu, Jing-Shin Chang, and Keh-Yi Su.
1990.The Semantic Score Approach to the Disambiguationof PP Attachment Problem.
In Proceedings of RO-CLING-90, TaiwanMitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of English: The Penn treebank.
Computa-tional Linguistics, 19(2) pp.313--330.Michael C. McCord.
1993.
Heuristics for Broad-Coverage Natural Language Parsing.
Proceedings ofthe workshop on Human Language Technology 1993,Princeton, NJRyan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
Non-projective dependency parsing usingspanning tree algorithms.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing(EMNLP 2005), Vancouver.Paula S. Newman.
2007.
RH: A Retro-Hybrid Parser.In Short Papers of Proceedings of NAACL/HLT2007, Rochester NYAlan Prince and Paul Smolensky (1993).
OptimalityTheory: Constraint interaction in generative gram-mar, Rutgers University Center for Cognitive Sci-ence, New Brunswick, NJ.
Report RuCCS-TR-2.
[Reprinted in John J McCarthy, ed., Optimality The-ory in Phonology: A Book of Readings, Blackwell(2003).
]Philip Resnik and Marti Hearst.
1993.
Structural Ambi-guity and Conceptual Relations, in Proc.
of 1stWorkshop on Very Large Corpora, 1993.Brian Roark.
2001.
Probabilistic top-down parsing andlanguage modeling.
In Computational Linguistics,27(2), pages 249-276.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proc.
9thInt'l Workshop on Parsing Technologies.
VancouverKristina Toutanova, Christopher D. Manning, DanFlickinger, and Stephan Oepen.
2005.
StochasticHPSG Parse Disambiguation using the RedwoodsCorpus.
Research in Language and Computation2005.Yorick A. Wilks.
1975.
An Intelligent Analyzer andUnderstander of English.
Communications of theACM 18(5), pp.264-274William Woods.
1970.
Transition network grammars fornatural language analysis.
Communications of theACM 13(10), pp.591-606XLE Online Documentation.
2006.
Available athttp://www2.parc.com/isl/groups/nltt/xle/doc/xle.html#SEC15Kiyoshi Yamabana, Shin'ichiro Kamei and KazunoriMuraki.
On Representation of Preference Scores.
InProceedings of The Fifth International Conferenceon Theoretical and Methodological Issues in Ma-chine Translation (TMI-93), Kyoto, pp.
92-10192
