Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
737?745, Prague, June 2007. c?2007 Association for Computational LinguisticsChinese Syntactic Reordering for Statistical Machine TranslationChao WangMIT CSAIL32 Vassar Street, Room 362Cambridge, MA 02139, USAwangc@csail.mit.eduMichael CollinsMIT CSAIL32 Vassar Street, Room G-484Cambridge, MA 02139, USAmcollins@csail.mit.eduPhilipp KoehnSchool of Informatics2 Buccleuch Place, 5BP 2L2Edinburgh, EH8 9LW, UKpkoehn@inf.ed.ac.ukAbstractSyntactic reordering approaches are an ef-fective method for handling word-order dif-ferences between source and target lan-guages in statistical machine translation(SMT) systems.
This paper introduces a re-ordering approach for translation from Chi-nese to English.
We describe a set of syntac-tic reordering rules that exploit systematicdifferences between Chinese and Englishword order.
The resulting system is usedas a preprocessor for both training and testsentences, transforming Chinese sentencesto be much closer to English in terms of theirword order.
We evaluated the reorderingapproach within the MOSES phrase-basedSMT system (Koehn et al, 2007).
Thereordering approach improved the BLEUscore for the MOSES system from 28.52 to30.86 on the NIST 2006 evaluation data.
Wealso conducted a series of experiments to an-alyze the accuracy and impact of differenttypes of reordering rules.1 IntroductionSyntactic reordering approaches are an effectivemethod for handling systematic differences in wordorder between source and target languages withinthe context of statistical machine translation (SMT)systems (Xia and McCord, 2004; Collins et al,2005).
In reordering approaches, sentences in thesource language are first parsed, for example using aTreebank-trained parser.
A series of transformationsis then applied to the resulting parse tree, with thegoal of transforming the source language sentenceinto a word order that is closer to that of the targetlanguage.
The reordering process is used to prepro-cess both the training and test data used within anexisting SMT system.
Reordering approaches havegiven significant improvements in performance fortranslation from French to English (Xia and Mc-Cord, 2004) and from German to English (Collinset al, 2005).This paper describes a syntactic reordering ap-proach for translation from Chinese to English.
Fig-ure 1 gives an example illustrating some of the dif-ferences in word order between the two languages.The example shows a Chinese sentence whose literaltranslation in English is:this is French delegation at WinterOlympics on achieve DEC best accom-plishmentand where a natural translation would bethis is the best accomplishment that theFrench delegation achieved at the WinterOlympicsAs exemplified by this sentence, Chinese differsfrom English in several important respects: for ex-ample, relative clauses appear before the noun beingmodified; prepositional phrases often appear beforethe head they modify; and so on.
It can be seen thatsome significant reordering of the input is requiredto produce a good English translation.
For this ex-ample, application of reordering rules leads to a newChinese string whose word-by-word English para-phrase is:737Before syntactic reordering After syntactic reorderingIP NP PN ?
(this)VP VC(is)NP CP IP NP NR {I(French)NN ?L?
(delegation)VP PP P 3(at)LCP NP NN ?G(Winter)NR$?
(Olympics)LC ?
(on)VP-A VV (achieve)DEC (DEC)ADJP JJ ?
(best)NPB NN ?1(accomplishment)IP NP PN ?
(this)VP VC(is)NP ADJP JJ ?
(best)NPB NN ?1(accomplishment)CP DEC (DEC)IP NP NR {I(French)NN ?L?
(delegation)VP VP-A VV (achieve)PP P 3(at)LCP LC ?
(on)NP NN ?G(Winter)NR$?
(Olympics)Figure 1: Original (left) and reordered (right) parse trees for the Chinese sentence ???{I?L?3?G$???
?1,?
which translates into ?This is the best accomplishment that the Frenchdelegation achieved at the Winter Olympics?
in English.this is best accomplishment DEC Frenchdelegation achieve at on Winter OlympicsThis reordering is relatively easy to express usingsyntactic transformations?for example, it is simpleto move the entire relative clause ?French delega-tion at Winter Olympics on achieve DEC?
to a posi-tion that is after the noun phrase it modifies, namely?best accomplishment.?
Phrase-based systems arequite limited in their ability to perform transforma-tions of this type.
More recently developed hier-archical systems (e.g., (Yamada and Knight, 2001;Chiang, 2005; Marcu et al, 2006)) may be betterequipped to deal with reordering of this type; how-ever, in this example they would effectively have tofirst identify the span of the relative clause, and thenmove it into the correct position, without any explicitrepresentation of the source language syntax.In this paper, we describe a set of syntactic re-ordering rules that exploit systematic differences be-tween Chinese and English word order.
The result-ing system is used as a preprocessor for both trainingand test sentences, transforming Chinese sentencesto be much closer to English.
We report results forthe method on the NIST 2006 evaluation data, us-ing the MOSES phrase-based SMT system (Koehnet al, 2007).
The reordering rules give an improve-ment in accuracy from 28.52 to 30.86 BLEU score.A concern for methods that make use of Chineseparsers is that these parsers are typically of relativelylow accuracy, particularly given that Chinese re-quires a word-segmentation step that is not requiredin languages such as English.
Our results show thatChinese parses are useful in SMT in spite of thisproblem.
We report results showing the precisionof the reordering rules?essentially testing how of-ten the Chinese sentences are correctly reordered?to give more insight into this issue.
We also reportexperiments which assess the impact of each type ofreordering rule on translation accuracy.2 Related WorkA number of researchers (Brown et al, 1992; Bergeret al, 1996; Niessen and Ney, 2004; Xia and Mc-Cord, 2004; Collins et al, 2005) have described ap-proaches that preprocess the source language inputin SMT systems.
We are not, however, aware ofwork on this topic for translation from Chinese toEnglish.
Brown et al (1992) describe an analysiscomponent for French which moves phrases around(in addition to other transformations) so the sourceand target sentences are closer to each other in wordorder.
Berger et al (1996) describe an approach forFrench that reorders phrases of the form NOUN1 deNOUN2.
Xia and McCord (2004) describe an ap-proach for French, where reordering rules that oper-ate on context-free rule productions are acquired au-738tomatically.
Niessen and Ney (2004) describe an ap-proach for translation from German to English thatcombines verbs with associated particles, and alsoreorders questions.
Collins et al (2005) also de-scribe an approach for German, concentrating on re-ordering German clauses, which have quite differentword order from clauses in English.
Our approachis most similar to that of Collins et al (2005).Most SMT systems employ some mechanism thatallows reordering of the source language duringtranslation (i.e., non-monotonic decoding).
TheMOSES phrase-based system that we use has a rel-atively simple reordering model which has a fixedpenalty for reordering moves in the decoder.
Moresophisticated models include reordering parame-ters that are sensitive to lexical information (Till-mann, 2004; Kumar and Byrne, 2005; Koehn etal., 2005).
The model of Chiang (2005) employsa synchronous context-free grammar to allow hi-erarchical approaches to reordering.
The syntax-based models of Yamada and Knight (2001) andMarcu et al (2006) build a full parse tree in the tar-get language, again effectively allowing hierarchi-cal reordering based on synchronous grammars.
Itis worth noting that none of these approaches to re-ordering make use of explicit syntactic informationin the source language?for example, none of themethods make use of an existing source-languageparser (the systems of Yamada and Knight (2001)and Marcu et al (2006) make use of a parser in thetarget language, i.e., English).Finally, note that a number of statistical MTsystems make use of source language syntax intransducer-style approaches; see (Lin, 2004; Dingand Palmer, 2005; Quirk et al, 2005; Liu et al,2006; Huang et al, 2006).
In contrast to the prepro-cessing approach, they attempt to incorporate syntaxdirectly into the decoding stage.3 Chinese Syntactic Reordering RulesWe used the Penn Chinese Treebank guidelines (Xueet al, 2005) in searching for a suitable set of reorder-ing rules.
We examined all phrase types in the Tree-bank; potentially phrases of any type could be can-didates for reordering rules.
Table 1 provides a listof Treebank phrase tags for easy reference.
We ruledout several phrase types as not requiring reorderingADJP adjective phraseADVP adverbial phrase headed by AD (adverb)CLP classifier phraseCP clause headed by C (complementizer)DNP phrase formed by ?XP+DEG?DP determiner phraseDVP phrase formed by ?XP+DEV?FRAG fragmentIP simple clause headed by I (INFL)LCP phrase formed by ?XP+LC?LST list markerNP noun phrasePP preposition phrasePRN parentheticalQP quantifier phraseUCP unidentical coordination phraseVP verb phraseTable 1: Penn Chinese Treebank phrase tags.rules.
For example, Chinese ADJPs, ADVPs, DPs,QPs, and PPs all have similar internal word order-ing to their English counterparts.
Also similar are agroup of special structures such as LST, FRAG, andPRN.We identified three categories that we consideredto be the most prominent candidates for reorder-ing.
These phrases include VPs (verb phrases), NPs(noun phrases), and LCPs (localizer phrases, whichfrequently map to prepositional phrases in English).In the following, we discuss each of the three maincategories in more detail.3.1 Verb PhrasesIn Chinese, verb phrase modifiers typically occur inpre-verbal position.
VP modifiers can be ADVPs,temporal and spatial NPs, QP, PPs, CPs, IPs,DVPs, and LCPs.
The ADVPs are simple adverbs,which can occur both preverbal and postverbal in anEnglish verb phrase, so we do not attempt to movethem.
Similarly, the CP, IP, and DVP modifiersare typically adverbial phrases, which do not have afixed position in English verb phrases.
In the follow-ing, we only consider cases involving PPs, LCPs,temporal and spatial NPs, and QPs.PPs and LCPs Figure 2 shows an example verbphrase with a PP modifier, which translates literally739VP PP P 3(at)NP-A NPB NN ??
(Eastern)NN ??
(Division)VP-A VV ?(rank)QP OD 1?
(10th)Figure 2: Example VP with PP modifier.
The phrasetranslates into ?ranks 10th in the Eastern Division.
?VP NP NPB NT U(same day)NT ??
(morning)VP-A VV uL(issue)NP-A NPB NN (?
(statement)Figure 3: Example VP with temporal NP modifier.The phrase translates into ?issued a statement thatmorning.
?into ?at Eastern Division rank 10th.?
Recognizingthat PPs in English verb phrases almost always oc-cur after the verb, we use a simple VP(PP:VP) re-ordering rule which states that a PP in a parent VPneeds to be repositioned after the sibling VP.
LCPsare similar to PPs and typically map to prepositionalphrases in English.
Thus they are handled similarlyto PPs, i.e., LCPs in a parent VP are repositionedafter the sibling VP.NPs Figure 3 gives an example of a verb phrasewith a temporal NP modifier, which literally trans-lates into ?same day morning issue statement.?
InEnglish, temporal phrases such as these almost al-ways occur after the head verb.
Conveniently, theChinese Treebank uses the part of speech (POS) tagNT for temporal nouns.
Thus, we use a rule whichstates that a preverbal NP will be repositioned af-ter the sibling VP if there is at least one NT in theNP subtree.
A similar rule might apply to locativeNPS; however, there is no special POS tag in theTreebank marking locations,1 so we do not have asyntax-based reordering rule to handle locative NPs.QPs QP modifiers in verb phrases often corre-spond to time-related concepts such as duration andfrequency.
Figure 4 shows an example verb phrasewith a QP modifier, literally translating into ?manytime injured.?
Since temporal phrases almost alwaysoccur after the verb in English verb phrases, we han-1One can argue that NR (proper nouns) in that context arelikely to be places.
However, there also exist many exceptions,and so we decided not to exploit the NR tag.VP QP CD ?
(many)CLP M g(time)VP-A VV ??
(injured)Figure 4: Example VP with QP modifier.
The phrasetranslates into ?injured many times.
?NP-A DNP PP P ?
(to)NP-A NPB NR 9n??
(Zimbabwe)DEG (DEG)NPB NN ?L(financial)NN ?
(aid)Figure 5: An example Chinese NP with a DNP mod-ifier headed by a PP.
The phrase translates into ?thefinancial aid to Zimbabwe?
in English.dle such cases by a simple rule which states that theQP in a parent VP will be repositioned after the sib-ling VP.3.2 Noun PhrasesNoun phrases in Chinese can take several types ofmodifiers: for example, phrases of type QP, DP,ADJP, NP, DNP, and CP.
The placement of QP, DP,and ADJP modifiers is somewhat similar to Englishin that these phrases typically occur before the nounthey modify.
The case of NP modifiers in NPs isvery limited in the Chinese Treebank, since mostnoun-noun sequences form compounds in a singleNP.
Hence we only developed reordering rules tohandle DNP and clausal (CP) modifiers.DNPs DNPs are formed by ?XP+DEG,?
where XPcan be a phrase of the type ADJP, QP, PP, LCP, orNP.
When the XP is an ADJP or a QP, no reorderingis needed because the word order is the same as thatof English.When the XP is a PP or an LCP, the DNP essen-tially corresponds to a prepositional phrase in En-glish, which almost always appears after the nounbeing modified.
Figure 5 shows an example wherethe XP in the DNP is a PP.
The reordering rule tohandle these two cases states that, if a parent NP hasa child DNP which in turn has a child PP or LCP,then the DNP is repositioned after the last sibling NP.Figure 6 shows an example noun phrase for whichthe XP in the DNP is NP.
On the surface, the Chinese?NP1 DEG NP2?
sequence is analogous to the En-glish possessive structure of ?NP1?s NP2?
and does740NP-A DNP NP DP DT T(this)CLP M ?
(measure word)NPB NN E?
(technique)DEG (DEG)NPB NN ??
(mastery)Figure 6: An example Chinese NP phrase with aDNP modifier headed by a NP.
The phrase translatesinto ?the mastery of this technique?
in English.not require reordering, for example, ??
(Sue) (?s)*l(friend)?
in Chinese and ?Sue?s friend?
in En-glish.
However, the Chinese possessive structure?NP1 DEG NP2?
can express more sophisticated re-lationships which are inappropriate for the ?NP1?sNP2?
expression.
For example, the phrase in Fig-ure 6 can only be translated into ?the mastery ofthis technique,?
but not ?this technique?s mastery.
?We decide to reorder DNPs of the ?NP+DEG?
for-mat, because they often can only map to the ?NP2 ofNP1?
expression in English.
Additionally, the ?NP2of NP1?
expression is more general and can replace?NP1?s NP2?
in many cases.
One exception is whenthe NP is a pronoun (PN), e.g., ?(he) (?s) ?i(name),?
in which case the DNP acts simply like apossessive pronoun.
Our reordering rule thus statesthat, if a parent NP has a child DNPwhich in turn hasa child NP that is not a PN, then the DNP is reposi-tioned after the last sibling NP.CPs Relative clauses correspond to the CP cate-gory in the Treebank.
Figure 7 shows an examplenoun phrase with two nested CP modifiers.
As illus-trated in the figure, relative clauses in Chinese alsooccur before the noun they modify, which makesthe word order of this sentence quite different fromthat of the English translation.
Such distortions inthe word reordering will be quite difficult for theword or phrase-based alignment model to capture.However, with the application of a reordering ruleto reposition the child CP after its sibling NP un-der a parent NP, and the PP VP reordering rule forVP introduced previously, the sentence can be easilytransformed into ?French delegation participate 8thhandicap people Winter Olympics hold at US SaltLake City,?
a sentence whose word order is muchcloser to that of English.CP is typically formed by ?IP+DEC?, in whichDEC?s only function is to mark the IP as a relativeNP CP IP VP VV ?\ (participate)NP CP IP VP PP P 3 (at)NP NR {I(US)NR ?
?
(Salt Lake City)VP VV ?1 (hold)DEC  (DEC)QP OD 1l (8th)CLP M 3 (measure word)NPB NN ?
;<(handicap people)NR ??
(Winter Olympics)DEC  (DEC)NPB NR {I (French)NPB NN ?L?
(delegation)Figure 7: An example with two nested CP modi-fiers.
The phrase translates into ?the French delega-tion participating in the 8th Special Winter Olympicsheld in Salt Lake City US.
?LCP IP NP-A NPB NN ?(accident)VP VV u)(happen)LC  (after)Figure 8: An example Chinese localizer phrase.
Thephrase translates into ?after the accident happened?in English.clause, similar to the function of ?that?
in English.We use a rule to bring DEC to the front of IP underCP, to make it more aligned with the ?that + clause?structure of English.3.3 LocalizersFigure 8 shows an example phrase of the type LCP.Localizers (tagged LC in the Treebank) in Chi-nese can be thought of as a post-phrasal preposi-tion which is often used with temporal and locativephrases or clauses to mark directional information.They function similarly to prepositions and conjunc-tions in English such as ?before,?
?on,?
?when,?
etc.Constituents of type LCP have a similar functionto prepositional phrases.
Sometimes they are com-bined with a pre-phrasal generic preposition ?3?
(roughly corresponding to ?at?
in English) to forma PP explicitly.
An example is shown in Figure 9.We developed a simple reordering rule whichmoves an LC node to immediately before its left sib-ling under a parent LCP node.
This will result in aword order that is more similar to that of the English741PP P 3(at)LCP IP NP-A NPB NN ?(accident)VP VV u)(happen)LC  (after)Figure 9: An example Chinese PP encompassing anLCP.
The phrase translates into ?after the accidenthappened?
in English.prepositional phrase: the example in Figure 8 hasthe paraphrase ?after accident happen?
after the re-ordering rule is applied.
In the case where an LCP isembedded in a parent PP phrase, the LC reorderingrule will essentially merge the post-phrasal localizerwith the pre-phrasal preposition.
For example, thephrase in Figure 9 becomes ?at after accident hap-pen?
after reordering.
The phrase-based SMT sys-tem will have little problem in learning that ?at af-ter?
translates into ?after?
in English.4 EvaluationOur baseline is a phrase-based MT system trainedusing the MOSES toolkit (Koehn et al, 2007).The training data consists of nearly 637K pairs ofsentences from various parallel news corpora dis-tributed by the Linguistic Data Consortium (LDC).2For tuning and testing, we use the official NISTMT evaluation data for Chinese from 2002 to 2006,which have four human generated English referencetranslations for each Chinese input.
The evaluationdata from 2002 to 2005 were split into two sets ofroughly equal sizes: a tuning set of 2347 sentencesis used for optimizing various parameters using min-imum error training (also using the MOSES toolkit),and a development set of 2320 sentences is used forvarious analysis experiments.
We report results onthe NIST 2006 evaluation data.A series of processing steps are needed before thereordering rules can be applied, which include seg-mentation, part-of-speech tagging, and parsing.
Wetrained a Chinese Treebank-style tokenizer and part-of-speech tagger, both using a tagging model basedon a perceptron learning algorithm (Collins, 2002).We used the Chinese parser described by Sun andJurafsky (2004), which was adapted from the parser2We used 8 corpora for training, including LDC2002E18,LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06,LDC2006E26, LDC2006E8, and LDC2006G05.Dev Nist06Baseline 31.57 28.52Reorder 32.86 30.86Gain +1.29 +2.34Table 2: BLEU score of the baseline and reorderedsystems.presented in Collins (1997).
We then applied the re-ordering rules described in the previous section tothe parse tree of each input.
The reordered sen-tence is then re-tokenized to be consistent with thebaseline system, which uses a different tokenizationscheme that is more friendly to the MT system.3We use BLEU scores as the performance measurein our evaluation (Papineni et al, 2002).
Table 2gives results for the baseline and reordered systemson both the development and test sets.
As shown inthe table, the reordering method is able to improvethe BLEU scores by 1.29 points on the developmentset, and by 2.34 on the NIST 2006 set.4.1 Frequency and Accuracy of ReorderingRulesWe collected statistics to evaluate how often and ac-curately the reordering rules are applied in the data.The accuracy is measured in terms of the percent-age of rule applications that correctly reorder sen-tences.
The vast majority of reordering errors aredue to parsing mistakes.Table 3 summarizes the count of each rule inthe training data, ignoring rules occurring less than500 times in the training data, and the numberof sentences each rule impacts.
The most fre-quent three rules are NP(CP:NP), VP(PP:VP),and DNP(NP):NP, which account for over 76% ofall the reordering instances and jointly affect 74%of all the training sentences.
This shows the preva-lence of systematic word order differences betweenChinese and English.
Only 122,076 (or 19.2%) sen-tences remain unchanged after the reordering rulesare applied.Each of the processing steps in producing the Chi-nese parse tree is prone to error and could lead tomistakes in the reordering of the Chinese sentence.3The tokenizer used by the MT system favors smaller wordunits, and backs off to a character by character scheme for un-known words.742Type Rule Name Counts # Sent.VP VP(PP:VP) 331,827 258,214VP(NT:VP) 23,353 22,926VP(LCP:VP) 8,674 8,661VP(QP:VP) 7,834 7,777NP NP(CP:NP) 345,165 262,588DNP(NP):NP 280,367 218,865DNP(PP):NP 38,225 36,295DNP(LCP):NP 15,801 15,253LC LCP(NP:LC) 146,784 12,8333LCP(IP:LC) 36,923 35,749LCP(QP:LC) 14,893 14,287Total 1,249,846 636,686Table 3: Statistics of various reordering rules in thetraining data.To assess the accuracy of reordering rules, we con-ducted human evaluations on a set of 200 sentencesrandomly selected from the development set.
Withinthis set, there were in total 155 sentences containingat least one reordering rule, with 339 rules in total.A bilingual speaker was presented with the Chineseparse tree, the sentence before and after the reorder-ing, and the particular reordering rules applied to thesentence.
The bilingual rater determined the correct-ness of each rule by first identifying the scope of therule and comparing the string before and after re-ordering, referencing the corresponding parse struc-ture if necessary.
Table 4 summarizes the accuracy(precision) for each type of rule.
Notice that our hu-man evaluation of the reordering rules does not takeinto account missed reordering.Overall, there are a lot of reordering errors causedby incorrect parses.
On a sentence level, only 57out of the 155 reordered sentences (36.8%) are errorfree.
Nevertheless, syntactic reordering seems to behelpful in improving the translation quality, despitenoise introduced into the data due to the errors.4.2 Impact of Individual Reordering RulesIn order to assess the relative effectiveness of thereordering rules, we conducted an experiment inwhich we trained and tested systems using datathat were reordered using different subsets of thereordering rules.
Table 5 summarizes the BLEUscores of the reordered system for each rule type.Count AccuracyVP rules 108 65.7%NP rules 209 54.6%LC rules 76 77.6%All rules 393 62.1%Table 4: Accuracy of reordering rules on a set of 200sentences randomly selected from the developmentset.BLEU GainBaseline 31.57 -VP rules 32.71 +1.14NP rules 32.23 +0.66LC rules 31.59 +0.02All rules 32.86 +1.29Table 5: Comparison of translation performancewith different types of reordering rules.
Gain is thechange in BLEU score when compared to the base-line system.
All results are on the development set.As shown in the table, the VP rules are more effec-tive than the NP rules, even though the NP rules aremore frequent than the VP rules in the data.
Thisis perhaps because the reordering of VP modifiersachieves a slightly higher accuracy than that of theNP modifiers.
We are a bit surprised by the lackof performance gains with the LC rules only.
Moreanalysis is needed to explain this behavior.4.3 Better Alignment?There could be two reasons why the syntacticreordering approach improves over the baselinephrase-based SMT system.
One obvious benefit isthat the word order of the transformed source sen-tence is much closer to that of the target sentence,which reduces the reliance on the distortion modelto perform reordering during decoding.
Another po-tential benefit is that the alignment between the twosides will be of higher quality because of fewer ?dis-tortions?
between the source and the target, so thatthe resulting phrase table of the reordered systemwould be better.
However, a counter argument is thatthe reordering is very error prone, so that the addednoise in the reordered data would actually hurt thealignments and hence the phrase table.Lacking a good way to measure the quality of743Original Dev Reordered DevBaseline 31.57 32.19Reorder 30.67 32.86Table 6: Comparison of BLEU scores in matchedand mismatched conditions.
The baseline and re-ordered systems were first tuned on mismatched databefore being tested on mismatched data.the phrase table directly, we conducted an experi-ment in which we tested the baseline and reorderedsystems with both the original and reordered devel-opment data.
The idea is to compare the two sys-tems given the same type of input: if the reorderedsystem learned a better phrase table, then it mightoutperform the baseline system on un-reordered in-puts despite the mismatch; on the other hand, if thebaseline system learned a better phrase table, then itmight outperform the reordered system on reorderedinputs despite the mismatch.
However, the results inTable 6 did not settle our question: the reorderedsystem performed worse than the baseline on unre-ordered data, while the baseline system performedworse than the reordered system on reordered data,both of which can be explained by the mismatchedconditions between training and testing.
Perhapsmore interesting is the performance gap of the base-line system on the reordered data vs. on the originaldata: it achieved 0.62 BLEU score gain despite themismatch in training and testing conditions.5 Discussion and Future WorkIn this paper, we described a set of syntactic reorder-ing rules that exploit systematic differences betweenChinese and English word order to transform Chi-nese sentences to be much closer to English in termsof their word order.
We evaluated the reordering ap-proach within the MOSES phrase-based SMT sys-tem (Koehn et al, 2007).
The reordering approachimproved the BLEU score for the MOSES systemfrom 28.52 to 30.86 on the NIST 2006 evaluationdata.
Our manual evaluation of the reordering accu-racy indicated that the reordering approach is help-ful at improving the translation quality despite rel-atively frequent reordering errors.
The reorderingapproach even achieved a 0.62 gain in BLEU scorewhen only the test data are reordered.An important category we examined but did notreorder was clauses of type IP, which generallycorresponds to declarative sentences in Chinese.Sentences of this form have quite similar top-levelconstituent ordering to English: both follow SVO(subject-verb-object) order.
There are several spe-cial cases in which English and Chinese differ, themost notable being the topicalization of objects ortemporal and locative noun phrases (which functionas adverbial phrases).
We did not try to restore themto the canonical order for several reasons.
First, top-icalization of temporal and locative phrases happensin English as well.
For example, ?In Israel yesterday,an explosion killed one person and injured twelve?is a perfectly acceptable English sentence.
Second,the parser?s performance on special constructions islikely to be poor, resulting in frequent reordering er-rors.
Third, special constructions that do not occuroften in the data are less likely to have a significantimpact on the translation performance.
Thus ourstrategy has been to find reordering rules for syntac-tic categories that are common in the data and sys-tematically different between the two languages.In our experiments, the phrase-based MT sys-tem uses an un-lexicalized reordering model, whichmight make the effects of the syntactic reorderingmethod more pronounced.
However, in an early ex-periment4 submitted to the official NIST 2006 MTevaluation, the reordered system also improved theBLEU score substantially (by 1.34 on NIST 2006data) over a phrase-based MT system with lexical-ized reordering models (Koehn et al, 2005).
Thesame set of reordering rules in the experimental set-ting in the current paper achieve a 1.82 BLEU im-provement on the same data set, which is compara-ble to the 1.34 gain for the lexicalized system.We plan to output reordered lattices in the future,so that the approach would be more robust to errorsmade during parsing/reordering.AcknowledgementsWe would like to thank Brooke Cowan, StephanieSeneff, and the three anonymous reviewers for theirvaluable comments.
Thanks to Yushi Xu for evalu-ating the accuracy of the reordering rules.
This work4This experiment made use of a subset of the reorderingrules we have presented here.744was supported under the GALE program of the De-fense Advanced Research Projects Agency, ContractNo.
HR0011-06-C-0022.ReferencesAdam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?69.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, John D. Lafferty, and Robert L. Mercer.
1992.Analysis, statistical transfer, and synthesis in machinetranslation.
In Proceedings of Conference on Theoret-ical and Methodological Issues in Machine Transla-tion.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL, pages 263?270.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Poceedings of ACL, pages 531?540.Michael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Proceedings of ACL.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP.Yuan Ding and Martha Palmer.
2005.
Machine transla-tion using probablistic synchronous dependency inser-tion grammars.
In Proceedings of ACL, pages 541?548, Ann Arbor, Michigan.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, and Chris Callison-Burch.
2005.
Edinburghsystem description.
In IWSLT Speech TranslationEvaluation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-strantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL, Demonstration Session.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In Proceedings of HLT-EMNLP.Dekang Lin.
2004.
A path-based transfer model formachine translation.
In Proceedings of Coling 2004,pages 625?630, Geneva, Switzerland, Aug 23?Aug27.
COLING.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of ACL, pages 609?616.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proceedings of EMNLP, pages 44?52, Sydney, Aus-tralia.Sonja Niessen and Hermann Ney.
2004.
Statistical ma-chine translation with scarce resources using morpho-syntactic information.
Computational Linguistics,30(2):181?204.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of ACL, pages 271?279,Ann Arbor, Michigan.Honglin Sun and Daniel Jurafsky.
2004.
Shallow se-mantic parsing of Chinese.
In Proceedings of NAACL-HLT.Christoph Tillmann.
2004.
A block orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL, Boston, MA, USA.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical MT system with automatically learned rewritepatterns.
In Proceedings of COLING.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese Treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of ACL.745
