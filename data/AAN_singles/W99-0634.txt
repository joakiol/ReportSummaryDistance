Corpus-Based Learning for Noun Phrase Coreference ResolutionWee Meng SoonHwee Tou  NgChung Yong L imDSO National Laboratories20 Science Park Drive, Singapore 118230{sweemeng ,nhweetou, ichungyo}?dso, org.
sgAbst ractIn this paper, we present a learning approachfor coreference resolution of noun phrases inunrestricted text.
The approach learns froma small, annotated corpus and the task in-cludes resolving not just pronouns but rathergeneral noun phrases.
In contrast o previouswork, we attempt o evaluate our approach ona common data set, the MUC-6 coreference or-pus.
We obtained encouraging results, indicat-ing that on the general noun phrase coreferencetask, the learning approach olds promise andachieves accuracy comparable to non-learningapproaches.1 In t roduct ionCoreference resolution refers to the process ofdetermining if two expressions in natural an-guage refer to the same entity in the world.It is an important subtask in natural anguageprocessing systems.
In particular, informa-tion extraction (IE) systems like those builtin the DAI:tPA Message Understanding Confer-ences (Chinchor, 1998; Sundheim, 1995) haverevealed that coreference resolution is such acritical component of IE systems that a sepa-rate coreference subtask has been defined andevaluated since MUC-6 (Committee, 1995).In this paper, we focus on the task of deter-mining coreference r lations as defined in MUC-6 (Committee, 1995).
Specifically, a coreferencerelation denotes an identity of reference andholds between two textual elements known asmarkables, which are nouns, noun phrases, orpronouns.
Thus, our coreference task resolvesgeneral noun phrases and not just pronouns,unlike in some previous work on anaphora res-olution.
The ability to link co-referring nounphrases both within and across entences i  crit-ical to discourse analysis and language under-standing in general.2 A Learn ing  Approach  forCore ference  Reso lu t ionWe adopt a corpus-based, learning approach fornoun phrase coreference r solution.
In this ap-proach, we need a relatively small corpus oftraining documents that have been annotatedwith coreference chains of noun phrases.
Allpossible markables in a training document aredetermined by a pipeline of language process-ing modules, and training examples in the formof feature vectors are generated for appropri-ate pairs of markables.
These training exam-ples are then given to a learning algorithm tobuild a classifier.
To determine the coreferencechains in a new document, all markables aredetermined and potential pairs of co-referringmarkables are presented to the classifier whichwill decide whether the two markables actuallyco-refer.
We give the details of these steps inthe following subsections.2.1 Determinat ion  of  MarkablesA pre-requisite for coreference resolution is toobtain most, if not all, of the possible mark-ables in a raw input text.
To determine themarkables, a pipeline of natural anguage pro-cessing (NLP) modules is used.
They consist ofsentence segmentation, tokenization, morpho-logical analysis, part-of-speech tagging, nounphrase identification, amed entity recognition,and semantic lass determination.
As far ascoreference r solution is concerned, the goal ofthese NLP modules is to determine the bound-ary of the markables, and to provide the neces-sary in.formation about each markable for sub-sequent generation of features in the trainingexamples.Our part-of-speech tagger is a standard sta-285tistical bigram tagger based on the HiddenMarkov Model (HMM) (Church, 1988).
Sim-ilarly, we built a statistical HMM-based nounphrase identification module where the nounphrase boundaries are determined solely basedon the part-of-speech tags assigned to the wordsin a sentence.
We also implemented a mod-ule that recognizes MUC-style named entities,i.e., organization, person, location, date, time,money, and percent.
Our named entity recogni-tion module uses the HMM approach of (Bikelet al, 1999; Bikel et al, 1997), which learnsfrom a tagged corpus of named entities.
Thatis, our part-of-speech tagger, noun phrase iden-tification module, and the named entity recog-nition module are all based on HMM and learnfrom corpora tagged with parts-of-speech, nounphrases, and named entities, respectively.
Themarkables needed for coreference resolution isthe union of the noun phrases and named enti-ties found.To achieve high accuracy in coreference r so-lution, it is most critical that the eligible candi-dates for coreference are identified correctly inthe first place.
In order to test the effective-ness of our system in determining the mark-ables, we attempted to match the markablesgenerated by our system against hose appear-ing in the coreference chains annotated in 100SGML documents, a subset of the documentsavailable in MUC-6.
We found that our systemis able to correctly identify about 85% of thenoun phrases appearing in coreference chains inthe 100 annotated SGML documents.
Most ofthe unmatched noun phrases are of the followingtypes: (1) Our system generated a head nounwhich is a subset of the noun phrase in the anno-tated corpus.
For example, "Saudi Arabia, thecartel's biggest producer," was annotated as amarkable but our system generated only "SaudiArabia".
(2) Our system extracted a sequenceof words that cannot be considered as a mark-able.
(3) Unclear notion of what constitutes amarkable.
For example, "wage reductions" wasannotated, but "selective wage reductions" wasidentified by our system instead.2.2 Determination of Feature VectorsFeature vectors are required for training andtesting the coreference ngine.
A feature vec-tor consists of 10 features described below, andis derived based on two extracted markables, iand j, where i is the antecedent and j is theanaphor.
In.formation eeded to derive the fea-ture vectors is provided by the pipeline of lan-guage modules prior to the coreference engine.1.
Distance Feature Its possible values are0, 1, 2, 3, .
.
.
.
This feature captures thedistance between i and j.
If i and j are inthe same sentence, the value is 0; if theyare 1 sentence apart, the value is 1; and soon.2.
Pronoun Feature Its possible values aretrue or false.
If j is a pronoun, return true;else return false.
Pronouns include reflexivepronouns (himself, herself), personal pro-nouns (he, him, you), and possessive pro-nouns (hers, her).3.
String Match Feature Its possible valuesare true or false.
If the string of i matchesthe string of j ,  return true; else return false.4.
Definite Noun Phrase Feature Its pos-sible values are true or false.
In our def-inition, a definite noun phrase is a nounphrase that starts with the word "the".For example, "the car" is a definite nounphrase.
If j is a definite noun phrase, re-turn true; else return false.5.
Demonstrat ive Noun Phrase FeatureIts possible values are true or false.
Ademonstrative noun phrase is one thatstarts with the word "this", "that", "these"or "those".
If j is a demonstrative nounphrase, then return true; else return false.6.
Number Agreement Feature Its possi-ble values are true or false.
If i and j agreein number, i.e., they are both singular orboth plural, the value is true; otherwisefalse.
Pronouns such as "they", "them",etc., are plural, while "it", "him", etc., aresingular.
The morphological root of a nounis used to determine whether it is singularor plural if the noun is not a pronoun.7.
Semantic Class Agreement FeatureIts possible values are true, false, orunknown.
In our system, we definedthe following semantic classes: "female","male", "person", "organization", "loca-tion", "date", "time", "money", "percent",and "object".
These semantic lasses are286arranged in a simple ISA hierarchy.
Each ofthe "female" and "male" semantic lassesis a subclass of the semantic lass "per-son", while each of the semantic lasses"organization", location", "date", "time","money", and "percent" is a subclass of thesemantic lass "object".
Each of these de-fined semantic lasses is then mapped to aWORDNET synset (Miller, 1990).
For ex-ample, "male" is mapped to sense 2 of thenoun "male" in WORDNET, "location" ismapped to sense 1 of the noun "location",etc.The semantic lass determination moduleassumes that the semantic lass for everymarkable xtracted is the first sense of thehead noun of the markable.
Since WORD-NET orders the senses of a noun by theirfrequency, this is equivalent to choosing themost frequent sense as the semantic lassfor each noun.
If the selected semantic classof a markable is a subclass of one of our de-fined semantic lass C, then the semanticclass of the markable is C, else its semanticclass is "unknown".The semantic lasses of markables i and jare in agreement if one is the parent of theother (e.g., "chairman" with semantic lass"person" and "Mr. Lim" with semanticclass "male"), or both of them are the same(e.g., "Mr. Lira" and "he" both of semanticclass "male").
The value returned for suchcases is true.
If the semantic lasses of iand j are not the same (e.g.
"IBM" with se-mantic lass "organization" and "Mr. Lim"with semantic lass "male"), return false.If either semantic lass is "unknown", thenthe head nouns of both markables are com-pared.
If they are the same, return true,else return unknown.8.
Gender  Agreement  Feature  Its possi-ble values are true, false, or unknown.
Thegender of a markable is determined in sev-erai ways.
Designators and pronouns uchas "Mr.", "Mrs.", "she", "he", etc., can de-termine the gender.
For a markable that isa person's name such as "Peter H. Diller',the gender cannot be determined by theabove method.
In our system, the gender ofsuch a markable can only be determined ifthere are markables found later in the doc-.10.ument hat refer to "Peter H. DiUer" by us-ing the designator-form of the name, suchas "Mr. Diller".
The gender of a mark-able will be unknown for noun phrases uchas "the president", chief executive officer",etc.
If the gender of either markable i orj is unknown, then the gender agreementfeature value is unknown; else if i and jagree in gender, then the feature value istrue; otherwise its value is false.P roper  Name Feature  Its possible val-ues are true or false.
A proper name is de-termined based on capitalization.
Preposi-tions appearing in the name such as "off,"and", etc., need not be in upper case.
If iand j are both proper names, return true;else return false.Alias Feature  Its possible values are trueor false.
If i is an alias of j or vice versa,return true; else return false.
That is, thisfeature value is true if i and j are propernames that refer to the same entity.
For ex-ample, the pairs "Mr. Simpson" and "BentSimpson", "IBM" and "International Busi-ness Machines Corp.", "SEC" and "the Se-curities and Exchange Commission", "Mr.Dingell" and "Chairman John DingeU", arealiases.
However, the pairs "Mrs. Washing-ton" and "her", "the talk" and "the meet-ing", are not aliases.2.3 Generat ing  Tra in ing ExamplesConsider a coreference hain A1 - A2 - A3 - A4found in an annotated training document.
Onlypairs of noun phrases in the chain that are im-mediately adjacent (i.e., A1 - A2, A2 - A3, andA3 - A4) are used to generate the positive train-ing examples.
The first noun phrase in a pair isalways considered the antecedent while the sec-ond is the anaphor.
On the other hand, nega-tive training examples are extracted as follows.For each antecedent-anaphor pai , first obtainall markables between the antecedent and theanaphor.
These markables are either not foundin any coreference hain or they appear in otherchains.
Each of them is then paired with theanaphor to form a negative xample.
For ex-ample, if markables a, b, B1 appear betweenA1 and A2, then the negative xamples are a- A2, b - A2 and B1 - A2.
Note that a and b287do not appear in any coreference chain while B1appears in another coreference chain.For an annotated noun phrase in a coreferencechain in a training document, the same nounphrase must be identified as a markable by ourpipeline of language processing modules beforethis noun phrase can be used to form a featurevector for use as a training example.
This isbecause the information ecessary to derive afeature vector, such as semantic lass and gen-der, is computed by the language modules.
Ifan annotated noun phrase is not identified as amarkable, it will not contribute any training ex-ample.
Note that the language modules are alsoneeded to identify markables not already anno-tated in the training document so that they canused for generating the negative xamples.2.4 Building a ClassifierThe next step is to use a machine learning algo-rithm to learn a classifier based on the featurevectors generated from the training documents.The learning algorithm used in our coreferenceengine is C4.5 (Quinlan, 1993).
C4.5 is a com-monly used machine learning algorithm andthus it may be considered as a baseline methodagainst which other learning algorithms can becompared.2.5 Generating Coreference Chains forTest DocumentsBefore determining the coreference chains for atest document, all possible markables need tobe extracted from the document.
Every mark-able is a possible anaphor, and every mark-able before the anaphor in document order isa possible antecedent of the anaphor, exceptwhen the anaphor is nested.
If the anaphoris a child or nested markable, then its possi-ble antecedents must not be any markable withthe same root markable as the current anaphor.However, the possible antecedents can be otherroot markables and their children that are be-fore the anaphor in document order.
For exam-ple, consider the 2 root markables, "Mr. Tom'sdaughter" and "His daughter's eyes", appearingin that order in a test document.
The possi-ble antecedents of "His" cannot be "His daugh-ter", nor "His daughter's eyes", but can be "Mr.Tom" or "Mr. Tom's daughter".The coreference resolution algorithm consid-ers every markable j starting from the secondmarkable in the document to be a potential can-didate as an anaphor.
For each j, the algorithmconsiders every markable before j as a potentialantecedent.
For each pair i and j ,  a feature vec-tor is generated and given to the decision treeclassifier.
A co-referring antecedent is found ifthe classifier eturns true.
The algorithm startsfrom the immediately preceding markable andproceeds backwards in the reverse order of themarkables in the document until there is nomore markable to test or an antecedent is found.3 Eva luat ionIn order to evaluate the performance of ourlearning approach to coreference resolution ona common data set, we utilized the annotatedcorpus and scoring program from MUC-6, whichassembled a set of newswire documents anno-tated with coreference chains.
Although we didnot participate in MUC-6, we were able to ob-tain the MUC-6 training and test corpus fromthe MUC organizers for research purpose.
1 30dry-run documents annotated with coreferenceinformation were used as the training docu-ments for our coreference ngine.
After train-ing the engine, we tested its accuracy on the30 formal test documents in MUC-6.
These 30test documents are exactly those used to evalu-ate the systems that participated in the MUC-6evaluation.Our implemented system runs on a PentiumII 400MHz PC.
The total size of the 30 train-ing documents i close to 13,000 words.
It tookless than five minutes to generate the trainingexamples from these training documents.
Thetraining time for the C4.5 algorithm to generatea decision tree from all the training exampleswas about 30 seconds.
The decision tree classi-fier learned (using a pruning confidence l vel of25%) is shown in Figure 1.One advantage of using a decision tree learn-ing algorithm is that the resulting decision treeclassifier built can be interpreted by human.The decision tree in Figure 1 seems to encap-sulate a reasonable rule-of-thumb that matchesour intuitive linguistic notion of when two nounphrases can co-refer.
It is also interesting tonote that only five out of the ten available fea-tures in the training examples are actually usedin the final decision tree built.1Contact Beth Sundheim at sundheim~nosc.mil288s t r ing  matching?+ pronoun anaphor ?gender matching?
a l ias?distance?
- - ++O.1009080706050403020.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i x  .
.
.
.
.
.
.
.
.
* i  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
\[\].
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~Y .
.
.
.
.
.
, .
.~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.x10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.010 20 30 40 50 60 70 80 90 1 O0RecallFigure 1: The decision tree classifier learnedWhen given new test documents, the outputof the coreference engine is in the form of SGMLfiles with the coreference chains properly anno-tated according to the MUC-6 guidelines.
Thetime taken to generate the coreference chainsfor 30 test documents of close to 14,000 wordswas less than three minutes.
We then used thescorer program of MUC-6 to generate the recalland precision score for our coreference engine.Our coreference ngine achieves a recall of52% and a precision of 68%, yielding a balancedF-measure of 58.9%.
We plotted the score ofour coreference engine (square-shaped) againstthe other official test scores of MUC-6 systems(cross-shaped) in Figure 2.
We also plotted thelearning curve of our coreference engine in Fig-ure 3, showing its accuracy averaged over fiverandom trials when trained on 5, 10, .
.
.
,  30training documents.Our score is in the upper region of the MUC-6 systems.
We performed a simple two-tailed,paired t-test at p = 0.05 to determine whetherthe difference between our system's F-measurescores and each of the other MUC-6 systems'F-measure scores on the 30 formal test docu-ments is statistically significant.
We found thatat the 95% significance level, our system per-formed worse than one, better than two, and aswell as the rest of the MUC-6 systems.
Our re-sult is encouraging as it indicates that a learningapproach using relatively shallow features and asmall number of training documents can lead toscores that are comparable to systems built us-Figure 2: Coreference scores of MUC-6 systemsand our system6260585654525048J /I I I I I5 10 15 20 25 30Number of Documents Trained OnFigure 3: Learning curve of coreference r solu-tion accuracying non-learning approaches.It should be noted that the accuracy of ourcoreference r solution engine depends to a largeextent on the performance of the NLP modulesthat are executed before the coreference engine.Our current learning-based, HMM named en-tity recognition module is trained on 318 doc-uments (a disjoint set from the 30 formal testdocuments) tagged with named entities, and itsscore on the MUC-6 named entity task for the30 formal test documents i only 88.9%, whichis not considered very high by MUC-6 standard.For example, our named entity recognizer could289not identify the two named entities "USAir" and"Piedmont" in the expression "USAir and Pied-mont" but instead treat it as one single namedentity.
Also, some of the features uch as num-ber agreement, gender agreement and seman-tic class agreement are difficult to determine attimes.
For example, "they" is sometimes usedto refer to "the government" even though su-perficially both do not seem to agree in number.All these problems hurt the performance of thecoreference engine.4 Re la ted  WorkThere is a long tradition of work on coreferenceresolution within computational linguistics, butmost of them are not subjected to empiricalevaluation until recently.
Among the work thatreported quantitative valuation results, mostare not based on learning from an annotatedcorpus (Baldwin, 1997; Kameyama, 1997; Lap-pin and Leass, 1994; Mitkov, 1997).To our knowledge, the work of (Aone andBennett, 1995; Ge et al, 1998; McCarthy andLehnert, 1995) are the only ones that are basedon learning from an annotated corpus.
Ge etal.
(Ge et al, 1998) used a statistical modelfor resolving pronouns.
In contrast, we useda decision tree learning algorithm and resolvedgeneral noun phrases, not just pronouns.
Boththe work of (Aone and Bennett, 1995; Mc-Carthy and Lehnert, 1995) employed ecisiontree learning.
However, the features they usedinclude domain-specific ones like DNP-F (def-inite NP whose referent is a facility) (Aoneand Bennett, 1995), JV-CHILD-i (does i re-fer to a joint venture formed as the result ofa tie-up) (McCarthy and Lehnert, 1995), etc.In contrast, all our 10 features are domain-independent, which makes our coreference en-gine a domain-independent module.
Moreover,both (Aone and Bennett, 1995) and (McCarthyand Lehnert, 1995) made simplifying assump-tions in their experimental evaluations.
Sincethe accuracy of coreference r solution relies onthe correct identification of the candidate nounphrases, both (Aone and Bennett, 1995) and(McCarthy and Lehnert, 1995) only evaluatedtheir systems on noun phrases that have beencorrectly identified.
In contrast, we evaluatedour coreference r solution engine as part of a to-tal system which has to first identify all the can-didate noun phrases and has to deal with the in-evitable noisy data when mistakes occur in nounphrase identification.
Also, the evaluation of(Aone and Bennett, 1995) and (McCarthy andLehnert, 1995) only focused on specific types ofnoun phrases (organizations and business enti-ties), and (Aone and Bennett, 1995) dealt onlywith Japanese texts.
Our evaluation was doneon all types of English noun phrases instead.None of the systems in MUC-7 adopteda learning approach to coreference resolution(Chinchor, 1998).
Among the MUC-6 systems,the only one that we can directly compare tois the UMass system, which also used C4.5 forcoreference r solution.
The other MUC-6 sys-tems were not based on a learning approach.The score of the UMass system is not high com-pared to the rest of the MUC-6 systems.
Inparticular, the system's recall is relatively low.As explained in (Fisher et al, 1995), the reasonfor this is that it only concentrated on coref-erence relationships among references to peo-ple and organizations.
Our system, as opposedto the UMass system, considered all types ofmarkables.
The score of our system is higherthan that of the UMass system, and the dif-ference is statistically significant at p = 0.05.Thus, the contribution of our work is in show-ing that a learning approach, when evaluatedon a common coreference data set, is able toachieve accuracy competitive with state-of-the-art systems using non-learning approaches.5 Conc lus ionIn this paper, we presented a learning approachfor coreference r solution of noun phrases in un-restricted text.
The approach learns from asmall, annotated corpus and the task includesresolving not just pronouns but rather generalnoun phrases.
In contrast o previous work, weevaluated our approach on a common data set,the MUC-6 coreference corpus.
We obtainedencouraging results, indicating that on the gen-eral noun phrase coreference task, the learningapproach olds promise and achieves accuracycomparable to non-learning approaches.6 AcknowledgementsWe would like to thank Beth Sundheim whomade available to us the MUC-6 data setwithout which this work would have been im-290possible.
We would also like to thank HaiLeong Chieu who implemented the HMM-basednamed entity learning module.ReferencesChinatsu Aone and Scott William Bennett.1995.
Evaluating automated and manual ac-quisition of anaphora resolution strategies.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics.Breck Baldwin.
1997.
CogNIAC: High precisioncoreference with limited knowledge and lin-guistic resources.
In Proceedings of the A CLWorkshop on Operational Factors in Practi-cal, Robust Anaphora Resolution for Unre-stricted Texts, pages 38-45.Daniel M. Bikel, Scott Miller, RichardSchwartz, and Ralph Weischedel.
1997.Nymble: A high-performance learning name-finder.
In Proceedings of the Fifth Confer-ence on Applied Natural Language Process-ing, pages 194-201.Daniel M. Bikel, Richard Schwartz, andRalph M. Weischedel.
1999.
An algorithmthat learns what's in a name.
Machine Learn-ing, 34(1-3), February.Nancy A. Chinchor.
1998.
Overview of MUC-7/MET-2.
In Proceedings of the SeventhMessage Understanding Conference (MUC-7) http://www, muc.
saic.
corn/proceedings/muc_7_toc.html.Kenneth Church.
1988.
A stochastic parts pro-gram and noun phrase parser for unrestrictedtext.
In Proceedings of the Second Confer-ence on Applied Natural Language Process-ing, pages 136-143.MUC-6 Program Committee.
1995.
Corefer-ence task definition (v2.3, 8 Sep 95).
In Pro-ceedings of the Sixth Message UnderstandingConference (MUC-6), pages 335-344.David Fisher, Stephen Soderland, Joseph Mc-Carthy, Faugfang Feug, and Wendy Lehnert.1995.
Description of the UMass system asused for MUC-6.
In Proceedings of the SixthMessage Understanding Conference (MUC-6), pages 127-140.Niyu Ge, John Hale, and Eugene Charniak.1998.
A statistical pproach to anaphora res-olution.
In Proceedings of the Sixth Workshopon Very Large Corpora, pages 161-170.Megumi Kameyama.
1997.
Recognizing refer-ential inks: An information extraction per-spective.
In Proceedings of the A CL Work-shop on Operational Factors in Practical, Ro-bust Anaphora Resolution for UnrestrictedTexts, pages 46-53.Shalom Lappin and Herbert J. Leass.
1994.
Analgorithm for pronominal anaphora resolu-tion.
Computational Linguistics, 20(4):535-561, December.Joseph F. McCarthy and Wendy Lehnert.
1995.Using decision trees for coreference resolu-tion.
In Proceedings of the Fourteenth Inter-national Joint Conference on Artificial Intel-ligence, pages 1050-1055.George A. Miller.
1990.
WordNet: an on-line lexical database.
International Journalof Lexicography, 3(4):235-312.Ruslan Mitkov.
1997.
Factors in anaphora res-olution: They are not the only things thatmatter, a case study based on two differ-ent approaches.
In Proceedings of the ACLWorkshop on Operational Factors in Prac-tical, Robust Anaphora Resolution for Unre-stricted Texts, pages 14-21.John Ross Quinlan.
1993.
C~.5: Programs forMachine Learning.
Morgan Kanfmann, SanFrancisco, CA.Beth M. Sundheim.
1995.
Overview of resultsof the MUC-6 evaluation.
In Proceedings ofthe Sixth Message Understanding Conference(MUC-6), pages 13-31.291
