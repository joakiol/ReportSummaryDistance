Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 325?328,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPLearning foci for Question Answering over Topic MapsAlexander Mikhailian?, Tiphaine Dalmas?and Rani Pinchuk?
?Space Application Services, Leuvensesteenweg 325, B-1932 Zaventem, Belgium{alexander.mikhailian, rani.pinchuk}@spaceapplications.com?Aethystiphaine.dalmas@aethys.comAbstractThis paper introduces the concepts of ask-ing point and expected answer type as vari-ations of the question focus.
They are ofparticular importance for QA over semi-structured data, as represented by TopicMaps, OWL or custom XML formats.We describe an approach to the identifica-tion of the question focus from questionsasked to a Question Answering systemover Topic Maps by extracting the askingpoint and falling back to the expected an-swer type when necessary.
We use knownmachine learning techniques for expectedanswer type extraction and we implementa novel approach to the asking point ex-traction.
We also provide a mathematicalmodel to predict the performance of thesystem.1 IntroductionTopic Maps is an ISO standard1for knowledgerepresentation and information integration.
It pro-vides the ability to store complex meta-data to-gether with the data itself.This work addresses domain portable QuestionAnswering (QA) over Topic Maps.
That is, a QAsystem capable of retrieving answers to a questionasked against one particular topic map or topicmaps collection at a time.
We concentrate on anempirical approach to extract the question focus.The extracted focus is then anchored to a topicmap construct.
This way, we map the type of theanswer as provided in the question to the type ofthe answer as available in the source data.Our system runs over semi-structured data thatencodes ontological information.
The classifica-tion scheme we propose is based on one dynamic1ISO/IEC 13250:2003,http://www.isotopicmaps.org/sam/and one static layer, contrasting with previouswork that uses static taxonomies (Li and Roth,2002).We use the term asking point or AP when thetype of the answer is explicit, e.g.
the wordoperas in the question What operas did Pucciniwrite?We use the term expected answer type or EATwhen the type of the answer is implicit but can bededuced from the question using formal methods.The question Who composed Tosca?
implies thatthe answer is a person.
That is, person is the ex-pected answer type.We consider that AP takes precedence over theEAT.
That is, if the AP (the explicit focus) hasbeen successfully identified in the question, it isconsidered to be the type of the question, and theEAT (the implicit focus) is left aside.The claim that the exploitation of AP yields bet-ter results in QA over Topic Maps has been testedwith 100 questions over the Italian Opera topicmap2.
AP, EAT and the answers of the ques-tions were manually annotated.
The answers to thequestions were annotated as topic map constructs(i.e.
as topics or as occurrences).An evaluation for QA over Topic Maps has beendevised that has shown that choosing APs as focileads to a much better recall and precision.
A de-tailed description of this test is beyond the scopeof this paper.2 System ArchitectureWe approach both AP and EAT extraction withthe same machine learning technology based onthe principle of maximum entropy (Ratnaparkhi,1998)3.2http://ontopia.net/omnigator/models/topicmap_complete.jsp?tm=opera.ltm3OpenNLP http://opennlp.sf.net was used fortokenization, POS tagging and parsing.
Maxent http://maxent.sf.net was used as the maximum entropy engine325What are Italian operas ?Gold O O AP AP OTable 1: Gold standard AP annotationClass Word count %AskingPoint 1842 9.3%Other 17997 90.7%Table 2: Distribution of AP classes (word level)We annotated a corpus of 2100 questions.
1500of those questions come from the Li & Roth cor-pus (Li and Roth, 2002), 500 questions were takenfrom the TREC-10 questions and 100 questionswere asked over the Italian Opera topic map.2.1 AP extractionWe propose a model for extracting AP that is basedon word tagging.
As opposed to EAT, AP is con-structed on word level not on the question level.Table 1 provides an annotated example of AP.Our annotation guidelines limit the AP to thenoun phrase that is expected to be the type of theanswer.
As such, it is different from the notionof focus as a noun likely to be present in the an-swer (Ferret et al, 2001) or as what the questionis all about (Moldovan et al, 1999).
For instance,a question such as Where is the Taj Mahal?
doesnot yield any AP.
Although the main topic is theTaj Mahal, the answer is not expected to be in aparent-child relationship with the subject.
Instead,the sought after type is the EAT class LOCATION.This distinction is important for QA over semi-structured data where the data itself is likely to behierarchically organized.Asking points were annotated in 1095 (52%)questions out of 2100.
The distribution of APclasses in the annotated data is shown in the Ta-ble 2.A study of the inter-annotator agreement be-tween two human annotators has been performedon a set of 100 questions.
The Cohen?s kappacoefficient (Cohen, 1960) was at 0.781, whichis lower than the same measure for the inter-annotator agreement on EAT.
This is an expectedresult, as the AP annotation is naturally perceivedas a more complex task.
Nevertheless, this allowsto qualify the inter-annotator agreement as good.For each word, a number of features were usedfor EAT and AP extraction.Class Count %TIME 136 6.5%NUMERIC 215 10.2%DEFINITION 281 13.4%LOCATION 329 15.7%HUMAN 420 20.0%OTHER 719 34.2%Table 3: Distribution of EAT classes (questionlevel)by the classifier, including strings and POS-tagson a 4-word window.
The WH-word and its com-plement were also used as features, as well as theparsed subject of the question and the first nominalphrase.A simple rule-based AP extraction has also beenimplemented, for comparison.
It operates by re-trieving the WH-complement from the syntacticparse of the question and stripping the initial arti-cles and numerals, to match the annotation guide-lines for AP.2.2 EAT extractionEAT was supported by a taxonomy of 6 coarseclasses: HUMAN, NUMERIC, TIME, LOCA-TION, DEFINITION and OTHER.
This selectionis fairly close to the MUC typology of NamedEntities4which has been the basis of numerousfeature-driven classifiers because of salient formalindices that help identify the correct class.We purposely limited the number of EATclasses to 6 as AP extraction already providesa fine-grained, dynamic classification from thequestion to drive the subsequent search in the topicmap.The distribution of EAT classes in the annotateddata is shown in the Table 3.A study of the inter-annotator agreement be-tween two human annotators has been performedon a set of 200 questions.
The resulting Cohen?skappa coefficient (Cohen, 1960) of 0.8858 allowsto qualify the inter-annotator agreement as verygood.We followed Li & Roth (Li and Roth, 2002)to implement the features for the EAT classifier.They included strings and POS-tags, as well assyntactic parse information (WH-words and theircomplements, auxiliaries, subjects).
Four lists for4http://www.cs.nyu.edu/cs/faculty/grishman/NEtask20.book_1.html326Accuracy Value Std dev Std errEAT 0.824 0.020 0.006Lenient AP 0.963 0.020 0.004Exact AP 0.888 0.052 0.009Focus (AP+EAT) 0.827 0.020 0.006Table 4: Accuracy of the classifiers (questionlevel)words related to locations, people, quantities andtime were derived from WordNet and encoded assemantic features.3 Evaluation ResultsThe performance of the classifiers was evaluatedon our corpus of 2100 questions annotated for APand EAT.
The corpus was split into 80% of trainingand 20% test data, and data re-sampled 10 times inorder to account for variance.Table 4 lists the figures for the accuracy of theclassifiers, that is, the ratio between the correct in-stances and the overall number of instances.
Asthe AP classifier operates on words while the EATclassifier operates on questions, we had to estimatethe accuracy of the AP classifier per question, toallow for comparison.
Two simple metrics are pos-sible.
A lenient metric assumes that the AP extrac-tor performed correctly in the question if there isan overlap between the system output and the an-notation on the question level.
An exact metric as-sumes that the AP extractor performed correctly ifthere is an exact match between the system outputand the annotation.In the example What are Italian Operas?
(Ta-ble 1), assuming the system only tagged operas asAP, lenient accuracy will be 1, exact accuracy willbe 0, precision for the AskingPoint class will be 1and its recall will be 0.5.Table 5 shows EAT results by class.
Tables 6and 7 show AP results by class for the machinelearning and the rule-based classifier.As shown in Figure 1, when AP classification isavailable it is used.
During the evaluation, AP wasfound in 49.4% of questions.A mathematical model has been devised to pre-dict the accuracy of the focus extractor on an an-notated corpus.It is expected that the focus accuracy, that is, theaccuracy of the focus extraction system, is depen-dent on the performance of the AP and the EATclassifiers.
Given N the total number of questions,Class Precision Recall F-ScoreDEFINITION 0.887 0.800 0.841LOCATION 0.834 0.812 0.821HUMAN 0.902 0.753 0.820TIME 0.880 0.802 0.838NUMERIC 0.943 0.782 0.854OTHER 0.746 0.893 0.812Table 5: EAT performance by class (questionlevel)Class Precision Recall F-ScoreAskingPoint 0.854 0.734 0.789Other 0.973 0.987 0.980Table 6: AP performance by class (word level)Class Precision Recall F-ScoreAskingPoint 0.608 0.479 0.536Other 0.948 0.968 0.958Table 7: Rule-based AP performance by class(word level)we define the branching factor, that is, the percent-age of questions for which AP is provided by thesystem, as follows:Y =(TPAP+ FPAP)NFigure 1 shows that the sum AP true posi-tives and EAT correct classifications represents theoverall number of questions that were classifiedcorrectly.
This accuracy can be further developedto present the dependencies as follows:AFOCUS= PAPY +AEAT(1?
Y )That is, the overall accuracy is dependent on theprecision of the AskingPoint class of the AP clas-sifier, the accuracy of EAT and the branching fac-tor.
The branching factor itself can be predictedusing the performance of the AP classifier and theratio between the number of questions annotatedwith AP and the total number of questions.Y =(TPAP+FNAPN)RAPPAP327AP extractionEAT extractionFocusTN    +FNTP   +FPC    +I AP EATEATAPAP APFigure 1: Focus extraction flow diagram4 Related work(Atzeni et al, 2004; Paggio et al, 2004) describeMOSES, a multilingual QA system delivering an-swers from Topic Maps.
MOSES extracts a focusconstraint (defined after (Rooth, 1992)) as part ofthe question analysis, which is evaluated to an ac-curacy of 76% for the 85 Danish questions and70% for the 83 Italian questions.
The focus isan ontological type dependent from the topic map,and its extraction is based on hand-crafted rules.In our case, focus extraction ?
though defined withtopic map retrieval in mind ?
stays clear of on-tological dependencies so that the same questionanalysis module can be applied to any topic map.In open domain QA, machine learning ap-proaches have proved successful since Li & Roth(Li and Roth, 2006).
Despite using similar fea-tures, the F-Score (0.824) for our EAT classes isslightly lower than reported by Li & Roth (Li andRoth, 2006) for coarse classes.
We may speculatethat the difference is primarily due to our limitedtraining set size (1,680 questions versus 21,500questions for Li & Roth).
On the other hand, weare not aware of any work attempting to extract APon word level using machine learning in order toprovide dynamic classes to a question classifica-tion module.5 Future work and conclusionWe presented a question classification systembased on our definition of focus geared towardsQA over semi-structured data where there is aparent-child relationship between answers andtheir types.
The specificity of the focus degradesgracefully in the approach described above.
Thatis, we attempt the extraction of the AP when possi-ble and fall back on the EAT extraction otherwise.We identify the focus dynamically, instead ofrelying on a static taxonomy of question types,and we do so using machine learning techniquesthroughout the application stack.A mathematical model has been devised to pre-dict the performance of the focus extractor.We are currently working on the exploitation ofthe results provided by the focus extractor in thesubsequent modules of the QA over Topic Maps,namely anchoring, navigation in the topic map,graph algorithms and reasoning.AcknowledgementsThis work has been partly funded by the Flemishgovernment (through IWT) as part of the ITEA2project LINDO (ITEA2-06011).ReferencesP.
Atzeni, R. Basili, D. H. Hansen, P. Missier, P. Pag-gio, M. T. Pazienza, and F. M. Zanzotto.
2004.Ontology-Based Question Answering in a Federa-tion of University Sites: The MOSES Case Study.In NLDB, pages 413?420.J.
Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Mea-surement, 20, No.1:37?46.O.
Ferret, B. Grau, M. Hurault-Plantet, G. Illouz,L.
Monceaux, I. Robba, and A. Vilnat.
2001.
Find-ing an Answer Based on the Recognition of theQuestion Focus.
In 10th Text Retrieval Conference.X.
Li and D. Roth.
2002.
Learning Question Classi-fiers.
In 19th International Conference on Compu-tational Linguistics (COLING), pages 556?562.X.
Li and D. Roth.
2006.
Learning Question Classi-fiers: The Role of Semantic Information.
Journal ofNatural Language Engineering, 12(3):229?250.D.
Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea,R.
Goodrum, R. Girju, and V. Rus.
1999.
LASSO:A Tool for Surfing the Answer Net.
In 8th Text Re-trieval Conference.P.
Paggio, D. H. Hansen, R. Basili, M. T. Pazienza,and F. M. Zanzotto.
2004.
Ontology-based questionanalysis in a multilingual environment: the MOSEScase study.
In OntoLex (LREC).A.
Ratnaparkhi.
1998.
Maximum Entropy Models forNatural Language Ambiguity Resolution.
Ph.D. the-sis, University of Pennsylvania, Philadelphia, PA.M.
Rooth.
1992.
A Theory of Focus Interpretation.Natural Language Semantics, 1(1):75?116.328
