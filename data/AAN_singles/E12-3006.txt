Proceedings of the EACL 2012 Student Research Workshop, pages 46?54,Avignon, France, 26 April 2012. c?2012 Association for Computational LinguisticsYet Another Language IdentifierMartin Majlis?Charles University in PragueInstitute of Formal and Applied LinguisticsFaculty of Mathematics and Physicsmajlis@ufal.mff.cuni.czAbstractLanguage identification of written text hasbeen studied for several decades.
Despitethis fact, most of the research is focusedon a few most spoken languages, whereasthe minor ones are ignored.
The identi-fication of a larger number of languagesbrings new difficulties that do not occurfor a few languages.
These difficulties arecausing decreased accuracy.
The objectiveof this paper is to investigate the sourcesof such degradation.
In order to isolatethe impact of individual factors, 5 differ-ent algorithms and 3 different number oflanguages are used.
The Support VectorMachine algorithm achieved an accuracy of98% for 90 languages and the YALI algo-rithm based on a scoring function had anaccuracy of 95.4%.
The YALI algorithmhas slightly lower accuracy but classifiesaround 17 times faster and its training ismore than 4000 times faster.Three different data sets with various num-ber of languages and sample sizes were pre-pared to overcome the lack of standardizeddata sets.
These data sets are now publiclyavailable.1 IntroductionThe task of language identification has been stud-ied for several decades, but most of the literatureis about identifying spoken language1.
This ismainly because language identification of writtenform is considered an easier task, because it doesnot contain such variability as the spoken form,such as dialects or emotions.1http://speech.inesc.pt/?dcaseiro/html/bibliografia.htmlLanguage identification is used in many NLPtasks and in some of them simple rules2 are of-ten good enough.
But for many other applica-tions, such as web crawling, question answeringor multilingual documents processing, more so-phisticated approaches need to be used.This paper first discusses previous work in Sec-tion 2, and then presents possible hypothesis fordecreased accuracy when a larger number of lan-guages is identified in Section 3.
Data used forexperiments is described in Section 4, along withmethods used in experiments for language iden-tification in Section 5.
Results for all methodsas well as comparison with other systems is pre-sented in Section 6.2 Related WorkThe methods used in language identification havechanged significantly during the last decades.
Inthe late sixties, Gold (1967) examined languageidentification as a task in automata theory.
In theseventies, Leonard and Doddington (1974) wasable to recognize five different languages, and inthe eighties, Beesley (1988) suggested using cryp-toanalytic techniques.Later on, Cavnar and Trenkle (1994) intro-duced their algorithm with a sliding window overa set of characters.
A list of the 300 most com-mon n-grams for n in 1..5 is created during train-ing for each training document.
To classify a newdocument, they constructed a list of the 300 mostcommon n-grams and compared n-grams positionwith the testing lists.
The list with the least dif-ferences is the most similar one and new doc-ument is likely to be written in same language.2http://en.wikipedia.org/wiki/Wikipedia:Language_recognition_chart46They classified 3478 samples in 14 languagesfrom a newsgroup and reported an achieved accu-racy of 99.8%.
This influenced many researchesthat were trying different heuristics for selectingn-grams, such as Martins and Silva (2005) whichachieved an accuracy of 91.25% for 12 languages,or Hayati (2004) with 93.9% for 11 languages.Sibun and Reynar (1996) introduced a methodfor language detection based on relative entropy, apopular measure also known as Kullback-Leiblerdistance.
Relative entropy is a useful measureof the similarity between probability distributions.She used texts in 18 languages from the EuropeanCorpus Initiative CD-ROM.
She achieved a 100%accuracy for bigrams.In recent years, standard classification tech-niques such as support vector machines also be-came popular and many researchers used themKruengkrai et al(2005) or Baldwin and Lui(2010) for identifying languages.Nowadays, language recognition is consideredas an elementary NLP task3 which can be usedfor educational purposes.
McNamee (2005) usedsingle documents for each language from projectGutenberg in 10 European languages.
He prepro-cessed the training documents ?
the texts werelower-cased, accent marks were retained.
Then,he computed a so-called profile of each language.Each profile consisted of a percentage of the train-ing data attributed to each observed word.
Fortesting, he used 1000 sentences per language fromthe Euro-parliament collection.
To classify a newdocument, the same preprocessing was done andinner product based on the words in the documentand the 1000 most common words in each lan-guage was computed.
Performance varied from80.0% for Portuguese to 99.5% for German.Some researches such as Hughes et al(2006)or Grothe et al(2008) focused in their paperson the comparison of different approaches to lan-guage identification and also proposed new goalsin that field, such as as minority languages or lan-guages written non-Roman script.Most of the researches in the past identifiedmostly up to twenty languages but in recentyears, language identification of minority lan-guages became the focus of Baldwin and Lui(2010), Choong et al(2011), and Majlis?
(2012).All of them observed that the task became much3http://alias-i.com/lingpipe/demos/tutorial/langid/read-me.htmlharder for larger numbers of languages and accu-racy of the system dropped.3 HypothesisThe accuracy degradation with a larger number oflanguages in the language identification systemmay have many reasons.
This section discussesthese reasons and suggests how to isolate them.In some hypotheses, charts involving data fromthe W2C Wiki Corpus are used, which are intro-duced in Section 4.3.1 Training Data SizeIn many NLP applications, size of the availabletraining data influences overall performance ofthe system, as was shown by Halevy et al(2009).To investigate the influence of training datasize, we decided to use two different sizes of train-ing data ?
1 MB and 4 MB.
If the drop in accu-racy is caused by the lack of training data, thenall methods used on 4 MB should outperform thesame methods used on 1 MB of data.3.2 Language DiversityThe increasing number of languages recognisedby the system decreases language diversity.
Thismay be another reason for the observed dropin the accuracy.
We used information aboutlanguage classes from the Ethnologue website(Lewis, 2009).
The number of different languageclasses is depicted in Figure 1.
Class 1 representsthe most distinguishable classes, such as Indo-European vs. Japonic, while Class 2 representsfiner classification, such as Indo-European, Ger-manic vs. Indo-European, Italic.05101520253010  20  30  40  50  60  70  80  90LanguageFamiliesLanguagesClass 1Class 2Figure 1: Language diversity on Wikipedia.
Lan-guages are sorted according to their text corpus size.The first 52 languages belong to 15 differentClass 1 classes and the number of classes does not47change until the 77th language, when the Swahililanguage from class Niger-Congo appears.3.3 ScalabilityAnother issue with increasing number of lan-guages is the scalability of used methods.
Thereare several pitfalls for machine learning algo-rithms ?
a) many languages may require manyfeatures which may lead to failures caused bycurse-of-dimensionality, b) differences in lan-guages may shrink, so the classifier will be forcedto learn minor differences and will lose its abil-ity to generalise, and become overfitted, and c)the classifier may internally use only binary clas-sifiers which may lead up to quadratic complexity(Dimitriadou et al 2011).4 Data SetsFor our experiments, we decided to use the W2CWiki Corpus (Majlis?, 2012) which contains arti-cles from Wikipedia.
The total size of all textswas 8 GB and available material for various lan-guages differed significantly, as is displayed inFigure 2.05010015020025030035040045010  20  30  40  50  60  70  80  90Size inMBLanguageW2C Wiki Corpus - Size in MBFigure 2: Available data in the W2C Wiki Corpus.Languages are sorted according to their size in the cor-pus.We used this corpus to prepare 3 different datasets.
We used one of them for testing hypothesispresented in the previous section and the remain-ing two for comparison with other systems.
Thesedata sets contain samples of length approximately30, 140, and 1000 bytes.
The sample of length 30represents image caption or book title, the sampleof length 140 represents tweet or user comment,and sample of length 1000 represents newspaperarticle.All datasets are available at http://ufal.mff.cuni.cz/?majlis/yali/.4.1 LongThe main purpose of this data set (yali-dataset-long) was testing hypothesis described in the pre-vious section.To investigate the drop, we intended to coveraround 100 languages, but the amount of availabledata limited us.
For example, the 80th languagehas 12 MB, whereas the 90th has 6 MB and tbe100th has only 1 MB of text.
To investigate thehypothesis of the influence of training data size,we decided to build a 1 MB and 4 MB corpus foreach language, where the 1 MB corpus is a subsetof the 4 MB one.Then, we divided the corpus for each languageinto chunks with 1000 bytes of text, so we gained1000 and 4000 chunks respectively.
These chunkswere divided into training and testing sets in a90:10 ratio, thus we had 900 and 3600 train-ing chunks, respectively, and 100 and 400 testingchunks respectively.To reduce the risk that the training and testingare influenced by the position from which theywere taken (the beginning or the end of the cor-pus), we decided to use every 10th sentence as atesting one and use the remaining ones for train-ing.Then, we created an n-gram for n in 1..4 fre-quency list for each language, each corpus size.From each frequency list, we preserved only thefirst m = 100 most frequent n-grams.
For exam-ple, from the raw frequency list ?
a: 5, b: 3, c: 1,d: 1, and m = 2, frequency list a: 5, b: 3 wouldbe created.
We used this n-grams as features fortesting classifiers.4.2 SmallThe second data set (yali-dataset-small ) was pre-pared for comparison with Google Translate4(GT).
The GT is paid service capable of recog-nizing 50 different languages.
This data set con-tains 50 samples of lengths 30 and 140 for 48 lan-guages, so it contains 4,800 samples in total.4.3 StandardThe purpose of the third data sets is compari-son with other systems for language identifica-tion.
This data set contains 700 samples of length30, 140, and 1000 for 90 languages, so it containsin total 189,000 samples.4http://translate.google.com48Size L\N 1 2 3 430 177 1361 2075 24221MB 60 182 1741 3183 414590 186 1964 3943 568230 176 1359 2079 24184MB 60 182 1755 3184 412590 187 1998 3977 5719Table 1: The number of unique N-grams in corpusSize with L languages.
(D(Size,L,n))5 MethodsTo investigate the influence of the language di-versity, we decided to use 3 different languagecounts ?
30, 60, and 90 languages sorted ac-cording to their raw text size.
For each cor-pus size (cS ?
{1000, 4000}), languagecount (lC ?
{30, 60, 90}), and n-gram size(n ?
{1, 2, 3, 4}) we constructed a separate dic-tionary D(cS,lC,n) containing the first 100 mostfrequent n-grams for each language.
The numberof items in each dictionary is displayed in Table 1and visualised for 1 MB corpus in Figure 3.The dictionary sizes for 4 MB corpora wereslightly higher when compared to 1 MB corpora,but surprisingly for 30 languages it was mostlyopposite.010002000300040005000600020  40  60  80  100  120Unique n-gramsLanguages (lC)n=1n=2n=3n=4Figure 3: The number of unique n-grams in the dic-tionary D(1000,lC,n).
Languages are sorted accordingto their text corpus size.Then, we converted all texts into matri-ces in the following way.
For each cor-pus size (cS ?
{1000, 4000}), languagecount (lC ?
{30, 60, 90}), and n-gram size(n ?
{1, 2, 3, 4}) we constructed a training ma-trix Tr(cS,lC,n) and a testing matrix Te(cS,lC,n),where element on Tr(cS,lC,n)i,j represents the num-ber of occurrences of j-th n-gram from dic-tionary D(cS,lC,n) in training sample i, andTr(cS,lC,n)i,0 represents language of that sample.The training matrix Tr(cS,lC,n) has dimension(0.9 ?
cS ?
lC) ?
(1 + | D(cS,lC,n) |)and the testing matrix Te(cS,lC,n) has dimension(0.1 ?
cS ?
lC)?
(1 + | D(cS,lC,n) |).For investigating the scalability of the differ-ent approaches to language identification, we de-cided to use five different methods.
Three of themwere based on standard classification algorithmsand two of them were based on scoring function.For experimenting with the classification algo-rithms, we used R (2009) environment which con-tains many packages with machine learning algo-rithms5, and for scoring functions we used Perl.5.1 Support Vector MachineThe Suport Vector Machine (SVM) is a state ofthe art algorithm for classification.
Hornik et al(2006) compared four different implementationsand concluded that Dimitriadou et al(2011) im-plementation available in the package e1071 is thefastest one.
We used SVM with sigmoid kernel,cost of constraints violation set to 10, and termi-nation criterion set to 0.01.5.2 Naive BayesThe Naive Bayes classifier (NB) is a simple prob-abilistic classifier.
We used Dimitriadou et al(2011) implementation from the package e1071with default arguments.5.3 Regression TreeRegression trees are implemented by Therneau etal.
(2010) in the package rpart.
We used it withdefault arguments.5.4 W2CThe W2C algorithm is the same as was used byMajlis?
(2011).
From the frequency list, probabil-ity is computed for each n-gram, which is used asa score in classification.
The language with thehighest score is the winning one.
For example,from the raw frequency list ?
a: 5, b: 3, c: 1, d: 1,and m=2, the frequency list a: 5; b: 3, and com-puted scores ?
a: 0.5, b: 0.3 would be created.5http://cran.r-project.org/web/views/MachineLearning.html495.5 Yet Another Language IdentifierThe Yet Another Language Identifier (YALI) al-gorithm is based on the W2C algorithm with twosmall modifications.
The first is modification inn-gram score computation.
The n-gram score isnot based on its probability in raw data, but ratheron its probability in the preserved frequency list.So for the numbers used in the W2C example, wewould receive scores ?
a: 0.625, b: 0.375.
Thesecond modification is using rather byte n-gramsinstead of character n-grams.6 Results & DiscussionAt the beginning we used only data set yali-dataset-long to investigate the influence of vari-ous set-ups.The accuracy of all experiments is presentedin Table 2, and visualised in Figure 4 and Fig-ure 5.
These experiments also revealed that algo-rithms are strong in different situations.
All clas-sification techniques outperform all scoring func-tions on short n-grams and small amount of lan-guages.
However, with increasing n-gram length,their accuracy stagnated or even dropped.
The in-creased number of languages is unmanageable forNB a RPART classifiers and their accuracy sig-nificantly decreased.
On the other hand, the ac-curacy of scoring functions does not decrease somuch with additional languages.
The accuracy ofthe W2C algorithm decreased when greater train-ing corpora was used or more languages wereclassified, whereas the YALI algorithm did nothave these problems, but moreover its accuracyincreased with greater training corpus.1020304050607080901001  2  3  4AccuracyN-GramSVMNBRPARTW2CYALIFigure 4: Accuracy for 90 languages and 1 MB cor-pus with respect to n-gram length.606570758085909510030  60  90AccuracyLanguage CountSVMn=2NBn=1RPARTn=1W2Cn=4YALIn=4Figure 5: Accuracy for 1 MB corpus and the bestn-gram length with respect to the number of languages.The highest accuracy for all languageamounts ?
30, 60, 90 was achieved by theSVM with accuracies of 100%, 99%, and 98.5%,respectively, followed by the YALI algorithmwith accuracies of 99.9%, 96.8%, and 95.4%respectively.From the obtained results, it is possible to no-tice that 1 MB of text is sufficient for training lan-guage identifiers, but some algorithms achievedhigher accuracy with more training material.Our next focus was on the scalability of theused algorithms.
Time required for training is pre-sented in Table 3, and visualised in Figures 6 and7.The training of scoring functions required onlyloading dictionaries and therefore is extremelyfast, whereas training classifiers required compli-cated computations.
The scoring functions did nothave any advantages, because all algorithms hadto load all training examples, segment them, ex-tract the most common n-grams, build dictionar-ies, and convert text to matrices as was describedin Section 5.1101001000100001000001  2  3  4Training Time(s)N-GramSVMNBRPARTW2CYALIFigure 6: Training time for 90 languages and 1 MBcorpus with respect to n-gram length.50N-Gram L 1 2 3 4Method S 1MB 4MB 1MB 4MB 1MB 4MB 1MB 4MB30 96.3% 96.7% 100.0% 99.9% 100.0% 99.9% 99.9% 99.9%SVM 60 91.5% 92.3% 98.5% 98.5% 99.0% 99.0% 98.6% 98.5%90 90.8% 91.6% 98.0% 98.0% 98.5% - 98.3% -30 91.8% 94.2% 91.3% 90.9% 82.2% 93.3% 32.1% 59.9%NB 60 78.7% 84.8% 70.6% 68.2% 71.7% 77.6% 25.7% 34.0%90 75.4% 82.7% 68.8% 66.5% 64.3% 71.0% 18.4% 17.5%30 97.3% 96.7% 98.8% 98.6% 98.4% 97.8% 97.7% 97.4%RPART 60 90.2% 91.2% 67.3% 72.0% 67.2% 68.8% 65.5% 74.6%90 64.3% 55.9% 39.7% 39.6% 43.0% 44.0% 38.5% 39.6%30 38.0% 38.6% 89.9% 91.0% 96.2% 96.5% 97.9% 98.1%W2C 60 34.7% 30.9% 83.0% 81.7% 86.0% 84.9% 89.1% 82.0%90 34.7% 30.9% 77.8% 77.6% 84.9% 83.4% 87.8% 82.7%30 38.0% 38.6% 96.7% 96.2% 99.6% 99.5% 99.9% 99.8%YALI 60 35.0% 31.2% 86.1% 86.1% 95.7% 96.4% 96.8% 97.4%90 34.9% 31.1% 86.8% 87.8% 95.0% 95.6% 95.4% 96.1%Table 2: Accuracy of classifiers for various corpora sizes, n-gram lengths, and language counts.11010010001000010000030  60  90Training Time(s)Language CountSVMn=2NBn=1RPARTn=1W2Cn=4YALIn=4Figure 7: Training time for 1 MB corpus and the bestn-gram length with respect to the number of languages.Time required for training increased dramat-ically for SVM and RPART algorithms whenthe number of languages or the corpora size in-creased.
It is possible to use the SVM only withunigrams or bigrams, because training on trigramsrequired 12 times more time for 60 languagescompared with 30 languages.
The SVM also hadproblems with increasing corpora sizes, because ittook almost 10-times more time when the corpussize increased 4 times.
Scoring functions scaledwell and were by far the fastest ones.
We ter-minated training the SVM on trigrams and quad-grams for 90 languages after 5 days of computa-tion.Finally, we also measured time required forclassifying all testing examples.
The results arein Table 4, and visualised in Figure 8 and Fig-ure 6.
Times displayed in the table and charts rep-resents the number of seconds needed for classi-fying 1000 chunks.0.11101001000100001  2  3  4PredictionTime (s/1000 chunks)N-GramSVMNBRPARTW2CYALIFigure 8: Prediction time for 90 languages and 1 MBcorpus with respect to n-gram length.01020304050607030  60  90PredictionTime (s/1000 chunks)Language CountSVMn=2NBn=1RPARTn=1W2Cn=4YALIn=4Prediction time for 1 MB corpus and the best n-gramlength with respect to the number of languages.The RPART algorithm was the fastest classifierfollowed by both scoring functions, whereas NBwas the slowest one.
All algorithms with 4 timesmore data achieved slightly higher accuracy, buttheir training took 4 times longer, with the ex-ception of the SVM which took at least 10 timeslonger.
The SVM algorithm is the least scalable51N-Gram L 1 2 3 4Method S 1MB 4MB 1MB 4MB 1MB 4MB 1MB 4MB30 215 1858 663 1774 627 7976 655 3587SVM 60 1499 13653 7981 87260 7512 44288 26943 20712390 2544 24841 12698 267824 76693 - 27964 -30 5 19 27 83 40 144 54 394NB 60 9 32 76 255 142 515 363 118790 12 56 188 683 298 1061 672 224530 44 189 144 946 267 1275 369 1360RPART 60 162 1332 736 3447 1270 11114 2583 749390 351 1810 1578 7647 5139 23413 6736 1765930 1 1 0 1 0 1 1 1W2C 60 1 2 1 2 2 1 2 290 1 1 1 1 3 1 2 130 1 1 1 1 1 1 1 1YALI 60 2 2 2 2 2 2 2 090 2 1 2 1 3 1 3 2Table 3: Training TimeMethod 30 60 90Acc 100.0% 98.5% 98.0%SVM Tre 663 7981 12698n=2 Pre 10.3 66.2 64.1Acc 91.8% 78.7% 75.4%NB Tre 5 9 12n=1 Pre 13.0 18.2 22.2Acc 97.3% 90.2% 64.3%RPART Tre 44 162 351n=1 Pre 0.1 0.2 0.1Acc 97.9% 89.1% 87.8%W2C Tre 1 2 2n=4 Pre 1.3 2.8 12.3Acc 99.9% 96.8% 95.4%YALI Tre 1 2 3n=4 Pre 1.3 2.7 3.6Table 5: Comparison of classifiers with best param-eters.
Label Acc represents accuracy, Tre representstraining time in seconds, and Pre represents predictiontime for 1000 chunks in seconds.algorithm of all the examined ?
all the rest re-quired proportionally more time for training andprediction when the greater training corpus wasused or more languages were classified.The comparison of all methods is presented inTable 5.
For each model we selected the n-gramssize with the best trade-off between accuracy andtime required for training and prediction.
The twomost accurate algorithms are SVM and YALI.
TheSVM achieved the highest accuracy for all lan-guages but its training took around 4000 timeslonger and classification was around 17 timesslower than the YALI.In the next step we evaluated the YALI algo-rithm for various size of selected n-grams.
TheseLanguagesSize 30 140 1000100 64.9% 85.7 % 93.8 %200 68.7% 87.3 % 93.9 %400 71.7% 88.0 % 94.0 %800 73.7% 88.5 % 94.0 %1600 75.0% 88.8% 94.0%Table 6: Effect of the number of selected 4-grams onaccuracy.experiments were evaluated on the data set yali-dataset-standard.
Achieved results are presentedin Table 6.
The number of used n-grams increasedthe accuracy for short samples from 64.9% to75.0% but it had no effect on long samples.As the last step in evaluation we decided tocompare the YALI with Google Translate (GT),which also provides language identification for 50languages through their API.6 For comparison weused data set yali-dataset-small which contains 50samples of length 30 and 140 for each language(4800 samples in total).
Achieved results are pre-sented in Table 7.
The GT and the YALI per-form comparably well on samples of length 30 onwhich they achieved accuracy 93.6% and 93.1%respectively, but on samples of length 140 GTwith accuracy 97.3% outperformed YALI with ac-curacy 94.8%.7 Conclusions & Future WorkIn this paper we compared 5 different algorithmsfor language identification ?
three based on the6http://code.google.com/apis/language/translate/v2/using_rest.html52N-Gram L 1 2 3 4Method S 1MB 4MB 1MB 4MB 1MB 4MB 1MB 4MB30 3.7 7.3 10.3 6.8 9.0 31.8 9.3 13.8SVM 60 13.3 30.1 66.2 189.7 59.8 92.8 236.7 375.290 16.1 36.7 64.1 381.4 414.9 - 133.4 -30 13.0 13.6 75.3 77.1 132.7 147.9 186.0 349.7NB 60 18.2 18.8 155.3 162.0 291.5 297.4 860.3 676.090 22.2 24.7 318.1 251.9 546.3 469.3 1172.8 1177.830 0.1 0.1 0.3 0.1 0.1 0.2 0.7 0.2RPART 60 0.2 0.1 0.2 0.0 0.2 0.4 0.8 0.290 0.1 0.1 0.2 0.1 0.4 0.3 1.2 0.330 0.7 0.8 1.7 1.6 3.3 1.5 1.3 2.2W2C 60 1.3 1.3 2.2 2.4 2.7 2.5 2.8 2.990 2.1 1.8 4.0 3.2 4.4 3.8 12.3 5.830 0.7 0.8 1.0 1.2 2.0 1.9 1.3 2.2YALI 60 1.3 1.5 1.8 2.2 2.5 2.2 2.7 2.590 2.2 1.8 2.7 2.9 4.4 3.5 3.6 3.7Table 4: Prediction TimeText Length30 140SystemGoogle 93.6% 97.3%YALI 93.1% 94.8%Table 7: Comparison of Google Translate and YALIon 48 languages.standard classification algorithms (Support Vec-tor Machine (SVM), Naive Bayes (NB), and Re-gression Tree (RPART)) and two based on scoringfunctions.
For investigating the influence of theamount of training data we constructed two cor-pora from the Wikipedia with 90 languages.
Toinvestigate the influence of number if identifiedlanguages we created three sets with 30, 60, and90 languages.
We also measured time required fortraining and classification.Our experiments revealed that the standardclassification algorithms requires at most bi-grams while the scoring ones required quad-grams.
We also showed that Regression Trees andNaive Bayes are not suitable for language identifi-cation because they achieved accuracy 64.3% and75.4% respectively.The best classifier for language identificationwas the SVM algorithm which achieved accuracy98% for 90 languages but its training took 4200times more and its classification was 16 timesslower than the YALI algorithm with accuracy95.4%.
This YALI algorithm has also potentialfor increasing accuracy and number of recognizedlanguages because it scales well.We also showed that the YALI algorithm iscomparable with the Google Translate system.Both systems achieved accuracy 93% for sam-ples of length 30.
On samples of length 140Google Translate with accuracy 97.3% outper-formed YALI with accuracy 94.8%.All data sets as well as source codes areavailable at http://ufal.mff.cuni.cz/?majlis/yali/.In the future we would like to focus on usingdescribed techniques not only on recognizing lan-guages but also on recognizing character encod-ings which is directly applicable for web crawl-ing.AcknowledgmentsThe research has been supported by the grantKhresmoi (FP7-ICT-2010-6-257528 of the EUand 7E11042 of the Czech Republic).References[Baldwin and Lui2010] Timothy Baldwin and MarcoLui.
2010.
Language identification: the long andthe short of the matter.
Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pp.
229?237.
[Beesley1988] Kenneth R. Beesley.
1988.
Lan-guage identifier: A computer program for automaticnatural-language identification of on-line text.
Lan-guages at Crossroads: Proceedings of the 29th An-nual Conferenceof the American Translators Asso-ciation, 12-16 October 1988, pp.
47-54.
[Cavnar and Trenkle1994] William B. Cavnar and JohnM.
Trenkle.
1994.
N-gram-based text categoriza-53tion.
In Proceedings of Symposium on DocumentAnalysis and Information Retrieval.
[Choong et al011] Chew Yew Choong, YoshikiMikami, and Robin Lee Nagano.
2011.
LanguageIdentification of Web Pages Based on Improved Ngram Algorithm.
IJCSI, issue 8, volume 3.
[Dimitriadou et al011] Evgenia Dimitriadou, KurtHornik, Friedrich Leisch, David Meyer, and andAndreas Weingessel 2011. e1071: Misc Func-tions of the Department of Statistics (e1071), TUWien.
R package version 1.5-27.
http://CRAN.R-project.org/package=e1071.
[Gold1967] E. Mark Gold.
1967.
Language iden-tification in the limit.
Information and Control,5:447474.
[Grothe et al008] Lena Grothe, Ernesto William DeLuca, and Andreas Nrnberger.
2008.
A Com-parative Study on Language Identification Meth-ods.
Proceedings of the Sixth International Lan-guage Resources and Evaluation (LREC?08).
Mar-rakech, 980-985.
[Halevy et al009] Alon Halevy, Peter Norvig, andFernando Pereira.
2009.
The unreasonable effec-tiveness of data.
IEEE Intelligent Systems, 24:8?12.
[Hayati 2004] Katia Hayati.
2004.
Language Iden-tification on the World Wide Web.
Master The-sis, University of California, Santa Cruz.
http://lily-field.net/work/masters.pdf.
[Hornik et al006] Kurt Hornik, Alexandros Karat-zoglou, and David Meyer.
2006.
Support Vec-tor Machines in R. Journal of Statistical Software2006., 15.
[Hughes et al006] Baden Hughes, Timothy Bald-win, Steven Bird, Jeremy Nicholson, and AndrewMackinlay.
2006.
Reconsidering language identifi-cation for written language resources.
Proceedingsof LREC2006, 485?488.
[Kruengkrai et al005] Canasai Kruengkrai, PrapassSrichaivattana, Virach Sornlertlamvanich, and Hi-toshi Isahara.
2005.
Language identification basedon string kernels.
In Proceedings of the 5th Interna-tional Symposium on Communications and Infor-mation Technologies (ISCIT2005), pages 896899,Beijing, China.
[Leonard and Doddington1974] Gary R. Leonard andGeorge R. Doddington.
1974.
Automatic languageidentification.
Technical report RADC-TR-74-200,Air Force Rome Air Development Center.
[Lewis2009] M. Paul Lewis.
2009.
Ethnologue: Lan-guages of the World, Sixteenth edition.
Dallas,Tex.
: SIL International.
Online version: http://www.ethnologue.com/[McNamee2005] Paul McNamee.
2005.
Languageidentification: a solved problem suitable for under-graduate instruction.
J. Comput.
Small Coll, vol-ume: 20, issue: 3, February 2005, 94?101.
Consor-tium for Computing Sciences in Colleges, USA.
[Majlis?2012] Martin Majlis?, Zdene?k Z?abokrtsky?.2012.
Language Richness of the Web.
In Proceed-ings of the Eight International Language Resourcesand Evaluation (LREC?12), Istanbul, Turkey, May2012.
[Majlis?2011] Martin Majlis?.
2011.
Large Multilin-gual Corpus.
Mater Thesis, Charles University inPrague.
[Martins and Silva2005] Bruno Martins and Ma?rio J.Silva.
2005.
Language identification in web pages.Proceedings of the 2005 ACM symposium on Ap-plied computing, SAC ?05, 764?768.
ACM, NewYork, NY, USA.
http://doi.acm.org/10.1145/1066677.1066852.
[R2009] R Development Core Team.
2009.
R: A Lan-guage and Environment for Statistical Computing.R Foundation for Statistical Computing.
ISBN 3-900051-07-0. http://www.R-project.org,[Sibun and Reynar1996] Penelope Sibun and Jeffrey C.Reynar.
1996.
Language identification: Examiningthe issues.
In Proceedings of the 5th Symposium onDocument Analysis and Information Retrieval.
[Therneau et al010] Terry M. Therneau, Beth Atkin-son, and R port by Brian Ripley.
2010.rpart: Recursive Partitioning.
R package ver-sion 3.1-48. http://CRAN.R-project.org/package=rpart.54
