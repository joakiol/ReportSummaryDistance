Large-Vocabulary Speaker-Independent Continuous Speech Recognitionwith Semi.Continuous Hidden Markov ModelsX.D.
Huang*, H.W.
Hon, and K.F.
LeeABSTRACTA semi-continuous hidden Markov model based on the muluplevector quantization codebooks is used here for large.vocabularyspeaker-independent continuous speech recognition in thetechn,ques employed here.
the semi-continuous output proba-b~hty densHy function for each codebook is represented by acomhinat,on of the corre,~ponding discrete output probablhttesof the hidden Markov model end the continuous Gauss,an den.stay functions of each individual codebook.
Parameters of vec.tor qusnttzation codebook and hidden Markov model are mutu-ully optimized to achJeve an optimal model'codebook comb,na-tion under a untried probab,hshc framework Another advan-tages of thts approach is the enhanced robustness of the semi.continuous output probability by the combination of multiplecodewords and multtple codebooks For a 1000.word speaker-mdependent continuous speech recognition using a word.pa,rgrammar, the recogmtion error rate of the semi-conhnuouq bud.den Markov model was reduced by more than 29'~ and 41"3 incomparison to the discrete and continuous mixture htdden Mar.kay model respectivelyI.
INTRODUCTIONIn the discrete hidden Msrkov model IHMML vector qusntization,VQ, produces the closet codebword from the codebook for eachacoust,c observation.
This mapping from continuous acousticspace to quantized discrete space may cause serious quantizationerrors for subsequent hidden Markov modeling.
To reduce VQerrors, varmus smoothing techniques have been proposed for VQand subsequent bidden Markov modeling \[9.12\] A distinctivetechn,que ts multiple VQ codebook hidden Markov modeling,wh*ch has been shown to offer improved speech recognition sccu.racy\[$.t2}.
In the multiple VQcodebook approach.
VQdistortioncan be stgntficantly minimized by partitioning the parsmeters into~eparatecodebooks.
Another disadvantage of the discrete HMM ~sthat the VQ codebook and tee discrete HMM are separste\]ymodeled, which may not be an optimal combination for patternclassification (8\].
The discrete HMM uses the discrete output pro-bability distributions to model various acoustic events, which ereinherently superior to the continuous mixture HMM with mlztureof I small number of probability density functions since thedistress distributions could model events with any shapes pro-vided enough training data exist.On the other hand.
the continuous mixture HMM models theacoustic observation directly us,ng estimated continuous probabil-ity density functions without VQ.
and has been shown to improvethe recognition accuracy in compartson to the discrete HMM l l f JFor speaker-independent speech recognition, mixture of a largenumber of probability density functions \[t4.16} or s large numberof states in single-mixture case \[4\] are generally requJred to modelcharacterisUcs of different speakers.
However.
mixture of a largenumber of probability density functions will considerably increasenot only the computational complexity, but also the number offree parameters that can be reliabiely estimated In addition, thecontinuous mixture HMM has to be used wsth care as continuous?
and Untventity ot rI~d,nburlh.
80 South St,die.
Edinburgh EHI tHN.
UKSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213USAprobability density functions make more assumption than thediscrete HMM, especially when the diagonal cov?riance Gsussianprobability density is used for simplicity \[15\].
To obtain a betterrecognition accuracy, acoustic parameters must be well chosenaccording to the assumption of the continuous probability densityfunctions usedThe sent.continuous hidden Markov model 'SCHMM~ has beenproposed to ext,'nd the d,screte tIMM by replacing discrete outputprobabthty d,strtbuttons wash a combination of the origtnaldiscrete output probabthty distributions and continuous probabil.~ty density functions of?
Gaussian codebook {6\].
In the SCHMM.each VQ codeword is regarded as a Gaussian probability dens,ty\[ntuttlvely.
from the discrete HMM point of view, the SCHMMtries to smooth the discrete output probabilities with multiplecodewordcandidates in VQ procedure From the continuous mix-ture HMM point of view, the SCHMM ties all the continuous out-put probability densities across each individual HMM to form ashared Gaussian codebook, i e. a mixture of Gaussian probabilitydensities.
With the SCHMM.
the codebook and HMM can bejointly re-estimated to achieve an optimal eodebookmodel combi-nation in sense of maxtmum likelihood criterion.
Such a tying canalso substantially reduce the number of free parameters and com-putational complexity in comparison to the continuous mixtureHMM.
while mains?in reasonablelv modeling power of a mixtureof ?
I'~t'ge number of probability density functions.
The SCHMMhas shown to offer improved recognition accuracy in severalspeech recognition experiments (6.8, 14,2\].\[n this study, the SCHMM is applied to Sphinx, ?
speaker-independent continuous speech recognition system.
Sphinx usesmultiple VQ codebooks for each acoustic observation \[12}.
Toapply the SCHMM to Sphinx.
the SCHMM algorithm must bemodified to accommodate multiple ?odebooks and multiplecodewords combination.
For the SCHMM re-estimation algorithm,the modified unified re.estimation algorithm for multiple VQ code-books and bidden Markov models are proposed in this paper.
Thespplicability of the SCHMM to speaker-independent conUnuousspeech is explored based on 200 generalized triphone models \[12\].In the t000-word speaker-independent continuous speech recogni-tion task using word-pair grammar, the error rate was reduced bymore than 29", and 41/', in comparison to the correspondingdiscrete HMM and continuous maxture HMM respectively.2.
SEMI.CONTINUOUS HIDDEN MARKOV MODELS2.1.
Discrete HMMs and Continuous HMMsAn N:state Markov chain with state transition matrix A=\[a,:\].i,j=l.
2 ..... N. where o, I denotes the transition probability fromstate i to state j; and a discrete output probability distribution.b:,O,', or continuous output probability density function b.tx)associated with each st?re j of the unobservable Markov chain isconszdered here.
Here O, represents discrete observation symbolstusually VQ mdicesL and x represents continuous observations~usually speech frame vectors~ of K-dimensional random vectors.With the discrete HMM.
there are L discrete output symbols froma L.level VQ.
and the output probability ts modeled with discreteprobability distrtbutmns of these discrete symbols.
Let O be theobserved sequence, O= O,O,~' ' .O , r  observed over T samples.Here O~, denotes the VQ codeword k, observed at time i. The276observation probability of such an observed sequence.
Pr' OI k ,.
canbe expressed as:TPr(O\[k) = ~ ~, .
l ' \ [n .
,_ , ,b , , (O.  )
(t)where S is a particular state sequence.
S ( fsss~ "'" .s,>.
s~ ( {t.2.
.
N!.
and the summation is taken over all of the possible state~equences.
S. of the given model,k, which is represented byler, A.B,, where t7 is the initial state probability vector, A is the statetr~nsition matrix, snd B ~s the output probability distributionmatrix.
In the discrete HMM.
classification of O,, from x, in theVQ may not be accurate.I f  the observation to be decoded is not v~ctor quantized, then theprohability dens=ty function, fl X IAL of producing an observat;on ofcontinuous vector sequences given the model A, would be com-puted.
~nstead of the probability of generating a discrete observa.turn ~ymboi.
Pr~OlL~.
Here X is a sequence of contlnuous acous.t,c, vectors x. X = x~x 2 ?
XT The principal advantage of u~ngthe continuous HMM is the ability to directly model speechparameters without involving VQ However, the cont inuousHMMrequires considerably longer training and recognition times, espe-cially when a mixture of several Gaussian probability densitycomponents is used.
In the continuous Gaussian ~M-component)mixture HMM I l t} .
the output probabil ity density of state y. b:,x).can be represented asMk=twhere N(x,#.~I denotes a multi-dimensional Gauss~an densityfunct=on of mean vector p and covariance matrix ~.
and ca, is aweighting coefficient for thekthGaussiancomponent With suchemixture, any arbitrary distr ibution can be approximatelymodeled, provided the mixture is large enough.22 Semi.Continuous Hidden Markoc ModelsIn the discrete HMM.
the discrete probability distr ihutmns ere?
ufficwnHy powerful to model any random events with a reason-able number of parameters.
The major problem with the discreteoutput probability is that the VQ operatmn partitions the acousticspace znto separate regions according to some distortion measureThis introduces errors as the partition operations may destroy theoriginal ssgnal structure An improvement is to model the VQcodebook as a family of Gaussian density functions such that thed~str ibut ionsare overlaped, rather thsnd is jo in ted  Each codewordnf the codehook can then be represented by one of the Gaussianprobability density functions and may be used together with oth-ers to model the acoustic event.
The use of a parametric family offimte m~xture densities la mixture density VQ~ can then beclosely combined with the HMM methodology.
From the continu-ous mixture HMM point of view.
the output probability in the con-tinuous mixture HMM is shored among the Gaussian probabilitydensity functions of the VQ.
This can reduce the number of freeparameters to be estimated as well as the computational complex.~ty.
From the discrete HMM point of view.
the pertihon of theVQ is unnecessary, and is replaced by the mixture density model.~ng with overlap, which can effectively minimize the VQ errorsThe procedure, known as the EM algorithm \[3\].
is a specialization.to the mixture density context, of a general algorithm for obtain.~ng maximum likelihood estimates.
This has been defined earlierby Baum \[l\] in a similar way and has been widely used in HMM-based speech recognition methods.
Thus, the VQ problems andHMM modeling problems can be unified under the same proba-bilistic framework to obtain an optimized VQ'HMM combination,which forms the foundation of the SCHMM.Provided that each eodeword of the VQ codeboek is represented bya Gaussian density function, for a given state s~ of HMM,  the pro-bability density function that sr produces a vector x can then bewritten as:b, , (x l  = f(zls,) = ,~ f(xlO,,,s,)Pr(Os, ls,) (3)where L denotes the VQ codebook level.
For the sake of simpli-city.
the output probabil ity density function conditioned on thecodewords can be assumed to be independent of the Markov statesst.
,31 can then be wrltten as:f~xlsA = ~ f~xlOl,,Pr~Oj, ls ,, = hxlO~,b,'O~,~ '4~j= t lThis equation is the key to the semi-continuous hidden Markovmodeling Given the VQ codebook index 04, the prob..hllity den-sity function f, xlOs ~ can be estimated with the EM algorithm !t71.or max lmum hkelihood clustering.
\[t can also be obtained fromthe HMM parameter estimatLondlrectiyasexplalned later.
Csingq4~ to represent the semi.continuous outpu!
probabihty density, itis possible to combine the codebook distortmn characteristics wRhthe parameters of the discrete HMM under a umfied probabilistieframework.
Here.
each discrete output probability is weighted bythe conUnuous conditional Gauss;an prohahility density functionderived from VQ If these continuous VQ densnty functions areconsidered as the continuous output probahility denslty functionin the continuous mlxture HMM.
this also resembles the L-mixture continuous \ [{MM with all the continuous output probabil-ity density functions shared with each other tn the VQ codebuokHere the discrete output probability in state ~.
h}O, ~, becomes theweightnng coefficients for the mixture componentsIn implementation of the SCHMM !8\], Eq.
14~ can be replaced byfinding M most significant values offlxlO s) lwith M be one to six.the algorithm converges well in practice) over all possible code-book indices O v which can he easily obtained in the VQ pro-cedure.
This can signif icantly reduce the amount of computat ionalload for subsequent output probabil ity computation since M is oflower order than L Experimental  results show this to performwell in speech recognmon \[8\], and result in an L-mixture continu-ous HMM with a computational complexity significantly lowerthan the continuous mixture HMM2.3.
Re-est~matton formulas for the SCHMMIf the bJO~) are considered as the weighting coefficients ofdifferent mixture output probability density functions in the con-tinuous mixture i{MM.
thc re.csHmatinn algor=thm for theweighting coefficients can be extended to re-estimate h,~O#~ I of theSCHMM till.
The re-estimation formulations can be more readdycomputed by defining a forward partial probability, nJH.
and abackward partial probability.
~fti) for any time t and state i as:af(i~ = Pr (x t ,x=. '
' '  x,.sf=il~t)(Sa~~f(i~ = Pr~xt.
l .x , .Z " " " z r ls~=i ,  ~)The intermediate probabilities, x,#ij ,k), "ft(ij~.
'?~(il, ~q| j ) .
and~'~lj) can be defined as follows for efficient re-estimation of themodel parameters:X~l j .k )  = Pr(s~=i, S l .
l= j ,  O~., \ ]X,  A,)7t=i j I  = Pros, = i .
st?~ =j IX ,  ~)7~)  = Pr{s~=ilX.
k) (6)~,q.ki  = Pr 's~=i.O,~lX,~)~lk~ = Pr(O,, lX , k~All these intermediate probabilities can be represented by X~II.Using Eq.
,5~ and ~6~.
the re-est=mation equatmns for ~,.
o:,.
endb,(O#) can be written as:~, = 711i).TN~t ' , J )~?
}i)t=|#ls i , jsN; (8)l s i sN;  L.~j~L.
~9~The means and covariances of the Gaussian probabil i ty densityfunctions can al?o be re-estimated to update the VQ codebookseparately with Eq tS~ and r6).The feedback from the HMM esti-277mat,on result~ to the VQ codebouk ~mplies that the VQ codebook~s opt,mixed based on the HMM likelihood maximization ratherthan mmimizing the total distortion errors from the set of tra,n.,ng data.
Although re-estimation of means and covariances ofdifferent models will involve inter-dependencies, the different den-sity functions which are re-estimated are strongly correlated.
Tore-estimate the parameters of the VQ codebook, i.e.
the means.
9:,and covariance matrices?
~,.
of the codcbook index j. it is notd~cult  to extend the continuous mixture HMM re-estimationalgorithm with modified Q function.
In general, it can be writtenBe:TEi E ~,,s ,,,I~; = r .
I~ j~L ;  ?I0)andr E\[ ~,*j,,x,-;,.a,-~,:l~s = " ~" , ISj~2L.
fll)where L. denotes the HMM used; and e.apression~ in !
\] ere vari.shies of model p. In Eo ,10~ and ,I lL the re e~timatinn for themeans and covariance matrices m the output probab~hty densityfunction of the SCHMM are tied up with all the t{MM models.which ,s similar to the approach w,th bed irene,lion probabdityinside the model \[10\].
From Eq ,10b and ~1l '.
,t can he observedthat they are merely a special form of EM algorithm for theparameter estimation of mixture dens,ty functions liT\].
which areclosely welded into the HMM re-estimahon equationsWhen multiple ?odebooks are used.
each codebook represents aset of different speech parameters.
One way to combine these mul-tiple output observations i  to assume that they are ,ndependent.and the output probability is computed as the product of the out-put probability of each codebook It has been shown that perfor-mance using multiple codebook can be substation~lly improved\[i3\] In the semi-continuous HMM.
the semi-continuous outputprobability of multiple codebooks can also be computed as the pro-duct of the semi-continuous output probability for each codebookas Eq '4'.
which consists of L-mixture continuous density fur, c.lions.
In other word, the semi-continuous output probability couldbe modified as'Lb,,x, : II E:,alo;,,~:,,o;,, ,12,whcrc c denotes the codebook used.
The re-estimation algorithmfor the mulhple codebook based HMM could be extended if Eq.~6 a* ,s computed for each codeword of each codebook c with thecombination of the rest codebook probability \[7\].3.
EXPERIMENTAL EVALUATION3.1, Ana lys ts  ConditionsFor both training and evaluation, the standard Sphinx front-endconsists of t2th order bilinear transformed LPC cepstrum \[12\].The complete database consists of 4358 training sentences from105 speakers Qune-train~ and 300 test sentences from 12 speakers.The vocabulary of the Resource Management database is 99Iwords.
There is also an uncial word.pair ecognition grammar.which is just a list of allowable word pairs without probabilitiesfor the purpose of reducing the recognition perplexity to about 60.3.2.
Experimental Results Us ing  Bilinear Transformed CepstrumDiscrete HMMs and continuous mixture HMMs based on 200 gen-eralized triphones are first experimented as benchmarks.
Thediscrete HMM is the same as Sphinx except only 200 generalizedtriphones are used \[12\].In the continuous mixture HMM implemented here.
the cepstrum,difference cepstrum, normalized energy, and difference energy arepacked into one vector.
This is similar to the one codebook imple-mentation of the discrete HMM \[12I.
Each continuous outputprobahllity consists of 4 diagonal q;aus~*an pruhahlhty densityfunction as Eq ,2~ To obte,n rehahle initial m,~dels for the con.tinuous mixture HMM.
the Viterbi alignment w,th the d,screteHMM is used to phonetically segment and label trammg speech.These labeled segments are then clustered by using the k-meansclustering algorithm to obtain initial means and diagonal covari-ances.
The forward-backward algorithm is used iteratively for themonophone models, which are then used as initial models for thegeneralized trlphone models.
Though contmuous mixture HMMwas reported to significantly better the performance of the discreteHMM \[15\].
for the experiments conducted here.
it is signi/~cantlyworse than the discrete HMM Why is this paradox?
One expla-.nation is that multiple codebooks are used in the discrete HMM.therefore the VQ errors for the discrete HMM are not so serioushere.
Another reason may be that the diagonal coverianceassumption is not appropriate for the bilinear transformed LPCcepstrum since many coefficients are strongly correlated after thetransformation.
I deed, observation of average covariance matrixfor the bilinear transformed LPC cepstrum shows that values ofoff-diagonal components are generally quite large.For the semi-continuous model, multiple codebooks are usedinstead of packing different feature parameters into one vectorThe initial model for the SCHMM comes directly from the discrete\[IMM with the VQ variance obtained from k-mean~ clustering foreach codeword In computing the semi cuntinuousoutput probabil.~ty den~*ty function, only the M ' i.
4 here, must qtcntficant code-wordq are used for subsequent processing Under the sameanalysis condition, the percent correct tcorrect word percentage,and word accuracy tpercent correct - percent insertiont results ofthe discrete HMM.
the continuous mixture IIMM, and theSCHMM are shown in Table 1.\[ Table 1Average recognition accuracyt#pes percent correct (u:ord accuracy)Discrete HMM 8915% (88.0qContinuous Mixture HMM 842~ ~81.3~SCHM.M - topl 87.2c~ ,84.0~)SCHMM ~ top4 90 8'~" ,89 .1 '3~From Table 1. it can be observed that the SCHMM with top 4codewords works better than both the discrete and continuousmixture HMM The SCHMM with top I codeword works actuallyworse than the discrete'HMM, which indicates that diagonalGaussian assumption may be inappropriate here Though bilineartransformed cepstral coefficients could not be well modeled by thediagonal Gaussian assumption ,which was proven by the poor per-formance of the continuous mixture HMM and the SCHMM wit__hGaussian assumption lwhich was proven by the poor performanceof the continuous mixture HMM and the SCHMM with top I code-word), the SCHMM with tdp 4 codewords works modestly betterthan the discrete HMM.
The improvement may primarily comefrom smoothing effect of theSCHMM, ie the robustness of multi.pie codewords and multiple codebooks in the sem,.continuous out-put probability representation, albeit 200 generalized triphonemodels are relatively well trained in comparison to standardSphinx version \[12\], where I000 generalized triphone models areused.3.3.
Experimental Results Using Less Correlated DataIf the diagonal Gaussian covariance is used, each dimension inspeech vector should be un-correlated.
In practice, this can be par-tially satisfied by using less correlated feature as acoustic observa-tion representation.
One way to reduce correlation is principalcomponent projection.
In the implementation here.
the projectionmatrix is computed by first pooling together the bilinear?
transformed cepstrum of the whole training sentences, and thencomputing the eigenvector of that pooled coveriance matrixUnfortunately.
only insignificant improvements are obtainedbased on such a projection\[7\] This is because the covariance foreach codeword is quite different, and suc.h a proJection only makesaverage covariance diagonal, which is inadequate278As bilinear transformed cepstral coefficients could not be wellmodeled hy diagnnal Gaus~ian prohahihty density function, exper-tments without bdmear transformation are conducted The lSthorder cepstrum is used here for the SCHMM because of less corre-lated characteristics of the cepstrum With 4:\]58 Ira=sang sen-tences {june-trainL test results of 300 sentences cjune.test~ arelisted in Table 2.Table 2Average accuracy of IRtb order cepstrum~~.______.~e.fcent correct (.,nrd nccurncv)-~'screte HMM \[ 863% '83.8~':ISCHMM -~ topl I ~6 e~ ,as s,-~,SCHMM .Mop2 t 898~,876~SCHMM + top4 \] :q93% ,~RSr;-~SCHMM + topfi \[ 896q ,~86~;~SCHMM + top8 I 89 3%,~g2"~*Here.
the recognition accuracy of the SCHMM is significantlyimproved in comparison with the discrete l tMM,  and error reduc-tion is over 29% Even the SCHMM with top one codeword isused.
it is still better than the discrete HMM '855~ vs, ~3S%,.Use of multiple codewords ,top4 and top6~ m the semi-conhnuousoutput probability density functmn greatly improves the wordaccuracy Ifrom 85.5% to 88.6'31.
Further increase of codewordsused in the semi-continuous output probability density functionsshows no improvement on word accuracy, but substantial growthof computational complexity.
From Table 2. it can be seen that theSCHMM with top four codewords is adequate 188 5% ~ \[n contrast.when bilinear transformed data was used.
the error reduction isless than 10% in comparison to the discrete HM:.I.
and theSCHMM with top one codeword is actually slightly worse than thediscrete HMM.
This strongly md~cates that appropriate feature ,svery ~mportant if continuous probability density function, rape.cially diagonal covariance a*sumption, is used If assumption isinappropriate, maximum likelihood estimation will only maximizethe tcrnng assumption Although more than 29"; error reductionhas been achieved for lath order LPC analyses using diagonalcovariance assumption, the last results with the discrete HMM'bdinear transformed cepstrum.
883~ and the SCHMM 1lathorder ccpstrum.
~3R";, are about the same This ~u~zgest that hil.ineur trunsformutmn is helpful for recognlt*on, hut have corre.luted coefT~c~ents, which is inappropriate to the dt,gonul L;auss,unussumptmn It can be expected that wHh the full covarlanceSCHMM and bilinear transformed cepstral data.
better recogni-tion accuracy can be obtained4.
CONCLUSIONSSemi-continuous hidden Markov models based on multiple vectorquantization codebooks take the advantages of both the discreteHMM and continuous HMM.
With the SCHMM, it is possible tomodel a mixture of a large number of probability density rune?tions with a limited amount of training data and computationalcomplexity Robustness is enhanced by using multiple codewordsand multiple codebooks for the semi-continuous output probabilityrepresentation.
In addition, the VQ codebook itself can beadjusted together with the HMM parameters in order to obtainthe optimum max imum likelihood of the HMM.
The applicab,lityof the continuous mixture HMM or the SCHMM relies onappropriately chosen acoustic parameters and assumption of thecontinuous probability density function.
Acoustic features must bewell represented if diagonal covariance is applied to the Geussianprobability density function.
This is strongly indicated by theexperimental results based on the bilineer transformed cepstrumand eepstrum.
With bilinear transformation, high frequency com-ponents are compressed in comparison to low frequency com.ponents \[2.3\].
Such a transformation converts the linear fre-quency axis into a mel.scale-like one.
The discrete HMM can besubstantially improved by bilinear transformation However.
bil.inear transformation introduces strong correlations, which is inap.propriate for the diagonal Gausstan assumption modeling.
Usingthe cepstrum without bilinear transformation, the diagonalSCHMM can be substantially improved in comparison to thediscrete HMMAll experiments conducted here were based on only 200 general-ired tr~phnnes as smoothing can play a more ~mportant role mthose less-well.tra=ned models, more improvement can be expectedfor 1000 generahzed triphones ,where the word accuracy for thediscrete HMM is 91~ with bilinear transformed ata~ \[n addi.tion.
removal of diagonal covariance assumption by use of fullcovariance can be expected to further improve recognition accu-racy\ [ l \ ] .
Regard,ng use of full covariance, theSC l (MM hasadis.tinctive advantage Since Gaussian probability density functionsare tied to the VQ codebook, by chosing M mo~t significant code-words, computational complexity can be several order lower thanthe conventional continuous mixture \ [{MM while mamtalning themodeling power of large mixture componentsExperimental results have clearly demonstrated that the SCIIMMofl'erx improved recognition accuracy in compurison to hoth thediscrete HMM and the continuous mixture HMM in speaker.independent continuous speech recognition, We conclude that theSCHMM is indeed a powerful technique for modeling non-stationary stochastic processes with multi-modal prohabilisticfunctions of Markov chainsACKNOWLEDGEMENTSWe would hke to thank Professor Mervyn Jack and Profe*sor Ra| Reddyfor their help ~nd ~nslght shared In this researchREFERENCES\[l\]Baum.
LE and et ~l .
"A maximi~atlontechniqueoccunngln the,to.tlstIcsl analysis of prohablhstlc functions of Marker chuln~".
J AnnMath ~tat Vol 41. pp 164.171.
1970iJI Bellegarda.
J and Nahamoo D .
"Tied mixture contmuous parametermode\[~ for large ~ocabuLar) isolated ~peech recogmtmn".
ICASSP 89.Glasgow.
Scotlund.
1989\[3!
Dempqter.
A P .
Laird N M .
and Ruhin.
D B "Maximum-likelihoodfrom incomplete data ~ia the EM algorithm".
J Royal Statist Soc.
Ser Brmethodologlcsl,.Vol 39. pp 1-38.1977f4\] Doddmg'ton.
G R. "Phonetically *ensltive discriminan~ for improved*peech recogmtion".
1CASSP a9.
GIs.gow.
Scotland.
1989\[51Gupta.
VN.
Lenn,?
M. and Mermelstein.
P "Tntegratioo of Acous.tic Information in u L,rge Vocebulury Word Recognizer".
ICASSP 87. pp.~97.700 19A7"6t Huung, X O ,Jr:~ ,l.~k.
MA "Hidden Markov modelling of speechh,~ed on u ~emt continuous model".
Electromcs Letter~.
Vul.
24. pp 6.7.f71 Huang.
X D .
Hen.
H W .
and Lee.
K P.. "Multiple codebook semi-continuous hidden Murkov models for speaker-independent continuousspee:h recognitmn".
CMU Technical Report CMU-CS-89.136.
C. Science.1989\[8}Hueng.
XD endJack.
MA.."Semi.continuoushidden Marker modelsfor speech recognltmn".
Computer Speech end Language.
VoI 3.
1989\[9\] Jelinek.
F. "Continuous speech recognition by statistical methods".Proceedings of IEEE.
Vol 64. pp 532.5.56.
1976\[10~ Jelioek.
F. and Mercer.
RL,  "Interpolated estimation of Markersource parameters from sparse data".
The workshop on pattern recogni-tion in practice.
Amsterdam.
\[980\ [ t l l  Juang.
B H .
"Maxsmum-hkehhood eqt~mation for mixture multivari-ate stochastic nE.er'various of Marker chain".
AT&T Technical JournalVol 64. pp 1235-1249.
19A5\[12T Lee.
K F, "Large-vocabulary ,peuker-lndependent contmuous ~peechrecognitmn The SPHINX %stem" Ph D thesis.
C Science.
CMU.
t9gA\[t3i Lea.
K F .
Hen.
H W .
and Redd~.. R. "The SPHINX speech recogni.tlon system", ICASSP ~9.
Glasgow.
Scotland.
1989\[141 Paul.
D. "The Lincoln continuous speech recognition system recentdevelopment~ and results".
DARPA \[989 Feb meeting, t989\[t5TRabiner.
LR  and et al"Recogn=tlonofisolated d=g=ts u,ln 8hiddenMarker models with contmuous mixture denslties".
AT&T TechnicalJournaI.
Vol 64. pp 1211-1234.
198.5It6\] Rabiner.
L R. Wflpon.
JG., end Soong.
FK.. "High PerformanceConnected Digit Recognition Using Hidden Marker Models".
ICASSP 88,NY, t988{17\] Redner.
R A and Walker.
H F. "Mixture densities, muaimum \[ikeli.hood and the EM algorithm".
SLAM review.
Vol.
26. pp.
195-239.
L984(18\] Sh~kano.
K .
"Evaluation of LPC spectral matching meuqures forphonetic unit recognition".
CMU Technical Report CMU.CS-86-\[0B.
CScience, 1.986279
