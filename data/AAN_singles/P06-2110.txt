Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 858?865,Sydney, July 2006. c?2006 Association for Computational LinguisticsWord Vectors and Two Kinds of SimilarityAkira Utsumi and Daisuke SuzukiDepartment of Systems EngineeringThe University of Electro-Communications1-5-1 Chofugaoka, Chofushi, Tokyo 182-8585, Japanutsumi@se.uec.ac.jp, dajie@utm.se.uec.ac.jpAbstractThis paper examines what kind of similar-ity between words can be represented bywhat kind of word vectors in the vectorspace model.
Through two experiments,three methods for constructing word vec-tors, i.e., LSA-based, cooccurrence-basedand dictionary-based methods, were com-pared in terms of the ability to representtwo kinds of similarity, i.e., taxonomicsimilarity and associative similarity.
Theresult of the comparison was that thedictionary-based word vectors better re-flect taxonomic similarity, while the LSA-based and the cooccurrence-based wordvectors better reflect associative similarity.1 IntroductionRecently, geometric models have been used to rep-resent words and their meanings, and proven tobe highly useful both for many NLP applicationsassociated with semantic processing (Widdows,2004) and for human modeling in cognitive sci-ence (Ga?rdenfors, 2000; Landauer and Dumais,1997).
There are also good reasons for studyinggeometric models in the field of computational lin-guistics.
First, geometric models are cost-effectivein that it takes much less time and less effort toconstruct large-scale geometric representation ofword meanings than it would take to construct dic-tionaries or thesauri.
Second, they can representthe implicit knowledge of word meanings that dic-tionaries and thesauri cannot do.
Finally, geomet-ric representation is easy to revise and extend.A vector space model is the most commonlyused geometric model for the meanings of words.The basic idea of a vector space model is thatwords are represented by high-dimensional vec-tors, i.e., word vectors, and the degree of seman-tic similarity between any two words can be easilycomputed as a cosine of the angle formed by theirvectors.A number of methods have been proposed forconstructing word vectors.
Latent semantic anal-ysis (LSA) is the most well-known method thatuses the frequency of words in a fraction of doc-uments to assess the coordinates of word vectorsand singular value decomposition (SVD) to reducethe dimension.
LSA was originally put forward asa document indexing technique for automatic in-formation retrieval (Deerwester et al, 1990), butseveral studies (Landauer and Dumais, 1997) haveshown that LSA successfully mimics many hu-man behaviors associated with semantic process-ing.
Other methods use a variety of other informa-tion: cooccurrence of two words (Burgess, 1998;Schu?tze, 1998), occurrence of a word in the sensedefinitions of a dictionary (Kasahara et al, 1997;Niwa and Nitta, 1994) or word association norms(Steyvers et al, 2004).However, despite the fact that there are differ-ent kinds of similarity between words, or differ-ent relations underlying word similarity such as asynonymous relation and an associative relation,no studies have ever examined the relationship be-tween methods for constructing word vectors andthe type of similarity involved in word vectors ina systematic way.
Some studies on word vec-tors have compared the performance among dif-ferent methods on some specific tasks such as se-mantic disambiguation (Niwa and Nitta, 1994) andcued/free recall (Steyvers et al, 2004), but it is notat all clear whether there are essential differencesin the quality of similarity among word vectorsconstructed by different methods, and if so, whatkind of similarity is involved in what kind of wordvectors.
Even in the field of cognitive psychol-ogy, although geometric models of similarity suchas multidimensional scaling have long been stud-ied and debated (Nosofsky, 1992), the possibilitythat different methods for word vectors may cap-858ture different kinds of word similarity has neverbeen addressed.This study, therefore, aims to examine the re-lationship between the methods for constructingword vectors and the type of similarity in a sys-tematic way.
Especially this study addresses threemethods, LSA-based, cooccurrence-based, anddictionary-based methods, and two kinds of sim-ilarity, taxonomic similarity and associative sim-ilarity.
Word vectors constructed by these meth-ods are compared in the performance of two tasks,i.e., multiple-choice synonym test and word asso-ciation, which measure the degree to which theyreflect these two kinds of similarity.2 Two Kinds of SimilarityIn this study, we divide word similarity into twocategories: taxonomic similarity and associativesimilarity.
Taxonomic similarity, or categoricalsimilarity, is a kind of semantic similarity betweenwords in the same level of categories or clusters ofthe thesaurus, in particular synonyms, antonyms,and other coordinates.
Associative similarity, onthe other hand, is a similarity between words thatare associated with each other by virtue of seman-tic relations other than taxonomic one such as acollocational relation and a proximity relation.
Forexample, the word writer and the word author aretaxonomically similar because they are synonyms,while the word writer and the word book are as-sociatively similar because they are associated byvirtue of an agent-subject relation.This dichotomy of similarity is practically im-portant.
Some tasks such as automatic thesaurusupdating and paraphrasing need assessing taxo-nomic similarity, while some other tasks such asaffective Web search and semantic disambiguationrequire assessing associative similarity rather thantaxonomic similarity.
This dichotomy is also psy-chologically motivated.
Many empirical studieson word searches and speech disorders have re-vealed that words in the mind (i.e., mental lex-icon) are organized by these two kinds of simi-larity (Aitchison, 2003).
This dichotomy is alsoessential to some cognitive processes.
For ex-ample, metaphors are perceived as being moreapt when their constituent words are associativelymore similar but categorically dissimilar (Utsumiet al, 1998).
These psychological findings suggestthat people distinguish between these two kinds ofsimilarity in certain cognitive processes.3 Constructing Word Vectors3.1 OverviewIn this study, word vectors (or word spaces) areconstructed in the following way.
First, all con-tent words ti in a corpus are represented as m-dimensional feature vectors wi.wi = (wi1, wi2, ?
?
?
, wim) (1)Each element wij is determined by statistical anal-ysis of the corpus, whose methods will be de-scribed in Section 3.3.
A matrix M is then con-structed using n feature vectors as rows.M =( w1...wn)(2)Finally, the dimension of row vectors wi is re-duced from m to k by means of a SVD tech-nique.
As a result, any words are represented ask-dimensional vectors.3.2 CorpusIn this study, we employ three kinds of Japanesecorpora: newspaper articles, novels and a dictio-nary.
As a newspaper corpus, we use 4 months?worth of Mainichi newspaper articles publishedin 1999.
They consist of 500,182 sentences in251,287 paragraphs, and words vectors are con-structed for 53,512 words that occur three timesor more in these articles.
Concerning a corpus ofnovels, we use a collection of 100 Japanese nov-els ?Shincho Bunko No 100 Satsu?
consisting of475,782 sentences and 230,392 paragraphs.
Wordvectors are constructed for 46,666 words that oc-cur at least three times.
As a Japanese dictionary,we use ?Super Nihongo Daijiten?
published byGakken, from which 89,007 words are extractedfor word vectors.3.3 Methods for Computing Vector ElementsLSA-based method (LSA)In the LSA-based method, a vector element wijis assessed as a tf-idf score of a word ti in a piecesj of document.wij = tfij ?
(log mdfi+ 1)(3)In this formula, tfij denotes the number of timesthe word ti occurs in a piece of text sj , and dfidenotes the number of pieces in which the wordti occurs.
As a unit of text piece sj , we consider859a sentence and a paragraph.
Hence, for example,when a sentence is used as a unit, the dimension offeature vectors wi is equal to the number of sen-tences in a corpus.
We also use two corpora, i.e.,newspapers and novels, and thus we obtain fourdifferent word spaces by the LSA-based method.Cooccurrence-based method (COO)In the cooccurrence-based method, a vector ele-ment wij is assessed as the number of times wordsti and tj occur in the same piece of text, and thusM is an n ?
n symmetric matrix.
As in the caseof the LSA-based method, we use two units of textpiece (i.e., a sentence or a paragraph) and two cor-pora (i.e., newspapers or novels), thus resulting infour different word spaces.Note that this method is similar to Schu?tze?s(1998) method for constructing a semantic spacein that both are based on the word cooccurrence,not on the word frequency.
However they are dif-ferent in that Schu?tze?s method uses the cooccur-rence with frequent content words chosen as in-dicators of primitive meanings.
Burgess?s (1998)?Hyperspace Analogue to Language (HAL)?
isalso based on the word cooccurrence but does notuse any technique of dimensionality reduction.Dictionary-based method (DIC)In the dictionary-based method, a vector ele-ment wij is assessed by the following formula:wij =?
?fij + ??
?kfikfkj + ?fji???
log ndfj(4)where fij denotes the number of times the wordtj occurs in the sense definitions of the word ti,and dfj denotes the number of words whose sensedefinitions contain the word tj .
The second termin parentheses in Equation (4) means the squareroot of the number of times the word tj occurs ina collection of sense definitions for any words thatare included in the sense definitions of the word ti,while the third term means the number of times tioccurs in the sense definitions of tj .
The param-eters ?
and ?
are positive real constants express-ing the weights for these information.
(FollowingKasahara et al (1997), these parameters are set to0.2 in this paper.
)Equation (4) was originally put forward byKasahara et al (1997), but our dictionary-basedmethod differs from their method in terms of howthe dimensions are reduced.
Their method groupstogether the dimensions for words in the same cat-egory of a thesaurus, but our method uses SVD aswe will described next.3.4 Reducing DimensionsUsing a SVD technique, a matrix M is factorizedas the product of three matrices U?V T , wherethe diagonal matrix ?
consists of r singular val-ues that are arranged in nonincreasing order suchthat r is the rank of M .
When we use a k ?
k ma-trix ?k consisting of the largest k singular values,the matrix M is approximated by Uk?kV Tk , wherethe i-th row of Uk corresponds to a k-dimensional?reduced word vector?
for the word ti.4 Experiment 1: Synonym Judgment4.1 MethodIn order to compare different word vectors interms of the ability to judge taxonomic similar-ity between words, we conducted a synonym judg-ment experiment using a standard multiple-choicesynonym test.
Each item of a synonym test con-sisted of a stem word and five alternative wordsfrom which the test-taker was asked to choose onewith the most similar meaning to the stem word.In the experiment, we used 32 items from thesynonym portions of Synthetic Personality In-ventory (SPI) test, which has been widely usedfor employment selection in Japanese companies.These items were selected so that all the vectorspaces could contain the stem word and at leastfour of the five alternative words.
For comparisonpurpose, we also used 38 antonym test items cho-sen from the same SPI test.
Furthermore, in orderto obtain a more reliable, unbiased result, we auto-matically constructed 200 test items in such a waythat we chose the stem word randomly, one correctalternative word randomly from words in the samedeepest category of a Japanese thesaurus as thestem word, and other four alternatives from wordsin other categories.
As a Japanese thesaurus, weused ?Goi-Taikei?
(Ikehara et al, 1999).In the computer simulation, the computer?schoices were determined by computing cosinesimilarity between the stem word and each of thefive alternative words using the vector spaces andchoosing the word with the highest similarity.4.2 Results and DiscussionFor each of the nine vector spaces, the synonymjudgment simulation described above was con-8600 100 200 300 400 500 600 700 800 900 10000.20.30.40.50.60.70.8                    ++++++++++++++++++++       LSA+ + COO DICNumber of DimensionsCorrectRate(a) SPI test items0 100 200 300 400 500 600 700 800 900 10000.20.30.40.50.60.7++++++++++++++++++++Number of DimensionsCorrectRate(b) Computer-generated test itemsFigure 1: Correct rates of synonym testsducted and the percentage of correct choices wascalculated.
This process was repeated using 20numbers of dimensions, i.e., every 50 dimensionsbetween 50 and 1000.Figure 1 shows the percentage of correctchoices for the three methods of matrix construc-tion.
Concerning the LSA-based method (denotedby LSA) and the cooccurrence-based method (de-noted by COO), Figure 1 plots the correct rates forthe word vectors derived from the paragraphs ofthe newspaper corpus.
(Such combination of cor-pus and text unit was optimal among all combi-nations, which will be justified later in this sec-tion.)
The most important result shown in Figure 1is that, regardless of the number of dimensions, thedictionary-based word vectors outperformed theother kinds of vectors on both SPI and computer-generated test items.
This result thus suggeststhat the dictionary-based vector space reflects tax-onomic similarity between words better than theLSA-based and the correlation-based spaces.Another interesting finding is that there was noclear peak in the graphs of Figure 1.
For SPI testitems, correct rates of the three methods increasedlinearly as the number of dimensions increased,r = .86 for the LSA-based method, r = .72 forthe correlation-based method and r = .93 for thedictionary-based method (all ps < .0001), whilecorrect rates for computer-generated test items0 100 200 300 400 500 600 700 800 900 10000.20.30.40.50.60.70.80.9  LSA (synonym)  LSA (antonym)  DIC (synonym) DIC (antonym)Number of DimensionsCorrectRateFigure 2: Synonym versus antonym judgmentwere steady.
Our finding of the absence of anyobvious optimal dimensions is in a sharp contrastto Landauer and Dumais?s (1997) finding that theLSA word vectors with 300 dimensions achievedthe maximum performance of 53% correct ratein a similar multiple-choice synonym test.
Notethat their maximum performance was a little bet-ter than that of our LSA vectors, but still worsethan that of our dictionary-based vectors.Figure 2 shows the performance of the LSA-based and the dictionary-based methods in anto-nym judgment, together with the result of syn-onym judgment.
(Since the performance of thecooccurrence-based method did not differ fromthat of the LSA-based method, the correct ratesof the cooccurrence-based method are not plottedin this figure.)
The dictionary-based method alsooutperformed the LSA-based method in antonymjudgment but their difference was much smallerthan that of synonym judgment; at 200 or lowerdimensions LSA-based method was better thanthe dictionary-based method.
Interestingly, thedictionary-based word vectors yielded better per-formance in synonym judgment than in antonymjudgment, while the LSA-based vectors showedbetter performance in antonym judgment.
Thesecontrasting results may be attributed to the differ-ence of corpus characteristics.
Dictionary?s defi-nitions for antonymous words are likely to involvedifferent words so that the differences betweentheir meanings can be made clear.
On the otherhand, in newspaper articles (or literary texts), con-text words with which antonymous words occurare likely to overlap because their meanings areabout the same domain.Finally, we show the results of comparisonamong four combinations of corpora and text unitsfor the LSA-based and the cooccurrence-based861Table 1: Comparison of mean correct rate amongthe combinations of two corpora and two text unitsNewspaper NovelMethod Para Sent Para SentSPI testLSA 0.383 0.366 0.238 0.369COO 0.413 0.369 0.255 0.280Computer-generated testLSA 0.410 0.377 0.346 0.379COO 0.375 0.363 0.311 0.310Note.
Para = Paragraph; Sent = Sentence.methods.
Table 1 lists mean correct rates of SPItest and computer-generated test averaged over allthe numbers of dimensions.
Regardless of con-struction methods and test items, the word vectorsconstructed using newspaper paragraphs achievedthe best performance, which are denoted by bold-faces.
Concerning an effect of corpus difference,the newspaper corpus was superior to the literarycorpus.
The difference of text units did not have aclear influence on the performance of word spaces.5 Experiment 2: Word Association5.1 MethodIn order to compare the ability of the wordspaces to judge associative similarity, we con-ducted a word association experiment using aJapanese word association norm ?Renso Kijun-hyo?
(Umemoto, 1969).
This free-associationdatabase was developed based on the responses of1,000 students to 210 stimulus words.
For exam-ple, when given the word writer as a stimulus, stu-dents listed the words shown in Table 2.
(Table 2also shows the original words in Japanese.
)For the simulation experiment, we selected 176stimulus words that all the three corpora con-tained.
These stimuli had 27 associate words onaverage.
We then removed any associate wordsthat were synonymous with the stimulus word(e.g., author in Table 2), since the purpose of thisexperiment was to examine the ability to assessassociative similarity between words.
Whether ornot each associate is synonymous with the stimu-lus was determined according to whether they be-long to the same deepest category of a Japanesethesaurus ?Goi-Taikei?
(Ikehara et al, 1999).In the computer simulation, cosine similarityTable 2: Associates for the stimulus word writerStimulus: writer Associates:novel pen literary work painter	 book author best-seller money  fiff flwrite literature play art workffi  !" # $	% '&popular human book paper pencil(*) +ffi*, - ./lucrative writing mystery music0132 !4 56 78between the stimulus word and each of all theother words included in the vector space was com-puted, and all the words were sorted in descendingorder of similarity.
The top i words were then cho-sen as associates.The ability of word spaces to mimic humanword association was evaluated on mean preci-sion.
Precision is the ratio of the number ofhuman-produced associates chosen by computerto the number i of computer-chosen associates.
Aprecision score was calculated every time a newhuman-produced associate was found in the top iwords when i was incremented by 1, and after thatmean precision was calculated as the average of allthese precision scores.
It must be noted here that,in order to eliminate the bias caused by the dif-ference in the number of contained words amongword spaces, we conducted the simulation using46,000 words that we randomly chose for eachcorpus so that they could include all the human-produced associates.Although this computational method of produc-ing associates is sufficient for the present purpose,it may be inadequate to model the psychologicalprocess of free association.
Some empirical stud-ies of word association (Nelson et al, 1998) re-vealed that frequent or familiar words were highlylikely to be produced as associates, but our meth-ods for constructing word vectors may not directlyaddress such frequency effect on word association.Hence, we conducted an additional experiment inwhich only familiar words were used for comput-ing similarity to a given stimulus word, i.e., lessfamiliar words were not used as candidates of as-8620 100 200 300 400 500 600 700 800 900100000.0050.0100.0150.0200.0250.030                    ???
?
??
?
?
??
?
?
?
??
?
?
?
?
?++++++++++++++++++++  LSA (Ave)   LSA (Max)+ + COO (Ave) ?
?
COO (Max)DICNumber of DimensionsMeanPrecisionFigure 3: Mean precision of word associationjudgmentsociates.
For a measure of word familiarity, weused word familiarity scores (ranging from 1 to 7)provided by ?Nihongo no Goitaikei?
(Amano andKondo, 2003).
Using this data, we selected thewords whose familiarity score is 5 or higher as fa-miliar ones.5.2 Results and DiscussionFor each of the nine vector spaces, the associationjudgment simulation was conducted and the meanprecision was calculated.
As in the synonym judg-ment experiment, this process was repeated by ev-ery 50 dimensions between 50 and 1000.Figure 3 shows the result of word associationexperiment.
For the LSA-based and the cooccur-rence-based methods, two kinds of mean precisionwere plotted: the average of mean precision scoresfor the four word vectors and the maximum scoreamong them.
(As we will show in Table 3, theLSA-based method achieved the maximum preci-sion when sentences of the newspaper corpus wereused, while the performance of the cooccurrence-based method was maximal when paragraphs ofthe newspaper corpus were used.)
The overallresult was that the dictionary-based word vectorsyielded the worst performance, as opposed to theresult of synonym judgment.
There was no bigdifference in performance between the LSA-basedmethod and the cooccurrence-based method, butthe maximal cooccurrence-based vectors (con-structed from newspaper paragraphs) considerablyoutperformed the other kinds of word vectors.
1These results clearly show that the LSA-based and1These results were replicated even when all the human-produced associates including synonymous ones were usedfor assessing the precision scores.0 100 200 300 400 500 600 700 800 90010000.0100.0150.0200.0250.0300.0350.040???
?
?
??
?
?
?
?
?
?
??
?
?
?
?
?++++++++++++++++++++  LSA (Ave)   LSA (Max)+ + COO (Ave) ?
?
COO (Max)  DICNumber of DimensionsMeanPrecisionFigure 4: Average precision of word associationjudgment for familiar wordsthe cooccurrence-based vector spaces reflect as-sociative similarity between words more than thedictionary-based space.The relation between the number of dimen-sions and the performance in association judgmentwas quite different from the relation observed inthe synonym judgment experiment.
Although thescore of the dictionary-based vectors increased asthe dimension of the vectors increased as in thecase of synonym judgment, the scores of bothLSA-based and cooccurrence-based vectors hada peak around 200 dimensions, as Landauer andDumais (1997) demonstrated.
This finding seemsto suggest that some hundred dimensions may beenough to represent the knowledge of associativesimilarity.Figure 4 shows the result of the additional ex-periment in which familiarity effects were takeninto account.
As compared to the result withoutfamiliarity filtering, there was a remarkable im-provement of the performance of the dictionary-based method; the dictionary-based method out-performed the LSA-based method at 350 or higherdimensions and the cooccurrence-based method at800 or higher dimensions.
This may be becauseword occurrence in the sense definitions of a dic-tionary does not reflect the actual frequency or fa-miliarity of words, and thus the dictionary-basedmethod may possibly overestimate the similarityof infrequent or unfamiliar words.
On the otherhand, since the corpus of newspaper articles ornovels is likely to reflect actual word frequency,the vector spaces derived from these corpora rep-resent the similarity of infrequent words as appro-priately as that of familiar words.
2The result that the cooccurrence-based word863Table 3: Comparison of mean precision among thecombinations of two corpora and two text unitsNewspaper NovelMethod Para Sent Para SentAll associatesLSA 0.016 0.017 0.015 0.015COO 0.023 0.018 0.012 0.008Familiar associatesLSA 0.0261 0.0258 0.024 0.023COO 0.033 0.027 0.018 0.014Note.
Para = Paragraph; Sent = Sentence.vectors constructed from newspaper paragraphsachieved the best performance was again obtainedin the additional experiment.
This consistent resultindicates that the cooccurrence-based method isparticularly useful for representing the knowledgeof associative similarity between words.
The rela-tion between the number of dimensions and meanprecision was unchanged even if a familiarity ef-fect was considered.Finally, Table 3 shows the comparison resultamong four kinds of word vectors constructedfrom different corpora and text units in the exper-iment with and without familiarity filtering.
Thelisted values are mean precisions averaged over allthe 20 numbers of dimensions.
As in the case ofsynonym judgment experiment, word vectors con-structed from newspaper paragraphs achieved thebest performance, although only the LSA-basedvectors had the highest precision when they werederived from sentences of newspaper articles.
Asin the case of synonym judgment, the newspa-per corpus showed better performance than thenovel corpus, and especially the cooccurrence-based method showed a fairly large differencein performance between two corpora.
This find-ing seems to suggest that word cooccurrence in anewspaper corpus is more likely to reflect associa-tive similarity.6 Semantic Network and SimilarityAs related work, Steyvers and Tenenbaum (2005)examined the properties of semantic network, an-2Indeed, the dictionary-based vector spaces contained alarger number of unfamiliar words than the other spaces; 63%of words in the dictionary were judged as unfamiliar, whileonly 42% and 50% of words in the newspapers and the novelswere judged as unfamiliar.other important geometric model for word mean-ings.
They found that three kinds of semantic net-works ?
WordNet, Roget?s thesaurus, and wordassociations ?
had a small-world structure anda scale-free pattern of connectivity, but semanticnetworks constructed from the LSA-based vectorspaces did not have these properties.
They inter-preted this finding as indicating a limitation of thevector space model such as LSA to model humanknowledge of word meanings.However, we can interpret their finding in a dif-ferent way by considering a possibility that dif-ferent semantic networks may capture differentkinds of word similarity.
Scale-free networks havea common characteristic that a small number ofnodes are connected to a very large number ofother nodes (Baraba?si and Albert, 1999).
In the se-mantic networks, such ?hub?
nodes correspond tobasic and highly polysemous words such as makeand money, and these words are likely to be tax-onomically similar to many other words.
Henceif semantic networks reflect in large part taxo-nomic similarity between words, they are likelyto have a scale-free structure.
On the other hand,since it is less likely to assume that only a fewwords are associatively similar to a large numberof other words, semantic networks reflecting asso-ciative similarity may not have a scale-free struc-ture.
Taken together, Steyvers and Tenenbaum?s(2005) finding can be reinterpreted as suggestingthat WordNet and Roget?s thesaurus better reflecttaxonomic similarity, while the LSA-based wordvectors better reflect associative similarity, whichis consistent with our finding.7 ConclusionThrough two simulation experiments, we obtainedthe following findings:?
The dictionary-based word vectors better re-flect the knowledge of taxonomic similarity,while the LSA-based and the cooccurrence-based word vectors better reflect the knowl-edge of associative similarity.
In particular,the cooccurrence-based vectors are useful forrepresenting associative similarity.?
The dictionary-based vectors yielded bet-ter performance in synonym judgment, butthe LSA-based vectors showed better perfor-mance in antonym judgment.?
These kinds of word vectors showed the dis-tinctive patterns of the relationship between864the number of dimensions of word vectorsand their performance.We are now extending this work to examine inmore detail the relationship between various kindsof word vectors and the quality of word similarityinvolved in these vectors.
It would be interestingfor further work to develop a method for extract-ing the knowledge of a specific similarity fromthe word space, e.g., extracting the knowledgeof taxonomic similarity from the dictionary-basedword space.
Vector negation (Widdows, 2003)may be a useful technique for this purpose.
At thesame time we are also interested in a method forcombining different word spaces into one space,e.g., combining the dictionary-based and the LSA-based spaces into one coherent word space.
Addi-tionally we are trying to simulate cognitive pro-cesses such as metaphor comprehension (Utsumi,2006).AcknowledgmentThis research was supported in part by Grant-in-Aid for Scientific Research(C) (No.17500171)from Japan Society for the Promotion of Science.ReferencesJean Aitchison.
2003.
Words in the Mind: An Intro-duction to the Mental Lexicon, 3rd Edition.
Oxford,Basil Blackwell.Shigeaki Amano and Kimihisa Kondo, editors.
2003.Nihongo-no Goitokusei CD-ROM (Lexical proper-ties of Japanese).
Sanseido, Tokyo.Albert-La?szlo?
Baraba?si and Re?ka Albert.
1999.
Emer-gence of scaling in random networks.
Science,286:509?512.Curt Burgess.
1998.
From simple associations to thebuilding blocks of language: Modeling meaning inmemory with the HAL model.
Behavior ResearchMethods, Instruments, & Computers, 30(2):188?198.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas L. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Journalof the American Society For Information Science,41(6):391?407.Peter G a?rdenfors.
2000.
Conceptual Spaces: The Ge-ometry of Thought.
MIT Press.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,Yoshifumi Ooyama, and Yoshihiko Hayashi.
1999.Goi-Taikei: A Japanese Lexicon CDROM.
IwanamiShoten, Tokyo.Kaname Kasahara, Kazumitsu Matsuzawa, and Tsu-tomu Ishikawa.
1997.
A method for judgment of se-mantic similarity between daily-used words by usingmachine readable dictionaries.
Transactions of In-formation Processing Society of Japan, 38(7):1272?1283.
in Japanese.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to Plato?s problem: The latent seman-tic analysis theory of the acquisition, induction, andrepresentation of knowledge.
Psychological Review,104:211?240.Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.Schreiber.
1998.
The university of south floridaword association, rhyme, and word fragment norms.http://www.usf.edu/FreeAssociation/.Yoshiki Niwa and Yoshihiko Nitta.
1994.
Co-occurrence vectors from corpora vs. distance vectorsfrom dictionaries.
In Proceedings of the 15th Inter-national Conference on Computational Linguistics(COLING94), pages 304?309.Robert M. Nosofsky.
1992.
Similarity scaling and cog-nitive process models.
Annual Review of Psychol-ogy, 43:25?53.Hinrich Sch u?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Mark Steyvers and Joshua B. Tenenbaum.
2005.
Thelarge-scale structure of semantic network: Statisticalanalyses and a model of semantic growth.
CognitiveScience, 29(1):41?78.Mark Steyvers, Richard M. Shiffrin, and Douglas L.Nelson.
2004.
Word association spaces for predict-ing semantic similarity effects in episodic memory.In Alice F. Healy, editor, Experimental CognitivePsychology and Its Applications.
American Psycho-logical Association, 2004.Takao Umemoto.
1969.
Renso Kijunhyo (Free Associ-ation Norm).
Tokyo Daigaku Shuppankai, Tokyo.Akira Utsumi, Koichi Hori, and Setsuo Ohsuga.
1998.An affective-similarity-based method for compre-hending attributional metaphors.
Journal of NaturalLanguage Processing, 5(3):3?32.Akira Utsumi.
2006.
Computational exploration ofmetaphor comprehension processes.
In Proceedingsof the 28th Annual Meeting of the Cognitive ScienceSociety (CogSci 2006).Dominic Widdows.
2003.
Orthogonal negation in vec-tor spaces for modelling word-meanings and docu-ment retrieval.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Lin-guistics, pages 136?143.Dominic Widdows.
2004.
Geometry and Meaning.CSLI Publications.865
