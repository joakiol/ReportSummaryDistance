Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 78?86,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUnsupervised techniques for discovering ontologyelements from Wikipedia article linksZareen Syed Tim FininUniversity of Maryland, Baltimore County University of Maryland, Baltimore County1000 Hilltop Circle 1000 Hilltop CircleBaltimore, MD 21250, USA Baltimore, MD 21250, USAzarsyed1@umbc.edu finin@umbc.eduAbstractWe present an unsupervised and unrestrictedapproach to discovering an infobox like on-tology by exploiting the inter-article linkswithin Wikipedia.
It discovers new slots andfillers that may not be available in theWikipedia infoboxes.
Our results demonstratethat there are certain types of properties thatare evident in the link structure of resourceslike Wikipedia that can be predicted with highaccuracy using little or no linguistic analysis.The discovered properties can be further usedto discover a class hierarchy.
Our experimentshave focused on analyzing people in Wikipe-dia, but the techniques can be directly appliedto other types of entities in text resources thatare rich with hyperlinks.1 IntroductionOne of the biggest challenges faced by the Seman-tic Web vision is the availability of structured datathat can be published as RDF.
One approach is todevelop techniques to translate information inspreadsheets, databases, XML documents andother traditional data formats into RDF (Syed et al2010).
Another is to refine the technology neededto extract structured information from unstructuredfree text (McNamee and Dang, 2009).For both approaches, there is a second problemthat must be addressed: do we start with an ontol-ogy or small catalog of ontologies that will be usedto encode the data or is extracting the right ontol-ogy part of the problem.
We describe exploratorywork on a system that can discover ontologicalelements as well as data from a free text with em-bedded hyperlinks.Wikipedia is a remarkable and rich online en-cyclopedia with a wealth of general knowledgeabout varied concepts, entities, events and facts inthe world.
Its size and coverage make it a valuableresource for extracting information about differententities and concepts.
Wikipedia contains both freetext and structured information related to conceptsin the form of infoboxes, category hierarchy andinter-article links.
Infoboxes are the most struc-tured form and are composed of a set of subject-attribute-value triples that summarize or highlightthe key features of the concept or subject of thearticle.
Resources like DBpedia (Auer et al, 2007)and Freebase (Bollacker et al, 2007) have har-vested this structured data and have made it avail-able as triples for semantic querying.While infoboxes are a readily available sourceof structured data, the free text of the article con-tains much more information about the entity.Barker et al (2007) unified the state of the art ap-proaches in natural language processing andknowledge representation in their prototype systemfor understanding free text.
Text resources whichare rich in hyperlinks especially to knowledgebased resources (such as encyclopedias or diction-aries) have additional information encoded in theform of links, which can be used to complementthe existing systems for text understanding andknowledge discovery.
Furthermore, systems suchas Wikify (Mihalcea and Csomai, 2007) can beemployed to link words in free text to knowledgeresources like Wikipedia and thus enrich the freetext with hyperlinks.We describe an approach for unsupervised on-tology discovery from links in the free text of theWikipedia articles, without specifying a relation orset of relations in advance.
We first identify candi-date slots and fillers for an entity, then classify en-78tities and finally derive a class hierarchy.
We haveevaluated our approach for the Person class, but itcan be easily generalized to other entity types suchas organizations, places, and products.The techniques we describe are not suggestedas alternatives to natural language understanding orinformation extraction, but as a source for addi-tional evidence that can be used to extract onto-logical elements and relations from the kind of textfound in Wikipedia and other heavily-linked textcollections.
This approach might be particularlyuseful in ?slot fillings?
tasks like the one in theKnowledge Base Population track (McNamee andDang, 2010) at the 2009 Text Analysis Confer-ence.
We see several contributions that this workhas to offer:?
Unsupervised and unrestricted ontology discov-ery.
We describe an automatic approach thatdoes not require a predefined list of relations ortraining data.
The analysis uses inter-articlelinks in the text and does not depend on existinginfoboxes, enabling it to suggest slots and fillersthat do not exist in any extant infoboxes.?
Meaningful slot labels.
We use WordNet (Mil-ler et al, 1990) nodes to represent and labelslots enabling us to exploit WordNet?s hy-pernym and hyponym relations as a property hi-erarchy.?
Entity classification and class labeling.
We in-troduce a new feature set for entity classifica-tion, i.e.
the discovered ranked slots, which per-forms better than other feature sets extractedfrom Wikipedia.
We also present an approachfor assigning meaningful class label vectors us-ing WordNet nodes.?
Deriving a class hierarchy.
We have developedan approach for deriving a class hierarchy basedon the ranked slot similarity between classesand the label vectors.In the remainder of the paper we describe the de-tails of the approach, mention closely related work,present and discuss preliminary results and providesome conclusions and possible next steps.2 ApproachFigure 1 shows our ontology discovery frameworkand its major steps.
We describe each step in therest of this section.2.1 Discovering Candidate Slots and FillersMost Wikipedia articles represent a concept, i.e., ageneric class of objects (e.g., Musician), an indi-vidual object (e.g., Michael_Jackson), or a genericrelation or property (e.g., age).
Inter-article linkswithin Wikipedia represent relations between con-cepts.
In our approach we consider the linked con-cepts as candidate fillers for slots related to theprimary article/concept.
There are several caseswhere the filler is subsumed by the slot label forexample, the infobox present in the article on ?Mi-chael_Jackson?
(Figure 2) mentions pop, rock andsoul as fillers for the slot Genre and all three ofthese are a type of Genre.
The Labels slot containsfillers such as Motown, Epic and Legacy which areall Record Label Companies.
Based on this obser-vation, we discover and exploit ?isa?
relations be-tween fillers (linked concepts) and WordNet nodesto serve as candidate slot labels.In order to find an ?isa?
relation between a con-cept and a WordNet synset we use manually cre-ated mappings by DBpedia, which links about467,000 articles to synsets.
However, Wikipediahas more than two million articles1, therefore, tomap any remaining concepts we use the automati-cally generated mappings available betweenWordNet synsets and Wikipedia categories(Ponzetto and Navigli, 2009).
A single Wikipediaarticle might have multiple categories associatedwith it and therefore multiple WordNet synsets.Wikipedia?s category system serves more as a wayto tag articles and facilitate navigation rather than1 This estimate is for the English version and does notinclude redirects and administrative pages such as dis-ambiguation pages.Figure 1: The ontology discovery framework com-prises a number of steps, including candidate slot andfiller discovery followed by slot ranking, slot selec-tion, entity classification, slot re-ranking, class label-ing, and class hierarchy discovery.79to categorize them.
The article on Michael Jordan,for example, has 36 categories associated with it.In order to select an individual WordNet synset asa label for the concept?s type, we use two heuris-tics:?
Category label extraction.
Since the first sen-tence in Wikipedia articles usually defines theconcept, we extract a category label from thefirst sentence using patterns based on POS tagssimilar to Kazama and Torisawa (2007).?
Assign matching WordNet synset.
We con-sider all the WordNet synsets associated withthe categories of the article using the categoryto WordNet mapping (Ponzetto and Navigli,2009) and assign the WordNet synset if any ofthe words in the synset matches with the ex-tracted category label.
We repeat the processwith hypernyms and hyponyms of the synsetup to three levels.2.2 Slot RankingAll slots discovered using outgoing links might notbe meaningful, therefore we have developed tech-niques for ranking and selecting slots.
Our ap-proach is based on the observation that entities ofthe same type have common slots.
For example,there is a set of slots common for musical artistswhereas, a different set is common for basketballplayers.
The Wikipedia infobox templates basedon classes also provide a set of properties or slotsto use for particular types of entities or concepts.In case of people, it is common to note thatthere is a set of slots that are generalized, i.e., theyare common across all types of persons.
Examplesare name, born, and spouse.
There are also sets ofspecialized slots, which are generally associatedwith a given profession.
For example, the slots forbasketball players have information for basketballrelated activities and musical artists have slots withmusic related activities.
The slots for ?Mi-chael_Jordan?
include Professional Team(s), NBADraft, Position(s) and slots for ?Michael_Jackson?include Genres, Instruments and Labels.Another observation is that people engaged in aparticular profession tend to be linked to otherswithin the same profession.
Hence the maxim ?Aman is known by the company he keeps.?
For ex-ample, basketball players are linked to other bas-ketball players and politicians are linked to otherpoliticians.
We rank the slots based on the numberof linked persons having the same slots.
We gener-ated a list of person articles in Wikipedia by get-ting all Wikipedia articles under the Person type inFreebase2.
We randomly select up to 25 linked per-sons (which also link back) and extract their candi-date slots and vote for a slot based on the numberof times it appears as a slot in a linked person nor-malized by the number of linked persons to assigna slot score.2.3 Entity Classification and Slot Re-RankingThe ranked candidate slots are used to classify en-tities and then further ranked based on number oftimes they appear among the entities in the cluster.We use complete link clustering using a simple slotsimilarity function:This similarity metric for slots is computed as thecosine similarity between tf.idf weighted slot vec-tors, where the slot score represents the term fre-2 We found that the Freebase classification for Personwas more extensive that DBpedia?s in the datasets avail-able to us in early 2009.Figure 2.
The Wikipedia infoboxfor the Michael_Jackson article hasa number of slots from appropriateinfobox templates.80quency component and the inverse document fre-quency is based on the number of times the slotappears in different individuals.We also collapsed location expressing slots(country, county, state, district, island etc.)
into theslot labeled location by generating a list of locationwords from WordNet as these slots were causingthe persons related to same type of geographicallocation to cluster together.After clustering, we re-score the slots based onnumber of times they appear among the individualsin the cluster normalized by the cluster size.
Theoutput of clustering is a vector of scored slots as-sociated with each cluster.2.4 Slot SelectionThe slot selection process identifies and filters outslots judged to be irrelevant.
Our intuition is thatspecialized slots or attributes for a particular entitytype should be somehow related to each other.
Forexample, we would expect attributes like league,season and team for basketball players and genre,label, song and album for musical artists.
If an at-tribute like album appears for basketball players itshould be discarded as it is not related to other at-tributes.We adopted a clustering approach for findingattributes that are related to each other.
For eachpair of attributes in the slot vector, we compute asimilarity score based on how many times the twoattribute labels appear together in Wikipedia per-son articles within a distance of 100 words ascompared to the number of times they appear intotal and weigh it using weights of the individualattributes in the slot vector.
This metric is capturedin the following equation, where Df is the docu-ment frequency and wt is the attribute weight.Our initial experiments using single and com-plete link clustering revealed that single link wasmore appropriate for slot selection.
We got clustersat a partition distance of 0.9 and selected the larg-est cluster from the set of clusters.
In addition, wealso added any attributes exceeding a 0.4 score intothe set of selected attributes.
Selected ranked slotsfor Michael Jackson are given in Table 1.2.5 Class LabelingAssigning class labels to clusters gives additionalinformation about the type of entities in a cluster.We generate a cluster label vector for each clusterwhich represents the type of entities in the cluster.We compute a list of person types by taking allhyponyms under the corresponding person sense inWordNet.
That list mostly contained the profes-sions list for persons such as basketball player,president, bishop etc.
To assign a WordNet type toa person in Wikipedia we matched the entries inthe list to the words in the first sentence of the per-son article and assigned it the set of types thatmatched.
For example, for Michael Jordan thematching types found were basketball_player,businessman and player.We assigned the most frequent sense to thematching word as followed by Suchanek et al(2008) and Wu and Weld (2008), which works formajority of the cases.
We then also add all the hy-pernyms of the matching types under the Personnode.
The vector for Michael Jordan has entriesbasketball_player, athlete, businessperson, person,contestant, businessman and player.
After gettingmatching types and their hypernyms for all themembers of the cluster, we score each type basedon the number of times it occurs in its membersnormalized by the cluster size.
For example for oneof the clusters with 146 basketball players we gotthe following label vector: {player:0.97, contest-ant:0.97, athlete:0.96, basketball_player:0.96}.
Toselect an individual label for a class we can pickthe label with the highest score (the most general-Slot Score Fillers ExampleMusician 1.00 ray_charles, sam_cooke ...Album 0.99 bad_(album), ...Location 0.97 gary,_indiana,  chicago,  ?Music_genre 0.90 pop_music, soul_music, ...Label 0.79 a&m_records, epic_records, ...Phonograph_record0.67give_in_to_me,this_place_hotel ?Act 0.59 singingMovie 0.46 moonwalker ?Company 0.43 war_child_(charity), ?Actor 0.41 stan_winston, eddie_murphy,Singer 0.40 britney_spears, ?Magazine 0.29 entertainment_weekly,?Writing_style 0.27 hip_hop_musicGroup 0.21 'n_sync, RIAASong 0.20 d.s._(song) ?Table 1: Fifteen slots were discovered for musicianMichael Jackson along with scores and example fillers.81ized label) or the most specialized label having ascore above a given threshold.2.6 Discovering Class HierarchyWe employ two different feature sets to discoverthe class hierarchy, i.e., the selected slot vectorsand the class label vectors and combine both func-tions using their weighted sum.
The similarityfunctions are described below.The common slot similarity function is the co-sine similarity between the common slot tf.idf vec-tors, where the slot score represents the tf and theidf is based on the number of times a particular slotappears in different clusters at that iteration.
Were-compute the idf term in each iteration.
We de-fine the common slot tf.idf vector for a cluster asone where we assign a non-zero weight to only theslots that have non-zero weight for all clustermembers.
The label similarity function is the co-sine similarity between the label vectors for clus-ters.
The hybrid similarity function is a weightedsum of the common slot and label similarity func-tions.
Using these similarity functions we applycomplete link hierarchical clustering algorithm todiscover the class hierarchy.3 Experiments and EvaluationFor our experiments and evaluation we used theWikipedia dump from March 2008 and the DBpe-dia infobox ontology created from Wikipediainfoboxes using hand-generated mappings (Auer etal., 2007).
The Person class is a direct subclass ofthe owl:Thing class and has 21 immediate sub-classes and 36 subclasses at the second level.
Weused the persons in different classes in DBpediaontology at level two to generate data sets for ex-periments.There are several articles in Wikipedia that arevery small and have very few out-links and in-links.
Our approach is based on the out-links andavailability of information about different relatedthings on the article, therefore, in order to avoiddata sparseness, we randomly select articles withgreater than 100 in-links and out-links, at least5KB page length and having at least five links toentities of the same type that link back (in our casepersons).We first compare our slot vector features withother features extracted from Wikipedia for entityclassification task and then evaluate their accuracy.We then discover the class hierarchy and comparethe different similarity functions.3.1 Entity ClassificationWe did some initial experiments to compare ourranked slot features with other feature sets ex-tracted from Wikipedia.
We created a dataset com-posed of 25 different classes of Persons present atlevel 2 in the DBpedia ontology by randomly se-lecting 200 person articles from each class.
Forseveral classes we got less than 200 articles whichfulfilled our selection criteria defined earlier.
Wegenerated twelve types of feature sets and evalu-ated them using ground truth from DBpedia ontol-ogy.We compare tf.idf vectors constructed usingtwelve different feature sets: (1) Ranked slot fea-tures, where tf is the slot score; (2) Words in firstsentence of an article; (3) Associated categories;(4) Assigned WordNet nodes (see section 2.2); (5)Associated categories tokenized into words; (6)Combined Feature Sets 1 to 5 (All); (7-11) Featuresets 7 to 11 are combinations excluding one featureset at a time; (12) Unranked slots where tf is 1 forall slots.
We applied complete link clustering andevaluated the precision, recall and F-measure atdifferent numbers of clusters ranging from one to100.
Table 2 gives the precision, recall and num-ber of clusters where we got the maximum F-measure using different feature sets.82Feature set 10 (all features except feature 2) gavethe best F-measure i.e.
0.74, whereas, feature set 1(ranked slots only) gave the second best F-measurei.e.
0.73 which is very close to the best result.
Fea-ture set 12 (unranked slots) gave a lower F-measure i.e.
0.61 which shows that ranking orweighing slots based on linked entities of the sametype performs better for classification.3.2 Slot and Filler EvaluationTo evaluate our approach to finding slot fillers, wefocused on DBpedia classes two levels below Per-son (e.g., Governor and FigureSkater).
We ran-domly selected 200 articles from each of theseclasses using the criteria defined earlier to avoiddata sparseness.
Classes for which fewer than 20articles were found were discarded.
The resultingdataset comprised 28 classes and 3810 articles3.We used our ranked slots tf.idf feature set andran a complete link clustering algorithm producingclusters at partition distance of 0.8.
The slots werere-scored based on the number of times they ap-peared in the cluster members normalized by thecluster size.
We applied slot selection over the re-scored slots for each cluster.
In order to evaluateour slots and fillers we mapped each cluster to aDBpedia class based on the maximum number ofmembers of a particular DBpedia class in our clus-ter.
This process predicted 124 unique propertiesfor the classes.
Of these, we were able to manuallyalign 46 to properties in either DBpedia or Free-3 For some of the classes, fewer than the full comple-ment of 200 articles were found.base for the corresponding class.
We initially triedto evaluate the discovered slots by comparing themwith those found in the ontologies underlyingDBpedia and Freebase, but were able to find anoverlap in the subject and object pairs for very fewproperties.We randomly selected 20 subject object pairsfor each of the 46 properties from the correspond-ing classes and manually judged whether or not therelation was correct by consulting the correspond-No.
Property Accuracy1 automobile_race 1.002 championship 1.003 expressive_style 1.004 fictional_character 1.005 label 1.006 racetrack 1.007 team_sport 1.008 writing_style 1.009 academic_degree 0.9510 album 0.9511 book 0.9512 contest 0.9513 election 0.9514 league 0.9515 phonograph_record 0.9516 race 0.9517 tournament 0.9418 award 0.9019 movie 0.9020 novel 0.9021 school 0.9022 season 0.9023 serial 0.9024 song 0.9025 car 0.8526 church 0.8527 game 0.8528 musical_instrument 0.8529 show 0.8530 sport 0.8531 stadium 0.8532 broadcast 0.8033 telecast 0.8034 hockey_league 0.7535 music_genre 0.7036 trophy 0.7037 university 0.6538 character 0.6039 disease 0.6040 magazine 0.5541 team 0.5042 baseball_club 0.4543 club 0.4544 party 0.4545 captain 0.3046 coach 0.25Avg.
Accuracy: 0.81Table 3: Manual evaluation of discovered propertiesNo.
Feature Set k P R F1 Ranked Slots  40 0.74 0.72 0.732 First Sentence 89 0.07 0.53 0.123 Categories 1 0.05 1.00 0.104 WordNet Nodes 87 0.40 0.22 0.295 (3 tokenized) 93 0.85 0.47 0.606 All (1 to 5) 68 0.87 0.62 0.727 (All ?
5) 82 0.79 0.46 0.588 (All ?
4) 58 0.78 0.63 0.709 (All ?
3) 53 0.76 0.65 0.7010 (All ?
2) 58 0.88 0.63 0.7411 (All ?
1) 57 0.77 0.60 0.6812 (1 unranked) 34 0.57 0.65 0.61Table 2: Comparison of the precision, recall and F-measure for different feature sets for entity classifi-cation.
The k column shows the number of clustersthat maximized the F score.83ing Wikipedia articles (Table 3).
The average ac-curacy for the 46 relations was 81%.3.3 Discovering Class HierarchyIn order to discover the class hierarchy, we took allof the clusters obtained earlier at partition distanceof 0.8 and their corresponding slot vectors afterslot selection.
We experimented with differentsimilarity functions and evaluated their accuracyby comparing the results with the DBpedia ontol-ogy.
A complete link clustering algorithm was ap-plied using different settings of the similarity func-tions and the resulting hierarchy compared toDBpedia?s Person class hierarchy.
Table 4 showsthe highest F measure obtained for Person?s imme-diate sub-classes (L1), ?sub-sub-classes?
(L2) andthe number of clusters (k) for which we got thehighest F-measure using a particular similarityfunction.The highest F-measure both at level 2 (0.63) andlevel 1 (0.79) was obtained by simhyb with wc=0.2,wl=0.8 and also at lowest number of clusters at L1(k=8).
The simhyb (wc=wl=0.5) and simlabel functionsgave almost the same F-measure at both levels.The simcom_slot function gave better performance atL1 (F=0.65) than the base line simslot (F=0.55)which was originally used for entity clustering.However, both these functions gave the same F-measure at L2 (F=0.61).4 DiscussionIn case of property evaluation, properties for whichthe accuracy was 60% or below include coach,captain, baseball_club, club, party, team andmagazine.
For the magazine property (correspond-ing to Writer and ComicsCreator class) we ob-served that many times a magazine name was men-tioned in an article because it published some newsabout a person rather than that person contributingany article in that magazine.
For all the remainingproperties we observed that these were related tosome sort of competition.
For example, a personplayed against a team, club, coach or captain.
Thepolitical party relation is a similar case, where arti-cles frequently mention a politician?s party affilia-tion as well as significant opposition parties.
Forsuch properties, we need to exploit additional con-textual information to judge whether the personcompeted ?for?
or ?against?
a particular team,club, coach or party.
Even if the accuracy for fill-ers for such slots is low, it can still be useful todiscover the kind of slots associated with an entity.We also observed that there were some caseswhere the property was related to a family memberof the primary person such as for disease, schooland university.
Certain other properties such asspouse, predecessor, successor, etc.
require morecontextual information and are not directly evidentin the link structure.
However, our experimentsshow that there are certain properties that can bepredicted with high accuracy using the article linksonly and can be used to enrich the existing infoboxontology or for other purposes.While our work has mostly experimented withperson entities, the approach can be applied to oth-er types as well.
For example, we were able to dis-cover software as a candidate slot for companieslike Microsoft, Google and Yahoo!, which ap-peared among the top three ranked slots using ourslot ranking scheme and corresponds to the prod-ucts slot in the infoboxes of these companies.For class hierarchy discovery, we have ex-ploited the specialized slots after slot selection.One way to incorporate generalized slots in thehierarchy is to consider all slots for class members(without slot selection) and recursively propagatethe common slots present at any level to the levelabove it.
For example, if we find the slot team tobe common for different types of Athletes such asbasketball players, soccer players etc.
we canpropagate it to the Athlete class, which is one levelhigher in the hierarchy.5 Related WorkUnsupervised relation discovery was initially in-troduced by Hasegawa et al (2004).
They devel-oped an approach to discover relations by cluster-ing pairs of entities based on intervening wordsrepresented as context vectors.
Shinyama and Se-kine (2006) generated basic patterns using parts oftext syntactically connected to the entity and thenSimilarity Function k (L=2)F(L=2)k(L=1)F(L=1)simslot  56 0.61 13 0.55simcom_slot  74 0.61 15 0.65simlabel  50 0.63 10 0.76simhyb wc=wl=0.5 59 0.63 10 0.76simhyb wc=0.2, wl=0.8 61 0.63 8 0.79Table 4: Evaluation results for class hierarchy predic-tion using different similarity functions.84generated a basic cluster composed of a set ofevents having the same relation.Several approaches have used linguistic analysisto generate features for supervised or un-supervised relation extraction (Nguyen et al, 2007;Etzioni et al, 2008; Yan et al, 2009).
Our ap-proach mainly exploits the heavily linked structureof Wikipedia and demonstrates that there are sev-eral relations that can be discovered with high ac-curacy without the need of features generated froma linguistic analysis of the Wikipedia article text.Suchanek et al (2008) used Wikipedia catego-ries and infoboxes to extract 92 relations by apply-ing specialized heuristics for each relation and in-corporated the relations in their YAGO ontology,whereas our techniques do not use specialized heu-ristics based on the type of relation.
Kylin (Weldet al, 2008) generated infoboxes for articles bylearning from existing infoboxes, whereas we candiscover new fillers for several existing slots andalso discover new slots for infoboxes.
KOG (Wuand Weld, 2008) automatically refined the Wiki-pedia infobox ontology and integrated Wikipedia?sinfobox-class schemata with WordNet.
Since wealready use the WordNet nodes for representingslots, it eliminates the need for several of KOG?sinfobox refinement steps.While YAGO, Kylin and KOG all rely on rela-tions present in the infoboxes, our approach cancomplement these by discovering new relationsevident in inter-article links in Wikipedia.
For ex-ample, we could add slots like songs and albums tothe infobox schema for Musical Artists, movies forthe Actors infobox schema, and party for the Poli-ticians schema.6 Conclusions and Future WorkPeople have been learning by reading for thou-sands of years.
The past decade, however, hasseen a significant change in the way people read.The developed world now does much of its readingonline and this change will soon be nearly univer-sal.
Most online content is read as hypertext via aWeb browser or custom reading device.
Unliketext, hypertext is semi-structured information, es-pecially when links are drawn from global name-space, making it easy for many documents to linkunambiguously to a common referent.The structured component of hypertext aug-ments the information in its plain text and providesan additional source of information from whichboth people and machines can learn.
The workdescribed in this paper is aimed at learning usefulinformation, both about the implicit ontology andfacts, from the links embedded in collection of hy-pertext documents.Our approach is fully unsupervised and doesnot require having a pre-defined catalogue of rela-tions.
We have discovered several new slots andfillers that are not present in existing Wikipediainfoboxes and also a scheme to rank the slots basedon linked entities of the same type.
We comparedour results with ground truth from the DBpediainfobox ontology and Freebase for the set of prop-erties that were common and manually evaluatedthe accuracy of the common properties.
Our resultsshow that there are several properties that can bediscovered with high accuracy from the link struc-ture in Wikipedia and can also be used to discovera class hierarchy.We plan to explore the discovery of slots fromnon-Wikipedia articles by linking them to Wikipe-dia concepts using existing systems like Wikify(Mihalcea and Csomai, 2007).
Wikipedia articlesare encyclopedic in nature with the whole articlerevolving around a single topic or concept.
Con-sequently, linked articles are a good source ofproperties and relations.
This might not be the casein other genres, such as news articles, that discussa number of different entities and events.
One wayto extend this work to other genres is by first de-tecting the entities in the article and then onlyprocessing links in sentences that mention an entityto discover its properties.AcknowledgementsThe research described in this paper was supportedin part by a Fulbright fellowship, a gift from Mi-crosoft Research, NSF award IIS-0326460 and theJohns Hopkins University Human Language Tech-nology Center of Excellence.85ReferencesS?ren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann and Zachary Ives.
2007.
DBpedia: A nu-cleus for a web of open data.
In Proceedings of the6th International Semantic Web Conference: 11?15.Ken Barker et al 2007.
Learning by reading: A proto-type system, performance baseline and lessonslearned, Proceedings of the 22nd National Confer-ence on Artificial Intelligence, AAAI Press.K.
Bollacker, R. Cook, and P. Tufts.
2007.
Freebase: AShared Database of Structured General HumanKnowledge.
Proceedings of the National Conferenceon Artificial Intelligence (Volume 2): 1962-1963.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S. Weld.
2008.
Open information extractionfrom the web.
Communications of the ACM 51, 12(December): 68-74.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-man.
2004.
Discovering relations among named enti-ties from large corpora.
In Proceedings of the 42ndAnnual Meeting of the Association for Computa-tional Linguistics: 415-422.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Exploit-ing Wikipedia as external knowledge for named en-tity recognition.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning: 698?707.Paul McNamee and Hoa Trang Dang.
2009.
Overviewof the TAC 2009 knowledge base population track.In Proceedings of the 2009 Text Analysis Confer-ence.
National Institute of Standards and Technol-ogy, November.Rada Mihalcea and Andras Csomai.
2007.
Wikify!
:linking documents to encyclopedic knowledge.
InProceedings of the 16th ACM Conference onInformation and Knowledge Management: 233?242.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1990.WordNet: An on-line lexical database.
InternationalJournal of Lexicography, 3:235?244.Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru Ishizu-ka.
2007.
Subtree mining for relation extraction fromWikipedia.
In Proceedings of Human LanguageTechnologies: The Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics:125?128.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
Yago: A large ontology fromWikipedia and WordNet.
Web Semantics, 6(3):203?217.Zareen Syed, Tim Finin, Varish Mulwad and AnupamJoshi.
2010.
Exploiting a Web of Semantic Data forInterpreting Tables, Proceedings of the Second WebScience Conference.Simone P. Ponzetto and Roberto Navigli.
2009.
Large-scale taxonomy mapping for restructuring and inte-grating Wikipedia.
In Proceedings of the Twenty-First International Joint Conference on Artificial In-telligence: 2083?2088.Yusuke Shinyama and Satoshi Sekine.
2006.
Pre-emp-tive information extraction using unrestricted relationdiscovery.
In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics:.Daniel S. Weld, Raphael Hoffmann, and Fei Wu.
2008.Using Wikipedia to bootstrap open information ex-trac-tion.
SIGMOD Record, 37(4): 62?68.Fei Wu and Daniel S. Weld.
2008.
Automatically refin-ing the Wikipedia infobox ontology.
In Proceedingsof the 17th International World Wide Web Confer-ence, pages 635?644.Wikipedia.
2008.
Wikipedia, the free encyclopedia.Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, ZhengluYang, and Mitsuru Ishizuka.
2009.
Unsupervised re-lation extraction by mining Wikipedia texts using in-formation from the web.
In Proceedings of the 47thAnnual Meeting of the Association for Computa-tional Linguistics: Volume 2: 1021?1029.86
