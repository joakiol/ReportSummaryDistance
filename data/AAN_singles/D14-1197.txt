Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1840?1845,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsImproving Word Alignment using Word SimilarityTheerawat SongyotDept of Computer ScienceUniversity of Southern Californiasongyot@usc.eduDavid Chiang?Dept of Computer Science and EngineeringUniversity of Notre Damedchiang@nd.eduAbstractWe show that semantic relationships canbe used to improve word alignment, in ad-dition to the lexical and syntactic featuresthat are typically used.
In this paper, wepresent a method based on a neural net-work to automatically derive word simi-larity from monolingual data.
We presentan extension to word alignment modelsthat exploits word similarity.
Our exper-iments, in both large-scale and resource-limited settings, show improvements inword alignment tasks as well as translationtasks.1 IntroductionWord alignment is an essential step for learn-ing translation rules in statistical machine trans-lation.
The task is to find word-level transla-tion correspondences in parallel text.
Formally,given a source sentence e consisting of wordse1, e2, .
.
.
, eland a target sentence f consistingof words f1, f2, .
.
.
, fm, we want to infer analignment a, a sequence of indices a1, a2, .
.
.
, amwhich indicates, for each target word fi, the corre-sponding source word eaior a null word.
Machinetranslation systems, including state-of-the-art sys-tems, then use the word-aligned corpus to extracttranslation rules.The most widely used methods, the IBM mod-els (Brown et al., 1993) and HMM (Vogel et al.,1996), define a probability distribution p(f ,a | e)that models how each target word fiis gener-ated from a source word eaiwith respect to analignment a.
The models, however, tend to mis-align low-frequency words as they have insuffi-cient training samples.
The problem can get worsein low-resource languages.
Two branches of re-search have tried to alleviate the problem.
The?Most of the work reported here was performed while thesecond author was at the University of Southern California.first branch relies solely on the parallel data; how-ever, additional assumptions about the data are re-quired.
This includes, but is not limited to, ap-plying prior distributions (Mermer and Sarac?lar,2011; Vaswani et al., 2012) or smoothing tech-niques (Zhang and Chiang, 2014).
The otherbranch uses information learned from monolin-gual data, which is generally easier to acquire thanparallel data.
Previous work in this branch mostlyinvolves applying syntactic constraints (Yamadaand Knight, 2001; Cherry and Lin, 2006; Wangand Zong, 2013) and syntactic features (Toutanovaet al., 2002) into the models.
The use of syntac-tic relationships can, however, be limited betweenhistorically unrelated language pairs.Our motivation lies in the fact that a meaningfulsentence is not merely a grammatically structuredsentence; its semantics can provide insightful in-formation for the task.
For example, suppose thatthe models are uncertain about aligning e to f .
Ifthe models are informed that e is semantically re-lated to e?, f is semantically related to f?, and f?isa translation of e?, it should intuitively increase theprobability that f is a translation of e. Our workfocuses on using such a semantic relationship, inparticular, word similarity, to improve word align-ments.In this paper, we propose a method to learn sim-ilar words from monolingual data (Section 2) andan extension to word alignment models in whichword similarity can be incorporated (Section 3).We demonstrate its application in word alignmentand translation (Section 4) and then briefly discussthe novelty of our work in comparison to othermethods (Section 5).2 Learning word similarityGiven a word w, we want to learn a word simi-larity model p(w?| w) of what words w?mightbe used in place of w. Word similarity can beused to improve word alignment, as in this pa-1840per, but can potentially be useful for other nat-ural language processing tasks as well.
Such amodel might be obtained from a monolingual the-saurus, in which humans manually provide sub-jective evaluation for word similarity probabilities,but an automatic method would be preferable.
Inthis section, we present a direct formulation of theword similarity model, which can automatically betrained from monolingual data, and then considera more practical variant, which we adopt in ourexperiments.2.1 ModelGiven an arbitrary word type w, we define a wordsimilarity model p(w?| w) for all word types w?in the vocabulary V asp(w?| w) =?cp(c | w) p(w?| c)where c is a word context represented by a se-quence w1, w2, .
.
.
, w2nconsisting of n word to-kens on the left and n word tokens on the rightof w, excluding w. The submodel p(c | w) canbe a categorical distribution.
However, modelingthe word context model, p(w?| c), as a categori-cal distribution would cause severe overfitting, be-cause the number of all possible contexts is |V |2n,which is exponential in the length of the context.We therefore parameterize it using a feedforwardneural network as shown in Figure 1, since thestructure has been shown to be effective for lan-guage modeling (Bengio et al., 2006; Vaswani etal., 2013).
The input to the network is a one-hotrepresentation of each word in c, where the spe-cial symbols <s>, </s>, <unk> are reserved forsentence beginning, sentence ending, and wordsnot in the vocabulary.
There is an output nodefor each w??
V , whose activation is p(w?| c).Following Bengio et al.
(2006), the network usesa shared linear projection matrix to the input em-bedding layer, which allows information sharingamong the context words and also substantiallyreduces the number of parameters.
The input em-bedding layer has a dimensionality of 150 for eachinput word.
The network uses two hidden layerswith 1,000 and 150 rectified linear units, respec-tively, and a softmax output layer.
We arbitrarilyuse n = 5 throughout this paper.2.2 TrainingWe extract training data by either collecting orsampling the target words w ?
V and their wordinputword.
.
.
.
.
.w1w2n.
.
.inputembeddings.
.
.
.
.
.hidden layer 1. .
.hidden layer 2. .
.outputlayer.
.
.Figure 1: The structure of the word context modelcontexts from monolingual data.
The submodelp(c | w) can be independently trained easily bymaximum likelihood estimation, while the wordcontext model p(w?| c) may be difficult to train atscale.
We follow previous work (Mnih and Teh,2012; Vaswani et al., 2013) in adopting noise-contrastive estimation (Gutmann and Hyv?arinen,2010), a fast and simple training algorithm thatscales independently of the vocabulary size.2.3 Model variantsThe above formulation of the word similaritymodel can be interpreted as a mixture model inwhich w?is similar to w if any of the context prob-abilities agrees.
However, to guard against falsepositives, we can alternatively reformulate it as aproduct of experts (Hinton, 1999),p(w?| w) =1Z(w)exp?cp(c | w) log p(w?| c)where Z(w) is a normalization constant.
Underthis model, w?is similar to w if all of the contextprobabilities agree.
Both methods produce reason-ably good word similarity; however, in practice,the latter performs better.Since most of the p(w?| w) will be closeto zero, for computational efficiency, we can se-lect the k most similar words and renormalizethe probabilities.
Table 1 shows some exampleslearned from the 402M-word Xinhua portion ofthe English Gigaword corpus (LDC2007T07), us-ing a vocabulary V of the 30,000 most frequentwords.
We set k = 5 for illustration purposes.3 Word alignment modelIn this section, we present our word alignmentmodels by extending the standard IBM models.1841p(w?| country) p(w?| region) p(w?| area)country 0.8363 region 0.8338 area 0.8551region 0.0558 area 0.0760 region 0.0524nation 0.0522 country 0.0524 zone 0.0338world 0.0282 province 0.0195 city 0.0326city 0.0273 city 0.0181 areas 0.0258Table 1: Examples of word similarityThe method can easily be applied to other relatedmodels, for example, the log-linear reparameteri-zation of Model 2 by Dyer et al.
(2013).
Basically,all the IBM models involve modeling lexical trans-lation probabilities p(f | e) which are parameter-ized as categorical distributions.
IBM Model 1, forinstance, is defined asp(f ,a | e) ?m?i=1p(fi| eai) =m?i=1t(fi| eai)where each t(f | e) denotes the model parametersdirectly corresponding to p(f | e).
Models 2?5and the HMM-based model introduce additionalcomponents in order to capture word ordering andword fertility.
However, they have p(f | e) incommon.3.1 ModelTo incorporate word similarity in word alignmentmodels, we redefine the lexical translation proba-bilities asp(f | e) =?e?,f?p(e?| e) t(f?| e?)
p(f | f?
)for all f, e, including words not in the vocabulary.While the factor p(e?| e) can be directly computedby the word similarity model, the factor p(f | f?
)can be problematic because it vanishes for f outof vocabulary.
One possible solution would be touse Bayes?
rulep(f | f?)
=p(f?| f) p(f)p(f?
)where p(f?| f) is computed by the word similar-ity model.
However, we find that this is prone tonumerical instability and other complications.
Inour experiments, we tried the simpler assumptionthat p(f | f?)
?
p(f?| f), with the rationale thatboth probabilities are measures of word similarity,which is intuitively a symmetric relation.
We alsocompared the performance of both methods.
Ta-ble 2 shows that this simple solution works as wellas the more exact method of using Bayes?
rule.
Wedescribe the experiment details in Section 4.Model F1BLEUTest 1 Test 2Chinese-EnglishBayes?
rule 75.7 30.0 27.0Symmetry assumption 75.3 29.9 27.0Arabic-EnglishBayes?
rule 70.4 37.9 36.7Symmetry assumption 69.5 38.2 36.8Table 2: Assuming that word similarity is sym-metric, i.e.
p(f | f?)
?
p(f?| f), works as wellas computing p(f | f?)
using Bayes?
rule.3.2 Re-estimating word similarityDepending on the quality of word similarity andthe distribution of words in the parallel data, ap-plying word similarity directly to the model couldlead to an undesirable effect where similar but notinterchangeable words rank in the top of the trans-lation probabilities.
On the other hand, if we setp(e?| e) = 1[e?= e]p(f?| f) = 1[f?= f ]where 1 denotes the indicator function, the modelreduces to the standard IBM models.
To get thebest of both worlds, we smooth the two modelstogether so that we rely more on word similarityfor rare words and less for frequent wordsp?
(w?| w) =count(w)1[w?= w] + ?p(w?| w)count(w) + ?This can be thought of as similar to Witten-Bellsmoothing, or adding ?
pseudocounts distributedaccording to our p(w?| w).
The hyperparame-ter ?
controls how much influence our word sim-ilarity model has.
We investigated the effect of ?by varying this hyperparameter in our word align-ment experiments whose details are described inSection 4.
Figure 2 shows that performance of themodel, as measured by F1 score, is rather insensi-tive to the choice of ?.
We used a value of 40 inour experiments.3.3 TrainingOur word alignment models can be trained in thesame way as the IBM models using the Expec-tation Maximization (EM) algorithm to maximizethe likelihood of the parallel data.
Our extensiononly introduces an additional time complexity onthe order of O(k2) on top of the base models,where k is the number of word types used to es-timate the full-vocabulary word similarity models.18420 10 20 30 40 50 60666870727476Value of ?F1(%)Chinese-EnglishArabic-EnglishFigure 2: Alignment F1 is fairly insensitive to ?over a large range of valuesThe larger the value of k is, the closer to the full-vocabulary models our estimations are.
In prac-tice, a small value of k seems to be effective sincep(w?| w) is negligibly small for most w?.4 Experiments4.1 Alignment experimentsWe conducted word alignment experimentson 2 language pairs: Chinese-English andArabic-English.
For Chinese-English, we used9.5M+12.3M words of parallel text from theNIST 2009 constrained task1and evaluatedon 39.6k+50.9k words of hand-aligned data(LDC2010E63, LDC2010E37).
For Arabic-English, we used 4.2M+5.4M words of paralleltext from the NIST 2009 constrained task2and evaluated on 10.7k+15.1k words of hand-aligned data (LDC2006E86).
To demonstrateperformance under resource-limited settings,we additionally experimented on only the firsteighth of the full data, specifically, 1.2M+1.6Mwords for Chinese-English and 1.0M+1.4Mwords for Arabic-English.
We trained wordsimilarity models on the Xinhua portions ofEnglish Gigaword (LDC2007T07), ChineseGigaword (LDC2007T38), and Arabic Gigaword(LDC2011T1), which are 402M, 323M, and125M words, respectively.
The vocabulary V wasthe 30,000 most frequent words from each corpus1Catalog numbers: LDC2003E07, LDC2003E14,LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34,LDC2006E85, LDC2006E86, LDC2006E92, andLDC2006E93.2Excluding: United Nations proceedings (LDC2004E13),ISI Automatically Extracted Parallel Text (LDC2007E08),and Ummah newswire text (LDC2004T18)1-10 11-20 21-30 31-40 41-50 51-6030405060708038.152.456.955.459.560.158.563.766.366.470.571.5Source word frequencyF1(%)BaselineOur modelFigure 3: F1 scores for words binned by fre-quency.
Our model gives the largest improvementsfor the lowest-frequency words.and the k = 10 most similar words were used.We modified GIZA++ (Och and Ney, 2003) toincorporate word similarity.
For all experiments,we used the default configuration of GIZA++: 5iterations each of IBM Model 1, 2, HMM, 3 and4.
We aligned the parallel texts in both forwardand backward directions and symmetrized themusing grow-diag-final-and (Koehn et al., 2005).We evaluated alignment quality using precision,recall, and F1.The results in Table 3 suggest that our modelingapproach produces better word alignments.
Wefound that our models not only learned smoothertranslation models for low frequency words butalso ranked the conditional probabilities more ac-curately with respect to the correct translations.To illustrate this, we categorized the alignmentlinks from the Chinese-English low-resource ex-periment into bins with respect to the Englishsource word frequency and individually evaluatedthem.
As shown in Figure 3, the gain for low fre-quency words is particularly large.4.2 Translation experimentsWe also ran end-to-end translation experiments.For both languages, we used subsets of the NIST2004 and 2006 test sets as development data.
Weused two different data sets as test data: differentsubsets of the NIST 2004 and 2006 test sets (calledTest 1) and the NIST 2008 test sets (called Test 2).We trained a 5-gram language model on the Xin-hua portion of English Gigaword (LDC2007T07).We used the Moses toolkit (Koehn et al., 2007) to1843Model Precision Recall F1BLEU METEORTest 1 Test 2 Test 1 Test 2Chinese-EnglishBaseline 65.2 76.9 70.6 29.4 26.7 29.7 28.5Our model 71.4 79.7 75.3 29.9 27.0 30.0 28.8Baseline (resource-limited) 56.1 68.1 61.5 23.6 20.3 26.0 24.4Our model (resource-limited) 66.5 74.4 70.2 24.7 21.6 26.8 25.6Arabic-EnglishBaseline 56.1 79.0 65.6 37.7 36.2 31.1 30.9Our model 60.0 82.4 69.5 38.2 36.8 31.6 31.4Baseline (resource-limited) 56.7 76.1 65.0 34.1 33.0 27.9 27.7Our model (resource-limited) 59.4 80.7 68.4 35.0 33.8 28.7 28.6Table 3: Experimental results.
Our model improves alignments and translations on both language pairs.build a hierarchical phrase-based translation sys-tem (Chiang, 2007) trained using MIRA (Chiang,2012).
Then, we evaluated the translation qual-ity using BLEU (Papineni et al., 2002) and ME-TEOR (Denkowski and Lavie, 2014), and per-formed significance testing using bootstrap resam-pling (Koehn, 2004) with 1,000 samples.Under the resource-limited settings, our meth-ods consistently show 1.1?1.3 BLEU (0.8?1.2METEOR) improvements on Chinese-English and0.8?0.9 BLEU (0.8?0.9 METEOR) improvementson Arabic-English, as shown in Table 3.
These im-provements are statistically significant (p < 0.01).On the full data, our method improves Chinese-English translation by 0.3?0.5 BLEU (0.3 ME-TEOR), which is unfortunately not statisticallysignificant, and Arabic-English translation by 0.5?0.6 BLEU (0.5 METEOR), which is statisticallysignificant (p < 0.01).5 Related workMost previous work on word alignment problemsuses morphosyntactic-semantic features, for ex-ample, word stems, content words, orthography(De Gispert et al., 2006; Hermjakob, 2009).
Avariety of log-linear models have been proposed toincorporate these features (Dyer et al., 2011; Berg-Kirkpatrick et al., 2010).
These approaches usu-ally require numerical optimization for discrimi-native training as well as language-specific engi-neering and may limit their applications to mor-phologically rich languages.A more semantic approach resorts to trainingword alignments on semantic word classes (Maet al., 2011).
However, the resulting alignmentsare only used to supplement the word alignmentslearned on lexical words.
To our knowledge, ourwork, which directly incorporates semantic rela-tionships in word alignment models, is novel.6 ConclusionWe have presented methods to extract word simi-larity from monolingual data and apply it to wordalignment models.
Our method can learn simi-lar words and word similarity probabilities, whichcan be used inside any probability model and inmany natural language processing tasks.
We havedemonstrated its effectiveness in statistical ma-chine translation.
The enhanced models can sig-nificantly improve alignment quality as well astranslation quality.AcknowledgmentsWe express our appreciation to Ashish Vaswanifor his advice and assistance.
We also thank HuiZhang, Tomer Levinboim, Qing Dou, Aliya Derifor helpful discussions and the anonymous review-ers for their insightful critiques.
This research wassupported in part by DOI/IBC grant D12AP00225and a Google Research Award to Chiang.ReferencesYoshua Bengio, Holger Schwenk, Jean-S?ebastienSen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.Springer.Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Proceedings ofHLT NAACL, pages 582?590.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:1844Parameter estimation.
Computational Linguistics,19(2):263?311.Colin Cherry and Dekang Lin.
2006.
Soft syntac-tic constraints for word alignment through discrim-inative training.
In Proceedings of COLING/ACL,pages 105?112.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.David Chiang.
2012.
Hope and fear for discriminativetraining of statistical translation models.
Journal ofMachine Learning Research, 13(1):1159?1187.Adri`a De Gispert, Deepa Gupta, Maja Popovi?c, PatrikLambert, Jose B. Mari?no, Marcello Federico, Her-mann Ney, and Rafael Banchs.
2006.
Improvingstatistical word alignments with morpho-syntactictransformations.
In Advances in Natural LanguageProcessing, pages 368?379.
Springer.Michael Denkowski and Alon Lavie.
2014.
Meteoruniversal: Language specific translation evaluationfor any target language.
In Proceedings of the EACL2014 Workshop on Statistical Machine Translation.Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A.Smith.
2011.
Unsupervised word alignment witharbitrary features.
In Proceedings of ACL: HLT, vol-ume 1, pages 409?419.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of IBM Model 2.
In Proceedings of NAACL-HLT, pages 644?648.Michael Gutmann and Aapo Hyv?arinen.
2010.
Noise-contrastive estimation: A new estimation princi-ple for unnormalized statistical models.
In Inter-national Conference on Artificial Intelligence andStatistics (AI-STATS), pages 297?304.Ulf Hermjakob.
2009.
Improved word alignment withstatistics and linguistic heuristics.
In Proceedings ofEMNLP, volume 1, pages 229?237.Geoffrey E. Hinton.
1999.
Products of experts.
InInternational Conference on Artificial Neural Net-works, volume 1, pages 1?6.Philipp Koehn, Amittai Axelrod, Chris Callison-Burch,Miles Osborne, and David Talbot.
2005.
Ed-inburgh system description for the 2005 IWSLTspeech translation evaluation.
In Proceedings of theInternational Workshop on Spoken Language Trans-lation (IWSLT).Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of ACL: Interactive Poster and Demon-stration Sessions, pages 177?180.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP.Jeff Ma, Spyros Matsoukas, and Richard Schwartz.2011.
Improving low-resource statistical machinetranslation with a novel semantic word clustering al-gorithm.
In Proceedings of MT Summit.Cos?kun Mermer and Murat Sarac?lar.
2011.
Bayesianword alignment for statistical machine translation.In Proceedings of ACL: HLT, volume 2, pages 182?187.Andriy Mnih and Yee Whye Teh.
2012.
A fast andsimple algorithm for training neural probabilisticlanguage models.
In Proceedings of ICML.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318.Kristina Toutanova, H. Tolga Ilhan, and Christopher D.Manning.
2002.
Extensions to HMM-based sta-tistical word alignment models.
In Proceedings ofEMNLP, pages 87?94.Ashish Vaswani, Liang Huang, and David Chiang.2012.
Smaller alignment models for better trans-lations: unsupervised word alignment with the `0-norm.
In Proceedings of ACL, volume 1, pages 311?319.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with large-scaleneural language models improves translation.
InProceedings of EMNLP.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In Proceedings of COLING, volume 2,pages 836?841.Zhiguo Wang and Chengqing Zong.
2013.
Large-scale word alignment using soft dependency cohe-sion constraints.
Transactions of the Association forComputational Linguistics, 1(6):291?300.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof ACL, pages 523?530.Hui Zhang and David Chiang.
2014.
Kneser-Neysmoothing on expected counts.
In Proceedings ofACL.1845
