Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2289?2294,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning principled bilingual mappings of word embeddings whilepreserving monolingual invarianceMikel Artetxe, Gorka Labaka, Eneko AgirreIXA NLP Group, University of the Basque Country (UPV/EHU){mikel.artetxe, gorka.labaka, e.agirre}@ehu.eusAbstractMapping word embeddings of different lan-guages into a single space has multiple appli-cations.
In order to map from a source spaceinto a target space, a common approach is tolearn a linear mapping that minimizes the dis-tances between equivalences listed in a bilin-gual dictionary.
In this paper, we proposea framework that generalizes previous work,provides an efficient exact method to learn theoptimal linear transformation and yields thebest bilingual results in translation inductionwhile preserving monolingual performance inan analogy task.1 IntroductionBilingual word embeddings have attracted a lot ofattention in recent times (Zou et al, 2013; Koc?isky?et al, 2014; Chandar A P et al, 2014; Gouws et al,2014; Gouws and S?gaard, 2015; Luong et al, 2015;Wick et al, 2016).
A common approach to obtainthem is to train the embeddings in both languagesindependently and then learn a mapping that mini-mizes the distances between equivalences listed in abilingual dictionary.
The learned transformation canalso be applied to words missing in the dictionary,which can be used to induce new translations witha direct application in machine translation (Mikolovet al, 2013b; Zhao et al, 2015).The first method to learn bilingual word em-bedding mappings was proposed by Mikolov et al(2013b), who learn the linear transformation thatminimizes the sum of squared Euclidean distancesfor the dictionary entries.
Subsequent work has pro-posed alternative optimization objectives to learnbetter mappings.
Xing et al (2015) incorporatelength normalization in the training of word embed-dings and try to maximize the cosine similarity in-stead, introducing an orthogonality constraint to pre-serve the length normalization after the projection.Faruqui and Dyer (2014) use canonical correlationanalysis to project the embeddings in both languagesto a shared vector space.Beyond linear mappings, Lu et al (2015) applydeep canonical correlation analysis to learn a non-linear transformation for each language.
Finally, ad-ditional techniques have been used to address thehubness problem in Mikolov et al (2013b), boththrough the neighbor retrieval method (Dinu et al,2015) and the training itself (Lazaridou et al, 2015).We leave the study of non-linear transformation andother additions for further work.In this paper, we propose a general framework tolearn bilingual word embeddings.
We start with abasic optimization objective (Mikolov et al, 2013b)and introduce several meaningful and intuitive con-straints that are equivalent or closely related to pre-viously proposed methods (Faruqui and Dyer, 2014;Xing et al, 2015).
Our framework provides a moregeneral view of bilingual word embedding map-pings, showing the underlying connection betweenthe existing methods, revealing some flaws in theirtheoretical justification and providing an alterna-tive theoretical interpretation for them.
Our exper-iments on an existing English-Italian word transla-tion induction and an English word analogy taskgive strong empirical evidence in favor of our the-oretical reasoning, while showing that one of ourmodels clearly outperforms previous alternatives.22892 Learning bilingual mappingsLet X and Z denote the word embedding matricesin two languages for a given bilingual dictionary sothat their ith row Xi?
and Zi?
are the word embed-dings of the ith entry in the dictionary.
Our goal is tofind a linear transformation matrix W so that XWbest approximates Z, which we formalize minimiz-ing the sum of squared Euclidean distances follow-ing Mikolov et al (2013b):arg minW?i?Xi?W ?
Zi?
?2Alternatively, this is equivalent to minimizing the(squared) Frobenius norm of the residual matrix:arg minW?XW ?
Z?2FConsequently, W will be the so called least-squares solution of the linear matrix equationXW = Z.
This is a well-known problem in lin-ear algebra and can be solved by taking the Moore-Penrose pseudoinverse X+ = (XTX)?1 XT asW = X+Z, which can be computed using SVD.2.1 Orthogonality for monolingual invarianceMonolingual invariance is needed to preserve thedot products after mapping, avoiding performancedegradation in monolingual tasks (e.g.
analogy).This can be obtained requiring W to be an orthog-onal matrix (W TW = I).
The exact solution un-der such orthogonality constraint is given by W =V UT , where ZTX = U?V T is the SVD factoriza-tion of ZTX (cf.
Appendix A).
Thanks to this, theoptimal transformation can be efficiently computedin linear time with respect to the vocabulary size.Note that orthogonality enforces an intuitive prop-erty, and as such it could be useful to avoid degen-erated solutions and learn better bilingual mappings,as we empirically show in Section 3.2.2 Length normalization for maximum cosineNormalizing word embeddings in both languages tobe unit vectors guarantees that all training instancescontribute equally to the optimization goal.
As longas W is orthogonal, this is equivalent to maximiz-ing the sum of cosine similarities for the dictionaryentries, which is commonly used for similarity com-putations:arg minW?i????Xi??Xi?
?W ?
Zi??Zi?????
?2= arg maxW?icos (Xi?W,Zi?
)This last optimization objective coincides withXing et al (2015), but their work was motivatedby an hypothetical inconsistency in Mikolov et al(2013b), where the optimization objective to learnword embeddings uses dot product, the objectiveto learn mappings uses Euclidean distance and thesimilarity computations use cosine.
However, thefact is that, as long as W is orthogonal, optimizingthe squared Euclidean distance of length-normalizedembeddings is equivalent to optimizing the cosine,and therefore, the mapping objective proposed byXing et al (2015) is equivalent to that used byMikolov et al (2013b) with orthogonality constraintand unit vectors.
In fact, our experiments show thatorthogonality is more relevant than length normal-ization, in contrast to Xing et al (2015), who intro-duce orthogonality only to ensure that unit length ispreserved after mapping.2.3 Mean centering for maximum covarianceDimension-wise mean centering captures the intu-ition that two randomly taken words would not beexpected to be semantically similar, ensuring thatthe expected product of two random embeddings inany dimension and, consequently, their cosine sim-ilarity, is zero.
As long as W is orthogonal, thisis equivalent to maximizing the sum of dimension-wise covariance for the dictionary entries:arg minW?CmXW ?
CmZ?2F= arg maxW?icov (XW?i, Z?i)where Cm denotes the centering matrixThis equivalence reveals that the method pro-posed by Faruqui and Dyer (2014) is closely re-lated to our framework.
More concretely, Faruquiand Dyer (2014) use Canonical Correlation Analysis(CCA) to project the word embeddings in both lan-guages to a shared vector space.
CCA maximizes2290the dimension-wise covariance of both projections(which is equivalent to maximizing the covarianceof a single projection if the transformations are con-strained to be orthogonal, as in our case) but addsan implicit restriction to the two mappings, makingdifferent dimensions have the same variance and beuncorrelated among themselves1:arg maxA,B?icov (XA?i, ZB?i)s.t.
ATXTCmXA = BTZTCmZB = ITherefore, the only fundamental difference be-tween both methods is that, while our model en-forces monolingual invariance, Faruqui and Dyer(2014) do change the monolingual embeddings tomeet this restriction.
In this regard, we think thatthe restriction they add could have a negative im-pact on the learning of the bilingual mapping, andit could also degrade the quality of the monolingualembeddings.
Our experiments (cf.
Section 3) showempirical evidence supporting this idea.3 ExperimentsIn this section, we experimentally test the proposedframework and all its variants in comparison withrelated methods.
For that purpose, we use the trans-lation induction task introduced by Mikolov et al(2013b), which learns a bilingual mapping on asmall dictionary and measures its accuracy on pre-dicting the translation of new words.
Unfortunately,the dataset they use is not public.
For that reason,we use the English-Italian dataset on the same taskprovided by Dinu et al (2015)2.
The dataset con-tains monolingual word embeddings trained with theword2vec toolkit using the CBOW method with neg-ative sampling (Mikolov et al, 2013a)3.
The Englishembeddings were trained on a 2.8 billion word cor-pus (ukWaC + Wikipedia + BNC), while the 1.6 bil-lion word corpus itWaC was used to train the Italian1While CCA is typically defined in terms of correlation (thusits name), correlation is invariant to the scaling of variables, soit is possible to constrain the canonical variables to have a fixedvariance, as we do, in which case correlation and covariancebecome equivalent2http://clic.cimec.unitn.it/?georgiana.dinu/down/3The context window was set to 5 words, the dimension ofthe embeddings to 300, the sub-sampling to 1e-05 and the num-ber of negative samples to 10embeddings.
The dataset alo contains a bilingualdictionary learned from Europarl, split into a train-ing set of 5,000 word pairs and a test set of 1,500word pairs, both of them uniformly distributed infrequency bins.
Accuracy is the evaluation measure.Apart from the performance of the projected em-beddings in bilingual terms, we are also interested inthe monolingual quality of the source language em-beddings after the projection.
For that purpose, weuse the word analogy task proposed by Mikolov etal.
(2013a), which measures the accuracy on answer-ing questions like ?what is the word that is similar tosmall in the same sense as biggest is similar to big?
?using simple word vector arithmetic.
The datasetthey use consists of 8,869 semantic and 10,675 syn-tactic questions of this type, and is publicly avail-able4.
In order to speed up the experiments, we fol-low the authors and perform an approximate eval-uation by reducing the vocabulary size accordingto a frequency threshold of 30,000 (Mikolov et al,2013a).
Since the original embeddings are the samein all the cases and it is only the transformation thatis applied to them that changes, this affects all themethods in the exact same way, so the results areperfectly comparable among themselves.
With thesesettings, we obtain a coverage of 64.98%.We implemented the proposed method in Pythonusing NumPy, and make it available as an opensource project5.
The code for Mikolov et al (2013b)and Xing et al (2015) is not publicly available, sowe implemented and tested them as part of the pro-posed framework, which only differs from the origi-nal systems in the optimization method (exact solu-tion instead of gradient descent) and the length nor-malization approach in the case of Xing et al (2015)(postprocessing instead of constrained training).
Asfor the method by Faruqui and Dyer (2014), we usedtheir original implementation in Python and MAT-LAB6, which we extended to cover cases where thedictionary contains more than one entry for the sameword.4https://code.google.com/archive/p/word2vec/5https://github.com/artetxem/vecmap6https://github.com/mfaruqui/crosslingual-cca2291EN-IT EN AN.Original embeddings - 76.66%Unconstrained mapping 34.93% 73.80%+ length normalization 33.80% 73.61%+ mean centering 38.47% 73.71%Orthogonal mapping 36.73% 76.66%+ length normalization 36.87% 76.66%+ mean centering 39.27% 76.59%Table 1: Our results in bilingual and monolingual tasks.3.1 Results of our frameworkThe rows in Table 1 show, respectively, the resultsfor the original embeddings, the basic mapping pro-posed by Mikolov et al (2013b) (cf.
Section 2) andthe addition of orthogonality constraint (cf.
Section2.1), with and without length normalization and, in-crementally, mean centering.
In all the cases, lengthnormalization and mean centering were applied toall embeddings, even if missing from the dictionary.The results show that the orthogonality constraintis key to preserve monolingual performance, andit also improves bilingual performance by enforc-ing a relevant property (monolingual invariance) thatthe transformation to learn should intuitively have.The contribution of length normalization alone ismarginal, but when followed by mean centeringwe obtain further improvements in bilingual perfor-mance without hurting monolingual performance.3.2 Comparison to other workTable 2 shows the results for our best performingconfiguration in comparison to previous work.
Asdiscussed before, (Mikolov et al, 2013b) and (Xinget al, 2015) were implemented as part of our frame-work, so they correspond to our uncostrained map-ping with no preprocessing and orthogonal mappingwith length normalization, respectively.As it can be seen, the method by Xing et al(2015) performs better than that of Mikolov et al(2013b) in the translation induction task, which is inline with what they report in their paper.
Moreover,thanks to the orthogonality constraint their mono-lingual performance in the word analogy task doesnot degrade, whereas the accuracy of Mikolov et al(2013b) drops by 2.86% in absolute terms with re-spect to the original embeddings.Since Faruqui and Dyer (2014) take advantage ofEN-IT EN AN.Original embeddings - 76.66%Mikolov et al (2013b) 34.93% 73.80%Xing et al (2015) 36.87% 76.66%Faruqui and Dyer (2014) 37.80% 69.64%Our method 39.27% 76.59%Table 2: Comparison of our method to other work.CCA to perform dimensionality reduction, we testedseveral values for it and report the best (180 dimen-sions).
This beats the method by Xing et al (2015)in the bilingual task, although it comes at the price ofa considerable degradation in monolingual quality.In any case, it is our proposed method with theorthogonality constraint and a global preprocessingwith length normalization followed by dimension-wise mean centering that achieves the best accuracyin the word translation induction task.
Moreover, itdoes not suffer from any considerable degradationin monolingual quality, with an anecdotal drop ofonly 0.07% in contrast with 2.86% for Mikolov etal.
(2013b) and 7.02% for Faruqui and Dyer (2014).When compared to Xing et al (2015), our resultsin Table 1 reinforce our theoretical interpretationfor their method (cf.
Section 2.2), as it empiricallyshows that its improvement with respect to Mikolovet al (2013b) comes solely from the orthogonalityconstraint, and not from solving any inconsistency.It should be noted that the implementation byFaruqui and Dyer (2014) also length-normalizes theword embeddings in a preprocessing step.
Follow-ing the discussion in Section 2.3, this means that ourbest performing configuration is conceptually veryclose to the method by Faruqui and Dyer (2014),as they both coincide on maximizing the averagedimension-wise covariance and length-normalizethe embeddings in both languages first, the only dif-ference being that our model enforces monolingualinvariance after the normalization while theirs doeschange the monolingual embeddings to make differ-ent dimensions have the same variance and be un-correlated among themselves.
However, our modelperforms considerably better than any configurationfrom Faruqui and Dyer (2014) in both the monolin-gual and the bilingual task, supporting our hypoth-esis that these two constraints that are implicit intheir method are not only conceptually confusing,2292but also have a negative impact.4 ConclusionsThis paper develops a new framework to learn bilin-gual word embedding mappings, generalizing previ-ous work and providing an efficient exact methodto learn the optimal transformation.
Our experi-ments show the effectiveness of the proposed modeland give strong empirical evidence in favor of ourreinterpretation of Xing et al (2015) and Faruquiand Dyer (2014).
It is the proposed method withthe orthogonality constraint and a global preprocess-ing with length normalization and dimension-wisemean centering that achieves the best overall resultsboth in monolingual and bilingual terms, surpassingthose previous methods.
In the future, we would liketo study non-linear mappings (Lu et al, 2015) andthe additional techniques in (Lazaridou et al, 2015).AcknowledgmentsThis research was partially supported by the Eu-ropean Commision (QTLeap FP7-ICT-2013-10-610516), a Google Faculty Award, and the Span-ish Ministry of Economy and Competitiveness(TADEEP TIN2015-70214-P).
Mikel Artetxe enjoysa doctoral grant from the Spanish Ministry of Edu-cation, Culture and Sports.A Proof of solution under orthogonalityConstraining W to be orthogonal (W TW = I), theoriginal minimization problem can be reformulatedas follows (cf.
Section 2.1):arg minW?i?Xi?W ?
Zi?
?2= arg minW?i(?Xi?W?2 + ?Zi?
?2 ?
2Xi?WZTi?
)= arg maxW?iXi?WZTi?= arg maxWTr(XWZT)= arg maxWTr(ZTXW)In the above expression, Tr(?)
denotes the traceoperator (the sum of all the elements in the main di-agonal), and the last equality is given by its cyclicproperty.
At this point, we can take the SVD ofZTX as ZTX = U?V T , so Tr (ZTXW ) =Tr(U?V TW)= Tr(?V TWU).
Since V T ,W and U are orthogonal matrices, their productV TWU will also be an orthogonal matrix.
In ad-dition to that, given that ?
is a diagonal matrix,its trace after an orthogonal transformation will bemaximal when the values in its main diagonal arepreserved after the mapping, that is, when the or-thogonal transformation matrix is the identity ma-trix.
This will happen when V TWU = I in ourcase, so the optimal solution will be W = V UT .ReferencesSarath Chandar A P, Stanislas Lauly, Hugo Larochelle,Mitesh Khapra, Balaraman Ravindran, Vikas CRaykar, and Amrita Saha.
2014.
An autoencoder ap-proach to learning bilingual word representations.
InAdvances in Neural Information Processing Systems27, pages 1853?1861.Georgiana Dinu, Angeliki Lazaridou, and Marco Baroni.2015.
Improving zero-shot learning by mitigatingthe hubness problem.
In Proceedings of the 3rd In-ternational Conference on Learning Representations(ICLR2015), workshop track.Manaal Faruqui and Chris Dyer.
2014.
Improving vectorspace word representations using multilingual correla-tion.
In Proceedings of the 14th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 462?471.Stephan Gouws and Anders S?gaard.
2015.
Simple task-specific bilingual word embeddings.
In Proceedingsof the 2015 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 1386?1390.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2014.
Bilbowa: Fast bilingual distributed repre-sentations without word alignments.
arXiv preprintarXiv:1410.2455.Toma?s?
Koc?isky?, Karl Moritz Hermann, and Phil Blun-som.
2014.
Learning bilingual word representationsby marginalizing alignments.
In Proceedings of the52nd Annual Meeting of the Association for Computa-tional Linguistics, volume 2, pages 224?229.Angeliki Lazaridou, Georgiana Dinu, and Marco Baroni.2015.
Hubness and pollution: Delving into cross-space mapping for zero-shot learning.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Process-ing, volume 1, pages 270?280.2293Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel, andKaren Livescu.
2015.
Deep multilingual correlationfor improved word embeddings.
In Proceedings ofthe 2015 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, pages 250?256.Min-Thang Luong, Hieu Pham, and Christopher D. Man-ning.
2015.
Bilingual word representations withmonolingual quality in mind.
In NAACL Workshop onVector Space Modeling for NLP, pages 151?159.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b.Exploiting similarities among languages for machinetranslation.
arXiv preprint arXiv:1309.4168.Michael Wick, Pallika Kanani, and Adam Pocock.
2016.Minimally-constrained multilingual embeddings viaartificial code-switching.
In Thirtieth AAAI confer-ence on Artificial Intelligence (AAAI).Chao Xing, Dong Wang, Chao Liu, and Yiye Lin.
2015.Normalized word embedding and orthogonal trans-form for bilingual word translation.
In Proceedingsof the 2015 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 1006?1011.Kai Zhao, Hany Hassan, and Michael Auli.
2015.
Learn-ing translation models from monolingual continuousrepresentations.
In Proceedings of the 2015 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 1527?1536.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
In Proceedingsof the 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1393?1398.2294
