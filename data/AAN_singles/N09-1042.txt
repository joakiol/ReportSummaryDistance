Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 371?379,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsGlobal Models of Document Structure Using Latent PermutationsHarr Chen, S.R.K.
Branavan, Regina Barzilay, David R. KargerComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{harr, branavan, regina, karger}@csail.mit.eduAbstractWe present a novel Bayesian topic model forlearning discourse-level document structure.Our model leverages insights from discoursetheory to constrain latent topic assignments ina way that reflects the underlying organiza-tion of document topics.
We propose a globalmodel in which both topic selection and order-ing are biased to be similar across a collectionof related documents.
We show that this spaceof orderings can be elegantly represented us-ing a distribution over permutations called thegeneralized Mallows model.
Our structure-aware approach substantially outperforms al-ternative approaches for cross-document com-parison and single-document segmentation.11 IntroductionIn this paper, we introduce a novel latent topic modelfor the unsupervised learning of document structure.Traditional topic models assume that topics are ran-domly spread throughout a document, or that thesuccession of topics in a document is Markovian.In contrast, our approach takes advantage of twoimportant discourse-level properties of text in de-termining topic assignments: first, that each docu-ment follows a progression of nonrecurring coher-ent topics (Halliday and Hasan, 1976); and sec-ond, that documents from the same domain tendto present similar topics, in similar orders (Wray,2002).
We show that a topic model incorporat-ing these long-range dependencies outperforms al-1Code, data, and annotations used in this work are availableat http://groups.csail.mit.edu/rbg/code/mallows/ternative approaches for segmentation and cross-document comparison.For example, consider a collection of encyclope-dia articles about cities.
The first constraint capturesthe notion that a single topic, such as Architecture,is expressed in a contiguous block within the docu-ment, rather than spread over disconnected sections.The second constraint reflects our intuition that allof these related articles will generally mention somemajor topics associated with cities, such as Historyand Culture, and will often exhibit similar topic or-derings, such as placing History before Culture.We present a Bayesian latent topic model over re-lated documents that encodes these discourse con-straints by positing a single distribution over a doc-ument?s entire topic structure.
This global view onordering is able to elegantly encode discourse-levelproperties that would be difficult to represent usinglocal dependencies, such as those induced by hid-den Markov models.
Our model enforces that thesame topic does not appear in disconnected portionsof the topic sequence.
Furthermore, our approachbiases toward selecting sequences with similar topicordering, by modeling a distribution over the spaceof topic permutations.Learning this ordering distribution is a key tech-nical challenge in our proposed approach.
For thispurpose, we employ the generalized Mallows model,a permutation distribution that concentrates proba-bility mass on a small set of similar permutations.It directly captures the intuition of the second con-straint, and uses a small parameter set to control howlikely individual topics are to be reordered.We evaluate our model on two challenging371document-level tasks.
In the alignment task, we aimto discover paragraphs across different documentsthat share the same topic.
We also consider the seg-mentation task, where the goal is to partition eachdocument into a sequence of topically coherent seg-ments.
We find that our structure modeling approachsubstantially outperforms state-of-the-art baselinesfor both tasks.
Furthermore, we demonstrate the im-portance of explicitly modeling a distribution overtopic permutations; our model yields significantlybetter results than variants that either use a fixed or-dering, or are order-agnostic.2 Related WorkTopic and ContentModels Our work is groundedin topic modeling approaches, which posit that la-tent state variables control the generation of words.In earlier topic modeling work such as latent Dirich-let alocation (LDA) (Blei et al, 2003; Griffiths andSteyvers, 2004), documents are treated as bags ofwords, where each word receives a separate topicassignment; the topic assignments are auxiliary vari-ables to the main task of language modeling.More recent work has attempted to adapt the con-cepts of topic modeling to more sophisticated repre-sentations than a bag of words; they use these rep-resentations to impose stronger constraints on topicassignments (Griffiths et al, 2005; Wallach, 2006;Purver et al, 2006; Gruber et al, 2007).
Theseapproaches, however, generally model Markoviantopic or state transitions, which only capture lo-cal dependencies between adjacent words or blockswithin a document.
For instance, content mod-els (Barzilay and Lee, 2004; Elsner et al, 2007)are implemented as HMMs, where the states cor-respond to topics of domain-specific information,and transitions reflect pairwise ordering prefer-ences.
Even approaches that break text into con-tiguous chunks (Titov and McDonald, 2008) as-sign topics based on local context.
While theselocally constrained models can implicitly reflectsome discourse-level constraints, they cannot cap-ture long-range dependencies without an explosionof the parameter space.
In contrast, our model cap-tures the entire sequence of topics using a compactrepresentation.
As a result, we can explicitly andtractably model global discourse-level constraints.Modeling Ordering Constraints Sentence order-ing has been extensively studied in the context ofprobabilistic text modeling for summarization andgeneration (Barzilay et al, 2002; Lapata, 2003;Karamanis et al, 2004).
The emphasis of that bodyof work is on learning ordering constraints fromdata, with the goal of reordering new text from thesame domain.
Our emphasis, however, is on ap-plications where ordering is already observed, andhow that ordering can improve text analysis.
Fromthe methodological side, that body of prior work islargely driven by local pairwise constraints, whilewe aim to encode global constraints.3 Problem FormulationOur document structure learning problem can be for-malized as follows.
We are given a corpus of Drelated documents.
Each document expresses somesubset of a common set of K topics.
We assign asingle topic to each paragraph,2 incorporating thenotion that paragraphs are internally topically con-sistent (Halliday and Hasan, 1976).
To capture thediscourse constraint on topic progression describedin Section 1, we require that topic assignments becontiguous within each document.3 Furthermore,we assume that the underlying topic sequences ex-hibit similarity across documents.
Our goal is to re-cover a topic assignment for each paragraph in thecorpus, subject to these constraints.Our formulation shares some similarity with thestandard LDA setup, in that a common set of topicsis assigned across a collection of documents.
How-ever, in LDA each word?s topic assignment is con-ditionally independent, following the bag of wordsview of documents.
In contrast, our constraints onhow topics are assigned let us connect word distri-butional patterns to document-level topic structure.4 ModelWe propose a generative Bayesian model that ex-plains how a corpus of D documents, given as se-quences of paragraphs, can be produced from a setof hidden topic variables.
Topic assignments to each2Note that our analysis applies equally to other levels of tex-tual granularity, such as sentences.3That is, if paragraphs i and j are assigned the same topic,every paragraph between them must have that topic.372paragraph, ranging from 1 to K, are the model?sfinal output, implicitly grouping topically similarparagraphs.
At a high level, the process first selectsthe bag of topics to be expressed in the document,and how they are ordered; these topics then deter-mine the selection of words for each paragraph.For each document dwithNd paragraphs, we sep-arately generate a bag of topics td and a topic order-ing pid.
The unordered bag of topics, which containsNd elements, expresses how many paragraphs of thedocument are assigned to each of theK topics.
Notethat some topics may not appear at all.
Variable tdis constructed by taking Nd samples from a distri-bution over topics ?
, a multinomial representing theprobability of each topic being expressed.
Sharing?
between documents captures the intuition that cer-tain topics are more likely across the entire corpus.The topic ordering variable pid is a permutationover the numbers 1 through K that defines the orderin which topics appear in the document.
We draw pidfrom the generalized Mallows model, a distributionover permutations that we explain in Section 4.1.
Aswe will see, this particular distribution biases thepermutation selection to be close to a single cen-troid, reflecting the discourse constraint of prefer-ring similar topic structures across documents.Together, a document?s bag of topics td and or-dering pid determine the topic assignment zd,p foreach of its paragraphs.
For example, in a corpuswith K = 4, a seven-paragraph document d withtd = {1, 1, 1, 1, 2, 4, 4} and pid = (2 4 3 1) wouldinduce the topic sequence zd = (2 4 4 1 1 1 1).
Theinduced topic sequence zd can never assign the sametopic to two unconnected portions of a document,thus satisfying the constraint of topic contiguity.As with LDA, we assume that each topic k is as-sociated with a language model ?k.
The words of aparagraph assigned to topic k are then drawn fromthat topic?s language model ?k.Before turning to a more formal discussion of thegenerative process, we first provide background onthe permutation model for topic ordering.4.1 The Generalized Mallows ModelA central challenge of the approach we take is mod-eling the distribution over possible topic permuta-tions.
For this purpose we use the generalized Mal-lows model (GMM) (Fligner and Verducci, 1986;Lebanon and Lafferty, 2002; Meila?
et al, 2007),which exhibits two appealing properties in the con-text of this task.
First, the model concentrates proba-bility mass on some ?canonical?
ordering and smallperturbations of that ordering.
This characteris-tic matches our constraint that documents from thesame domain exhibit structural similarity.
Second,its parameter set scales linearly with the permuta-tion length, making it sufficiently constrained andtractable for inference.
In general, this distributioncould potentially be applied to other NLP applica-tions where ordering is important.Permutation Representation Typically, permuta-tions are represented directly as an ordered sequenceof elements.
The GMM utilizes an alternative rep-resentation defined as a vector (v1, .
.
.
, vK?1) of in-version counts with respect to the identity permuta-tion (1, .
.
.
,K).
Term vj counts the number of timesa value greater than j appears before j in the permu-tation.4 For instance, given the standard-form per-mutation (3 1 5 2 4), v2 = 2 because 3 and 5 appearbefore 2; the entire inversion count vector would be(1 2 0 1).
Every vector of inversion counts uniquelyidentifies a single permutation.The Distribution The GMM assigns proba-bility mass according to the distance of agiven permutation from the identity permutation{1, .
.
.
,K}, based on K ?
1 real-valued parameters(?1, .
.
.
?K?1).5 Using the inversion count represen-tation of a permutation, the GMM?s probability massfunction is expressed as an independent product ofprobabilities for each vj :GMM(v | ?)
= e?
?j ?jvj?(?)=n?1?j=1e?
?jvj?j(?j) , (1)where ?j(?j) is a normalization factor with value:?j(?j) = 1?
e?(K?j+1)?j1?
e?
?j .4The sum of a vector of inversion counts is simply that per-mutation?s Kendall?s ?
distance to the identity permutation.5In our work we take the identity permutation to be the fixedcentroid, which is a parameter in the full GMM.
As we explainlater, our model is not hampered by this apparent restriction.373Due to the exponential form of the distribution, re-quiring that ?j > 0 constrains the GMM to assignhighest probability mass to each vj being zero, cor-responding to the identity permutation.
A highervalue for ?j assigns more probability mass to vj be-ing close to zero, biasing j to have fewer inversions.The GMM elegantly captures our earlier require-ment for a probability distribution that concentratesmass around a global ordering, and uses few param-eters to do so.
Because the topic numbers in ourtask are completely symmetric and not linked to anyextrinsic observations, fixing the identity permuta-tion to be that global ordering does not sacrifice anyrepresentational power.
Another major benefit ofthe GMM is its membership in the exponential fam-ily of distributions; this means that it is particularlyamenable to a Bayesian representation, as it admitsa natural conjugate prior:GMM0(?j | vj,0, ?0) ?
e(?
?jvj,0?log?j(?j))?0 .
(2)Intuitively, this prior states that over ?0 prior trials,the total number of inversions was ?0vj,0.
This dis-tribution can be easily updated with the observed vjto derive a posterior distribution.64.2 Formal Generative ProcessWe now fully specify the details of our model.
Weobserve a corpus of D documents, each an orderedsequence of paragraphs, and a specification of anumber of topics K. Each paragraph is representedas a bag of words.
The model induces a set of hid-den variables that probabilistically explain how thewords of the corpus were produced.
Our final de-sired output is the distributions over the paragraphs?hidden topic assignment variables.
In the following,variables subscripted with 0 are fixed prior hyperpa-rameters.1.
For each topic k, draw a language model ?k ?Dirichlet(?0).
As with LDA, these are topic-specific word distributions.2.
Draw a topic distribution ?
?
Dirichlet(?0),which expresses how likely each topic is to ap-pear regardless of position.6Because each vj has a different range, it is inconvenientto set the prior hyperparameters vj,0 directly.
In our work, weinstead fix the mode of the prior distribution to a value ?0, whichworks out to setting vj,0 = 1exp(?0)?1 ?
K?j+1exp((K?j+1)?0)?1 .3.
Draw the topic ordering distribution parame-ters ?j ?
GMM0(?0, ?0) for j = 1 to K ?
1.These parameters control how rapidly probabil-ity mass decays for having more inversions foreach topic.
A separate ?j for every topic allowsus to learn that some topics are more likely tobe reordered than others.4.
For each document d with Nd paragraphs:(a) Draw a bag of topics td by sampling Ndtimes from Multinomial(?).
(b) Draw a topic ordering pid by sampling avector of inversion counts vd ?
GMM(?).
(c) Compute the vector of topic assignmentszd for document d?s paragraphs, by sortingtd according to pid.7(d) For each paragraph p in document d:i.
Sample each word wd,p,j according tothe language model of p: wd,p,j ?Multinomial(?zd,p).5 InferenceThe variables that we aim to infer are the topic as-signments z of each paragraph, which are deter-mined by the bag of topics t and ordering pi for eachdocument.
Thus, our goal is to estimate the marginaldistributions of t and pi given the document text.We accomplish this inference task through Gibbssampling (Bishop, 2006).
A Gibbs sampler buildsa Markov chain over the hidden variable state spacewhose stationary distribution is the actual posteriorof the joint distribution.
Each new sample is drawnfrom the distribution of a single variable conditionedon previous samples of the other variables.
We can?collapse?
the sampler by integrating over some ofthe hidden variables in the model, in effect reducingthe state space of the Markov chain.
Collapsed sam-pling has been previously demonstrated to be effec-tive for LDA and its variants (Griffiths and Steyvers,2004; Porteous et al, 2008; Titov and McDonald,2008).
Our sampler integrates over all but three sets7Multiple permutations can contribute to the probability of asingle document?s topic assignments zd, if there are topics thatdo not appear in td.
As a result, our current formulation is bi-ased toward assignments with fewer topics per document.
Inpractice, we do not find this to negatively impact model perfor-mance.374of hidden variables: bags of topics t, orderings pi,and permutation inversion parameters ?.
After aburn-in period, we treat the last samples of t andpi as a draw from the true posterior.Document Probability As a preliminary step,consider how to calculate the probability of a singledocument?s words wd given the document?s para-graph topic assignments zd, and other documentsand their topic assignments.
Note that this proba-bility is decomposable into a product of probabil-ities over individual paragraphs, where paragraphswith different topics have conditionally independentword probabilities.
Let w?d and z?d indicate thewords and topic assignments to documents otherthan d, and W be the vocabulary size.
The proba-bility of the words in d is then:P (wd | z,w?d, ?0)=K?k=1?
?kP (wd | zd, ?k)P (?k | z,w?d, ?0)d?k=K?k=1DCM({wd,i : zd,i = k}| {w?d,i : z?d,i = k}, ?0), (3)where DCM(?)
refers to the Dirichlet compoundmultinomial distribution, the result of integrat-ing over multinomial parameters with a Dirichletprior (Bernardo and Smith, 2000).
For a Dirichletprior with parameters ?
= (?1, .
.
.
, ?W ), the DCMassigns the following probability to a series of ob-servations x = {x1, .
.
.
, xn}:DCM(x | ?)
= ?
(?j ?j)?j ?(?j)W?i=1?
(N(x, i) + ?i)?
(|x|+?j ?j),where N(x, i) refers to the number of times wordi appears in x.
Here, ?(?)
is the Gamma function,a generalization of the factorial for real numbers.Some algebra shows that the DCM?s posterior prob-ability density function conditioned on a series ofobservations y = {y1, .
.
.
, yn} can be computed byupdating each ?i with counts of how often word iappears in y:DCM(x | y, ?
)= DCM(x | ?1 +N(y, 1), .
.
.
, ?W +N(y,W )).
(4)Equation 3 and 4 will be used again to compute theconditional distributions of the hidden variables.We now turn to a discussion of how each individ-ual random variable is resampled.Bag of Topics First we consider how to resampletd,i, the ith topic draw for document d conditionedon all other parameters being fixed (note this is notthe topic of the ith paragraph, as we reorder topicsusing pid):P (td,i = t | .
.
.)?
P (td,i = t | t?
(d,i), ?0)P (wd | td, pid,w?d, z?d, ?0)?N(t?
(d,i), t) + ?0|t?
(d,i)|+K?0 P (wd | z,w?d, ?0),where td is updated to reflect td,i = t, and zd is de-terministically computed by mapping td and pid toactual paragraph topic assignments.
The first stepreflects an application of Bayes rule to factor out theterm for wd.
In the second step, the first term arisesout of the DCM, by updating the parameters ?0 withobservations t?
(d,i) as in equation 4 and droppingconstants.
The document probability term is com-puted using equation 3.
The new td,i is selectedby sampling from this probability computed over allpossible topic assignments.Ordering The parameterization of a permutationpi as a series of inversion values vj reveals a naturalway to decompose the search space for Gibbs sam-pling.
For a single ordering, each vj can be sampledindependently, according to:P (vj = v | .
.
.)?
P (vj = v | ?j)P (wd | td, pid,w?d, z?d, ?0)= GMMj(v | ?j)P (wd | zd,w?d, z?d, ?0),where pid is updated to reflect vj = v, and zd is com-puted according to td and pid.
The first term refersto the jth multiplicand of equation 1; the second iscomputed using equation 3.
Term vj is sampled ac-cording to the resulting probabilities.GMM Parameters For each j = 1 to K ?
1, weresample ?j from its posterior distribution:P (?j | .
.
.
)= GMM0(?j????
?i vj,i + vj,0?0N + ?0 , N + ?0),375where GMM0 is evaluated according to equation 2.The normalization constant of this distribution is un-known, meaning that we cannot directly computeand invert the cumulative distribution function tosample from this distribution.
However, the distri-bution itself is univariate and unimodal, so we canexpect that an MCMC technique such as slice sam-pling (Neal, 2003) should perform well.
In practice,the MATLAB black-box slice sampler provides a ro-bust draw from this distribution.6 Experimental SetupData Sets We evaluate our model on two data setsdrawn from the English Wikipedia.
The first setis 100 articles about large cities, with topics suchas History, Culture, and Demographics.
The sec-ond is 118 articles about chemical elements in theperiodic table, including topics such as BiologicalRole, Occurrence, and Isotopes.
Within each cor-pus, articles often exhibit similar section orderings,but many have idiosyncratic inversions.
This struc-tural variability arises out of the collaborative natureof Wikipedia, which allows articles to evolve inde-pendently.
Corpus statistics are summarized below.Corpus Docs Paragraphs Vocab WordsCities 100 6,670 41,978 492,402Elements 118 2,810 18,008 191,762In each data set, the articles?
noisy section head-ings induce a reference structure to compare against.This reference structure assumes that two para-graphs are aligned if and only if their section head-ings are identical, and that section boundaries pro-vide the correct segmentation of each document.These headings are only used for evaluation, and arenot provided to any of the systems.Using the section headings to build the referencestructure can be problematic, as the same topic maybe referred to using different titles across differentdocuments, and sections may be divided at differinglevels of granularity.
Thus, for the Cities data set, wemanually annotated each article?s paragraphs with aconsistent set of section headings, providing us anadditional reference structure to evaluate against.
Inthis clean section headings set, we found approxi-mately 18 topics that were expressed in more thanone document.Tasks and Metrics We study performance on thetasks of alignment and segmentation.
In the formertask, we measure whether paragraphs identified tobe the same topic by our model have the same sec-tion headings, and vice versa.
First, we identify the?closest?
topic to each section heading, by findingthe topic that is most commonly assigned to para-graphs under that section heading.
We compute theproportion of paragraphs where the model?s topic as-signment matches the section heading?s topic, giv-ing us a recall score.
High recall indicates thatparagraphs of the same section headings are alwaysbeing assigned to the same topic.
Conversely, wecan find the closest section heading to each topic,by finding the section heading that is most com-mon for the paragraphs assigned to a single topic.We then compute the proportion of paragraphs fromthat topic whose section heading is the same as thereference heading for that topic, yielding a preci-sion score.
High precision means that paragraphsassigned to a single topic usually correspond to thesame section heading.
The harmonic mean of recalland precision is the summary F-score.Statistical significance in this setup is measuredwith approximate randomization (Noreen, 1989), anonparametric test that can be directly applied tononlinear metrics such as F-score.
This test has beenused in prior evaluations for information extractionand machine translation (Chinchor, 1995; Riezlerand Maxwell, 2005).For the second task, we take the boundaries atwhich topics change within a document to be asegmentation of that document.
We evaluate us-ing the standard penalty metrics Pk and WindowD-iff (Beeferman et al, 1999; Pevzner and Hearst,2002).
Both pass a sliding window over the doc-uments and compute the probability of the wordsat the ends of the windows being improperly seg-mented with respect to each other.
WindowDiff re-quires that the number of segmentation boundariesbetween the endpoints be correct as well.8Our model takes a parameter K which controlsthe upper bound on the number of latent topics.
Notethat our algorithm can select fewer thanK topics foreach document, soK does not determine the number8Statistical significance testing is not standardized and usu-ally not reported for the segmentation task, so we omit thesetests in our results.376of segments in each document.
We report resultsusing both K = 10 and 20 (recall that the cleanlyannotated Cities data set had 18 topics).Baselines andModel Variants We consider base-lines from the literature that perform either align-ment or segmentation.
For the first task, wecompare against the hidden topic Markov model(HTMM) (Gruber et al, 2007), which representstopic transitions between adjacent paragraphs in aMarkovian fashion, similar to the approach taken incontent modeling work.
Note that HTMM can onlycapture local constraints, so it would allow topics torecur noncontiguously throughout a document.We also compare against the structure-agnosticapproach of clustering the paragraphs using theCLUTO toolkit,9 which uses repeated bisection tomaximize a cosine similarity-based objective.For the segmentation task, we compare toBayesSeg (Eisenstein and Barzilay, 2008),10a Bayesian topic-based segmentation modelthat outperforms previous segmentation ap-proaches (Utiyama and Isahara, 2001; Galley et al,2003; Purver et al, 2006; Malioutov and Barzilay,2006).
BayesSeg enforces the topic contiguityconstraint that motivated our model.
We providethis baseline with the benefit of knowing the correctnumber of segments for each document, which isnot provided to our system.
Note that BayesSegprocesses each document individually, so it cannotcapture structural relatedness across documents.To investigate the importance of our orderingmodel, we consider two variants of our model thatalternately relax and tighten ordering constraints.
Inthe constrained model, we require all documents tofollow the same canonical ordering of topics.
Thisis equivalent to forcing the topic permutation distri-bution to give all its probability to one ordering, andcan be implemented by fixing all inversion counts vto zero during inference.
At the other extreme, weconsider the uniform model, which assumes a uni-form distribution over all topic permutations insteadof biasing toward a small related set.
In our im-plementation, this can be simulated by forcing the9http://glaros.dtc.umn.edu/gkhome/views/cluto/10We do not evaluate on the corpora used in their work, sinceour model relies on content similarity across documents in thecorpus.GMM parameters ?
to always be zero.
Both variantsstill enforce topic contiguity, and allow segmentsacross documents to be aligned by topic assignment.Evaluation Procedures For each evaluation ofour model and its variants, we run the Gibbs samplerfrom five random seed states, and take the 10,000thiteration of each chain as a sample.
Results shownare the average over these five samples.
All Dirich-let prior hyperparameters are set to 0.1, encouragingsparse distributions.
For the GMM, we set the priordecay parameter ?0 to 1, and the sample size prior?0 to be 0.1 times the number of documents.For the baselines, we use implementations pub-licly released by their authors.
We set HTMM?s pri-ors according to values recommended in the authors?original work.
For BayesSeg, we use its built-in hy-perparameter re-estimation mechanism.7 ResultsAlignment Table 1 presents the results of thealignment evaluation.
In every case, the best per-formance is achieved using our full model, by a sta-tistically significant and usually substantial margin.In both domains, the baseline clustering methodperforms competitively, indicating that word cuesalone are a good indicator of topic.
While the sim-pler variations of our model achieve reasonable per-formance, adding the richer GMM distribution con-sistently yields superior results.Across each of our evaluations, HTMM greatlyunderperforms the other approaches.
Manual ex-amination of the actual topic assignments revealsthat HTMM often selects the same topic for discon-nected paragraphs of the same document, violatingthe topic contiguity constraint, and demonstratingthe importance of modeling global constraints fordocument structure tasks.We also compare performance measured on themanually annotated section headings against the ac-tual noisy headings.
The ranking of methods by per-formance remains mostly unchanged between thesetwo evaluations, indicating that the noisy headingsare sufficient for gaining insight into the compara-tive performance of the different approaches.Segmentation Table 2 presents the segmentationexperiment results.
On both data sets, our model377Cities: clean headings Cities: noisy headings Elements: noisy headingsRecall Prec F-score Recall Prec F-score Recall Prec F-scoreK=10Clustering 0.578 0.439 ?
0.499 0.611 0.331 ?
0.429 0.524 0.361 ?
0.428HTMM 0.446 0.232 ?
0.305 0.480 0.183 ?
0.265 0.430 0.190 ?
0.264Constrained 0.579 0.471 ?
0.520 0.667 0.382 ?
0.485 0.603 0.408 ?
0.487Uniform 0.520 0.440 ?
0.477 0.599 0.343 ?
0.436 0.591 0.403 ?
0.479Our model 0.639 0.509 0.566 0.705 0.399 0.510 0.685 0.460 0.551K=20Clustering 0.486 0.541 ?
0.512 0.527 0.414 ?
0.464 0.477 0.402 ?
0.436HTMM 0.260 0.217 ?
0.237 0.304 0.187 ?
0.232 0.248 0.243 ?
0.246Constrained 0.458 0.519 ?
0.486 0.553 0.415 ?
0.474 0.510 0.421 ?
0.461Uniform 0.499 0.551 ?
0.524 0.571 0.423 ?
0.486 0.550 0.479  0.512Our model 0.578 0.636 0.606 0.648 0.489 0.557 0.569 0.498 0.531Table 1: Comparison of the alignments produced by our model and a series of baselines and model variations, for both10 and 20 topics, evaluated against clean and noisy sets of section headings.
Higher scores are better.
Within the sameK, the methods which our model significantly outperforms are indicated with ?
for p < 0.001 and  for p < 0.01.Cities: clean headings Cities: noisy headings Elements: noisy headingsPk WD # Segs Pk WD # Segs Pk WD # SegsBayesSeg 0.321 0.376 ?
12.3 0.317 0.376 ?
13.2 0.279 0.316 ?
7.7K=10 Constrained 0.260 0.281 7.7 0.267 0.288 7.7 0.227 0.244 5.4Uniform 0.268 0.300 8.8 0.273 0.304 8.8 0.226 0.250 6.6Our model 0.253 0.283 9.0 0.257 0.286 9.0 0.201 0.226 6.7K=20 Constrained 0.274 0.314 10.9 0.274 0.313 10.9 0.231 0.257 6.6Uniform 0.234 0.294 14.0 0.234 0.290 14.0 0.209 0.248 8.7Our model 0.221 0.278 14.2 0.222 0.278 14.2 0.203 0.243 8.6Table 2: Comparison of the segmentations produced by our model and a series of baselines and model variations, forboth 10 and 20 topics, evaluated against clean and noisy sets of section headings.
Lower scores are better.
?BayesSegis given the true number of segments, so its segments count reflects the reference structure?s segmentation.outperforms the BayesSeg baseline by a substantialmargin regardless of K. This result provides strongevidence that learning connected topic models overrelated documents leads to improved segmentationperformance.
In effect, our model can take advan-tage of shared structure across related documents.In all but one case, the best performance is ob-tained by the full version of our model.
This resultindicates that enforcing discourse-motivated struc-tural constraints allows for better segmentation in-duction.
Encoding global discourse-level constraintsleads to better language models, resulting in moreaccurate predictions of segment boundaries.8 ConclusionsIn this paper, we have shown how an unsupervisedtopic-based approach can capture document struc-ture.
Our resulting model constrains topic assign-ments in a way that requires global modeling of en-tire topic sequences.
We showed that the generalizedMallows model is a theoretically and empirically ap-pealing way of capturing the ordering componentof this topic sequence.
Our results demonstrate theimportance of augmenting statistical models of textanalysis with structural constraints motivated by dis-course theory.AcknowledgmentsThe authors acknowledge the funding support ofNSF CAREER grant IIS-0448168, the NSF Grad-uate Fellowship, the Office of Naval Research,Quanta, Nokia, and the Microsoft Faculty Fellow-ship.
We thank the members of the NLP group atMIT and numerous others who offered suggestionsand comments on this work.
We are especially grate-ful to Marina Meila?
for introducing us to the Mal-lows model.
Any opinions, findings, conclusions, orrecommendations expressed in this paper are thoseof the authors, and do not necessarily reflect theviews of the funding organizations.378ReferencesRegina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofNAACL/HLT.Regina Barzilay, Noemie Elhadad, and Kathleen McKe-own.
2002.
Inferring strategies for sentence orderingin multidocument news summarization.
Journal of Ar-tificial Intelligence Research, 17:35?55.Doug Beeferman, Adam Berger, and John D. Lafferty.1999.
Statistical models for text segmentation.
Ma-chine Learning, 34:177?210.Jose?
M. Bernardo and Adrian F.M.
Smith.
2000.Bayesian Theory.
Wiley Series in Probability andStatistics.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
Journal of Machine Learn-ing Research, 3:993?1022.Nancy Chinchor.
1995.
Statistical significance of MUC-6 results.
In Proceedings of the 6th Conference onMessage Understanding.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedings ofEMNLP.Micha Elsner, Joseph Austerweil, and Eugene Charniak.2007.
A unified local and global model for discoursecoherence.
In Proceedings of NAACL/HLT.M.A.
Fligner and J.S.
Verducci.
1986.
Distance basedranking models.
Journal of the Royal Statistical Soci-ety, Series B, 48(3):359?369.Michel Galley, Kathleen R. McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse segmen-tation of multi-party conversation.
In Proceedings ofACL.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101:5228?5235.Thomas L. Griffiths, Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics andsyntax.
In Advances in NIPS.Amit Gruber, Michal Rosen-Zvi, and Yair Weiss.
2007.Hidden topic markov models.
In Proceedings of AIS-TATS.M.
A. K. Halliday and Ruqaiya Hasan.
1976.
Cohesionin English.
Longman.Nikiforos Karamanis, Massimo Poesio, Chris Mellish,and Jon Oberlander.
2004.
Evaluating centering-based metrics of coherence for text structuring usinga reliably annotated corpus.
In Proceedings of ACL.Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proceedings ofACL.Guy Lebanon and John Lafferty.
2002.
Cranking: com-bining rankings using conditional probability modelson permutations.
In Proceedings of ICML.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of ACL.Marina Meila?, Kapil Phadnis, Arthur Patterson, and JeffBilmes.
2007.
Consensus ranking under the exponen-tial model.
In Proceedings of UAI.Radford M. Neal.
2003.
Slice sampling.
Annals ofStatistics, 31:705?767.Eric W. Noreen.
1989.
Computer Intensive Methods forTesting Hypotheses.
An Introduction.
Wiley.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28:19?36.Ian Porteous, David Newman, Alexander Ihler, ArthurAsuncion, Padhraic Smyth, and Max Welling.
2008.Fast collapsed gibbs sampling for latent dirichlet alo-cation.
In Proceedings of SIGKDD.Matthew Purver, Konrad Ko?rding, Thomas L. Griffiths,and Joshua B. Tenenbaum.
2006.
Unsupervised topicmodelling for multi-party spoken discourse.
In Pro-ceedings of ACL/COLING.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization.Ivan Titov and Ryan McDonald.
2008.
Modeling onlinereviews with multi-grain topic models.
In Proceedingsof WWW.Masao Utiyama and Hitoshi Isahara.
2001.
A statisticalmodel for domain-independent text segmentation.
InProceedings of ACL.Hanna M. Wallach.
2006.
Topic modeling: beyond bagof words.
In Proceedings of ICML.Alison Wray.
2002.
Formulaic Language and the Lexi-con.
Cambridge University Press, Cambridge.379
