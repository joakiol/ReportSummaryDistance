Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 657?662,Dublin, Ireland, August 23-24, 2014.TJP: Identifying the Polarity of Tweets from ContextTawunrat Chalothorn Jeremy EllmanDepartment of Computer Science andDigital TechnologiesUniversity of Northumbria at Newcas-tle, Pandon Building, Camden StreetNewcastle Upon Tyne, NE2 1XE, UKDepartment of Computer Science andDigital TechnologiesUniversity of Northumbria at Newcas-tle, Pandon Building, Camden StreetNewcastle Upon Tyne, NE2 1XE, UKtawunrat.chalothorn@northumbria.ac.ukjeremy.ellman@northumbria.ac.ukAbstractThe TJP system is presented, which partici-pated in SemEval 2014 Task 9, Part A:Contextual Polarity Disambiguation.
Oursystem is ?constrained?, using only dataprovided by the organizers.
The goal of thistask is to identify whether marking contextsare positive, negative or neutral.
Our systemuses a support vector machine, with exten-sive pre-processing and achieved an overallF-score of 81.96%.1 IntroductionThe aim of sentiment analysis is to identifywhether the subject of a text is intended to beviewed positively of negatively by a reader.
Suchemotions are sometimes hidden in long sentencesand are difficult to identify.
Consequently senti-ment analysis is an active research area in naturallanguage processing.
*Sentiment is currently conceived terms of po-larity.
This has numerous interesting applica-tions.
For example, Grabner et al.
(2012) usedsentiment analysis to classify customers?
reviewsof hotels by using a star rating to categorize the*     This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/reviews as bad, neutral and good.
Similarly,Tumasjan et al.
(2010) tried to predict the out-come of the German federal election through theanalysis more than 100,000 tweets posted in thelead up.
Sentiment analysis has also used toclassify whether dreams are positive or nega-tive (Nadeau et al.
2006).This paper presents the TJP system whichwas  submitted to SemEval 2014 Task 9, Part A:Contextual Polarity Disambiguation (Rosenthalet al., 2014).
TJP focused on the ?Constrained?task.The ?Constrained?
task only uses data provid-ed by the organizers.
That is, external resourcessuch as sentiment inventories (e.g.
Sentiwordnet(Esuli, and Sebastiani 2006) are excluded.
Theobjective of the TJP system was to use the resultsfor comparison with our previous experiment(Chalothorn and Ellman, 2013).
More details ofthese can be found in section 5.The TJP system was implemented using asupport vector machine (SVM, e.g.
Joachims,1999) with the addition of extensive pre-processing such as stopword removal, negation,slang, contraction,  and emoticon expansions.The remainder of this paper is constructed asfollows: firstly, related work is discussed in sec-tion 2; the methodology, the experiment and re-sults are presented in sections 3 and 4,657respectively.
Finally a discussion and futurework are given in section 5.2 Related WorkTwitter is a popular social networking and mi-croblogging site that allows users to post mes-sages of up to 140 characters; known as?Tweets?.
Tweets are extremely attractive to themarketing sector, since tweets may be searchedin real-time.
This means marketing can find cus-tomer sentiment (both positive and negative) farmore quickly than through the use of web pagesor traditional media.
Consequently analyzing thesentiment of tweets is currently active researchtask.The word 'emoticon' is a neologistic contrac-tion of 'emotional icon'.
It refers specifically tothe use of combinations of punctuation charac-ters to indicate sentiment in a text.
Well knownemoticons include :) to represent a happy face,and :( a sad one.
Emoticons allow writers toaugment the impact of limited texts (such as inSMS messages or tweets) using few characters.Read (2005) used emoticons from a trainingset downloaded from Usenet newsgroups as an-notations (positive and negative).
Using the ma-chine learning techniques of Na?ve Bayes andSVM, Read (2005) achieved up to 61.50 % and70.10%, accuracy respectively in determiningtext polarity from the emoticons used.Go et al.
(2009) used distant supervision toclassify sentiment of Twitter, similar to Read(2005).
Emoticons were used as noisy labels intraining data.
This allowed the performance ofsupervised learning (positive and negative) at adistance.
Three classifiers were used: Na?veBayes, Maximum   Entropy and SVM.
Theseclassifiers were able to   obtain more than81.30%, 80.50% and 82.20%, respectively accu-racy on their unigram testing data .Aramaki et al.
(2011) classified contexts onTwitter related to influenza using a SVM.
Thetraining data was annotated with the polarity la-bel by humans, whether they are positive or neg-ative.
The contexts will be labelled as positive ifthe contexts mention the user or someone closeto them has the flu, or if they mention a timewhen they caught the flu.
The results demon-strated that they obtained a 0.89 correction ratiofor their testing data against a gold standard.Finally, a well known paper by Bollen andMao (2011) identified a correlation between themovements of the Dow Jones stock marketindex, and prevailing sentiment as determinedfrom twitter's live feed.
This application hasprompted considerable work such as Makrehchiet al (2013) that has attempted to create success-ful trading strategies from sentiment analysis oftweets.These work both the wide ranging applica-tions of analysing twitter data, and theimportance of Sentiment Analysis.
We nowmove on to look at our approach to SemEval2014 task 9.3 Methodology3.1 CorpusThe training and development dataset ofSemEval was built using Tweets from more thanone thousand pieces of context.
The contextshave various features often used in Tweets, suchas emoticons, tags, usernames etc.
These featureswere extracted from the datasets before trainingfor the  supervised machine learning model.During initial pre-processing of the datasets,emoticons were labelled by matching with theemoticons that have been collect manually fromthe dataset.
Those labelled were matched againsta well-known collection of emoticons?.Subsequently, negative contractions?
wereexpanded in place and converted to full form(e.g.
don?t -> do not).
Moreover, the features of?http://en.wikipedia.org/wiki/List_of_emoticons?http://en.wikipedia.org/wiki/English_auxiliaries_and_contractions#Negative_contractions658twitters were also removed or replaced by wordssuch as twitter usernames, URLs and hashtags.A Twitter username is a unique name thatshows in the user's profile and may be used forboth authentication and identification.
This isshown by prefacing the username with an @symbol.
When a tweet is directed at an individualor particular entity this can be shown in the tweetby including @username.
For example a tweetdirected at ?tawunrat?
would include the text@tawunrat.
Before URLs are posted in twitterthey are shortened automatically to use the t.codomain whose modified URLs are at most 22characters.
However, both features have beenremoved from the datasets.
For the hashtags,they are used for represent keyword and topics intwitter by using # follow by words or phrasesuch as #newcastleuk.
This feature has been re-placed with the following word after # symbol.For example, #newcastleuk was replaced bynewcastleuk.Frequently repeated letters are used in tweetsfor emphasis.
These were reduced and replacedusing a simple regular expression by two of thesame character.
For example, happpppppy willbe replaced with happy, and coollllll will be re-placed by cooll.
Next, special character such as[,],{,},?,and !
were also removed.
Slang and con-tracted words were converted to their full form.E.g.
?fyi?
was converted to ?for your infor-mation?.
Finally, NLTK (Bird et al.
2009) stop-words such as ?a?, ?the?, etc., were removed fromthe datasets.3.2 ClassifierOur system uses the SVM classifier model(Hearst et al., 1998, Cristianini and Shawe-Taylor, 2000), which is based on SVM-light (Jo-achims, 1999).
SVM is a binary linear classifica-tion model with the learning algorithm forclassification and regression analyzing the dataand recognizing the pattern.Training SVMLight requires data to be for-mulated into vectors of attribute value pairs pre-ceded by a numeric value.
For example,<target>  <feature>:<value> <feature>:<value> ... <feature>:<value> #<info>Here, ?target?
represents the polarity of a sen-tence or tweet; ?feature?
refers to a term in thedocument, and ?value?
refers to a feature weight.This could be used as the relative frequency of aterm in the set of documents, or Tf-Idf.
Tf-idf isthe combination of term frequency (tf) and in-verse document frequency (idf), is a weight valueoften used in text mining and information re-trieval.
This weight is a statistical measure usedto evaluate the relative important of word in adocument in the collection (Manning et al.,2008).
(1)where           is the weighting the scheme assigns toterm   in documentTerm frequency (tf) is used to measure how fre-quent the term appears in the document.?
(2)where     is the number of term   appears in a document?
is the total number of terms   in the document  .Inverse document frequency (idf) is used tomeasure how important the term is ?
i.e.
whetherthe term is common or rare in the collection.
(3)where   is the total number of documents in the collectionin corpus.
is the number of documents   which termappears.Therefore, we chose to work with both of theseto observe which yielded the best results in thepolarity classification.659The default settings of SVMLight were usedthroughout.
This meant that we used a linearkernel that did not require any parameters.
?4 Experiment and ResultsIn our experiment, we used the datasets andevaluated the system using the F-score measure-ment.
During pre-processing features were ex-tracted from both datasets.
First, we used afrequency of word as a featured weight by calcu-lating the frequency of word in the dataset and,during pre-processing, we labelled the emotionsin both datasets.
The results revealed a lowerthan average F-score at 34.80%.
As this wasquite low we disregarded further use of term fre-quency as a feature weight.
We moved on to useTf-Idf as the feature weight and, again, emoti-cons in both datasets were labelled.
The score of78.10% was achieved.
Then, we kept the pre-possessing of the training set stable by combin-ing the features to extract from the testing data.These results are presented in Table 1**.The highest score of 81.96% was recordedwhen all the features were combined and extract-ed from both datasets.The lowest score of 36.48% was recordedwhen emoticons were extracted from testing dataand all features were extracted from training da-tasets.
The results of the highest scoring experi-ment were submitted to the task organizers.Following solution submissions, the task or-ganizers announced the scores by separating thedata into the following five groups: LiveJour-nal2014; SMS2013; Twitter2013; Twitter2014;and Twitter2014 Sarcasm.
This would allow theidentification of any domain dependent effects.However, the results showed that we achievedabove average in all the datasets, as illustrated inFigure 1.?Based on SVMLight**The results in the table are from the test set 2014 in task2A.5 Conclusion and Future WorkThe TJP system participated in SemEval 2014Task 9, Part A: Contextual Polarity Disambigua-tion.
The system exploited considerable pre-processing, before using the well known,SVMLight machine learning algorithm (Joa-chims.
1999).
The pre-processing used severaltwitter specific features, such as hashtags andids, in addition to more traditional InformationRetrieval concepts such as the Tf-Idf heuristic(Manning et al., 2008).
The results showed thatthe combination of all features in both datasetsachieved the best results, at 81.96%.An aspect of this contribution is the compara-tive analysis of feature effectiveness.
That is, weattempted to identify which factor(s) made themost significant improvement to system perfor-mance.
It is clear the pre-processing had a con-siderable effect on system performance.
The useof a different learning algorithm also contributedto performance since, on this task, SVMLightperformed better than the Naive Bayes algorithmthat was used by our team in 2013.Sentiment resources was not been used in oursystem in SemEval 2014 as same as in SemEval2013 whilst other user groups have employed avariety of resources of different sizes, and accu-racy (Wilson et al., 2013).
These points lead tothe following plan for future activities.Our future work is to rigorously investigatethe success factors for sentiment analysis, espe-cially in the twitter domain.
More specifically,we have formulated the following research ques-tions as a result of our participation in SemEval?
Are Sentiment resources essential for theSentiment Analysis task??
Can the accuracy and effectiveness ofsentiment lexicons be measured?
If so,which feature of the resource (accuracyvs.
coverage) is the most effective met-ric.?
Might it be more effective to use a rangeof sentiments (e.g.
[-1.0 .. 1.0]), rather660than binary approach(e.g.
positive andnegative) taken in SemEval 2013, and2014??
Is one machine learning algorithm suffi-cient, and if so which is it?
Or, alternate-ly would an ensemble approach (Rokach,2005) significantly improve perfor-mance?Table 1: The results of each feature analyzed in the approach of TF-IDFFigure 1: The comparison of TJP and average scoresReferencesAlec Go, Richa Bhayani and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervi-sion.
CS224N Project Report, Stanford, 1-12.Andrea Esuli, Fabrizio Sebastiani 2006"SENTIWORDNET: A Publicly Available Lexi-cal Resource for Opinion Mining" in Proceedingsof the 5th Conference on Language Resources andEvaluation, LREC (2006), pp.
417-422Andranik Tumasjan, Timm O. Sprenger, Philipp G.Sandner and Isabell M. Welpe.
2010.
"Predictingelections with twitter: What 140 characters revealabout political sentiment," in Proceedings of theFourth International AAAI Conference on Web-logs and Social Media, pp.
178-185.Christopher D. Manning, Prabhakar Raghavan andHinrich Sch?tze, Introduction to Information Re-trieval, Cambridge University Press.
2008.
ISBN:0521865719.David Nadeau, Catherine Sabourin, Joseph De Kon-inck, Stan Matwin and Peter D. Turney.
2006.661"Automatic dream sentiment analysis," presentedat the In: Proceedings of the Workshop on Compu-tational Aesthetics at the Twenty-First NationalConference on Artificial Intelligence, Boston,Massachussetts, USA.Dietmar Grabner, Markus Zanker, Gunther Fliedl andMatthias Fuchs.
2012.
"Classification of customerreviews based on sentiment analysis," presented atthe 19th Conference on Information and Commu-nication Technologies in Tourism (ENTER), Hel-singborg, Sweden.Eiji Aramaki, Sachiko Maskawa and Mizuki Morita.2011.
Twitter catches the flu: detecting influenzaepidemics using Twitter.
Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing.
Edinburgh, United Kingdom:Association for Computational Linguistics.Johan.
Bollen and Huina.
Mao.
Twitter mood as astock market predictor.
IEEE Computer,44(10):91?94.Jonathon Read.
2005.
Using emoticons to reduce de-pendency in machine learning techniques for sen-timent classification.
Proceedings of the ACLStudent Research Workshop.
Ann Arbor, Michi-gan: Association for Computational Linguistics.Lior Rokach.
2005.
Chapter 45 Ensemble Methods forClassifiers.
In: Oded Maimon and Lior Rokach(eds.)
Data Mining and Knowledge DiscoveryHandbook.
Springer US.Marti A. Hearst, Susan T. Dumais, Edgar Osman,John Platt and Bernhard Scholkopf .
1998.
Sup-port vector machines.
IEEE, Intelligent Systemsand their Applications, 13, 18-28.Masoud Makrehchi, Sameena Shah and Wenhui Liao.2013.
Stock Prediction Using Event-Based Senti-ment Analysis.
Web Intelligence (WI) and Intelli-gent Agent Technologies (IAT), 2013IEEE/WIC/ACM International Joint Conferenceson,.
337-342.Nello Cristianini and John Shawe-Taylor.
2000.
Anintroduction to support vector machines and otherkernel-based learning methods, Cambridge uni-versity press.Sara Rosenthal, Preslav Nakov, Alan Ritter andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
InternationalWorkshop on Semantic Evaluation (SemEval-2014).
Dublin, Ireland.Steven Bird, Ewan Klein and Edward Loper.
2009.NLTK: Natural language processing with Python,O'Reilly.Takeshi Sakaki, Makoto Okazaki and Yutaka Matsuo.2010.
Earthquake shakes Twitter users: real-timeevent detection by social sensors.
Proceedings ofthe 19th international conference on World wideweb.
Raleigh, North Carolina, USA: ACM.Tawunrat Chalothorn and Jeremy Ellman.
2013.
TJP:Using Twitter to Analyze the Polarity of Contexts.Second Joint Conference on Lexical and Compu-tational Semantics (*SEM), Volume 2: Proceed-ings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013).
Atlanta,Georgia, USA: Association for ComputationalLinguistics.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,Alan Ritter, Sara Rosenthal and Veselin Stoyanov.2013.
SemEval-2013 Task 2: Sentiment Analysisin Twitter.
Proceedings of the 7th InternationalWorkshop on Semantic Evaluation.
Associationfor Computational Linguistics.Thorsten Joachims.
1999.
Making large-scale supportvector machine learning practical.
Advances inkernel methods.
MIT Press.662
