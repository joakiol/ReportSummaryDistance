Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 213?216, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSemantic Role Labeling Using Lexical Statistical InformationSimone Paolo Ponzetto and Michael StrubeEML Research gGmbHSchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germanyhttp://www.eml-research.de/nlp/AbstractOur system for semantic role labeling ismulti-stage in nature, being based on treepruning techniques, statistical methods forlexicalised feature encoding, and a C4.5decision tree classifier.
We use both shal-low and deep syntactic information fromautomatically generated chunks and parsetrees, and develop a model for learningthe semantic arguments of predicates as amulti-class decision problem.
We evalu-ate the performance on a set of relatively?cheap?
features and report an F1 score of68.13% on the overall test set.1 IntroductionThis paper presents a system for the CoNLL 2005Semantic Role Labeling shared task (Carreras &Ma`rquez, 2005), which is based on the current re-lease of the English PropBank data (Palmer et al,2005).
For the 2005 edition of the shared task areavailable both syntactic and semantic information.Accordingly, we make use of both clausal, chunkand deep syntactic (tree structure) features, namedentity information, as well as statistical representa-tions for lexical item encoding.The set of features and their encoding reflect thenecessity of limiting the complexity and dimension-ality of the input space.
They also provide the clas-sifier with enough information.
We explore here theuse of a minimal set of compact features for seman-tic role prediction, and show that a feature-basedstatistical encoding of lexicalised features such aspredicates, head words, local contexts and PoS bymeans of probability distributions provides an effi-cient way of representing the data, with the featurevectors having a small dimensionality and allowingto abstract from single words.2 System description2.1 PreprocessingDuring preprocessing the predicates?
semantic argu-ments are mapped to the nodes in the parse trees, aset of hand-crafted shallow tree pruning rules are ap-plied, probability distributions for feature represen-tation are generated from training data1, and featurevectors are extracted.
Those are finally fed into theclassifier for semantic role classification.2.1.1 Tree node mapping of semanticarguments and named entitiesFollowing Gildea & Jurafsky (2002), (i) labelsmatching more than one constituent due to non-branching nodes are taken as labels of higher con-stituents, (ii) in cases of labels with no correspond-ing parse constituent, these are assigned to the par-tial match given by the constituent spanning theshortest portion of the sentence beginning at the la-bel?s span left boundary and lying entirely within it.We drop the role or named entity label if such suit-able constituent could not be found2.1All other processing steps assume a uniform treatment ofboth training and test data.2The percentage of roles for which no valid tree node couldbe found amounts to 3% for the training and 7% for the devel-opment set.
These results are compatible with the performanceof the employed parser (Collins, 1999).2132.1.2 Tree pruningThe tagged trees are further processed by applyingthe following pruning rules:?
All punctuation nodes are removed.
This is forremoving punctuation information, as well asfor aligning spans of the syntactic nodes withPropBank constituents3.?
If a node is unary branching and its daughter isalso unary branching, the daughter is removed.This allows to remove redundant nodes span-ning the same tokens in the sentence.?
If a node has only preterminal children, theseare removed.
This allows to internally collapsebase phrases such as base NPs.Tree pruning was carried out in order to reduce thenumber of nodes from which features were to be ex-tracted later.
This limits the number of candidateconstituents for role labeling, and removes redun-dant information produced by the pipeline of previ-ous components (i.e.
PoS tags of preterminal labels),as well as the sparseness and fragmentation of theinput data.
These simple rules reduce the numberof constituents given by the parser output by 38.4%on the training set, and by 38.7% on the develop-ment set, at the cost of limiting the coverage of thesystem by removing approximately 2% of the tar-get role labeled constituents.
On the developmentset, the number of constituents remaining on top ofpruning is 81,193 of which 7,558 are semantic ar-guments, with a performance upper-bound of 90.6%F1.2.1.3 FeaturesGiven the pruned tree structures, we traverse the treebottom-up left-to-right.
For each non-terminal nodewhose span does not overlap the predicate we extractthe following features:Phrase type: the syntactic category of the con-stituent (NP, PP, ADVP, etc.).
In order to reducethe number of phrase labels, we retained only3We noted during prototyping that in many cases no treenode fully matching a role constituent could be found, as thelatter did not include punctuation tokens, whereas in Collins?trees the punctuation terminals are included within the preced-ing phrases.
This precludes a priori the output to align to thegold standard PropBank annotation and we use therefore prun-ing as a recovery strategy.those labels which account for at least 0.1% ofthe overall available semantic arguments in thetraining data.
We replace the label for everyphrase type category below this threshold witha generic UNK label.
This reduces the numberof labels from 72 to 18.Position: the position of the constituent with re-spect to the target predicate (before or after).Adjacency: whether the right (if before) or left (ifafter) boundary of the constituent is adjacent,non-adjacent or inside the predicate?s chunk.Clause: whether the constituent belongs to theclause of the predicate or not.Proposition size: measures relative to the proposi-tion size, such as (i) the number of constituentsand (ii) predicates in the proposition.Constituent size: measures relative to the con-stituent size, namely (i) the number of tokensand (ii) subconstituents (viz., non-leaf rootedsubtrees) of the constituent.Predicate: the predicate lemma, represented as theprobability distribution P (r|p) of the predicatep of taking one of the available r semanticroles.
For unseen predicates we assume a uni-form distribution.Voice: whether the predicate is in active or passiveform.
Passive voice is identified if the predi-cate?s PoS tag is VBN and either it follows aform of to be or to get, or it does not belong toa VP chunk, or is immediately preceded by anNP chunk.Head word: the head word of the constituent,represented as the probability distributionP (r|hw) of the head word hw of heading aphrase filling one of the available r seman-tic roles.
For unseen words we back off on aphrasal model by using the probability distri-bution P (r|pt) of the phrase type pt of filling asemantic slot r.Head word PoS: the PoS of the head word of theconstituent, similarly represented as the proba-bility distribution P (r|pos) of a PoS pos of be-longing to a constituent filling one of the avail-able r semantic roles.Local lexical context: the words in the constituentother than the head word, represented as the214averaged probability distributions of each i-th non-head word wi of occurring in oneof the available r semantic roles, namely1m?mi=1 P (r|wi) for m non-head words in theconstituent.
For each unseen word we back offby using the probability distribution P (r|posi)of the PoS posi of filling a semantic role r4.Named entities: the label of the named entitywhich spans the same words as the constituent,as well as the label of the largest named en-tity embedded within the constituent.
Both val-ues are set to NULL if such labels could not befound.Path: the number of intervening NPB, NP, VP, VP-A, PP, PP-A, S, S-A and SBAR nodes along thepath from the constituent to the predicate.Distance: the distance from the target predicate,measured as (i) the number of nodes from theconstituent to the lowest node in the tree dom-inating both the constituent and the predicate,(ii) the number of nodes from the predicate tothe former common dominating node5, (iii) thenumber of chunks between the base phrase ofthe constituent?s head and the predicate chunk,(iv) the number of tokens between the head ofthe constituent and the predicate.2.2 ClassifierWe used the YaDT6 implementation of the C4.5 de-cision tree algorithm (Quinlan, 1993).
Parameterselection (99% pruning confidence, at least 10 in-stances per leaf node) was carried out by performing10-fold cross-validation on the development set.Data preprocessing and feature vector generationtook approximately 2.5 hours (training set, includingprobability distribution generation), 5 minutes (de-velopment) and 7 minutes (test) on a 2GHz Opteron4This feature was introduced as the information provided bylexical heads does not seem to suffice in many cases.
This isshown by head word ambiguities, such as LOC and TMP ar-guments occurring in similar prepositional syntactic configu-rations ?
i.e.
the preposition in, which can be head of bothAM-TMP and AM-LOC constituents, as in in October and inNew York.
The idea is therefore to look at the words in the con-stituents other than the head, and build up an overall constituentrepresentation, thus making use of statistical lexical informationfor role disambiguation.5These distance measures along the tree path between theconstituent and the predicate were kept separate, in order to in-directly include embedding level information into the model.6http://www.di.unipi.it/?ruggieri/software.htmldual processor server with 2GB memory7.
Trainingtime was of approximately 17 minutes.
The finalsystem was trained using all of the available trainingdata from sections 2?21 of the Penn TreeBank.
Thisamounts to 2,250,887 input constituents of which10% are non-NULL examples.
Interestingly, duringprototyping we first limited ourselves to training anddrawing probability distributions for feature repre-sentation from sections 15?18 only.
This yieldeda very low performance (57.23% F1, developmentset).
A substantial performance increase was givenby still training on sections 15?18, but using theprobability distributions generated from sections 2?21 (64.43% F1, development set).
This suggests thatthe system is only marginally sensitive to the train-ing dataset size, but pivotally relies on taking proba-bility distributions from a large amount of data.In order to make the task easier and overcome theuneven role class distribution, we limited the learnerto classify only those 16 roles accounting for at least0.5% of the total number of semantic arguments inthe training data8.2.3 Post-processingAs our system does not build an overall sen-tence contextual representation, it systematicallyproduced errors such as embedded role labeling.
Inparticular, since no embedding is observed for thesemantic arguments of predicates, in case of (multi-ple) embeddings the classifier output was automat-ically post-processed to retain only the largest em-bedding constituent.
Evaluation on the developmentset has shown that this does not significantly im-prove performance, still it provides a much more?sane?
output.
Besides, we make use of a simpletechnique for avoiding multiple A0 or A1 role as-signments within the same proposition, based onconstituent position and predicate voice.
In case ofmultiple A0 labels, if the predicate is in active form,the second A0 occurrence is replaced with A1, elsewe replace the first occurrence.
Similarly, in case ofmultiple A1 labels, if the predicate is in active form,the first A1 occurrence is replaced with A0, else we7We used only a single CPU at runtime, since the implemen-tation is not parallelised.8These include numbered arguments (A0 to A4), adjuncts(ADV, DIS, LOC, MNR, MOD, NEG, PNC, TMP), and references(R-A0 and R-A1).215Precision Recall F?=1Development 71.82% 61.60% 66.32Test WSJ 75.05% 64.81% 69.56Test Brown 66.69% 52.14% 58.52Test WSJ+Brown 74.02% 63.12% 68.13Test WSJ Precision Recall F?=1Overall 75.05% 64.81% 69.56A0 78.52% 72.52% 75.40A1 75.53% 65.39% 70.10A2 62.28% 52.07% 56.72A3 63.81% 38.73% 48.20A4 73.03% 63.73% 68.06A5 0.00% 0.00% 0.00AM-ADV 60.00% 42.69% 49.88AM-CAU 0.00% 0.00% 0.00AM-DIR 0.00% 0.00% 0.00AM-DIS 75.97% 73.12% 74.52AM-EXT 0.00% 0.00% 0.00AM-LOC 54.09% 47.38% 50.51AM-MNR 58.67% 46.22% 51.71AM-MOD 97.43% 96.37% 96.90AM-NEG 97.78% 95.65% 96.70AM-PNC 42.17% 30.43% 35.35AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 75.41% 71.11% 73.20R-A0 82.09% 73.66% 77.65R-A1 72.03% 66.03% 68.90R-A2 0.00% 0.00% 0.00R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 0.00% 0.00% 0.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 0.00% 0.00% 0.00R-AM-MNR 0.00% 0.00% 0.00R-AM-TMP 0.00% 0.00% 0.00V 98.63% 98.63% 98.63Table 1: Overall results (top) and detailed results onthe WSJ test (bottom).replace the second occurrence.3 ResultsTable 1 shows the results on the test set.
Problemsare inherently related with the skewed distribution ofrole classes, so that roles which have a limited num-ber of occurrences are harder to classify correctly.This explains the performance gap on the A0 andA1 roles on one hand, and the A2, A3, A4, AM- ar-guments on the other.One advantage of using a decision tree learningalgorithm is that it outputs a model which includes afeature ranking, since the most informative featuresare those close to the root of the tree.
In the presentcase, the most informative features were both dis-tance/position metrics (distance and adjacency) andlexicalized features (head word and predicate).4 ConclusionSemantic role labeling is a difficult task, and accord-ingly, how to achieve an accurate and robust perfor-mance is still an open question.
In our work weused a limited set of syntactic tree based distanceand size metrics coupled with raw lexical statistics,and showed that such ?lazy learning?
configurationcan still achieve a reasonable performance.We concentrated on reducing the complexitygiven by the number and dimensionality of the in-stances to be classified during learning.
This is thecore motivation behind performing tree pruning andstatistical feature encoding.
This also helped us toavoid the use of sparse features such as the explicitpath in the parse tree between the candidate con-stituent and the predicate, and the predicate?s sub-categorization rule (cf.
e.g.
Pradhan et al (2004)).Future work will concentrate on benchmarkingthis approach within alternative architectures (i.e.two-phase with filtering) and different learningschemes (i.e.
vector-based methods such as SupportVector Machines and Artificial Neural Networks).Acknowledgements: This work has been fundedby the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first author has been supported by a KTFgrant (09.003.2004).ReferencesCarreras, Xavier & Llu?
?s Ma`rquez (2005).
Introduction to theCoNLL-2005 Shared Task: Semantic Role Labeling.
In Pro-ceedings of CoNLL-2005.Collins, Michael (1999).
Head-driven statistical models for nat-ural language parsing, (Ph.D. thesis).
Philadelphia, Penn.,USA: University of Pennsylvania.Gildea, Daniel & Daniel Jurafsky (2002).
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3):245?288.Palmer, Martha, Dan Gildea & Paul Kingsbury (2005).
Theproposition bank: An annotated corpus of semantic roles.Computational Linguistics, 31(1):71?105.Pradhan, Sameer, Kadri Hacioglu, Valeri Krugler, Wayne Ward,James H. Martin & Daniel Jurafsky (2004).
Support vec-tor learning for semantic argument classification.
Journalof Machine Learning, Special issue on Speech and NaturalLanguage Processing.
To appear.Quinlan, J. Ross (1993).
C4.5: programs for machine learn-ing.
San Francisco, Cal., USA: Morgan Kaufmann Publish-ers Inc.216
