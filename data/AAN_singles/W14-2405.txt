Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 22?27,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsA Deep Architecture for Semantic ParsingEdward Grefenstette, Phil Blunsom, Nando de Freitas and Karl Moritz HermannDepartment of Computer ScienceUniversity of Oxford, UK{edwgre, pblunsom, nando, karher}@cs.ox.ac.ukAbstractMany successful approaches to semanticparsing build on top of the syntactic anal-ysis of text, and make use of distribu-tional representations or statistical mod-els to match parses to ontology-specificqueries.
This paper presents a novel deeplearning architecture which provides a se-mantic parsing system through the unionof two neural models of language se-mantics.
It allows for the generation ofontology-specific queries from natural lan-guage statements and questions withoutthe need for parsing, which makes it es-pecially suitable to grammatically mal-formed or syntactically atypical text, suchas tweets, as well as permitting the devel-opment of semantic parsers for resource-poor languages.1 IntroductionThe ubiquity of always-online computers in theform of smartphones, tablets, and notebooks hasboosted the demand for effective question answer-ing systems.
This is exemplified by the grow-ing popularity of products like Apple?s Siri orGoogle?s Google Now services.
In turn, this cre-ates the need for increasingly sophisticated meth-ods for semantic parsing.
Recent work (Artzi andZettlemoyer, 2013; Kwiatkowski et al., 2013; Ma-tuszek et al., 2012; Liang et al., 2011, inter alia)has answered this call by progressively movingaway from strictly rule-based semantic parsing, to-wards the use of distributed representations in con-junction with traditional grammatically-motivatedre-write rules.
This paper seeks to extend this lineof thinking to its logical conclusion, by provid-ing the first (to our knowledge) entirely distributedneural semantic generative parsing model.
It doesso by adapting deep learning methods from relatedwork in sentiment analysis (Socher et al., 2012;Hermann and Blunsom, 2013), document classifi-cation (Yih et al., 2011; Lauly et al., 2014; Her-mann and Blunsom, 2014a), frame-semantic pars-ing (Hermann et al., 2014), and machine trans-lation (Mikolov et al., 2010; Kalchbrenner andBlunsom, 2013a), inter alia, combining two em-pirically successful deep learning models to forma new architecture for semantic parsing.The structure of this short paper is as follows.We first provide a brief overview of the back-ground literature this model builds on in ?2.
In ?3,we begin by introducing two deep learning modelswith different aims, namely the joint learning ofembeddings in parallel corpora, and the generationof strings of a language conditioned on a latentvariable, respectively.
We then discuss how bothmodels can be combined and jointly trained toform a deep learning model supporting the gener-ation of knowledgebase queries from natural lan-guage questions.
Finally, in ?4 we conclude bydiscussing planned experiments and the data re-quirements to effectively train this model.2 BackgroundSemantic parsing describes a task within the largerfield of natural language understanding.
Withincomputational linguistics, semantic parsing is typ-ically understood to be the task of mapping nat-ural language sentences to formal representationsof their underlying meaning.
This semantic rep-resentation varies significantly depending on thetask context.
For instance, semantic parsing hasbeen applied to interpreting movement instruc-tions (Artzi and Zettlemoyer, 2013) or robot con-trol (Matuszek et al., 2012), where the underlyingrepresentation would consist of actions.Within the context of question answering?thefocus of this paper?semantic parsing typicallyaims to map natural language to database queriesthat would answer a given question.
Kwiatkowski22et al.
(2013) approach this problem using a multi-step model.
First, they use a CCG-like parserto convert natural language into an underspecifiedlogical form (ULF).
Second, the ULF is convertedinto a specified form (here a FreeBase query),which can be used to lookup the answer to thegiven natural language question.3 Model DescriptionWe describe a semantic-parsing model that learnsto derive quasi-logical database queries from nat-ural language.
The model follows the structure ofKwiatkowski et al.
(2013), but relies on a series ofneural networks and distributed representations inlieu of the CCG and ?-Calculus based representa-tions used in that paper.The model described here borrows heavily fromtwo approaches in the deep learning literature.First, a noise-contrastive neural network similar tothat of Hermann and Blunsom (2014a, 2014b) isused to learn a joint latent representation for nat-ural language and database queries (?3.1).
Sec-ond, we employ a structured conditional neurallanguage model in ?3.2 to generate queries givensuch latent representations.
Below we provide thenecessary background on these two components,before introducing the combined model and de-scribing its learning setup.3.1 Bilingual Compositional Sentence ModelsThe bilingual compositional sentence model(BiCVM) of Hermann and Blunsom (2014a) pro-vides a state-of-the-art method for learning se-mantically informative distributed representationsfor sentences of language pairs from parallel cor-pora.
Through the joint production of a shared la-tent representation for semantically aligned sen-tence pairs, it optimises sentence embeddingsso that the respective representations of dissim-ilar cross-lingual sentence pairs will be weaklyaligned, while those of similar sentence pairs willbe strongly aligned.
Both the ability to jointlylearn sentence embeddings, and to produce latentshared representations, will be relevant to our se-mantic parsing pipeline.The BiCVM model shown in Fig.
1 assumesvector composition functions g and h, which mapan ordered set of vectors (here, word embed-dings from DA,DB) onto a single vector in Rn.As stated above, for semantically equivalent sen-tences a, b across languages LA,LB, the modelaims to minimise the distance between these com-posed representations:Ebi(a, b) = ?g(a)?
h(b)?2In order to avoid strong alignment between dis-similar cross-lingual sentence pairs, this erroris combined with a noise-contrastive hinge loss,where n ?
LBis a randomly sampled sentence,dissimilar to the parallel pair {a, b}, and m de-notes some margin:Ehl(a, b, n) = [m+ Ebi(a, b)?
Ebi(a, n)]+,where [x]+= max(0, x).
The resulting objectivefunction is as followsJ(?)
=?
(a,b)?C(k?i=1Ehl(a, b, ni) +?2???2),with?2??
?2as the L2regularization term and?={g, h,DA,DB} as the set of model variables....L1 sentence embeddingL1 word embeddingsL2 sentence embeddingL2 word embeddingscontrastive estimationghFigure 1: Diagrammatic representation of aBiCVM.While Hermann and Blunsom (2014a) appliedthis model only to parallel corpora of sentences,it is important to note that the model is agnosticconcerning the inputs of functions g and h. In thispaper we will discuss how this model can be ap-plied to non-sentential inputs.233.2 Conditional Neural Language ModelsNeural language models (Bengio et al., 2006) pro-vide a distributed alternative to n-gram languagemodels, permitting the joint learning of a pre-diction function for the next word in a sequencegiven the distributed representations of a subsetof the last n?1 words alongside the representa-tions themselves.
Recent work in dialogue act la-belling (Kalchbrenner and Blunsom, 2013b) andin machine translation (Kalchbrenner and Blun-som, 2013a) has demonstrated that a particularkind of neural language model based on recurrentneural networks (Mikolov et al., 2010; Sutskeveret al., 2011) could be extended so that the nextword in a sequence is jointly generated by theword history and the distributed representation fora conditioning element, such as the dialogue classof a previous sentence, or the vector representationof a source sentence.
In this section, we briefly de-scribe a general formulation of conditional neurallanguage models, based on the log-bilinear mod-els of Mnih and Hinton (2007) due to their relativesimplicity.A log-bilinear language model is a neural net-work modelling a probability distribution over thenext word in a sequence given the previous n?1,i.e.
p(wn|w1:n?1).
Let |V | be the size of our vo-cabulary, and R be a |V | ?
d vocabulary matrixwhere the Rwidemnotes the row containing theword embedding in Rdof a word wi, with d be-ing a hyper-parameter indicating embedding size.Let Cibe the context transform matrix in Rd?dwhich modifies the representation of the ith wordin the word history.
Let bwibe a scalar bias as-sociated with a word wi, and bRbe a bias vectorin Rdassociated with the model.
A log-bilinearmodel expressed the probability of wngiven a his-tory of n?1 words as a function of the energy ofthe network:E(wn;w1:n?1) =?(n?1?i=1RTwiCi)Rwn?
bTRRwn?
bwnFrom this, the probability distribution over thenext word is obtained:p(wn|w1:n?1) =e?E(wn;w1:n?1)?wne?E(wn;w1:n?1)To reframe a log-bilinear language model as aconditional language model (CNLM), illustrated?wnwn-1wn-2wn-3Figure 2: Diagrammatic representation of a Con-ditional Neural Language Model.in Fig.
2, let us suppose that we wish to jointlycondition the next word on its history and somevariable ?, for which an embedding r?has beenobtained through a previous step, in order to com-pute p(wn|w1:n?1, ?).
The simplest way to do thisadditively, which allows us to treat the contribu-tion of the embedding for ?
as similar to that of anextra word in the history.
We define a new energyfunction:E(wn;w1:n?1, ?)
=?
((n?1?i=1RTwiCi)+ rT?C?)Rwn?
bTRRwn?
bwnto obtain the probabilityp(wn|w1:n?1, ?)
=e?E(wn;w1:n?1,?)?wne?E(wn;w1:n?1,?
)Log-bilinear language models and their condi-tional variants alike are typically trained by max-imising the log-probability of observed sequences.3.3 A Combined Semantic Parsing ModelThe models in ?
?3.1?3.2 can be combined to forma model capable of jointly learning a shared la-tent representation for question/query pairs usinga BiCVM, and using this latent representation tolearn a conditional log-bilinear CNLM.
The fullmodel is shown in Fig.
3.
Here, we explain thefinal model architecture both for training and forsubsequent use as a generative model.
The detailsof the training procedure will be discussed in ?3.4.The combination is fairly straightforward, andhappens in two steps at training time.
For the24...Knowledgebase queryQuestionLatentrepresentationQuery embeddingQuestion embeddingRelation/objectembeddingsWord embeddingsConditionalLog-bilinearLanguage ModelghFigure 3: Diagrammatic representation of the fullmodel.
First the mappings for obtaining latentforms of questions and queries are jointly learnedthrough a BiCVM.
The latent form for questionsthen serves as conditioning element in a log-bilinear CNLM.first step, shown in the left hand side of Fig.
3,a BiCVM is trained against a parallel corporaof natural language question and knowledgebasequery pairs.
Optionally, the embeddings for thequery symbol representations and question wordsare initialised and/or fine-tuned during training,as discussed in ?3.4.
For the natural languageside of the model, the composition function g canbe a simple additive model as in Hermann andBlunsom (2014a), although the semantic informa-tion required for the task proposed here wouldprobably benefit from a more complex composi-tion function such as a convolution neural net-work.
Function h, which maps the knowledgebasequeries into the shared space could also rely onconvolution, although the structure of the databasequeries might favour a setup relying primarily onbi-gram composition.Using function g and the original training data,the training data for the second stage is createdby obtaining the latent representation for the ques-tions of the original dataset.
We thereby obtainpairs of aligned latent question representations andknowledgebase queries.
This data allows us totrain a log-bilinear CNLM as shown on the rightside of Fig.
3.Once trained, the models can be fully joined toproduce a generative neural network as shown inFig.
4.
The network modelling g from the BiCVM...QuestionGenerated QuerygFigure 4: Diagrammatic representation of the finalnetwork.
The question-compositional segment ofthe BiCVM produces a latent representation, con-ditioning a CNLM generating a query.takes the distributed representations of questionwords from unseen questions, and produces a la-tent representation.
The latent representation isthen passed to the log-bilinear CNLM, which con-ditionally generates a knowledgebase query corre-sponding to the question.3.4 Learning Model ParametersWe propose training the model of ?3.3 in a twostage process, in line with the symbolic model ofKwiatkowski et al.
(2013).First, a BiCVM is trained on a parallel corpusC of question-query pairs ?Q,R?
?
C, using com-position functions g for natural language questionsand h for database queries.
While functions g andhmay differ from those discussed in Hermann andBlunsom (2014a), the basic noise-contrastive op-timisation function remains the same.
It is possi-ble to initialise the model fully randomly, in which25case the model parameters ?
learned at this stageinclude the two distributed representation lexicafor questions and queries, DQand DRrespec-tively, as well as all parameters for g and h.Alternatively, word embeddings inDQcould beinitialised with representations learned separately,for instance with a neural language model or asimilar system (Mikolov et al., 2010; Turian et al.,2010; Collobert et al., 2011, inter alia).
Likewise,the relation and object embeddings inDRcould beinitialised with representations learned from dis-tributed relation extraction schemas such as thatof Riedel et al.
(2013).Having learned representations for queries inDRas well as function g, the second training phaseof the model uses a new parallel corpus consistingof pairs ?g(Q), R?
?
C?to train the CNLM as pre-sented in ?3.3.The two training steps can be applied iteratively,and further, it is trivial to modify the learningprocedure to use composition function h as an-other input for the CNLM training phrase in anautoencoder-like setup.4 Experimental Requirements andFurther WorkThe particular training procedure for the modeldescribed in this paper requires aligned ques-tion/knowledgebase query pairs.
There exist somesmall corpora that could be used for this task(Zelle and Mooney, 1996; Cai and Yates, 2013).
Inorder to scale training beyond these small corpora,we hypothesise that larger amounts of (potentiallynoisy) training data could be obtained using aboot-strapping technique similar to Kwiatkowskiet al.
(2013).To evaluate this model, we will follow the ex-perimental setup of Kwiatkowski et al.
(2013).With the provisio that the model can generatefreebase queries correctly, further work will seekto determine whether this architecture can gener-ate other structured formal language expressions,such as lambda expressions for use in textual en-tailement tasks.AcknowledgementsThis work was supported by a Xerox FoundationAward, EPSRC grants number EP/I03808X/1 andEP/K036580/1, and the Canadian Institute for Ad-vanced Research (CIFAR) Program on AdaptivePerception and Neural Computation.ReferencesYoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mappinginstructions to actions.
Transactions of the Associa-tion for Computational Linguistics, 1(1):49?62.Yoshua Bengio, Holger Schwenk, Jean-S?ebastienSen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.Springer.Qingqing Cai and Alexander Yates.
2013.
Large-scaleSemantic Parsing via Schema Matching and Lexi-con Extension.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL).Ronan Collobert, Jason Weston, Leon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Karl Moritz Hermann and Phil Blunsom.
2013.
TheRole of Syntax in Vector Space Models of Composi-tional Semantics.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), Sofia, Bulgaria,August.
Association for Computational Linguistics.Karl Moritz Hermann and Phil Blunsom.
2014a.
Mul-tilingual Distributed Representations without WordAlignment.
In Proceedings of the 2nd InternationalConference on Learning Representations, Banff,Canada, April.Karl Moritz Hermann and Phil Blunsom.
2014b.
Mul-tilingual Models for Compositional DistributionalSemantics.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), Baltimore, USA,June.
Association for Computational Linguistics.Karl Moritz Hermann, Dipanjan Das, Jason Weston,and Kuzman Ganchev.
2014.
Semantic Frame Iden-tification with Distributed Word Representations.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), Baltimore, USA, June.
Associationfor Computational Linguistics.Nal Kalchbrenner and Phil Blunsom.
2013a.
Re-current continuous translation models.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing (EMNLP), Seattle,USA.
Association for Computational Linguistics.Nal Kalchbrenner and Phil Blunsom.
2013b.
Re-current convolutional neural networks for discoursecompositionality.
arXiv preprint arXiv:1306.3584.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings of26the 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1545?1556, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Stanislas Lauly, Alex Boulanger, and Hugo Larochelle.2014.
Learning multilingual word representa-tions using a bag-of-words autoencoder.
CoRR,abs/1401.1803.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies - Volume 1, HLT ?11,pages 590?599, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Cynthia Matuszek, Nicholas FitzGerald, Luke S.Zettlemoyer, Liefeng Bo, and Dieter Fox.
2012.
Ajoint model of language and perception for groundedattribute learning.
In Proceedings of the 29th Inter-national Conference on Machine Learning, ICML2012, Edinburgh, Scotland, UK, June 26 - July 1,2012.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of the 24th international conferenceon Machine learning, pages 641?648.
ACM.Sebastian Riedel, Limin Yao, Benjamin M. Marlin, andAndrew McCallum.
2013.
Relation extraction withmatrix factorization and universal schemas.
In JointHuman Language Technology Conference/AnnualMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics (HLT-NAACL?13), June.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InProceedings of EMNLP-CoNLL, pages 1201?1211.Ilya Sutskever, James Martens, and Geoffrey E Hin-ton.
2011.
Generating text with recurrent neuralnetworks.
In Proceedings of the 28th InternationalConference on Machine Learning (ICML-11), pages1017?1024.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings ofACL, Stroudsburg, PA, USA.Wen-Tau Yih, Kristina Toutanova, John C. Platt, andChristopher Meek.
2011.
Learning Discrimina-tive Projections for Text Similarity Measures.
InProceedings of the Fifteenth Conference on Compu-tational Natural Language Learning, CoNLL ?11,pages 247?256, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the National Con-ference on Artificial Intelligence, pages 1050?1055.27
