DECIS ION LISTS FOR LEX ICAL  AMBIGUITYRESOLUTION:Appl icat ion to Accent Restorat ionin Spanish and FrenchDavid Yarowsky*Depar tment  of Computer  and  In format ion  Sc ienceUn ivers i ty  of Pennsy lvan iaPh i lade lph ia ,  PA  19104yarowsky?unagi, cis.
upenn, eduAbst ractThis paper presents a statistical decision procedure forlexical ambiguity resolution.
The algorithm exploitsboth local syntactic patterns and more distant collo-cational evidence, generating an efficient, effective, andhighly perspicuous recipe for resolving a given ambigu-ity.
By identifying and utilizing only the single best dis-ambiguating evidence in a target context, the algorithmavoids the problematic omplex modeling of statisticaldependencies.
Although directly applicable to a wideclass of ambiguities, the algorithm is described and eval-uated in a realistic case study, the problem of restoringmissing accents in Spanish and French text.
Currentaccuracy exceeds 99% on the full task, and typically isover 90% for even the most difficult ambiguities.INTRODUCTIONThis paper presents a general-purpose tatistical deci-sion procedure for lexical ambiguity resolution based ondecision lists (Rivest, 1987).
The algorithm considersmultiple types of evidence in the context of an ambigu-ous word, exploiting differences in collocational distri-bution as measured by log-likelihoods.
Unlike standardBayesian approaches, however, it does not combine thelog-likelihoods of all available pieces of contextual evi-dence, but bases its classifications solely on the singlemost reliable piece of evidence identified in the targetcontext.
Perhaps surprisingly, this strategy appears toyield the same or even slightly better precision thanthe combination of evidence approach when trained onthe same features.
It also brings with it several ad-ditional advantages, the greatest of which is the abil-ity to include multiple, highly non-independent sourcesof evidence without complex modeling of dependencies.Some other advantages are significant simplicity andease of implementation, transparent understandability*This research was supported by an NDSEG Fellowship,ARPA grant N00014-90-J-1863 and ARO grant DAAL 03-89-C0031 PRI.
The author is also affiliated with the Lin-guistics Research Department of AT&T Bell Laboratories,and greatly appreciates the use of its resources in supportof this work.
He would like to thank Jason Eisner, LibbyLevison, Mark Liberman, Mitch Marcus, Joseph Rosenzweigand Mark Zeren for their valuable feedback.of the resulting decision list, and easy adaptability tonew domains.
The particular domain chosen here as acase study is the problem of restoring missing accents 1to Spanish and French text.
Because it requires the res-olution of both semantic and syntactic ambiguity, andoffers an objective ground truth for automatic evalua-tion, it is particularly well suited for demonstrating andtesting the capabilities of the given algorithm.
It is alsoa practical problem with immediate application.PROBLEM DESCRIPT IONThe general problem considered here is the resolu-tion of lexical ambiguity, both syntactic and seman-tic, based on properties of the surrounding context.Accent restoration is merely an instance of a closely-related class of problems including word-sense disam-biguation, word choice selection in machine translation,homograph and homophone disambiguation, and capi-talization restoration.
The given algorithm may be usedto solve each of these problems, and has been appliedwithout modification to the case of homograph disam-biguation in speech synthesis (Sproat, Hirschberg andYarowsky, 1992).It may not be immediately apparent o the readerwhy this set of problems forms a natural class, similarin origin and solvable by a single type of algorithm.
Ineach case it is necessary to disambiguate two or moresemantically distinct word-forms which have been con-flated into the same representation i  some medium.In the prototypical instance of this class, word-sense disambiguation, such distinct semantic onceptsas river bank, financial bank and to bank an airplane areconflated in ordinary text.
Word associations and syn-tactic patterns are sufficient o identify and label thecorrect form.
In homophone disambiguation, distinctsemantic oncepts uch as ceiling and sealing have alsobecome represented by the same ambiguous form, butin the medium of speech and with similar disambiguat-ing clues.Capitalization restoration is a similar problem in thatdistinct semantic oncepts uch as AIDS/aids (diseaseor helpful tools) and Bush~bush (president or shrub)1For brevity, the term accent will typically refer to thegeneral class of accents and other diacritics, including $,$,$,588are ambiguous, but in the medium of all-capitalized (orcasefree) text, which includes titles and the beginningof sentences.
Note that what was once just a capital-ization ambiguity between Prolog (computer language)and prolog (introduction) has is becoming a "sense" am-biguity since the computer language is now often writ-ten in lower case, indicating the fundamental similarityof these problems.Accent restoration involves lexical ambiguity, suchas between the concepts cSle (coast) and cSld (side),in textual mediums where accents are missing.
It istraditional in Spanish and French for diacritics to beomitted from capitalized letters.
This is particularly aproblem in all-capitalized text such as headlines.
Ac-cents in on-line text may also be systematically strippedby many computational processes which are not 8-bitclean (such as some e-mail transmissions), and may beroutinely omitted by Spanish and French typists in in-formal computer correspondence.Missing accents may create both semantic and syn-tactic ambiguities, including tense or mood distinctionswhich may only be resolved by distant temporal mark-ers or non-syntactic ues.
The most common accentambiguity in Spanish is between the endings -o and-5, such as in the case of completo vs. complet6.
Thisis a present/preterite nse ambiguity for nearly all-at verbs, and very often also a part of speech ambi-guity, as the -o form is a frequently a noun as well.The second most common general ambiguity is betweenthe past-subjunctive and future tenses of nearly al l -atverbs (eg: terminara vs. lerminard), both of whichare 3rd person singular forms.
This is a particularlychallenging class and is not readily amenable to tradi-tional part-of-speech tagging algorithms uch as localtrigram-based taggers.
Some purely semantic ambigui-ties include the nouns secretaria (secretary) vs. secre-tarla (secretariat), sabana (grassland) vs. sdbana (bedsheet), and politica (female politician) vs. polilica (pol-itics).
The distribution of ambiguity types in French issimilar.
The most common case is between -e and -d,which is both a past participle/present tense ambigu-ity, and often a part-of-speech ambiguity (with nounsand adjectives) as well.
Purely semantic ambiguities aremore common than in Spanish, and include traitd/traile(treaty/draft), marche/raarchd (step/market), and thecole example mentioned above.Accent restoration provides several advantages as acase study for the explication and evaluation of the pro-posed decision-list algorithm.
First, as noted above, itoffers a broad spectrum of ambiguity types, both syn-tactic and semantic, and shows the ability of the algo-rithm to handle these diverse problems.
Second, thecorrect accent pattern is directly recoverable: unlim-ited quantities of test material may be constructed bystripping the accents from correctly-accented text andthen using the original as a fully objective standardfor automatic evaluation.
By contrast, in traditionalword-sense disambiguation, hand-labeling training andtest data is a laborious and subjective task.
Third, thetask of restoring missing accents and resolving ambigu-ous forms shows considerable commercial applicability,both as a stand-alone application or part of the front-end to NLP systems.
There is also a large potentialcommercial market in its use in grammar and spellingcorrectors, and in aids for inserting the proper diacrit-ics automatically when one types 2.
Thus while accentrestoration may not be be the prototypical member ofthe class of lexical-ambiguity resolution problems, it isan especially useful one for describing and evaluating aproposed solution to this class of problems.PREVIOUS WORKThe problem of accent restoration in text has receivedminimal coverage in the literature, especially in En-glish, despite its many interesting aspects.
Most workin this area appears to done in the form of in-houseor commercial software, so for the most part the prob-lem and its potential solutions are without comprehen-sive published analysis.
The best treatment I've discov-ered is from Fernand Marly (1986, 1992), who for morethan a decade has been painstakingly crafting a systemwhich includes accent restoration as part of a compre-hensive system of syntactic, morphological nd phoneticanalysis, with an intended application in French text-to-speech synthesis.
He incorporates information ex-tracted from several French dictionaries and uses basiccollocational nd syntactic evidence in hand-built rulesand heuristics.
While the scope and complexity of thiseffort is remarkable, this paper will focus on a solutionto the problem which requires considerably less effortto implement.The scope of work in lexical ambiguity resolution isvery large.
Thus in the interest of space, discussionwill focus on the direct historic precursors and sourcesof inspiration for the approach presented here.
Thecentral tradition from which it emerges is that of theBayesian classifier (Mosteller and Wallace, 1964).
Thiswas expanded upon by (Gale et al, 1992), and in aclass-based variant by (Yarowsky, 1992).
Decision trees(Brown, 1991) have been usefully applied to word-senseambiguities, and HMM part-of-speech taggers (Jelinek1985, Church 1988, Merialdo 1990) have addressed thesyntactic ambiguities presented here.
Hearst (1991)presented an effective approach to modeling local con-textual evidence, while Resnik (1993) gave a classictreatment of the use of word classes in selectional con-straints.
An algorithm for combining syntactic and se-mantic evidence in lexical ambiguity resolution has beenrealized in (Chang et al, 1992).
A particularly success-ful algorithm for integrating a wide diversity of evidencetypes using error driven learning was presented in Brill(1993).
While it has been applied primarily to syntac-tic problems, it shows tremendous promise for equallyimpressive results in the area of semantic ambiguity res-olution.2Such a tool would particularly useful for typing Spanishor French on Anglo-centric omputer keyboards, where en-tering accents and other diacritic marks every few keystrokescan be laborious.89The formal model of decision lists was presented in(Pdvest, 1987).
I have restricted feature conjuncts to amuch narrower complexity than allowed in the originalmode l -  namely to word and class trigrams.
The currentapproach was initiMly presented in (Sproat et al, 1992),applied to the problem of homograph resolution in text-to-speech synthesis.
The algorithm achieved 97% meanaccuracy on a disambiguation task involving a sampleof 13 homographs 3.ALGORITHMStep  1: Ident i fy  the Ambiguit ies in AccentPatternMost words in Spanish and French exhibit only one ac-cent pattern.
Basic corpus analysis will indicate whichis the most common pattern for each word, and may beused in conjunction with or independent of dictionariesand other lexical resources.The initial step is to take a histogram of a corpus withaccents and diacritics retained, and compute a table ofaccent pattern distributions as follows:De-accented Form Accent Patterncesse cessecessdcout cofitcoutacoutecofitacofit6cofitecote c6t~c6tecotecot6cotiere c6ti~re% Number53% 66947% 593100% 330100% 4153% 10747% 9669% 264528% 10403% 99<1% 15100% 296For words with multiple accent patterns, steps 2-5are applied.Step 2: Collect Training ContextsFor a particular case of accent ambiguity identifiedabove, collect 4-k words of context around all occur-rences in the corpus, label the concordance line withthe observed accent pattern, and then strip the accentsfrom the data.
This will yield a training set such as thefollowing:Pattern Context(1) c6td du laisser de cote faute de temps(1) c6td appeler l' autre cote de l' atlantique(1) c6td passe de notre cote de la frontiere(2) cSte vivre sur notre cote ouest toujours verte(2) c6te creer sur la cote du labrador des(2) cSte travaillaient cote a cote , ils avaientThe training corpora used in this experiment were theSpanish AP Newswire (1991-1993, 49 million words),SBaseline accuracy for this data (using the most commonpronunciation) is 67%.the French Canadian Hansards (1986-1988, 19 millionwords), and a collection from Le Monde (1 millionwords).Step 3: Measure Collocational DistributionsThe driving force behind this disambiguation Mgorithmis the uneven distribution of collocations 4 with respectto the ambiguous token being classified.
Certain collo-cations will indicate one accent pattern, while differentcollocations will tend to indicate another.
The goal ofthis stage of the algorithm is to measure a large num-ber of collocational distributions to select those whichare most useful in identifying the accent pattern of theambiguous word.The following are the initial types of collocations con-sidered:?
Word immediately to the right (+1 W)?
Word immediately to the left (-1 W)?
Word found in =t=k word window 5 (+k W)?
Pair of words at offsets -2 and -1?
Pair of words at offsets -1 and +1?
Pair of words at offsets +1 and +2For the two major accent patterns of the French wordcote, below is a small sample of these distributions forseveral types of collocations:Position-1 w+ lw+lw,+2w-2w,-lw+k w+k w+k wCollocation c6te cSt~du cote 0 536la cote 766 1un cote 0 216notre cote 10 70cole ouest 288 1cole est 174 3cote du 55 156cote du gouvernement 0 62cote a cole 23 0poisson (in +k words) 20 0ports (in =t=k words) 22 0opposition (in +k words ) 0 39This core set of evidence presupposes no language-specific knowledge.
However, if additional language re-sources are available, it may be desirable to include alarger feature set.
For example, if lemmatization proce-dures are available, collocational measures for morpho-logical roots will tend to yield more succinct and gener-alizable evidence than measuring the distributions foreach of the inflected forms.
If part-of-speech informa-tion is available in a lexicon, it is useful to compute the4The term collocation is used here in its broad sense,meaning words appearing adjacent o or near each other(literally, in the same location), and does not imply onlyidiomatic or non-compositional associations.SThe optimal value of k is sensitive to the type of ambi-guity.
Semantic or topic-based ambiguities warrant a largerwindow (k ~ 20-50), while more local syntactic ambiguitieswarrant a smaller window (k ~ 3 or 4)90distributions for part-of-speech bigrams and trigramsas above.
Note that it's not necessary to determine theactual parts-of-speech of words in context; using onlythe most likely part of speech or a set of all possibil-ities will produce adequate, if somewhat diluted, dis-tributional evidence.
Similarly, it is useful to computecollocational statistics for arbitrary word classes, suchas the class WEEKDAY ----( domingo, lunes, martes, ... }.Such classes may cover many types of associations, andneed not be mutually exclusive.For the French experiments, no additional inguisticknowledge or lexical resources were used.
The decisionlists were trained solely on raw word associations with-out additional patterns based on part of speech, mor-phological analysis or word class.
Hence the reportedperformance is representative of what may be achievedwith a rapid, inexpensive implementation based strictlyon the distributional properties of raw text.For the Spanish experiments, a richer set of evidencewas utilized.
Use of a morphological analyzer (devel-oped by Tzoukermann and Liberman (1990)) alloweddistributional measures to be computed for associationsof lemmas (morphological roots), improving general-ization to different inflected forms not observed in thetraining data.
Also, a basic lexicon with possible partsof speech (augmented by the morphological analyzer)allowed adjacent part-of-speech sequences to be usedas disambiguating evidence.
A relatively coarse level ofanalysis (e.g.
NOUN, ADJECTIVE, SUBJECT-PRONOUN,ARTICLE, etc.
), augmented with independently mod-eled features representing ender, person, and num-ber, was found to be most effective.
However, whena word was listed with multiple parts-of-speech, no rel-ative frequency distribution was available.
Such wordswere given a part-of-speech tag consisting of the unionof the possibilities (eg ADJECTIVE-NOUN), as in Ku-piec (1989).
Thus sequences of pure part-of-speech tagswere highly reliable, while the potential sources of noisewere isolated and modeled separately.
In addition, sev-eral word classes such as WEEKDAY and MONTH weredefined, primarily focusing on time words because somany accent ambiguities involve tense distinctions.To build a full part of speech tagger for Spanish wouldbe quite costly (and require special tagged corpora).The current approach uses just the information avail-able in dictionaries, exploiting only that which is usefulfor the accent restoration task.
Were dictionaries notavailable, a productive approximation could have beenmade using the associational distributions of suffixes(such as -aba, -aste, -amos) which are often satisfactoryindicators of part of speech in morphologically rich lan-guages uch as Spanish.The use of the word-class and part-of-speech data isillustrated below, with the example of distinguishingterminara/terminard (a subjunctive/future t nse am-biguity):CollocationPREPOSITION que ~erminarade que terminarapara que terminaraNOUN que terminaracarrera que terminarareunion que terminaraacuerdo que terminaraque terminaraWEEKDAY (within ?k words)domingo (within ?k words) 0viernes (within ?k  words) 0S tep  4: Sor t  by  Log-L ike l ihoodDecis ion L is tste rmin -  te r in in -a ra  ar~31 015 014 00 130 30 20 242 370 23104intoThe next step is to compute the ratio called the log-likelihood:A .
.
.
.
P r (Accent_Pat tern l  \[Collocationi) ,~ostLogt ~ ~ j~The collocations most strongly indicative of a partic-ular pattern will have the largest log-likelihood.
Sortingby this value will list the strongest and most reliable ev-idence first 6.Evidence sorted in the above manner will yield a deci-sion list like the following, highly abbreviated exampleT:LogL Evidence Classification8.28t7.24t7.146.876.645.82t5.45PREPOSITION que terminara ~ terminarade que terminara ==~ terminarapara que terminara ==~ terminaray terminara =:~ terminar?WEEKDAY (within ?k words) ::~ terminar?NOUN que terminara ==~ terminar?domingo (within ?k words) ==~ terminar?The resulting decision list is used to classify new ex-amples by identifying the highest line in the list thatmatches the given context and returning the indicatedSProblems arise when an observed count is 0.
Clearlythe probability of seeing c~td in the context of poisson isnot 0, even though no such collocation was observed in thetraining data.
Finding a more accurate probability estimatedepends on several factors, including the size of the train-ing sample, nature of the collocation (adjacent bigrams orwider context), our prior expectation about the similarityof contexts, and the amount of noise in the training data.Several smoothing methods have been explored here, includ-ing those discussed in (Gale et al, 1992).
In one technique,all observed istributions with the same 0-denominator rawfrequency ratio (such as 2/0) are taken collectively, the av-erage agreement rate of these distributions with additionalheld-out training data is measured, and from this a morerealistic estimate of the likelihood ratio (e.g.
1.8/0.2) iscomputed.
However, in the simplest implementation, satis-factory results may be achieved by adding a small constanta to the numerator and denominator, where c~ is selectedempirically to optimize classification performance.
For thisdata, relatively small a (between 0.1 and 0.25) tended to beeffective, while noisier training data warrant larger a.rEntries marked with t are pruned in Step 5, below.91classification.
See Step 7 for a full description of thisprocess.S tep  5: Opt iona l  P run ing  and  In terpo la t ionA potentially useful optional procedure is the interpo-lation of log-likelihood ratios between those computedfrom the full data set (the globalprobabilities) and thosecomputed from the residual training data left at a givenpoint in the decision list when all higher-ranked pat-terns failed to match (i.e.
the residual probabilities).The residual probabilities are more relevant, but sincethe size of the residual training data shrinks at eachlevel in the list, they are often much more poorly es-t imated (and in many cases there may be no relevantdata left in the residual on which to compute the dis-tribution of accent patterns for a given collocation).
Incontrast, the global probabilities are better estimatedbut less relevant.
A reasonable compromise is to inter-polate between the two, where the interpolated estimateis/3 ?
global + 7 ?
residual.
When the residual proba-bilities are based on a large training set and are well es-t imated, 7 should dominate, while in cases the relevantresidual is small or non-existent, /3 should dominate.If always/3 = 0 and 3' = 1 (exclusive use of the resid-ual), the result is a degenerate (strictly right-branching)decision tree with severe sparse data problems.
Alter-nately, if one assumes that likelihood ratios for a givencollocation are functionally equivalent at each line of adecision list, then one could exclusively use the global(always/3 = 1 and 3' = 0).
This is clearly the easiestand fastest approach, as probability distributions donot need to be recomputed as the list is constructed.Which approach is best?
Using only the global proa-bilities does surprisingly well, and the results cited hereare based on this readily replicatable procedure.
Thereason is grounded in the strong tendency of a word toexhibit only one sense or accent pattern per collocation(discussed in Step 6 and (Yarowsky, 1993)).
Most clas-sifications are based on a x vs. 0 distribution, and whilethe magnitude of the log-likelihood ratios may decreasein the residual, they rarely change sign.
There are caseswhere this does happen and it appears that some inter-polation helps, but for this problem the relatively smalldifference in performance does not seem to justify thegreatly increased computational cost.Two kinds of optional pruning can also increase theefficiency of the decision lists.
The first handles theproblem of "redundancy by subsumption," which isclearly visible in the example decision lists above (inWEEKDAY and domingo).
When lemmas and word-classes precede their member words in the list, the latterwill be ignored and can be pruned.
If  a bigram is un-ambiguous, probability distributions for dependent tri-grams will not even be generated, since they will provideno additional information.The second, pruning in a cross-validation phase, com-pensates for the minimM observed over-modeling of thedata.
Once a decision list is built it is applied to its owntraining set plus some held-out cross-validation data(not the test data).
Lines in the list which contributeto more incorrect classifications than correct ones areremoved.
This also indirectly handles problems thatmay result from the omission of the interpolation step.If space is at a premium, lines which are never used inthe cross-validation step may also be pruned.
However,useful information is lost here, and words pruned in thisway may have contributed to the classification of test-ing examples.
A 3% drop in performance is observed,but an over 90% reduction in space is realized.
The op-t imum pruning strategy is subject to cost-benefit anal-ysis.
In the results reported below, all pruning exceptthis final space-saving step was utilized.S tep  6: T ra in  Dec is ion  L i s ts  fo r  Genera lC lasses  o f  Ambigu i tyFor many similar types of ambiguities, uch as the Span-ish subjunctive/future distinction between -ara andard, the decision lists for individual cases will be quitesimilar and use the same basic evidence for the classifi-cation (such as presence of nearby time adverbials).
Itis useful to build a general decision list for all -ara/ardambiguities.
This also tends to improve performanceon words for which there is inadequate training datato build a full individual decision lists.
The processfor building this general class disambiguator is basicallyidentical to that described in Steps 2-5 above, exceptthat in Step 2, training contexts are pooled for all in-dividual instances of the class (such as all -ara/-ardambiguities).
It is important o give each individual -ara word roughly equal representation i the trainingset, however, lest the list model the idiosyncrasies ofthe most frequent class members, rather than identifythe shared common features representative of the fullclass.In Spanish, decision lists are trained for the generalambiguity classes including -o/-6, -e/-d, -ara/-ard, and-aran/-ardn.
For each ambiguous word belonginging toone of these classes, the accuracy of the word-specificdecision list is compared with the class-based list.
If theclass's list performs adequately it is used.
Words withidiosyncrasies that are not modeled well by the class'slist retain their own word-specific decision list.S tep  7: Us ing  the  Dec is ion  L i s tsOnce these decision lists have been created, they maybe used in real time to determine the accent pattern forambiguous words in new contexts.At run time, each word encountered in a text islooked up in a table.
If the accent pattern is unam-biguous, as determined in Step 1, the correct patternis printed.
Ambiguous words have a table of the pos-sible accent patterns and a pointer to a decision list,either for that specific word or its ambiguity class (asdetermined in Step 6).
This given list is searched forthe highest ranking match in the word's context, anda classification umber is returned, indicating the mostlikely of the word's accent patterns given the context s .Slf all entries in a decision list fail to match in a par-ticular new context, a final entry called DEFAULT is used;92From a statistical perspective, the evidence at the topof this list will most reliably disambiguate the targetword.
Given a word in a new context o be assigned anaccent pattern, if we may only base the classificationon a single line in the decision list, it should be thehighest ranking pattern that is present in the targetcontext.
This is uncontroversial, nd is solidly based inBayesian decision theory.The question, however, is what to do with the less-reliable evidence that may also be present in the targetcontext.
The common tradition is to combine the avail-able evidence in a weighted sum or product.
This isdone by Bayesian classifiers, neural nets, IR-based clas-sifiers and N-gram part-of-speech taggers.
The systemreported here is unusual in that it does no such combi-nation.
Only the single most reliable piece of evidencematched in the target context is used.
For example, ina context of cote containing poisson, ports and allan-tique, if the adjacent feminine article la cote (the coast)is present, only this best evidence is used and the sup-porting semantic information ignored.
Note that if themasculine article le cote (the side) were present in a sim-ilar maritime context, the most reliable evidence (gen-der agreement) would override the semantic lues whichwould otherwise dominate if all evidence was combined.If no gender agreement constraint were present in thatcontext, the first matching semantic evidence would beused.There are several motivations for this approach.
Thefirst is that combining all available vidence rarely pro-duces a different classification than just using the singlemost reliable evidence, and when these differ it is aslikely to hurt as to help.
In a study comparing resultsfor 20 words in a binary homograph disambiguationtask, based strictly on words in local (4-4 word) con-text, the following differences were observed between analgorithm taking the single best evidence, and an other-wise identical algorithm combining all available match-ing evidence: 9Combin ing  vs. Not  Combin ing  Probab i l i t iesAgree - Both classifications correct 92%Both classifications incorrect 6%Disagree - Single best evidence correct 1.3%Combined evidence correct 0.7%Total - 100%Of course that this behavior does not hold for allclassification tasks, but does seem to be characteristicof lexically-based word classifications.
This may be ex-plained by the empirical observation that in most cases,and with high probability, words exhibit only one sensein a given collocation (Yarowsky, 1993).
Thus for thistype of ambiguity resolution, there is no apparent detri-ment, and some apparent performance gain, from us-it indicates the most likely accent pattern in cases wherenothing matches.9In cases of disagreement, using the single best evidenceoutperforms the combination of evidence 65% to 35%.
Thisobserved ifference is 1.9 standard eviations greater thanexpected by chance and is statistically significant.ing only the single most reliable evidence in a classifi-cation.
There are other advantages as well, includingrun-time fficiency and ease of parallelization.
However,the greatest gain comes from the ability to incorporatemultiple, non-independent i formation types in the de-cision procedure.
As noted above, a given word in con-text (such as Castillos) may match several times in thedecision list, once for its parts of speech, \]emma, capi-talized and capitalization-free forms, and possible word-classes as well.
By only using one of these matches, thegross exaggeration of probability from combining all ofthese non-independent log-likelihoods i avoided.
Whilethese dependencies may be modeled and corrected forin Bayesian formalisms, it is difficult and costly to doso.
Using only one log-likelihood ratio without combi-nation frees the algorithm to include a wide spectrum ofhighly non-independent i formation without additionalalgorithmic omplexity or performance loss.EVALUATIONBecause we have only stripped accents artificially fortesting purposes, and the "correct" patterns exist on-line in the original corpus, we can evaluate perfor-mance objectively and automatically.
This contrastswith other classification tasks such as word-sense dis-ambiguation and part-of-speech tagging, where at somepoint human judgements are required.
Regrettably,however, there are errors in the original corpus, whichcan be quite substantial depending on the type of ac-cent.
For example, in the Spanish data, accents overthe i (1) are frequently omitted; in a sample test 3.7%of the appropriate i accents were missing.
Thus the fol-lowing results must be interpreted as agreement rateswith the corpus accent pattern; the true percent correctmay be several percentage points higher.The following table gives a breakdown of the differ-ent types of Spanish accent ambiguities, their relativefrequency in the training corpus, and the algorithm'sperformance on each: 1?Summary  of  Per formance on Spanish:Ambiguous Cases (18% of tokens):Type Freq.
Agreement Prior-o/-5 81% 98 % 86%-ara/-ard,-aran/-ardn 4 % 92 % 84%Function Words 13 % 98 % 94%Other 2 % 97 % 95%Total 98 % 93% "Unambiguous Cases (82% of tokens):\] I 100 % \] 100%Overall Performance: I I 99.6 % I 98.7%As observed before, the prior probabilities in favor ofthe most common accent pattern are highly skewed, soone does reasonably well at this task by always usingthe most common pattern.
But the error rate is still1?The term prioris a measure of the baseline performanceone would expect if the algorithm always chose the mostcommon option.93roughly 1 per every 75 words, which is unacceptablyhigh.
This algorithm reduces that error rate by over65%.
However, to get a better picture of the algorithm'sperformance, the following table gives a breakdown ofresults for a random set of the most problematic ases- words exhibiting the largest absolute number of thenon-majority accent patterns.
Collectively they consti-tute the most common potential sources of error.Per fo rmance  on Ind iv idua lSpanish:Pattern 1anuncioregistromarcocompletoretiroduropasoregaloterminarallegaradejeganePattern 2anunci5registr6marc6complet6retir6dur6pas6regal6terminar?llegar~dej6gan6secretaria secretariaseriahaciaestamiserfahaciaest~mlAmbigu i t iesF rench :cessed6cid6laissecommencec6t~trait~cessed6cidelaiss6commenc6c6tetraiteAgrmnt Prior N98.4% 57% 945998.4% 60% 259698.2% 52% 206998.1% 54% 170197.5% 56% 371396.8% 52% 146696.4% 50% 638390.7% 56% 28082.9% 59% 21878.4% 64% 86089.1% 68% 31380.7% 60% 27984.5% 52% 106597.7% 93% 106597.3% 91% 248397.1% 61% 1414093.7% 82% 122197.7% 53% 126296.5% 64% 366795.5% 50% 262495.2% 54% 210598.1% 69% 389395.6% 71% 2865Evaluation is based on the corpora described in thealgorithm's Step 2.
In all experiments, 4/5 of the datawas used for training and the remaining 1/5 held outfor testing.
More accurate measures of algorithm per-formance were obtained by repeating each experiment5 times, using a different 1/5 of the data for each test,and averaging the results.
Note that in every experi-ment, results were measured on independent test datanot seen in the training phase.It should be emphasized that the actual percent cor-rect is higher than these agreement figures, due to errorsin the original corpus.
The relatively low agreementrate on words with accented i's (1) is a result of this.To study this discrepancy further, a human judge fluentin Spanish determined whether the corpus or decisionlist algorithm was correct in two cases of disagreement.For the ambiguity case of mi/ml, the corpus was incor-rect in 46% of the disputed tokens.
For the ambiguityanuncio/anunciS, the corpus was incorrect in 56% ofthe disputed tokens.
I hope to obtain a more reliablesource of test material.
However, it does appear thatin some cases the system's precision may rival that ofthe AP Newswire's Spanish writers and translators.DISCUSSIONThe algorithm presented here has several advantageswhich make it suitable for general exical disambigua-tion tasks that require integrating both semantic andsyntactic distinctions.
The incorporation of word (andoptionally part-of-speech) trigrams allows the modelingof many local syntactic onstraints, while colloeationalevidence in a wider context allows for more semanticdistinctions.
A key advantage of this approach is thatit allows the use of multiple, highly non-independent ev-idence types (such as root form, inflected form, part ofspeech, thesaurus category or application-specific clus-ters) and does so in a way that avoids the complexmodeling of statistical dependencies.
This allows thedecision lists to find the level of representation that bestmatches the observed probability distributions.
It is akitchen-sink approach of the best kind - throw in manytypes of potentially relevant features and watch whatfloats to the top.
While there are certainly other waysto combine such evidence, this approach as many ad-vantages.
In particular, precision seems to be at least asgood as that achieved with Bayesian methods appliedto the same evidence.
This is not surprising, given theobservation i  (Leacock et al, 1993) that widely diver-gent sense-disambiguation algorithms tend to performroughly the same given the same evidence.
The distin-guishing criteria therefore become:?
How readily can new and multiple types of evidencebe incorporated into the algorithm??
How easy is the output to understand??
Can the resulting decision procedure be easily editedby hand??
Is it simple to implement and replicate, and can it beapplied quickly to new domains?The current algorithm rates very highly on all thesestandards of evaluation, especially relative to some ofthe impenetrable black boxes produced by many ma-chine learning algorithms.
Its output is highly perspicu-ous: the resulting decision list is organized like a recipe,with the most useful evidence first and in highly read-able form.
The generated ecision procedure is alsoeasy to augment by hand, changing or adding patternsto the list.
The algorithm is also extremely flexible - itis quite straightforward touse any new feature for whicha probability distribution can be calculated.
This is aconsiderable strength relative to other algorithms whichare more constrained in their ability to handle diversetypes of evidence.
In a comparative study (Yarowsky,1994), the decision list algorithm outperformed bothan N-Gram tagger and Bayesian classifier primarily be-cause it could effectively integrate a wider range ofavailable vidence types than its competitors.
Althougha part-of-speech tagger exploiting gender and numberagreement might resolve many accent ambiguities, uchconstraints will fail to apply in many cases and are dif-ficult to apply generally, given the the problem of iden-tifying agreement relationships.
It would also be atconsiderable cost, as good taggers or parsers typically94involve several person-years of development, plus oftenexpensive proprietary lexicons and hand-tagged train-ing corpora.
In contrast, the current algorithm couldbe applied quite quickly and cheaply to this problem.
Itwas originally developed for homograph disambiguationin text-to-speech synthesis (Sproat et al, 1992), andwas applied to the problem of accent restoration withvirtually no modifications in the code.
It was applied toa new language, French, in a matter of days and with nospecial exical resources or linguistic knowledge, basingits performance upon a strictly self-organizing analysisof the distributional properties of French text.
The flex-ibility and generality of the algorithm and its potentialfeature set makes it readily applicable to other prob-lems of recovering lost information from text corpora; Iam currently pursuing its application to such problemsas capitalization restoration and the task of recoveringvowels in Hebrew text.CONCLUSIONThis paper has presented a general-purpose algorithmfor lexical ambiguity resolution that is perspicuous,easy to implement, flexible and applied quickly to newdomains.
It incorporates class-based models at sev-eral levels, and while it requires no special lexical re-sources or linguistic knowledge, it effectively and trans-parently incorporates those which are available.
It suc-cessfully integrates part-of-speech patterns with localand longer-distance ollocational information to resolveboth semantic and syntactic ambiguities.
Finally, al-though the case study of accent restoration in Spanishand French was chosen for its diversity of ambiguitytypes and plentiful source of data for fully automaticand objective valuation, the algorithm solves a worth-while problem in its own right with promising commer-cial potential.Re ferences\[1\] Brill, Eric, "A Corpus-Based Approach to LanguageLearning," Ph.D. Thesis, University of Pennsylvania,1993.\[2\] Brown, Peter, Stephen Della Pietra, Vincent DellaPietra, and Robert Mercer, "Word Sense Disam-biguation using Statistical Methods," Proceedings ofthe 29th Annual Meeting of the Association for Com-putational Linguistics, pp.
264-270, 1991.\[3\] Chang, Jing-Shin, Yin-Fen Luo and Keh-YihSu, "GPSM: A Generalized Probabilistie SemanticModel for Ambiguity Resolution," in Proceedings ofthe 30th Annual Meeting of the Association for Com-putational Linguistics, pp.
177-184, 1992.\[4\] Church, K.W., "A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text," in Pro-ceedings of the Second Conference on Applied NaturalLanguage Processing, ACL, 136-143, 1988.\[5\] Gale, W., K. Church, and D. Yarowsky, "A Methodfor Disambiguating Word Senses in a Large Corpus,"Computers and the Humanities, 26,415-439, 1992.\[6\] Hearst, Marti, "Noun Homograph DisambiguationUsing Local Context in Large Text Corpora," in Us-ing Corpora, University of Waterloo, Waterloo, On-tario, 1991.\[7\] Jelinek, F., "Markov Source Modeling of TextGeneration," in Impact of Processing Techniqueson Communication, J. Skwirzinski, ed., Dordrecht,1985.\[8\] Kupiec, Julian, "Probabilistic Models of Shortand Long Distance Word Dependencies in RunningText," in Proceedings, DARPA Speech and Natu-ral Language Workshop, Philadelphia, February, pp.290-295, 1989.\[9\] Leacock, Claudia, Geoffrey Towell and EllenVoorhees "Corpus-Based Statistical Sense Resolu-tion," in Proceedings, ARPA Human Language Tech-nology Workshop, 1993.\[10\] Marty, Fernand, "Trois syst~mes informatiquesde transcription phon~tique t graph@mique", in LeFrangais Moderne, pp.
179-197, 1992.\[11\] Marty, F. and R.S.
Hart, "Computer Program toTranscribe French Text into Speech: Problems andSuggested Solutions", Technical Report No.
LLL-T-6-85.
Language Learning Laboratory; University ofIllinois.
Urbana, Illinois, 1985.\[12\] Merialdo, B., 'Tagging Text with a ProbabilisticModel," in Proceedings of the IBM Natural LanguageITL, Paris, France, pp.
161-172, 1990.\[13\] Mosteller, Frederick, and David Wallace, Inferenceand Disputed Authorship: The Federalist, Addison-Wesley, Reading, Massachusetts, 1964.\[14\] Resnik, Philip, "Selection and Information: AClass-Based Approach to Lexical Relationships,"Ph.D. Thesis, University of Pennsylvania, 1993.\[15\] Rivest, R. L., "Learning Decision Lists," in Ma-chine Learning, 2, 229-246, 1987.\[16\] Sproat, Richard, Julia Hirschberg and DavidYarowsky "A Corpus-based Synthesizer," in Proceed-ings, International Conference on Spoken LanguageProcessing, Banff, Alberta, October 1992.\[17\] Tzoukermann, Evelyne and Mark Liberman, " AFinite-state Morphological Processor for Spanish," inProceedings, COLING-90, Helsinki, 1990.\[18\] Yarowsky, David, "Word-Sense Disambigua-tion Using Statistical Models of Roget's Cate-gories Trained on Large Corpora," in Proceedings,COLING-92, Nantes, France, 1992.\[19\] Yarowsky, David, "One Sense Per Collocation,"in Proceedings, ARPA Human Language TechnologyWorkshop, Princeton, 1993.\[20\] Yarowsky, David, "A Comparison of Corpus-basedTechniques for Restoring Accents in Spanish andFrench Text," to appear in Proceedings, 2nd An-nual Workshop on Very Large Text Corpora, Kyoto,Japan, 1994.95
