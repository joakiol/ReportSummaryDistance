Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1224?1234,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsExploiting Multiple Sources for Open-domain Hypernym DiscoveryRuiji Fu, Bing Qin, Ting Liu?Research Center for Social Computing and Information RetrievalSchool of Computer Science and TechnologyHarbin Institute of Technology, China{rjfu, bqin, tliu}@ir.hit.edu.cnAbstractHypernym discovery aims to extract suchnoun pairs that one noun is a hypernym ofthe other.
Most previous methods are basedon lexical patterns but perform badly on open-domain data.
Other work extracts hypernymrelations from encyclopedias but has limitedcoverage.
This paper proposes a simple yet ef-fective distant supervision framework for Chi-nese open-domain hypernym discovery.
Giv-en an entity name, we try to discover its hy-pernyms by leveraging knowledge from mul-tiple sources, i.e., search engine results, ency-clopedias, and morphology of the entity name.First, we extract candidate hypernyms fromthe above sources.
Then, we apply a statisticalranking model to select correct hypernyms.
Aset of novel features is proposed for the rank-ing model.
We also present a heuristic strate-gy to build a large-scale noisy training data forthe model without human annotation.
Exper-imental results demonstrate that our approachoutperforms the state-of-the-art methods on amanually labeled test dataset.1 IntroductionHypernym discovery is a task to extract such nounpairs that one noun is a hypernym of the other (S-now et al 2005).
A noun H is a hypernym of an-other noun E if E is an instance or subclass of H. Inother word, H is a semantic class of E. For instance,?actor?
is a hypernym of ?Mel Gibson?
; ?dog?
is ahypernym of ?Caucasian sheepdog?
; ?medicine?
isa hypernym of ?Aspirin?.
Hypernym discovery isan important subtask of semantic relation extraction?Email correspondence.and has many applications in ontology construction(Suchanek et al 2008), machine reading (Etzion-i et al 2006), question answering (McNamee et al2008), and so on.Some manually constructed thesauri such asWordNet can also provide some semantic relationssuch as hypernyms.
However, these thesauri are lim-ited in its scope and domain, and manual construc-tion is knowledge-intensive and time-consuming.Therefore, many researchers try to automatically ex-tract semantic relations or to construct taxonomies.Most previous methods on automatic hypernymdiscovery are based on lexical patterns and sufferfrom the problem that such patterns can only cov-er a small part of complex linguistic circumstances(Hearst, 1992; Turney et al 2003; Zhang et al2011).
Other work tries to extract hypernym rela-tions from large-scale encyclopedias like Wikipediaand achieves high precision (Suchanek et al 2008;Hoffart et al 2012).
However, the coverage is limit-ed since there exist many infrequent and new entitiesthat are missing in encyclopedias (Lin et al 2012).We made similar observation that more than a halfof entities in our data set have no entries in the en-cyclopedias.This paper proposes a simple yet effective distan-t supervision framework for Chinese open-domainhypernym discovery.
Given an entity name, our goalis to discover its hypernyms by leveraging knowl-edge from multiple sources.
Considering the casewhere a person wants to know the meaning of an un-known entity, he/she may search it in a search engineand then finds out the answer after going through thesearch results.
Furthermore, if he/she finds an en-try about the entity in an authentic web site, such asWikipedia, the information will help him/her under-1224stand the entity.
Also, the morphology of the enti-ty name can provide supplementary information.
Inthis paper, we imitate the process.
The evidencesfrom the above sources are integrated in our hyper-nym discovery model.Our approach is composed of two major steps:hypernym candidate extraction and ranking.
In thefirst step, we collect hypernym candidates from mul-tiple sources.
Given an entity name, we search it ina search engine and extract high-frequency nouns asits main candidate hypernyms from the search re-sults.
We also collect the category tags for the entityfrom two Chinese encyclopedias and the head wordof the entity as the candidates.In the second step, we identify correct hypernymsfrom the candidates.
We view this task as a rank-ing problem and propose a set of effective featuresto build a statistical ranking model.
For the param-eter learning of the model, we also present a heuris-tic strategy to build a large-scale noisy training datawithout human annotation.Our contributions are as follows:?
We are the first to discover hypernym for Chi-nese open-domain entities by exploiting mul-tiple sources.
The evidences from differentsources can authenticate and complement eachother to improve both precision and recall.?
We manually annotate a dataset containing1,879 Chinese entities and their hypernyms,which will be made publicly available.
To thebest of our knowledge, this is the first datasetfor Chinese hypernyms.?
We propose a set of novel and effective fea-tures for hypernym ranking.
Experimental re-sults show that our method achieves the bestperformance.Furthermore, our approach can be easily portedfrom Chinese to English and other languages, exceptthat a few language dependent features need to bechanged.The remainder of the paper is organized as fol-lows: Section 2 discusses the related work.
Section3 introduces our method in detail.
Section 4 de-scribes the experimental setup.
Section 5 shows theexperimental results.
Conclusion and future workare presented in Section 6.2 Related WorkPrevious methods for hypernym discovery can besummarized into two major categories, i.e., pattern-based methods and encyclopedia-based methods.Pattern-based methods make use of manuallyor automatically constructed patterns to mine hyper-nym relations from text corpora.
The pioneer workby Hearst (1992) finds that linking two noun phras-es (NPs) via certain lexical constructions often im-plies hypernym relations.
For example, NP1 is a hy-pernym of NP2 in the lexical pattern ?such NP1 asNP2?.
Similarly, succeeding researchers follow herwork and use handcrafted patterns to extract hyper-nym pairs from corpora (Caraballo, 1999; Scott andDominic, 2003; Ciaramita and Johnson, 2003; Tur-ney et al 2003; Pasca, 2004; Etzioni et al 2005;Ritter et al 2009; Zhang et al 2011).Evans (2004) considers the web data as a largecorpus and uses search engines to identify hyper-nyms based on lexical patterns.
Given an arbitrarydocument, he takes each capitalized word sequenceas an entity and aims to find its potential hypernymsthrough pattern-based web searching.
Suppose X isa capitalized word sequence.
Some pattern querieslike ?such as X?
are threw into the search engine.Then, in the retrieved documents, the nouns that im-mediately precede the pattern are recognized as thehypernyms of X.
This work is most related to ours.However, the patterns used in his work are too strictto cover many low-frequency entities, and our ex-periments show the weakness of the method.Snow et al(2005) for the first time propose to au-tomatically extract large numbers of lexico-syntacticpatterns and then detect hypernym relations froma large newswire corpus.
First, they use someknown hypernym-hyponym pairs from WordNet asseeds and collect many patterns from a syntactical-ly parsed corpus in a bootstrapping way.
Then, theyconsider all noun pairs in the same sentence as po-tential hypernym-hyponym pairs and use a statisticalclassifier to recognize the correct ones.
All patternscorresponding to the noun pairs in the corpus arefed into the classifier as features.
Their method re-lies on accurate syntactic parsers and it is difficult toguarantee the quality of the automatically extractedpatterns.
Our experiments show that their method isinferior to ours.1225Encyclopedia-based methods extract hyper-nym relations from encyclopedias like Wikipedia(Suchanek et al 2008; Hoffart et al 2012).
Theuser-labeled information in encyclopedias, such ascategory tags in Wikipedia, is often used to derivehypernym relations.In the construction of the famous ontology YA-GO, Suchanek et al(2008) consider the title of eachWikipedia page as an entity and the correspondingcategory tags as its potential hypernyms.
They ap-ply a shallow semantic parser and some rules to dis-tinguish the correct hypernyms.
Heuristically, theyfind that if the head of the category tag is a pluralword, the tag is most likely to be a correct hyper-nym.
However, this method cannot be used in Chi-nese because of the lack of plurality information.The method of Suchanek et al(2008) cannot han-dle the case when the entity is absent in Wikipedia.To solve this problem, Lin et al(2012) connect theabsent entities with the entities present in Wikipediasharing common contexts.
They utilize the Freebasesemantic types to label the present entities and thenpropagate the types to the absent entities.
The Free-base contains most of entities in Wikipedia and as-signs them semantic types defined in advance.
Butthere are no such resources in Chinese.Compared with previous work, our approach triesto identify hypernyms from multiple sources.
Theevidences from different sources can authenticateand complement each other to improve both preci-sion and recall.
Our experimental results show theeffectiveness of our method.3 MethodOur method is composed of two steps.
First, we col-lect candidate hypernyms from multiple sources fora given entity.
Then, a statistical model is built forhypernym ranking based on a set of effective fea-tures.
Besides, we also present a heuristic strategyto build a large-scale training data.3.1 Candidate Hypernym Collection fromMultiple SourcesIn this work, we collect potential hypernyms fromfour sources, i.e., search engine results, two ency-clopedias, and morphology of the entity name.We count the co-occurrence frequency betweenthe target entities and other words in the returnedsnippets and titles, and select top N nouns (or nounphrases) as the main candidates.
As the experimentsshow, this method can find at least one hypernymfor 86.91% entities when N equals 10 (see Section5.1).
This roughly explains why people often can in-fer semantic meaning of unknown entities after go-ing through several search results.Furthermore, the user-generated encyclopediacategory tags are important clues if the entity exist-s in a encyclopedia.
Thus we add these tags intothe candidates.
In this work, we consider two Chi-nese encyclopedias, Baidubaike and Hudongbaike1,as hypernym sources.In addition, the head words of entities are alsotheir hypernyms sometimes.
For example, the headword of ?
?2? (Emperor Penguin)?
indicatesthat it?s a kind of ?
? (penguins)?.
Thus we puthead words into the hypernym candidates.
In Chi-nese, head words are often laid after their modifiers.Therefore, we try to segment a given entity.
If it canbe segmented and the last word is a noun, we takethe last word as the head word.
In our data set, thehead words of 41.35% entities are real hypernyms(see Section 5.1).We combine all of these hypernym candidates to-gether as the input of the second stage.
The finalcoverage rate reaches 93.24%.3.2 Hypernym RankingAfter getting the candidate hypernyms, we thenadopt a ranking model to determine the correct hy-pernym.
In this section, we propose several effectivefeatures for the model.
The model needs training da-ta for learning how to rank the data in addition toparameter setting.
Considering that manually anno-tating a large-scale hypernym dataset is costly andtime-consuming, we present a heuristic strategy tocollect training data.
We compare three hypernymranking models on this data set, including SupportVector Machine (SVM) with a linear kernel, SVMwith a radial basis function (RBF) kernel and Logis-tic Regression (LR).1Baidubaike (http://baike.baidu.com) andHudongbaike (http://www.baike.com) are two largestChinese encyclopedias containing more than 6.26 million and7.87 million entries respectively, while Chinese Wikipediacontains about 0.72 million entries until September, 2013.1226Feature Comment Value RangePrior the prior probability of a candidate being a potential hypernym [0, 1]Is Tagwhether a candidate is a category tag in the encyclopediapage of the entity if it exists0 or 1Is Head whether a candidate is the head word of the entity 0 or 1In Titlessome binary features based on the frequency of occurrence ofa candidate in the document titles in the search results0 or 1Synonymsthe ratio of the synonyms of the candidate in the candidatelist of the entity[0, 1]Radicalsthe ratio of the radicals of characters in a candidate matchedwith the last character of the entity[0, 1]Source Num the number of sources where the candidate is extracted 1, 2, 3, or 4Lexicon the hypernym candidate itself and its head word 0 or 1Table 1: The features for ranking3.2.1 Features for RankingThe features for hypernym ranking are shown inTable 1.
We illustrate them in detail in the following.Hypernym Prior: Intuitively, different wordshave different probabilities as hypernyms of someother words.
Some are more probable as hypernyms,such as animal, plant and fruit.
Some other wordssuch as sun, nature and alias, are not usually usedas hypernyms.
Thus we use a prior probability toexpress this phenomenon.
The assumption is that ifthe more frequent that a noun appears as categorytags, the more likely it is a hypernym.
We extractcategory tags from 2.4 million pages in Baidubaike,and compute the prior probabilities prior(w) for aword w being a potential hypernym using Equation1.
countCT (w) denotes the times a word appearedas a category tag in the encyclopedia pages.prior(w) =countCT (w)?w?
countCT (w?
)(1)In Titles: When we enter a query into a searchengine, the engine returns a search result list, whichcontains document titles and their snippet text.
Thedistributions of hypernyms and non-hypernyms in ti-tles are compared with that in snippets respectivelyin our training data.
We discover that the averagefrequency of occurrence of hypernyms in titles is15.60 while this number of non-hypernyms is only5.18, while the difference in snippets is very small(Table 2).
Thus the frequency of candidates in titlescan be used as features.
In this work the frequencyAvg.
Frequency intitles snippetsHypernym 15.60 33.69Non-Hypernym 5.18 30.61Table 2: Distributions of candidate hypernyms in titlesand snippetsis divided into three cases: greater than 15.60, lessthan 5.18, and between 5.18 and 15.60.
Three binaryfeatures are used to represent these cases.Synonyms: If there exist synonyms of a candi-date hypernym in the candidate list, the candidate isprobably correct answer.
For example, when ???(medicine)?
and ???
(medicine)?
both appear inthe candidate list of an entity, the entity is probablya kind of medicine.
We get synonyms of a candidatefrom a Chinese semantic thesaurus ?
Tongyi Cilin(Extended) (CilinE for short)2 and compute the s-core as a feature using Equation 2.ratiosyn(h, le) =countsyn(h, le)len(le)(2)Given a hypernym candidate h of an entity e andthe list of all candidates le, we compute the ratio ofthe synonyms of h in le.
countsyn(h, le) denotes thecount of the synonyms of h in le.
len(le) is the totalcount of candidates.2CilinE contains synonym and hypernym relations among77 thousand words, which is manually organized as a hierarchyof five levels.1227Radicals: Chinese characters are a form ofideogram.
By far, the bulk of Chinese characterswere created by linking together a character with arelated meaning and another character to indicate itspronunciation.
The character with a related meaningis called radical.
Sometimes, it is a important clue toindicate the semantic class of the whole character.For example, the radical ???
means insects, so ithints ?|n (dragonfly)?
is a kind of insects.
Simi-larly ???
hints ?nJ (lymphoma)?
is a kind ofdiseases.
Thus we use radicals as a feature the valueof which is computed by using Equation 3.radical(e, h) =countRM (e, h)len(h)(3)Here radical(e, h) denotes the ratio of charactersradical-matched with the last character of the entitye in the hypernym h. countRM (e, h) denotes thecount of the radical-matched characters in h. len(h)denotes the total count of the characters in h.3.2.2 Training Data CollectionNow training data is imperative to learn theweights of the features in Section 3.2.1.
Hence, wepropose a heuristic strategy to collect training datafrom encyclopedias.Firstly, we extract a number of open-domain enti-ties from encyclopedias randomly.
Then their hyper-nym candidates are collected by using the methodproposed in Section 3.1.
We select positive traininginstances following two principles:?
Principle 1: Among the four sources used forcandidate collection, the more sources fromwhich the hypernym candidate is extracted, themore likely it is a correct one.?
Principle 2: The higher the prior of the candi-date being a hypernym is, the more likely it is acorrect one.We select the best candidates following Principle1 and then select the best one in them as a positiveinstance following Principle 2.
And we select a can-didate as a negative training instance when it is fromonly one source and its prior is the lowest.
If thereare synonyms of training instances in the candidateslist, the synonyms are also extended into the trainingset.Domain# of entitiesDev.
TestBiology 72 351Health Care 61 291Food 75 303Movie 51 204Industry 56 224Others 35 136Total 350 1529Table 3: The evaluation dataIn this way, we collect training data automatically,which are used to learn the feature weights of theranking models.4 Experimental SetupIn this work, we use Baidu3 search engine, the mostpopular search engine for Chinese, and get the top100 search results for each entity.
The Chinese seg-mentation, POS tagging and dependency parsing isprovided by an open-source Chinese language pro-cessing platform LTP4 (Che et al 2010).4.1 Experimental DataIn our experiments, we prepare open-domain enti-ties from dictionaries in wide domains, which arepublished by a Chinese input method editor soft-ware Sogou Pinyin5.
The domains include biology,health care, food, movie, industry, and so on.
Wesample 1,879 entities from these domain dictionariesand randomly split them into 1/5 for developmen-t and 4/5 for test (Table 3).
We find that only 865(46.04%) entities exist in Baidubaike or Hudong-baike.
Then we extract candidate hypernyms for theentities and ask two annotators to judge each hyper-nym relation pair true or false manually.
A pair (E,H) is annotated as true if the annotators judge ?E is a(or a kind of) H?
is true.
Finally, we get 12.53 candi-date hypernyms for each entity on average in whichabout 2.09 hypernyms are correct.
4,330 hypernymrelation pairs are judged by both the annotators.
Wemeasure the agreement of the judges using the Kap-pa coefficient (Siegel and Castellan Jr, 1988).
The3http://www.baidu.com4http://ir.hit.edu.cn/demo/ltp/5http://pinyin.sogou.com/dict/12280 5 10 15 200.20.40.60.81.0Top NCoverageRateSRNSRN + ET+HWFigure 1: Effect of candidate hypernym coverage ratewhile varying NKappa value is 0.79.Our training data, containing 11,481 positive in-stances and 18,378 negative ones, is extracted fromBaidubaike and Hudongbaike using the heuristic s-trategy proposed in Section 3.2.2.4.2 Experimental MetricsThe evaluation metrics for our task include:Coverage Rate: We evaluate coverage rate of thecandidate hypernyms.
Coverage rate is the numberof entities for which at least one correct hypernym isfound divided by the total number of all entities.Precision@1: Our method returns a ranked listof hypernyms for each entity.
We evaluate precisionof top-1 hypernyms (the most probable ones) in theranked lists, which is the number of correct top-1hypernyms divided by the number of all entities.R-precision: It is equivalent to Precision@Rwhere R is the total number of candidates labeledas true hypernyms of an entity.Precision, Recall, and F-score: Besides, we canconvert our ranking models to classification modelsby setting thresholds.
Varying the thresholds, we canget different precisions, recalls, and F-scores.5 Results and Analysis5.1 The Coverage of Candidate HypernymsIn this section, we evaluate the coverage rate of thecandidate hypernyms.
We check the candidate hy-pernyms of the whole 1,879 entities in the develop-ment and test sets and see how many entities we cancollect at least one correct hypernym for.SourceCoverageRateAvg.
#SR10 0.8691 9.44?ET 0.3938 3.07HW 0.4135 0.87?SR10 + ET 0.8909 12.02SR10 + HW 0.9117 9.75ET + HW 0.7073 3.92SR10 + ET + HW 0.9324 12.53Table 4: Coverage evaluation of the candidate hypernymextractionThere are four different sources to collect candi-dates as described in Section 3.1, which can be di-vided into three kinds: search results (SR for short),encyclopedia tags (ET) and head words (HW).
ForSR, we select top N frequent nouns (SRN ) in thesearch results of an entity as its hypernym candi-dates.
The effect of coverage rate while varying Nis shown in Figure 1.
As we can see from the fig-ure, the coverage rate is improved significantly byincreasing N until N reaches 10.
After that, theimprovement becomes slight.
When the candidatesfrom all sources are merged, the coverage rate is fur-ther improved.Thus we set N as 10 in the remaining experi-ments.
The detail evaluation is shown in Table 4.We can see that top 10 frequent nouns in the searchresults contain at least one correct hypernym for86.91% entities in our data set.
This coincides withthe intuition that people usually can infer the seman-tic classes of unknown entities by searching them inweb search engines.The coverage rate of ET merely reaches 39.38%.We find the reason is that more than half of the enti-ties have no encyclopedia pages.
The average num-ber of candidate hypernyms from ET is 3.07.
Notethat the number is calculated among all the enti-ties.
We also calculate the average number only forthe present entities in encyclopedias.
The numberreaches 6.68.
The reason is that for many present en-tities, the category tags include not only hypernyms?For some of entities are rare, there may be less than 10nouns in the search results.
So the average count of candidatesis less than 10.?Not all of the entities can be segmented.
We cannot get thehead words of the ones that cannot be segmented.1229MethodPresent Entities Absent Entities All EntitiesP@1 R-Prec P@1 R-Prec P@1 R-PrecMPattern 0.5542 0.4937 0.4306 0.3638 0.5229 0.4608MSnow 0.3199 0.2592 0.2827 0.2610 0.3092 0.2597MPrior 0.7339 0.5483 0.3940 0.3531 0.5494 0.4423MSVM?linear 0.8569 0.6899 0.6157 0.5837 0.7260 0.6322MSVM?rbf 0.8484 0.6940 0.6241 0.5901 0.7266 0.6376MLR 0.8612 0.7052 0.6807 0.6258 0.7632 0.6621Table 5: Precision@1 and R-Precision results on the test set.
Here the present entities mean the entities existing in theencyclopedias.
The absent entities mean the ones not existing in the encyclopedias.but also related words.
For example, ?
?.|?% (Bradley Center)?
in Baidubaike have 5 tags, i.e.,?NBA?, ?N?
(sports)?, ?N?$?
(sports)?, ?;?
(basketball)?, and ?|, (arena)?.
Among them,only ?|, (arena)?
is a proper hypernym whereasthe others are some related words indicating mere-ly thematic vicinity.
Comparing the results of SR10and SR10 + ET, we can see that collecting candidatesfrom ET can improve coverage, although many in-correct candidates are added in at the same time.The HW source provides 0.87 candidates on av-erage with 41.35% coverage rate.
That is to say, forthese entities, people can infer the semantic classeswhen they see the surface lexicon.At last, we combine the candidates from all of thethree sources as the input of the ranking methods.The coverage rate reaches 93.24%.We also compare with the manually construct-ed semantic thesaurus CilinE mentioned in Section3.2.1.
Only 29 entities exist in CilinE (coverage rateis only 1.54%).
That is why we try to automaticallyextract hypernym relations.5.2 Evaluation of the Ranking5.2.1 Overall Performance ComparisonIn this section, we compare our proposed methodswith other methods.
Table 5 lists the performancemeasured by precision at rank 1 and R-precision ofsome key methods.
The precision-recall curves ofall the methods are shown in Figure 2.
Table 7 liststhe maximum F-scores.MPattern refers to the pattern-based method ofHearst (1992).
We craft Chinese Hearst-stylepatterns (Table 6), in which E represents an entityand H represents one of its hypernyms.
FollowingPattern TranslationE?(??/??)
H E is a (a kind of) HE (!)
H E(,) and other HH (?)(?)
E H(,) called EH (?)(?
)X E H(,) such as EH (?)AO?
E H(,) especially ETable 6: Chinese Hearst-style lexical patternsEvans (2004), we combine each pattern and each en-tity and submit them into the Baidu search engine.For example, for an entity E, we search ?E ???
(E is a)?, ?E  (E and other)?, and so on.
Weselect top 100 search results of each query and get1,285,209 results in all for the entities in the test set.Then we use the patterns to extract hypernyms fromthe search results.
The result shows that 508 cor-rect hypernyms are extracted for 568 entities (1,529entities in total).
Only a small part of the entitiescan be extracted hypernyms for.
This is mainly be-cause only a few hypernym relations are expressedin these fixed patterns in the web, and many ones areexpressed in more flexible manners.
The hypernymsare ranked based on the count of evidences wherethe hypernyms are extracted.MSnow is the method originally proposed by S-now et al(2005) for English but we adapt it for Chi-nese.
We consider the top 100 search results for eachknown hypernym-hyponym pairs as a corpus to ex-tract lexico-syntactic patterns.
Then, an LR classi-fier is built based on this patterns to recognize hy-pernym relations.
This method considers all noun-s co-occurred with the focused entity in the samesentences as candidate hypernyms.
So the numberof candidates is huge, which causes inefficiency.
In12300.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Precision?Recall Curves on the Test SetRecallPrecisionl l l l llll l llll ll l l l l ll l l ll ll lll ll lllllllMSnowMPriorMSVM?l inearMSVM?rbfMLRMPatternMHeurist icFigure 2: Precision-Recall curves on the test setour corpus, there are 652,181 candidates for 1,529entities (426.54 for each entity on average), most ofwhich are not hypernyms.
One possible reason isthat this method relies on an accurate syntactic pars-er and it is difficult to guarantee the quality of theautomatically extracted patterns.
Even worse, thelow quality of the language in the search results maymake this problem more serious.MPrior refers to the ranking method based on on-ly the prior of a candidate being a hypernym.
AsTable 5 shows, it outperforms MSnow and achievescomparable results with MPattern on Precision@1and R-Precision.Based on the features proposed in Section 3.2.1,we train several statistical models based on SVMand LR on the training data.
MSVM?linear andMSVM?rbf refer to the SVM models based on linearkernels and RBF kernels respectively.
MLR refersto the LR model.
The probabilities6 output by themodels are used to rank the candidate hypernyms.All of the parameters which need to be set in themodels are selected on the development set.
Table5 shows the best models based on each algorithm.These supervised models outperform the previousmethods.
MLR achieves the best performance.The precision-recall plot of the methods on thetest set is presented in Figure 2.
MHeuristic refersto the heuristic approach, proposed in Section 3.2.2,to collect training data.
Because this method cannot6The output of an SVM is the distance from the decisionhyper-plane.
Sigmoid functions can be used to convert this un-calibrated distance into a calibrated posterior probability (Platt,1999).Method Max.
F-scoreMPattern 0.2061MSnow 0.1514MHeuristic 0.2803MPrior 0.5591MSVM?linear 0.5868MSVM?rbf 0.6014MLR 0.5998Table 7: Summary of maximum F-score on the test setFeature P@1 R-PrecMax.F-scoreAll 0.7632 0.6621 0.5998?
Prior 0.7534 0.6546 0.5837?
Is Tag 0.6965 0.6039 0.5605?
Is Head 0.7018 0.6036 0.5694?
In Titles 0.7436 0.6513 0.5868?
Synonyms 0.7495 0.6493 0.5831?
Radicals 0.7593 0.6584 0.5890?
Source Num 0.7364 0.6556 0.5984?
Lexicon 0.7377 0.6422 0.5851?
Source Info 0.6128 0.5221 0.5459Table 8: Performance of LR models with different fea-tures on the test setprovide ranking information, it is not listed in Ta-ble 5.
For fair comparison of R-precision and recall,we add the extra correct hypernyms from MPatternand MSnow to the test data set.
The models basedon SVM and LR still perform better than the othermethods.
MPattern and MSnow suffer from low re-call and precision.
MHeuristic get a high precisionbut a low recall, because it can only deal with a partof entities appearing in encyclopedias.
The preci-sion of MHeuristic reflects the quality of our trainingdata.
We summarize the maximum F-score of dif-ferent methods in Table 7.5.2.2 Feature EffectTable 8 shows the impact of each feature on theperformance of LR models.
When we remove anyone of the features, the performance is degradedmore or less.
The most effective features are Is Tagand Is Head.
The last line in Table 8 shows theperformance when we remove all features aboutthe source information, i.e., Is Tag, Is Head, and1231EntityTop-1HypernymEntityTop-1Hypernym??b??
(cefoperazone sodium) ??
(drug) ?y(bullet tuna) ~a(fish)???
(finger citron rolls) ?
(snack) =?
(zirconite) ??(ore)E????
(The Avengers) >K(movie) ?|?d?
(Felixstowe) l?
(port)@U=(mastigium) ?O(datum) ?!?
(coxal cavity) ??
(plant)?UX?=?s(Ethanolamine phosphotransferase))?(organism)?u(coma)?
(knowledge)Table 10: Examples of entity-hypernym pairs extracted by MLRDomain P@1 R-PrecMax.F-scoreBiology 0.8165 0.7203 0.6424Health Care 0.7354 0.5962 0.6061Food 0.7450 0.6634 0.6938Movie 0.9310 0.8069 0.7031Industry 0.6286 0.5841 0.4624Others 0.6324 0.4936 0.4318Table 9: Performance of MLR in various domainsSource Num.
The performance is degraded sharply.This indicates the importance of the source informa-tion for hypernym ranking.5.2.3 The Performance in Each DomainIn this section, we evaluate the performance ofMLR method in various domains.
We can see fromTable 9 that the performance in movie domain is bestwhile the performance in industry domain is worst.That is because the information about movies isabundant on the web.
Furthermore, most of movieshave encyclopedia pages.
It is easy to get the hy-pernyms.
In contrast, the entities in industry domainare more uncommon.
On the whole, our method isrobust for different domains.
In Table 10, some in-stances in various domains are presented.5.3 Error AnalysisThe uncovered entities7 and the false positives8 areanalyzed after the experiments.
Some error exam-ples are shown in Table 10 (in red font).7Uncovered entities are entities which we do not collect anycorrect hypernyms for in the first step.8False positives are hypernyms ranked at the first places, butactually are not correct hypernyms.Uncovered entities: About 34% of the errors arecaused by uncovered entities.
It is found that manyof the uncovered entities are rare entities.
Nearly36% of them are very rare and have only less than100 search results in all.
When we can?t get enoughinformation of an unknown entity from the searchengine, it?s difficult to know its semantic meaning,such as ?
@U= (mastigium)?, ??!?
(coxal cav-ity)?, ?
?u (coma)?.
The identification of their hy-pernyms requires more human-crafted knowledge.The ranking models we used are unable to selectthem, as the true synonyms are often below rank 10.False positives: The remained 66% errors arefalse positives.
They are mainly owing to thefact that some other related words in the candi-date lists are more likely hypernyms.
For exam-ple, ?)?
(organism)?
is wrongly recognized asthe most probable hypernym of ?
?UX?=?s (Ethanolamine phosphotransferase)?, becausethe entity often co-occurs with word ?)?
(organ-ism)?
and the latter is often used as a hypernym ofsome other entities.
The correct hypernyms actu-ally are ?s (enzyme)?, ?z???
(chemical sub-stance)?, and so on.6 ConclusionThis paper proposes a novel method for findinghypernyms of Chinese open-domain entities frommultiple sources.
We collect candidate hypernymswith wide coverage from search results, encyclope-dia category tags and the head word of the entity.Then, we propose a set of features to build statisti-cal models to rank the candidate hypernyms on thetraining data collected automatically.
In our exper-iments, we show that our method outperforms thestate-of-the-art methods and achieves the best preci-1232sion of 76.32% on a manually labeled test dataset.All of the features which we propose are effective,especially the features of source information.
More-over, our method works well in various domains, e-specially in the movie and biology domains.
We al-so conduct detailed analysis to give more insightson the error distribution.
Except some language de-pendent features, our approach can be easily trans-fered from Chinese to other languages.
For futurework, we would like to explore knowledge frommore sources to enhance our model, such as seman-tic thesauri and infoboxes in encyclopedias.AcknowledgmentsThis work was supported by National Natu-ral Science Foundation of China (NSFC) viagrant 61133012, 61073126 and the National 863Leading Technology Research Project via grant2012AA011102.
Special thanks to Zhenghua Li,Wanxiang Che, Wei Song, Yanyan Zhao, YuhangGuo and the anonymous reviewers for insightfulcomments and suggestions.
Thanks are also due toour annotators Ni Han and Zhenghua Li.ReferencesSharon A. Caraballo.
1999.
Automatic construction of ahypernym-labeled noun hierarchy from text.
In Pro-ceedings of the 37th Annual Meeting of the Associ-ation for Computational Linguistics, pages 120?126,College Park, Maryland, USA, June.Wanxiang Che, Zhenghua Li, and Ting Liu.
2010.
Ltp:A chinese language technology platform.
In Coling2010: Demonstrations, pages 13?16, Beijing, China,August.Massimiliano Ciaramita and Mark Johnson.
2003.
Su-persense tagging of unknown nouns in wordnet.
InProceedings of the 2003 conference on Empiricalmethods in natural language processing, pages 168?175.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S Weld, and Alexander Yates.
2005.
Unsuper-vised named-entity extraction from the web: An exper-imental study.
Artificial Intelligence, 165(1):91?134.Oren Etzioni, Michele Banko, and Michael J Cafarella.2006.
Machine reading.
In AAAI, volume 6, pages1517?1519.Richard Evans.
2004.
A framework for named enti-ty recognition in the open domain.
Recent Advancesin Natural Language Processing III: Selected Papersfrom RANLP 2003, 260:267?274.Marti A Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings of the14th conference on Computational linguistics-Volume2, pages 539?545.Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,and Gerhard Weikum.
2012.
Yago2: a spatially andtemporally enhanced knowledge base from wikipedia.Artificial Intelligence, pages 1?63.Thomas Lin, Mausam, and Oren Etzioni.
2012.
No nounphrase left behind: Detecting and typing unlinkableentities.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 893?903, Jeju Island, Korea, July.Paul McNamee, Rion Snow, Patrick Schone, and JamesMayfield.
2008.
Learning named entity hyponymsfor question answering.
In Proceedings of the ThirdInternational Joint Conference on Natural LanguageProcessing, pages 799?804.Marius Pasca.
2004.
Acquisition of categorized namedentities for web search.
In Proceedings of the thir-teenth ACM international conference on Informationand knowledge management, pages 137?145.John Platt.
1999.
Probabilistic outputs for support vec-tor machines and comparisons to regularized likeli-hood methods.
Advances in large margin classifiers,10(3):61?74.Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009.What is this, anyway: Automatic hypernym discovery.In Proceedings of the 2009 AAAI Spring Symposiumon Learning by Reading and Learning to Read, pages88?93.Cederberg Scott and Widdows Dominic.
2003.
Using lsaand noun coordination information to improve the pre-cision and recall of automatic hyponymy extraction.
InProceedings of the seventh conference on Natural lan-guage learning at HLT-NAACL 2003-Volume 4, pages111?118.Sidney Siegel and N John Castellan Jr. 1988.
Nonpara-metric statistics for the behavioral sciences.
McGraw-Hill, New York.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In Lawrence K. Saul, Yair Weiss, and Le?onBottou, editors, Advances in Neural Information Pro-cessing Systems 17, pages 1297?1304.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
Yago: A large ontology fromwikipedia and wordnet.
Web Semantics: Science, Ser-vices and Agents on the World Wide Web, 6(3):203?217.1233Peter Turney, Michael L Littman, Jeffrey Bigham, andVictor Shnayder.
2003.
Combining independent mod-ules to solve multiple-choice synonym and analogyproblems.
In Proceedings of the International Con-ference RANLP-2003, pages 482?489.Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and Chin-Yew Lin.
2011.
Nonlinear evidence fusion and prop-agation for hyponymy relation mining.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 1159?1168, Portland, Oregon, USA,June.1234
