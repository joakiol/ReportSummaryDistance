Paragraph-, word-, and coherence-based approaches to sentence ranking:A comparison of algorithm and human performanceFlorian WOLFMassachusetts Institute of TechnologyMIT NE20-448, 3 Cambridge CenterCambridge, MA 02139, USAfwolf@mit.eduEdward GIBSONMassachusetts Institute of TechnologyMIT NE20-459, 3 Cambridge CenterCambridge, MA 02139, USAegibson@mit.eduAbstractSentence ranking is a crucial part ofgenerating text summaries.
We comparedhuman sentence rankings obtained in apsycholinguistic experiment to three differentapproaches to sentence ranking: A simpleparagraph-based approach intended as abaseline, two word-based approaches, and twocoherence-based approaches.
In theparagraph-based approach, sentences in thebeginning of paragraphs received higherimportance ratings than other sentences.
Theword-based approaches determined sentencerankings based on relative word frequencies(Luhn (1958); Salton & Buckley (1988)).Coherence-based approaches determinedsentence rankings based on some property ofthe coherence structure of a text (Marcu(2000); Page et al (1998)).
Our resultssuggest poor performance for the simpleparagraph-based approach, whereas word-based approaches perform remarkably well.The best performance was achieved by acoherence-based approach where coherencestructures are represented in a non-treestructure.
Most approaches also outperformedthe commercially available MSWordsummarizer.1 IntroductionAutomatic generation of text summaries is anatural language engineering application that hasreceived considerable interest, particularly due tothe ever-increasing volume of text informationavailable through the internet.
The task of ahuman generating a summary generally involvesthree subtasks (Brandow et al (1995); Mitra et al(1997)): (1) understanding a text; (2) ranking textpieces (sentences, paragraphs, phrases, etc.)
forimportance; (3) generating a new text (thesummary).
Like most approaches tosummarization, we are concerned with the secondsubtask (e.g.
Carlson et al (2001); Goldstein et al(1999); Gong & Liu (2001); Jing et al (1998);Luhn (1958); Mitra et al (1997); Sparck-Jones &Sakai (2001); Zechner (1996)).
Furthermore, weare concerned with obtaining generic rather thanquery-relevant importance rankings (cf.
Goldsteinet al (1999), Radev et al (2002) for thatdistinction).We evaluated different approaches to sentenceranking against human sentence rankings.
Toobtain human sentence rankings, we asked peopleto read 15 texts from the Wall Street Journal on awide variety of topics (e.g.
economics, foreign anddomestic affairs, political commentaries).
For eachof the sentences in the text, they provided aranking of how important that sentence is withrespect to the content of the text, on an integerscale from 1 (not important) to 7 (very important).The approaches we evaluated are a simpleparagraph-based approach that serves as a baseline,two word-based algorithms, and two coherence-based approaches1.
We furthermore evaluated theMSWord summarizer.2 Approaches to sentence ranking2.1 Paragraph-based approachSentences at the beginning of a paragraph areusually more important than sentences that arefurther down in a paragraph, due in part to the waypeople are instructed to write.
Therefore, probablythe simplest approach conceivable to sentenceranking is to choose the first sentences of each1 We did not use any machine learning techniques toboost performance of the algorithms we tested.Therefore performance of the algorithms tested herewill almost certainly be below the level of performancethat could be reached if we had augmented thealgorithms with such techniques (e.g.
Carlson et al(2001)).
However, we think that a comparison between?bare-bones?
algorithms is viable because it allows tosee how performance differs due to different basicapproaches to sentence ranking, and not due topotentially different effects of different machinelearning algorithms on different basic approaches tosentence ranking.
In future research we plan to addressthe impact of machine learning on the algorithms testedhere.paragraph as important, and the other sentences asnot important.
We included this approach merelyas a simple baseline.2.2 Word-based approachesWord-based approaches to summarization arebased on the idea that discourse segments areimportant if they contain ?important?
words.Different approaches have different definitions ofwhat an important word is.
For example, Luhn(1958), in a classic approach to summarization,argues that sentences are more important if theycontain many significant words.
Significant wordsare words that are not in some predefined stoplistof words with high overall corpus frequency2.Once significant words are marked in a text,clusters of significant words are formed.
A clusterhas to start and end with a significant word, andfewer than n insignificant words must separate anytwo significant words (we chose n = 3, cf.
Luhn(1958)).
Then, the weight of each cluster iscalculated by dividing the square of the number ofsignificant words in the cluster by the total numberof words in the cluster.
Sentences can containmultiple clusters.
In order to compute the weightof a sentence, the weights of all clusters in thatsentence are added.
The higher the weight of asentence, the higher is its ranking.A more recent and frequently used word-basedmethod used for text piece ranking is tf.idf (e.g.Manning & Schuetze (2000); Salton & Buckley(1988); Sparck-Jones & Sakai (2001); Zechner(1996)).
The tf.idf measure relates the frequencyof words in a text piece, in the text, and in acollection of texts respectively.
The intuitionbehind tf.idf is to give more weight to sentencesthat contain terms with high frequency in adocument but low frequency in a reference corpus.Figure 1 shows a formula for calculating tf.idf,where dsij is the tf.idf weight of sentence i indocument j, nsi is the number of words in sentencei, k is the kth word in sentence i, tfjk is thefrequency of word k in document j, nd is thenumber of documents in the reference corpus, anddfk is the number of documents in the referencecorpus in which word k appears.????????
?=?= dfntfdskdkjkijnsilog1Figure 1.
Formula for calculating tf.idf (Salton &Buckley (1988)).2 Instead of stoplists, tf.idf values have also been usedto determine significant words (e.g.
Buyukkokten et al(2001)).We compared both Luhn (1958)?s measure andtf.idf scores to human rankings of sentenceimportance.
We will show that both methodsperformed remarkably well, although onecoherence-based method performed better.2.3 Coherence-based approachesThe sentence ranking methods introduced in thetwo previous sections are solely based on layout oron properties of word distributions in sentences,texts, and document collections.
Other approachesto sentence ranking are based on the informationalstructure of texts.
With informational structure, wemean the set of informational relations that holdbetween sentences in a text.
This set can berepresented in a graph, where the nodes representsentences, and labeled directed arcs representinformational relations that hold between thesentences (cf.
Hobbs (1985)).
Often, informationalstructures of texts have been represented as trees(e.g.
Carlson et al (2001), Corston-Oliver (1998),Mann & Thompson (1988), Ono et al (1994)).
Wewill present one coherence-based approach thatassumes trees as a data structure for representingdiscourse structure, and one approach that assumesless constrained graphs.
As we will show, theapproach based on less constrained graphsperforms better than the tree-based approach whencompared to human sentence rankings.3 Coherence-based summarization revisitedThis section will discuss in more detail the datastructures we used to represent discourse structure,as well as the algorithms used to calculate sentenceimportance, based on discourse structures.3.1 Representing coherence structures3.1.1 Discourse segmentsDiscourse segments can be defined as non-overlapping spans of prosodic units (Hirschberg &Nakatani (1996)), intentional units (Grosz &Sidner (1986)), phrasal units (Lascarides & Asher(1993)), or sentences (Hobbs (1985)).
We adopteda sentence unit-based definition of discoursesegments for the coherence-based approach thatassumes non-tree graphs.
For the coherence-basedapproach that assumes trees, we used Marcu(2000)?s more fine-grained definition of discoursesegments because we used the discourse trees fromCarlson et al (2002)?s database of coherence-annotated texts.3.1.2 Kinds of coherence relationsWe assume a set of coherence relations that issimilar to that of Hobbs (1985).
Below areexamples of each coherence relation.
(1) Cause-Effect[There was bad weather at the airport]a [and so ourflight got delayed.
]b(2) Violated Expectation[The weather was nice]a [but our flight gotdelayed.
]b(3) Condition[If the new software works,]a [everyone will behappy.
]b(4) Similarity[There is a train on Platform A.
]a [There is anothertrain on Platform B.
]b(5) Contrast[John supported Bush]a [but Susan opposed him.
]b(6) Elaboration[A probe to Mars was launched this week.
]a [TheEuropean-built ?Mars Express?
is scheduled toreach Mars by late December.
]b(7) Attribution[John said that]a [the weather would be nicetomorrow.
]b(8) Temporal Sequence[Before he went to bed,]a [John took a shower.
]bCause-effect, violated expectation, condition,elaboration, temporal sequence, and attributionare asymmetrical or directed relations, whereassimilarity, contrast, and temporal sequence aresymmetrical or undirected relations (Mann &Thompson, 1988; Marcu, 2000).
In the non-tree-based approach, the directions of asymmetrical ordirected relations are as follows: cause ?
effectfor cause-effect; cause ?
absent effect for violatedexpectation; condition ?
consequence forcondition; elaborating ?
elaborated forelaboration, and source ?
attributed forattribution.
In the tree-based approach, theasymmetrical or directed relations are between amore important discourse segment, or a Nucleus,and a less important discourse segment, or aSatellite (Marcu (2000)).
The Nucleus is theequivalent of the arc destination, and the Satelliteis the equivalent of the arc origin in the non-tree-based approach.
The symmetrical or undirectedrelations are between two discourse elements ofequal importance, or two Nuclei.
Below we willexplain how the difference between Satellites andNuclei is considered in tree-based sentencerankings.3.1.3 Data structures for representing discoursecoherenceAs mentioned above, we used two alternativerepresentations for discourse structure, tree- andnon-tree based.
In order to illustrate both datastructures, consider (9) as an example:(9) Example text0.
Susan wanted to buy some tomatoes.1.
She also tried to find some basil.2.
The basil would probably be quite expensiveat this time of the year.Figure 2 shows one possible tree representationof the coherence structure of (9)3.
Sim represents asimilarity relation, and elab an elaborationrelation.
Furthermore, nodes with a ?Nuc?subscript are Nuclei, and nodes with a ?Sat?subscript are Satellites.Figure 2.
Coherence tree for (9).Figure 3 shows a non-tree representation of thecoherence structure of (9).
Here, the heads of thearrows represent the directionality of a relation.Figure 3.
Non-tree coherence graph for (9).3.2 Coherence-based sentence rankingThis section explains the algorithms for the tree-and the non-tree-based sentence ranking approach.3.2.1 Tree-based approachWe used Marcu (2000)?s algorithm to determinesentence rankings based on tree discoursestructures.
In this algorithm, sentence salience isdetermined based on the tree level of a discoursesegment in the coherence tree.
Figure 4 showsMarcu (2000)?s algorithm, where r(s,D,d) is therank of a sentence s in a discourse tree D withdepth d.  Every node in a discourse tree D has apromotion set promotion(D), which is the union ofall Nucleus children of that node.
Associated withevery node in a discourse tree D is also a set ofparenthetical nodes parentheticals(D) (forexample, in ?Mars ?
half the size of Earth ?
isred?, ?half the size of earth?
would be aparenthetical node in a discourse tree).
Bothpromotion(D) and parentheticals(D) can be emptysets.
Furthermore, each node has a left subtree,3 Another possible tree structure might be( elab ( par ( 0 1 ) 2 ) ).0Nuc 1Nuc 2SatelabNucsimelabsim0 1 2lc(D), and a right subtree, rc(D).
Both lc(D) andrc(D) can also be empty.?????????????
?=otherwisedDrcsrdDlcsrDcalsparenthetisifdDpromotionsifdNILisDifdDsr))1),(,(),1),(,(max(),(1),(,0),,(Figure 4.
Formula for calculating coherence-tree-based sentence rank (Marcu (2000)).The discourse segments in Carlson et al(2002)?s database are often sub-sentential.Therefore, we had to calculate sentence rankingsfrom the rankings of the discourse segments thatform the sentence under consideration.
We didthis by calculating the average ranking, theminimal ranking, and the maximal ranking of alldiscourse segments in a sentence.
Our resultsshowed that choosing the minimal rankingperformed best, followed by the average ranking,followed by the maximal ranking (cf.
Section 4.4).3.2.2 Non-tree-based approachWe used two different methods to determinesentence rankings for the non-tree coherencegraphs4.
Both methods implement the intuitionthat sentences are more important if othersentences relate to them (Sparck-Jones (1993)).The first method consists of simply determiningthe in-degree of each node in the graph.
A noderepresents a sentence, and the in-degree of a noderepresents the number of sentences that relate tothat sentence.The second method uses Page et al (1998)?sPageRank algorithm, which is used, for example,in the Google?
search engine.
Unlike justdetermining the in-degree of a node, PageRanktakes into account the importance of sentences thatrelate to a sentence.
PageRank thus is a recursivealgorithm that implements the idea that the moreimportant sentences relate to a sentence, the moreimportant that sentence becomes.
Figure 5 showshow PageRank is calculated.
PRn is the PageRankof the current sentence, PRn-1 is the PageRank ofthe sentence that relates to sentence n, on-1 is theout-degree of sentence n-1, and ?
is a dampingparameter that is set to a value between 0 and 1.We report results for ?
set to 0.85 because this is avalue often used in applications of PageRank (e.g.Ding et al (2002); Page et al (1998)).
We also4 Neither of these methods could be implemented forcoherence trees since Marcu (2000)?s tree-basedalgorithm assumes binary branching trees.
Thus, the in-degree for all non-terminal nodes is always 2.calculated PageRanks for ?
set to values between0.05 and 0.95, in increments of 0.05; changing ?did not affect performance.oPRPRnnn111?
?+?= ?
?Figure 5.
Formula for calculating PageRank (Pageet al (1998)).4 ExperimentsIn order to test algorithm performance, wecompared algorithm sentence rankings to humansentence rankings.
This section describes theexperiments we conducted.
In Experiment 1, thetexts were presented with paragraph breaks; inExperiment 2, the texts were presented withoutparagraph breaks.
This was done to control for theeffect of paragraph information on human sentencerankings.4.1 Materials for the coherence-basedapproachesIn order to test the tree-based approach, we tookcoherence trees for 15 texts from a database of 385texts from the Wall Street Journal that wereannotated for coherence (Carlson et al (2002)).The database was independently annotated by sixannotators.
Inter-annotator agreement wasdetermined for six pairs of two annotators each,resulting in kappa values (Carletta (1996)) rangingfrom 0.62 to 0.82 for the whole database (Carlsonet al (2003)).
No kappa values for just the 15 textswe used were available.For the non-tree based approach, we usedcoherence graphs from a database of 135 textsfrom the Wall Street Journal and the APNewswire, annotated for coherence.
Each text wasindependently annotated by two annotators.
Forthe 15 texts we used, kappa was 0.78, for thewhole database, kappa was 0.84.4.2 Experiment 1: With paragraphinformation15 participants from the MIT community werepaid for their participation.
All were nativespeakers of English and were na?ve as to thepurpose of the study (i.e.
none of the subjects wasfamiliar with theories of coherence in naturallanguage, for example).Participants were asked to read 15 texts from theWall Street Journal, and, for each sentence in eachtext, to provide a ranking of how important thatsentence is with respect to the content of the text,on an integer scale from 1 to 7 (1 = not important;7 = very important).
The   texts  were  selected  so123456781 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19sentence numberimportance rankingNoParagraphWithParagraphFigure 6.
Human ranking results for one text (wsj_1306).that there was a coherence tree annotationavailable in Carlson et al (2002)?s database.
Textlengths for the 15 texts we selected ranged from130 to 901 words (5 to 47 sentences); average textlength was 442 words (20 sentences), median was368 words (16 sentences).
Additionally, texts wereselected so that they were about as diverse topicsas possible.The experiment was conducted in front ofpersonal computers.
Texts were presented in aweb browser as one webpage per text; for sometexts, participants had to scroll to see the wholetext.
Each sentence was presented on a new line.Paragraph breaks were indicated by empty lines;this was pointed out to the participants during theinstructions for the experiment.4.3 Experiment 2: Without paragraphinformationThe method was the same as in Experiment 1,except that texts in Experiment 2 did not includeparagraph information.
Each sentence waspresented on a new line.
None of the 15participants who participated in Experiment 2 hadparticipated in Experiment 1.4.4 Results of the experimentsHuman sentence rankings did not differsignificantly between Experiment 1 andExperiment 2 for any of the 15 texts (all Fs < 1).This suggests that paragraph information does nothave a big effect on human sentence rankings, atleast not for the 15 texts that we examined.
Figure6 shows the results from both experiments for onetext.We compared human sentence rankings todifferent algorithmic approaches.
The paragraph-based rankings do not provide scaled importancerankings but only ?important?
vs. ?not important?.Therefore, in order to compare human rankings tothe paragraph-based baseline approach, wecalculated point biserial correlations (cf.
Bortz(1999)).
We obtained significant correlationsbetween paragraph-based rankings and humanrankings only for one of the 15 texts.All other algorithms provided scaled importancerankings.
Many evaluations of scalable sentenceranking algorithms are based on precision/recall/F-scores (e.g.
Carlson et al (2001); Ono et al(1994)).
However, Jing et al (1998) argue thatsuch measures are inadequate because they onlydistinguish between hits and misses or falsealarms, but do not account for a degree ofagreement.
For example, imagine a situationwhere the human ranking for a given sentence is?7?
(?very important?)
on an integer scale rangingfrom 1 to 7, and Algorithm A gives the samesentence a ranking of ?7?
on the same scale,Algorithm B gives a ranking of ?6?, and AlgorithmC gives a ranking of ?2?.
Intuitively, Algorithm B,although it does not reach perfect performance,still performs better than Algorithm C.Precision/recall/F-scores do not account for thatdifference and would rate Algorithm A as ?hit?
butAlgorithm B as well as Algorithm C as ?miss?.
Inorder to collect performance measures that aremore adequate to the evaluation of scaledimportance rankings, we computed Spearman?srank correlation coefficients.
The rank correlationcoefficients were corrected for tied ranks becausein our rankings it was possible for more than onesentence to have the same importance rank, i.e.
tohave tied ranks (Horn (1942); Bortz (1999)).In addition to evaluating word-based andcoherence-based algorithms, we evaluated onecommercially available summarizer, the MSWordsummarizer, against human sentence rankings.Our reason for including an evaluation of theMSWord summarizer was to have a more usefulbaseline for scalable sentence rankings than theparagraph-based approach provides.00.10.20.30.40.50.6MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRankmeanrankcorrelationcoefficientNoParagraphWithParagraphFigure 7.
Average rank correlations of algorithm and human sentence rankings.Figure 7 shows average rank correlations (?avg)of each algorithm and human sentence ranking forthe 15 texts.
MarcuAvg refers to the version ofMarcu (2000)?s algorithm where we calculatedsentence rankings as the average of the rankings ofall discourse segments that constitute that sentence;for MarcuMin, sentence rankings were theminimum of the rankings of all discourse segmentsin that sentence; for MarcuMax we selected themaximum of the rankings of all discoursesegments in that sentence.Figure 7 shows that the MSWord summarizerperformed numerically worse than most otheralgorithms, except MarcuMin.
Figure 7 alsoshows that PageRank performed numerically betterthan all other algorithms.
Performance wassignificantly better than most other algorithms(MSWord, NoParagraph: F(1,28) = 21.405, p =0.0001; MSWord, WithParagraph: F(1,28) =26.071, p = 0.0001; Luhn, WithParagraph: F(1,28)= 5.495, p = 0.026; MarcuAvg, NoParagraph:F(1,28) = 9.186, p = 0.005; MarcuAvg,WithParagraph: F(1,28) = 9.097, p = 0.005;MarcuMin, NoParagraph: F(1,28) = 4.753, p =0.038; MarcuMax, NoParagraph F(1,28) = 24.633,p = 0.0001; MarcuMax, WithParagraph: F(1,28) =31.430, p =0.0001).
Exceptions are Luhn,NoParagraph (F(1,28) = 1.859, p = 0.184); tf.idf,NoParagraph (F(1,28) = 2.307, p = 0.14);MarcuMin, WithParagraph (F(1,28) = 2.555, p =0.121).
The difference between PageRank andtf.idf, WithParagraph was marginally significant(F(1,28) = 3.113, p = 0.089).As mentioned above, human sentence rankingsdid not differ significantly between Experiment 1and Experiment 2 for any of the 15 texts (all Fs <1).
Therefore, in order to lend more power to ourstatistical tests, we collapsed the data for each textfor the WithParagraph and the NoParagraphcondition, and treated them as one experiment.Figure 8 shows that when the data fromExperiments 1 and 2 are collapsed, PageRankperformed significantly better than all otheralgorithms except in-degree (two-tailed t-testresults: MSWord: F(1, 58) = 48.717, p = 0.0001;Luhn: F(1,58) = 6.368, p = 0.014; tf.idf: F(1,58) =5.522, p = 0.022; MarcuAvg: F(1,58) = 18.922, p =0.0001; MarcuMin: F(1,58) = 7.362, p = 0.009;MarcuMax: F(1,58) = 56.989, p = 0.0001; in-degree: F(1,58) < 1).00.10.20.30.40.5MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRankmeanrankcorrelationcoefficientFigure 8.
Average rank correlations of algorithmand human sentence rankings with collapsed data.5 ConclusionThe goal of this paper was to evaluate the resultsof three different kinds of sentence rankingalgorithms and one commercially availablesummarizer.
In order to evaluate the algorithms,we compared their sentence rankings to humansentence rankings of fifteen texts of varying lengthfrom the Wall Street Journal.Our results indicated that a simple paragraph-based algorithm that was intended as a baselineperformed very poorly, and that word-based andsome coherence-based algorithms showed the bestperformance.
The only commercially availablesummarizer that we tested, the MSWordsummarizer, showed worse performance than mostother algorithms.
Furthermore, we found that acoherence-based algorithm that uses PageRank andtakes non-tree coherence graphs as inputperformed better than most versions of acoherence-based algorithm that operates oncoherence trees.
When data from Experiments 1and 2 were collapsed, the PageRank algorithmperformed significantly better than all otheralgorithms, except the coherence-based algorithmthat uses in-degrees of nodes in non-tree coherencegraphs.ReferencesJ?rgen Bortz.
1999.
Statistik f?r Sozialwissen-schaftler.
Berlin: Springer Verlag.Ronald Brandow, Karl Mitze, & Lisa F Rau.
1995.Automatic condensation of electronicpublications by sentence selection.Information Processing and Management,31(5), 675-685.Orkut Buyukkokten, Hector Garcia-Molina, &Andreas Paepcke.
2001.
Seeing the wholein parts: Text summarization for webbrowsing on handheld devices.
Paperpresented at the 10th International WWWConference, Hong Kong, China.Jean Carletta.
1996.
Assessing agreement onclassification tasks: The kappa statistic.Computational Linguistics, 22(2), 249-254.Lynn Carlson, John M Conroy, Daniel Marcu,Dianne P O'Leary, Mary E Okurowski,Anthony Taylor, et al 2001.
An empiricalstudy on the relation between abstracts,extracts, and the discourse structure oftexts.
Paper presented at the DUC-2001,New Orleans, LA, USA.Lynn Carlson, Daniel Marcu, & Mary EOkurowski.
2002.
RST DiscourseTreebank.
Philadelphia, PA: LinguisticData Consortium.Lynn Carlson, Daniel Marcu, & Mary EOkurowski.
2003.
Building a discourse-tagged corpus in the framework ofrhetorical structure theory.
In J. vanKuppevelt & R. Smith (Eds.
), Currentdirections in discourse and dialogue.
NewYork: Kluwer Academic Publishers.Simon Corston-Oliver.
1998.
Computingrepresentations of the structure of writtendiscourse.
Redmont, WA.Chris Ding, Xiaofeng He, Perry Husbands,Hongyuan Zha, & Horst Simon.
2002.PageRank, HITS, and a unified frameworkfor link analysis.
(No.
49372).
Berkeley,CA, USA.Jade Goldstein, Mark Kantrowitz, Vibhu O Mittal,& Jamie O Carbonell.
1999.
Summarizingtext documents: Sentence selection andevaluation metrics.
Paper presented at theSIGIR-99, Melbourne, Australia.Yihong Gong, & Xin Liu.
2001.
Generic textsummarization using relevance measureand latent semantic analysis.
Paperpresented at the Annual ACM Conferenceon Research and Development inInformation Retrieval, New Orleans, LA,USA.Barbara J Grosz, & Candace L Sidner.
1986.Attention, intentions, and the structure ofdiscourse.
Computational Linguistics,12(3), 175-204.Julia Hirschberg, & Christine H Nakatani.
1996.
Aprosodic analysis of discourse segments indirection-giving monologues.
Paperpresented at the 34th Annual Meeting ofthe Association for ComputationalLinguistics, Santa Cruz, CA.Jerry R Hobbs.
1985.
On the coherence andstructure of discourse.
Stanford, CA.D Horn.
1942.
A correction for the effect of tiedranks on the value of the rank differencecorrelation coefficient.
Journal ofEducational Psychology, 33, 686-690.Hongyan Jing, Kathleen R McKeown, ReginaBarzilay, & Michael Elhadad.
1998.Summarization evaluation methods:Experiments and analysis.
Paper presentedat the AAAI-98 Spring Symposium onIntelligent Text Summarization, Stanford,CA, USA.Alex Lascarides, & Nicholas Asher.
1993.Temporal interpretation, discourserelations and common sense entailment.Linguistics and Philosophy, 16(5), 437-493.Hans Peter Luhn.
1958.
The automatic creation ofliterature abstracts.
IBM Journal ofResearch and Development, 2(2), 159-165.William C Mann, & Sandra A Thompson.
1988.Rhetorical structure theory: Toward afunctional theory of text organization.Text, 8(3), 243-281.Christopher D Manning, & Hinrich Schuetze.2000.
Foundations of statistical naturallanguage processing.
Cambridge, MA,USA: MIT Press.Daniel Marcu.
2000.
The theory and practice ofdiscourse parsing and summarization.Cambridge, MA: MIT Press.Mandar Mitra, Amit Singhal, & Chris Buckley.1997.
Automatic text summarization byparagraph extraction.
Paper presented atthe ACL/EACL-97 Workshop onIntelligent Scalable Text Summarization,Madrid, Spain.Kenji Ono, Kazuo Sumita, & Seiji Miike.
1994.Abstract generation based on rhetoricalstructure extraction.
Paper presented at theCOLING-94, Kyoto, Japan.Lawrence Page, Sergey Brin, Rajeev Motwani, &Terry Winograd.
1998.
The PageRankcitation ranking: Bringing order to theweb.
Stanford, CA.Dragomir R Radev, Eduard Hovy, & Kathleen RMcKeown.
2002.
Introduction to thespecial issue on summarization.Computational Linguistics, 28(4), 399-408.Gerard Salton, & Christopher Buckley.
1988.Term-weighting approaches in automatictext retrieval.
Information Processing andManagement, 24(5), 513-523.Karen Sparck-Jones.
1993.
What might be in asummary?
In G. Knorz, J. Krause & C.Womser-Hacker (Eds.
), Informationretrieval 93: Von der Modellierung zurAnwendung (pp.
9-26).
Konstanz:Universitaetsverlag.Karen Sparck-Jones, & Tetsuya Sakai.
2001,September 2001.
Generic summaries forindexing in IR.
Paper presented at theACM SIGIR-2001, New Orleans, LA,USA.Klaus Zechner.
1996.
Fast generation of abstractsfrom general domain text corpora byextracting relevant sentences.
Paperpresented at the COLING-96,Copenhagen, Denmark.
