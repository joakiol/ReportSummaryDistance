Proceedings of the EACL 2009 Student Research Workshop, pages 88?96,Athens, Greece, 2 April 2009. c?2009 Association for Computational LinguisticsExtraction of definitions using grammar-enhanced machine learningEline WesterhoutUtrecht UniversityTrans 10, 3512 JK, Utrecht, The NetherlandsE.N.Westerhout@uu.nlAbstractIn this paper we compare different ap-proaches to extract definitions of fourtypes using a combination of a rule-basedgrammar and machine learning.
We col-lected a Dutch text corpus containing 549definitions and applied a grammar on it.Machine learning was then applied to im-prove the results obtained with the gram-mar.
Two machine learning experimentswere carried out.
In the first experi-ment, a standard classifier and a classi-fier designed specifically to deal with im-balanced datasets are compared.
The al-gorithm designed specifically to deal withimbalanced datasets for most types outper-forms the standard classifier.
In the secondexperiment we show that classification re-sults improve when information on defini-tion structure is included.1 IntroductionDefinition extraction can be relevant in differ-ent areas.
It is most times used in the do-main of question answering to answer ?What-is?-questions.
The context in which we apply defini-tion extraction is the automatic creation of glos-saries within elearning.
This is a new area andprovides its own requirements to the task.
Glos-saries can play an important role within this do-main since they support the learner in decodingthe learning object he is confronted with and inunderstanding the central concepts which are be-ing conveyed in the learning material.Different approaches for the detection of def-initions can be distinguished.
We use a sequen-tial combination of a rule-based approach and ma-chine learning to extract definitions.
As a first stepa grammar is used and thereafter, machine learn-ing techniques are applied to filter the incorrectlyextracted data.Our approach has different innovative aspectscompared to other research in the area of defini-tion extraction.
The first aspect is that we addressless common definition patterns also.
Second, wecompared a common classification algorithm withan algorithm designed specifically to deal with im-balanced datasets (experiment 1), which seems tobe more appropriate for us because we have somedata sets in which the proportion of ?yes?-cases isextremely low.
A third innovative aspect is thatwe examined the influence of the type of gram-mar used in the first step (sophisticated or basic)on the final machine learning results (experiment1).
The sophisticated grammar aims at getting thebest balance between precision and recall whereasthe basic grammar only focuses at getting a highrecall.
We investigated to which extent machinelearning can improve the low precision obtainedwith the basic grammar while keeping the recallas high as possible and then compare the resultsto the performance of the sophisticated grammarin combination with machine learning.
As a lastpoint, we investigated the influence of definitionstructure on the classification results (experiment2).
We expect this information to be especiallyuseful when a basic grammar is used in the firststep, because the patterns matched with such agrammar can have very diverse structures.The paper is organized as follows.
Section 2 in-troduces some relevant work in definition extrac-tion.
Section 3 explains the data used in the experi-ments and the definition categories we distinguish.Section 4 discusses the way in which grammarshave been applied to extract definitions and theresults obtained with them.
Section 5 then talksabout the machine learning approach, covering is-sues such as the classifiers, the features and the ex-periments.
Section 6 and section 7 report and dis-cuss the results obtained in the experiments.
Sec-tion 8 provides the conclusions and presents somefuture work.882 Related researchResearch on the detection of definitions has beenpursued in the context of automatic building ofdictionaries from text, question-answering and re-cently also within ontology learning.In the area of automatic glossary creation, theDEFINDER system combines shallow natural lan-guage processing with deep grammatical analysisto identify and extract definitions and the termsthey define from on-line consumer health litera-ture (Muresan and Klavans, 2002).
Their approachrelies entirely on manually crafted patterns.
Animportant difference with our approach is that theystart with the concept and then search for a defini-tion of it, whereas in our approach we search forcomplete definitions.A lot of research on definition extraction hasbeen pursued in the area of question-answering,where the answers to ?What is?-questions usuallyare definitions of concepts.
In this area, they mosttimes start with a known concept (extracted fromthe question) and then search the corpus for snip-pets or sentences explaining the meaning of thisconcept.
The texts used are often well structured,which is not the case in our approach where anytext can be used.
Research in this area initiallyrelied almost totally on pattern identification andextraction (cf.
(Tjong Kim Sang et al, 2005)) andonly later, machine learning techniques have beenemployed (cf.
(Blair-Goldensohn et al, 2004;Fahmi and Bouma, 2006; Miliaraki and Androut-sopoulos, 2004)).Fahmi and Bouma (2006) combine patternmatching and machine learning.
First, candidatedefinitions which consist of a subject, a copularverb and a predicative phrase are extracted from afully parsed text using syntactic properties.
There-after, machine learning methods are applied on theset of candidate definitions to distinguish defini-tions from non-definitions; to this end a combina-tion of attributes has been exploited which refer totext properties, document properties, and syntac-tic properties of the sentences.
They show that theapplication of standard machine learning meth-ods for classification tasks (Naive Bayes, SVMand RBF) considerably improves the accuracy ofdefinition extraction based only on syntactic pat-terns.
However, they only applied their approachon the most common definition type, that are thedefinitions with a copular verb.
In our approachwe also distinguish other, less common definitiontypes.
Because the patterns of the other typesare more often also observed in non-definitions,the precision with a rule-based approach will belower.
As a consequence, the dataset for machinelearning will be less balanced.
In our approachwe applied ?
besides a standard classification al-gorithm (Naive Bayes) ?
also a classification al-gorithm designed specifically to deal with imbal-anced datasets.In the domain of automatic glossary creation,Kobylinski and Przepio?rkowski (2008) describean approach in which a machine learning algo-rithm specifically developed to deal with imbal-anced datasets is used to extract definitions fromPolish texts.
They compared the results obtainedwith this approach to results obtained on the samedata in which hand crafted grammars were used(Przepio?rkowski et al, 2007) and to results withstandard classifiers (Dego?rski et al, 2008).
Thebest results were obtained with their new ap-proach.
The differences with our approach arethat (1) they use either only machine learning oronly a grammar and not a combination of the twoand (2) they do not distinguish different defini-tion types.
The advantage of using a combina-tion of a grammar and machine learning, is thatthe dataset on which machine learning needs to beapplied is much smaller and less imbalanced.
Asecond advantage of applying a grammar first, isthat the grammar can be used to add informationto the candidate definitions which can be used inthe machine learning features.
Besides, applyingthe grammar first, gives us the opportunity to sep-arate the four definition types.3 DefinitionsDefinitions are expected to contain at least threeparts.
The definiendum is the element that is de-fined (Latin: that which is to be defined).
Thedefiniens provides the meaning of the definiendum(Latin: that which is doing the defining).
Definien-dum and definiens are connected by a verb orpunctuation mark, the connector, which indicatesthe relation between definiendum and definiens(Walter and Pinkal, 2006).To be able to write grammar rules we first ex-tracted 549 definitions manually from 45 Dutchtext documents.
Those documents consisted ofmanuals and texts on computing (e.g.
Word, La-tex) and descriptive documents on academic skillsand elearning.
All of them could be relevant learn-89Type Example sentenceto be Gnuplot is een programma om grafieken te maken?Gnuplot is a program for drawing graphs?verb E-learning omvat hulpmiddelen en toepassingen die via het internet beschikbaar zijn en creatieve mogeli-jkheden bieden om de leerervaring te verbeteren .
?eLearning comprises resources and application that are available via the Internet and provide creativepossibilities to improve the learning experience?punctuation Passen: plastic kaarten voorzien van een magnetische strip, die door een gleuf gehaald worden, waardoorde gebruiker zich kan identificeren en toegang krijgt tot bepaalde faciliteiten.
?Passes: plastic cards equipped with a magnetic strip, that can be swiped through a card reader, by meansof which the identity of the user can be verified and the user gets access to certain facilities.
?pronoun Dedicated readers.
Dit zijn speciale apparaten, ontwikkeld met het exclusieve doel e-boeken te kunnenlezen.
?Dedicated readers.
These are special devices, developed with the exclusive goal to make it possible to reade-books.
?Table 1: Examples for each of the definition types.ing objects in an elearning enivronment and arethus representative for the glossary creation con-text in which we will use definition extraction.Based on the connectors used in the found pat-terns, four common definition types were distin-guished.
The first type are the definitions in whicha form of the verb to be is used as connector.
Thesecond group consists of definitions in which averb (or verbal phrase) other than to be is used asconnector (e.g.
to mean, to comprise).
It also hap-pens that a punctuation character is used as con-nector (mainly :), such patterns are contained inthe third type.
The fourth category contains thedefinitory contexts in which relative or demonstra-tive pronouns are used to point back to a definedterm that is mentioned in a preceding sentence.The definition of the term then follows after thepronoun.
Table 1 shows an example for each ofthe four types.
To be able to test the grammar onunseen data, the definition corpus was split in adevelopment and a test part.
Table 2 shows somegeneral statistics of the corpus.Development Test Total# documents 33 12 45# words 286091 95722 381813# definitions 409 140 549Table 2: General statistics of the definition corpus.4 Using a grammarTo extract definition patterns two grammars havebeen written on the basis of 409 manually selecteddefinitions from the development corpus.
TheXML transducer lxtransduce developed by Tobin(2005) is used to match the grammars against filesin XML format.
Lxtransduce is an XML trans-ducer that supplies a format for the developmentof grammars which are matched against eitherpure text or XML documents.
The grammars areXML documents which conform to a DTD (lx-transduce.dtd, which is part of the software).The grammars consist of four parts.
In the firstpart, part-of-speech information is used to makerules for matching separate words.
The secondpart consists of rules to match chunks (e.g.
nounphrases, prepositional phrases).
We did not usea chunker, because we want to be able to put re-strictions on the chunks.
For example, to matchthe definiendum, we only want to select relativelysimple NPs (mainly of the pattern (Article) - (Ad-jective) - Noun(s)).
The third part contains rulesfor matching and marking definiendums and con-nectors.
In the last part the pieces are put togetherand the complete definition patterns are matched.The rules were made as general as possible to pre-vent overfitting to the corpus.Two types of grammars have been used: a basicgrammar and a sophisticated grammar.
With thebasic grammar, the goal is to obtain a high recallwithout bothering too much about precision.
Thenumber of rules for detecting the patterns is 26 ofwhich 6 fall in the first category (matching words),15 fall in the third part (matching parts of defi-nitions) and 5 fall in the fourth category (match-ing complete definitions).
There are no rules ofthe second category in this grammar (matchingchunks), because the focus is on the connector pat-terns only and not on the pattern of the definien-dum and definiens.
In the sophisticated grammarthe aim is to design rules in such a way that a highrecall is obtained while at the same time the pre-cision does not become very low.
This grammarcontains 40 rules, which is 14 more than containedin the basic grammar.
There are 12 rules in part 1,905 in part 2, 11 rules in the third part and 12 rulesin the last part.The first difference between the basic and thesophisticated grammar is thus the number of rules.However, the main difference is that the basicgrammar puts fewer restrictions on the patterns.Restrictions on phrases present in the sophisti-cated grammar such as ?the definiendum should bean NP of a certain structure?
are not present in thebasic grammar.
For example, to detect is patterns,the basic grammar simply marks all words beforea form of to be as definiendum and the completesentence containing a form of to be as definition.
(Westerhout and Monachesi, 2007) describes thedesign of the sophisticated grammar and the re-sults obtained with it in more detail.Table 3 shows that the recall is always higherwith the basic grammar is considerably, which iswhat you would expect because fewer restrictionsare used.
The consequence of using a less strictgrammar is that the precision decreases.
The gainof recall is much smaller than the loss in precision,and therefore the f-score is also lower when thebasic grammar is used.type corpus precision recall f-measureis SG 0.25 0.82 0.38BG 0.03 0.98 0.06verb SG 0.29 0.71 0.41BG 0.08 0.81 0.15punct SG 0.04 0.67 0.08BG 0.01 0.97 0.02pron SG 0.05 0.47 0.10BG 0.03 0.66 0.06all SG 0.13 0.70 0.22BG 0.03 0.86 0.06Table 3: Results with sophisticated grammar (SG)and basic grammar (BG) on the complete corpus.5 Machine learningThe second step is aimed at improving the preci-sion obtained with the grammars, while trying tokeep the recall as high as possible.
The sentencesextracted with the grammars are input for this step(table 3).
We thus have two datasets: the firstdataset contains sentences extracted with the ba-sic grammar and the second dataset contains sen-tences extracted with the sophisticated grammar.Because the datasets are relatively small, both de-velopment and test results have been included toget as much training data as possible.
As a con-sequence of using the output of the grammars asdataset, the definitions not detected by the gram-mar are lost already and cannot be retrieved any-more.
So, for example, the overall recall for the istype where the sophisticated grammar is used as afirst step can not become more than 0.82.The first classifier used is the Naive Bayes clas-sifier, a common algorithm for text classificationtasks.
However, because some of our datasetsare quite imbalanced and have an extremely lowpercentage of correct definitions, the Naive Bayesclassifier did not always perform very well.
There-fore, a balanced classifier has been used also forclassifying the data.
After describing the classi-fiers, the experiments and the features used withinthe experiments are discussed.5.1 Classifiers5.1.1 Naive Bayes classifierThe Naive Bayes classifier has often been usedin text classification tasks (Lewis, 1998; Mitchell,1997; Fahmi and Bouma, 2006).
Because of therelatively small size of our dataset and sparse-ness of the feature vector, the calculated numbersof occurrences were very small and we expectedthem to provide no additional information to theclassifier.
For this reason, we used superviseddiscretization (instead of normal distribution), inwhich numeric attributes are converted to nominalones, and in this way removed the information onthe number of times n-grams occurred in a partic-ular sentence.5.1.2 Balanced Random Forest classifierThe Naive Bayes (NB) classifier is aimed at get-ting the best possible overall accuracy and is there-fore not the best method when dealing with imbal-anced data sets.
In our experiments, all datasetsare more or less imbalanced and consist of a mi-nority part with definitions and a majority partwith non-definitions.
The extent to which thedataset is imbalanced differs depending on thetype and the grammar that has been applied.
Table4 shows for each type the proportion that consti-tutes the minority class with definitions.
As canbe seen from this table, the sets for is and verbdefinitions obtained with the sophisticated gram-mar are the most balanced sets, whereas the othersare heavily imbalanced.The problem of heavily imbalanced data canbe addressed in different ways.
The approach weadopted consists in a modification of the Random91SG (%) BG (%)is 24.6 3.0verb 28.9 8.1punct 4.8 1.0pron 5.4 2.9Table 4: Percentage of correct definitions in sen-tences extracted with sophisticated (SG) and basic(BG) grammar.Forest classifier (RF; (Breiman, 2001)).
In Bal-anced Random Forest (BRF; (Chen et al, 2004)),for each decision tree two bootstrapped sets of thesame size, equal to the size of the minority class,are constructed: one for the minority class, theother for the majority class.
Jointly, these two setsconstitute the training set.
In our experiments wemade 100 trees in which at each node from 20randomly selected features out of the total set offeatures the best feature was selected.
The finalclassifier is the ensemble of the 100 trees and de-cisions are reached by simple voting.
We expectthe BRF classifier to outperform the NB classifier,especially on the less balanced types.5.2 ExperimentsTwo experiments have been conducted.
Becausethe datasets are relatively small 10-fold cross val-idation has been used in all experiments for betterreliability of the classifier results.5.2.1 Comparing classifier typesIn the first experiment, the Naive Bayes and theBalanced Random Forest classifiers are compared,both on the data obtained with the sophisticatedand basic grammar.
As features n-grams of thepart-of-speech tags were used with n being 1, 2and 3.
The main purpose of this experiment is tocompare the performance of the two classifiers tosee which method performs best on our data.
Weexpect the advantage of using the BRF method tobe bigger when the datasets are more imbalanced,since the BRF classifier has been designed specifi-cally to deal with imbalanced datasets.
The secondpurpose of the experiment is to investigate whethercombining a basic grammar with machine learningcan give better results than a sophisticated gram-mar combined with machine learning.
Because thedatasets will be more imbalanced for each typewhen the basic grammar is used, we expect theBRF method to perform better than the NB classi-fier on the definition class.
However, the countereffect of using the balanced method will be that thescores on the non-definition class will be worse.5.2.2 Influence of definition structureIn the second experiment, we investigated whetherthe structure of a definition provides informa-tion that helps when classifying instances for thedatasets created with the basic grammar.
Asfeatures the part-of-speech tag n-grams of thedefiniendum, the first part-of-speech tag n-gramof the definiens and the part-of-speech tag n-grams of the complete sentence.
Because we haveseen when developing the sophisticated grammarthat the structure of the definiendum is very im-portant for distinguishing definitions from non-definitions, we decided to add information on thestructure of this part in the features of the data ob-tained with the basic grammar.
Also the first partof the definiens often seemed to have a comparablestructure, therefore we included this part as well inour features.
We expect that including this infor-mation will result in a better classification result.6 Results6.1 Comparing classifier typesTable 5 shows the results of the different classi-fiers.
When we look at the results for the sophis-ticated grammar, we see that for the less balanceddatasets (i.e.
the punct and pron types) the BRFclassifier outperforms the NB classifier.
For thesetwo types there were no definitions classified cor-rectly and as a consequence both the precision andthe recall are 0.
For the other two types the re-sults of the different classifiers are comparable.When the classifiers are used after the basic gram-mar has been applied, the recall is substantiallybetter for all four types when the BRF method isused.
However, the precision is quite low withthis approach, mainly due to the low scores forthe punct and pron types.
The accuracy of the re-sults, that is, the over all proportion of correctlyclassified instances, is in all cases higher whenthe Naive Bayes classifier is used.
This is dueto the fact that the number of misclassified non-definition sentences is higher when the BRF clas-sifier is used.Table 6 shows a comparison of the final resultsobtained with the sophisticated grammar and thebasic grammar in combination with the two ma-chine learning algorithms.
The performance varieslargely per type and the overall score is highly in-fluenced by the is and verb type, which together92Naive BayesSophisticated grammar Basic grammarprecision recall f-measure accuracy precision recall f-measure accuracyis 0.82 0.76 0.79 0.90 0.26 0.66 0.38 0.93verb 0.77 0.75 0.76 0.86 0.67 0.17 0.27 0.93punct 0 0 0 0.95 0 0 0 0.98pron 0.36 0.30 0.33 0.93 0 0 0 0.97all 0.72 0.61 0.66 0.92 0.29 0.32 0.31 0.95Balanced Random ForestSophisticated grammar Basic grammarprecision recall f-measure accuracy precision recall f-measure accuracyis 0.77 0.79 0.78 0.89 0.18 0.82 0.30 0.88verb 0.76 0.78 0.77 0.87 0.29 0.65 0.40 0.84punct 0.13 0.61 0.22 0.79 0.06 0.61 0.10 0.79pron 0.18 0.62 0.28 0.83 0.08 0.41 0.13 0.83all 0.43 0.74 0.55 0.84 0.15 0.68 0.24 0.85Table 5: Performance of Naive Bayes classifier and Balanced Random Forest classifier on the resultsobtained with the grammars.contain 69.8 % of the definitions.
For the othertwo types, the BRF classifier performs consider-ably better, independent of which grammar hasbeen used in the first step.
The overall f-measureis best when the sophisticated grammar is used,where the recall is higher with the BRF classifierand the precision is better with the NB classifier.Naive Bayesgrammar precision recall f-measureis SG 0.82 0.62 0.70BG 0.26 0.65 0.37verb SG 0.77 0.53 0.63BG 0.67 0.14 0.23punct SG 0 0 0BG 0 0 0pron SG 0.36 0.14 0.20BG 0 0 0all SG 0.72 0.43 0.54BG 0.29 0.27 0.28Balanced Random Forestgrammar precision recall f-measureis SG 0.77 0.65 0.70BG 0.18 0.80 0.30verb SG 0.76 0.55 0.64BG 0.29 0.53 0.37punct SG 0.13 0.42 0.20BG 0.06 0.52 0.10pron SG 0.18 0.29 0.22BG 0.08 0.27 0.12all SG 0.43 0.52 0.47BG 0.15 0.57 0.24Table 6: Final results of sophisticated grammar(SG) and basic grammar (BG) in combination withNaive Bayes classifier and Balanced Random For-est classifier.6.2 Influence of definition structureTable 7 shows the results obtained with the BRFclassifier on the sentences extracted with the ba-sic grammar when sentence structure is taken intoaccount.
When we compare these results to ta-ble 5, we see that the overall recall is higher whenstructural information is provided to the classifier.However, to which extent the structural informa-tion contributes to a correct classification of thedefinitions is different per type and also dependson the amount of structural information provided.When only information on the definiendum andfirst part of the definiens are included, the pre-cision scores are lower than the results obtainedwith n-grams of the complete sentence.
Providingall information, that is, information on definien-dum, first part of the definiens and the completesentence, gives the best results.All informationprecision recall f-measure accuracyis 0.24 0.82 0.38 0.92verb 0.29 0.81 0.43 0.82punct 0.04 0.84 0.08 0.58pron 0.09 0.54 0.16 0.83all 0.14 0.78 0.24 0.82Definiendum and first n-gram of definiensprecision recall f-measure accuracyis 0.19 0.82 0.31 0.89verb 0.25 0.78 0.38 0.80punct 0.03 0.96 0.05 0.23pron 0.05 0.57 0.09 0.65all 0.09 0.78 0.16 0.71Table 7: Performance of Balanced Random Forestclassifier with information on sentence structure infeatures applied on the results obtained with thebasic grammar.For the is type, the recall remains the samewhen structural information is added and the pre-cision increases, especially when all structural in-93formation is used.
Information on the structure ofthe definiens and the first n-gram of the definiensthus improves the classification results for thistype.The recall of verb definitions is higher whenstructural information is used whereas the preci-sion does not change.
The fact that the precision ishardly influenced by adding structural informationmight be explained by the fact that connectors andconnector phrases are quite diverse for this type.As a consequence, different types of first n-gramsof the definiens might be used and the predictingquality of structural information is smaller.The classification of the punct patterns is quitedifferent depending on the amount of structural in-formation used.
The recall increases when struc-tural information is added, whereas the precisiondecreases.
Adding structural information thus re-sults in a low accuracy, especially when only then-grams of the definiendum and the first n-gram ofthe definiens are used.
For this type of patterns thestructure of the complete definition is thus impor-tant for obtaining a reasonable precision.For the pronoun patterns the recall is higherwhen structural information is included.
The pre-cision is slightly higher when all structural infor-mation is included, but remarkably lower whenonly the n-grams of the definiendum and the firstn-gram of the definiens are used.
From this we canconclude that for this pattern type information onthe structure of the complete definition is crucialto get a reasonable precision.7 Evaluation and discussionWhich classifier performs best depends on the bal-ance of the corpus.
For the more balanced datasetsthe results of the NB and the BRF method are al-most the same.
The more imbalanced the corpus,the bigger the difference between the two meth-ods, where BRF outperforms the NB classifier.The accuracy is in all cases higher when the NBclassifier is used, due to the fact that this classi-fier scores better on the majority part with non-definitions.
The inevitable counter effect of usingthe BRF method is that the scores on this part arelower, because the two classes now get the sameweight.The answer to the question which grammarshould be used in the first step can be viewed fromdifferent perspectives, by looking either at the goalor the definition type.When aiming at getting the highest possible re-call, the BRF method in combination with the ba-sic grammar gives the best overall results.
How-ever, when using these settings, the precision isquite low.
When the goal is to obtain the bestbalance between recall and precision, this mighttherefore not be the best choice.
In this case, thebest option would be to use a combination of thesophisticated grammar and the BRF method, inwhich the recall is slightly lower than when thebasic grammar is used, but the precision is muchhigher.We can also view the question which gram-mar should be used from a different perspective,namely by looking at the definition type.
To getthe best result for each of the separate types, wewould need to use different approaches for the dif-ferent types.
When the BRF method is used, fortwo types the recall is considerably higher whenthe basic grammar is used, whereas for the othertwo types the recall scores are comparable for thetwo grammars.
However, again this goes with alower precision score, and therefore this may notbe the favourable solution in a practical applica-tion.
So, also when looking at a per type basis, us-ing the sophisticated grammar seems to be the bestoption when the aim is to get the best balance.We are now able to answer the questions ad-dressed in the first experiment and summarizeour conclusions on which classifier and grammarshould be used in table 8.
The conclusions arebased on the final results obtained after both thegrammar and machine learning have been applied(table 6).
Although the recall is very important,because of the context in which we want to applydefinition extraction the precision also cannot betoo low.
In a practical application a user wouldnot like it to get 5 or 6 incorrect sentences for eachcorrect definition.Best recall Best balanceis BG + BRF SG + NB / BRFverb SG + NB / BRF SG + NB / BRFpunct BG + BRF SG + BRFpron SG / BG + BRF SG + BRFTable 8: Best combination of grammar and classi-fier when aiming at best recall or best balance.Information on structure in all cases results ina higher number of correctly classified definitions.The recall for the definition class is for all typesremarkably higher when only the n-grams of the94definiendum and the first n-gram of the definiensare considered.
However, this goes with a muchlower precision and f-score and might thereforenot be the best option.
When using all informa-tion, the best results are obtained: the recall goesup while the precision and f-score do not changeconsiderably.
However, although the results areimproved, they are still lower then the results ob-tained with the sophisticated grammar.A question that might rise when looking at theresults for the different types, is whether the punc-tuation and pronoun patterns should be includedwhen building an application for extracting defini-tions.
Although these types are present in texts ?they make up 30 % of the total number of defini-tions ?
and can be extracted with our methods, theresults are poor compared to the results obtainedfor the other two types.
Especially the bad preci-sion for these types gives reasons to have a closerlook at these patterns to discover the reason forthese low scores.
The bad results might be causedby the amount of training data, which might be toolow.
Another reason might be that the patterns aremore diverse than the patterns of the other types,and therefore more difficult to detect.It is difficult to compare our results to otherwork on definition extraction, because we are theonly who distinguish different types.
However, wetry to compare research conducted by Fahmi andBouma (2006) on the first pattern and Kobylin?skiand Przepio?rkowski (2008) on definitions in gen-eral.
Fahmi and Bouma (2006) combined a rule-based approach and machine learning for the de-tection of is definitions in Wikipedia articles.
Al-though they used more structured texts, the accu-racy they obtained is the same as the accuracy weobtained in our experiments.
However, they didnot report precision, recall, and f-score for the def-inition class separately, which makes it difficultto compare their result to ours.
Kobylin?ski andPrzepio?rkowski (2008) applied machine learningon unstructured texts using a balanced classifierand obtained a precision of 0.21, a recall of 0.69and an f-score of 0.33 with an overall accuracy of0.85.
These scores are comparable to the scoreswe obtained with the basic grammar in combina-tion with the BRF classifier.
Using the sophisti-cated grammar in combination with BRF outper-forms the results they obtained.
From this we canconclude that using a sophisticated grammar hasadvantages over using machine learning only.8 Conclusions and future workOn the basis of the results we can draw some con-clusions.
First, the type of grammar used in thefirst step influences the final results.
With the fea-tures and classifiers used in our approach, the so-phisticated grammar gives the best results for alltypes.
The added value of a sophisticated gram-mar is also confirmed by the fact that the resultsKobylin?ski and Przepio?rkowski (2008) obtainedwithout using a grammar are lower then our re-sults with the sophisticated grammar.
A secondlesson learned is that it is useful to distinguish dif-ferent definition types.
As the results vary depend-ing on which type has to be extracted, adaptingthe approach to the type to be extracted will re-sult in a better overall performance.
Third, the de-gree to which the dataset is imbalanced influencesthe choice for a classifier, where the BRF performsbetter on less balanced datasets.
As there are manyother NLP problems in which there is an interest-ing minority class, the BRF method might be ap-plied to those problems also.
From the second ex-periment, we can conclude that taking definitionstructure into account helps to get better classifi-cation results.
This information has not been im-plemented in other approaches yet and other workon definition extraction can thus profit from thisnew insight.The results obtained so far clearly indicate thata combination of a rule-based approach and ma-chine learning is a good way to extract defini-tions from texts.
However, there is still room forimprovement, and we will work on this in thenext months.
In near future, we will investigatewhether our results improve when more linguisticinformation is added in the features.
Especiallyfor the basic grammar, we expect it to be possi-ble to get a better recall when more informationis added.
We can make use of the grammar rulesimplemented in the sophisticated grammar to seethere which information might be relevant.
To im-prove the precision scores obtained with the so-phisticated grammar, we will also look at linguis-tic information that might be relevant.
However,improving this score using linguistic informationwill be more difficult, because the grammar al-ready filtered out a lot of incorrect patterns.
Toimprove results obtained with this grammar, wewill therefore look at different features, such asfeatures based on document structure, keywordi-ness of definiendum and similarity measures.95ReferencesS.
Blair-Goldensohn, K. R. McKeown, andA.
Hazen Schlaikjer, 2004.
New Directions InQuestion Answering, chapter Answering Defini-tional Questions: A Hybrid Approach.
AAAIPress.L.
Breiman.
2001.
Random Forests.
Machine Learn-ing, 46:5?42.C.
Chen, A. Liaw, and L. Breiman.
2004.
Using ran-dom forest to learn imbalanced data.
Technical Re-port 666, University of California, Berkeley.?.
Dego?rski, M. Marcin?czuk, and A. Przepio?rkowski.2008.
Definition extraction using a sequential com-bination of baseline grammars and machine learningclassifiers.
In Proceedings of the Sixth InternationalConference on Language Resources and Evaluation,LREC 2008.I.
Fahmi and G. Bouma.
2006.
Learning to iden-tify definitions using syntactic features.
In R. Basiliand A. Moschitti, editors, Proceedings of the EACLworkshop on Learning Structured Information inNatural Language Applications.?.
Kobylin?ski and A. Przepio?rkowski.
2008.
Defi-nition extraction with balanced random forests.
InB.
Nordstro?m and A. Ranta, editors, Advances inNatural Language Processing: Proceedings of the6th International Conference on Natural LanguageProcessing, GoTAL 2008, pages 237?247.
SpringerVerlag, LNAI series 5221.D.
D. Lewis.
1998.
Naive (Bayes) at forty: The in-dependence assumption in information retrieval.
InClaire Ne?dellec and Ce?line Rouveirol, editors, Pro-ceedings of ECML-98, 10th European Conferenceon Machine Learning, number 1398, pages 4?15,Chemnitz, DE.
Springer Verlag, Heidelberg, DE.S.
Miliaraki and I. Androutsopoulos.
2004.
Learn-ing to identify single-snippet answers to definitionquestions.
In Proceedings of COLING 2004, pages1360?1366.T.
M. Mitchell.
1997.
Machine learning.
McGraw-Hill.S.
Muresan and J. Klavans.
2002.
A method for au-tomatically building and evaluating dictionary re-sources.
In Proceedings of the Language Resourcesand Evaluation Conference (LREC 2002).A.
Przepio?rkowski, ?.
Dego?rski, M. Spousta,K.
Simov, P. Osenova, L. Lemnitzer, V. Kubon,and B. Wo?jtowicz.
2007.
Towards the automaticextraction of denitions in Slavic.
In Proceedings ofBSNLP workshop at ACL.E.
Tjong Kim Sang, G. Bouma, and M. de Rijke.
2005.Developing offline strategies for answering medicalquestions.
In D. Molla?
and J. L. Vicedo, editors,Proceedings AAAI 2005 Workshop on Question An-swering in Restricted Domains.R.
Tobin.
2005.
Lxtransduce, a replace-ment for fsgmatch.
http://www.ltg.ed.ac.uk/?richard/ltxml2/lxtransduce-manual.html.S.
Walter and M. Pinkal.
2006.
Automatic extractionof definitions from German court decisions.
In Pro-ceedings of the workshop on information extractionbeyond the document, pages 20?28.E.
N. Westerhout and P. Monachesi.
2007.
Extractionof Dutch definitory contexts for elearning purposes.In Proceedings of CLIN 2006.96
