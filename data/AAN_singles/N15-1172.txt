Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1483?1493,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsEchoes of Persuasion:The Effect of Euphony in Persuasive CommunicationMarco GueriniTrento RISEPovo, I-38100 Trentomarco.guerini@trentorise.euG?ozde?OzbalFBK-IrstPovo, I-38100 Trentogozbalde@gmail.comCarlo StrapparavaFBK-IrstPovo, I-38100 Trentostrappa@fbk.euAbstractWhile the effect of various lexical, syntactic,semantic and stylistic features have been ad-dressed in persuasive language from a com-putational point of view, the persuasive effectof phonetics has received little attention.
Bymodeling a notion of euphony and analyzingfour datasets comprising persuasive and non-persuasive sentences in different domains (po-litical speeches, movie quotes, slogans andtweets), we explore the impact of sounds ondifferent forms of persuasiveness.
We con-duct a series of analyses and prediction exper-iments within and across datasets.
Our resultshighlight the positive role of phonetic deviceson persuasion.1 Hocus PocusHistorically, in human sciences, several definitionsof persuasion have been proposed ?
see for exam-ple (Toulmin, 1958; Walton, 1996; Chaiken, 1980;Cialdini, 1993; Petty and Cacioppo, 1986).
Mostof them have a common core addressing: method-ologies aiming to change the mental state of the re-ceiver by means of communication in view of a pos-sible action to be performed by her/him.
(Perelmanand Olbrechts-Tyteca, 1969; Moulin et al, 2002).These methodologies might take into account theoverall structure of a text such as the ordering of thearguments or simply single word choices.
For a suc-cessful text both of them are often required.
The fo-cus of persuasion may vary according to the goal ofthe communication and it can take different formsaccording to the domain: from memorability (e.g.,making people remember a statement or a product)to diffusion (e.g., making people pass on a contentin social networks by sharing it), from behavioralchange (e.g., political communication) to influenc-ing purchasing decisions (e.g., slogans to convincepeople to try or buy a product) ?
see for exam-ple (Heath and Heath, 2007).
While many tech-niques such as resorting to expert opinion, utilizingthe framing effect, emotive language or exaggera-tion can be used to obtain such persuasive effects,we devote this study to explore particular techniquespertaining to euphony.Euphony refers to the inherent pleasantness ofthe sounds of words, phrases and sentences, and itis utilized to achieve pleasant, rhythmical and har-monious effects.
The idea that the pleasantnessof the sounds in a sentence can foster its effec-tiveness is rooted in our culture, and is connectedto the concepts of rhythm and music.
The factthat language and music interact in our brain hasbeen shown by localizing low-level syntactic pro-cesses of music and language in the temporal lobe(Sammler et al, 2013).
It has also been shown thatchanges in the cardiovascular and respiratory sys-tems can be induced by music ?
specifically tempo,rhythm, melodic structure (Bernardi et al, 2006).The importance of euphony has its roots also in an-cient human psychology.
As Julian Jaynes suggests(Jaynes, 2000), poetry used to be divine knowledge.It was the sound and tenor of authorization and itcommanded where plain prose could only ask.
Aparadigmatic example of this conception is the act ofcasting a spell.
Spells (incantations) are special lin-guistic objects that are meant not only to change how1483people think or behave but they are also so powerfulthat they can ?
allegedly ?
change reality.
Spells areoften very euphonic (and meaningless) sentences,e.g.
?Hocus Pocus?.Various psycholinguistic studies addressed the ef-fects of phonetics on the audience in different as-pects such as memorability (Wales, 2001; Benczes,2013) or more specifically advertisement (Leech,1966; Bergh et al, 1984).
There are also com-putational studies that address the problem of rec-ognizing persuasive sentences according to varioussyntactic, lexical and semantic features (Danescu-Niculescu-Mizil et al, 2012; Tan et al, 2014).
How-ever, to the best of our knowledge, the direct impactof phonetic elements on persuasiveness has not beenexplored in computational settings yet.In this paper, we fill in this gap by conductinga series of analyses and prediction experiments onfour datasets representing different aspects of per-suasive language to evaluate the importance of a setof phonetic devices (i.e.
rhyme, alliteration, homo-geneity and plosives) on various forms of persua-siveness.
Our experiments show that phonetic fea-tures play an important role in the detection of per-suasiveness and encode a notion of ?melodious lan-guage?
that operates both within and across datasets.2 Related WorkIn the following, we first revise some NLP studiesaddressing linguistic features of successful commu-nication.
Then, we summarize a selection of studiesdevoted to the effects of phonetics on persuasion.2.1 NLP studies on persuasionBerger and Milkman (2009) focus on a particularform of persuasion by using New York Times ar-ticles to examine the relationship between virality(i.e., the tendency of a content to be circulated onthe Web) and emotions evoked by the content.
Theyconduct semi-automated sentiment analysis to quan-tify the affectivity and emotionality of each article.Results suggest a strong relationship between affectand virality, in this case measured as the count ofhow many people emailed each article.
As sug-gested by the authors, this metric represents a formof ?narrowcasting?, as opposed to other ?broadcast-ing?
actions such as sharing on Twitter.Another line of research investigates the impact ofvarious textual features on audience reactions.
Thework by Guerini et al (2011) correlates several vi-ral phenomena with the wording of a post, whileGuerini et al (2012) show that features such as thereadability level of an abstract influence the numberof downloads, bookmarking and citations.A particular approach to content virality is pre-sented by Simmons et al (2011), who explore theimpact of different types of modification on memesspreading from one person to another.Danescu-Niculescu-Mizil et al (2012) measure adifferent ingredient of persuasion by analyzing thefeatures of a movie quote that make it ?memorable?.They compile a corpus consisting of memorable andnon-memorable movie quote pairs and conduct a de-tailed analysis to investigate the lexical and syntacticdifferences between these pairs.Louis and Nenkova (2013) focus on influentialscience articles in newspapers by considering char-acteristics such as readability, description vividness,use of unusual words and affective content.
Highquality articles (NYT articles appearing in ?The BestAmerican Science Writing?
anthology) are com-pared against typical NYT articles.Borghol et al (2012) investigate how differencesin textual description affect the spread of content-controlled videos.
Lakkaraju et al (2013) focus onthe act of resubmissions (i.e., content that is submit-ted multiple times with multiple titles to multipledifferent communities) to understand the extent towhich each factor influences the success of a con-tent.
Tan et al (2014) consider how content spreadsin an on-line community by pinpointing the effect ofwording in terms of content informativeness, gen-erality and affect.
Althoff et al (2014) develop amodel that can predict the success of requests for afree pizza gifted from the Reddit community.
Theauthors consider high-level textual features such aspoliteness, reciprocity, narrative and gratitude.2.2 Studies on the effects of phoneticsBenczes (2013) states that alliteration and rhyme canbe considered as attention-seeking devices as theyenhance emphasis.
The author also suggests thatthey are useful for acceptability and long-term reten-tion of original expressions, decrypting their mean-ings, indicating informality, and breaking the ice be-1484tween an audience and a speaker.
Therefore, thesedevices are commonly used in original metaphoricaland metonymical compounds.According to Leech (1966), phonetic devices suchas rhyme and alliteration are systematically ex-ploited by advertisers to achieve memorability.
Sim-ilarly, Wales (2001) underlines the effectiveness ofalliteration and rhyme on emphasis and memorabil-ity of an expression.The relation between the usage of plosives (i.e.,consonants in which the vocal tract is blocked so thatall airflow ceases, such as ?p?, ?t?
or ?k?)
and mem-orability has also been investigated.
According tothe study carried out by Bergh et al (1984) brandnames starting with plosive sounds are recalled andrecognized more than the ones starting with othersounds.
?Ozbal et al (2012) carry out an analysisof brand names and discover that plosives are verycommonly used.Danescu-Niculescu-Mizil et al (2012), whom wepreviously mentioned, carry out an auxiliary analy-sis and observe the differences in letter and sounddistribution (e.g.
usage of labials or front vowels,back sounds, coordinating conjunctions) of memo-rable and non-memorable quotes.
?Ozbal et al (2013) propose a phonetic scorerfor creative sentence generation such that generatedsentences can contain various phonetic features in-cluding alliteration, rhyme and plosive sounds.
Theauthors evaluate the proposed model on automaticslogan generation.
In a more recent work (?Ozbal etal., 2014), they enforce the existence of these fea-tures in the sentences that are automatically gener-ated for second language learning to introduce hooksto echoic memory.3 Phonetic ScorerFor the design of the phonetic features, we weremostly inspired by the work of?Ozbal et al (2013),who built and used three phonetic scorers for cre-ative sentence generation.
Similarly to this work,all the phonetic features that we used are basedon the phonetic representation of English words ofthe Carnegie Mellon University pronouncing dictio-nary1.
We selected four classes of phonetic devices,1The CMU pronunciation dictionary is freely avail-able at http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
We have used version 0.7a in our implementation.namely plosives, alliteration, rhyme and homogene-ity, which can easily be modeled by observing thedistribution of specific classes of phonemes withinthe sentence.
The plosive score is calculated as theratio of the number of plosive sounds in a sentenceto the overall number of phonemes.
For both alliter-ation and rhyme scorers, we provide a na?
?ve imple-mentation that does not consider stresses or sylla-bles, but only counts the number of repeated soundsat the beginning or end of words in the sentence.The alliteration score is calculated as the numberof repeated phonetic prefixes in a sentence normal-ized by the total number of phonemes.
Similarly, therhyme score is calculated as the ratio of the numberof repeated phonetic endings in a sentence to the to-tal number of phonemes.
Lastly, the homogeneityscorer simply calculates the degree of homogeneityin terms of phonemes used in a sentence indepen-dently from their positions.
If we let dphbe the countof distinct phonemes and tphbe the total count ofphonemes in a sentence, then the homogeneity scoreis calculated as 1?
(dph/tph).4 DatasetIn this section, we describe the four datasets we usedto conduct our analyses and experiments.
As wementioned previously, the definition of persuasionis a debated topic and it can comprise distinct strate-gies or facets.
For this reason, we experimented withdatasets where at least one ingredient is clearly inthe equation.
To explore the effects of wording andeuphonics on persuasion, the datasets were built ina controlled setting (topic, author, sentence length)to avoid confounding factors such as author or topicpopularity, by following the procedure described in(Danescu-Niculescu-Mizil et al, 2012; Tan et al,2014).
In addition, these datasets comprise shorttexts (mostly single sentences) to focus on surfacerealization of persuasion, where strategic planning?
which might act as a confounding factor ?
plays aminor role.
The idea of using controlled experiments(usually in an A/B test setting) to study persuasivecommunication can be traced back at least to Hov-land et al (1953).
While two of these datasets (Twit-ter and Movies) were already available, the othertwo (CORPS and Slogans) were collected by fol-lowing the methodology proposed in the first two as1485closely as possible2.All datasets are built around the core idea of col-lecting pairs consisting of a persuasive sentence (P )and a non-persuasive counterpart (?P ), where Pand ?P are structurally very similar and controlledfor the above mentioned confounding factors.Twitter.
A set of 11,404 tweet pairs, where eachpair comes from the same user (author control) andcontains the same URL (topic control).
P and ?Pare determined based on their retweet counts (Tanet al, 2014).
It is worth noting that in our experi-ments we were able to collect only 11,019 of suchtweet pairs since some of them were deleted in themeanwhile.Movie.
A set of 2,198 single-sentence memo-rable movie quotes (P ) paired with non-memorablequotes (?P ).
For each P , the dataset contains a con-trasting quote ?P from the same movie such that (i)P and?P are uttered by the same speaker, (ii) P and?P have the same number of words, (iii) ?P doesnot occur in the IMDb list of memorable quotes and(iv) P and ?P are as close as possible to each otherin the script (Danescu-Niculescu-Mizil et al, 2012).CORPS.
A set of 2,600 sentence pairs utteredby various politicians.
We collected these pairsfrom CORPS, a freely available corpus of politicalspeeches tagged with audience reactions (Gueriniet al, 2013).
The methodology that we used tobuild the pairs is very similar to Danescu-Niculescu-Mizil et al (2012): for each P , where P isthe sentence preceding an audience reaction (e.g.APPLAUSE, LAUGHTER), we selected a contrast-ing single-sentence ?P from the same speech.
Werequired ?P to be close to P in the speech transcrip-tion, subject to the conditions that (i) P and ?P areuttered by the same speaker - which is trivial sincethese are monologues, where a single speaker is ad-dressing the audience - (ii) P and ?P have the samenumber of words, and (iii) ?P is 5 to 15 sentencesaway from P .
This last condition had to be imposedsince, differently from movie quotes, we do not havethe evidence of which fragment of the speech ex-actly provoked the audience reaction (i.e.
it could bethe combination of more than one sentence).Slogan.
A set of 1,533 slogans taken from on-2CORPS and Slogans datasets can be downloaded at the fol-lowing link: https://github.com/marcoguerini/paired_datasets_for_persuasion/line resources paired with non-slogans that are sim-ilar in content.
We collected the non-slogans fromthe subset of the New York Times articles in En-glish GigaWord ?
5th Edition ?
released by Linguis-tic Data Consortium (LDC)3.
For each slogan, wepicked the most similar sentence in the New YorkTimes articles having the same length and the high-est LSA similarity (Deerwester et al, 1990) with theslogan.
The LSA similarity approach that we usedto collect the non-slogans is very similar to the ap-proach used by Louis and Nenkova (2013) to collectthe non-persuasive counterparts of successful newsarticles.In Table 1, we sum up the criteria used in theconstruction of each dataset.
As can be observedfrom the table, each dataset satisfies at least two ofthe three criteria described above.
In the last twoCriterion LengthDATASET Author Length Topic P ?PCORPS 3 3 7 14.0 14.0Movie 3 3 7 9.7 9.7Slogan 7 3 3 5.0 5.0Twitter 3 7 3 16.2 15.4Table 1: Criteria used in the construction of eachdataset and average token length of persuasive and non-persuasive pairscolumns of the table, we also provide the averagetoken length of the persuasive and non-persuasivesentences in each dataset.
Finally, in Table 2 we pro-vide examples of euphonic and persuasive sentencesfor each dataset together with their phonetic scores.5 Data AnalysisTo provide a first insight on the data, in Table 3 wereport the average phonetic scores for each data set(Mann-Whitney U Test is used for statistical sig-nificance between P and ?P samples, with Bon-ferroni correction to ameliorate issues with multiplecomparisons).
The results are partially in line withour expectations of the euphony phenomena beingmore relevant in the persuasive sentences across thedatasets.As can be observed from the table, the averagerhyme scores are higher in persuasive sentences and3http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T071486Dataset Example Rhyme Alliteration Plosive HomogeneityCORPSI think we can do better and I think we must do better.
0.789 0.737 0.342 0.631It will be waged with determination and it will be waged until we win.
0.566 0.679 0.189 0.736MovieThe night time is the right time.
0.818 0.545 0.181 0.636Beautiful.... beautiful butterfly... 0.667 0.708 0.250 0.583Dog eat dog, brother.
0.533 0.533 0.400 0.400SloganDifferent Stores, Different Stories.
0.621 0.896 0.207 0.690Why ask why?
Try Bud Dry 0.625 0.625 0.312 0.437Live, Love, Life.
0.818 0.909 0.0 0.636TwitterA Nerd in Need is a Nerd indeed.
0.636 0.727 0.227 0.681Easter cupcake baking!!
0.0 0.0 0.412 0.470Table 2: Euphonic examples of persuasive sentences from each dataset, along with their phonetic scores.Rhyme Alliteration Plosive HomogeneityDataset ?
?
?
?
?
?
?
?CORPS?P0.233 0.143 0.208 0.142 0.187 0.058 0.603 0.173CORPSP0.245?
0.152 0.223** 0.154 0.194*** 0.060 0.588** 0.179Movie?P0.196 0.143 0.167 0.142 0.191 0.073 0.485 0.155MovieP0.214* 0.165 0.196*** 0.164 0.185?
0.067 0.526*** 0.164Slogan?P0.071 0.111 0.047 0.092 0.204 0.098 0.343 0.163SloganP0.140*** 0.194 0.123*** 0.185 0.189*** 0.098 0.366*** 0.156Twitter?P0.204 0.116 0.180 0.114 0.188 0.058 0.617 0.134TwitterP0.216*** 0.121 0.193*** 0.120 0.185** 0.055 0.636*** 0.128Table 3: Average phonetic scores for our datasets - ***, p < .001; **, p < .01; *, p < .05; ?, not significantthe difference is highly significant for Slogan andTwitter (p < .001), slightly significant for Moviequotes (p < .05), but not significant for CORPS.The average alliteration scores are again higherin persuasive sentences and all the differences arehighly significant in all datasets (apart from CORPSwith p < .01).
Plosives seem not to correlate wellwith our intuition of persuasiveness and euphony:either there is no significance (movie quotes) or theaverages of euphonic scores are higher in the non-persuasive sentences (the difference is highly sig-nificant in slogans, and significant in Twitter).
Theonly dataset that meets our expectation is CORPSwith a highly significant difference in favor of per-suasive sentences.
Finally, the average homogeneityscores are significantly (p < .001) higher in persua-sive sentences in all datasets except CORPS, wherethe scores of non-persuasive sentences are signifi-cantly higher (p < .01) than persuasive ones.Without going into details of cross-dataset com-parisons we would like to note that CORPS seemsa very peculiar dataset in terms of average scores,as compared to the others.
In terms of rhyme andalliteration, the average scores of non-persuasivesentences (?P ) in CORPS are always higher thanthe persuasive sentences (P ) in the other datasets(p < .001 in all cases), while for homogeneity thesame holds apart from Twitter.
These results mayderive from the fact that a political speech is a care-fully crafted text ?
aimed at influencing the audience?
in its entirety, so also ?non-persuasive?
sentencesin CORPS are on average more persuasive than inother datasets.As a next step, we conducted another analysis onthe distribution of ?extreme cases?, i.e.
sentencesthat have a very high phonetic score at least in onefeature.
This analysis derives from the intuition thata euphonic sentence might be recognized as such byhumans only if its phonetic scores are above a cer-tain threshold.
In fact, sound repetition in a sentencemay occur by chance, as in ?I saw the knife in thedrawer?, and the longer the sentence is, the higherthe probability that phonetic scores will be non-zeroeven in absence of a euphonic effect.
Therefore, theaverage scores for each phonetic device, as reportedin Table 3, are only partially informative.Given this premise, to evaluate the ?persuasivepower?
of the phonetic devices taken into account,1487Dataset?Frh(t)?Fal(t)?Fpl(t)?Fho(t)CORPS?P0.025 0.012 0.362 0.394CORPSP0.033** 0.023** 0.415*** 0.363?Movie?P0.018 0.011 0.397 0.092MovieP0.041*** 0.025*** 0.363?
0.173***Slogan?P0.005 0.003 0.460 0.011SloganP0.055*** 0.043*** 0.410*** 0.018***Twitter?P0.006 0.003 0.385 0.377TwitterP0.012*** 0.008*** 0.364** 0.449***Table 4: Probability of examples above threshold, - ***,p < .001; **, p < .01; *, p < .05; ?, not significantwe compare them in terms of empirical Complemen-tary Cumulative Distribution Functions (CCDFs)of the persuasive/non-persuasive pairs in variousdatasets.
These functions are commonly used toanalyze online social networks in terms of growthin size and activity (see for example (Ahn et al,2007; Jiang et al, 2010; Leskovec, 2008)) andalso for measuring content diffusion, e.g.
thenumber of retweets of a given content (Kwak etal., 2010).
Here, we use CCDFs to account forthe probability P that the score of a phonetic de-vice d will be greater than n indicating it with?Fd(n).
For example, the probability of having atext with more than .75 rhyme score is indicatedwith?Frh(.75) = P(#rhyme > .75).
To as-sess whether the CCDFs of the several types oftexts we take into account show significant differ-ences, we use the Kolmogorov-Smirnov goodness-of-fit test, which specifically targets cumulative dis-tribution functions.
In particular, for each phoneticdevice and dataset, we use a two-tailed Kolmogorov-Smirnov test (again with Bonferroni correction) totest whether the number of examples above thethreshold is higher in the persuasive sentences thanin their non-persuasive counterparts for that device.Since we do not have a theoretical way to definesuch thresholds, we resort to empirically define themby using a specific dataset of euphonic sentences.Even if it might seem reasonable to consider po-ems as paradigmatic examples of ?euphonic?
writ-ing, we discard them as the phonetic devices used inpoems may span across sentences.
Instead, we re-sort to tongue twisters as a gold reference of how aeuphonic sentence should be.
Accordingly, we col-lected a set of 534 tongue twisters from various on-line resources.
Then, for each phonetic index we de-fined our thresholds as the average of the phoneticscores in this data, in particular: trh= 0.55 forrhyme, tal= 0.58 for alliteration, tpl= 0.20 forplosives and tho= 0.68 for homogeneity.In Table 4, we report the results of our CCDFanalysis.
After analyzing the ?extreme cases?,where euphony is granted, we see that the trendsfound in Table 3 on the correlation between persua-siveness and euphony are confirmed and strength-ened.
The number of persuasive sentences with arhyme score above threshold is 30% more than thenon-persuasive ones in CORPS, while the differ-ence is 90% in Twitter4.
The ratio of persuasivesentences above threshold to non-persuasive ones isvery high in movies and slogans (more than 2 and10 respectively).
All results are either highly sig-nificant or significant.
For comparison, in Table 3these differences are not significant for CORPS andonly slightly significant (p < .05) for movies.
Con-cerning alliteration, there are 85% more cases abovethreshold in the persuasive sentences of CORPS thanthe non-persuasive ones.
For movie quotes andTwitter, the persuasive sentences above threshold aremore than two times as many as the non-persuasiveones, while the ratio is more than 13 for slogans.All results are either highly significant or signifi-cant in line with the results of Table 3.
Instead, forplosive scores we observe a negative or no correla-tion with persuasiveness, the only exception beingCORPS.
Regarding homogeneity, for CORPS thedifference between persuasive and non-persuasivesentences is not significant (in Table 3 it was signifi-cantly in favor of non-persuasive sentences), whilefor the other datasets there is a highly significantdifference in favor of persuasive sentences (between20% and 80%).
As a whole, these results confirmour intuition that phonetic features play a significantrole with respect to persuasiveness.
In the next sec-tion we will validate this claim by means of predic-tion experiments.6 Prediction ExperimentsIn this section, we describe the prediction tasks (bothwithin and across datasets) that we carried out to in-4In the following the ratios are computed on the real valueswhile Table 4 presents the rounded values.1488vestigate the impact of the phonetic features on thedetection of various forms of persuasiveness.
Wecompare three different sets of features, namely pho-netic, n-grams and their combination to understandwhether phonetic information can improve the per-formance of standard lexical approaches.
Similarlyto Danescu-Niculescu-Mizil et al (2012) and Tanet al (2014), we formulate a pairwise classifica-tion problem such that given a pair (s1, s2) consist-ing of sentences s1and s2, the goal is to determinethe more persuasive one (i.e., the one on the left orright).
We can consider this as a binary classificationtask where for each instance (i.e., pair) the possiblelabels are left or right.6.1 Dataset and preprocessingFor the prediction experiments, we used the fourdatasets described in Section 4 (i.e., CORPS, Twit-ter, Slogan and Movie), all of which consist of a per-suasive sentence P and its non-persuasive counter-part (?P ) labeled as either left or right.
To make thepositions of the sentences in a pair irrelevant (i.e.
toprovide symmetry), for each instance occurring inthe original datasets (e.g., (s1, s2) with label left),we added another instance including the same sen-tence pair in reverse order (i.e., (s2, s1) with labelright).
As a preprocessing step, all the sentenceswere tokenized by using Stanford CoreNLP (Man-ning et al, 2014).6.2 Classifier and featuresWe performed a 10-fold cross-validation on eachdataset and experimented with three feature setsby using a Support Vector Machine (SVM) classi-fier (Cortes and Vapnik, 1995).
We preferred SVMas a classifier due to its characteristic property toespecially perform well on high-dimensional data(Weichselbraun et al, 2011).The first feature set consists of the phonetic fea-tures (i.e.
plosive, alliteration, rhyme and homo-geneity scores as detailed in Section 3).
The secondfeature set is a standard bag of word n-grams includ-ing unigrams, bigrams and trigrams.
All the non-ascii characters, punctuations and numbers were ig-nored.
The URLs and mentions in Twitter data werereplaced with tags (i.e.
URL and MENTION re-spectively).
In addition, for the unigram features,stop words were filtered out.
We did not apply thisfiltering for bigrams and trigrams to capture longer-range usage patterns such as propositional phrases.The third feature set is simply the union of both pho-netic and n-gram features.To find the best configuration for each dataset andfeature set, we conducted a grid search over the de-gree of the polynomial kernel (1 or 2) and the num-ber of features to be used (in the range between1,000 and 20,000).
Due to the low dimensionality ofthe phonetic feature set, feature selection was per-formed only for the feature sets including n-grams.The selection was performed based on the informa-tion gain of each feature.Dataset Phonetic N-Gram AllCORPS 0.589 (-, 1) 0.733???
(4k, 1) 0.736?
(2k, 1)Movie 0.600 (-, 2) 0.694???
(1k, 1) 0.722???
(1k, 1)Slogan 0.700 (-, 2) 0.826???
(3k, 1) 0.883???
(5k, 1)Twitter 0.563 (-, 2) 0.732???
(5k, 1) 0.745???
(4k, 1)Table 5: Results of the within-dataset experiments.6.3 Within-dataset experimentsFor this set of experiments, we conducted a 10-foldcross validation on each dataset separately.
In Ta-ble 5, for each dataset listed in the first column, inthe subsequent columns we report the performanceof the best model obtained with 10-fold cross vali-dation using i) only phonetic features (Phonetic), ii)only n-grams (N-Gram), iii) both phonetic and n-gram features (All).
As mentioned previously, foreach pair (s1, s2) consisting of sentences s1and s2,our dataset contains another pair including the samesentences in reverse order (i.e., (s2, s1)), resulting ina symmetric and balanced dataset.
Therefore, clas-sification performance is measured in terms of ac-curacy (i.e., the percentage of pairs of which labelswere correctly predicted).
For each accuracy value,we also report in parenthesis the number of featuresselected and the kernel degree of the correspond-ing model.
While the kernel degree did not makea big difference in the performance, the number ofselected features had an important effect on the ac-curacy of the models.
As can be observed from thesevalues, the best performance on all the datasets isachieved with a relatively small number of features.Among the values reported in the table, the onesfollowed by??
?are significantly different (p < .001)1489Dataset N-Gram N-Gram+Rhyme N-Gram+Plosive N-Gram+Homogeneity N-Gram+AlliterationCORPS 0.733 0.738?
(3k, 1) 0.740?
(2k, 1) 0.738?
(3k, 1) 0.738?
(2k, 1)Movie 0.694 0.694?
(1k, 1) 0.692?
(1k, 1) 0.721???
(1k, 1) 0.709??
(1k, 1)Slogan 0.826 0.864???
(2k, 1) 0.824?
(2k, 1) 0.867???
(3k, 1) 0.859???
(3k, 1)Twitter 0.732 0.740??
(4k, 1) 0.733?
(4k, 1) 0.746???
(4k, 1) 0.742???
(4k, 1)Table 6: Contribution of the phonetic features.TestCORPS Twitter Slogan MovieTraining Phonetic N-Gram All Phonetic N-Gram All Phonetic N-Gram All Phonetic N-Gram AllCORPS - - - 0.463 0.508 0.523 0.508 0.517 0.539 0.411 0.506 0.516Twitter 0.439 0.494 0.462 - - - 0.564 0.531 0.637 0.596 0.544 0.589Slogan 0.535 0.512 0.514 0.535 0.510 0.539 - - - 0.532 0.545 0.588Movie 0.431 0.513 0.498 0.562 0.533 0.560 0.581 0.537 0.589 - - -Table 7: Results of the cross-dataset prediction experiments optimized on the training set.from the ones to their left, while ?
represents nosignificance, as calculated according to McNemar?stest (McNemar, 1947).
For each dataset, the weakestmodels (i.e.
the ones using only the phonetic fea-tures in all cases) are still significantly (p < .001)more accurate than a random baseline (accuracy =50%).
As can be observed from the table, the mod-els using only n-grams significantly outperform theones only based on phonetic features in all datasets.However, while the phonetic features are not verystrong by themselves, their combination with n-grams results in models outperforming the n-grambased models in all cases.
The difference is highlysignificant for all datasets except CORPS, where n-grams alone are sufficient to achieve a good perfor-mance.
We speculate that the kind of persuasivenessused in political speeches is more dependent on thelexical choices of the speaker and on the use of a spe-cific set of semantically loaded words such as bless,victory, God and justice or military.
This is in linewith the work of Guerini et al (2008), who built adomain specific lexicon to study the persuasive im-pact of words in political speeches.We also conducted an additional set of experi-ments to investigate if some phonetic features standout among the others, and to find out the contri-bution and importance of each phonetic feature inisolation.
To achieve that, for each dataset we con-ducted a 10-fold cross validation to obtain the bestfour models containing a single phonetic featureon top of n-gram features (i.e.
N-Gram+Rhyme,N-Gram+Plosive, N-Gram+Homogeneity and N-Gram+Alliteration).
In Table 6, we report the ac-curacy of the n-gram model and these four modelsfor each dataset.
Similarly to Table 5, for each accu-racy value, we also report in parenthesis the numberof features selected and the kernel degree of the cor-responding model obtained with grid search.
Theresults demonstrate that homegeneity is the most ef-fective feature when added on top of n-grams, result-ing in highly significant improvement against the ba-sic n-gram models in three out of four datasets.
Al-literation and rhyme closely follow homogeneity byyielding models that significantly outperform the n-gram models in three and two datasets respectively.Finally, the models containing plosives do not im-prove over the n-gram models in any of the fourdatasets.
It is worth noting that in CORPS none ofthe n-gram models enriched with phonetic featuresimproves over the basic n-gram models as in linewith the results of the within-dataset experiments re-ported in Table 5.6.4 Cross-dataset experimentsAfter observing that the combination of phoneticand n-gram features can be effective in the within-dataset prediction experiments, we took a furtherstep and investigated the interaction of the three fea-ture sets across datasets.
More specifically, we clas-sified each dataset with the best models (one for eachfeature set) trained on the other datasets.
With theseexperiments, we investigated the ability of phonetic1490features to generalize across the different lexiconsof the datasets.
As we discussed previously, the fourdatasets represent different forms of persuasiveness.In this respect, the results of the cross-dataset exper-iments can also be interpreted as a measure of thedegree of compatibility among these kinds of per-suasiveness.In Table 7, we present the results of the cross-dataset prediction experiments.
For each trainingand test set pair, we report the accuracy of the bestmodels, one for each feature set, based on cross-validation on the training set.
As can be observedfrom the table, the figures are generally low andvarious domain adaptation techniques could be em-ployed to improve the results.
However, the objec-tive of this evaluation is not to train an optimizedcross-domain classifier, but to assess the potential ofthe feature sets to model different kinds of persua-siveness.As expected, n-gram features show poor perfor-mance due to the lexical and stylistic differencesamong the datasets.
In many cases, the phoneticmodels outperform the n-gram models, and in sev-eral cases the combination of the two feature sets de-teriorates the performance of the phonetic featuresalone.
These findings support our hypothesis thatphonetic features, due to their generality, have bettercorrelation with different forms of persuasivenessthan lexical features.
The experiments involving theCORPS dataset, both for training and testing, do notshare this behavior.
Indeed, when CORPS is usedas a training or test dataset, the performance of themodels is quite low (very close to or worse than thebaseline in many cases) independently from the fea-ture sets.
These results suggest that the notion ofpersuasiveness encoded in this dataset is remarkablydifferent from the others, as previously discussed inthe data analysis in Section 5.
As seen in the withindataset experiments (see Table 5), CORPS is theonly dataset in which the combination of lexical andphonetic features do not improve the classificationaccuracy.
This explains the inability of the phoneticfeatures to improve the accuracy in cross-dataset ex-periments when this dataset is employed.7 ConclusionIn this paper, we focused on the impact of a setof phonetic features ?
namely rhyme, alliteration,homogeneity and plosives ?
on various forms ofpersuasiveness including memorability of slogansand movie quotes, re-tweet counts of tweets, andeffectiveness of political speeches.
We conductedour analysis and experiments on four datasets com-prising pairs of a persuasive sentence and a non-persuasive counterpart.Our data analysis shows that persuasive sentencesare generally euphonic.
This finding is confirmedby the prediction experiments, in which we observedthat phonetic features consistently help in the detec-tion of persuasiveness.
When combined with lexicalfeatures, they help improving classification perfor-mance on three of the four datasets that we consid-ered.
The key role played by phonetic features isfurther underlined by the cross-dataset experiments,in which we observed that phonetic features alonegenerally outperform the lexical features.
To thebest of our knowledge, this is the first systematicanalysis of the impact of phonetic features on sev-eral types of persuasiveness.
Our results should en-courage researchers dealing with different aspects ofpersuasiveness to consider the inclusion of phoneticattributes in their models.As future work, we will investigate the impactof other phonetic devices such as assonance, conso-nance and rhythm on persuasiveness.
It would alsobe interesting to focus on the connection betweensound symbolism and persuasiveness, and investi-gate how the context or domain of persuasive state-ments interacts with the sounds in those statements.We would like to conclude this paper with the mostfavorite and retweeted tweet of @NAACL2015 (theTwitter account of the conference whose proceed-ings comprise this paper), which is a good exampleof the positive effect of euphony in persuasiveness:The deadline for @NAACL2015 papersubmissions is approaching:Remember, remember, the 4th of December!AcknowledgmentsThis work has been partially supported by the TrentoRISE PerTe project.1491ReferencesYong-Yeol Ahn, Seungyeop Han, Haewoon Kwak, SueMoon, and Hawoong Jeong.
2007.
Analysis of topo-logical characteristics of huge online social network-ing services.
In Proceedings of the 16th internationalconference on World Wide Web, pages 835?844.
ACM.Tim Althoff, Cristian Danescu-Niculescu-Mizil, and DanJurafsky.
2014.
How to ask for a favor: A case studyon the success of altruistic requests.
Proocedings ofICWSM.Rka Benczes.
2013.
The role of alliteration and rhymein novel metaphorical and metonymical compounds.Metaphor and Symbol, 28(3):167?184.Jonah A. Berger and Katherine L. Milkman.
2009.
So-cial Transmission, Emotion, and the Virality of OnlineContent.
Social Science Research Network WorkingPaper Series, December.Bruce G. Vanden Bergh, Janay Collins, Myrna Schultz,and Keith Adler.
1984.
Sound advice on brand names.Journalism Quarterly, 61(4):835, dec.Luciano Bernardi, Cesare Porta, and Peter Sleight.2006.
Cardiovascular, cerebrovascular, and respira-tory changes induced by different types of music inmusicians and non-musicians: the importance of si-lence.
Heart, 92(4):445?452.Youmna Borghol, Sebastien Ardon, Niklas Carlsson,Derek Eager, and Anirban Mahanti.
2012.
The un-told story of the clones: Content-agnostic factors thatimpact youtube video popularity.
In Proceedings ofthe 18th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 1186?1194.
ACM.Shelly Chaiken.
1980.
Heuristic vs. sistematic informa-tion processing and the use of source vs message cuesin persuasion.
Journal of Personality and Social Psy-chology, 39:752?766.Robert B. Cialdini.
1993.
Influence.
The psychology ofpersuasion.
William Morrow & Company, Inc., NewYork.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Mach.
Learn., 20(3):273?297,September.Cristian Danescu-Niculescu-Mizil, Justin Cheng, JonKleinberg, and Lillian Lee.
2012.
You had me athello: How phrasing affects memorability.
In Pro-ceedings of the ACL.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Marco Guerini, Carlo Strapparava, and Oliviero Stock.2008.
Corps: A corpus of tagged political speechesfor persuasive communication processing.
Journal ofInformation Technology & Politics, 5(1):19?32.Marco Guerini, Carlo Strapparava, and G?ozde?Ozbal.2011.
Exploring text virality in social networks.
InProceedings of ICWSM-11, Barcelona, Spain, July.Marco Guerini, Alberto Pepe, and Bruno Lepri.
2012.Do linguistic style and readability of scientific ab-stracts affect their virality.
Proceedings of ICWSM-12.Marco Guerini, Danilo Giampiccolo, Giovanni Moretti,Rachele Sprugnoli, and Carlo Strapparava.
2013.
Thenew release of corps: A corpus of political speechesannotated with audience reactions.
In MultimodalCommunication in Political Speech.
Shaping Mindsand Social Action, pages 86?98.
Springer.Chip Heath and Dan Heath.
2007.
Made to stick: Whysome ideas survive and others die.
Random House.Julian Jaynes.
2000.
The origin of consciousness in thebreakdown of the bicameral mind.
Houghton MifflinHarcourt.Jing Jiang, Christo Wilson, Xiao Wang, Peng Huang,Wenpeng Sha, Yafei Dai, and Ben Y Zhao.
2010.Understanding latent interactions in online social net-works.
In Proceedings of the 10th ACM SIGCOMMconference on Internet measurement, pages 369?382.ACM.Haewoon Kwak, Changhyun Lee, Hosung Park, and SueMoon.
2010.
What is twitter, a social network or anews media?
In Proceedings of the 19th internationalconference on World wide web, pages 591?600.
ACM.Himabindu Lakkaraju, Julian J McAuley, and JureLeskovec.
2013.
What?s in a name?
understandingthe interplay between titles, content, and communitiesin social media.
In ICWSM.Geoffrey N. Leech.
1966.
English in advertising : alinguistic study of advertising in Great Britain / [by]Geoffrey N. Leech.
Longmans London.Jurij Leskovec.
2008.
Dynamics of large networks.
Pro-Quest.Annie Louis and Ani Nenkova.
2013.
What makes writ-ing great?
first experiments on article quality predic-tion in the science journalism domain.
TACL, 1:341?352.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of 52nd Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations, pages 55?60.Quinn McNemar.
1947.
Note on the sampling error ofthe difference between correlated proportions or per-centages.
Psychometrika, 12(2):153?157, jun.Bernard Moulin, Hengameh Irandoust, Micheline Be-langer, and Ga?elle Desordes.
2002.
Explanation and1492argumentation capabilities: Towards the creation ofmore persuasive agents.
Artificial Intelligence Review,17:169?222.G?ozde?Ozbal, Carlo Strapparava, and Marco Guerini.2012.
Brand pitt: A corpus to explore the art ofnaming.
In Proceedings of the Eighth InternationalConference on Language Resources and Evaluation(LREC-2012).
European Language Resources Associ-ation (ELRA).G?ozde?Ozbal, Daniele Pighin, and Carlo Strapparava.2013.
BRAINSUP: Brainstorming Support for Cre-ative Sentence Generation.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (ACL 2013), pages 1446?1455, Sofia, Bul-garia, August.
Association for Computational Linguis-tics.G?ozde?Ozbal, Daniele Pighin, and Carlo Strapparava.2014.
Automation and evaluation of the keywordmethod for second language learning.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 352?357.
Association for ComputationalLinguistics.Chaim Perelman and Lucie Olbrechts-Tyteca.
1969.
Thenew Rhetoric: a treatise on Argumentation.
NotreDame Press.Richard E. Petty and John T. Cacioppo.
1986.
The elab-oration likelihood model of persuasion.
Advances inExperimental Social Psychology, 19:123?205.Daniela Sammler, Stefan Koelsch, Tonio Ball, ArminBrandt, Maren Grigutsch, Hans-J?urgen Huppertz,Thomas R Kn?osche, J?org Wellmer, Guido Widman,Christian E Elger, et al 2013.
Co-localizing linguisticand musical syntax with intracranial eeg.
NeuroImage,64:134?146.Matthew Simmons, Lada A Adamic, and Eytan Adar.2011.
Memes online: Extracted, subtracted, injected,and recollected.
Proceedings of ICWSM-11.Chenhao Tan, Lillian Lee, and Bo Pang.
2014.
The ef-fect of wording on message propagation: Topic-andauthor-controlled natural experiments on twitter.
ACL.Stephen Toulmin.
1958.
The Use of Arguments.
Cam-bridge University Press, Cambridge MA.Katie Wales.
2001.
A dictionary of stylistics.
Harlow,Eng.
; New York : Longman, 2nd ed edition.
Includesbibliographical references (p. 413-429).Douglas N. Walton.
1996.
Argumentation Schemes forPresumptive Reasoning.
Lawrence Erlbaum Asso-ciates, Mahwah, New Jersey.Albert Weichselbraun, Stefan Gindl, and Arno Scharl.2011.
Using games with a purpose and bootstrappingto create domain-specific sentiment lexicons.
In Pro-ceedings of the 20th ACM International Conference onInformation and Knowledge Management, CIKM ?11,pages 1053?1060, New York, NY, USA.
ACM.1493
