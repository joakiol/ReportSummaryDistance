Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 878?883,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsWord surprisal predicts N400 amplitude during readingStefan L. Frank1,2 Leun J. Otten3 Giulia Galli3 Gabriella Vigliocco2{s.frank, l.otten, g.galli, g.vigliocco}@ucl.ac.uk1Centre for Language Studies, Radboud University Nijmegen2Department of Cognitive, Perceptual and Brain Sciences, University College London3Institute of Cognitive Neuroscience, University College LondonAbstractWe investigated the effect of word sur-prisal on the EEG signal during sen-tence reading.
On each word of 205 ex-perimental sentences, surprisal was esti-mated by three types of language model:Markov models, probabilistic phrase-structure grammars, and recurrent neu-ral networks.
Four event-related poten-tial components were extracted from theEEG of 24 readers of the same sentences.Surprisal estimates under each model typeformed a significant predictor of the am-plitude of the N400 component only, withmore surprising words resulting in morenegative N400s.
This effect was mostlydue to content words.
These findingsprovide support for surprisal as a gener-ally applicable measure of processing dif-ficulty during language comprehension.1 IntroductionMany studies of human language comprehensionmeasure the brain?s electrical activity during read-ing.
Such electroencephalography (EEG) experi-ments have revealed that the EEG signal displayssystematic variation in response to the appearanceof each word.
The different components that canbe observed in this signal are known as event-related potentials (ERPs).
Probably the most reli-ably observed (and most studied) of these compo-nents is a negative-going deflection at centropari-etal electrodes that peaks at around 400 ms afterword onset and is therefore referred to as the N400component.It is well known that the N400 increases in am-plitude (i.e., becomes more negative) when theword leads to comprehension difficulty.
To studythe general relation between word predictabilityand the N400, Dambacher et al(2006) obtainedsubjective word-probability estimates (so-calledcloze probabilities) by asking participants to pre-dict the upcoming word at each point in a largenumber of sentences.
A different group of subjectsread these same sentences while their EEG signalwas recorded.
Results showed a correlation be-tween N400 amplitude and cloze probability: Lesspredictable words yielded stronger N400s.We investigated whether similar results can beobtained using more objective, model-based wordprobabilities.
For each word in a collection ofEnglish sentences, estimates of its surprisal (i.e.,its negative log-transformed conditional probabil-ity: ?
logP (wt|w1, .
.
.
, wt?1)) were generatedby three types of language model: Markov (i.e., n-gram) models, phrase-structure grammars (PSGs),and recurrent neural networks (RNNs).
Next, EEGsignals of participants reading the same sentenceswere recorded.
A comparison of word surprisal todifferent ERP components revealed that, indeed,N400 amplitude was predicted by surprisal values:More surprising words resulted in more negativeN400s, at least for content words.2 Language modelsA range of models of each type was trained, al-lowing to investigate whether models that capturethe language statistics more accurately also yieldbetter predictions of ERP size.
Such a relation isgenerally found in studies that use word-readingtime as the dependent variable (Fernandez Mon-salve et al 2012; Frank and Bod, 2011; Frankand Thompson, 2012), providing additional sup-port that these psychological data are indeed ex-plained by the surprisal values and not by someconfounding variable.2.1 Corpus dataAll models were trained on sentences from thewritten texts in the British National Corpus(BNC).
First, the 10,000 word types with highest878frequency were selected from the BNC.
Next, allsentences were extracted that contained only thosewords.
This resulted in a training corpus of 1.06million sentences (12.6 million word tokens).Each trained model estimated a surprisal valuefor each word of the 205 sentences (1931 word to-kens) for which eye-tracking data are available inthe UCL corpus of reading times (Frank et al inpress).
These sentences, which were selected fromthree unpublished novels, only contained wordsfrom the 10,000 high-frequency word list.2.2 Markov modelsMarkov models were trained with modifiedKneser-Ney smoothing (Chen and Goodman,1999) as implemented in SRILM (Stolcke, 2002).Model order was varied: n = 2, 3, 4.
No unigrammodel was computed because word frequency wasfactored out during data analysis (see Section 4.2).2.3 Recurrent neural networksThe RNN model architecture has been thoroughlydescribed elsewhere (Fernandez Monsalve et al2012; Frank, in press) so it is not discussed here.The only difference with previous versions wasthat the current RNN was trained on a substantiallylarger data set with more word types.
A range ofRNN models was obtained by training on nine in-creasingly large subsets of the BNC data, compris-ing 2K, 5K, 10K, 20K, 50K, 100K, 200K, 400K,and all 1.06M sentences.
In addition, the networkwas trained on the full set twice, making a total often instantiations of the RNN model.2.4 Phrase-structure grammarsTo prepare data for PSG training, the selectedBNC sentences were parsed by the Stanford parser(Klein and Manning, 2003).
The resulting tree-bank was divided into nine increasingly large sub-sets, equal to those used for RNN training.1 Gram-mars were induced from these subsets using thealgorithm by Roark (2001) with its standard set-tings.
Next, surprisal values on the experimentalsentences were generated by Roark?s incrementalparser.
Since increasing the parser?s beam widthhas been shown to improve both word-probabilityestimates and the fit to word-reading times (Frank,2009), the parser?s ?base beam threshold?
parame-ter was reduced to 10?20.1Because not all experimental sentences could be parsedwhen the treebank comprised only 2K sentences, 1K sen-tences were added to the smallest subset.3 EEG data collectionTwenty-four healthy, adult volunteers from theUCL Psychology subject pool took part in thereading study.
Their EEG was recorded contin-uously from 32 channels during the presentationof 5 practice sentences and the 205 experimentalitems.
Participants were asked to minimise blinks,eye movements, and head movements during sen-tence presentation.Each sentence was preceded by a centrally pre-sented fixation cross.
As soon as the partici-pant pressed a key, the cross was replaced by thesentence?s first word, which was then automati-cally replaced by each subsequent word.
Wordpresentation duration (in milliseconds) equalled190 + 20k, where k is the number of charactersin the word (including any attached punctuation).After the word disappeared, there was a 390 msinterval before the next word appeared.The sentences were presented in random or-der, one word at a time, always centrally locatedon the monitor.
One-hundred and ten of the ex-perimental sentences were followed by a yes/no-comprehension question, to ensure that partici-pants tried to understand the sentences.
All par-ticipants answered at least 80% of the comprehen-sion questions correctly.4 Data analysis4.1 ERP componentsFour ERP components of interest were identifiedfrom the literature on EEG and sentence reading:Early Left Anterior Negativity (ELAN), P200,N400, and a post-N400 positivity (PNP).
Table 1lists the corresponding time windows and approx-imate electrode sites.2 For each component, theaverage electrode potential over the correspondingtime window and electrodes was computed.
Theseaverage ERP amplitudes served as the four depen-dent variables for data analysis.The ELAN component is generally thought ofas indicative of difficulty with constructing syntac-tic phrase structure (Friederici et al 1999; Gunteret al 1999; Neville et al 1991).
Hence, if anyof the model types predicts ELAN size, we wouldexpect this to be the PSG.Dambacher et al(2006) found effects of wordfrequency or length (which are strongly correlated2The P600 component (Osterhout and Holcomb, 1992)was not included because the shortest interval between con-secutive word onsets was only 600 ms.879Component Time window LocationELAN 125?175 ms left anteriorP200 140?200 ms frontocentralN400 300?500 ms centroparietalPNP 400?600 ms frontopolarTable 1: Investigated ERP components, their timewindows, and approximate scalp locations.and therefore difficult to tease apart) on the P200amplitude.
Since we factor out these two lexicalfactors in the analysis, we expect no additional ef-fect of surprisal on P200.If any of the components is sensitive to wordsurprisal, this is most likely to be the N400 asmany studies have already shown that N400 am-plitude depends on subjective word predictabil-ity (Dambacher et al 2006; Kutas and Hillyard,1984; Moreno et al 2002).
Whether an effectwill appear on the PNP is more doubtful.
VanPetten and Luka (2012) argue that word expec-tations that are confirmed result in reduced N400size, whereas expectations that are disconfirmedincrease the PNP.
However, in a probabilistic set-ting, expectations are not all-or-nothing so thereis no strict distinction between confirmation anddisconfirmation.
Nevertheless, surprisal effects onPNP may occur.
Since the PNP has received rel-atively little attention, the component may not besuch a reliable index of comprehension difficultyas the N400 has proven to be.4.2 Regression analysisData were discarded on words attached to acomma, clitics, sentence-initial, and sentence-final words.
Moreover, artifacts in the EEG data(mostly due to eye blinks) were identified and re-moved, leaving 32,010 analysed data points per in-vestigated ERP component.
For each data pointand ERP component, a baseline potential was de-termined by averaging over the component?s elec-trodes in the 100 ms leading up to word onset.In order to quantify the fit of surprisal to ERPsize, a linear mixed-effects regression model wasfitted to each of the four ERPs, using the pre-dictors: baseline potential, log-transformed wordfrequency, word length (number of characters),word position in the sentence, and sentence po-sition in the experiment.3 Also, all significant3For word and sentence position, both linear and squaredfactors were included in order to capture possible non-lineartwo-way interactions were included (main effectswere removed if they were not significant and didnot appear in any interaction).
In addition, therewere by-subject and by-item random intervals, aswell as significant by-subject and by-item randomslopes.
Parameters for the correlation betweenrandom intercept and slope where also estimated,if they significantly contributed to model fit.When the surprisal estimates by a particular lan-guage model are included in the analysis, the re-gression model?s deviance decreases.
The size ofthis decrease is the ?2-statistic of a likelihood-ratio test for significance of the surprisal effect,and was taken as the measure of the surprisal val-ues?
fit to the ERP data.4 Negative values will beused to indicate effects in the negative direction,that is, when higher surprisal results in more neg-ative (or less positive) going ERP deflections.5 Results5.1 Surprisal effectsFigure 1 plots the fit of each model?s surprisal esti-mates to ERP amplitude as a function of the aver-age natural logP (wt|w1, .
.
.
, wt?1), which quan-tifies to what extent the model has acquired accu-rate language statistics.5 For the ELAN, P200 andPNP components, there were no significant effectsafter correcting for multiple comparisons.
In con-trast, effects on the N400 were highly significant.5.2 Model comparisonTable 2 shows results of pairwise comparisons be-tween the best models of each type (i.e., thosewhose surprisal estimates fit the N400 data best).Clearly, RNN-based surprisal explains varianceover and above each of the other two modelswhereas neither the n-gram nor the PSG modeloutperforms the RNN.
Moreover, the RNN?s sur-prisals explain a marginally significant (?2 =3.47; p < .07) amount of variance over and abovethe combined PSG and n-gram surprisals.changes over the course of the sentence or experiment.4This definition equals what Frank and Bod (2011) call?psychological accuracy?
in an analysis of reading times.5This measure, which Frank and Bod (2011) call ?linguis-tic accuracy?, equals the negative logarithm of the model?sperplexity.
Increasing the amount of training data (or thevalue of n) resulted in higher linguistic accuracy, except forthe three PSG models trained on the smallest amounts of data.This shows that the models did not suffer from overfitting.880?6.5 ?6 ?5.5 ?5?20?10010ELANav.
log P(wt|w1,?,wt?1)fitof surprisal (?
?2)?6.5 ?6 ?5.5 ?5?20?10010P200av.
log P(wt|w1,?,wt?1)?6.5 ?6 ?5.5 ?5?20?10010N400av.
log P(wt|w1,?,wt?1)?6.5 ?6 ?5.5 ?5?20?10010PNPav.
log P(wt|w1,?,wt?1)RNNPSGn?gramFigure 1: Fit to surprisal of ERP amplitude (for ELAN, P200, N400, and PNP components) as a functionof average logP (wt|w1, .
.
.
, wt?1).
Each plotted point corresponds to predictions by one of the trainedmodels.
Dotted lines indicate ?2 = ?3.84, beyond which effects are statistically significant (p < .05)if no correction for multiple comparisons is applied.
The dashed line indicates the level below whicheffects are significant after applying the correction proposed by Benjamini and Hochberg (1995), oneach ERP component separately because of our prior expectation that effects would occur mostly (if notexclusively) on the N400 component.Model n-gram RNN PSGn-gram ?2 = 1.34 ?2 = 1.66p > .2 p > .1RNN ?2 = 6.52 ?2 = 4.78p < .02 p < .05PSG ?2 = 4.20 ?2 = 2.14p < .05 p > .1Table 2: Pairwise comparisons between surprisalestimates by the best models of each type.
Shownare the results of likelihood-ratio tests for the ef-fect of one set of surprisal estimates (rows) overand above the other (columns).5.3 Comparing word classesN400 effects are nearly exclusively investigatedon content (i.e., open-class) words.
Dambacher etal.
(2006), too, investigated the relation betweenERP amplitudes and cloze probabilities on con-tent words only.
When running separate analyseson content and function words (constituting 53.2%and 46.8% of the data, respectively), we found thatthe N400 effect of Figure 1 is nearly fully drivenby content words (see Figure 2).
None of the mod-els?
surprisal estimates formed a significant pre-dictor of N400 amplitude on function words, aftercorrection for multiple comparisons.6 DiscussionWe demonstrated a clear effect of word surprisal,as estimated by different language models, on theEEG signal: The larger a (content) word?s sur-prisal value, the more negative the resulting N400.
?8.5 ?8 ?7.5 ?7 ?6.5?20?100content wordsav.
log P(wt|w1,?,wt?1)fitof surprisal (?
?2)?4.5 ?4 ?3.5?20?100av.
log P(wt|w1,?,wt?1)function wordsRNNPSGn?gramFigure 2: Fit to surprisal of N400 amplitude, forcontent words (left) and function words (right).Dotted lines indicate ?2 = ?3.84, beyond whicheffects are statistically significant (p < .05) with-out correcting for multiple comparisons.
Dashedlines indicates the levels beyond which effectsare significant after multiple-comparison correc-tion (Benjamini and Hochberg, 1995).The N400 component is generally viewed as in-dicative of lexical rather than syntactic processing(Kaan, 2007), which may explain why surprisalunder the PSG model did not have any significantexplanatory value over and above RNN-based sur-prisal.
The relatively weak performance of ourMarkov models is most likely due to their strict(and cognitively unrealistic) limit on the size ofthe prior context upon which word-probability es-timates are conditioned.Unlike the ELAN, P200, and PNP components,the N400 is known to be sensitive to the clozeprobability of content words.
The fact that sur-prisal effects were found on the N400 only, there-fore suggests that subjective predictability scoresand model-based surprisal estimates form opera-881tionalisations of one and the same underlying cog-nitive factor.
Needless to say, our statistical mod-els fail to capture many information sources, suchas semantics and discourse, that do affect clozeprobabilities.
However, it is possible in principleto integrate these into probabilistic language mod-els (Dubey et al 2011; Mitchell et al 2010).To the best of our knowledge, only one otherpublished study relates language model predic-tions to the N400: Parviz et al(2011) found thatsurprisal estimates (corrected for word frequency)from an n = 4 Markov model predicted N400 sizeas measured by magnetoencephalography (ratherthan EEG).
Although their PSG-based surprisalsdid not correlate with N400 size, a related measurederived from the PSG ?lexical entropy?
did.
How-ever, Parviz et al(2011) only looked at effectson the sentence-final content word of items con-structed for a speech perception experiment (Ka-likow et al 1977), rather than investigating sur-prisal?s general predictive value across words ofnaturally occurring sentences, as we did here.Our experimental design was parametric ratherthan factorial, which allowed us to study the effectof surprisal over a sample of English sentencesrather than carefully manipulating surprisal whileholding other factors constant.
This has the ad-vantage that our findings are likely to generalise toother sentence stimuli, but it can also raise a pos-sible concern: The N400 effect may not be dueto surprisal itself, but to an unknown confound-ing variable that was not included in the regressionanalysis.
However, this seems unlikely because oftwo additional findings that only follow naturallyif surprisal is indeed the relevant predictor: Signif-icant results only appeared where they were mostexpected a priori (i.e., on N400 but not on othercomponents) and there was a nearly monotonic re-lation between the models?
word-prediction accu-racy and their ability to account for N400 size.7 ConclusionAlthough word surprisal has often been shownto be predictive of word-reading time (Fernan-dez Monsalve et al 2012; Frank and Thompson,2012; Smith and Levy, in press), a general effecton the EEG signal has not before been demon-strated.
Hence, these results provide additional ev-idence in support of surprisal as a reliable measureof cognitive processing difficulty during sentencecomprehension (Hale, 2001; Levy, 2008).AcknowledgmentsThe research presented here was funded by theEuropean Union Seventh Framework Programme(FP7/2007-2013) under grant number 253803.The authors acknowledge the use of the UCL Le-gion High Performance Computing Facility, andassociated support services, in the completion ofthis work.ReferencesY.
Benjamini and Y. Hochberg.
1995.
Controlling thefalse discovery rate: a practical and powerful ap-proach to multiple testing.
Journal of the Royal Sta-tistical Society B, 57:289?300.S.
F. Chen and J. Goodman.
1999.
An empiricalstudy of smoothing techniques for language model-ing.
Computer Speech and Language, 13:359?394.M.
Dambacher, R. Kliegl, M. Hofmann, and A. M. Ja-cobs.
2006.
Frequency and predictability effect onevent-related potentials during reading.
Brain Re-search, 1084:89?103.A.
Dubey, F. Keller, and P. Sturt.
2011.
A model ofdiscourse predictions in human sentence processing.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages304?312.
Edinburgh, UK: Association for Compu-tational Linguistics.I.
Fernandez Monsalve, S. L. Frank, and G. Vigliocco.2012.
Lexical surprisal as a general predictor ofreading time.
In Proceedings of the 13th Confer-ence of the European Chapter of the Associationfor Computational Linguistics, pages 398?408.
Avi-gnon, France: Association for Computational Lin-guistics.S.
L. Frank and R. Bod.
2011.
Insensitivity of thehuman sentence-processing system to hierarchicalstructure.
Psychological Science, 22:829?834.S.
L. Frank and R. L. Thompson.
2012.
Early ef-fects of word surprisal on pupil size during read-ing.
In Proceedings of the 34th Annual Conferenceof the Cognitive Science Society, pages 1554?1559.Austin, TX: Cognitive Science Society.S.
L. Frank, I. Fernandez Monsalve, R. L. Thompson,and G. Vigliocco.
in press.
Reading time data forevaluating broad-coverage models of English sen-tence processing.
Behavior Research Methods.S.
L. Frank.
2009.
Surprisal-based comparison be-tween a symbolic and a connectionist model of sen-tence processing.
In N. A. Taatgen and H. van Rijn,editors, Proceedings of the 31st Annual Conferenceof the Cognitive Science Society, pages 1139?1144.Austin, TX: Cognitive Science Society.882S.
L. Frank.
in press.
Uncertainty reduction as a mea-sure of cognitive processing load in sentence com-prehension.
Topics in Cognitive Science.A.
D. Friederici, K. Steinhauer, and S. Frisch.
1999.Lexical integration: sequential effects of syntacticand semantic information.
Memory & Cognition,27:438?453.T.
C. Gunter, A. D. Friederici, and A. Hahne.
1999.Brain responses during sentence reading: Visualinput affects central processes.
NeuroReport,10:3175?3178.J.
T. Hale.
2001.
A probabilistic Early parser asa psycholinguistic model.
In Proceedings of the2nd Conference of the North American Chapter ofthe Association for Computational Linguistics, vol-ume 2, pages 159?166.
Pittsburgh, PA: Associationfor Computational Linguistics.E.
Kaan.
2007.
Event-related potentials and languageprocessing: a brief overview.
Language and Lin-guistics Compass, 1:571?591.D.
N. Kalikow, K. N. Stevens, and L. L. Elliott.
1977.Development of a test of speech intelligibility innoise using sentence materials with controlled wordpredictability.
Journal of the Acoustical Society ofAmerica, 61:1337?1351.D.
Klein and C. D. Manning.
2003.
Accurate unlex-icalized parsing.
In Proceedings of the 41st Meet-ing of the Association for Computational Linguis-tics, pages 423?430.
Sapporo, Japan: Associationfor Computational Linguistics.M.
Kutas and S. A. Hillyard.
1984.
Brain potentialsduring reading reflect word expectancy and semanticassociation.
Nature, 307:161?163.R.
Levy.
2008.
Expectation-based syntactic compre-hension.
Cognition, 106:1126?1177.J.
Mitchell, M. Lapata, V. Demberg, and F. Keller.2010.
Syntactic and semantic factors in process-ing difficulty: An integrated measure.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 196?206.
Up-psala, Sweden: Association for Computational Lin-guistics.E.
M. Moreno, K. D. Federmeier, and M. Kutas.
2002.Switching languages, switching palabras (words):an electrophysiological study of code switching.Brain and Language, 80:188?207.H.
Neville, J. L. Nicol, A. Barss, K. I. Forster, and M. F.Garrett.
1991.
Syntactically based sentence pro-cessing classes: evidence from event-related brainpotentials.
Journal of Cognitive Neuroscience,3:151?165.L.
Osterhout and P. J. Holcomb.
1992.
Event-relatedbrain potentials elicited by syntactic anomaly.
Jour-nal of Memory and Language, 31:785?806.M.
Parviz, M. Johnson, B. Johnson, and J. Brock.2011.
Using language models and Latent SemanticAnalysis to characterise the N400m neural response.In Proceedings of the Australasian Language Tech-nology Association Workshop 2011, pages 38?46.Canberra, Australia.B.
Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27:249?276.N.
J. Smith and R. Levy.
in press.
The effect of wordpredictability on reading time is logarithmic.
Cog-nition.A.
Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of the Interna-tional Conference on Spoken Language Processing,pages 901?904.
Denver, Colorado.C.
Van Petten and B. J. Luka.
2012.
Prediction duringlanguage comprehension: benefits, costs, and ERPcomponents.
International Journal of Psychophysi-ology, 83:176?190.883
