Analyzing the Reading Comprehension TaskAmit  BaggaGE Corporate Research and Development1 Research CircleNiskayuna, NY 12309bagga@crd.ge, cornAbst rac tIn this paper we describe a method for analyzingthe reading comprehension task.
First, we describea method of classifying facts (information) into cat-egories or levels; where each level signifies a differentdegree of difficulty of extracting a fact from a pieceof text containing it.
We then proceed to show howone can use this model the analyze the complexityof the reading comprehension task.
Finally, we ana-lyze five different reading comprehension tasks andpresent results from this analysis.1 In t roduct ionRecently there has been a spate of activity for build-ing question-answering systems (QA systems) drivenlargely by the recently organized QA track at theEighth Text Retrieval Conference (TREC-8) (Har-man, 1999).
This increase in research activity hasalso fueled research in a related area: building Read-ing Comprehension systems (Hirschman and others,1999).
But while a number of successful systemshave been developed for each of these tasks, little,if any, work has been done on analyzing the com-plexities of the tasks themselves.
In this paper wedescribe a method of classifying facts (information)into categories or levels; where each level signifiesa different degree of difficulty of extracting a factfrom a piece of text containing it.
We then proceedto show how one can use this model the analyze thecomplexity of the reading comprehension task.
Fi-nally, we analyze five different reading comprehen-sion tasks and present results from this analysis.2 The  Complex i ty  o f  Ext rac t ing  aFact  F rom TextAny text document is a collection of facts (infor-mation).
These facts may be explicitly or implicitlystated in the text.
In addition, there are "easy" factswhich may be found in a single sentence (example:the name of a city) as well as "difficult" facts whichare spread across several sentences (example: thereason for a particular event).For a computer system to be able to process textdocuments in applications like information extrac-35tion (IE), question answering, and reading compre-hension, it has to have the ability to extract factsfrom text.
Obviously, the performance of the systemwill depend upon the type of fact it has to extract:explicit or implicit, easy or difficult, etc.
(by nomeans is this list complete).
In addition, the perfor-mance of such systems varies greatly depending onvarious additional factors including known vocabu-lary, sentence l ngth, the amount of training, qualityof parsing, etc.
Despite the great variations in theperformances of such systems, it has been hypothe-sized that there are facts that are simply harder toextract han others (Hirschman, 1992).In this section we describe a method for estimat-ing the complexity of extracting a fact from text.The proposed model was initially used to analyze theinformation extraction task (Bagga and Biermann,1997).
In addition to verifying Hirschman's hypoth-esis, the model  also provided us with a frameworkfor analyzing and understanding the performance ofseveral IE systems (Bagga and Biermann, 1998).
Wehave also proposed using this model to analyze thecomplexity of the QA task Which is related to boththe IE, and the reading comprehension tasks (Baggaet al, 1999).
The  remainder of this section describesthe model in detail, and provides a sample applica-tion of the model  to an IE task.
In the followingsection, we discuss how this model  can be used toanalyze the reading comprehension task.2.1 Def in i t ionsNetwork :A network consists of a collection of nodes intercon-nected by an accompanying set of arcs.
Each nodedenotes an object and each arc represents a binaryrelation between the objects.
(Hendrix, 1979)A Part ia l  Network :A partial network is a collection of nodes intercon-nected by an accompanying set of arcs where thecollection of nodes is a subset of a collection of nodesforming a network, and the accompanying set of arcsis a subset of the se.t of arcs accompanying the setof nodes which form the network.ofonsibilit~/ ofon O~ El Espectador )Figure 1: A Sample NetworkFigure 1 shows a sample network for the followingpiece of text:"The Extraditables," orthe Armed Branchof the Medellin Cartel have claimed respon-sibility for the murder of two employees ofBogota's daily E1 Espectador on Nov 15.The murders took place in Medellin.2.2 The Level of A FactThe level of a fact, F, in a piece of text is definedby the following algorithm:1.
Build a network, S, for the piece of text.2.
Identify the nodes that are relevant to the fact,F.
Suppose {xl,x~,... ,Xn} are the nodes rel-evant to F. Let s be the partial network con-sisting of the set of nodes {xl, x~,.. .
,  x~} inter-connected by the set of arcs {tl, t2, .
.
.
,  tk}.We define the level of the fact, F, with respect tothe network, S to be equal to k, the number ofarcs linking the nodes which comprise the factF ins .2.2.1 ObservationsGiven the definition of the level of a fact, the follow-ing observations can be made:?
The level of a fact is related to the conceptof "semantic vicinity" defined by Schubert et.al.
(Schubert and others, 1979).
The semanticvicinity of a node in a network consists of thenodes and the arcs reachable from that node bytraversing a small number of arcs.
The funda-mental assumption used here is that "the knowl-edge required to perform an intellectual taskgenerally lies in the semantic vicinity of the con-cepts involved in the task" (Schubert and oth-ers, 1979).The level of a fact is equal to the number ofarcs that one needs to traverse to reach all theconcepts (nodes) which comprise the fact of in-terest.?
A level-0 fact consists of a single node (i.e.
notransitions) in a network.?
A level-k fact is a union of k level-1 facts:?
Conjunctions/disjunctions increase the level ofa fact.?
A higher level fact is likely to be harder to ex-tract than a lower level fact.?
A fact appear ing at one level in a piece of textmay appear  at some other level in the samepiece of text.?
The  level of a fact in a piece of text dependson the granularity of the network  constructedfor that piece of text.
Therefore, the level of afact with respect to a network  built at the wordlevel (i.e.
words  represent objects and  the re-lationships between the objects) will be greaterthan the level of a fact with respect to a networkbuilt at the phrase level (i.e.
noun  groups repre-sent objects while verb groups and  prepositiongroups represent the relationships between theobjects).2.2.2 ExamplesLet S be the network shown in Figure 1.
S has beenbuilt at the phrase level.?
The city mentioned, in S, is an example of alevel-0 fact because the "city" fact consists onlyof one node "Medellin."?
The type of attack, in S, is an example of alevel-1 fact.36We define the type o/attack in the network to bean attack designator such as "murder, .... bomb-ing," or "assassination" with one modifier giv-ing the victim, perpetrator, date, location, orother information.In this case the type of attack fact is composedof the "the murder" and the "two employees"nodes and their connector.
This makes the typeof attack a level-1 fact.The type of attack could appear as a level-0 factas in "the Medellin bombing" (assuming thatthe network is built at the phrase level) becausein this case both the attack designator (bomb-ing) and the modifier (Medellin) occur in thesame node.
The type of attack fact occurs as alevel-2 fact in the following sentence (once againassuming that the network is built at the phraselevel): "10 people were killed in the offensivewhich included several bombings."
In this casethere is no direct connector between the attackdesignator (several bombings) and its modifier(10 people).
They are connected by the inter-mediatory "the offensive" node; thereby makingthe type of attack a level-2 fact.
The type of at-tack can also appear at higher levels.?
In S, the date of the murder of the two employ-ees is an example of a level-2 fact.This is because the attack designator (the tour-der) along with its modifier (two employees) ac-count for one level and the arc to "Nov 15" ac-counts for the second level.The date of the attack, in this case, is not alevel-1 fact (because of the two nodes "the tour-der" and "Nov 15") because the phrase "themurder on Nov 15" does not tell one that an at-tack actually took place.
The article could havebeen talking about a seminar on murders thattook place on Nov 15 and not about the murderof two employees which took place then.?
In S, the location of the murder of the two em-ployees is an example of a level-2 fact.The exact same argument as the date of themurder of' the two employees applies here.?
The complete information, in S, about the vic-tiros is an example of a level-2 fact because toknow that two employees of Bogota's Daily E1Espectador were victims, one has to know thatthey were murdered.
The attack designator (themurder) with its modifier (two employees) ac-counts for one level, while the connector be-tween "two employees" and "Bogota's Daily E1Espectador" accounts for the other.2.3 Bu i ld ing  the NetworksAs mentioned earlier, the level of a fact for a pieceof text depends on the network constructed for thetext.
Since there is no unique network correspondingto a piece of text, care has to be taken so that thenetworks are built consistently.We used the following algorithm to build the net-works:1.
Every article was broken up into a non-overlapping sequence of noun groups (NGs),verb groups (VGs), and preposition groups(PGs).
The rules employed to identify the NGs,VGs, and PGs were almost he same as the onesemployed by SRI's FASTUS system 1.2.
The nodes of the network consisted of the NGswhile the transitions between the nodes con-sisted of the VGs and the PGs.3.
Identification of coreferent nodes and preposi-tional phrase attachments were done manually.The networks are built based largely upon the syn-tactic structure of the text contained in the articles.However, there is some semantics encoded into thenetworks because identification of coreferent nodesand preposition phrase attachments are done manu-ally.Obviously, if one were to employ a different al-gorithm for building the networks, one would getdifferent numbers for the level of a fact.
But, if thealgorithm were employed consistently across all thefacts of interest and across all articles in a domain,the numbers on the level of a fact would be consis-tently different and one would still be able to analyzethe relative complexity of extracting that fact froma piece of text in the domain.3 Example: Analyzing theComplex i ty  of  an In fo rmat ionExt rac t ion  TaskIn order to validate our model of complexity we ap-plied it to the Information Extraction (IE) task,or the Message Understanding task (DAR, 1991),(DAR, 1992), (ARP, 1993), (DAR, 1995), (DAR,1998).
The goal of an IE task is to extract pre-specified facts from text and fill in predefined tem-plates containing labeled slots.We analyzed the complexity of the task usedfor the Fourth Message Understanding Conference(MUC-4) (DAR, 1992).
In this task, the partici-pangs were asked to extract he following facts fromarticles describing terrorist activities in Latin Amer-ica:?
The type of attack.?
The date of the attack.?
The location of the attack.1We wish to thank Jerry Hobbs of SRI for providing uswith the rules of their partial parser.37LLE=z110105100959O858O757O6560555O4540353O2520151050i):':--.x ........ x0 1 2 3 4 5 6 7 B 9 10LevelsI * I IAttack FactDate Fact -~--.Location Fact -E3--'Victim Fact ..x ......Perpetrator Fact --,~.--11 12 13 14 15Figure 2: MUC-4: Level Distribution of Each of the Five Facts50 i4540353020\ ]5\ ]0500f I I I I l I I i I I I I IMUC 4:5 Facts1 2 3 4 5 6 7 S 9 101\ ]  "12"131415LevelsFigure 3: MUC-4: Level Distribution of the FiveFacts Combined?
The victim (including damage to property).?
The perpetrator(s) (including suspects).We analyzed a set of 100 articles from the MUC-4domain each of which reported one or more terror-ist attacks.
Figure 2 shows the level distribution foreach of the five facts.
A closer analysis of the figureshows that the "type of attack" fact is the easiest toextract while the "perpetrator" fact is the hardest(the curve peaks at level-2 for this fact).
In addition,Figure 3 shows the level distribution of the five factscombined.
This figure gives some indication of thecomplexity of the MUC-4 task because it shows thatalmost 50% of the MUC-4 facts occur at level-1.
Theexpected level of the five facts in the MUC-4 domainwas 1.74 (this is simply the weighted average of thelevel distributions of the facts).
We define this num-ber to be the Task Complexity for the MUC-4 task.Therefore, the MUC-4 task can now be compared to,say, the MUC-5 task by comparing their Task Com-plexities.
In fact, we computed the Task Complexityof the MUC-5 task and discovered that it was equalto 2.5.
In comparison, an analysis, using more "su-perficial" features, done by Beth Sundheim, showsthat the nature of the MUC-5 EJV task is approx-imately twice as hard as the nature of the MUC-4task (Sundheim, 1993).
The features used in thestudy included vocabulary size, the average numberof words per sentence, and the average number ofsentences per article.
More details about this anal-ysis can be found in (Bagga and Biermann, 1998).4 Analyzing the ReadingComprehension TaskThe reading comprehension task differs from the QAtask in the following way: while the goal of the QAtask is to find answers for a set of questions from acollection of documents, the goal of the reading com-prehension task is to find answers to a set of ques-tions from a single related document.
Since the QAtask involves extracting answers from a collection ofdocuments, the complexity of this task depends onthe expected level of occurrence of the answers ofthe questions.
While it is theoretically possible tocompute the average level of any fact in the entire38TestBasicBasic-Interm# ofsentences13avg # oflevels/sent4.112.69avg # ofcorefs/sent2.332.39# ofquestions6avg # oflevels/answer3.753.33avg # ofcorers/answer2.252.50Intermediate 56 3.50 2.55 9 4.44 3.33Interm-Adv 17 6.47 1.00 6 7.83 1.33Advanced 27 6.93 2.08 10 8.20 2.90Figure 4: Summary of Resultsdocument collection, it is not humanly possible toanalyze every document in such large collections tocompute this.
For example, the TREC collectionused for the QA track is approximately 5GB.
How-ever, since the reading comprehension task involvesextracting the answers from a single document, it ispossible to analyze the document itself in additionto computing the level of the occurrence of each an-swer.
Therefore, the results presented in this paperwill provide both these values.4.1 Ana lys is  and Resu l t sWe analyzed a set of five reading comprehensiontests offered by the English Language Center atthe University of Victoria in Canada 2.
Thesefive tests are listed in increasing order of diffi-culty and are classified by the Center as: Ba-sic, Basic-Intermediate, Intermediate, Intermediate-Advanced, and Advanced.
For each of these tests, wecalculated the level number of each sentence in thetext, and the level number of the sentences contain-ing the answers to each question for every test.
Inaddition, we also calculated the number of corefer-ences present in each sentence in the texts, and thecorresponding number in the sentences containingeach answer.
It should be noted that we were forcedto calculate the level number of the sentences con-taining the answer as opposed to calculating the levelnumber of the answer itself because several ques-tions had only true/false answers.
Since there wasno way to compute the level numbers of true/falseanswers, we decided to calculate the level numbers ofthe sentences containing the answers in order to beconsistent.
For true/false answers this implied an-alyzing all the sentences which help determine thetruth value of the question.Figure 4 shows for each text, the number of sen-tences in the text, the average level number of a sen-tence, the average number of coreferences per sen-tence, the number of questions corresponding to thetest, the average level number of each answer, andthe average number of coreferences per answer.The results shown in Figure 4 are consistent withthe model.
The figure shows that as the difficultylevel of the tests increase, so do the correspondinglevel numbers per sentence, and the answers.
One2 http://web2.uvcs.uvic.ca/elc/studyzone/index.htmconclusion that we can draw from the numbers isthat the Basic-Intermediate st, based upon theanalysis, is slightly more easy than the Basic test.We will address this issue in the next section.The numbers of coreferences, urprisingly, do noincrease with the difficulty of the tests.
However,a closer look at the types of coreference shows thatwhile most of the coreferences in the first two tests(Basic, and Basic-Intermediate) are simple pronom-inal coreferences (he, she, it, etc.
), the coreferencesused in the last two tests (Intermediate-Advanced,and Advanced) require more knowledge to process.Some examples include marijuana coreferent withthe drug, hemp with the pant, etc.
Not being ableto capture the complexity of the coreferences is one,among several, shortcomings of this model.4.2 A Compar i son  w i th  QandaMITRE 3 ran its Qanda reading comprehension sys-tem on the five tests analyzed in the previous sec-tion.
However, instead of producing a single answerfor each question, Qanda produces a list of answerslisted in decreasing order of confidence.
The rest ofthis section describes an evaluation of Qanda's per-formance on the five tests and a comparison withthe analysis done in the previous ection.In order to evaluate Qanda's performance on thefive tests we decided to use the Mean ReciprocalAnswer Rank (MRAR) technique which was usedfor evaluating question-answering systems at TREC-8 (Singhal, 1999).
For each answer, this techniquesassigns a score between 0 and 1 depending on itsrank in the list of answers output.
The score foranswer, i, is computed as:1Scorel = rank of answeriIf no correct answer is found in the list, a score of0 is assigned.
Therefore, MRAR for a reading com-prehension test is the sum of the scores for answerscorresponding to each question for that test.Figure 5 summarizes Qanda's results for the fivetests.
The figure shows, for each test, the number ofquestions, the cumulative MRAR for all answers forthe test, and the average MRAR per answer.3We would like to thank Marc Light and Eric Breck fortheir help with running Qanda on our data.39Test !
# of ' MRAR for avg MRARquestions all answers per answerBasic 8 2.933 0.367Basic-Interm 6 3.360 0.560Intermediate 9 2.029 0.226Interm- Adv 6 1.008 0.16810 Advanced 7.833 0.783Figure 5: Summary of Qanda's ResultsThe results from Qanda are more or less consis-tent with the analysis done earlier.
Except for theAdvanced test, the average Mean Reciprocal AnswerRank is consistent with the average number of levelsper sentence (from Figure 4).
It should be pointedout that the system performed significantly better onthe Basic-Intermediate Test compared to the Basictest consistent with the numbers in Figure 4.
How-ever, contrary to expectation, Qanda performed ex-ceedingly well on the Advanced test answering 7 outof the 10 questions with answers whose rank is 1 (i.e.the first answer among the list of possible answersfor each question is the correct one).
We are cur-rently consulting the developers of the system forconducting an analysis of the performance on thistest in more detail.5 ShortcomingsThis measure is just the beginning of a search foruseful complexity measures.
Although the measureis a big step up from the measures used earlier, it hasa number of shortcomings.
The main shortcoming isthe ambiguity regarding the selection of nodes fromthe network regarding the fact of interest.
Considerthe following sentence: "This is a report from theStraits of Taiwan .
.
.
.
.
.
.
.
.
Yesterday, China testfired a missile."
Suppose we are interested in thelocation of the launch of the missile.
The ambiguityhere arises from the fact that the article does notexplicitly mention that the missile was launched inthe Straits of Taiwan.
The decision to infer thatfact from the information present depends upon theperson building the network.In addition, the measure does not account for thefollowing factors (the list is not complete):coreference: If the extraction of a fact requires theresolution of several coreferences, it is clearlymore difficult than an extraction which doesnot.
In addition, the degree of difficulty of re-solving coreferences it elf varies from simple ex-act matches~ and pronominal coreferences, toones that require external world knowledge.frequency of answers: The frequency of occur-rence of facts in a collection of documents hasan impact on the performance ofsystems.occurrence of multiple (s imi lar)  facts:Clearly, if several similar facts are presentin the same article, the systems will find itharder to extract he correct fact.vocabulary size: Unknown words present someproblems to systems making it harder for themto perform well.On the other hand, no measure can take into ac-count all possible features in natural language.
Con-sider the following example.
In an article, supposeone initially encounters a series of statements hatobliquely imply that the following statement is false.Then the statement is given: "Bill Clinton visitedTaiwan last week."
Processing such discourse re-quires an ability to perfectly understand the initialseries of statements before the truth value of tlie laststatement can be properly evaluated.
Such completeunderstanding is beyond the state of the art and islikely to remain so for many years.Despite these shortcomings, the current measuredoes quantify complexity on one very important di-mension, namely the number of clauses (or phrases)required to specify a fact.
For the short term itappears to be the best available vehicle for under-standing the complexity of extracting a fact.6 ConclusionsIn this paper we have described a model that can beused to analyze the complexity of a reading compre-hension task.
The model has been used to analyzefive different reading comprehension tests, and thepaper presents the results from the analysis.ReferencesARPA.
1993.
Fifth Message Understanding Confer-ence (MUC-5); San Mateo, August.
Morgan Kauf-mann Publishers, Inc.Amit Bagga and Alan W. Biermann.
1997.
Ana-lyzing the Complexity of a Domain With RespectTo An Information Extraction Task.
In Tenth In-ternational Conference on Research on Computa-tional Linguistics (ROCLING X), pages 175-194,August.Amit Bagga and Alan W. Biermann.
1998.
Ana-lyzing the Performance of Message Understand-ing Systems.
Journal of Computational Linguis-40tics and Chinese Language Processing, 3(1):1-26,February.Amit Bagga, Wlodek Zadrozny, and James Puste-jovsky.
1999.
Semantics and Complexity of Ques-tion Answering Systems: Towards a Moore's Lawfor Natural Language Engineering.
In 1999 AAAIFall Symposium Series on Question AnsweringSystems, pages 1-10, November.DARPA.
1991.
Third Message Understanding Con-ference (MUC-3), San Mateo, May.
Morgan Kauf-mann Publishers, Inc.DARPA.
1992.
Fourth Message UnderstandingConference (MUC-4), San Mateo, June.
MorganKaufmann Publishers, Inc.DARPA: TIPSTER Text Program.
1995.
SixthMessage Understanding Conference (MUC-6),San Mateo, November.
Morgan Kaufmann Pub-lishers, Inc.DARPA: TIPSTER Text Program.
1998.
SeventhMessage Understanding Conference (MUC-7).
http://www.muc.saic.com/proceedings/muc_7_toc.html, April.D.
K. Harman, editor.
1999.
Eighth Text RE-trieval Conference (TREC-8).
National Instituteof Standards and Technology (NIST), U.S. De-partment of Commerce, National Technical Infor-mation Service, November.Gary G. Hendrix.
1979.
Encoding Knowledge inPartitioned Networks.
In Nicholas V. Findler, edi-tor, Associative Networks, pages 51-92.
AcademicPress, New York.Lynette Hirschman et al 1999.
Deep Read: A Read-ing Comprehension System.
In 37th Annual Meet-ing of the Association of Computational Linguis-tics, pages 325-332, June.Lynette Hirschman.
1992.
An Adjunct Test forDiscourse Processing in MUC-4.
In Fourth Mes-sage Understanding Conference (MUC-4) (DAR,1992), pages 67-77.Lenhart K. Schubert et al 1979.
The Structure andOrganization of a Semantic Net for Comprehen-sion and Inference.
In Nicholas V. Findler, editor,Associative Networks, pages 121-175.
AcademicPress, New York.Amit Singhal.
1999.
Question Answering Track atTREC-8.
http://www, research, art.
com/~ singhal/qa-track-spec.txt, November.Beth M. Sundheim.
1993.
Tipster/MUC-5 Informa-tion Extraction System Evaluation.
In Fifth Mes-sage Understanding Conference (MUC-5) (ARP,1993), pages 27-44.41
