MODELING THE USER IN NATURAL LANGUAGE SYSTEMSRobert Kass and Tim FininDepartment of Computer  and Information Science/D2School  of Engineering and Appl ied ScienceUniversity of PennsylvaniaPhi ladelphia, PA 19104For intelligent interactive systems to communicate with humans in a natural manner, they must haveknowledge about the system users.
This paper explores the role of user modeling in such systems.
Itbegins with a characterization of what a user model is and how it can be used.
The types of informationthat a user model may be required to keep about a user are then identified and discussed.
User modelsthemselves can vary greatly depending on the requirements of the situation and the implementation, soseveral dimensions along which they can be classified are presented.
Since acquiring the knowledge fora user model is a fundamental problem in user modeling, a section is devoted to this topic.
Next, thebenefits and costs of implementing a user modeling component for a system are weighed in light ofseveral aspects of the interaction requirements that may be imposed by the system.
Finally, the currentstate of research in user modeling is summarized, and future research topics that must be addressed inorder to achieve powerful, general user modeling systems are assessed.1 INTRODUCTIONSystems that use natural anguage as a means of com-munication must do so in a natural manner.
One of thefeatures of communication between people is that theyacquire and use considerable knowledge about theirconversational partners.
In order for machines to inter-act with people in a comfortable, natural manner, theytoo will have to acquire and use knowledge of thepeople with whom they are interacting.Early research on natural anguage interfaces tendedto view natural anguage as a "very high level" querylanguage.
One of the important results of research in thelatter half of the 1970s (Waltz 1978, Kaplan 1982) is therealization that natural language communication ismuch more.
The use of natural anguage for communi-cation includes a host of conventions that must befollowed in the dialog (Grice 1975).
A person interactingwith a computer via natural anguage will assume thatthese conventions are being followed, and will be quiteunsatisfied if they are not.
Most of these conventionsrequire, in one way or another, that a conversationalparticipant have particular knowledge about the goals,plans, capabilities, attitudes, and beliefs of the otherperson.This paper analyzes the role of user models insystems that interact with individual users in a naturallanguage.
Although the necessity of having and using amodel of the user has been seen for some time, onlywithin the last few years has it been actively pursued asa research topic.
This research as been driven, in part,by attempts to create natural language interfaces tosystems that can be characterized as cooperative prob-lem solvers.
Examples of such systems include intelli-gent interfaces to expert systems (Finin et al1986,Carbonell et al1983), database systems (Carberry 1985,Webber 1986), intelligent tutoring systems (Kass1987b), and help and advisory systems (Wilensky et al1984).1.1 AN OVERVIEW OF THIS PAPERIn the remainder of this section, the kinds of usermodels and systems to be discussed in this paper will becharacterized, including a general definition of a usermodel and an outline of how it can be used by acooperative, interactive system that converses in natu-ral language.
The next section addresses the question"What is to be modeled?"
by looking in some depth atthe types of information that might be contained in auser model.
These can be broadly classified as theuser's goals (and the plans he may use to achieve them),capabilities, attitudes, and knowledge or belief.
In sec-tion 3 a set of dimensions along which user models canCopyright 1988 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material isgranted providedthat he copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires afee and/or specific permission.0362-613X/88/0100o-0503.00Computational Linguistics, Volume 14, Number 3, September 1988 5Kass and Finin Modeling the User in Natural Language Systemsbe classified is presented, while section 4 considers themethods that might be used to acquire information ofthe user, especially of his goals, plans, and beliefs.Section 5 considers everal high-level features that havean impact on the design of a user modeling system, suchas which participant in the interaction bears responsi-bility for ensuring the communication, or what thepenalty for an error in the user model is.
These consid-erations have an impact on the potential benefits andcosts of employing a user model.
The concluding sec-tion raises some issues that will require additionalresearch in order to produce a powerful, general usermodeling system.1.2 WHAT IS A USER MODEL?Specifying what a user model is is not an easy task.
Aninitial, general definition is presented here, but is thennarrowed to focus on explicit, knowledge-based mod-els.
The various ways in which these user models cansupport a cooperative problem solving system are thenoutlined.The term "user model" has been used in manydifferent contexts to describe knowledge that is used tosupport a man-machine interface.
An initial definitionfor "user model" might be the following:A user model is the knowledge about he user, eitherexplicitly or implicitly encoded, that is used by thesystem to improve the interaction.This definition is at once too strong and too weak.
Thedefinition is too strong in that it limits the range ofmodeling a natural language system might do to the userof the system only.
Many situations require a naturallanguage system to deal with several models concur-rently, as will be demonstrated later in this paper.
Thedefinition is too weak since it endows every interactivesystem with some kind of user model, usually of theimplicit variety.
The following paragraphs clarify theseissues, and in so doing restrict he class of models to beconsidered.AGENT MODELSImagine a futuristic data base query system: not only dohumans communicate with the system to obtain infor-mation, but other software systems, or even othercomputer systems might query the data base as well.The individuals using the data base might be quitediverse.
Rather than force all users to conform tointeraction requirements imposed by the system, thesystem strives to communicate with them at their ownlevel.
Such a system will need to model both people andmachines.
A second situation is when a person uses anapplication such as an advisory system on behalf ofanother individual; the advisor in this case may berequired to concurrently model both individuals.A useful distinction when discussing situations inwhich multiple models may be required is one betweenagent models and user models.
Agent models are mod-els of individual entities, regardless of their relation tothe sy,~tem doing the modeling, while user models aremodels of the individuals currently using the system.The class of user models is thus a subclass of the classof agent models.
Most of the discussion in this paperapplies to the broader class of agent models, however,theterm "user model" is well established and hard toavoid.
Thus "user model" will be used in the remainderof this paper, even in situations where "agent model" istechnically more correct.EXPLICIT MODELSAgent models that encode the knowledge of the agentimplicitly are not very interesting.
In such systems, themodel knowledge really consists of the assumptionsabout the agent made by the designers of the system.Thus even the FORTRAN compiler can be said to havean implicit agent model.A more interesting class of models is one in which theinformation about the agent is explicitly encoded, suchas models that are designed along the lines of knowledgebases.
In the context of agent models, four features ofexplicitly encoded models are important.I.
Separate Knowledge Base: Information about anagent is collected in a separate module rather thendistributed throughout the system.2.
Explicit Representation: The knowledge in the agentmodel is encoded in a representation language that issufficiently expressive.
Such a representation lan-guage will typically provide a set of inferential serv-ices, allowing some of the knowledge of an agent obe implicit, but automatically inferred when needed.3.
Support for Abstraction: The modeling system pro-vides ways to describe abstract as well as concreteentities.
For example, the system might be able todiscuss classes of users and their general propertiesas well as individuals.4.
Multiple Use: Since the user model is explicitlyrepresented asa separate module, it can be used inseveral different ways (e.g., to support a dialog or toclassify a new user).
This requires that the knowl-edge be represented in a more general way that doesnot favor one use at the expense of another.
It ishighly desirable to express the knowledge in a waythat allows it to be reasoned about as well as rea-soned with.Agent models that have these features fit nicely intocurrent work in the broader field of knowledge repre-sentation.
In fact, Brian Smith's knowledge representa-tion hypothesis (Smith 1982) could be paraphrased toaddress agent modeling as follows:Any agent model will be comprised of structuralingredients that a) we as external observers naturallytake to represent a propositional account of theknowledge the system has of the agent and b) inde-pendent of such external semantical attribution, playa figrmal but causal and essential role in the behaviorthat manifests that knowledge.6 Computational Linguistics, Volume 14, Number 3, September 1988Kass and Finin Modeling the User in Natural Language SystemsUser Model Uses0*=, .
-  j r..o,.n.
".n,oInterpreting \] ~ v qlunleer/ .,oo.
info,ma,iunReliolving P q, lnlurprating \[ambiguity referring \] \ Recognizing CorrectingisconcepllonsRecognizing Recognizing ~user goals user plans Modellin 9 Providing Constructing Lexicalrelevance prarequisile referrring choiceinformation expressionsFigure 1.
Uses for Knowledge of the User.1.3 HOW USER MODELS CAN BE USEDThe knowledge about a user that a model provides canbe used in a number of ways in a natural anguagesystem.
These uses are generally categorized in thetaxonomy in Figure 1.
At the top level, user models canbe used to support (1) the task of recognizing andinterpreting the information seeking behavior of a user,(2) providing the user with help and advice, (3) elicitinginformation from the user, and (4) providing informationto him.
Situations where user models are used for manyof these purposes can be seen in the examples presentedthroughout this paper.The characterization f user models remains quitebroad to allow consideration of a wide range of factorsinvolved in building user models.
These factors providedimensions upon which the various types of user mod-els can be plotted.
Section 3 explores these dimensionsto provide a better understanding of the range of usermodeling possibilities.
Given lhis range of possibletypes of user models, methods for their acquisition canbe discussed (section 4), along with factors that influ-ence the feasibility and attractiveness of particulartypes of user models for given applications ( ection 5).First, however, the types of information a user modelshould be expected to keep are discussed.2 THE CONTENTS OF A USER MODELA primary means of characterizing user models is by thetype of knowledge they contain.
This knowledge can beclassified into four categories: goals and plans, capabil-ities, attitudes, and knowledge or belief.
Each of thesecategories will be examined in this section to seesituations where such knowledge is needed, and exam-ples of how that knowledge is used in natural languagesystems.2.1 GOALS AND PLANSThe goal of a user is some state of affairs he wishes toachieve.
A plan is some sequence of actions or eventsthat is expected to result in the realization of a particularstate of affairs.
Thus plans are means for accomplishinggoals.
Furthermore, each step in a plan has its ownsubgoal to achieve, which may be realized by yetanother subplan of the overall plan.
As a result, goalsand plans are intimately related to one another, and onecan seldom discuss one without discussing the other.Knowledge of user goals and plans is essential in anatural anguage system.
Individuals participate in aconversation with particular goals they wish to achieve.Examples of such goals are obtaining information, com-municating information, causing an action to be per-formed, and so on.
A cooperative participant in aconversation will attempt to discover the goals of otherparticipants in an effort to help those goals to beachieved, if possible.Recognizing an individual's goal (or goals) may rangefrom being a straightforward task, to one that is verydifficult.
Situations in which a natural language systemmust infer goals or plans of user (roughly in order ofincreasing difficulty) include:?
the user directly states a goal?
the user's goal may be indirectly inferred from theuser's utterances?
the user has incorrect or incomplete goals and plans?
the user has multiple goals and plans.DIRECT GOALSIn the simplest situations the user may directly state agoal, such as"How do I get to Twelve Oaks Mall from here?
"The speaker's goal is to obtain information.
A hearer iscapable of recognizing this goal directly from the ques-tion, without further inference.INDIRECT GOALSUnfortunately, people frequently do not state theirgoals directly.
Instead, they may expect he hearer toinfer their goal from their utterance.
For example, whena speaker says,"Can you tell me what time it is?
"the hearer eadily infers that the questioner wishes toknow what the current ime is.
The inferences requiredby the hearer may often be rather involved.
Gershmanlooked at this problem with respect o an AutomaticYellow Pages Advisor (AYPA) (Gershman 1981).
Asample interaction with this system might begin with theuser stating:"My windshield is broken, help.
"Computational Linguistics, Volume 14, Number 3, September 1988 7Kass and Finin Modeling the User in Natural Language SystemsThe AYPA system must infer that the user wishes toreplace the windshield and hence needs to know aboutautomotive repair shops that replace windshields, orglass shops that handle automotive glass.Allen and Perrault (1980) studied interactions thatoccur between an information-booth attendant ina trainstation and people who come to the booth to askquestions.
An example of such an interaction isQ.
The 3:15 train to Windsor?A.
Gate 10.From the question alone it is unclear what goal Q has inmind.
However, the attendant has a model of the goalsindividuals who ask questions at train stations have.The attendant assumes Q has the goal of meeting orboarding the 3:15 train to Windsor.
Once the attendanthas determined Q's goal, he then tries to provideinformation to help Q achieve that goal.
In Allen'smodel, the attendant seeks to find obstacles to thequestioner's goal.
Obstacles are subgoals in the plan ofthe Q that cannot be easily achieved by Q withoutassistance.
In this case the obstacle in Q's plan ofboarding the train is finding the location of the train,which the attendant resolves by telling Q which gate thetrain will leave from.INCORRECT OR INCOMPLETE GOALS AND PLANSSometimes the plans or goals that can be inferred fromthe user's utterances may be incomplete or incorrect.Goodman (1985) has addressed the problem of incorrectutterances in the context of miscommunication n refer-ring to objects.
He currently is working on dealing withmiscommunication o  a larger scale to deal with mis-communication at the level of plans and goals(Goodman 1986).
Sidner and Israel (1981) have alsostudied the problem of recognizing when a user's plan isincorrect, by keeping a library of "buggy" plans.
1Incomplete specification of a goal by the user can bedealt with via clarification subdialogs, where the systemattempts to elicit more information from the user beforecontinuing.
Litman and Allen (1984) have presented amodel for recognizing plans in such situations.Situations where user goals are incomplete or incor-rect violate what Pollack calls the appropriate queryassumption (Pollack 1985).
The appropriate query as-sumption is adopted by many systems when they as-sume that the user is capable of correctly formulating aquestion to a system that will result in the systemproviding the information they need.
As pointed out inPollack et al(1982) this is frequently not the case.Individuals eeking advice from an expert often do notknow what information they need, or how to expressthat need.
Consequently such individuals will tend tomake statements hat do not provide enough informa-tion, or that indicate they have a plan that will not work.A system that makes the appropriate query assumptionmust be able to reason about the true intentions of theuser when making a response.
Often this response mustaddress the user goals inferred by the system, and notthe goal explicit in the user's question.MULTIPLE GOALS AND PLANSA further complication is the need to recognize multiplegoals that a user might have.
Allen, Frisch, and Litmandistinguish between task goals and communicative goalsin a discourse.
The communicative goal is the immedi-ate goal of the utterance.
Thus in the question"Can you tell me what time the next train to theairport departs?
"the cornmunicative goal of the questioner is to discoverwhen the next train leaves.
The task goal of the user isto board the train.
Carberry's TRACK system (Car-berry 1983, and this issue) allows for a complex domainof goals and plans.
TRACK builds a tree of goals andplans that have been mentioned ina dialog.
One node inthe tree is recognized as the focused goal, the goal theuser is currently pursuing.
The path from the focusedgoal to the root of the tree represents he global contextof the focused goal.
The global context represents goalsthat are still viewed as active by the system.
Othernodes in the tree represent goals that have been activein the past, or have been considered as possible goals ofthe user by the system.
As the user shifts plans, some ofthese other nodes in the tree may become reactivated.2.2 CAPABILITIESSome natural language systems need to model thecapabilities of their users.
These capabilities may be oftwo types: physical capabilities, such as the ability tophysically perform some action that the system mayrecommend, or (for lack of a better term) mental capabil-ities, such as the ability of a user to understand a recom-mendation or explanation provided by the system.Systems that make recommendations i volving ac-tions on the part of the user must have knowledge ofwhether the user is physically capable of performingsuch actions.
Expert and advisory systems have per-haps the strongest need for this form of knowledge.
Anexpert system frequently asks the user questions to getinformation about the world.
For example, medicaldiagnostic systems often need to know the results ofparticular tests that have been run or could be run.
Thesystem needs to know whether the user is capable ofperforming such tests or acquiring such data.
Likewise,a recommendation made by an expert system or anadvisor is of little use if the user is not capable offollowing the recommendation.A natural language system also needs to judgewhether the user will be able to understand a responseor explanation the system might make.
Wallis andShortliffe (1982) addressed this issue by controlling theamount of explanation provided, based on the expertiselevel of the current user.
Paris's TAILOR system (Paris1987) goes beyond the work of Wallis and Shortliffe byproviding different ypes of explanations depending on8 Computational Linguistics, Volume 14, Number 3, September 1988Kass and Finin Modeling the User in Natural Language Systemsthe user's domain knowledge.
Paris, comparing expla-nations of phenomena from a range of encyclopedias,found that explanations geared towards persons naiveto the domain focused on procedural accounts of thephenomena, while explanations for domain expertstended to give a hierarchical explanation of the compo-nents of the phenomena.
TAILOR consequently gener-ates radically different explanations depending onwhether the user is considered to be naive or expertwith respect o the domain of explanation.
Webber andFinin (1984) have surveyed ways that an interactivesystem might reason about its user's capabilities toimprove the interaction.Care should be taken to distinguish between mentalcapabilities and domain knowledge possessed by theuser.
In each of the examples above, some globalcategorization f the user has been made (into classessuch as naive or expert) with respect o the domain.This category is used as the basis for a judgment of theuser's mental capabilities.
Much more could be done:modeling of mental capabilities of users should alsoinvolve modeling of human learning, memory, andcognitive load limitations.
Such modeling capabilitieswould allow a natural anguage system to tailor thelength and content of explanations, based on theamount of information the user is capable of assimulat-ing.
Modeling of this sort seems a long way off, how-ever.
Cognitive scientists are just beginning to addresssome of the issues raised here, with current workfocusing on very simple domains, such as how humanslearn to use a four-function calculator (Halasz andMoran 1983).2.3 ATTITUDESPeople are subjective.
They hold beliefs on variousissues that may be well founded or totally unfounded.They exhibit preferences and bias toward particularoptions or solutions.
A natural anguage system mayoften need to recognize the bias and preferences a userhas in order to communicate effectively.One of the earliest user modeling systems dealt withmodeling user preferences.
GRUNDY (Rich 1979) rec-ommended books to users, based on a set of self-descriptive attributes that the users provided and onuser reactions to books recommended by the system.Although GRUNDY dealt with personal preferencesand attitudes, it had the advantage of being able todirectly acquire these attitudes by asking the user.
Inmost situations it is not socially acceptable to questiona user about particular attitudes, hence the system mustresort o acquiring this information implicitly--based onthe behavior of the user.
The Real-Estate Advisor(Morik and Rollinger 1985) and HAM-ANS (Hoeppneret al1983, Morik 1988) do this to some degree in thedomains of apartment and hotel room rentals.
The userwill express ome preferences about particular types ofrooms or locations, and each system can then makedeeper inferences about preferences the user mighthave.
This information is used to tailor the informationprovided and the suggestions made by the systems.A natural anguage system needs to consider per-sonal attitudes when generating responses.
The choiceof words used, the order of presentation r the presenceor lack of specific items in an answer can drasticallyalter the impact a response has on the user.
Jameson(1983, 1988) addresses this issue in the system IMP.IMP takes the role of an informant who responds toquestions from a user concerned with evaluating aparticular object (in this case, an apartment).
IMP canassume aparticular bias (for or against the apartment inquestion, or neutral) and uses this bias in the responsesit makes to the user.
Thus if IMP is favorably biasedtowards aparticular apartment, i  will include additionalbut related information in responses that favorablyrepresent the apartment, while attempting to tempernegative features with qualifiers or additional non-negative features.
Thus IMP strives to be a cooperative,biased system while appearing to be objective.Swartout (1983) and McKeown (1985a) address theeffects of the user's perspective or point of view on theexplanations generated by a system.
In the XPLAINsystem built to generate xplanations for the DigitalisTherapy Advisor, Swartout uses a very rudimentarytechnique to represent points of view.
Attached to eachrule in the knowledge base is a list of viewpoints.
Onlyrules with a viewpoint held by the user are used ingenerating an explanation.
McKeown uses intersectingmultiple hierarchies in the domain knowledge base torepresent the different perspectives a user might have.This partitioning of the knowledge base allows thesystem to distinguish between different ypes of infor-mation that support a particular fact.
When selectingwhat to say the system can choose information thatsupports the point the system is trying to make, and thatagrees with the perspective of the user.Utterances from the user must be considered in lightof potential bias as well.
Sparck Jones (1984) considersa situation where an expert system is used to computebenefits for retired people.
The system is used directlyby an agent who talks to the actual people underconsideration by the system (the patients).2 In this casethe system must recognize potential bias on the parts ofboth agent and patient.
The patient may withhold infor-mation or try to "fudge" information in order to im-prove their benefits, while the bias of the agent maycolor information about he patient by the way the agentprovides the information to the system.2.4 KNOWLEDGE AND BELIEFAny complete model of a user will include informationabout what the user knows, or what he believes.
In thecontext of modeling other individuals, an agent does nothave access to objective truth and hence cannot reallydistinguish whether a proposition is known or simplybelieved to be true.
Thus the terms knowledge andbelief will be used interchangeably.Computational Linguistics, Volume 14, Number 3, September 1988 9Kass and Finin Modeling the User in Natural Language SystemsModeling the knowledge of a user involves a varietyof things.
First, there is the knowledge the user has ofthe domain of the application system itself.
In addition,a user model may need to model information the userhas about concepts beyond the actual domain of theapplication (which might be called commonsense orworm knowledge).
Finally, any user, being an intelligentagent, has a model of other agents (including the sys-tem) and even of himself or herself.
These models arerecursive, in that the user's model of the system willinclude information about what the user believes thesystem believes about the user, about what the userbelieves the system believes the user believes about hesystem, and so on.
In the following paragraphs eachtype of belief is explored in more detail.DOMAIN KNOWLEDGEKnowing what the user believes to be true about theapplication domain is useful for many types of naturallanguage systems.
In generating responses, knowledgeof the concepts and terms the user understands or isfamiliar with allows the system to produce responsesincorporating those concepts and terms, while avoidingconcepts the system feels the user might not under-stand.
This is especially true for intelligent help systems(Finin 1982), which must provide clear, understandableexplanations tobe truly helpful.
Providing definitions ofdatabase items (such as the TEXT system does (Mc-Keown 1985b)) has a similar requirement toexpress thedefinition at a level of detail and in terms the userunderstands.
UC also uses its user model (KNOME)(Chin 1988) to help tailor responses, uch as determin-ing whether to explain a command by using an analogyto commands the user already knows.Knowing what the user believes is also importantwhen requesting information from the user.
As Webberand Finin have pointed out (Webber and Finin 1984),systems that ask questions of the user (such as expertsystems) should recognize that users may not be able tounderstand some questions, particularly when the sys-tem uses terminology or concepts the user is unfamiliarwith.
Such systems need knowledge of the user to aid informalizing such questions.Modeling user knowledge of the application domaincan take on two forms: overlay models and perturbationmodels.
3An overlay model is based on the assumptionthat the user's knowledge is a subset of the domainknowledge.
An overlay user model can thus be thoughtof as a template that is "laid over" the domain knowl-edge base.
Domain concepts can then be marked as"known" or "not known" (or with some other method,such as an evidential scheme), reflecting beliefs inferredabout the user.
Overlay modeling is a very attractivetechnique because it is easy to implement and can bevery effective.
Unfortunately the underlying assump-tion of an overlay model, that the user's knowledge is asubset of the domain knowledge of the system, is quitewrong.
An overlay model can not account for users whoorganize their knowledge of the domain in a structuredifferent from that used in the domain model, nor can itaccount for misconceptions users may hold aboutknowledge in the knowledge base.The perturbation model is capable of representinguser beliefs that the overlay model cannot handle.
Aperturbation user model assumes that the beliefs held bythe user are similar to the knowledge the system has,although the user may hold beliefs that differ from thesystem's in some areas.
These differences in the usermodel can be viewed as perturbations of the knowledgein the domain knowledge base.
Thus the perturbationuser model is still built with respect to the domainmodel, but allows for some deviation in the structure ofthat knowledge.McCoy's ROMPER system (McCoy 1985, and thisissue) assumes a perturbation model in dealing withmisconceptions the user might have about the meaningof terms or the relationship of concepts in the domain offinancial instruments.
When the user is recognized tohold a belief that is inconsistent with its own domainmodel, ROMPER tries to correct his misconception byproviding an explanation that refutes the incorrect in-formation and supplies the user with corrective infor-mation.
The domain knowledge in the ROMPER systemis represented in a KL-ONE-like semantic network.ROMPER considers user misconceptions that resultfrom misclassification of a concept ("I  thought a whalewas a fish") or misattribution ("What is the interest rateon this stock?
").WORLD KNOWLEDGEOften a natural language system requires knowledgebeyond the narrow scope of the application domain inorder to interact with the user in an appropriate manner.Sparck Jones (1984) has classified three types of knowl-edge about the user that an expert system might keep:?
Decision Properties: domain-related properties usedby the system in its reasoning process.?
Non-Decision Properties: properties not directly usedin making a decision, but that may be useful.
Exam-ples of such properties might be the name, age, or sexof the user.?
Subjective Properties: non-decision properties thattend to change over time.Decision properties primarily influence the effective-ness of expert system performance.
Non-decision prop-erties can influence the efficiency of the system byenabling inferences that reduce the number of questionsthe system may need to ask the user.
All three types ofproperties influence the acceptability of the system, themanner in which the system interacts with the user.Static non-decision properties and subjective propertiescomprise knowledge of the user outside the domain ofthe underlying application system.
While such knowl-edge may not influence the effectiveness of the under-10 Computational Linguistics, Volume 14, Number 3, September 1988Kass and Finin Modeling the User in Natural Language Systemslying system, it has a great impact on the efficiency andacceptability of the system.
Hence world or common-sense knowledge is useful for a natural language systemto enhance its ability to interact with the user.A special case of modeling information outside thedomain of the application is when that information isclosely related to the domain.
Schuster (1984, 1985) hasexplored this in the context of the tutoring system VP 2for students learning a second language.
Such studentstend to use the grammar of their native language as amodel for the grammar of the language they are learn-ing.
Since VP 2 has knowledge of the native language ofthe student, it can be much more effective in recogniz-ing misconceptions the student might have when theymake mistakes.
A tutoring system would also be able touse this second language knowledge in introducing newmaterial, since frequently such material would havemuch in common with the student's native language.KNOWLEDGE OF OTHER AGENTSA final form of user knowledge that is very importantfor natural language systems is knowledge about otheragents.
As an interaction with a user progresses, notonly will the system be building a model of the beliefs,goals, capabilities, and attitudes of the user, the userwill also be building a model of the system.
Sidner andIsrael (1981) make the point that when individualscommunicate, the speaker will have an intended mean-ing, consisting of both a propositional attitude and thepropositional content of the utterance.
The speakerexpects the hearer to recognize the intended meaning,even though it is not explicitly stated.
Thus a systemmust reason about what model the user has of thesystem when making an utterance, because this willaffect what the system can conclude about what theuser intends the system to understand by the user'sstatement.A further complication in the modeling a user'sknowledge of other individuals are infinite-reflexivebeliefs (Kobsa 1984).
An example of such a belief is thefollowing situation:S believes that U believes p.S believes that U believes that S believes that Ubelieves p..An important instance of such infinite-reflexive b liefsare mutual beliefs.
A mutual belief occurs when twoagents believe a fact, and further believe that the otherbelieves the fact, and believes that they both believe thefact, and so on.
Kobsa has pointed out that in thecontext of user modeling only one-sided mutual beliefs,i.e., what the system believes is mutually believed, areof interest.User's beliefs about other agents and mutual beliefscause significant representational difficulties.
Kobsa(1985) lists three techniques that have been used torepresent beliefs of other agents:?
The syntactic approach, where the beliefs of an agentare represented in terms of derivability in a first-orderobject-language theory of the agent (Konolige 1983,Joshi et al1984, Joshi 1982);?
The semantic approach, where knowledge and wantsare represented by the accessibility relationships be-tween possible worlds in a modal ogic (Moore 1984,Halpern and Moses 1985, Fagin and Halpern 1985);?
The partition approach, where beliefs and wants ofagents are represented in separate structures that canbe nested within each other to arbitrary depths(Kobsa 1985, Kobsa 1988, Wilks and Bien 1983).While the first two approaches are primarily formalattempts, the partition approach as been implementedby Kobsa in the VIE-DPM system.
VIE-DPM uses aKL-ONE-like semantic network to represent both ge-neric and individual concepts.
The individual concepts(and associated individualized roles) form elementarysituation descriptions.
Every agent modeled by thesystem (including the system itself) can be thought of aslooking at this knowledge base from a particular point ofview, or context.
The context contains the acceptanceattitude the agent has towards each individual conceptand role in the knowledge base.
An acceptance attitudecan be either belief, disbelief, or no belief.
4An agentA's beliefs about another agent B is formed by applyingacceptance attitudes in A's context o the acceptanceattitudes of B.
This technique can be applied as often asneeded to build complex belief structures involvingmultiple agents.
Kobsa has further extended the repre-sentation to handle infinite-reflexive b liefs in a straight-forward manner.To summarize, several types of knowledge may berequired for a natural anguage system to effectivelycommunicate with the user.
This knowledge can beclassified into four categories: goals and plans, capabil-ities, attitudes, and knowledge or belief.
Not all of thisinformation may be required for any given application.Each type of information is needed in some forms ofinteraction, however, and a truly versatile natural lan-guage system would require all forms.3 THE DIMENSIONS OF A USER MODELUser models are not a homogeneous lot.
The range ofapplications for which they may be used and the differ-ent types of knowledge they may contain indicate that avariety of user models exist.
In this section the types ofuser models themselves, classified according to severaldimensions are studied.Several user modeling dimensions have been pro-posed in the past.
Finin and Drager (1986) have distin-guished between models for individual users and modelsfor classes of users (the degree of specialization) andbetween long- or short-term odels (the temporal ex-tent of the model).
Sparck Jones (1984) adds a third,whether the model is static or dynamic.
Static modelsComputational Linguistics, Volume 14, Number 3, September 1988 11Kass and Finin Modeling the User in Natural Language Systemsdo not change once they are built, while dynamicmodels change over time.
This dimension is the modi-fiability dimension of the model.Rich (1979, 1983), likewise has proposed these threedimensions, but treats the modifiability category a littledifferently.
Instead of static models, she describesexplicit models, models defined explicitly by the userand that remain permanent for the extent of the session.Examples of explicit models are "login" files or cus-tomizable environments.
She uses the term implicitmodel for models that are acquired uring the course ofa session and that are hence dynamic.
This characteri-zation seems to mix two separate issues: the method ofmodel acquisition, and the modifiability of the model.Thus the modifiability category will be limited to referonly to whether the model can change during a session,while the acquisition issues will be discussed in the nextsection.Three other modeling dimensions are of interest: themethod of use (either descriptive or prescriptive), thenumber of agents (modeling a given agent may dependupon the models of other agents as well), and thenumber of models (more than one model may be nec-essary to model an individual agent).
Figure 2 summa-rizes these dimensions.3.1 DEGREE OF SPECIALIZATIONUser models may be generic or individual.
A genericuser model assumes a homogeneous set of users--allindividuals using the program are similar enough withrespect to the application that they can be treated as thesame type of user.
Most of the natural language systemsthat focus on inferring the goals and plans of the useruse a single, generic model.
These systems includeARGOT (Allen et al1982), TRACK (Carberry 1983, andthis issue), EXCALIBUR (Carbonell et al1983) andAYPA (Gershman 1981).Individual user models contain information specificto a single user.
A user modeling system that keepsindividual models thus will have a separate model foreach user of the system.
This may become very expen-sive in terms of storage requirements, particularly if thesystem has a large number of users.A natural way to combine the system's knowledgeabout classes of users with its knowledge of individualsis through the use of stereotype models.
A stereotype isa cluster of characteristics that tend to be related toeach other.
When building a model of a user, certainpieces of information serve as triggers (Rich 1979) to astereotype.
A trigger will cause the system to include itsassociated cluster of characteristics into the individualuser model (unless overridden by other information).Systems that have used stereotypes such as GRUNDY(Rich 1979), the Real-Estate Advisor (Morik and Rol-linger 1985) and GUMS 1 (Finin and Drager 1986) furtherenhance the use of stereotypes by allowing them to bearranged in a hierarchy.
As more information is discov-ered about the user, more specific stereotypes areDegree of Specialization ~.
, , -  individual gener=cModifiability.~l-.-~-m.. - dynamic ~':~ staticTemporal Extent4.___  short term long termMethod of Use~descriptive prescrip tive~Number of Agents--, le ~ "single multi pNumber of Models~single multiple"Figure 2.
Dimensions of a User Model.activated (moving down the tree as in GUMS,), or theuser model invokes everal stereotypes concurrently (asin GRUNDY).A user modeling system might use a combination ofthese approaches.
Consider a database query system.
Ageneric user model may be employed for areas wherethe user population is homogeneous, such as modelingthe goals of users of the system.
At the same time,individual models might be kept of the domain knowl-edge of the users, their perspective on the system, andthe level of detail they expect from the system.3.2 MODIFIABILITYUsers models can be static or dynamic.
A static usermodel is one where the model does not change duringthe course of interaction with the user, while dynamicmodels can be updated as new information is learned.
Astatic model can be either pre-encoded (as is implicitlydone with most programs) or acquired uring an initialsession with the user before entering the actual topic ofthe discourse.
Dynamic models will incorporate newinformation about the user as it becomes availableduring the course of an interaction.
User models thattrack the goals and plans of the user must be dynamic.Different ypes of knowledge may require differentdegrees of modifiability.
Goal and plan modeling re-quires a dynamic model, but user attitudes or beliefsabout domain knowledge in many situations may effec-tively be modeled with static information.
Sparck Jones(19841) refers to objective properties of the user (thingslike age and sex) that are not expected to change overthe course of a session.
Objective properties, consistingof the decision and non-decision properties in herclassification, require only static modeling.
On the otherhand, subjective properties are changeable and hencerequire a dynamic model.12 Computational Linguistics, Volume 14, Number 3, September 1988Kass and Finin Modeling the User in Natural Language Systems3.3 TEMPORAL EXTENTAt the extremes, user models can be short term or longterm.
A short-term model might be one that is builtduring the course of a conversation, or even during thecourse of discussing a particular topic, then discarded atthe end.
Generic, dynamic user models are thus usuallyshort term since they have no facility for rememberinginformation about an individual user.
5 On the otherhand, individual models and static models will be longterm.
Static models by their nature are long term, whileindividual models are of little use if the information theyretain from session to session is no longer applicable.3.4 METHOD OF USEUser models may be used either descriptively or pre-scriptively.
The descriptive use of a user model is themore "traditional" approach to user models.
In thisview the user model is simply a data base of informationabout the user.
An application queries the user model todiscover the current view the system has of the user.Prescriptive use of a user model involves letting themodel simulate the user for the benefit of the system.An example of a prescriptive use of a user model is inanticipation feedback loops (Wahlster and Kobsa 1988).In an anticipation feedback loop the system's languageanalysis and interpretation components are used tosimulate the user's interpretation of a potential responseof the system.
The HAM-ANS system (Hoeppner et al1983) uses an anticipation feedback loop in its ellipsisgeneration component to ensure that the response con-templated by the system is not so brief as to beambiguous or misleading.
Jameson's IMP system (Ja-meson 1983, 1988) also makes use of an anticipationfeedback loop to consider how its proposed responsewill affect the user's evaluation of the apartment underconsideration.3.5 NUMBER OF AGENTSUser-machine interaction eed not be one-on-one.
Insome situations a system may need to actively deal withseveral individuals, or at least with their models.
RecallSparck Jones's (1984) distinction between the agent andpatient in an expert system: the agent is the actualindividual communicating with the system, while thepatient is the object of the expert system's diagnosis oranalysis.
The patient may be human or not (for exam-ple, it might be a broken piece of equipment).
In thecase where the patient is a human, the system must beaware that system requests, explanations, and recom-mendations will have an impact on both the agent andpatient, and that impact may be decidedly different oneach individual.
In her example of an expert system thatadv ises  on benefits for retired people, the agent isresponsible for providing information to the systemabout the patient.
The system must have a model of thepatient not only for its analysis, but also to guide thecommunication with the patient.
In this case, however,the only way of obtaining that model is through anotherindividual who will filter information based on his ownbias.
Thus the system must use its model of the modelthe agent has of the patient in building its own model ofthe patient.3.6 NUMBER OF MODELSIt is even possible to have multiple models for a givenuser.
Some of the systems that employ stereotypes,such as GRUNDY, address this by allowing the usermodel to inherit characteristics from several stereo-types at once.
When interaction with an individualtriggers everal different stereotypes, conflicts betweenstereotypes must be resolved in some manner.GRUNDY uses a numeric weighting method to indicatethe degree of belief the system has in each item in theuser model.
When new information is added, eitherdirectly or through the triggering of another stereotype,evidence combination rules are invoked to resolvedifferences and strengthen similarities.
Thus GRUNDYstill maintains a single model of the user and attempts toresolve differences within that model.The ability to combine stereotypes i  also useful forbuilding composite models that cover more than onedomain.
For example, consider building a modelingsystem for a person's familiarity with the operatingsystem of a computer, such as was done with the VMSoperating system in (Shrager 1981, Shrager and Finin1982, Finin 1983).
The overall domain, knowledge of theVMS system, is quite large and non-homogeneous andcan be broken down into many subdomains (e.g., thefile system, text editors, the DCL commands interface,interprocess communication, etc).
It is more reasonableto build stereotypes that represent a person's familiaritywith the subdomains rather than the overall domain.Rather than build global stereotypes uch as VMS-Novice and VMS-Expert that attempt o model a ste-reotypical user's knowledge of the entire domain, it ismore appropriate to build separate stereotype systemsto cover each subdomain.
This allows one to model aparticular user as being simultaneously an emacs-noviceand a teco-expert.Wahlster and Kobsa (1988) consider a situationwhere a system may require multiple, independentmodels for a single individual.
Among humans thishappens all the time when individuals represent busi-nesses or different organizations.
Quite often two state-ments like the following will occur during the course ofa business conversation.
"Last  time we met we had an excellent dinnertogether.
""This product is going to be a big seller.
"The first statement is made by a salesman speaking as a"normal human," perhaps as a friend of the client.
Thesecond statement is made with the "salesman hat" on.Modeling such a situation cannot be handled by multiplestereotype inheritance, because frequently the two hatsof the user will be drastically inconsistent.
Further-Computational Linguistics, Volume 14, Number 3, September 1988 13Kass and Finin Modeling the User in Natural Language Systemsmore, the inconsistencies should not be resolved.Rather it is necessary to be able to switch from one hatto another.
This problem is compounded because thetwo models of an individual are not separate.
Forexample, the goals and plans of the individual mayinvolve switching hats at various points in the conver-sation.
Thus there needs to be a central model of theuser, with submodels that are disjoint from each other.The system must hen be able to decide which submodelis necessary, and recognize when to switch submodels.4 ACQUIRING USER MODELSHow a user model is acquired is central to the wholeenterprise of building user models.
A user model is notuseful unless it can support the needs of the largersystem that uses it.
The ability of a user model tosupport requests to it depends crucially on the rele-vance, accuracy, and amount of knowledge the usermodel has.
This in turn depends on the acquisition ofsuch knowledge for the user model.
In this section twomethods of user model acquisition are discussed, andtechniques that have been used to acquire various typesof knowledge about the user, particularly the user'sgoals, plans, and beliefs, will be described.4.1 METHOD OF ACQUISITIONThe knowledge that a user model contains can beacquired in two ways: explicitly or implicitly.
Explicitlyacquired knowledge is knowledge that is obtained whenan individual provides pecific facts to the user model.Explicit knowledge acquisition most often occurs withknowledge acquired for generic user models or forstereotypes.
In these cases the user model is usuallyhand built by the system implementor according to theexpectations the designers have for the class or classesof users of the system.Knowledge can also be acquired explicitly from theuser.
For example, when a user accesses the system forthe first time, the system may begin by asking the usera series of questions that will give the system anadequate amount of information about the new user.This is how GRUNDY acquires most of its individual-ized information about he user.
When a person uses thesystem for the first time GRUNDY asks for a list ofwords describing the user.
From this list GRUNDYmakes judgments about which stereotypes most accu-rately fit the user (the stereotypes had been hand codedby the system designer) and thus forms an opinion aboutthe preferences of the user based on this initial list ofattributes.Acquiring knowledge about the user implicitly isusually more difficult than acquiring it explicitly.
Im-plicit user model acquisition means that the user modelis built by observing the behavior of the user andinferring facts about he user from the observed behav-ior.
For a natural language system this means that theuser modeller must be able to "eavesdrop" on thesystem-user interaction and make its judgments basedon the conversation between the two.4.2 TECHNIQUES FOR ACQUIRING USER MODELSIn this section techniques that have been used toacquire information for a user model are presented,focu,dng primarily on how to acquire knowledge aboutuser goals, plans, and beliefs, since these areas havereceived the most attention to date.GOALSAt any given time, a computer system user will usuallyhave several goals that he is trying to accomplish.
Someof these goals may be assumed to apply to all users ofthe system.
For example, a database query system canassume at the very least that the user has the goal ofobtaining information from the system.
These generalgoals may either be encoded explicitly in a generic usermodel, or may be omitted altogether, being assumed inthe design of the system itself.A user modeling system will also need to modeluser's immediate goals.
Sometimes the goals are explic-itly stated by the user.
For example:" I  want to get to the airport, when does the next traindepart?
"Often they are not.
Frequently people do not explicitlystate their goal, but expect he hearer to infer that goalfrom the utterance.
Thus a speaker who says,"When does the next train to the airport depart?
"probably has the same goal as the speaker of the firstsentence, but the hearer must reason from the statementto determine that goal.
This sort of goal inference fromindirect questions was part of the work done by Allenand Perault (1980).PLANSAs goals become more complex, the task of inferring auser's goals becomes mixed with the task of inferringthe plans held by the user.
Much work has been done inrecognizing plans held by users.
Kautz and Allen (1986)have categorized past approaches to plan inference asusing either the explanation-based approach, the pars-ing approach, or the likely inference approach.In the explanation approach, the system attempts tocome up with a set of assumptions that will explain thebehavior of the user.
The TRACK system (Carberry1983, and this issue) uses such an approach.
In thecontext of a system to advise students about collegecourses, a user might ask,"Is Professor Smith teaching Expert Systems nextsemester?
"TRACK will recognize three possible plans the usermight have that would explain this statement.1.
The student may want to take Expert Systems,taught by Professor Smith.14 Computational Linguistics, Volume 14, Number 3, September 1988Kass and Finin Modeling the User in Natural Language Systems2.
The student may want to take Expert Systems,regardless of the professor.3.
The student may want to take a course taught byProfessor Smith.TRACK maintains a tree of the possible plans the usermay have and refines its judgment as more informationbecomes available.The plan parsing approach was first used by Gene-sereth for the MACSYMA Advisor (Genesereth 1979,1982).
Available to the MACSYMA Advisor is a recordof the past interaction of the user with the symbolicmathematics system MACSYMA.
When the user en-counters a problem and asks the Advisor for help, theMACSYMA Advisor is able to parse the past interac-tion of the user with the system to come up with the planthe user is pursuing.
Such an approach depends on theavailability of a great deal of information about he plansteps executed by the user.
Plan parsing has not beenused for user modeling in natural language systemsbecause of the difficulty in getting such informationfrom a solely natural language interaction.The likely inference approach relies on heuristics toreduce the space of possible plans that a system mightattribute to the user.
This approach is used by Pollack(Pollack 1985, Pollack 1986) to infer the plans of userswho present inappropriate queries to the system.
Pol-lack reasons that the inappropriate query by the userwas an attempt to achieve some subgoal in the user'slarger plan.
Since this subgoal has failed, Pollack'ssystem tries to identify what the overall goal is, andsuggest an action that will salvage the user's plan.The plan inference approaches rely on two things toaccomplish their task.
First, all plan inference mecha-nisms must have a lot of knowledge about the domainand about he kinds of plans the user might have.
Manysystems implicitly assume that they know all possibleplans that may be used to achieve the goals recognizableby the system.
Some systems (such as the systemdescribed by Sidner and Israel (1981) and Shrager andFinin (1982) augment their domain knowledge with abad plan library--a collection of plans that will notachieve the goals they seek, but that are likely to beemployed by a user.BELIEFSAcquiring knowledge about user beliefs is a much moreopen-ended task than acquiring knowledge about goalsand plans.
Goals and plans have an inherent structurethat helps acquisition of such information.
Inferring theuser's plan reaps the side benefit of inferring not onlythe main goal of the user, but also a number of subgoalsfor the steps in the plan.
User plans tend to persistduring a conversation, so new plan inference does notneed to be going on continuously.
Beliefs of the user, onthe other hand, lack that unifying structure.
Inferringuser beliefs implicitly requires the user modeling systemto be constantly alert for clues it can use to makeinferences about user beliefs.Knowledge about user beliefs can be acquired inmany ways.
Sometimes users make explicit statementsabout what they do or don't know.
If the systempresumes that a user has accurate knowledge of his ownbeliefs and that the user is not lying (a reasonableassumption for the level of systems today), such explicitstatements can be used to directly update the usermodel.Even when users do not explicitly state their beliefs,statements they make may contain information that canbe used to infer user beliefs.
Kaplan (1982) points outthat user questions to a database system (as well asother systems) often depend on presuppositions held bythe user.
For example, the question"Who was the 39th president?presupposes that there was a 39th president.
A usermodeling system may thus add this belief to its model ofthe user.
When a presupposition is wrong (does notagree with the domain knowledge of the system), it maybe possible to infer more information about the beliefsof the user.
The incorrect presupposition may reflect anobject-related misconception, in which case a systemsuch as ROMPER (McCoy 1985, 1986) could detectwhether the misconception was due to a misclassifica-tion of the concept, or a misattribution.
Such a miscon-ception may indicate a misunderstanding about other,related terms as well.
6Other techniques can be used to infer beliefs of theuser based on the user's interaction with the system, butwith conclusions that are less certain.
These approachescan be classified as either primarily recognition orientedor primarily constructive.The recognition approaches use the statements madeby the user in an attempt o recognize pre-encodedinformation i  the user model that applies to the user.Stereotype modeling uses this approach: a stereotype isa way of making assumptions about an individual user'sbeliefs that cannot be directly inferred from interactionwith the system.
Thus if the user indicates knowledge ofa concept hat triggers a stereotype, the whole collec-tion of assumptions in the stereotype can be added tothe model of the individual user (Rich 1979, Morik andRollinger 1985, Chin 1988, Finin and Drager 1986).Stereotype modeling enables a robust model of anindividual user to be developed after only a short periodof interaction.Constructive modeling attempts to build up an indi-vidual user model primarily from the information pro-vided in the interaction between the user and thesystem.
For example, a user modeling system mightassume that the information provided by the system tothe user is believed by the user thereafter.
This assump-tion is reasonable, since if the user does not understandwhat the system says (or does not believe it), he is likelyto seek clarification (Rich 1983), in which case theComputational Linguistics, Volume 14, Number 3, September 1988 15Kass and Finin Modeling the User in Natural Language Systemserrant assumption will be quickly corrected.
Anotherapproach is based on Grice's Cooperative Principle(Grice 1975).
If the system assumes that the user isbehaving in a cooperative manner, it can draw infer-ences about what the user believes is relevant, andabout the user's knowledge or lack of knowledge.Perrault (1987) has recently proposed a theory of speechacts that implements Grice's Maxims as default rules(Reiter 1980).
Kass and Finin (Kass 1987a, Kass andFinin 1987c) have taken a related approach, suggestinga set of default rules for acquiring knowledge about theuser in cooperative advisory systems, based on assump-tions about the type of interaction and general featuresof human behavior.Another technique mixes the implicit and explicitmethods of acquiring knowledge about the user, byallowing the user modeling module to directly query theuser.
In human conversation this seems to happenfrequently: often a hearer will interrupt he speaker toclarify a statement the speaker has made, or to seekelaboration or justification for a statement.
In the envi-ronment of a natural anguage system one could envi-sion a user modeling module that occasionally proposesa question to the user that would help the user modelingmodule choose between two or more possible assump-tions about he user that are considered important to themain focus of the conversation.
7Finally, there is a close relationship between knowl-edge acquisition and knowledge representation.
Thevery nature of user modeling implies uncertainty of theknowledge acquired about he user.
Often a user modelmay make assumptions about the user that need to be?
retracted when more information is obtained.
In addi-tion, the subject being modeled is dynamic--as aninteraction progresses the user being modeled will learnnew information, alter plans, and change goals.
Theknowledge representation fora user model must be ableto accommodate his change in knowledge about theuser.
To cope with the non-monotonicity of the usermodel, the knowledge representation system used willneed to have some form of a truth maintenance system(Doyle 1979), or employ a form of evidential reasoning.5 DESIGN CONSIDERATIONS FOR USER MODELSIncorporating a user model into a natural languagesystem may provide great benefits, but it also has someassociated costs.
The type of information the model isexpected to maintain and how the model is used willaffect the overall cost for employing a user modelingsystem.
This section focuses primarily on how to weighthe benefits of employing a user model against he costof acquiring that model.
The benefit provided by a usermodel can be measured by comparing the performanceof the system with a user model to the performance ofthe system without the user model.
The cost of a usermodel may manifest itself in various ways.
On systemsthat must do a lot of implicit modeling, the cost mayappear as a great demand for computational resourcessuch as; processor time and memory space.
On systemsthat employ stereotypes or a generic user model, thecost may be in development time: the man hours spentby the system implementors encoding knowledge aboutthe user.
For some systems the cost of employing a usermodel may be very great, while the benefit is slight.Thus the issue of when user models should not be usedis important as well.Several characteristics of the underlying applicationdetermine the relative benefits and costs of using a usermodeling system.
These issues are: 8?
Who bears the burden of responsibility for communi-cation in the interaction??
What is the penalty for error??
How rich is the interaction space??
How adaptable must the system be, and how quicklymust it adapt??
What mode of interaction will be used by the system?The following subsections will discuss how each ofthese issues affects the costs and benefits of a usermodeling module, concluding with a summary of whattypes of systems may be expected to profitably employa user model.5.1 RESPONSIBILITYIn any dialog, one or more of the participants akes theresponsibility to ensure that the communication is suc-cessful.
In human dialogs this burden is usually sharedby all participants, but not always.
Tutors and advisorsoften assume most of the burden of responsibility forensuring that the student or advisee understands thematerial presented, and that questions from the studentor advisee are correctly handled by the tutor or advisor.Systems that make the appropriate query assumptionplace the communication responsibility primarily on theshoulders of the user.
Since the system assumes theuser atways provides appropriate queries, the usermodeling module has much less work to do.
The systemcan be content o answer the user's queries withouthaving to worry about the possibility of bad plans, orgoals that differ from those inferred directly from theuser's statement.
In the extreme, any failure in under-standing can be blamed on the user.
Thus the cost ofacquiring a user model is not high.
On the other hand, auser model may not provide much benefit since thesystem need not worry about user goals outside therange of those explicitly stated by the user.A system that bears the responsibility for communi-cation (thus not assuming the user makes appropriatequeries) has different user modeling requirements.
Suchsystems (for example, consultative expert systems likeMYCIN) need to know the knowledge of the user to aidin generating explanations and in posing questions tothe user.
Goal and plan recognition is not very impor-tant since these tend to be defined by the system itself.16 Computational Linguistics, Volume 14, Number 3, September 1988Kass and Finin Modeling the User in Natural Language SystemsA user model can be quite beneficial in improving theacceptability (and maybe the efficiency) of the system.On the other hand, implicit acquisition of knowledgeabout he user is difficult since the user participation isconstrained to responding to the system.
Thus the usermodel will probably need to be acquired explicitly,either through generic models and stereotypes, or byexplicit query of the user.Systems that share the burden of responsibility withthe user require the most complex user models.
Whenresponsibility is shared, the system must be able torecognize when the user wants to shift topics or alter thefocus of the interaction.
Thus the system will require avery rich representation f possible user goals and plansto be able to recognize when the user shifts away fromthe system's plan or goal.
A user model thus seemsessential to support such mixed initiative interactions.Although goal and plan inference will be more difficult,the user modeling module should have more opportu-nity to acquire information from the user in a free-flowing exchange.
Consequently the costs for acquiringknowledge about user beliefs may be less than in thetwo previous ituations.
Systems in which there is a realsharing of the responsibility are, for the most part, stilla research goal.
Reichman (1981) has analyzed this inthe context of human-human dialogs in some detail.Sergot (1983) has studied the architecture of interactivelogic programming systems where the initiative of ask-ing and answering queries can be mixed.
In the author'sown work, the assumption of a shared responsibilitybetween system and user has proven beneficial inacquiring knowledge about the user implicitly.5.2 PENALTY FOR ERRORHow will an error in the user model influence theperformance of the application system?
A high penaltyfor error means the user modeling module must limit theassumptions it makes about the user to those that arewell justified.
Use of stereotypes would be severelylimited and inferences that were less than certain wouldbe avoided.
As a consequence, the user model may beless helpful to the application system.
A high penalty forerror thus reduces the benefits that may be obtained byemploying a user modeling system.
A low penalty forerror, on the other hand, allows the user model to makeassumptions if it has some justification.
Mistakes will bemade, but overall the model should be very helpful tothe underlying system.Penalty for error is related to responsibility for com-munication.
A high penalty for error in the user modelcan only occur when the system assumes ome respon-sibility for the communication.
I  fact, systems that aresolely responsible for ensuring that communication suc-ceeds in an interaction will tend to have the highestpenalty for error.
In mixed initiative dialogs both userand system are free to interrupt he conversation tocorrect mistakes that may occur.
When the systemassumes ole responsibility, the user has no method tostop the system and try to correct a mistake that hasbeen made.
Thus the lack of flexibility in such systemsseverely impairs the benefits of a user model.5.3 RICHNESS OF INTERACTION SPACEThe range of interaction a system is expected to handlegreatly affects the user modeling requirements.
If thepossible user goals are very limited (such as meeting orboarding trains) or the domain is limited, a user modelneed not record much information about user.
Suchsituations do not require individual models of the user,and need only very simple acquisition techniques.
Ac-quisition of knowledge about he user might be a simplesearch to see which collection of information bestmatches the behavior of the user.When the range of interaction increases, more isrequired of the user model.
Inferring user plans is atypical example.
The number of possible plans a usermight have grows explosively as the complexity of thetask increases.
It is not possible to record all possibleplans and simply search for a match.
Instead, typical orlikely plans must be entered by the system designers, orcomplex inferencing techniques must be employed.The range of possible users also influences the degreeof specialization needed in the user model.
If the usersform a homogeneous class, a generic user model can bebuilt that encompasses much of the information that asystem might need to know about the user.
Thusknowledge acquisition costs are limited to the timerequired by the system designers to encode the genericmodel, with very little effort for implicit modeling.
Asthe range of possible users increases, o does the cost ofacquiring information about them.
On the other hand,user modeling is more important when the set of users isdiverse, so the system is able to tailor its interaction tofit the particular user.5.4 ADAPTABILITYAdaptability is closely related to the richness of theinteraction space and to the penalty for error.
Thegreater the range of possible users, the more the systemwill be required to adapt.
If the penalty for error is highas well, the acquisition abilities of the user model mustbe very good.
The more adaptable the system must be,the greater the learning ability of the user modelingmodule must be.Adaptability also concerns how quickly the system isrequired to adapt.
Some systems may deal with a widerange of users, but the user modeler has a relatively longtime to develop amodel of the individual.
Such systemshave a low penalty for error.
If the system must adaptvery quickly, stereotyping will be necessary, includingthe ability for the system to synthesize new, usefulstereotypes when it recognizes the need.
Such a usermodel will need to be concerned not only with modelingthe current user, but also potential future users.Computational Linguistics, Volume 14, Number 3, September 1988 17Kass and Finin Modeling the User in Natural Language SystemsDifficultySimpleQuestion Cooperative Non-cooperativeAnswering Consultation ConsultTtionlow I I highCooperative BiasedQuestion ConsultationAnswering Pretending ?ObjectivityFigure 3.
Relative Difficulty of Modeling the User inDifferent Types of Interaction.5.5 MODE OF INTERACTIONThe mode of interaction with the user will also influencethe relative cost and benefits of employing a user model.Wahlster and Kobsa (1988) present a scale of fourmodes of man-machine interaction that place increasingrequirements on the user modeling capabilities of asystem:?
Simple question answering or biased consultation?
Cooperative question answering?
Cooperative consultation?
Biased consultation pretending objectivityFigure 3 shows these four modes plus a final, verydifficult category:?
Non-cooperative interactionThe following paragraphs take a short look at the usermodeling requirements of each.No explicit user model is required for simple ques-tion answering systems uch as current database querysystems.
Such systems are not concerned with usergoals and plans, beyond the assumption that the user isseeking information.
A minimal user model might beemployed to model user knowledge of the domain itself.Biased consultation has similar requirements.
No mat-ter what the user says the consultant will make the samerecommendation.
The only aid a user model might be isin helping the system select information likely to swaythe user.Cooperative question answering requires the systemto have some idea of the goals of the user.
Typically therange of goals the system can be expected to recognizewill be quite limited, since the system is being usedprimarily as an information source.
The system mustalso be able to recognize when a response could lead toa user misconception.
Such systems typically can em-ploy a generic user model since there will be littledifferentiation among users from the standpoint of thequestion answering system.Cooperative consultation requires an extensive usermodel.
As noted in Pollack et al(1982), a consultationbetween an expert and the individual asking advice islike a negotiation.
A consultation system must be ableto recognize and understand a wide variety of usergoals, further compounded by the fact that they mayinvolve many misconceptions about facts in the domainof consultation.
A good consultant should even be ableto recognize analogies to other domains that the user ismaking (Schuster 1984, 1985).
Such consultations fre-quently involve extended interactions where much in-formation about he user can be collected.
In most casesthis information about he user should be retained, sinceit is likely further consultations will occur.
Thus usermodels for cooperative consultation eed to record alltypes of information about the user, and save thisinformation in long-term individual user models.A biased consultation i which the system pretendsobjectivity (such as an electronic salesman) requireseven more inferences about the user than cooperativeconsultation.
Biased consultation requires a deep modelof user attitudes, and how particular terms or conceptsaffect the attitude of the user.
The system must havegood models of what the user feels is cooperativeconversation (since the system must appear objective)and of the user's model of the system (since the systemmust ensure that the user feels the system is objective).Non-cooperative interaction makes the acquisition ofinformation about the user very difficult.
Even withcooperative interaction, much of the information as-sumed about the user is uncertain.
If the user is notcooperating with the system, the possibility of the userlying, or withholding the truth, further complicates theacquisition of knowledge about the user.
The systemmust be able to reason about he motivations of the userand be able to discern what information is likely to beuntrue, and what information should not be influencedby the non-cooperative goals or attitudes of the user.User models in such situations require very extensiveknowledge about people in general, and categories ofpeople in particular.5.6 SUMMARYGiven these criteria for judging the costs and benefits ofa user model, some conclusions can be drawn about hetypes of systems that can profitably employ a usermodel.
First, user models should only be used insituations where the range of interaction is sufficientlygreat that the user model can significantly affect theperformance of the system.
This does not preclude theiruse in more limited interactions, but the costs of imple-menting the user model can easily exceed the benefitsthat might be gained, particularly compared to otherinteraction techniques ( uch as menus) that are easier toimplement and quite effective when the range of inter-action is limited.The fact that the user model will be used to alter thebehavior of the system implies that the system willassume some degree of responsibility for ensuring thecommunication between user and system.
This meansthe mode of interaction should at least be cooperative.18 Computational Linguistics, Volume 14, Number 3, September 1988Kass and Finin Modeling the User in Natural Language SystemsGiven the range of interaction types presented in Figure3, cooperative question answering and cooperative con-sultation are appropriate types of interactions for usinga user model.
The more difficult forms of interaction,such as biased consultation pretending objectivity ornon-cooperative forms of interaction, are very difficultand at present have little practical use in the types ofapplications being built.Finally, user models are currently viable only insituations where there is a low penalty for error.
A highpenalty for error demands very robust user models,requiring either extensive xplicit coding of the usermodel, or sophisticated acquisition techniques.
Thehuman costs of coding a robust user model are veryhigh, while sophisticated acquisition techniques will notbe forthcoming soon.
Thus in applications where thepenalty for error is high, responsibility needs to remainon the shoulders of the user, with user modeling playingat most a secondary role.6 CONCLUSIONThe ability to interact with people in an easy and naturalmanner is the promise natural anguage interfaces holdfor computer systems.
To realize this promise, systemsneed to acquire and use various kinds of informationabout the people with whom they are interacting.
Thatis, they need models of their users.Sophisticated user models can serve many importantfunctions in natural language systems: they can be usedto tailor the interaction to an individual user, to increasethe system's cooperativeness, and to correct or evenprevent misconceptions by the user.
This paper hasmade several general points about the role of usermodels in question answering systems.?
What constitutes a user model is a matter of somedebate.
The view taken in this paper is that a usermodel is an explicit source of knowledge containingthe beliefs and assumptions the system holds aboutthe user.?
User models must hold many diverse types of infor-mation.
Natural language systems need to knowabout the user's goals and plans, capabilities, atti-tudes, and beliefs.?
User models can be classified along various dimen-sions.
In general terms, these dimensions character-ize the agents being modeled, how the model changeswith time, and how it is used.?
The acquisition of information about the user is acentral problem that must be faced.
The process canbe explicit, implicit, or a mixture of the two.
Thetechniques used for acquisition depend on the kind ofinformation.?
Environmental issues, such as how the model will beused, place added constraints on the type of usermodel that may be employed in a particular imple-mentation.To date, most of the work involving the kind of usermodels discussed in this paper is in an early researchstage.
This research typically focuses on just one aspectof the overall user modeling problem, such as planrecognition or modeling multiple agents.
There is still agreat deal of research to be done in these individualareas.
Goal recognition and modeling is central to manyAI problems and has not yet been adequately handled inany real systems.
Many of the ways that a user modelcan improve natural anguage interaction have not yetbeen explored.
In the context of generation systems, forexample, no existing systems use their knowledge of theuser as a factor in the lexical choice problem.Addressing individual problems in user modeling andlooking at particular applications where a user modelcan help have been appropriate research strategies inearly investigations.
Ultimately, however, user model-ing must be addressed from a more global point of view.A rich, interactive system will need to model manythings about many human agents.
This information canform a central knowledge base for reasoning aboutagents in many contexts.The notion of a central user modeling facility hasmotivated work on a general user modeling system orgeneral user modeling module (Finin and Drager 1986,Kass 1987a, Kass and Finin 1987c).
A general usermodeling system would provide an environment forbuilding systems that used a user model, includingvarious facilities for maintaining and updating usermodels.
A general user modeling module is an indepen-dent component of a larger system that provides infor-mation about the user to other modules, much like adata base or knowledge base.
The interface to thegeneral user modeling module is well-defined, enablingit to be used in a variety of systems with little or nocustomization.Future work in user modeling for natural anguagesystems should focus in two directions: establishinghow user models should be used in systems that com-municate in natural language, and determining how usermodels can be built more effectively.
Many authorshave emphasized the need for user models in certaincontexts, or have demonstrated that the availability ofuser model information can improve the behavior of asystem.
This work needs to be extended to identifywhat information applications will expect a user modelto have, how that information should be provided to theapplication, and when the information needs to beavailable.
Answers to these questions will help definethe services that a user modeling component mustprovide.The second focus of research should be on buildinguser models.
This work could progress in two ways.First, the task of explicitly building user models (such asbuilding stereotypes) could be made easier.
Research inthis area seems to parallel efforts to find better ways toacquire knowledge for knowledge bases from experts.However, if general user modeling modules that canComputational Linguistics, Volume 14, Number 3, September 1988 19Kass and Finin Modeling the User in Natural Language Systemsfunction in diverse systems are to be built, the focusmust be placed on the second approach: implicit usermodel acquisition.
In this regard, a user modelingmodule could be general either with respect to theunderlying domain or to the type of interaction.
At thistime, domain generality seems both a useful and prac-tical goal.
The work described in Kass (1987a) and Kassand Finin (1987c) is a beginning in this area, presentinga set of domain general user model acquisition rules forcooperative consultation situations.User modeling is not an easy task.
Effective usermodeling requires ophisticated knowledge representa-tion, acquisition, and reasoning abilities--no wonderuser modeling is such a new field.
On the other hand,advances in any of these areas should provide immedi-ate benefits to user modeling.
Thus progress in some ofthe fundamental reas of AI can result in progress inuser modeling as well.ACKNOWLEDGEMENTSThis work was partially supported by grant ARMY/DAAG-29-84-K-0061 from the Army Research Office, grant DARPA/ONR-N00014-85-K-0807 from DARPA, and a grant from the Digital EquipmentCorporation.REFERENCESAllen, James F. and Perrault, C. Raymond 1980 Analyzing Intentionin Utterances.
Artificial Intelligence 15: 143-178.Allen, James F.; Frisch, Alan M.; and Litman, Diane J.
1982 ARGOT:the Rochester Dialogue System.
In Proceedings of the 2nd Na-tional Conference on Artificial Intelligence: 66-70.Brown, J.S.
and Burton, R.R.
1978 Diagnostic Models for ProceduralBugs in Basic Mathematical Skills.
Cognitive Science 2: 155-192.Carberry, Sandra 1983 Tracking User Goals in an Information Seek-ing Environment.
In Proceedings of the 3rd National Conferenceon Artificial Intelligence: 59--63.Carberry, Sandra 1985 Pragmatic Modeling in Information SystemInterfaces.
Ph.D. thesis, Department of Computer and Informa-tion Science, University of Delaware, Newark, DE.Carberry, Sandra (this issue) Modeling the User's Plans and Goals.Carbonell, J.R. 1970 AI in CAI: An Artificial Intelligence Approach toComputer-Aided Instruction.
IEEE Transactions on Man-Ma-chine Systems 11: 190-202.Carbonell, Jaime G.; Boggs, W. Mark; Mauldin, Michael L.; andAnick, Peter G. 1983 The XCALIBUR Project: a Natural Lan-guage Interface to Expert Systems.
In 8th International Confer-ence on Artificial Intelligence: 653-656.Carr, Brian and Goldstein, Ira P. 1977 Overlays: A Theory of.Modeling for Computer-Aided Instruction.
Technical Report AIMemo 406, MIT Artificial Intelligence Laboratory, Cambridge,MA.Chin, David N. 1988 KNOME: Modeling What the User Knows inUC.
In Kobsa, Alfred and Wahlster, Wolfgang (eds.
), UserModels in Dialog Systems, Springer Verlag, Berlin--New York.Doyle, Jon 1979 A Truth Maintenance System.
Artificial Intelligence12(3): 231-272.Fagin, Ronald and Halpern, Joseph Y.
1985 Belief, Awareness andLimited Reasoning: Preliminary Report.
In 9th International Con-ference on Artificial Intelligence: 491-501.Finin, Tim 1982 Help and Advice in Task-Oriented Systems.
Techni-cal Report MS-CIS-82-22, Department of Computer and Informa-tion Science, University of Pennsylvania, Philadelphia, PA.Finin, Tim 1983 Providing Help and Advice in Task-Oriented Sys-tems.
In 8th International Conference on Artificial Intelligence:176-178.Finin, Tim and Drager, David 1986 GUMS j: a General User ModelingSystem.
In Proceedings of the 1986 Conference of the CanadianSociety for Computational Studies of Intelligence: 24-30.Finin, Tim; Joshi, Aravind; and Webber, Bonnie 1986 Natural Lan-guage Interactions with Artificial Experts.
Proceedings of theIEEE 74: 921-938.Genesereth, Michael 1979 The Role of Plans in Automated Consulta-tion.
In 6th International Conference on Artificial Intelligence:311-319.Genesereth, Michael R. 1982 The Role of Plans in Intelligent TeachingSystems.
In Sleeman, D. and Brown, J. S.
(eds.
), IntelligentTutoring Systems, 137-156, Academic Press, New York, NY.Gershman, A.
1981 Finding Out What the User Wants--Steps Towardan Automated Yellow Pages Assistant.
In 7th International Con-ference on Artificial Intelligence: 423-425.Goodman, Bradley A.
1985 Communication and Miscommunication.Technical Report 5681, Bolt, Beranek, and Newman.Goodman, Bradley A.
1986 Miscommunication a d Plan Recognition.Unpublished paper from UM86, the International Workshop onUser Modeling, Maria Laach, West Germany.Grice, H.P.
1975 Logic and Conversation.
In Cole, P. and Morgan,J.L.
(eds.
), Syntax and Semantics 3, Academic Press, New York,NY.Halasz, Frank G. and Moran, Thomas P. 1983 Mental Models andProblem Solving in Using a Calculator.
In Proceedings of theHuman Factors in Computer Systems Conference: 212-216.Halpern, Joseph Y. and Moses, Yoram 1985 A Guide to the ModalLogics of Knowledge and Belief: Preliminary Draft.
In 9th Inter-national Conference on Artificial Intelligence: 480-490.Hoeppner, Wolfgang; Christaller, Thomas; Marburger, Heinz; Morik,Katharina; Nebel, Bernhard; O'Leary, Mike; and Wahlster, Wolf-gang 1983 Beyond Domain Independence: Experience with theDevelopment of a German Language Access System to HighlyDiverse Background Systems.
In 8th International Conference onArtificial Intelligence: 588-594.Jameson, A.
1983 Impression Monitoring in Evaluation-OrientedDialog: The Role of the Listener's Assumed Expectations andValues in the Generation of Informative Statements.
In 8th Inter-national Conference on Artificial Intelligence: 616-620.Jameson, Anthony 1988 But What Will the Listener Think?
BeliefAscription and Image Maintenance in Dialog.
In Kobsa, Alfredand Wahlster, Wolfgang (eds.
), User Models in Dialog Systems,Springer Verlag, Berlin--New York.Johnson, W. Lewis and Soloway, Elliot 1984 Intention-Based Diag-nosis of Programming Errors.
In Proceedings of the 4th NationalConference on Artificial Intelligence: 162-168.Joshi, Aravind K. 1982 Mutual Beliefs in Question Answering Sys-tems.
In Smith, N.
(ed.
), Mutual Belief, Academic Press, NewYork.Joshi, A.; Webber, Bonnie; and Weischedel, Ralph 1984 Living Up toExpectations: Computing Expert Responses.
In Proceedings ofthe 4th National Conference on Artificial Intelligence.Kaplan, S.J.
1982 Cooperative Responses from a Portable NaturalLanguage Database Query System.
Artificial Intelligence 19(2):165-188.Kass, Robert 1987 Implicit Acquisition of User Models in CooperativeAdvisory Systems.
Technical Report MS-CIS-87-05, Departmentof Computer and Information Science, University of Pennsylva-nia, Philadelphia, PA.Kass, Robert 1987 The Role of User Modeling in Intelligent TutoringSystems.
In Kobsa, Alfred and Wahlster, Wolfgang (eds.
), UserModels in Dialog Systems.
Springer Verlag, Berlin--New York.
(An earlier version of this paper appeared as Technical ReportNumber MS-CIS-86-58, Department of Computer Science, Uni-versity of Pennsylvania, Philadelphia, PA.)20 Computational Linguistics, Volume 14, Number 3, September 1988Kass and Finin Modeling the User in Natural Language SystemsKass, Robert 1987 Rules for the Implicit Acquisition of KnowledgeAbout the User.
In Proceedings of the 6th National Conference onArtificial Intelligence.
(Also available as Technical Report NumberMS-CIS-87-10, Department of Computer Science, University ofPennsylvania, Philadelphia, PA.)Kautz, Henry A. and Allen, James F. 1986 Generalized Plan Recog-nition.
In Proceedings of the 5th National Conference on ArtificialIntelligence: 32-37.Kobsa, Alfred 1984 Three Steps in Constructing Mutual Belief Modelsfrom User Assertions.
In Proceedings of the 6th European Con-ference on Artificial Intelligence: 423--427.Kobsa, Alfred 1985 Using Situation Descriptions and RussellianAttitudes for Representing Beliefs and Wants.
In 9th InternationalConference on Artificial Intelligence: 513-515.Kobsa, Alfred 1988 A Taxonomy of Beliefs and Goals for UserModels in Dialog Systems.
In Kobsa, Alfred and Wahlster,Wolfgang (eds.
), User Models in Dialog Systems, Springer Verlag,Berlin--New York.Konolige, Kurt 1983 A Deductive Model of Belief.
In 8th Interna-tional Conference on Artificial Intelligence: 377-381.Litman, D. and Allen, J.
1984 A Plan Recognition Model for Clarifi-cation Subdialogs.
In Proceedings of the lOth International Con-ference on Computational Linguistics: 302-311.McCoy, Kathleen F. 1985 Correcting Object-Related Misconcep-tions.
Technical Report MS-CIS-85-57, Department of Computerand Information Science, University of Pennsylvania, Philadel-phia, PA.McCoy, Kathleen F. 1988 Highlighting User Model to Respond toMisconceptions.
In Kobsa; Alfred and Wahlster, Wolfgang (eds.
),User Models in Dialog Systems, Springer Verlag, Berlin--NewYork.McCoy, Kathleen F. (this issue) Reasoning on a Highlighted UserModel to Respond to Misconceptions.McKeown, Kathleen R. 1985 Discourse Strategies for GeneratingNatural-Language T xt.
Artificial Intelligence 27: 1--41.McKeown, Kathleen R. 1985 Tailoring Explanations for the User.
In9th International Conference on Artificial Intelligence: 794-798.Moore, Robert C. 1984 A Formal Theory of Knowledge and Action.In Moore, R.C.
and Hobbs, J.
(eds.
), Formal Theories of theCommonsense World, Ablex Publishing, Norwood, NJ; 31%358.Morik, Katharina 1986 User Modeling and Conversational Settings:Modeling the User's Wants.
In Kobsa, Alfred and Wahlster,Wolfgang (eds.
), User Models in Dialog Systems, Springer Verlag,Berlin--New York.Morik, Katharina and Rollinger, Claus-Rainer 1985 The Real-EstateAgent--Modeling Users by Uncertain Reasoning.
AI Magazine 6:44-52.Paris, Cecile L. (this issue) Tailoring Object Descriptions to theUser's Level of Expertise.
Linguistics Special Issue on UserModeling.Perrault, C. Raymond 1987 An Application of Default Logic to SpeechAct Theory.
Report No.
CSLI-87-90, Center for the Study ofLanguage and Information, Stanford, CA.Pollack, Martha E. 1985 Information Sought and Information Pro-vided: An Empirical Study of User/Expert Dialogues.
In Proceed-ings of the Human Factors in Computer Systems Conference: 155-159.Pollack, Martha E. 1986 Inferring Domain Plans in Question Answer-ing.
Ph.D. thesis, Department of Computer and InformationScience, University of Pennsylvania, Philadelphia, PA.Pollack, Martha E.; Hirschberg, Julia; and Webber, Bonnie 1982 UserParticipation in the Reasoning Processes of Expert Systems.
InProceedings of the 2nd National Conference on Artificial Intelli-gence: 358-361.
(A longer version of this paper appears asTechnical Report MS-CIS-82-9, Department of Computer andInformation Science, University of Pennsylvania, Philadelphia,PA.
)Reichman, Rachel 1981 Plain-Speaking: A Theory and Grammar ofSpontaneous Discourse.
Ph.D. thesis, Harvard University, Cam-bridge, MA.Reiter, Raymond 1980 A Logic for Default Reasoning.
ArtificialIntelligence 13(1): 81-132.Rich, Elaine 1979 User Modeling Via Stereotypes.
Cognitive Science3: 329-354.Rich, Elaine 1983 Users as Individuals: Individualizing User Models.International Journal of Man-Machine Studies 18: 19%214.Schuster, Ethel 1984 VPe : The Role of User Modeling in CorrectingErrors in Second Language Learning.
Technical Report MS-CIS-84-66, Department of Computer and Information Science, Univer-sity of Pennsylvania, Philadelphia, PA.Schuster, Ethel 1985 Grammars as User Models.
In 9th InternationalConference on Artificial Intelligence: 20-22.Sergot, M. 1983 A Query-the-User Facility of Logic Programming.
InDegano, P. and Sandewall, E.
(eds.
), Integrated Interactive Com-puting Systems, North-Holland.Shrager, J.
1981 Invoking a Beginner's Aid Processor by RecognizingJCL Goal.
Technical Report MS-CIS-81-07, Department ofCom-puter and Information Science, University of Pennsylvania, Phil-adelphia, PA.Shrager, J. and Finin, Tim 1982 An Expert System that VolunteersAdvice.
In Proceedings of the 2nd National Conference onArtificial Intelligence: 33%340.Sidner, Candace L. and Israel, David J.
1981 Recognizing IntendedMeaning and Speakers' Plans.
In 7th International Conference onArtificial Intelligence: 203-208.Sleeman, D. 1982 Assessing Aspects of Competence in Basic Algebra.In Sleeman, D. and Brown, J.S.
(eds.
), Intelligent TutoringSystems, Academic Press, New York, NY; 185-200.Sleeman, D. and Brown, J.S.
1982 Intelligent Tutoring Systems,Academic Press, New York, NY.Sleeman, D.; Appelt, Doug; Konolige, Kurt; Rich, Elaine; Sridharan,N.S.
; and Swartout, Bill 1985 User Modeling Panel.
In 9thInternational Conference on Artificial Intelligence: 1298-1302.Smith, Brian 1982 Reflection and Semantics in a Procedural Lan-guage.
Ph.D. thesis, MIT, Cambridge, MA.
(Also available asTechnical Report MIT/LCSfrR-272.
)Sparck Jones, Karen 1984 User Models and Expert Systems.
Techni-cal Report 61, Computer Laboratory, University of Cambridge,Cambridge, England.Swartout, William R. 1983 XPLAIN: A System for Creating andExplaining Expert Consulting Programs.
Artificial Intelligence 21:285-325.Wahlster, W. and Kobsa, Alfred (eds.)
1988 User Models in DialogSystems, Springer Verlag, Berlin--New York.Wallis, J. W. and Shortliffe, E.H. 1982 Explanatory Power forMedical Reasoning Expert Systems: Studies in the Representationof Causal Relationships for Clinical Consultations.
TechnicalReport STAN-CS-82-923, Department ofComputer Science, Stan-ford University, Stanford, CA.Waltz, D.L.
1978 An English Language Question Answering Systemfor a Large Relational Database.
Communications of the ACM21 (7):526--39.Webber, Bonnie Lynn 1986 Questions, Answers, and Responses:Interacting with Knowledge Base Systems.
In Brodie, M. andMylopolis, J.
(eds.
), On Knowledge Base Systems, SpringerVerlag, Berlin--New York.Webber, Bonnie Lynn and Finin, Tim 1984 In Response: Next Stepsin Natural Language Interaction.
In Reitman, W.
(ed.
), ArtificialIntelligence Applications for Business, Ablex Publishing Com-pany, Norwood, NJ.Wilensky, R.; Arens, Y.; and Chin, D. 1984 Talking to UNIX inEnglish: an Overview of UC.
Communications of the ACM 27:574-593.Wilks, Y. and Bien, J.
1983 Beliefs, Points of View, and MultipleEnvironments.
Cognitive Science 7:95-119.Computational Linguistics, Volume 14, Number 3, September 1988 21Kass and Finin Modeling the User in Natural Language SystemsNOTESAuthors' current addresses:Robert Kass, Center for Machine Intelligence, 2001 CommonwealthBlvd., Ann Arbor, MI 48105;Tim Finin, Unisys Paoli Research Center, P.O.
Box 517, Paoli, PA19301.1.
The use of such "bug libraries" has proven very successful instudent modeling for intelligent utoring systems.
(Brown andBurton 1978, Sleeman 1982, Johnson and Soloway 1984) areexamples of just a few intelligent tutoring systems that profitablyemploy this idea.2.
There is an unfortunate conflict in terminology here.
Sparck Jonesuses the term "agent" in the sense of an individual who performsa task for another.
Thus for Sparck-Jones the agent is the actualindividual interacting with the system.
Hence in our terminologythe system may have agent models for both Sparck-Jones's"agent" and "patient," with the model for the individual Sparck-Jones calls the "agent" actually being a user model.3.
Both the overlay and perturbation models were developed inwork on intelligent tutoring systems.
The overlay model was firstdefined by Carr and Goldstein (1977) and used in their WumpusAdvisor (WUSOR) user model, although Carbonell (1970) used anoverlay technique in the SCHOLAR program, considered to bethe first of the intelligent tutoring systems.
A perturbation modelwas used by Brown and Burton in representing bugs students hadin learning multicolumn subtraction (Brown and Burton 1978) andhas since been used by many others.
See Sleeman and Brown(1982) for a collection of seminal papers on intelligent tutoringsy,ltems, or Kass (1987b) for a look at user modeling for intelligenttutoring systems.4.
This is how acceptance attitudes were implemented in VIE-DPM.A wider range of values for the acceptance attitudes, such as afour-valued logic or numeric weights, could easily be used in-stead.5.
Although it is conceivable that each interaction with an individualuser might refine the generic model of all users in some way.
Thussuch a user model would converge on the "average user" aftermany sessions.6.
The terms used in a user's statements also provide informationabout beliefs of the user, but not as much as one might hope.
Atfirst glance, it seems that if the user makes use of a word, he hasknowledge about he concept to which that word refers.
Most ofthe time this is true.
However, people will sometimes use a termthat they really don't understand, simply because others haveused it.
Inferences based simply on the use of terms should bemade with care (or with a low level of trust).7.
A very clever system might even be able to incorporate questionsfrom the user modeling module into questions from the applica-tion in an attempt to meet wo needs simultaneously.8.
The first three issues are suggested by Sridharan in Sleeman et al(1985).22 Computational Linguistics, Volume 14, Number 3, September 1988j
