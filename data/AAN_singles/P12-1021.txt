Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 194?203,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsDiscriminative Pronunciation Modeling:A Large-Margin, Feature-Rich ApproachHao Tang, Joseph Keshet, and Karen LivescuToyota Technological Institute at ChicagoChicago, IL USA{haotang,jkeshet,klivescu}@ttic.eduAbstractWe address the problem of learning the map-ping between words and their possible pro-nunciations in terms of sub-word units.
Mostprevious approaches have involved genera-tive modeling of the distribution of pronuncia-tions, usually trained to maximize likelihood.We propose a discriminative, feature-rich ap-proach using large-margin learning.
This ap-proach allows us to optimize an objectiveclosely related to a discriminative task, toincorporate a large number of complex fea-tures, and still do inference efficiently.
Wetest the approach on the task of lexical access;that is, the prediction of a word given a pho-netic transcription.
In experiments on a sub-set of the Switchboard conversational speechcorpus, our models thus far improve classi-fication error rates from a previously pub-lished result of 29.1% to about 15%.
Wefind that large-margin approaches outperformconditional random field learning, and thatthe Passive-Aggressive algorithm for large-margin learning is faster to converge than thePegasos algorithm.1 IntroductionOne of the problems faced by automatic speechrecognition, especially of conversational speech, isthat of modeling the mapping between words andtheir possible pronunciations in terms of sub-wordunits such as phones.
While pronouncing dictionar-ies provide each word?s canonical pronunciation(s)in terms of phoneme strings, running speech of-ten includes pronunciations that differ greatly fromthe dictionary.
For example, some pronunciationsof ?probably?
in the Switchboard conversationalspeech database are [p r aa b iy], [p r aa l iy], [p ray], and [p ow ih] (Greenberg et al, 1996).
Whilesome words (e.g., common words) are more proneto such variation than others, the effect is extremelygeneral: In the phonetically transcribed portion ofSwitchboard, fewer than half of the word tokensare pronounced canonically (Fosler-Lussier, 1999).In addition, pronunciation variants sometimes in-clude sounds not present in the dictionary at all,such as nasalized vowels (?can?t?
?
[k ae n n t])or fricatives introduced due to incomplete consonantclosures (?legal?
?
[l iy g fr ix l]).1 This varia-tion makes pronunciation modeling one of the majorchallenges facing speech recognition (McAllaster etal., 1998; Jurafsky et al, 2001; Sarac?lar and Khu-danpur, 2004; Bourlard et al, 1999).
2Most efforts to address the problem have involvedeither learning alternative pronunciations and/ortheir probabilities (Holter and Svendsen, 1999) orusing phonetic transformation (substitution, inser-tion, and deletion) rules, which can come from lin-guistic knowledge or be learned from data (Rileyet al, 1999; Hazen et al, 2005; Hutchinson andDroppo, 2011).
These have produced some im-provements in recognition performance.
However,they also tend to cause additional confusability dueto the introduction of additional homonyms (Fosler-1We use the ARPAbet phonetic alphabet with additional di-acritics, such as [ n] for nasalization and [ fr] for frication.2This problem is separate from the grapheme-to-phonemeproblem, in which pronunciations are predicted from a word?sspelling; here, we assume the availability of a dictionary ofcanonical pronunciations as is usual in speech recognition.194Lussier et al, 2002).
Some other alternatives arearticulatory pronunciation models, in which wordsare represented as multiple parallel sequences of ar-ticulatory features rather than single sequences ofphones, and which outperform phone-based modelson some tasks (Livescu and Glass, 2004; Jyothi etal., 2011); and models for learning edit distances be-tween dictionary and actual pronunciations (Ristadand Yianilos, 1998; Filali and Bilmes, 2005).All of these approaches are generative?i.e., theyprovide distributions over possible pronunciationsgiven the canonical one(s)?and they are typicallytrained by maximizing the likelihood over train-ing data.
In some recent work, discriminative ap-proaches have been proposed, in which an objectivemore closely related to the task at hand is optimized.For example, (Vinyals et al, 2009; Korkmazskiyand Juang, 1997) optimize a minimum classificationerror (MCE) criterion to learn the weights (equiv-alently, probabilities) of alternative pronunciationsfor each word; (Schramm and Beyerlein, 2001) usea similar approach with discriminative model com-bination.
In this work, the weighted alternatives arethen used in a standard (generative) speech recog-nizer.
In other words, these approaches optimizegenerative models using discriminative criteria.We propose a general, flexible discriminative ap-proach to pronunciation modeling, rather than dis-criminatively optimizing a generative model.
Weformulate a linear model with a large numberof word-level and subword-level feature functions,whose weights are learned by optimizing a discrim-inative criterion.
The approach is related to the re-cently proposed segmental conditional random field(SCRF) approach to speech recognition (Zweig etal., 2011).
The main differences are that we opti-mize large-margin objective functions, which leadto sparser, faster, and better-performing models thanconditional random field optimization in our exper-iments; and we use a large set of different featurefunctions tailored to pronunciation modeling.In order to focus attention on the pronunciationmodel alone, our experiments focus on a task thatmeasures only the mapping between words and sub-word units.
Pronunciation models have in the pastbeen tested using a variety of measures.
For gener-ative models, phonetic error rate of generated pro-nunciations (Venkataramani and Byrne, 2001) andphone- or frame-level perplexity (Riley et al, 1999;Jyothi et al, 2011) are appropriate measures.
Forour discriminative models, we consider the taskof lexical access; that is, prediction of a singleword given its pronunciation in terms of sub-wordunits (Fissore et al, 1989; Jyothi et al, 2011).
Thistask is also sometimes referred to as ?pronunciationrecognition?
(Ristad and Yianilos, 1998) or ?pro-nunciation classification?
(Filali and Bilmes, 2005).
)As we show below, our approach outperforms bothtraditional phonetic rule-based models and the bestpreviously published results on our data set obtainedwith generative articulatory approaches.2 Problem settingWe define a pronunciation of a word as a representa-tion of the way it is produced by a speaker in termsof some set of linguistically meaningful sub-wordunits.
A pronunciation can be, for example, a se-quence of phones or multiple sequences of articu-latory features such as nasality, voicing, and tongueand lip positions.
For purposes of this paper, we willassume that a pronunciation is a single sequence ofunits, but the approach applies to other representa-tions.
We distinguish between two types of pronun-ciations of a word: (i) canonical pronunciations, theones typically found in the dictionary, and (ii) sur-face pronunciations, the ways a speaker may actu-ally produce the word.
In the task of lexical accesswe are given a surface pronunciation of a word, andour goal is to predict the word.Formally, we define a pronunciation as a sequenceof sub-word units p = (p1, p2, .
.
.
, pK), where pk ?P for all 1 ?
k ?
K and P is the set of all sub-wordunits.
The index k can represent either a fixed-lengthframe or a variable-length segment.
P?
denotes theset of all finite-length sequences over P .
We denotea word by w ?
V where V is the vocabulary.
Ourgoal is to find a function f : P?
?
V that takes asinput a surface pronunciation and returns the wordfrom the vocabulary that was spoken.In this paper we propose a discriminative super-vised learning approach for learning the function ffrom a training set of pairs (p, w).
We aim to find afunction f that performs well on the training set aswell as on unseen examples.
Let w?
= f(p) be thepredicted word given the pronunciation p. We assessthe quality of the function f by the zero-one loss: if195w 6= w?
then the error is one, otherwise the error iszero.
The goal of the learning process is to mini-mize the expected zero-one loss, where the expec-tation is taken with respect to a fixed but unknowndistribution over words and surface pronunciations.In the next section we present a learning algorithmthat aims to minimize the expected zero-one loss.3 AlgorithmSimilarly to previous work in structured prediction(Taskar et al, 2003; Tsochantaridis et al, 2005),we construct the function f from a predefined setof N feature functions, {?j}Nj=1, each of the form?j : P?
?V ?
R. Each feature function takes a sur-face pronunciation p and a proposed word w and re-turns a scalar which, intuitively, should be correlatedwith whether the pronunciation p corresponds to theword w. The feature functions map pronunciationsof different lengths along with a proposed word to avector of fixed dimension in RN .
For example, onefeature function might measure the Levenshtein dis-tance between the pronunciation p and the canonicalpronunciation of the word w. This feature functioncounts the minimum number of edit operations (in-sertions, deletions, and substitutions) that are neededto convert the surface pronunciation to the canonicalpronunciation; it is low if the surface pronunciationis close to the canonical one and high otherwise.The function f maximizes a score relating theword w to the pronunciation p. We restrict our-selves to scores that are linear in the feature func-tions, where each ?j is scaled by a weight ?j :N?j=1?j?j(p, w) = ?
?
?
(p, w),where we have used vector notation for the featurefunctions ?
= (?1, .
.
.
, ?N ) and for the weights?
= (?1, .
.
.
, ?N ).
Linearity is not a very strongrestriction, since the feature functions can be arbi-trarily non-linear.
The function f is defined as theword w that maximizes the score,f(p) = argmaxw?V?
?
?
(p, w).Our goal in learning ?
is to minimize the expectedzero-one loss:??
= argmin?E(p,w)??
[1w 6=f(p)],where 1pi is 1 if predicate pi holds and 0 other-wise, and where ?
is an (unknown) distribution fromwhich the examples in our training set are sampledi.i.d.
Let S = {(p1, w1), .
.
.
, (pm, wm)} be thetraining set.
Instead of working directly with thezero-one loss, which is non-smooth and non-convex,we use the surrogate hinge loss, which upper-boundsthe zero-one loss:L(?, pi, wi) = maxw?V[1wi 6=w?
?
?
?
(pi, wi) + ?
?
?
(pi, w)].
(1)Finding the weight vector ?
that minimizes the`2-regularized average of this loss function is thestructured support vector machine (SVM) problem(Taskar et al, 2003; Tsochantaridis et al, 2005):??
= argmin??2??
?2 +1mm?i=1L(?, pi, wi), (2)where ?
is a user-defined tuning parameter that bal-ances between regularization and loss minimization.In practice, we have found that solving thequadratic optimization problem given in Eq.
(2) con-verges very slowly using standard methods such asstochastic gradient descent (Shalev-Shwartz et al,2007).
We use a slightly different algorithm, thePassive-Aggressive (PA) algorithm (Crammer et al,2006), whose average loss is comparable to that ofthe structured SVM solution (Keshet et al, 2007).The Passive-Aggressive algorithm is an efficientonline algorithm that, under some conditions, canbe viewed as a dual-coordinate ascent minimizer ofEq.
(2) (The connection to dual-coordinate ascentcan be found in (Hsieh et al, 2008)).
The algorithmbegins by setting ?
= 0 and proceeds in rounds.In the t-th round the algorithm picks an example(pi, wi) from S at random uniformly without re-placement.
Denote by ?t?1 the value of the weightvector before the t-th round.
Let w?ti denote the pre-dicted word for the i-th example according to ?t?1:w?ti = argmaxw?V?t?1 ?
?
(pi, w) + 1wi 6=w.Let ?
?ti = ?
(pi, wi) ?
?
(pi, w?ti).
Then the algo-rithm updates the weight vector ?t as follows:?t = ?t?1 + ?ti?
?ti (3)196where?ti = min{1?m,1wi 6=w?ti?
?
???ti???ti?
}.In practice we iterate over the m examples in thetraining set several times; each such iteration is anepoch.
The final weight vector is set to the averageover all weight vectors during training.An alternative loss function that is often used tosolve structured prediction problems is the log-loss:L(?, pi, wi) = ?
logP?
(wi|pi) (4)where the probability is defined asP?
(wi|pi) =e???
(pi,wi)?w?V e???
(p,w).Minimization of Eq.
(2) under the log-loss results ina probabilistic model commonly known as a condi-tional random field (CRF) (Lafferty et al, 2001).
Bytaking the sub-gradient of Eq.
(4), we can obtain anupdate rule similar to the one shown in Eq.
(3).4 Feature functionsBefore defining the feature functions, we definesome notation.
Suppose p ?
P?
is a sequence ofsub-word units.
We use p1:n to denote the n-gramsubstring p1 .
.
.
pn.
The two substrings a and b aresaid to be equal if they have the same length andai = bi for 1 ?
i ?
n. For a given sub-word unit n-gram u ?
Pn, we use the shorthand u ?
p to meanthat we can find u in p; i.e., there exists an index isuch that pi:i+n = u.
We use |p| to denote the lengthof the sequence p.We assume we have a pronunciation dictionary,which is a set of words and their baseforms.
We ac-cess the dictionary through the function pron, whichtakes a word w ?
V and returns a set of baseforms.4.1 TF-IDF feature functionsTerm frequency (TF) and inverse document fre-quency (IDF) are measures that have been heavilyused in information retrieval to search for documentsusing word queries (Salton et al, 1975).
Similarly to(Zweig et al, 2010), we adapt TF and IDF by treat-ing a sequence of sub-word units as a ?document?and n-gram sub-sequences as ?words.?
In this anal-ogy, we use sub-sequences in surface pronunciationsto ?search?
for baseforms in the dictionary.
Thesefeatures measure the frequency of each n-gram inobserved pronunciations of a given word in the train-ing set, along with the discriminative power of the n-gram.
These features are therefore only meaningfulfor words actually observed in training.The term frequency of a sub-word unit n-gramu ?
Pn in a sequence p is the length-normalizedfrequency of the n-gram in the sequence:TFu(p) =1|p| ?
|u|+ 1|p|?|u|+1?i=11u=pi:i+|u|?1 .Next, define the set of words in the training set thatcontain the n-gram u as Vu = {w ?
V | (p, w) ?S, u ?
p}.
The inverse document frequency (IDF)of an n-gram u is defined asIDFu = log|V||Vu|.IDF represents the discriminative power of an n-gram: An n-gram that occurs in few words is betterat word discrimination than a very common n-gram.Finally, we define word-specific features using TFand IDF.
Suppose the vocabulary is indexed: V ={w1, .
.
.
, wn}.
Define ew as a binary vector withelements(ew)i = 1wi=w.We define the TF-IDF feature function of u as?u(p, w) = (TFu(p)?
IDFu)?
ew,where ?
: Ra?b ?
Rc?d ?
Rac?bd is the tensorproduct.
We therefore have as many TF-IDF featurefunctions as we have n-grams.
In practice, we onlyconsider n-grams of a certain order (e.g., bigrams).The following toy example demonstrates how theTF-IDF features are computed.
Suppose we haveV = {problem, probably}.
The dictionary maps?problem?
to /pcl p r aa bcl b l ax m/ and ?prob-ably?
to /pcl p r aa bcl b l iy/, and our input is(p, w) = ([p r aa b l iy], problem).
Then for the bi-gram /l iy/, we have TF/l iy/(p) = 1/5 (one out offive bigrams in p), and IDF/l iy/ = log(2/1) (oneword out of two in the dictionary).
The indicatorvector is eproblem =[1 0]>, so the final feature is?/l iy/(p, w) =[15 log210].1974.2 Length feature functionThe length feature functions measure how the lengthof a word?s surface form tends to deviate from thebaseform.
These functions are parameterized by aand b and are defined as?a?
?`<b(p, w) = 1a?
?`<b ?
ew,where ?` = |p| ?
|v|, for some baseform v ?pron(w).
The parameters a and b can be either posi-tive or negative, so the model can learn whether thesurface pronunciations of a word tend to be longeror shorter than the baseform.
Like the TF-IDF fea-tures, this feature is only meaningful for words ac-tually observed in training.As an example, suppose we have V ={problem, probably}, and the word ?probably?
hastwo baseforms, /pcl p r aa bcl b l iy/ (of lengtheight) and /pcl p r aa bcl b ax bcl b l iy/ (of lengtheleven).
If we are given an input (p, w) =([pcl p r aa bcl l ax m], probably), whose length ofthe surface form is eight, then the length features forthe ranges 0 ?
?` < 1 and ?3 ?
?` < ?2 are?0?
?`<1(p, w) =[0 1]>,??3?
?`<?2(p, w) =[0 1]>,respectively.
Other length features are all zero.4.3 Phonetic alignment feature functionsBeyond the length, we also measure specific pho-netic deviations from the dictionary.
We define pho-netic alignment features that count the (normalized)frequencies of phonetic insertions, phonetic dele-tions, and substitutions of one surface phone for an-other baseform phone.
Given (p, w), we use dy-namic programming to align the surface form p withall of the baseforms of w. Following (Riley et al,1999), we encode a phoneme/phone with a 4-tuple:consonant manner, consonant place, vowel manner,and vowel place.
Let the dash symbol ???
be agap in the alignment (corresponding to an inser-tion/deletion).
Given p, q ?
P ?
{?
}, we say thata pair (p, q) is a deletion if p ?
P and q = ?, isan insertion if p = ?
and q ?
P , and is a substi-tution if both p, q ?
P .
Given p, q ?
P ?
{?
}, let(s1, s2, s3, s4) and (t1, t2, t3, t4) be the correspond-ing 4-tuple encoding of p and q, respectively.
Thepcl p r aa pcl p er l iypcl p r aa bcl b ?
l iypcl p r aa pcl p er ?
?
l iypcl p r aa bcl b ax bcl b l iyTable 1: Possible alignments of [p r aa pcl p er l iy] withtwo baseforms of ?probably?
in the dictionary.similarity between p and q is defined ass(p, q) ={1, if p = ?
or q = ?
;?4i=1 1si=ti , otherwise.Consider aligning p with the Kw = |pron(w)|baseforms of w. Define the length of the align-ment with the k-th baseform as Lk, for 1 ?
k ?Kw.
The resulting alignment is a sequence of pairs(ak,1, bk,1), .
.
.
, (ak,Lk , bk,Lk), where ak,i, bk,i ?P ?
{?}
for 1 ?
i ?
Lk.
Now we define the align-ment features, given p, q ?
P ?
{?
}, as?p?q(p, w) =1ZpKw?k=1Lk?i=11ak,i=p, bk,i=q,where the normalization term isZp ={?Kwk=1?Lki=1 1ak,i=p, if p ?
P ;|p| ?Kw if p = ?.The normalization for insertions differs from thenormalization for substitutions and deletions, so thatthe resulting values always lie between zero and one.As an example, consider the input pair (p, w) =([p r aa pcl p er l iy], probably) and suppose thereare two baseforms of the word ?probably?
in thedictionary.
Let one possible alignments be the oneshown in Table 1.
Since /p/ occurs four times in thealignments and two of them are aligned to [b], thefeature for p?
b is then ?p?b(p, w) = 2/4.Unlike the TF-IDF feature functions and thelength feature functions, the alignment feature func-tions can assign a non-zero score to words that arenot seen at training time (but are in the dictionary),as long as there is a good alignment with their base-forms.
The weights given to the alignment fea-tures are the analogue of substitution, insertion, anddeletion rule probabilities in traditional phone-basedpronunciation models such as (Riley et al, 1999);they can also be seen as a generalized version of theLevenshtein features of (Zweig et al, 2011).1984.4 Dictionary feature functionThe dictionary feature is an indicator of whethera pronunciation is an exact match to a baseform,which also generalizes to words unseen in training.We define the dictionary feature as?dict(p, w) = 1p?pron(w).For example, assume there is a baseform/pcl p r aa bcl b l iy/ for the word ?probably?
inthe dictionary, and p = /pcl p r aa bcl b l iy/.
Then?dict(p, probably) = 1, while ?dict(p, problem) = 0.4.5 Articulatory feature functionsArticulatory models represented as dynamicBayesian networks (DBNs) have been successfulin the past on the lexical access task (Livescuand Glass, 2004; Jyothi et al, 2011).
In suchmodels, pronunciation variation is seen as theresult of asynchrony between the articulators (lips,tongue, etc.)
and deviations from the intendedarticulatory positions.
Given a sequence p and aword w, we use the DBN to produce an alignmentat the articulatory level, which is a sequence of7-tuples, representing the articulatory variables3 lipopening, tongue tip location and opening, tonguebody location and opening, velum opening, andglottis opening.
We extract three kinds of featuresfrom the output?substitutions, asynchrony, andlog-likelihood.The substitution features are similar to the pho-netic alignment features in Section 4.3, except thatthe alignment is not a sequence of pairs but a se-quence of 14-tuples (7 for the baseform and 7 for thesurface form).
The DBN model is based on articu-latory phonology (Browman and Goldstein, 1992),in which there are no insertions and deletions, onlysubstitutions (apparent insertions and deletions areaccounted for by articulatory asynchrony).
For-mally, consider the seven sets of articulatory vari-able values F1, .
.
.
, F7.
For example, F1 could beall of the values of lip opening, F1 ={closed, crit-ical, narrow, wide}.
Let F = {F1, .
.
.
, F7}.
Con-sider an articulatory variable F ?
F .
Suppose thealignment for F is (a1, b1), .
.
.
, (aL, bL), where L3We use the term ?articulatory variable?
for the ?articulatoryfeatures?
of (Livescu and Glass, 2004; Jyothi et al, 2011), inorder to avoid confusion with our feature functions.is the length of the alignment and ai, bi ?
F , for1 ?
i ?
L. Here the ai are the intended articulatoryvariable values according to the baseform, and thebi are the corresponding realized values.
For eacha, b ?
F we define a substitution feature function:?a?b(p, w) =1LL?i=11ai=a, bi=b.The asynchrony features are also extracted fromthe DBN alignments.
Articulators are not alwayssynchronized, which is one cause of pronunciationvariation.
We measure this by looking at the phonesthat two articulators are aiming to produce, and findthe time difference between them.
Formally, weconsider two articulatory variables Fh, Fk ?
F .Let the alignment between the two variables be(a1, b1), .
.
.
, (aL, bL), where now ai ?
Fh and bi ?Fk.
Each ai and bi can be mapped back to the cor-responding phone index th,i and tk,i, for 1 ?
i ?
L.The average degree of asynchrony is then defined asasync(Fh, Fk) =1LL?i=1(th,i ?
tk,i) .More generally, we compute the average asynchronybetween any two sets of variables F1,F2 ?
F asasync(F1,F2) =1LL?i=1?
?1|F1|?Fh?F1th,i ?1|F2|?Fk?F2tk,i??
.We then define the asynchrony features as?a?async(F1,F2)?b = 1a?async(F1,F2)?b.Finally, the log-likelihood feature is the DBNalignment score, shifted and scaled so that the valuelies between zero and one,?dbn-LL(p, w) =L(p, w)?
hc,where L is the log-likelihood function of the DBN,h is the shift, and c is the scale.Note that none of the DBN features are word-specific, so that they generalize to words in the dic-tionary that are unseen in the training set.5 ExperimentsAll experiments are conducted on a subset of theSwitchboard conversational speech corpus that has199been labeled at a fine phonetic level (Greenberg etal., 1996); these phonetic transcriptions are the inputto our lexical access models.
The data subset, phoneset P , and dictionary are the same as ones previ-ously used in (Livescu and Glass, 2004; Jyothi et al,2011).
The dictionary contains 3328 words, consist-ing of the 5000 most frequent words in Switchboard,excluding ones with fewer than four phones in theirbaseforms.
The baseforms use a similar, slightlysmaller phone set (lacking, e.g., nasalization).
Wemeasure performance by error rate (ER), the propor-tion of test examples predicted incorrectly.The TF-IDF features used in the experimentsare based on phone bigrams.
For all of the ar-ticulatory DBN features, we use the DBN from(Livescu, 2005) (the one in (Jyothi et al, 2011)is more sophisticated and may be used in fu-ture work).
For the asynchrony features, the ar-ticulatory pairs are (F1,F2) ?
{({tongue tip},{tongue body}), ({lip opening}, {tongue tip,tongue body}), and ({lip opening, tongue tip,tongue body}, {glottis, velum})}, as in (Livescu,2005).
The parameters (a, b) of the length andasynchrony features are drawn from (a, b) ?
{(?3,?2), (?2,?1), .
.
.
(2, 3)}.We compare the CRF4, Passive-Aggressive (PA),and Pegasos learning algorithms.
The regularizationparameter ?
is tuned on the development set.
We runall three algorithms for multiple epochs and pick thebest epoch based on development set performance.For the first set of experiments, we use the samedivision of the corpus as in (Livescu and Glass,2004; Jyothi et al, 2011) into a 2492-word train-ing set, a 165-word development set, and a 236-word test set.
To give a sense of the difficulty ofthe task, we test two simple baselines.
One is a lex-icon lookup: If the surface form is found in the dic-tionary, predict the corresponding word; otherwise,guess randomly.
For a second baseline, we calcu-late the Levenshtein (0-1 edit) distance between theinput pronunciation and each dictionary baseform,and predict the word corresponding to the baseformclosest to the input.
The results are shown in the firsttwo rows of Table 2.
We can see that, by adding justthe Levenshtein distance, the error rate drops signif-4We use the term ?CRF?
since the learning algorithm corre-sponds to CRF learning, although the task is multiclass classifi-cation rather than a sequence or structure prediction task.Model ERlexicon lookup (from (Livescu, 2005)) 59.3%lexicon + Levenshtein distance 41.8%(Jyothi et al, 2011) 29.1%CRF/DP+ 21.5%PA/DP+ 15.2%Pegasos/DP+ 14.8%PA/ALL 15.2%Table 2: Lexical access error rates (ER) on the same datasplit as in (Livescu and Glass, 2004; Jyothi et al, 2011).Models labeled X/Y use learning algorithm X and featureset Y.
The feature set DP+ contains TF-IDF, DP align-ment, dictionary, and length features.
The set ALL con-tains DP+ and the articulatory DBN features.
The bestresults are in bold; the differences among them are in-significant (according to McNemar?s test with p = .05).icantly.
However, both baselines do quite poorly.Table 2 shows the best previous result on this dataset from the articulatory model of Jyothi et al, whichgreatly improves over our baselines as well as overa much more complex phone-based model (Jyothiet al, 2011).
The remaining rows of Table 2 giveresults with our feature functions and various learn-ing algorithms.
The best result for PA/DP+ (the PAalgorithm using all features besides the DBN fea-tures) on the development set is with ?
= 100 and 5epochs.
Tested on the test set, this model improvesover (Jyothi et al, 2011) by 13.9% absolute (47.8%relative).
The best result for Pegasos with the samefeatures on the development set is with ?
= 0.01 and10 epochs.
On the test set, this model gives a 14.3%absolute improvement (49.1% relative).
CRF learn-ing with the same features performs about 6% worsethan the corresponding PA and Pegasos models.The single-threaded running time for PA/DP+ andPegasos/DP+ is about 40 minutes per epoch, mea-sured on a dual-core AMD 2.4GHz CPU with 8GBof memory; for CRF, it takes about 100 minutes foreach epoch, which is almost entirely because theweight vector ?
is less sparse with CRF learning.In the PA and Pegasos algorithms, we only update ?for the most confusable word, while in CRF learn-ing, we sum over all words.
In our case, the numberof non-zero entries in ?
for PA and Pegasos is around800,000; for CRF, it is over 4,000,000.
Though PAand Pegasos take roughly the same amount of timeper epoch, Pegasos tends to require more epochs to200Figure 1: 5-fold cross validation (CV) results.
The lex-icon lookup baseline is labeled lex; lex + lev = lexi-con lookup with Levenshtein distance.
Each point cor-responds to the test set error rate for one of the 5 datasplits.
The horizontal red line marks the mean of the re-sults with means labeled, and the vertical red line indi-cates the mean plus and minus one standard deviation.achieve the same performance as PA.For the second experiment, we perform 5-foldcross-validation.
We combine the training, devel-opment, and test sets from the previous experiment,and divide the data into five folds.
We take threefolds for training, one fold for tuning ?
and the bestepoch, and the remaining fold for testing.
The re-sults on the test fold are shown in Figure 1, whichcompares the learning algorithms, and Figure 2,which compares feature sets.
Overall, the resultsare consistent with our first experiment.
The fea-ture selection experiments in Figure 2 shows thatthe TF-IDF features alone are quite weak, while thedynamic programming alignment features alone arequite good.
Combining the two gives close to ourbest result.
Although the marginal improvement getssmaller as we add more features, in general perfor-mance keeps improving the more features we add.6 DiscussionThe results in Section 5 are the best obtained thusfar on the lexical access task on this conversationaldata set.
Large-margin learning, using the Passive-Aggressive and Pegasos algorithms, has benefitsover CRF learning for our task: It produces sparsermodels, is faster, and produces better lexical accessresults.
In addition, the PA algorithm is faster thanPegasos on our task, as it requires fewer epochs.Our ultimate goal is to incorporate such modelsinto complete speech recognizers, that is to predictword sequences from acoustics.
This requires (1)Figure 2: Feature selection results for five-fold cross val-idation.
In the figure, phone bigram TF-IDF is labeledp2; phonetic alignment with dynamic programming is la-beled DP.
The dots and lines are as defined in Figure 1.extension of the model and learning algorithm toword sequences and (2) feature functions that re-late acoustic measurements to sub-word units.
Theextension to sequences can be done analogously tosegmental conditional random fields (SCRFs).
Themain difference between SCRFs and our approachwould be the large-margin learning, which can bestraightforwardly applied to sequences.
To incorpo-rate acoustics, we can use feature functions based onclassifiers of sub-word units, similarly to previouswork on CRF-based speech recognition (Gunawar-dana et al, 2005; Morris and Fosler-Lussier, 2008;Prabhavalkar et al, 2011).
Richer, longer-span (e.g.,word-level) feature functions are also possible.Thus far we have restricted the pronunciation-to-word score to linear combinations of feature func-tions.
This can be extended to non-linear combi-nations using a kernel.
This may be challenging ina high-dimensional feature space.
One possibilityis to approximate the kernels as in (Keshet et al,2011).
Additional extensions include new featurefunctions, such as context-sensitive alignment fea-tures, and joint inference and learning of the align-ment models embedded in the feature functions.AcknowledgmentsWe thank Raman Arora, Arild N?ss, and the anony-mous reviewers for helpful suggestions.
This re-search was supported in part by NSF grant IIS-0905633.
The opinions expressed in this work arethose of the authors and do not necessarily reflectthe views of the funding agency.201ReferencesH.
Bourlard, S. Furui, N. Morgan, and H. Strik.
1999.Special issue on modeling pronunciation variation forautomatic speech recognition.
Speech Communica-tion, 29(2-4).C.
P. Browman and L. Goldstein.
1992.
Articulatoryphonology: an overview.
Phonetica, 49(3-4).K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive aggressive al-gorithms.
Journal of Machine Learning Research, 7.K.
Filali and J. Bilmes.
2005.
A dynamic Bayesianframework to model context and memory in edit dis-tance learning: An application to pronunciation classi-fication.
In Proc.
Association for Computational Lin-guistics (ACL).L.
Fissore, P. Laface, G. Micca, and R. Pieraccini.
1989.Lexical access to large vocabularies for speech recog-nition.
IEEE Transactions on Acoustics, Speech, andSignal Processing, 37(8).E.
Fosler-Lussier, I. Amdal, and H.-K. J. Kuo.
2002.
Onthe road to improved lexical confusability metrics.
InISCA Tutorial and Research Workshop (ITRW) on Pro-nunciation Modeling and Lexicon Adaptation for Spo-ken Language Technology.J.
E. Fosler-Lussier.
1999.
Dynamic Pronunciation Mod-els for Automatic Speech Recognition.
Ph.D. thesis, U.C.
Berkeley.S.
Greenberg, J. Hollenback, and D. Ellis.
1996.
Insightsinto spoken language gleaned from phonetic transcrip-tion of the Switchboard corpus.
In Proc.
InternationalConference on Spoken Language Processing (ICSLP).A.
Gunawardana, M. Mahajan, A. Acero, and J. Platt.2005.
Hidden conditional random fields for phoneclassification.
In Proc.
Interspeech.T.
J. Hazen, I. L. Hetherington, H. Shu, and K. Livescu.2005.
Pronunciation modeling using a finite-statetransducer representation.
Speech Communication,46(2).T.
Holter and T. Svendsen.
1999.
Maximum likelihoodmodelling of pronunciation variation.
Speech Commu-nication.C.-J.
Hsieh, K.-W. Chang, C.-J.
Lin, S. S. Keerthi, andS.
Sundararajan.
2008.
A dual coordinate descentmethod for large-scale linear SVM.
In Proc.
Interna-tional Conference on Machine Learning (ICML).B.
Hutchinson and J. Droppo.
2011.
Learning non-parametric models of pronunciation.
In Proc.
Inter-national Conference on Acoustics, Speech, and SignalProcessing (ICASSP).D.
Jurafsky, W. Ward, Z. Jianping, K. Herold, Y. Xi-uyang, and Z. Sen. 2001.
What kind of pronunciationvariation is hard for triphones to model?
In Proc.
In-ternational Conference on Acoustics, Speech, and Sig-nal Processing (ICASSP).P.
Jyothi, K. Livescu, and E. Fosler-Lussier.
2011.
Lex-ical access experiments with context-dependent artic-ulatory feature-based models.
In Proc.
InternationalConference on Acoustics, Speech, and Signal Process-ing (ICASSP).J.
Keshet, S. Shalev-Shwartz, Y.
Singer, and D. Chazan.2007.
A large margin algorithm for speech and au-dio segmentation.
IEEE Transactions on Acoustics,Speech, and Language Processing, 15(8).J.
Keshet, D. McAllester, and T. Hazan.
2011.
PAC-Bayesian approach for minimization of phoneme errorrate.
In Proc.
International Conference on Acoustics,Speech, and Signal Processing (ICASSP).F.
Korkmazskiy and B.-H. Juang.
1997.
Discriminativetraining of the pronunciation networks.
In Proc.
IEEEWorkshop on Automatic Speech Recognition and Un-derstanding (ASRU).J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional Random Fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
Interna-tional Conference on Machine Learning (ICML).K.
Livescu and J.
Glass.
2004.
Feature-based pronun-ciation modeling with trainable asynchrony probabil-ities.
In Proc.
International Conference on SpokenLanguage Processing (ICSLP).K.
Livescu.
2005.
Feature-based Pronunciation Model-ing for Automatic Speech Recognition.
Ph.D. thesis,Massachusetts Institute of Technology.D.
McAllaster, L. Gillick, F. Scattone, and M. Newman.1998.
Fabricating conversational speech data withacoustic models : A program to examine model-datamismatch.
In Proc.
International Conference on Spo-ken Language Processing (ICSLP).J.
Morris and E. Fosler-Lussier.
2008.
Conditional ran-dom fields for integrating local discriminative classi-fiers.
IEEE Transactions on Acoustics, Speech, andLanguage Processing, 16(3).R.
Prabhavalkar, E. Fosler-Lussier, and K. Livescu.
2011.A factored conditional random field model for artic-ulatory feature forced transcription.
In Proc.
IEEEWorkshop on Automatic Speech Recognition and Un-derstanding (ASRU).M.
Riley, W. Byrne, M. Finke, S. Khudanpur, A. Ljolje,J.
McDonough, H. Nock, M. Saraclar, C. Wooters, andG.
Zavaliagkos.
1999.
Stochastic pronunciation mod-elling from hand-labelled phonetic corpora.
SpeechCommunication, 29(2-4).E.
S. Ristad and P. N. Yianilos.
1998.
Learning stringedit distance.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 20(2).G.
Salton, A. Wong, and C. S. Yang.
1975.
A vectorspace model for automatic indexing.
Commun.
ACM,18.202M.
Sarac?lar and S. Khudanpur.
2004.
Pronunciationchange in conversational speech and its implicationsfor automatic speech recognition.
Computer Speechand Language, 18(4).H.
Schramm and P. Beyerlein.
2001.
Towards discrimi-native lexicon optimization.
In Proc.
Eurospeech.S.
Shalev-Shwartz, Y.
Singer, and N. Srebro.
2007.
Pega-sos: Primal Estimated sub-GrAdient SOlver for SVM.In Proc.
International Conference on Machine Learn-ing (ICML).B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In Advances in Neural InformationProcessing Systems (NIPS) 17.I.
Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-tun.
2005.
Large margin methods for structured andinterdependent output variables.
Journal of MachineLearning Research, 6.V.
Venkataramani and W. Byrne.
2001.
MLLR adap-tation techniques for pronunciation modeling.
InProc.
IEEE Workshop on Automatic Speech Recogni-tion and Understanding (ASRU).O.
Vinyals, L. Deng, D. Yu, and A. Acero.
2009.
Dis-criminative pronunciation learning using phonetic de-coder and minimum-classification-error criterion.
InProc.
International Conference on Acoustics, Speech,and Signal Processing (ICASSP).G.
Zweig, P. Nguyen, and A. Acero.
2010.
Continuousspeech recognition with a TF-IDF acoustic model.
InProc.
Interspeech.G.
Zweig, P. Nguyen, D. Van Compernolle, K. De-muynck, L. Atlas, P. Clark, G. Sell, M. Wang, F. Sha,H.
Hermansky, D. Karakos, A. Jansen, S. Thomas,G.S.V.S.
Sivaram, S. Bowman, and J. Kao.
2011.Speech recognition with segmental conditional ran-dom fields: A summary of the JHU CLSP 2010 sum-mer workshop.
In Proc.
International Conference onAcoustics, Speech, and Signal Processing (ICASSP).203
