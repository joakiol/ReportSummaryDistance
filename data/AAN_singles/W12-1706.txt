In: R. Levy & D. Reitter (Eds.
), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 61?69,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsSequential vs. Hierarchical Syntactic Models of Human IncrementalSentence ProcessingVictoria Fossum and Roger LevyDepartment of LinguisticsUniversity of California, San Diego9500 Gilman Dr.La Jolla, CA 92093{vfossum,rlevy}@ucsd.eduAbstractExperimental evidence demonstrates that syn-tactic structure influences human online sen-tence processing behavior.
Despite this ev-idence, open questions remain: which typeof syntactic structure best explains observedbehavior?hierarchical or sequential, and lexi-calized or unlexicalized?
Recently, Frank andBod (2011) find that unlexicalized sequen-tial models predict reading times better thanunlexicalized hierarchical models, relative toa baseline prediction model that takes word-level factors into account.
They conclude thatthe human parser is insensitive to hierarchi-cal syntactic structure.
We investigate theseclaims and find a picture more complicatedthan the one they present.
First, we show thatincorporating additional lexical n-gram prob-abilities estimated from several different cor-pora into the baseline model of Frank and Bod(2011) eliminates all differences in accuracybetween those unlexicalized sequential and hi-erarchical models.
Second, we show that lexi-calizing the hierarchical models used in Frankand Bod (2011) significantly improves pre-diction accuracy relative to the unlexicalizedversions.
Third, we show that using state-of-the-art lexicalized hierarchical models fur-ther improves prediction accuracy.
Our resultsdemonstrate that the claim of Frank and Bod(2011) that sequential models predict readingtimes better than hierarchical models is pre-mature, and also that lexicalization matters forprediction accuracy.1 IntroductionVarious factors influence human reading times dur-ing online sentence processing, including word-levelfactors such as word length, unigram and bigramprobabilities, and position in the sentence.
Yet word-level factors cannot explain many observed process-ing phenomena; ample experimental evidence ex-ists for the influence of syntax on human behav-ior during online sentence processing, beyond whatcan be predicted using word-level factors alone.Examples include the English subject/object rela-tive clause asymmetry (Gibson et al, 2005; Kingand Just, 1991) and anti-locality effects in German(Konieczny, 2000; Konieczny and Do?ring, 2003),Hindi (Vasishth and Lewis, 2006), and Japanese(Nakatani and Gibson, 2008).
Levy (2008) showsthat these processing phenomena can be explainedby surprisal theory under a hierarchical probabilis-tic context-free grammar (PCFG).
Other evidenceof syntactic expectation in sentence processing in-cludes the facilitation of processing at ?or?
follow-ing ?either?
(Staub and Clifton, 2006); expectationsof heavy noun phrase shifts (Staub et al, 2006); el-lipsis processing (Lau et al, 2006); and syntacticpriming (Sturt et al, 2010).Experimental evidence for the influence of syn-tax on human behavior is not limited to experimentscarefully designed to isolate a particular processingphenomenon.
Several broad-coverage experimentalstudies have shown that surprisal under hierarchi-cal syntactic models predicts human processing dif-ficulty on large corpora of naturally occurring text,even after word-level factors have been taken into61account (Boston et al, 2008; Demberg and Keller,2008; Roark et al, 2009).Despite this evidence, in recent work Frank andBod (2011) challenge the notion that hierarchicalsyntactic structure is strictly necessary to predictreading times.
They compare per-word surprisalpredictions from unlexicalized hierarchical and se-quential models of syntactic structure along twoaxes: linguistic accuracy (how well the model pre-dicts the test corpus) and psychological accuracy(how well the model predicts observed reading timeson the test corpus).
They find that, while hierar-chical phrase-structure grammars (PSG?s) achievebetter linguistic accuracy, sequential echo state net-works (ESN?s) achieve better psychological accu-racy on the English Dundee corpus (Kennedy andPynte, 2005).
Frank and Bod (2011) do not in-clude lexicalized syntactic models in the compar-ison on the grounds that, once word-level factorshave been included as control predictors in the read-ing times model, lexicalized syntactic models do notpredict reading times better than unlexicalized syn-tactic models (Demberg and Keller, 2008).
Based onthe results of their comparisons between unlexical-ized models, they conclude that the human parser isinsensitive to hierarchical syntactic structure.In light of the existing evidence that hierarchicalsyntax influences human sentence processing, theclaim of Frank and Bod (2011) is surprising.
In thiswork, we investigate this claim, and find a picturemore complicated than the one they present.
Wefirst replicate the results of Frank and Bod (2011)using the dataset provided by the authors, verifyingthat we obtain the same linguistic and psychologi-cal accuracies reported by the authors.
We then ex-tend their work in several ways.
First, we repeattheir comparisons using additional, more robustlyestimated lexical n-gram probabilities as control pre-dictors in the baseline model.1 We show that whenthese additional lexical n-gram probabilities are usedas control predictors, any differences in psycholog-ical accuracy between the hierarchical and sequen-tial models used in Frank and Bod (2011) vanish.Second, while they restrict their comparisons to un-1By robustly estimated, we mean that these probabilitiesare estimated from larger corpora and use a better smoothingmethod (Kneser-Ney) than the lexical n-grams of Frank andBod (2011).lexicalized models over part-of-speech (POS) tags,we investigate the lexicalized versions of each hi-erarchical model, and show that lexicalization sig-nificantly improves psychological accuracy.
Third,while they explore only a subset of the PSG?s im-plemented under the incremental parser of Roark(2001), we explore a state-of-the-art lexicalized hi-erarchical model that conditions on richer contexts,and show that this model performs still better.
Ourfindings demonstrate that Frank and Bod (2011)?sstrong claim that sequential models predict readingtimes better than hierarchical models is premature,and also that lexicalization improves the psycholog-ical accuracy of hierarchical models.2 Related WorkSeveral broad-coverage experimental studiesdemonstrate that surprisal under a hierarchical syn-tactic model predicts human processing difficultyon a corpus of naturally occurring text, even afterword-level factors have been taken into account.Under surprisal theory (Hale, 2001; Levy, 2008),processing difficulty at word wi is proportional toreading time at wi, which in turn is proportional tothe surprisal of wi in the context in which it is ob-served: surprisal(wi) = ?log(pr(wi|context)).Typically, context ?
w1...wi?1.
Comput-ing surprisal(wi) thus reduces to computing?log(pr(wi|w1...wi?
1)).
Henceforth, we referto this original formulation of surprisal as totalsurprisal.Boston et al (2008) show that surprisal estimatesfrom a lexicalized dependency parser (Nivre, 2006)and an unlexicalized PCFG are significant predic-tors of reading times on the German Potsdam Cor-pus.
Demberg and Keller (2008) propose to isolatesyntactic surprisal from total surprisal by replacingeach word with its POS tag, then calculating sur-prisal as usual under the incremental probabilisticphrase-structure parser of Roark (2001).
(FollowingRoark et al (2009), we hereafter refer to this type ofsurprisal as POS surprisal.)
They find that only POSsurprisal, not total surprisal, is a significant predictorof reading time predictions on the English Dundeecorpus.Demberg and Keller (2008)?s definition of POSsurprisal introduces two constraints.
First, by omit-62ting lexical information from the conditioning con-text, they ignore differences among words within asyntactic category that can influence syntactic ex-pectations about upcoming material.
Second, by re-placing words with their most likely POS tags, theytreat POS tags as veridical, observed input ratherthan marginalizing over all possible latent POS tagsequences consistent with the observed words.Roark et al (2009) propose a more principled wayof decomposing total surprisal into its syntactic andlexical components, defining the syntactic surprisalof wi as:?log?D:yield(D)=w1...wi pr(D minus last step)?D:yield(D)=w1...wi?1 pr(D)and the lexical surprisal of wi as:?log?D:yield(D)=w1...wi pr(D)?D:yield(D)=w1...wi pr(D minus last step)where D is the set of derivations in the parser?sbeam at any given point; D : yield(D) = w1...wiis the set of all derivations in D consistent withw1...wi; and D minus last step includes all stepsin the derivation except for the last step, in which wiis generated by conditioning upon all previous stepsof D (including ti).Roark et al (2009) show that syntactic surprisalproduces more accurate reading time predictions onan English corpus than POS surprisal, and that de-composing total surprisal into its syntactic and lex-ical components produces more accurate readingtime predictions than total surprisal taken as a singlequantity.
In this work, we compare not only differ-ent types of syntactic models, but also different mea-sures of surprisal under each of those models (total,POS, syntactic-only, and lexical-only).3 ModelsEstimating surprisal(wi) amounts to calculating?log(pr(wi|w1...wi?1)).
Language models differin the way they estimate the conditional proba-bility of the event wi given the observed contextw1...wi?1.
In the traditional formulation of surprisalunder a hierarchical model, the event wi is condi-tioned not only on the observed context w1...wi?1but also on the latent context consisting of the syn-tactic trees T whose yield is w1...wi?1; computingpr(wi|w1...wi?1) therefore requires marginalizingover all possible latent contexts T .
In this formu-lation of surprisal, the context includes lexical infor-mation (w1...wi?1) as well as syntactic information(T : yield(T ) = w1...wi?1), and the predicted eventitself (wi) contains lexical information.Other formulations of surprisal are also possible,in which the event, observed context, and latent con-text are otherwise defined.
In this work, we classifysyntactic models as follows: lexicalized models in-clude lexical information in the context, in the pre-dicted event, or both; unlexicalized models includelexical information neither in the context nor in thepredicted event; hierarchical models induce a latentcontext of trees compatible with the input; sequen-tial models either induce no latent context at all,or induce a latent sequence of POS tags compati-ble with the input.
Table 1 summarizes the syntacticmodels and various formulations of surprisal used inthis work.Following Frank and Bod (2011), we consider onetype of hierarchical model (PSG?s) and two types ofsequential models (Markov models and ESN?s).3.1 Phrase-Structure GrammarsPSG?s consists of rules expanding a parent node intochildren nodes in the syntactic tree, with associ-ated probabilities.
Frank and Bod (2011) use PSG?sthat generate POS tag sequences, not words.
Undersuch grammars, the prefix probability of a tag se-quence t is the sum of the probabilities of all treesT : yield(T ) = t1...ti, where the probability ofeach tree T is the product of the probabilities of therules used in the derivation of T .Vanilla PCFG?s, a special case of PSG?s in whichthe probability of a rule depends only on the identityof the parent node, achieve sub-optimal parsing ac-curacy relative to grammars in which the probabilityof each rule depends on a richer context (Charniak,1996; Johnson, 1998; Klein and Manning, 2003).To this end, Frank and Bod (2011) explore severalvariants of PSG?s conditioned on successively richercontexts, including ancestor models (which condi-tion rule expansions on ancestor nodes from 1-4levels up in the tree) and ancestor+sibling models(which condition rule expansions on the ancestor?sleft sibling as well).
Both sets of grammars also con-63Authors Model Surprisal Observed Latent PredictedContext Context EventBoston et al (2008) Hier.
POS ti....ti?1 Trees T with yield t1...ti?1 tiDemberg and Keller (2008)Roark et al (2009)Frank and Bod (2011)This WorkDemberg and Keller (2008) Hier.
Total w1...wi?1 Trees T with yield t1...ti?1 wiRoark et al (2009)This WorkRoark et al (2009) Hier.
Syntactic- w1...wi?1 Trees T with yield w1...wi?1 tiThis Work OnlyRoark et al (2009) Hier.
Lexical- w1...wi?1 Trees T with yield w1...wi?1; ti wiThis Work OnlyFrank and Bod (2011) Seq.
POS ti....ti?1 ?
tiThis Work?
Seq.
Total w1...wi?1 t1...ti?1 with yield w1...wi?1 wiTable 1: Contexts and events used to produce surprisal measures under various probabilistic syntactic models.
T refersto trees; t refers to POS tags; and w refers to words.dition rule expansions on the current head node2.In addition to the grammars over POS tag se-quences used by Frank and Bod (2011), we evalu-ate PSG?s over word sequences.
We also includethe state-of-the-art Berkeley grammar (Petrov andKlein, 2007) in our comparison.
Syntactic cate-gories in the Berkeley grammar are automaticallysplit into fine-grained subcategories to improve thelikelihood of the training corpus under the model.This increased expressivity allows the parser toachieve state-of-the-art automatic parsing accuracy,but increases grammar size considerably.33.2 Markov ModelsFrank and Bod (2011) use Markov models overPOS tag sequences, where the prefix probabilityof a sequence t is?i pr(ti|ti?n+1, ti?n+2...ti?1).They use three types of smoothing: additive, Good-Turing, and Witten-Bell, and explore values of nfrom 1 to 3.2or rightmost child node, if the head node is not yet avail-able(Roark, 2001).3To make parsing with the Berkeley grammar tractable un-der the prefix probability parser, we prune away all rules withprobability less than 10?4.3.3 Echo State NetworksUnlike Markov models, ESN?s (Ja?ger, 2001) cancapture long-distance dependencies.
ESN?s are atype of recurrent neural network (Elman, 1991) inwhich only the weights from the hidden layer to theoutput layer are trained; the weights from the inputlayer to the hidden layer and from the hidden layerto itself are set randomly and do not change.
In re-current networks, the activation of the hidden layerat tag ti depends not only on the activation of the in-put layer at tag ti, but also on the activation of thehidden layer at tag ti?1, which in turn depends onthe activation of the hidden layer at tag ti?2, and soforth.
The activation of the output layer at tag ti istherefore a function of all previous input symbolst1...ti?1 in the sequence.
The prefix probability ofa sequence t under this model is?i pr(ti|t1...ti?1),where pr(ti|t1...ti?1) is the normalized activation ofthe output layer at tag ti.
Frank and Bod (2011) eval-uate ESN?s with 100, 200...600 hidden nodes.4 MethodsWe use two incremental parsers to calculate sur-prisals under the hierarchical models.
For the PSG?savailable under the Roark et al (2009) parser, weuse that parser to calculate approximate prefix prob-64abilities using beam search.
For the Berkeley gram-mar, we use a probabilistic Earley parser modifiedby Levy4 to calculate exact prefix probabilities us-ing the algorithm of Stolcke (1995).
We evaluateeach hierarchical model under each type of surprisal(POS, total, lexical-only, and syntactic-only), wherepossible.4.1 Data SetsEach syntactic model is trained on sections 2-21 ofthe Wall Street Journal (WSJ) portion of the PennTreebank (Marcus et al, 1994), and tested on theDundee Corpus (Kennedy and Pynte, 2005), whichcontains reading time measures for 10 subjects overa corpus of 2,391 sentences of naturally occurringtext.
Gold-standard POS tags for the Dundee cor-pus are obtained automatically using the Brill tagger(Brill, 1995).Frank and Bod (2011) exclude subject/word pairsfrom evaluation if any of the following conditionshold true: ?the word was not fixated, was presentedas the first or last on a line, was attached to punc-tuation, contained more than one capital letter, orcontained a non-letter (this included clitics)?.
Thisleaves 191,380 subject/word pairs in the data setpublished by Frank and Bod (2011).
Because weconsider lexicalized hierarchical models in additionto unlexicalized ones, we additionally exclude sub-ject/word pairs where the word is ?unknown?
to themodel.5 This leaves us with a total of 148,829 sub-ject/word pairs; all of our reported results refer tothis data set.4.2 EvaluationFollowing Frank and Bod (2011), we compare theper-word surprisal predictions from hierarchical andsequential models of syntactic structure along twoaxes: linguistic accuracy (how well the model ex-plains the test corpus) and psychological accuracy(how well the model explains observed readingtimes on the test corpus).4The prefix parser is available at:www.http://idiom.ucsd.edu/ rlevy/prefixprobabilityparser.html5We consider words appearing fewer than 5 times in thetraining data to be unknown.4.2.1 Linguistic AccuracyEach model provides surprisal estimatessurprisal(wi).
The linguistic accuracy overthe test corpus is 1n?ni=1 surprisal(wi), where nis the number of words in the test corpus.4.2.2 Psychological AccuracyWe add each model?s per-word surprisal predic-tions to a linear mixed-effects model of first-passreading times, then measure the improvement inreading time predictions (according to the de-viance information criterion) relative to a baselinemodel; the resulting decrease in deviance is thepsychological accuracy of the language model.Using the lmer package for linear mixed-effectsmodels in R (Baayen et al, 2008), we first fit abaseline model to first-pass readings times overthe test corpus.
Each baseline model containsthe following control predictors for each sub-ject/word pair: sentpos (position of the word inthe sentence), nrchar (number of characters inthe word), prevnonfix (whether the previousword was fixated by the subject), nextnonfix(whether the next word was fixated by the subject),logwordprob (log(pr(wi))), logforwprob(log(pr(wi|wi?1))), and logbackprob(log(pr(wi|wi+1))).
When fitting each base-line model, we include all control predictors; allsignificant two-way interactions between them(|t| ?
1.96); by-subject and by-word intercepts;and a by-subject random slope for the predictor thatshows the most significant effect (nrchar).6We evaluate the statistical significance of the dif-ference in psychological accuracy between two pre-dictors using a nested model comparison.
If themodel containing both predictors performs signifi-cantly better than the model containing only the firstpredictor under a ?2 test (p ?
0.05), then the sec-ond predictor accounts for variance in reading timesabove and beyond the first predictor, and vice versa.6In accordance with the methods of Frank and Bod (2011),?Surprisal was not included as a by-subject random slope be-cause of the possibility that participants?
sensitivity to surprisalvaries more strongly for some sets of surprisal estimates thanfor others, making the comparisons between language modelsunreliable.
Since subject variability is not currently of interest,it is safer to leave out random surprisal effects.
?655 ResultsWe first replicate the results of Frank and Bod(2011) by obtaining POS surprisal values directlyfrom the authors?
published dataset for each syntac-tic model, then evaluating the psychological accu-racy of each of those models relative to the baselinemodel defined above.7Baseline Model with Additional Lexical N-gramsNext, we explore the impact of the lexical n-gramprobabilities used as control predictors upon psy-chological accuracy.
Frank and Bod (2011) statethat they compute lexical unigram and bigram prob-abilities via linear interpolation between estimatesfrom the British National Corpus and the Dundeecorpus itself (p.c.
); upon inspection, we find that thebigram probabilities released in their published dataset (which are consistent with their published exper-imental results) more closely resemble probabilitiesestimated from the Dundee corpus alone.
Because ofthe small size of the Dundee corpus, lexical bigramsfrom this corpus alone are unlikely to be representa-tive of a human?s language experience.We augment the lexical bigram probabilities usedin the baseline model of Frank and Bod (2011)with additional lexical unigram and bigrams esti-mated using the SRILM toolkit (Stolcke, 2002) withKneser-Ney smoothing from three corpora: sec-tions 2-21 of the WSJ portion of the Penn Tree-bank, the Brown corpus, and the British Nationalcorpus.
We include these additional predictors andall two-way interactions between them in the base-line model.
Figure 1 shows that the relative differ-ences in psychological accuracy between unlexical-ized hierarchical and sequential models vanish underthis stronger baseline condition.8Unlexicalized Hierarchical Models We then cal-culate POS surprisal values under each of the ances-tor (a1-a4) and the ancestor+sibling (s1-s4) hierar-chical models ourselves, using the parser of Roark7The only difference between our results and the originalresults in Figure 2 of Frank and Bod (2011) is that we evaluateaccuracy over a subset of the subject/items pairs used in Frankand Bod (2011) (see Section 4.1 for details).8The psychological accuracies of the best sequential model(e4) and the best hierarchical model (s3) used in Frank and Bod(2011) relative to the stronger baseline with additional lexicaln-grams are not significantly different, according to a ?2 test.et al (2009).
We also calculate POS surprisal un-der the Berkeley grammar (b) using the Levy prefixprobability parser.
Figure 2 shows the accuracies ofthese models.9Lexicalized Hierarchical Models Next, we lex-icalize the hierarchical models.
Figure 3 showsthe results of computing total surprisal undereach lexicalized hierarchical model (a1-a4T, s1-s4T,and bT).
The lexicalized models improve signifi-cantly upon their unlexicalized counterparts (?2 =7.52 to 12.47, p ?
0.01) in all cases; by con-trast, the unlexicalized models improve signifi-cantly upon their lexicalized counterparts (?2 =4.05 to 5.92, p ?
0.05) only in some cases (s1-s4).
Each lexicalized model improves significantlyupon e4, the best unlexicalized model of Frankand Bod (2011) (?2 = 6.96 to 23.45, p ?
0.01),though e4 also achieves a smaller but still signifi-cant improvement upon each of the lexicalized mod-els (?2 = 4.49 to 7.58, p ?
0.05).
The lexical-ized Berkeley grammar (bT) achieves the highestlinguistic and psychological accuracy; the improve-ment of bT upon e4 is substantial and significant(?2(1) = 23.45, p ?
0.001), while the improve-ment of e4 upon bT is small but still significant(?2(1) = 4.50, p ?
0.1).
Estimated coefficientsfor surprisal estimates under each lexicalized hierar-chical model are shown in Table 2.10Decomposing Total Surprisal Figure 3 shows theresults of decomposing total surprisal (a1-a4T, s1-s4T) into its lexical and syntactic components, thenentering both components as predictors into themixed-effects model (a1-a4LS, s1-s4LS).11 For eachgrammar, the psychological accuracy of the surprisalestimates is slightly higher when both lexical andsyntactic surprisal are entered as predictors, thoughthe differences are not statistically significant.9Our POS surprisal estimates have slightly worse linguisticaccuracy but slightly better psychological accuracy than Frankand Bod (2011); these differences are likely due to differencesin beam settings and in the subset of the WSJ used as trainingdata.10Each surprisal estimate predicts reading times in the ex-pected (positive) direction.11Decomposing surprisal into its lexical and syntactic com-ponents is possible with the Levy prefix probability parser aswell, but requires modifications to the parser; the Roark et al(2009) parser computes these quantities explicitly by default.66Figure 1: Psychological vs. linguistic accuracy of POS sur-prisal estimates from unlexicalized sequential and hierar-chical models of Frank and Bod (2011) relative to baselinesystem of Frank and Bod (2011) (shown above dotted line),and relative to a baseline system including additional lex-ical unigrams and bigrams (shown below dotted line).
In-corporating additional lexical n-grams into baseline systemvirtually eliminates all differences in psychological accu-racy among models.Figure 2: Psychological vs. linguistic accuracy of POSsurprisal estimates from unlexicalized hierarchical modelsused in this work, relative to a baseline system with ad-ditional lexical unigrams and bigrams.
Horizontal line in-dicates most psychologically accurate model of Frank andBod (2011) for ease of comparison.POS vs. Syntactic-only Surprisal Figures 2 and4 show the results of computing POS surprisal (a1-a4, s1-s4) and syntactic-only surprisal (a1-a4S, s1-s4S), respectively, under each of the Roark gram-mars.
While syntactic surprisal achieves slightlyhigher psychological accuracy than POS surprisalfor each model, the difference is statistically signifi-cant in only one case (s1).6 DiscussionIn the presence of additional lexical n-gram controlpredictors, all gaps in performance between the un-lexicalized sequential and hierarchical models usedin Frank and Bod (2011) vanish (Figure 1).
Frankand Bod (2011) do not include lexicalized hierarchi-cal models in their study; our results indicate thatlexicalizing hierarchical models improves their psy-chological accuracy significantly compared to theunlexicalized versions.
Overall, the lexicalized hier-archical model with the highest linguistic accuracy(Berkeley) also achieves the highest psychologicalaccuracy.Decomposing total surprisal into its lexical- andsyntactic-only components improves psychologicalaccuracy, but this improvement is not statisticallysignificant.
Computing syntactic-only surprisal in-stead of POS surprisal improves psychological accu-racy, but this improvement is statistically significantin only one case (s1).7 Conclusion and Future WorkFrank and Bod (2011) claim that sequential unlexi-calized syntactic models predict reading times bet-ter than hierarchical unlexicalized syntactic models,and conclude that the human parser is insensitiveto hierarchical syntactic structure.
We find that thepicture is more complicated than this.
We show,first, that the gap in psychological accuracy betweenthe unlexicalized hierarchical and sequential modelsof Frank and Bod (2011) vanishes when additional,67Figure 3: Psychological vs. linguistic accuracy of lexi-cal+syntactic (LS) and total (T) surprisal estimates fromlexicalized hierarchical models used in this work, relativeto baseline system with additional lexical unigrams and bi-grams as control predictors.
Decomposing total surprisalinto lexical-only and syntactic-components improves psy-chological accuracy.
Horizontal line indicates most psy-chologically accurate model of (Frank and Bod, 2011).Figure 4: Psychological vs. linguistic accuracy of lexical-only (L) and syntactic-only (S) surprisal estimates fromlexicalized hierarchical models used in this work, relativeto baseline system with additional lexical unigrams and bi-grams as control predictors.
On its own, syntactic-only sur-prisal predicts reading times better than lexical-only sur-prisal.
Horizontal line indicates most psychologically ac-curate model of (Frank and Bod, 2011).Surprisal Coef.
|t| Surprisal Coef.
|t|a1LS 0.82 2.61 a1T 1.30 2.98a2LS 1.01 3.24 a2T 1.38 3.19a3LS 1.14 3.65 a3T 1.56 3.60a4LS 1.17 3.76 a4T 1.56 3.64s1LS 1.38 4.43 s1T 1.71 4.00s2LS 1.37 4.44 s2T 1.75 4.16s3LS 1.20 3.90 s3T 1.64 3.91s4LS 1.21 3.97 s4T 1.62 3.89bT 3.15 5.34Table 2: Estimated coefficients and |t|-values for sur-prisal estimates shown in Figure 3.
Coefficients are es-timated by adding each surprisal estimate, one at a time,to the baseline model of reading times used in Figure 3.robustly estimated lexical n-gram probabilities areincorporated as control predictors into the baselinemodel of reading times.
Next, we show that lexical-izing hierarchical grammars improves psychologicalaccuracy significantly.
Finally, we show that usingbetter lexicalized hierarchical models improves psy-chological accuracy still further.
Our results demon-strate that the claim of Frank and Bod (2011) thatsequential models predict reading times better thanhierarchical models is premature, and that further in-vestigation is required.In future work, we plan to incorporate lexical in-formation into the sequential syntactic models usedin Frank and Bod (2011) so that we can comparethe hierarchical lexicalized models described hereagainst sequential lexicalized models.AcknowledgmentsThe authors thank Stefan Frank for providing thedataset of Frank and Bod (2011) and a detailed spec-ification of their experimental configuration.
Thisresearch was supported by NSF grant 0953870,NIH grant 1R01HD065829, and funding from theArmy Research Laboratory?s Cognition & Neuroer-gonomics Collaborative Technology Alliance.68ReferencesR.
H. Baayen, D. J. Davidson, and D. M. Bates.
2008.Mixed-effects modeling with crossed random effectsfor subjects and items.
In Journal of Memory and Lan-guage, 59, pp.
390-412.Marisa Ferrara Boston, John Hale, Reinhold Kliegl,Umesh Patil, and Shravan Vasishth.
2008.
Parsingcosts as predictors of reading difficulty: An evaluationusing the potsdam sentence corpus.
In Journal of EyeMovement Research, 2(1):1, pages 1-12.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational lin-guistics, 21(4).Eugene Charniak.
1996.
Tree-bank grammars.
In AAAI.Vera Demberg and Frank Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
In Cognition, Volume 109, Is-sue 2, pages 193-210.J.L.
Elman.
1991.
Distributed representations, simple re-current networks, and grammatical structure.
MachineLearning, 7(2).Stefan Frank and Rens Bod.
2011.
Insensitivity ofthe human sentence-processing system to hierarchicalstructure.
In Psychological Science.Edward Gibson, Timothy Desmet, Daniel Grodner, Du-ane Watson, and Kara Ko.
2005.
Reading relativeclauses in english.
Cognitive Linguistics, 16(2).John Hale.
2001.
A probabilistic earley parser as a psy-cholinguistic model.
In Proceedings of NAACL.Herbert Ja?ger.
2001.
The?
echo state?
approach toanalysing and training recurrent neural networks.
InTechnical Report GMD 148, German National Re-search Center for Information Technology.Mark Johnson.
1998.
Pcfg models of linguistic tree rep-resentations.
Computational Linguistics, 24.A.
Kennedy and J. Pynte.
2005.
Parafoveal-on-fovealeffects in normal reading.
Vision research, 45(2).Jonathan King and Marcel Just.
1991.
Individual dif-ferences in syntactic processing: The role of workingmemory.
Journal of memory and language, 30(5).Dan Klein and Chris Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of ACL.Lars Konieczny and Philipp Do?ring.
2003.
Anticipationof clause-final heads: Evidence from eye-tracking andsrns.
In Proceedings of ICCS/ASCS.Lars Konieczny.
2000.
Locality and parsing complexity.Journal of Psycholinguistic Research, 29(6).E.
Lau, C. Stroud, S. Plesch, and C. Phillips.
2006.
Therole of structural prediction in rapid syntactic analysis.Brain and Language, 98(1).Roger Levy.
2008.
Expectation-based syntactic compre-hension.
Cognition.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert Macintyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The penn tree-bank: Annotating predicate argument structure,.
InProceedings of ARPA Human Language TechnologyWorkshop.Kentaro Nakatani and Edward Gibson.
2008.
Distin-guishing theories of syntactic expectation cost in sen-tence comprehension: Evidence from japanese.
Lin-guistics, 46(1).Joakim Nivre.
2006.
Inductive dependency parsing, vol-ume 34.
Springer Verlag.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedinngs of HLT-NAACL.Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical and syn-tactic expectation-based measures for psycholinguisticmodeling via incremental top-down parsing.
In Pro-ceedings of EMNLP.Brian Roark.
2001.
Probabilistic top-down parsing andlanguage modeling.
Computational linguistics, 27(2).A.
Staub and C. Clifton.
2006.
Syntactic prediction inlanguage comprehension: Evidence from either... or.Journal of Experimental Psychology: Learning, Mem-ory, and Cognition, 32(2).A.
Staub, C. Clifton, and L. Frazier.
2006.
Heavy np shiftis the parsers last resort: Evidence from eye move-ments.
Journal of memory and language, 54(3).Andreas Stolcke.
1995.
An efficient probabilisticcontext-free parsing algorithm that computes prefixprobabilities.
Computational Linguistics, 21(2).A.
Stolcke.
2002.
Srilm-an extensible language mod-eling toolkit.
In Seventh International Conference onSpoken Language Processing.P.
Sturt, F. Keller, and A. Dubey.
2010.
Syntactic prim-ing in comprehension: Parallelism effects with andwithout coordination.
Journal of Memory and Lan-guage, 62(4).Shravan Vasishth and Richard Lewis.
2006.
Argument-head distance and processing complexity: Explainingboth locality and antilocality effects.
Linguistic Soci-ety of America, 82(4).69
