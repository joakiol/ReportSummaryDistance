Proceedings of the 6th Workshop on Statistical Machine Translation, pages 337?343,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe CMU-ARK German-English Translation SystemChris Dyer Kevin Gimpel Jonathan H. Clark Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{cdyer,kgimpel,jhclark,nasmith}@cs.cmu.eduAbstractThis paper describes the German-Englishtranslation system developed by the ARK re-search group at Carnegie Mellon Universityfor the Sixth Workshop on Machine Trans-lation (WMT11).
We present the results ofseveral modeling and training improvementsto our core hierarchical phrase-based trans-lation system, including: feature engineeringto improve modeling of the derivation struc-ture of translations; better handing of OOVs;and using development set translations intoother languages to create additional pseudo-references for training.1 IntroductionWe describe the German-English translation systemsubmitted to the shared translation task in the SixthWorkshop on Machine Translation (WMT11) by theARK research group at Carnegie Mellon Univer-sity.1 The core translation system is a hierarchicalphrase-based machine translation system (Chiang,2007) that has been extended in several ways de-scribed in this paper.Some of our innovations focus on modeling.Since German and English word orders can divergeconsiderably, particularly in non-matrix clauses,we focused on feature engineering to improve themodeling of long-distance relationships, which arepoorly captured in standard hierarchical phrase-based translation models.
To do so, we devel-oped features that assess the goodness of the source1http://www.ark.cs.cmu.edulanguage parse tree under the translation grammar(rather than of a ?linguistic?
grammar).
To train thefeature weights, we made use of a novel two-phasetraining algorithm that incorporates a probabilistictraining objective and standard minimum error train-ing (Och, 2003).
These segmentation features weresupplemented with a 7-gram class-based languagemodel, which more directly models long-distancerelationships.
Together, these features provide amodest improvement over the baseline and suggestinteresting directions for future work.
While ourwork on parse modeling was involved and requiredsubstantial changes to the training pipeline, someother modeling enhancements were quite simple: forexample, improving how out-of-vocabulary wordsare handled.
We propose a very simple change, andshow that it provides a small, consistent gain.On the training side, we had two improvementsover our baseline system.
First, we were inspiredby the work of Madnani (2010), who showed thatwhen training to optimize BLEU (Papineni et al,2002), overfitting is reduced by supplementing a sin-gle human-generated reference translation with ad-ditional computer-generated references.
We gener-ated supplementary pseudo-references for our de-velopment set (which is translated into many lan-guages, but once) by using MT output from a sec-ondary Spanish-English translation system.
Second,following Foster and Kuhn (2009), we used a sec-ondary development set to select from among manyoptimization runs, which further improved general-ization.We largely sought techniques that did not requirelanguage-specific resources (e.g., treebanks, POS337annotations, morphological analyzers).
An excep-tion is a compound segmentation model used forpreprocessing that was trained on a corpus of man-ually segmented German.
Aside from this, no fur-ther manually annotated data was used, and we sus-pect many of the improvements described here canbe had in other language pairs.
Despite avoidinglanguage-specific resources and using only the train-ing data provided by the workshop, an extensivemanual evaluation determined that the outputs pro-duced were of significantly higher quality than bothstatistical and rule-based systems that made use oflanguage-specific resources (Callison-Burch et al,2011).2 Baseline system and dataOur translation system is based on a hierarchicalphrase-based translation model (Chiang, 2007), asimplemented in the cdec decoder (Dyer et al,2010).
Since German is a language that makesproductive use of ?closed?
compounds (compoundwords written as a single orthographic token), weuse a CRF segmentation model of to evaluate theprobability of all possible segmentations, encodingthe most probable ones compactly in a lattice (Dyer,2009).
For the purposes of grammar induction, thesingle most probable segmentation of each word inthe source side of the parallel training data under themodel was inferred.The parallel data were aligned using theGiza++ implementation of IBM Model 4 runin both directions and then symmetrized usingthe grow-diag-final-and heuristic (Och andNey, 2002; Brown et al, 1993; Koehn et al, 2003).The aligned corpus was encoded as a suffix array(Lopez, 2008) and lattice-specific grammars (con-taining just the rules that are capable of matchingspans in the input lattice) were extracted for eachsentence in the test and development sets, using theheuristics recommended by Chiang (2007).A 4-gram modified Kneser-Ney language model(Chen and Goodman, 1996) was constructed usingthe SRI language modeling toolkit (Stolcke, 2002)from the English side of the parallel text, the mono-lingual English data, and the English version 4 Giga-word corpus (Parker et al, 2009).
Since there weremany duplicate segments in the training data (muchof which was crawled from the web), duplicate seg-ments and segments longer than 100 words were re-moved.
Inference was carried out using the languagemodeling library described by Heafield (2011).The newstest-2009 set (with the 500 longestsegments removed) was used for development,2 andnewstest-2010 was used as a development testset.
Results in this paper are reported on the dev-test set using uncased BLEU4 with a single refer-ence translation.
Minimum error rate training (Och,2003) was used to optimize the parameters of thesystem to maximize BLEU on the development data,and inference was performed over a pruned hyper-graph representation of the translation hypothesisspace (Kumar et al, 2009).For the experiments reported in this paper, Viterbi(max-derivation) decoding was used.
The systemsubmitted for manual evaluation used segment-levelMBR decoding with 1 ?
BLEU as the loss function,approximated over a 500-best list for each sentence.This reliably results in a small but consistent im-provement in translation quality, but is much moretime consuming to compute (Kumar and Byrne,2004).3 Source parse structure modelingImproving phrase-based translation systems is chal-lenging in part because our intuitions about whatmakes a ?good?
phrase or translation derivation areoften poor.
For example, restricting phrases andrules to be consistent with syntactic constituentsconsistently harms performance (Chiang, 2007; Gal-ley et al, 2006; Koehn et al, 2003), although ourintuitions might suggest this is a reasonable thingto do.
On the other hand, it has been shown thatincorporating syntactic information in the form offeatures can lead to improved performance (Chiang,2010; Gimpel and Smith, 2009; Marton and Resnik,2008).
Syntactic features that are computed by as-sessing the overlap of the translation parse with alinguistic parse can be understood to improve trans-lation because they lead to a better model of what a?correct?
parse of the source sentence is under thetranslation grammar.Like the ?soft syntactic features?
used in pre-2Removing long segments substantially reduces trainingtime and does not appear to negatively affect performance.338vious work (Marton and Resnik, 2008; Chiang etal., 2008), we propose features to assess the treestructure induced during translation.
However, un-like that work, we do not rely on linguistic sourceparses, but instead only make use of features thatare directly computable from the source sentenceand the parse structure being considered in the de-coder.
In particular, we take inspiration from themodel of Klein and Manning (2002), which mod-els constituency in terms of the contexts that ruleproductions occur in.
Additionally, we make use ofsalient aspects of the spans being dominated by anonterminal, such as the words at the beginning andend of the span, and the length of the span.
Impor-tantly, the features do not rely on the target wordsbeing predicted, but only look at the structure of thetranslation derivation.
As such, they can be under-stood as monolingual parse features.3Table 1 lists the feature templates that were used.Template DescriptionCTX:fi?1, fj context bigramCTX:fi?1, fj , x context bigram + NTCTX:fi?1, fj , x, (j ?
i) context bigram + NT + lenLU:fi?1 left unigramLB:fi?1, fi left bigram (overlapping)RU:fj right unigramRB:fj?1, fj right bigram (overlapping)Table 1: Context feature templates for features extractedfrom every translation rule used; i and j indicate hypothe-sized constituent span, x is its nonterminal category label(in our grammar, X or S), and fk is the kth word of thesource sentence, with f<1 = ?s?
and f>|f| = ?/s?.
If aword fk is not among the 1000 most frequent words inthe training corpus, it is replaced by a special unknowntoken.
The SMALLCAPS prefixes prevent accidental fea-ture collisions.3.1 Two-phase discriminative learningThe parse features just introduced are numerous andsparse, which means that MERT can not be usedto infer their weights.
Instead, we require a learn-ing algorithm that can cope with millions of fea-tures and avoid overfitting, perhaps by eliminatingmost of the features and keeping only the most valu-able (which would also keep the model compact).3Similar features have been proposed for use in discrimina-tive monolingual parsing models (Taskar et al, 2004).Furthermore, we would like to be able to still tar-get the BLEU measure of translation quality duringlearning.
While large-scale discriminative trainingfor machine translation is a widely studied problem(Hopkins and May, 2011; Li and Eisner, 2009; De-vlin, 2009; Blunsom et al, 2008; Watanabe et al,2007; Arun and Koehn, 2007; Liang et al, 2006), notractable algorithm exists for learning a large num-ber of feature weights while directly optimizing acorpus-level metric like BLEU.
Rather than resortingto a decomposable approximation, we have exploreda new two-phase training algorithm in developmentof this system.The two-phase algorithm works as follows.
Inphase 1, we use a non-BLEU objective to train atranslation model that includes the large feature set.Then, we use this model to compute a small num-ber of coarse ?summary features,?
which summa-rize the ?opinion?
of the first model about a trans-lation hypothesis in a low dimensional space.
Then,in the second training pass, MERT is used to deter-mine how much weight to give these summary fea-tures together with the other standard coarse trans-lation features.
At test time, translation becomes amulti-step process as well.
The hypothesis space isfirst scored using the phase-1 model, then summaryfeatures are computed, then the hypothesis space isrescored with the phase-2 model.
As long as the fea-tures used factor with the edges in the translationspace (which ours do), this can be carried out in lin-ear time in the size of the translation forest.3.1.1 Phase 1 trainingFor the first model, which includes the sparse parsefeatures, we learn weights in order to optimize pe-nalized conditional log likelihood (Blunsom et al,2008).
We are specifically interested in modelingan unobserved variable (i.e., the parse tree underly-ing a translation derivation), this objective is quitenatural, since probabilistic models offer a principledaccount of unobserved data.
Furthermore, becauseour features factor according to edges in the trans-lation forest (they are ?stateless?
in standard MTterminology), there are efficient dynamic program-ming algorithms that can be used to exactly computethe expected values of the features (Lari and Young,1990), which are necessary for computing the gradi-ents used in optimization.339We are therefore optimizing the following objec-tive, given a set T of parallel training sentences:L = ?R(?)???f,e??Tlog?dp?
(e,d | f)where p?
(e,d | f) =exp ?>h(f, e,d)Z(f),where d is a variable representing the unobservedsynchronous parses giving rise to the pair of sen-tences ?f, e?, and where R(?)
is a penalty that favorsless complex models.
Since we not only want to pre-vent over fitting but also want a small model, we useR(?)
=?k |?k|, the `1 norm, which forces manyparameters to be exactly 0.Although L is not convex in ?
(on account of thelatent derivation variable), we make use of an on-line stochastic gradient descent algorithm that im-poses an `1 penalty on the objective (Tsuruoka etal., 2009).
Online algorithms are often effective fornon-convex objectives (Liang and Klein, 2009).We selected 12,500 sentences randomly from thenews-commentary portion of the training data to useto train the latent variable model.
Using the stan-dard rule extraction heuristics (Chiang, 2007), 9,967of the sentence pairs could be derived.4 In additionto the parse features describe above, the standardphrase features (relative frequency and lexical trans-lation probabilities), and a rule count feature wereincluded.
Training was run for 48 hours on a sin-gle machine, which resulted in 8 passes through thetraining data, instantiating over 8M unique features.The regularization strength ?
was chosen so that ap-proximately 10, 000 (of the 8M) features would benon-zero.53.1.2 Summary featuresAs outlined above, the phase 1 model will be incor-porated into the final translation model using a lowdimensional ?summary?
of its opinion.
Because weare using a probabilistic model, posterior probabili-ties (given the source sentence f) under the parsing4When optimizing conditional log likeligood, it is necessaryto be able to exactly derive the training pair.
See Blunsom et al(2008) for more information.5Ideally, ?
would have been tuned to optimize held-out like-lihood or BLEU; however, the evaluation deadline prevented usfrom doing this.model are easily defined and straightforward to com-pute with dynamic programming.
We made use offour summary features: the posterior log probabilitylog p?
(e,d|f); for every rule r ?
d, the probability ofits span being a constituent under the parse model;the probabilities that some span starts at the r?s start-ing index, or that some rule ends at r?s ending index.Once these summary features have been com-puted, the sparse features are discarded, and thesummary features are reweighted using coefficientslearned by MERT, together with the standard MTfeatures (language model, word penalty, etc.).
Thisprovides a small improvement over our already verystrong baseline, as the first two rows in Table 2 show.Condition BLEUbaseline 25.0+ parse features 25.2+ parse features + 7-gram LM 25.4Table 2: Additional features designed to improve modelof long-range reordering.3.2 7-gram class-based LMThe parsing features above were intended to im-prove long range reordering quality.
To further sup-port the modeling of larger spans, we incorporateda 7-gram class-based language model.
Automaticword clusters are attractive because they can belearned for any language without supervised data,and, unlike part-of-speech annotations, each wordis in only a single class, which simplifies inference.We performed Brown clustering (Brown et al, 1992)on 900k sentences from our language modeling data(including the news commentary corpus and a sub-set of Gigaword).
We obtained 1,000 clusters us-ing an implementation provided by Liang (2005),6as Turian et al (2010) found that relatively largenumbers clusters gave better performance for infor-mation extraction tasks.
We then replaced wordswith their clusters in our language modeling dataand built a 7-gram LM with Witten-Bell smoothing(Witten and Bell, 1991).7 The last two rows of Ta-6http://www.cs.berkeley.edu/?pliang/software7The distributional assumptions made by the more com-monly used Kneser-Ney estimator do not hold in the word-340ble 2 shows that in conjunction with the source parsefeatures, a slight improvement comes from includ-ing the 7-gram LM.4 Non-translating tokensWhen two languages share a common alphabet (asGerman and English largely do), it is often appro-priate to leave some tokens untranslated when trans-lating.
Named entities, numbers, and graphical el-ements such as emoticons are a few common ex-amples of such ?non-translating?
elements.
To en-sure that such elements are well-modeled, we aug-ment our translation grammar so that every tokenin the input can translate as itself and add a featurethat counts the number of times such self-translationrules are used in a translation hypothesis.
This is incontrast to the behavior of most other decoders, suchas Moses, which only permit a token to translate asitself if it is learned from the training data, or if thereis no translation in the phrase table at all.Since many non-translating tokens are out-of-vocabulary (OOV) in the target LM, we also adda feature that fires each time the LM encounters aword that is OOV.8 This behavior be understood asdiscriminatively learning the unknown word penaltythat is part of the LM.
Again, this is in contrast tothe behavior of other decoders, which typically adda fixed (and very large) cost to the LM feature forevery OOV.
Our multi-feature parameterization per-mits the training algorithm to decide that, e.g., someOOVs are acceptable if they occur in a ?good?
con-text rather than forcing the decoder to avoid themat all costs.
Table 3 shows that always providinga non-translating translation option together with adiscriminative learned OOV feature improves thequality of German-English translation.9Condition BLEU?OOV (baseline) 24.6+OOV and non-translating rules 25.0Table 3: Effect of discriminatively learned penalties forOOV words.classified corpus.8When multiple LMs are used, there is an extra OOV featurefor each LM.9Both systems were trained using the human+ES-EN refer-ence set described below (?5).5 Computer-generated referencesMadnani (2010) shows that models learned by op-timizing BLEU are liable to overfit if only a sin-gle reference is used, but that this overfitting canbe mitigated by supplementing the single referencewith supplemental computer-generated referencesproduced by paraphrasing the human reference us-ing a whole-sentence statistical paraphrase system.These computer-generated paraphrases are just usedto compute ?better?
BLEU scores, but not directly asexamples of target translations.Although we did not have access to a paraphrasegenerator, we took advantage of the fact that our de-velopment set (newstest-2009) was translatedinto several languages other than English.
By trans-lating these back into English, we hypothesized wewould get suitable pseudo-references that could beused in place of computer-generated paraphrases.Table 4 shows the results obtained on our held-outtest set simply by altering the reference translationsused to score the development data.
These systemsall contain the OOV features described above.Condition BLEU1 human 24.71 human + ES-EN 25.01 human + FR-EN 24.01 human + ES-EN + FR-EN 24.2Table 4: Effect of different sets of reference translationsused during tuning.While the effect is somewhat smaller than Mad-nani (2010) reports using a sentential paraphraser,the extremely simple technique of adding the outputof a Spanish-English (ES-EN) system was found toconsistently improve the quality of the translationsof the held-out data.
However, a comparable effectwas not found when using references generated froma French-English (FR-EN) translation system, indi-cating that the utility of this technique must be as-sessed empirically and depends on several factors.6 Case restorationOur translation system generates lowercased out-put, so we must restore case as a post-processingstep.
We do so using a probabilistic transducer asimplemented in SRILM?s disambig tool.
Each341lowercase token in the input can be mapped to acased variant that was observed in the target lan-guage training data.
Ambiguities are resolved us-ing a language model that predicts true-cased sen-tences.10 We used the same data sources to con-struct this model as were used above.
During devel-opment, it was observed that many named entitiesthat did not require translation required some casechange, from simple uppercasing of the first letter,to more idiosyncratic casings (e.g., iPod).
To ensurethat these were properly restored, even when theydid not occur in the target language training data, wesupplement the true-cased LM training data and casetransducer training data with the German source testset.Condition BLEU (Cased)English-only 24.1English+test-set 24.3Table 5: Effect of supplementing recasing model trainingdata with the test set source.7 Model selectionMinimum error rate training (Och, 2003) is astochastic optimization algorithm that typically findsa different weight vector each time it is run.
Fosterand Kuhn (2009) showed that while the variance onthe development set objective may be narrow, theheld-out test set variance is typically much greater,but that a secondary development set can be used toselect a system that will have better generalization.We therefore replicated MERT 6 times and selectedthe output that performed best on NEWSTEST-2010.Since we had no additional blind test set, we can-not measure what the impact is.
However, the BLEUscores we selected on varied from 25.4 to 26.1.8 SummaryWe have presented a summary of the enhancementsmade to a hierarchical phrase-based translation sys-tem for the WMT11 shared translation task.
Someof our results are still preliminary (the source parse10The model used is p(y | x)p(y).
While this model is some-what unusual (the conditional probability is backwards from anoisy channel model), it is a standard and effective techniquefor case restoration.model), but a number of changes we made werequite simple (OOV handling, using MT output toprovide additional references for training) and alsoled to improved results.AcknowledgmentsThis research was supported in part by the NSF throughgrant IIS-0844507, the U. S. Army Research Laboratoryand the U. S. Army Research Office under contract/grantnumber W911NF-10-1-0533, and Sandia National Labo-ratories (fellowship to K. Gimpel).
We thank the anony-mous reviewers for their thorough feedback.ReferencesA.
Arun and P. Koehn.
2007.
Online learning methodsfor discriminative training of phrase based statisticalmachine translation.
In Proc.
of MT Summit XI.P.
Blunsom, T. Cohn, and M. Osborne.
2008.
A discrim-inative latent variable model for statistical machinetranslation.
In Proc.
of ACL-HLT.P.
F. Brown, P. V. de Souza, R. L. Mercer, V. J.Della Pietra, and J. C. Lai.
1992.
Class-based n-grammodels of natural language.
Computational Linguis-tics, 18:467?479.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Computa-tional Linguistics, 19(2):263?311.C.
Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan.2011.
Findings of the 2011 workshop on statisticalmachine translation.
In Proc.
of the Sixth Workshopon Statistical Machine Translation.S.
F. Chen and J. Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.
In Proc.of ACL, pages 310?318.D.
Chiang, Y. Marton, and P. Resnik.
2008.
Online large-margin training of syntactic and structural translationfeatures.
In Proc.
EMNLP, pages 224?233.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.D.
Chiang.
2010.
Learning to translate with source andtarget syntax.
In Proc.
of ACL, pages 1443?1452.J.
Devlin.
2009.
Lexical features for statistical machinetranslation.
Master?s thesis, University of Maryland.C.
Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,P.
Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.2010.
cdec: A decoder, alignment, and learningframework for finite-state and context-free translationmodels.
In Proc.
of ACL (demonstration session).C.
Dyer.
2009.
Using a maximum entropy model to buildsegmentation lattices for MT.
In Proc.
of NAACL.342G.
Foster and R. Kuhn.
2009.
Stabilizing minimum errorrate training.
Proc.
of WMT.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable inference andtraining of context-rich syntactic translation models.In Proc.
of ACL, pages 961?968.K.
Gimpel and N. A. Smith.
2009.
Feature-rich transla-tion by quasi-synchronous lattice parsing.
In Proc.
ofEMNLP, pages 219?228.K.
Heafield.
2011.
KenLM: Faster and smaller languagemodel queries.
In Proc.
of the Sixth Workshop on Sta-tistical Machine Translation.M.
Hopkins and J.
May.
2011.
Tuning as ranking.
InProc.
of EMNLP.D.
Klein and C. D. Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In Proc.
of ACL, pages 128?135.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of NAACL.S.
Kumar and W. Byrne.
2004.
Minimum Bayes-riskdecoding for statistical machine translation.
In Pro-cessings of HLT-NAACL.S.
Kumar, W. Macherey, C. Dyer, and F. Och.
2009.
Effi-cient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.In Proc.
of ACL-IJCNLP.K.
Lari and S. Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech and Language.Z.
Li and J. Eisner.
2009.
First- and second-order ex-pectation semirings with applications to minimum-risktraining on translation forests.
In Proc.
of EMNLP,pages 40?51.P.
Liang and D. Klein.
2009.
Online EM for unsuper-vised models.
In Proc.
of NAACL.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In Proc.
of ACL.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.A.
Lopez.
2008.
Tera-scale translation models via pat-tern matching.
In Proc.
of COLING.N.
Madnani.
2010.
The Circle of Meaning: From Trans-lation to Paraphrasing and Back.
Ph.D. thesis, De-partment of Computer Science, University of Mary-land College Park.Y.
Marton and P. Resnik.
2008.
Soft syntactic constraintsfor hierarchical phrased-based translation.
In Proc.
ofACL, pages 1003?1011, Columbus, Ohio.F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proceedings of ACL, pages 295?302.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.R.
Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.2009.
English gigaword fourth edition.A.
Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In Intl.
Conf.
on Spoken Language Pro-cessing.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proc.
of EMNLP.Y.
Tsuruoka, J. Tsujii, and S. Ananiadou.
2009.
Stochas-tic gradient descent training for l1-regularized log-linear models with cumulative penalty.
In Proc.
ofACL-IJCNLP.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proc.
of ACL, pages 384?394.T.
Watanabe, J. Suzuki, H. Tsukuda, and H. Isozaki.2007.
Online large-margin training for statistical ma-chine translation.
In Proc.
of EMNLP.I.
H. Witten and T. C. Bell.
1991.
The zero-frequencyproblem: Estimating the probabilities of novel eventsin adaptive text compression.
IEEE Trans.
Informa-tion Theory, 37(4).343
