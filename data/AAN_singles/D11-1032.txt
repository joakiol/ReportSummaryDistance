Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 344?354,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLarge-Scale Cognate RecoveryDavid Hall and Dan KleinComputer Science DivisionUniversity of California at Berkeley{dlwh,klein}@cs.berkeley.eduAbstractWe present a system for the large scale in-duction of cognate groups.
Our model ex-plains the evolution of cognates as a sequenceof mutations and innovations along a phy-logeny.
On the task of identifying cognatesfrom over 21,000 words in 218 different lan-guages from the Oceanic language family, ourmodel achieves a cluster purity score over91%, while maintaining pairwise recall over62%.1 IntroductionThe critical first step in the reconstruction of anancient language is the recovery of related cog-nate words in its descendants.
Unfortunately, thisprocess has largely been a manual, linguistically-intensive undertaking for any sizable number of de-scendant languages.
The traditional approach usedby linguists?the comparative method?iterates be-tween positing putative cognates and then identify-ing regular sound laws that explain correspondencesbetween those words (Bloomfield, 1938).Successful computational approaches have beendeveloped for large-scale reconstruction of phyloge-nies (Ringe et al, 2002; Daume?
III and Campbell,2007; Daume?
III, 2009; Nerbonne, 2010) and an-cestral word forms of known cognate sets (Oakes,2000; Bouchard-Co?te?
et al, 2007; Bouchard-Co?te?et al, 2009), enabling linguists to explore deep his-torical relationships in an automated fashion.
How-ever, computational approaches thus far have notbeen able to offer the same kind of scale for iden-tifying cognates.
Previous work in cognate identi-fication has largely focused on identifying cognatesin pairs of languages (Mann and Yarowsky, 2001;Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak,2001; Mulloni, 2007), with a few recent exceptionsthat can find sets in a handful of languages (Bergsmaand Kondrak, 2007; Hall and Klein, 2010).While it may seem surprising that cognate de-tection has not successfully scaled to large num-bers of languages, the task poses challenges notseen in reconstruction and phylogeny inference.
Forinstance, morphological innovations and irregularsound changes can completely obscure relationshipsbetween words in different languages.
However, inthe case of reconstruction, an unexplainable word issimply that: one can still correctly reconstruct its an-cestor using words from related languages.In this paper, we present a system that uses twogenerative models for large-scale cognate identi-fication.
Both models describe the evolution ofwords along a phylogeny according to automaticallylearned sound laws in the form of parametric editdistances.
The first is an adaptation of the genera-tive model of Hall and Klein (2010), and the otheris a new generative model called PARSIM with con-nections to parsimony methods in computational bi-ology (Cavalli-Sforza and Edwards, 1965; Fitch,1971).
Our model supports simple, tractable infer-ence via message passing, at the expense of beingunable to model some cognacy relationships.
Tohelp correct this deficiency, we also describe an ag-glomerative inference procedure for the model ofHall and Klein (2010).
By using the output of oursystem as input to this system, we can find cognategroups that PARSIM alone cannot recover.We apply these models to identifying cognategroups from two language families using the Aus-tronesian Basic Vocabulary Database (Greenhill etal., 2008), a catalog of words from about 40% ofthe Austronesian languages.
We focus on data fromtwo subfamilies of Austronesian: Formosan and344Oceanic.
The datasets are by far the largest onwhich automated cognate recovery has ever been at-tempted, with 18 and 271 languages respectively.On the larger Oceanic data, our model can achievecluster purity scores of 91.8%, while maintainingpairwise recall of 62.1%.
We also analyze the mis-takes of our system, where we find that some of theerroneous cognate groups our system finds may notbe errors at all.
Instead, they may be previouslyunknown cognacy relationships that were not anno-tated in the data.2 BackgroundBefore we present our model, we first describe ba-sic facts of the Austronesian language family, alongwith a description of the Austronesian Basic Vocab-ulary Database, which forms the dataset that we usefor our experiments.
For far more detailed coverageof the Austronesian languages, we direct the inter-ested reader to Blust (2009)?s comprehensive mono-graph.2.1 The Austronesian Language FamilyThe Austronesian language family is one of thelargest in the world, comprising about one-fifth ofthe world?s languages.
Geographically, it stretchesfrom its homeland on Formosa (Taiwan) to Mada-gascar in the west, and as far as Hawai?i and (at onepoint) the Easter Islands to the east.
Until the ad-vent of European colonialism spread Indo-Europeanlanguages to every continent, Austronesian was themost widespread of all language families.Linguistically, the language family is as diverseas it is large, but a few regularities hold.
Froma phonological perspective, two features stand out.First, the phoneme inventories of these languagesare typically small.
For example, it is well-knownthat Hawaiian has only 13 phonemes.
Moreover, thephonotactics of these languages are often restrictive.Sticking with the same example, Hawaiian only al-lows (C)V syllables: consonants clusters are forbid-den, and no syllable may end with a consonant.2.2 The Austronesian Basic VocabularyDatabaseThe Austronesian Basic Vocabulary Database(ABVD) (Greenhill et al, 2008) is an ambitious, on-going effort to catalog the lexicons and basic factsabout all of the languages in the Austronesian lan-guage family.
It also contains manual reconstruc-tions for select ancestor languages produced by lin-guists.The sample we use?from Bouchard-Co?te?
et al(2009)?contains about 50,000 words across 471languages spanning all the major divisions of Aus-tronesian.
These words are grouped into cognategroups and arranged by gloss.
For instance, there are37 distinct cognate groups for the gloss ?tail.?
Oneof these groups includes the words /ekor/, /ingko/,/iNkot/, /kiikiPu/, and /PiPina/, among others.
Mostof these words have been transcribed into the Inter-national Phonetic Alphabet, though it appears thatsome words are transcribed using the Roman alpha-bet.
For instance, the second word in the example islikely /iNko/, which is a much more likely sequencethan what is transcribed.In this sample, there are 6307 such cognategroups and 210 distinct glosses.
The data issomewhat sparse: fewer than 50% of the possiblegloss/language pairs are present.
Moreover, there issome amount of homoplasy?that is, languages witha word from more than one cognate group for a givengloss.Finally, it is important to note that the ABVD isstill a work in progress: they have data from only50% of extant Austronesian languages.2.3 Subfamilies of AustronesianIn this paper we focus on two branches of the Aus-tronesian language family, one as a development setand one as a test set.
For our development set, weuse the Formosan branch.
The languages in thisgroup are exclusively found on the Austronesianhomeland of Formosa.
The family encompasses asubstantial portion of the linguistic diversity of Aus-tronesian: Blust (2009) argues that Formosan con-tains 9 of the 10 first-order splits of the Austrone-sian family.
Formosan?s diversity is surprising sinceit contains a mere 18 languages.
Thus, Formosan isa smaller development set that nevertheless is repre-sentative of larger families.For our final test set, we use the Oceanic sub-family, which includes almost 50% of the languagesin the Austronesian family, meaning that it repre-sents around 10% of all languages in the world.Oceanic also represents a large fraction of the ge-345ographic diversity of Austronesian, stretching fromNew Zealand in the south to Hawai?i in the north.Our sample includes 21863 words from 218 lan-guages in the Oceanic family.3 ModelsIn this section we describe two models, one based onHall and Klein (2010)?which we call HK10?andanother new model that shares some connection toparsimony methods in computational biology, whichwe call PARSIM.
Both are generative models thatdescribe the evolution of words w` from a set of lan-guages {`} in a cognate group g along a fixed phy-logeny T .1 Each cognate group and word is alsoassociated with a gloss or meaning m, which we as-sume to be fixed.2 In both models, words evolveaccording to regular sound laws ?`, which are spe-cific to each language.
Also, both models will makeuse of a language model ?, which is used for gen-erating words that are not dependent on the word inthe parent language.
(We leave ?` and ?
as abstractparameters for now.
We will describe them in sub-sequent sections.
)3.1 HK10The first model we describe is a small modificationof the phylogenetic model of Hall and Klein (2010).In HK10, there is an unknown number of cognategroups G where each cognate group g consists of aset of words {wg,`}.
In each cognate group, wordsevolve along a phylogeny, where each word in a lan-guage is the result of that word evolving from itsparent according to regular sound laws.
To modelthe fact that not all languages have a cognate ineach group, each language in the tree has an asso-ciated ?survival?
variable Sg,`, where a word maybe lost on that branch (and its descendants) insteadof evolving.
Once the words are generated, they arethen ?permuted?
so that the cognacy relationships1Both of these models therefore are insensitive to geo-graphic and historical factors that cannot be easily approxi-mated by this tree.
See Nichols (1992) for an excellent dis-cussion of these factors.2One could easily envision allowing the meaning of a wordto change as well.
Modeling this semantic drift has been consid-ered by Kondrak (2001).
In the ABVD, however, any semanticdrift has already been elided, since the database has coarsenedglosses to the extent that there is no meaningful way to modelsemantic drift given our data.MGWForWAta?
?
?SForSAtaSPaiSSquSCiuwptwesL?wPaiwPaiwPaiwPaiwPaiwPaiWPaiWPaiSurvivalEvolutionPermutation(a)IAtaIPaiISquICiuWAtaWSquWCiuWPai(b)WFor???
?Figure 1: Plate diagrams for (a) HK10 (Hall and Klein,2010) and (b) PARSIM, our new parsimony model, fora small set of languages.
In HK10, words are generatedfollowing a phylogenetic tree according to sound laws ?,and then ?scrambled?
with a permutation pi so that theoriginal cognate groups are lost.
In PARSIM, all wordsfor each of the M glosses are generated in a single tree,with innovations I starting new cognate groups.
Thelanguages depicted are Formosan (For), Paiwan (Pai),Atayalic (Ata), Ciuli Atayalic (Ciu), and Squliq Atayalic(Squ).346are obscured.
The task of inference then is to re-cover the original cognate groups.The generative process for their model is as fol-lows:?
For each cognate group g, choose a root wordWroot ?
p(W |?
), a language model overwords.?
For each language ` in a pre-order traversal ofthe phylogeny:1.
Choose S` ?
Bernoulli(?`), indicatingwhether or not the word survives.2.
If the word survives, choose W` ?p(W |?`,Wpar(`)).3.
Otherwise, stop generating words in thatlanguage and its descendants.?
For each language, choose a random permuta-tion pi of the observed data, and rearrange thecognates according to this permutation.We reproduce the graphical model for HK10 for asmall phylogeny in Figure 1a.Inference in this model is intractable; to performinference exactly, one has to reason over all parti-tions of the data into cognate groups.
To addressthis problem, Hall and Klein (2010) propose an it-erative bipartite matching scheme where one lan-guage is held out from the others, and then wordsare assigned to the remaining groups to maximizethe probability of the attachment.
That is, for somelanguage ` and fixed assignments pi?` for the otherlanguages, they seek an assignment pi` that maxi-mizes:pi?
= argmaxpi?glog p(w(`,pi`(g))|?, pi,w?`)Unfortunately, while this approach was effectivewith only a few languages (they tested on three), thisalgorithm cannot scale to the eighteen languages inFormosan, let alne the hundreds of languages inOceanic.
Therefore, we make two simple modifi-cations.
First, we restrict the cognate assignmentsto stay within a gloss.
Thus, there are many fewerpotential matchings to consider.
Second, we use anagglomerative inference procedure, which greedilymerges cognate groups that result in the greatest gainin likelihood.
That is, for all pairs of cognate groupsga with words wa and gb with words wb, we com-pute the score:log p(wa?b|?)?
log p(wa|?)?
log p(wb|?
)This score is the difference between the log proba-bility of generating two cognate groups jointly andgenerating them separately.
We then merge the twothat generate the highest gain in likelihood.
Likethe iterative bipartite matching algorithm describedabove, this algorithm is not exact.
However, it isO(n2 log n) (where n is the size of the largest gloss,which for Oceanic is 153), while the bipartite match-ing algorithm is O(n3) (Kuhn, 1955).Actually, the original HK10 is doubly intractable.They use weighted automata to represent distribu-tions over strings, but these automata?particularlyif they are non-deterministic?make inference inany non-trivial graphical model intractable.
We dis-cuss this issue in more detail in Section 6.3.2 A Parsimony-Inspired ModelWe now describe a new model called PARSIM thatsupports exact inference tractably, though it sacri-fices some of the expressive power of HK10.
Inour model, each language has at most one word foreach gloss, and this one word changes from onelanguage to its children according to some edge-specific Markov process.
These changes may eitherbe mutations, which merely change the surface formof the word, or innovations, which start a new wordin a new cognate group that is unrelated to the previ-ous word.
Mutations take the form of a conditionaledit operation that models insertions, substitutions,and deletions that correspond to regular (and, withlower probability, irregular) sound changes that arelikely to occur between a language and its parent.Innovations, on the other hand, are generated from alanguage model independent of the parent?s word.Specifically, our generative process takes the fol-lowing form:?
For each gloss m, choose a root word Wroot ?
?, a language model over words.?
For each language ` in a pre-order traversal ofthe phylogeny:347malapzo mappoCiuli SquliqpurrokPaiwanRukaipouroukouFigure 2: A small example of how PARSIM works.Listed here are the words for ?ten?
in four languagesfrom the Formosan family, along with the tree that ex-plains them.
The dashed line indicates an innovation onthe branch.1.
Choose I` ?
Bernoulli(?`), indicatingwhether or not the word is an innovationor a mutation.2.
If it is a mutation, choose W` ?p(W |?`,Wpar(`)).3.
Otherwise, choose W` ?
?.We also depict our model as a plate diagram for asmall phylogeny in Figure 1b.Because there is only one tree per gloss, thereis no assignment problem to consider, which is themain source of the intractability of HK10.
Instead,pieces of the phylogeny are simply ?cut?
into sub-trees whenever an innovation occurs.
Thus, messagepassing can be used to perform inference.As an example of how our process works, con-sider Figure 2.
The Formosan word for ?ten?probably resembled either /purrok/ or /pouroukou/.There was an innovation in Ciuli and Squliq?s an-cestor Atayalic that produced a new word for ten.This word then mutated separately into the words/malapzo/ and /mappo/, respectively.4 Relation to ParsimonyPARSIM is related to the parsimony principlefrom computational biology (Cavalli-Sforza and Ed-wards, 1965; Fitch, 1971), where it is used to searchfor phylogenies.
When using parsimony, a phy-logeny is scored according to the derivation that re-quires the fewest number of changes of state, wherea state is typically thought of as a gene or some othertrait in a species.
These genes are typically called?characters?
in the computational biology literature,and two species would have the same value for acharacter if they share the same property that thatstate represents.When inducing phylogenies of languages, a natu-ral choice for characters are glosses from a restrictedvocabulary like a Swadesh list, and two words arerepresented as the same value for a character if theyare cognate (Ringe et al, 2002).
Other features canbe used (Daume?
III and Campbell, 2007; Daume?
III,2009), but they are not relevant to our discussion.Consider the small example in Figure 3a with justfour languages.
Here, cognacy is encoded usingcharacters.
In this example, at least two changes ofstate are required to explain the data: both C and Bmust have evolved from A.
Therefore, the parsimonyscore for this tree is two.Of course, there is no reason why all changesshould be equally likely.
For instance, it might beextremely likely that B changes into both A andC, but that A never changes into B or C, and soweighted variants of parsimony might be neces-sary (Sankoff and Cedergren, 1983).With this in mind, PARSIM can be thought of aweighted variant of parsimony, with two differences.First, the characters do not indicate ahead of timewhich words are related.
Instead, the characters arethe words themselves.
Second, the transitions be-tween different states (words) are not uniform.
In-stead, they are weighted by the log probability ofone word changing into another, including both mu-tations and innovations.Thus, the task of inference in PARSIM is to findthe most ?parsimonious?
explanation for the wordswe have observed, which is the same as finding themost likely derivation.
Because the distances be-tween words (that is, the transition probabilities)are not known ahead of time, they must instead belearned, which we discuss in Section 7.35 Limitations of the Parsimony ModelPotentially, our parsimony model sacrifices a cer-tain amount of power to make inference tractable.Specifically, it cannot model homoplasy, the pres-ence of more than one word in a language for a given3It is worth noting that we are not the first to point out aconnection between parsimony and likelihood.
Indeed, manyauthors in the computational biology literature have formallydemonstrated a connection (Farris, 1973; Felsenstein, 1973).348A B C AAAA(a)A B B A{A,B}{A,B}(b){A,B}A B B AA(c)AAA AA(d)AAB BBBBFigure 3: Trees illustrating parsimony and its limitations.
In these trees, there are four languages, with words A, B, andC in various configurations.
(a) The most parsimonious derivation for this tree has all intermediate states as A. Thereare thus two changes.
(b) An example of homoplasy.
Here, given this tree, it seems likely that the ancestral languagescontained both A and B.
(c) PARSIM cannot recover the example from (b), and so it encodes two innovations (shownas dashed lines).
(d) The HK10 model can recover this relationship, but this power makes the model intractable.gloss.
Homoplasy can arise for a variety of reasonsin phylogenetic models of cognates, and we describesome in this section.Consider the example illustrated in Figure 3b,where the two central languages share a cognate, asdo the two outer languages.
This is the canonical ex-ample of homoplasy, and PARSIM cannot correctlyrecover this grouping.
Instead, it can at best only se-lect group A or group B as the value for the parent,and leave the other group fragmented as two innova-tions, as in Figure 3c.
On the other hand, HK10 canrecover this relationship (Figure 3d), but this poweris precisely what makes it intractable.There are two reasons this kind of homoplasycould arise.
The first is that there were indeed twowords in the parent language for this gloss, or thatthere were two words with similar meanings andthe two meanings drifted together.
Second, the treecould be an inadequate model of the evolution inthis case.
For instance, there could have been a cer-tain amount of borrowing between two of these lan-guages, or there was not a single coherent parent lan-guage, but rather a language continuum that cannotbe explained by any tree.However, homoplasy seems to be relatively un-common (though not unheard of) in the Oceanic andFormosan families.
Where it does appear, our modelshould simply fail to get one of the cognate groups,instead explaining all of them via innovation.
Torepair this shortcoming, we can simply run the ag-glomerative clustering procedure for the model ofHall and Klein (2010), starting from the groups thatPARSIM has recovered.
Using this procedure, wecan hopefully recover many of the under-groupingscaused by homoplasy.6 Inference and Scale6.1 InferenceIn this section we describe the basics of infer-ence in the PARSIM model.
We have a nearlytree-structured graphical model (Figure 1); it isnot a tree only because of the innovation param-eters.
Therefore, we apply the common trick ofgrouping variables to form a tree.
Specifically, wegroup each word variable W` with its innovationparameter I`.
The distribution of interest is thenp(W`, I`|Wpar(`), ?`, ?`), and the primary operationis summing out messages ?
from the children of alanguage and sending a new message to its parent:?`(wpar(`)) =?w`p(w`|?)?`?
?
child(`)?`?(w`)p(w`|?)
= p(w`|I` = 0, wpar(`), ?`)p(I` = 0|?`)+ p(w`|I` = 1, ?`)P (I` = 1|?`) (1)The first term involves computing the probability ofthe word mutating from its parent, and the secondinvolves the probability of the child word from a lan-guage model.
We describe the parameters and pro-cedures for these operations in 7.1.6.2 ScaleEven though inference by message-passing in ourmodel is tractable, we needed to make certain con-cessions to make inference acceptably fast.
Thesechoices mainly affect how we represent distributionsover strings.349First, we need to model distributions and mes-sages over words on the internal nodes of a phy-logeny.
The natural choice in this scenario is to useweighted finite automata (Mohri et al, 1996).
Au-tomata have been used to successfully model distri-butions of strings for inferring morphology (Dreyerand Eisner, 2009) as well as cognate detection (Halland Klein, 2010).
Even in models that would betractable with ?ordinary?
messages, inference withautomata quickly becomes intractable, because thesize of the automata grow exponentially with thenumber of messages passed.
Therefore, approxima-tions must be used.
Dreyer and Eisner (2009) useda mixture of a k-best list and a unigram languagemodel, while Hall and Klein (2010) used an approx-imation procedure that projected complex automatato simple, tractable automata using a modified KLdivergence.While either approach could be used here in prin-ciple, we found that automata machinery was simplytoo slow for our application.
Instead, we exploit theintuition that we do not need to accurately recon-struct the word for any ancestral language.
More-over, it is inefficient to keep track of probabilities forall strings.
Therefore, we only track scores for wordsthat actually exist in a given gloss, which means thatinternal nodes only have mass on those words.
Thatis, if a gloss has 10 distinct words across all the lan-guages in our dataset, we pass messages that onlycontain information about those 10 words.Now, this representation?while more efficientthan the automata representations?results in infer-ence that is still quadratic in the number of wordsin a gloss, since we have distributions of the formp(w`|wpar(`), ?`).
Intuitively, it is unlikely that aword from one distant branch of tree resembles aword in another branch.
Therefore, rather than scoreall of these unlikely words, we use a beam where weonly factor in words whose score is at most a fac-tor of e?10 less than the maximum score.
Our initialexperiments found that using a beam provides largesavings in time with little impact on prediction qual-ity.7 LearningPARSIM has three kinds of parameters that we needto learn: the mutation parameters ?`, the innovationprobabilities ?`, and the global language model ?for generating new words.
We learn these parame-ters via Expectation Maximization (Dempster et al,1977), iterating between computing expected countsand adjusting parameters to maximize the posteriorprobability of the parameters.
In this section, we de-scribe those parameters.7.1 Sound LawsThe core piece of our system is learning the soundlaws associated with each edge.
Since the founda-tion of historical linguists with the neogrammari-ans, linguists have argued for the regularity of soundchange at the phonemic level (Schleicher, 1861;Bloomfield, 1938).
That is to say, if in some lan-guage a /t/ changes to a /d/ in some word, it is al-most certain that it will change in every other placethat has the same surrounding context.In practice, of course, sound change is not entirelyregular, and complex extralinguistic events can leadto sound changes that are irregular.
For example,in some cultures in which Oceanic languages arespoken, the name of the chief is taboo: one cannotspeak his name, nor say any word that sounds toomuch like his name.
Speakers of these languagesdo find ways around this prohibition, often resultingin sound changes that cannot be explained by soundlaws alone (Keesing and Fifi?i, 1969).Nevertheless, we find it useful to model soundchange as a largely regular if stochastic process.We employ a sound change model whose expressivepower is equivalent to that of Hall and Klein (2010),though with a different parameterization.
We modelthe evolution of a word w` to its child w`?
as asequence of unigram edits that include insertions,deletions, and substitutions.
Specifically, we use astandard three-state pair hidden Markov model thatis closely related to the classic alignment algorithmof Needleman and Wunsch (1970) (Durbin et al,2006).The three states in this HMM correspond tomatches/substitutions, insertions, and deletions.
Thetransitions are set up such that insertions and dele-tions cannot be interleaved.
This prevents spuriousequivalent alignments, which would cause the modelto assign unnecessarily higher probability to transi-tions with many insertions and deletions.Actually learning these parameters involves learn-350ing the transition probabilities of this HMM (whichmodel the overall probability of insertion and dele-tion) as well as the emission probabilities (whichmodel the particular edits).
Because there are rel-atively few words for each language (96 on averagein Oceanic), we found it important to tie togetherthe parameters for the various languages, in contrastto Hall and Klein (2010) who did not.
In our maxi-mization step, we fit a joint log-linear model for eachlanguage, using features that are both specific to alanguage and shared across languages.
Our featuresincluded indicators on each substitution, insertion,and deletion operation, along with an indicator forthe outcome of each edit operation.
This last fea-ture reflects the propensity of a particular phonemeto appear in a given language at all, no matter whatits ancestral phoneme was.
This parameterizationis similar to the one used in the reconstruction sys-tem of Bouchard-Co?te?
et al (2009), except that theyused edit operations that conditioned on the contextof the surrounding word, which is crucial when try-ing to accurately reconstruct ancestral word forms.To encourage parameter sharing, we used an `2 reg-ularization penalty.7.2 Innovation ParametersThe innovation parameters ?` are parameters forsimple Bernoulli distribution that govern the propen-sity for a language to start a new word.
These pa-rameters can be learned separately, though due todata sparsity, we found it better to use a tied param-eterization as with the sound laws.
Specifically, wefit a log linear model whose features are indicatorson the specific language, as well as a global inno-vation parameter that is shared across all languages.As with the sound laws, we used an `2 regularizationpenalty to encourage the use of the global innovationparameter.7.3 Language ModelFinally, we have a single language model ?
that isalso shared across all languages.
?
is a simple bi-gram language model over characters in the Interna-tional Phonetic Alphabet.
?
is used when generatingnew words either via innovation or from the root ofthe tree.In principle, we could of course have languagemodels specific to each language, but because thereFormosanSystem Prec Recall F1 PurityAgg.
HK10 77.6 83.2 80.0 84.7PARSIM 87.8 71.0 78.5 94.6Combination 85.2 81.3 83.2 92.3OceanicSystem Prec Recall F1 PurityPARSIM 84.4 62.1 71.5 91.8Combination 76.0 73.8 74.9 85.5Table 1: Results on the Formosan and Oceanic fami-lies.
PARSIM is the new parsimony model in this pa-per, Agg.
HK10 is our agglomerative variant of Hall andKlein (2010) and Combination uses PARSIM?s output toseed the agglomerative matcher.
For the agglomerativesystems, we report the point with maximal F1 score, butwe also show precision/recall curves.
(See Figure 4.
)are so few words per language, we found thatbranch-specific language models caused the modelto prefer to innovate at almost every node since thelanguage models could essentially memorize the rel-atively small vocabularies of these languages.8 Experiments8.1 Cognate RecoveryWe ran both PARSIM and our agglomerative ver-sion of HK10 on the Formosan datasets.
For PAR-SIM, we initialized the mutation parameters ?
to amodel that preferred matches to insertions, substi-tutions and deletions by a factor of e3, innovationparameters to 0.5, and the language model to a uni-form distribution over characters.
For the agglomer-ative HK10, we initialized its parameters to the val-ues found by our model.4Based on our observations about homoplasy, wealso considered a combined system where we ranPARSIM, and then seeded the agglomerative cluster-ing algorithm with the clusters found by PARSIM.For evaluation, we report a few metrics.
First,we report cluster purity, which is a kind of pre-cision measure for clusterings.
Specifically, eachcluster is assigned to the cognate group that is themost common cognate word in that group, and thenpurity is computed as the fraction of words that4Attempts to learn parameters directly with the agglomera-tive clustering algorithm were not effective.3510.60.70.80.910.4 0.5 0.6 0.7 0.8 0.9 1PrecisionRecallCombined SystemPARSIMAgg.
HK10Figure 4: Precision/Recall curves for our systems.
TheCombined System starts from PARSIM?s output, so ithas fewer points to plot, and starts from a point withlower precision.
As PARSIM outputs only one result, itis starred.are in a cluster whose gold cognate group matchesthe cognate group of the cluster.
For gold parti-tions G = {G1, G2, .
.
.
, Gg} and found partitionsF = {F1, F2, .
.
.
, Ff}, we have: purity(G,F ) =1N?f maxg |Gg?Ff |.
We also report pairwise pre-cision and recall computed over pairs of words.5 Fi-nally, because agglomerative clustering does not de-fine a natural ?stopping point?
other than when thelikelihood gain decreases to 0?which did not per-form well in our initial tests?we will report botha precision/recall curve, as well the maximum pair-wise F1 obtained by the agglomerative HK10 andthe combined system.The results are in Table 1.
On Formosan, PAR-SIM has much higher precision and purity than ouragglomerative version of HK10 at its highest point,though its recall and F1 suffer somewhat.
Of course,the comparison is not quite fair, since we have se-lected the best possible point for HK10.However, our combination of the two systemsdoes even better.
By feeding our high-precision re-sults into the agglomerative system and sacrificingjust a little precision, our combined system achievesmuch higher F1 scores than either of the systemsalone.Next, we also examined precision and recallcurves for the two agglomerative systems on For-5The main difference between precision and purity is thatpairwise precision is inherently quadratic, meaning that it pe-nalizes mistakes in large groups much more heavily than mis-takes in small groups.mosan, which we have plotted in Figure 4, alongwith the one point output by PARSIM.We then ran PARSIM and the combined systemon the much larger Oceanic dataset.
Performanceon all metrics decreased somewhat, but this is to beexpected since there is so much more data.
As withFormosan, PARSIM has higher precision than thecombined system, but it has much lower recall.8.2 ReconstructionWe also wanted to see how well our cognates couldbe used to actually reconstruct the ancestral forms ofwords.
To do so, we ran a version of Bouchard-Co?te?et al (2009)?s reconstruction system using both thecognate groups PARSIM found in the Oceanic lan-guage family and the gold cognate groups providedby the ABVD.
We then evaluated the average Leven-shtein distance of the reconstruction for each wordto the reconstruction of that word?s Proto-Oceanicancestor provided by linguists.
Our evaluation dif-fers from Bouchard-Co?te?
et al (2009) in that theyaveraged over cognate groups, which does not makesense for our task because there are different cognategroups.
Instead, we average over per-modern-wordreconstruction error.Using this metric, reconstructions using our sys-tem?s cognates are an average of 2.47 edit opera-tions from the gold reconstruction, while with goldcognates the error is 2.19 on average.
This repre-sents an error increase of 12.8%.
To see if therewas some pattern to these errors, we also plotted thefraction of words with each Levenshtein distance forthese reconstructions in Figure 5.
While the plots aresimilar, the automatic cognates exhibit a longer tail.Thus, even with automatic cognates, the reconstruc-tion system can reconstruct words faithfully in manycases, but in a few instances our system fails.9 AnalysisWe now consider some of the errors made by oursystem.
Broadly, there are two kinds of mistakesin a model like ours: those affecting precision andthose affecting recall.9.1 PrecisionMany of our precision errors seem to be due toour somewhat limited model of sound change.
Forinstance, the language Pazeh has two words for3520?0.05?0.1?0.15?0.2?0.25?0.3?0.35?0.4?0?
1?
2?
3?
4?
5?
6?
7?
8?
9?FractionofWordsLevenshtein DistanceAutomatic CognatesGold CognatesFigure 5: Percentage of words with varying levels ofLevenshtein distance from the gold reconstruction.
GoldCognates were hand-annotated by linguists, while Auto-matic Cognates were found by our system.
?to sleep:?
/mudamai/ and /mid@m/.
Somewhatsurprisingly the former word is cognate with Pai-wan /qmereN/ and Saisiat /maPr@m/ while the lat-ter is not.
Our system, however, makes the mistakeof grouping /mid@m/ with the Paiwan and Saisiatwords.
Our system has inferred that the insertions of/u/ and /ai/ (which are required to bring /mudamai/into alignment with the Saisiat and Paiwan words)are less likely than substituting a few vowels and theconsonant /r/ for /d/ (which are required to align/mid@m/).
Perhaps a more sophisticated model ofsound change could correctly learn this relationship.However, a preliminary inspection of the dataseems to indicate that not all of our precision errorsare actually errors, but rather places where the datais insufficiently annotated (and indeed, the ABVD isstill a work in progress).
For instance, consider thewords for ?meat/flesh?
in the Formosan languages:Squliq /hiP/, Bunun /titiP/, Paiwan /seti/, Kavalan/PisiP/, CentralAmi /titi/, Our system groups all ofthese words except for Squliq /hiP/.
However, de-spite these words?
similarity, there are actually threecognate groups here.
One includes Squliq /hiP/ andKavalan /PisiP/, another includes just Paiwan /seti/,and the third includes Bunun /titiP/ and CentralAmi/titi/.
Crucially, these cognate groups do not fol-low the phylogeny closely.
Thus, either there was asignificant amount of borrowing between these lan-guages, or there was a striking amount of homoplasyin Proto-Formosan, or these words are in fact mostlycognate.
While a more thorough, linguistically-informed analysis is needed to ensure that these areactually cognates, we believe that our system, inconjunction with a trained Austronesian specialist,could potentially find many more cognate groups,speeding up the process of completing the ABVD.9.2 RecallOur system can also fail to group words that shouldbe grouped.
One recurring problem seems tobe reduplication, which is a fairly common phe-nomenon in Austronesian languages.
For instance,there is a cognate group for ?to eat?
that includesBunun /maun/, Thao /kman/, Favorlang /man/, andSediq /manakamakan/, among others.
Our systemcorrectly finds this group, with the exception of/manakamakan/, which is clearly the result of redu-plication.
Reduplication cannot be modeled usingmere sound laws, and so a more complex transitionmodel is needed to correctly identify these kinds ofchanges.10 ConclusionWe have presented a new system for automaticallyfinding cognates across many languages.
Our sys-tem is comprised of two parts.
The first, PAR-SIM, is a new high-precision generative model withtractable inference.
The second, HK10, is a mod-ification of Hall and Klein (2010) that makes theirapproximate inference more efficient.
We discusscertain trade-offs needed to make both models scale,and demonstrated its performance on the Formosanand Oceanic language families.ReferencesShane Bergsma and Greg Kondrak.
2007.
Multilingualcognate identification using integer linear program-ming.
In RANLP Workshop on Acquisition and Man-agement of Multilingual Lexicons, Borovets, Bulgaria,September.Leonard Bloomfield.
1938.
Language.
Holt, New York.R.
A. Blust.
2009.
The Austronesian languages.
Aus-tralian National University.Alexandre Bouchard-Co?te?, Percy Liang, Thomas Grif-fiths, and Dan Klein.
2007.
A probabilistic approachto diachronic phonology.
In EMNLP.Alexandre Bouchard-Co?te?, Thomas L. Griffiths, and DanKlein.
2009.
Improved reconstruction of protolan-guage word forms.
In NAACL, pages 65?73.L.
L. Cavalli-Sforza and A. W. F. Edwards.
1965.
Analy-sis of human evolution.
In S. J. Geerts Genetics Today,353editor, Proceedings of XIth International Congress ofGenetics, 1963, Vol, page 923?933.
3, 3.Hal Daume?
III and Lyle Campbell.
2007.
A Bayesianmodel for discovering typological implications.
InConference of the Association for Computational Lin-guistics (ACL).Hal Daume?
III.
2009.
Non-parametric Bayesian areallinguistics.
In NAACL.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum Likelihood from Incomplete Data via theEM Algorithm.
Journal of the Royal Statistical Soci-ety.
Series B (Methodological), 39(1):1?38.Markus Dreyer and Jason Eisner.
2009.
Graphical mod-els over multiple strings.
In EMNLP, Singapore, Au-gust.R.
Durbin, S. Eddy, A. Krogh, and G. Mitchison.
2006.Biological sequence analysis.
eleventh edition.James S. Farris.
1973.
On Comparing the Shapes ofTaxonomic Trees.
Systematic Zoology, 22(1):50?54,March.J.
Felsenstein.
1973.
Maximum likelihood and mini-mum steps methods for estimating evolutionnary treesfrom data on discrete characters.
Systematic Zoology,23:240?249.W.
M. Fitch.
1971.
Toward defining the course of evo-lution: minimal change for a specific tree topology.Systematic Zoology, 20:406?416.S.J.
Greenhill, R. Blust, and R.D.
Gray.
2008.
TheAustronesian basic vocabulary database: from bioin-formatics to lexomics.
Evolutionary Bioinformatics,4:271?283.David Hall and Dan Klein.
2010.
Finding cognates usingphylogenies.
In Association for Computational Lin-guistics (ACL).Robert M. Keesing and Jonathan Fifi?i.
1969.
Kwaioword tabooing in its cultural context.
Journal of thePolynesian Society, 78(2):154?177.Grzegorz Kondrak.
2001.
Identifying cognates by pho-netic and semantic similarity.
In NAACL.Harold W. Kuhn.
1955.
The Hungarian method for theassignment problem.
Naval Research Logistics Quar-terly, 2:83?97.John B. Lowe and Martine Mazaudon.
1994.
The re-construction engine: a computer implementation ofthe comparative method.
Computational Linguistics,20(3):381?417.Gideon S. Mann and David Yarowsky.
2001.
Multipathtranslation lexicon induction via bridge languages.
InNAACL.Mehryar Mohri, Fernando Pereira, and Michael Riley.1996.
Weighted automata in text and speech process-ing.
In ECAI-96 Workshop.
John Wiley and Sons.Andrea Mulloni.
2007.
Automatic prediction of cognateorthography using support vector machines.
In ACL,pages 25?30.Saul B. Needleman and Christian D. Wunsch.
1970.
Ageneral method applicable to the search for similaritiesin the amino acid sequence of two proteins.
Journal ofMolecular Biology, 48(3):443 ?
453.John Nerbonne.
2010.
Measuring the diffusion of lin-guistic change.
Philosophical Transactions of theRoyal Society B: Biological Sciences.J.
Nichols.
1992.
Linguistic diversity in space and time.University of Chicago Press.Michael P. Oakes.
2000.
Computer estimation of vocab-ulary in a protolanguage from word lists in four daugh-ter languages.
Quantitative Linguistics, 7(3):233?243.Don Ringe, Tandy Warnow, and Ann Taylor.
2002.
Indo-european and computational cladistics.
Transactionsof the Philological Society, 100(1):59?129.D.
Sankoff and R. J. Cedergren, 1983.
Simultaneuouscomparison of three or more sequences related by atree, page 253?263.
Addison-Wesley, Reading, MA.August Schleicher.
1861.
A Compendium of the Com-parative Grammar of the Indo-European, Sanskrit,Greek and Latin Languages.354
