Exploiting auxiliary distributions in stochastic unification-basedgrammarsMark  Johnson*Cognitive and Linguistic SciencesBrown UniversityMark_Johnson@Brown.eduSte fan  R iez le rInst i tut  fiir Maschinelle SprachverarbeitungUniversit?t Stut tgartriezler~ims.uni-stuttgart.deAbst rac tThis paper describes a method for estimat-ing conditional probability distributions overthe parses of "unification-based" grammarswhich can utilize auxiliary distributions thatare estimated by other means.
We show howthis can be used to incorporate informationabout lexical selectional preferences gatheredfrom other sources into Stochastic "Unification-based" Grammars (SUBGs).
While we ap-ply this estimator to a Stochastic Lexical-Functional Grammar, the method is general,and should be applicable to stochastic versionsof HPSGs, categorial grammars and transfor-mational grammars.1 In t roduct ion"Unification-based" Grammars (UBGs) cancapture a wide variety of linguistically impor-tant syntactic and semantic onstraints.
How-ever, because these constraints can be non-localor context-sensitive, developing stochastic ver-sions of UBGs and associated estimation pro-cedures is not as straight-forward as it is for,e.g., PCFGs.
Recent work has shown how todefine probability distributions over the parsesof UBGs (Abney, 1997) and efficiently estimateand use conditional probabilities for parsing(Johnson et al, 1999).
Like most other practicalstochastic grammar estimation procedures, thislatter estimation procedure requires a parsedtraining corpus.Unfortunately, large parsed UBG corpora arenot yet available.
This restricts the kinds ofmodels one can realistically expect to be ableto estimate.
For example, a model incorporat-ing lexical selectional preferences of the kind* This research was supported by NSF awards 9720368,9870676 and 9812169.described below might have tens or hundredsof thousands of parameters, which one couldnot reasonably attempt o estimate from a cor-pus with on the order of a thousand clauses.However, statistical models of lexical selec-tional preferences can be estimated from verylarge corpora based on simpler syntactic struc-tures, e.g., those produced by a shallow parser.While there is undoubtedly disagreement be-tween these simple syntactic structures and thesyntactic structures produced by the UBG, onemight hope that they are close enough for lexicalinformation gathered from the simpler syntacticstructures to be of use in defining a probabilitydistribution over the UBG's structures.In the estimation procedure described here,we call the probability distribution estimatedfrom the larger, simpler corpus an auxiliary dis-tribution.
Our treatment of auxiliary distribu-tions is inspired by the treatment of referencedistributions in Jelinek's (1997) presentation ofMaximum Entropy estimation, but in our es-timation procedure we simply regard the loga-r ithm of each auxiliary distribution as another(real-valued) feature.
Despite its simplicity, ourapproach seems to offer several advantages overthe reference distribution approach.
First, itis straight-forward to utilize several auxiliarydistributions simultaneously: each is treated asa distinct feature.
Second, each auxiliary dis-tribution is associated with a parameter whichscales its contribution to the final distribution.In applications such as ours where the auxiliarydistribution may be of questionable relevanceto the distribution we are trying to estimate, itseems reasonable to permit the estimation pro-cedure to discount or even ignore the auxiliarydistribution.
Finally, note that neither Jelinek'snor our estimation procedures require that anauxiliary or reference distribution Q be a prob-154ability distribution; i.e., it is not necessary thatQ(i2) -- 1, where f~ is the set of well-formedlinguistic structures.The rest of this paper is structured as fol-lows.
Section 2 reviews how exponential mod-els can be defined over the parses of UBGs,gives a brief description of Stochastic Lexical-Functional Grammar, and reviews why maxi-mum pseudo-likelihood estimation is both feasi-ble and sufficient of parsing purposes.
Section 3presents our new estimator, and shows how itis related to the minimization of the Kullback-Leibler divergence between the conditional es-t imated and auxiliary distributions.
Section 4describes the auxiliary distribution used in ourexperiments, and section 5 presents the resultsof those experiments.2 S tochast i c  Un i f i ca t ion -basedGrammarsMost of the classes of probabilistic languagemodels used in computational linguistic are ex-ponential families.
That is, the probability P(w)of a well-formed syntactic structure w E ~ is de-fined by a function of the formPA(w) = Q(~v) eX.f(oj ) (1)where f (w) E R m is a vector of feature values,)~ E It m is a vector of adjustable feature param-eters, Q is a function of w (which Jelinek (1997)calls a reference distribution when it is not an in-dicator function), and ZA = fn Q(w) ex'f(~)dw isa normalization factor called the partition func-tion.
(Note that a feature here is just a real-valued function of a syntactic structure w; toavoid confusion we use the term "attribute" torefer to a feature in a feature structure).
IfQ(w) = 1 then the class of exponential dis-tributions is precisely the class of distributionswith maximum entropy satisfying the constraintthat the expected values of the features is a cer-tain specified value (e.g., a value estimated fromtraining data), so exponential models are some-times also called "Maximum Entropy" models.For example, the class of distributions ob-tained by varying the parameters of a PCFGis an exponential family.
In a PCFG each ruleor production is associated with a feature, so mis the number of rules and the j th  feature valuef j  (o.,) is the number of times the j rule is usedin the derivation of the tree w E ~.
Simple ma-nipulations how that P,x (w) is equivalent to thePCFG distribution ifAj = logpj, where pj is therule emission probability, and Q(w) = Z~ = 1.If the features atisfy suitable Markovian in-dependence constraints, estimation from fullyobserved training data is straight-forward.
Forexample, because the rule features of a PCFGmeet "context-free" Markovian independenceconditions, the well-known "relative frequency"estimator for PCFGs both maximizes the likeli-hood of the training data (and hence is asymp-totically consistent and efficient) and minimizesthe Kullback-Leibler divergence between train-ing and estimated istributions.However, the situation changes dramaticallyif we enforce non-local or context-sensitive con-straints on linguistic structures of the kind thatcan be expressed by a UBG.
As Abney (1997)showed, under these circumstances the relativefrequency estimator is in general inconsistent,even if one restricts attention to rule features.Consequently, maximum likelihood estimationis much more complicated, as discussed in sec-tion 2.2.
Moreover, while rule features are natu-ral for PCFGs given their context-free indepen-dence properties, there is no particular easonto use only rule features in Stochastic UBGs(SUBGs).
Thus an SUBG is a triple (G, f,  A),where G is a UBG which generates a set of well-formed linguistic structures i2, and f and A arevectors of feature functions and feature param-eters as above.
The probability of a structurew E ~ is given by (1) with Q(w) = 1.
Given abase UBG, there are usually infinitely many dif-ferent ways of selecting the features f to makea SUBG, and each of these makes an empiricalclaim about the class of possible distributionsof structures.2.1 S tochast i c  Lexica l  Funct iona lGrammarStochastic Lexical-Functional Grammar(SLFG) is a stochastic extension of Lexical-Functional Grammar (LFG), a UBG formalismdeveloped by Kaplan and Bresnan (1982).Given a base LFG, an SLFG is constructedby defining features which identify salientconstructions in a linguistic structure (in LFGthis is a c-structure/f-structure pair and itsassociated mapping; see Kaplan (1995)).
Apartfrom the auxiliary distributions, we based our155features on those used in Johnson et al (1999),which should be consulted for further details.Most of these feature values range over thenatural numbers, counting the number of timesthat a particular construction appears in alinguistic structure.
For example, adjunct andargument features count the number of adjunctand argument attachments, permitting SLFGto capture a general argument attachment pref-erence, while more specialized features countthe number of attachments oeach grammaticalfunction (e.g., SUB J, OBJ,  COMP, etc.
).The flexibility of features in stochastic UBGspermits us to include features for relativelycomplex constructions, such as date expres-sions (it seems that date interpretations, ifpossible, are usually preferred), right-branchingconstituent structures (usually preferred) andnon-parallel coordinate structures (usuallydispreferred).
Johnson et al remark that theywould have liked to have included features forlexical selectional preferences.
While such fea-tures are perfectly acceptable in a SLFG, theyfelt that their corpora were so small that thelarge number of lexical dependency parameterscould not be accurately estimated.
The presentpaper proposes a method to address this byusing an auxiliary distribution estimated froma corpus large enough to (hopefully) providereliable estimates for these parameters.2.2 Estimating stochasticunification-based grammarsSuppose ~ = Wl,...,Wn is a corpus of n syn-tactic structures.
Letting fj(fJ) = ~--~=1 fj(oJi)and assuming each wi E 12, the likelihood of thecorpus L~(&) is:T~L~(~) = 1-I Px(w,)i=1= e ~/(c~) Z-~ n (2)0logan(&) = fj(Co) - -  nEa(fj) (3) 0Ajwhere E~(fj) is the expected value of f~ un-der the distribution P~.
The maximum likeli-hood estimates are the )~ which maximize (2), orequivalently, which make (3) zero, but as John-son et al (1999) explain, there seems to be nopractical way of computing these for realisticSUBGs since evaluating (2) and its derivatives(3) involves integrating over all syntactic struc-tures ft.However, Johnson et al observe that parsingapplications require only the conditional prob-ability distribution P~(wly), where y is the ter-minal string or yield being parsed, and that thiscan be estimated by maximizing the pseudo-likelihood of the corpus PL~(SJ):rzPLx(SJ) = I I  P~(wilyi)i=-In= eA'f(w) ~I Z;  l(yi)i=1In (4), Yi is the yield of wi andZ~(yi) = f~(y,) e~I(~)dw,(4)where f~(Yi) is the set of all syntactic structuresin f~ with yield yi (i.e., all parses of Yi gener-ated by the base UBG).
It turns out that cal-culating the pseudo-likelihood f a corpus onlyinvolves integrations over the sets of parses ofits yields f~(Yi), which is feasible for many inter-esting UBGs.
Moreover, the maximum pseudo-likelihood estimator isasymptotically consistentfor the conditional distribution P(w\]y).
For thereasons explained in Johnson et al (1999) we ac-tually estimate )~ by maximizing a regularizedversion of the log pseudo-likelihood (5), whereaj is 7 times the maximum value of fj found inthe training corpus:m ~2logPL~(~) - ~ 2"~2 (5)j= l  v jSee Johnson et al (1999) for details of the calcu-lation of this quantity and its derivatives, andthe conjugate gradient routine used to calcu-late the )~ which maximize the regularized logpseudo-likelihood f the training corpus.3 Aux i l i a ry  d i s t r ibut ionsWe modify the estimation problem presented insection 2.2 by assuming that in addition to thecorpus ~ and the m feature functions f we aregiven k auxiliary distributions Q1,.
.
.
,  Qk whosesupport includes f~ that we suspect may be re-lated to the joint distribution P(w) or condi-tional distribution P(wly ) that we wish to esti-156mate.
We do not require that the Qj be proba-bility distributions, i.e., it is not necessary thatf~ Qj(w)dw = 1, but we do require that theyare strictly positive (i.e., Qj(w) > O, Vw E ~).We define k new features fro+l,..., fm+k wherefm+j(w) = log Qj(w), which we call auxiliaryfeatures.
The m + k parameters associated withthe resulting m+k features can be estimated us-ing any method for estimating the parametersof an exponential family with real-valued fea-tures (in our experiments we used the pseudo-likelihood estimation procedure reviewed in sec-tion 2.2).
Such a procedure stimates parame-ters )~m+l,.-., Am+k associated with the auxil-iary features, so the estimated istributions takethe form (6) (for simplicity we only discuss jointdistributions here, but the treatment of condi-tional distributions i parallel).P,(w) = I'Ik=l QJ(w)A~+J eZ_,~=lAjlj(~)(6 ) v - - ~Note that the auxiliary distributions Qj aretreated as fixed distributions for the purposesof this estimation, even though each Qj may it-self be a complex model obtained via a previousestimation process.
Comparing (6) with (1) onpage 2, we see that the two equations becomeidentical if the reference distribution Q in (1) isreplaced by a geometric mixture of the auxiliarydistributions Qj, i.e., if:kQ(w) = ~I Q~(w) xm+i-j= lThe parameter associated with an auxiliary fea-ture represents he weight of that feature in themixture.
If a parameter ~m+j = 1 then thecorresponding auxiliary feature Qj is equivalentto a reference distribution in Jelinek's sense,while if ~m+j = 0 then Qj is effectively ig-nored.
Thus our approach can be regarded asa smoothed version Jelinek's reference distribu-tion approach, generalized to permit multipleauxiliary distributions.4 Lex ica l  se lec t iona l  p re ferencesThe auxiliary distribution we used here is basedon the probabilistic model of lexical selectionalpreferences described in Rooth et al (1999).
Anexisting broad-coverage parser was used to findshallow parses (compared to the LFG parses)for the 117 million word British National Cor-pus (Carroll and Rooth, 1998).
We based ourauxiliary distribution on 3.7 million (g, r, a) tu-ples (belonging to 600,000 types) we extractedthese parses, where g is a lexical governor (forthe shallow parses, g is either a verb or a prepo-sition), a is the head of one of its NP argumentsand r is the the grammatical relationship be-tween the governor and argument (in the shal-low parses r is always OBJ for prepositional gov-ernors, and r is either SUBJ or OBJ for verbalgovernors).In order to avoid sparse data problems wesmoothed this distribution over tuples as de-scribed in (Rooth et al, 1999).
We assume thatgovernor-relation pairs (g, r) and arguments aare independently generated from 25 hiddenclasses C, i.e.
:P((g,r,a)) = ~'~ Pe((g,r)lc)~)e(alc)ee(c)cECwhere the distributions Pe are estimated fromthe training tuples using the Expectation-Maximization algorithm.
While the hiddenclasses axe not given any prior interpretationthey often cluster semantically coherent pred-icates and arguments, as shown in Figure 1.The smoothing power of a clustering model suchas this can be calculated explicitly as the per-centage of possible tuples which are assigned anon-zero probability.
For the 25-class modelwe get a smoothing power of 99%, comparedto only 1.7% using the empirical distribution ofthe training data.5 Empi r i ca l  eva luat ionHadar Shemtov and Ron Kaplan at Xerox PARCprovided us with two LFG parsed corpora calledthe Verbmobil corpus and the Homecentre cor-pus.
These contain parse forests for each sen-tence (packed according to scheme described inMaxwell and Kaplan (1995)), together with amanual annotation as to which parse is cor-rect.
The Verbmobil corpus contains 540 sen-tences relating to appointment planning, whilethe Homecentre corpus contains 980 sentencesfrom Xerox documentation their "homecen-tre" multifunction devices.
Xerox did not pro-vide us with the base LFGs for intellectual prop-erty reasons, but from inspection of the parses157Class  16PROB 0.0340 o?5 d d ~ c ;d  ?~ d dd  ~ d d d?5?~ d ?5~ ~d~dd dd  ?~d ?~I .0.3183 say :s  \] ?
?0 .0405 say :o  i ?
?0 .0345 ask:s  ?0.0276 te l l : s  ?
?0 .0214 be:s  ?0 .0193 know:s  ?0 .0147 h&ve:s0.0144 nod:s  ?0 .0137 th lnk :s  ?0 .0130 shake:s  ?0.0128 take :s  ?0 .0104 rep ly :s  ?0 .0096 smi le :s  ?10.0094 do:s0.0094 laugh:s  ?0.0089 te lho0.0084 saw:s  ?~ 0.0082 add:s  ?0.0078 feehs ?0.0071 make:s  ?0.0070 g ive:s  ?
?0 .0067 ask :o  ?0.0066 shrug:s  ?0 .0061 exp la in :s  ?
?0 .0051 l ike:s ?0 .0050 Iook:s0 .0050 s igh:s  ?0 .0049 watch :s  ?0 .0049 hear :s0.0047 answer :s  ??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
o ?
?
?
?
?
?
?
?
?
?
: : : : : : : : : : : : : : : : : : : : : : : : : : : :?
?
Q ?
?
?
?
?
o ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
o ??
?
?
?
?
?
?
?
o ?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
o ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
o ?
?
?
?
?
?
??
?
?
o ?
?
?
?
?
?
?
?
?
?
?
?
o ?
o ?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
o ?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
o ?
?
?
?
o ?.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: : : : : : : :?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
6 ?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
o ?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
: : : : : : : : : : : : : : : : : : : : : : : : : : : :?
?
?
?
?
?
?
?
?
?
?
o o ?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
Q t ?
?
?
??
?
?
?
o ?
?
?
?
?
?
?
?
?
?
?
?
Q ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
o ?
?
?
?
?
?
?
?
?
?
?
?
?
?Figure 1: A depiction of the highest probability predicates and arguments in Class 16.
The classmatrix shows at the top the 30 most probable nouns in the Pe (a116) distribution and their probabil-ities, and at the left the 30 most probable verbs and prepositions listed according to Pre((g, r)116)and their probabilities.
Dots in the matrix indicate that the respective pair was seen in the trainingdata.
Predicates with suffix : s indicate the subject slot of an intransitive or transitive verb; thesuffix : o specifies the nouns in the corresponding row as objects of verbs or prepositions.it seems that slightly different grammars wereused with each corpus, so we did not merge thecorpora.
We chose the features of our SLFGbased solely on the basis of the Verbmobil cor-pus, so the Homecentre corpus can be regardedas a held-out evaluation corpus.We discarded the unambiguous sentences ineach corpus for both training and testing (asexplained in Johnson et al (1999), pseudo-likelihood estimation ignores unambiguous en-tences), leaving us with a corpus of 324 am-biguous sentences in the Verbmobil corpus and481 sentences in the Homecentre corpus; thesesentences had a total of 3,245 and 3,169 parsesrespectively.The (non-auxiliary) features used in werebased on those described by Johnson etal.
(1999).
Different numbers of featureswere used with the two corpora becausesome of the features were generated semi-automatically (e.g., we introduced a feature forevery attribute-value pair found in any featurestructure), and "pseudo-constant" features (i.e.,features whose values never differ on the parsesof the same sentence) are discarded.
We used172 features in the SLFG for the Verbmobil cor-pus and 186 features in the SLFG for the Home-centre corpus.We used three additional auxiliary featuresderived from the lexical selectional preferencemodel described in section 4.
These were de-fined in the following way.
For each governingpredicate g, grammatical relation r and argu-ment a, let n(g,r,a)(w) be the number of timesthat the f-structure:PRED ~ g \]r = \[PRED=a\]appears as a subgraph of the f-structure ofw, i.e., the number of times that a fills the158grammatical role r of g. We used the lexicalmodel described in the last section to estimateP(alg , r), and defined our first auxiliary featureas :ft(w) = logP(g0) + Z n(g,r,a)(w)l?gP(al g,r)(g,r,a)where g0 is the predicate of the root featurestructure.
The justification for this feature isthat if f-structures were in fact a tree, ft(w)would be the (logarithm of) a probability dis-tribution over them.
The auxiliary feature ftis defective in many ways.
Because LFG f-structures are DAGs with reentrancies ratherthan trees we double count certain arguments,so ft is certainly not the logarithm of a prob-ability distribution (which is why we stressedthat our approach does not require an auxiliarydistribution to be a distribution).The number of governor-argument tuplesfound in different parses of the same sentencecan vary markedly.
Since the conditional prob-abilities P(alg, r) are usually very small, wefound that ft(w) was strongly related to thenumber of tuples found in w, so the parse withthe smaller number of tuples usually obtains thehigher fl score.
We tried to address this byadding two additional features.
We set fc(w) tobe the number of tuples in w, i.e.
:fc(w) = Z n(g,r,a)(w) ?
(g,r,a)Then we set .Q(w) = h(w)/L(w), i .
e .
, / , (w)  isthe average log probability of a lexical depen-dency tuple under the auxiliary lexical distribu-tion.
We performed our experiments with ft asthe sole auxiliary distribution, and with ft, feand fn as three auxiliary distributions.Because our corpora were so small, we trainedand tested these models using a 10-fold cross-validation paradigm; the cumulative results areshown in Table 1.
On each fold we evaluatedeach model in two ways.
The correct parsesmeasure simply counts the number of test sen-tences for which the estimated model assignsits maximum parse probability to the correctparse, with ties broken randomly.
The pseudo-likelihood measure is the pseudo-likelihood ftest set parses; i.e., the conditional probabilityof the test parses given their yields.
We actu-ally report he negative log of this measure, so asmaller score corresponds to better performancehere.
The correct parses measure is most closelyrelated to parser performance, but the pseudo-likelihood measure is more closely related to thequantity we are optimizing and may be morerelevant to applications where the parser has toreturn a certainty factor associated with eachparse.Table 1 also provides the number of indistin-guishable sentences under each model.
A sen-tence y is indistinguishable with respect o fea-tures f iff f(wc) : f(w'), where wc is the correctparse of y and wc ~ w I E ~(y), i.e., the featurevalues of correct parse of y are identical to thefeature values of some other parse of y.
If asentence is indistinguishable it is not possibleto assign its correct parse a (conditional) prob-ability higher than the (conditional) probabilityassigned to other parses, so all else being equalwe would expect a SUBG with with fewer indis-tinguishable sentences to perform better thanone with more.Adding auxiliary features reduced the alreadylow number of indistinguishable sentences in theVerbmobil corpus by only 11%, while it reducedthe number of indistinguishable sentences in theHomecentre corpus by 24%.
This probably re-flects the fact that the feature set was designedby inspecting only the Verbmobil corpus.We must admit disappointment with theseresults.
Adding auxiliary lexical features im-proves the correct parses measure only slightly,and degrades rather than improves performanceon the pseudo-likelihood measure.
Perhaps thisis due to the fact that adding auxiliary featuresincreases the dimensionality of the feature vec-tor f ,  so the pseudo-likelihood scores with dif-ferent numbers of features are not strictly com-parable.The small improvement in the correct parsesmeasure is typical of the improvement we mightexpect to achieve by adding a "good" non-auxiliary feature, but given the importance usu-ally placed on lexical dependencies in statisticalmodels one might have expected more improve-ment.
Probably the poor performance is duein part to the fairly large differences betweenthe parses from which the lexical dependencieswere estimated and the parses produced by theLFG.
LFG parses are very detailed, and manyambiguities depend on the precise grammatical159Verbmobi l  corpus  (324 sentences, 172 non-auxiliary features)Auxi l iary  features  used Ind is t ingu ishab le  Cor rect  - log PL(none) 9 180 401.3fl 8 183 401.6f,, fc, .f.
8 180.5 404.0Homecentre  corpus (481 sentences, 186 non-auxiliary features)Aux i l ia ry  features  used Ind is t ingu ishab le  Cor rect  - log PL(none) 45 283.25 580.6fl 34 284 580.6f l, f c, f n 34 285 582.2Table h The effect of adding auxiliary lexical dependency features to a SLFG.
The auxiliaryfeatures are described in the text.
The column labelled "indistinguishable" gives the number ofindistinguishable s ntences with respect o each feature set, while "correct" and "- log PL" givethe correct parses and pseudo-likelihood measures respectively.relationship holding between a predicate and itsargument.
It could also be that better perfor-mance could be achieved if the lexical dependen-cies were estimated from a corpus more closelyrelated to the actual test corpus.
For example,the verb feed in the Homecentre corpus is used inthe sense of "insert (paper into printer)", whichhardly seems to be a prototypical usage.Note that overall system performance is quitegood; taking the unambiguous sentences intoaccount he combined LFG parser and statisti-cal model finds the correct parse for 73% of theVerbmobil test sentences and 80% of the Home-centre test sentences.
On just the ambiguoussentences, our system selects the correct parsefor 56% of the Verbmobil test sentences and 59%of the Homecentre test sentences.6 Conc lus ionThis paper has presented a method for incorpo-rating auxiliary distributional information gath-ered by other means possibly from other corporainto a Stochastic "Unification-based" Grammar(SUBG).
This permits one to incorporate de-pendencies into a SUBG which probably can-not be estimated irectly from the small UBGparsed corpora vailable today.
It has the virtuethat it can incorporate several auxiliary dis-tributions imultaneously, and because it asso-ciates each auxiliary distribution with its own"weight" parameter, it can scale the contribu-tions of each auxiliary distribution toward thefinal estimated istribution, or even ignore itentirely.
We have applied this to incorporatelexical selectional preference information intoa Stochastic Lexical-Functional Grammar, butthe technique generalizes to stochastic versionsof HPSGs, categorial grammars and transfor-mational grammars.
An obvious extension ofthis work, which we hope will be persued in thefuture, is to apply these techniques in broad-coverage feature-based TAG parsers.ReferencesSteven P. Abney.
1997.
Stochastic Attribute-Value Grammars.
Computational Linguis-tics, 23(4):597-617.Glenn Carroll and Mats Rooth.
1998.
Valenceinduction with a head-lexicalized PCFG.
InProceedings of EMNLP-3, Granada.Frederick Jelinek.
1997.
Statistical Methods forSpeech Recognition.
The MIT Press, Cam-bridge, Massachusetts.Mark Johnson, Stuart Geman, Stephen Canon,Zhiyi Chi, and Stefan Riezler.
1999.
Estima-tors for stochastic "unification-based" gram-mars.
In The Proceedings of the 37th AnnualConference of the Association for Computa-tional Linguistics, pages 535-541, San Fran-cisco.
Morgan Kaufmann.Ronald M. Kaplan and Joan Bresnan.
1982.Lexical-Functional Grammar: A formal sys-tem for grammatical representation.
In JoanBresnan, editor, The Mental Representationof Grammatical Relations, chapter 4, pages173-281.
The MIT Press.160Ronald M. Kaplan.
1995.
The formal architec-ture of LFG.
In Mary Dalrymple, Ronald M.Kaplan, John T. Maxwell III, and AnnieZaenen, editors, Formal Issues in Lexical-Functional Grammar, number 47 in CSLILecture Notes Series, chapter 1, pages 7-28.CSLI Publications.John T. Maxwell III and Ronald M. Kaplan.1995.
A method for disjunctive constraintsatisfaction.
In Mary Dalrymple, Ronald M.Kaplan, John T. Maxwell III, and AnnieZaenen, editors, Formal Issues in Lexical-Functional Grammar, number 47 in CSLILecture Notes Series, chapter 14, pages 381-481.
CSLI Publications.Mats Rooth, Stefan Riezler, Detlef Prescher,Glenn Carroll,, and Franz Beil.
1999.
Induc-ing a semantically annotated lexicon via EM-based clustering.
In Proceedings of the 37thAnnual Meeting of the Association .for Com-putational Linguistics, San Francisco.
Mor-gan Kaufmann.161
