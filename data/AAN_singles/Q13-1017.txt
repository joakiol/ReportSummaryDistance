Transactions of the Association for Computational Linguistics, 1 (2013) 207?218.
Action Editor: Ben Taskar.Submitted 10/2012; Revised 3/2013; Published 5/2013.
c?2013 Association for Computational Linguistics.Dual Coordinate Descent Algorithms for EfficientLarge Margin Structured PredictionMing-Wei Chang Wen-tau YihMicrosoft ResearchRedmond, WA 98052, USA{minchang,scottyih}@microsoft.comAbstractDue to the nature of complex NLP problems,structured prediction algorithms have beenimportant modeling tools for a wide range oftasks.
While there exists evidence showingthat linear Structural Support Vector Machine(SSVM) algorithm performs better than struc-tured Perceptron, the SSVM algorithm is stillless frequently chosen in the NLP communitybecause of its relatively slow training speed.In this paper, we propose a fast and easy-to-implement dual coordinate descent algorithmfor SSVMs.
Unlike algorithms such as Per-ceptron and stochastic gradient descent, ourmethod keeps track of dual variables and up-dates the weight vector more aggressively.
Asa result, this training process is as efficient asexisting online learning methods, and yet de-rives consistently better models, as evaluatedon four benchmark NLP datasets for part-of-speech tagging, named-entity recognition anddependency parsing.1 IntroductionComplex natural language processing tasks are in-herently structured.
From sequence labeling prob-lems like part-of-speech tagging and named entityrecognition to tree construction tasks like syntacticparsing, strong dependencies exist among the la-bels of individual components.
By modeling suchrelations in the output space, structured output pre-diction algorithms have been shown to outperformsignificantly simple binary or multi-class classi-fiers (Lafferty et al 2001; Collins, 2002; McDonaldet al 2005).Among the existing structured output predictionalgorithms, the linear Structural Support VectorMachine (SSVM) algorithm (Tsochantaridis et al2004; Joachims et al 2009) has shown outstandingperformance in several NLP tasks, such as bilingualword alignment (Moore et al 2007), constituencyand dependency parsing (Taskar et al 2004b; Kooet al 2007), sentence compression (Cohn and La-pata, 2009) and document summarization (Li et al2009).
Nevertheless, as a learning method for NLP,the SSVM algorithm has been less than popular al-gorithms such as the structured Perceptron (Collins,2002).
This may be due to the fact that cur-rent SSVM implementations often suffer from sev-eral practical issues.
First, state-of-the-art imple-mentations of SSVM such as cutting plane meth-ods (Joachims et al 2009) are typically compli-cated.1 Second, while methods like stochastic gradi-ent descent are simple to implement, tuning learningrates can be difficult.
Finally, while SSVM mod-els can achieve superior accuracy, this often requireslong training time.In this paper, we propose a novel optimiza-tion method for efficiently training linear SSVMs.Our method not only is easy to implement, butalso has excellent training speed, competitive withboth structured Perceptron (Collins, 2002) andMIRA (Crammer et al 2005).
When evaluated onseveral NLP tasks, including POS tagging, NER anddependency parsing, this optimization method alsooutperforms other approaches in terms of predictionaccuracy.
Our final algorithm is a dual coordinate1Our algorithm is easy to implement mainly because we usethe square hinge loss function.207descent (DCD) algorithm for solving a structuredoutput SVM problem with a 2-norm hinge loss func-tion.
The algorithm consists of two main compo-nents.
One component behaves analogously to on-line learning methods and updates the weight vectorimmediately after inference is performed.
The othercomponent is similar to the cutting plane methodand updates the dual variables (and the weight vec-tor) without running inference.
Conceptually, thishybrid approach operates at a balanced trade-offpoint between inference and weight update, per-forming better than with either component alone.Our contributions in this work can be summarizedas follows.
Firstly, our proposed algorithm showsthat even for structured output prediction, an SSVMmodel can be trained as efficiently as a structuredPerceptron one.
Secondly, we conducted a carefulexperimental study on three NLP tasks using fourdifferent benchmark datasets.
When compared withprevious methods for training SSVMs (Joachimset al 2009), our method achieves similar perfor-mance using less training time.
When compared tocommonly used learning algorithms such as Percep-tron and MIRA, the model trained by our algorithmperforms consistently better when given the sameamount of training time.
We believe our method canbe a powerful tool for many different NLP tasks.The rest of our paper is organized as follows.We first describe our approach by formally definingthe problem and notation in Sec.
2, where we alsoreview some existing, closely-related structured-output learning algorithms and optimization tech-niques.
We introduce the detailed algorithmic de-sign in Sec.
3.
The experimental comparisons ofvariations of our approach and the existing methodson several NLP benchmark tasks and datasets are re-ported in Sec.
4.
Finally, Sec.
5 concludes the paper.2 Background and Related WorkWe first introduce notations used throughout this pa-per.
An input example is denoted by x and an out-put structure is denoted by y.
The feature vector?
(x,y) is a function defined over an input-outputpair (x,y).
We focus on linear models with predic-tions made by solving the decoding problem:arg maxy?Y(xi)wT?(xi,y).
(1)The set Y(xi) represents all possible (exponentiallymany) structures that can be generated from the ex-ample xi.
Let yi be the true structured label of xi.The difference between the feature vectors of thecorrect label yi and y is denoted as ?yi,y(xi) ??
(xi,yi) ?
?(xi,y).
We define ?
(yi,y) as a dis-tance function between two structures.2.1 Perceptron and MIRAStructured Perceptron First introduced byCollins (2002), the structured Perceptron algorithmruns two steps iteratively: first, it finds the beststructured prediction y for an example with thecurrent weight vector using Eq.
(1); then theweight vector is updated according to the differencebetween the feature vectors of the true label andthe prediction: w ?
w + ?yi,y(xi).
Inspired byFreund and Schapire (1999), Collins (2002) alsoproposed the averaged structured Perceptron, whichmaintains an averaged weight vector throughout thetraining procedure.
This technique has been shownto improve the generalization ability of the model.MIRA The Margin Infused Relaxed Algo-rithm (MIRA), which was introduced by Crammeret al(2005), explicitly uses the notion of margin toupdate the weight vector.
The MIRA updates theweight vector by calculating the step size usingminw12?w ?w0?2S.T.
wT?yi,y(xi) ?
?
(y,yi),?y ?
Hk,where Hk is a set containing the best-k structuresaccording to the weight vector w0.
MIRA is avery popular method in the NLP community and hasbeen applied to NLP tasks like word segmentationand part-of-speech tagging (Kruengkrai et al 2009),NER and chunking (Mejer and Crammer, 2010) anddependency parsing (McDonald et al 2005).2.2 Structural SVMStructural SVM (SSVM) is a maximum marginmodel for the structured output prediction setting.Training SSVM is equivalent to solving the follow-ing global optimization problem:minw?w?22 + Cl?i=1L(xi,yi,w), (2)208where l is the number of labeled examples andL(xi,yi,w) = `(maxy[?
(yi,y)?wT?yxi,y(xi)])The typical choice of ` is `(a) = at.
If t = 2 is used,we refer to the SSVM defined in Eq.
(2) as the L2-Loss SSVM.
If hinge loss (t = 1) is used in Eq.
(2),we refer to it as the L1-Loss SSVM.
Note that thefunction ?
is not only necessary,2 but also enablesus to use more information on the differences be-tween the structures in the training phase.
For ex-ample, using Hamming distance for sequence label-ing is a reasonable choice, as the model can expressfiner distinctions between structures yi and y.When training an SSVM model, we often need tosolve the loss-augmented inference problem,arg maxy?Y(xi)[wT?
(xi,y) + ?(yi,y)].
(3)Note that it is a different inference problem than thedecoding problem in Eq.
(1).Algorithms for training SSVM Cuttingplane (CP) methods (Tsochantaridis et al 2004;Joachims et al 2009) have been the dominantmethod for learning the L1-Loss SSVM.
Eq.
(2)contains an exponential number of constraints.The cutting plane (CP) methods iteratively selecta subset of active constraints for each examplethen solve a sub-problem which contains activeconstraints to improve the model.
CP has provenuseful for solving SSVMs.
For instance, Yu andJoachims (2009) proposed using CP methods tosolve a 1-slack variable formulation, and showedthat solving for a 1-slack variable formulationis much faster than solving the l-slack variableone (Eq.
(2)).
Chang et al(2010) also proposeda variant of cutting plane method for solving theL2-Loss SSVM.
This method uses a dual coordinatedescent algorithm to solve the sub-problems.
Wecall their approach the CPD method.Several other algorithms also aim at solv-ing the L1-Loss SSVM.
Stochastic gradient de-scent (SGD) (Bottou, 2004; Shalev-Shwartz et al2007) is a technique for optimizing general con-vex functions and has been applied to solving the2Without ?
(y,yi) in Eq.
2, the optimal w would be zero.L1-Loss SSVM (Ratliff et al 2007).
Taskar etal.
(2004a) proposed a structured SMO algorithm.Because the algorithm solves the dual formulationof the L1-Loss SSVM, it requires picking a vio-lation pair for each update.
In contrast, becauseeach dual variable can be updated independently inour DCD algorithm, the implementation is relativelysimple.
The extragradient algorithm has also beenapplied to solving the L1-Loss SSVM (Taskar et al2005).
Unlike our DCD algorithm, the extragradientmethod requires the learning rate to be specified.The connections between dual methods and theonline algorithms have been previously discussed.Specifically, Shalev-Shwartz and Singer (2006) con-nects the dual methods to a wide range of onlinelearning algorithms.
In (Martins et al 2010), the au-thors apply similar techniques on L1-Loss SSVMsand show that the proposed algorithm can be fasterthan the SGD algorithm.Exponentiated Gradient (EG) descent (Kivinenand Warmuth, 1995; Collins et al 2008) has re-cently been applied to solving the L1-Loss SSVM.Compared to other SSVM learners, EG requiresmanual tuning of the step size.
In addition, EG re-quires solution of the sum-product inference prob-lem, which can be more expensive than solvingEq.
(3) (Taskar et al 2006).
Very recently, Lacoste-Julien et al(2013) proposed a block-coordinate de-scent algorithm for the L1-Loss SSVM based on theFrank-Wolfe algorithm (FW-Struct), which has beenshown to outperform the EG algorithm significantly.Similar to our DCD algorithm, FW calculates theoptimal learning rate when updating the dual vari-ables.The Sequential Dual Method (SDM) (Shevade etal., 2011) is probably the most related to this paper.SDM solves the L1-Loss SSVM problem using mul-tiple updating policies, which is similar to our ap-proach.
However, there are several important differ-ences in the detailed algorithmic design.
As will beclear in Sec.
3, our dual coordinate descent (DCD)algorithm is very simple, while SDM (which is nota DCD algorithm) uses a complicated procedure tobalance different update policies.
By targeting theL2-Loss SSVM formulation, our methods can up-date the weight vector more efficiently, since thereare no equality constraints in the dual.2093 Dual Coordinate Descent Algorithms forStructural SVMIn this work, we focus on solving the dual of linearL2-Loss SSVM, which can be written as follows:min?i,y?012?
?i,y?i,y?yi,y(xi)?2 (4)+ 14C?i(?y?Y(xi)?i,y)2 ??i,y?
(y,yi)?i,y.In the above equation, a dual variable ?i,y is asso-ciated with a structure y ?
Y(xi).
Therefore, thetotal number of dual variables can be quite large: itsupper bound is lB, where B = maxi |Y(xi)|.The connection between the dual variables andthe weight vector w at optimal solutions is throughthe following equation:w =l?i=1?y?Y(xi)?i,y?yi,y(xi).
(5)Advantages of L2-Loss SSVM The use of the2-norm hinge loss function eliminates the need ofequality constraints3; only non-negative constraints(?i,y ?
0) remain.
This is important because noweach dual variable can be updated without changingvalues of the other dual variables.
We can then up-date one single dual variable at a time.
As a result,this dual formulation allows us to design a simpleand principled dual coordinate descent (DCD) opti-mization method.DCD algorithms consist of two iterative steps:1.
Pick a dual variable ?i,y.2.
Update the dual variable and the weight vector.Go to 1.In the normal binary classification case, how toselect dual variables to solve is not an issue aschoosing them randomly works effectively in prac-tice (Hsieh et al 2008).
However, this is not a prac-tical scheme for training SSVM models given thatthe number of dual variables in Eq.
(4) can be verylarge because of the exponentially many legitimateoutput structures.
To address this issue, we intro-duce the concept of working set below.3For L1-Loss SSVM, there are the equality constraints:?y?Y(xi) ?i,y = C, ?i.Working Set The number of non-zero variables inthe optimal ?
can be small when solving Eq.
(4).Hence, it is often feasible to use a small working setWi for each example to keep track of the structuresfor non-zero ??s.
More formally,Wi = {y | ?y ?
Y(xi), ?i,y > 0}.Intuitively, the working set Wi records the outputstructures that are similar to the true structure yi.Weset aldual variables to be zero initially (therefore,w = 0 as well), soWi = ?
for all i.
Then the algo-rithm starts to build the working set in the trainingprocedure.
After training, the weight vector is com-puted using dual variables in the working set andthus equivalent tow =l?i=1?y?Wi?i,y?yi,y(xi).
(6)Connections to Structured Perceptron The pro-cess of updating a dual variable is in fact very simi-lar to the update rule used in Perceptron and MIRA.Take structured Perceptron for example, its weightvector can be determined using the following equa-tion:wperc =l?i=1?y??
(xi)?i,y?yi,y(xi), (7)where ?
(xi) is the set containing all structures Per-ceptron predicts for xi during training, and ?i,y isthe number of times Perceptron predicts y for xiduring training.
By comparing Eq.
(6) and Eq.
(7), itis clear that SSVM is just a more principled way toupdate the weight vector, as ?
?s are computed basedon the notion of margin.4Updating Dual Variables and Weights Afterpicking a dual variable ?i,y?, we first show how toupdate it optimally.
Recall that a dual variable ?i,y?is associated with the i-th example and a structure y?.The optimal update size d for ?i,y?
can be calculatedanalytically from the following optimization prob-4Of course, Wi could be very different from ?
(xi), the con-struction of the working sets will be discussed in Sec.
3.1.210Algorithm 1 UPDATEWEIGHT(i,w):Update the weight vector w and the dual variablesin the working set of the i-th example.
C is the reg-ularization parameter defined in Eq.
(2).1: Shuffle the elements inWi (but retain the newestmember of the working set to be updated first.See Theorem 1 for the reasons.
)2: for y?
?
Wi do3: d?
?(y?,yi)?wT?yi,y?(xi)?
?y?Wi ?i,y2C??yi,y?
(xi)?2+ 12C4: ??
?
max(?i,y?
+ d, 0)5: w?
w + (??
?
?i,y?)?yi,y?
(xi)6: ?i,y?
?
?
?7: end forlem (derived from Eq.
(4)):mind??
?i,y?12?w + d?yi,y?
(x)?2+14C (d+?y?Wi?i,y)2 ?
d?
(yi, y?
), (8)where the w is defined in Eq.
(6).
Compared tostochastic gradient descent, DCD algorithms keeptrack of dual variables and do not need to tune thelearning rate.Instead of updating one dual variable at a time,our algorithm updates all dual variables once in theworking set.
This step is important for the conver-gence of the DCD algorithms.5 The exact updatealgorithm is presented in Algorithm 1.
Line 3 cal-culates the optimal step size (the analytical solutionto the above optimization problem).
Line 4 makessure that dual variables are non-negative.
Lines 5and 6 update the weight vectors and the dual vari-ables.
Note that every update ensures Eq.
(4) to beno greater than the original value.3.1 Two DCD Optimization AlgorithmsNow we are ready to present two novel DCD algo-rithms for L2-Loss SSVM: DCD-Light and DCD-SSVM.3.1.1 DCD-LightThe basic idea of DCD-Light is just like onlinelearning algorithms.
Instead of doing inference for5Specifically, updating all of the structures in the workingset is a necessary condition for our algorithms to converge.the whole batch of examples before updating theweight vector in each iteration, as done in CPD and1-slack variable formulation of SVM-Struct, DCD-Light updates the model weights after solving the in-ference problem for each individual example.
Algo-rithm 2 depicts the detailed steps.
In Line 5, the loss-augmented inference (Eq.
(3)) is performed; thenthe weight vector is updated in Line 9 ?
all of thestructures and dual variables in the working set areused to update the weight vector.
Note that there isa ?
parameter in Line 6 to control how precise wewould like to solve this SSVM problem.
As sug-gested in (Hsieh et al 2008), we shuffle the exam-ples in each iteration (Line 3) as it helps the algo-rithm converge faster.DCD-Light has several noticeable differenceswhen compared to the most popular online learn-ing method, averaged Perceptron.
First, DCD-Lightperforms the loss-augmented inference (Eq.
(3)) atLine 5 instead of the argmax inference (Eq.
(1)).Second, the algorithm updates the weight vectorwith all structures in the working set.
Finally, DCD-light does not average the weight vectors.3.1.2 DCD-SSVMObserving that DCD-Light does not fully utilizethe saved dual variables in the working set, we pro-pose a hybrid approach called DCD-SSVM, whichcombines ideas from DCD-Light and cutting planemethods.
In short, after running the updates on abatch of examples, we refine the model by solvingthe dual variables further in the current working sets.The key advantage of keeping track of these dualvariables is that it allows us to update the saved dualvariables without performing any inference, whichis often an expensive step in structured predictionalgorithms.DCD-SSVM is summarized in Algorithm 3.Lines 10 to 16 are from DCD-Light.
In Lines 3 to8, we grab the idea from cutting plane methods byupdating the weight vector using the saved dual vari-ables in the working sets without any inference (notethat Lines 3 to 8 do not have any effect at the firstiteration).
By revisiting the dual variables, we canderive a better intermediate model, resulting in run-ning the inference procedure less frequently.
Similarto DCD-Light, we also shuffle the examples in eachiteration.211Algorithm 2 DCD-Light: The lightweight dual co-ordinate descent algorithm for optimizing Eq.
(4).1: w?
0,Wi ?
?,?i2: for t = 1 .
.
.
T do3: Shuffle the order of the training examples4: for i = 1 .
.
.
l do5: y??
arg maxy wT?
(xi,y) + ?
(y,yi)6: if ?(y?,yi)?wT?yi,y?(xi)?
?y?Wi?i,y2C ?
?then7: Wi ?Wi ?
{y?
}8: end if9: UPDATEWEIGHT(i,w) {Algo.
1}10: end for11: end forDCD algorithms are similar to column generationalgorithms for linear programming (Desrosiers andLu?bbecke, 2005), where the master problem is tosolve the dual problem that focuses on the variablesin the working sets, and the subproblem is to findnew variables for the working sets.
In Sec.
4, wewill demonstrate the importance of balancing thesetwo problems by comparing DCD-SSVM and DCD-Light.3.2 Convergence AnalysisWe now present the theoretic analysis of both DCD-Light and DCD-SSVM, and address two main top-ics: (1) whether the working sets will grow expo-nentially and (2) the convergence rate.
Due to thelack of space, we show only the main theorems.Leveraging Theorem 5 in (Joachims et al 2009),we can prove that the DCD algorithms only add alimited number of variables in the working sets, andhave the following theorem.Theorem 1.
The number of times that DCD-Lightor DCD-SSVM adds structures into working sets isbounded by O(2(R2+ 12C )lC?2?2), where R2 is de-fined as maxi,y?
??yi,y?
(xi)?2, and ?
is the upperbound of ?
(yi, y?
), ?yi, y?
?
Y(xi).We discuss next the convergence rates of ourDCD algorithms under two different conditions ?when the working sets are fixed and the general case.If the working sets are fixed in DCD algorithms,they become cyclic dual coordinate descent meth-Algorithm 3 DCD-SSVM: a hybrid dual coor-dinate descent algorithm that combines ideas fromDCD-Light and cutting plane algorithms.1: w?
0,Wi ?
?,?i2: for t = 1 .
.
.
T do3: for j = 1 .
.
.
r do4: Shuffle the order of the training examples5: for i = 1 .
.
.
l do6: UPDATEWEIGHT(i,w) {Algo.
1}7: end for8: end for9: Shuffle the order of the training examples10: for i = 1 .
.
.
l do11: y??
arg maxy wT?
(xi,y) + ?
(y,yi)12: if ?(y?,yi)?wT?yi,y?(xi)?
?y?Wi?i,y2C ?
?then13: Wi ?Wi ?
{y?
}14: end if15: UPDATEWEIGHT(i,w) {Algo.
1}16: end for17: end forods.
In this case, we denote the minimization prob-lem Eq.
(4) as F (?).
For fixed working sets {Wi},we denote FS(?)
as the minimization problem thatfocuses on the dual variables in the working set only.By applying the results from (Luo and Tseng, 1993;Wang and Lin, 2013) to L2-Loss SSVM, we havethe following theorem.Theorem 2.
For any given non-empty working sets{Wi}, if the DCD algorithms do not extend theworking sets (i.e., line 6-8 in Algorithm 2 are not ex-ecuted), then the DCD algorithms will obtain the -optimal solution for FS(?)
in O(log(1 )) iterations.Based on Theorem 1 and Theorem 2, we have thefollowing theorem.Theorem 3.
DCD-SSVM obtains an -optimal solu-tion in O( 12 log(1 )) iterations.To the best of our knowledge, this is the first con-vergence analysis result for L2-Loss SSVM.
Com-pared to other theoretic analysis results for L1-LossSSVM, a tighter bound might exist given a bettertheoretic analysis.
We leave this for future work.66Noticeably, the use of working sets complicates the theo-2120 20 40 60 80 1007880828486Training Time (seconds)TestF1(a) Test F1 vs. Time in NER-CoNLL0 100 200 300 40096.696.89797.2Training Time (seconds)TestAcc(b) Test Acc vs. Time in POS0 2,000 4,000 6,000868890Training Time (seconds)TestAcc(c) Test Acc vs. Time in DP-WSJ0 20 40 60 80 1002,0004,0006,000Training Time (seconds)PrimalObjectiveValue(d) Primal Objective Value in NER-CoNLL0 100 200 300 400 50011.52 ?104Training Time (seconds)PrimalObjectiveValue(e) Primal Objective Value in POS0 2,000 4,000 6,000246 ?104Training Time (seconds)PrimalObjectiveValueDCD-SSVMDCD-LightCPD(f) Primal Objective Value in DP-WSJFigure 1: We plot the testing performance (top row) and the primal objective function (bottom row) versus trainingtime for three optimization methods for learning the L2-Loss SSVM.
In general, DCD-SSVM is the best algorithm forboth the objective function and the testing performance.4 ExperimentsIn order to verify the effectiveness of the proposedalgorithm, we conduct a set of experiments on dif-ferent optimization and learning algorithms.
Beforegoing to the experimental results, we first introducethe tasks and settings used in the experiments.4.1 Tasks and DataWe evaluated our method and existing structuredoutput learning approaches on named entity recog-nition (NER), part-of-speech tagging (POS) and de-pendency parsing (DP) on four benchmark datasets.NER-MUC7 MUC-7 data contains a subset ofNorth American News Text Corpora annotated withmany types of entities.
We followed the settingsin (Ratinov and Roth, 2009) and consider three mainentities categories: PER, LOC and ORG.
We evalu-ated the results using phrase-level F1.retic analysis significantly.
Also note that Theorem 2 showsthat if we put all possible structures in the working sets (i.e.,F (?)
= FS(?
)), then the DCD algorithms can obtain -optimalsolution in O(log( 1 )) iterations.NER-CoNLL This is the English dataset from theCoNLL 2003 shared task (T. K. Sang and De Meul-der, 2003).
The data set labels sentences from theReuters Corpus, Volume 1 (Lewis et al 2004) withfour different entity types: PER, LOC, ORG andMISC.
We evaluated the results using phrase-levelF1.POS-WSJ The standard set for evaluating the per-formance of a part-of-speech tagger.
The training,development and test sets consist of sections 0-18,19-21 and 22-24 of the Penn Treebank data (Marcuset al 1993), respectively.
We evaluated the resultsby token-level accuracy.DP-WSJ We took sections 02-21 of Penn Tree-bank as the training set, section 00 as the develop-ment set and section 23 as the test set.
We imple-ment a simple version of hash kernel to speed up oftraining procedure for this task (Bohnet, 2010).
Wereported the unlabeled attachment accuracy for thistask (McDonald et al 2005).2130 10 20 307677787980Training Time (seconds)TestF1(a) Test F1 vs. Time in NER-MUC70 20 40 60 80 100 12080828486Training Time (seconds)TestF1(b) Test F1 vs. Time in NER-CoNLL0 200 400 600959697Training Time (seconds)TestAccDCD-SSVMFW-StructSVM-Struct(c) Test Acc vs. Time in POS-WSJFigure 2: Comparisons between the testing performance of DCD-SSVM, FW-Struct and SVM-Struct.
Note thatDCD-SSVM often obtain a better model with much less training time when comparing to SVM-Struct.4.2 Features and Inference AlgorithmsFor the sequence labeling tasks, NER and POS,we followed the discriminative HMM settings usedin (Joachims et al 2009) and defined the features as?
(x,y) =N?i=l??????
?emi(xi, yi)[yi = 1][yi?1 = 1][yi = 1][yi?1 = 2].
.
.
[yi = k][yi?1 = k]?????
?,where ?emi is the feature vector dedicated to the i-thtoken (or, the emission features), N represents thenumber of tokens in this sequence, yi represents thei-th token in the sequence y, [yi = 1] is the indictorvariable and k is the number of tags.The inference problems are solved by the Viterbialgorithm.
The emission features used in both POSand NER are the standard ones, including word fea-tures, word-shape features, etc.
For NER, we usedadditional simple gazetteer features7 and word clus-ter features (Turian et al 2010)For dependency parsing, we followed the settingdescribed in (McDonald et al 2005) and used sim-ilar features.
The decoding algorithm is the first-order Eisner?s algorithm (Eisner, 1997).4.3 Algorithms and Implementation DetailFor all SSVM algorithms (including SGD), C waschosen among the set {0.01, 0.05, 0.1, 0.5, 1, 5} ac-cording to the accuracy/F1 on the development set.For each task, the same features were used by all7Adding Wikipedia gazetteers would likely increase the per-formance significantly (Ratinov and Roth, 2009).algorithms.
For NER-MUC7, NER-CoNLL andPOS-WSJ, we ran the online algorithms and DCD-SSVM for 25 iterations.
For DP-WSJ, we only letthe algorithms run for 10 iterations as the inferenceprocedure is very expensive computationally.
Thealgorithms in the experiments are:DCD Our dual coordinate descent method on theL2-Loss SSVM.
For DCD-SSVM, r is set to be 5.For both DCD-Light and DCD-SSVM , we followthe suggestion in (Joachims et al 2009): if the valueof a dual variable becomes zero, its correspondingstructure will be removed from the working set toimprove the speed.SVM-Struct We used the latest (v3.10) of SVM-HMM.8 This version uses the cutting plane methodon a 1-slack variable formulation (Joachims et al2009) for the L1-Loss SSVM.
SVM-Struct was im-plemented in C and all the other algorithms are im-plemented in C#.
We did not apply SVM-Struct toDP-WSJ because there is no native implementation.Perceptron This refers to the averaged structuredPerceptron method introduced by Collins (2002).
Tospeed up the convergence rate, we shuffle the train-ing examples at each iteration.MIRA Margin Infused Relaxed Algorithm(MIRA) (Crammer et al 2005) is the onlinelearning algorithm that explicitly uses the notionof margin to update the weight vector.
We use1-best MIRA in our experiments.
To increase8http://www.cs.cornell.edu/People/tj/svm_light/svm_hmm.html214the convergence speed, we shuffle the trainingexamples at each iteration.
Following (McDonald etal., 2005), we did not tune the C parameter for theMIRA algorithm.SGD Stochastic gradient descent (SGD) (Bottou,2004) is a technique for optimizing general convexfunctions.
In this paper, we use SGD as an alterna-tive baseline for optimizing the L1-Loss SSVM ob-jective function (Eq.
(2) with higne loss).9 When us-ing SGD, the learning rate must be carefully tuned.Following (Bottou, 2004), the learning rate is ob-tained by?0(1.0 + (?0T/C))0.75,where C is the regularization parameter, T is thenumber of updates so far and ?0 is the initial stepsize.
The parameter ?0 was selected among the set{2?1, 2?2, 2?3, 2?4, 2?5} by running the SGD al-gorithm on a set of 1000 randomly sampled exam-ples, and then choosing the ?0 with lowest primalobjective function on these examples.FW-Struct FW-Struct represents the Frank-Wolfealgorithm for the L1-Loss SSVM (Lacoste-Julien etal., 2013).In order to improve the training speed, we cachedall the feature vectors generated by the gold la-beled data once computed.
This applied to all al-gorithms except SVM-Struct, which has its owncaching mechanism.
We report the performanceof the averaged weight vectors for Perceptron andMIRA.4.4 ResultsWe present the experimental results below on com-paring different dual coordinate descent methods,as well as comparing our main algorithm, DCD-SSVM, with other structured learning approaches.4.4.1 Comparisons of DCD MethodsWe compared three DCD methods: DCD-Light,DCD-SSVM and CPD.
CPD is a cutting planemethod proposed by Chang et al(2010), which uses9To compare with SGD using its best setting, we report onlythe results of SGD on the L1-Loss SSVM as we found tuningthe step size for the L2-Loss SSVM is more difficult.a dual coordinate descent algorithm to solve the in-ternal sub-problems.
We specifically included CPDas it also targets at the L2-Loss SSVM.Because different optimization strategies willreach the same objective values eventually, compar-ing them on prediction accuracy of the final modelsis not meaningful.
Instead, here we compare howfast each algorithm converges as shown in Figure 1.Each marker on the line in this figure represents oneiteration of the corresponding algorithm.
Generallyspeaking, CPD improves the model very slowly inthe early stages, but much faster after several iter-ations.
In comparison, DCD-Light often behavesmuch better initially, and DCD-SSVM is generallythe most efficient algorithm here.The reason behind the slow performance of CPDis clear.
During early rounds of the algorithm,the weight vector is far from optimal, so it spendstoo much time using ?bad?
weight vectors to findthe most violated structures.
On the other hand,DCD-Light updates the weight vector more fre-quently, so it behaves much better in general.
DCD-SSVM spends more time on updating models duringeach batch, but keeps the same amount of time doinginference as DCD-Light.
As a result, it finds a bettertrade-off between inference and learning.4.4.2 DCD-SSVM, SVM-Struct and FW-StructJoachims et al(2009) proposed a 1-slack vari-able method for the L1-Loss SSVM.
They showedthat solving a 1-slack variable formulation is anorder-of-magnitude faster than solving the originalformulation (l-slack variables formulation).
Nev-ertheless, from Figure 2, we can see the clear ad-vantage of DCD-SSVM over SVM-Struct.
Al-though using 1-slack variable has improved thelearning speed, SVM-Struct still converges slowerthan DCD-SSVM.
In addition, the performance ofmodels trained by SVM-Struct in the early stage isquite unstable, which makes early stopping an in-effective strategy in practice when training time islimited.We also compared our algorithms to FW-Struct.Our results agree with (Lacoste-Julien et al 2013),which shows that the FW-Struct outperforms theSVM-Struct.
In our experiments, we found that ourDCD algorithms were competitive, sometimes con-verged faster than the FW-Struct.2150 10 20 307677787980Training Time (seconds)TestF1(a) Test F1 vs. Time in NER-MUC70 100 200 300 40096.696.89797.2Training Time (seconds)TestAcc(b) Test Acc vs. Time in POS-WSJ0 2,000 4,000 6,00088899091Training Time (seconds)TestAccDCD-SSVMPERPMIRASGD(c) Test Acc vs. Time in DP-WSJFigure 3: Comparisons between DCD-SSVM and popular online learning algorithms.
Note that the results divergewhen comparing Perceptron and MIRA.
In general, DCD-SSVM is the most stable algorithm.Task/Data DCD Percep MIRA SGDNER-MUC7 79.4 78.5 78.8 77.8NER-CoNLL 85.6 85.3 85.1 84.2POS-WSJ 97.1 96.9 96.9 96.9DP-WSJ 90.8 90.3 90.2 90.9Table 1: Performance of online learning algorithms andthe DCD-SSVM algorithm on the testing sets.
NER ismeasured by F1 while others by accuracy.4.4.3 DCD-SSVM, MIRA, Perceptron andSGDAs in binary classification, large-margin methodslike SVMs often perform better than algorithms likePerceptron and SGD (Hsieh et al 2008; Shalev-Shwartz and Zhang, 2013), here we observe similarbehaviors in the structured output domain.
Table 1shows the final test accuracy numbers or F1 scores ofmodels trained by algorithms including Perceptron,MIRA and SGD, compared to those of the SSVMmodels trained by DCD-SSVM.
Among the bench-mark datasets and tasks we have experimented with,DCD-SSVM derived the most accurate models, ex-cept for DP-WSJ when compared to SGD.Perhaps a more interesting comparison is onthe training speed, which can be observed in Fig-ure 3.
Compared to other online algorithms, DCD-SSVM can take advantage of cached dual variablesand structures.
We show that the training speed ofDCD-SSVM can be competitive to that of the on-line learning algorithms, unlike SVM-Struct.
Notethat SGD is not very stable for NER-MUC7, eventhough we tuned the step size very carefully.5 ConclusionIn this paper, we present a novel approach for learn-ing the L2-Loss SSVM model.
By combining theideas of dual coordinate descent and cutting planemethods, the hybrid approach, DCD-SSVM outper-forms other SSVM training methods both in termsof objective value reduction and testing error ratereduction.
As demonstrated in our experimentson several NLP tasks, our approach also tends tolearn more accurate models than commonly usedstructured learning algorithms, including structuredPerceptron, MIRA and SGD.
Perhaps more inter-estingly, our SSVM learning method is very effi-cient: the model training time is competitive to on-line learning algorithms such as structured Percep-tron and MIRA.
These unique qualities make DCD-SSVM an excellent choice for solving a variety ofcomplex NLP problems.In the future, we would like to compare our algo-rithm to other structured prediction approaches, suchas conditional random fields (Lafferty et al 2001)and exponential gradient descent methods (Collinset al 2008).
Expediting the learning process fur-ther by leveraging approximate inference is also aninteresting direction to investigate.AcknowledgmentsWe sincerely thank John Platt, Lin Xiao and Kaiwei Chang forthe discussions and feedback.
We are grateful to Po-Wei Wangand Chih-Jen Lin for providing their work on convergence rateanalysis on feasible descent methods.
We also thank the review-ers for their detailed comments on this paper.216ReferencesB.
Bohnet.
2010.
Very high accuracy and fast depen-dency parsing is not a contradiction.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics, Proceedings the International Conferenceon Computational Linguistics (COLING).L.
Bottou.
2004.
Stochastic learning.
In Olivier Bous-quet and Ulrike von Luxburg, editors, Advanced Lec-tures on Machine Learning, Lecture Notes in Artifi-cial Intelligence, LNAI 3176, pages 146?168.
SpringerVerlag, Berlin.M.
Chang, V. Srikumar, D. Goldwasser, and D. Roth.2010.
Structured output learning with indirect super-vision.
In Proceedings of the International Conferenceon Machine Learning (ICML).T.
Cohn and M. Lapata.
2009.
Sentence compressionas tree transduction.
Journal of AI Research, 34:637?674, April.M.
Collins, A. Globerson, T. Koo, X. Carreras, and P. L.Bartlett.
2008.
Exponentiated gradient algorithmsfor conditional random fields and max-margin Markovnetworks.
Journal of Machine Learning Research, 9.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In Proceedings of the Confer-ence on Empirical Methods for Natural Language Pro-cessing (EMNLP).K.
Crammer, R. Mcdonald, and F. Pereira.
2005.
Scal-able large-margin online learning for structured clas-sification.
Technical report, Department of Computerand Information Science, University of Pennsylvania.J.
Desrosiers and M. E. Lu?bbecke.
2005.
A primer incolumn generation.
In Column Generation, pages 1?32.
Springer.J.
M. Eisner.
1997.
Three new probabilistic models fordependency parsing: An exploration.
In Proceedingsthe International Conference on Computational Lin-guistics (COLING), pages 340?345.Y.
Freund and R. Schapire.
1999.
Large margin clas-sification using the Perceptron algorithm.
MachineLearning, 37(3):277?296.C.-J.
Hsieh, K.-W. Chang, C.-J.
Lin, S. S. Keerthi, andS.
Sundararajan.
2008.
A dual coordinate descentmethod for large-scale linear SVM.
In Proceedingsof the International Conference on Machine Learning(ICML), New York, NY, USA.
ACM.T.
Joachims, T. Finley, and Chun-Nam Yu.
2009.Cutting-plane training of structural SVMs.
MachineLearning, 77(1):27?59.J.
Kivinen and M. K. Warmuth.
1995.
Exponentiatedgradient versus gradient descent for linear predictors.In ACM Symp.
of the Theory of Computing.T.
Koo, A. Globerson, X. Carreras, and M. Collins.
2007.Structured prediction models via the matrix-tree theo-rem.
In Proceedings of the 2007 Joint Conference ofEMNLP-CoNLL, pages 141?150.C.
Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang,K.
Torisawa, and H. Isahara.
2009.
An error-drivenword-character hybrid model for joint chinese wordsegmentation and pos tagging.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 513?521.S.
Lacoste-Julien, M. Jaggi, M. W. Schmidt, andP.
Pletscher.
2013.
Stochastic block-coordinate Frank-Wolfe optimization for structural SVMs.
In Pro-ceedings of the International Conference on MachineLearning (ICML).J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof the International Conference on Machine Learning(ICML).D.
D. Lewis, Y. Yang, T. Rose, and F. Li.
2004.
RCV1:A new benchmark collection for text categorizationresearch.
Journal of Machine Learning Research,5:361?397.L.
Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu.
2009.Enhancing diversity, coverage and balance for summa-rization through structure learning.
In Proceedings ofthe 18th international conference on World wide web,The International World Wide Web Conference, pages71?80, New York, NY, USA.
ACM.Z.-Q.
Luo and P. Tseng.
1993.
Error bounds and conver-gence analysis of feasible descent methods: A generalapproach.
Annals of Operations Research, 46(1):157?178.M.
P. Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330, June.A.
F. Martins, K. Gimpel, N. A. Smith, E. P. Xing, M. A.Figueiredo, and P. M. Aguiar.
2010.
Learning struc-tured classifiers with dual coordinate ascent.
Technicalreport, Technical report CMU-ML-10-109.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Pro-ceedings of the Annual Meeting of the Association forComputational Linguistics (ACL), pages 91?98, AnnArbor, Michigan.A.
Mejer and K. Crammer.
2010.
Confidence instructured-prediction using confidence-weighted mod-els.
In Proceedings of the 2010 Conference on Em-pirical Methods in Natural Language Processing, Pro-ceedings of the Conference on Empirical Methods forNatural Language Processing (EMNLP), pages 971?981.217R.
C. Moore, W. Yih, and A.
Bode.
2007.
Improved dis-criminative bilingual word alignment.
In Proceedingsof the Annual Meeting of the Association for Compu-tational Linguistics (ACL).L.
Ratinov and D. Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Proc.of the Annual Conference on Computational NaturalLanguage Learning (CoNLL), Jun.N.
Ratliff, J. Andrew (Drew) Bagnell, and M. Zinkevich.2007.
(Online) subgradient methods for structuredprediction.
In Proceedings of the International Work-shop on Artificial Intelligence and Statistics, March.S.
Shalev-Shwartz and Y.
Singer.
2006.
Online learn-ing meets optimization in the dual.
In Proceedings ofthe Annual ACM Workshop on Computational Learn-ing Theory (COLT).S.
Shalev-Shwartz and T. Zhang.
2013.
Stochastic dualcoordinate ascent methods for regularized loss min-imization.
Journal of Machine Learning Research,14:567?599.S.
Shalev-Shwartz, Y.
Singer, and N. Srebro.
2007.
Pe-gasos: primal estimated sub-gradient solver for SVM.In Zoubin Ghahramani, editor, Proceedings of the In-ternational Conference on Machine Learning (ICML),pages 807?814.
Omnipress.S.
Shevade, P. Balamurugan, S. Sundararajan, andS.
Keerthi.
2011.
A sequential dual method for struc-tural SVMs.
In IEEE International Conference onData Mining(ICDM).E.
F. T. K. Sang and F. De Meulder.
2003.
Introduction tothe CoNLL-2003 shared task: Language-independentnamed entity recognition.
In Walter Daelemans andMiles Osborne, editors, Proceedings of CoNLL-2003,pages 142?147.
Edmonton, Canada.B.
Taskar, C. Guestrin, and D. Koller.
2004a.
Max-margin markov networks.
In The Conference onAdvances in Neural Information Processing Systems(NIPS).B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004b.
Max-margin parsing.
In Proceedingsof the Conference on Empirical Methods for NaturalLanguage Processing (EMNLP).B.
Taskar, S. Lacoste-julien, and M. I. Jordan.
2005.Structured prediction via the extragradient method.
InThe Conference on Advances in Neural InformationProcessing Systems (NIPS).B.
Taskar, S. Lacoste-Julien, and M. I Jordan.
2006.Structured prediction, dual extragradient and bregmanprojections.
Journal of Machine Learning Research,7:1627?1653.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interde-pendent and structured output spaces.
In Proceedingsof the International Conference on Machine Learning(ICML).J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 384?394, Stroudsburg, PA, USA.Association for Computational Linguistics.Po-Wei Wang and Chih-Jen Lin.
2013.
Iteration com-plexity of feasible descent methods for convex opti-mization.
Technical report, National Taiwan Univer-sity.C.
Yu and T. Joachims.
2009.
Learning structural SVMswith latent variables.
In Proceedings of the Interna-tional Conference on Machine Learning (ICML).218
