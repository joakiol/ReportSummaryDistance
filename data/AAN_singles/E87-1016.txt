STOCHASTIC  MODEL ING OF  LANGUAGE VIA SENTENCE SPACE PART IT IONINGAlex MartelliIBM Rome Scientific Centervia Giorgione 159, ROME (Italy)ABSTRACTIn some computer applications of linguistics (such asmaximum-likelihood decoding of speech or handwriting), thepurpose of the language-handling component (LanguageModel) is to estimate the linguistic (a priori) probability ofarbitrary natural-language s ntences.
This paper discussestheoretical and practical issues regarding an approach tobuilding such a language model based on any equivalencecriterion defined on incomplete sentences, and experimentalresults and measurements performed on such a model of theItalian language, which is a part of the prototype for therecognition of spoken Italian built at the IBM RomeScintific Center.STOCHASTIC  MODELS OF  LANGUAGEIn some computer applications, it is necessary to have away to estimate the probability of any arbitrarynatural-language sentence.
A prominent example ismaximum-likelihood speech recognition (as discussed in \[1\],\[4\], \[7\]), whose underlying mathematical pproach can begeneralized to recognition of natural anguage "encoded" inany medium (e.g.
handwriting).
The subsystem whichestimates this probability can be called a stochastic model ofthe target language.If the sentence is to be recognized while it is beingproduced (as necessary for a real-time application), thecomputation of its probability should proceed"left-to-right," i.e.
word by word from the beginningtowards the end of the sentence, allowing application of fasttree-search algorithms such as stack decoding\[5\]Left-to-right computation of the probability of any wordstring is made possible by a formal manipulation based onthe definition of condit__ional probability: if W i is the i-thword in the sequence 14' of length N, then:Ne(W)= 1--IP(EI w , t  , ~_~ ..... ~'t)i=1In other terms, the probability of a sequence of words is theproduct of the conditional probability of each word, givenall of the previous ones.
As a formal step, this holds for fullsentences as well as for any subsequence within a sentence,and also for multi-sentence pieces of text, as long assentence boundaries are explicitly accounted for (typically byintroducing a pseudo-word as sentence boundary marker).We shall apply this equation only to subsequences occurringat the start of sentences (i.e.
"incomplete" sentences); thus,the unconditional probability P(WI) can meaningfully beread as the probability that the particular word WI, ratherthan any other word, will be the one starting a sentence.The language model will thus consist essentially of away to compute the conditional probability of any (target)word given all of the words that precede it in the sentence.For brevity, we shall call this (possibly empty) subsequenceof the sentence to the left of the target word its prefix, usingthis term intcrchangeably with incomplete sentence, and weshall refer to the operation of conditional probabilityestimation given an incomplete sentence as predicang thenext word in the sentence.
A stochastic language model inthis form may be said to be in predictive normal form \[2\].The predictive power of two language models inpredictive normal form can always be compared on anempirical basis, no matter how different their internalstructures may be, by using the perplexity statisticintroduced in \[6\]; the perplexity, computed by applying alanguage model in predictive normal form to an arbitrarybody of text, can be interpreted as the average number ofwords among which the model is "in doubt" at everycontext along the text (this can be made rigorous along thelines of the argument in \[13\]).TRA IN ING THE MODELA naive statistical approach to the estimation of theconditional probabilities of words given prefixes, to build alanguage model in predictive normal form, would simplycollect occurrences of each prefix in a large corpus, usingthe relative frequencies of following words as estimates ofprobability.
'l'i~is is clearly unfeasible: no matter how largethe available corpus, the possible prefixes will be yet morenumerous; thus, most of them will not be observed in thecorpus, and those which are observed will only be seenfollowed by a very limited and unrepresentative subset ofthe words that can come after them.This problem stems directly from the fact that thenumber of elements in the set ("space") of different possible(incomplete) sentences is too high; thus, it can be methead-on by simply reducing the number of incompletesentences which are deemed to differ significantly forpredictinn purposes, i.e.
by passing to the quotient space ofthe sentence space on a suitable equivalence relation; inother words, by using as, contexts of the language model,the equivalence classes in a partition of the set of allprefixes, rather than the prefixes themselves.
Theequivalence classification of prefixes can be based on anykind of linguistical knowledge, as long as it can be applied totwo prefixes to judge if they can be deemed "similarenough" to allow us to expect hat they should lead to thesame prediction regarding the next word to Le expected inthe sentence.
Indeed, the knowledge embodied in theequivalence classification eed not be of the kind that wouldbe commonly labeled "\[inguistical"; the equivalence criterion91between two sentence prefixes need not be any more thanthe purely pragmatical "they behave similarly in predictingthe next following word.
"Let us assume that we already had a stochastic languagemodel, in predictive normal form, somehow trained to oursatisfaction.
To each string of words, considered as asentence prefix, there would be attached a probabilitydistribution over all words in the dictionary, correspondingto the conditional probability that the word should followthis prefix.
We could now apply sentence-space partitioningas follows: define a distance measure between probabilitydistributions over the dictionary; apply any clusteringalgorithm to obtain the desired number of classes (or,cluster iteratively until further clustering would requiremerging of equivalence classes which are at a distance abovesome threshold).
By this hypothetical process, we would beextracting linguistical knowledge (namely, which sequencesof words can be deemed equivalent as regards the wordwhich can be expected to follow them) from the model itself(thus, presumably, from the data it was trained upon).Since we don't have such a well-trained model to begin with,we will actually have to reverse the process: start byinjecting some knowledge in the form of equivalencecriteria, obtain from this a way to practically train themodel.One way to obtain the initial sentence-space partitioncould be from a parser able to work left-to-right on naturallanguage sentences; each class in the partition would be theset of all sentence prefixes that take the parser's tate to agiven string of non-terminals (or rather, given the possibilityof ambiguous parses, to a given set of such strings).
Wehave not attempted this.
What we have attempted isobtaining the equivalence relation on string of words froman equivalence r lation on single words, which is far simplerto define (although, being a further approximation, it can beexpected to give poorer results).
Thus, if we define theequivalences:Miche le  ~--- G iuseppepensa  - -  d lcewe will have that "Michele dice" is equivalent to "Giuseppepensa," and so on.
One big advantage is that suchequivalence classes on single words are relatively easy toobtain automatically (by clustering over any appropriatedistance measure, as outlined in the hypothetical exampleabove - the difference being that we can train single wordsadequately, without having to resort to a previousclassification), thus leading to an automatical (although farfrom optimal) sentence-space partitioning on which themodel's training can be based.It should be noted at this point that this approachsuffers from the "synonym problem": since equivalencerelationships enjoy the transitive property, we risk deeming"equivalent" two items A and B which are actually quitedifferent, by virtue of the fact that they both "resemble" athird item C. This problem depends on the "all or nothing"nature of equivalence relationships, and could be bypassedby a mathematically more general approach, based on thetheory of Marker Sources (as outlined in \['3\], \[g\]).
Thelatter can be said to stern from a generalization ofsentence-space partitions to "fuzzy partitions" (probabilisticcovers), i.e.
frnm usage of a nondeterministic equivalencerelation.
I lowever, as argued in rlO\], the greater generality,although aesthetically appealing, and no doubt useful againstthe "synonym problem," does not necessarily add enoughpower to the language model to offset the addedcomputational burden; in many cases, Markov-sourcemodels can be practically reduced to sentence-spacepartitioning models.One further generalization is the identification ofequivalence relationships between word strings of differentlength.
For example, verb forms such as "dice" or "pensa"could be deemed equivalent to themselves prefixed by theword "non," finally leading to equivalence between, say,"Marie dice" and "Giuseppe non pensa."
Such equivalencescould also, in principle, be tested automatically on statisticalgrounds.
Finally, equivalence criteria thus obtained viastatistical means are by no means ends in themselves, butcan be integrated with other linguistical knowledgeexpressed as a partition of the sentence space, to build astronger model.
Indeed, the set of language models built onsentence space partitions inherits mathematical latticeproperties from the set of partitions itself, through theirnatural correspondence, allowing simple but usefuloperation on language models to yield new language models.For example, the "least upper bound" operation on twolanguage models gives the model based on the equivalencecriterion which requires both equivalence criteria from theoriginal models to be satisfied.
Thus, for example, we couldstart from an equivalence criterion O defined on purelygrammatical grounds (for example, by using a parser, suchas suggested above), and another equivalence criterion Sdefined on statistical grounds (such as we have built asoutlined above), and merge them into a new criterion SO,the laxer one which is still stronger than either, to obtain afiner partition (and thus, presumably, a better performingstochastical language model, assuming a reasonably largecorpus is available to train it on).APPL ICAT ION AND RESULTSGiven a suitable quivalence criterion over prefixes, anda large corpus, the language model can now in principle bebuilt by purely statistical means, by collecting the multiset ofwords following each equivalence class (context), and usingrelative frequencies as estimators of conditionalprobabilities.
However, this would require that theequivalence criterion be so lax (i.e., that it have so fewcontexts) that each of its contexts can be guaranteed tooccur in the corpus followed by all different words that canpossibly follow it, despite possible statisUcal fluctuations.This is an overly severe restriction that, even for a quitelarge corpus, would in practice constrain the model builderto use very weak equivalence classifications (i.e.
ones of littlediscriminatory power).A generalization of the backing-off methodology firstproposed in \[q\] can be used to overcome this limitation.Rather than a single sentence-space partition, the model willneed a chain of such partitions, progressively weaker, andending with the weakest possible "partition" - the one whichconsiders any prefix equivalent o any other (the maximalelement in lhe above-mentioned lattice).
"Elementary"92models will be built, with the above statistical procedure,over each partition of the chain.When using the model (now built as a chain ofelementary models) in predictive form, if a prediction cannotbe reliably obtained from the strongest model in the chain,the algorithm will then bacl~-off to the next weakest model,and proceed recursively along the chain of elementarymodels until it finds one that can give a reliable prediction(the existence in the chain of the weakest conceivable modelensures termination).The method requires that, along with its predictions, anelementary model deliver, for any given context, a measureof its own reliability.
This can be quantified as follows: inany context, an elementary model must estimate theprobability that the next word will not be in the set actuallyobserved for that model in that context (i.e., the set ofwords it is able to predict).
Thus, each step of backing-offwill be performed in two cases: unconditionally, if anelementary model has no observations at all for prefixesequivalent o the target one; conditionally, if that contextwas indeed observed, but the target word was not observedin it (and in this latter case, the self-estimate of reliability ofthe elementary model will come into play).For the estimation of the global probability ofunobserved words in a context ("new" observations), therecould be used the general approaches, based on Turing'sheuristic, discussed in \[ I 1 \] and \[ 12\], which lead, in practice,to estimating the probability of "new" observations as theratio of words observed once to total observations.
Wehave found it more reliable to use a simpler approach (the.
"First-Time" heuristic), which directly estimates theprobability of new observations as the ratio of differentwords observed to total observations.This idea leads to strictly more pessimistic estimates ofreliability of elementary models (in particular, it treats anyword observed only once in a context as if never observedat all) and, judging from experimental results, seems tobetter model actual linguistic behavior.
As expected, itproves particularly valuable when judging predictive powerover poorly-trained material, specifically Italian sentences ina domain of discourse different from that of the trainingcorpus.
Using training data from the "II Mondo" weeklymagazine, the perplexity (with an 8000-word vocabulary)over other test sentences from the same magazine came to113, and over news flashes from the Ansa agency to 174,using Turing's heuristic; while using the First-Time heuristicunder the same experimental conditions gave values of II Iand 150 respectively.Particularly with this heuristic, cross-domain behaviorof such models appears quite acceptable.
Our main trainingcorpus was a set of articles and news flashes on economyand finance, from the "II Mondo" weekly magazine and the"Ansa" new agency, for a total of about 6 million words;addition of just 50,000 words of inter-office memorandamade the perplexity of another test set of such memoranda(on a 3000-word vocabulary) decrease from 149 to 115,while naturally perplexity on test material homogeneous tothe main body of the training corpus remained fixed (at 76).REFERENCES\ [ l \ ]  I..R. Bahl, F. Jelinek, R.L Mercer, A maximumlikelihood approach to eontinuous speechrecognition, IEEE Trans.
PAMI, March 1983.\[2\] R. Campo, L. Fissore, A. Martelli, G. Micca, G.Volpi, Prohahilistie Models of the ItalianLanguage for Speech Recognition, Proc.
Int.Work.
Authomatic Speech Recognition, Roma,Ilaly, May 1986.\[3\] A.M. Derouault, B. Merialdo, Languagemodeling at the syntactic level, Proc.
Seventh Int.Con\]: Pattern Recognition, Montreal, Canada,July 30-August 2, 1984.1-4.1Is\]P. D'Orta, M. Ferretti, A. Martelli, S.Melecrinis, S. Scarci, G. Volpi, II prototipo IBMper il riconoscimento del parlato, Note diInformatica, n. 13, September 1986.F.
Jelinek, A fast sequential decoding algorithmusing a Mack, IBM Journal of Research andDevelopment, November 1969.\[63 F..lelinek, R.L.
Mercer, L.R.
Bahl, J.K. Baker,Perplexity - a measure of difficulty of speechrecognition tasks, 94th Meeting Acoustical Societyof America, Miami Beach, FL, December 15,1977.\[7\] F. Jelinek, The development of an experimentaldiscrete dictation recognizer, Proceedings ofIEEE, November 1985.\[81 F. Jelinek, Self-Organized Language Modelingfor Speech Recognition, IBM internal memo,February 1986.I-9\] S. Katz, Recursive M-gram Language Model viaa Smoothing of Turing's Formula, IBM TechnicalDisclo.~tre Bulletin, 1985.ElO\] A. Martelli, Modelli probabilistici della linguaitaliana, Note dl Informatica, n. 13, September19~6.Ell3\[123A.
Nadas, Estimation of probabilities in thelanguage model of the IBM speech recognitionsystem, IEEE Trans.
on Acoustic, Speech andSignal Processing, August 1984.A.
Nadas, On Turing's Formula for WordProlmhilities, IEEE Trans.
on Acoustic, Speechand Signal Processing, December 1985.\['13\] C.E.
Shannon, Prediction and entropy of printedF.nglish, I~ell.
S),st. Tech.
Journal, 1951.93
