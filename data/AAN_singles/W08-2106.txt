CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 41?48Manchester, August 2008Using LDA to detect semantically incoherent documentsHemant Misra and Olivier Cappe?LTCI/CNRS and TELECOM ParisTech{misra,cappe}@enst.frFranc?ois YvonUniv Paris-Sud 11 and LMISI-CNRSyvon@limsi.frAbstractDetecting the semantic coherence of a doc-ument is a challenging task and has sev-eral applications such as in text segmenta-tion and categorization.
This paper is anattempt to distinguish between a ?semanti-cally coherent?
true document and a ?ran-domly generated?
false document throughtopic detection in the framework of latentDirichlet analysis.
Based on the premisethat a true document contains only a fewtopics and a false document is made up ofmany topics, it is asserted that the entropyof the topic distribution will be lower fora true document than that for a false docu-ment.
This hypothesis is tested on severalfalse document sets generated by variousmethods and is found to be useful for fakecontent detection applications.1 IntroductionThe ?Internet revolution?
has dramatically in-creased the monetary value of higher ranking onthe web search engines index, fostering the ex-pansion of techniques, collectively known as ?WebSpam?, that fraudulently help to do so.
Internet isindeed ?polluted?
with fake Web sites whose onlypurpose is to deceive the search engines by arti-ficially pushing up the popularity of commercialsites, or sites promoting illegal content 1.
Thesefake sites are often forged using very crude contentgeneration techniques, ranging from web scrap-ping (blending of chunks of actual contents) tosimple-minded text generation techniques basedc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1The annual AirWeb challenge http://airweb.cse.lehigh.edu gives a state-of-the art on current WebSpam detection techniques.on random sampling of words (?word salads?
),or randomly replacing words in actual documents(?word stuffing?)
2.
Among these, the latter twoare easy to detect using simple statistical modelsof natural texts, but the former is more challeng-ing, it being made up of actual sentences: recog-nizing these texts as forged requires either to resortto plagiarism detection techniques, or to automati-cally identify their lack of semantic consistency.Detecting the consistency of texts or of textchunks has many applications in Natural LanguageProcessing.
So far, it has been used mainly in thecontext of automatic text segmentation, where achange in vocabulary is often the mark of topicchange (Hearst, 1997), and, to a lesser extent, indiscourse studies (see, e.g., (Foltz et al, 1998)).It could also serve to devise automatic metrics fortext summarization or machine translation tasks.This paper is an attempt to address the issueof differentiating between ?true?
and ?false?
doc-uments on the basis of their consistency throughtopic modeling approach.
We have used La-tent Dirichlet alocation (LDA) (Blei et al, 2002)model as our main topic modeling tool.
One of theaims of LDA and similar methods, including prob-abilistic latent semantic analysis (PLSA) (Hof-mann, 2001), is to produce low dimensionality rep-resentations of texts in a ?semantic space?
suchthat most of their inherent statistical characteristicsare preserved.
A reduction in dimensionality facil-itates storage as well as faster retrieval.
Modelingdiscrete data has many applications in classifica-tion, categorization, topic detection, data mining,information retrieval (IR), summarization and col-laborative filtering (Buntine and Jakulin, 2004).The aim of this paper is to test LDA for es-tablishing the semantic coherence of a documentbased on the premise that a real (coherent) docu-ment should discuss only a few number of topics,2The same techniques are commonly used in mail spamsalso.41a property hardly granted for forged documentswhich are often made up of random assemblageof words or sentences.
As a consequence, the co-herence of a document may reflect in the entropyof its posterior topic distribution or in its perplex-ity for the model.
The entropy of the estimatedtopic distribution of a true document is expected tobe lower than that of a fake document.
Moreover,the length normalized log-likelihood of a true andcoherent document may be higher as compared tothat of a false and incoherent document.In this paper, we compare two methods to esti-mate the posterior topic distribution of test docu-ments, and this study is also an attempt to inves-tigate the role of different parameters on the effi-ciency of these methods.This paper is organized as follows: In Section 2,the basics of the LDA model are set.
We then dis-cuss and contrast several approaches to the prob-lem of inferring the topic distribution of a newdocument in Section 3.
In Section 4, we describethe corpus and experimental set-up that are usedto produce the results presented in Section 5.
Wesummarize our main findings and draw perspec-tives for future research in Section 6.2 Latent Dirichlet Allocation2.1 BasicsLDA is a probabilistic model of text data whichprovides a generative analog of PLSA (Blei et al,2002), and is primarily meant to reveal hidden top-ics in text documents.
In (Griffiths and Steyvers,2004), the authors used LDA for identifying ?hottopics?
by analyzing the temporal dynamics of top-ics over a period of time.
More recently LDA hasalso been used for unsupervised language model(LM) adaptation in the context of automatic speechrecognition (ASR) (Hsu and Glass, 2006; Tamand Schultz, 2007; Heidel et al, 2007).
Severalextensions of the LDA model, such as hierarchi-cal LDA (Blei et al, 2004), HMM-LDA (Grif-fiths et al, 2005), correlated topic models (Bleiand Lafferty, 2005) and hidden topic Markov mod-els (Gruber et al, 2007), have been proposed, thatintroduce more complex dependency patterns inthe model.Like most of the text mining techniques, LDAassumes that documents are made up of words andthe ordering of the words within a document isunimportant (?bag-of-words?
assumption).
Con-trary to the simpler Multinomial Mixture Model(see, e.g., (Nigam et al, 2000) and Section 2.4),LDA assumes that every document is representedby a topic distribution and that each topic definesan underlying distribution on words.The generative history of a document (a bag-of-words) collection is the following: Assuminga fixed and known number of topics nT, for eachtopic t, a distribution ?tover the indexing vocab-ulary (w = 1 .
.
.
nW) is drawn from a Dirichletdistribution.
Then, for each document d, a distri-bution ?dover the topics (t = 1 .
.
.
nT) is drawnfrom a Dirichlet distribution.
For a document d,the document length ldbeing an exogenous vari-able, the next step consists of drawing a topic tifrom ?dfor each position i = 1...ld.
Finally, aword is selected from the chosen topic ti.
Giventhe topic distribution, each word is thus drawn in-dependently from every other word using a docu-ment specific mixture model.
The probability ofith word token is thus:P (wi|?d, ?)
=nT?t=1P (ti= t|?d)P (wi|ti, ?)
(1)=nT?t=1?dt?tw(2)Conditioned on ?
and ?d, the likelihood of doc-ument d is a mere product of terms such as (2),which can be rewritten as:P (Cd|?d, ?)
=nW?w=1[nT?t=1(?dt?tw)]Cdw(3)where Cdwis the count of word w in d.2.2 LDA: TrainingLDA training consists of estimating the followingtwo parameter vectors from a text collection: thetopic distribution in each document d (?dt, t =1...nT, d = 1...nD) and the word distribution ineach topic (?tw, t = 1...nT, w = 1...nW).
Both?dand ?tdefine discrete distributions, respectivelyover the set of topics and over the set of words.Various methods have been proposed to estimateLDA parameters, such as variational method (Bleiet al, 2002), expectation propagation (Minka andLafferty, 2002) and Gibbs sampling (Griffiths andSteyvers, 2004).
In this paper, we have usedthe latter approach, which boils down to repeat-edly going through the training data and samplingthe topic assigned to each word token conditioned42on the topic assigned to all the other word to-kens.
Given a particular Gibbs sample, the pos-teriors for ?
and ?
are 3: Dirichlet with parameters(Kt1+?, .
.
.
,KtnW+?)
and Dirichlet with param-eters (Jd1+ ?, .
.
.
, JdnT+ ?
), respectively, whereKtwis the number of times word w is assigned totopic t and Jdtis the number of times topic t is as-signed to some word token in document d. Hence,?tw=Ktw+ ?
?nWk=1Ktk+ nW?
(4)?dt=Jdt+ ?
?nTk=1Jdk+ nT?
(5)During the Gibbs sampling phase, ?tand ?daresampled from the above posteriors while the finalestimates for these parameters are obtained by av-eraging the posterior means over the complete setof Gibbs iteration.2.3 LDA: TestingTraining LDA model on a text collection alreadyprovides interesting insights regarding the the-matic structure of the collection.
This has been theprimary application of LDA in (Blei et al, 2002;Griffiths and Steyvers, 2004).
Even better, beinga generative model, LDA can be used to makeprediction regarding novel documents (assumingthey use the same vocabulary as the training cor-pus).
In a typical IR setting, where the main fo-cus is on computing the similarity between a doc-ument d and a query d?, a natural similarity mea-sure is given by P (Cd?
|?d, ?
), computed accordingto (3) (Buntine et al, 2004).An alternative would be to compute the KL di-vergence between the topic distribution in d and d?,which however requires to infer the latter quantity.As the topic distribution of a (new) document givesits representation along the latent semantic dimen-sions, computing this value is helpful for manyapplications, including text segmentation and textclassification.
Methods for efficiently and accu-rately estimating topic distribution for text docu-ments are presented and evaluated in Section 3.2.4 Baseline: Multinomial Mixture ModelThe performance of LDA model is comparedwith that of the simpler multinomial mixturemodel (Nigam et al, 2000; Rigouste et al, 2007).3assuming non-informative priors with hyper-parameters?
and ?
for the Dirichlet distribution over topics and theDirichlet distribution over words respectivelyIn this model, every word in a document belongsto the same topic, as if the document specific topicdistribution ?din LDA were bound to lie on onevertex of the [0, 1]nT simplex.
Using the same no-tations as before (except for ?t, which now denotesthe position independent probability of topic t inthe collection), the probability of a document is:P (Cd|?t, ?)
=nT?t=1?tnW?w=1?Cdwtw(6)This model can be trained through expectationmaximization (EM), using the following reestima-tion formulas, where (7) defines the E-step; (8) and(9) define the M-step.P (t|Cd, ?, ?)
=?t?nWw=1(??tw)Cdw?nTt=1?t?nWw=1(?tw)Cdw(7)??t?
?
+nD?d=1P (t|Cd, ?, ?)
(8)??tw?
?
+nD?d=1CdwP (t|Cd, ?, ?)
(9)As suggested in (Rigouste et al, 2007), we ini-tialize the EM algorithm by drawing initial topicdistributions from a prior Dirichlet distributionwith hyper-parameter ?
= 1. ?
= 0.1 in all theexperiments.During testing, the parameters of the multino-mial models are used to estimate the posterior topicdistribution in each document using (7).
The like-lihood of a test document is given by (6).3 Inferring the Topic Distribution of TestDocumentsP (Cd|?d), the conditional probability of a docu-ment d given ?dis obtained using (3) 4.
Computingthe likelihood of a test document requires to inte-grate this quantity over ?
; likewise for the compu-tation of the posterior distribution of ?.
This inte-gral has no close form solution, but can be approx-imated using Monte-Carlo sampling techniques as:P (Cd) ?
1MM?m=1P (Cd|?
(m)) (10)where ?
(m) denotes the mth sample from theDirichlet prior, and M is the number of Monte4The dependence on ?
is dropped for simplicity.
?
islearned during training and kept fixed during testing.43Carlo samples.
Given the typical length of doc-uments and the large vocabulary size, small scaleexperiments convinced us that a cruder approxima-tion was in order, as the sum in (10) is dominatedby the maximum value.
We thus contend ourselvesto solve:?
?= argmax?,Pt?t=1P (Cd|?)
(11)and use this value to approximate P (Cd) using (3).The maximization program (11) has no closeform solution.
However, the objective function isdifferentiable and log-concave, and can be opti-mized in a number of ways.
We considered twodifferent algorithms: an EM-like approach, ini-tially introduced in (Heidel et al, 2007), and an ex-ponentiated gradient approach (Kivinen and War-muth, 1997; Globerson et al, 2007).The first approach implements an iterative pro-cedure based on the following update rule:?dt?
1ldnW?w=1Cdw?dt?tw?nTt?=1?dt?
?t?w(12)Although no justification was given in (Hei-del et al, 2007), it can be shown that thisupdate rule converges towards a global opti-mum of the likelihood.
Let ?
and ??
be twotopic distributions in the nT-dimensional simplex,L(?)
= log P (Cd|?
), and ?t(w, ?)
=?t?twPt??t?
?t?w.We define an auxiliary function Q(?, ??)
=?wCw(?t?t(w, ?)
log(??t)).
Q(?, ??)
is concavein ?
?, and performs the role played by the auxil-iary function in the EM algorithm.
Simple cal-culus suffices to prove that (i) the update (12)maximizes in ??
the function Q(?, ??
), and (ii)Q(?, ??)
?
Q(?, ?)
?
L(??)
?
L(?
), which stemsfrom the concavity of the log.
At an optimum ofQ(?, ??)
the positivity of the first term implies thepositivity of the second.
Maximizing Q using theupdate rule (12) thus increases the likelihood andrepeating this update converges towards the opti-mum value.
We experimented both with an un-smoothed (12) and with a smoothed version of thisupdate rule.
The unsmoothed version yielded aslightly better result than the smoothed one.Exponentiated gradient (Kivinen and Warmuth,1997; Globerson et al, 2007) yields an alternativeupdate rule:?dt?
?dtexp(?nW?w=1Cdw?tw?nTt?=1?dt?
?t?w)(13)where ?
defines the convergence rate.
In this form,the update rule does not preserve the normaliza-tion of ?, which needs to be performed after everyiteration.A systematic comparison of these rules was car-ried out, yielding the following conclusions:?
the convergence of the EM-like method isvery fast.
Typically, it requires less than halfa dozen iterations to converge.
After conver-gence, the topic distribution estimated by thismethod for a subset of train documents wasalways very close (as measured by the KL-divergence) to the respective topic distribu-tion of the same documents observed at theend of the LDA training.
Taking nT= 50,the average KL divergence for a set of 4,500documents was found to be less than 0.5.?
exponentiated gradient has a more erratic be-haviour, and requires a careful tuning of ?
ona per document basis.
For large values of ?,the update rule (13) sometimes fails to con-verge; smaller values of ?
allowed to consis-tently reach convergence, but required moreiterations (typically 20-30).
On a positiveside, on an average, the topic distributionsestimated by this method are better than theones obtained with the EM-like algorithm.Based on these findings, we decided to use theEM-like algorithm in all our subsequent experi-ments.4 Experimental protocol4.1 Training and test corporaThe Reuters Corpus Volume 1 (RCV1) (Lewis etal., 2004) is a collection of over 800,000 newsitems in English from August 1996 to August1997.
Out of the entire RCV1 dataset, we se-lected 27,672 documents (news items) for training(TrainReuters) and 23,326 documents for testing(TestReuters).
The first 4000 documents from theTestReuters dataset were used as true documents(TrueReuters) in the experiments reported in thispaper.
The vocabulary size in the train set, afterremoving the function words, is 93, 214.Along with these datasets of ?true?
documents,three datasets of fake documents were also cre-ated.
Document generation techniques are many:here we consider documents made by mixing shortpassages from various texts and documents made44by assembling randomly chosen words (sometimescalled as ?word salads?).
In addition, we alsoconsider the case of documents generated with astochastic language model (LM).
Our ?fake?
testdocuments are thus composed of:?
(SentenceSalad) obtained by randomly pick-ing sentences from TestReuters.?
(WordSalad) created by generating randomsentences from a conventional unigram LMtrained on TrainReuters.?
(Markovian) created by generating randomsentences from a conventional 3-gram LMtrained on TrainReuters.Each of these forged document set contains 4,000documents.To assess the performance on out-of-domaindata, we replicated the same tests using 2,000Medline abstracts (Ohta et al, 2002).
1,500 doc-uments were used either to generate fake docu-ments by picking sentences randomly or to train anLM and then using the LM to generate fake docu-ments.
The remaining 500 abstracts were set asideas ?true?
documents (TrueMedline).4.2 Performance Measurements : EERThe entropy of the topic distribution is computedas H = ??Tj=1??djlog??dj.
The other measureof interest is the average ?log-likelihood per word?
(LLPW) 5.While evaluating the performance of our sys-tem, two types of errors are encountered: false ac-ceptance (FA) when a false document is acceptedas a true document and false rejection (FR) when atrue document is rejected as a false document.
Therate of FA and FR is dependent on the thresholdused for taking the decision, and usually the per-formance of a system is shown by its receiver op-erating characteristic (ROC) curve which is a plotbetween FA and FR rates for different values ofthreshold.
Instead of reporting the performance ofa system based on two error rates (FA and FR),the general practice is to report the performance interms of equal-error-rate (EER).
The EER is theerror rate at the threshold where FA rate = FR rate.In our system, a threshold on entropy (orLLPW) is used for taking the decision, and all the5This measure is directly related to the text per-plexity in the model, according to perplexity =2?average log-likelihood per worddocuments having their entropy (or LLPW) below(or above) the threshold are accepted as true doc-uments.
The EER is obtained on the test set bychanging the threshold on the test set itself, andthe best results thus obtained are reported.5 Detecting semantic inconsistency5.1 Detecting fake documents with LDA andMultinomial mixturesIn the first set of experiments, the LLPW and en-tropy of the topic distribution (the two measures)of the Multinomial mixture and LDA models werecompared to check the ability of these two mea-sures and models in discriminating between trueand false documents.
These results are summa-rized in Table 1.TrueReuters vs. MultinomialLLPW EntropySentenceSalad 15.3% 48.8%WordSalad 9.3% 35.8%Markovian 17.6% 38.9%TrueReuters vs. LDALLPW EntropySentenceSalad 18.9% 0.88%WordSalad 9.9% 0.13%Markovian 25.0% 0.28%Table 1: Performance of the Multinomial Mixtureand LDAFor the multinomial mixture model, the LLPWmeasure is able to discriminate between true andfalse documents to a certain extent.
As expected(not shown here), the LLPW of the true documentsis usually higher than that of the false documents.In contrast, the entropy of the posterior topic dis-tribution does not help much in discriminating be-tween true and false documents.
In fact it remainsclose to zero (meaning that only one topic is ?ac-tive?)
both for true and false documents.The behaviour of the LDA scores is entirely dif-ferent.
The perplexity scores (LLPW) of true andfake texts are comparable, and do not make usefulpredictors.
In contrast, the entropy of the topic dis-tribution allows to sort true documents from fakeones with a very high accuracy for all kinds of faketexts considered in this paper.
Both results stemfrom the ability of LDA to assign a different topicto each word occurrence.Similar pattern is observed for our three falsetest sets (against the TrueReuters set) with small45variations The texts generated with a Markovmodel, no matter the order, have the highest en-tropy, reflecting the absence of long range corre-lation in the generation model.
Though the textsgenerated by mixing sentences are more confus-ing with the true documents, the performance isstill less than 1% EER.
Texts mixing a high num-ber of topics (e.g., Sentence Salads) are almost aslikely as natural texts that address only a few top-ics.
However, the former has much higher entropyof the topic distribution due to a large number oftopics being active in such texts (see also Figure 1).0 1 2 3 4 5 600.010.020.030.040.050.060.070.08EntropyNormalizedfrequencyofoccurenceTrue: TrueReutersFalse: SentenceSaladFalse: WordSaladFalse: MarkovianFigure 1: Histogram of entropy of ?
for differenttrue and false document sets.It is noteworthy that both the predictors (LLPWand Entropy) give complementary clues regardinga text category.
A linear combination of these twoscores (the weight to the LLPW score is 0.1) al-lows to substantially improve over these baselineresults, yielding a relative improvement (in EER)of +20.0% for the sentence salads, +20.8% for theword salads, and +27.3% for the Markov Models.5.2 Effect of the number of topicsIn this part, we investigate the performance ofLDA in detecting false documents when the num-ber of topics is changed.
Increasing the numberof topics means higher memory requirements bothduring training and testing.
Though the results areshown only for SentenceSalad, similar trend is ob-served for WordSalad and Markovian.The numbers in Table 2 show that the perfor-mance obtained with the LLPW score consistentlyimprove with an increase in the number of top-ics, though the % improvement obtained when thenumber of topics exceeds 200 is marginal.
In con-trast, the best performance in case of entropy isachieved at 50 topics and slowly degrades when amore complex model is used.Number of Topics LLPW Entropy10 27.9 1.8850 18.9 0.88100 16.0 0.93200 14.8 0.90300 13.8 1.05400 13.6 1.10Table 2: EER from LLPW and Entropy distributionfor TrueReuters against SentenceSalad.5.3 Detecting ?noisy?
documentsIn this section, we study fake documents producedby randomly changing words in true documents(the TrueReuters dataset).
In each document, afixed percentage of content words is randomly re-placed by any other word from the training vocab-ulary 6.
This percentage was varied from 5 to 100and EER for these corrupted document sets is com-puted at each % corruption level (Figure 2).
As0 5 10 15 20 25 30 35 40 45 50 60 70 80 90 10005101520253035404550Noise level (% words changed)EER: LLPWandEntropyLLPWEntropyFigure 2: EER at various noise levelsexpected, the EER is very high at low noise levels,and as the noise level is increased, EER gets lower.When only a few words are changed in a true doc-ument, it retrains the properties of a true document(high LLPW and low entropy).
However, as morenumber of words are changed in a true document,6When the replacement words are chosen from a small setof very specific words, the fake document generation strategyis termed as ?word stuffing?.46it starts showing the characteristics of a false docu-ment (low LLPW and high entropy).
These resultssuggest that our semantic consistency tests are toocrude a measure to detect a small number of in-consistencies, such as the ones found in the state-of-the-art OCR or ASR systems?
outputs.
On theother hand, it confirms the numerous studies thathave shown that topic detection (and topic adapta-tion) or text categorization tasks can be performedwith the same accuracy for moderately noisy textsand clean texts, a finding which warrants the topic-based LM adaptation strategies deployed in (Hei-del et al, 2007; Tam and Schultz, 2007).The difference in the behavior of our two pre-dictors is striking.
The EER obtained using LLPWdrops more quickly than the one obtained with en-tropy of the topic distribution.
It suggests that theinfluence of ?corrupting?
content words (mostlywith low ?tw) is heavy on the LLPW, but the topicinformation is not lost till a majority of the ?uncor-rupted?
content words belong to the same topic.5.4 Effect of the document lengthIn this section, we study the robustness of ourtwo predictors with respect to the document lengthby progressively increasing the number of contentwords in a document (true or fake).
As can be seenfrom Figure 3, the entropy of the posterior topicdistribution starts to provide a reasonable discrim-ination (5% EER) when the test documents containabout 80 to 100 content words, and attains resultscomparable to those reported earlier in this paperwhen this number doubles.
This definitely rulesout this method as a predictor of the semantic con-sistency of a sentence: we need to consider at leasta paragraph to get acceptable results.5.5 Testing with out-of-domain dataIn this section, we study the robustness of our pre-dictors on out-of-domain data using a small ex-cerpt of abstracts from the Medline database.
Bothtrue and fake documents are from this dataset.The results are summarized in Table 3.
The per-TrueMedline vs. LLPW EntropySentenceSalad 31.23% 22.13%WordSalad 30.03% 19.46%Markovian 36.51% 23.63%Table 3: Performance of LDA on PubMed ab-stractsformance on out-of-domain documents is poor,10 20 30 40 50 60 70 80 90100 125 150 175 200 22505101520253035404550Number of content wordsEER: LLPWandEntropyTrueReuters against False setsLLPW: SentenceSaladLLPW: WordSaladLLPW: MarkovianEntropy: SentenceSaladEntropy: WordSaladEntropy: MarkovianFigure 3: EER with change in number of con-tent words used for LDA analysis.
EER basedon: LLPW of TrueReuters and false document sets(solid line) and Entropy of topic distribution ofTrueReuters and false document sets (dashed line).though the entropy of the topic distribution is stillthe best predictor.
The reasons for this failure areobvious: a majority of the words occurring in thesedocuments (true or fake) are, from the perspectiveof the model, characteristic of one single Reuterstopic (health and medicine).
They cannot be dis-tinguished either in terms of perplexity or in termsof topic distribution (the entropy is low for all thedocuments).
It is interesting to note that all theout-of-domain Medline data can be separated fromthe in-domain TrueReuters data with good accu-racy on the basis of the lower LLPW of the formeras compared to the higher LLPW of the latter.6 ConclusionIn the LDA framework, this paper investigated twomethods to infer the topic distribution in a testdocument.
Further, the paper suggested that thecoherence of a document can be evaluated basedon its topic distribution and average LLPW, andthese measures can help to discriminate betweentrue and false documents.
Indeed, through exper-imental results, it was shown that entropy of thetopic distribution is lower and average LLPW oftrue documents is higher for true documents andthe former measure was found to be more effective.However, the poor performance of this method onout-of-domain data suggests that we need to use amuch larger training corpus to build a robust fakedocument detector.
This raises the issue of train-47ing LDA model with very large collections.
In fu-ture we would like to explore the potential of thismethod for text segmentation tasks.AcknowledgmentThis research was supported by the EuropeanCommission under the contract FP6-027026-K-Space.
The views expressed in this paper are thoseof the authors and do not necessarily represent theviews of the commission.ReferencesBlei, David and John Lafferty.
2005.
Correlated topicmodels.
In Advances in Neural Information Process-ing Systems (NIPS?18), Vancouver, Canada.Blei, David M., Andrew Y. Ng, and Michael I. Jordan.2002.
Latent Dirichlet alocation.
In Dietterich,Thomas G., Suzanna Becker, and Zoubin Ghahra-mani, editors, Advances in Neural Information Pro-cessing Systems (NIPS), volume 14, pages 601?608,Cambridge, MA.
MIT Press.Blei, David M., Thomas L. Griffiths, Michael I. Jordan,and Joshua B. Tenenbaum.
2004.
Hierarchical topicmodels and the nested Chinese restaurant process.
InAdvances in Neural Information Processing Systems(NIPS), volume 16, Vancouver, Canada.Buntine, Wray and Aleks Jakulin.
2004.
Apply-ing discrete PCA in data analysis.
In Chickering,M.
and J. Halpern, editors, Proceedings of the 20thConference on Uncertainty in Artificial Intelligence(UAI?04), pages 59?66.
AUAI Press 2004.Buntine, Wray, Jaakko Lo?fstro?m, Jukka Perkio?, SamiPerttu, Vladimir Poroshin, Tomi Silander, HenryTirri, Antti Tuominen, and Ville Tuulos.
2004.A scalable topic-based open source search engine.In Proceedings of the IEEE/WIC/ACM InternationalConference on Web Intelligence, pages 228?234,Beijing, China.Foltz, P.W., W. Kintsch, and T.K.
Landauer.
1998.
Themeasurement of textual coherence with Latent Se-mantic Analysis.
Discourse Processes, 25(2-3):285?307.Globerson, Amir, Terry Y. Koo, Xavier Carreras, andMichael Collins.
2007.
Exponentiated gradient al-gorithms for log-linear structured prediction.
In In-ternational Conference on Machine Learning, Cor-vallis, Oregon.Griffiths, Thomas L. and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101 (supl 1):5228?5235.Griffiths, Thomas L., Mark Steyvers, David M. Blei,and Joshua Tenenbaum.
2005.
Integrating topicsand syntax.
In Proceedings of NIPS, 17, Vancouver,CA.Gruber, Amit, Michal Rosen-Zvi, and Yair Weiss.2007.
Hidden topic Markov models.
In Proceedingsof International Conference on Artificial Intelligenceand Statistics, San Juan, Puerto Rico, March.Hearst, Marti.
1997.
TextTiling: Segmenting texts intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.Heidel, Aaron, Hung an Chang, and Lin shan Lee.2007.
Language model adaptation using latentDirichlet alocation and an efficient topic inferencealgorithm.
In Proceedings of European Confer-ence on Speech Communication and Technology,Antwerp, Belgium.Hofmann, Thomas.
2001.
Unsupervised learningby probabilistic latent semantic analysis.
MachineLearning Journal, 42(1):177?196.Hsu, Bo-June (Paul) and Jim Glass.
2006.
Style& topic language model adaptation using HMM-LDA.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, Syd-ney, Australia.Kivinen, Jyrki and Manfrud K. Warmuth.
1997.
Expo-nentiated gradient versus gradient descent for linearpredictors.
Information and Computation, 132:1?63.Lewis, David D., Yiming Yang, Tony Rose, and FanLi.
2004.
RCV1: A new benchmark collection fortext categorization research.
Machine Learning Re-search, 5:361?397.Minka, Thomas and John Lafferty.
2002.
Expectation-propagation for the generative aspect model.
In Pro-ceedings of the conference on Uncertainty in Artifi-cial Intelligence (UAI).Nigam, K., A. K. McCallum, S. Thrun, and T. M.Mitchell.
2000.
Text classification from labeled andunlabeled documents using EM.
Machine Learning,39(2/3):103?134.Ohta, Tomoko, Yuka Tateisi, Hideki Mima, Jun ichiTsujii, and Jin-Dong Kim.
2002.
The GENIA cor-pus: an annotated research abstract corpus in molec-ular biology domain.
In Proceedings of Human Lan-guage Technology Conference, pages 73?77.Rigouste, Lo?
?s, Olivier Cappe?, and Franc?ois Yvon.2007.
Inference and evaluation of the multino-mial mixture model for text clustering.
Informa-tion Processing and Management, 43(5):1260?1280,September.Tam, Yik-Cheung and Tanja Schultz.
2007.
Correlatedlatent semantic model for unsupervised LM adapta-tion.
In Proceedings of the International Conferenceon Acoustics, Speech and Signal Processing, Hon-olulu, Hawaii, U.S.A.48
