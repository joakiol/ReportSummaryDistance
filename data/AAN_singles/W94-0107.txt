Complexity of Description of Primitives: Relevance toLocal Statistical ComputationsAravind K. Joshi and B. SrinivasDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USA{joshi, srini} @linc.cis.upenn.eduIntroductionIn this paper we pursue the idea that by making thedescriptions of primitive items (lexical items in the lin-guistic context) more complex, we can make the com-putation of linguistic structure more local 1.
The idea isthat by making the descriptions of primitives more com-plex, we can not only make more complex constraintsoperate more locally but also verify these constraintsmore locally.
Statistical techniques work better whensuch localities are taken into account 2 Of course, thereis a price for making the descriptions of primitives morecomplex.
The number of different descriptions for eachprimitive item is now much larger than when the de-scriptions are less complex.
For example, in a lexical-ized tree-adjoining rammar (LTAG), the number oftrees associated with each lexical item is much largerthan the number of standard parts-of-speech (POS) as-sociated with that item.
Even when the POS ambiguityis removed the number of LTAG trees associated witheach item can be large, on the order of 10 trees in thecurrent English grammar in the XTAG system s. Thisis because in LTAG, roughly speaking, each lexical item1 Let ~ be the alphabet consisting of the names of el-mentary trees in an LTAG.
Then ~*  is the set of all stringsover this alphabet including the null string.
The tree 71and 7~ in a string of tree names axe said to be ~*-local ifthey are separated by any string in ~* .
For brevity, we willcontinue to use the term local instead of the term ~*-local.2The work described here is completely different fromthe work reported in (Resnik, 1992) and (Schabes, 1992)concerning stochastic TAGs.3See Section on Data Collectionis associated with as many trees as the numb~,r of dif-ferent syntactic ontexts in which the iexical item canappear.
This, of course, increases the local ambiguil.yfor the parser.
The parser has to decide which com-plex description (LTAG tree) out of the set of descrip-tions associated with each lexical item is to be used fora given reading of a sentence, even before combiningthe descriptions together.
The obvious solution is toput the burden of this job entirely on the parser.
Theparser will eventually disambiguate all the descriptionsand pick one per object, for a given reading of the sen-tence.
This is what the parser is expected to do for dis-ambiguating the standard POS, unless a separate POSdisambiguation module is used (Church, 1988).
Manyparsers, including XTAG, use such a module ('alh'd aPOS tagger.LTAGs present a novel opportunity to reduce theamount of disambiguation done by the parser.
Wecan treat the LTAG trees associated with each lexic'alitem as more complex parts-of-speech which we call su-pertags.
In this paper, we report on some experimentson direct supertag disambiguation, without parsing inthe strict sense, using lexical preference and local lexi-cal dependencies (acquired from a corpus parsed by theXTAG system).
The information extracted from theXTAG-parsed corpus contains, for each item and itssupertag, a probability distribution of the distances ofother items and their supertags that are expectcd by it..We have devised a method somewhat akin to tile staredard POS tagger that disambiguates supertags without53doing any parsing.
'File idea of using complex descriptions for primitivesto capture constraints locally has some precursors in AI.For example, the Waltz algorithm (Waltz, 1975) for la-I)eling vertices of polygonal solid objects can be thoughtof in these terms, although it is not usually describedin this way.
There is no statistical computations in theWaltz algorithm, however.
The supertag disambigua-tion experiments, as far as we know, are the first touse these ideas in the linguistic context.
Of course, we:ds(~ show how the supertag disambiguation naturallylends itself to the application of statistical techniques.I1, tl,, lbllowing sections we will briefly describe ourapproach and some preliminary results of supertag dis-ambiguation as an illustration of our main theme: therelationship of the complexity of descriptions of primi-tives to local statistical computations.
A more completeanalysis of this technique and experimental results willeventually be reported elsewhere.Lexicalized Tree Adjoining Grammarsl,exicalized Tree Adjoining Grammar (LTAG) is a lex-icalized tree rewriting grammar formalism (Schabes,1990).
The primary structures of LTAG are called EL-EMEN'FARY TREES.
Each elementary tree has a lexi-cal item (anchor) on its frontier and serves as a com-plt~x description of the anchor.
An elementary treeprovides a domain of locality larger than that pro-vided by CFG rules over which syntactic and semantic(predicate-argument) constraints can be specified.
El-ementary trees are of two kinds: INITIAL TREES andAUXI,,IARY TREES.
Examples of initial trees (as) and~u\]xi\[iary trees (,Ss) are shown in Figure 1.
Nodes onth(.
frontier of initial trees are marked as substitutionsites by a '~', while exactly one node on the frontier~)\[" an auxiliary tree, whose label matches the label ofthe root of the tree, is marked as a foot node by a ' . '
.
'l'hv other nodes on the frontier of an auxiliary tree aremarked as substitution sites.
LTAG factors out recur-si()n f,-om the statement of the syntactic dependencies.Eh,n,,,,,tary tr~,es (initial and auxiliary) are the domainI;,r sp,,cifying dependencies.
Recursion is specified viai,h~" auxiliary trees.Hcm('nt.ary trees are combined by Subst i tu t ion  andAdjunct i , )n  operations.
Substitution inserts elemen-l;iry I.i',.,~s at the substitution odes of other elementarytrees.
Adjunction inserts auxiliary trees into elemen-tary trees at the node whose label is the same as theroot label of the auxiliary tree.
As an example, thecomponent trees ( as, c~2, aa, c~4,/38, as, as), shown inFigure 1 can be combined to form the parse tree for thesentence John saw a man with the telescope 4 as follows:1. ors substitutes at the NP0 node in a2.2.
aa substitutes at the DetP node in c~4, the result ofwhich is substituted at the NP1 node in c~.3.
a5 substitutes at the DetP node in as, the result ofwhich is substituted at the NP node in/3s.4.
The result of step (3) above adjoins to the VP nodeof the result of step (2).
The resulting parse tree isshown in Figure 2.The process of combining the elementary trees thatyield a parse of the sentence is represented by thederivation tree, shown in Figure 2.
The nodes of thederivation tree are the tree names that are anchored bythe appropriate lexical item.
The composition opera-tion is indicated by the nature of the arcs-broken linefor substitution and bold line for adjunction-while theaddress of the operation is indicated as part of the nodelabel.
The derivation tree can also be interpreted as adependency graph with unlabeled arcs between wordsof the sentence as shown in Figure 2.We will call the elementary trees associated with eachlexical item super part-of-speech tags or supertags.4The parse with the PP attached to the NP has not beenshown.54NPDetP $ NJohnSrV NIpIC~2DetPIDJI~3NPDetP,I, N NS AP NIg I Imn .kh~4 ~01DetPIDItheof 5NPDetP $ NItelescopeN,\]JohnSr~t\]~ vpHA IAImaw~7DetP rD D~PpIiA ~HA P NKIDet P ?D DetPfItheN,Ntele~:ope~7NIJohn0C8Av ~IDetPAD Dd,I8INImvP ITP NI~I,IwUhDetPIthe0~9 ~10 O~ll ~8 ~12NPINItelescopeOr18Figure 1: E lementary  t rees  o f  LTAG55S,NPINIJohnVPV NPmw DetP NI ID ~nIiPPP NIPwith I~tP NI IId~~..,2\[Mw\]a_8\[John\] (1) p~8\[with\] (2) a_4\[man\] (2.2)I !a_6\[telescope\] (2.2) a.fl\[a\] (1)IIa_S\[the\] (1)Parse Tree Derivation TreeMWJ-----7-----------....John withItelueopeIIaDependency GraphFigure 2: S t ruc tures  of  LTAGExample  of  Super tagg ingAs a result of localization in LTAG, a lexical item maybe associated with more than one supertag.
The ex-ample in Figure 3 illustrates the initial set of supertagsa.,~sigm~d to each word of the sentence John saw a manwith the telescope.
The order of the supertags for eachh'xi~'al item in tile example is completely irrelevant.I"iglire 3 also shows the final supertag sequence assignedI,y the s.pertagger, which picks the best supertag se-q.,,mlce .sing statistical information (described in thev.,,x!
s,,cl.i(m) ahout individual supertags and their de-p,'mh'm:i~s on other supertags.
The chosen supertagsaxe combined to derive a parse, as explained in the pre-vious section.The parser without the supertagger would have to pro-cess combinations ofthe entire set of 28 trees; the parserwith it need only process combinations of 7 trees.Dependency model of SupertaggingOne might think that a n-gram model of standard POStagging would be applicable to supertagging as well.However, in the n-gram model for standard POS tag-ging, dependencies between parts-of-speech of wordsthat appear beyond the n-word window cannot be incor-porated into the model.
This limitation does not have56Sentence:Initial Supertag set:Final Assignment:John saw a, man with the telescope.~2 a7 ~3 ~4 ~S & fir~8 if9 fflO ~11 flS ff12 ~i3~8 ~2 ~3 ~4 ~8 ~5 06Figure 3: Supertag Assignment for John saw a man with the telescopea significant effect on the performance of a standardtrigram POS tagger, since it is rare for dependenciesto occur between POS tags beyond a three-word win-dow.
However, since dependencies between supertagsdo not occur in a fixed sized window, the n-gram modelis unsuitable for supertagging.
This limitation can beovercome if no a priori bound is set on the size of thewindow, but instead a probability distribution of thedistances of the dependent supertags for each supertagis maintained.
A supertag is dependent on anothersupertag if the former substitutes or adjoins into thelater.Exper iments .and  Resu l tsTable (1) shows the data required for the dependencymodel of supertag disambiguation.
Ideally each entrywould be indexed by a (word, supertag) pair but, dueto sparseness of data, we have backed-off to a (POS,supertag) pair.
Each entry contains the following infor-mation.?
POS and Supertag pair.?
List of + and - ,  representing the direction of thedependent supertags with respect o the indexed su-pertag.
(Size of this list indicates the total numberof dependent supertags required.)?
Dependent supertag.?
Signed number epresenting the direction and the or-dinal position of the particular dependent supertagmentioned in the entry from the position of the in-dexed supertag.?
A probability of occurrence of such a dependency.The sum probability over all the dependent supcrt:agsat all ordinal positions in the same direction is one.For example, the fourth entry in the Table 1 readsthat the tree a2, anchored by a verb (V), has a left,and a right dependent ( - ,  +) and the first word tothe left ( -1 )  with the tree as serves as a dependent ofthe current word.
The strength of this association isrepresented by the probabilit3/0.300.The dependency model of disambiguation works asfollows.
Suppose a2 is a member of the set of supertagsassociated with a word at position n in the sentence.The algorithm proceeds to satis|~ the dependency re-quirement of a2 by picking up the dependency entriesfor each of the directions.
It picks a dependency dataentry (fourth entry, say) from the database that is in-dexed by a2 and proceeds to sct up a path with thefirst word to the left that has the dependent supertag(as) as a member of its set of supertags.
If the first.word that has as as a member of its set of supertagsis at position m, then an arc is set up between c~ andas.. Also, the arc is verified so that it does not kite-string-tangle s with any other arcs in the path up toa2.
The path probability up to a2 is incremcntcd bylog0.300 to reflect the success of the match.
The pathprobability up to as incorporates the unigram proba-bility of as.
On the other hand, if no word is foundthat has as as a member of its set of supertags thenthe entry is ignored.
A successflH supertag sequence isone which assigns a supertag to each position such thatSTwo arcs (a,c) and (b,d) kite-string-tangle if a < b <c<dorb<a<d<c.57(P.O.S,Supertag)(D,as)Dire'cti'on ofDependentSupertag()DependentSupertagOrdinalposition Prob()( - )  a3 -1  0.999(-, +) -1  0.300(V,o 2) (-, +) 1 0.374-Table 1: Dependency Data,'m'h supertag has all of its dependents and maximizesthe accumulated path probability.
The direction of thedcp~mdcmt supertag and the probability information areus?.,d t.o prune the search.
A more detailed and formaldescription of this algorithm will appear elsewhere."l'l/t.
implementation a d testing of this model of su-I,,'rl.ag disanlbiguation is underway.
Preliminary exper-ilm,ld.s oil short fragments how a success rate of 88%i.e.a, sequence of correct supertags i  assigned.Data CollectionThe data needed for disambiguating supertags (Sec-t.ion ) have been collected by parsing the Wall StreetJournal s. IBM-manual and ATIS corpora using thewide-cow:rag c English grammar being developed aspart of the XTAG system (XTAG Tech.
Report, 1994).The parses generated for these sentences are not sub-.iectcd to any kind of filtering or selection.
All thederivation structures are used in the collection of thesta.l.istics.XTAG is a large ongoing project to develop a wide-cov,.rage grammar for English, based on the LTAG for-realism.
It also serves as an LTAG grammar devel-olnuent system and includes a predictive left-to-rightparser, a morphological analyzer and a POS tagger.The wide-coverage English grammar of the XTAG sys-t,.m contains 317,000 inflected items in the morphology(21;L000 h~r nouns amt 46,500 for verbs among others)and 37,00(I eul.ries in the syntactic lexicon.
The syntac-tic h,xicon associates words with the trees that they an-,'l,,r.
There arc 385 l.rt'cs in all, in the grammar whichis ,',,,Ul,.scd of 411 dilG'rcut sul~catcgorization frames.
'~S~.ntuuces of length <_ 15 words.Each word in the syntactic lexicon, on the average, de-pending on the standard POS of the word, is an anchorfor about 8 to 40 elementary trees.ConclusionIn this paper we have shown that increasing the com-plexity of descriptions of primitive objects, lexical itemsin the linguistic context, enables more complex con-straints to be applied locally.
However, increasing thecomplexity of descriptions greatly increases the num-ber of such descriptions for the primitive object.
In alexicalized grammar such as LTAG each lexical item isassociated with complex descriptions (supertags) on theaverage of 10 descriptions.
A parser for LTAG,  givena sentence, disambiguates a large set of supertags toselect one supertag for each lexical item before combin-ing them to derive a parse of the sentence.
We havepresented a new technique that performs the disam-biguation of supertags using local information such aslexical preference and local lexical dependencies as anillustration of our main theme of the relationship ofcomplexity of descriptions of primitives to local statis-tical computations.
This technique, like POS disam-biguation, reduces the disambiguation task that needsto be done by the parser.
After the disambiguation, wehave effectively completed the parse of the sentence andthe parser needs 'only' to complete the adjunctions andsubstitutions.ReferencesKenneth Ward Church.
1988.
A Stochastic Parts Pro-gram and Nouu Phrase Parser lbr Unrestricted Text.In gnd Applied Natural Language Processing Confer-ence 1988.58Philip Resnik.
1992.
Probabilistic Tree-AdjoiningGrammar as a Framework for Statistical NaturalLanguage Processing Proceedings of the FourteenthInternational Conference on Computational Linguis-tics (COLING '9~), Nantes, France, July 1992Yves Schabes, Anne Abeill~, and Aravind K. Joshi.1988.
Parsing strategies with 'lexicalized' grammars:Application to tree adjoining rammars.
In Proceed-ings of the 12 ta International Conference on Compu-tational Linguistics (COLING'88), Budapest, Hun-gary, August.Yves Schabes.
1990.
Mathematical nd ComputationalAspects of Lexicalized Grammars.
Ph.D. thesis, Uni-versity of Pennsylvania, Philadelphia, PA, August1990.
Available as technical report (MS-CIS-90-48,LINC LAB179) from the Department of Computerand Information Science.Yves Schabes.
1992.
Stochastic Lexicalized Tree-Adjoining Grammars Proceedings of the FourteenthInternational Conference on Computational Linguis-tics (COLING 'g2), Nantes, France, July 1992.David Waltz.
1975.
Understanding Line Drawings ofScenes with Shadows in Psychology of Computer Vi-sion by Patrick Winston, 1975.XTAG Technical Report.
1994.
Department of Com-puter and Information Sciences, University of Penn-sylvania, Philadelphia, PA.
In progress.59
