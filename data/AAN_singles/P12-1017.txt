Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156?164,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsDeciphering Foreign Language by Combining Language Models andContext VectorsMalte Nuhn and Arne Mauser?
and Hermann NeyHuman Language Technology and Pattern Recognition GroupRWTH Aachen University, Germany<surname>@cs.rwth-aachen.deAbstractIn this paper we show how to train statis-tical machine translation systems on real-life tasks using only non-parallel monolingualdata from two languages.
We present a mod-ification of the method shown in (Ravi andKnight, 2011) that is scalable to vocabularysizes of several thousand words.
On the taskshown in (Ravi and Knight, 2011) we obtainbetter results with only 5% of the computa-tional effort when running our method withan n-gram language model.
The efficiencyimprovement of our method allows us to runexperiments with vocabulary sizes of around5,000 words, such as a non-parallel version ofthe VERBMOBIL corpus.
We also report re-sults using data from the monolingual Frenchand English GIGAWORD corpora.1 IntroductionIt has long been a vision of science fiction writersand scientists to be able to universally communi-cate in all languages.
In these visions, even previ-ously unknown languages can be learned automati-cally from analyzing foreign language input.In this work, we attempt to learn statistical trans-lation models from only monolingual data in thesource and target language.
The reasoning behindthis idea is that the elements of languages share sta-tistical similarities that can be automatically identi-fied and matched with other languages.This work is a big step towards large-scale andlarge-vocabulary unsupervised training of statisticaltranslation models.
Previous approaches have facedconstraints in vocabulary or data size.
We show how?Author now at Google Inc., amauser@google.com.to scale unsupervised training to real-life transla-tion tasks and how large-scale experiments can bedone.
Monolingual data is more readily available,if not abundant compared to true parallel or evenjust translated data.
Learning from only monolin-gual data in real-life translation tasks could improveespecially low resource language pairs where few orno parallel texts are available.In addition to that, this approach offers the op-portunity to decipher new or unknown languagesand derive translations based solely on the availablemonolingual data.
While we do tackle the full unsu-pervised learning task for MT, we make some verybasic assumptions about the languages we are deal-ing with:1.
We have large amounts of data available insource and target language.
This is not a verystrong assumption as books and text on the in-ternet are readily available for almost all lan-guages.2.
We can divide the given text in tokens andsentence-like units.
This implies that we knowenough about the language to tokenize andsentence-split a given text.
Again, for the vastmajority of languages, this is not a strong re-striction.3.
The writing system is one-dimensional left-to-right.
It has been shown (Lin and Knight, 2006)that the writing direction can be determinedseparately and therefore this assumption doesnot pose a real restriction.Previous approaches to unsupervised training forSMT prove feasible only for vocabulary sizes up toaround 500 words (Ravi and Knight, 2011) and data156sets of roughly 15,000 sentences containing onlyabout 4 tokens per sentence on average.
Real dataas it occurs in texts such as web pages or news textsdoes not meet any of these characteristics.In this work, we will develop, describe, andevaluate methods for large vocabulary unsupervisedlearning of machine translation models suitable forreal-world tasks.
The remainder of this paper isstructured as follows: In Section 2, we will reviewthe related work and describe how our approach ex-tends existing work.
Section 3 describes the modeland training criterion used in this work.
The im-plementation and the training of this model is thendescribed in Section 5 and experimentally evaluatedin Section 6.2 Related WorkUnsupervised training of statistical translations sys-tems without parallel data and related problems havebeen addressed before.
In this section, we will re-view previous approaches and highlight similaritiesand differences to our work.
Several steps have beenmade in this area, such as (Knight and Yamada,1999), (Ravi and Knight, 2008), or (Snyder et al,2010), to name just a few.
The main difference ofour work is, that it allows for much larger vocab-ulary sizes and more data to be used than previouswork while at the same time not being dependent onseed lexica and/or any other knowledge of the lan-guages.Close to the methods described in this work,Ravi and Knight (2011) treat training and transla-tion without parallel data as a deciphering prob-lem.
Their best performing approach uses an EM-Algorithm to train a generative word based trans-lation model.
They perform experiments on aSpanish/English task with vocabulary sizes of about500 words and achieve a performance of around20 BLEU compared to 70 BLEU obtained by a sys-tem that was trained on parallel data.
Our work usesthe same training criterion and is based on the samegenerative story.
However, we use a new trainingprocedure whose critical parts have constant timeand memory complexity with respect to the vocab-ulary size so that our methods can scale to muchlarger vocabulary sizes while also being faster.In a different approach, Koehn and Knight (2002)induce a bilingual lexicon from only non-paralleldata.
To achieve this they use a seed lexicon whichthey systematically extend by using orthographic aswell as distributional features such as context, andfrequency.
They perform their experiments on non-parallel German-English news texts, and test theirmappings against a bilingual lexicon.
We use agreedy method similar to (Koehn and Knight, 2002)for extending a given lexicon, and we implicitly alsouse the frequency as a feature.
However, we performfully unsupervised training and do not start with aseed lexicon or use linguistic features.Similarly, Haghighi et al (2008) induce a one-to-one translation lexicon only from non-parallelmonolingual data.
Also starting with a seed lexi-con, they use a generative model based on canoni-cal correlation analysis to systematically extend thelexicon using context as well as spelling features.They evaluate their method on a variety of tasks,ranging from inherently parallel data (EUROPARL)to unrelated corpora (100k sentences of the GIGA-WORD corpus).
They report F-measure scores of theinduced entries between 30 to 70.
As mentionedabove, our work neither uses a seed lexicon nor or-thographic features.3 Translation ModelIn this section, we describe the statistical trainingcriterion and the translation model that is trained us-ing monolingual data.
In addition to the mathemat-ical formulation of the model we describe approxi-mations used.Throughout this work, we denote the source lan-guage words as f and target language words as e.The source vocabulary is Vf and we write the sizeof this vocabulary as |Vf |.
The same notation holdsfor the target vocabulary with Ve and |Ve|.As training criterion for the translation model?sparameters ?, Ravi and Knight (2011) suggestarg max????
?f?eP (e) ?
p?
(f |e)???
(1)We would like to obtain ?
from Equation 1 usingthe EM Algorithm (Dempster et al, 1977).
Thisbecomes increasingly difficult with more complextranslation models.
Therefore, we use a simplified157translation model that still contains all basic phe-nomena of a generic translation process.
We formu-late the translation process with the same generativestory presented in (Ravi and Knight, 2011):1.
Stochastically generate the target sentence ac-cording to an n-gram language model.2.
Insert NULL tokens between any two adjacentpositions of the target string with uniform prob-ability.3.
For each target token ei (including NULL)choose a foreign translation fi (includingNULL) with probability P?(fi|ei).4.
Locally reorder any two adjacent foreign wordsfi?1, fi with probability P (SWAP) = 0.1.5.
Remove the remaining NULL tokens.In practice, however, it is not feasible to deal withthe full parameter table P?
(fi|ei) which models thelexicon.
Instead we only allow translation modelswhere for each source word f the number of wordse?
with P (f |e?)
6= 0 is below some fixed value.
Wewill refer to this value as the maximum number ofcandidates of the translation model and denote itwith NC .
Note that for a given e this does not nec-essarily restrict the number of entries P (f ?|e) 6= 0.Also note that with a fixed value of NC , time andmemory complexity of the EM step isO(1) with re-spect to |Ve| and |Vf |.In the following we divide the problem of maxi-mizing Equation 1 into two parts:1.
Determining a set of active lexicon entries.2.
Choosing the translation probabilities for thegiven set of active lexicon entries.The second task can be achieved by running theEM algorithm on the restricted translation model.We deal with the first task in the following section.4 Monolingual Context SimilarityAs described in Section 3 we need some mecha-nism to iteratively choose an active set of translationcandidates.
Based on the assumption that some ofthe active candidates and their respective probabili-ties are already correct, we induce new active candi-dates.
In the context of information retrieval, Saltonet al (1975) introduce a document space where eachdocument identified by one or more index terms isrepresented by a high dimensional vector of termweights.
Given two vectors v1 and v2 of two doc-uments it is then possible to calculate a similaritycoefficient between those given documents (whichis usually denoted as s(v1, v2)).
Similar to this werepresent source and target words in a high dimen-sional vector space of target word weights which wecall context vectors and use a similarity coefficientto find possible translation pairs.
We first initializethese context vectors using the following procedure:1.
Using only the monolingual data for the targetlanguage, prepare the context vectors vei withentries vei,ej :(a) Initialize all vei,ej = 0(b) For each target sentence E:For each word ei in E:For each word ej 6= ei in E:vei,ej = vei,ej + 1.
(c) Normalize each vector vei such that?ej(vei,ej )2 != 1 holds.Using the notation ei =(ej : vei,ej , .
.
.
)thesevectors might for example look likework = (early : 0.2, late : 0.1, .
.
.
)time = (early : 0.2, late : 0.2, .
.
.
).2.
Prepare context vectors vfi,ej for the sourcelanguage using only the monolingual data forthe source language and the translation model?scurrent parameter estimate ?
:(a) Initialize all vfi,ej = 0(b) Let E??
(F ) denote the most probabletranslation of the foreign sentence F ob-tained by using the current estimate ?.
(c) For each source sentence F :For each word fi in F :For each word ej 6= E?
(fi)1 inE?
(F ):vfi,ej = vfi,ej + 1(d) Normalize each vector vfi such that?ej(vfi,ej )2 != 1 holds.1denoting that ej is not the translation of fi in E?
(F )158Adapting the notation described above, thesevectors might for example look likeArbeit = (early : 0.25, late : 0.05, .
.
.
)Zeit = (early : 0.15, late : 0.25, .
.
.
)Once we have set up the context vectors ve andvf , we can retrieve translation candidates for somesource word f by finding those words e?
that maxi-mize the similarity coefficient s(ve?
, vf ), as well ascandidates for a given target word e by finding thosewords f ?
that maximize s(ve, vf ?).
In our implemen-tation we use the Euclidean distanced(ve, vf ) = ||ve ?
vf ||2.
(2)as distance measure.2 The normalization of contextvectors described above is motivated by the fact thatthe context vectors should be invariant with respectto the absolute number of occurrences of words.3Instead of just finding the best candidates for agiven word, we are interested in an assignment thatinvolves all source and target words, minimizing thesum of distances between the assigned words.
Incase of a one-to-one mapping the problem of assign-ing translation candidates such that the sum of dis-tances is minimal can be solved optimally in poly-nomial time using the hungarian algorithm (Kuhn,1955).
In our case we are dealing with a many-to-many assignment that needs to satisfy the max-imum number of candidates constraints.
For this,we solve the problem in a greedy fashion by simplychoosing the best pairs (e, f) first.
As soon as a tar-get word e or source word f has reached the limitof maximum candidates, we skip all further candi-dates for that word e (or f respectively).
This stepinvolves calculating and sorting all |Ve| ?
|Vf | dis-tances which can be done in time O(V 2 ?
log(V )),with V = max(|Ve|, |Vf |).
A simplified example ofthis procedure is depicted in Figure 1.
The examplealready shows that the assignment obtained by thisalgorithm is in general not optimal.2We then obtain pairs (e, f) that minimize d.3This gives the same similarity ordering as using un-normalized vectors with the cosine similarity measureve?vf||ve||2?||vf ||2which can be interpreted as measuring the cosineof the angle between the vectors, see (Manning et al, 2008).Still it is noteworthy that this procedure is not equivalent to thetf-IDF context vectors described in (Salton et al, 1975).xytime (e)Arbeit (f)work (e)Zeit (f)Figure 1: Hypothetical example for a greedy one-to-oneassignment of translation candidates.
The optimal assign-ment would contain (time,Zeit) and (work,Arbeit).5 Training Algorithm and ImplementationGiven the model presented in Section 3 and themethods illustrated in Section 4, we now describehow to train this model.As described in Section 4, the overall procedureis divided into two alternating steps: After initializa-tion we first perform EM training of the translationmodel for 20-30 iterations using a 2-gram or 3-gramlanguage model in the target language.
With the ob-tained best translations we induce new translationcandidates using context similarity.
This procedureis depicted in Figure 2.5.1 InitializationLet NC be the maximum number of candidates persource word we allow, Ve and Vf be the target/sourcevocabulary and r(e) and r(f) the frequency rank ofa source/target word.
Each word f ?
Vf with fre-quency rank r(f) is assigned to all words e ?
Vewith frequency rankr(e) ?
[ start(f) , end(f) ] (3)wherestart(f) = max(0 , min(|Ve| ?Nc ,?|Ve||Vf |?
r(f)?Nc2?
))(4)end(f) = min (start(f) +Nc, |Ve|) .
(5)This defines a diagonal beam4 when visualizingthe lexicon entries in a matrix where both sourceand target words are sorted by their frequency rank.However, note that the result of sorting by frequency4The diagonal has some artifacts for the highest and lowestfrequency ranks.
See, for example, left side of Figure 2.159Initializationtargetwordssource wordsEMIterationstargetwordssource words ContextVectorstargetwordssource wordsEMIterations.
.
.Figure 2: Visualization of the training procedure.
The big rectangles represent word lexica in different stages of thetraining procedure.
The small rectangles represent word pairs (e, f) for which e is a translation candidate of f , whiledots represent word pairs (e, f) for which this is not the case.
Source and target words are sorted by frequency so thatthe most frequent source words appear on the very left, and the most frequent target words appear at the very bottom.and thus the frequency ranks are not unique whenthere are words with the same frequency.
In thiscase, we initially obtain some not further specifiedfrequency ordering, which is then kept throughoutthe procedure.This initialization proves useful as we show bytaking an IBM1 lexicon P (f |e) extracted on theparallel VERBMOBIL corpus (Wahlster, 2000): Foreach word e we calculate the weighted rank differ-ence?ravg(e) =?fP (f |e) ?
|(r(e)?
r(f)| (6)and count how many of those weighted rank dif-ferences are smaller than a given value NC2 .
Herewe see that for about 1% of the words the weightedrank difference lies withinNC = 50, and even about3% for NC = 150 respectively.
This shows that theinitialization provides a first solid guess of possibletranslations.5.2 EM AlgorithmThe generative story described in Section 3 is im-plemented as a cascade of a permutation, insertion,lexicon, deletion and language model finite statetransducers using OpenFST (Allauzen et al, 2007).Our FST representation of the LM makes use offailure transitions as described in (Allauzen et al,2003).
We use the forward-backward algorithm onthe composed transducers to efficiently train the lex-icon model using the EM algorithm.5.3 Context Vector StepGiven the trained parameters ?
from the previous runof the EM algorithm we set the context vectors veand vf up as described in Section 4.
We then calcu-late and sort all |Ve|?|Vf | distances which proves fea-sible in a few CPU hours even for vocabulary sizesof more than 50,000 words.
This is achieved withthe GNU SORT tool, which uses external sorting forsorting large amounts of data.To set up the new lexicon we keep the bNC2 cbest translations for each source word with respectto P (e|f), which we obtained in the previous EMrun.
Experiments showed that it is helpful to alsolimit the number of candidates per target words.
Wetherefore prune the resulting lexicon using P (f |e)to a maximum of bN?C2 c candidates per target wordafterwards.
Then we fill the lexicon with new can-didates using the previously sorted list of candidatepairs such that the final lexicon has at most NCcandidates per source word and at most N ?C can-didates per target word.
We set N ?C to some valueN ?C > NC .
All experiments in this work were runwith N ?C = 300.
Values of N?C ?
NC seem to pro-duce poorer results.
Not limiting the number of can-didates per target word at all also typically results inweaker performance.
After the lexicon is filled withcandidates, we initialize the probabilities to be uni-form.
With this new lexicon the process is iteratedstarting with the EM training.6 Experimental EvaluationWe evaluate our method on three different corpora.At first we apply our method to non-parallel Span-ish/English data that is based on the OPUS corpus(Tiedemann, 2009) and that was also used in (Raviand Knight, 2011).
We show that our method per-forms better by 1.6 BLEU than the best performingmethod described in (Ravi and Knight, 2011) while160Name Lang.
Sent.
Words Voc.OPUSSpanish 13,181 39,185 562English 19,770 61,835 411VERBMOBILGerman 27,861 282,831 5,964English 27,862 294,902 3,723GIGAWORDFrench 100,000 1,725,993 68,259English 100,000 1,788,025 64,621Table 1: Statistics of the corpora used in this paper.being approximately 15 to 20 times faster than theirn-gram based approach.After that we apply our method to a non-parallelversion of the German/English VERBMOBIL corpus,which has a vocabulary size of 6,000 words on theGerman side, and 3,500 words on the target side andwhich thereby is approximately one order of magni-tude larger than the previous OPUS experiment.We finally run our system on a subset of the non-parallel French/English GIGAWORD corpus, whichhas a vocabulary size of 60,000 words for bothFrench and English.
We show first interesting re-sults on such a big task.In case of the OPUS and VERBMOBIL corpus,we evaluate the results using BLEU (Papineni et al,2002) and TER (Snover et al, 2006) to referencetranslations.
We report all scores in percent.
ForBLEU higher values are better, for TER lower val-ues are better.
We also compare the results on thesecorpora to a system trained on parallel data.In case of the GIGAWORD corpus we show lexi-con entries obtained during training.6.1 OPUS Subtitle Corpus6.1.1 Experimental SetupWe apply our method to the corpus described inTable 6.
This exact corpus was also used in (Raviand Knight, 2011).
The best performing methodsin (Ravi and Knight, 2011) use the full 411 ?
579lexicon model and apply standard EM training.
Us-ing a 2-gram LM they obtain 15.3 BLEU and witha whole segment LM, they achieve 19.3 BLEU.
Incomparison to this baseline we run our algorithmwith NC = 50 candidates per source word for both,a 2-gram and a 3-gram LM.
We use 30 EM iterationsbetween each context vector step.
For both cases werun 7 EM+Context cycles.6.1.2 ResultsFigure 3 and Figure 4 show the evolution of BLEUand TER scores for applying our method using a 2-gram and a 3-gram LM.In case of the 2-gram LM (Figure 3) the transla-tion quality increases until it reaches a plateau after5 EM+Context cycles.
In case of the 3-gram LM(Figure 4) the statement only holds with respect toTER.
It is notable that during the first iterations TERonly improves very little until a large chunk of thelanguage unravels after the third iteration.
This be-havior may be caused by the fact that the corpus onlyprovides a relatively small amount of context infor-mation for each word, since sentence lengths are 3-4words on average.0 1 2 3 4 5 6 7 8810121416 Full EM best (BLEU)IterationBLEU6668707274767880TERBLEUTERFigure 3: Results on the OPUS corpus with a 2-gram LM,NC = 50, and 30 EM iterations between each contextvector step.
The dashed line shows the best result using a2-gram LM in (Ravi and Knight, 2011).Table 2 summarizes these results and comparesthem with (Ravi and Knight, 2011).
Our 3-grambased method performs by 1.6 BLEU better thantheir best system which is a statistically significantimprovement at 95% confidence level.
Furthermore,Table 2 compares the CPU time needed for training.Our 3-gram based method is 15-20 times faster thanrunning the EM based training procedure presentedin (Ravi and Knight, 2011) with a 3-gram LM5.5(Ravi and Knight, 2011) only report results using a 2-gramLM and a whole-segment LM.1610 1 2 3 4 5 6 7 881012141618202224Full EM best (BLEU)IterationBLEU6466687072TERBLEUTERFigure 4: Results on the OPUS corpus with a 3-gram LM,NC = 50, and 30 EM iterations between each contextvector step.
The dashed line shows the best result using awhole-segment LM in (Ravi and Knight, 2011)Method CPU BLEU TEREM, 2-gram LM411 cand.
p. source word(Ravi and Knight, 2011)?850h6 15.3 ?EM, Whole-segment LM411 cand.
p. source word(Ravi and Knight, 2011)?7 19.3 ?EM+Context, 2-gram LM50 cand.
p. source word(this work)50h8 15.2 66.6EM+Context, 3-gram LM50 cand.
p. source word(this work)200h8 20.9 64.5Table 2: Results obtained on the OPUS corpus.To summarize: Our method is significantly fasterthan n-gram LM based approaches and obtains bet-ter results than any previously published method.6Estimated by running full EM using the 2-gram LM usingour implementation for 90 Iterations yielding 15.2 BLEU.7?4,000h when running full EM using a 3-gram LM, usingour implementation.
Estimated by running only the first itera-tion and by assuming that the final result will be obtained after90 iterations.
However, (Ravi and Knight, 2011) report resultsusing a whole segment LM, assigning P (e) > 0 only to se-quences seen in training.
This seems to work for the given taskbut we believe that it can not be a general replacement for higherorder n-gram LMs.8Estimated by running our method for 5?
30 iterations.6.2 VERBMOBIL Corpus6.2.1 Experimental SetupThe VERBMOBIL corpus is a German/Englishcorpus dealing with short sentences for making ap-pointments.
We prepared a non-parallel subset ofthe original VERBMOBIL (Wahlster, 2000) by split-ting the corpus into two parts and then selecting onlythe German side from the first half, and the Englishside from the second half such that the target sideis not the translation of the source side.
The sourceand target vocabularies of the resulting non-parallelcorpus are both more than 9 times bigger comparedto the OPUS vocabularies.
Also the total amount ofword tokens is more than 5 times larger comparedto the OPUS corpus.
Table 6 shows the statistics ofthis corpus.
We run our method for 5 EM+Contextcycles (30 EM iterations each) using a 2-gram LM.After that we run another five EM+Context cyclesusing a 3-gram LM.6.2.2 ResultsOur results on the VERBMOBIL corpus are sum-marized in Table 3.
Even on this more complextask our method achieves encouraging results: TheMethod BLEU TER5?
30 Iterations EM+Context50 cand.
p. source word, 2-gram LM11.7 67.4+ 5?
30 Iterations EM+Context50 cand.
p. source word, 3-gram LM15.5 63.2Table 3: Results obtained on the VERBMOBIL corpus.translation quality increases from iteration to itera-tion until the algorithm finally reaches 11.7 BLEUusing only the 2-gram LM.
Running further fivecycles using a 3-gram LM achieves a final perfor-mance of 15.5 BLEU.
Och (2002) reports results of48.2 BLEU for a single-word based translation sys-tem and 56.1 BLEU using the alignment templateapproach, both trained on parallel data.
However, itshould be noted that our experiment only uses 50%of the original VERBMOBIL training data to simulatea truly non-parallel setup.162Iter.
e p(f1|e) f1 p(f2|e) f2 p(f3|e) f3 p(f4|e) f4 p(f5|e) f51.
the 0.43 la 0.31 l?
0.11 une 0.04 le 0.04 les2.
several 0.57 plusieurs 0.21 les 0.09 des 0.03 nombreuses 0.02 deux3.
where 0.63 ou` 0.17 mais 0.06 indique 0.04 pre?cise 0.02 appelle4.
see 0.49 e?viter 0.09 effet 0.09 voir 0.05 envisager 0.04 dire5.
January 0.25 octobre 0.22 mars 0.09 juillet 0.07 aou?t 0.07 janvier?
Germany 0.24 Italie 0.12 Espagne 0.06 Japon 0.05 retour 0.05 SuisseTable 4: Lexicon entries obtained by running our method on the non-parallel GIGAWORD corpus.
The first columnshows in which iteration the algorithm found the first correct translations f (compared to a parallely trained lexicon)among the top 5 candidates6.3 GIGAWORD6.3.1 Experimental SetupThis setup is based on a subset of the monolingualGIGAWORD corpus.
We selected 100,000 Frenchsentences from the news agency AFP and 100,000sentences from the news agency Xinhua.
To have amore reliable set of training instances, we selectedonly sentences with more than 7 tokens.
Note thatthese corpora form true non-parallel data which, be-sides the length filtering, were not specifically pre-selected or pre-processed.
More details on thesenon-parallel corpora are summarized in Table 6.
Thevocabularies have a size of approximately 60,000words which is more than 100 times larger than thevocabularies of the OPUS corpus.
Also it incor-porates more than 25 times as many tokens as theOPUS corpus.After initialization, we run our method withNC = 150 candidates per source word for 20 EMiterations using a 2-gram LM.
After the first contextvector step with NC = 50 we run another 4 ?
20iterations with NC = 50 with a 2-gram LM.6.3.2 ResultsTable 4 shows example lexicon entries we ob-tained.
Note that we obtained these results by us-ing purely non-parallel data, and that we neitherused a seed lexicon, nor orthographic features to as-sign e.g.
numbers or proper names: All results areobtained using 2-gram statistics and the context ofwords only.
We find the results encouraging andthink that they show the potential of large-scale un-supervised techniques for MT in the future.7 ConclusionWe presented a method for learning statistical ma-chine translation models from non-parallel data.
Thekey to our method lies in limiting the translationmodel to a limited set of translation candidates andthen using the EM algorithm to learn the probabil-ities.
Based on the translations obtained with thismodel we obtain new translation candidates usinga context vector approach.
This method increasedthe training speed by a factor of 10-20 comparedto methods known in literature and also resultedin a 1.6 BLEU point increase compared to previ-ous approaches.
Due to this efficiency improvementwe were able to tackle larger tasks, such as a non-parallel version of the VERBMOBIL corpus havinga nearly 10 times larger vocabulary.
We also had alook at first results of our method on an even largerTask, incorporating a vocabulary of 60,000 words.We have shown that, using a limited set of trans-lation candidates, we can significantly reduce thecomputational complexity of the learning task.
Thiswork serves as a big step towards large-scale unsu-pervised training for statistical machine translationsystems.AcknowledgementsThis work was realized as part of the Quaero Pro-gramme, funded by OSEO, French State agency forinnovation.
The authors would like to thank Su-jith Ravi and Kevin Knight for providing us with theOPUS subtitle corpus and David Rybach for kindlysharing his knowledge about the OpenFST library.163ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.2003.
Generalized algorithms for constructing sta-tistical language models.
In Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics-Volume 1, pages 40?47.
Association forComputational Linguistics.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
Openfst: Ageneral and efficient weighted finite-state transducerlibrary.
In Jan Holub and Jan Zda?rek, editors, CIAA,volume 4783 of Lecture Notes in Computer Science,pages 11?23.
Springer.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the Royal StatisticalSociety, B, 39.Aria Haghighi, Percy Liang, T Berg-Kirkpatrick, andDan Klein.
2008.
Learning Bilingual Lexicons fromMonolingual Corpora.
In Proceedings of ACL08 HLT,pages 771?779.
Association for Computational Lin-guistics.Kevin Knight and Kenji Yamada.
1999.
A computa-tional approach to deciphering unknown scripts.
InACL Workshop on Unsupervised Learning in NaturalLanguage Processing, number 1, pages 37?44.
Cite-seer.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
In Pro-ceedings of the ACL02 workshop on Unsupervised lex-ical acquisition, number July, pages 9?16.
Associationfor Computational Linguistics.Harold W. Kuhn.
1955.
The Hungarian method for theassignment problem.
Naval Research Logistic Quar-terly, 2:83?97.Shou-de Lin and Kevin Knight.
2006.
Discoveringthe linear writing order of a two-dimensional ancienthieroglyphic script.
Artificial Intelligence, 170:409?421, April.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schuetze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, 1 edition, July.Franz J. Och.
2002.
Statistical Machine Translation:From Single-Word Models to Alignment Templates.Ph.D.
thesis, RWTH Aachen University, Aachen, Ger-many, October.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Sujith Ravi and Kevin Knight.
2008.
Attacking decipher-ment problems optimally with low-order n-gram mod-els.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 812?819, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Sujith Ravi and Kevin Knight.
2011.
Deciphering for-eign language.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 12?21,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.Gerard M. Salton, Andrew K. C. Wong, and Chang S.Yang.
1975.
A vector space model for automatic in-dexing.
Commun.
ACM, 18(11):613?620, November.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human Anno-tation.
In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Americas,pages 223?231, Cambridge, Massachusetts, USA, Au-gust.Benjamin Snyder, Regina Barzilay, and Kevin Knight.2010.
A statistical model for lost language decipher-ment.
In 48th Annual Meeting of the Association forComputational Linguistics, number July, pages 1048?1057.Jo?rg Tiedemann.
2009.
News from OPUS - A collec-tion of multilingual parallel corpora with tools and in-terfaces.
In N. Nicolov, K. Bontcheva, G. Angelova,and R. Mitkov, editors, Recent Advances in NaturalLanguage Processing, volume V, pages 237?248.
JohnBenjamins, Amsterdam/Philadelphia, Borovets, Bul-garia.Wolfgang Wahlster, editor.
2000.
Verbmobil: Foun-dations of speech-to-speech translations.
Springer-Verlag, Berlin.164
