Multi-Document Person Name ResolutionMichael Ben FleischmanMassachusetts Institute of Technology77 Massachusetts Ave.Cambridge, MA 02139mbf@mit.eduEduard HovyUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292hovy@isi.eduAbstractMulti-document person name resolution fo-cuses on the problem of determining if twoinstances with the same name and from dif-ferent documents refer to the same individ-ual.
We present a two-step approach inwhich a Maximum Entropy model istrained to give the probability that twonames refer to the same individual.
Wethen apply a modified agglomerative clus-tering technique to partition the instancesaccording to their referents.1 IntroArtists and philosophers have long noted that mul-tiple distinct entities are often referred to by oneand the same name (Cohen and Cohen, 1998; Mar-tinich, 2000).
Recently, this referential ambiguityof names has become of increasing concern tocomputational linguists, as well.
As the Internetincreases in size and coverage, it becomes less andless likely that a single name will refer to the sameindividual on two different web sites.
This poses agreat challenge to information retrieval (IR) andquestion-answering (QA) applications, which oftenrely on little data when responding to user queries.Another area in which referential ambiguity isproblematic involves the automatic population ofontologies with instances.
For such tasks, concept-instance pairs (such as Paul Simon/pop star) areextracted from the web, cleaned of noise, and theninserted into an already existing ontology.
Theprocess of insertion requires that concept-instancepairs that have the same referent be merged to-gether (e.g.
Paul Simon/pop star and Paul Simon/singer).
Further, instances of the same name, butwith different referents, must be distinguished (e.g.Paul Simon/pop star and Paul Simon /politician).We propose a two-step approach: first, we train amaximum entropy model to generate the probabil-ity that any two concept-instance pairs refer to oneand the same referent.
Then, a modified agglom-erative clustering technique is used to merge themost likely instances together, forming clusters thatcorrespond to individual referents.2 Related WorkWhile there has been a great deal of work oncoreference resolution within a single document,little work has focused on the challenges associatedwith resolving the reference of identical personnames across multiple documents.Mann and Yarowsky (2003) are amongst the fewwho have examined this problem.
They treat it as aclustering task, in which, a combination of features(such as, a weighted bag of words and biographicinformation extracted from the text) are given to anagglomerative clustering algorithm, which outputstwo clusters representing the two referents of thequery name.Mann and Yarowsky (2003) report results on twotypes of evaluations: using hand-annotated web-pages returned from truly ambiguous searches, theyreport precision/recall scores of 0.88/0.73; using?psuedonames?1 they report an accuracy of 86.4%.1 Borrowing from techniques in word sense disambigua-tion, they create a test set of 28 ?pseudonames?
by ran-While Mann and Yarowsky (2003) describe anumber of useful features for multi-document per-son name resolution, their technique is limited byonly allowing a set number of referent clusters.Further, as discussed below, their use of artificialtest data makes it difficult to determine how well itgeneralize to real world problems.Bagga and Baldwin (1998) also present an ex-amination of multi-document person name resolu-tion.
They first perform within-documentcoreference resolution to form coreference chainsfor each entity in each document.
They then usethe text surrounding each reference chain to createsummaries about each entity in each document.These summaries are then converted to a bag ofwords feature vector and are clustered using thestandard vector space model often employed in IR.They evaluated their system on 11 entities namedJohn Smith taken from a set of 173 New YorkTimes articles.
Using an evaluation metric similarto a weighted sum of precision and recall they getan F-measure of 0.846.Although their technique allows for the discoveryof a variable number of referents, its use ofsimplistic bag of words clustering is an inherentlylimiting aspect of their methodology.
Further, thatthey only evaluate their system, on a single personname begs the question of how well such a tech-nique would fair on a more real-world challenge.3 Maximum Entropy Model3.1 DataFleischman et al (2003) describe a dataset of con-cept-instance pairs extracted automatically from avery large corpus of newspaper articles.
The data-set (referred to here as the ACL dataset) containsapproximately 2 million pairs (of which 93% arelegitimate) in which the concept is represented by acomplex noun phrase (e.g.
president of the Uniteddomly selecting two names from a hand crafted list of 8individuals (e.g., Haifa Al-Faisal and Tom Cruise) andtreat the pair as one name with two referents.States) and the instance by a name (e.g.
WilliamJefferson Clinton).2A set of 2675 legitimate concept-instance pairswas randomly selected from the ACL dataset de-scribed above; each of these was then matchedwith another concept-instance pair that had anidentical instance name, but a different conceptname.
This set of matched pairs was hand taggedby a human annotator to reflect whether or not theidentical instance names actually referred to thesame individual.
The set was then randomly splitinto a training set of 1875 matched pairs (84% re-ferring to the same individual), a development setof 400 matched pairs (85.5% referring to the sameindividual), and a test set of 400 matched pairs(83.5% referring to the same individual).3.2 FeaturesIn designing a binary classifier to determinewhether two concept-instance pairs refer to thesame individual, we formulate a number of differ-ent features used to describe each matched pair.These features are summarized in Table 1, and de-scribed in more detail below.Name FeaturesWe use a number of methods meant to express in-formation available from the orthography of theinstance name itself.
The first of these features(Name-Common) seeks to estimate the commonal-ity of the instance name.
With this features wehope to capture the intuition that more commonnames (such as John Smith) will be more likely torefer to different individuals than more uncommonnames (such as Yasir Arafat).
We calculate thisfeature by splitting the instance name into first,middle (if necessary) and last sub-names.
We thenuse a table of name frequencies downloaded fromthe US census website to give each sub-name ascore; these scores are then multiplied together fora final value.The second name statistic feature estimates howfamous the instance name is.
With this features we2 Although the dataset includes multiple types of namedentities, we focus here only on person names.hope to capture the intuition that names of veryfamous people (such as Michael Jackson) are lesslikely to refer to different individuals than less fa-mous, yet equally common, names (such as JohnSmith).
We calculate this feature in two ways:first, we use the frequency of the instance name asit appears in the ACL dataset to give a representa-tion of how often the name appears in newspapertext (Name-Fame); second, we use the number ofhits reported on google.com for a query consistingof the quoted instance name itself (Web-Fame).These fame features are used both as is and scaledby the commonality feature described above.Web FeaturesAside from the fame features described above, weuse a number of other features derived from websearch results.
The first of which, called WebInter-section, is simply the number of hits returned for aquery using the instance name and the heads ofeach concept noun phrase in the match pair; i.e.,(name + head1 +head2).The second, called WebDifference, is the abso-lute value of the difference between the hits re-turned from a query on the instance name and justthe head of concept 1 vs. the instance name andjust the head of concept 2; i.e., abs ((name + head1)-(name +head2)).The third, called WebRatio, is the ratio betweenthe WebIntersection score and the sum of the hitsreturned when querying the instance name and justthe head of concept 1 and the instance name andjust the head of concept 2; i.e., (name + head1+head2) / ((name + head1) +(name +head2)).Overlap FeaturesIn order to capture some aspects of the contex-tual cues to referent disambiguation, we includefeatures representing the similarity between thesentential contexts from which each concept-instance pair was extracted.
The similarity metricthat we use is a simple word overlap score basedon the number of words that are shared amongstboth sentences.
We include scores in which eachnon-stop-word is treated equally (Sentence-Count),as well as, in which each non-stop-word isweighted according to its term frequency in a largecorpus (Sentence-TF).
We further include twosimilar features that only examine the overlap inthe concepts (Concept-Count and Concept-TF).Name FeaturesFeature Name DescriptionName-Common  frequency of name in census dataName-Fame frequency of name in ACL datasetWeb-Fame # of hits from name queryWeb FeaturesWeb Intersection query(name + head1 +head2)Web Differenceabs( query(name + head1)+ query(name +head2))Web Ratioquery(name + head1 +head2)/ ( qry(name + head1) +qry(name +head2))Overlap FeaturesSentence-Count# of words common tocontext of both instancesSentence-TFas above but weightedby term frequencyConcept-Count# of words common toconcept of both instancesConcept-TFas above but weightedby term frequencySemantic FeaturesJCN sem.
dist.
of Jiang and ConrathHSO sem.
dist.
of Hirst and St. OngeLCH sem.
dist.
of Leacock and ChodrowLin sem.
dist.
of LinRes sem.
dist.
of ResnikEstimated StatisticsF1 p(i1=i2 | i1?A, i2?B)F2 p(i1?A, i2?B | i1=i2)F3 p(i1?A | i2?B) + p(i2?B | i1?A)F4 p(i1?A, i2?B) / (p(i1?A) + p(I2?B))Table 1.
Features used in Max.
Ent.
model split accord-ing to feature type.Semantic FeaturesAnother important clue in determining the corefer-ence of instances is the semantic relatedness of theconcepts with which they are associated.
In orderto capture this, we employ five metrics described inthe literature that use the WordNet ontology to de-termine a semantic distance between two lexicalitems (Budanitsky and Hirst.
2001).
We use theimplementation described in Pedersen (2004) tocreate features corresponding to the scores on thefollowing metrics shown in Table 1.
Due to prob-lems associated with word sense ambiguity, wetake the maximum score amongst all possible com-binations of senses for the heads of the concepts inthe matched pair.
The final output to the model isa single similarity measure for each of the eightmetrics described in Pedersen (2004).Estimated Statistics FeaturesIn developing features useful for referent disambigua-tion, it is clear that the concept information to which wehave access is very useful.
For example, given that wesee John Edwards /politician and John Edwards/lawyer, our knowledge that politicians are often lawyersis very useful in judging referential identity.3  In order toexploit this information, we leverage the strong correla-tion between orthographic identity of instance namesand their referential identity.As described above, approximately 84% of thosematched pairs that had identical instance namesreferred to the same referent.
In a separate exami-nation, we found, not surprisingly, that nearly100% of pairs that were matched to instances withdifferent names (such as Bill Clinton vs. GeorgeClinton) referred to different referents.We take advantage of this strong correlation indeveloping features by first making the (admittedlywrong) assumption that orthographic identity isequivalent to referential identity, and then usingthat assumption to calculate a number of statisticsover the large ACL dataset.
We postulate that thenoise introduced by our assumption will be offsetby the large size of the dataset, yielding a numberof highly informative features.The statistics we calculate are as follows:P1:  The probability that instance 1 and in-stance 2 have the same referent given that in-stance 1 is paired with concept A and instance 2with concept B; i.e., p(i1=i2 | i1?A, i2?B)P2:  The probability that instance 1 is pairedwith concept A and instance 2 with concept Bgiven that instance 1 and instance 2 have thesame referent; i.e., p(i1?A, i2?B | i1=i2)P3:  The probability that instance 1 is pairedwith concept A given that instance 2 is pairedwith concept B plus the probability that instance3 It should be noted that this feature is attempting to en-code knowledge about what concepts occur together inthe real world, which is different than, what is beingencoded in the semantic features, described above.2 is paired with concept B given that instance 1is paired with concept A; i.e., p(i1?A | i2?B)+ p(i2?B | i1?A)P4:  The probability that instance 1 is pairedwith concept A and instance 2 is paired withconcept B divided by the probability that in-stance 1 is paired with concept A plus the prob-ability that instance 2 is paired with concept B;i.e., p(i1?A, i2?B) / (p(i1?A) + p(i2?B))Figure 1.
Results of Max.
Ent.
classifier on held out testdata compared to baseline (i.e., always same referent).Aside from the noise introduced by the assump-tion described above, another problem with thesefeatures arises when the derived probabilities arebased on very low frequency counts.
Thus, whenadding these features to the model, we bin eachfeature according to the number of counts that thescore was based on.3.3 ModelMaximum Entropy (Max.
Ent.)
models implementthe intuition that the best model will be the one thatis consistent with the set of constrains imposed bythe evidence, but otherwise is as uniform as possi-ble (Berger et al, 1996).
We model the probabilityof two instances having the same referent (r=[1,0])given a vector of features x according to the Max.Ent.
formulation below:?==niixxrfZxrp0i )],(exp[1)|( ?Here Zx is a normalization constant, fi(r,x) is afeature function over values of r and vector ele-ments, n is the total number of feature functions,and ?i is the weight for a given feature function.The final output of the model is the probability83.50%90.75%78%80%82%84%86%88%90%92%Baseline Max Ent%Correctgiven a feature vector that r=1; i.e., the probabilitythat the referents are the same.We train the Max.
Ent.
model using theYASMET Max.
Ent.
package (Och, 2002).
Featureweights are smoothed using Gaussian priors withmean 0.
The standard deviation of this distributionis optimized on the development set, as is the num-ber of training iterations and the probability thresh-old used to make the hard classifications reportedin the following experiment.3.4 Experimental ResultsResults for the classifier on the held out test set arereported in Figure 1.
Baseline here represents al-ways choosing the most common classification(i.e., instance referents are the same).
Figure 2represents the learning curve associated with thistask.
Figure 3 shows the effect on performance ofincrementally adding the best feature set (as deter-mined by greedily trying each one) to the model.Figure 2.
Learning curve of Max.
Ent.
model.3.5 DiscussionIt is clear from the results that this model outper-forms the baseline for this task (p>0.01) (p<0.01)(Mitchell, 1997).
Interestingly, although the num-ber of labeled examples that were used to train thesystem was by no means extravagant, it appearsfrom the learning curve that increasing the size ofthe training set will not have a large effect on clas-sifier performance.
Also of interest, Figure 3shows that the greedy feature selection techniquefound that the most powerful features for this taskare the estimated statistic features and the web fea-tures.
While the benefit of such large corpora fea-tures is not surprising, the relative lack of powerfrom the semantic and overlap features (which ex-ploit ontolological and contextual information) wassurprising.
4  In future work, we will examine howmore sophisticated similarity metrics and largerwindows of context (e.g., the whole document)might improve performance.4 ClusteringFigure 3.
Results of Max.
Ent.
classifier on held outdata using different subsets of feature types.
Featuretypes are greedily added one at a time, starting with Es-timated Statistics and ending with Semantic Features.Having generated a model to predict the probabilitythat two concept-instance pairs with the same namerefer to the same individual, we are faced with theproblem of using such a model to partition all ofour concept-instance pairs according to the indi-viduals to which they actually refer.
Although,ideally, we should be able to simply apply themodel to all possible pairs, in reality, such a meth-odology may lead to a contradiction.For example, given that the model predicts in-stance A is identical to instance B, and in addition,that instance B is identical to C, because of thetransitivity of the identity relation, we must assumethat A is identical to C.  However, if the modelpredicts that A is not identical to C, (which can anddoes occur) we must assume the model is wrong inat least one of its three predictions.4 Note that for these tests, the model parameters are notoptimized for each run; thus, the performance is slightlyworse than in Figure 1.11%12%13%14%15%16%10 100 1000# of Training Examples%Error88.3%83.5%87.0%89.3% 89.0%88.3%80%81%82%83%84%85%86%87%88%89%90%baseline stat.
+web +name +over +sem(all feats)%CorrectFollowing Ng and Cardie (2002), we address thisproblem by clustering each set of concept-instancepairs with identical names, using a form of group-average agglomerative clustering, in which thesimilarity score between instances is just the prob-ability output by the model.
Because standard ag-glomerative clustering algorithms are O(n3) ifcosign similarity metrics are not used (Manningand Schutze, 2001), we adapt the method to ourframework.
Our algorithm operates as follows5:On input D={concept-instance pairs of same name},build a fully connected graph G with vertex set D:1) Label each edge (d,d?)
in G with a score correspond-ing to the probability of identity predicted by theMax.
Ent.
model2) While the edge with max score in G > threshold:a.
Merge the two nodes connected by the edge withthe max score.b.
For each node in the grapha.
Merge the two edges connecting it to thenewly merged nodeb.
Assign the new edge a score equal to the avg.of the two old edge scores.The final output of this algorithm is a new graphin which each node represents a single referent as-sociated with a set of concept-instance pairs.
Thisalgorithm provides an efficient way, O(n2), to com-pose the pair-wise information given by the model.Further, because the only free parameter is a merg-ing threshold (which can be determined throughcross-validation) the algorithm is free to choose adifferent number of referents for each instancename it is tested on.
This is critical for the taskbecause each instance name can have any numberof referents associated with it.4.1 Test DataIn order to test clustering, we randomly selected aset of 31 instance names from the ACL dataset, 11of which referred to multiple individuals and 20 ofwhich had only a single referent6.
Each concept-5 This algorithm was developed with Hal Daume (tech-nical report, in prep.
).6 In an examination of 113 different randomly selectedinstance names from the ACL dataset we found that 32instance pair with that instance name was then ex-tracted and hand annotated such that each individ-ual referent was given a unique identifying code.We chose not to test on artificially generated testexamples (such as the pseudo-names described inMann and Yarowsky, 2003) because of our reli-ance on name orthography in feature generation(see section 3.2).
Further, such pseudo-names ig-nore the fact that names often correlate with otherfeatures (such as occupation or birthplace), and thatthey do not guarantee clean test data (i.e., the twonames chosen for artificial identity may themselveseach refer to multiple individuals).4.2 Experimental DesignIn examining the results of the clustering, we choseto use a simple clustering accuracy as our perform-ance metric.
According to this technique, wematch the output of our system to a gold standardclustering (defined by the hand annotations de-scribed above).7We compare our algorithm on the 31 sets of con-cept-instance pairs described above against twobaseline systems.
The first (baseline1) is simply asingle clustering of all pairs into one cluster; i.e.,all instances have the same referent.
The second(baseline2) is a simple greedy clustering algorithmthat sequentially adds elements to the previouscluster whose last-added element is most similar(and exceeds some threshold set by cross valida-tion).4.3 ResultsIn examining performance, we present a weightedaverage over these 31 instance sets, based on thenumber of nodes (i.e., concept-instance pairs) ineach set of instances (total nodes = 1256).
Cross-validation is used to set the threshold for both thebaseline2 and modified agglomerative algorithm.appeared only once in the dataset, 53 appeared morethan once but always referred to the same referent, and28 had multiple referents.7 While this is a relatively simple measure, we believethat, if anything, it is overly conservative, and thus,valid for the comparisons that we are making.These results are presented in Table 2.
Figure 4examines performance as a function of the numberof referents within each of the 31 instance sets.4.4 DiscussionFigure 4.
Plot of performance for modified agglomera-tive clustering and Baseline system as a function of thenumber of referents in the test set.While the algorithm we present clearly outper-forms the baseline2 method over all 31 instancesets (p<0.01), we can see that it only marginallyoutperforms our most simple baseline1 method(p<0.10) (Mitchell, 1997).
This is due to the factthat for each of the 20 instance sets that only have asingle referent, the baseline achieves a perfectscore, while the modified agglomerative methodonly achieves a score of 96.4%.
Given this aspectof the baseline, and the distribution of the data, thefact that our algorithm outperforms the baseline atall speaks to its usefulness for this task.A better sense of the usefulness of this algorithm,however, can be seen by looking at its performanceonly on instance sets with multiple referents.
Asseen in Table 3, on multiple referent instance sets,modified agglomerative clustering outperformsboth the baseline1 and baseline2 methods by a sta-tistically significant margin (p<0.01) (Mitchell,1997).5 ConclusionThe problem of cross-document person name dis-ambiguation is of growing concern in many areasof natural language processing.
We have presenteda two-step methodology for the disambiguation ofautomatically extracted concept-instance pairs.Our approach first applies a Maximum Entropymodel to all concept-instance pairs that share thesame instance name.
The output probabilities ofthis model are then inputted to a modified agglom-erative clustering algorithm that partitions the pairsaccording to the individuals to which they refer.This algorithm not only allows for a dynamicallyset number of referents, but also, outperforms twobaseline methods.A clear example of the success of this algorithmcan be seen in the output of the system for the in-stance set for Michael Jackson (Appendix A, Table2).
Here, a name that refers to many individuals isfairly well partitioned into appropriate clusters.With the instance set for Sonny Bono (Appendix A,Table 1), however, we can see why this task is sochallenging.
Here, although, Sonny Bono only re-fers to one individual, the system finds (like manyof the rest of us) that the likelihood of a singer alsobeing a politician is so low that the name must re-fer to two different people.
While this assumptionis often true (as is the case with Paul Simon), wewould have hoped that information from our weband fame features would have overridden the sys-tem?s bias in this circumstance.In future work we will examine how other fea-tures may be useful in attacking such hard cases.Also, we will examine how this technique can beapplied more generally to problems that exist be-tween non-identical, but similar names (e.g.
BillClinton vs. William Jefferson Clinton).AcknowledgementsThe authors would like to thank Regina Barzilay for herhelpful comments and advice, and Hal Daume for hisuseful insights into and discussion of the problem.
Wealso thank Deb Roy for his continued support.ReferencesA.
Bagga, and B. Baldwin.
1998.
Entity-Based Cross-Document Coreferencing Using the Vector SpaceModel.
.
COLING-ACL'98.40%50%60%70%80%90%100%1 2-3 4-5 >6# of Referents%CorrectBaseline Graph ClusteringA.
Berger, S. Della Pietra and V. Della Pietra, 1996.
AMaximum Entropy Approach to Natural LanguageProcessing.
Computational Linguistics, vol.
22, no.
1.A.
Budanitsky and G. Hirst .
2001.
Semantic distance inWordNet: An experimental, application-orientedevaluation of five measures.
In Workshop on Word-Net and Other Lexical Resources,  NAACL.J.
Cohen and E. Cohen.
1998.
The Big Lebowski.
Film.Columbia TriStar Pictures.M.
Fleischman, E. Hovy, and A. Echihabi.
2003.
Off-line Strategies for Online Question Answering: An-swering Questions Before They Are Asked.
ACL,Sapporo, Japan.G.
S. Mann, David Yarowsky, 2003 Unsupervised Per-sonal Name Disambiguation, CoNLL, Edmonton,Canada.C.
Manning and H. Schutze.
2001 Foundations of Sta-tistical Natural Language Processing.
MIT Press,Cambridge, Ma.A.P.
Martinich, ed.
2000.
The Philosophy of Language,Oxford University Press, Oxford, UK.T.
Mitchell.
1997.
Machine Learning.
McGraw-HillInternational Editions, New York, NY.
Pgs.
143-145.V.
Ng and C. Cardie Improving Machine Learning Ap-proaches to Coreference Resolution.
ACL, 2002.F.J.
Och.
2002.
Yet another maxent toolkit: YASMET.www-i6.informatik.rwth-aachen.de/Colleagues/och/.T.
Pedersen.
2004.
WordNet::Similarity v0.07 .http://www.d.umn.edu/~tpederse/similarity.html.Appendix A.
Sample Cluster Output.Cluster 1time pop star Cluster 1 (cont)the singer former rock staronetime singer Cluster 2former singer Lawmakerpop singer crooning lawmakerformer entertainer mayoral candidateformer pop star republican politicianformer pop singer congressmanentertainer Cluster 3onetime beau A freshman republicanSingerTable 1.
Output clusters for the name Sonny Bono.Cluster 1platinum recording artist Cluster 2 (cont)cbs records artist rockerartist american pop superstarCluster 2 visiting idolsinger idolpop idol pop music superstarday pop superstar package entertainerinternational pop star another entertainerstarring singer american pop singeramerican singer Cluster 3rock superstar local talk radio personalitysuing pop superstar kabc radio talk show hostpop superstar los angeles radio personalityenigmatic pop superstar veteran kabc radio talk show hostfeaturing pop star ubiquitous radio commentatorembattled pop star radio broadcastercontroversial pop star broadcasterincluding singer Cluster 4featuring singer authoreven singer british beer gurusigning pop performer beer expertpop singer Cluster 5surrounding entertainer mannequin collectorjoining entertainer Cluster 6including entertainer kfor commandersinging superstar the commander of kforincluding superstar commander of kforamerican superstar british commandersuperstar Cluster 7ailing superstar the nato commander of    the kosovo liberation forcereuter pop superstar Cluster 8reclusive pop superstar Designerquiet pop superstar Cluster 9embattled pop superstar deputy secretary    of transportationalleging pop superstar deputy secretary of the    department of transportationmusic superstar Cluster 10the us pop star historianrock star Cluster 11pop star education department spokesmanentertainer company spokesmanpop recording star dow corning spokesmannewlywed pop star Cluster 12fellow pop star judgethe singer Cluster 13superstar singer receiversetting singer career browns receiverrock singer trading receiversurrounding pop singer ravens receiversuing pop singer baltimore wide receiverreuter pop singer agent wide receiverprague pop singer wide receiverpop singer Cluster 14rock sensation baylor offensive tacklemusic sensation Cluster 15pop sensation beer writerTable 2.
Output clusters for the name Michael Jackson
